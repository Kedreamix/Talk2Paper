<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-19  CoFi A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement   Membrane Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0480971fd3e9425b1778d2a6a9c613cb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-30
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    85 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-19-更新"><a href="#2025-08-19-更新" class="headerlink" title="2025-08-19 更新"></a>2025-08-19 更新</h1><h2 id="CoFi-A-Fast-Coarse-to-Fine-Few-Shot-Pipeline-for-Glomerular-Basement-Membrane-Segmentation"><a href="#CoFi-A-Fast-Coarse-to-Fine-Few-Shot-Pipeline-for-Glomerular-Basement-Membrane-Segmentation" class="headerlink" title="CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement   Membrane Segmentation"></a>CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement   Membrane Segmentation</h2><p><strong>Authors:Hongjin Fang, Daniel Reisenbüchler, Kenji Ikemura, Mert R. Sabuncu, Yihe Yang, Ruining Deng</strong></p>
<p>Accurate segmentation of the glomerular basement membrane (GBM) in electron microscopy (EM) images is fundamental for quantifying membrane thickness and supporting the diagnosis of various kidney diseases. While supervised deep learning approaches achieve high segmentation accuracy, their reliance on extensive pixel-level annotation renders them impractical for clinical workflows. Few-shot learning can reduce this annotation burden but often struggles to capture the fine structural details necessary for GBM analysis. In this study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot segmentation pipeline designed for GBM delineation in EM images. CoFi first trains a lightweight neural network using only three annotated images to produce an initial coarse segmentation mask. This mask is then automatically processed to generate high-quality point prompts with morphology-aware pruning, which are subsequently used to guide SAM in refining the segmentation. The proposed method achieved exceptional GBM segmentation performance, with a Dice coefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that CoFi not only alleviates the annotation and computational burdens associated with conventional methods, but also achieves accurate and reliable segmentation results. The pipeline’s speed and annotation efficiency make it well-suited for research and hold strong potential for clinical applications in renal pathology. The pipeline is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ddrrnn123/CoFi">https://github.com/ddrrnn123/CoFi</a>. </p>
<blockquote>
<p>对电子显微镜（EM）图像中的肾小球基底膜（GBM）进行精确分割是量化膜厚度和支持各种肾脏疾病诊断的基础。虽然监督深度学习的方法可以实现高分割精度，但它们对大量像素级注释的依赖使它们不适用于临床工作流程。小样本学习可以减少注释工作量，但往往难以捕获用于GBM分析所需的结构细节。在这项研究中，我们介绍了CoFi，这是一个快速有效的从粗糙到精细的小样本分割管道，旨在用于EM图像中GBM的轮廓描绘。CoFi首先使用仅三个注释的图像训练一个轻量级神经网络，以产生初始的粗略分割掩膜。然后，该掩膜被自动处理以产生高质量的点提示，通过形态感知修剪，随后用于指导SAM进行分割细化。所提出的方法实现了出色的GBM分割性能，Dice系数为74.54%，推理速度为每秒1.9帧。我们证明了CoFi不仅减轻了与传统方法相关的注释和计算负担，而且实现了准确可靠的分割结果。该管道的速度和注释效率使其成为肾脏病理学研究和临床应用的强大潜力工具。管道可在以下网址公开访问：<a target="_blank" rel="noopener" href="https://github.com/ddrrnn123/CoFi">https://github.com/ddrrnn123/CoFi</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11469v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为CoFi的快速、高效的粗到细少数镜头分割管道，用于电子显微镜图像中的肾小球基底膜（GBM）分割。该方法使用仅三个标注图像训练轻量级神经网络，生成初始粗略分割掩膜，再通过形态感知修剪生成高质量点提示，引导SAM进行精细分割。该方法实现了GBM分割的高性能，Dice系数为74.54%，推理速度为1.9 FPS。CoFi不仅减轻了传统方法的标注和计算负担，还实现了准确可靠的分割结果，适合在肾病理学中研究和临床应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoFi是一种针对电子显微镜图像中肾小球基底膜（GBM）分割的粗到细少数镜头分割方法。</li>
<li>仅需三个标注图像，轻量级神经网络即可生成初始粗略分割掩膜。</li>
<li>自动处理初始掩膜以生成高质量点提示，通过形态感知修剪提高分割质量。</li>
<li>CoFi使用SAM进行精细分割，实现高性能的GBM分割。</li>
<li>CoFi的Dice系数为74.54%，推理速度为1.9 FPS。</li>
<li>CoFi减轻了传统GBM分割方法的标注和计算负担。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11469">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c16009aa1defb25c79478f432f0dd19d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7591720e15f7cd6fc4fdaeb93b6064fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a51a9aedb2d40f49612501d9a6cf0557.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e2173017a1258f89349dcb24a017666.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f691cd7b3e59b9a8b3d9622d8ab2084e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c21b8ecb170e1a7d4ebb8a5f71ca5090.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ed9d13b57d0d4b89ab5e64b7afc8189.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71d22e53ff8ac15be450774aad733899.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Group-Fairness-Meets-the-Black-Box-Enabling-Fair-Algorithms-on-Closed-LLMs-via-Post-Processing"><a href="#Group-Fairness-Meets-the-Black-Box-Enabling-Fair-Algorithms-on-Closed-LLMs-via-Post-Processing" class="headerlink" title="Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed   LLMs via Post-Processing"></a>Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed   LLMs via Post-Processing</h2><p><strong>Authors:Ruicheng Xian, Yuxuan Wan, Han Zhao</strong></p>
<p>Instruction fine-tuned large language models (LLMs) enable a simple zero-shot or few-shot prompting paradigm, also known as in-context learning, for building prediction models. This convenience, combined with continued advances in LLM capability, has the potential to drive their adoption across a broad range of domains, including high-stakes applications where group fairness – preventing disparate impacts across demographic groups – is essential. The majority of existing approaches to enforcing group fairness on LLM-based classifiers rely on traditional fair algorithms applied via model fine-tuning or head-tuning on final-layer embeddings, but they are no longer applicable to closed-weight LLMs under the in-context learning setting, which include some of the most capable commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we propose a framework for deriving fair classifiers from closed-weight LLMs via prompting: the LLM is treated as a feature extractor, and features are elicited from its probabilistic predictions (e.g., token log probabilities) using prompts strategically designed for the specified fairness criterion to obtain sufficient statistics for fair classification; a fair algorithm is then applied to these features to train a lightweight fair classifier in a post-hoc manner. Experiments on five datasets, including three tabular ones, demonstrate strong accuracy-fairness tradeoffs for the classifiers derived by our framework from both open-weight and closed-weight LLMs; in particular, our framework is data-efficient and outperforms fair classifiers trained on LLM embeddings (i.e., head-tuning) or from scratch on raw tabular features. </p>
<blockquote>
<p>指令微调的大型语言模型（LLM）能够实现简单的零样本或少样本提示范式，也称为上下文学习，用于构建预测模型。这种便利结合LLM能力的持续进步，有可能推动其在广泛领域的应用，包括高风险应用，其中群体公平性——防止对不同群体的不同影响——至关重要。现有大多数基于LLM的分类器实施群体公平性的方法都依赖于通过模型微调或头调在最终层嵌入上应用传统的公平算法，但它们不再适用于采用上下文学习设置的固定权重LLM，其中包括目前一些功能最强大的商业模型，如GPT-4、双子座和Claude。在本文中，我们提出了一个通过提示从固定权重LLM派生公平分类器的框架：将LLM视为特征提取器，并使用针对特定公平性标准战略设计的提示来激发其概率预测（例如，令牌对数概率），以获得公平分类的充足统计数据；然后对这些特征应用公平算法，以事后方式训练轻量级公平分类器。在五个数据集上的实验，包括三个表格数据集，证明了我们的框架从公开权重和固定权重LLM派生的分类器在准确性公平性方面的强大权衡；特别是我们的框架数据效率较高，优于在LLM嵌入上训练的公平分类器（即头调）或从原始表格特征开始训练的公平分类器。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11258v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在零样本或少量样本提示框架下，大型语言模型（LLM）可以通过上下文学习进行预测模型构建。随着LLM能力的不断提升，其在多个领域的应用潜力日益显现，特别是在高风险的公平性要求严格的领域。然而，现有的基于LLM的分类器实现群体公平的方法主要依赖于传统的公平算法，这些算法不适用于封闭的LLM权重模型。针对这一问题，本文提出了一种基于提示的公平分类器派生框架，将LLM视为特征提取器，并使用针对特定公平标准设计的提示从概率预测中提取特征，以获得用于公平分类的充足统计数据；然后，在这些特征上应用公平算法以进行事后轻量级公平分类器的训练。实验结果表明，该框架在开放和封闭权重LLM上生成的分类器具有较强的精度公平权衡。特别是在数据效率方面，该框架优于基于LLM嵌入训练的公平分类器或从头开始在原始表格特征上训练的公平分类器。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）能够通过上下文学习实现预测模型构建，这种方法的便捷性有望推动其在多个领域的广泛应用。</li>
<li>在高风险且要求公平的领域中，对公平性维护的需求尤为关键。</li>
<li>当前在封闭权重的大型语言模型上实现群体公平的方法面临挑战。</li>
<li>本文提出了一种基于提示的公平分类器派生框架，适用于封闭权重的大型语言模型。</li>
<li>该框架将LLM视为特征提取器，并使用针对特定公平标准设计的提示来提取特征并进行分类。</li>
<li>实验结果表明该框架具有良好的精度和公平性权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e32332491511e7511d0f624631ed12d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e7d4dde3b350ada0b657aa3ff662c62.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-VLM-Fine-tuning-via-Latent-Hierarchical-Adapter-Learning"><a href="#Fine-Grained-VLM-Fine-tuning-via-Latent-Hierarchical-Adapter-Learning" class="headerlink" title="Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning"></a>Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning</h2><p><strong>Authors:Yumiao Zhao, Bo Jiang, Yuhe Ding, Xiao Wang, Jin Tang, Bin Luo</strong></p>
<p>Adapter-based approaches have garnered attention for fine-tuning pre-trained Vision-Language Models (VLMs) on few-shot classification tasks. These methods strive to develop a lightweight module that better aligns visual and (category) textual representations, thereby enhancing performance on downstream few-shot learning tasks. However, existing adapters generally learn&#x2F;align (category) textual-visual modalities via explicit spatial proximity in the underlying embedding space, which i) fails to capture the inherent one-to-many associations between categories and image samples and ii) struggles to establish accurate associations between the unknown categories and images. To address these issues, inspired by recent works on hyperbolic learning, we develop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs on downstream few-shot classification tasks. The core of LatHAdapter is to exploit the latent semantic hierarchy of downstream training data and employ it to provide richer, fine-grained guidance for the adapter learning process. Specifically, LatHAdapter first introduces some learnable &#96;attribute’ prompts as the bridge to align categories and images. Then, it projects the categories, attribute prompts, and images within each batch in a hyperbolic space, and employs hierarchical regularization to learn the latent semantic hierarchy of them, thereby fully modeling the inherent one-to-many associations among categories, learnable attributes, and image samples. Extensive experiments on four challenging few-shot tasks show that the proposed LatHAdapter consistently outperforms many other fine-tuning approaches, particularly in adapting known classes and generalizing to unknown classes. </p>
<blockquote>
<p>基于适配器的方法已经引起了在少量样本分类任务上微调预训练好的视觉语言模型（VLMs）的关注。这些方法致力于开发一个轻量级的模块，以更好地对齐视觉和（类别）文本表示，从而提高下游少量样本学习任务上的性能。然而，现有的适配器通常通过底层嵌入空间中的明确空间邻近关系来学习&#x2F;对齐（类别）文本视觉模式，这i）无法捕获类别和图像样本之间固有的多对一关联，并且ii）在建立未知类别和图像之间的准确关联方面表现困难。为了解决这些问题，我们受到最近关于双曲学习的研究的启发，开发了一种用于下游少量样本分类任务上微调VLMs的新型潜在层次适配器（LatHAdapter）。LatHAdapter的核心是利用下游训练数据的潜在语义层次结构，并对其进行利用，为适配器的学习过程提供更丰富、更精细的指导。具体来说，LatHAdapter首先引入一些可学习的“属性”提示作为桥梁来对齐类别和图像。然后，它在双曲空间中投影每个批次中的类别、属性提示和图像，并采用层次正则化来学习它们的潜在语义层次结构，从而充分建模类别、可学习属性和图像样本之间的固有的一对多关联。在四个具有挑战性的少量样本任务上的大量实验表明，所提出的LatHAdapter在许多其他微调方法上表现更优，特别是在适应已知类和泛化到未知类上。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11176v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注于利用基于适配器的策略对预训练的视觉语言模型进行微调，以解决少量样本分类任务。文中指出现有适配器通过明确的空间邻近在底层嵌入空间对齐文本视觉模式的方法存在局限性。为此，作者受超球面学习的启发，提出了新型的潜在层次适配器（LatHAdapter）。它通过发掘下游训练数据的潜在语义层次，为适配器的学习过程提供更丰富的精细指导。LatHAdapter通过引入可学习的属性提示来对齐类别和图像，并在超球面空间中投影它们，利用层次正则化来学习潜在语义层次结构。实验表明，LatHAdapter在多个具有挑战性的少量样本任务上表现优异，尤其在适应已知类和泛化到未知类上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>适配器方法被用于微调预训练的视觉语言模型，以应对少量样本分类任务。</li>
<li>现有适配器主要通过明确的空间邻近在嵌入空间中对齐文本和视觉模式，但这种方法存在局限性。</li>
<li>LatHAdapter受超球面学习启发，能发掘下游训练数据的潜在语义层次。</li>
<li>LatHAdapter通过引入可学习的属性提示，对齐类别和图像。</li>
<li>在超球面空间中投影类别、属性提示和图像，并利用层次正则化学习它们的潜在语义结构。</li>
<li>LatHAdapter能充分建模类别、学习属性和图像样本之间的固有的一对多关联。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11176">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eb31b32390fc716efd866e568867b0c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87c4de5c04b2b2bcaddfe515950057ff.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Are-Large-Pre-trained-Vision-Language-Models-Effective-Construction-Safety-Inspectors"><a href="#Are-Large-Pre-trained-Vision-Language-Models-Effective-Construction-Safety-Inspectors" class="headerlink" title="Are Large Pre-trained Vision Language Models Effective Construction   Safety Inspectors?"></a>Are Large Pre-trained Vision Language Models Effective Construction   Safety Inspectors?</h2><p><strong>Authors:Xuezheng Chen, Zhengbo Zou</strong></p>
<p>Construction safety inspections typically involve a human inspector identifying safety concerns on-site. With the rise of powerful Vision Language Models (VLMs), researchers are exploring their use for tasks such as detecting safety rule violations from on-site images. However, there is a lack of open datasets to comprehensively evaluate and further fine-tune VLMs in construction safety inspection. Current applications of VLMs use small, supervised datasets, limiting their applicability in tasks they are not directly trained for. In this paper, we propose the ConstructionSite 10k, featuring 10,000 construction site images with annotations for three inter-connected tasks, including image captioning, safety rule violation visual question answering (VQA), and construction element visual grounding. Our subsequent evaluation of current state-of-the-art large pre-trained VLMs shows notable generalization abilities in zero-shot and few-shot settings, while additional training is needed to make them applicable to actual construction sites. This dataset allows researchers to train and evaluate their own VLMs with new architectures and techniques, providing a valuable benchmark for construction safety inspection. </p>
<blockquote>
<p>施工安全检测通常涉及人工检测人员在现场识别安全隐患。随着强大的视觉语言模型（VLMs）的兴起，研究人员正在探索其用于检测现场图像中的安全违规等任务。然而，缺乏开放数据集来全面评估和调整施工安全检查中的VLMs。目前VLMs的应用使用的是小规模、有监督的数据集，限制了其在非直接训练任务中的应用。在本文中，我们提出了“ConstructionSite 10k”数据集，包含1万张施工现场图像，针对三个相互关联的任务进行标注，包括图像描述、安全违规视觉问答（VQA）和施工元素视觉定位。我们对当前最先进的预训练大型VLMs的后续评估显示，它们在零样本和少样本环境中具有显著的泛化能力，但需要额外的训练才能适用于实际施工现场。该数据集允许研究人员使用新的架构和技术训练和评估他们自己的VLMs，为施工安全检测提供了一个有价值的基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11011v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于强大的视觉语言模型（VLMs）的崛起，研究人员开始探索其在建筑安全检测领域的应用，如从现场图像中检测安全违规。然而，缺乏开放数据集以全面评估和在建筑安全检测中进一步微调VLMs。本文提出了“ConstructionSite 10k”数据集，包含1万张建筑工地图像，针对三项互联任务进行标注，包括图像描述、安全违规视觉问答（VQA）和建筑元素视觉定位。对当前最先进的预训练VLMs的评估表明，它们在零样本和少样本环境中具有显著的泛化能力，但需要额外的训练才能适用于实际建筑工地。此数据集为研究人员使用新架构和技术训练和评估自己的VLMs提供了宝贵的基准测试，为建筑安全检测领域的发展提供了重要支持。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs正在被探索用于建筑安全检测任务，如从现场图像中检测安全违规。</li>
<li>缺乏开放数据集以全面评估和在建筑安全检测中微调VLMs。</li>
<li>“ConstructionSite 10k”数据集包含1万张建筑工地图像，针对三项任务进行标注。</li>
<li>当前先进的预训练VLMs在零样本和少样本环境中具有泛化能力。</li>
<li>需要额外的训练才能使VLMs适用于实际建筑工地环境。</li>
<li>此数据集为研究人员提供了训练和评估VLMs的基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11011">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9388e7e63c6aa4a69e86e6b79b71e3be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ef3afb5b314bc3bb4a0fd6ced858dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69ed00533d9bcf145f834f543e9a9a48.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Rule2Text-A-Framework-for-Generating-and-Evaluating-Natural-Language-Explanations-of-Knowledge-Graph-Rules"><a href="#Rule2Text-A-Framework-for-Generating-and-Evaluating-Natural-Language-Explanations-of-Knowledge-Graph-Rules" class="headerlink" title="Rule2Text: A Framework for Generating and Evaluating Natural Language   Explanations of Knowledge Graph Rules"></a>Rule2Text: A Framework for Generating and Evaluating Natural Language   Explanations of Knowledge Graph Rules</h2><p><strong>Authors:Nasim Shirvani-Mahdavi, Chengkai Li</strong></p>
<p>Knowledge graphs (KGs) can be enhanced through rule mining; however, the resulting logical rules are often difficult for humans to interpret due to their inherent complexity and the idiosyncratic labeling conventions of individual KGs. This work presents Rule2Text, a comprehensive framework that leverages large language models (LLMs) to generate natural language explanations for mined logical rules, thereby improving KG accessibility and usability. We conduct extensive experiments using multiple datasets, including Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically evaluate several LLMs across a comprehensive range of prompting strategies, including zero-shot, few-shot, variable type incorporation, and Chain-of-Thought reasoning. To systematically assess models’ performance, we conduct a human evaluation of generated explanations on correctness and clarity. To address evaluation scalability, we develop and validate an LLM-as-a-judge framework that demonstrates strong agreement with human evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge, and human-in-the-loop feedback, we construct high-quality ground truth datasets, which we use to fine-tune the open-source Zephyr model. Our results demonstrate significant improvements in explanation quality after fine-tuning, with particularly strong gains in the domain-specific dataset. Additionally, we integrate a type inference module to support KGs lacking explicit type information. All code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/idirlab/KGRule2NL">https://github.com/idirlab/KGRule2NL</a>. </p>
<blockquote>
<p>知识图谱（KG）可以通过规则挖掘进行增强；然而，由于逻辑规则的内在复杂性以及各个知识图谱特有的标签约定，所得的逻辑规则往往难以被人类解读。本研究提出了Rule2Text，这是一个全面的框架，它利用大型语言模型（LLM）来生成对挖掘到的逻辑规则的自然语言解释，从而提高知识图谱的可用性和可访问性。我们使用了多个数据集进行了广泛实验，包括Freebase的各种变体（FB-CVT-REV、FB+CVT-REV和FB15k-237），以及ogbl-biokg数据集，并使用AMIE 3.5.1进行规则挖掘。我们系统地评估了几种LLM，涵盖了多种提示策略，包括零样本、少样本、变量类型融合和链式思维推理。为了系统地评估模型的表现，我们对生成的解释进行了正确性和清晰度的人工评估。为了解决评估的可扩展性问题，我们开发并验证了一个LLM法官框架，该框架与人类评估者之间表现出了强烈的共识。利用表现最佳的模型（Gemini 2.0 Flash）、LLM法官和人类实时反馈，我们构建了高质量的真实数据集，用于微调开源的Zephyr模型。我们的结果表明，在微调后，解释质量得到了显著提高，特别是在特定领域的数据集上获得了显著的收益。此外，我们集成了一个类型推断模块，以支持缺乏显式类型信息的知识图谱。所有代码和数据均可在<a target="_blank" rel="noopener" href="https://github.com/idirlab/KGRule2NL">https://github.com/idirlab/KGRule2NL</a>公开访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10971v1">PDF</a> arXiv admin note: text overlap with arXiv:2507.23740</p>
<p><strong>摘要</strong></p>
<p>知识图谱可以通过规则挖掘进行增强，但结果逻辑规则对人类来说难以解读，因为它们的复杂性以及各个知识图谱特有的标签约定。本研究提出Rule2Text框架，利用大型语言模型生成自然语言的解释，为挖掘的逻辑规则提供说明，从而提高知识图谱的可用性和可访问性。我们在多个数据集上进行广泛实验，包括Freebase变体以及ogbl-biokg数据集，并使用AMIE 3.5.1挖掘规则。我们系统地评估了多种语言模型在各种提示策略下的表现，包括零样本、少样本、变量类型融入和链式思维推理。为了系统地评估模型性能，我们对生成的解释进行了正确性评估清晰度的人类评估。为了解决评估的可扩展性问题，我们开发并验证了一个语言模型作为法官的框架，与人类评估者之间达成强一致性。利用表现最佳的模型（Gemini 2.0 Flash）、语言模型法官和人类循环反馈，我们构建了高质量的真实数据集，用于微调开源Zephyr模型。结果显示，在微调后，解释质量显著提高，特别是在特定领域的数据集上取得了显著的进步。此外，我们集成了一个类型推断模块，以支持缺乏显式类型信息的知识图谱。所有代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/idirlab/KGRule2NL">链接</a>找到。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>利用大型语言模型（LLMs）生成自然语言的解释，以提高知识图谱（KGs）的易用性和可访问性。</li>
<li>在多种数据集上进行广泛实验，包括Freebase变体以及ogbl-biokg数据集。</li>
<li>系统地评估了多种语言模型在各种提示策略下的表现。</li>
<li>开发了语言模型作为法官的框架，以进行模型性能评估并验证与人类评估者的一致性。</li>
<li>利用最佳模型构建高质量真实数据集，用于微调开源Zephyr模型。</li>
<li>在微调后显著提高了解释质量，特别是在特定领域的数据集上取得了显著进步。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10971">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4f47d0417d1e6759374ad65e0ef683be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-477ef50f38ec1216a956d18a47339ca1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f0c5f446e58daef6e1071e40e1eaa1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f251463bb43ff2d5eb2d851ba404d25c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="An-Efficient-Model-Driven-Groupwise-Approach-for-Atlas-Construction"><a href="#An-Efficient-Model-Driven-Groupwise-Approach-for-Atlas-Construction" class="headerlink" title="An Efficient Model-Driven Groupwise Approach for Atlas Construction"></a>An Efficient Model-Driven Groupwise Approach for Atlas Construction</h2><p><strong>Authors:Ziwei Zou, Bei Zou, Xiaoyan Kui, Wenqi Lu, Haoran Dou, Arezoo Zakeri, Timothy Cootes, Alejandro F Frangi, Jinming Duan</strong></p>
<p>Atlas construction is fundamental to medical image analysis, offering a standardized spatial reference for tasks such as population-level anatomical modeling. While data-driven registration methods have recently shown promise in pairwise settings, their reliance on large training datasets, limited generalizability, and lack of true inference phases in groupwise contexts hinder their practical use. In contrast, model-driven methods offer training-free, theoretically grounded, and data-efficient alternatives, though they often face scalability and optimization challenges when applied to large 3D datasets. In this work, we introduce DARC (Diffeomorphic Atlas Registration via Coordinate descent), a novel model-driven groupwise registration framework for atlas construction. DARC supports a broad range of image dissimilarity metrics and efficiently handles arbitrary numbers of 3D images without incurring GPU memory issues. Through a coordinate descent strategy and a centrality-enforcing activation function, DARC produces unbiased, diffeomorphic atlases with high anatomical fidelity. Beyond atlas construction, we demonstrate two key applications: (1) One-shot segmentation, where labels annotated only on the atlas are propagated to subjects via inverse deformations, outperforming state-of-the-art few-shot methods; and (2) shape synthesis, where new anatomical variants are generated by warping the atlas mesh using synthesized diffeomorphic deformation fields. Overall, DARC offers a flexible, generalizable, and resource-efficient framework for atlas construction and applications. </p>
<blockquote>
<p>医学图像分析中，图谱构建是一项基础工作，它为群体层面的解剖学建模等任务提供了一个标准化的空间参考。虽然数据驱动型的注册方法在近期的配对设置中显示出了一定的潜力，但它们对大型训练数据集的依赖、有限的通用性以及群组环境下的真实推理阶段的缺失，阻碍了它们的实际应用。相比之下，模型驱动的方法提供了免训练、理论扎实和数据高效的选择，但在应用于大型3D数据集时，它们常常面临可扩展性和优化挑战。在这项工作中，我们介绍了DARC（基于坐标下降的微分同胚图谱注册），这是一种用于图谱构建的新型模型驱动群组注册框架。DARC支持广泛的图像不相似度度量，并能高效地处理任意数量的3D图像，而不会引发GPU内存问题。通过坐标下降策略和中心强制激活函数，DARC产生无偏见、微分同胚的图谱，具有高解剖保真度。除了图谱构建之外，我们还展示了两个关键应用：（1）单次分割，其中仅在图谱上标注的标签通过反向变形传播到主体上，超越了最新的少镜头方法；（2）形状合成，通过利用合成的微分同胚变形场对图谱网格进行变形，生成新的解剖变体。总体而言，DARC为图谱构建和应用提供了一个灵活、通用和资源高效的框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10743v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了DARC（基于坐标下降的微分形态学图谱注册方法），这是一种用于图谱构建的新型模型驱动群组注册框架。DARC支持广泛的图像差异度量，并能有效地处理任意数量的3D图像，而不会导致GPU内存问题。通过坐标下降策略和中心强制激活函数，DARC能够产生具有高精度解剖结构的不偏不倚的微分图谱。此外，DARC还支持图谱构建之外的两个关键应用：一是单张图像分割，即仅通过图谱进行标注标签的传播到目标图像，超越了最先进的少量样本方法；二是形状合成，通过合成微分形态学变形场扭曲图谱网格生成新的解剖变体。总的来说，DARC提供了一个灵活、通用且资源高效的图谱构建与应用框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DARC是一种用于图谱构建的模型驱动群组注册框架，适用于任意数量的3D图像。</li>
<li>DARC支持广泛的图像差异度量，提高了注册的准确性和鲁棒性。</li>
<li>通过坐标下降策略和中心强制激活函数，DARC能生成不偏不倚、具有高解剖真实性的微分图谱。</li>
<li>DARC在图谱构建应用上表现出优秀的性能，包括单张图像分割和形状合成。</li>
<li>单张图像分割应用实现了仅通过图谱进行标注标签的传播到目标图像，性能超越现有少量样本方法。</li>
<li>形状合成应用通过扭曲图谱网格生成新的解剖变体，展示了DARC的创造性和灵活性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10743">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e3bd8c9ad4accf949235cf23748e9dff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0480971fd3e9425b1778d2a6a9c613cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30db0f15b0b5e73d21a94bcfc2c00412.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ca3a1ca760ff6dedf0a8daf6e8263b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0d50cd237d5f3bd21c53f8700189d08.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SemPT-Semantic-Prompt-Tuning-for-Vision-Language-Models"><a href="#SemPT-Semantic-Prompt-Tuning-for-Vision-Language-Models" class="headerlink" title="SemPT: Semantic Prompt Tuning for Vision-Language Models"></a>SemPT: Semantic Prompt Tuning for Vision-Language Models</h2><p><strong>Authors:Xiao Shi, Yangjun Ou, Zhenzhong Chen</strong></p>
<p>Visual transfer learning for unseen categories presents an active research topic yet a challenging task, due to the inherent conflict between preserving category-specific representations and acquiring transferable knowledge. Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs offer a promising solution. However, existing prompt tuning methods rely on sparse category labels or disparate LLM-generated descriptions, which fragment knowledge representation and hinder transferability. To address this limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that tackles the generalization challenge by leveraging shared attribute-level knowledge across categories. Specifically, SemPT adopts a two-step prompting strategy to guide LLM in extracting shared visual attributes and generating attribute-level descriptions, capturing transferable semantic cues beyond labels while ensuring coherent structure. Then, visually guided weighting is applied to the embeddings of attribute-level descriptions to reduce noise from irrelevant attributes and enhance the text embeddings. Additionally, image embeddings are jointly aligned with both label and attribute-enhanced text embeddings, balancing discrimination for seen categories and transferability to unseen ones. Considering the availability of category exposure, our inference dynamically selects between standard label embeddings for seen categories and attribute-enhanced embeddings for unseen ones to ensure effective adaptation. Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves state-of-the-art performance across various settings, including base-to-novel generalization, cross-dataset transfer, cross-domain transfer, and few-shot learning. </p>
<blockquote>
<p>视觉迁移学习对于未见类别是一个热门且具挑战性的课题，因为保持特定类别的表示和获取可迁移知识之间存在内在冲突。预训练在大量图像文本对上的视觉语言模型（VLMs）提供了一个有前景的解决方案。然而，现有的提示调整方法依赖于稀疏的类别标签或分散的LLM生成描述，这导致知识表示碎片化并阻碍知识的迁移。为了解决这一局限性，我们引入了语义提示调整（SemPT）这一新框架，通过利用类别间的共享属性级知识来解决泛化挑战。具体来说，SemPT采用两步提示策略来指导LLM提取共享的视觉属性并生成属性级描述，捕捉超越标签的可转移语义线索，同时确保连贯的结构。然后，对属性级描述的嵌入应用视觉引导加权，以减少来自无关属性的噪声并增强文本嵌入。此外，图像嵌入与标签和属性增强文本嵌入共同对齐，平衡对可见类别的区分度和对未见类别的可迁移性。考虑到类别的可见性，我们的推理过程会动态选择在可见类别和不可见类别之间使用标准的标签嵌入和属性增强嵌入，以确保有效的适应。在15个基准数据集上的广泛实验表明，SemPT在各种设置下均达到了最先进的性能，包括从基础到新颖的泛化、跨数据集迁移、跨域迁移和小样本学习。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10645v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>视觉迁移学习对于未见类别是一个活跃的研究课题，但也是一个具有挑战性的任务。由于保持特定类别的表示和获取可迁移知识之间的内在冲突，使得该任务更具挑战性。预训练在大量图像文本对上的视觉语言模型（VLMs）提供了一个有前景的解决方案。然而，现有的提示调整方法依赖于稀疏的类别标签或分散的LLM生成描述，这破坏了知识表示并阻碍了可迁移性。为了解决这一局限性，我们引入了语义提示调整（SemPT）这一新框架，通过利用类别之间的共享属性级知识来解决泛化挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉迁移学习对于未见类别是一个具有挑战性的任务，因为需要平衡保持类别特异性表示和获取可迁移知识。</li>
<li>视觉语言模型（VLMs）在解决这一挑战方面提供了有前景的解决方案。</li>
<li>现有提示调整方法存在局限性，依赖于稀疏类别标签或LLM生成的描述，这破坏了知识表示并阻碍了可迁移性。</li>
<li>SemPT框架通过利用类别之间的共享属性级知识来解决泛化挑战。</li>
<li>SemPT采用两步提示策略，引导LLM提取共享视觉属性和生成属性级描述，捕捉可迁移的语义线索。</li>
<li>SemPT通过视觉引导加权减少了来自无关属性的噪声，增强了文本嵌入。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10645">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1616057c449bdf030c62fc7153ded7ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a7e3f73e77e13e56ee03baff6258bc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5722113344929679e94d6ba228691cbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-137d70338d2d00a469864a7d66ec70db.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Increasing-the-Utility-of-Synthetic-Images-through-Chamfer-Guidance"><a href="#Increasing-the-Utility-of-Synthetic-Images-through-Chamfer-Guidance" class="headerlink" title="Increasing the Utility of Synthetic Images through Chamfer Guidance"></a>Increasing the Utility of Synthetic Images through Chamfer Guidance</h2><p><strong>Authors:Nicola Dall’Asen, Xiaofeng Zhang, Reyhane Askari Hemmat, Melissa Hall, Jakob Verbeek, Adriana Romero-Soriano, Michal Drozdzal</strong></p>
<p>Conditional image generative models hold considerable promise to produce infinite amounts of synthetic training data. Yet, recent progress in generation quality has come at the expense of generation diversity, limiting the utility of these models as a source of synthetic training data. Although guidance-based approaches have been introduced to improve the utility of generated data by focusing on quality or diversity, the (implicit or explicit) utility functions oftentimes disregard the potential distribution shift between synthetic and real data. In this work, we introduce Chamfer Guidance: a training-free guidance approach which leverages a handful of real exemplar images to characterize the quality and diversity of synthetic data. We show that by leveraging the proposed Chamfer Guidance, we can boost the diversity of the generations w.r.t. a dataset of real images while maintaining or improving the generation quality on ImageNet-1k and standard geo-diversity benchmarks. Our approach achieves state-of-the-art few-shot performance with as little as 2 exemplar real images, obtaining 96.4% in terms of precision, and 86.4% in terms of distributional coverage, which increase to 97.5% and 92.7%, respectively, when using 32 real images. We showcase the benefits of the Chamfer Guidance generation by training downstream image classifiers on synthetic data, achieving accuracy boost of up to 15% for in-distribution over the baselines, and up to 16% in out-of-distribution. Furthermore, our approach does not require using the unconditional model, and thus obtains a 31% reduction in FLOPs w.r.t. classifier-free-guidance-based approaches at sampling time. </p>
<blockquote>
<p>条件图像生成模型具有产生无限合成训练数据的巨大潜力。然而，生成质量方面的最新进展是以牺牲生成多样性为代价的，这限制了这些模型作为合成训练数据来源的实用性。虽然基于引导的方法已经被引入，通过关注质量或多样性来提高生成数据的实用性，但（隐式或显式）效用函数通常忽略了合成数据和真实数据之间潜在分布差异。在这项工作中，我们引入了Chamfer Guidance：一种无需训练的引导方法，它利用少量真实示例图像来表征合成数据的质量和多样性。我们表明，通过利用提出的Chamfer Guidance，我们可以在针对真实图像数据集生成时提高生成的多样性，同时保持在ImageNet-1k和标准地理多样性基准测试集上生成质量的维持或提高。我们的方法使用仅2张真实示例图像即可实现最先进的性能，在精确度方面达到96.4%，在分布覆盖方面达到86.4%，在使用32张真实图像时，这些数字分别提高到97.5%和92.7%。我们通过使用合成数据训练下游图像分类器来展示Chamfer Guidance生成的优势，与基准相比，在内部分布上提高了高达15%的准确率，在外部分布上提高了高达16%的准确率。此外，我们的方法不需要使用无条件模型，因此在采样时间方面与基于无分类器引导的方法相比，实现了31%的FLOPs减少。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10631v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为Chamfer Guidance的方法，这是一种无需训练的新型指导方法，它通过利用少量真实示例图像来表征合成数据的质量和多样性。该方法能够在提高生成图像多样性的同时，维持或提高生成图像的质量。在ImageNet-1k和地理多样性标准基准测试中，使用仅两个示例真实图像即可实现最先进的性能。此外，本文还展示了Chamfer Guidance在训练下游图像分类器方面的优势，通过合成数据实现了高达15%的准确率提升。此方法无需使用无条件模型，因此在采样时与基于分类器免费的指导方法相比，实现了31%的FLOPs减少。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Chamfer Guidance是一种新型的无训练指导方法，旨在提高合成数据的生成质量和多样性。</li>
<li>该方法利用少量真实示例图像来表征合成数据的质量和多样性。</li>
<li>Chamfer Guidance在ImageNet-1k和地理多样性标准基准测试中实现了先进的性能。</li>
<li>通过在下游图像分类器中使用合成数据，Chamfer Guidance实现了显著的准确率提升。</li>
<li>与其他方法相比，Chamfer Guidance无需使用无条件模型，因此在采样时效率更高。</li>
<li>Chamfer Guidance在采样时实现了显著的FLOPs减少，与基于分类器免费的指导方法相比，减少了31%。</li>
<li>Chamfer Guidance具有广泛的应用前景，特别是在需要高质量合成数据的领域，如图像分类、图像生成等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10631">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-541b5c9f33e6ab89c2ea7af11be954fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae05322b526cb12802dab6e7c2810e03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8af44d93925d383c641ae779c1f45ccd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning"><a href="#Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning" class="headerlink" title="Few-shot Vision-based Human Activity Recognition with MLLM-based Visual   Reinforcement Learning"></a>Few-shot Vision-based Human Activity Recognition with MLLM-based Visual   Reinforcement Learning</h2><p><strong>Authors:Wenqi Zheng, Yutaka Arakawa</strong></p>
<p>Reinforcement learning in large reasoning models enables learning from feedback on their outputs, making it particularly valuable in scenarios where fine-tuning data is limited. However, its application in multi-modal human activity recognition (HAR) domains remains largely underexplored. Our work extends reinforcement learning to the human activity recognition domain with multimodal large language models. By incorporating visual reinforcement learning in the training process, the model’s generalization ability on few-shot recognition can be greatly improved. Additionally, visual reinforcement learning can enhance the model’s reasoning ability and enable explainable analysis in the inference stage. We name our few-shot human activity recognition method with visual reinforcement learning FAVOR. Specifically, our approach first utilizes a multimodal large language model (MLLM) to generate multiple candidate responses for the human activity image, each containing reasoning traces and final answers. These responses are then evaluated using reward functions, and the MLLM model is subsequently optimized using the Group Relative Policy Optimization (GRPO) algorithm. In this way, the MLLM model can be adapted to human activity recognition with only a few samples. Extensive experiments on four human activity recognition datasets and five different settings demonstrate the superiority of the proposed method. </p>
<blockquote>
<p>强化学习在大型推理模型中的应用使得模型能够从其输出的反馈中学习，这在精细调整数据有限的场景中尤其有价值。然而，其在多模态人类活动识别（HAR）领域的应用仍被大大忽视。我们的工作将强化学习扩展到人类活动识别领域，采用多模态大型语言模型。通过在训练过程中融入视觉强化学习，可以极大地提高模型在少量样本识别上的泛化能力。此外，视觉强化学习还可以增强模型的推理能力，并在推理阶段实现可解释性分析。我们将带有视觉强化学习的少量人类活动识别方法命名为FAVOR。具体来说，我们的方法首先利用多模态大型语言模型（MLLM）为人体活动图像生成多个候选响应，每个响应都包含推理轨迹和最终答案。然后使用奖励函数对这些响应进行评估，随后使用群体相对策略优化（GRPO）算法对MLLM模型进行优化。通过这种方式，MLLM模型可以适应仅使用少量样本的人类活动识别。在四个人类活动识别数据集和五种不同设置上的大量实验证明了所提出方法的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10371v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>强化学习可从输出反馈中学习，在数据有限的情况下特别有价值。本文将强化学习扩展到人类活动识别领域，结合多模态大型语言模型，通过视觉强化学习提高模型的泛化能力和推理能力。命名为FAVOR的方法利用多模态大型语言模型生成多个活动图像候选响应，使用奖励函数评估并利用群体相对策略优化算法优化模型。在仅有少量样本的情况下，该模型可适应人类活动识别。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习能够从输出反馈中学习，在数据有限的情况下具有特别价值。</li>
<li>本文将强化学习扩展到人类活动识别领域。</li>
<li>结合多模态大型语言模型，通过视觉强化学习提高模型的泛化能力。</li>
<li>视觉强化学习能够增强模型的推理能力，并在推理阶段实现可解释性分析。</li>
<li>本文提出的少样本人类活动识别方法命名为FAVOR。</li>
<li>FAVOR方法利用多模态大型语言模型生成多个候选响应，并使用奖励函数评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10371">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4d701f72dd0648272bd6ee6fd4bddc5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-286823a3a1d7792f71ef33c09cba0489.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14027dd6d3c9cd50a4dd2533cdc7318b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c938b694e12f24aa0c80ea9ec1ec8671.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ced82577192fe3e31773064b8f6d1bea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-319d03bdfb285d82d7db1da0449814d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-164b5b2e4b9bbe981a46977a09a50972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ceb93b5a52d5b1eea48b1129d771e6f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Improving-Generative-Cross-lingual-Aspect-Based-Sentiment-Analysis-with-Constrained-Decoding"><a href="#Improving-Generative-Cross-lingual-Aspect-Based-Sentiment-Analysis-with-Constrained-Decoding" class="headerlink" title="Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with   Constrained Decoding"></a>Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with   Constrained Decoding</h2><p><strong>Authors:Jakub Šmíd, Pavel Přibáň, Pavel Král</strong></p>
<p>While aspect-based sentiment analysis (ABSA) has made substantial progress, challenges remain for low-resource languages, which are often overlooked in favour of English. Current cross-lingual ABSA approaches focus on limited, less complex tasks and often rely on external translation tools. This paper introduces a novel approach using constrained decoding with sequence-to-sequence models, eliminating the need for unreliable translation tools and improving cross-lingual performance by 5% on average for the most complex task. The proposed method also supports multi-tasking, which enables solving multiple ABSA tasks with a single model, with constrained decoding boosting results by more than 10%.   We evaluate our approach across seven languages and six ABSA tasks, surpassing state-of-the-art methods and setting new benchmarks for previously unexplored tasks. Additionally, we assess large language models (LLMs) in zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in zero-shot and few-shot settings, fine-tuning achieves competitive results compared to smaller multilingual models, albeit at the cost of longer training and inference times.   We provide practical recommendations for real-world applications, enhancing the understanding of cross-lingual ABSA methodologies. This study offers valuable insights into the strengths and limitations of cross-lingual ABSA approaches, advancing the state-of-the-art in this challenging research domain. </p>
<blockquote>
<p>基于方面的情感分析（ABSA）虽然取得了重大进展，但对于资源匮乏的语言来说，仍然面临挑战。通常这些语言会被忽略而优先选择英语。当前的跨语言ABSA方法主要集中在有限且不太复杂的任务上，并且经常依赖外部翻译工具。本文介绍了一种使用序列到序列模型的约束解码的新方法，该方法消除了对不可靠翻译工具的依赖，并在最复杂的任务上平均提高了5%的跨语言性能。所提出的方法还支持多任务处理，能够使用一个模型解决多个ABSA任务，约束解码将结果提高了超过10%。我们对七种语言和六个ABSA任务评估了我们的方法，超越了最新方法并设立了以前未探索任务的新基准。此外，我们还在零样本、少样本和微调情况下评估了大型语言模型（LLMs）。虽然LLMs在零样本和少样本设置中的表现不佳，但微调实现了与较小的多语言模型相当的结果，尽管需要更长的训练和推理时间。我们为实际应用提供了实际建议，增强了跨语言ABSA方法的理解。本研究深入探讨了跨语言ABSA方法的优缺点，为该挑战领域的研究提供了先进的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10369v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>基于序列到序列模型的约束解码新方法，无需依赖不可靠的翻译工具，提高了跨语言方面的情感分析性能。该方法支持多任务处理，并在七种语言和六个情感分析任务上超越现有方法，设置新的基准。同时评估了大型语言模型在不同场景下的表现，提供实际应用的建议。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>引入了一种新型约束解码方法，适用于跨语言的方面情感分析（ABSA）。</li>
<li>方法无需依赖外部翻译工具，提高了跨语言性能。</li>
<li>支持多任务处理，能在多个ABSA任务上取得良好表现。</li>
<li>在七个语言和六个ABSA任务上的性能超越现有方法。</li>
<li>大型语言模型（LLMs）在零样本、少样本场景下的表现不佳，但在微调后结果具有竞争力。</li>
<li>提供了关于跨语言ABSA方法在实际应用中的建议。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10369">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0052cb3d101f80c7e6e8c47e0ab582d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-879c31ddf7338f94910d013bde346192.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning"><a href="#SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning" class="headerlink" title="SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic   Representation Learning"></a>SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic   Representation Learning</h2><p><strong>Authors:Weijian Mai, Jiamin Wu, Yu Zhu, Zhouheng Yao, Dongzhan Zhou, Andrew F. Luo, Qihao Zheng, Wanli Ouyang, Chunfeng Song</strong></p>
<p>Deciphering how visual stimuli are transformed into cortical responses is a fundamental challenge in computational neuroscience. This visual-to-neural mapping is inherently a one-to-many relationship, as identical visual inputs reliably evoke variable hemodynamic responses across trials, contexts, and subjects. However, existing deterministic methods struggle to simultaneously model this biological variability while capturing the underlying functional consistency that encodes stimulus information. To address these limitations, we propose SynBrain, a generative framework that simulates the transformation from visual semantics to neural responses in a probabilistic and biologically interpretable manner. SynBrain introduces two key components: (i) BrainVAE models neural representations as continuous probability distributions via probabilistic learning while maintaining functional consistency through visual semantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantic transmission pathway, projecting visual semantics into the neural response manifold to facilitate high-fidelity fMRI synthesis. Experimental results demonstrate that SynBrain surpasses state-of-the-art methods in subject-specific visual-to-fMRI encoding performance. Furthermore, SynBrain adapts efficiently to new subjects with few-shot data and synthesizes high-quality fMRI signals that are effective in improving data-limited fMRI-to-image decoding performance. Beyond that, SynBrain reveals functional consistency across trials and subjects, with synthesized signals capturing interpretable patterns shaped by biological neural variability. The code will be made publicly available. </p>
<blockquote>
<p>解析视觉刺激如何转化为皮层反应是计算神经科学中的一项基本挑战。这种视觉到神经的映射本质上是一种一对应多的关系，因为相同的视觉输入在不同的试验、情境和受试者中可靠地引发了可变的血流动力学反应。然而，现有的确定性方法很难同时模拟这种生物变异性和编码刺激信息的潜在功能一致性。为了解决这个问题，我们提出了SynBrain，这是一个生成性框架，以概率和生物学上可解释的方式模拟从视觉语义到神经反应的转变。SynBrain有两个关键组成部分：（i）BrainVAE通过概率学习将神经表征建模为连续概率分布，同时通过视觉语义约束保持功能一致性；（ii）语义到神经映射器充当语义传输途径，将视觉语义投射到神经响应流形，以促进高保真度fMRI合成。实验结果表明，SynBrain在特定受试者的视觉到fMRI编码性能方面超越了最新方法。此外，SynBrain能够高效适应新的受试者并处理少量数据，合成高质量的fMRI信号，有效提高数据有限的fMRI到图像解码性能。除此之外，SynBrain揭示了试验和受试者之间的功能一致性，合成的信号捕捉到了由生物神经变异性塑造的可解释模式。代码将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10298v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本文提出一种名为SynBrain的生成框架，以概率和可生物解读的方式模拟视觉语义到神经响应的转化。该框架包括BrainVAE和语义到神经映射器两个关键组件，能够在保持功能一致性的同时模拟神经表示的连续概率分布，并投影视觉语义到神经响应流形以合成高保真fMRI信号。实验结果显示，SynBrain在特定的视觉到fMRI编码性能上超越了现有方法，并能有效地适应新的受试者进行少样本数据合成高质量fMRI信号。此外，SynBrain揭示了试验和受试者之间的功能一致性，合成的信号捕捉到了由生物神经变异塑造的可解释模式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynBrain是一个生成框架，用于模拟视觉语义到神经响应的转化，以概率和可生物解读的方式呈现这一转化过程。</li>
<li>该框架包括BrainVAE和语义到神经映射器两个关键组件，其中BrainVAE能够模拟神经表示的连续概率分布，语义到神经映射器则扮演语义传输通道的角色。</li>
<li>SynBrain能够在保持功能一致性的同时模拟生物变异。</li>
<li>实验证明，SynBrain在视觉到fMRI编码性能上超越现有方法，尤其在新受试者的少样本数据上表现出高效的适应性。</li>
<li>SynBrain能合成高质量的fMRI信号，并用于提高数据有限时的fMRI到图像解码性能。</li>
<li>SynBrain揭示了试验和受试者之间的功能一致性，展示其在实际应用中的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10298">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-99a65c101276668bb9411cbe68683803.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ee7ddb89a786e4b1d7882ab2d8e09da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2d9b676ccf8eccaae4ff150a7c1b174.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d029bae6dd5ee7aeb397324e42c124b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Deep-Learning-for-Crack-Detection-A-Review-of-Learning-Paradigms-Generalizability-and-Datasets"><a href="#Deep-Learning-for-Crack-Detection-A-Review-of-Learning-Paradigms-Generalizability-and-Datasets" class="headerlink" title="Deep Learning for Crack Detection: A Review of Learning Paradigms,   Generalizability, and Datasets"></a>Deep Learning for Crack Detection: A Review of Learning Paradigms,   Generalizability, and Datasets</h2><p><strong>Authors:Xinan Zhang, Haolin Wang, Yung-An Hsieh, Zhongyu Yang, Anthony Yezzi, Yi-Chang Tsai</strong></p>
<p>Crack detection plays a crucial role in civil infrastructures, including inspection of pavements, buildings, etc., and deep learning has significantly advanced this field in recent years. While numerous technical and review papers exist in this domain, emerging trends are reshaping the landscape. These shifts include transitions in learning paradigms (from fully supervised learning to semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation and fine-tuning foundation models), improvements in generalizability (from single-dataset performance to cross-dataset evaluation), and diversification in dataset reacquisition (from RGB images to specialized sensor-based data). In this review, we systematically analyze these trends and highlight representative works. Additionally, we introduce a new dataset collected with 3D laser scans, 3DCrack, to support future research and conduct extensive benchmarking experiments to establish baselines for commonly used deep learning methodologies, including recent foundation models. Our findings provide insights into the evolving methodologies and future directions in deep learning-based crack detection. Project page: <a target="_blank" rel="noopener" href="https://github.com/nantonzhang/Awesome-Crack-Detection">https://github.com/nantonzhang/Awesome-Crack-Detection</a> </p>
<blockquote>
<p>裂缝检测在民事基础设施中发挥着至关重要的作用，包括路面、建筑等的检测，并且近年来深度学习已在此领域取得了重大进展。尽管该领域存在大量的技术和综述性论文，但新兴趋势正在改变这一领域的格局。这些变化包括学习范式（从全监督学习转变到半监督、弱监督、无监督、小样本、域适应和微调基础模型）、通用性的提高（从单一数据集性能到跨数据集评估），以及数据集重新采集的多样化（从RGB图像到基于专业传感器的数据）。在这篇综述中，我们系统地分析了这些趋势，并重点介绍了具有代表性的作品。此外，我们还介绍了使用3D激光扫描收集的新数据集“3DCrack”，以支持未来的研究，并对常用的深度学习方法进行广泛的基准测试，包括最近的基础模型。我们的研究结果提供了对不断发展的方法和未来深度学习裂缝检测方向的新见解。项目页面：<a target="_blank" rel="noopener" href="https://github.com/nantonzhang/Awesome-Crack-Detection">https://github.com/nantonzhang/Awesome-Crack-Detection</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10256v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>深度学习方法在裂缝检测领域已发挥重要作用，近期出现众多趋势变化，包括学习范式转变、泛化性能提升和多样化数据集采集等。本文系统性分析这些趋势并介绍代表性工作，同时推出新数据集支持未来研究，并开展基准实验评估常用深度学习方法。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>深度学习方法在裂缝检测领域具有关键作用，涉及道路、建筑等基础设施检测。</li>
<li>学习范式转变，包括从全监督学习到多种衍生形式。</li>
<li>泛化性能提升，不仅局限于单一数据集表现。</li>
<li>数据集采集方式趋向多样化，引入特殊传感器数据。</li>
<li>推出基于三维激光扫描的新数据集3DCrack，支持未来研究。</li>
<li>基准实验评估常见深度学习方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10256">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-02fa315276506fa9b6891490ba772a26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-477b5f821b568aedf7731a91c6334aa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4d570672b8f5db97a94976dead4504d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8c6adf44ca36c20c2fcdd0aae31cd27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5be5415f943b66740546ec48f095744.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8300b4475656e6ae627d34c794e112bf.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Stochastic-based-Patch-Filtering-for-Few-Shot-Learning"><a href="#Stochastic-based-Patch-Filtering-for-Few-Shot-Learning" class="headerlink" title="Stochastic-based Patch Filtering for Few-Shot Learning"></a>Stochastic-based Patch Filtering for Few-Shot Learning</h2><p><strong>Authors:Javier Rodenas, Eduardo Aguilar, Petia Radeva</strong></p>
<p>Food images present unique challenges for few-shot learning models due to their visual complexity and variability. For instance, a pasta dish might appear with various garnishes on different plates and in diverse lighting conditions and camera perspectives. This problem leads to losing focus on the most important elements when comparing the query with support images, resulting in misclassification. To address this issue, we propose Stochastic-based Patch Filtering for Few-Shot Learning (SPFF) to attend to the patch embeddings that show greater correlation with the class representation. The key concept of SPFF involves the stochastic filtering of patch embeddings, where patches less similar to the class-aware embedding are more likely to be discarded. With patch embedding filtered according to the probability of appearance, we use a similarity matrix that quantifies the relationship between the query image and its respective support images. Through a qualitative analysis, we demonstrate that SPFF effectively focuses on patches where class-specific food features are most prominent while successfully filtering out non-relevant patches. We validate our approach through extensive experiments on few-shot classification benchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing SoA methods. </p>
<blockquote>
<p>食品图像为小样学习模型带来了独特的挑战，因为它们的视觉复杂性和可变性。例如，一道意大利面可能会出现在不同的盘子上，有着不同的配料，并且在不同的照明条件和相机视角下呈现。这个问题导致在查询与支持图像进行比较时失去对最重要元素的关注，从而导致误分类。为了解决这一问题，我们提出了基于随机性的小样学习补丁过滤（SPFF）方法，以关注与类别表示相关性更大的补丁嵌入。SPFF的关键概念涉及补丁嵌入的随机过滤，其中与类别感知嵌入不太相似的补丁更有可能被丢弃。根据出现概率过滤补丁嵌入后，我们使用相似度矩阵来量化查询图像与其相应的支持图像之间的关系。通过定性分析，我们证明SPFF可以有效地关注食品特征最突出的补丁，同时成功过滤掉不相关的补丁。我们通过大量实验在少量样本分类基准上验证了我们的方法：Food-101、VireoFood-172和UECFood-256，性能优于现有的最先进方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10066v1">PDF</a> CVPR Workshop MetaFood 2025</p>
<p><strong>Summary</strong></p>
<p>该文本指出食品图像对少样本学习模型构成独特挑战，因图像视觉复杂多变。如面食料理在不同盘子上展示各种配菜，照明条件与相机视角各异，导致在查询与样本图像比较时容易失去对重点元素的关注，从而产生误判。为解决此问题，本文提出基于随机性的少样本学习（SPFF）方法，重点关注与类别表征更相关的补丁嵌入。SPFF的关键概念在于随机过滤补丁嵌入，对于与类别感知嵌入差异较大的补丁更容易被舍弃。经过出现的概率筛选的补丁嵌入数据，利用相似性矩阵量化查询图像与对应样本图像之间的关系。定性分析表明，SPFF能够有效聚焦于类特定食品特征最突出的补丁，同时成功过滤掉非相关补丁。通过广泛的实验验证，在少样本分类基准数据集Food-101、VireoFood-172和UECFood-256上的表现优于现有最先进的模型。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是文本的关键见解要点：</p>
<ol>
<li>食品图像对少样本学习模型提出了独特挑战，因其视觉上的复杂性和变化多样性。</li>
<li>不同的面食料理在不同盘子上呈现多种配菜，照明和相机视角的变化导致识别困难。</li>
<li>为解决误判问题，提出了基于随机性的少样本学习（SPFF）方法。</li>
<li>SPFF通过随机过滤补丁嵌入数据来关注与类别表征更相关的部分。</li>
<li>SPFF通过相似性矩阵量化查询图像与样本图像之间的关系。</li>
<li>定性分析证明了SPFF能够有效识别食品的关键特征并过滤掉非相关元素。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10066">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-82d0beddc412344b502e5406070a6fe1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-880641f592ff64877d4f00fc34e195fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2036d058792151053501a8deceecc79b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c43375d7c296c1983e9edecce39e8e3f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MOC-Meta-Optimized-Classifier-for-Few-Shot-Whole-Slide-Image-Classification"><a href="#MOC-Meta-Optimized-Classifier-for-Few-Shot-Whole-Slide-Image-Classification" class="headerlink" title="MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image   Classification"></a>MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image   Classification</h2><p><strong>Authors:Tianqi Xiang, Yi Li, Qixiang Zhang, Xiaomeng Li</strong></p>
<p>Recent advances in histopathology vision-language foundation models (VLFMs) have shown promise in addressing data scarcity for whole slide image (WSI) classification via zero-shot adaptation. However, these methods remain outperformed by conventional multiple instance learning (MIL) approaches trained on large datasets, motivating recent efforts to enhance VLFM-based WSI classification through fewshot learning paradigms. While existing few-shot methods improve diagnostic accuracy with limited annotations, their reliance on conventional classifier designs introduces critical vulnerabilities to data scarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC) comprising two core components: (1) a meta-learner that automatically optimizes a classifier configuration from a mixture of candidate classifiers and (2) a classifier bank housing diverse candidate classifiers to enable a holistic pathological interpretation. Extensive experiments demonstrate that MOC outperforms prior arts in multiple few-shot benchmarks. Notably, on the TCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art few-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions, offering a critical advancement for clinical deployments where diagnostic training data is severely limited. Code is available at <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/MOC">https://github.com/xmed-lab/MOC</a>. </p>
<blockquote>
<p>近期，组织病理学视觉语言基础模型（VLFMs）的进步在通过零样本适应解决全幻灯片图像（WSI）分类的数据稀缺问题上表现出了前景。然而，这些方法仍被在大型数据集上训练的常规多实例学习（MIL）方法所超越，这促使最近的努力通过小样学习模式增强基于VLFM的WSI分类。虽然现有的小样方法可以在有限的注释下提高诊断准确性，但它们对常规分类器设计的依赖引发了数据稀缺的关键漏洞。为了解决这个问题，我们提出了一种由两个核心组件组成的元优化分类器（MOC）：（1）元学习者会自动优化来自混合候选分类器的分类器配置；（2）一个包含多种候选分类器的分类器库，以实现全面的病理解释。大量实验表明，MOC在多个小样基准测试中优于以前的技术。值得注意的是，在TCGA-NSCLC基准测试中，MOC比最新的基于小样的VLFM方法提高了10.4%的AUC，在单样本条件下提高幅度高达26.25%，这对于诊断训练数据严重受限的临床部署而言是一个重要的进步。代码可在<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/MOC%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xmed-lab/MOC找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09967v1">PDF</a> Accepted in MICCAI 2025</p>
<p><strong>Summary</strong><br>     针对组织病理学图像分类任务中数据稀缺的问题，最近的基于视觉语言基础模型的零样本适应策略虽然展现出了潜力，但仍无法匹敌传统的多任务实例学习（MIL）方法。为了改进基于视觉语言基础模型的WSI分类，研究者提出了通过小样本学习模式来增强性能的策略。现有的小样本方法虽然能在有限标注的情况下提高诊断准确性，但它们依赖于传统分类器的设计，在数据稀缺的情况下存在重大漏洞。为了解决这个问题，提出了包含两个核心组件的元优化分类器（MOC）：（1）一个元学习者，可以从候选分类器的组合中自动优化分类器配置；（2）一个分类器库，包含各种候选分类器以实现全面的病理解释。实验表明，MOC在多个小样本测试中超过了先前的技术。在TCGA-NSCLC测试中，MOC相较于最新的基于小样本视觉语言基础模型的方法提高了AUC值10.4%，在单样本条件下提升甚至高达26.25%，这对于诊断训练数据严重受限的临床部署环境来说是一个重大进步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言基础模型在处理组织病理学图像分类时面临数据稀缺的挑战。</li>
<li>元优化分类器（MOC）由两个核心组件组成：元学习器和分类器库。</li>
<li>元学习者能够自动从候选分类器的组合中优化分类器配置。</li>
<li>分类器库包含多种候选分类器，以实现全面的病理解释。</li>
<li>MOC在多个小样本测试中表现优异，相较于现有技术有显著的提升。</li>
<li>在TCGA-NSCLC测试中，MOC相较于基于小样本视觉语言基础模型的方法提高了诊断准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09967">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0b06b099d6b1e2a111a926a96b5d0d9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf7b35f5ed0d248da2801e0b3f8c4ebc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-866131f8f226f5087ba6a80863bd6679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f822e446b36d9d8df550a8d63232427f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DSS-Prompt-Dynamic-Static-Synergistic-Prompting-for-Few-Shot-Class-Incremental-Learning"><a href="#DSS-Prompt-Dynamic-Static-Synergistic-Prompting-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot   Class-Incremental Learning"></a>DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot   Class-Incremental Learning</h2><p><strong>Authors:Linpu He, Yanan Li, Bingze Li, Elvis Han Cui, Donghui Wang</strong></p>
<p>Learning from large-scale pre-trained models with strong generalization ability has shown remarkable success in a wide range of downstream tasks recently, but it is still underexplored in the challenging few-shot class-incremental learning (FSCIL) task. It aims to continually learn new concepts from limited training samples without forgetting the old ones at the same time. In this paper, we introduce DSS-Prompt, a simple yet effective approach that transforms the pre-trained Vision Transformer with minimal modifications in the way of prompts into a strong FSCIL classifier. Concretely, we synergistically utilize two complementary types of prompts in each Transformer block: static prompts to bridge the domain gap between the pre-training and downstream datasets, thus enabling better adaption; and dynamic prompts to capture instance-aware semantics, thus enabling easy transfer from base to novel classes. Specially, to generate dynamic prompts, we leverage a pre-trained multi-modal model to extract input-related diverse semantics, thereby generating complementary input-aware prompts, and then adaptively adjust their importance across different layers. In this way, on top of the prompted visual embeddings, a simple prototype classifier can beat state-of-the-arts without further training on the incremental tasks. We conduct extensive experiments on four benchmarks to validate the effectiveness of our DSS-Prompt and show that it consistently achieves better performance than existing approaches on all datasets and can alleviate the catastrophic forgetting issue as well. </p>
<blockquote>
<p>从具有强大泛化能力的大规模预训练模型中学习，最近在广泛的下游任务中取得了显著的成功，但在具有挑战性的少样本类增量学习（FSCIL）任务中仍然探索不足。其目标是从有限的训练样本中持续学习新概念，同时不忘记旧知识。在本文中，我们介绍了DSS-Prompt，这是一种简单而有效的方法，它通过最小化的提示方式将预训练的视觉转换器转变为强大的FSCIL分类器。具体来说，我们协同利用Transformer块中的两种互补提示：静态提示，以缩小预训练和下游数据集之间的域差距，从而实现更好的适应；动态提示，以捕获实例感知语义，从而实现从基础类别到新颖类别的轻松迁移。特别是，为了生成动态提示，我们利用预训练的多模态模型来提取与输入相关的多样语义，从而产生互补的输入感知提示，然后自适应地调整它们在不同层中的重要性。通过这种方式，基于提示的视觉嵌入，一个简单的原型分类器可以在无需对增量任务进行进一步训练的情况下超越现有技术。我们在四个基准上进行了大量实验，验证了DSS-Prompt的有效性，并表明它在所有数据集上始终实现了比现有方法更好的性能，并可以缓解灾难性遗忘问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09785v1">PDF</a> Accepted to ACMMM 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在具有挑战性的少样本类增量学习（FSCIL）任务中，利用大型预训练模型进行学习的方法。通过引入DSS-Prompt技术，将预训练的视觉转换器进行微调，转化为强大的FSCIL分类器。该技术利用静态提示和动态提示，缩小了预训练和下游数据集之间的领域差距，并捕捉实例感知语义，从而实现了从基础类别到新颖类别的轻松迁移。通过预训练的多模态模型生成动态提示，并自适应调整其在不同层的重要性。实验证明，DSS-Prompt在四个基准测试上的表现均优于现有方法，并能有效缓解灾难性遗忘问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DSS-Prompt技术将预训练的视觉转换器转化为少样本类增量学习（FSCIL）分类器。</li>
<li>DSS-Prompt结合了静态提示和动态提示来适应FSCIL任务。</li>
<li>静态提示缩小了预训练和下游数据集之间的领域差距。</li>
<li>动态提示通过捕捉实例感知语义，实现了从基础类别到新颖类别的迁移。</li>
<li>利用预训练的多模态模型生成动态提示，增强了模型的适应性。</li>
<li>DSS-Prompt在多个基准测试上的表现优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09785">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-23e17bb1974619be7b4e59ce5f8aa86e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4392d6a946e05eed2f7c1c401b45cb93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-84996b577390ef714d83db092a04de8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5dcd1900d2ce5ecd184266e8b3349dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a07a24d7cbe5292a03b6f667d982d7d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-888a77e0270999da81cef26a5754532b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-027920dc4f40478a969149e9e5140d68.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Slot-Attention-based-Feature-Filtering-for-Few-Shot-Learning"><a href="#Slot-Attention-based-Feature-Filtering-for-Few-Shot-Learning" class="headerlink" title="Slot Attention-based Feature Filtering for Few-Shot Learning"></a>Slot Attention-based Feature Filtering for Few-Shot Learning</h2><p><strong>Authors:Javier Rodenas, Eduardo Aguilar, Petia Radeva</strong></p>
<p>Irrelevant features can significantly degrade few-shot learn ing performance. This problem is used to match queries and support images based on meaningful similarities despite the limited data. However, in this process, non-relevant fea tures such as background elements can easily lead to confu sion and misclassification. To address this issue, we pro pose Slot Attention-based Feature Filtering for Few-Shot Learning (SAFF) that leverages slot attention mechanisms to discriminate and filter weak features, thereby improving few-shot classification performance. The key innovation of SAFF lies in its integration of slot attention with patch em beddings, unifying class-aware slots into a single attention mechanism to filter irrelevant features effectively. We intro duce a similarity matrix that computes across support and query images to quantify the relevance of filtered embed dings for classification. Through experiments, we demon strate that Slot Attention performs better than other atten tion mechanisms, capturing discriminative features while reducing irrelevant information. We validate our approach through extensive experiments on few-shot learning bench marks: CIFAR-FS, FC100, miniImageNet and tieredIma geNet, outperforming several state-of-the-art methods. </p>
<blockquote>
<p>不相关的特征可能会显著影响小样本学习的性能。此问题的解决方案是用于在有限数据的基础上，根据有意义的相似性来匹配查询图像和支持图像。然而，在此过程中，不相关的特征（如背景元素）很容易引起混淆和误分类。为了解决这一问题，我们提出了基于插槽注意力的小样本学习特征过滤（SAFF），它利用插槽注意力机制来区分和过滤弱特征，从而提高小样本分类性能。SAFF的关键创新之处在于它将插槽注意力与补丁嵌入相结合，将类感知插槽统一到一个单一的注意力机制中，有效地过滤掉不相关的特征。我们引入了一个相似度矩阵，该矩阵跨支持图像和查询图像进行计算，以量化过滤嵌入物在分类中的相关性。通过实验，我们证明了插槽注意力比其他注意力机制表现更好，能够捕捉区分特征同时减少无关信息。我们在小样本学习的基准测试上进行了广泛的实验验证，包括CIFAR-FS、FC100、miniImageNet和tieredImageNet，我们的方法优于几种最新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09699v1">PDF</a> CVPR Workshop LatinX 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了少样本学习中的特征选择问题，指出无关特征会严重影响模型性能。为此，提出了基于插槽注意力的特征过滤方法（SAFF），通过结合插槽注意力和补丁嵌入，有效过滤掉不相关特征，提高少样本分类性能。实验证明，SAFF在多个少样本学习基准测试集上表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>无关特征对少样本学习性能有重要影响。</li>
<li>提出基于插槽注意力的特征过滤方法（SAFF）以改善少样本分类性能。</li>
<li>SAFF通过结合插槽注意力和补丁嵌入，统一类感知插槽，有效过滤掉不相关特征。</li>
<li>引入相似度矩阵来计算支持图像和查询图像之间的相关性，以量化过滤嵌入的分类重要性。</li>
<li>插槽注意力机制在捕获判别特征的同时减少了无关信息。</li>
<li>在多个少样本学习基准测试集上，SAFF表现优于其他最先进的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09699">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4d6851298b16eb3cbe8659010533bd52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa2d14040fb4c157fd83161fd0a29eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77414d83b68a566ffec1a31d1bde70d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa190912fc79db5e6548a22c1335c1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d814c8a789f8c6cd29f50f7fff524c2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Leveraging-Failed-Samples-A-Few-Shot-and-Training-Free-Framework-for-Generalized-Deepfake-Detection"><a href="#Leveraging-Failed-Samples-A-Few-Shot-and-Training-Free-Framework-for-Generalized-Deepfake-Detection" class="headerlink" title="Leveraging Failed Samples: A Few-Shot and Training-Free Framework for   Generalized Deepfake Detection"></a>Leveraging Failed Samples: A Few-Shot and Training-Free Framework for   Generalized Deepfake Detection</h2><p><strong>Authors:Shibo Yao, Renshuai Tao, Xiaolong Zheng, Chao Liang, Chunjie Zhang</strong></p>
<p>Recent deepfake detection studies often treat unseen sample detection as a <code>zero-shot&quot; task, training on images generated by known models but generalizing to unknown ones. A key real-world challenge arises when a model performs poorly on unknown samples, yet these samples remain available for analysis. This highlights that it should be approached as a </code>few-shot” task, where effectively utilizing a small number of samples can lead to significant improvement. Unlike typical few-shot tasks focused on semantic understanding, deepfake detection prioritizes image realism, which closely mirrors real-world distributions. In this work, we propose the Few-shot Training-free Network (FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet differs from traditional methods that rely on large-scale known data for training. Instead, FTNet uses only one fake samplefrom an evaluation set, mimicking the scenario where new samples emerge in the real world and can be gathered for use, without any training or parameter updates. During evaluation, each test sample is compared to the known fake and real samples, and it is classified based on the category of the nearest sample. We conduct a comprehensive analysis of AI-generated images from 29 different generative models and achieve a new SoTA performance, with an average improvement of 8.7% compared to existing methods. This work introduces a fresh perspective on real-world deepfake detection: when the model struggles to generalize on a few-shot sample, leveraging the failed samples leads to better performance. </p>
<blockquote>
<p>最近关于深度伪造检测的研究通常将未见样本检测视为“零样本”任务，通过在已知模型生成的图像上进行训练，然后推广到未知模型。当模型在未知样本上表现不佳时，现实世界的关键挑战就会出现，但这些样本仍然可用于分析。这强调应该将其视为一个“小样本”任务，有效利用少量样本可以导致显著改进。与关注语义理解的典型小样本任务不同，深度伪造检测更侧重于图像的真实性，这紧密地反映了现实世界的分布。在这项工作中，我们提出了用于现实世界中深度伪造检测的小样本无训练网络（FTNet）。FTNet简单有效，不同于传统方法依赖于大规模已知数据进行训练。相反，FTNet仅使用评估集中的一个伪造样本，模仿现实中新样本的出现并可以收集使用的情况，无需任何训练或参数更新。在评估过程中，每个测试样本都与已知的伪造和真实样本进行比较，并基于最接近的样本类别进行分类。我们对29种不同生成模型生成的AI图像进行了综合分析，达到了最新的技术水平，平均而言，与现有方法相比提高了8.7%。这项工作为现实世界的深度伪造检测提供了新的视角：当模型在少量样本上难以推广时，利用失败的样本会导致更好的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09475v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出将深度伪造检测视为一种“少样本”任务，当模型在未知样本上表现不佳时，可以利用少量样本进行有效改善。针对现实世界的深度伪造检测问题，本文提出了一种无需训练的少样本网络（FTNet），该网络仅需使用评价集中的单个伪造样本即可实现高效的检测。通过对来自29种不同生成模型的AI生成图像进行全面分析，FTNet实现了先进性能，平均比现有方法提高了8.7%。这表明在实际场景中遇到新的挑战时，应灵活运用样本以增强模型性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>最近深度伪造检测研究倾向于将未见样本检测视为“零样本”任务，但实际应用中面临未知样本性能不佳的问题。</li>
<li>当模型在未知样本上表现不佳时，应将其视为“少样本”任务，利用少量样本提升性能。</li>
<li>FTNet网络被提出用于现实世界的少样本深度伪造检测，无需大规模已知数据进行训练。</li>
<li>FTNet仅需评价集中的单个伪造样本进行分类比较。它模仿现实中新样本的出现并能即时收集利用的情况，无需任何额外的训练或参数更新。</li>
<li>FTNet实现了全面分析AI生成图像的性能，并显著提高现有方法的平均性能达8.7%。这表明有效利用失败样本能够提高模型性能。</li>
<li>本文提出了一种全新的视角看待现实世界的深度伪造检测问题，强调了在实际场景中遇到新的挑战时灵活使用样本的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09475">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6c87c5bf639b23e22561373475ebb393.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a24d069e89ad366f8c74a51ae58e78d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22321da32255cc4c08aeaa9424606bc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2041f2e7f484907ce8dba6736ad395c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a70cee19c166b8355609001277fa22a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9589099d6543e838a53419588cdba26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-211ce55a91cb12154f6287c125ca9f48.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Explainable-Sentiment-Analysis-with-DeepSeek-R1-Performance-Efficiency-and-Few-Shot-Learning"><a href="#Explainable-Sentiment-Analysis-with-DeepSeek-R1-Performance-Efficiency-and-Few-Shot-Learning" class="headerlink" title="Explainable Sentiment Analysis with DeepSeek-R1: Performance,   Efficiency, and Few-Shot Learning"></a>Explainable Sentiment Analysis with DeepSeek-R1: Performance,   Efficiency, and Few-Shot Learning</h2><p><strong>Authors:Donghao Huang, Zhaoxia Wang</strong></p>
<p>Large language models (LLMs) have transformed sentiment analysis, yet balancing accuracy, efficiency, and explainability remains a critical challenge. This study presents the first comprehensive evaluation of DeepSeek-R1–an open-source reasoning model–against OpenAI’s GPT-4o and GPT-4o-mini. We test the full 671B model and its distilled variants, systematically documenting few-shot learning curves. Our experiments show DeepSeek-R1 achieves a 91.39% F1 score on 5-class sentiment and 99.31% accuracy on binary tasks with just 5 shots, an eightfold improvement in few-shot efficiency over GPT-4o. Architecture-specific distillation effects emerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant by 6.69 percentage points. While its reasoning process reduces throughput, DeepSeek-R1 offers superior explainability via transparent, step-by-step traces, establishing it as a powerful, interpretable open-source alternative. </p>
<blockquote>
<p>大型语言模型（LLM）已经改变了情感分析领域，但在平衡准确性、效率和可解释性方面仍然是一个关键挑战。本研究首次对DeepSeek-R1这一开源推理模型进行全面评估，并与OpenAI的GPT-4o和GPT-4o-mini进行对比。我们测试了完整的671B模型及其蒸馏变体，系统地记录了小样本学习曲线。实验表明，DeepSeek-R1在5类情感分析上达到91.39%的F1分数，在二元任务上达到99.31%的准确率，仅需要5个样本，相比于GPT-4o，其在小样本效率上提高了八倍。出现特定架构的蒸馏效应，其中基于32B Qwen2.5的模型优于基于70B Llama的变体，高出6.69个百分点。虽然其推理过程降低了吞吐量，但DeepSeek-R1通过透明、分步的跟踪提供了卓越的可解释性，使其成为强大、可解释的开源替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11655v3">PDF</a> 10 pages, 2 figures, 6 tables, revised and re-submitted to an IEEE   journal</p>
<p><strong>Summary</strong></p>
<p>DeepSeek-R1模型在情感分析领域表现卓越，与OpenAI的GPT-4o系列模型相比，其在少量样本学习方面展现出显著优势。本研究全面评估了DeepSeek-R1的性能，发现其在5类情感分析的F1分数上达到91.39%，在二元任务上的准确率高达99.31%，且仅需少量样本。此外，DeepSeek-R1具有优越的可解释性，且作为开源模型，具有强大的竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1在情感分析领域表现出卓越性能，特别是在少量样本学习方面。</li>
<li>与GPT-4o系列模型相比，DeepSeek-R1在F1分数和准确率方面表现出显著优势。</li>
<li>DeepSeek-R1的可解释性强大，通过透明的步骤追踪，使用户能够理解其推理过程。</li>
<li>架构特定的蒸馏效果在DeepSeek-R1中显现，其中基于Qwen2.5的模型性能优于基于Llama的模型。</li>
<li>尽管推理过程可能会降低吞吐量，但DeepSeek-R1仍具有高效的性能。</li>
<li>DeepSeek-R1是一个强大的开源模型，提供了可替代大型语言模型的另一种选择。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11655">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-460d9c42a1273b1e8ff53103c50bf2c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0d8464ab5d8fe50885dbbd5dc203001.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d36e984e7d14a64ee3e71bacf3f18e1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7590fb4d39c0493b7347574098649fa9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c64596a47d3fd0db43bfa85906188e9e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Leveraging-Audio-and-Text-Modalities-in-Mental-Health-A-Study-of-LLMs-Performance"><a href="#Leveraging-Audio-and-Text-Modalities-in-Mental-Health-A-Study-of-LLMs-Performance" class="headerlink" title="Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs   Performance"></a>Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs   Performance</h2><p><strong>Authors:Abdelrahman A. Ali, Aya E. Fouda, Radwa J. Hanafy, Mohammed E. Fouda</strong></p>
<p>Mental health disorders are increasingly prevalent worldwide, creating an urgent need for innovative tools to support early diagnosis and intervention. This study explores the potential of Large Language Models (LLMs) in multimodal mental health diagnostics, specifically for detecting depression and Post Traumatic Stress Disorder through text and audio modalities. Using the E-DAIC dataset, we compare text and audio modalities to investigate whether LLMs can perform equally well or better with audio inputs. We further examine the integration of both modalities to determine if this can enhance diagnostic accuracy, which generally results in improved performance metrics. Our analysis specifically utilizes custom-formulated metrics; Modal Superiority Score and Disagreement Resolvement Score to evaluate how combined modalities influence model performance. The Gemini 1.5 Pro model achieves the highest scores in binary depression classification when using the combined modality, with an F1 score of 0.67 and a Balanced Accuracy (BA) of 77.4%, assessed across the full dataset. These results represent an increase of 3.1% over its performance with the text modality and 2.7% over the audio modality, highlighting the effectiveness of integrating modalities to enhance diagnostic accuracy. Notably, all results are obtained in zero-shot inferring, highlighting the robustness of the models without requiring task-specific fine-tuning. To explore the impact of different configurations on model performance, we conduct binary, severity, and multiclass tasks using both zero-shot and few-shot prompts, examining the effects of prompt variations on performance. The results reveal that models such as Gemini 1.5 Pro in text and audio modalities, and GPT-4o mini in the text modality, often surpass other models in balanced accuracy and F1 scores across multiple tasks. </p>
<blockquote>
<p>精神健康障碍在全球范围内越来越普遍，这迫切需要创新工具来支持早期诊断和治疗。本研究探讨大型语言模型（LLM）在多模式精神健康诊断中的潜力，特别是通过文本和音频模式检测抑郁症和创伤后应激障碍。我们使用E-DAIC数据集，比较文本和音频模式，以研究LLM是否可以通过音频输入实现同等或更好的性能。我们进一步探讨了两种模式的融合，以确定这是否能提高诊断的准确性，从而提高性能指标。我们的分析特别利用定制的指标，即模态优势评分和分歧解决评分，来评估组合模式如何影响模型性能。在二元抑郁症分类中，使用组合模式的Gemini 1.5 Pro模型得分最高，F1分数为0.67，平衡精度（BA）为77.4%，在全数据集上进行了评估。与仅使用文本模式或音频模式相比，这些结果分别提高了3.1%和提高了2.7%，这突显了融合模式以提高诊断准确性的有效性。值得注意的是，所有结果均是在零样本推断中获得的，这表明模型在不需要特定任务微调的情况下具有很强的稳健性。为了探讨不同配置对模型性能的影响，我们使用零样本和少量样本提示进行二元、严重性和多类别任务，并研究提示变化对性能的影响。结果显示，在各种任务中，如文本和音频模式中的Gemini 1.5 Pro以及文本模式中的GPT-4o mini等模型经常在平衡精度和F1分数方面超越其他模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10417v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探索了大型语言模型（LLMs）在多模态心理健康诊断中的潜力，研究通过文本和音频模态检测抑郁症和创伤后应激障碍。研究使用E-DAIC数据集比较文本和音频模态，发现LLMs在音频输入方面表现优异。同时，研究也考察了两种模态的整合是否能提高诊断准确性，结果显示整合后的性能有所提升。其中，Gemini 1.5 Pro模型在二元抑郁症分类中表现最佳，结合模态的F1分数为0.67，平衡准确率为77.4%。值得注意的是，所有结果均为零样本推断，突显了模型的稳健性，无需特定任务微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在多模态心理健康诊断中具有潜力。</li>
<li>通过文本和音频模态检测抑郁症和创伤后应激障碍的研究得到开展。</li>
<li>LLMs在音频输入方面的表现优异。</li>
<li>整合文本和音频模态能提高诊断准确性。</li>
<li>Gemini 1.5 Pro模型在二元抑郁症分类中表现最佳。</li>
<li>模型表现稳健，可在零样本推断下取得良好结果。</li>
<li>不同配置和提示方式对模型性能有影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10417">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8aa8138d29022730cfa5798aae6ee84f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9ab003ab22b64fa810b0915fed799a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b24c30624a7772336ab2be74ca005b67.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Multi-Step-Reasoning-with-Large-Language-Models-a-Survey"><a href="#Multi-Step-Reasoning-with-Large-Language-Models-a-Survey" class="headerlink" title="Multi-Step Reasoning with Large Language Models, a Survey"></a>Multi-Step Reasoning with Large Language Models, a Survey</h2><p><strong>Authors:Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas Back</strong></p>
<p>Language models with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model was not specifically trained for. Traditional models achieve breakthrough performance on language tasks, but do not perform well on basic reasoning benchmarks. However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step reasoning abilities on these benchmarks.   The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems, and has expanded to other tasks in the past few years. This paper reviews the field of multi-step reasoning with LLMs. We propose a taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future.   We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools. Many studies in multi-step methods are using reinforcement learning for finetuning, external optimization loops, in context reinforcement learning, and self-reflection. </p>
<blockquote>
<p>具有数十亿参数的语言模型展现出上下文学习能力，能够在未专门训练的任务上实现少量学习。传统模型在语言任务上取得了突破性进展，但在基本推理基准测试上的表现并不出色。然而，一种新的上下文学习方法——思维链（Chain-of-thought）在这些基准测试中表现出了强大的多步推理能力。关于大型语言模型（LLM）推理能力的研究始于大型语言模型是否能解决小学数学文字题的问题，并在过去几年中扩展到了其他任务。本文综述了大型语言模型的多步推理领域。我们提出了一种分类法，确定了生成、评估和控制多步推理的不同方法。我们对核心方法和开放问题进行了深入探讨，并提出了今后的研究议程。我们发现，多步推理方法已经超越了数学文字题的范围，现在能够成功解决逻辑、组合游戏和机器人技术等方面的挑战，有时是先生成代码，然后交由外部工具执行。多步方法的研究正在使用强化学习进行微调、外部优化循环、上下文强化学习和自我反思等方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11511v2">PDF</a> revised version</p>
<p><strong>Summary</strong><br>     语言模型拥有数十亿参数，展现出上下文学习能力，可在未专门训练的任务中实现少量学习。传统模型在语言任务上表现卓越，但在基本推理基准测试上表现不佳。一种新型的上下文学习法——Chain-of-thought展现出强大的多步骤推理能力。本文回顾了大型语言模型的多步骤推理领域，提出了识别生成、评估和控制多步骤推理的不同方式的新分类法，深入探讨了核心方法和开放问题，并为近期未来提出了研究议程。研究发现，多步骤推理方法已不仅限于解决数学文字问题，还能成功应对逻辑、组合游戏和机器人技术中的挑战，有时通过生成代码并由外部工具执行来完成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言模型具备在无需专门训练的情况下处理新任务的能力，这被称为上下文学习或少量学习。</li>
<li>传统语言模型在基本推理测试上的表现不佳。</li>
<li>Chain-of-thought是一种新型上下文学习方法，具有强大的多步骤推理能力。</li>
<li>多步骤推理方法不仅可以解决数学文字问题，还能成功应对逻辑、组合游戏和机器人技术中的挑战。</li>
<li>多步骤推理方法有时会生成代码并由外部工具执行来完成任务。</li>
<li>目前的研究领域涵盖了多种方法，包括使用强化学习进行微调、外部优化循环、上下文强化学习和自我反思。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.11511">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b9757c0cb9b28ecd5dcda57fe9c8898e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6939a4f1c350b8e1047c459171e1d304.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fbf02856ab90883492ec326e1cae83ba.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-08-19  StyleMM Stylized 3D Morphable Face Model via Text-Driven Aligned Image   Translation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-28bf8a8110a9746a3695944324442f4d.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-08-19  DiCriTest Testing Scenario Generation for Decision-Making Agents   Considering Diversity and Criticality
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
