<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  CoFi A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement   Membrane Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0480971fd3e9425b1778d2a6a9c613cb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-30
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-19-æ›´æ–°"><a href="#2025-08-19-æ›´æ–°" class="headerlink" title="2025-08-19 æ›´æ–°"></a>2025-08-19 æ›´æ–°</h1><h2 id="CoFi-A-Fast-Coarse-to-Fine-Few-Shot-Pipeline-for-Glomerular-Basement-Membrane-Segmentation"><a href="#CoFi-A-Fast-Coarse-to-Fine-Few-Shot-Pipeline-for-Glomerular-Basement-Membrane-Segmentation" class="headerlink" title="CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement   Membrane Segmentation"></a>CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement   Membrane Segmentation</h2><p><strong>Authors:Hongjin Fang, Daniel ReisenbÃ¼chler, Kenji Ikemura, Mert R. Sabuncu, Yihe Yang, Ruining Deng</strong></p>
<p>Accurate segmentation of the glomerular basement membrane (GBM) in electron microscopy (EM) images is fundamental for quantifying membrane thickness and supporting the diagnosis of various kidney diseases. While supervised deep learning approaches achieve high segmentation accuracy, their reliance on extensive pixel-level annotation renders them impractical for clinical workflows. Few-shot learning can reduce this annotation burden but often struggles to capture the fine structural details necessary for GBM analysis. In this study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot segmentation pipeline designed for GBM delineation in EM images. CoFi first trains a lightweight neural network using only three annotated images to produce an initial coarse segmentation mask. This mask is then automatically processed to generate high-quality point prompts with morphology-aware pruning, which are subsequently used to guide SAM in refining the segmentation. The proposed method achieved exceptional GBM segmentation performance, with a Dice coefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that CoFi not only alleviates the annotation and computational burdens associated with conventional methods, but also achieves accurate and reliable segmentation results. The pipelineâ€™s speed and annotation efficiency make it well-suited for research and hold strong potential for clinical applications in renal pathology. The pipeline is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ddrrnn123/CoFi">https://github.com/ddrrnn123/CoFi</a>. </p>
<blockquote>
<p>å¯¹ç”µå­æ˜¾å¾®é•œï¼ˆEMï¼‰å›¾åƒä¸­çš„è‚¾å°çƒåŸºåº•è†œï¼ˆGBMï¼‰è¿›è¡Œç²¾ç¡®åˆ†å‰²æ˜¯é‡åŒ–è†œåšåº¦å’Œæ”¯æŒå„ç§è‚¾è„ç–¾ç—…è¯Šæ–­çš„åŸºç¡€ã€‚è™½ç„¶ç›‘ç£æ·±åº¦å­¦ä¹ çš„æ–¹æ³•å¯ä»¥å®ç°é«˜åˆ†å‰²ç²¾åº¦ï¼Œä½†å®ƒä»¬å¯¹å¤§é‡åƒç´ çº§æ³¨é‡Šçš„ä¾èµ–ä½¿å®ƒä»¬ä¸é€‚ç”¨äºä¸´åºŠå·¥ä½œæµç¨‹ã€‚å°æ ·æœ¬å­¦ä¹ å¯ä»¥å‡å°‘æ³¨é‡Šå·¥ä½œé‡ï¼Œä½†å¾€å¾€éš¾ä»¥æ•è·ç”¨äºGBMåˆ†ææ‰€éœ€çš„ç»“æ„ç»†èŠ‚ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CoFiï¼Œè¿™æ˜¯ä¸€ä¸ªå¿«é€Ÿæœ‰æ•ˆçš„ä»ç²—ç³™åˆ°ç²¾ç»†çš„å°æ ·æœ¬åˆ†å‰²ç®¡é“ï¼Œæ—¨åœ¨ç”¨äºEMå›¾åƒä¸­GBMçš„è½®å»“æç»˜ã€‚CoFié¦–å…ˆä½¿ç”¨ä»…ä¸‰ä¸ªæ³¨é‡Šçš„å›¾åƒè®­ç»ƒä¸€ä¸ªè½»é‡çº§ç¥ç»ç½‘ç»œï¼Œä»¥äº§ç”Ÿåˆå§‹çš„ç²—ç•¥åˆ†å‰²æ©è†œã€‚ç„¶åï¼Œè¯¥æ©è†œè¢«è‡ªåŠ¨å¤„ç†ä»¥äº§ç”Ÿé«˜è´¨é‡çš„ç‚¹æç¤ºï¼Œé€šè¿‡å½¢æ€æ„ŸçŸ¥ä¿®å‰ªï¼Œéšåç”¨äºæŒ‡å¯¼SAMè¿›è¡Œåˆ†å‰²ç»†åŒ–ã€‚æ‰€æå‡ºçš„æ–¹æ³•å®ç°äº†å‡ºè‰²çš„GBMåˆ†å‰²æ€§èƒ½ï¼ŒDiceç³»æ•°ä¸º74.54%ï¼Œæ¨ç†é€Ÿåº¦ä¸ºæ¯ç§’1.9å¸§ã€‚æˆ‘ä»¬è¯æ˜äº†CoFiä¸ä»…å‡è½»äº†ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å…³çš„æ³¨é‡Šå’Œè®¡ç®—è´Ÿæ‹…ï¼Œè€Œä¸”å®ç°äº†å‡†ç¡®å¯é çš„åˆ†å‰²ç»“æœã€‚è¯¥ç®¡é“çš„é€Ÿåº¦å’Œæ³¨é‡Šæ•ˆç‡ä½¿å…¶æˆä¸ºè‚¾è„ç—…ç†å­¦ç ”ç©¶å’Œä¸´åºŠåº”ç”¨çš„å¼ºå¤§æ½œåŠ›å·¥å…·ã€‚ç®¡é“å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/ddrrnn123/CoFi">https://github.com/ddrrnn123/CoFi</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11469v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCoFiçš„å¿«é€Ÿã€é«˜æ•ˆçš„ç²—åˆ°ç»†å°‘æ•°é•œå¤´åˆ†å‰²ç®¡é“ï¼Œç”¨äºç”µå­æ˜¾å¾®é•œå›¾åƒä¸­çš„è‚¾å°çƒåŸºåº•è†œï¼ˆGBMï¼‰åˆ†å‰²ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ä»…ä¸‰ä¸ªæ ‡æ³¨å›¾åƒè®­ç»ƒè½»é‡çº§ç¥ç»ç½‘ç»œï¼Œç”Ÿæˆåˆå§‹ç²—ç•¥åˆ†å‰²æ©è†œï¼Œå†é€šè¿‡å½¢æ€æ„ŸçŸ¥ä¿®å‰ªç”Ÿæˆé«˜è´¨é‡ç‚¹æç¤ºï¼Œå¼•å¯¼SAMè¿›è¡Œç²¾ç»†åˆ†å‰²ã€‚è¯¥æ–¹æ³•å®ç°äº†GBMåˆ†å‰²çš„é«˜æ€§èƒ½ï¼ŒDiceç³»æ•°ä¸º74.54%ï¼Œæ¨ç†é€Ÿåº¦ä¸º1.9 FPSã€‚CoFiä¸ä»…å‡è½»äº†ä¼ ç»Ÿæ–¹æ³•çš„æ ‡æ³¨å’Œè®¡ç®—è´Ÿæ‹…ï¼Œè¿˜å®ç°äº†å‡†ç¡®å¯é çš„åˆ†å‰²ç»“æœï¼Œé€‚åˆåœ¨è‚¾ç—…ç†å­¦ä¸­ç ”ç©¶å’Œä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoFiæ˜¯ä¸€ç§é’ˆå¯¹ç”µå­æ˜¾å¾®é•œå›¾åƒä¸­è‚¾å°çƒåŸºåº•è†œï¼ˆGBMï¼‰åˆ†å‰²çš„ç²—åˆ°ç»†å°‘æ•°é•œå¤´åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>ä»…éœ€ä¸‰ä¸ªæ ‡æ³¨å›¾åƒï¼Œè½»é‡çº§ç¥ç»ç½‘ç»œå³å¯ç”Ÿæˆåˆå§‹ç²—ç•¥åˆ†å‰²æ©è†œã€‚</li>
<li>è‡ªåŠ¨å¤„ç†åˆå§‹æ©è†œä»¥ç”Ÿæˆé«˜è´¨é‡ç‚¹æç¤ºï¼Œé€šè¿‡å½¢æ€æ„ŸçŸ¥ä¿®å‰ªæé«˜åˆ†å‰²è´¨é‡ã€‚</li>
<li>CoFiä½¿ç”¨SAMè¿›è¡Œç²¾ç»†åˆ†å‰²ï¼Œå®ç°é«˜æ€§èƒ½çš„GBMåˆ†å‰²ã€‚</li>
<li>CoFiçš„Diceç³»æ•°ä¸º74.54%ï¼Œæ¨ç†é€Ÿåº¦ä¸º1.9 FPSã€‚</li>
<li>CoFiå‡è½»äº†ä¼ ç»ŸGBMåˆ†å‰²æ–¹æ³•çš„æ ‡æ³¨å’Œè®¡ç®—è´Ÿæ‹…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c16009aa1defb25c79478f432f0dd19d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7591720e15f7cd6fc4fdaeb93b6064fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a51a9aedb2d40f49612501d9a6cf0557.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e2173017a1258f89349dcb24a017666.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f691cd7b3e59b9a8b3d9622d8ab2084e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c21b8ecb170e1a7d4ebb8a5f71ca5090.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ed9d13b57d0d4b89ab5e64b7afc8189.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71d22e53ff8ac15be450774aad733899.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Group-Fairness-Meets-the-Black-Box-Enabling-Fair-Algorithms-on-Closed-LLMs-via-Post-Processing"><a href="#Group-Fairness-Meets-the-Black-Box-Enabling-Fair-Algorithms-on-Closed-LLMs-via-Post-Processing" class="headerlink" title="Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed   LLMs via Post-Processing"></a>Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed   LLMs via Post-Processing</h2><p><strong>Authors:Ruicheng Xian, Yuxuan Wan, Han Zhao</strong></p>
<p>Instruction fine-tuned large language models (LLMs) enable a simple zero-shot or few-shot prompting paradigm, also known as in-context learning, for building prediction models. This convenience, combined with continued advances in LLM capability, has the potential to drive their adoption across a broad range of domains, including high-stakes applications where group fairness â€“ preventing disparate impacts across demographic groups â€“ is essential. The majority of existing approaches to enforcing group fairness on LLM-based classifiers rely on traditional fair algorithms applied via model fine-tuning or head-tuning on final-layer embeddings, but they are no longer applicable to closed-weight LLMs under the in-context learning setting, which include some of the most capable commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we propose a framework for deriving fair classifiers from closed-weight LLMs via prompting: the LLM is treated as a feature extractor, and features are elicited from its probabilistic predictions (e.g., token log probabilities) using prompts strategically designed for the specified fairness criterion to obtain sufficient statistics for fair classification; a fair algorithm is then applied to these features to train a lightweight fair classifier in a post-hoc manner. Experiments on five datasets, including three tabular ones, demonstrate strong accuracy-fairness tradeoffs for the classifiers derived by our framework from both open-weight and closed-weight LLMs; in particular, our framework is data-efficient and outperforms fair classifiers trained on LLM embeddings (i.e., head-tuning) or from scratch on raw tabular features. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿå®ç°ç®€å•çš„é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬æç¤ºèŒƒå¼ï¼Œä¹Ÿç§°ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œç”¨äºæ„å»ºé¢„æµ‹æ¨¡å‹ã€‚è¿™ç§ä¾¿åˆ©ç»“åˆLLMèƒ½åŠ›çš„æŒç»­è¿›æ­¥ï¼Œæœ‰å¯èƒ½æ¨åŠ¨å…¶åœ¨å¹¿æ³›é¢†åŸŸçš„åº”ç”¨ï¼ŒåŒ…æ‹¬é«˜é£é™©åº”ç”¨ï¼Œå…¶ä¸­ç¾¤ä½“å…¬å¹³æ€§â€”â€”é˜²æ­¢å¯¹ä¸åŒç¾¤ä½“çš„ä¸åŒå½±å“â€”â€”è‡³å…³é‡è¦ã€‚ç°æœ‰å¤§å¤šæ•°åŸºäºLLMçš„åˆ†ç±»å™¨å®æ–½ç¾¤ä½“å…¬å¹³æ€§çš„æ–¹æ³•éƒ½ä¾èµ–äºé€šè¿‡æ¨¡å‹å¾®è°ƒæˆ–å¤´è°ƒåœ¨æœ€ç»ˆå±‚åµŒå…¥ä¸Šåº”ç”¨ä¼ ç»Ÿçš„å…¬å¹³ç®—æ³•ï¼Œä½†å®ƒä»¬ä¸å†é€‚ç”¨äºé‡‡ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ è®¾ç½®çš„å›ºå®šæƒé‡LLMï¼Œå…¶ä¸­åŒ…æ‹¬ç›®å‰ä¸€äº›åŠŸèƒ½æœ€å¼ºå¤§çš„å•†ä¸šæ¨¡å‹ï¼Œå¦‚GPT-4ã€åŒå­åº§å’ŒClaudeã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€šè¿‡æç¤ºä»å›ºå®šæƒé‡LLMæ´¾ç”Ÿå…¬å¹³åˆ†ç±»å™¨çš„æ¡†æ¶ï¼šå°†LLMè§†ä¸ºç‰¹å¾æå–å™¨ï¼Œå¹¶ä½¿ç”¨é’ˆå¯¹ç‰¹å®šå…¬å¹³æ€§æ ‡å‡†æˆ˜ç•¥è®¾è®¡çš„æç¤ºæ¥æ¿€å‘å…¶æ¦‚ç‡é¢„æµ‹ï¼ˆä¾‹å¦‚ï¼Œä»¤ç‰Œå¯¹æ•°æ¦‚ç‡ï¼‰ï¼Œä»¥è·å¾—å…¬å¹³åˆ†ç±»çš„å……è¶³ç»Ÿè®¡æ•°æ®ï¼›ç„¶åå¯¹è¿™äº›ç‰¹å¾åº”ç”¨å…¬å¹³ç®—æ³•ï¼Œä»¥äº‹åæ–¹å¼è®­ç»ƒè½»é‡çº§å…¬å¹³åˆ†ç±»å™¨ã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒï¼ŒåŒ…æ‹¬ä¸‰ä¸ªè¡¨æ ¼æ•°æ®é›†ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶ä»å…¬å¼€æƒé‡å’Œå›ºå®šæƒé‡LLMæ´¾ç”Ÿçš„åˆ†ç±»å™¨åœ¨å‡†ç¡®æ€§å…¬å¹³æ€§æ–¹é¢çš„å¼ºå¤§æƒè¡¡ï¼›ç‰¹åˆ«æ˜¯æˆ‘ä»¬çš„æ¡†æ¶æ•°æ®æ•ˆç‡è¾ƒé«˜ï¼Œä¼˜äºåœ¨LLMåµŒå…¥ä¸Šè®­ç»ƒçš„å…¬å¹³åˆ†ç±»å™¨ï¼ˆå³å¤´è°ƒï¼‰æˆ–ä»åŸå§‹è¡¨æ ¼ç‰¹å¾å¼€å§‹è®­ç»ƒçš„å…¬å¹³åˆ†ç±»å™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11258v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨é›¶æ ·æœ¬æˆ–å°‘é‡æ ·æœ¬æç¤ºæ¡†æ¶ä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ è¿›è¡Œé¢„æµ‹æ¨¡å‹æ„å»ºã€‚éšç€LLMèƒ½åŠ›çš„ä¸æ–­æå‡ï¼Œå…¶åœ¨å¤šä¸ªé¢†åŸŸçš„åº”ç”¨æ½œåŠ›æ—¥ç›Šæ˜¾ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©çš„å…¬å¹³æ€§è¦æ±‚ä¸¥æ ¼çš„é¢†åŸŸã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMçš„åˆ†ç±»å™¨å®ç°ç¾¤ä½“å…¬å¹³çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºä¼ ç»Ÿçš„å…¬å¹³ç®—æ³•ï¼Œè¿™äº›ç®—æ³•ä¸é€‚ç”¨äºå°é—­çš„LLMæƒé‡æ¨¡å‹ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„å…¬å¹³åˆ†ç±»å™¨æ´¾ç”Ÿæ¡†æ¶ï¼Œå°†LLMè§†ä¸ºç‰¹å¾æå–å™¨ï¼Œå¹¶ä½¿ç”¨é’ˆå¯¹ç‰¹å®šå…¬å¹³æ ‡å‡†è®¾è®¡çš„æç¤ºä»æ¦‚ç‡é¢„æµ‹ä¸­æå–ç‰¹å¾ï¼Œä»¥è·å¾—ç”¨äºå…¬å¹³åˆ†ç±»çš„å……è¶³ç»Ÿè®¡æ•°æ®ï¼›ç„¶åï¼Œåœ¨è¿™äº›ç‰¹å¾ä¸Šåº”ç”¨å…¬å¹³ç®—æ³•ä»¥è¿›è¡Œäº‹åè½»é‡çº§å…¬å¹³åˆ†ç±»å™¨çš„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¼€æ”¾å’Œå°é—­æƒé‡LLMä¸Šç”Ÿæˆçš„åˆ†ç±»å™¨å…·æœ‰è¾ƒå¼ºçš„ç²¾åº¦å…¬å¹³æƒè¡¡ã€‚ç‰¹åˆ«æ˜¯åœ¨æ•°æ®æ•ˆç‡æ–¹é¢ï¼Œè¯¥æ¡†æ¶ä¼˜äºåŸºäºLLMåµŒå…¥è®­ç»ƒçš„å…¬å¹³åˆ†ç±»å™¨æˆ–ä»å¤´å¼€å§‹åœ¨åŸå§‹è¡¨æ ¼ç‰¹å¾ä¸Šè®­ç»ƒçš„å…¬å¹³åˆ†ç±»å™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å®ç°é¢„æµ‹æ¨¡å‹æ„å»ºï¼Œè¿™ç§æ–¹æ³•çš„ä¾¿æ·æ€§æœ‰æœ›æ¨åŠ¨å…¶åœ¨å¤šä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚</li>
<li>åœ¨é«˜é£é™©ä¸”è¦æ±‚å…¬å¹³çš„é¢†åŸŸä¸­ï¼Œå¯¹å…¬å¹³æ€§ç»´æŠ¤çš„éœ€æ±‚å°¤ä¸ºå…³é”®ã€‚</li>
<li>å½“å‰åœ¨å°é—­æƒé‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šå®ç°ç¾¤ä½“å…¬å¹³çš„æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæç¤ºçš„å…¬å¹³åˆ†ç±»å™¨æ´¾ç”Ÿæ¡†æ¶ï¼Œé€‚ç”¨äºå°é—­æƒé‡çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶å°†LLMè§†ä¸ºç‰¹å¾æå–å™¨ï¼Œå¹¶ä½¿ç”¨é’ˆå¯¹ç‰¹å®šå…¬å¹³æ ‡å‡†è®¾è®¡çš„æç¤ºæ¥æå–ç‰¹å¾å¹¶è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶å…·æœ‰è‰¯å¥½çš„ç²¾åº¦å’Œå…¬å¹³æ€§æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e32332491511e7511d0f624631ed12d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e7d4dde3b350ada0b657aa3ff662c62.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-VLM-Fine-tuning-via-Latent-Hierarchical-Adapter-Learning"><a href="#Fine-Grained-VLM-Fine-tuning-via-Latent-Hierarchical-Adapter-Learning" class="headerlink" title="Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning"></a>Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning</h2><p><strong>Authors:Yumiao Zhao, Bo Jiang, Yuhe Ding, Xiao Wang, Jin Tang, Bin Luo</strong></p>
<p>Adapter-based approaches have garnered attention for fine-tuning pre-trained Vision-Language Models (VLMs) on few-shot classification tasks. These methods strive to develop a lightweight module that better aligns visual and (category) textual representations, thereby enhancing performance on downstream few-shot learning tasks. However, existing adapters generally learn&#x2F;align (category) textual-visual modalities via explicit spatial proximity in the underlying embedding space, which i) fails to capture the inherent one-to-many associations between categories and image samples and ii) struggles to establish accurate associations between the unknown categories and images. To address these issues, inspired by recent works on hyperbolic learning, we develop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs on downstream few-shot classification tasks. The core of LatHAdapter is to exploit the latent semantic hierarchy of downstream training data and employ it to provide richer, fine-grained guidance for the adapter learning process. Specifically, LatHAdapter first introduces some learnable &#96;attributeâ€™ prompts as the bridge to align categories and images. Then, it projects the categories, attribute prompts, and images within each batch in a hyperbolic space, and employs hierarchical regularization to learn the latent semantic hierarchy of them, thereby fully modeling the inherent one-to-many associations among categories, learnable attributes, and image samples. Extensive experiments on four challenging few-shot tasks show that the proposed LatHAdapter consistently outperforms many other fine-tuning approaches, particularly in adapting known classes and generalizing to unknown classes. </p>
<blockquote>
<p>åŸºäºé€‚é…å™¨çš„æ–¹æ³•å·²ç»å¼•èµ·äº†åœ¨å°‘é‡æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒé¢„è®­ç»ƒå¥½çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å…³æ³¨ã€‚è¿™äº›æ–¹æ³•è‡´åŠ›äºå¼€å‘ä¸€ä¸ªè½»é‡çº§çš„æ¨¡å—ï¼Œä»¥æ›´å¥½åœ°å¯¹é½è§†è§‰å’Œï¼ˆç±»åˆ«ï¼‰æ–‡æœ¬è¡¨ç¤ºï¼Œä»è€Œæé«˜ä¸‹æ¸¸å°‘é‡æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é€‚é…å™¨é€šå¸¸é€šè¿‡åº•å±‚åµŒå…¥ç©ºé—´ä¸­çš„æ˜ç¡®ç©ºé—´é‚»è¿‘å…³ç³»æ¥å­¦ä¹ &#x2F;å¯¹é½ï¼ˆç±»åˆ«ï¼‰æ–‡æœ¬è§†è§‰æ¨¡å¼ï¼Œè¿™iï¼‰æ— æ³•æ•è·ç±»åˆ«å’Œå›¾åƒæ ·æœ¬ä¹‹é—´å›ºæœ‰çš„å¤šå¯¹ä¸€å…³è”ï¼Œå¹¶ä¸”iiï¼‰åœ¨å»ºç«‹æœªçŸ¥ç±»åˆ«å’Œå›¾åƒä¹‹é—´çš„å‡†ç¡®å…³è”æ–¹é¢è¡¨ç°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å—åˆ°æœ€è¿‘å…³äºåŒæ›²å­¦ä¹ çš„ç ”ç©¶çš„å¯å‘ï¼Œå¼€å‘äº†ä¸€ç§ç”¨äºä¸‹æ¸¸å°‘é‡æ ·æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒVLMsçš„æ–°å‹æ½œåœ¨å±‚æ¬¡é€‚é…å™¨ï¼ˆLatHAdapterï¼‰ã€‚LatHAdapterçš„æ ¸å¿ƒæ˜¯åˆ©ç”¨ä¸‹æ¸¸è®­ç»ƒæ•°æ®çš„æ½œåœ¨è¯­ä¹‰å±‚æ¬¡ç»“æ„ï¼Œå¹¶å¯¹å…¶è¿›è¡Œåˆ©ç”¨ï¼Œä¸ºé€‚é…å™¨çš„å­¦ä¹ è¿‡ç¨‹æä¾›æ›´ä¸°å¯Œã€æ›´ç²¾ç»†çš„æŒ‡å¯¼ã€‚å…·ä½“æ¥è¯´ï¼ŒLatHAdapteré¦–å…ˆå¼•å…¥ä¸€äº›å¯å­¦ä¹ çš„â€œå±æ€§â€æç¤ºä½œä¸ºæ¡¥æ¢æ¥å¯¹é½ç±»åˆ«å’Œå›¾åƒã€‚ç„¶åï¼Œå®ƒåœ¨åŒæ›²ç©ºé—´ä¸­æŠ•å½±æ¯ä¸ªæ‰¹æ¬¡ä¸­çš„ç±»åˆ«ã€å±æ€§æç¤ºå’Œå›¾åƒï¼Œå¹¶é‡‡ç”¨å±‚æ¬¡æ­£åˆ™åŒ–æ¥å­¦ä¹ å®ƒä»¬çš„æ½œåœ¨è¯­ä¹‰å±‚æ¬¡ç»“æ„ï¼Œä»è€Œå……åˆ†å»ºæ¨¡ç±»åˆ«ã€å¯å­¦ä¹ å±æ€§å’Œå›¾åƒæ ·æœ¬ä¹‹é—´çš„å›ºæœ‰çš„ä¸€å¯¹å¤šå…³è”ã€‚åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å°‘é‡æ ·æœ¬ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„LatHAdapteråœ¨è®¸å¤šå…¶ä»–å¾®è°ƒæ–¹æ³•ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é€‚åº”å·²çŸ¥ç±»å’Œæ³›åŒ–åˆ°æœªçŸ¥ç±»ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11176v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨äºåˆ©ç”¨åŸºäºé€‚é…å™¨çš„ç­–ç•¥å¯¹é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥è§£å†³å°‘é‡æ ·æœ¬åˆ†ç±»ä»»åŠ¡ã€‚æ–‡ä¸­æŒ‡å‡ºç°æœ‰é€‚é…å™¨é€šè¿‡æ˜ç¡®çš„ç©ºé—´é‚»è¿‘åœ¨åº•å±‚åµŒå…¥ç©ºé—´å¯¹é½æ–‡æœ¬è§†è§‰æ¨¡å¼çš„æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…å—è¶…çƒé¢å­¦ä¹ çš„å¯å‘ï¼Œæå‡ºäº†æ–°å‹çš„æ½œåœ¨å±‚æ¬¡é€‚é…å™¨ï¼ˆLatHAdapterï¼‰ã€‚å®ƒé€šè¿‡å‘æ˜ä¸‹æ¸¸è®­ç»ƒæ•°æ®çš„æ½œåœ¨è¯­ä¹‰å±‚æ¬¡ï¼Œä¸ºé€‚é…å™¨çš„å­¦ä¹ è¿‡ç¨‹æä¾›æ›´ä¸°å¯Œçš„ç²¾ç»†æŒ‡å¯¼ã€‚LatHAdapteré€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„å±æ€§æç¤ºæ¥å¯¹é½ç±»åˆ«å’Œå›¾åƒï¼Œå¹¶åœ¨è¶…çƒé¢ç©ºé—´ä¸­æŠ•å½±å®ƒä»¬ï¼Œåˆ©ç”¨å±‚æ¬¡æ­£åˆ™åŒ–æ¥å­¦ä¹ æ½œåœ¨è¯­ä¹‰å±‚æ¬¡ç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼ŒLatHAdapteråœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å°‘é‡æ ·æœ¬ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨é€‚åº”å·²çŸ¥ç±»å’Œæ³›åŒ–åˆ°æœªçŸ¥ç±»ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€‚é…å™¨æ–¹æ³•è¢«ç”¨äºå¾®è°ƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»¥åº”å¯¹å°‘é‡æ ·æœ¬åˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰é€‚é…å™¨ä¸»è¦é€šè¿‡æ˜ç¡®çš„ç©ºé—´é‚»è¿‘åœ¨åµŒå…¥ç©ºé—´ä¸­å¯¹é½æ–‡æœ¬å’Œè§†è§‰æ¨¡å¼ï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>LatHAdapterå—è¶…çƒé¢å­¦ä¹ å¯å‘ï¼Œèƒ½å‘æ˜ä¸‹æ¸¸è®­ç»ƒæ•°æ®çš„æ½œåœ¨è¯­ä¹‰å±‚æ¬¡ã€‚</li>
<li>LatHAdapteré€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„å±æ€§æç¤ºï¼Œå¯¹é½ç±»åˆ«å’Œå›¾åƒã€‚</li>
<li>åœ¨è¶…çƒé¢ç©ºé—´ä¸­æŠ•å½±ç±»åˆ«ã€å±æ€§æç¤ºå’Œå›¾åƒï¼Œå¹¶åˆ©ç”¨å±‚æ¬¡æ­£åˆ™åŒ–å­¦ä¹ å®ƒä»¬çš„æ½œåœ¨è¯­ä¹‰ç»“æ„ã€‚</li>
<li>LatHAdapterèƒ½å……åˆ†å»ºæ¨¡ç±»åˆ«ã€å­¦ä¹ å±æ€§å’Œå›¾åƒæ ·æœ¬ä¹‹é—´çš„å›ºæœ‰çš„ä¸€å¯¹å¤šå…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eb31b32390fc716efd866e568867b0c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87c4de5c04b2b2bcaddfe515950057ff.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Are-Large-Pre-trained-Vision-Language-Models-Effective-Construction-Safety-Inspectors"><a href="#Are-Large-Pre-trained-Vision-Language-Models-Effective-Construction-Safety-Inspectors" class="headerlink" title="Are Large Pre-trained Vision Language Models Effective Construction   Safety Inspectors?"></a>Are Large Pre-trained Vision Language Models Effective Construction   Safety Inspectors?</h2><p><strong>Authors:Xuezheng Chen, Zhengbo Zou</strong></p>
<p>Construction safety inspections typically involve a human inspector identifying safety concerns on-site. With the rise of powerful Vision Language Models (VLMs), researchers are exploring their use for tasks such as detecting safety rule violations from on-site images. However, there is a lack of open datasets to comprehensively evaluate and further fine-tune VLMs in construction safety inspection. Current applications of VLMs use small, supervised datasets, limiting their applicability in tasks they are not directly trained for. In this paper, we propose the ConstructionSite 10k, featuring 10,000 construction site images with annotations for three inter-connected tasks, including image captioning, safety rule violation visual question answering (VQA), and construction element visual grounding. Our subsequent evaluation of current state-of-the-art large pre-trained VLMs shows notable generalization abilities in zero-shot and few-shot settings, while additional training is needed to make them applicable to actual construction sites. This dataset allows researchers to train and evaluate their own VLMs with new architectures and techniques, providing a valuable benchmark for construction safety inspection. </p>
<blockquote>
<p>æ–½å·¥å®‰å…¨æ£€æµ‹é€šå¸¸æ¶‰åŠäººå·¥æ£€æµ‹äººå‘˜åœ¨ç°åœºè¯†åˆ«å®‰å…¨éšæ‚£ã€‚éšç€å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å…´èµ·ï¼Œç ”ç©¶äººå‘˜æ­£åœ¨æ¢ç´¢å…¶ç”¨äºæ£€æµ‹ç°åœºå›¾åƒä¸­çš„å®‰å…¨è¿è§„ç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç¼ºä¹å¼€æ”¾æ•°æ®é›†æ¥å…¨é¢è¯„ä¼°å’Œè°ƒæ•´æ–½å·¥å®‰å…¨æ£€æŸ¥ä¸­çš„VLMsã€‚ç›®å‰VLMsçš„åº”ç”¨ä½¿ç”¨çš„æ˜¯å°è§„æ¨¡ã€æœ‰ç›‘ç£çš„æ•°æ®é›†ï¼Œé™åˆ¶äº†å…¶åœ¨éç›´æ¥è®­ç»ƒä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œConstructionSite 10kâ€æ•°æ®é›†ï¼ŒåŒ…å«1ä¸‡å¼ æ–½å·¥ç°åœºå›¾åƒï¼Œé’ˆå¯¹ä¸‰ä¸ªç›¸äº’å…³è”çš„ä»»åŠ¡è¿›è¡Œæ ‡æ³¨ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€å®‰å…¨è¿è§„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œæ–½å·¥å…ƒç´ è§†è§‰å®šä½ã€‚æˆ‘ä»¬å¯¹å½“å‰æœ€å…ˆè¿›çš„é¢„è®­ç»ƒå¤§å‹VLMsçš„åç»­è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸­å…·æœ‰æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†éœ€è¦é¢å¤–çš„è®­ç»ƒæ‰èƒ½é€‚ç”¨äºå®é™…æ–½å·¥ç°åœºã€‚è¯¥æ•°æ®é›†å…è®¸ç ”ç©¶äººå‘˜ä½¿ç”¨æ–°çš„æ¶æ„å’ŒæŠ€æœ¯è®­ç»ƒå’Œè¯„ä¼°ä»–ä»¬è‡ªå·±çš„VLMsï¼Œä¸ºæ–½å·¥å®‰å…¨æ£€æµ‹æä¾›äº†ä¸€ä¸ªæœ‰ä»·å€¼çš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11011v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å´›èµ·ï¼Œç ”ç©¶äººå‘˜å¼€å§‹æ¢ç´¢å…¶åœ¨å»ºç­‘å®‰å…¨æ£€æµ‹é¢†åŸŸçš„åº”ç”¨ï¼Œå¦‚ä»ç°åœºå›¾åƒä¸­æ£€æµ‹å®‰å…¨è¿è§„ã€‚ç„¶è€Œï¼Œç¼ºä¹å¼€æ”¾æ•°æ®é›†ä»¥å…¨é¢è¯„ä¼°å’Œåœ¨å»ºç­‘å®‰å…¨æ£€æµ‹ä¸­è¿›ä¸€æ­¥å¾®è°ƒVLMsã€‚æœ¬æ–‡æå‡ºäº†â€œConstructionSite 10kâ€æ•°æ®é›†ï¼ŒåŒ…å«1ä¸‡å¼ å»ºç­‘å·¥åœ°å›¾åƒï¼Œé’ˆå¯¹ä¸‰é¡¹äº’è”ä»»åŠ¡è¿›è¡Œæ ‡æ³¨ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°ã€å®‰å…¨è¿è§„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œå»ºç­‘å…ƒç´ è§†è§‰å®šä½ã€‚å¯¹å½“å‰æœ€å…ˆè¿›çš„é¢„è®­ç»ƒVLMsçš„è¯„ä¼°è¡¨æ˜ï¼Œå®ƒä»¬åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸­å…·æœ‰æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†éœ€è¦é¢å¤–çš„è®­ç»ƒæ‰èƒ½é€‚ç”¨äºå®é™…å»ºç­‘å·¥åœ°ã€‚æ­¤æ•°æ®é›†ä¸ºç ”ç©¶äººå‘˜ä½¿ç”¨æ–°æ¶æ„å’ŒæŠ€æœ¯è®­ç»ƒå’Œè¯„ä¼°è‡ªå·±çš„VLMsæä¾›äº†å®è´µçš„åŸºå‡†æµ‹è¯•ï¼Œä¸ºå»ºç­‘å®‰å…¨æ£€æµ‹é¢†åŸŸçš„å‘å±•æä¾›äº†é‡è¦æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsæ­£åœ¨è¢«æ¢ç´¢ç”¨äºå»ºç­‘å®‰å…¨æ£€æµ‹ä»»åŠ¡ï¼Œå¦‚ä»ç°åœºå›¾åƒä¸­æ£€æµ‹å®‰å…¨è¿è§„ã€‚</li>
<li>ç¼ºä¹å¼€æ”¾æ•°æ®é›†ä»¥å…¨é¢è¯„ä¼°å’Œåœ¨å»ºç­‘å®‰å…¨æ£€æµ‹ä¸­å¾®è°ƒVLMsã€‚</li>
<li>â€œConstructionSite 10kâ€æ•°æ®é›†åŒ…å«1ä¸‡å¼ å»ºç­‘å·¥åœ°å›¾åƒï¼Œé’ˆå¯¹ä¸‰é¡¹ä»»åŠ¡è¿›è¡Œæ ‡æ³¨ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„é¢„è®­ç»ƒVLMsåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸­å…·æœ‰æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>éœ€è¦é¢å¤–çš„è®­ç»ƒæ‰èƒ½ä½¿VLMsé€‚ç”¨äºå®é™…å»ºç­‘å·¥åœ°ç¯å¢ƒã€‚</li>
<li>æ­¤æ•°æ®é›†ä¸ºç ”ç©¶äººå‘˜æä¾›äº†è®­ç»ƒå’Œè¯„ä¼°VLMsçš„åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9388e7e63c6aa4a69e86e6b79b71e3be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09ef3afb5b314bc3bb4a0fd6ced858dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69ed00533d9bcf145f834f543e9a9a48.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Rule2Text-A-Framework-for-Generating-and-Evaluating-Natural-Language-Explanations-of-Knowledge-Graph-Rules"><a href="#Rule2Text-A-Framework-for-Generating-and-Evaluating-Natural-Language-Explanations-of-Knowledge-Graph-Rules" class="headerlink" title="Rule2Text: A Framework for Generating and Evaluating Natural Language   Explanations of Knowledge Graph Rules"></a>Rule2Text: A Framework for Generating and Evaluating Natural Language   Explanations of Knowledge Graph Rules</h2><p><strong>Authors:Nasim Shirvani-Mahdavi, Chengkai Li</strong></p>
<p>Knowledge graphs (KGs) can be enhanced through rule mining; however, the resulting logical rules are often difficult for humans to interpret due to their inherent complexity and the idiosyncratic labeling conventions of individual KGs. This work presents Rule2Text, a comprehensive framework that leverages large language models (LLMs) to generate natural language explanations for mined logical rules, thereby improving KG accessibility and usability. We conduct extensive experiments using multiple datasets, including Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically evaluate several LLMs across a comprehensive range of prompting strategies, including zero-shot, few-shot, variable type incorporation, and Chain-of-Thought reasoning. To systematically assess modelsâ€™ performance, we conduct a human evaluation of generated explanations on correctness and clarity. To address evaluation scalability, we develop and validate an LLM-as-a-judge framework that demonstrates strong agreement with human evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge, and human-in-the-loop feedback, we construct high-quality ground truth datasets, which we use to fine-tune the open-source Zephyr model. Our results demonstrate significant improvements in explanation quality after fine-tuning, with particularly strong gains in the domain-specific dataset. Additionally, we integrate a type inference module to support KGs lacking explicit type information. All code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/idirlab/KGRule2NL">https://github.com/idirlab/KGRule2NL</a>. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å¯ä»¥é€šè¿‡è§„åˆ™æŒ–æ˜è¿›è¡Œå¢å¼ºï¼›ç„¶è€Œï¼Œç”±äºé€»è¾‘è§„åˆ™çš„å†…åœ¨å¤æ‚æ€§ä»¥åŠå„ä¸ªçŸ¥è¯†å›¾è°±ç‰¹æœ‰çš„æ ‡ç­¾çº¦å®šï¼Œæ‰€å¾—çš„é€»è¾‘è§„åˆ™å¾€å¾€éš¾ä»¥è¢«äººç±»è§£è¯»ã€‚æœ¬ç ”ç©¶æå‡ºäº†Rule2Textï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”Ÿæˆå¯¹æŒ–æ˜åˆ°çš„é€»è¾‘è§„åˆ™çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œä»è€Œæé«˜çŸ¥è¯†å›¾è°±çš„å¯ç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨äº†å¤šä¸ªæ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬Freebaseçš„å„ç§å˜ä½“ï¼ˆFB-CVT-REVã€FB+CVT-REVå’ŒFB15k-237ï¼‰ï¼Œä»¥åŠogbl-biokgæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨AMIE 3.5.1è¿›è¡Œè§„åˆ™æŒ–æ˜ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†å‡ ç§LLMï¼Œæ¶µç›–äº†å¤šç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€å˜é‡ç±»å‹èåˆå’Œé“¾å¼æ€ç»´æ¨ç†ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ï¼Œæˆ‘ä»¬å¯¹ç”Ÿæˆçš„è§£é‡Šè¿›è¡Œäº†æ­£ç¡®æ€§å’Œæ¸…æ™°åº¦çš„äººå·¥è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¯„ä¼°çš„å¯æ‰©å±•æ€§é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘å¹¶éªŒè¯äº†ä¸€ä¸ªLLMæ³•å®˜æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸äººç±»è¯„ä¼°è€…ä¹‹é—´è¡¨ç°å‡ºäº†å¼ºçƒˆçš„å…±è¯†ã€‚åˆ©ç”¨è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼ˆGemini 2.0 Flashï¼‰ã€LLMæ³•å®˜å’Œäººç±»å®æ—¶åé¦ˆï¼Œæˆ‘ä»¬æ„å»ºäº†é«˜è´¨é‡çš„çœŸå®æ•°æ®é›†ï¼Œç”¨äºå¾®è°ƒå¼€æºçš„Zephyræ¨¡å‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨å¾®è°ƒåï¼Œè§£é‡Šè´¨é‡å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®é›†ä¸Šè·å¾—äº†æ˜¾è‘—çš„æ”¶ç›Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é›†æˆäº†ä¸€ä¸ªç±»å‹æ¨æ–­æ¨¡å—ï¼Œä»¥æ”¯æŒç¼ºä¹æ˜¾å¼ç±»å‹ä¿¡æ¯çš„çŸ¥è¯†å›¾è°±ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/idirlab/KGRule2NL">https://github.com/idirlab/KGRule2NL</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10971v1">PDF</a> arXiv admin note: text overlap with arXiv:2507.23740</p>
<p><strong>æ‘˜è¦</strong></p>
<p>çŸ¥è¯†å›¾è°±å¯ä»¥é€šè¿‡è§„åˆ™æŒ–æ˜è¿›è¡Œå¢å¼ºï¼Œä½†ç»“æœé€»è¾‘è§„åˆ™å¯¹äººç±»æ¥è¯´éš¾ä»¥è§£è¯»ï¼Œå› ä¸ºå®ƒä»¬çš„å¤æ‚æ€§ä»¥åŠå„ä¸ªçŸ¥è¯†å›¾è°±ç‰¹æœ‰çš„æ ‡ç­¾çº¦å®šã€‚æœ¬ç ”ç©¶æå‡ºRule2Textæ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè‡ªç„¶è¯­è¨€çš„è§£é‡Šï¼Œä¸ºæŒ–æ˜çš„é€»è¾‘è§„åˆ™æä¾›è¯´æ˜ï¼Œä»è€Œæé«˜çŸ¥è¯†å›¾è°±çš„å¯ç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬Freebaseå˜ä½“ä»¥åŠogbl-biokgæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨AMIE 3.5.1æŒ–æ˜è§„åˆ™ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†å¤šç§è¯­è¨€æ¨¡å‹åœ¨å„ç§æç¤ºç­–ç•¥ä¸‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ã€å˜é‡ç±»å‹èå…¥å’Œé“¾å¼æ€ç»´æ¨ç†ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘ä»¬å¯¹ç”Ÿæˆçš„è§£é‡Šè¿›è¡Œäº†æ­£ç¡®æ€§è¯„ä¼°æ¸…æ™°åº¦çš„äººç±»è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¯„ä¼°çš„å¯æ‰©å±•æ€§é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘å¹¶éªŒè¯äº†ä¸€ä¸ªè¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜çš„æ¡†æ¶ï¼Œä¸äººç±»è¯„ä¼°è€…ä¹‹é—´è¾¾æˆå¼ºä¸€è‡´æ€§ã€‚åˆ©ç”¨è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼ˆGemini 2.0 Flashï¼‰ã€è¯­è¨€æ¨¡å‹æ³•å®˜å’Œäººç±»å¾ªç¯åé¦ˆï¼Œæˆ‘ä»¬æ„å»ºäº†é«˜è´¨é‡çš„çœŸå®æ•°æ®é›†ï¼Œç”¨äºå¾®è°ƒå¼€æºZephyræ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨å¾®è°ƒåï¼Œè§£é‡Šè´¨é‡æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é›†æˆäº†ä¸€ä¸ªç±»å‹æ¨æ–­æ¨¡å—ï¼Œä»¥æ”¯æŒç¼ºä¹æ˜¾å¼ç±»å‹ä¿¡æ¯çš„çŸ¥è¯†å›¾è°±ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/idirlab/KGRule2NL">é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆè‡ªç„¶è¯­è¨€çš„è§£é‡Šï¼Œä»¥æé«˜çŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰çš„æ˜“ç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚</li>
<li>åœ¨å¤šç§æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬Freebaseå˜ä½“ä»¥åŠogbl-biokgæ•°æ®é›†ã€‚</li>
<li>ç³»ç»Ÿåœ°è¯„ä¼°äº†å¤šç§è¯­è¨€æ¨¡å‹åœ¨å„ç§æç¤ºç­–ç•¥ä¸‹çš„è¡¨ç°ã€‚</li>
<li>å¼€å‘äº†è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜çš„æ¡†æ¶ï¼Œä»¥è¿›è¡Œæ¨¡å‹æ€§èƒ½è¯„ä¼°å¹¶éªŒè¯ä¸äººç±»è¯„ä¼°è€…çš„ä¸€è‡´æ€§ã€‚</li>
<li>åˆ©ç”¨æœ€ä½³æ¨¡å‹æ„å»ºé«˜è´¨é‡çœŸå®æ•°æ®é›†ï¼Œç”¨äºå¾®è°ƒå¼€æºZephyræ¨¡å‹ã€‚</li>
<li>åœ¨å¾®è°ƒåæ˜¾è‘—æé«˜äº†è§£é‡Šè´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4f47d0417d1e6759374ad65e0ef683be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-477ef50f38ec1216a956d18a47339ca1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f0c5f446e58daef6e1071e40e1eaa1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f251463bb43ff2d5eb2d851ba404d25c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="An-Efficient-Model-Driven-Groupwise-Approach-for-Atlas-Construction"><a href="#An-Efficient-Model-Driven-Groupwise-Approach-for-Atlas-Construction" class="headerlink" title="An Efficient Model-Driven Groupwise Approach for Atlas Construction"></a>An Efficient Model-Driven Groupwise Approach for Atlas Construction</h2><p><strong>Authors:Ziwei Zou, Bei Zou, Xiaoyan Kui, Wenqi Lu, Haoran Dou, Arezoo Zakeri, Timothy Cootes, Alejandro F Frangi, Jinming Duan</strong></p>
<p>Atlas construction is fundamental to medical image analysis, offering a standardized spatial reference for tasks such as population-level anatomical modeling. While data-driven registration methods have recently shown promise in pairwise settings, their reliance on large training datasets, limited generalizability, and lack of true inference phases in groupwise contexts hinder their practical use. In contrast, model-driven methods offer training-free, theoretically grounded, and data-efficient alternatives, though they often face scalability and optimization challenges when applied to large 3D datasets. In this work, we introduce DARC (Diffeomorphic Atlas Registration via Coordinate descent), a novel model-driven groupwise registration framework for atlas construction. DARC supports a broad range of image dissimilarity metrics and efficiently handles arbitrary numbers of 3D images without incurring GPU memory issues. Through a coordinate descent strategy and a centrality-enforcing activation function, DARC produces unbiased, diffeomorphic atlases with high anatomical fidelity. Beyond atlas construction, we demonstrate two key applications: (1) One-shot segmentation, where labels annotated only on the atlas are propagated to subjects via inverse deformations, outperforming state-of-the-art few-shot methods; and (2) shape synthesis, where new anatomical variants are generated by warping the atlas mesh using synthesized diffeomorphic deformation fields. Overall, DARC offers a flexible, generalizable, and resource-efficient framework for atlas construction and applications. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œå›¾è°±æ„å»ºæ˜¯ä¸€é¡¹åŸºç¡€å·¥ä½œï¼Œå®ƒä¸ºç¾¤ä½“å±‚é¢çš„è§£å‰–å­¦å»ºæ¨¡ç­‰ä»»åŠ¡æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„ç©ºé—´å‚è€ƒã€‚è™½ç„¶æ•°æ®é©±åŠ¨å‹çš„æ³¨å†Œæ–¹æ³•åœ¨è¿‘æœŸçš„é…å¯¹è®¾ç½®ä¸­æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œä½†å®ƒä»¬å¯¹å¤§å‹è®­ç»ƒæ•°æ®é›†çš„ä¾èµ–ã€æœ‰é™çš„é€šç”¨æ€§ä»¥åŠç¾¤ç»„ç¯å¢ƒä¸‹çš„çœŸå®æ¨ç†é˜¶æ®µçš„ç¼ºå¤±ï¼Œé˜»ç¢äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ¨¡å‹é©±åŠ¨çš„æ–¹æ³•æä¾›äº†å…è®­ç»ƒã€ç†è®ºæ‰å®å’Œæ•°æ®é«˜æ•ˆçš„é€‰æ‹©ï¼Œä½†åœ¨åº”ç”¨äºå¤§å‹3Dæ•°æ®é›†æ—¶ï¼Œå®ƒä»¬å¸¸å¸¸é¢ä¸´å¯æ‰©å±•æ€§å’Œä¼˜åŒ–æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DARCï¼ˆåŸºäºåæ ‡ä¸‹é™çš„å¾®åˆ†åŒèƒšå›¾è°±æ³¨å†Œï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå›¾è°±æ„å»ºçš„æ–°å‹æ¨¡å‹é©±åŠ¨ç¾¤ç»„æ³¨å†Œæ¡†æ¶ã€‚DARCæ”¯æŒå¹¿æ³›çš„å›¾åƒä¸ç›¸ä¼¼åº¦åº¦é‡ï¼Œå¹¶èƒ½é«˜æ•ˆåœ°å¤„ç†ä»»æ„æ•°é‡çš„3Då›¾åƒï¼Œè€Œä¸ä¼šå¼•å‘GPUå†…å­˜é—®é¢˜ã€‚é€šè¿‡åæ ‡ä¸‹é™ç­–ç•¥å’Œä¸­å¿ƒå¼ºåˆ¶æ¿€æ´»å‡½æ•°ï¼ŒDARCäº§ç”Ÿæ— åè§ã€å¾®åˆ†åŒèƒšçš„å›¾è°±ï¼Œå…·æœ‰é«˜è§£å‰–ä¿çœŸåº¦ã€‚é™¤äº†å›¾è°±æ„å»ºä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†ä¸¤ä¸ªå…³é”®åº”ç”¨ï¼šï¼ˆ1ï¼‰å•æ¬¡åˆ†å‰²ï¼Œå…¶ä¸­ä»…åœ¨å›¾è°±ä¸Šæ ‡æ³¨çš„æ ‡ç­¾é€šè¿‡åå‘å˜å½¢ä¼ æ’­åˆ°ä¸»ä½“ä¸Šï¼Œè¶…è¶Šäº†æœ€æ–°çš„å°‘é•œå¤´æ–¹æ³•ï¼›ï¼ˆ2ï¼‰å½¢çŠ¶åˆæˆï¼Œé€šè¿‡åˆ©ç”¨åˆæˆçš„å¾®åˆ†åŒèƒšå˜å½¢åœºå¯¹å›¾è°±ç½‘æ ¼è¿›è¡Œå˜å½¢ï¼Œç”Ÿæˆæ–°çš„è§£å‰–å˜ä½“ã€‚æ€»ä½“è€Œè¨€ï¼ŒDARCä¸ºå›¾è°±æ„å»ºå’Œåº”ç”¨æä¾›äº†ä¸€ä¸ªçµæ´»ã€é€šç”¨å’Œèµ„æºé«˜æ•ˆçš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10743v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DARCï¼ˆåŸºäºåæ ‡ä¸‹é™çš„å¾®åˆ†å½¢æ€å­¦å›¾è°±æ³¨å†Œæ–¹æ³•ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå›¾è°±æ„å»ºçš„æ–°å‹æ¨¡å‹é©±åŠ¨ç¾¤ç»„æ³¨å†Œæ¡†æ¶ã€‚DARCæ”¯æŒå¹¿æ³›çš„å›¾åƒå·®å¼‚åº¦é‡ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°å¤„ç†ä»»æ„æ•°é‡çš„3Då›¾åƒï¼Œè€Œä¸ä¼šå¯¼è‡´GPUå†…å­˜é—®é¢˜ã€‚é€šè¿‡åæ ‡ä¸‹é™ç­–ç•¥å’Œä¸­å¿ƒå¼ºåˆ¶æ¿€æ´»å‡½æ•°ï¼ŒDARCèƒ½å¤Ÿäº§ç”Ÿå…·æœ‰é«˜ç²¾åº¦è§£å‰–ç»“æ„çš„ä¸åä¸å€šçš„å¾®åˆ†å›¾è°±ã€‚æ­¤å¤–ï¼ŒDARCè¿˜æ”¯æŒå›¾è°±æ„å»ºä¹‹å¤–çš„ä¸¤ä¸ªå…³é”®åº”ç”¨ï¼šä¸€æ˜¯å•å¼ å›¾åƒåˆ†å‰²ï¼Œå³ä»…é€šè¿‡å›¾è°±è¿›è¡Œæ ‡æ³¨æ ‡ç­¾çš„ä¼ æ’­åˆ°ç›®æ ‡å›¾åƒï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„å°‘é‡æ ·æœ¬æ–¹æ³•ï¼›äºŒæ˜¯å½¢çŠ¶åˆæˆï¼Œé€šè¿‡åˆæˆå¾®åˆ†å½¢æ€å­¦å˜å½¢åœºæ‰­æ›²å›¾è°±ç½‘æ ¼ç”Ÿæˆæ–°çš„è§£å‰–å˜ä½“ã€‚æ€»çš„æ¥è¯´ï¼ŒDARCæä¾›äº†ä¸€ä¸ªçµæ´»ã€é€šç”¨ä¸”èµ„æºé«˜æ•ˆçš„å›¾è°±æ„å»ºä¸åº”ç”¨æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DARCæ˜¯ä¸€ç§ç”¨äºå›¾è°±æ„å»ºçš„æ¨¡å‹é©±åŠ¨ç¾¤ç»„æ³¨å†Œæ¡†æ¶ï¼Œé€‚ç”¨äºä»»æ„æ•°é‡çš„3Då›¾åƒã€‚</li>
<li>DARCæ”¯æŒå¹¿æ³›çš„å›¾åƒå·®å¼‚åº¦é‡ï¼Œæé«˜äº†æ³¨å†Œçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>é€šè¿‡åæ ‡ä¸‹é™ç­–ç•¥å’Œä¸­å¿ƒå¼ºåˆ¶æ¿€æ´»å‡½æ•°ï¼ŒDARCèƒ½ç”Ÿæˆä¸åä¸å€šã€å…·æœ‰é«˜è§£å‰–çœŸå®æ€§çš„å¾®åˆ†å›¾è°±ã€‚</li>
<li>DARCåœ¨å›¾è°±æ„å»ºåº”ç”¨ä¸Šè¡¨ç°å‡ºä¼˜ç§€çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å•å¼ å›¾åƒåˆ†å‰²å’Œå½¢çŠ¶åˆæˆã€‚</li>
<li>å•å¼ å›¾åƒåˆ†å‰²åº”ç”¨å®ç°äº†ä»…é€šè¿‡å›¾è°±è¿›è¡Œæ ‡æ³¨æ ‡ç­¾çš„ä¼ æ’­åˆ°ç›®æ ‡å›¾åƒï¼Œæ€§èƒ½è¶…è¶Šç°æœ‰å°‘é‡æ ·æœ¬æ–¹æ³•ã€‚</li>
<li>å½¢çŠ¶åˆæˆåº”ç”¨é€šè¿‡æ‰­æ›²å›¾è°±ç½‘æ ¼ç”Ÿæˆæ–°çš„è§£å‰–å˜ä½“ï¼Œå±•ç¤ºäº†DARCçš„åˆ›é€ æ€§å’Œçµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e3bd8c9ad4accf949235cf23748e9dff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0480971fd3e9425b1778d2a6a9c613cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30db0f15b0b5e73d21a94bcfc2c00412.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ca3a1ca760ff6dedf0a8daf6e8263b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0d50cd237d5f3bd21c53f8700189d08.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SemPT-Semantic-Prompt-Tuning-for-Vision-Language-Models"><a href="#SemPT-Semantic-Prompt-Tuning-for-Vision-Language-Models" class="headerlink" title="SemPT: Semantic Prompt Tuning for Vision-Language Models"></a>SemPT: Semantic Prompt Tuning for Vision-Language Models</h2><p><strong>Authors:Xiao Shi, Yangjun Ou, Zhenzhong Chen</strong></p>
<p>Visual transfer learning for unseen categories presents an active research topic yet a challenging task, due to the inherent conflict between preserving category-specific representations and acquiring transferable knowledge. Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs offer a promising solution. However, existing prompt tuning methods rely on sparse category labels or disparate LLM-generated descriptions, which fragment knowledge representation and hinder transferability. To address this limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that tackles the generalization challenge by leveraging shared attribute-level knowledge across categories. Specifically, SemPT adopts a two-step prompting strategy to guide LLM in extracting shared visual attributes and generating attribute-level descriptions, capturing transferable semantic cues beyond labels while ensuring coherent structure. Then, visually guided weighting is applied to the embeddings of attribute-level descriptions to reduce noise from irrelevant attributes and enhance the text embeddings. Additionally, image embeddings are jointly aligned with both label and attribute-enhanced text embeddings, balancing discrimination for seen categories and transferability to unseen ones. Considering the availability of category exposure, our inference dynamically selects between standard label embeddings for seen categories and attribute-enhanced embeddings for unseen ones to ensure effective adaptation. Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves state-of-the-art performance across various settings, including base-to-novel generalization, cross-dataset transfer, cross-domain transfer, and few-shot learning. </p>
<blockquote>
<p>è§†è§‰è¿ç§»å­¦ä¹ å¯¹äºæœªè§ç±»åˆ«æ˜¯ä¸€ä¸ªçƒ­é—¨ä¸”å…·æŒ‘æˆ˜æ€§çš„è¯¾é¢˜ï¼Œå› ä¸ºä¿æŒç‰¹å®šç±»åˆ«çš„è¡¨ç¤ºå’Œè·å–å¯è¿ç§»çŸ¥è¯†ä¹‹é—´å­˜åœ¨å†…åœ¨å†²çªã€‚é¢„è®­ç»ƒåœ¨å¤§é‡å›¾åƒæ–‡æœ¬å¯¹ä¸Šçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„æç¤ºè°ƒæ•´æ–¹æ³•ä¾èµ–äºç¨€ç–çš„ç±»åˆ«æ ‡ç­¾æˆ–åˆ†æ•£çš„LLMç”Ÿæˆæè¿°ï¼Œè¿™å¯¼è‡´çŸ¥è¯†è¡¨ç¤ºç¢ç‰‡åŒ–å¹¶é˜»ç¢çŸ¥è¯†çš„è¿ç§»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰æç¤ºè°ƒæ•´ï¼ˆSemPTï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨ç±»åˆ«é—´çš„å…±äº«å±æ€§çº§çŸ¥è¯†æ¥è§£å†³æ³›åŒ–æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼ŒSemPTé‡‡ç”¨ä¸¤æ­¥æç¤ºç­–ç•¥æ¥æŒ‡å¯¼LLMæå–å…±äº«çš„è§†è§‰å±æ€§å¹¶ç”Ÿæˆå±æ€§çº§æè¿°ï¼Œæ•æ‰è¶…è¶Šæ ‡ç­¾çš„å¯è½¬ç§»è¯­ä¹‰çº¿ç´¢ï¼ŒåŒæ—¶ç¡®ä¿è¿è´¯çš„ç»“æ„ã€‚ç„¶åï¼Œå¯¹å±æ€§çº§æè¿°çš„åµŒå…¥åº”ç”¨è§†è§‰å¼•å¯¼åŠ æƒï¼Œä»¥å‡å°‘æ¥è‡ªæ— å…³å±æ€§çš„å™ªå£°å¹¶å¢å¼ºæ–‡æœ¬åµŒå…¥ã€‚æ­¤å¤–ï¼Œå›¾åƒåµŒå…¥ä¸æ ‡ç­¾å’Œå±æ€§å¢å¼ºæ–‡æœ¬åµŒå…¥å…±åŒå¯¹é½ï¼Œå¹³è¡¡å¯¹å¯è§ç±»åˆ«çš„åŒºåˆ†åº¦å’Œå¯¹æœªè§ç±»åˆ«çš„å¯è¿ç§»æ€§ã€‚è€ƒè™‘åˆ°ç±»åˆ«çš„å¯è§æ€§ï¼Œæˆ‘ä»¬çš„æ¨ç†è¿‡ç¨‹ä¼šåŠ¨æ€é€‰æ‹©åœ¨å¯è§ç±»åˆ«å’Œä¸å¯è§ç±»åˆ«ä¹‹é—´ä½¿ç”¨æ ‡å‡†çš„æ ‡ç­¾åµŒå…¥å’Œå±æ€§å¢å¼ºåµŒå…¥ï¼Œä»¥ç¡®ä¿æœ‰æ•ˆçš„é€‚åº”ã€‚åœ¨15ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSemPTåœ¨å„ç§è®¾ç½®ä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ä»åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–ã€è·¨æ•°æ®é›†è¿ç§»ã€è·¨åŸŸè¿ç§»å’Œå°æ ·æœ¬å­¦ä¹ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10645v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†è§‰è¿ç§»å­¦ä¹ å¯¹äºæœªè§ç±»åˆ«æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶è¯¾é¢˜ï¼Œä½†ä¹Ÿæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç”±äºä¿æŒç‰¹å®šç±»åˆ«çš„è¡¨ç¤ºå’Œè·å–å¯è¿ç§»çŸ¥è¯†ä¹‹é—´çš„å†…åœ¨å†²çªï¼Œä½¿å¾—è¯¥ä»»åŠ¡æ›´å…·æŒ‘æˆ˜æ€§ã€‚é¢„è®­ç»ƒåœ¨å¤§é‡å›¾åƒæ–‡æœ¬å¯¹ä¸Šçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„æç¤ºè°ƒæ•´æ–¹æ³•ä¾èµ–äºç¨€ç–çš„ç±»åˆ«æ ‡ç­¾æˆ–åˆ†æ•£çš„LLMç”Ÿæˆæè¿°ï¼Œè¿™ç ´åäº†çŸ¥è¯†è¡¨ç¤ºå¹¶é˜»ç¢äº†å¯è¿ç§»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰æç¤ºè°ƒæ•´ï¼ˆSemPTï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨ç±»åˆ«ä¹‹é—´çš„å…±äº«å±æ€§çº§çŸ¥è¯†æ¥è§£å†³æ³›åŒ–æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¿ç§»å­¦ä¹ å¯¹äºæœªè§ç±»åˆ«æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºéœ€è¦å¹³è¡¡ä¿æŒç±»åˆ«ç‰¹å¼‚æ€§è¡¨ç¤ºå’Œè·å–å¯è¿ç§»çŸ¥è¯†ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜æ–¹é¢æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰æç¤ºè°ƒæ•´æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œä¾èµ–äºç¨€ç–ç±»åˆ«æ ‡ç­¾æˆ–LLMç”Ÿæˆçš„æè¿°ï¼Œè¿™ç ´åäº†çŸ¥è¯†è¡¨ç¤ºå¹¶é˜»ç¢äº†å¯è¿ç§»æ€§ã€‚</li>
<li>SemPTæ¡†æ¶é€šè¿‡åˆ©ç”¨ç±»åˆ«ä¹‹é—´çš„å…±äº«å±æ€§çº§çŸ¥è¯†æ¥è§£å†³æ³›åŒ–æŒ‘æˆ˜ã€‚</li>
<li>SemPTé‡‡ç”¨ä¸¤æ­¥æç¤ºç­–ç•¥ï¼Œå¼•å¯¼LLMæå–å…±äº«è§†è§‰å±æ€§å’Œç”Ÿæˆå±æ€§çº§æè¿°ï¼Œæ•æ‰å¯è¿ç§»çš„è¯­ä¹‰çº¿ç´¢ã€‚</li>
<li>SemPTé€šè¿‡è§†è§‰å¼•å¯¼åŠ æƒå‡å°‘äº†æ¥è‡ªæ— å…³å±æ€§çš„å™ªå£°ï¼Œå¢å¼ºäº†æ–‡æœ¬åµŒå…¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1616057c449bdf030c62fc7153ded7ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a7e3f73e77e13e56ee03baff6258bc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5722113344929679e94d6ba228691cbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-137d70338d2d00a469864a7d66ec70db.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Increasing-the-Utility-of-Synthetic-Images-through-Chamfer-Guidance"><a href="#Increasing-the-Utility-of-Synthetic-Images-through-Chamfer-Guidance" class="headerlink" title="Increasing the Utility of Synthetic Images through Chamfer Guidance"></a>Increasing the Utility of Synthetic Images through Chamfer Guidance</h2><p><strong>Authors:Nicola Dallâ€™Asen, Xiaofeng Zhang, Reyhane Askari Hemmat, Melissa Hall, Jakob Verbeek, Adriana Romero-Soriano, Michal Drozdzal</strong></p>
<p>Conditional image generative models hold considerable promise to produce infinite amounts of synthetic training data. Yet, recent progress in generation quality has come at the expense of generation diversity, limiting the utility of these models as a source of synthetic training data. Although guidance-based approaches have been introduced to improve the utility of generated data by focusing on quality or diversity, the (implicit or explicit) utility functions oftentimes disregard the potential distribution shift between synthetic and real data. In this work, we introduce Chamfer Guidance: a training-free guidance approach which leverages a handful of real exemplar images to characterize the quality and diversity of synthetic data. We show that by leveraging the proposed Chamfer Guidance, we can boost the diversity of the generations w.r.t. a dataset of real images while maintaining or improving the generation quality on ImageNet-1k and standard geo-diversity benchmarks. Our approach achieves state-of-the-art few-shot performance with as little as 2 exemplar real images, obtaining 96.4% in terms of precision, and 86.4% in terms of distributional coverage, which increase to 97.5% and 92.7%, respectively, when using 32 real images. We showcase the benefits of the Chamfer Guidance generation by training downstream image classifiers on synthetic data, achieving accuracy boost of up to 15% for in-distribution over the baselines, and up to 16% in out-of-distribution. Furthermore, our approach does not require using the unconditional model, and thus obtains a 31% reduction in FLOPs w.r.t. classifier-free-guidance-based approaches at sampling time. </p>
<blockquote>
<p>æ¡ä»¶å›¾åƒç”Ÿæˆæ¨¡å‹å…·æœ‰äº§ç”Ÿæ— é™åˆæˆè®­ç»ƒæ•°æ®çš„å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”Ÿæˆè´¨é‡æ–¹é¢çš„æœ€æ–°è¿›å±•æ˜¯ä»¥ç‰ºç‰²ç”Ÿæˆå¤šæ ·æ€§ä¸ºä»£ä»·çš„ï¼Œè¿™é™åˆ¶äº†è¿™äº›æ¨¡å‹ä½œä¸ºåˆæˆè®­ç»ƒæ•°æ®æ¥æºçš„å®ç”¨æ€§ã€‚è™½ç„¶åŸºäºå¼•å¯¼çš„æ–¹æ³•å·²ç»è¢«å¼•å…¥ï¼Œé€šè¿‡å…³æ³¨è´¨é‡æˆ–å¤šæ ·æ€§æ¥æé«˜ç”Ÿæˆæ•°æ®çš„å®ç”¨æ€§ï¼Œä½†ï¼ˆéšå¼æˆ–æ˜¾å¼ï¼‰æ•ˆç”¨å‡½æ•°é€šå¸¸å¿½ç•¥äº†åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ä¹‹é—´æ½œåœ¨åˆ†å¸ƒå·®å¼‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Chamfer Guidanceï¼šä¸€ç§æ— éœ€è®­ç»ƒçš„å¼•å¯¼æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å°‘é‡çœŸå®ç¤ºä¾‹å›¾åƒæ¥è¡¨å¾åˆæˆæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡åˆ©ç”¨æå‡ºçš„Chamfer Guidanceï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é’ˆå¯¹çœŸå®å›¾åƒæ•°æ®é›†ç”Ÿæˆæ—¶æé«˜ç”Ÿæˆçš„å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒåœ¨ImageNet-1kå’Œæ ‡å‡†åœ°ç†å¤šæ ·æ€§åŸºå‡†æµ‹è¯•é›†ä¸Šç”Ÿæˆè´¨é‡çš„ç»´æŒæˆ–æé«˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä»…2å¼ çœŸå®ç¤ºä¾‹å›¾åƒå³å¯å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ç²¾ç¡®åº¦æ–¹é¢è¾¾åˆ°96.4%ï¼Œåœ¨åˆ†å¸ƒè¦†ç›–æ–¹é¢è¾¾åˆ°86.4%ï¼Œåœ¨ä½¿ç”¨32å¼ çœŸå®å›¾åƒæ—¶ï¼Œè¿™äº›æ•°å­—åˆ†åˆ«æé«˜åˆ°97.5%å’Œ92.7%ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨åˆæˆæ•°æ®è®­ç»ƒä¸‹æ¸¸å›¾åƒåˆ†ç±»å™¨æ¥å±•ç¤ºChamfer Guidanceç”Ÿæˆçš„ä¼˜åŠ¿ï¼Œä¸åŸºå‡†ç›¸æ¯”ï¼Œåœ¨å†…éƒ¨åˆ†å¸ƒä¸Šæé«˜äº†é«˜è¾¾15%çš„å‡†ç¡®ç‡ï¼Œåœ¨å¤–éƒ¨åˆ†å¸ƒä¸Šæé«˜äº†é«˜è¾¾16%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦ä½¿ç”¨æ— æ¡ä»¶æ¨¡å‹ï¼Œå› æ­¤åœ¨é‡‡æ ·æ—¶é—´æ–¹é¢ä¸åŸºäºæ— åˆ†ç±»å™¨å¼•å¯¼çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†31%çš„FLOPså‡å°‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10631v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºChamfer Guidanceçš„æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹æŒ‡å¯¼æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å°‘é‡çœŸå®ç¤ºä¾‹å›¾åƒæ¥è¡¨å¾åˆæˆæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æé«˜ç”Ÿæˆå›¾åƒå¤šæ ·æ€§çš„åŒæ—¶ï¼Œç»´æŒæˆ–æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚åœ¨ImageNet-1kå’Œåœ°ç†å¤šæ ·æ€§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨ä»…ä¸¤ä¸ªç¤ºä¾‹çœŸå®å›¾åƒå³å¯å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†Chamfer Guidanceåœ¨è®­ç»ƒä¸‹æ¸¸å›¾åƒåˆ†ç±»å™¨æ–¹é¢çš„ä¼˜åŠ¿ï¼Œé€šè¿‡åˆæˆæ•°æ®å®ç°äº†é«˜è¾¾15%çš„å‡†ç¡®ç‡æå‡ã€‚æ­¤æ–¹æ³•æ— éœ€ä½¿ç”¨æ— æ¡ä»¶æ¨¡å‹ï¼Œå› æ­¤åœ¨é‡‡æ ·æ—¶ä¸åŸºäºåˆ†ç±»å™¨å…è´¹çš„æŒ‡å¯¼æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†31%çš„FLOPså‡å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Chamfer Guidanceæ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒæŒ‡å¯¼æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åˆæˆæ•°æ®çš„ç”Ÿæˆè´¨é‡å’Œå¤šæ ·æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å°‘é‡çœŸå®ç¤ºä¾‹å›¾åƒæ¥è¡¨å¾åˆæˆæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚</li>
<li>Chamfer Guidanceåœ¨ImageNet-1kå’Œåœ°ç†å¤šæ ·æ€§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åœ¨ä¸‹æ¸¸å›¾åƒåˆ†ç±»å™¨ä¸­ä½¿ç”¨åˆæˆæ•°æ®ï¼ŒChamfer Guidanceå®ç°äº†æ˜¾è‘—çš„å‡†ç¡®ç‡æå‡ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒChamfer Guidanceæ— éœ€ä½¿ç”¨æ— æ¡ä»¶æ¨¡å‹ï¼Œå› æ­¤åœ¨é‡‡æ ·æ—¶æ•ˆç‡æ›´é«˜ã€‚</li>
<li>Chamfer Guidanceåœ¨é‡‡æ ·æ—¶å®ç°äº†æ˜¾è‘—çš„FLOPså‡å°‘ï¼Œä¸åŸºäºåˆ†ç±»å™¨å…è´¹çš„æŒ‡å¯¼æ–¹æ³•ç›¸æ¯”ï¼Œå‡å°‘äº†31%ã€‚</li>
<li>Chamfer Guidanceå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜è´¨é‡åˆæˆæ•°æ®çš„é¢†åŸŸï¼Œå¦‚å›¾åƒåˆ†ç±»ã€å›¾åƒç”Ÿæˆç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-541b5c9f33e6ab89c2ea7af11be954fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae05322b526cb12802dab6e7c2810e03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8af44d93925d383c641ae779c1f45ccd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning"><a href="#Few-shot-Vision-based-Human-Activity-Recognition-with-MLLM-based-Visual-Reinforcement-Learning" class="headerlink" title="Few-shot Vision-based Human Activity Recognition with MLLM-based Visual   Reinforcement Learning"></a>Few-shot Vision-based Human Activity Recognition with MLLM-based Visual   Reinforcement Learning</h2><p><strong>Authors:Wenqi Zheng, Yutaka Arakawa</strong></p>
<p>Reinforcement learning in large reasoning models enables learning from feedback on their outputs, making it particularly valuable in scenarios where fine-tuning data is limited. However, its application in multi-modal human activity recognition (HAR) domains remains largely underexplored. Our work extends reinforcement learning to the human activity recognition domain with multimodal large language models. By incorporating visual reinforcement learning in the training process, the modelâ€™s generalization ability on few-shot recognition can be greatly improved. Additionally, visual reinforcement learning can enhance the modelâ€™s reasoning ability and enable explainable analysis in the inference stage. We name our few-shot human activity recognition method with visual reinforcement learning FAVOR. Specifically, our approach first utilizes a multimodal large language model (MLLM) to generate multiple candidate responses for the human activity image, each containing reasoning traces and final answers. These responses are then evaluated using reward functions, and the MLLM model is subsequently optimized using the Group Relative Policy Optimization (GRPO) algorithm. In this way, the MLLM model can be adapted to human activity recognition with only a few samples. Extensive experiments on four human activity recognition datasets and five different settings demonstrate the superiority of the proposed method. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„åº”ç”¨ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä»å…¶è¾“å‡ºçš„åé¦ˆä¸­å­¦ä¹ ï¼Œè¿™åœ¨ç²¾ç»†è°ƒæ•´æ•°æ®æœ‰é™çš„åœºæ™¯ä¸­å°¤å…¶æœ‰ä»·å€¼ã€‚ç„¶è€Œï¼Œå…¶åœ¨å¤šæ¨¡æ€äººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰é¢†åŸŸçš„åº”ç”¨ä»è¢«å¤§å¤§å¿½è§†ã€‚æˆ‘ä»¬çš„å·¥ä½œå°†å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°äººç±»æ´»åŠ¨è¯†åˆ«é¢†åŸŸï¼Œé‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥è§†è§‰å¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥æå¤§åœ°æé«˜æ¨¡å‹åœ¨å°‘é‡æ ·æœ¬è¯†åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè§†è§‰å¼ºåŒ–å­¦ä¹ è¿˜å¯ä»¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå®ç°å¯è§£é‡Šæ€§åˆ†æã€‚æˆ‘ä»¬å°†å¸¦æœ‰è§†è§‰å¼ºåŒ–å­¦ä¹ çš„å°‘é‡äººç±»æ´»åŠ¨è¯†åˆ«æ–¹æ³•å‘½åä¸ºFAVORã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸ºäººä½“æ´»åŠ¨å›¾åƒç”Ÿæˆå¤šä¸ªå€™é€‰å“åº”ï¼Œæ¯ä¸ªå“åº”éƒ½åŒ…å«æ¨ç†è½¨è¿¹å’Œæœ€ç»ˆç­”æ¡ˆã€‚ç„¶åä½¿ç”¨å¥–åŠ±å‡½æ•°å¯¹è¿™äº›å“åº”è¿›è¡Œè¯„ä¼°ï¼Œéšåä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•å¯¹MLLMæ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMLLMæ¨¡å‹å¯ä»¥é€‚åº”ä»…ä½¿ç”¨å°‘é‡æ ·æœ¬çš„äººç±»æ´»åŠ¨è¯†åˆ«ã€‚åœ¨å››ä¸ªäººç±»æ´»åŠ¨è¯†åˆ«æ•°æ®é›†å’Œäº”ç§ä¸åŒè®¾ç½®ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10371v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ å¯ä»è¾“å‡ºåé¦ˆä¸­å­¦ä¹ ï¼Œåœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ç‰¹åˆ«æœ‰ä»·å€¼ã€‚æœ¬æ–‡å°†å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°äººç±»æ´»åŠ¨è¯†åˆ«é¢†åŸŸï¼Œç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è§†è§‰å¼ºåŒ–å­¦ä¹ æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ã€‚å‘½åä¸ºFAVORçš„æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šä¸ªæ´»åŠ¨å›¾åƒå€™é€‰å“åº”ï¼Œä½¿ç”¨å¥–åŠ±å‡½æ•°è¯„ä¼°å¹¶åˆ©ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ä¼˜åŒ–æ¨¡å‹ã€‚åœ¨ä»…æœ‰å°‘é‡æ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹å¯é€‚åº”äººç±»æ´»åŠ¨è¯†åˆ«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿä»è¾“å‡ºåé¦ˆä¸­å­¦ä¹ ï¼Œåœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹å…·æœ‰ç‰¹åˆ«ä»·å€¼ã€‚</li>
<li>æœ¬æ–‡å°†å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°äººç±»æ´»åŠ¨è¯†åˆ«é¢†åŸŸã€‚</li>
<li>ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è§†è§‰å¼ºåŒ–å­¦ä¹ æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è§†è§‰å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå®ç°å¯è§£é‡Šæ€§åˆ†æã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„å°‘æ ·æœ¬äººç±»æ´»åŠ¨è¯†åˆ«æ–¹æ³•å‘½åä¸ºFAVORã€‚</li>
<li>FAVORæ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šä¸ªå€™é€‰å“åº”ï¼Œå¹¶ä½¿ç”¨å¥–åŠ±å‡½æ•°è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4d701f72dd0648272bd6ee6fd4bddc5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-286823a3a1d7792f71ef33c09cba0489.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14027dd6d3c9cd50a4dd2533cdc7318b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c938b694e12f24aa0c80ea9ec1ec8671.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ced82577192fe3e31773064b8f6d1bea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-319d03bdfb285d82d7db1da0449814d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-164b5b2e4b9bbe981a46977a09a50972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ceb93b5a52d5b1eea48b1129d771e6f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Improving-Generative-Cross-lingual-Aspect-Based-Sentiment-Analysis-with-Constrained-Decoding"><a href="#Improving-Generative-Cross-lingual-Aspect-Based-Sentiment-Analysis-with-Constrained-Decoding" class="headerlink" title="Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with   Constrained Decoding"></a>Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with   Constrained Decoding</h2><p><strong>Authors:Jakub Å mÃ­d, Pavel PÅ™ibÃ¡Åˆ, Pavel KrÃ¡l</strong></p>
<p>While aspect-based sentiment analysis (ABSA) has made substantial progress, challenges remain for low-resource languages, which are often overlooked in favour of English. Current cross-lingual ABSA approaches focus on limited, less complex tasks and often rely on external translation tools. This paper introduces a novel approach using constrained decoding with sequence-to-sequence models, eliminating the need for unreliable translation tools and improving cross-lingual performance by 5% on average for the most complex task. The proposed method also supports multi-tasking, which enables solving multiple ABSA tasks with a single model, with constrained decoding boosting results by more than 10%.   We evaluate our approach across seven languages and six ABSA tasks, surpassing state-of-the-art methods and setting new benchmarks for previously unexplored tasks. Additionally, we assess large language models (LLMs) in zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in zero-shot and few-shot settings, fine-tuning achieves competitive results compared to smaller multilingual models, albeit at the cost of longer training and inference times.   We provide practical recommendations for real-world applications, enhancing the understanding of cross-lingual ABSA methodologies. This study offers valuable insights into the strengths and limitations of cross-lingual ABSA approaches, advancing the state-of-the-art in this challenging research domain. </p>
<blockquote>
<p>åŸºäºæ–¹é¢çš„æƒ…æ„Ÿåˆ†æï¼ˆABSAï¼‰è™½ç„¶å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å¯¹äºèµ„æºåŒ®ä¹çš„è¯­è¨€æ¥è¯´ï¼Œä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚é€šå¸¸è¿™äº›è¯­è¨€ä¼šè¢«å¿½ç•¥è€Œä¼˜å…ˆé€‰æ‹©è‹±è¯­ã€‚å½“å‰çš„è·¨è¯­è¨€ABSAæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æœ‰é™ä¸”ä¸å¤ªå¤æ‚çš„ä»»åŠ¡ä¸Šï¼Œå¹¶ä¸”ç»å¸¸ä¾èµ–å¤–éƒ¨ç¿»è¯‘å·¥å…·ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä½¿ç”¨åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„çº¦æŸè§£ç çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ¶ˆé™¤äº†å¯¹ä¸å¯é ç¿»è¯‘å·¥å…·çš„ä¾èµ–ï¼Œå¹¶åœ¨æœ€å¤æ‚çš„ä»»åŠ¡ä¸Šå¹³å‡æé«˜äº†5%çš„è·¨è¯­è¨€æ€§èƒ½ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¿˜æ”¯æŒå¤šä»»åŠ¡å¤„ç†ï¼Œèƒ½å¤Ÿä½¿ç”¨ä¸€ä¸ªæ¨¡å‹è§£å†³å¤šä¸ªABSAä»»åŠ¡ï¼Œçº¦æŸè§£ç å°†ç»“æœæé«˜äº†è¶…è¿‡10%ã€‚æˆ‘ä»¬å¯¹ä¸ƒç§è¯­è¨€å’Œå…­ä¸ªABSAä»»åŠ¡è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¶…è¶Šäº†æœ€æ–°æ–¹æ³•å¹¶è®¾ç«‹äº†ä»¥å‰æœªæ¢ç´¢ä»»åŠ¡çš„æ–°åŸºå‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¾®è°ƒæƒ…å†µä¸‹è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚è™½ç„¶LLMsåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸­çš„è¡¨ç°ä¸ä½³ï¼Œä½†å¾®è°ƒå®ç°äº†ä¸è¾ƒå°çš„å¤šè¯­è¨€æ¨¡å‹ç›¸å½“çš„ç»“æœï¼Œå°½ç®¡éœ€è¦æ›´é•¿çš„è®­ç»ƒå’Œæ¨ç†æ—¶é—´ã€‚æˆ‘ä»¬ä¸ºå®é™…åº”ç”¨æä¾›äº†å®é™…å»ºè®®ï¼Œå¢å¼ºäº†è·¨è¯­è¨€ABSAæ–¹æ³•çš„ç†è§£ã€‚æœ¬ç ”ç©¶æ·±å…¥æ¢è®¨äº†è·¨è¯­è¨€ABSAæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œä¸ºè¯¥æŒ‘æˆ˜é¢†åŸŸçš„ç ”ç©¶æä¾›äº†å…ˆè¿›çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10369v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>åŸºäºåºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„çº¦æŸè§£ç æ–°æ–¹æ³•ï¼Œæ— éœ€ä¾èµ–ä¸å¯é çš„ç¿»è¯‘å·¥å…·ï¼Œæé«˜äº†è·¨è¯­è¨€æ–¹é¢çš„æƒ…æ„Ÿåˆ†ææ€§èƒ½ã€‚è¯¥æ–¹æ³•æ”¯æŒå¤šä»»åŠ¡å¤„ç†ï¼Œå¹¶åœ¨ä¸ƒç§è¯­è¨€å’Œå…­ä¸ªæƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œè®¾ç½®æ–°çš„åŸºå‡†ã€‚åŒæ—¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œæä¾›å®é™…åº”ç”¨çš„å»ºè®®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çº¦æŸè§£ç æ–¹æ³•ï¼Œé€‚ç”¨äºè·¨è¯­è¨€çš„æ–¹é¢æƒ…æ„Ÿåˆ†æï¼ˆABSAï¼‰ã€‚</li>
<li>æ–¹æ³•æ— éœ€ä¾èµ–å¤–éƒ¨ç¿»è¯‘å·¥å…·ï¼Œæé«˜äº†è·¨è¯­è¨€æ€§èƒ½ã€‚</li>
<li>æ”¯æŒå¤šä»»åŠ¡å¤„ç†ï¼Œèƒ½åœ¨å¤šä¸ªABSAä»»åŠ¡ä¸Šå–å¾—è‰¯å¥½è¡¨ç°ã€‚</li>
<li>åœ¨ä¸ƒä¸ªè¯­è¨€å’Œå…­ä¸ªABSAä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬åœºæ™¯ä¸‹çš„è¡¨ç°ä¸ä½³ï¼Œä½†åœ¨å¾®è°ƒåç»“æœå…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>æä¾›äº†å…³äºè·¨è¯­è¨€ABSAæ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„å»ºè®®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0052cb3d101f80c7e6e8c47e0ab582d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-879c31ddf7338f94910d013bde346192.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning"><a href="#SynBrain-Enhancing-Visual-to-fMRI-Synthesis-via-Probabilistic-Representation-Learning" class="headerlink" title="SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic   Representation Learning"></a>SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic   Representation Learning</h2><p><strong>Authors:Weijian Mai, Jiamin Wu, Yu Zhu, Zhouheng Yao, Dongzhan Zhou, Andrew F. Luo, Qihao Zheng, Wanli Ouyang, Chunfeng Song</strong></p>
<p>Deciphering how visual stimuli are transformed into cortical responses is a fundamental challenge in computational neuroscience. This visual-to-neural mapping is inherently a one-to-many relationship, as identical visual inputs reliably evoke variable hemodynamic responses across trials, contexts, and subjects. However, existing deterministic methods struggle to simultaneously model this biological variability while capturing the underlying functional consistency that encodes stimulus information. To address these limitations, we propose SynBrain, a generative framework that simulates the transformation from visual semantics to neural responses in a probabilistic and biologically interpretable manner. SynBrain introduces two key components: (i) BrainVAE models neural representations as continuous probability distributions via probabilistic learning while maintaining functional consistency through visual semantic constraints; (ii) A Semantic-to-Neural Mapper acts as a semantic transmission pathway, projecting visual semantics into the neural response manifold to facilitate high-fidelity fMRI synthesis. Experimental results demonstrate that SynBrain surpasses state-of-the-art methods in subject-specific visual-to-fMRI encoding performance. Furthermore, SynBrain adapts efficiently to new subjects with few-shot data and synthesizes high-quality fMRI signals that are effective in improving data-limited fMRI-to-image decoding performance. Beyond that, SynBrain reveals functional consistency across trials and subjects, with synthesized signals capturing interpretable patterns shaped by biological neural variability. The code will be made publicly available. </p>
<blockquote>
<p>è§£æè§†è§‰åˆºæ¿€å¦‚ä½•è½¬åŒ–ä¸ºçš®å±‚ååº”æ˜¯è®¡ç®—ç¥ç»ç§‘å­¦ä¸­çš„ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ã€‚è¿™ç§è§†è§‰åˆ°ç¥ç»çš„æ˜ å°„æœ¬è´¨ä¸Šæ˜¯ä¸€ç§ä¸€å¯¹åº”å¤šçš„å…³ç³»ï¼Œå› ä¸ºç›¸åŒçš„è§†è§‰è¾“å…¥åœ¨ä¸åŒçš„è¯•éªŒã€æƒ…å¢ƒå’Œå—è¯•è€…ä¸­å¯é åœ°å¼•å‘äº†å¯å˜çš„è¡€æµåŠ¨åŠ›å­¦ååº”ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¡®å®šæ€§æ–¹æ³•å¾ˆéš¾åŒæ—¶æ¨¡æ‹Ÿè¿™ç§ç”Ÿç‰©å˜å¼‚æ€§å’Œç¼–ç åˆºæ¿€ä¿¡æ¯çš„æ½œåœ¨åŠŸèƒ½ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SynBrainï¼Œè¿™æ˜¯ä¸€ä¸ªç”Ÿæˆæ€§æ¡†æ¶ï¼Œä»¥æ¦‚ç‡å’Œç”Ÿç‰©å­¦ä¸Šå¯è§£é‡Šçš„æ–¹å¼æ¨¡æ‹Ÿä»è§†è§‰è¯­ä¹‰åˆ°ç¥ç»ååº”çš„è½¬å˜ã€‚SynBrainæœ‰ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šï¼ˆiï¼‰BrainVAEé€šè¿‡æ¦‚ç‡å­¦ä¹ å°†ç¥ç»è¡¨å¾å»ºæ¨¡ä¸ºè¿ç»­æ¦‚ç‡åˆ†å¸ƒï¼ŒåŒæ—¶é€šè¿‡è§†è§‰è¯­ä¹‰çº¦æŸä¿æŒåŠŸèƒ½ä¸€è‡´æ€§ï¼›ï¼ˆiiï¼‰è¯­ä¹‰åˆ°ç¥ç»æ˜ å°„å™¨å……å½“è¯­ä¹‰ä¼ è¾“é€”å¾„ï¼Œå°†è§†è§‰è¯­ä¹‰æŠ•å°„åˆ°ç¥ç»å“åº”æµå½¢ï¼Œä»¥ä¿ƒè¿›é«˜ä¿çœŸåº¦fMRIåˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynBrainåœ¨ç‰¹å®šå—è¯•è€…çš„è§†è§‰åˆ°fMRIç¼–ç æ€§èƒ½æ–¹é¢è¶…è¶Šäº†æœ€æ–°æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSynBrainèƒ½å¤Ÿé«˜æ•ˆé€‚åº”æ–°çš„å—è¯•è€…å¹¶å¤„ç†å°‘é‡æ•°æ®ï¼Œåˆæˆé«˜è´¨é‡çš„fMRIä¿¡å·ï¼Œæœ‰æ•ˆæé«˜æ•°æ®æœ‰é™çš„fMRIåˆ°å›¾åƒè§£ç æ€§èƒ½ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒSynBrainæ­ç¤ºäº†è¯•éªŒå’Œå—è¯•è€…ä¹‹é—´çš„åŠŸèƒ½ä¸€è‡´æ€§ï¼Œåˆæˆçš„ä¿¡å·æ•æ‰åˆ°äº†ç”±ç”Ÿç‰©ç¥ç»å˜å¼‚æ€§å¡‘é€ çš„å¯è§£é‡Šæ¨¡å¼ã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10298v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSynBrainçš„ç”Ÿæˆæ¡†æ¶ï¼Œä»¥æ¦‚ç‡å’Œå¯ç”Ÿç‰©è§£è¯»çš„æ–¹å¼æ¨¡æ‹Ÿè§†è§‰è¯­ä¹‰åˆ°ç¥ç»å“åº”çš„è½¬åŒ–ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬BrainVAEå’Œè¯­ä¹‰åˆ°ç¥ç»æ˜ å°„å™¨ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒåŠŸèƒ½ä¸€è‡´æ€§çš„åŒæ—¶æ¨¡æ‹Ÿç¥ç»è¡¨ç¤ºçš„è¿ç»­æ¦‚ç‡åˆ†å¸ƒï¼Œå¹¶æŠ•å½±è§†è§‰è¯­ä¹‰åˆ°ç¥ç»å“åº”æµå½¢ä»¥åˆæˆé«˜ä¿çœŸfMRIä¿¡å·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSynBrainåœ¨ç‰¹å®šçš„è§†è§‰åˆ°fMRIç¼–ç æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°é€‚åº”æ–°çš„å—è¯•è€…è¿›è¡Œå°‘æ ·æœ¬æ•°æ®åˆæˆé«˜è´¨é‡fMRIä¿¡å·ã€‚æ­¤å¤–ï¼ŒSynBrainæ­ç¤ºäº†è¯•éªŒå’Œå—è¯•è€…ä¹‹é—´çš„åŠŸèƒ½ä¸€è‡´æ€§ï¼Œåˆæˆçš„ä¿¡å·æ•æ‰åˆ°äº†ç”±ç”Ÿç‰©ç¥ç»å˜å¼‚å¡‘é€ çš„å¯è§£é‡Šæ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynBrainæ˜¯ä¸€ä¸ªç”Ÿæˆæ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿè§†è§‰è¯­ä¹‰åˆ°ç¥ç»å“åº”çš„è½¬åŒ–ï¼Œä»¥æ¦‚ç‡å’Œå¯ç”Ÿç‰©è§£è¯»çš„æ–¹å¼å‘ˆç°è¿™ä¸€è½¬åŒ–è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬BrainVAEå’Œè¯­ä¹‰åˆ°ç¥ç»æ˜ å°„å™¨ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œå…¶ä¸­BrainVAEèƒ½å¤Ÿæ¨¡æ‹Ÿç¥ç»è¡¨ç¤ºçš„è¿ç»­æ¦‚ç‡åˆ†å¸ƒï¼Œè¯­ä¹‰åˆ°ç¥ç»æ˜ å°„å™¨åˆ™æ‰®æ¼”è¯­ä¹‰ä¼ è¾“é€šé“çš„è§’è‰²ã€‚</li>
<li>SynBrainèƒ½å¤Ÿåœ¨ä¿æŒåŠŸèƒ½ä¸€è‡´æ€§çš„åŒæ—¶æ¨¡æ‹Ÿç”Ÿç‰©å˜å¼‚ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSynBrainåœ¨è§†è§‰åˆ°fMRIç¼–ç æ€§èƒ½ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨æ–°å—è¯•è€…çš„å°‘æ ·æœ¬æ•°æ®ä¸Šè¡¨ç°å‡ºé«˜æ•ˆçš„é€‚åº”æ€§ã€‚</li>
<li>SynBrainèƒ½åˆæˆé«˜è´¨é‡çš„fMRIä¿¡å·ï¼Œå¹¶ç”¨äºæé«˜æ•°æ®æœ‰é™æ—¶çš„fMRIåˆ°å›¾åƒè§£ç æ€§èƒ½ã€‚</li>
<li>SynBrainæ­ç¤ºäº†è¯•éªŒå’Œå—è¯•è€…ä¹‹é—´çš„åŠŸèƒ½ä¸€è‡´æ€§ï¼Œå±•ç¤ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-99a65c101276668bb9411cbe68683803.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ee7ddb89a786e4b1d7882ab2d8e09da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2d9b676ccf8eccaae4ff150a7c1b174.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d029bae6dd5ee7aeb397324e42c124b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Deep-Learning-for-Crack-Detection-A-Review-of-Learning-Paradigms-Generalizability-and-Datasets"><a href="#Deep-Learning-for-Crack-Detection-A-Review-of-Learning-Paradigms-Generalizability-and-Datasets" class="headerlink" title="Deep Learning for Crack Detection: A Review of Learning Paradigms,   Generalizability, and Datasets"></a>Deep Learning for Crack Detection: A Review of Learning Paradigms,   Generalizability, and Datasets</h2><p><strong>Authors:Xinan Zhang, Haolin Wang, Yung-An Hsieh, Zhongyu Yang, Anthony Yezzi, Yi-Chang Tsai</strong></p>
<p>Crack detection plays a crucial role in civil infrastructures, including inspection of pavements, buildings, etc., and deep learning has significantly advanced this field in recent years. While numerous technical and review papers exist in this domain, emerging trends are reshaping the landscape. These shifts include transitions in learning paradigms (from fully supervised learning to semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation and fine-tuning foundation models), improvements in generalizability (from single-dataset performance to cross-dataset evaluation), and diversification in dataset reacquisition (from RGB images to specialized sensor-based data). In this review, we systematically analyze these trends and highlight representative works. Additionally, we introduce a new dataset collected with 3D laser scans, 3DCrack, to support future research and conduct extensive benchmarking experiments to establish baselines for commonly used deep learning methodologies, including recent foundation models. Our findings provide insights into the evolving methodologies and future directions in deep learning-based crack detection. Project page: <a target="_blank" rel="noopener" href="https://github.com/nantonzhang/Awesome-Crack-Detection">https://github.com/nantonzhang/Awesome-Crack-Detection</a> </p>
<blockquote>
<p>è£‚ç¼æ£€æµ‹åœ¨æ°‘äº‹åŸºç¡€è®¾æ–½ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼ŒåŒ…æ‹¬è·¯é¢ã€å»ºç­‘ç­‰çš„æ£€æµ‹ï¼Œå¹¶ä¸”è¿‘å¹´æ¥æ·±åº¦å­¦ä¹ å·²åœ¨æ­¤é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ã€‚å°½ç®¡è¯¥é¢†åŸŸå­˜åœ¨å¤§é‡çš„æŠ€æœ¯å’Œç»¼è¿°æ€§è®ºæ–‡ï¼Œä½†æ–°å…´è¶‹åŠ¿æ­£åœ¨æ”¹å˜è¿™ä¸€é¢†åŸŸçš„æ ¼å±€ã€‚è¿™äº›å˜åŒ–åŒ…æ‹¬å­¦ä¹ èŒƒå¼ï¼ˆä»å…¨ç›‘ç£å­¦ä¹ è½¬å˜åˆ°åŠç›‘ç£ã€å¼±ç›‘ç£ã€æ— ç›‘ç£ã€å°æ ·æœ¬ã€åŸŸé€‚åº”å’Œå¾®è°ƒåŸºç¡€æ¨¡å‹ï¼‰ã€é€šç”¨æ€§çš„æé«˜ï¼ˆä»å•ä¸€æ•°æ®é›†æ€§èƒ½åˆ°è·¨æ•°æ®é›†è¯„ä¼°ï¼‰ï¼Œä»¥åŠæ•°æ®é›†é‡æ–°é‡‡é›†çš„å¤šæ ·åŒ–ï¼ˆä»RGBå›¾åƒåˆ°åŸºäºä¸“ä¸šä¼ æ„Ÿå™¨çš„æ•°æ®ï¼‰ã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†è¿™äº›è¶‹åŠ¿ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº†å…·æœ‰ä»£è¡¨æ€§çš„ä½œå“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä½¿ç”¨3Dæ¿€å…‰æ‰«ææ”¶é›†çš„æ–°æ•°æ®é›†â€œ3DCrackâ€ï¼Œä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ï¼Œå¹¶å¯¹å¸¸ç”¨çš„æ·±åº¦å­¦ä¹ æ–¹æ³•è¿›è¡Œå¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æœ€è¿‘çš„åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæä¾›äº†å¯¹ä¸æ–­å‘å±•çš„æ–¹æ³•å’Œæœªæ¥æ·±åº¦å­¦ä¹ è£‚ç¼æ£€æµ‹æ–¹å‘çš„æ–°è§è§£ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/nantonzhang/Awesome-Crack-Detection">https://github.com/nantonzhang/Awesome-Crack-Detection</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10256v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è£‚ç¼æ£€æµ‹é¢†åŸŸå·²å‘æŒ¥é‡è¦ä½œç”¨ï¼Œè¿‘æœŸå‡ºç°ä¼—å¤šè¶‹åŠ¿å˜åŒ–ï¼ŒåŒ…æ‹¬å­¦ä¹ èŒƒå¼è½¬å˜ã€æ³›åŒ–æ€§èƒ½æå‡å’Œå¤šæ ·åŒ–æ•°æ®é›†é‡‡é›†ç­‰ã€‚æœ¬æ–‡ç³»ç»Ÿæ€§åˆ†æè¿™äº›è¶‹åŠ¿å¹¶ä»‹ç»ä»£è¡¨æ€§å·¥ä½œï¼ŒåŒæ—¶æ¨å‡ºæ–°æ•°æ®é›†æ”¯æŒæœªæ¥ç ”ç©¶ï¼Œå¹¶å¼€å±•åŸºå‡†å®éªŒè¯„ä¼°å¸¸ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è£‚ç¼æ£€æµ‹é¢†åŸŸå…·æœ‰å…³é”®ä½œç”¨ï¼Œæ¶‰åŠé“è·¯ã€å»ºç­‘ç­‰åŸºç¡€è®¾æ–½æ£€æµ‹ã€‚</li>
<li>å­¦ä¹ èŒƒå¼è½¬å˜ï¼ŒåŒ…æ‹¬ä»å…¨ç›‘ç£å­¦ä¹ åˆ°å¤šç§è¡ç”Ÿå½¢å¼ã€‚</li>
<li>æ³›åŒ–æ€§èƒ½æå‡ï¼Œä¸ä»…å±€é™äºå•ä¸€æ•°æ®é›†è¡¨ç°ã€‚</li>
<li>æ•°æ®é›†é‡‡é›†æ–¹å¼è¶‹å‘å¤šæ ·åŒ–ï¼Œå¼•å…¥ç‰¹æ®Šä¼ æ„Ÿå™¨æ•°æ®ã€‚</li>
<li>æ¨å‡ºåŸºäºä¸‰ç»´æ¿€å…‰æ‰«æçš„æ–°æ•°æ®é›†3DCrackï¼Œæ”¯æŒæœªæ¥ç ”ç©¶ã€‚</li>
<li>åŸºå‡†å®éªŒè¯„ä¼°å¸¸è§æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02fa315276506fa9b6891490ba772a26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-477b5f821b568aedf7731a91c6334aa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4d570672b8f5db97a94976dead4504d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8c6adf44ca36c20c2fcdd0aae31cd27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5be5415f943b66740546ec48f095744.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8300b4475656e6ae627d34c794e112bf.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Stochastic-based-Patch-Filtering-for-Few-Shot-Learning"><a href="#Stochastic-based-Patch-Filtering-for-Few-Shot-Learning" class="headerlink" title="Stochastic-based Patch Filtering for Few-Shot Learning"></a>Stochastic-based Patch Filtering for Few-Shot Learning</h2><p><strong>Authors:Javier Rodenas, Eduardo Aguilar, Petia Radeva</strong></p>
<p>Food images present unique challenges for few-shot learning models due to their visual complexity and variability. For instance, a pasta dish might appear with various garnishes on different plates and in diverse lighting conditions and camera perspectives. This problem leads to losing focus on the most important elements when comparing the query with support images, resulting in misclassification. To address this issue, we propose Stochastic-based Patch Filtering for Few-Shot Learning (SPFF) to attend to the patch embeddings that show greater correlation with the class representation. The key concept of SPFF involves the stochastic filtering of patch embeddings, where patches less similar to the class-aware embedding are more likely to be discarded. With patch embedding filtered according to the probability of appearance, we use a similarity matrix that quantifies the relationship between the query image and its respective support images. Through a qualitative analysis, we demonstrate that SPFF effectively focuses on patches where class-specific food features are most prominent while successfully filtering out non-relevant patches. We validate our approach through extensive experiments on few-shot classification benchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing SoA methods. </p>
<blockquote>
<p>é£Ÿå“å›¾åƒä¸ºå°æ ·å­¦ä¹ æ¨¡å‹å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬çš„è§†è§‰å¤æ‚æ€§å’Œå¯å˜æ€§ã€‚ä¾‹å¦‚ï¼Œä¸€é“æ„å¤§åˆ©é¢å¯èƒ½ä¼šå‡ºç°åœ¨ä¸åŒçš„ç›˜å­ä¸Šï¼Œæœ‰ç€ä¸åŒçš„é…æ–™ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„ç…§æ˜æ¡ä»¶å’Œç›¸æœºè§†è§’ä¸‹å‘ˆç°ã€‚è¿™ä¸ªé—®é¢˜å¯¼è‡´åœ¨æŸ¥è¯¢ä¸æ”¯æŒå›¾åƒè¿›è¡Œæ¯”è¾ƒæ—¶å¤±å»å¯¹æœ€é‡è¦å…ƒç´ çš„å…³æ³¨ï¼Œä»è€Œå¯¼è‡´è¯¯åˆ†ç±»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºéšæœºæ€§çš„å°æ ·å­¦ä¹ è¡¥ä¸è¿‡æ»¤ï¼ˆSPFFï¼‰æ–¹æ³•ï¼Œä»¥å…³æ³¨ä¸ç±»åˆ«è¡¨ç¤ºç›¸å…³æ€§æ›´å¤§çš„è¡¥ä¸åµŒå…¥ã€‚SPFFçš„å…³é”®æ¦‚å¿µæ¶‰åŠè¡¥ä¸åµŒå…¥çš„éšæœºè¿‡æ»¤ï¼Œå…¶ä¸­ä¸ç±»åˆ«æ„ŸçŸ¥åµŒå…¥ä¸å¤ªç›¸ä¼¼çš„è¡¥ä¸æ›´æœ‰å¯èƒ½è¢«ä¸¢å¼ƒã€‚æ ¹æ®å‡ºç°æ¦‚ç‡è¿‡æ»¤è¡¥ä¸åµŒå…¥åï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸ä¼¼åº¦çŸ©é˜µæ¥é‡åŒ–æŸ¥è¯¢å›¾åƒä¸å…¶ç›¸åº”çš„æ”¯æŒå›¾åƒä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡å®šæ€§åˆ†æï¼Œæˆ‘ä»¬è¯æ˜SPFFå¯ä»¥æœ‰æ•ˆåœ°å…³æ³¨é£Ÿå“ç‰¹å¾æœ€çªå‡ºçš„è¡¥ä¸ï¼ŒåŒæ—¶æˆåŠŸè¿‡æ»¤æ‰ä¸ç›¸å…³çš„è¡¥ä¸ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒåœ¨å°‘é‡æ ·æœ¬åˆ†ç±»åŸºå‡†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šFood-101ã€VireoFood-172å’ŒUECFood-256ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10066v1">PDF</a> CVPR Workshop MetaFood 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æŒ‡å‡ºé£Ÿå“å›¾åƒå¯¹å°‘æ ·æœ¬å­¦ä¹ æ¨¡å‹æ„æˆç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå› å›¾åƒè§†è§‰å¤æ‚å¤šå˜ã€‚å¦‚é¢é£Ÿæ–™ç†åœ¨ä¸åŒç›˜å­ä¸Šå±•ç¤ºå„ç§é…èœï¼Œç…§æ˜æ¡ä»¶ä¸ç›¸æœºè§†è§’å„å¼‚ï¼Œå¯¼è‡´åœ¨æŸ¥è¯¢ä¸æ ·æœ¬å›¾åƒæ¯”è¾ƒæ—¶å®¹æ˜“å¤±å»å¯¹é‡ç‚¹å…ƒç´ çš„å…³æ³¨ï¼Œä»è€Œäº§ç”Ÿè¯¯åˆ¤ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåŸºäºéšæœºæ€§çš„å°‘æ ·æœ¬å­¦ä¹ ï¼ˆSPFFï¼‰æ–¹æ³•ï¼Œé‡ç‚¹å…³æ³¨ä¸ç±»åˆ«è¡¨å¾æ›´ç›¸å…³çš„è¡¥ä¸åµŒå…¥ã€‚SPFFçš„å…³é”®æ¦‚å¿µåœ¨äºéšæœºè¿‡æ»¤è¡¥ä¸åµŒå…¥ï¼Œå¯¹äºä¸ç±»åˆ«æ„ŸçŸ¥åµŒå…¥å·®å¼‚è¾ƒå¤§çš„è¡¥ä¸æ›´å®¹æ˜“è¢«èˆå¼ƒã€‚ç»è¿‡å‡ºç°çš„æ¦‚ç‡ç­›é€‰çš„è¡¥ä¸åµŒå…¥æ•°æ®ï¼Œåˆ©ç”¨ç›¸ä¼¼æ€§çŸ©é˜µé‡åŒ–æŸ¥è¯¢å›¾åƒä¸å¯¹åº”æ ·æœ¬å›¾åƒä¹‹é—´çš„å…³ç³»ã€‚å®šæ€§åˆ†æè¡¨æ˜ï¼ŒSPFFèƒ½å¤Ÿæœ‰æ•ˆèšç„¦äºç±»ç‰¹å®šé£Ÿå“ç‰¹å¾æœ€çªå‡ºçš„è¡¥ä¸ï¼ŒåŒæ—¶æˆåŠŸè¿‡æ»¤æ‰éç›¸å…³è¡¥ä¸ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œåœ¨å°‘æ ·æœ¬åˆ†ç±»åŸºå‡†æ•°æ®é›†Food-101ã€VireoFood-172å’ŒUECFood-256ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬çš„å…³é”®è§è§£è¦ç‚¹ï¼š</p>
<ol>
<li>é£Ÿå“å›¾åƒå¯¹å°‘æ ·æœ¬å­¦ä¹ æ¨¡å‹æå‡ºäº†ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå› å…¶è§†è§‰ä¸Šçš„å¤æ‚æ€§å’Œå˜åŒ–å¤šæ ·æ€§ã€‚</li>
<li>ä¸åŒçš„é¢é£Ÿæ–™ç†åœ¨ä¸åŒç›˜å­ä¸Šå‘ˆç°å¤šç§é…èœï¼Œç…§æ˜å’Œç›¸æœºè§†è§’çš„å˜åŒ–å¯¼è‡´è¯†åˆ«å›°éš¾ã€‚</li>
<li>ä¸ºè§£å†³è¯¯åˆ¤é—®é¢˜ï¼Œæå‡ºäº†åŸºäºéšæœºæ€§çš„å°‘æ ·æœ¬å­¦ä¹ ï¼ˆSPFFï¼‰æ–¹æ³•ã€‚</li>
<li>SPFFé€šè¿‡éšæœºè¿‡æ»¤è¡¥ä¸åµŒå…¥æ•°æ®æ¥å…³æ³¨ä¸ç±»åˆ«è¡¨å¾æ›´ç›¸å…³çš„éƒ¨åˆ†ã€‚</li>
<li>SPFFé€šè¿‡ç›¸ä¼¼æ€§çŸ©é˜µé‡åŒ–æŸ¥è¯¢å›¾åƒä¸æ ·æœ¬å›¾åƒä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>å®šæ€§åˆ†æè¯æ˜äº†SPFFèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«é£Ÿå“çš„å…³é”®ç‰¹å¾å¹¶è¿‡æ»¤æ‰éç›¸å…³å…ƒç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-82d0beddc412344b502e5406070a6fe1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-880641f592ff64877d4f00fc34e195fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2036d058792151053501a8deceecc79b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c43375d7c296c1983e9edecce39e8e3f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MOC-Meta-Optimized-Classifier-for-Few-Shot-Whole-Slide-Image-Classification"><a href="#MOC-Meta-Optimized-Classifier-for-Few-Shot-Whole-Slide-Image-Classification" class="headerlink" title="MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image   Classification"></a>MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image   Classification</h2><p><strong>Authors:Tianqi Xiang, Yi Li, Qixiang Zhang, Xiaomeng Li</strong></p>
<p>Recent advances in histopathology vision-language foundation models (VLFMs) have shown promise in addressing data scarcity for whole slide image (WSI) classification via zero-shot adaptation. However, these methods remain outperformed by conventional multiple instance learning (MIL) approaches trained on large datasets, motivating recent efforts to enhance VLFM-based WSI classification through fewshot learning paradigms. While existing few-shot methods improve diagnostic accuracy with limited annotations, their reliance on conventional classifier designs introduces critical vulnerabilities to data scarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC) comprising two core components: (1) a meta-learner that automatically optimizes a classifier configuration from a mixture of candidate classifiers and (2) a classifier bank housing diverse candidate classifiers to enable a holistic pathological interpretation. Extensive experiments demonstrate that MOC outperforms prior arts in multiple few-shot benchmarks. Notably, on the TCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art few-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions, offering a critical advancement for clinical deployments where diagnostic training data is severely limited. Code is available at <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/MOC">https://github.com/xmed-lab/MOC</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œç»„ç»‡ç—…ç†å­¦è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆVLFMsï¼‰çš„è¿›æ­¥åœ¨é€šè¿‡é›¶æ ·æœ¬é€‚åº”è§£å†³å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ä¸Šè¡¨ç°å‡ºäº†å‰æ™¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»è¢«åœ¨å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„å¸¸è§„å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ–¹æ³•æ‰€è¶…è¶Šï¼Œè¿™ä¿ƒä½¿æœ€è¿‘çš„åŠªåŠ›é€šè¿‡å°æ ·å­¦ä¹ æ¨¡å¼å¢å¼ºåŸºäºVLFMçš„WSIåˆ†ç±»ã€‚è™½ç„¶ç°æœ‰çš„å°æ ·æ–¹æ³•å¯ä»¥åœ¨æœ‰é™çš„æ³¨é‡Šä¸‹æé«˜è¯Šæ–­å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬å¯¹å¸¸è§„åˆ†ç±»å™¨è®¾è®¡çš„ä¾èµ–å¼•å‘äº†æ•°æ®ç¨€ç¼ºçš„å…³é”®æ¼æ´ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆçš„å…ƒä¼˜åŒ–åˆ†ç±»å™¨ï¼ˆMOCï¼‰ï¼šï¼ˆ1ï¼‰å…ƒå­¦ä¹ è€…ä¼šè‡ªåŠ¨ä¼˜åŒ–æ¥è‡ªæ··åˆå€™é€‰åˆ†ç±»å™¨çš„åˆ†ç±»å™¨é…ç½®ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªåŒ…å«å¤šç§å€™é€‰åˆ†ç±»å™¨çš„åˆ†ç±»å™¨åº“ï¼Œä»¥å®ç°å…¨é¢çš„ç—…ç†è§£é‡Šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMOCåœ¨å¤šä¸ªå°æ ·åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä»¥å‰çš„æŠ€æœ¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨TCGA-NSCLCåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMOCæ¯”æœ€æ–°çš„åŸºäºå°æ ·çš„VLFMæ–¹æ³•æé«˜äº†10.4%çš„AUCï¼Œåœ¨å•æ ·æœ¬æ¡ä»¶ä¸‹æé«˜å¹…åº¦é«˜è¾¾26.25%ï¼Œè¿™å¯¹äºè¯Šæ–­è®­ç»ƒæ•°æ®ä¸¥é‡å—é™çš„ä¸´åºŠéƒ¨ç½²è€Œè¨€æ˜¯ä¸€ä¸ªé‡è¦çš„è¿›æ­¥ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/MOC%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xmed-lab/MOCæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09967v1">PDF</a> Accepted in MICCAI 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç»„ç»‡ç—…ç†å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæœ€è¿‘çš„åŸºäºè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„é›¶æ ·æœ¬é€‚åº”ç­–ç•¥è™½ç„¶å±•ç°å‡ºäº†æ½œåŠ›ï¼Œä½†ä»æ— æ³•åŒ¹æ•Œä¼ ç»Ÿçš„å¤šä»»åŠ¡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ–¹æ³•ã€‚ä¸ºäº†æ”¹è¿›åŸºäºè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„WSIåˆ†ç±»ï¼Œç ”ç©¶è€…æå‡ºäº†é€šè¿‡å°æ ·æœ¬å­¦ä¹ æ¨¡å¼æ¥å¢å¼ºæ€§èƒ½çš„ç­–ç•¥ã€‚ç°æœ‰çš„å°æ ·æœ¬æ–¹æ³•è™½ç„¶èƒ½åœ¨æœ‰é™æ ‡æ³¨çš„æƒ…å†µä¸‹æé«˜è¯Šæ–­å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬ä¾èµ–äºä¼ ç»Ÿåˆ†ç±»å™¨çš„è®¾è®¡ï¼Œåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å­˜åœ¨é‡å¤§æ¼æ´ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶çš„å…ƒä¼˜åŒ–åˆ†ç±»å™¨ï¼ˆMOCï¼‰ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªå…ƒå­¦ä¹ è€…ï¼Œå¯ä»¥ä»å€™é€‰åˆ†ç±»å™¨çš„ç»„åˆä¸­è‡ªåŠ¨ä¼˜åŒ–åˆ†ç±»å™¨é…ç½®ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªåˆ†ç±»å™¨åº“ï¼ŒåŒ…å«å„ç§å€™é€‰åˆ†ç±»å™¨ä»¥å®ç°å…¨é¢çš„ç—…ç†è§£é‡Šã€‚å®éªŒè¡¨æ˜ï¼ŒMOCåœ¨å¤šä¸ªå°æ ·æœ¬æµ‹è¯•ä¸­è¶…è¿‡äº†å…ˆå‰çš„æŠ€æœ¯ã€‚åœ¨TCGA-NSCLCæµ‹è¯•ä¸­ï¼ŒMOCç›¸è¾ƒäºæœ€æ–°çš„åŸºäºå°æ ·æœ¬è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ–¹æ³•æé«˜äº†AUCå€¼10.4%ï¼Œåœ¨å•æ ·æœ¬æ¡ä»¶ä¸‹æå‡ç”šè‡³é«˜è¾¾26.25%ï¼Œè¿™å¯¹äºè¯Šæ–­è®­ç»ƒæ•°æ®ä¸¥é‡å—é™çš„ä¸´åºŠéƒ¨ç½²ç¯å¢ƒæ¥è¯´æ˜¯ä¸€ä¸ªé‡å¤§è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨å¤„ç†ç»„ç»‡ç—…ç†å­¦å›¾åƒåˆ†ç±»æ—¶é¢ä¸´æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
<li>å…ƒä¼˜åŒ–åˆ†ç±»å™¨ï¼ˆMOCï¼‰ç”±ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆï¼šå…ƒå­¦ä¹ å™¨å’Œåˆ†ç±»å™¨åº“ã€‚</li>
<li>å…ƒå­¦ä¹ è€…èƒ½å¤Ÿè‡ªåŠ¨ä»å€™é€‰åˆ†ç±»å™¨çš„ç»„åˆä¸­ä¼˜åŒ–åˆ†ç±»å™¨é…ç½®ã€‚</li>
<li>åˆ†ç±»å™¨åº“åŒ…å«å¤šç§å€™é€‰åˆ†ç±»å™¨ï¼Œä»¥å®ç°å…¨é¢çš„ç—…ç†è§£é‡Šã€‚</li>
<li>MOCåœ¨å¤šä¸ªå°æ ·æœ¬æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
<li>åœ¨TCGA-NSCLCæµ‹è¯•ä¸­ï¼ŒMOCç›¸è¾ƒäºåŸºäºå°æ ·æœ¬è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ–¹æ³•æé«˜äº†è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b06b099d6b1e2a111a926a96b5d0d9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf7b35f5ed0d248da2801e0b3f8c4ebc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-866131f8f226f5087ba6a80863bd6679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f822e446b36d9d8df550a8d63232427f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DSS-Prompt-Dynamic-Static-Synergistic-Prompting-for-Few-Shot-Class-Incremental-Learning"><a href="#DSS-Prompt-Dynamic-Static-Synergistic-Prompting-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot   Class-Incremental Learning"></a>DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot   Class-Incremental Learning</h2><p><strong>Authors:Linpu He, Yanan Li, Bingze Li, Elvis Han Cui, Donghui Wang</strong></p>
<p>Learning from large-scale pre-trained models with strong generalization ability has shown remarkable success in a wide range of downstream tasks recently, but it is still underexplored in the challenging few-shot class-incremental learning (FSCIL) task. It aims to continually learn new concepts from limited training samples without forgetting the old ones at the same time. In this paper, we introduce DSS-Prompt, a simple yet effective approach that transforms the pre-trained Vision Transformer with minimal modifications in the way of prompts into a strong FSCIL classifier. Concretely, we synergistically utilize two complementary types of prompts in each Transformer block: static prompts to bridge the domain gap between the pre-training and downstream datasets, thus enabling better adaption; and dynamic prompts to capture instance-aware semantics, thus enabling easy transfer from base to novel classes. Specially, to generate dynamic prompts, we leverage a pre-trained multi-modal model to extract input-related diverse semantics, thereby generating complementary input-aware prompts, and then adaptively adjust their importance across different layers. In this way, on top of the prompted visual embeddings, a simple prototype classifier can beat state-of-the-arts without further training on the incremental tasks. We conduct extensive experiments on four benchmarks to validate the effectiveness of our DSS-Prompt and show that it consistently achieves better performance than existing approaches on all datasets and can alleviate the catastrophic forgetting issue as well. </p>
<blockquote>
<p>ä»å…·æœ‰å¼ºå¤§æ³›åŒ–èƒ½åŠ›çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ä¸­å­¦ä¹ ï¼Œæœ€è¿‘åœ¨å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰ä»»åŠ¡ä¸­ä»ç„¶æ¢ç´¢ä¸è¶³ã€‚å…¶ç›®æ ‡æ˜¯ä»æœ‰é™çš„è®­ç»ƒæ ·æœ¬ä¸­æŒç»­å­¦ä¹ æ–°æ¦‚å¿µï¼ŒåŒæ—¶ä¸å¿˜è®°æ—§çŸ¥è¯†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DSS-Promptï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡æœ€å°åŒ–çš„æç¤ºæ–¹å¼å°†é¢„è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨è½¬å˜ä¸ºå¼ºå¤§çš„FSCILåˆ†ç±»å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ååŒåˆ©ç”¨Transformerå—ä¸­çš„ä¸¤ç§äº’è¡¥æç¤ºï¼šé™æ€æç¤ºï¼Œä»¥ç¼©å°é¢„è®­ç»ƒå’Œä¸‹æ¸¸æ•°æ®é›†ä¹‹é—´çš„åŸŸå·®è·ï¼Œä»è€Œå®ç°æ›´å¥½çš„é€‚åº”ï¼›åŠ¨æ€æç¤ºï¼Œä»¥æ•è·å®ä¾‹æ„ŸçŸ¥è¯­ä¹‰ï¼Œä»è€Œå®ç°ä»åŸºç¡€ç±»åˆ«åˆ°æ–°é¢–ç±»åˆ«çš„è½»æ¾è¿ç§»ã€‚ç‰¹åˆ«æ˜¯ï¼Œä¸ºäº†ç”ŸæˆåŠ¨æ€æç¤ºï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„å¤šæ¨¡æ€æ¨¡å‹æ¥æå–ä¸è¾“å…¥ç›¸å…³çš„å¤šæ ·è¯­ä¹‰ï¼Œä»è€Œäº§ç”Ÿäº’è¡¥çš„è¾“å…¥æ„ŸçŸ¥æç¤ºï¼Œç„¶åè‡ªé€‚åº”åœ°è°ƒæ•´å®ƒä»¬åœ¨ä¸åŒå±‚ä¸­çš„é‡è¦æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒåŸºäºæç¤ºçš„è§†è§‰åµŒå…¥ï¼Œä¸€ä¸ªç®€å•çš„åŸå‹åˆ†ç±»å™¨å¯ä»¥åœ¨æ— éœ€å¯¹å¢é‡ä»»åŠ¡è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒçš„æƒ…å†µä¸‹è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚æˆ‘ä»¬åœ¨å››ä¸ªåŸºå‡†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†DSS-Promptçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¡¨æ˜å®ƒåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå§‹ç»ˆå®ç°äº†æ¯”ç°æœ‰æ–¹æ³•æ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶å¯ä»¥ç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09785v1">PDF</a> Accepted to ACMMM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰ä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨å¤§å‹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå­¦ä¹ çš„æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥DSS-PromptæŠ€æœ¯ï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨è¿›è¡Œå¾®è°ƒï¼Œè½¬åŒ–ä¸ºå¼ºå¤§çš„FSCILåˆ†ç±»å™¨ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨é™æ€æç¤ºå’ŒåŠ¨æ€æç¤ºï¼Œç¼©å°äº†é¢„è®­ç»ƒå’Œä¸‹æ¸¸æ•°æ®é›†ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œå¹¶æ•æ‰å®ä¾‹æ„ŸçŸ¥è¯­ä¹‰ï¼Œä»è€Œå®ç°äº†ä»åŸºç¡€ç±»åˆ«åˆ°æ–°é¢–ç±»åˆ«çš„è½»æ¾è¿ç§»ã€‚é€šè¿‡é¢„è®­ç»ƒçš„å¤šæ¨¡æ€æ¨¡å‹ç”ŸæˆåŠ¨æ€æç¤ºï¼Œå¹¶è‡ªé€‚åº”è°ƒæ•´å…¶åœ¨ä¸åŒå±‚çš„é‡è¦æ€§ã€‚å®éªŒè¯æ˜ï¼ŒDSS-Promptåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DSS-PromptæŠ€æœ¯å°†é¢„è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨è½¬åŒ–ä¸ºå°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰åˆ†ç±»å™¨ã€‚</li>
<li>DSS-Promptç»“åˆäº†é™æ€æç¤ºå’ŒåŠ¨æ€æç¤ºæ¥é€‚åº”FSCILä»»åŠ¡ã€‚</li>
<li>é™æ€æç¤ºç¼©å°äº†é¢„è®­ç»ƒå’Œä¸‹æ¸¸æ•°æ®é›†ä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚</li>
<li>åŠ¨æ€æç¤ºé€šè¿‡æ•æ‰å®ä¾‹æ„ŸçŸ¥è¯­ä¹‰ï¼Œå®ç°äº†ä»åŸºç¡€ç±»åˆ«åˆ°æ–°é¢–ç±»åˆ«çš„è¿ç§»ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤šæ¨¡æ€æ¨¡å‹ç”ŸæˆåŠ¨æ€æç¤ºï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
<li>DSS-Promptåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-23e17bb1974619be7b4e59ce5f8aa86e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4392d6a946e05eed2f7c1c401b45cb93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-84996b577390ef714d83db092a04de8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5dcd1900d2ce5ecd184266e8b3349dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a07a24d7cbe5292a03b6f667d982d7d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-888a77e0270999da81cef26a5754532b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-027920dc4f40478a969149e9e5140d68.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Slot-Attention-based-Feature-Filtering-for-Few-Shot-Learning"><a href="#Slot-Attention-based-Feature-Filtering-for-Few-Shot-Learning" class="headerlink" title="Slot Attention-based Feature Filtering for Few-Shot Learning"></a>Slot Attention-based Feature Filtering for Few-Shot Learning</h2><p><strong>Authors:Javier Rodenas, Eduardo Aguilar, Petia Radeva</strong></p>
<p>Irrelevant features can significantly degrade few-shot learn ing performance. This problem is used to match queries and support images based on meaningful similarities despite the limited data. However, in this process, non-relevant fea tures such as background elements can easily lead to confu sion and misclassification. To address this issue, we pro pose Slot Attention-based Feature Filtering for Few-Shot Learning (SAFF) that leverages slot attention mechanisms to discriminate and filter weak features, thereby improving few-shot classification performance. The key innovation of SAFF lies in its integration of slot attention with patch em beddings, unifying class-aware slots into a single attention mechanism to filter irrelevant features effectively. We intro duce a similarity matrix that computes across support and query images to quantify the relevance of filtered embed dings for classification. Through experiments, we demon strate that Slot Attention performs better than other atten tion mechanisms, capturing discriminative features while reducing irrelevant information. We validate our approach through extensive experiments on few-shot learning bench marks: CIFAR-FS, FC100, miniImageNet and tieredIma geNet, outperforming several state-of-the-art methods. </p>
<blockquote>
<p>ä¸ç›¸å…³çš„ç‰¹å¾å¯èƒ½ä¼šæ˜¾è‘—å½±å“å°æ ·æœ¬å­¦ä¹ çš„æ€§èƒ½ã€‚æ­¤é—®é¢˜çš„è§£å†³æ–¹æ¡ˆæ˜¯ç”¨äºåœ¨æœ‰é™æ•°æ®çš„åŸºç¡€ä¸Šï¼Œæ ¹æ®æœ‰æ„ä¹‰çš„ç›¸ä¼¼æ€§æ¥åŒ¹é…æŸ¥è¯¢å›¾åƒå’Œæ”¯æŒå›¾åƒã€‚ç„¶è€Œï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œä¸ç›¸å…³çš„ç‰¹å¾ï¼ˆå¦‚èƒŒæ™¯å…ƒç´ ï¼‰å¾ˆå®¹æ˜“å¼•èµ·æ··æ·†å’Œè¯¯åˆ†ç±»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ’æ§½æ³¨æ„åŠ›çš„å°æ ·æœ¬å­¦ä¹ ç‰¹å¾è¿‡æ»¤ï¼ˆSAFFï¼‰ï¼Œå®ƒåˆ©ç”¨æ’æ§½æ³¨æ„åŠ›æœºåˆ¶æ¥åŒºåˆ†å’Œè¿‡æ»¤å¼±ç‰¹å¾ï¼Œä»è€Œæé«˜å°æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚SAFFçš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºå®ƒå°†æ’æ§½æ³¨æ„åŠ›ä¸è¡¥ä¸åµŒå…¥ç›¸ç»“åˆï¼Œå°†ç±»æ„ŸçŸ¥æ’æ§½ç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæœ‰æ•ˆåœ°è¿‡æ»¤æ‰ä¸ç›¸å…³çš„ç‰¹å¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç›¸ä¼¼åº¦çŸ©é˜µï¼Œè¯¥çŸ©é˜µè·¨æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒè¿›è¡Œè®¡ç®—ï¼Œä»¥é‡åŒ–è¿‡æ»¤åµŒå…¥ç‰©åœ¨åˆ†ç±»ä¸­çš„ç›¸å…³æ€§ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ’æ§½æ³¨æ„åŠ›æ¯”å…¶ä»–æ³¨æ„åŠ›æœºåˆ¶è¡¨ç°æ›´å¥½ï¼Œèƒ½å¤Ÿæ•æ‰åŒºåˆ†ç‰¹å¾åŒæ—¶å‡å°‘æ— å…³ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨å°æ ·æœ¬å­¦ä¹ çš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬CIFAR-FSã€FC100ã€miniImageNetå’ŒtieredImageNetï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå‡ ç§æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09699v1">PDF</a> CVPR Workshop LatinX 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„ç‰¹å¾é€‰æ‹©é—®é¢˜ï¼ŒæŒ‡å‡ºæ— å…³ç‰¹å¾ä¼šä¸¥é‡å½±å“æ¨¡å‹æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†åŸºäºæ’æ§½æ³¨æ„åŠ›çš„ç‰¹å¾è¿‡æ»¤æ–¹æ³•ï¼ˆSAFFï¼‰ï¼Œé€šè¿‡ç»“åˆæ’æ§½æ³¨æ„åŠ›å’Œè¡¥ä¸åµŒå…¥ï¼Œæœ‰æ•ˆè¿‡æ»¤æ‰ä¸ç›¸å…³ç‰¹å¾ï¼Œæé«˜å°‘æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒSAFFåœ¨å¤šä¸ªå°‘æ ·æœ¬å­¦ä¹ åŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— å…³ç‰¹å¾å¯¹å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>æå‡ºåŸºäºæ’æ§½æ³¨æ„åŠ›çš„ç‰¹å¾è¿‡æ»¤æ–¹æ³•ï¼ˆSAFFï¼‰ä»¥æ”¹å–„å°‘æ ·æœ¬åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>SAFFé€šè¿‡ç»“åˆæ’æ§½æ³¨æ„åŠ›å’Œè¡¥ä¸åµŒå…¥ï¼Œç»Ÿä¸€ç±»æ„ŸçŸ¥æ’æ§½ï¼Œæœ‰æ•ˆè¿‡æ»¤æ‰ä¸ç›¸å…³ç‰¹å¾ã€‚</li>
<li>å¼•å…¥ç›¸ä¼¼åº¦çŸ©é˜µæ¥è®¡ç®—æ”¯æŒå›¾åƒå’ŒæŸ¥è¯¢å›¾åƒä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»¥é‡åŒ–è¿‡æ»¤åµŒå…¥çš„åˆ†ç±»é‡è¦æ€§ã€‚</li>
<li>æ’æ§½æ³¨æ„åŠ›æœºåˆ¶åœ¨æ•è·åˆ¤åˆ«ç‰¹å¾çš„åŒæ—¶å‡å°‘äº†æ— å…³ä¿¡æ¯ã€‚</li>
<li>åœ¨å¤šä¸ªå°‘æ ·æœ¬å­¦ä¹ åŸºå‡†æµ‹è¯•é›†ä¸Šï¼ŒSAFFè¡¨ç°ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d6851298b16eb3cbe8659010533bd52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa2d14040fb4c157fd83161fd0a29eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77414d83b68a566ffec1a31d1bde70d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa190912fc79db5e6548a22c1335c1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d814c8a789f8c6cd29f50f7fff524c2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Leveraging-Failed-Samples-A-Few-Shot-and-Training-Free-Framework-for-Generalized-Deepfake-Detection"><a href="#Leveraging-Failed-Samples-A-Few-Shot-and-Training-Free-Framework-for-Generalized-Deepfake-Detection" class="headerlink" title="Leveraging Failed Samples: A Few-Shot and Training-Free Framework for   Generalized Deepfake Detection"></a>Leveraging Failed Samples: A Few-Shot and Training-Free Framework for   Generalized Deepfake Detection</h2><p><strong>Authors:Shibo Yao, Renshuai Tao, Xiaolong Zheng, Chao Liang, Chunjie Zhang</strong></p>
<p>Recent deepfake detection studies often treat unseen sample detection as a <code>zero-shot&quot; task, training on images generated by known models but generalizing to unknown ones. A key real-world challenge arises when a model performs poorly on unknown samples, yet these samples remain available for analysis. This highlights that it should be approached as a </code>few-shotâ€ task, where effectively utilizing a small number of samples can lead to significant improvement. Unlike typical few-shot tasks focused on semantic understanding, deepfake detection prioritizes image realism, which closely mirrors real-world distributions. In this work, we propose the Few-shot Training-free Network (FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet differs from traditional methods that rely on large-scale known data for training. Instead, FTNet uses only one fake samplefrom an evaluation set, mimicking the scenario where new samples emerge in the real world and can be gathered for use, without any training or parameter updates. During evaluation, each test sample is compared to the known fake and real samples, and it is classified based on the category of the nearest sample. We conduct a comprehensive analysis of AI-generated images from 29 different generative models and achieve a new SoTA performance, with an average improvement of 8.7% compared to existing methods. This work introduces a fresh perspective on real-world deepfake detection: when the model struggles to generalize on a few-shot sample, leveraging the failed samples leads to better performance. </p>
<blockquote>
<p>æœ€è¿‘å…³äºæ·±åº¦ä¼ªé€ æ£€æµ‹çš„ç ”ç©¶é€šå¸¸å°†æœªè§æ ·æœ¬æ£€æµ‹è§†ä¸ºâ€œé›¶æ ·æœ¬â€ä»»åŠ¡ï¼Œé€šè¿‡åœ¨å·²çŸ¥æ¨¡å‹ç”Ÿæˆçš„å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œç„¶åæ¨å¹¿åˆ°æœªçŸ¥æ¨¡å‹ã€‚å½“æ¨¡å‹åœ¨æœªçŸ¥æ ·æœ¬ä¸Šè¡¨ç°ä¸ä½³æ—¶ï¼Œç°å®ä¸–ç•Œçš„å…³é”®æŒ‘æˆ˜å°±ä¼šå‡ºç°ï¼Œä½†è¿™äº›æ ·æœ¬ä»ç„¶å¯ç”¨äºåˆ†æã€‚è¿™å¼ºè°ƒåº”è¯¥å°†å…¶è§†ä¸ºä¸€ä¸ªâ€œå°æ ·æœ¬â€ä»»åŠ¡ï¼Œæœ‰æ•ˆåˆ©ç”¨å°‘é‡æ ·æœ¬å¯ä»¥å¯¼è‡´æ˜¾è‘—æ”¹è¿›ã€‚ä¸å…³æ³¨è¯­ä¹‰ç†è§£çš„å…¸å‹å°æ ·æœ¬ä»»åŠ¡ä¸åŒï¼Œæ·±åº¦ä¼ªé€ æ£€æµ‹æ›´ä¾§é‡äºå›¾åƒçš„çœŸå®æ€§ï¼Œè¿™ç´§å¯†åœ°åæ˜ äº†ç°å®ä¸–ç•Œçš„åˆ†å¸ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºç°å®ä¸–ç•Œä¸­æ·±åº¦ä¼ªé€ æ£€æµ‹çš„å°æ ·æœ¬æ— è®­ç»ƒç½‘ç»œï¼ˆFTNetï¼‰ã€‚FTNetç®€å•æœ‰æ•ˆï¼Œä¸åŒäºä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¤§è§„æ¨¡å·²çŸ¥æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ç›¸åï¼ŒFTNetä»…ä½¿ç”¨è¯„ä¼°é›†ä¸­çš„ä¸€ä¸ªä¼ªé€ æ ·æœ¬ï¼Œæ¨¡ä»¿ç°å®ä¸­æ–°æ ·æœ¬çš„å‡ºç°å¹¶å¯ä»¥æ”¶é›†ä½¿ç”¨çš„æƒ…å†µï¼Œæ— éœ€ä»»ä½•è®­ç»ƒæˆ–å‚æ•°æ›´æ–°ã€‚åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªæµ‹è¯•æ ·æœ¬éƒ½ä¸å·²çŸ¥çš„ä¼ªé€ å’ŒçœŸå®æ ·æœ¬è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶åŸºäºæœ€æ¥è¿‘çš„æ ·æœ¬ç±»åˆ«è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬å¯¹29ç§ä¸åŒç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„AIå›¾åƒè¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œè¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œå¹³å‡è€Œè¨€ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”æé«˜äº†8.7%ã€‚è¿™é¡¹å·¥ä½œä¸ºç°å®ä¸–ç•Œçš„æ·±åº¦ä¼ªé€ æ£€æµ‹æä¾›äº†æ–°çš„è§†è§’ï¼šå½“æ¨¡å‹åœ¨å°‘é‡æ ·æœ¬ä¸Šéš¾ä»¥æ¨å¹¿æ—¶ï¼Œåˆ©ç”¨å¤±è´¥çš„æ ·æœ¬ä¼šå¯¼è‡´æ›´å¥½çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09475v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºå°†æ·±åº¦ä¼ªé€ æ£€æµ‹è§†ä¸ºä¸€ç§â€œå°‘æ ·æœ¬â€ä»»åŠ¡ï¼Œå½“æ¨¡å‹åœ¨æœªçŸ¥æ ·æœ¬ä¸Šè¡¨ç°ä¸ä½³æ—¶ï¼Œå¯ä»¥åˆ©ç”¨å°‘é‡æ ·æœ¬è¿›è¡Œæœ‰æ•ˆæ”¹å–„ã€‚é’ˆå¯¹ç°å®ä¸–ç•Œçš„æ·±åº¦ä¼ªé€ æ£€æµ‹é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å°‘æ ·æœ¬ç½‘ç»œï¼ˆFTNetï¼‰ï¼Œè¯¥ç½‘ç»œä»…éœ€ä½¿ç”¨è¯„ä»·é›†ä¸­çš„å•ä¸ªä¼ªé€ æ ·æœ¬å³å¯å®ç°é«˜æ•ˆçš„æ£€æµ‹ã€‚é€šè¿‡å¯¹æ¥è‡ª29ç§ä¸åŒç”Ÿæˆæ¨¡å‹çš„AIç”Ÿæˆå›¾åƒè¿›è¡Œå…¨é¢åˆ†æï¼ŒFTNetå®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œå¹³å‡æ¯”ç°æœ‰æ–¹æ³•æé«˜äº†8.7%ã€‚è¿™è¡¨æ˜åœ¨å®é™…åœºæ™¯ä¸­é‡åˆ°æ–°çš„æŒ‘æˆ˜æ—¶ï¼Œåº”çµæ´»è¿ç”¨æ ·æœ¬ä»¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€è¿‘æ·±åº¦ä¼ªé€ æ£€æµ‹ç ”ç©¶å€¾å‘äºå°†æœªè§æ ·æœ¬æ£€æµ‹è§†ä¸ºâ€œé›¶æ ·æœ¬â€ä»»åŠ¡ï¼Œä½†å®é™…åº”ç”¨ä¸­é¢ä¸´æœªçŸ¥æ ·æœ¬æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚</li>
<li>å½“æ¨¡å‹åœ¨æœªçŸ¥æ ·æœ¬ä¸Šè¡¨ç°ä¸ä½³æ—¶ï¼Œåº”å°†å…¶è§†ä¸ºâ€œå°‘æ ·æœ¬â€ä»»åŠ¡ï¼Œåˆ©ç”¨å°‘é‡æ ·æœ¬æå‡æ€§èƒ½ã€‚</li>
<li>FTNetç½‘ç»œè¢«æå‡ºç”¨äºç°å®ä¸–ç•Œçš„å°‘æ ·æœ¬æ·±åº¦ä¼ªé€ æ£€æµ‹ï¼Œæ— éœ€å¤§è§„æ¨¡å·²çŸ¥æ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
<li>FTNetä»…éœ€è¯„ä»·é›†ä¸­çš„å•ä¸ªä¼ªé€ æ ·æœ¬è¿›è¡Œåˆ†ç±»æ¯”è¾ƒã€‚å®ƒæ¨¡ä»¿ç°å®ä¸­æ–°æ ·æœ¬çš„å‡ºç°å¹¶èƒ½å³æ—¶æ”¶é›†åˆ©ç”¨çš„æƒ…å†µï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒæˆ–å‚æ•°æ›´æ–°ã€‚</li>
<li>FTNetå®ç°äº†å…¨é¢åˆ†æAIç”Ÿæˆå›¾åƒçš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æé«˜ç°æœ‰æ–¹æ³•çš„å¹³å‡æ€§èƒ½è¾¾8.7%ã€‚è¿™è¡¨æ˜æœ‰æ•ˆåˆ©ç”¨å¤±è´¥æ ·æœ¬èƒ½å¤Ÿæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„è§†è§’çœ‹å¾…ç°å®ä¸–ç•Œçš„æ·±åº¦ä¼ªé€ æ£€æµ‹é—®é¢˜ï¼Œå¼ºè°ƒäº†åœ¨å®é™…åœºæ™¯ä¸­é‡åˆ°æ–°çš„æŒ‘æˆ˜æ—¶çµæ´»ä½¿ç”¨æ ·æœ¬çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09475">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c87c5bf639b23e22561373475ebb393.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a24d069e89ad366f8c74a51ae58e78d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22321da32255cc4c08aeaa9424606bc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2041f2e7f484907ce8dba6736ad395c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a70cee19c166b8355609001277fa22a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9589099d6543e838a53419588cdba26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-211ce55a91cb12154f6287c125ca9f48.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Explainable-Sentiment-Analysis-with-DeepSeek-R1-Performance-Efficiency-and-Few-Shot-Learning"><a href="#Explainable-Sentiment-Analysis-with-DeepSeek-R1-Performance-Efficiency-and-Few-Shot-Learning" class="headerlink" title="Explainable Sentiment Analysis with DeepSeek-R1: Performance,   Efficiency, and Few-Shot Learning"></a>Explainable Sentiment Analysis with DeepSeek-R1: Performance,   Efficiency, and Few-Shot Learning</h2><p><strong>Authors:Donghao Huang, Zhaoxia Wang</strong></p>
<p>Large language models (LLMs) have transformed sentiment analysis, yet balancing accuracy, efficiency, and explainability remains a critical challenge. This study presents the first comprehensive evaluation of DeepSeek-R1â€“an open-source reasoning modelâ€“against OpenAIâ€™s GPT-4o and GPT-4o-mini. We test the full 671B model and its distilled variants, systematically documenting few-shot learning curves. Our experiments show DeepSeek-R1 achieves a 91.39% F1 score on 5-class sentiment and 99.31% accuracy on binary tasks with just 5 shots, an eightfold improvement in few-shot efficiency over GPT-4o. Architecture-specific distillation effects emerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant by 6.69 percentage points. While its reasoning process reduces throughput, DeepSeek-R1 offers superior explainability via transparent, step-by-step traces, establishing it as a powerful, interpretable open-source alternative. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ”¹å˜äº†æƒ…æ„Ÿåˆ†æé¢†åŸŸï¼Œä½†åœ¨å¹³è¡¡å‡†ç¡®æ€§ã€æ•ˆç‡å’Œå¯è§£é‡Šæ€§æ–¹é¢ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹DeepSeek-R1è¿™ä¸€å¼€æºæ¨ç†æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå¹¶ä¸OpenAIçš„GPT-4oå’ŒGPT-4o-miniè¿›è¡Œå¯¹æ¯”ã€‚æˆ‘ä»¬æµ‹è¯•äº†å®Œæ•´çš„671Bæ¨¡å‹åŠå…¶è’¸é¦å˜ä½“ï¼Œç³»ç»Ÿåœ°è®°å½•äº†å°æ ·æœ¬å­¦ä¹ æ›²çº¿ã€‚å®éªŒè¡¨æ˜ï¼ŒDeepSeek-R1åœ¨5ç±»æƒ…æ„Ÿåˆ†æä¸Šè¾¾åˆ°91.39%çš„F1åˆ†æ•°ï¼Œåœ¨äºŒå…ƒä»»åŠ¡ä¸Šè¾¾åˆ°99.31%çš„å‡†ç¡®ç‡ï¼Œä»…éœ€è¦5ä¸ªæ ·æœ¬ï¼Œç›¸æ¯”äºGPT-4oï¼Œå…¶åœ¨å°æ ·æœ¬æ•ˆç‡ä¸Šæé«˜äº†å…«å€ã€‚å‡ºç°ç‰¹å®šæ¶æ„çš„è’¸é¦æ•ˆåº”ï¼Œå…¶ä¸­åŸºäº32B Qwen2.5çš„æ¨¡å‹ä¼˜äºåŸºäº70B Llamaçš„å˜ä½“ï¼Œé«˜å‡º6.69ä¸ªç™¾åˆ†ç‚¹ã€‚è™½ç„¶å…¶æ¨ç†è¿‡ç¨‹é™ä½äº†ååé‡ï¼Œä½†DeepSeek-R1é€šè¿‡é€æ˜ã€åˆ†æ­¥çš„è·Ÿè¸ªæä¾›äº†å“è¶Šçš„å¯è§£é‡Šæ€§ï¼Œä½¿å…¶æˆä¸ºå¼ºå¤§ã€å¯è§£é‡Šçš„å¼€æºæ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11655v3">PDF</a> 10 pages, 2 figures, 6 tables, revised and re-submitted to an IEEE   journal</p>
<p><strong>Summary</strong></p>
<p>DeepSeek-R1æ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†æé¢†åŸŸè¡¨ç°å“è¶Šï¼Œä¸OpenAIçš„GPT-4oç³»åˆ—æ¨¡å‹ç›¸æ¯”ï¼Œå…¶åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ æ–¹é¢å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚æœ¬ç ”ç©¶å…¨é¢è¯„ä¼°äº†DeepSeek-R1çš„æ€§èƒ½ï¼Œå‘ç°å…¶åœ¨5ç±»æƒ…æ„Ÿåˆ†æçš„F1åˆ†æ•°ä¸Šè¾¾åˆ°91.39%ï¼Œåœ¨äºŒå…ƒä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡é«˜è¾¾99.31%ï¼Œä¸”ä»…éœ€å°‘é‡æ ·æœ¬ã€‚æ­¤å¤–ï¼ŒDeepSeek-R1å…·æœ‰ä¼˜è¶Šçš„å¯è§£é‡Šæ€§ï¼Œä¸”ä½œä¸ºå¼€æºæ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1åœ¨æƒ…æ„Ÿåˆ†æé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ æ–¹é¢ã€‚</li>
<li>ä¸GPT-4oç³»åˆ—æ¨¡å‹ç›¸æ¯”ï¼ŒDeepSeek-R1åœ¨F1åˆ†æ•°å’Œå‡†ç¡®ç‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>DeepSeek-R1çš„å¯è§£é‡Šæ€§å¼ºå¤§ï¼Œé€šè¿‡é€æ˜çš„æ­¥éª¤è¿½è¸ªï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿç†è§£å…¶æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>æ¶æ„ç‰¹å®šçš„è’¸é¦æ•ˆæœåœ¨DeepSeek-R1ä¸­æ˜¾ç°ï¼Œå…¶ä¸­åŸºäºQwen2.5çš„æ¨¡å‹æ€§èƒ½ä¼˜äºåŸºäºLlamaçš„æ¨¡å‹ã€‚</li>
<li>å°½ç®¡æ¨ç†è¿‡ç¨‹å¯èƒ½ä¼šé™ä½ååé‡ï¼Œä½†DeepSeek-R1ä»å…·æœ‰é«˜æ•ˆçš„æ€§èƒ½ã€‚</li>
<li>DeepSeek-R1æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºæ¨¡å‹ï¼Œæä¾›äº†å¯æ›¿ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„å¦ä¸€ç§é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-460d9c42a1273b1e8ff53103c50bf2c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0d8464ab5d8fe50885dbbd5dc203001.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d36e984e7d14a64ee3e71bacf3f18e1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7590fb4d39c0493b7347574098649fa9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c64596a47d3fd0db43bfa85906188e9e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Leveraging-Audio-and-Text-Modalities-in-Mental-Health-A-Study-of-LLMs-Performance"><a href="#Leveraging-Audio-and-Text-Modalities-in-Mental-Health-A-Study-of-LLMs-Performance" class="headerlink" title="Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs   Performance"></a>Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs   Performance</h2><p><strong>Authors:Abdelrahman A. Ali, Aya E. Fouda, Radwa J. Hanafy, Mohammed E. Fouda</strong></p>
<p>Mental health disorders are increasingly prevalent worldwide, creating an urgent need for innovative tools to support early diagnosis and intervention. This study explores the potential of Large Language Models (LLMs) in multimodal mental health diagnostics, specifically for detecting depression and Post Traumatic Stress Disorder through text and audio modalities. Using the E-DAIC dataset, we compare text and audio modalities to investigate whether LLMs can perform equally well or better with audio inputs. We further examine the integration of both modalities to determine if this can enhance diagnostic accuracy, which generally results in improved performance metrics. Our analysis specifically utilizes custom-formulated metrics; Modal Superiority Score and Disagreement Resolvement Score to evaluate how combined modalities influence model performance. The Gemini 1.5 Pro model achieves the highest scores in binary depression classification when using the combined modality, with an F1 score of 0.67 and a Balanced Accuracy (BA) of 77.4%, assessed across the full dataset. These results represent an increase of 3.1% over its performance with the text modality and 2.7% over the audio modality, highlighting the effectiveness of integrating modalities to enhance diagnostic accuracy. Notably, all results are obtained in zero-shot inferring, highlighting the robustness of the models without requiring task-specific fine-tuning. To explore the impact of different configurations on model performance, we conduct binary, severity, and multiclass tasks using both zero-shot and few-shot prompts, examining the effects of prompt variations on performance. The results reveal that models such as Gemini 1.5 Pro in text and audio modalities, and GPT-4o mini in the text modality, often surpass other models in balanced accuracy and F1 scores across multiple tasks. </p>
<blockquote>
<p>ç²¾ç¥å¥åº·éšœç¢åœ¨å…¨çƒèŒƒå›´å†…è¶Šæ¥è¶Šæ™®éï¼Œè¿™è¿«åˆ‡éœ€è¦åˆ›æ–°å·¥å…·æ¥æ”¯æŒæ—©æœŸè¯Šæ–­å’Œæ²»ç–—ã€‚æœ¬ç ”ç©¶æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šæ¨¡å¼ç²¾ç¥å¥åº·è¯Šæ–­ä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ–‡æœ¬å’ŒéŸ³é¢‘æ¨¡å¼æ£€æµ‹æŠ‘éƒç—‡å’Œåˆ›ä¼¤ååº”æ¿€éšœç¢ã€‚æˆ‘ä»¬ä½¿ç”¨E-DAICæ•°æ®é›†ï¼Œæ¯”è¾ƒæ–‡æœ¬å’ŒéŸ³é¢‘æ¨¡å¼ï¼Œä»¥ç ”ç©¶LLMæ˜¯å¦å¯ä»¥é€šè¿‡éŸ³é¢‘è¾“å…¥å®ç°åŒç­‰æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢è®¨äº†ä¸¤ç§æ¨¡å¼çš„èåˆï¼Œä»¥ç¡®å®šè¿™æ˜¯å¦èƒ½æé«˜è¯Šæ–­çš„å‡†ç¡®æ€§ï¼Œä»è€Œæé«˜æ€§èƒ½æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„åˆ†æç‰¹åˆ«åˆ©ç”¨å®šåˆ¶çš„æŒ‡æ ‡ï¼Œå³æ¨¡æ€ä¼˜åŠ¿è¯„åˆ†å’Œåˆ†æ­§è§£å†³è¯„åˆ†ï¼Œæ¥è¯„ä¼°ç»„åˆæ¨¡å¼å¦‚ä½•å½±å“æ¨¡å‹æ€§èƒ½ã€‚åœ¨äºŒå…ƒæŠ‘éƒç—‡åˆ†ç±»ä¸­ï¼Œä½¿ç”¨ç»„åˆæ¨¡å¼çš„Gemini 1.5 Proæ¨¡å‹å¾—åˆ†æœ€é«˜ï¼ŒF1åˆ†æ•°ä¸º0.67ï¼Œå¹³è¡¡ç²¾åº¦ï¼ˆBAï¼‰ä¸º77.4%ï¼Œåœ¨å…¨æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ä¸ä»…ä½¿ç”¨æ–‡æœ¬æ¨¡å¼æˆ–éŸ³é¢‘æ¨¡å¼ç›¸æ¯”ï¼Œè¿™äº›ç»“æœåˆ†åˆ«æé«˜äº†3.1%å’Œæé«˜äº†2.7%ï¼Œè¿™çªæ˜¾äº†èåˆæ¨¡å¼ä»¥æé«˜è¯Šæ–­å‡†ç¡®æ€§çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ‰€æœ‰ç»“æœå‡æ˜¯åœ¨é›¶æ ·æœ¬æ¨æ–­ä¸­è·å¾—çš„ï¼Œè¿™è¡¨æ˜æ¨¡å‹åœ¨ä¸éœ€è¦ç‰¹å®šä»»åŠ¡å¾®è°ƒçš„æƒ…å†µä¸‹å…·æœ‰å¾ˆå¼ºçš„ç¨³å¥æ€§ã€‚ä¸ºäº†æ¢è®¨ä¸åŒé…ç½®å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œæˆ‘ä»¬ä½¿ç”¨é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬æç¤ºè¿›è¡ŒäºŒå…ƒã€ä¸¥é‡æ€§å’Œå¤šç±»åˆ«ä»»åŠ¡ï¼Œå¹¶ç ”ç©¶æç¤ºå˜åŒ–å¯¹æ€§èƒ½çš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­ï¼Œå¦‚æ–‡æœ¬å’ŒéŸ³é¢‘æ¨¡å¼ä¸­çš„Gemini 1.5 Proä»¥åŠæ–‡æœ¬æ¨¡å¼ä¸­çš„GPT-4o miniç­‰æ¨¡å‹ç»å¸¸åœ¨å¹³è¡¡ç²¾åº¦å’ŒF1åˆ†æ•°æ–¹é¢è¶…è¶Šå…¶ä»–æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10417v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ¨¡æ€å¿ƒç†å¥åº·è¯Šæ–­ä¸­çš„æ½œåŠ›ï¼Œç ”ç©¶é€šè¿‡æ–‡æœ¬å’ŒéŸ³é¢‘æ¨¡æ€æ£€æµ‹æŠ‘éƒç—‡å’Œåˆ›ä¼¤ååº”æ¿€éšœç¢ã€‚ç ”ç©¶ä½¿ç”¨E-DAICæ•°æ®é›†æ¯”è¾ƒæ–‡æœ¬å’ŒéŸ³é¢‘æ¨¡æ€ï¼Œå‘ç°LLMsåœ¨éŸ³é¢‘è¾“å…¥æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚åŒæ—¶ï¼Œç ”ç©¶ä¹Ÿè€ƒå¯Ÿäº†ä¸¤ç§æ¨¡æ€çš„æ•´åˆæ˜¯å¦èƒ½æé«˜è¯Šæ–­å‡†ç¡®æ€§ï¼Œç»“æœæ˜¾ç¤ºæ•´åˆåçš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚å…¶ä¸­ï¼ŒGemini 1.5 Proæ¨¡å‹åœ¨äºŒå…ƒæŠ‘éƒç—‡åˆ†ç±»ä¸­è¡¨ç°æœ€ä½³ï¼Œç»“åˆæ¨¡æ€çš„F1åˆ†æ•°ä¸º0.67ï¼Œå¹³è¡¡å‡†ç¡®ç‡ä¸º77.4%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ‰€æœ‰ç»“æœå‡ä¸ºé›¶æ ·æœ¬æ¨æ–­ï¼Œçªæ˜¾äº†æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€å¿ƒç†å¥åº·è¯Šæ–­ä¸­å…·æœ‰æ½œåŠ›ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬å’ŒéŸ³é¢‘æ¨¡æ€æ£€æµ‹æŠ‘éƒç—‡å’Œåˆ›ä¼¤ååº”æ¿€éšœç¢çš„ç ”ç©¶å¾—åˆ°å¼€å±•ã€‚</li>
<li>LLMsåœ¨éŸ³é¢‘è¾“å…¥æ–¹é¢çš„è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ•´åˆæ–‡æœ¬å’ŒéŸ³é¢‘æ¨¡æ€èƒ½æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>Gemini 1.5 Proæ¨¡å‹åœ¨äºŒå…ƒæŠ‘éƒç—‡åˆ†ç±»ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>æ¨¡å‹è¡¨ç°ç¨³å¥ï¼Œå¯åœ¨é›¶æ ·æœ¬æ¨æ–­ä¸‹å–å¾—è‰¯å¥½ç»“æœã€‚</li>
<li>ä¸åŒé…ç½®å’Œæç¤ºæ–¹å¼å¯¹æ¨¡å‹æ€§èƒ½æœ‰å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10417">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8aa8138d29022730cfa5798aae6ee84f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9ab003ab22b64fa810b0915fed799a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b24c30624a7772336ab2be74ca005b67.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Multi-Step-Reasoning-with-Large-Language-Models-a-Survey"><a href="#Multi-Step-Reasoning-with-Large-Language-Models-a-Survey" class="headerlink" title="Multi-Step Reasoning with Large Language Models, a Survey"></a>Multi-Step Reasoning with Large Language Models, a Survey</h2><p><strong>Authors:Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, Thomas Back</strong></p>
<p>Language models with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model was not specifically trained for. Traditional models achieve breakthrough performance on language tasks, but do not perform well on basic reasoning benchmarks. However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step reasoning abilities on these benchmarks.   The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems, and has expanded to other tasks in the past few years. This paper reviews the field of multi-step reasoning with LLMs. We propose a taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future.   We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools. Many studies in multi-step methods are using reinforcement learning for finetuning, external optimization loops, in context reinforcement learning, and self-reflection. </p>
<blockquote>
<p>å…·æœ‰æ•°åäº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹å±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æœªä¸“é—¨è®­ç»ƒçš„ä»»åŠ¡ä¸Šå®ç°å°‘é‡å­¦ä¹ ã€‚ä¼ ç»Ÿæ¨¡å‹åœ¨è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œä½†åœ¨åŸºæœ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å¹¶ä¸å‡ºè‰²ã€‚ç„¶è€Œï¼Œä¸€ç§æ–°çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•â€”â€”æ€ç»´é“¾ï¼ˆChain-of-thoughtï¼‰åœ¨è¿™äº›åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„ç ”ç©¶å§‹äºå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½è§£å†³å°å­¦æ•°å­¦æ–‡å­—é¢˜çš„é—®é¢˜ï¼Œå¹¶åœ¨è¿‡å»å‡ å¹´ä¸­æ‰©å±•åˆ°äº†å…¶ä»–ä»»åŠ¡ã€‚æœ¬æ–‡ç»¼è¿°äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ­¥æ¨ç†é¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†ç±»æ³•ï¼Œç¡®å®šäº†ç”Ÿæˆã€è¯„ä¼°å’Œæ§åˆ¶å¤šæ­¥æ¨ç†çš„ä¸åŒæ–¹æ³•ã€‚æˆ‘ä»¬å¯¹æ ¸å¿ƒæ–¹æ³•å’Œå¼€æ”¾é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œå¹¶æå‡ºäº†ä»Šåçš„ç ”ç©¶è®®ç¨‹ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¤šæ­¥æ¨ç†æ–¹æ³•å·²ç»è¶…è¶Šäº†æ•°å­¦æ–‡å­—é¢˜çš„èŒƒå›´ï¼Œç°åœ¨èƒ½å¤ŸæˆåŠŸè§£å†³é€»è¾‘ã€ç»„åˆæ¸¸æˆå’Œæœºå™¨äººæŠ€æœ¯ç­‰æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæœ‰æ—¶æ˜¯å…ˆç”Ÿæˆä»£ç ï¼Œç„¶åäº¤ç”±å¤–éƒ¨å·¥å…·æ‰§è¡Œã€‚å¤šæ­¥æ–¹æ³•çš„ç ”ç©¶æ­£åœ¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒã€å¤–éƒ¨ä¼˜åŒ–å¾ªç¯ã€ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ å’Œè‡ªæˆ‘åæ€ç­‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11511v2">PDF</a> revised version</p>
<p><strong>Summary</strong><br>     è¯­è¨€æ¨¡å‹æ‹¥æœ‰æ•°åäº¿å‚æ•°ï¼Œå±•ç°å‡ºä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œå¯åœ¨æœªä¸“é—¨è®­ç»ƒçš„ä»»åŠ¡ä¸­å®ç°å°‘é‡å­¦ä¹ ã€‚ä¼ ç»Ÿæ¨¡å‹åœ¨è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä½†åœ¨åŸºæœ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¸ä½³ã€‚ä¸€ç§æ–°å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ³•â€”â€”Chain-of-thoughtå±•ç°å‡ºå¼ºå¤§çš„å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡å›é¡¾äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ­¥éª¤æ¨ç†é¢†åŸŸï¼Œæå‡ºäº†è¯†åˆ«ç”Ÿæˆã€è¯„ä¼°å’Œæ§åˆ¶å¤šæ­¥éª¤æ¨ç†çš„ä¸åŒæ–¹å¼çš„æ–°åˆ†ç±»æ³•ï¼Œæ·±å…¥æ¢è®¨äº†æ ¸å¿ƒæ–¹æ³•å’Œå¼€æ”¾é—®é¢˜ï¼Œå¹¶ä¸ºè¿‘æœŸæœªæ¥æå‡ºäº†ç ”ç©¶è®®ç¨‹ã€‚ç ”ç©¶å‘ç°ï¼Œå¤šæ­¥éª¤æ¨ç†æ–¹æ³•å·²ä¸ä»…é™äºè§£å†³æ•°å­¦æ–‡å­—é—®é¢˜ï¼Œè¿˜èƒ½æˆåŠŸåº”å¯¹é€»è¾‘ã€ç»„åˆæ¸¸æˆå’Œæœºå™¨äººæŠ€æœ¯ä¸­çš„æŒ‘æˆ˜ï¼Œæœ‰æ—¶é€šè¿‡ç”Ÿæˆä»£ç å¹¶ç”±å¤–éƒ¨å·¥å…·æ‰§è¡Œæ¥å®Œæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹å…·å¤‡åœ¨æ— éœ€ä¸“é—¨è®­ç»ƒçš„æƒ…å†µä¸‹å¤„ç†æ–°ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè¿™è¢«ç§°ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ æˆ–å°‘é‡å­¦ä¹ ã€‚</li>
<li>ä¼ ç»Ÿè¯­è¨€æ¨¡å‹åœ¨åŸºæœ¬æ¨ç†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚</li>
<li>Chain-of-thoughtæ˜¯ä¸€ç§æ–°å‹ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ï¼Œå…·æœ‰å¼ºå¤§çš„å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¤šæ­¥éª¤æ¨ç†æ–¹æ³•ä¸ä»…å¯ä»¥è§£å†³æ•°å­¦æ–‡å­—é—®é¢˜ï¼Œè¿˜èƒ½æˆåŠŸåº”å¯¹é€»è¾‘ã€ç»„åˆæ¸¸æˆå’Œæœºå™¨äººæŠ€æœ¯ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤šæ­¥éª¤æ¨ç†æ–¹æ³•æœ‰æ—¶ä¼šç”Ÿæˆä»£ç å¹¶ç”±å¤–éƒ¨å·¥å…·æ‰§è¡Œæ¥å®Œæˆä»»åŠ¡ã€‚</li>
<li>ç›®å‰çš„ç ”ç©¶é¢†åŸŸæ¶µç›–äº†å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒã€å¤–éƒ¨ä¼˜åŒ–å¾ªç¯ã€ä¸Šä¸‹æ–‡å¼ºåŒ–å­¦ä¹ å’Œè‡ªæˆ‘åæ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.11511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b9757c0cb9b28ecd5dcda57fe9c8898e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6939a4f1c350b8e1047c459171e1d304.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fbf02856ab90883492ec326e1cae83ba.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  StyleMM Stylized 3D Morphable Face Model via Text-Driven Aligned Image   Translation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-28bf8a8110a9746a3695944324442f4d.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  DiCriTest Testing Scenario Generation for Decision-Making Agents   Considering Diversity and Criticality
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
