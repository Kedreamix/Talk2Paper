<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  Thyme Think Beyond Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b0d785a72dc0d89eb17a05729b6956a1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-19-æ›´æ–°"><a href="#2025-08-19-æ›´æ–°" class="headerlink" title="2025-08-19 æ›´æ–°"></a>2025-08-19 æ›´æ–°</h1><h2 id="Thyme-Think-Beyond-Images"><a href="#Thyme-Think-Beyond-Images" class="headerlink" title="Thyme: Think Beyond Images"></a>Thyme: Think Beyond Images</h2><p><strong>Authors:Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, Haonan Fan, Kaibing Chen, Jiankang Chen, Haojie Ding, Kaiyu Tang, Zhang Zhang, Liang Wang, Fan Yang, Tingting Gao, Guorui Zhou</strong></p>
<p>Following OpenAIâ€™s introduction of the <code>thinking with images&#39;&#39; concept, recent efforts have explored stimulating the use of visual information in the reasoning process to enhance model performance in perception and reasoning tasks. However, to the best of our knowledge, no open-source work currently offers a feature set as rich as proprietary models (O3), which can perform diverse image manipulations and simultaneously enhance logical reasoning capabilities through code. In this paper, we make a preliminary attempt in this direction by introducing Thyme (Think Beyond Images), a novel paradigm for enabling MLLMs to transcend existing </code>think with imagesâ€™â€™ approaches by autonomously generating and executing diverse image processing and computational operations via executable code. This approach not only facilitates a rich, on-the-fly set of image manipulations (e.g., cropping, rotation, contrast enhancement) but also allows for mathematical computations, all while maintaining high autonomy in deciding when and how to apply these operations. We activate this capability through a two-stage training strategy: an initial SFT on a curated dataset of 500K samples to teach code generation, followed by a RL phase to refine decision-making. For the RL stage, we manually collect and design high-resolution question-answer pairs to increase the learning difficulty, and we propose GRPO-ATS (Group Relative Policy Optimization with Adaptive Temperature Sampling), an algorithm that applies distinct temperatures to text and code generation to balance reasoning exploration with code execution precision. We conduct extensive experimental analysis and ablation studies. Comprehensive evaluations on nearly 20 benchmarks show that Thyme yields significant and consistent performance gains, particularly in challenging high-resolution perception and complex reasoning tasks. </p>
<blockquote>
<p>åœ¨OpenAIæå‡ºâ€œå›¾åƒæ€è€ƒâ€æ¦‚å¿µä¹‹åï¼Œæœ€è¿‘çš„åŠªåŠ›éƒ½åœ¨æ¢ç´¢åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆºæ¿€è§†è§‰ä¿¡æ¯çš„ä½¿ç”¨ï¼Œä»¥æé«˜æ¨¡å‹åœ¨æ„ŸçŸ¥å’Œæ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç›®å‰æ²¡æœ‰ä»»ä½•å¼€æºå·¥ä½œèƒ½æä¾›ä¸ä¸“æœ‰æ¨¡å‹ï¼ˆO3ï¼‰ä¸€æ ·ä¸°å¯Œçš„åŠŸèƒ½é›†ï¼Œåè€…å¯ä»¥è¿›è¡Œå¤šæ ·åŒ–çš„å›¾åƒæ“ä½œï¼Œå¹¶é€šè¿‡ä»£ç åŒæ—¶æé«˜é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æœç€è¿™ä¸ªæ–¹å‘è¿›è¡Œäº†åˆæ­¥å°è¯•ï¼Œå¼•å…¥äº†Thymeï¼ˆè¶…è¶Šå›¾åƒæ€è€ƒï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹èŒƒå¼ï¼Œä½¿MLLMsèƒ½å¤Ÿé€šè¿‡è‡ªä¸»ç”Ÿæˆå’Œæ‰§è¡Œå„ç§å›¾åƒå¤„ç†å’Œè®¡ç®—æ“ä½œï¼Œè¶…è¶Šç°æœ‰çš„â€œå›¾åƒæ€è€ƒâ€æ–¹æ³•ï¼Œé€šè¿‡å¯æ‰§è¡Œä»£ç å®ç°ã€‚è¿™ç§æ–¹æ³•ä¸ä»…ä¾¿äºä¸°å¯Œçš„å³æ—¶å›¾åƒæ“ä½œï¼ˆä¾‹å¦‚è£å‰ªã€æ—‹è½¬ã€å¯¹æ¯”åº¦å¢å¼ºï¼‰ï¼Œè¿˜å…è®¸æ•°å­¦è®¡ç®—ï¼ŒåŒæ—¶ä¿æŒé«˜åº¦è‡ªä¸»æ€§ï¼Œä»¥å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•åº”ç”¨è¿™äº›æ“ä½œã€‚æˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¿€æ´»æ­¤åŠŸèƒ½ï¼šé¦–å…ˆåœ¨50ä¸‡ä¸ªç²¾é€‰æ ·æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œåˆå§‹çš„è‡ªç›‘ç£è®­ç»ƒï¼Œä»¥æ•™æˆä»£ç ç”Ÿæˆï¼Œç„¶åå¼ºåŒ–å­¦ä¹ é˜¶æ®µæ¥å®Œå–„å†³ç­–åˆ¶å®šã€‚å¯¹äºå¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œæˆ‘ä»¬æ‰‹åŠ¨æ”¶é›†å’Œè®¾è®¡äº†é«˜åˆ†è¾¨ç‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œä»¥å¢åŠ å­¦ä¹ éš¾åº¦ï¼Œå¹¶æå‡ºäº†GRPO-ATSï¼ˆå…·æœ‰è‡ªé€‚åº”æ¸©åº¦é‡‡æ ·çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯¹æ–‡æœ¬å’Œä»£ç ç”Ÿæˆåº”ç”¨ä¸åŒçš„æ¸©åº¦ï¼Œä»¥å¹³è¡¡æ¨ç†æ¢ç´¢ä¸ä»£ç æ‰§è¡Œç²¾åº¦ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒåˆ†æå’Œæ¶ˆèç ”ç©¶ã€‚è¿‘20é¡¹åŸºå‡†æµ‹è¯•çš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒThymeå¸¦æ¥äº†æ˜¾è‘—ä¸”æŒç»­çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åˆ†è¾¨ç‡æ„ŸçŸ¥å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11630v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://thyme-vl.github.io/">https://thyme-vl.github.io/</a></p>
<p><strong>Summary</strong>ï¼šéšç€OpenAIæå‡ºçš„â€œä»¥å›¾æ€è€ƒâ€æ¦‚å¿µçš„å¼•å…¥ï¼Œæœ€æ–°åŠªåŠ›å¼€å§‹æ¢ç´¢åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œä»¥æé«˜æ¨¡å‹åœ¨æ„ŸçŸ¥å’Œæ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç›®å‰è¿˜æ²¡æœ‰å¼€æºå·¥ä½œèƒ½æä¾›ä¸ä¸“æœ‰æ¨¡å‹ä¸€æ ·ä¸°å¯Œçš„åŠŸèƒ½é›†ï¼Œå®ƒä»¬å¯ä»¥è¿›è¡Œå¤šç§å›¾åƒæ“ä½œï¼Œå¹¶é€šè¿‡ä»£ç åŒæ—¶æé«˜é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºThymeï¼ˆè¶…è¶Šå›¾åƒæ€è€ƒï¼‰çš„æ–°å‹èŒƒå¼ï¼Œä½¿MLLMsèƒ½å¤Ÿé€šè¿‡è‡ªä¸»ç”Ÿæˆå’Œæ‰§è¡Œå„ç§å›¾åƒå¤„ç†å’Œè®¡ç®—æ“ä½œæ¥è¶…è¶Šç°æœ‰çš„â€œä»¥å›¾æ€è€ƒâ€æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ä¸ä»…ä¾¿äºè¿›è¡Œä¸°å¯Œçš„å³æ—¶å›¾åƒæ“ä½œï¼ˆå¦‚è£å‰ªã€æ—‹è½¬ã€å¯¹æ¯”åº¦å¢å¼ºï¼‰ï¼Œè¿˜å…è®¸è¿›è¡Œæ•°å­¦è®¡ç®—ï¼ŒåŒæ—¶ä¿æŒé«˜åº¦è‡ªä¸»å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•åº”ç”¨è¿™äº›æ“ä½œçš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¿€æ´»æ­¤åŠŸèƒ½ï¼šé¦–å…ˆåœ¨ç²¾é€‰çš„50ä¸‡æ ·æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œåˆå§‹SFTï¼ˆè‡ªè®­ç»ƒï¼‰ä»¥æ•™æˆä»£ç ç”Ÿæˆï¼Œå…¶æ¬¡æ˜¯RLï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰é˜¶æ®µä»¥ä¼˜åŒ–å†³ç­–åˆ¶å®šã€‚æˆ‘ä»¬æå‡ºäº†GRPO-ATSç®—æ³•ï¼Œå°†ä¸åŒçš„æ¸©åº¦åº”ç”¨äºæ–‡æœ¬å’Œä»£ç ç”Ÿæˆï¼Œä»¥å¹³è¡¡æ¨ç†æ¢ç´¢ä¸ä»£ç æ‰§è¡Œç²¾åº¦ã€‚ç»è¿‘20é¡¹åŸºå‡†æµ‹è¯•çš„ç»¼åˆè¯„ä¼°å’Œæ¶ˆèç ”ç©¶ï¼ŒThymeæ˜¾ç¤ºå‡ºæ˜¾è‘—ä¸”ä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åˆ†è¾¨ç‡æ„ŸçŸ¥å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Thymeæ˜¯ä¸€ç§æ–°å‹èŒƒå¼ï¼Œä½¿MLLMsèƒ½å¤Ÿè¶…è¶Šâ€œä»¥å›¾æ€è€ƒâ€çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªä¸»ç”Ÿæˆå’Œæ‰§è¡Œå›¾åƒå¤„ç†å’Œè®¡ç®—æ“ä½œæ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>Thymeæ”¯æŒä¸°å¯Œçš„å›¾åƒæ“ä½œï¼Œå¦‚è£å‰ªã€æ—‹è½¬ã€å¯¹æ¯”åº¦å¢å¼ºç­‰ï¼Œå¹¶å…è®¸æ•°å­¦è®¡ç®—ã€‚</li>
<li>Thymeé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œåˆå§‹é˜¶æ®µé€šè¿‡è‡ªè®­ç»ƒæ•™æˆä»£ç ç”Ÿæˆï¼Œéšåæ˜¯å¼ºåŒ–å­¦ä¹ é˜¶æ®µä¼˜åŒ–å†³ç­–åˆ¶å®šã€‚</li>
<li>GRPO-ATSç®—æ³•åº”ç”¨äºæ–‡æœ¬å’Œä»£ç ç”Ÿæˆçš„ä¸åŒæ¸©åº¦ï¼Œä»¥å¹³è¡¡æ¨ç†æ¢ç´¢ä¸ä»£ç æ‰§è¡Œç²¾åº¦ã€‚</li>
<li>Thymeåœ¨è¿‘ä¹20é¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åˆ†è¾¨ç‡æ„ŸçŸ¥å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸­ã€‚</li>
<li>å¼€æºå·¥ä½œç›®å‰å°šæ— æ³•æä¾›ä¸Thymeç›¸å½“çš„ä¸°å¯ŒåŠŸèƒ½é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e8d35a5bf53ef25fc0f6b164aac24e32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4579b4fe851b009d1be808bb3bbe6d3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-451d5a7d7e42a964d1f2fbce4b7342e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5622babad56bfc29a031c662ff852a2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Reinforcing-Video-Reasoning-Segmentation-to-Think-Before-It-Segments"><a href="#Reinforcing-Video-Reasoning-Segmentation-to-Think-Before-It-Segments" class="headerlink" title="Reinforcing Video Reasoning Segmentation to Think Before It Segments"></a>Reinforcing Video Reasoning Segmentation to Think Before It Segments</h2><p><strong>Authors:Sitong Gong, Lu Zhang, Yunzhi Zhuge, Xu Jia, Pingping Zhang, Huchuan Lu</strong></p>
<p>Video reasoning segmentation (VRS) endeavors to delineate referred objects in videos guided by implicit instructions that encapsulate human intent and temporal logic. Previous approaches leverage large vision language models (LVLMs) to encode object semantics into <SEG> tokens for mask prediction. However, this paradigm suffers from limited interpretability during inference and suboptimal performance due to inadequate spatiotemporal reasoning. Drawing inspiration from seminal breakthroughs in reinforcement learning, we introduce Veason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in segmentation. Veason-R1 is trained through Group Relative Policy Optimization (GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we curate high-quality CoT training data to instill structured reasoning trajectories, bridging video-level semantics and frame-level spatial grounding, yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO fine-tuning encourages efficient exploration of the reasoning space by optimizing reasoning chains. To this end, we incorporate a holistic reward mechanism that synergistically enhances spatial alignment and temporal consistency, bolstering keyframe localization and fine-grained grounding. Comprehensive empirical evaluations demonstrate that Veason-R1 achieves state-of-the-art performance on multiple benchmarks, surpassing prior art by significant margins (e.g., +1.3 J &amp;F in ReVOS and +10.0 J &amp;F in ReasonVOS), while exhibiting robustness to hallucinations (+8.8 R). Our code and model weights will be available at Veason-R1. </p>
<blockquote>
<p>è§†é¢‘æ¨ç†åˆ†å‰²ï¼ˆVRSï¼‰æ—¨åœ¨æ ¹æ®éšå«æŒ‡ä»¤ï¼ˆè¿™äº›æŒ‡ä»¤åŒ…å«äººç±»æ„å›¾å’Œæ—¶é—´é€»è¾‘ï¼‰æ¥æç»˜è§†é¢‘ä¸­æ¶‰åŠçš„å¯¹è±¡ã€‚ä¹‹å‰çš„æ–¹æ³•æ˜¯åˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å°†å¯¹è±¡è¯­ä¹‰ç¼–ç ä¸º<SEG>æ ‡è®°æ¥è¿›è¡Œæ©è†œé¢„æµ‹ã€‚ç„¶è€Œï¼Œç”±äºæ¨ç†è¿‡ç¨‹ä¸­çš„æ—¶ç©ºæ¨ç†ä¸è¶³ï¼Œè¿™ç§èŒƒå¼åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨å¯è§£é‡Šæ€§æœ‰é™å’Œæ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚æˆ‘ä»¬ä»å¼ºåŒ–å­¦ä¹ çš„å¼€åˆ›æ€§çªç ´ä¸­æ±²å–çµæ„Ÿï¼Œå¼•å…¥äº†é’ˆå¯¹VRSçš„ä¸“ç”¨LVLMâ€”â€”Veason-R1ï¼Œå®ƒå¼ºè°ƒåˆ†å‰²ä¸­çš„ç»“æ„åŒ–æ¨ç†ã€‚Veason-R1é€šè¿‡Group Relative Policy Optimizationï¼ˆGRPOï¼‰å’ŒChain-of-Thoughtï¼ˆCoTï¼‰åˆå§‹åŒ–è¿›è¡Œè®­ç»ƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç²¾å¿ƒæ•´ç†é«˜è´¨é‡çš„CoTè®­ç»ƒæ•°æ®ï¼Œä»¥åŸ¹å…»ç»“æ„åŒ–æ¨ç†è½¨è¿¹ï¼Œä»è€Œå»ºç«‹è§†é¢‘çº§è¯­ä¹‰å’Œå¸§çº§ç©ºé—´å®šä½ä¹‹é—´çš„è”ç³»ï¼Œå¾—åˆ°ç»è¿‡ç›‘ç£å¾®è°ƒè®­ç»ƒçš„æ¨¡å‹Veason-SFTã€‚éšåï¼ŒGRPOå¾®è°ƒé€šè¿‡ä¼˜åŒ–æ¨ç†é“¾æ¥é¼“åŠ±æ¨ç†ç©ºé—´çš„æœ‰æ•ˆæ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å…¨é¢çš„å¥–åŠ±æœºåˆ¶ï¼ŒååŒå¢å¼ºç©ºé—´å¯¹é½å’Œæ—¶åºä¸€è‡´æ€§ï¼Œæå‡å…³é”®å¸§å®šä½å’Œç²¾ç»†å®šä½èƒ½åŠ›ã€‚ç»¼åˆå®è¯è¯„ä¼°è¡¨æ˜ï¼ŒVeason-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†å…ˆå‰æŠ€æœ¯ï¼ˆä¾‹å¦‚åœ¨ReVOSä¸­çš„J&amp;Få¢åŠ 1.3ï¼Œåœ¨ReasonVOSä¸­çš„J&amp;Få¢åŠ 10.0ï¼‰ï¼ŒåŒæ—¶å¯¹å¹»è§‰å…·æœ‰é²æ£’æ€§ï¼ˆ+8.8 Rï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹æƒé‡å°†åœ¨Veason-R1ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11538v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘æ¨ç†åˆ†å‰²ï¼ˆVRSï¼‰é¢†åŸŸçš„ä¸€ç§æ–°æ–¹æ³•â€”â€”Veason-R1ã€‚è¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å®ç°æ›´ç»“æ„åŒ–ã€æ›´å…·è§£é‡Šæ€§çš„è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼Œå®ƒé€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ çš„æŠ€æœ¯å¦‚Group Relative Policy Optimization (GRPO) å’Œ Chain-of-Thought (CoT) åˆå§‹åŒ–æ¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ¨¡å‹é€šè¿‡åœ¨è§†é¢‘çº§åˆ«è¯­ä¹‰å’Œå¸§çº§åˆ«ç©ºé—´å®šä½ä¹‹é—´å»ºç«‹æ¡¥æ¢ï¼Œè¿›è¡Œç»“æ„åŒ–æ¨ç†è½¨è¿¹çš„è®­ç»ƒï¼Œå¹¶é‡‡ç”¨æ•´ä½“å¥–åŠ±æœºåˆ¶æ¥ä¼˜åŒ–æ¨ç†é“¾çš„ç©ºé—´å¯¹é½å’Œæ—¶åºä¸€è‡´æ€§ï¼Œä»è€Œæé«˜å…³é”®å¸§å®šä½å’Œç²¾ç»†å®šä½çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒVeason-R1è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ¨ç†åˆ†å‰²ï¼ˆVRSï¼‰æ—¨åœ¨æ ¹æ®éšå«æŒ‡ä»¤è¿›è¡Œè§†é¢‘ä¸­çš„å¯¹è±¡åˆ†å‰²ï¼Œè¿™äº›æŒ‡ä»¤åæ˜ äº†äººç±»æ„å›¾å’Œæ—¶é—´é€»è¾‘ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰è¿›è¡Œå¯¹è±¡è¯­ä¹‰ç¼–ç ï¼Œä½†å­˜åœ¨è§£é‡Šæ€§å·®å’Œæ€§èƒ½ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>Veason-R1é€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œå¼ºè°ƒç»“æ„åŒ–æ¨ç†ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>Veason-R1é‡‡ç”¨Group Relative Policy Optimization (GRPO) å’Œ Chain-of-Thought (CoT) åˆå§‹åŒ–è¿›è¡Œè®­ç»ƒã€‚</li>
<li>é€šè¿‡é«˜è´¨é‡çš„ç»“æ„åŒ–æ¨ç†è½¨è¿¹è®­ç»ƒæ•°æ®ï¼ŒVeason-R1åœ¨è§†é¢‘çº§åˆ«è¯­ä¹‰å’Œå¸§çº§åˆ«ç©ºé—´å®šä½ä¹‹é—´å»ºç«‹äº†æ¡¥æ¢ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æ•´ä½“å¥–åŠ±æœºåˆ¶æ¥ä¼˜åŒ–ç©ºé—´å¯¹é½å’Œæ—¶åºä¸€è‡´æ€§ï¼Œæé«˜äº†å…³é”®å¸§å®šä½å’Œç²¾ç»†å®šä½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11538">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a4316c3a606f5d58d823174c7327302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f195b32f2f3e7a3b09fa88bd7d57f34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66d09e3e41227d2e34870ea7e5bf87a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-594f29fb4573c19fbafac91d60ed3ca2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Remove360-Benchmarking-Residuals-After-Object-Removal-in-3D-Gaussian-Splatting"><a href="#Remove360-Benchmarking-Residuals-After-Object-Removal-in-3D-Gaussian-Splatting" class="headerlink" title="Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian   Splatting"></a>Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian   Splatting</h2><p><strong>Authors:Simona Kocour, Assia Benbihi, Torsten Sattler</strong></p>
<p>Understanding what semantic information persists after object removal is critical for privacy-preserving 3D reconstruction and editable scene representations. In this work, we introduce a novel benchmark and evaluation framework to measure semantic residuals, the unintended semantic traces left behind, after object removal in 3D Gaussian Splatting. We conduct experiments across a diverse set of indoor and outdoor scenes, showing that current methods can preserve semantic information despite the absence of visual geometry. We also release Remove360, a dataset of pre&#x2F;post-removal RGB images and object-level masks captured in real-world environments. While prior datasets have focused on isolated object instances, Remove360 covers a broader and more complex range of indoor and outdoor scenes, enabling evaluation of object removal in the context of full-scene representations. Given ground truth images of a scene before and after object removal, we assess whether we can truly eliminate semantic presence, and if downstream models can still infer what was removed. Our findings reveal critical limitations in current 3D object removal techniques and underscore the need for more robust solutions capable of handling real-world complexity. The evaluation framework is available at github.com&#x2F;spatial-intelligence-ai&#x2F;Remove360.git. Data are available at huggingface.co&#x2F;datasets&#x2F;simkoc&#x2F;Remove360. </p>
<blockquote>
<p>ç†è§£åœ¨ç§»é™¤ç‰©ä½“åå“ªäº›è¯­ä¹‰ä¿¡æ¯å¾—ä»¥ä¿ç•™ï¼Œå¯¹äºä¿æŠ¤éšç§çš„3Dé‡å»ºå’Œå¯ç¼–è¾‘åœºæ™¯è¡¨ç¤ºè‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ï¼Œæ¥æµ‹é‡3Dé«˜æ–¯æ‹¼è´´ä¸­ç‰©ä½“ç§»é™¤åç•™ä¸‹çš„æ— æ„ä¸­çš„è¯­ä¹‰ç—•è¿¹ã€‚æˆ‘ä»¬åœ¨å®¤å†…å’Œå®¤å¤–åœºæ™¯è¿›è¡Œäº†å¤šæ ·åŒ–çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æ²¡æœ‰è§†è§‰å‡ ä½•çš„æƒ…å†µä¸‹ï¼Œå½“å‰çš„æ–¹æ³•ä¹Ÿå¯ä»¥ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†Remove360æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«åœ¨ç°å®ç¯å¢ƒä¸­æ•è·çš„é¢„ç§»é™¤å’Œåç§»é™¤RGBå›¾åƒä»¥åŠç‰©ä½“çº§æ©è†œã€‚è™½ç„¶å…ˆå‰çš„æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨å­¤ç«‹çš„ç‰©ä½“å®ä¾‹ä¸Šï¼Œä½†Remove360æ¶µç›–äº†æ›´å¹¿æ³›å’Œæ›´å¤æ‚çš„å®¤å†…å’Œå®¤å¤–åœºæ™¯ï¼Œèƒ½å¤Ÿåœ¨å®Œæ•´çš„åœºæ™¯è¡¨ç¤ºä¸­è¯„ä¼°ç‰©ä½“ç§»é™¤ã€‚ç»™å®šåœºæ™¯ç§»é™¤ç‰©ä½“å‰åçš„çœŸå®å›¾åƒï¼Œæˆ‘ä»¬è¯„ä¼°æ˜¯å¦çœŸæ­£æ¶ˆé™¤äº†è¯­ä¹‰å­˜åœ¨æ€§ï¼Œä»¥åŠä¸‹æ¸¸æ¨¡å‹æ˜¯å¦ä»ç„¶å¯ä»¥æ¨æ–­å‡ºè¢«ç§»é™¤çš„å†…å®¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†å½“å‰3Dç‰©ä½“ç§»é™¤æŠ€æœ¯çš„å…³é”®å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆæ¥å¤„ç†ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚è¯„ä¼°æ¡†æ¶å¯åœ¨github.com&#x2F;spatial-intelligence-ai&#x2F;Remove360.gitä¸Šæ‰¾åˆ°ã€‚æ•°æ®å¯åœ¨huggingface.co&#x2F;datasets&#x2F;simkoc&#x2F;Remove360ä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11431v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2503.17574</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæµ‹é‡åœ¨ä¸‰ç»´é«˜æ–¯æ··åˆæŠ€æœ¯ä¸­ç§»é™¤ç‰©ä½“åç•™ä¸‹çš„è¯­ä¹‰æ®‹ç•™ä¿¡æ¯ã€‚ç ”ç©¶å›¢é˜Ÿä¸ºæ­¤å¼€å‘äº†ä¸€ä¸ªåä¸ºRemove360çš„æ•°æ®é›†ï¼ŒåŒ…å«å®¤å†…å¤–åœºæ™¯çš„å‰åç§»é™¤RGBå›¾åƒå’Œç‰©ä½“çº§æ©è†œã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ²¡æœ‰è§†è§‰å‡ ä½•çš„æƒ…å†µä¸‹ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸‰ç»´ç‰©ä½“ç§»é™¤æŠ€æœ¯åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­ä»å­˜åœ¨å±€é™ï¼Œéœ€è¦è¿›ä¸€æ­¥å¼€å‘æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆæ¥å¤„ç†çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰ä¿¡æ¯åœ¨ä¸‰ç»´é‡å»ºå’Œå¯ç¼–è¾‘åœºæ™¯è¡¨ç¤ºä¸­çš„éšç§ä¿æŠ¤è‡³å…³é‡è¦ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæµ‹é‡ä¸‰ç»´é«˜æ–¯æ··åˆæŠ€æœ¯ä¸­ç‰©ä½“ç§»é™¤åçš„è¯­ä¹‰æ®‹ç•™ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªåä¸ºRemove360çš„æ•°æ®é›†ï¼ŒåŒ…å«çœŸå®ç¯å¢ƒä¸­å®¤å†…å¤–åœºæ™¯çš„å‰åç§»é™¤å›¾åƒå’Œç‰©ä½“çº§æ©è†œã€‚</li>
<li>å½“å‰æ–¹æ³•èƒ½å¤Ÿåœ¨æ²¡æœ‰è§†è§‰å‡ ä½•çš„æƒ…å†µä¸‹ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰çš„ä¸‰ç»´ç‰©ä½“ç§»é™¤æŠ€æœ¯åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­ä»å­˜åœ¨å±€é™ã€‚</li>
<li>éœ€è¦å¼€å‘æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆæ¥å¤„ç†çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dda73ce4bf307171b3c527d62058bafd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebdd180ff5a98bea5ddb945be357bcd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a9b37d6fe72e083cc1d24f0ad6fc95c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93b7eb6b39b301e19653935a55555719.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting"><a href="#On-Policy-RL-Meets-Off-Policy-Experts-Harmonizing-Supervised-Fine-Tuning-and-Reinforcement-Learning-via-Dynamic-Weighting" class="headerlink" title="On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised   Fine-Tuning and Reinforcement Learning via Dynamic Weighting"></a>On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised   Fine-Tuning and Reinforcement Learning via Dynamic Weighting</h2><p><strong>Authors:Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, Jingren Zhou</strong></p>
<p>Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two prominent post-training paradigms for refining the capabilities and aligning the behavior of Large Language Models (LLMs). Existing approaches that integrate SFT and RL often face the risk of disrupting established model patterns and inducing overfitting to expert data. To address this, we present a novel investigation into the unified view of SFT and RL through an off-policy versus on-policy lens. We propose CHORD, a framework for the Controllable Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic Weighting, which reframes SFT not as a separate stage but as a dynamically weighted auxiliary objective within the on-policy RL process. Based on an analysis of off-policy expert dataâ€™s influence at both holistic and granular levels, we incorporate a dual-control mechanism in CHORD. Specifically, the framework first employs a global coefficient to holistically guide the transition from off-policy imitation to on-policy exploration, and then applies a token-wise weighting function that enables granular learning from expert tokens, which preserves on-policy exploration and mitigates disruption from off-policy data. We conduct extensive experiments on widely used benchmarks, providing empirical evidence that CHORD achieves a stable and efficient learning process. By effectively harmonizing off-policy expert data with on-policy exploration, CHORD demonstrates significant improvements over baselines. We release the implementation at <a target="_blank" rel="noopener" href="https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord">https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord</a> to inspire further research. </p>
<blockquote>
<p>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸¤ç§ç”¨äºç²¾ç‚¼å’Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½åŠ›å¹¶è°ƒæ•´å…¶è¡Œä¸ºçš„é‡è¦åè®­ç»ƒèŒƒå¼ã€‚ç°æœ‰å°†SFTå’ŒRLç»“åˆçš„æ–¹æ³•å¸¸å¸¸é¢ä¸´ç ´åå·²æœ‰æ¨¡å‹æ¨¡å¼å’Œå¯¹ä¸“å®¶æ•°æ®è¿‡åº¦æ‹Ÿåˆçš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹off-policyå’Œon-policyè§†è§’çš„é€é•œï¼Œå¯¹SFTå’ŒRLçš„ç»Ÿä¸€è§†è§’è¿›è¡Œäº†æ–°å‹æ¢ç©¶ã€‚æˆ‘ä»¬æå‡ºäº†CHORDæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åä¸ºåŸºäºåŠ¨æ€åŠ æƒçš„åœ¨çº¿å’Œç¦»çº¿å¼ºåŒ–å­¦ä¹ å¯æ§åè°ƒæ¡†æ¶ï¼ˆControllable Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic Weightingï¼‰ã€‚æˆ‘ä»¬è®¤ä¸ºSFTå¹¶éæ˜¯ä¸€ä¸ªç‹¬ç«‹é˜¶æ®µï¼Œè€Œæ˜¯ä½œä¸ºåŠ¨æ€åŠ æƒè¾…åŠ©ç›®æ ‡å­˜åœ¨äºåœ¨çº¿RLè¿‡ç¨‹ä¸­ã€‚åŸºäºå¯¹ç¦»çº¿ä¸“å®¶æ•°æ®åœ¨æ•´ä½“å’Œé¢—ç²’åŒ–å±‚é¢å½±å“çš„å…¨é¢åˆ†æï¼Œæˆ‘ä»¬åœ¨CHORDæ¡†æ¶ä¸­èå…¥äº†ä¸€ç§åŒé‡æ§åˆ¶æœºåˆ¶ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶é¦–å…ˆä½¿ç”¨ä¸€ä¸ªå…¨å±€ç³»æ•°æ¥å…¨é¢æŒ‡å¯¼ä»ç¦»çº¿æ¨¡ä»¿åˆ°åœ¨çº¿æ¢ç´¢çš„è¿‡æ¸¡è¿‡ç¨‹ï¼Œç„¶ååº”ç”¨ä¸€ç§åŸºäºä»£å¸çš„åŠ æƒå‡½æ•°æ¥åˆ©ç”¨ä¸“å®¶ä»£å¸è¿›è¡Œç²¾ç»†åŒ–å­¦ä¹ ï¼Œä¿æŒåœ¨çº¿æ¢ç´¢çš„åŒæ—¶ç¼“è§£ç¦»çº¿æ•°æ®çš„å¹²æ‰°ç ´åã€‚æˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œæä¾›äº†å®è¯è¯æ®è¡¨æ˜CHORDèƒ½å¤Ÿå®ç°ç¨³å®šä¸”é«˜æ•ˆçš„å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡æœ‰æ•ˆåœ°åè°ƒç¦»çº¿ä¸“å®¶æ•°æ®å’Œåœ¨çº¿æ¢ç´¢ï¼ŒCHORDåœ¨åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬å·²å°†å®ç°å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord%E4%B8%8A%EF%BC%8C%E4%BB%A5%E6%BF%80%E5%8F%91%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chordä¸Šï¼Œä»¥æ¿€å‘è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11408v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„ç»“åˆæ–¹å¼ï¼Œæå‡ºä¸€ç§åä¸ºCHORDçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºå¯æ§åœ°å’Œè°åœ¨çº¿å’Œç¦»çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡åŠ¨æ€åŠ æƒå®ç°ã€‚è¯¥æ¡†æ¶å°†SFTè§†ä¸ºåŠ¨æ€åŠ æƒè¾…åŠ©ç›®æ ‡ï¼Œèå…¥åœ¨çº¿ç­–ç•¥RLè¿‡ç¨‹ä¸­ã€‚é€šè¿‡å…¨å±€ç³»æ•°å’Œæ ‡è®°çº§åŠ æƒå‡½æ•°å®ç°åŒé‡æ§åˆ¶æœºåˆ¶ï¼Œå¹³è¡¡ç¦»çº¿ç­–ç•¥ä¸“å®¶æ•°æ®ä¸åœ¨çº¿ç­–ç•¥æ¢ç´¢ä¹‹é—´çš„è½¬æ¢ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜äº†CHORDçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CHORDæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæ—¨åœ¨å’Œè°åœ¨çº¿å’Œç¦»çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>CHORDå°†SFTè§†ä¸ºåŠ¨æ€åŠ æƒçš„è¾…åŠ©ç›®æ ‡ï¼Œèå…¥åœ¨çº¿ç­–ç•¥RLè¿‡ç¨‹ä¸­ï¼Œé¿å…æ¨¡å‹æ¨¡å¼è¢«æ‰“ç ´å’Œè¿‡åº¦æ‹Ÿåˆä¸“å®¶æ•°æ®çš„é£é™©ã€‚</li>
<li>é€šè¿‡å…¨å±€ç³»æ•°å’Œæ ‡è®°çº§åŠ æƒå‡½æ•°å®ç°åŒé‡æ§åˆ¶æœºåˆ¶ï¼Œå¹³è¡¡ç¦»çº¿ç­–ç•¥ä¸“å®¶æ•°æ®ä¸åœ¨çº¿ç­–ç•¥æ¢ç´¢ä¹‹é—´çš„è½¬æ¢ã€‚</li>
<li>CHORDæ¡†æ¶åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>é‡Šæ”¾äº†CHORDæ¡†æ¶çš„å®ç°ä»£ç ï¼Œä»¥æ¿€å‘è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11408">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e801cff730ba3dbc24fc18b0654e7e08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26bf1ea5a92d2559738e99a3a3619c91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2316cd27d6392fa2d8729a77692bda43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7bf47cf6bbbe5bb5c4f1b7127407d20.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CRAFT-GUI-Curriculum-Reinforced-Agent-For-GUI-Tasks"><a href="#CRAFT-GUI-Curriculum-Reinforced-Agent-For-GUI-Tasks" class="headerlink" title="CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks"></a>CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks</h2><p><strong>Authors:Songqin Nong, Jingxuan Xu, Sheng Zhou, Jianfeng Chen, Xiaoxuan Tang, Tao Jiang, Wenhao Xu</strong></p>
<p>As autonomous agents become adept at understanding and interacting with graphical user interface (GUI) environments, a new era of automated task execution is emerging. Recent studies have demonstrated that Reinforcement Learning (RL) can effectively enhance agentsâ€™ performance in dynamic interactive GUI environments. However, these methods face two key limitations: (1) they overlook the significant variation in difficulty across different GUI tasks by treating the entire training data as a uniform set, which hampers the agentâ€™s ability to adapt its learning process; and (2) most approaches collapse task-specific nuances into a single, coarse reward, leaving the agent with a uniform signal that yields inefficient policy updates. To address these limitations, we propose CRAFT-GUI, a curriculum learning framework based on Group Relative Policy Optimization (GRPO) that explicitly accounts for the varying difficulty across trajectories. To enable more fine-grained policy optimization, we design a reward function that combines simple rule-based signals with model-judged evaluation, providing richer and more nuanced feedback during training. Experimental results demonstrate that our method achieves significant improvements over previous state-of-the-art approaches, outperforming them by 5.6% on public benchmarks Android Control and 10.3% on our internal online benchmarks, respectively. These findings empirically validate the effectiveness of integrating reinforcement learning with curriculum learning in GUI interaction tasks. </p>
<blockquote>
<p>éšç€è‡ªä¸»ä»£ç†ï¼ˆagentsï¼‰åœ¨ç†è§£å’Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ç¯å¢ƒäº¤äº’æ–¹é¢çš„èƒ½åŠ›é€æ¸å¢å¼ºï¼Œä¸€ä¸ªæ–°çš„è‡ªåŠ¨åŒ–ä»»åŠ¡æ‰§è¡Œæ—¶ä»£æ­£åœ¨å…´èµ·ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æœ‰æ•ˆåœ°æé«˜ä»£ç†åœ¨åŠ¨æ€äº¤äº’å¼GUIç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰å®ƒä»¬å¿½ç•¥äº†ä¸åŒGUIä»»åŠ¡ä¹‹é—´éš¾åº¦çš„æ˜¾è‘—å·®å¼‚ï¼Œå°†æ•´ä¸ªè®­ç»ƒæ•°æ®è§†ä¸ºç»Ÿä¸€é›†ï¼Œè¿™é˜»ç¢äº†ä»£ç†é€‚åº”å…¶å­¦ä¹ è¿‡ç¨‹çš„èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰å¤§å¤šæ•°æ–¹æ³•å°†ä»»åŠ¡ç‰¹å®šçš„ç»†å¾®å·®åˆ«å½’ç»“ä¸ºå•ä¸€ã€ç²—ç•¥çš„å¥–åŠ±ï¼Œä½¿ä»£ç†æ¥æ”¶åˆ°ç»Ÿä¸€çš„ä¿¡å·ï¼Œä»è€Œäº§ç”Ÿä½æ•ˆçš„ç­–ç•¥æ›´æ–°ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„CURAFER-GUIè¯¾ç¨‹å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¾å¼è€ƒè™‘äº†è½¨è¿¹é—´çš„ä¸åŒéš¾åº¦ã€‚ä¸ºäº†å®ç°æ›´ç²¾ç»†çš„ç­–ç•¥ä¼˜åŒ–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œå®ƒå°†åŸºäºç®€å•è§„åˆ™çš„ä¿¡å·ä¸æ¨¡å‹åˆ¤æ–­çš„è¯„ä»·ç›¸ç»“åˆï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æä¾›æ›´ä¸°å¯Œã€æ›´ç»†å¾®çš„åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¬å…±åŸºå‡†æµ‹è¯•Android Controlä¸Šè¾ƒä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•æé«˜äº†5.6%ï¼Œåœ¨æˆ‘ä»¬çš„å†…éƒ¨åœ¨çº¿åŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†10.3%ã€‚è¿™äº›å‘ç°å®è¯åœ°éªŒè¯äº†å¼ºåŒ–å­¦ä¹ ä¸è¯¾ç¨‹å­¦ä¹ åœ¨GUIäº¤äº’ä»»åŠ¡ä¸­ç»“åˆçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11360v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€è‡ªä¸»ä»£ç†åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ç¯å¢ƒä¸­çš„ç†è§£å’Œäº¤äº’èƒ½åŠ›é€æ¸å¢å¼ºï¼Œä¸€ä¸ªæ–°çš„è‡ªåŠ¨åŒ–ä»»åŠ¡æ‰§è¡Œæ—¶ä»£æ­£åœ¨å…´èµ·ã€‚ç„¶è€Œï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤„ç†åŠ¨æ€äº¤äº’å¼GUIç¯å¢ƒæ—¶å­˜åœ¨ä¸¤å¤§å±€é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„è¯¾ç¨‹å­¦ä¹ æ¡†æ¶CRAFT-GUIï¼Œå¹¶è®¾è®¡äº†ä¸°å¯Œçš„å¥–åŠ±å‡½æ•°ä»¥æä¾›æ›´ç²¾ç»†çš„ç­–ç•¥ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è‡ªä¸»ä»£ç†åœ¨GUIç¯å¢ƒä¸­çš„ç†è§£å’Œäº¤äº’èƒ½åŠ›å¢å¼ºï¼Œå¼€å¯äº†æ–°çš„è‡ªåŠ¨åŒ–ä»»åŠ¡æ‰§è¡Œæ—¶ä»£ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤„ç†åŠ¨æ€äº¤äº’å¼GUIç¯å¢ƒæ—¶é¢ä¸´ä¸¤å¤§å±€é™ï¼šå¿½è§†ä¸åŒGUIä»»åŠ¡çš„éš¾åº¦å·®å¼‚ï¼Œä»¥åŠä»»åŠ¡ç‰¹å®šç»†èŠ‚è¢«ç®€åŒ–ä¸ºå•ä¸€çš„ç²—ç³™å¥–åŠ±ã€‚</li>
<li>CRAFT-GUIè¯¾ç¨‹å­¦ä¹ æ¡†æ¶åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæ˜ç¡®è€ƒè™‘è½¨è¿¹çš„ä¸åŒéš¾åº¦ã€‚</li>
<li>CRAFT-GUIè®¾è®¡äº†ç»“åˆåŸºäºè§„åˆ™çš„ç®€å•ä¿¡å·å’Œæ¨¡å‹åˆ¤æ–­çš„è¯„ä¼°çš„å¥–åŠ±å‡½æ•°ï¼Œä¸ºè®­ç»ƒæœŸé—´æä¾›æ›´ä¸°å¯Œå’Œç»†å¾®çš„åé¦ˆã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCRAFT-GUIåœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¾ƒå…ˆå‰çš„æ–¹æ³•æœ‰æ˜¾è‘—æ”¹å–„ï¼Œåˆ†åˆ«æé«˜äº†5.6%å’Œ10.3%ã€‚</li>
<li>CRAFT-GUIæ•´åˆäº†å¼ºåŒ–å­¦ä¹ ä¸è¯¾ç¨‹å­¦ä¹ ï¼Œåœ¨GUIäº¤äº’ä»»åŠ¡ä¸­å±•ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>CRAFT-GUIèƒ½å¤Ÿä¸ºä¸åŒéš¾åº¦çš„ä»»åŠ¡æä¾›é€‚åº”æ€§çš„å­¦ä¹ ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37f8b92eebe108f496c235dea7a9ec1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87776881c56b10d0709bc94082100d70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9272669b22ed3b5e0072aa2b4d8e74bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-254f63654a267c120efa8d70235ef454.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28bf8a8110a9746a3695944324442f4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63e5a43bbff24d4537a4a7cf2c193321.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ETTRL-Balancing-Exploration-and-Exploitation-in-LLM-Test-Time-Reinforcement-Learning-Via-Entropy-Mechanism"><a href="#ETTRL-Balancing-Exploration-and-Exploitation-in-LLM-Test-Time-Reinforcement-Learning-Via-Entropy-Mechanism" class="headerlink" title="ETTRL: Balancing Exploration and Exploitation in LLM Test-Time   Reinforcement Learning Via Entropy Mechanism"></a>ETTRL: Balancing Exploration and Exploitation in LLM Test-Time   Reinforcement Learning Via Entropy Mechanism</h2><p><strong>Authors:Jia Liu, ChangYi He, YingQiao Lin, MingMin Yang, FeiYang Shen, ShaoGuo Liu, TingTing Gao</strong></p>
<p>Recent advancements in Large Language Models have yielded significant improvements in complex reasoning tasks such as mathematics and programming. However, these models remain heavily dependent on annotated data and exhibit limited adaptability in unsupervised scenarios. To address these limitations, test-time reinforcement learning (TTRL) has been proposed, which enables self-optimization by leveraging model-generated pseudo-labels. Despite its promise, TTRL faces several key challenges, including high inference costs due to parallel rollouts and early-stage estimation bias that fosters overconfidence, reducing output diversity and causing performance plateaus. To address these challenges, we introduce an entropy-based mechanism to enhance the exploration-exploitation balance in test-time reinforcement learning through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our approach enables Llama3.1-8B to achieve a 68 percent relative improvement in Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of the rollout tokens budget. This highlights our methodâ€™s ability to effectively optimize the trade-off between inference efficiency, diversity, and estimation robustness, thereby advancing unsupervised reinforcement learning for open-domain reasoning tasks. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼ˆå¦‚æ•°å­¦å’Œç¼–ç¨‹ï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶ä¸¥é‡ä¾èµ–äºæ³¨é‡Šæ•°æ®ï¼Œå¹¶åœ¨æ— ç›‘ç£åœºæ™¯ä¸­è¡¨ç°å‡ºæœ‰é™çš„é€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæå‡ºäº†æµ‹è¯•æ—¶é—´å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡åˆ©ç”¨æ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾è¿›è¡Œè‡ªæˆ‘ä¼˜åŒ–ã€‚å°½ç®¡TTRLå…·æœ‰æ½œåŠ›ï¼Œä½†å®ƒé¢ä¸´ç€å‡ ä¸ªå…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç”±äºå¹¶è¡Œæ»šåŠ¨è€Œäº§ç”Ÿçš„æ¨ç†æˆæœ¬é«˜ä»¥åŠæ—©æœŸé˜¶æ®µä¼°è®¡åå·®å¯¼è‡´çš„è¿‡åº¦è‡ªä¿¡ï¼Œè¿™å‡å°‘äº†è¾“å‡ºå¤šæ ·æ€§å¹¶å¯¼è‡´æ€§èƒ½é«˜åŸã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºç†µçš„æœºåˆ¶ï¼Œé€šè¿‡ä¸¤ç§ç­–ç•¥â€”â€”ç†µå‰æ ‘å¤šæ•°æ»šåŠ¨ï¼ˆETMRï¼‰å’ŒåŸºäºç†µçš„ä¼˜åŠ¿é‡å¡‘ï¼ˆEARï¼‰â€”â€”æ¥å¢å¼ºæµ‹è¯•æ—¶é—´å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢-åˆ©ç”¨å¹³è¡¡ã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿Llama3.1-8Båœ¨AIME 2024åŸºå‡†æµ‹è¯•ä¸Šçš„Pass at 1æŒ‡æ ‡ç›¸å¯¹æé«˜äº†68%ï¼ŒåŒæ—¶åªæ¶ˆè€—äº†60%çš„æ»šåŠ¨ä»¤ç‰Œé¢„ç®—ã€‚è¿™å‡¸æ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³è¡¡æ¨ç†æ•ˆç‡ã€å¤šæ ·æ€§å’Œä¼°è®¡ç¨³å¥æ€§æ–¹é¢çš„ä¼˜åŒ–èƒ½åŠ›ï¼Œä»è€Œæ¨åŠ¨äº†æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ åœ¨å¼€æ”¾åŸŸæ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11356v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¿›æ­¥æ˜¾è‘—ï¼Œä½†ä»ä¾èµ–æ ‡æ³¨æ•°æ®ï¼Œåœ¨æ— äººç›‘ç£çš„åœºæ™¯ä¸‹é€‚åº”æ€§æœ‰é™ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰å¹¶åˆ©ç”¨æ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾è¿›è¡Œè‡ªæˆ‘ä¼˜åŒ–ã€‚ç„¶è€Œï¼ŒTTRLé¢ä¸´é«˜æ¨ç†æˆæœ¬ã€æ—©æœŸé˜¶æ®µä¼°è®¡åå·®ç­‰é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥åŸºäºç†µçš„æœºåˆ¶ï¼Œé€šè¿‡ç†µå‰æ ‘å¤šæ•°æ»šåŠ¨å’ŒåŸºäºç†µçš„ä¼˜åŠ¿é‡å¡‘ä¸¤ç§ç­–ç•¥ï¼Œæé«˜æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡ã€‚æ–°æ–¹æ³•ä½¿Llama3.1-8Båœ¨AIME 2024åŸºå‡†æµ‹è¯•ä¸­Pass at 1æŒ‡æ ‡ç›¸å¯¹æé«˜68%ï¼ŒåŒæ—¶åªæ¶ˆè€—60%çš„æ»šåŠ¨ä»¤ç‰Œé¢„ç®—ã€‚è¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†æ•ˆç‡ã€å¤šæ ·æ€§å’Œä¼°è®¡ç¨³å¥æ€§ä¹‹é—´å–å¾—äº†æœ‰æ•ˆå¹³è¡¡ï¼Œæ¨åŠ¨äº†æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ åœ¨å¼€æ”¾é¢†åŸŸæ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†ä»é¢ä¸´ä¾èµ–æ ‡æ³¨æ•°æ®å’Œæœ‰é™é€‚åº”æ— äººç›‘ç£åœºæ™¯çš„æŒ‘æˆ˜ã€‚</li>
<li>æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰é€šè¿‡æ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾è¿›è¡Œè‡ªæˆ‘ä¼˜åŒ–ï¼Œä½†å­˜åœ¨é«˜æ¨ç†æˆæœ¬å’Œæ—©æœŸé˜¶æ®µä¼°è®¡åå·®çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥åŸºäºç†µçš„æœºåˆ¶ï¼Œä»¥æé«˜æµ‹è¯•æ—¶å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡ã€‚</li>
<li>æå‡ºçš„ä¸¤ç§ç­–ç•¥ï¼šç†µå‰æ ‘å¤šæ•°æ»šåŠ¨å’ŒåŸºäºç†µçš„ä¼˜åŠ¿é‡å¡‘ï¼Œæ—¨åœ¨è§£å†³TTRLé¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>æ–°æ–¹æ³•ä½¿Llama3.1-8Båœ¨AIME 2024åŸºå‡†æµ‹è¯•ä¸­å–å¾—ç›¸å¯¹æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
<li>æ–¹æ³•åœ¨æ¨ç†æ•ˆç‡ã€å¤šæ ·æ€§å’Œä¼°è®¡ç¨³å¥æ€§ä¹‹é—´å–å¾—äº†æœ‰æ•ˆå¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cb210d3d91b4b6cafc7ff4a7995835a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-542e6f6ea0cbbb951544515492961cd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be0798f12e09a46f6b79a6b4bf2cc63e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="HOID-R1-Reinforcement-Learning-for-Open-World-Human-Object-Interaction-Detection-Reasoning-with-Multimodal-Large-Language-Model"><a href="#HOID-R1-Reinforcement-Learning-for-Open-World-Human-Object-Interaction-Detection-Reasoning-with-Multimodal-Large-Language-Model" class="headerlink" title="HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction   Detection Reasoning with Multimodal Large Language Model"></a>HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction   Detection Reasoning with Multimodal Large Language Model</h2><p><strong>Authors:Zhenhao Zhang, Hanqing Wang, Xiangyu Zeng, Ziyu Cheng, Jiaxin Liu, Haoyu Yan, Zhirui Liu, Kaiyang Ji, Tianxiang Gui, Ke Hu, Kangyi Chen, Yahao Fan, Mokai Pan</strong></p>
<p>Understanding and recognizing human-object interaction (HOI) is a pivotal application in AR&#x2F;VR and robotics. Recent open-vocabulary HOI detection approaches depend exclusively on large language models for richer textual prompts, neglecting their inherent 3D spatial understanding capabilities. To address this shortcoming, we introduce HOID-R1, the first HOI detection framework that integrates chain-of-thought (CoT) guided supervised fine-tuning (SFT) with group relative policy optimization (GRPO) within a reinforcement learning (RL) paradigm. Specifically, we initially apply SFT to imbue the model with essential reasoning capabilities, forcing the model to articulate its thought process in the output. Subsequently, we integrate GRPO to leverage multi-reward signals for policy optimization, thereby enhancing alignment across diverse modalities. To mitigate hallucinations in the CoT reasoning, we introduce an â€œMLLM-as-a-judgeâ€ mechanism that supervises the CoT outputs, further improving generalization. Extensive experiments show that HOID-R1 achieves state-of-the-art performance on HOI detection benchmarks and outperforms existing methods in open-world generalization to novel scenarios. </p>
<blockquote>
<p>ç†è§£å’Œè¯†åˆ«äººä¸ç‰©ä½“ä¹‹é—´çš„äº¤äº’ï¼ˆHOIï¼‰æ˜¯AR&#x2F;VRå’Œæœºå™¨äººæŠ€æœ¯ä¸­çš„ä¸€é¡¹é‡è¦åº”ç”¨ã€‚æœ€è¿‘çš„å¼€æ”¾è¯æ±‡HOIæ£€æµ‹æ–¹å¼å®Œå…¨ä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆæ›´ä¸°å¯Œçš„æ–‡æœ¬æç¤ºï¼Œå´å¿½è§†äº†å…¶å†…åœ¨çš„3Dç©ºé—´ç†è§£èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†HOID-R1ï¼Œè¿™æ˜¯é¦–ä¸ªå°†æ€ç»´é“¾å¼•å¯¼çš„ç›‘ç£ç²¾ç»†è°ƒæ•´ï¼ˆSFTï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èŒƒå¼å†…çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸ç»“åˆçš„HOIæ£€æµ‹æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æœ€åˆåº”ç”¨SFTæ¥èµ‹äºˆæ¨¡å‹åŸºæœ¬çš„æ¨ç†èƒ½åŠ›ï¼Œè¿«ä½¿æ¨¡å‹åœ¨è¾“å‡ºä¸­é˜è¿°å…¶æ€è€ƒè¿‡ç¨‹ã€‚éšåï¼Œæˆ‘ä»¬æ•´åˆGRPOæ¥åˆ©ç”¨å¤šå¥–åŠ±ä¿¡å·è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œä»è€Œå¢å¼ºä¸åŒæ¨¡æ€ä¹‹é—´çš„å¯¹é½ã€‚ä¸ºäº†å‡è½»æ€ç»´é“¾æ¨ç†ä¸­çš„å¹»è§‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œMLLMä½œä¸ºæ³•å®˜â€çš„æœºåˆ¶æ¥ç›‘ç£æ€ç»´é“¾çš„è¾“å‡ºï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHOID-R1åœ¨HOIæ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¼€æ”¾ä¸–ç•Œä¸­å¯¹æ–°åœºæ™¯è¿›è¡Œæ³›åŒ–æ—¶è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11350v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„äººæœºäº¤äº’æ£€æµ‹æ¡†æ¶HOID-R1ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†é“¾å¼æ€ç»´å¼•å¯¼çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ã€‚è¯¥æ¡†æ¶ä¸ä»…ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸°å¯Œæ–‡æœ¬æç¤ºï¼Œè¿˜èåˆäº†æ¨¡å‹æœ¬èº«çš„3Dç©ºé—´ç†è§£èƒ½åŠ›ã€‚å®ƒå®ç°äº†ä¼˜ç§€æ€§èƒ½å¹¶æ”¹å–„äº†é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>HOID-R1æ˜¯é¦–ä¸ªé›†æˆé“¾å¼æ€ç»´å¼•å¯¼çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„äººæœºäº¤äº’æ£€æµ‹æ¡†æ¶ã€‚</li>
<li>SFTèµ‹äºˆæ¨¡å‹å…³é”®æ¨ç†èƒ½åŠ›ï¼Œä¿ƒä½¿æ¨¡å‹åœ¨è¾“å‡ºæ—¶å‘ˆç°å…¶æ€è€ƒè¿‡ç¨‹ã€‚</li>
<li>GRPOåˆ©ç”¨å¤šå¥–åŠ±ä¿¡å·è¿›è¡Œç­–ç•¥ä¼˜åŒ–ï¼Œæé«˜äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å¯¹é½æ€§ã€‚</li>
<li>â€œMLLM-as-a-judgeâ€æœºåˆ¶ç”¨äºç›‘ç£é“¾å¼æ€ç»´æ¨ç†ï¼Œå‡å°‘å¹»è§‰ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>HOID-R1åœ¨äººæœºäº¤äº’æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11350">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-502e5b5d75a6bfed728892601a371c3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56c1453aa06f73a51a90fceb1239d399.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd7e6e47fcb6c145ff273297e536994e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-749067e61376f49100750e1205faf6a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9033a5d4f0f160d09be2e4fec66f066d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SGSimEval-A-Comprehensive-Multifaceted-and-Similarity-Enhanced-Benchmark-for-Automatic-Survey-Generation-Systems"><a href="#SGSimEval-A-Comprehensive-Multifaceted-and-Similarity-Enhanced-Benchmark-for-Automatic-Survey-Generation-Systems" class="headerlink" title="SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced   Benchmark for Automatic Survey Generation Systems"></a>SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced   Benchmark for Automatic Survey Generation Systems</h2><p><strong>Authors:Beichen Guo, Zhiyuan Wen, Yu Yang, Peng Gao, Ruosong Yang, Jiaxing Shen</strong></p>
<p>The growing interest in automatic survey generation (ASG), a task that traditionally required considerable time and effort, has been spurred by recent advances in large language models (LLMs). With advancements in retrieval-augmented generation (RAG) and the rising popularity of multi-agent systems (MASs), synthesizing academic surveys using LLMs has become a viable approach, thereby elevating the need for robust evaluation methods in this domain. However, existing evaluation methods suffer from several limitations, including biased metrics, a lack of human preference, and an over-reliance on LLMs-as-judges. To address these challenges, we propose SGSimEval, a comprehensive benchmark for Survey Generation with Similarity-Enhanced Evaluation that evaluates automatic survey generation systems by integrating assessments of the outline, content, and references, and also combines LLM-based scoring with quantitative metrics to provide a multifaceted evaluation framework. In SGSimEval, we also introduce human preference metrics that emphasize both inherent quality and similarity to humans. Extensive experiments reveal that current ASG systems demonstrate human-comparable superiority in outline generation, while showing significant room for improvement in content and reference generation, and our evaluation metrics maintain strong consistency with human assessments. </p>
<blockquote>
<p>å¯¹äºè‡ªåŠ¨ç”Ÿæˆè°ƒæŸ¥ï¼ˆASGï¼‰è¿™ä¸€ä¼ ç»Ÿä¸Šéœ€è¦å¤§é‡æ—¶é—´å’Œç²¾åŠ›çš„ä»»åŠ¡ï¼Œæ—¥ç›Šå¢é•¿çš„å…´è¶£å¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ã€‚éšç€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„å…´èµ·å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„æ™®åŠï¼Œä½¿ç”¨LLMåˆæˆå­¦æœ¯è°ƒæŸ¥å·²æˆä¸ºä¸€ç§å¯è¡Œçš„æ–¹æ³•ï¼Œä»è€Œæé«˜äº†å¯¹æ­¤é¢†åŸŸç¨³å¥è¯„ä¼°æ–¹æ³•çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•å­˜åœ¨è¯¸å¤šå±€é™æ€§ï¼ŒåŒ…æ‹¬æŒ‡æ ‡åè§ã€ç¼ºä¹äººç±»åå¥½ä»¥åŠè¿‡åº¦ä¾èµ–LLMä½œä¸ºè¯„åˆ¤æ ‡å‡†ç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SGSimEvalï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†ç›¸ä¼¼æ€§å¢å¼ºè¯„ä¼°çš„ç»¼åˆè°ƒæŸ¥ç”ŸæˆåŸºå‡†ï¼Œå®ƒé€šè¿‡è¯„ä¼°å¤§çº²ã€å†…å®¹å’Œå‚è€ƒæ¥è¯„ä¼°è‡ªåŠ¨è°ƒæŸ¥ç”Ÿæˆç³»ç»Ÿï¼Œå¹¶å°†LLMè¯„åˆ†ä¸å®šé‡æŒ‡æ ‡ç›¸ç»“åˆï¼Œæä¾›äº†ä¸€ä¸ªå¤šæ–¹é¢çš„è¯„ä¼°æ¡†æ¶ã€‚åœ¨SGSimEvalä¸­ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†äººç±»åå¥½æŒ‡æ ‡ï¼Œå¼ºè°ƒå†…åœ¨è´¨é‡å’Œä¸äººç±»ç›¸ä¼¼åº¦çš„é‡è¦æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå½“å‰ASGç³»ç»Ÿåœ¨ç”Ÿæˆå¤§çº²æ–¹é¢å±•ç°äº†ä¸äººç±»ç›¸å½“çš„ä¼˜åŠ¿ï¼Œè€Œåœ¨å†…å®¹å’Œå‚è€ƒç”Ÿæˆæ–¹é¢ä»å¤§æœ‰æå‡ç©ºé—´ï¼Œæˆ‘ä»¬çš„è¯„ä¼°æŒ‡æ ‡ä¸äººç±»è¯„ä¼°ç»“æœä¿æŒé«˜åº¦ä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11310v1">PDF</a> Accepted to The 21st International Conference on Advanced Data Mining   and Applications (ADMA2025)</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸æ–­å‘å±•ä»¥åŠæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„å…´èµ·å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASsï¼‰çš„æ™®åŠï¼Œè‡ªåŠ¨æ‘˜è¦ç”Ÿæˆï¼ˆASGï¼‰ä»»åŠ¡é€æ¸å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œå½“å‰è¯„ä¼°æ–¹æ³•å­˜åœ¨è¯¸å¤šå±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºSGSimEvalï¼Œä¸€ä¸ªé›†æˆäº†è½®å»“ã€å†…å®¹å’Œå‚è€ƒè¯„ä¼°çš„è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆç»¼åˆåŸºå‡†æµ‹è¯•ã€‚SGSimEvalç»“åˆLLMè¯„åˆ†å’Œå®šé‡æŒ‡æ ‡ï¼Œæä¾›äº†ä¸€ä¸ªå¤šå…ƒåŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶å¼•å…¥äººç±»åå¥½æŒ‡æ ‡ï¼Œå¼ºè°ƒå†…åœ¨è´¨é‡å’Œä¸äººç±»çš„ç›¸ä¼¼æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆï¼ˆASGï¼‰çš„å‘å±•ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASsï¼‰çš„æ™®åŠä½¿å¾—ä½¿ç”¨LLMsåˆæˆå­¦æœ¯æ‘˜è¦æˆä¸ºå¯èƒ½ã€‚</li>
<li>ç°æœ‰çš„è¯„ä¼°æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚åç½®æŒ‡æ ‡ã€ç¼ºä¹äººç±»åå¥½å’Œå¯¹LLMsä½œä¸ºåˆ¤æ–­æ ‡å‡†çš„è¿‡åº¦ä¾èµ–ã€‚</li>
<li>SGSimEvalæ˜¯ä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆç³»ç»Ÿï¼ŒåŒ…æ‹¬è½®å»“ã€å†…å®¹å’Œå‚è€ƒè¯„ä¼°ã€‚</li>
<li>SGSimEvalç»“åˆäº†LLMè¯„åˆ†å’Œå®šé‡æŒ‡æ ‡ï¼Œæä¾›äº†ä¸€ä¸ªå¤šå…ƒåŒ–çš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>SGSimEvalå¼•å…¥çš„äººç±»åå¥½æŒ‡æ ‡å¼ºè°ƒå†…åœ¨è´¨é‡å’Œä¸äººç±»çš„ç›¸ä¼¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11310">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-48dffd76bc7f8d86738576a067f1a268.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee04c7677b6e17042c4ee35e5df47c28.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d341d1e8713a1506a9fd065c3030dd0f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Enhancing-Supervised-Composed-Image-Retrieval-via-Reasoning-Augmented-Representation-Engineering"><a href="#Enhancing-Supervised-Composed-Image-Retrieval-via-Reasoning-Augmented-Representation-Engineering" class="headerlink" title="Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented   Representation Engineering"></a>Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented   Representation Engineering</h2><p><strong>Authors:Jun Li, Kai Li, Shaoguo Liu, Tingting Gao</strong></p>
<p>Composed Image Retrieval (CIR) presents a significant challenge as it requires jointly understanding a reference image and a modified textual instruction to find relevant target images. Some existing methods attempt to use a two-stage approach to further refine retrieval results. However, this often requires additional training of a ranking model. Despite the success of Chain-of-Thought (CoT) techniques in reducing training costs for language models, their application in CIR tasks remains limited â€“ compressing visual information into text or relying on elaborate prompt designs. Besides, existing works only utilize it for zero-shot CIR, as it is challenging to achieve satisfactory results in supervised CIR with a well-trained model. In this work, we proposed a framework that includes the Pyramid Matching Model with Training-Free Refinement (PMTFR) to address these challenges. Through a simple but effective module called Pyramid Patcher, we enhanced the Pyramid Matching Modelâ€™s understanding of visual information at different granularities. Inspired by representation engineering, we extracted representations from COT data and injected them into the LVLMs. This approach allowed us to obtain refined retrieval scores in the Training-Free Refinement paradigm without relying on explicit textual reasoning, further enhancing performance. Extensive experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art methods in supervised CIR tasks. The code will be made public. </p>
<blockquote>
<p>å›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰æŠ€æœ¯é¢ä¸´ç€ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒéœ€è¦åŒæ—¶ç†è§£å‚è€ƒå›¾åƒå’Œä¿®æ”¹åçš„æ–‡æœ¬æŒ‡ä»¤æ¥æ‰¾åˆ°ç›¸å…³çš„ç›®æ ‡å›¾åƒã€‚ä¸€äº›ç°æœ‰çš„æ–¹æ³•è¯•å›¾é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•æ¥è¿›ä¸€æ­¥æ”¹è¿›æ£€ç´¢ç»“æœã€‚ç„¶è€Œï¼Œè¿™é€šå¸¸éœ€è¦é¢å¤–è®­ç»ƒæ’åºæ¨¡å‹ã€‚å°½ç®¡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æŠ€æœ¯åœ¨é™ä½è¯­è¨€æ¨¡å‹è®­ç»ƒæˆæœ¬æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬åœ¨CIRä»»åŠ¡ä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™â€”â€”éœ€è¦å‹ç¼©è§†è§‰ä¿¡æ¯ä¸ºæ–‡æœ¬æˆ–ä¾èµ–äºå¤æ‚çš„æç¤ºè®¾è®¡ã€‚æ­¤å¤–ï¼Œç°æœ‰å·¥ä½œä»…å°†å…¶ç”¨äºé›¶æ ·æœ¬å›¾åƒæ£€ç´¢ï¼ˆZero-Shot CIRï¼‰ï¼Œå› ä¸ºåœ¨æœ‰ç›‘ç£çš„å›¾åƒæ£€ç´¢ï¼ˆSupervised CIRï¼‰ä»»åŠ¡ä¸­ä½¿ç”¨ç»è¿‡è‰¯å¥½è®­ç»ƒçš„æ¨¡å‹å®ç°æ»¡æ„ç»“æœå…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…æ‹¬é‡‘å­—å¡”åŒ¹é…æ¨¡å‹ä¸å…è®­ç»ƒä¼˜åŒ–ï¼ˆPMTFRï¼‰çš„æ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é€šè¿‡ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¨¡å—â€”â€”é‡‘å­—å¡”è¡¥ä¸ç¨‹åºï¼ˆPyramid Patcherï¼‰ï¼Œæˆ‘ä»¬å¢å¼ºäº†é‡‘å­—å¡”åŒ¹é…æ¨¡å‹å¯¹ä¸åŒç²’åº¦è§†è§‰ä¿¡æ¯çš„ç†è§£ã€‚å—è¡¨ç¤ºå·¥ç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬ä»CoTæ•°æ®ä¸­æå–è¡¨ç¤ºå¹¶å°†å…¶æ³¨å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ã€‚è¿™ç§æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å…è®­ç»ƒä¼˜åŒ–èŒƒå¼ä¸­è·å¾—ç²¾ç»†çš„æ£€ç´¢åˆ†æ•°ï¼Œæ— éœ€ä¾èµ–æ˜ç¡®çš„æ–‡æœ¬æ¨ç†ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚åœ¨CIRåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPMTFRåœ¨ç›‘ç£å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸­è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å°†å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11272v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æè¿°äº†Composed Image Retrievalï¼ˆCIRï¼‰çš„æŒ‘æˆ˜æ€§ï¼Œå¹¶ä»‹ç»äº†ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜è€Œæå‡ºçš„Pyramid Matching Model with Training-Free Refinementï¼ˆPMTFRï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡Pyramid Patcheræ¨¡å—å¢å¼ºäº†å¯¹ä¸åŒç²’åº¦è§†è§‰ä¿¡æ¯çš„ç†è§£ï¼Œä»Chain-of-Thoughtï¼ˆCoTï¼‰æ•°æ®ä¸­æå–è¡¨ç¤ºå¹¶æ³¨å…¥åˆ°æ¨¡å‹ä¸­ï¼Œä»è€Œè·å¾—æ›´ç²¾ç¡®çš„æ£€ç´¢åˆ†æ•°ï¼Œä¸”ä¸éœ€è¦ä¾é æ˜ç¡®çš„æ–‡æœ¬æ¨ç†ã€‚è¯¥æ¡†æ¶åœ¨CIRä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CIRéœ€è¦åŒæ—¶ç†è§£å‚è€ƒå›¾åƒå’Œä¿®æ”¹åçš„æ–‡æœ¬æŒ‡ä»¤ä»¥æ‰¾åˆ°ç›¸å…³çš„ç›®æ ‡å›¾åƒï¼Œæ„æˆä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å°è¯•ä½¿ç”¨ä¸¤é˜¶æ®µæ–¹æ³•æ¥è¿›ä¸€æ­¥æ”¹è¿›æ£€ç´¢ç»“æœï¼Œä½†è¿™é€šå¸¸éœ€è¦é¢å¤–è®­ç»ƒæ’åæ¨¡å‹ã€‚</li>
<li>Chain-of-Thoughtï¼ˆCoTï¼‰æŠ€æœ¯åœ¨é™ä½è¯­è¨€æ¨¡å‹è®­ç»ƒæˆæœ¬æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨CIRä»»åŠ¡ä¸­çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚</li>
<li>PMTFRæ¡†æ¶é€šè¿‡Pyramid Patcheræ¨¡å—å¢å¼ºäº†å¯¹ä¸åŒç²’åº¦è§†è§‰ä¿¡æ¯çš„ç†è§£ï¼Œå¹¶ä»CoTæ•°æ®ä¸­æå–è¡¨ç¤ºæ³¨å…¥åˆ°æ¨¡å‹ä¸­ã€‚</li>
<li>è¯¥æ–¹æ³•å…è®¸åœ¨æ²¡æœ‰ä¾é æ˜ç¡®çš„æ–‡æœ¬æ¨ç†çš„æƒ…å†µä¸‹è·å¾—æ›´ç²¾ç¡®çš„æ£€ç´¢åˆ†æ•°ã€‚</li>
<li>PMTFRæ¡†æ¶åœ¨CIRåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›‘ç£CIRä»»åŠ¡ä¸­ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b0b52024738b0dafe0f5ad0cadb94e26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48af97fa1df610601271d8af9517ed61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-438b4172765e662ac24d24f60b56a1bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4858b1a781104af4432690735a8186b4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="UAV-VL-R1-Generalizing-Vision-Language-Models-via-Supervised-Fine-Tuning-and-Multi-Stage-GRPO-for-UAV-Visual-Reasoning"><a href="#UAV-VL-R1-Generalizing-Vision-Language-Models-via-Supervised-Fine-Tuning-and-Multi-Stage-GRPO-for-UAV-Visual-Reasoning" class="headerlink" title="UAV-VL-R1: Generalizing Vision-Language Models via Supervised   Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning"></a>UAV-VL-R1: Generalizing Vision-Language Models via Supervised   Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning</h2><p><strong>Authors:Jiajin Guan, Haibo Mei, Bonan Zhang, Dan Liu, Yuanshuang Fu, Yue Zhang</strong></p>
<p>Recent advances in vision-language models (VLMs) have demonstrated strong generalization in natural image tasks. However, their performance often degrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features high resolution, complex spatial semantics, and strict real-time constraints. These challenges limit the applicability of general-purpose VLMs to structured aerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a lightweight VLM explicitly designed for aerial visual reasoning. It is trained using a hybrid method that combines supervised fine-tuning (SFT) and multi-stage reinforcement learning (RL). We leverage the group relative policy optimization (GRPO) algorithm to promote structured and interpretable reasoning through rule-guided rewards and intra-group policy alignment. To support model training and evaluation, we introduce a high-resolution visual question answering dataset named HRVQA-VL, which consists of 50,019 annotated samples covering eight UAV-relevant reasoning tasks, including object counting, transportation recognition, and spatial scene inference. Experimental results show that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the Qwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which is 36x larger, on multiple tasks. Ablation studies reveal that while SFT improves semantic alignment, it may reduce reasoning diversity in mathematical tasks. GRPO-based RL compensates for this limitation by enhancing logical flexibility and the robustness of inference. Additionally, UAV-VL-R1 requires only 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with INT8, supporting real-time deployment on resource-constrained UAV platforms. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥åœ¨è‡ªç„¶å›¾åƒä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŸºäºæ— äººæœºçš„èˆªç©ºå›¾åƒä¸Šçš„æ€§èƒ½å¾€å¾€ä¼šä¸‹é™ï¼Œè¿™äº›å›¾åƒå…·æœ‰åˆ†è¾¨ç‡é«˜ã€ç©ºé—´è¯­ä¹‰å¤æ‚å’Œå®æ—¶çº¦æŸä¸¥æ ¼ç­‰ç‰¹ç‚¹ã€‚è¿™äº›æŒ‘æˆ˜é™åˆ¶äº†é€šç”¨VLMsåœ¨ç»“æ„åŒ–èˆªç©ºæ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UAV-VL-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºèˆªç©ºè§†è§‰æ¨ç†è®¾è®¡çš„è½»é‡çº§VLMã€‚å®ƒé‡‡ç”¨æ··åˆæ–¹æ³•è®­ç»ƒï¼Œç»“åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚æˆ‘ä»¬åˆ©ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œé€šè¿‡è§„åˆ™å¼•å¯¼çš„å¥–åŠ±å’Œç»„å†…ç­–ç•¥å¯¹é½ï¼Œä¿ƒè¿›ç»“æ„åŒ–å’Œå¯è§£é‡Šæ€§çš„æ¨ç†ã€‚ä¸ºäº†æ”¯æŒæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé«˜åˆ†è¾¨ç‡è§†è§‰é—®ç­”æ•°æ®é›†HRVQA-VLï¼Œå®ƒåŒ…å«50,019ä¸ªæ³¨é‡Šæ ·æœ¬ï¼Œæ¶µç›–å…«ä¸ªä¸æ— äººæœºç›¸å…³çš„æ¨ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç›®æ ‡è®¡æ•°ã€äº¤é€šè¯†åˆ«ä»¥åŠç©ºé—´åœºæ™¯æ¨æ–­ç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUAV-VL-R1çš„é›¶æ ·æœ¬å‡†ç¡®ç‡æ¯”Qwen2-VL-2B-InstructåŸºçº¿é«˜å‡º48.17%ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªä»»åŠ¡ä¸Šç”šè‡³è¶…è¶Šäº†å…¶72Bè§„æ¨¡çš„å˜ä½“ã€‚æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œè™½ç„¶SFTæé«˜äº†è¯­ä¹‰å¯¹é½æ€§ï¼Œä½†å®ƒå¯èƒ½ä¼šå‡å°‘æ•°å­¦ä»»åŠ¡ä¸­çš„æ¨ç†å¤šæ ·æ€§ã€‚åŸºäºGRPOçš„RLå¼¥è¡¥äº†è¿™ä¸€å±€é™æ€§ï¼Œé€šè¿‡æé«˜é€»è¾‘çµæ´»æ€§å’Œæ¨ç†çš„ç¨³å¥æ€§æ¥è¡¥å¿ã€‚æ­¤å¤–ï¼ŒUAV-VL-R1åœ¨FP16æ¨ç†ä¸‹åªéœ€3.9GBçš„å†…å­˜ï¼Œå¯ä»¥ä½¿ç”¨INT8è¿›è¡Œé‡åŒ–è‡³2.5GBï¼Œæ”¯æŒåœ¨èµ„æºå—é™çš„æ— äººæœºå¹³å°ä¸Šè¿›è¡Œå®æ—¶éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11196v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹æ— äººæœºï¼ˆUAVï¼‰åŸºäºç©ºä¸­è§†è§‰çš„ä»»åŠ¡æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§ä¸ºç©ºä¸­è§†è§‰æ¨ç†è®¾è®¡çš„è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹UAV-VL-R1ã€‚è¯¥æ¨¡å‹ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶åˆ©ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ä¿ƒè¿›ç»“æ„åŒ–ã€å¯è§£é‡Šçš„æ¨ç†ã€‚ä¸ºæ”¯æŒæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ï¼Œå¼•å…¥äº†é«˜åˆ†è¾¨ç‡è§†è§‰é—®ç­”æ•°æ®é›†HRVQA-VLã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUAV-VL-R1åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œç”šè‡³è¶…è¶Šäº†è§„æ¨¡æ›´å¤§çš„æ¨¡å‹ã€‚å¼ºåŒ–å­¦ä¹ è¡¥å¿äº†è¯­ä¹‰å¯¹é½ä¸­çš„æ¨ç†å¤šæ ·æ€§æŸå¤±ï¼Œå¢å¼ºäº†é€»è¾‘çµæ´»æ€§å’Œæ¨ç†ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼ŒUAV-VL-R1åœ¨FP16æ¨ç†ä¸‹ä»…éœ€3.9GBå†…å­˜ï¼Œå¯é‡åŒ–è‡³2.5GBçš„INT8ï¼Œé€‚åˆåœ¨èµ„æºå—é™çš„UAVå¹³å°ä¸Šå®æ—¶éƒ¨ç½²ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†æ— äººæœºç©ºä¸­å›¾åƒæ—¶æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹éš¾ä»¥å¤„ç†é«˜ç©ºå›¾åƒçš„å¤æ‚ç©ºé—´è¯­ä¹‰å’Œå®æ—¶çº¦æŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹ç©ºä¸­è§†è§‰æ¨ç†ä»»åŠ¡çš„è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹UAV-VL-R1ã€‚</li>
<li>ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ä¿ƒè¿›ç»“æ„åŒ–ã€å¯è§£é‡Šçš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥HRVQA-VLæ•°æ®é›†ç”¨äºæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ï¼Œæ¶µç›–å¤šç§æ— äººæœºç›¸å…³æ¨ç†ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºUAV-VL-R1åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬å‡†ç¡®ç‡ä¸Šæ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e3327e0240399463ef9446e4ecc1a0f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e998a96ff5f1cbcb8bcacdc35cb3dfa9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-579351710e42d2c2f55d08be909960d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b67bfd73078bc1b1f160c498995bbda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faafee312f402413bb826428431323f8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Personalized-Distractor-Generation-via-MCTS-Guided-Reasoning-Reconstruction"><a href="#Personalized-Distractor-Generation-via-MCTS-Guided-Reasoning-Reconstruction" class="headerlink" title="Personalized Distractor Generation via MCTS-Guided Reasoning   Reconstruction"></a>Personalized Distractor Generation via MCTS-Guided Reasoning   Reconstruction</h2><p><strong>Authors:Tao Wu, Jingyuan Chen, Wang Lin, Jian Zhan, Mengze Li, Kun Kuang, Fei Wu</strong></p>
<p>Distractors, incorrect but plausible answer choices in multiple-choice questions (MCQs), play a critical role in educational assessment by diagnosing student misconceptions. Recent work has leveraged large language models (LLMs) to generate shared, group-level distractors by learning common error patterns across large student populations. However, such distractors often fail to capture the diverse reasoning errors of individual students, limiting their diagnostic effectiveness. To address this limitation, we introduce the task of personalized distractor generation, which aims to generate tailored distractors based on individual misconceptions inferred from each studentâ€™s past question-answering (QA) records, ensuring every student receives options that effectively exposes their specific reasoning errors. While promising, this task is challenging because each student typically has only a few QA records, which often lack the studentâ€™s underlying reasoning processes, making training-based group-level approaches infeasible. To overcome this, we propose a training-free two-stage framework. In the first stage, we construct a student-specific misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover the studentâ€™s reasoning trajectories from past incorrect answers. In the second stage, this prototype guides the simulation of the studentâ€™s reasoning on new questions, enabling the generation of personalized distractors that align with the studentâ€™s recurring misconceptions. Experiments show that our approach achieves the best performance in generating plausible, personalized distractors for 140 students, and also effectively generalizes to group-level settings, highlighting its robustness and adaptability. </p>
<blockquote>
<p>å¹²æ‰°é¡¹åœ¨å¤šé€‰é¢˜ï¼ˆMCQsï¼‰ä¸­æ‰®æ¼”ç€è¯Šæ–­å­¦ç”Ÿè¯¯è§£çš„é‡è¦è§’è‰²ï¼Œè¿™äº›é€‰é¡¹è™½ç„¶ä¸æ­£ç¡®ä½†å…·æœ‰è¿·æƒ‘æ€§ã€‚æœ€è¿‘çš„ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡åœ¨å¤§è§„æ¨¡å­¦ç”Ÿç¾¤ä½“ä¸­è¯†åˆ«å¸¸è§çš„é”™è¯¯æ¨¡å¼æ¥ç”Ÿæˆå…±äº«çš„å›¢é˜Ÿçº§åˆ«çš„å¹²æ‰°é¡¹ã€‚ç„¶è€Œï¼Œè¿™ç§å¹²æ‰°é¡¹å¾€å¾€æ— æ³•æ•æ‰åˆ°ä¸ªä½“å­¦ç”Ÿçš„å¤šæ ·åŒ–æ¨ç†é”™è¯¯ï¼Œä»è€Œé™åˆ¶äº†å…¶è¯Šæ–­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸ªæ€§åŒ–å¹²æ‰°é¡¹ç”Ÿæˆä»»åŠ¡ã€‚è¯¥ä»»åŠ¡æ—¨åœ¨æ ¹æ®ä»æ¯ä¸ªå­¦ç”Ÿè¿‡å»çš„é—®ç­”è®°å½•ä¸­æ¨æ–­å‡ºçš„ä¸ªäººè¯¯è§£æ¥ç”Ÿæˆé’ˆå¯¹æ€§çš„å¹²æ‰°é¡¹ï¼Œç¡®ä¿æ¯ä¸ªå­¦ç”Ÿè·å¾—çš„é€‰é¡¹èƒ½æœ‰æ•ˆåœ°æš´éœ²ä»–ä»¬çš„ç‰¹å®šæ¨ç†é”™è¯¯ã€‚å°½ç®¡å‰æ™¯å¹¿é˜”ï¼Œä½†è¿™ä¸ªä»»åŠ¡å¾ˆæœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ¯ä¸ªå­¦ç”Ÿé€šå¸¸åªæœ‰å‡ ä¸ªé—®ç­”è®°å½•ï¼Œè€Œä¸”å¾€å¾€ç¼ºä¹å­¦ç”Ÿæ½œåœ¨çš„æ€è€ƒè¿‡ç¨‹ï¼Œä½¿å¾—åŸºäºè®­ç»ƒçš„æ¨¡å¼ç¾¤ä½“å±‚é¢çš„æ–¹æ³•å˜å¾—ä¸å¯è¡Œã€‚ä¸ºäº†å…‹æœè¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡åº”ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥æ¢å¤å­¦ç”Ÿè¿‡å»çš„é”™è¯¯ç­”æ¡ˆä¸­çš„æ¨ç†è½¨è¿¹ï¼Œä»è€Œæ„å»ºå­¦ç”Ÿç‰¹å®šçš„è¯¯è§£åŸå‹ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œè¿™ä¸ªåŸå‹æŒ‡å¯¼å­¦ç”Ÿåœ¨æ–°é—®é¢˜ä¸Šçš„æ€è€ƒæ¨¡æ‹Ÿï¼Œä½¿ä¸ªæ€§åŒ–å¹²æ‰°é¡¹çš„ç”Ÿæˆä¸å­¦ç”Ÿçš„åå¤è¯¯è§£ä¿æŒä¸€è‡´ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆé’ˆå¯¹140åå­¦ç”Ÿçš„åˆç†ä¸ªæ€§åŒ–å¹²æ‰°é¡¹æ–¹é¢å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°ç¾¤ä½“çº§åˆ«è®¾ç½®ï¼Œå‡¸æ˜¾äº†å…¶ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11184v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨æ•™è‚²è¯„ä¼°ä¸­åˆ©ç”¨ä¸ªæ€§åŒ–å¹²æ‰°é¡¹ç”ŸæˆæŠ€æœ¯æ¥è¯Šæ–­å­¦ç”Ÿè¯¯è§£çš„é‡è¦æ€§ã€‚é’ˆå¯¹ç¾¤ä½“ç”Ÿæˆçš„å¹²æ‰°é¡¹æ— æ³•æ•æ‰ä¸ªä½“å­¦ç”Ÿå¤šæ ·åŒ–æ¨ç†é”™è¯¯çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸ªæ€§åŒ–å¹²æ‰°é¡¹ç”Ÿæˆä»»åŠ¡ã€‚æ­¤ä»»åŠ¡æ—¨åœ¨åŸºäºæ¯ä¸ªå­¦ç”Ÿè¿‡å»çš„ç­”é¢˜è®°å½•ä¸­æ¨æ–­å‡ºçš„ä¸ªäººè¯¯è§£æ¥ç”Ÿæˆå®šåˆ¶åŒ–çš„å¹²æ‰°é¡¹ï¼Œä»¥ç¡®ä¿æ¯ä¸ªå­¦ç”Ÿé‡åˆ°çš„é€‰é¡¹èƒ½å¤Ÿæœ‰æ•ˆæš´éœ²å…¶ç‰¹å®šçš„æ¨ç†é”™è¯¯ã€‚é’ˆå¯¹æ¯ä¸ªå­¦ç”Ÿä»…æœ‰å‡ ä»½ç­”é¢˜è®°å½•ã€ç¼ºä¹å­¦ç”Ÿåº•å±‚æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µåº”ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä»è¿‡å»çš„ä¸æ­£ç¡®ç­”æ¡ˆä¸­æ¢å¤å­¦ç”Ÿçš„æ¨ç†è½¨è¿¹ï¼Œæ„å»ºå­¦ç”Ÿç‰¹å®šçš„è¯¯è§£åŸå‹ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œè¯¥åŸå‹æ¨¡æ‹Ÿå­¦ç”Ÿåœ¨æ–°é—®é¢˜ä¸Šçš„æ¨ç†ï¼Œç”Ÿæˆä¸å­¦ç”Ÿåå¤å‡ºç°çš„è¯¯è§£ç›¸ç¬¦çš„ä¸ªæ€§åŒ–å¹²æ‰°é¡¹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆé’ˆå¯¹140åå­¦ç”Ÿçš„åˆç†ä¸ªæ€§åŒ–å¹²æ‰°é¡¹æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°é€‚åº”ç¾¤ä½“çº§åˆ«è®¾ç½®ï¼Œå‡¸æ˜¾äº†å…¶ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¹²æ‰°é¡¹åœ¨æ•™è‚²è¯„ä¼°ä¸­èµ·ç€è¯Šæ–­å­¦ç”Ÿè¯¯è§£çš„é‡è¦ä½œç”¨ã€‚</li>
<li>ç¾¤ä½“ç”Ÿæˆçš„å¹²æ‰°é¡¹æ— æ³•æ•æ‰ä¸ªä½“å­¦ç”Ÿçš„å¤šæ ·åŒ–æ¨ç†é”™è¯¯ï¼Œéœ€è¦ä¸ªæ€§åŒ–å¹²æ‰°é¡¹ç”Ÿæˆã€‚</li>
<li>ä¸ªæ€§åŒ–å¹²æ‰°é¡¹ç”Ÿæˆä»»åŠ¡æ—¨åœ¨åŸºäºå­¦ç”Ÿçš„ä¸ªäººè¯¯è§£ç”Ÿæˆå®šåˆ¶åŒ–çš„å¹²æ‰°é¡¹ã€‚</li>
<li>å­¦ç”Ÿä»…æœ‰å‡ ä»½ç­”é¢˜è®°å½•ï¼Œç¼ºä¹åº•å±‚æ¨ç†è¿‡ç¨‹ï¼Œä½¿å¾—è®­ç»ƒåŸºäºç¾¤ä½“çš„æ–¹æ³•ä¸å¯è¡Œã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ä¸¤é˜¶æ®µæ¡†æ¶æ¥ç”Ÿæˆä¸ªæ€§åŒ–å¹²æ‰°é¡¹ï¼ŒåŒ…æ‹¬æ„å»ºå­¦ç”Ÿç‰¹å®šçš„è¯¯è§£åŸå‹å’Œæ¨¡æ‹Ÿå­¦ç”Ÿåœ¨æ–°é—®é¢˜ä¸Šçš„æ¨ç†ã€‚</li>
<li>å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨ç”Ÿæˆä¸ªæ€§åŒ–å¹²æ‰°é¡¹æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå¹¶èƒ½å¤Ÿæœ‰æ•ˆé€‚åº”ç¾¤ä½“çº§åˆ«è®¾ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-99ad2c9d127191c8db3d013f71bfdbc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14554122ceac1756e60b7938eb16be31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c50c4bb2dd8705a6351392808b179327.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb3af9b22f8a9038e5db0e62748c87b5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MobQA-A-Benchmark-Dataset-for-Semantic-Understanding-of-Human-Mobility-Data-through-Question-Answering"><a href="#MobQA-A-Benchmark-Dataset-for-Semantic-Understanding-of-Human-Mobility-Data-through-Question-Answering" class="headerlink" title="MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility   Data through Question Answering"></a>MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility   Data through Question Answering</h2><p><strong>Authors:Hikaru Asano, Hiroki Ouchi, Akira Kasuga, Ryo Yonetani</strong></p>
<p>This paper presents MobQA, a benchmark dataset designed to evaluate the semantic understanding capabilities of large language models (LLMs) for human mobility data through natural language question answering.   While existing models excel at predicting human movement patterns, it remains unobvious how much they can interpret the underlying reasons or semantic meaning of those patterns. MobQA provides a comprehensive evaluation framework for LLMs to answer questions about diverse human GPS trajectories spanning daily to weekly granularities. It comprises 5,800 high-quality question-answer pairs across three complementary question types: factual retrieval (precise data extraction), multiple-choice reasoning (semantic inference), and free-form explanation (interpretive description), which all require spatial, temporal, and semantic reasoning. Our evaluation of major LLMs reveals strong performance on factual retrieval but significant limitations in semantic reasoning and explanation question answering, with trajectory length substantially impacting model effectiveness. These findings demonstrate the achievements and limitations of state-of-the-art LLMs for semantic mobility understanding.\footnote{MobQA dataset is available at <a target="_blank" rel="noopener" href="https://github.com/CyberAgentAILab/mobqa.%7D">https://github.com/CyberAgentAILab/mobqa.}</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†MobQAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºäº†è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è‡ªç„¶è¯­è¨€é—®ç­”å¯¹äººç±»ç§»åŠ¨æ•°æ®çš„è¯­ä¹‰ç†è§£èƒ½åŠ›è€Œè®¾è®¡çš„åŸºå‡†æ•°æ®é›†ã€‚è™½ç„¶ç°æœ‰æ¨¡å‹åœ¨é¢„æµ‹äººç±»ç§»åŠ¨æ¨¡å¼æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬èƒ½å¦è§£é‡Šè¿™äº›æ¨¡å¼èƒŒåçš„åŸå› æˆ–è¯­ä¹‰å«ä¹‰ä»ä¸æ˜æ˜¾ã€‚MobQAä¸ºLLMæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå›ç­”å…³äºæ—¥å¸¸åˆ°æ¯å‘¨ç²’åº¦çš„ä¸ä¸€æ ·çš„GPSè½¨è¿¹äººç±»æ•°æ®é—®é¢˜ã€‚å®ƒåŒ…å«ä¸‰ç§è¡¥å……é—®é¢˜ç±»å‹å…±è®¡5800ä¸ªé«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼šäº‹å®æ£€ç´¢ï¼ˆç²¾ç¡®æ•°æ®æå–ï¼‰ã€å¤šé€‰æ¨ç†ï¼ˆè¯­ä¹‰æ¨æ–­ï¼‰å’Œè‡ªç”±å½¢å¼è§£é‡Šï¼ˆè§£é‡Šæ€§æè¿°ï¼‰ï¼Œæ‰€æœ‰è¿™äº›éƒ½éœ€è¦ç©ºé—´ã€æ—¶é—´å’Œè¯­ä¹‰æ¨ç†ã€‚æˆ‘ä»¬å¯¹ä¸»è¦LLMçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬åœ¨äº‹å®æ£€ç´¢æ–¹é¢çš„è¡¨ç°å¼ºåŠ²ï¼Œä½†åœ¨è¯­ä¹‰æ¨ç†å’Œè§£é‡Šé—®é¢˜å›ç­”æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œè½¨è¿¹é•¿åº¦å¯¹æ¨¡å‹çš„æœ‰æ•ˆæ€§æœ‰é‡è¦å½±å“ã€‚è¿™äº›å‘ç°å±•ç¤ºäº†æœ€æ–°LLMåœ¨è¯­ä¹‰ç§»åŠ¨ç†è§£æ–¹é¢çš„æˆå°±å’Œå±€é™æ€§ã€‚MobQaæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CyberAgentAILab/mobqa%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/CyberAgentAILab/mobqaè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11163v1">PDF</a> 23 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MobQAæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¯¹äººç±»ç§»åŠ¨æ€§æ•°æ®çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€é—®ç­”çš„å½¢å¼è¿›è¡Œã€‚ç°æœ‰æ¨¡å‹è™½èƒ½é¢„æµ‹äººç±»ç§»åŠ¨æ¨¡å¼ï¼Œä½†å¯¹å…¶èƒŒåçš„åŸå› æˆ–è¯­ä¹‰æ„ä¹‰çš„è§£è¯»èƒ½åŠ›å°šå¾…éªŒè¯ã€‚MobQAä¸ºLLMsæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œå›ç­”å…³äºæ—¥å¸¸åˆ°æ¯å‘¨ç²’åº¦çš„äººç±»GPSè½¨è¿¹çš„å„ç§é—®é¢˜ã€‚åŒ…å«5800ä¸ªé«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ¶µç›–ä¸‰ç§äº’è¡¥æ€§é—®é¢˜ç±»å‹ï¼šäº‹å®æ£€ç´¢ã€å¤šé¡¹é€‰æ‹©æ¨ç†å’Œè‡ªç”±å½¢å¼è§£é‡Šï¼Œè¿™äº›éƒ½éœ€è¦ç©ºé—´ã€æ—¶é—´å’Œè¯­ä¹‰æ¨ç†ã€‚å¯¹ä¸»è¦LLMsçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå®ƒä»¬åœ¨äº‹å®æ£€ç´¢æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è¯­ä¹‰æ¨ç†å’Œè§£é‡Šé—®é¢˜å›ç­”æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ï¼Œè½¨è¿¹é•¿åº¦å¯¹æ¨¡å‹æ•ˆæœæœ‰å®è´¨å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MobQAæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹äººç±»ç§»åŠ¨æ€§æ•°æ®è¯­ä¹‰ç†è§£èƒ½åŠ›çš„æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡è‡ªç„¶è¯­è¨€é—®ç­”çš„å½¢å¼è®¾è®¡ï¼Œæ—¨åœ¨æµ‹è¯•LLMså¯¹GPSè½¨è¿¹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚</li>
<li>MobQAåŒ…å«ä¸‰ç§äº’è¡¥æ€§é—®é¢˜ç±»å‹ï¼šäº‹å®æ£€ç´¢ã€å¤šé¡¹é€‰æ‹©æ¨ç†å’Œè‡ªç”±å½¢å¼è§£é‡Šã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨é¢„æµ‹äººç±»ç§»åŠ¨æ¨¡å¼æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§£è¯»ç§»åŠ¨èƒŒåçš„è¯­ä¹‰æ„ä¹‰æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>LLMsåœ¨äº‹å®æ£€ç´¢æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è¯­ä¹‰æ¨ç†å’Œè§£é‡Šé—®é¢˜å›ç­”æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ã€‚</li>
<li>è½¨è¿¹é•¿åº¦å¯¹LLMsæ¨¡å‹çš„æ•ˆæœæœ‰å®è´¨å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69c8127b583f2d2e587ac0f53bc70348.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eeeef0951a17b8ff80f7b4dc7b789fb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02a705b2c3eaa0e6069493d455d4292e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05ec478bf7aa919c4420e0ce405e64ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdec83fd0598be60eeedae45cb1ae0c9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MoNaCo-More-Natural-and-Complex-Questions-for-Reasoning-Across-Dozens-of-Documents"><a href="#MoNaCo-More-Natural-and-Complex-Questions-for-Reasoning-Across-Dozens-of-Documents" class="headerlink" title="MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens   of Documents"></a>MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens   of Documents</h2><p><strong>Authors:Tomer Wolfson, Harsh Trivedi, Mor Geva, Yoav Goldberg, Dan Roth, Tushar Khot, Ashish Sabharwal, Reut Tsarfaty</strong></p>
<p>Large language models (LLMs) are emerging as a go-to tool for querying information. However, current LLM benchmarks rarely feature natural questions that are both information-seeking as well as genuinely time-consuming for humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural and complex questions that require dozens, and at times hundreds, of intermediate steps to solve â€“ far more than any existing QA benchmark. To build MoNaCo, we developed a decomposed annotation pipeline to elicit and manually answer natural time-consuming questions at scale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and hallucinations. Our results underscore the need for reasoning models that better handle the complexity and sheer breadth of real-world information-seeking questions â€“ with MoNaCo providing an effective resource for tracking such progress. The MONACO benchmark, codebase, prompts and models predictions are publicly available at: <a target="_blank" rel="noopener" href="https://tomerwolgithub.github.io/monaco">https://tomerwolgithub.github.io/monaco</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£é€æ¸æˆä¸ºæŸ¥è¯¢ä¿¡æ¯çš„é¦–é€‰å·¥å…·ã€‚ç„¶è€Œï¼Œå½“å‰çš„LLMåŸºå‡†æµ‹è¯•å¾ˆå°‘åŒ…å«æ—¢æ˜¯ä¿¡æ¯æ£€ç´¢æ‰€éœ€åˆå¯¹äººç±»æ¥è¯´çœŸæ­£è€—æ—¶è´¹åŠ›çš„è‡ªç„¶é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MoNaCoï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1315ä¸ªè‡ªç„¶ä¸”å¤æ‚çš„é—®é¢˜çš„åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›é—®é¢˜éœ€è¦æ•°åç”šè‡³æ•°ç™¾ä¸ªä¸­é—´æ­¥éª¤æ¥è§£å†³ï¼Œè¿œè¿œè¶…è¿‡ç°æœ‰é—®ç­”åŸºå‡†æµ‹è¯•çš„èŒƒç•´ã€‚ä¸ºäº†æ„å»ºMoNaCoï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåˆ†è§£æ³¨é‡Šç®¡é“ï¼Œä»¥æ¿€å‘å¹¶å¤§è§„æ¨¡æ‰‹åŠ¨å›ç­”è‡ªç„¶è€—æ—¶çš„é—®é¢˜ã€‚åœ¨MoNaCoä¸Šè¯„ä¼°çš„å‰æ²¿LLMæœ€å¤šè¾¾åˆ°61.2%çš„F1åˆ†æ•°ï¼Œå—åˆ°å¬å›ç‡ä½ä¸‹å’Œå¹»è§‰çš„é˜»ç¢ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†éœ€è¦æ›´å¥½çš„å¤„ç†ç°å®ä¸–ç•Œä¿¡æ¯æ£€ç´¢é—®é¢˜çš„å¤æ‚æ€§å’Œå¹¿é˜”æ€§çš„æ¨ç†æ¨¡å‹ï¼Œè€ŒMoNaCoä¸ºè·Ÿè¸ªæ­¤ç±»è¿›å±•æä¾›äº†æœ‰æ•ˆçš„èµ„æºã€‚MONACOåŸºå‡†æµ‹è¯•ã€ä»£ç åº“ã€æç¤ºå’Œæ¨¡å‹é¢„æµ‹å¯åœ¨ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://tomerwolgithub.github.io/monaco">https://tomerwolgithub.github.io/monaco</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11133v1">PDF</a> Accepted for publication in Transactions of the Association for   Computational Linguistics (TACL), 2025. Authors pre-print</p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²æˆä¸ºè·å–ä¿¡æ¯çš„é‡è¦å·¥å…·ã€‚ä¸ºè§£å†³ç°æœ‰LLMåŸºå‡†æµ‹è¯•ç¼ºä¹è‡ªç„¶ä¸”å¤æ‚çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MoNaCoåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1315ä¸ªéœ€è¦æ•°åç”šè‡³æ•°ç™¾ä¸ªä¸­é—´æ­¥éª¤è§£å†³çš„é—®é¢˜ï¼Œè¿œè¶…ç°æœ‰é—®ç­”åŸºå‡†æµ‹è¯•çš„å¤æ‚åº¦ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åˆ†è§£æ³¨é‡Šç®¡é“ï¼Œä»¥å¤§è§„æ¨¡åœ°æå‡ºå¹¶æ‰‹åŠ¨å›ç­”è€—æ—¶çš„é—®é¢˜ã€‚åœ¨MoNaCoä¸Šè¯„ä¼°çš„å‰æ²¿LLMæœ€å¤šåªèƒ½è¾¾åˆ°61.2ï¼…çš„F1åˆ†æ•°ï¼Œå­˜åœ¨å¬å›ç‡ä½å’Œè™šæ„ç­”æ¡ˆçš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†éœ€è¦èƒ½å¤Ÿæ›´å¥½å¤„ç†ç°å®ä¸–ç•Œå¤æ‚ä¿¡æ¯æ£€ç´¢é—®é¢˜çš„æ¨ç†æ¨¡å‹ï¼Œè€ŒMoNaCoåˆ™ä¸ºè·Ÿè¸ªæ­¤ç±»è¿›å±•æä¾›äº†æœ‰æ•ˆèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²æˆä¸ºè·å–ä¿¡æ¯çš„é‡è¦å·¥å…·ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•æœªèƒ½å……åˆ†åæ˜ å®é™…å¤æ‚é—®é¢˜çš„éœ€æ±‚ã€‚</li>
<li>æ¨å‡ºäº†MoNaCoåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è‡ªç„¶ä¸”å¤æ‚çš„é—®é¢˜ï¼Œéœ€è¦å¤§é‡çš„ä¸­é—´æ­¥éª¤æ¥è§£å†³ã€‚</li>
<li>MoNaCoé—®é¢˜çš„å¤æ‚æ€§è¿œè¶…ç°æœ‰é—®ç­”åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ›´å¥½åœ°æ¨¡æ‹Ÿäººç±»åœ¨å®é™…ä¿¡æ¯æ£€ç´¢ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>æˆ‘ä»¬é‡‡ç”¨åˆ†è§£æ³¨é‡Šç®¡é“æ¥å¤§è§„æ¨¡åœ°æå‡ºå¹¶æ‰‹åŠ¨å›ç­”è€—æ—¶çš„é—®é¢˜ã€‚</li>
<li>åœ¨MoNaCoä¸Šè¯„ä¼°çš„å‰æ²¿LLMæ€§èƒ½å—é™ï¼Œå­˜åœ¨å¬å›ç‡ä½å’Œè™šæ„ç­”æ¡ˆçš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰çš„LLMåœ¨å¤„ç†å¤æ‚ã€å¹¿æ³›çš„ç°å®ä¸–ç•Œä¿¡æ¯æ£€ç´¢é—®é¢˜æ—¶è¡¨ç°ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11133">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-33bc51bbea632d1c5e39304026d5fbb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02e65c562289756ac730013511fae723.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adbabc51e0134f64bcdaff7167c60a6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa106bb1f64eb560feb6e1333c6f3978.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Can-Multi-modal-reasoning-LLMs-detect-document-manipulation"><a href="#Can-Multi-modal-reasoning-LLMs-detect-document-manipulation" class="headerlink" title="Can Multi-modal (reasoning) LLMs detect document manipulation?"></a>Can Multi-modal (reasoning) LLMs detect document manipulation?</h2><p><strong>Authors:Zisheng Liang, Kidus Zewde, Rudra Pratap Singh, Disha Patil, Zexi Chen, Jiayu Xue, Yao Yao, Yifei Chen, Qinzhe Liu, Simiao Ren</strong></p>
<p>Document fraud poses a significant threat to industries reliant on secure and verifiable documentation, necessitating robust detection mechanisms. This study investigates the efficacy of state-of-the-art multi-modal large language models (LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus, Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and 3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against each other and prior work on document fraud detection techniques using a standard dataset with real transactional documents. Through prompt optimization and detailed analysis of the modelsâ€™ reasoning processes, we evaluate their ability to identify subtle indicators of fraud, such as tampered text, misaligned formatting, and inconsistent transactional sums. Our results reveal that top-performing multi-modal LLMs demonstrate superior zero-shot generalization, outperforming conventional methods on out-of-distribution datasets, while several vision LLMs exhibit inconsistent or subpar performance. Notably, model size and advanced reasoning capabilities show limited correlation with detection accuracy, suggesting task-specific fine-tuning is critical. This study underscores the potential of multi-modal LLMs in enhancing document fraud detection systems and provides a foundation for future research into interpretable and scalable fraud mitigation strategies. </p>
<blockquote>
<p>æ–‡ä»¶æ¬ºè¯ˆå¯¹ä¾èµ–äºå®‰å…¨å’Œå¯éªŒè¯æ–‡ä»¶çš„è¡Œä¸šæ„æˆé‡å¤§å¨èƒï¼Œéœ€è¦å¯é çš„æ£€æµ‹æœºåˆ¶ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†æœ€å…ˆè¿›çš„è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€æµ‹æ¬ºè¯ˆæ–‡ä»¶æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬OpenAI O1ã€OpenAI 4oã€Gemini Flashï¼ˆæ€è€ƒï¼‰ã€Deepseek Janusã€Grokã€Llama 3.2å’Œ4ã€Qwen 2å’Œ2.5 VLã€Mistral Pixtralä»¥åŠClaude 3.5å’Œ3.7 Sonnetç­‰æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«çœŸå®äº¤æ˜“æ–‡ä»¶çš„æ ‡å‡†æ•°æ®é›†ï¼Œå¯¹è¿™äº›æ¨¡å‹å’Œä¹‹å‰çš„æ–‡ä»¶æ¬ºè¯ˆæ£€æµ‹æŠ€æœ¯åœ¨æ–‡æ¡£æ¬ºè¯ˆæ£€æµ‹æ–¹é¢çš„è¡¨ç°è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚é€šè¿‡å¯¹æ¨¡å‹çš„æç¤ºä¼˜åŒ–å’Œè¯¦ç»†åˆ†ææ¨ç†è¿‡ç¨‹ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å®ƒä»¬è¯†åˆ«æ¬ºè¯ˆçš„å¾®å¦™è¿¹è±¡çš„èƒ½åŠ›ï¼Œå¦‚ç¯¡æ”¹æ–‡æœ¬ã€æ ¼å¼ä¸åŒ¹é…å’Œä¸ä¸€è‡´çš„äº¤æ˜“æ€»å’Œç­‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¡¨ç°æœ€ä½³çš„è·¨æ¨¡æ€LLMæ˜¾ç¤ºå‡ºå“è¶Šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨è¶…å‡ºåˆ†å¸ƒçš„æ•°æ®é›†ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè€Œä¸€äº›è§†è§‰LLMåˆ™è¡¨ç°å‡ºä¸ä¸€è‡´æˆ–è¾ƒå·®çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¨¡å‹å¤§å°ä¸å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ä¸æ£€æµ‹ç²¾åº¦çš„ç›¸å…³æ€§æœ‰é™ï¼Œè¿™è¡¨æ˜é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒè‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†è·¨æ¨¡æ€LLMåœ¨å¢å¼ºæ–‡ä»¶æ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿæ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„å¯è§£é‡Šå’Œå¯æ‰©å±•çš„æ¬ºè¯ˆç¼“è§£ç­–ç•¥ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11021v1">PDF</a> arXiv admin note: text overlap with arXiv:2503.20084</p>
<p><strong>Summary</strong>ï¼šè¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€æµ‹æ¬ºè¯ˆæ–‡ä»¶æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¯¹ä¸€ç³»åˆ—LLMsè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶ä¸å…ˆå‰çš„æ–‡æ¡£æ¬ºè¯ˆæ£€æµ‹æŠ€æœ¯è¿›è¡Œæ¯”å¯¹ï¼Œå‘ç°é¡¶çº§çš„å¤šæ¨¡æ€LLMsåœ¨é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢å…·æœ‰å‡ºè‰²çš„è¡¨ç°ï¼Œèƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿçš„æ£€æµ‹æ‰‹æ®µã€‚ç„¶è€Œï¼Œæ¨¡å‹å¤§å°å’Œæ¨ç†èƒ½åŠ›å¹¶ä¸å®Œå…¨ä¸æ£€æµ‹ç²¾åº¦ç›¸å…³ï¼Œä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒè‡³å…³é‡è¦ã€‚è¿™ä¸ºå¤šæ¨¡æ€LLMsåœ¨å¢å¼ºæ–‡æ¡£æ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿä¸­çš„åº”ç”¨æä¾›äº†æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€æµ‹æ¬ºè¯ˆæ–‡ä»¶æ–¹é¢å±•ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬æ³›åŒ–æ–¹é¢ï¼Œé¡¶çº§çš„å¤šæ¨¡æ€LLMsè¡¨ç°å‡ºå‡ºè‰²çš„æ€§èƒ½ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ£€æµ‹æ‰‹æ®µç›¸æ¯”ï¼Œå¤šæ¨¡æ€LLMsåœ¨æ–‡æ¡£æ¬ºè¯ˆæ£€æµ‹æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>æ¨¡å‹å¤§å°å’Œæ¨ç†èƒ½åŠ›å¹¶ä¸å®Œå…¨ä¸æ£€æµ‹ç²¾åº¦ç›¸å…³ã€‚</li>
<li>ä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒå¯¹äºæé«˜æ£€æµ‹ç²¾åº¦è‡³å…³é‡è¦ã€‚</li>
<li>å¤šæ¨¡æ€LLMsåœ¨æ–‡æ¡£æ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿä¸­å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ea75aab339b6a6b9e18127f6bdd461d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1254f447f723f150b2e3ecb2a7f52f7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-036abd633784929e6f852e4e80077d81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04f9889640913fdff96a2b5f20a5c886.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98f50b728fb796b79564d362b30470f0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CURE-Critical-Token-Guided-Re-concatenation-for-Entropy-collapse-Prevention"><a href="#CURE-Critical-Token-Guided-Re-concatenation-for-Entropy-collapse-Prevention" class="headerlink" title="CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse   Prevention"></a>CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse   Prevention</h2><p><strong>Authors:Qingbin Li, Rongkun Xue, Jie Wang, Ming Zhou, Zhi Li, Xiaofeng Ji, Yongqi Wang, Miao Liu, Zheming Yang, Minghui Qiu, Jing Yang</strong></p>
<p>Recent advances in Reinforcement Learning with Verified Reward (RLVR) have driven the emergence of more sophisticated cognitive behaviors in large language models (LLMs), thereby enhancing their reasoning capabilities. However, in prior RLVR pipelines, the repeated use of static initial-state sampling drawn exactly from the dataset distribution during each sampling phase produced overly deterministic, low diversity model behavior, which manifested as rapid entropy collapse and hindered sustained performance gains during prolonged training. To address this issue, we introduce CURE (Critical-token-gUided Re concatenation for Entropy-collapse prevention), a two-stage framework that balances exploration and exploitation. Specifically, in the first stage, to deliberately steer the model toward novel yet coherent contexts, we re-generate at high-entropy critical tokens and jointly optimize the original and the branched trajectories. The further comparison with vanilla DAPO shows that the regeneration process achieves a better performance on math reasoning tasks while sustaining a high-level entropy degree for exploration. In the second stage, we continue training with static initial-state sampling by DAPO, intentionally placing the model in a familiar state to gradually strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that, compared to other RLVR methods, CURE achieves a 5% performance gain across six math benchmarks, establishing state-of-the-art performance in both entropy and accuracy. A series of experiments further validate the effectiveness of our approach. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CURE-Project/CURE">https://github.com/CURE-Project/CURE</a>. </p>
<blockquote>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ ä¸éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„è¿›å±•ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‚¬ç”Ÿå‡ºæ›´å¤æ‚çš„è®¤çŸ¥è¡Œä¸ºï¼Œä»è€Œå¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨ä¹‹å‰çš„RLVRç®¡é“ä¸­ï¼Œæ¯ä¸ªé‡‡æ ·é˜¶æ®µé‡å¤ä½¿ç”¨ç›´æ¥ä»æ•°æ®é›†åˆ†å¸ƒä¸­æŠ½å–çš„é™æ€åˆå§‹çŠ¶æ€é‡‡æ ·ï¼Œäº§ç”Ÿäº†è¿‡äºç¡®å®šæ€§çš„ã€ä½å¤šæ ·æ€§çš„æ¨¡å‹è¡Œä¸ºï¼Œè¡¨ç°ä¸ºç†µçš„å¿«é€Ÿå´©æºƒï¼Œé˜»ç¢äº†åœ¨å»¶é•¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒç»­æ€§èƒ½æå‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CUREï¼ˆç”¨äºç†µå´©æºƒé¢„é˜²çš„å…³é”®ä»¤ç‰Œå¼•å¯¼é‡è¿æ¥ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„ä¸¤é˜¶æ®µæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä¸ºäº†æ•…æ„å¼•å¯¼æ¨¡å‹èµ°å‘æ–°é¢–è€Œè¿è´¯çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬é‡æ–°ç”Ÿæˆé«˜ç†µå…³é”®ä»¤ç‰Œï¼Œå¹¶è”åˆä¼˜åŒ–åŸå§‹å’Œåˆ†æ”¯è½¨è¿¹ã€‚ä¸æ™®é€šçš„DAPOç›¸æ¯”ï¼Œå†ç”Ÿè¿‡ç¨‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒé«˜æ°´å¹³çš„ç†µåº¦ç”¨äºæ¢ç´¢ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ç»§ç»­é‡‡ç”¨DAPOçš„é™æ€åˆå§‹çŠ¶æ€é‡‡æ ·ï¼Œæœ‰æ„å°†æ¨¡å‹ç½®äºç†Ÿæ‚‰çš„çŠ¶æ€ï¼Œä»¥é€æ­¥åŠ å¼ºåˆ©ç”¨ã€‚åœ¨Qwen-2.5-Math-7Bä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–RLVRæ–¹æ³•ç›¸æ¯”ï¼ŒCUREåœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†5%çš„æ€§èƒ½æå‡ï¼Œåœ¨ç†µå’Œå‡†ç¡®æ€§æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚ä¸€ç³»åˆ—å®éªŒè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/CURE-Project/CURE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/CURE-Project/CUREè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11016v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æŠ€æœ¯çš„è¿›å±•ä¿ƒä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºæ›´å¤æ‚çš„è®¤çŸ¥è¡Œä¸ºï¼Œä»è€Œæå‡å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…ˆå‰RLVRç®¡é“ä¸­æ¯æ¬¡é‡‡æ ·é˜¶æ®µé‡å¤ä½¿ç”¨ä»æ•°æ®é›†åˆ†å¸ƒä¸­ç²¾ç¡®æŠ½å–çš„åˆå§‹çŠ¶æ€é‡‡æ ·å¯¼è‡´æ¨¡å‹è¡Œä¸ºè¿‡äºç¡®å®šæ€§ï¼Œè¡¨ç°ä¸ºç†µå¿«é€Ÿå´©æºƒï¼Œé˜»ç¢äº†é•¿æœŸè®­ç»ƒä¸­çš„æŒç»­æ€§èƒ½æå‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CUREï¼ˆå…³é”®ä»¤ç‰Œå¼•å¯¼é‡ç»„åˆé˜²ç†µå´©æºƒä¸¤é˜¶æ®µæ¡†æ¶ï¼‰ï¼Œå®ƒå¹³è¡¡äº†æ¢ç´¢ä¸åˆ©ç”¨ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡é‡æ–°ç”Ÿæˆé«˜ç†µå…³é”®ä»¤ç‰Œå¹¶è”åˆä¼˜åŒ–åŸå§‹å’Œåˆ†æ”¯è½¨è¿¹ï¼Œæ•…æ„å¼•å¯¼æ¨¡å‹è¿›å…¥æ–°é¢–è€Œè¿è´¯çš„ä¸Šä¸‹æ–‡ã€‚ä¸DAPOç›¸æ¯”ï¼Œå†ç”Ÿè¿‡ç¨‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶ç»´æŒé«˜æ°´å¹³çš„æ¢ç´¢ç†µã€‚ç¬¬äºŒé˜¶æ®µç»§ç»­ä½¿ç”¨é™æ€åˆå§‹çŠ¶æ€é‡‡æ ·è¿›è¡Œè®­ç»ƒï¼Œæ•…æ„å°†æ¨¡å‹ç½®äºç†Ÿæ‚‰çš„çŠ¶æ€ä»¥é€æ­¥å¼ºåŒ–åˆ©ç”¨ã€‚åœ¨Qwen-2.5-Math-7Bä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–RLVRæ–¹æ³•ç›¸æ¯”ï¼ŒCUREåœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†5%çš„æ€§èƒ½æå‡ï¼Œåœ¨ç†µå’Œå‡†ç¡®æ€§æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRæŠ€æœ¯çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†LLMçš„æ›´å¤æ‚è®¤çŸ¥è¡Œä¸ºçš„å‡ºç°ï¼Œå¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ—©æœŸRLVRæ–¹æ³•ä¸­çš„æ¨¡å‹è¡Œä¸ºè¿‡äºç¡®å®šæ€§ï¼Œå¯¼è‡´ç†µå¿«é€Ÿå´©æºƒï¼Œå½±å“é•¿æœŸè®­ç»ƒæ€§èƒ½ã€‚</li>
<li>CUREæ¡†æ¶è¢«å¼•å…¥ä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>CUREçš„ç¬¬ä¸€é˜¶æ®µé€šè¿‡é‡æ–°ç”Ÿæˆé«˜ç†µå…³é”®ä»¤ç‰Œå¼•å¯¼æ¨¡å‹è¿›å…¥æ–°é¢–ä¸”è¿è´¯çš„ä¸Šä¸‹æ–‡ã€‚</li>
<li>ä¸DAPOç›¸æ¯”ï¼ŒCUREåœ¨å†ç”Ÿè¿‡ç¨‹ä¸­å®ç°äº†æ›´å¥½çš„æ•°å­¦æ¨ç†æ€§èƒ½ã€‚</li>
<li>CUREåœ¨Qwen-2.5-Math-7Bå®éªŒä¸Šä¸å…¶ä»–RLVRæ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11016">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7397460c1ee6a6fa94a37aa769e93602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17abac936cfe168fa4564d3a1ed91d70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2fa01e8c1902301d8084dcb542ae40cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3429990a23133fc356a9e7808b40ddf7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf657d4bb8e586cf437ed1a93ca60f00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fff6f68ea062031212ee2265eb73b75.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ORBIT-An-Object-Property-Reasoning-Benchmark-for-Visual-Inference-Tasks"><a href="#ORBIT-An-Object-Property-Reasoning-Benchmark-for-Visual-Inference-Tasks" class="headerlink" title="ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks"></a>ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks</h2><p><strong>Authors:Abhishek Kolari, Mohammadhossein Khojasteh, Yifan Jiang, Floris den Hengst, Filip Ilievski</strong></p>
<p>While vision-language models (VLMs) have made remarkable progress on many popular visual question answering (VQA) benchmarks, it remains unclear whether they abstract and reason over depicted objects. Inspired by human object categorisation, object property reasoning involves identifying and recognising low-level details and higher-level abstractions. While current VQA benchmarks consider a limited set of object property attributes like size, they typically blend perception and reasoning, and lack representativeness in terms of reasoning and image categories. To this end, we introduce a systematic evaluation framework with images of three representative types, three reasoning levels of increasing complexity, and four object property dimensions driven by prior work on commonsense reasoning. We develop a procedure to instantiate this benchmark into ORBIT, a multi-level reasoning VQA benchmark for object properties comprising 360 images paired with a total of 1,080 count-based questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings reveal significant limitations compared to humans, with the best-performing model only reaching 40% accuracy. VLMs struggle particularly with realistic (photographic) images, counterfactual reasoning about physical and functional properties, and higher counts. ORBIT points to the need to develop methods for scalable benchmarking, generalize annotation guidelines, and explore additional reasoning VLMs. We make the ORBIT benchmark and the experimental code available to support such endeavors. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è®¸å¤šæµè¡Œçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬æ˜¯å¦èƒ½å¤ŸæŠ½è±¡å’Œæ¨ç†æè¿°å¯¹è±¡ä»ç„¶ä¸æ˜ç¡®ã€‚å—äººç±»ç‰©ä½“åˆ†ç±»çš„å¯å‘ï¼Œç‰©ä½“å±æ€§æ¨ç†æ¶‰åŠè¯†åˆ«å’Œè¯†åˆ«ä½çº§åˆ«ç»†èŠ‚å’Œé«˜çº§åˆ«æŠ½è±¡ã€‚è™½ç„¶å½“å‰çš„VQAåŸºå‡†æµ‹è¯•åªè€ƒè™‘æœ‰é™çš„å¯¹è±¡å±æ€§å±æ€§ï¼Œå¦‚å¤§å°ï¼Œä½†å®ƒä»¬é€šå¸¸å°†æ„ŸçŸ¥å’Œæ¨ç†æ··åˆåœ¨ä¸€èµ·ï¼Œå¹¶ä¸”åœ¨æ¨ç†å’Œå›¾åƒç±»åˆ«æ–¹é¢çš„ä»£è¡¨æ€§ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ç§æœ‰ä»£è¡¨æ€§çš„å›¾åƒç±»å‹ã€ä¸‰ä¸ªé€æ¸å¢åŠ å¤æ‚åº¦çš„æ¨ç†å±‚æ¬¡ï¼Œä»¥åŠç”±å¸¸è¯†æ¨ç†çš„å…ˆå‰å·¥ä½œé©±åŠ¨çš„å››ä¸ªå¯¹è±¡å±æ€§ç»´åº¦ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–¹æ³•ï¼Œå°†è¿™ä¸€åŸºå‡†æµ‹è¯•å®ä¾‹åŒ–ä¸ºORBITï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šå±‚æ¬¡æ¨ç†çš„VQAåŸºå‡†æµ‹è¯•ï¼Œç”¨äºå¯¹è±¡å±æ€§ï¼ŒåŒ…å«360å¼ å›¾åƒå’Œæ€»è®¡çš„1080ä¸ªåŸºäºè®¡æ•°çš„é—®ç­”ã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ä¸æœ€æ–°çš„è§†è§‰é—®ç­”æ¨¡å‹çš„å®éªŒæ˜¾ç¤ºä¸äººç±»ç›¸æ¯”å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼Œæ€§èƒ½æœ€ä½³çš„æ¨¡å‹åªæœ‰40%çš„å‡†ç¡®æ€§ã€‚æ¨¡å‹å¯¹äºç°å®ç…§ç‰‡å›¾åƒçš„å›¾åƒè¡¨ç°æœ€ä¸ºåƒåŠ›çš„æ˜¯å¯¹äºç‰©ç†å±æ€§å’ŒåŠŸèƒ½å±æ€§çš„åäº‹å®æ¨ç†å’Œæ›´é«˜çš„è®¡æ•°é—®é¢˜ã€‚ORBITæŒ‡å‡ºéœ€è¦å¼€å‘å¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•æ–¹æ³•ã€é€šç”¨æ³¨é‡ŠæŒ‡å—å’Œæ¢ç´¢é¢å¤–çš„æ¨ç†è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬å…¬å¼€æä¾›ORBITåŸºå‡†æµ‹è¯•å’Œå®éªŒä»£ç ä»¥æ”¯æŒæ­¤ç±»ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10956v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹è§†è§‰é—®ç­”æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç‰©ä½“å±æ€§æ¨ç†æ–¹é¢çš„ç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡æ„å»ºåŒ…å«ä¸åŒç±»å‹å›¾åƒã€ä¸åŒéš¾åº¦çº§åˆ«çš„æ¨ç†é—®é¢˜ä»¥åŠåŸºäºå¸¸è¯†æ¨ç†çš„å››ä¸ªç‰©ä½“å±æ€§ç»´åº¦çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºå‡ºå½“å‰æœ€å…ˆè¿›çš„è§†è§‰é—®ç­”æ¨¡å‹åœ¨ç‰©ä½“å±æ€§æ¨ç†æ–¹é¢çš„å±€é™æ€§ã€‚ORBITåŸºå‡†æµ‹è¯•å±•ç¤ºäº†åœ¨çœŸå®å›¾åƒã€ç‰©ç†å’ŒåŠŸèƒ½å±æ€§ä¸Šçš„åäº‹å®æ¨ç†ä»¥åŠé«˜è®¡æ•°æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†æå‡VLMçš„æ€§èƒ½ï¼Œéœ€è¦è¿›ä¸€æ­¥å¼€å‘å¯è§„æ¨¡åŒ–çš„åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼Œæ”¹è¿›æ ‡æ³¨æŒ‡å—ï¼Œå¹¶æ¢ç´¢æ›´å¤šå…·å¤‡æ¨ç†èƒ½åŠ›çš„è§†è§‰é—®ç­”æ¨¡å‹ã€‚æä¾›çš„ä»£ç å’Œèµ„æºå¯æ”¯æŒæ­¤ç±»ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡ç« é‡ç‚¹è®¨è®ºäº†è§†è§‰é—®ç­”æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç‰©ä½“å±æ€§æ¨ç†ä¸Šçš„æ€§èƒ½è¡¨ç°å’ŒæŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ORBITï¼Œæ¶µç›–ä¸åŒç±»å‹å›¾åƒã€ä¸åŒçº§åˆ«çš„æ¨ç†éš¾åº¦å’Œå››ä¸ªç‰©ä½“å±æ€§ç»´åº¦ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„VLMåœ¨ç‰©ä½“å±æ€§æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨çœŸå®å›¾åƒã€ç‰©ç†å’ŒåŠŸèƒ½å±æ€§çš„åäº‹å®æ¨ç†ä»¥åŠé«˜è®¡æ•°æ–¹é¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b144b13284f2c87f6e816337a713c14.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76ba8a2a42a257b4fcec32feb69fd495.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e678ad538138f91706c5ff48e0a1ab8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81dced22a07fd86883daa85b76ac163a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-757805c52f187ced385e9ff6a5fc3380.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Performance-of-GPT-5-in-Brain-Tumor-MRI-Reasoning"><a href="#Performance-of-GPT-5-in-Brain-Tumor-MRI-Reasoning" class="headerlink" title="Performance of GPT-5 in Brain Tumor MRI Reasoning"></a>Performance of GPT-5 in Brain Tumor MRI Reasoning</h2><p><strong>Authors:Mojtaba Safari, Shansong Wang, Mingzhe Hu, Zach Eidex, Qiang Li, Xiaofeng Yang</strong></p>
<p>Accurate differentiation of brain tumor types on magnetic resonance imaging (MRI) is critical for guiding treatment planning in neuro-oncology. Recent advances in large language models (LLMs) have enabled visual question answering (VQA) approaches that integrate image interpretation with natural language reasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and GPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor Segmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain metastases (MET). Each case included multi-sequence MRI triplanar mosaics and structured clinical features transformed into standardized VQA items. Models were assessed in a zero-shot chain-of-thought setting for accuracy on both visual and reasoning tasks. Results showed that GPT-5-mini achieved the highest macro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%), and GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single model dominating across all cohorts. These findings suggest that GPT-5 family models can achieve moderate accuracy in structured neuro-oncological VQA tasks, but not at a level acceptable for clinical use. </p>
<blockquote>
<p>åœ¨ç¥ç»è‚¿ç˜¤å­¦ä¸­ï¼Œé€šè¿‡ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å‡†ç¡®åŒºåˆ†è„‘è‚¿ç˜¤ç±»å‹å¯¹æ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å·²ç»å®ç°äº†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å›¾åƒè§£è¯»å’Œè‡ªç„¶è¯­è¨€æ¨ç†ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†GPT-4oã€GPT-5-nanoã€GPT-5-miniå’ŒGPT-5åœ¨ä¸€ç»„ç»è¿‡ç­›é€‰çš„è„‘è‚¿ç˜¤VQAåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œè¯¥æµ‹è¯•æ•°æ®æ¥è‡ªä¸‰ä¸ªè„‘è‚¿ç˜¤åˆ†å‰²ï¼ˆBraTSï¼‰æ•°æ®é›†â€”â€”èƒ¶è´¨æ¯ç»†èƒç˜¤ï¼ˆGLIï¼‰ã€è„‘è†œç˜¤ï¼ˆMENï¼‰å’Œè„‘è½¬ç§»ç˜¤ï¼ˆMETï¼‰ã€‚æ¯ä¸ªç—…ä¾‹å‡åŒ…æ‹¬å¤šåºåˆ—MRIä¸‰å¹³é¢é©¬èµ›å…‹å›¾åƒå’Œè½¬åŒ–ä¸ºæ ‡å‡†åŒ–VQAé¡¹ç›®çš„ç»“æ„åŒ–ä¸´åºŠç‰¹å¾ã€‚æ¨¡å‹åœ¨é›¶å°„å‡»æ€ç»´é“¾ç¯å¢ƒä¸­å¯¹è§†è§‰å’Œæ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-5-miniçš„å®å¹³å‡å‡†ç¡®ç‡æœ€é«˜ï¼ˆ44.19%ï¼‰ï¼Œå…¶æ¬¡æ˜¯GPT-5ï¼ˆ43.71%ï¼‰ã€GPT-4oï¼ˆ41.49%ï¼‰å’ŒGPT-5-nanoï¼ˆ35.85%ï¼‰ã€‚ä¸åŒè‚¿ç˜¤äºšå‹çš„æ€§èƒ½è¡¨ç°æœ‰æ‰€ä¸åŒï¼Œæ²¡æœ‰ä»»ä½•å•ä¸€æ¨¡å‹åœ¨æ‰€æœ‰ç¾¤ä½“ä¸­å æ®ä¸»å¯¼åœ°ä½ã€‚è¿™äº›å‘ç°è¡¨æ˜GPT-5ç³»åˆ—æ¨¡å‹å¯ä»¥åœ¨ç»“æ„åŒ–çš„ç¥ç»è‚¿ç˜¤å­¦VQAä»»åŠ¡ä¸­å®ç°ä¸­ç­‰å‡†ç¡®ç‡ï¼Œä½†å°šæœªè¾¾åˆ°ä¸´åºŠä½¿ç”¨æ‰€è¦æ±‚çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10865v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¥ç»è‚¿ç˜¤å­¦ä¸­å¯¹è„‘è‚¿ç˜¤ç±»å‹è¿›è¡Œç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åŒºåˆ†çš„åº”ç”¨æ•ˆæœã€‚å®éªŒè¯„ä¼°äº†GPT-4oã€GPT-5-nanoã€GPT-5-miniå’ŒGPT-5åœ¨åŒ…å«èƒ¶è´¨æ¯ç»†èƒç˜¤ï¼ˆGLIï¼‰ã€è„‘è†œç˜¤ï¼ˆMENï¼‰å’Œè„‘è½¬ç§»ç˜¤ï¼ˆMETï¼‰ä¸‰ç§è„‘è‚¿ç˜¤åˆ†å‰²ï¼ˆBraTSï¼‰æ•°æ®é›†çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºGPT-5-miniçš„å®è§‚å¹³å‡å‡†ç¡®ç‡æœ€é«˜ï¼ˆ44.19%ï¼‰ï¼Œä½†æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½å‡æœªè¾¾åˆ°ä¸´åºŠæ¥å—æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¥ç»è‚¿ç˜¤å­¦çš„ç»“æ„åŒ–ç¥ç»è‚¿ç˜¤è§†è§‰é—®ç­”ä»»åŠ¡ä¸­èƒ½å¤Ÿè¾¾åˆ°ä¸­ç­‰å‡†ç¡®ç‡ã€‚</li>
<li>GPT-5å®¶æ—æ¨¡å‹åœ¨è¯„ä¼°ä¸­çš„è¡¨ç°æœ‰æ‰€ä¸åŒï¼ŒGPT-5-miniçš„å®è§‚å¹³å‡å‡†ç¡®ç‡æœ€é«˜ã€‚</li>
<li>ä¸åŒè‚¿ç˜¤äºšå‹çš„æ€§èƒ½è¡¨ç°å­˜åœ¨å·®å¼‚ï¼Œæ²¡æœ‰å•ä¸€æ¨¡å‹èƒ½åœ¨æ‰€æœ‰é˜Ÿåˆ—ä¸­å æ®ä¸»å¯¼åœ°ä½ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¥ç»è‚¿ç˜¤å­¦é¢†åŸŸçš„VQAä»»åŠ¡ä¸­çš„æ€§èƒ½å°šæœªè¾¾åˆ°ä¸´åºŠä½¿ç”¨æ°´å¹³ã€‚</li>
<li>æ­¤ç ”ç©¶åˆ©ç”¨å¤šåºåˆ—MRIä¸‰å¹³é¢é•¶åµŒå›¾åƒå’Œæ ‡å‡†åŒ–VQAé¡¹ç›®è¿›è¡Œè¯„ä¼°ï¼Œå±•ç¤ºäº†VQAåœ¨åŒ»å­¦å›¾åƒè§£è¯»ä¸­çš„æ½œåŠ›ã€‚</li>
<li>æ¨¡å‹æ˜¯åœ¨é›¶æ ·æœ¬æ€ç»´é“¾ç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°ï¼Œè¿™ä¸€ç¯å¢ƒå¯¹æ¨¡å‹çš„è§†è§‰å’Œæ¨ç†ä»»åŠ¡å‡†ç¡®æ€§è¿›è¡Œäº†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-65757aa75cba60480bcd4e9914f323f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-887371aa9ee61f1ad137075539cdf0dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0d785a72dc0d89eb17a05729b6956a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12ad61e8baba484d901e5b656f86bd43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f117733e7235ec3fb7883e9cfbbf6345.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reinforced-Language-Models-for-Sequential-Decision-Making"><a href="#Reinforced-Language-Models-for-Sequential-Decision-Making" class="headerlink" title="Reinforced Language Models for Sequential Decision Making"></a>Reinforced Language Models for Sequential Decision Making</h2><p><strong>Authors:Jim Dilkes, Vahid Yazdanpanah, Sebastian Stein</strong></p>
<p>Large Language Models (LLMs) show potential as sequential decision-making agents, but their application is often limited due to a reliance on large, computationally expensive models. This creates a need to improve smaller models, yet existing post-training methods are designed for single-turn interactions and cannot handle credit assignment in multi-step agentic tasks. To address this, we introduce Multi-Step Group-Relative Policy Optimization (MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP) frameworks. For credit assignment, MS-GRPO attributes the entire cumulative episode reward to each individual episode step. We supplement this algorithm with a novel absolute-advantage-weighted episode sampling strategy that we show improves training performance. We evaluate our approach by post-training a 3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate that the method is effective in improving decision-making performance: our post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on the Frozen Lake task. This work demonstrates that targeted post-training is a practical and efficient alternative to relying on model scale for creating sequential decision-making agents using LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºåºè´¯å†³ç­–ä»£ç†çš„æ½œåŠ›å·²ç»æ˜¾ç°ï¼Œä½†ç”±äºå®ƒä»¬ä¾èµ–äºè®¡ç®—æˆæœ¬é«˜æ˜‚çš„å¤§å‹æ¨¡å‹ï¼Œå…¶åº”ç”¨å¾€å¾€å—åˆ°é™åˆ¶ã€‚è¿™äº§ç”Ÿäº†æ”¹è¿›å°å‹æ¨¡å‹çš„éœ€æ±‚ï¼Œç„¶è€Œç°æœ‰çš„åè®­ç»ƒæ–¹æ³•æ˜¯é’ˆå¯¹å•è½®äº¤äº’è®¾è®¡çš„ï¼Œæ— æ³•å¤„ç†å¤šæ­¥ä»£ç†ä»»åŠ¡ä¸­çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæ­£å¼æ–‡æœ¬ä»‹å¯¼éšæœºæ¸¸æˆï¼ˆTSMGï¼‰å’Œè¯­è¨€ä»£ç†ç­–ç•¥ï¼ˆLAPï¼‰æ¡†æ¶çš„å¤šæ­¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆMS-GRPOï¼‰è¿™ä¸€æ–°å‹LLMä»£ç†åè®­ç»ƒç®—æ³•ã€‚åœ¨ä¿¡ç”¨åˆ†é…æ–¹é¢ï¼ŒMS-GRPOå°†æ•´ä¸ªç´¯ç§¯ç‰‡æ®µå¥–åŠ±åˆ†é…ç»™æ¯ä¸ªå•ç‹¬ç‰‡æ®µçš„æ­¥éª¤ã€‚æˆ‘ä»¬å¯¹è¯¥ç®—æ³•è¿›è¡Œäº†è¡¥å……ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹ç»å¯¹ä¼˜åŠ¿åŠ æƒç‰‡æ®µé‡‡æ ·ç­–ç•¥ï¼Œæˆ‘ä»¬è¯æ˜è¿™ç§ç­–ç•¥èƒ½æé«˜è®­ç»ƒæ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨è›‡å’Œå†°å†»æ¹–ä»»åŠ¡å¯¹è§„æ¨¡ä¸º3äº¿å‚æ•°çš„åè®­ç»ƒæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æå‡å†³ç­–æ€§èƒ½æ–¹é¢æ˜¯æœ‰æ•ˆçš„ï¼šæˆ‘ä»¬çš„åè®­ç»ƒè§„æ¨¡ä¸º3äº¿çš„å‚æ•°æ¨¡å‹åœ¨å†°å†»æ¹–ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºè§„æ¨¡ä¸º72äº¿çš„åŸºå‡†æ¨¡å‹ï¼Œæé«˜äº†50%ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œæœ‰é’ˆå¯¹æ€§çš„åè®­ç»ƒæ˜¯ä¸€ç§å®ç”¨ä¸”é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä¸å†ä¾èµ–æ¨¡å‹è§„æ¨¡æ¥åˆ›å»ºä½¿ç”¨LLMçš„åºè´¯å†³ç­–ä»£ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10839v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½œä¸ºåºåˆ—å†³ç­–ä»£ç†æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶åº”ç”¨å—é™äºå¤§è§„æ¨¡è®¡ç®—æ¨¡å‹ã€‚å› æ­¤ï¼Œéœ€è¦æ”¹è¿›å°å‹æ¨¡å‹ã€‚ç°æœ‰çš„åè®­ç»ƒæ–¹æ³•è®¾è®¡ç”¨äºå•å›åˆäº¤äº’ï¼Œæ— æ³•å¤„ç†å¤šæ­¥éª¤ä»£ç†ä»»åŠ¡ä¸­çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ­£å¼æ–‡æœ¬ä»‹å¯¼éšæœºæ¸¸æˆå’Œè¯­è¨€ä»£ç†ç­–ç•¥æ¡†æ¶çš„å¤šæ­¥éª¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆMS-GRPOï¼‰æ–°ç®—æ³•ã€‚é€šè¿‡ç´¯ç§¯é›†å›æŠ¥å°†æ¯ä¸ªä¸ªä½“æ­¥éª¤å½’åŠŸäºä¿¡ç”¨åˆ†é…ã€‚æˆ‘ä»¬è¡¥å……äº†ä¸€ç§æ–°å‹ç»å¯¹ä¼˜åŠ¿åŠ æƒé›†é‡‡æ ·ç­–ç•¥ï¼Œä»¥æé«˜è®­ç»ƒæ€§èƒ½ã€‚åœ¨Snakeå’ŒFrozen Lakeä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜è¯¥æ–¹æ³•å¯æœ‰æ•ˆæé«˜å†³ç­–æ€§èƒ½ï¼šæˆ‘ä»¬åè®­ç»ƒçš„3äº¿å‚æ•°æ¨¡å‹åœ¨Frozen Lakeä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äº72äº¿å‚æ•°åŸºçº¿æ¨¡å‹ï¼Œé«˜å‡º50%ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œæœ‰é’ˆå¯¹æ€§çš„åè®­ç»ƒæ˜¯åˆ›å»ºä½¿ç”¨LLMçš„åºåˆ—å†³ç­–ä»£ç†çš„ä¸€ç§å®ç”¨ä¸”é«˜æ•ˆçš„é€‰æ‹©ï¼Œä¸ä¸€å®šä¾èµ–æ¨¡å‹è§„æ¨¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åºåˆ—å†³ç­–æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å­˜åœ¨è®¡ç®—èµ„æºé™åˆ¶é—®é¢˜ã€‚</li>
<li>å½“å‰çš„åè®­ç»ƒæ–¹æ³•ä¸»è¦é’ˆå¯¹å•å›åˆäº¤äº’è®¾è®¡ï¼Œéš¾ä»¥å¤„ç†å¤šæ­¥éª¤ä»»åŠ¡ä¸­çš„ä¿¡ç”¨åˆ†é…é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åè®­ç»ƒç®—æ³•â€”â€”å¤šæ­¥éª¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆMS-GRPOï¼‰ï¼Œç”¨äºå¤„ç†LLMä»£ç†çš„å¤šæ­¥éª¤å†³ç­–é—®é¢˜ã€‚</li>
<li>MS-GRPOåŸºäºæ–‡æœ¬ä»‹å¯¼éšæœºæ¸¸æˆå’Œè¯­è¨€ä»£ç†ç­–ç•¥æ¡†æ¶ï¼Œå¯å°†ç´¯ç§¯é›†å›æŠ¥åˆ†é…ç»™æ¯ä¸ªä¸ªä½“æ­¥éª¤ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹é‡‡æ ·ç­–ç•¥â€”â€”ç»å¯¹ä¼˜åŠ¿åŠ æƒé›†é‡‡æ ·ç­–ç•¥ï¼Œç”¨äºæé«˜è®­ç»ƒæ€§èƒ½ã€‚</li>
<li>åœ¨Snakeå’ŒFrozen Lakeå®éªŒä¸Šçš„ç»“æœè¡¨æ˜ï¼Œæ”¹è¿›åçš„æ¨¡å‹åœ¨ä»»åŠ¡æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-42b401fec70b2729c049a4396479d1b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f360bddd613950c308195c31d4bb6279.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77cd6c7ea1488a97014d5343dd43401e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1313e3b7c5a2005c2b3caeeb2de4080.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3652c61671265e44e4b4ab0a2f9afcbb.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="From-Diagnosis-to-Improvement-Probing-Spatio-Physical-Reasoning-in-Vision-Language-Models"><a href="#From-Diagnosis-to-Improvement-Probing-Spatio-Physical-Reasoning-in-Vision-Language-Models" class="headerlink" title="From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in   Vision Language Models"></a>From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in   Vision Language Models</h2><p><strong>Authors:Tiancheng Han, Yunfei Gao, Yong Li, Wuzhou Yu, Qiaosheng Zhang, Wenqi Shao</strong></p>
<p>Spatio-physical reasoning, a foundation capability for understanding the real physics world, is a critical step towards building robust world models. While recent vision language models (VLMs) have shown remarkable progress in specialized domains like multimodal mathematics and pure spatial understanding, their capability for spatio-physical reasoning remains largely unexplored. This paper provides a comprehensive diagnostic analysis of mainstream VLMs, revealing that current models perform inadequately on this crucial task. Further detailed analysis shows that this underperformance is largely attributable to biases caused by human-like prior and a lack of deep reasoning. To address these challenges, we apply supervised fine-tuning followed by rule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significant improvements in spatio-physical reasoning capabilities and surpassing leading proprietary models. Nevertheless, despite this success, the modelâ€™s generalization to new physics scenarios remains limited â€“ underscoring the pressing need for new approaches in spatio-physical reasoning. </p>
<blockquote>
<p>æ—¶ç©ºç‰©ç†æ¨ç†æ˜¯ç†è§£çœŸå®ç‰©ç†ä¸–ç•Œçš„åŸºç¡€èƒ½åŠ›ï¼Œä¹Ÿæ˜¯æ„å»ºç¨³å¥ä¸–ç•Œæ¨¡å‹çš„å…³é”®æ­¥éª¤ã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€æ•°å­¦å’Œçº¯ç©ºé—´ç†è§£ç­‰ç‰¹å®šé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨æ—¶ç©ºç‰©ç†æ¨ç†æ–¹é¢çš„èƒ½åŠ›ä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡å¯¹æ‰€æå‡ºçš„ä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„è¯Šæ–­åˆ†æï¼Œå‘ç°å½“å‰æ¨¡å‹åœ¨è¿™ä¸€å…³é”®ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚è¿›ä¸€æ­¥çš„è¯¦ç»†åˆ†æè¡¨æ˜ï¼Œè¿™ç§è¡¨ç°ä¸ä½³åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºäººä¸ºå…ˆéªŒå¯¼è‡´çš„åè§å’Œç¼ºä¹æ·±åº¦æ¨ç†æ‰€é€ æˆçš„ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¯¹Qwen2.5-VL-7Bæ¨¡å‹åº”ç”¨äº†åŸºäºç›‘ç£å¾®è°ƒåçš„è§„åˆ™å¼ºåŒ–å­¦ä¹ ï¼Œè¿™å¤§å¤§æé«˜äº†å…¶æ—¶ç©ºç‰©ç†æ¨ç†èƒ½åŠ›å¹¶è¶…è¶Šäº†é¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ã€‚ç„¶è€Œï¼Œå°½ç®¡å–å¾—äº†æˆåŠŸï¼Œè¯¥æ¨¡å‹åœ¨æ–°ç‰©ç†åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œè¿™çªæ˜¾äº†å¯¹æ—¶ç©ºç‰©ç†æ¨ç†æ–°æ–¹æ³•è¿«åˆ‡çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10770v1">PDF</a> 9 pages, 6 figures</p>
<p><strong>Summary</strong><br>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç‰¹å®šé¢†åŸŸå¦‚å¤šæ¨¡æ€æ•°å­¦å’Œçº¯ç©ºé—´ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç©ºé—´ç‰©ç†æ¨ç†è¿™ä¸€å…³é”®ä»»åŠ¡ä¸Šçš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ä¸»æµVLMsè¿›è¡Œäº†å…¨é¢çš„è¯Šæ–­åˆ†æï¼Œå‘ç°å½“å‰æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œä¸»è¦åŸå› æ˜¯äººä¸ºå…ˆéªŒçš„åè§å’Œç¼ºä¹æ·±åº¦æ¨ç†ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå¯¹Qwen2.5-VL-7Bè¿›è¡Œäº†ç›‘ç£å¾®è°ƒï¼Œå¹¶é‡‡ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ˜¾è‘—æé«˜äº†å…¶ç©ºé—´ç‰©ç†æ¨ç†èƒ½åŠ›å¹¶è¶…è¶Šäº†é¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ã€‚ç„¶è€Œï¼Œæ¨¡å‹åœ¨æ–°ç‰©ç†åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œå‡¸æ˜¾å‡ºç©ºé—´ç‰©ç†æ¨ç†çš„æ–°æ–¹æ³•è¿«åˆ‡éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´ç‰©ç†æ¨ç†æ˜¯æ„å»ºç¨³å¥ä¸–ç•Œæ¨¡å‹çš„å…³é”®æ­¥éª¤ï¼Œå¯¹äºç†è§£çœŸå®ç‰©ç†ä¸–ç•Œè‡³å…³é‡è¦ã€‚</li>
<li>ä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç©ºé—´ç‰©ç†æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚</li>
<li>å½“å‰æ¨¡å‹çš„ä¸è¶³ä¸»è¦æºäºäººä¸ºå…ˆéªŒçš„åè§å’Œç¼ºä¹æ·±åº¦æ¨ç†ã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒåŠè§„åˆ™å¼ºåŒ–å­¦ä¹ ï¼ŒQwen2.5-VL-7Bæ¨¡å‹çš„ç©ºé—´ç‰©ç†æ¨ç†èƒ½åŠ›å¾—åˆ°æ˜¾è‘—æé«˜ã€‚</li>
<li>å°½ç®¡æœ‰æ‰€è¿›æ­¥ï¼Œæ¨¡å‹åœ¨æ–°ç‰©ç†åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚</li>
<li>éœ€è¦æ¢ç´¢æ–°çš„ç©ºé—´ç‰©ç†æ¨ç†æ–¹æ³•ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10770">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39bf8189f5af34235301ae3d11b6d812.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8e66bde05e5c7a6e2183eff035f12c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ef870dc4f04762836f83c8b91c0e95f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7249b055421ab1969956da362483b071.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12a3202c8e1b15c24dcf3a8d414ceaf7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b1e234cf982abadc1032af6da1cf87e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb090e168e1dba209204e3c4830f1d1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9628701cb42ab3ab7d1e577a2f547718.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs"><a href="#HumanSense-From-Multimodal-Perception-to-Empathetic-Context-Aware-Responses-through-Reasoning-MLLMs" class="headerlink" title="HumanSense: From Multimodal Perception to Empathetic Context-Aware   Responses through Reasoning MLLMs"></a>HumanSense: From Multimodal Perception to Empathetic Context-Aware   Responses through Reasoning MLLMs</h2><p><strong>Authors:Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Yi Yuan, Jingdong Chen, Le Wang</strong></p>
<p>While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutorâ€™s needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: \textcolor{brightpink}<a target="_blank" rel="noopener" href="https://digital-avatar.github.io/ai/HumanSense/">https://digital-avatar.github.io/ai/HumanSense/</a> </p>
<blockquote>
<p>è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å®ç°çœŸæ­£äººç±»èˆ¬çš„äº¤äº’æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†è¿›å±•å—åˆ°ç¼ºä¹ä»¥äººç±»ä¸ºä¸­å¿ƒçš„æƒ…æ™¯çš„ç²¾ç»†è¯„ä¼°æ¡†æ¶çš„é˜»ç¢ï¼Œè¿™äº›æ¡†æ¶éœ€è¦åŒ…å«å¯¹å¤æ‚äººç±»æ„å›¾çš„ç†è§£å’Œæä¾›å¯Œæœ‰åŒæƒ…å¿ƒå’Œæƒ…å¢ƒæ„è¯†çš„å“åº”ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†HumanSenseï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsä»¥äººç±»ä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥å’Œäº¤äº’èƒ½åŠ›ï¼Œç‰¹åˆ«ä¾§é‡äºå¯¹æ‰©å±•çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡çš„æ·±å…¥ç†è§£ä»¥åŠç†æ€§åé¦ˆçš„å½¢æˆã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å‘é«˜çº§äº¤äº’çš„ä»»åŠ¡æ–¹é¢ã€‚é€šè¿‡è¡¥å……è§†è§‰è¾“å…¥ä¸éŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯ç›¸ç»“åˆçš„æ–¹å¼ï¼Œäº§ç”Ÿäº†å®è´¨æ€§çš„æ”¹è¿›ï¼Œå¤šæ¨¡æ€æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¤ä¸ºé€‚å½“çš„åé¦ˆæ¥è‡ªäºå¯¹è¯è€…éœ€æ±‚å’Œæƒ…ç»ªçš„ä¸Šä¸‹æ–‡åˆ†æï¼Œæ¨ç†èƒ½åŠ›æ˜¯è§£é”å®ƒçš„å…³é”®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šé˜¶æ®µã€æ¨¡æ€æ¸è¿›å¼å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºOmniæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨è¯„ä¼°ç»“æœä¸Šå–å¾—äº†é‡å¤§æ”¶è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°æˆåŠŸçš„æ¨ç†è¿‡ç¨‹å±•ç°å‡ºé«˜åº¦ä¸€è‡´çš„æ€è€ƒæ¨¡å¼ã€‚é€šè¿‡è®¾è®¡ç›¸åº”çš„æç¤ºï¼Œæˆ‘ä»¬è¿˜ä»¥æ— åŸ¹è®­çš„æ–¹å¼å¢å¼ºäº†éæ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<brightpink>é“¾æ¥[å¾…æ·»åŠ å®é™…é“¾æ¥åœ°å€]ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10576v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šä»‹ç»äº†é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„äººç±»ä¸­å¿ƒæ„ŸçŸ¥äº¤äº’èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ç³»ç»ŸHumanSenseï¼Œç”¨äºè¯„ä¼°å¯¹å¤æ‚äººç±»æ„å›¾çš„ç†è§£å’Œå¯¹ç§»æƒ…å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å“åº”æä¾›çš„èƒ½åŠ›ã€‚è™½ç„¶å½“å‰çš„æ¨¡å‹åœ¨è®¸å¤šä»»åŠ¡ä¸Šè¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œä½†ç ”ç©¶å‘ç°å°†è§†è§‰è¾“å…¥ä¸éŸ³é¢‘å’Œæ–‡æœ¬ä¿¡æ¯ç›¸ç»“åˆèƒ½å¸¦æ¥æ˜¾è‘—çš„æ”¹è¿›ï¼ŒåŒæ—¶å¼ºè°ƒæ¨ç†èƒ½åŠ›åœ¨æä¾›é€‚å½“åé¦ˆä¸­çš„å…³é”®ä½œç”¨ã€‚ç ”ç©¶è¿˜é€šè¿‡é‡‡ç”¨å¤šé˜¶æ®µã€æ¨¡æ€æ¸è¿›çš„å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡è®¾è®¡ç›¸åº”çš„æç¤ºæ¥å¢å¼ºéæ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç¼ºä¹é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç²¾ç»†è¯„ä»·æ¡†æ¶ï¼Œæ— æ³•å…¨é¢è¯„ä¼°å…¶åœ¨ç†è§£å¤æ‚äººç±»æ„å›¾å’Œæä¾›ç§»æƒ…ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥å“åº”æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>HumanSenseåŸºå‡†æµ‹è¯•ç³»ç»Ÿæ—¨åœ¨è¯„ä¼°MLLMsåœ¨äººç±»ä¸­å¿ƒæ„ŸçŸ¥äº¤äº’æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«å¼ºè°ƒå¯¹æ‰©å±•çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡çš„ç†è§£å’Œæä¾›åˆç†åé¦ˆçš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰MLLMsåœ¨é«˜çº§äº¤äº’ä»»åŠ¡ä¸Šæœ‰è¾ƒå¤§æ”¹è¿›ç©ºé—´ï¼Œèåˆå¤šç§æ¨¡æ€ï¼ˆå¦‚è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ï¼‰ä¿¡æ¯æœ‰åŠ©äºæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨ç†èƒ½åŠ›æ˜¯æä¾›é€‚å½“åé¦ˆçš„å…³é”®ï¼Œç ”ç©¶é‡‡ç”¨å¤šé˜¶æ®µã€æ¨¡æ€æ¸è¿›çš„å¼ºåŒ–å­¦ä¹ æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æˆåŠŸæ¨ç†è¿‡ç¨‹å±•ç°å‡ºé«˜åº¦ä¸€è‡´çš„æ€è€ƒæ¨¡å¼ï¼Œé€šè¿‡è®¾è®¡ç›¸åº”æç¤ºï¼Œå¯ä»¥åœ¨ä¸å¢åŠ è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹æé«˜éæ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>HumanSenseé¡¹ç›®æä¾›äº†ä¸€ä¸ªè¯„ä¼°å’Œå¢å¼ºMLLMsäººç±»ä¸­å¿ƒäº¤äº’èƒ½åŠ›çš„å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b83ea706616e262900441f4d7b7decd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3818844182b80fb9e6a37ba50610be74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a4e5342f09ed5d27e783f40a47fc400.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fba128ef14a0d72f1733c9ba87baf748.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72b6153732c96b8f73de8713786883bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db01aff0d86d8a6951a47fe53dcfd880.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0606902618f78f9d245caa59ddf1a1f8.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-169f8e5805d16b63539c2e6567304608.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  Is ChatGPT-5 Ready for Mammogram VQA?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5d422ced9ca55ff30099e1cbd849d7fd.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  FantasyTalking2 Timestep-Layer Adaptive Preference Optimization for   Audio-Driven Portrait Animation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
