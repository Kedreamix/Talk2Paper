<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  Is ChatGPT-5 Ready for Mammogram VQA?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-169f8e5805d16b63539c2e6567304608.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-19-æ›´æ–°"><a href="#2025-08-19-æ›´æ–°" class="headerlink" title="2025-08-19 æ›´æ–°"></a>2025-08-19 æ›´æ–°</h1><h2 id="Is-ChatGPT-5-Ready-for-Mammogram-VQA"><a href="#Is-ChatGPT-5-Ready-for-Mammogram-VQA" class="headerlink" title="Is ChatGPT-5 Ready for Mammogram VQA?"></a>Is ChatGPT-5 Ready for Mammogram VQA?</h2><p><strong>Authors:Qiang Li, Shansong Wang, Mingzhe Hu, Mojtaba Safari, Zachary Eidex, Xiaofeng Yang</strong></p>
<p>Mammogram visual question answering (VQA) integrates image interpretation with clinical reasoning and has potential to support breast cancer screening. We systematically evaluated the GPT-5 family and GPT-4o model on four public mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment, abnormality detection, and malignancy classification tasks. GPT-5 consistently was the best performing model but lagged behind both human experts and domain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores among GPT variants in density (56.8%), distortion (52.5%), mass (64.5%), calcification (63.5%), and malignancy (52.8%) classification. On InBreast, it attained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0% malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection and 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared with human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and specificity (52.3%). While GPT-5 exhibits promising capabilities for screening tasks, its performance remains insufficient for high-stakes clinical imaging applications without targeted domain adaptation and optimization. However, the tremendous improvements in performance from GPT-4o to GPT-5 show a promising trend in the potential for general large language models (LLMs) to assist with mammography VQA tasks. </p>
<blockquote>
<p>ä¹³è…ºXå…‰æ‘„å½±è§†è§‰é—®ç­”ï¼ˆVQAï¼‰èåˆäº†å›¾åƒè§£è¯»ä¸ä¸´åºŠæ¨ç†ï¼Œå¹¶æ”¯æŒä¹³è…ºç™Œç­›æŸ¥ã€‚æˆ‘ä»¬åœ¨å››ä¸ªå…¬å…±ä¹³è…ºXå…‰æ‘„å½±æ•°æ®é›†ï¼ˆEMBEDã€InBreastã€CMMDå’ŒCBIS-DDSMï¼‰ä¸Šï¼Œé’ˆå¯¹BI-RADSè¯„ä¼°ã€å¼‚å¸¸æ£€æµ‹åŠæ¶æ€§åˆ†ç±»ä»»åŠ¡ï¼Œå¯¹GPT-5ç³»åˆ—å’ŒGPT-4oæ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚GPT-5è¡¨ç°æœ€ä¸ºå‡ºè‰²ä¸”æŒç»­é¢†å…ˆï¼Œä½†åœ¨ä¸äººç±»ä¸“å®¶å’Œç‰¹å®šé¢†åŸŸå¾®è°ƒæ¨¡å‹çš„å¯¹æ¯”ä¸­ä»æœ‰æ‰€ä¸è¶³ã€‚åœ¨EMBEDæ•°æ®é›†ä¸Šï¼ŒGPT-5åœ¨å¯†åº¦ï¼ˆ56.8%ï¼‰ã€æ‰­æ›²ï¼ˆ52.5%ï¼‰ã€è‚¿å—ï¼ˆ64.5%ï¼‰ã€é’™åŒ–ï¼ˆ63.5%ï¼‰å’Œæ¶æ€§åˆ†ç±»ï¼ˆ52.8%ï¼‰æ–¹é¢ï¼Œåœ¨GPTç³»åˆ—ä¸­å¾—åˆ†æœ€é«˜ã€‚åœ¨InBreastæ•°æ®é›†ä¸Šï¼Œå…¶è¾¾åˆ°äº†36.9%çš„BI-RADSå‡†ç¡®ç‡ã€45.9%çš„å¼‚å¸¸æ£€æµ‹ç‡å’Œ35.0%çš„æ¶æ€§åˆ†ç±»å‡†ç¡®ç‡ã€‚åœ¨CMMDæ•°æ®é›†ä¸Šï¼ŒGPT-5è¾¾åˆ°32.3%çš„å¼‚å¸¸æ£€æµ‹ç‡å’Œ55.0%çš„æ¶æ€§å‡†ç¡®ç‡ã€‚åœ¨CBIS-DDSMæ•°æ®é›†ä¸Šï¼Œå…¶å–å¾—äº†69.3%çš„BI-RADSå‡†ç¡®ç‡ã€66.0%çš„å¼‚å¸¸æ£€æµ‹ç‡å’Œ58.2%çš„æ¶æ€§åˆ†ç±»å‡†ç¡®ç‡ã€‚ä¸äººç±»ä¸“å®¶ä¼°ç®—ç›¸æ¯”ï¼ŒGPT-5çš„æ•æ„Ÿæ€§è¾ƒä½ï¼ˆ63.5%ï¼‰ï¼Œç‰¹å¼‚æ€§ä¹Ÿè¾ƒä½ï¼ˆ52.3%ï¼‰ã€‚å°½ç®¡GPT-5åœ¨ç­›æŸ¥ä»»åŠ¡ä¸­å±•ç°å‡ºæœ‰å‰æ™¯çš„èƒ½åŠ›ï¼Œä½†å…¶æ€§èƒ½å¯¹äºé«˜é£é™©çš„ä¸´åºŠæˆåƒåº”ç”¨ä»ç„¶ä¸è¶³ï¼Œéœ€è¦è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„é¢†åŸŸé€‚åº”å’Œä¼˜åŒ–ã€‚ç„¶è€Œï¼Œä»GPT-4oåˆ°GPT-5çš„æ€§èƒ½å·¨å¤§æå‡æ˜¾ç¤ºå‡ºå¤§å‹é€šç”¨è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¹³è…ºXå…‰æ‘„å½±VQAä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11628v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†Mammogramè§†è§‰é—®ç­”ï¼ˆVQAï¼‰é¢†åŸŸï¼Œè¯„ä»·äº†GPT-5ç³»åˆ—æ¨¡å‹å’ŒGPT-4oæ¨¡å‹åœ¨å››ä¸ªå…¬å…±ä¹³è…ºXå…‰æ‘„å½±æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚GPT-5åœ¨BI-RADSè¯„ä¼°ã€å¼‚å¸¸æ£€æµ‹ä»¥åŠæ¶æ€§åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€å¥½ï¼Œä½†ç›¸è¾ƒäºäººç±»ä¸“å®¶å’Œé’ˆå¯¹ç‰¹å®šé¢†åŸŸå¾®è°ƒè¿‡çš„æ¨¡å‹ä»æœ‰ä¸è¶³ã€‚åœ¨ç‰¹å®šçš„æ•°æ®é›†ä¸Šï¼ŒGPT-5çš„è¡¨ç°åœ¨ä¸åŒä»»åŠ¡ä¸­æœ‰æ‰€ä¸åŒã€‚å°½ç®¡GPT-5åœ¨ç­›æŸ¥ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨é«˜é£é™©çš„ä¸´åºŠæˆåƒåº”ç”¨ä¸­ï¼Œå…¶æ€§èƒ½ä»éœ€é€šè¿‡é’ˆå¯¹æ€§çš„é¢†åŸŸé€‚åº”å’Œä¼˜åŒ–æ¥æå‡ã€‚ä»GPT-4oåˆ°GPT-5çš„æ”¹è¿›æ˜¾ç¤ºå‡ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¹³è…ºXå…‰æ‘„å½±VQAä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GPT-5ç³»åˆ—æ¨¡å‹åœ¨ä¹³è…ºXå…‰æ‘„å½±æ•°æ®é›†ä¸Šçš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿè¯„ä»·ã€‚</li>
<li>GPT-5åœ¨BI-RADSè¯„ä¼°ã€å¼‚å¸¸æ£€æµ‹å’Œæ¶æ€§åˆ†ç±»ä»»åŠ¡ä¸Šå±•ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>GPT-5åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰æ‰€å·®å¼‚ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„é¢†åŸŸé€‚åº”å’Œä¼˜åŒ–ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>GPT-5åœ¨ç­›æŸ¥ä»»åŠ¡ä¸­å±•ç°å‡ºäº†æ½œåŠ›ï¼Œä½†åœ¨é«˜é£é™©çš„ä¸´åºŠæˆåƒåº”ç”¨ä¸­çš„æ€§èƒ½ä»ä¸è¶³ã€‚</li>
<li>ä¸äººç±»ä¸“å®¶ç›¸æ¯”ï¼ŒGPT-5çš„æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§è¾ƒä½ã€‚</li>
<li>ä»GPT-4oåˆ°GPT-5çš„æ”¹è¿›æ˜¾ç¤ºå‡ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¹³è…ºXå…‰æ‘„å½±VQAä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†é›†æˆå›¾åƒè§£è¯»ä¸ä¸´åºŠæ¨ç†çš„é‡è¦æ€§ï¼Œä»¥æ”¯æŒä¹³è…ºç™Œç­›æŸ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1bf4c43494e129336db926ba74969f29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-add27e41ddd1667a8fdd527fe98431a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8020fa57790a77c7f537e89339737665.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff94a7ade7cc31f168952835c0f7440e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e74574778e952f48c4cfc4cafa101dbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b93aac947dd3f14403a0dab9f5d28a5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c976de7bb4e2f5da17ac0b8a8bb8c3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b39478e16c633a81028083feea9b1ff.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Controlling-Multimodal-LLMs-via-Reward-guided-Decoding"><a href="#Controlling-Multimodal-LLMs-via-Reward-guided-Decoding" class="headerlink" title="Controlling Multimodal LLMs via Reward-guided Decoding"></a>Controlling Multimodal LLMs via Reward-guided Decoding</h2><p><strong>Authors:Oscar MaÃ±as, Pierluca Dâ€™Oro, Koustuv Sinha, Adriana Romero-Soriano, Michal Drozdzal, Aishwarya Agrawal</strong></p>
<p>As Multimodal Large Language Models (MLLMs) gain widespread applicability, it is becoming increasingly desirable to adapt them for diverse user needs. In this paper, we study the adaptation of MLLMs through controlled decoding. To achieve this, we introduce the first method for reward-guided decoding of MLLMs and demonstrate its application in improving their visual grounding. Our method involves building reward models for visual grounding and using them to guide the MLLMâ€™s decoding process. Concretely, we build two separate reward models to independently control the degree of object precision and recall in the modelâ€™s output. Our approach enables on-the-fly controllability of an MLLMâ€™s inference process in two ways: first, by giving control over the relative importance of each reward function during decoding, allowing a user to dynamically trade off object precision for recall in image captioning tasks; second, by giving control over the breadth of the search during decoding, allowing the user to control the trade-off between the amount of test-time compute and the degree of visual grounding. We evaluate our method on standard object hallucination benchmarks, showing that it provides significant controllability over MLLM inference, while consistently outperforming existing hallucination mitigation methods. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œå°†å…¶é€‚åº”ä¸åŒçš„ç”¨æˆ·éœ€æ±‚å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†é€šè¿‡å—æ§è§£ç é€‚åº”MLLMsçš„æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MLLMsçš„å¥–åŠ±å¼•å¯¼è§£ç æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å®ƒåœ¨æé«˜è§†è§‰å®šä½æ–¹é¢çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬å»ºç«‹è§†è§‰å®šä½çš„å¥–åŠ±æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬æ¥å¼•å¯¼MLLMçš„è§£ç è¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸¤ä¸ªç‹¬ç«‹çš„å¥–åŠ±æ¨¡å‹ï¼Œåˆ†åˆ«æ§åˆ¶æ¨¡å‹è¾“å‡ºä¸­ç›®æ ‡ç²¾åº¦å’Œå¬å›çš„ç¨‹åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»¥ä¸¤ç§æ–¹å¼å®æ—¶æ§åˆ¶MLLMçš„æ¨ç†è¿‡ç¨‹ï¼šé¦–å…ˆï¼Œé€šè¿‡æ§åˆ¶åœ¨è§£ç è¿‡ç¨‹ä¸­æ¯ä¸ªå¥–åŠ±å‡½æ•°çš„ç›¸å¯¹é‡è¦æ€§ï¼Œå…è®¸ç”¨æˆ·åœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸­åŠ¨æ€æƒè¡¡ç›®æ ‡ç²¾åº¦å’Œå¬å›çš„æƒè¡¡ï¼›å…¶æ¬¡ï¼Œé€šè¿‡æ§åˆ¶åœ¨è§£ç è¿‡ç¨‹ä¸­çš„æœç´¢èŒƒå›´ï¼Œå…è®¸ç”¨æˆ·æ§åˆ¶æµ‹è¯•æ—¶é—´è®¡ç®—é‡ä¸è§†è§‰å®šä½ç¨‹åº¦ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬åœ¨æ ‡å‡†çš„ç›®æ ‡å¹»è§‰è¯„ä¼°åŸºå‡†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜å®ƒåœ¨å¯¹MLLMæ¨ç†çš„æ§åˆ¶æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„å¯æ§æ€§ï¼ŒåŒæ—¶ä¸€è‡´åœ°ä¼˜äºç°æœ‰çš„å¹»è§‰ç¼“è§£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11616v1">PDF</a> Published at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•é€šè¿‡æ§åˆ¶è§£ç æ¥é€‚åº”å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»¥æ»¡è¶³ä¸åŒç”¨æˆ·éœ€æ±‚ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§å¥–åŠ±å¼•å¯¼è§£ç æ–¹æ³•ï¼Œç”¨äºæ”¹å–„MLLMsçš„è§†è§‰å®šä½åŠŸèƒ½ã€‚é€šè¿‡æ„å»ºä¸¤ä¸ªç‹¬ç«‹çš„å¥–åŠ±æ¨¡å‹ï¼Œåˆ†åˆ«æ§åˆ¶æ¨¡å‹è¾“å‡ºä¸­çš„ç›®æ ‡ç²¾åº¦å’Œå¬å›ç‡ï¼Œå®ç°äº†å¯¹MLLMæ¨ç†è¿‡ç¨‹çš„å®æ—¶æ§åˆ¶ã€‚è¯¥æ–¹æ³•å…è®¸ç”¨æˆ·åœ¨è§£ç è¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å¥–åŠ±å‡½æ•°çš„ç›¸å¯¹é‡è¦æ€§ï¼Œå¹¶åœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸­æƒè¡¡ç›®æ ‡ç²¾åº¦å’Œå¬å›ç‡ï¼›åŒæ—¶ï¼Œç”¨æˆ·è¿˜å¯ä»¥æ§åˆ¶æœç´¢èŒƒå›´ï¼Œå¹³è¡¡æµ‹è¯•æ—¶é—´è®¡ç®—å’Œè§†è§‰å®šä½ç¨‹åº¦ä¹‹é—´çš„æƒè¡¡ã€‚åœ¨æ ‡å‡†ç›®æ ‡å¹»è§‰è¯„ä¼°æŒ‡æ ‡ä¸Šï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„æ§åˆ¶èƒ½åŠ›ï¼Œå¹¶ä¸€è‡´ä¼˜äºç°æœ‰å¹»è§‰ç¼“è§£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰éœ€è¦é€‚åº”ä¸åŒç”¨æˆ·éœ€æ±‚ã€‚</li>
<li>å¥–åŠ±å¼•å¯¼è§£ç æ–¹æ³•ç”¨äºæ”¹å–„MLLMsçš„è§†è§‰å®šä½åŠŸèƒ½ã€‚</li>
<li>é€šè¿‡æ„å»ºä¸¤ä¸ªç‹¬ç«‹çš„å¥–åŠ±æ¨¡å‹ï¼Œåˆ†åˆ«æ§åˆ¶æ¨¡å‹è¾“å‡ºä¸­çš„ç›®æ ‡ç²¾åº¦å’Œå¬å›ç‡ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥åŠ¨æ€è°ƒæ•´å¥–åŠ±å‡½æ•°çš„ç›¸å¯¹é‡è¦æ€§ï¼Œåœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸­æƒè¡¡ç›®æ ‡ç²¾åº¦å’Œå¬å›ç‡ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥æ§åˆ¶æœç´¢èŒƒå›´ï¼Œä»¥å¹³è¡¡æµ‹è¯•æ—¶é—´è®¡ç®—å’Œè§†è§‰å®šä½ç¨‹åº¦ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ ‡å‡†ç›®æ ‡å¹»è§‰è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ§åˆ¶èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-30b343fbed8a2e117d71ee5a2bbcdfce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5672e3d2d9f8c796d307f3b8b3d5ff30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bee580f0a4eb1558350f9b016ed75b7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a037ba831b4f188fc4642c4d869e853.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2965b45bd3652e599aaf86edd8648b59.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TinyTim-A-Family-of-Language-Models-for-Divergent-Generation"><a href="#TinyTim-A-Family-of-Language-Models-for-Divergent-Generation" class="headerlink" title="TinyTim: A Family of Language Models for Divergent Generation"></a>TinyTim: A Family of Language Models for Divergent Generation</h2><p><strong>Authors:Christopher J. Agostino</strong></p>
<p>This work introduces TinyTim, a family of large language models fine-tuned on James Joyceâ€™s &#96;Finnegans Wakeâ€™. Through quantitative evaluation against baseline models, we demonstrate that TinyTim V1 produces a statistically distinct generative profile characterized by high lexical diversity and low semantic coherence. These findings are interpreted through theories of creativity and complex problem-solving, arguing that such specialized models can function as divergent knowledge sources within more extensive creative architectures, powering automated discovery mechanisms in diverse settings. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†TinyTimï¼Œè¿™æ˜¯ä¸€ç³»åˆ—åŸºäºè©¹å§†æ–¯Â·ä¹”ä¼Šæ–¯çš„ã€ŠèŠ¬å°¼æ ¹å®ˆçµå¤œã€‹è°ƒè°çš„å¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—ã€‚é€šè¿‡å¯¹åŸºçº¿æ¨¡å‹çš„å®šé‡è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜TinyTim V1äº§ç”Ÿäº†å…·æœ‰é«˜è¯æ±‡å¤šæ ·æ€§å’Œä½è¯­ä¹‰è¿è´¯æ€§çš„ç»Ÿè®¡ç‹¬ç‰¹ç”Ÿæˆç‰¹å¾ã€‚è¿™äº›å‘ç°é€šè¿‡åˆ›é€ åŠ›å’Œå¤æ‚é—®é¢˜è§£å†³çš„ç†è®ºè¿›è¡Œäº†è§£é‡Šï¼Œè®¤ä¸ºè¿™ç§ä¸“ä¸šæ¨¡å‹å¯ä»¥åœ¨æ›´å¹¿æ³›çš„åˆ›é€ æ€§æ¶æ„ä¸­å‘æŒ¥ä¸åŒçš„çŸ¥è¯†æºä½œç”¨ï¼Œä¸ºå„ç§ç¯å¢ƒä¸­çš„è‡ªåŠ¨åŒ–å‘ç°æœºåˆ¶æä¾›åŠ¨åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11607v1">PDF</a> 7 pages, 3 figures, submitted to NeurIPS Creative AI track, code and   model available at <a target="_blank" rel="noopener" href="https://hf.co/npc-worldwide/TinyTimV1">https://hf.co/npc-worldwide/TinyTimV1</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†TinyTimç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åŸºäºè©¹å§†æ–¯Â·ä¹”ä¼Šæ–¯ä½œå“ã€ŠèŠ¬å°¼æ ¹å®ˆå¤œã€‹è¿›è¡Œå¾®è°ƒåçš„TinyTim V1æ¨¡å‹ã€‚é€šè¿‡å®šé‡è¯„ä¼°ï¼Œå‘ç°è¯¥æ¨¡å‹ç”Ÿæˆçš„å†…å®¹å…·æœ‰æ˜¾è‘—çš„é«˜è¯æ±‡å¤šæ ·æ€§å’Œä½è¯­ä¹‰è¿è´¯æ€§ç‰¹å¾ã€‚ç»“åˆåˆ›é€ åŠ›å’Œå¤æ‚é—®é¢˜è§£å†³çš„ç†è®ºï¼Œè®ºè¯è¿™ç§ä¸“ä¸šæ¨¡å‹å¯ä»¥ä½œä¸ºå¹¿æ³›åˆ›é€ æ€§æ¶æ„ä¸­çš„å‘æ•£çŸ¥è¯†æºï¼Œåœ¨å¤šç§åœºæ™¯ä¸­ä¸ºè‡ªåŠ¨å‘ç°æœºåˆ¶æä¾›åŠ¨åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>TinyTimç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹åŸºäºè©¹å§†æ–¯Â·ä¹”ä¼Šæ–¯çš„ã€ŠèŠ¬å°¼æ ¹å®ˆå¤œã€‹è¿›è¡Œå¾®è°ƒã€‚</li>
<li>TinyTim V1æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å…·æœ‰é«˜è¯æ±‡å¤šæ ·æ€§å’Œä½è¯­ä¹‰è¿è´¯æ€§ç‰¹å¾ã€‚</li>
<li>é€šè¿‡å®šé‡è¯„ä¼°éªŒè¯äº†TinyTim V1æ¨¡å‹çš„ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>è¿™ç§æ¨¡å‹å¯ä»¥ä½œä¸ºåˆ›é€ æ€§æ¶æ„ä¸­çš„å‘æ•£çŸ¥è¯†æºã€‚</li>
<li>æ¨¡å‹æœ‰åŠ©äºåœ¨å¤šç§åœºæ™¯ä¸­å®ç°è‡ªåŠ¨åŒ–å‘ç°æœºåˆ¶ã€‚</li>
<li>æ¨¡å‹çš„ç†è®ºåŸºç¡€ç»“åˆäº†åˆ›é€ åŠ›å’Œå¤æ‚é—®é¢˜è§£å†³çš„ç†è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11607">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2fd7e297d5dc64c97ba1a8895e79613e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-299d0d5913c5a260312162a7b1fb9a54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64cb0c045ea08521a1e19a2b8d12f53e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0071c7fb8bdf1c6d1f8aa41dfa1d2267.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ADMIRE-BayesOpt-Accelerated-Data-MIxture-RE-weighting-for-Language-Models-with-Bayesian-Optimization"><a href="#ADMIRE-BayesOpt-Accelerated-Data-MIxture-RE-weighting-for-Language-Models-with-Bayesian-Optimization" class="headerlink" title="ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language   Models with Bayesian Optimization"></a>ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language   Models with Bayesian Optimization</h2><p><strong>Authors:Shengzhuang Chen, Xu Ouyang, Michael Arthur Leopold Pearce, Thomas Hartvigsen, Jonathan Richard Schwarz</strong></p>
<p>Determining the optimal data mixture for large language model training remains a challenging problem with an outsized impact on performance. In practice, language model developers continue to rely on heuristic exploration since no learning-based approach has emerged as a reliable solution. In this work, we propose to view the selection of training data mixtures as a black-box hyperparameter optimization problem, for which Bayesian Optimization is a well-established class of appropriate algorithms. Firstly, we cast data mixture learning as a sequential decision-making problem, in which we aim to find a suitable trade-off between the computational cost of training exploratory (proxy-) models and final mixture performance. Secondly, we systematically explore the properties of transferring mixtures learned at a small scale to larger-scale experiments, providing insights and highlighting opportunities for research at a modest scale. By proposing Multi-fidelity Bayesian Optimization as a suitable method in this common scenario, we introduce a natural framework to balance experiment cost with model fit, avoiding the risks of overfitting to smaller scales while minimizing the number of experiments at high cost. We present results for pre-training and instruction finetuning across models ranging from 1 million to 7 billion parameters, varying from simple architectures to state-of-the-art models and benchmarks spanning dozens of datasets. We demonstrate consistently strong results relative to a wide range of benchmarks, showingspeed-ups of over 500% in determining the best data mixture on our largest experiments relative to recent baselines. In addition, we broaden access to research by sharing ADMIRE IFT Runs, a dataset of 460 full training &amp; evaluation runs across various model sizes worth over 13,000 GPU hours, greatly reducing the cost of conducting research in this area. </p>
<blockquote>
<p>ç¡®å®šå¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æœ€ä¼˜æ•°æ®æ··åˆä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå®ƒå¯¹æ€§èƒ½æœ‰ç€é‡å¤§å½±å“ã€‚åœ¨å®è·µä¸­ï¼Œè¯­è¨€æ¨¡å‹å¼€å‘è€…ç»§ç»­ä¾èµ–å¯å‘å¼æ¢ç´¢ï¼Œå› ä¸ºæ²¡æœ‰å‡ºç°å¯é çš„åŸºäºå­¦ä¹ çš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºè®®å°†è®­ç»ƒæ•°æ®æ··åˆçš„é€‰æ‹©è§†ä¸ºä¸€ä¸ªé»‘ç›’è¶…å‚æ•°ä¼˜åŒ–é—®é¢˜ï¼Œä¸ºæ­¤ï¼Œè´å¶æ–¯ä¼˜åŒ–å·²ç»å»ºç«‹äº†ä¸€ä¸ªé€‚å½“çš„ç®—æ³•ç±»ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ•°æ®æ··åˆå­¦ä¹ è½¬åŒ–ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–é—®é¢˜ï¼Œæ—¨åœ¨æ‰¾åˆ°è®­ç»ƒæ¢ç´¢æ€§ï¼ˆä»£ç†ï¼‰æ¨¡å‹çš„è®¡ç®—æˆæœ¬ä¸æœ€ç»ˆæ··åˆæ€§èƒ½ä¹‹é—´çš„é€‚å½“æƒè¡¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢è®¨äº†å°†å°è§„æ¨¡å­¦ä¹ çš„æ··åˆç‰©è½¬ç§»åˆ°å¤§è§„æ¨¡å®éªŒçš„ç‰¹æ€§ï¼Œæä¾›äº†è§è§£å¹¶çªå‡ºäº†å°è§„æ¨¡ç ”ç©¶çš„æœºä¼šã€‚é€šè¿‡æå‡ºå¤šä¿çœŸè´å¶æ–¯ä¼˜åŒ–ä½œä¸ºè¿™ä¸€å¸¸è§åœºæ™¯çš„åˆé€‚æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªç„¶çš„æ¡†æ¶æ¥å¹³è¡¡å®éªŒæˆæœ¬ä¸æ¨¡å‹æ‹Ÿåˆï¼Œé¿å…äº†åœ¨å°è§„æ¨¡ä¸Šè¿‡æ‹Ÿåˆçš„é£é™©ï¼ŒåŒæ—¶æœ€å°åŒ–äº†é«˜æˆæœ¬å®éªŒçš„æ•°é‡ã€‚æˆ‘ä»¬å±•ç¤ºäº†åœ¨1ç™¾ä¸‡åˆ°7äº¿å‚æ•°èŒƒå›´å†…çš„æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒçš„ç»“æœï¼Œä»ç®€å•æ¶æ„åˆ°å…ˆè¿›æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ï¼Œè·¨è¶Šæ•°åä¸ªæ•°æ®é›†ã€‚ä¸å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œæˆ‘ä»¬å§‹ç»ˆè¡¨ç°å‡ºå¼ºå¤§çš„ç»“æœï¼Œåœ¨æˆ‘ä»¬æœ€å¤§çš„å®éªŒä¸­ç¡®å®šæœ€ä½³æ•°æ®æ··åˆçš„åŠ é€Ÿè¶…è¿‡500%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å…±äº«ADMIRE IFTRunsï¼ˆä¸€ä¸ªåŒ…å«460æ¬¡å„ç§æ¨¡å‹å¤§å°çš„å®Œæ•´è®­ç»ƒä¸è¯„ä¼°è¿è¡Œçš„æ•°æ®é›†ï¼Œä»·å€¼è¶…è¿‡13000ä¸ªGPUå°æ—¶ï¼‰ï¼Œæ¥æ‰©å¤§ç ”ç©¶äººå‘˜çš„è®¿é—®èŒƒå›´ï¼Œå¤§å¤§é™ä½äº†è¯¥é¢†åŸŸçš„ç ”ç©¶æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11551v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¤§æ•°æ®æ··åˆè®­ç»ƒå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å½±å“è‡³å…³é‡è¦ï¼Œä½†ç¡®å®šæœ€ä½³æ•°æ®æ··åˆä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚ç›®å‰è¯­è¨€æ¨¡å‹å¼€å‘è€…ä¸»è¦ä¾èµ–å¯å‘å¼æ¢ç´¢ï¼Œå°šæ— å¯é çš„å­¦ä¹ å‹è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºå°†è®­ç»ƒæ•°æ®æ··åˆçš„é€‰æ‹©è§†ä¸ºé»‘ç›’è¶…å‚æ•°ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å¼•å…¥è´å¶æ–¯ä¼˜åŒ–ä½œä¸ºé€‚ç”¨çš„ç®—æ³•ã€‚é¦–å…ˆï¼Œæœ¬æ–‡å°†æ•°æ®æ··åˆå­¦ä¹ è§†ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–é—®é¢˜ï¼Œæ—¨åœ¨æ‰¾åˆ°è®­ç»ƒæ¢ç´¢æ€§ä»£ç†æ¨¡å‹çš„è®¡ç®—æˆæœ¬ä¸æœ€ç»ˆæ··åˆæ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚å…¶æ¬¡ï¼Œæœ¬æ–‡ç³»ç»Ÿåœ°æ¢è®¨äº†ä»å°è§„æ¨¡å®éªŒå­¦ä¹ åˆ°çš„æ··åˆåœ¨å¤§è§„æ¨¡å®éªŒä¸­çš„è¿ç§»ç‰¹æ€§ï¼Œæä¾›äº†ä¸€ä¸ªè‡ªç„¶æ¡†æ¶æ¥å¹³è¡¡å®éªŒæˆæœ¬ä¸æ¨¡å‹æ‹Ÿåˆåº¦ã€‚é€šè¿‡æå‡ºå¤šä¿çœŸè´å¶æ–¯ä¼˜åŒ–ä½œä¸ºè¿™ä¸€å¸¸è§åœºæ™¯çš„åˆé€‚æ–¹æ³•ï¼Œæœ¬æ–‡é¿å…äº†åœ¨å°è§„æ¨¡ä¸Šè¿‡æ‹Ÿåˆçš„é£é™©ï¼ŒåŒæ—¶å‡å°‘äº†é«˜æˆæœ¬å®éªŒçš„æ•°é‡ã€‚æœ¬æ–‡å±•ç¤ºäº†åœ¨æ•°ç™¾ä¸‡å‚æ•°è‡³æ•°åäº¿å‚æ•°æ¨¡å‹ä¸Šçš„é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒç»“æœï¼Œæ¶‰åŠç®€å•æ¶æ„å’Œæœ€æ–°æ¨¡å‹ä»¥åŠè·¨è¶Šæ•°åä¸ªæ•°æ®é›†çš„åŸºå‡†æµ‹è¯•ã€‚ä¸è¿‘æœŸåŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œåœ¨ç¡®å®šæœ€ä½³æ•°æ®æ··åˆæ–¹é¢ï¼Œæˆ‘ä»¬çš„æœ€å¤§å®éªŒé€Ÿåº¦æé«˜äº†è¶…è¿‡500%ã€‚æ­¤å¤–ï¼Œé€šè¿‡å…±äº«ADMIRE IFTRunsæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡13,000 GPUå°æ—¶çš„460æ¬¡å®Œæ•´è®­ç»ƒä¸è¯„ä¼°è¿è¡Œï¼Œæœ¬æ–‡é™ä½äº†è¯¥é¢†åŸŸçš„ç ”ç©¶æˆæœ¬ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¡®å®šæœ€ä½³æ•°æ®æ··åˆå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒè‡³å…³é‡è¦ï¼Œä½†ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºå°†è®­ç»ƒæ•°æ®æ··åˆçš„é€‰æ‹©è§†ä¸ºé»‘ç›’è¶…å‚æ•°ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶é‡‡ç”¨è´å¶æ–¯ä¼˜åŒ–æ¥è§£å†³ã€‚</li>
<li>ä»‹ç»äº†æ•°æ®æ··åˆå­¦ä¹ ä½œä¸ºåºåˆ—å†³ç­–é—®é¢˜çš„è§‚ç‚¹ï¼Œå¹³è¡¡äº†æ¢ç´¢æ€§æ¨¡å‹è®­ç»ƒçš„è®¡ç®—æˆæœ¬ä¸æœ€ç»ˆæ€§èƒ½ã€‚</li>
<li>æ¢è®¨äº†ä»å°è§„æ¨¡å®éªŒåˆ°å¤§è§„æ¨¡å®éªŒçš„è¿ç§»ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡å¤šä¿çœŸè´å¶æ–¯ä¼˜åŒ–ï¼Œé¿å…äº†åœ¨å°è§„æ¨¡ä¸Šè¿‡æ‹Ÿåˆçš„é£é™©ï¼Œå‡å°‘äº†é«˜æˆæœ¬å®éªŒã€‚</li>
<li>åœ¨å¤šç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½è¡¨ç°ï¼Œä¸è¿‘æœŸåŸºå‡†ç›¸æ¯”æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1758d5adbab8ff37d057f6895570f3ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-244583dc4ba4d4d07eeb3478589b4635.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbced2ecbe19f13822373ab244fb3c81.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="OVSegDT-Segmenting-Transformer-for-Open-Vocabulary-Object-Goal-Navigation"><a href="#OVSegDT-Segmenting-Transformer-for-Open-Vocabulary-Object-Goal-Navigation" class="headerlink" title="OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal   Navigation"></a>OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal   Navigation</h2><p><strong>Authors:Tatiana Zemskova, Aleksei Staroverov, Dmitry Yudin, Aleksandr Panov</strong></p>
<p>Open-vocabulary Object Goal Navigation requires an embodied agent to reach objects described by free-form language, including categories never seen during training. Existing end-to-end policies overfit small simulator datasets, achieving high success on training scenes but failing to generalize and exhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a lightweight transformer policy that tackles these issues with two synergistic components. The first component is the semantic branch, which includes an encoder for the target binary mask and an auxiliary segmentation loss function, grounding the textual goal and providing precise spatial cues. The second component consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample scheduler that continuously balances imitation and reinforcement signals according to the policy entropy, eliminating brittle manual phase switches. These additions cut the sample complexity of training by 33%, and reduce collision count in two times while keeping inference cost low (130M parameters, RGB-only input). On HM3D-OVON, our model matches the performance on unseen categories to that on seen ones and establishes state-of-the-art results (40.1% SR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CognitiveAISystems/OVSegDT">https://github.com/CognitiveAISystems/OVSegDT</a>. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡å¯¹è±¡ç›®æ ‡å¯¼èˆªéœ€è¦å®ä½“ä»£ç†æ¥å®ç°é€šè¿‡è‡ªç”±å½¢å¼è¯­è¨€æè¿°çš„å¯¹è±¡ï¼ŒåŒ…æ‹¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»æœªè§è¿‡çš„ç±»åˆ«ã€‚ç°æœ‰çš„ç«¯åˆ°ç«¯ç­–ç•¥è¿‡äºé€‚åº”å°å‹æ¨¡æ‹Ÿå™¨æ•°æ®é›†ï¼Œåœ¨è®­ç»ƒåœºæ™¯ä¸Šå–å¾—é«˜æˆåŠŸç‡ï¼Œä½†æ— æ³•æ¨å¹¿ï¼Œå¹¶è¡¨ç°å‡ºä¸å®‰å…¨è¡Œä¸ºï¼ˆé¢‘ç¹ç¢°æ’ï¼‰ã€‚æˆ‘ä»¬å¼•å…¥äº†OVSegDTï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å˜å‹å™¨ç­–ç•¥ï¼Œé€šè¿‡ä¸¤ä¸ªååŒç»„ä»¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç¬¬ä¸€ä¸ªç»„ä»¶æ˜¯è¯­ä¹‰åˆ†æ”¯ï¼Œå®ƒåŒ…æ‹¬ç›®æ ‡äºŒè¿›åˆ¶æ©ç çš„ç¼–ç å™¨å’Œè¾…åŠ©åˆ†å‰²æŸå¤±å‡½æ•°ï¼Œå®ƒæ ¹æ®æ–‡æœ¬ç›®æ ‡æä¾›ç²¾ç¡®çš„ç©ºé—´çº¿ç´¢ã€‚ç¬¬äºŒä¸ªç»„ä»¶æ˜¯æå‡ºçš„ç†µè‡ªé€‚åº”æŸå¤±è°ƒåˆ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªæŒ‰æ ·æœ¬è°ƒåº¦çš„ç¨‹åºï¼Œæ ¹æ®ç­–ç•¥ç†µè¿ç»­å¹³è¡¡æ¨¡ä»¿å’Œå¼ºåŒ–ä¿¡å·ï¼Œæ¶ˆé™¤äº†è„†å¼±çš„æ‰‹åŠ¨ç›¸ä½å¼€å…³ã€‚è¿™äº›æ·»åŠ å°†è®­ç»ƒæ ·æœ¬çš„å¤æ‚æ€§é™ä½äº†33%ï¼Œåœ¨ä¿æŒæ¨ç†æˆæœ¬ä½çš„åŒæ—¶ï¼ˆä»…æœ‰1300ä¸‡ä¸ªå‚æ•°ï¼Œä»…ä½¿ç”¨RGBè¾“å…¥ï¼‰å°†ç¢°æ’è®¡æ•°å‡å°‘äº†ä¸€åŠã€‚åœ¨HM3D-OVONä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æœªè§è¿‡å’Œè§è¿‡çš„ç±»åˆ«ä¸Šçš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œå¹¶åœ¨æ²¡æœ‰æ·±åº¦ã€é‡Œç¨‹è®¡æˆ–å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹å»ºç«‹äº†æœ€æ–°çš„ç»“æœï¼ˆSRä¸º40.1%ï¼Œvalæœªè§SPLä¸º20.9%ï¼‰ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CognitiveAISystems/OVSegDT">https://github.com/CognitiveAISystems/OVSegDT</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11479v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOVSegDTçš„è½»é‡çº§è½¬æ¢å™¨ç­–ç•¥ï¼Œç”¨äºè§£å†³å¼€æ”¾è¯æ±‡å¯¹è±¡ç›®æ ‡å¯¼èˆªé—®é¢˜ã€‚è¯¥ç­–ç•¥åŒ…æ‹¬ä¸¤ä¸ªååŒç»„ä»¶ï¼šè¯­ä¹‰åˆ†æ”¯å’Œç†µè‡ªé€‚åº”æŸå¤±è°ƒåˆ¶ã€‚è¯­ä¹‰åˆ†æ”¯åŒ…æ‹¬ç›®æ ‡äºŒè¿›åˆ¶æ©ç çš„ç¼–ç å™¨å’Œè¾…åŠ©åˆ†å‰²æŸå¤±å‡½æ•°ï¼Œç”¨äºå°†æ–‡æœ¬ç›®æ ‡ä¸ç©ºé—´çº¿ç´¢ç›¸ç»“åˆã€‚ç†µè‡ªé€‚åº”æŸå¤±è°ƒåˆ¶åˆ™æ˜¯ä¸€ç§æ¯æ ·æœ¬è°ƒåº¦ç¨‹åºï¼Œå¯æ ¹æ®ç­–ç•¥ç†µè¿ç»­å¹³è¡¡æ¨¡ä»¿å’Œå¼ºåŒ–ä¿¡å·ï¼Œæ¶ˆé™¤äº†è„†å¼±çš„æ‰‹åŠ¨ç›¸ä½åˆ‡æ¢ã€‚è¿™äº›æ”¹è¿›é™ä½äº†è®­ç»ƒæ ·æœ¬çš„å¤æ‚æ€§ï¼Œå‡å°‘äº†ç¢°æ’æ¬¡æ•°ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒä½æ¨ç†æˆæœ¬ã€‚OVSegDTåœ¨HM3D-OVONæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°ç»“æœï¼Œå¯¹æœªè§ç±»åˆ«å’Œå·²è§ç±»åˆ«çš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œä¸”æ— éœ€æ·±åº¦ã€é‡Œç¨‹è®¡æˆ–å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OVSegDTæ˜¯ä¸€ç§é’ˆå¯¹å¼€æ”¾è¯æ±‡å¯¹è±¡ç›®æ ‡å¯¼èˆªé—®é¢˜çš„è½»é‡çº§è½¬æ¢å™¨ç­–ç•¥ã€‚</li>
<li>ç­–ç•¥åŒ…å«ä¸¤ä¸ªååŒç»„ä»¶ï¼šè¯­ä¹‰åˆ†æ”¯å’Œç†µè‡ªé€‚åº”æŸå¤±è°ƒåˆ¶ã€‚</li>
<li>è¯­ä¹‰åˆ†æ”¯ç»“åˆæ–‡æœ¬ç›®æ ‡å’Œç©ºé—´çº¿ç´¢ï¼Œé€šè¿‡ç¼–ç ç›®æ ‡äºŒè¿›åˆ¶æ©ç å’Œè¾…åŠ©åˆ†å‰²æŸå¤±å‡½æ•°å®ç°ã€‚</li>
<li>ç†µè‡ªé€‚åº”æŸå¤±è°ƒåˆ¶æ˜¯ä¸€ç§æ¯æ ·æœ¬è°ƒåº¦ç¨‹åºï¼Œèƒ½è‡ªåŠ¨å¹³è¡¡æ¨¡ä»¿å’Œå¼ºåŒ–ä¿¡å·ï¼Œæ¶ˆé™¤æ‰‹åŠ¨ç›¸ä½åˆ‡æ¢çš„éœ€è¦ã€‚</li>
<li>OVSegDTé™ä½äº†è®­ç»ƒæ ·æœ¬å¤æ‚æ€§ï¼Œå‡å°‘ç¢°æ’æ¬¡æ•°ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„æ¨ç†æˆæœ¬ã€‚</li>
<li>åœ¨HM3D-OVONæ•°æ®é›†ä¸Šï¼ŒOVSegDTå®ç°äº†æœ€æ–°ç»“æœï¼Œæœªè§ç±»åˆ«ä¸å·²è§ç±»åˆ«çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4cf4f162709c466072a74b336781b55c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-068e49275a68fa461ced6da69d56fc55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8342c1f173d69e7d4d144790f37ff58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0908d75fcaccd47f61f1149ac3042eaf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Reference-Points-in-LLM-Sentiment-Analysis-The-Role-of-Structured-Context"><a href="#Reference-Points-in-LLM-Sentiment-Analysis-The-Role-of-Structured-Context" class="headerlink" title="Reference Points in LLM Sentiment Analysis: The Role of Structured   Context"></a>Reference Points in LLM Sentiment Analysis: The Role of Structured   Context</h2><p><strong>Authors:Junichiro Niimi</strong></p>
<p>Large language models (LLMs) are now widely used across many fields, including marketing research. Sentiment analysis, in particular, helps firms understand consumer preferences. While most NLP studies classify sentiment from review text alone, marketing theories, such as prospect theory and expectationâ€“disconfirmation theory, point out that customer evaluations are shaped not only by the actual experience but also by additional reference points. This study therefore investigates how the content and format of such supplementary information affect sentiment analysis using LLMs. We compare natural language (NL) and JSON-formatted prompts using a lightweight 3B parameter model suitable for practical marketing applications. Experiments on two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with additional information outperforms all baselines without fine-tuning: Macro-F1 rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it deployable in resource-constrained edge devices. Furthermore, a follow-up analysis confirms that performance gains stem from genuine contextual reasoning rather than label proxying. This work demonstrates that structured prompting can enable smaller models to achieve competitive performance, offering a practical alternative to large-scale model deployment. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç°å·²å¹¿æ³›åº”ç”¨äºè®¸å¤šé¢†åŸŸï¼ŒåŒ…æ‹¬å¸‚åœºç ”ç©¶ã€‚å°¤å…¶æ˜¯æƒ…æ„Ÿåˆ†æï¼Œå®ƒæœ‰åŠ©äºä¼ä¸šäº†è§£æ¶ˆè´¹è€…åå¥½ã€‚è™½ç„¶å¤§å¤šæ•°è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶ä»…ä»è¯„è®ºæ–‡æœ¬ä¸­åˆ†ç±»æƒ…æ„Ÿï¼Œä½†å¸‚åœºè¥é”€ç†è®ºï¼Œå¦‚å‰æ™¯ç†è®ºå’ŒæœŸæœ›-ç¡®è®¤ç†è®ºï¼ŒæŒ‡å‡ºå®¢æˆ·è¯„ä»·ä¸ä»…å—å®é™…ä½“éªŒçš„å½±å“ï¼Œè¿˜å—å…¶ä»–å‚è€ƒç‚¹çš„å½±å“ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶è°ƒæŸ¥äº†è¿™ç§è¡¥å……ä¿¡æ¯çš„å†…å®¹å’Œæ ¼å¼å¦‚ä½•å½±å“ä½¿ç”¨LLMçš„æƒ…æ„Ÿåˆ†æã€‚æˆ‘ä»¬ä½¿ç”¨é€‚åˆå®é™…å¸‚åœºè¥é”€åº”ç”¨çš„å°å‹3Bå‚æ•°æ¨¡å‹ï¼Œæ¯”è¾ƒäº†è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰å’ŒJSONæ ¼å¼çš„æç¤ºã€‚å¯¹Yelpä¸¤ä¸ªç±»åˆ«ï¼ˆé¤å…å’Œå¤œç”Ÿæ´»ï¼‰çš„å®éªŒè¡¨æ˜ï¼Œå¸¦æœ‰é™„åŠ ä¿¡æ¯çš„JSONæç¤ºä¼˜äºæ‰€æœ‰åŸºçº¿ï¼Œæ— éœ€å¾®è°ƒï¼šå®è§‚F1åˆ†æ•°æé«˜äº†1.6%å’Œ4%ï¼ŒåŒæ—¶RMSEåˆ†åˆ«ä¸‹é™äº†16%å’Œ9.1%ï¼Œä½¿å…¶é€‚ç”¨äºèµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ã€‚æ­¤å¤–ï¼Œåç»­åˆ†æè¯å®ï¼Œæ€§èƒ½æå‡æºäºçœŸæ­£çš„ä¸Šä¸‹æ–‡æ¨ç†ï¼Œè€Œéæ ‡ç­¾ä»£ç†ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼Œç»“æ„åŒ–æç¤ºå¯ä»¥ä½¿è¾ƒå°çš„æ¨¡å‹å®ç°æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ºå¤§è§„æ¨¡æ¨¡å‹éƒ¨ç½²æä¾›äº†å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11454v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¥é”€ç ”ç©¶ç­‰é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œæƒ…æ„Ÿåˆ†æå¸®åŠ©ä¼ä¸šç†è§£æ¶ˆè´¹è€…åå¥½ã€‚æœ¬ç ”ç©¶ç»“åˆè¥é”€ç†è®ºï¼Œå‘ç°å®¢æˆ·è¯„ä»·ä¸ä»…å—å®é™…ä½“éªŒå½±å“ï¼Œè¿˜å—å…¶ä»–å‚è€ƒç‚¹çš„å½±å“ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨LLMè¿›è¡Œæƒ…æ„Ÿåˆ†ææ—¶ï¼Œé™„åŠ ä¿¡æ¯çš„å†…å®¹å’Œæ ¼å¼å¦‚ä½•å½±å“ç»“æœã€‚é€šè¿‡å¯¹æ¯”è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰å’ŒJSONæ ¼å¼çš„æç¤ºï¼Œå®éªŒç»“æœæ˜¾ç¤ºJSONæ ¼å¼çš„æç¤ºè¡¨ç°æ›´ä½³ï¼Œæ— éœ€å¾®è°ƒå³å¯è¶…è¶Šæ‰€æœ‰åŸºçº¿ã€‚åœ¨Yelpçš„ä¸¤ä¸ªç±»åˆ«ï¼ˆé¤é¥®å’Œå¤œç”Ÿæ´»ï¼‰çš„å®éªŒä¸­ï¼ŒMacro-F1åˆ†åˆ«æé«˜äº†1.6%å’Œ4%ï¼ŒRMSEåˆ†åˆ«ä¸‹é™äº†16%å’Œ9.1%ã€‚æ­¤å¤–ï¼Œåç»­åˆ†æè¯å®æ€§èƒ½æå‡æºäºçœŸæ­£çš„ä¸Šä¸‹æ–‡æ¨ç†è€Œéæ ‡ç­¾ä»£ç†ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œç»“æ„åŒ–æç¤ºå¯ä½¿è¾ƒå°çš„æ¨¡å‹å®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ºå¤§è§„æ¨¡æ¨¡å‹éƒ¨ç½²æä¾›äº†å®ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æƒ…æ„Ÿåˆ†æé¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œå°¤å…¶æ˜¯è¥é”€ç ”ç©¶é¢†åŸŸã€‚</li>
<li>å®¢æˆ·è¯„ä»·ä¸ä»…å—å®é™…ä½“éªŒå½±å“ï¼Œè¿˜å—å…¶ä»–å‚è€ƒç‚¹çš„å½±å“ï¼Œè¿™è¢«è¥é”€ç†è®ºæ‰€å¼ºè°ƒã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰å’ŒJSONæ ¼å¼çš„æç¤ºåœ¨æƒ…æ„Ÿåˆ†æä¸­çš„æ•ˆæœã€‚</li>
<li>JSONæ ¼å¼çš„æç¤ºè¡¨ç°ä¼˜äºNLæ ¼å¼ï¼Œæ— éœ€å¾®è°ƒå³å¯è¶…è¶Šæ‰€æœ‰åŸºçº¿ã€‚</li>
<li>åœ¨Yelpçš„ä¸¤ä¸ªç±»åˆ«å®éªŒä¸­ï¼ŒJSONæ ¼å¼æç¤ºæ˜¾è‘—æé«˜äº†Macro-F1å¹¶é™ä½äº†RMSEã€‚</li>
<li>åç»­åˆ†æè¯å®æ€§èƒ½æå‡æºäºçœŸæ­£çš„ä¸Šä¸‹æ–‡æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11454">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-421cffb9e41f60216129c227e90903cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25809cfb725d16b5cfa9e604d69bd9ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac887adab2891994522778d3a090cb74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26cb470d204134e86bd8febdc77a7f8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aca12021b3c50d6cae4f104284e43bf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20d03c8f6f054c14812e72e933afa8e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f4c280d81d339c1b77cc0ae378ab555.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Inclusion-Arena-An-Open-Platform-for-Evaluating-Large-Foundation-Models-with-Real-World-Apps"><a href="#Inclusion-Arena-An-Open-Platform-for-Evaluating-Large-Foundation-Models-with-Real-World-Apps" class="headerlink" title="Inclusion Arena: An Open Platform for Evaluating Large Foundation Models   with Real-World Apps"></a>Inclusion Arena: An Open Platform for Evaluating Large Foundation Models   with Real-World Apps</h2><p><strong>Authors:Kangyu Wang, Hongliang He, Lin Liu, Ruiqi Liang, Zhenzhong Lan, Jianguo Li</strong></p>
<p>Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have ushered in a new era of AI capabilities, demonstrating near-human-level performance across diverse scenarios. While numerous benchmarks (e.g., MMLU) and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the development of LLMs and MLLMs, most rely on static datasets or crowdsourced general-domain prompts, often falling short of reflecting performance in real-world applications. To bridge this critical gap, we present Inclusion Arena, a live leaderboard that ranks models based on human feedback collected directly from AI-powered applications. Our platform integrates pairwise model comparisons into natural user interactions, ensuring evaluations reflect practical usage scenarios. For robust model ranking, we employ the Bradley-Terry model augmented with two key innovations: (1) Placement Matches, a cold-start mechanism to quickly estimate initial ratings for newly integrated models, and (2) Proximity Sampling, an intelligent comparison strategy that prioritizes battles between models of similar capabilities to maximize information gain and enhance rating stability. Extensive empirical analyses and simulations demonstrate that Inclusion Arena yields reliable and stable rankings, exhibits higher data transitivity compared to general crowdsourced datasets, and significantly mitigates the risk of malicious manipulation. By fostering an open alliance between foundation models and real-world applications, Inclusion Arena aims to accelerate the development of LLMs and MLLMs truly optimized for practical, user-centric deployments. The platform is publicly accessible at <a target="_blank" rel="noopener" href="https://doraemon.alipay.com/model-ranking">https://doraemon.alipay.com/model-ranking</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å…´èµ·å¼€å¯äº†äººå·¥æ™ºèƒ½èƒ½åŠ›çš„æ–°æ—¶ä»£ï¼Œåœ¨å„ç§åœºæ™¯ä¸­è¡¨ç°å‡ºäº†æ¥è¿‘äººç±»çš„æ€§èƒ½ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šåŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚MMLUï¼‰å’Œæ’è¡Œæ¦œï¼ˆä¾‹å¦‚Chatbot Arenaï¼‰æ¥å¸®åŠ©æ¨åŠ¨LLMå’ŒMLLMçš„å‘å±•ï¼Œä½†å¤§å¤šæ•°éƒ½ä¾èµ–äºé™æ€æ•°æ®é›†æˆ–ä¼—åŒ…é€šç”¨é¢†åŸŸæç¤ºï¼Œå¾€å¾€æ— æ³•åæ˜ å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ€§èƒ½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Inclusion Arenaï¼Œè¿™æ˜¯ä¸€ä¸ªå®æ—¶æ’è¡Œæ¦œï¼Œæ ¹æ®ä»AIåº”ç”¨ç¨‹åºä¸­ç›´æ¥æ”¶é›†çš„äººç±»åé¦ˆå¯¹æ¨¡å‹è¿›è¡Œæ’åã€‚æˆ‘ä»¬çš„å¹³å°å°†æˆå¯¹æ¨¡å‹æ¯”è¾ƒé›†æˆåˆ°è‡ªç„¶ç”¨æˆ·äº¤äº’ä¸­ï¼Œç¡®ä¿è¯„ä¼°åæ˜ å®é™…ä½¿ç”¨åœºæ™¯ã€‚ä¸ºäº†å¯¹æ¨¡å‹è¿›è¡Œç¨³å¥æ’åï¼Œæˆ‘ä»¬é‡‡ç”¨Bradley-Terryæ¨¡å‹ï¼Œå¹¶è¾…ä»¥ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯ä½ç½®åŒ¹é…ï¼Œè¿™æ˜¯ä¸€ç§å†·å¯åŠ¨æœºåˆ¶ï¼Œå¯ä»¥å¿«é€Ÿä¼°è®¡æ–°é›†æˆæ¨¡å‹çš„åˆå§‹è¯„åˆ†ï¼›äºŒæ˜¯é‚»è¿‘é‡‡æ ·ï¼Œè¿™æ˜¯ä¸€ç§æ™ºèƒ½æ¯”è¾ƒç­–ç•¥ï¼Œä¼˜å…ˆå®‰æ’èƒ½åŠ›ç›¸ä¼¼çš„æ¨¡å‹ä¹‹é—´çš„æ¯”èµ›ï¼Œä»¥æœ€å¤§åŒ–ä¿¡æ¯è·å–å¹¶å¢å¼ºè¯„åˆ†ç¨³å®šæ€§ã€‚å¤§é‡çš„å®è¯åˆ†æå’Œæ¨¡æ‹Ÿè¡¨æ˜ï¼ŒInclusion Arenaäº§ç”Ÿçš„æ’åå¯é ä¸”ç¨³å®šï¼Œä¸ä¸€èˆ¬çš„ä¼—åŒ…æ•°æ®é›†ç›¸æ¯”ï¼Œæ˜¾ç¤ºå‡ºæ›´é«˜çš„æ•°æ®ä¼ é€’æ€§ï¼Œå¹¶èƒ½æ˜¾è‘—å‡å°‘æ¶æ„æ“ä½œçš„é£é™©ã€‚é€šè¿‡ä¿ƒè¿›åŸºç¡€æ¨¡å‹ä¸å®é™…åº”ç”¨ä¹‹é—´çš„å¼€æ”¾è”ç›Ÿï¼ŒInclusion Arenaæ—¨åœ¨åŠ é€Ÿé’ˆå¯¹å®é™…ã€ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„éƒ¨ç½²çœŸæ­£ä¼˜åŒ–çš„LLMå’ŒMLLMçš„å‘å±•ã€‚è¯¥å¹³å°å¯åœ¨<a target="_blank" rel="noopener" href="https://doraemon.alipay.com/model-ranking%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://doraemon.alipay.com/model-rankingå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11452v1">PDF</a> Our platform is publicly accessible at   <a target="_blank" rel="noopener" href="https://doraemon.alipay.com/model-ranking">https://doraemon.alipay.com/model-ranking</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¼€å¯äº†äººå·¥æ™ºèƒ½èƒ½åŠ›çš„æ–°æ—¶ä»£ï¼Œåœ¨å„ç§åœºæ™¯ä¸‹å±•ç°å‡ºæ¥è¿‘äººç±»æ°´å¹³çš„æ€§èƒ½ã€‚å°½ç®¡æœ‰è®¸å¤šåŸºå‡†æµ‹è¯•ï¼ˆå¦‚MMLUï¼‰å’Œæ’è¡Œæ¦œï¼ˆå¦‚Chatbot Arenaï¼‰ï¼Œå®ƒä»¬å¤§å¤šä¾èµ–äºé™æ€æ•°æ®é›†æˆ–ä¼—åŒ…é€šç”¨é¢†åŸŸæç¤ºï¼Œå¾€å¾€æ— æ³•åæ˜ å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Inclusion Arenaï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºäººç±»åé¦ˆçš„å®æ—¶æ’è¡Œæ¦œï¼Œç›´æ¥æ”¶é›†æ¥è‡ªäººå·¥æ™ºèƒ½åº”ç”¨ç¨‹åºçš„åé¦ˆæ¥å¯¹æ¨¡å‹è¿›è¡Œæ’åã€‚æˆ‘ä»¬çš„å¹³å°å°†æˆå¯¹æ¨¡å‹æ¯”è¾ƒé›†æˆåˆ°è‡ªç„¶ç”¨æˆ·äº¤äº’ä¸­ï¼Œç¡®ä¿è¯„ä¼°åæ˜ å®é™…ä½¿ç”¨åœºæ™¯ã€‚ä¸ºäº†ç¨³å¥çš„æ¨¡å‹æ’åï¼Œæˆ‘ä»¬é‡‡ç”¨Bradley-Terryæ¨¡å‹ï¼Œå¹¶è¾…ä»¥ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šä¸€æ˜¯ä½ç½®åŒ¹é…ï¼Œè¿™æ˜¯ä¸€ç§å†·å¯åŠ¨æœºåˆ¶ï¼Œå¯ä»¥å¿«é€Ÿä¼°è®¡æ–°é›†æˆæ¨¡å‹çš„åˆå§‹è¯„åˆ†ï¼›äºŒæ˜¯é‚»è¿‘é‡‡æ ·ï¼Œè¿™æ˜¯ä¸€ç§æ™ºèƒ½å¯¹æ¯”ç­–ç•¥ï¼Œä¼˜å…ˆæ¯”è¾ƒèƒ½åŠ›ç›¸è¿‘çš„æ¨¡å‹ï¼Œä»¥æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šå¹¶å¢å¼ºè¯„åˆ†ç¨³å®šæ€§ã€‚ç»è¿‡å¹¿æ³›çš„å®è¯åˆ†æå’Œæ¨¡æ‹Ÿï¼Œè¯æ˜Inclusion Arenaçš„æ’åå¯é ç¨³å®šï¼Œä¸é«˜å±‚æ¬¡çš„ä¼—åŒ…æ•°æ®é›†ç›¸æ¯”å…·æœ‰æ›´é«˜çš„æ•°æ®å¯è½¬æ¢æ€§ï¼Œå¹¶èƒ½æœ‰æ•ˆå‡å°‘æ¶æ„æ“ä½œçš„é£é™©ã€‚Inclusion Arenaæ—¨åœ¨ä¿ƒè¿›åŸºç¡€æ¨¡å‹ä¸å®é™…åº”ç”¨ä¹‹é—´çš„å¼€æ”¾è”ç›Ÿï¼Œä»¥åŠ é€Ÿé’ˆå¯¹å®ç”¨ã€ç”¨æˆ·ä¸ºä¸­å¿ƒçš„éƒ¨ç½²çœŸæ­£ä¼˜åŒ–çš„LLMå’ŒMLLMçš„å‘å±•ã€‚å¹³å°å¯å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://doraemon.alipay.com/model-ranking%E3%80%82">https://doraemon.alipay.com/model-rankingã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMå’ŒMLLMçš„å‘å±•å¼€å¯äº†AIèƒ½åŠ›çš„æ–°æ—¶ä»£ï¼Œå±•ç°äº†åœ¨å„ç§åœºæ™¯ä¸‹çš„è¿‘äººç±»è¡¨ç°ã€‚</li>
<li>å½“å‰åŸºå‡†æµ‹è¯•å’Œæ’è¡Œæ¦œå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå³å®ƒä»¬å¾€å¾€æ— æ³•åæ˜ æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½ã€‚</li>
<li>Inclusion Arenaå¹³å°é€šè¿‡åŸºäºäººç±»åé¦ˆçš„å®æ—¶æ’åæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç›´æ¥æ”¶é›†æ¥è‡ªAIåº”ç”¨ç¨‹åºçš„åé¦ˆã€‚<br>4.è¯¥å¹³å°é›†æˆäº†æˆå¯¹æ¨¡å‹æ¯”è¾ƒåˆ°è‡ªç„¶ç”¨æˆ·äº¤äº’ä¸­ï¼Œç¡®ä¿è¯„ä¼°åæ˜ å®é™…ä½¿ç”¨åœºæ™¯ã€‚</li>
<li>é‡‡ç”¨Bradley-Terryæ¨¡å‹å’Œä¸¤é¡¹å…³é”®åˆ›æ–°ï¼ˆä½ç½®åŒ¹é…å’Œé‚»è¿‘é‡‡æ ·ï¼‰æ¥ç¨³å¥åœ°æ’åæ¨¡å‹ã€‚</li>
<li>å®è¯åˆ†æè¯æ˜äº†Inclusion Arenaçš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ï¼ŒåŒ…æ‹¬å…¶æ•°æ®å¯è½¬æ¢æ€§å’Œå¯¹æ¶æ„æ“ä½œçš„æŠµå¾¡èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4df3a5fdb8b736abdd590e8614de3445.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b759c5e3c228a8595aacdb8538c3705.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05e3410f90532ea73a03a275f943cb6d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="STream3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer"><a href="#STream3R-Scalable-Sequential-3D-Reconstruction-with-Causal-Transformer" class="headerlink" title="STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer"></a>STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer</h2><p><strong>Authors:Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, Xingang Pan</strong></p>
<p>We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: <a target="_blank" rel="noopener" href="https://nirvanalan.github.io/projects/stream3r">https://nirvanalan.github.io/projects/stream3r</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†STream3Rï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„3Dé‡å»ºæ–¹æ³•ï¼Œå®ƒå°†ç‚¹äº‘é¢„æµ‹é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªä»…è§£ç å™¨çš„Transformeré—®é¢˜ã€‚ç°æœ‰çš„æœ€å…ˆè¿›çš„å¤šå…ƒè§†å›¾é‡å»ºæ–¹æ³•è¦ä¹ˆä¾èµ–äºæ˜‚è´µçš„å…¨å±€ä¼˜åŒ–ï¼Œè¦ä¹ˆä¾èµ–äºç®€å•çš„è®°å¿†æœºåˆ¶ï¼Œè¿™äº›æœºåˆ¶éšç€åºåˆ—é•¿åº¦çš„å¢åŠ è€Œè¡¨ç°ä¸ä½³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒSTream3Rå¼•å…¥äº†ä¸€ä¸ªæµå¼æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç°ä»£è¯­è¨€å»ºæ¨¡çš„è¿›å±•ï¼Œé€šè¿‡å› æœæ³¨æ„åŠ›æœ‰æ•ˆåœ°å¤„ç†å›¾åƒåºåˆ—ã€‚é€šè¿‡ä»å¤§è§„æ¨¡3Dæ•°æ®é›†ä¸­å­¦ä¹ å‡ ä½•å…ˆéªŒçŸ¥è¯†ï¼ŒSTream3Råœ¨å¤šæ ·åŒ–å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿæ–¹æ³•å¸¸å¸¸å¤±æ•ˆçš„åŠ¨æ€åœºæ™¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é™æ€å’ŒåŠ¨æ€åœºæ™¯åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å§‹ç»ˆä¼˜äºå…ˆå‰çš„å·¥ä½œã€‚è€Œä¸”ï¼ŒSTream3Rä¸LLMé£æ ¼çš„è®­ç»ƒåŸºç¡€è®¾æ–½æœ¬è´¨ä¸Šå…¼å®¹ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°æ”¯æŒå¤§è§„æ¨¡é¢„è®­ç»ƒå’Œé’ˆå¯¹å„ç§ä¸‹æ¸¸3Dä»»åŠ¡çš„å¾®è°ƒã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†å› æœTransformeræ¨¡å‹åœ¨åœ¨çº¿3Dæ„ŸçŸ¥æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºæµå¼ç¯å¢ƒä¸­å®æ—¶3Dç†è§£é“ºå¹³äº†é“è·¯ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://nirvanalan.github.io/projects/stream3r%E3%80%82">https://nirvanalan.github.io/projects/stream3rã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10893v1">PDF</a> TL;DR: Streaming 4D reconstruction using causal transformer. Project   page: <a target="_blank" rel="noopener" href="https://nirvanalan.github.io/projects/stream3r">https://nirvanalan.github.io/projects/stream3r</a></p>
<p><strong>Summary</strong></p>
<p>STream3Ræ˜¯ä¸€ç§æ–°å‹çš„ä¸‰ç»´é‡å»ºæ–¹æ³•ï¼Œå®ƒå°†ç‚¹äº‘é¢„æµ‹è½¬åŒ–ä¸ºä»…è§£ç å™¨å‚ä¸çš„Transformeré—®é¢˜ã€‚ä¸ä¼ ç»Ÿå¤šè§†è§’é‡å»ºæ–¹æ³•ç›¸æ¯”ï¼ŒSTream3Ré‡‡ç”¨æµå¼æ¡†æ¶ï¼Œåˆ©ç”¨å› æœæ³¨æ„åŠ›é«˜æ•ˆå¤„ç†å›¾åƒåºåˆ—ï¼Œå¹¶ä»å¤§è§„æ¨¡ä¸‰ç»´æ•°æ®é›†ä¸­å­¦ä¹ å‡ ä½•å…ˆéªŒï¼Œèƒ½å¤Ÿé€‚åº”å¤šç§å¤æ‚åœºæ™¯ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿæ–¹æ³•å¸¸å¤±æ•ˆçš„åŠ¨æ€åœºæ™¯ã€‚å®éªŒè¯æ˜ï¼ŒSTream3Råœ¨é™æ€å’ŒåŠ¨æ€åœºæ™¯åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œä¸”ä¸å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒåŸºç¡€è®¾æ–½å…¼å®¹ï¼Œå¯ä¸ºå„ç§ä¸‹æ¸¸ä¸‰ç»´ä»»åŠ¡æä¾›é«˜æ•ˆçš„å¤§å‹é¢„è®­ç»ƒå’Œå¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>STream3Ræ˜¯ä¸€ç§æ–°çš„ä¸‰ç»´é‡å»ºæ–¹æ³•ï¼Œå°†ç‚¹äº‘é¢„æµ‹è½¬åŒ–ä¸ºä»…è§£ç å™¨Transformeré—®é¢˜ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒSTream3Råˆ©ç”¨æµå¼æ¡†æ¶å’Œå› æœæ³¨æ„åŠ›å¤„ç†å›¾åƒåºåˆ—ã€‚</li>
<li>STream3Rä»å¤§è§„æ¨¡ä¸‰ç»´æ•°æ®é›†ä¸­å­¦ä¹ å‡ ä½•å…ˆéªŒï¼Œé€‚åº”å¤šç§å¤æ‚åœºæ™¯ã€‚</li>
<li>å®éªŒè¯æ˜STream3Råœ¨é™æ€å’ŒåŠ¨æ€åœºæ™¯åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>STream3Rä¸å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒåŸºç¡€è®¾æ–½å…¼å®¹ï¼Œä¾¿äºå„ç§ä¸‹æ¸¸ä¸‰ç»´ä»»åŠ¡çš„é¢„è®­ç»ƒå’Œå¾®è°ƒã€‚</li>
<li>STream3Rä¸ºå®æ—¶ä¸‰ç»´ç†è§£åœ¨æµå¼ç¯å¢ƒä¸­é“ºå¹³äº†é“è·¯ã€‚</li>
<li>æ›´å¤šç»†èŠ‚å¯æŸ¥é˜…é¡¹ç›®ç½‘é¡µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33f6ccd939a82a2366987da9061f14d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-175e0c0a6d1f695806a75d2ace9e11c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a720cba1f94fd5a63362036385f7390.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT"><a href="#UI-Venus-Technical-Report-Building-High-performance-UI-Agents-with-RFT" class="headerlink" title="UI-Venus Technical Report: Building High-performance UI Agents with RFT"></a>UI-Venus Technical Report: Building High-performance UI Agents with RFT</h2><p><strong>Authors:Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, Weiqiang Wang</strong></p>
<p>We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% &#x2F; 50.8% and 95.3% &#x2F; 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 &#x2F; Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5. To show UI-Venusâ€™s summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models. To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies. To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment &amp; Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at <a target="_blank" rel="noopener" href="https://github.com/inclusionAI/UI-Venus">https://github.com/inclusionAI/UI-Venus</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†UI-Venusï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸç”Ÿç”¨æˆ·ç•Œé¢ä»£ç†ï¼Œå®ƒä»…é€šè¿‡æˆªå±ä½œä¸ºè¾“å…¥ã€‚UI-Venusé€šè¿‡ä½¿ç”¨åŸºäºQwen2.5-VLçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å’Œä»…æ•°ç™¾ä¸‡çš„é«˜è´¨é‡è®­ç»ƒæ ·æœ¬ï¼Œåœ¨UIå®šä½å’Œå¯¼èˆªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒUI-Venusçš„7Bå’Œ72Bå˜ç§åœ¨æ ‡å‡†çš„å®šä½åŸºå‡†æµ‹è¯•ï¼ˆå³Screenspot-V2&#x2F;Proï¼‰ä¸Šåˆ†åˆ«è¾¾åˆ°äº†94.1%&#x2F;50.8%å’Œ95.3%&#x2F;61.9%ï¼Œè¶…è¶Šäº†åŒ…æ‹¬å¼€æºGTA1å’Œé—­æºUI-TARS-1.5åœ¨å†…çš„å…ˆå‰æœ€æ–°åŸºçº¿ã€‚ä¸ºäº†å±•ç¤ºUI-Venusçš„æ€»ç»“å’Œè§„åˆ’èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜å°†å…¶è¯„ä¼°äº†ä¸€ä¸ªåœ¨çº¿UIå¯¼èˆªå¹³å°AndroidWorldï¼Œæˆ‘ä»¬çš„7Bå’Œ72Bå˜ç§åœ¨è¯¥å¹³å°ä¸Šåˆ†åˆ«è¾¾åˆ°äº†49.1%å’Œ65.9%çš„æˆåŠŸç‡ï¼Œä¹Ÿå‡»è´¥äº†ç°æœ‰æ¨¡å‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä¸ºUIå®šä½å’Œå¯¼èˆªä»»åŠ¡ç²¾å¿ƒè®¾è®¡äº†å¥–åŠ±å‡½æ•°å’Œç›¸åº”çš„æœ‰æ•ˆæ•°æ®æ¸…ç†ç­–ç•¥ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å¯¼èˆªæ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªæˆ‘è¿›åŒ–çš„è½¨è¿¹å†å²å¯¹é½å’Œç¨€ç–åŠ¨ä½œå¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä¼˜åŒ–å†å²æ¨ç†è½¨è¿¹å¹¶å¹³è¡¡ç¨€ç–ä½†å…³é”®åŠ¨ä½œçš„åˆ†å¸ƒï¼Œä»è€Œå¯¼è‡´æ›´è¿è´¯çš„è§„åˆ’ä»¥åŠåœ¨å¤æ‚UIä»»åŠ¡ä¸­æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬å‘å¸ƒé¢†å…ˆçš„å¼€æºUIä»£ç†ã€å…¨é¢çš„æ•°æ®æ¸…ç†åè®®ä»¥åŠä¸€ç§æé«˜å¯¼èˆªæ€§èƒ½çš„è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œè¿™é¼“åŠ±äº†ç¤¾åŒºå†…çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/inclusionAI/UI-Venus%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/inclusionAI/UI-Venusæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10833v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸç”Ÿç”¨æˆ·ç•Œé¢ä»£ç†UI-Venusã€‚è¯¥ä»£ç†ä»…é€šè¿‡æˆªå›¾ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰åœ¨UIæ¥åœ°å’Œå¯¼èˆªä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚UI-Venusçš„7Bå’Œ72Bå˜ä½“åœ¨æ ‡å‡†æ¥åœ°åŸºå‡†æµ‹è¯•Screenspot-V2 &#x2F; Proä¸Šåˆ†åˆ«è¾¾åˆ°äº†94.1% &#x2F; 50.8%å’Œ95.3% &#x2F; 61.9%çš„æ€§èƒ½ï¼Œè¶…è¿‡äº†åŒ…æ‹¬å¼€æºGTA1å’Œé—­æºUI-TARS-1.5åœ¨å†…çš„å…ˆå‰æœ€æ–°åŸºçº¿ã€‚æ­¤å¤–ï¼ŒUI-Venusåœ¨åœ¨çº¿UIå¯¼èˆªåŒºåŸŸAndroidWorldä¸Šçš„æˆåŠŸç‡ä¹Ÿè¶…è¿‡äº†ç°æœ‰æ¨¡å‹ã€‚ä¸ºå®ç°è¿™äº›æ€§èƒ½ï¼Œæœ¬æ–‡å¼•å…¥äº†é’ˆå¯¹UIæ¥åœ°å’Œå¯¼èˆªä»»åŠ¡çš„ç²¾å¿ƒè®¾è®¡å¥–åŠ±å‡½æ•°å’Œç›¸åº”çš„æœ‰æ•ˆæ•°æ®æ¸…ç†ç­–ç•¥ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜å¯¼èˆªæ€§èƒ½ï¼Œæå‡ºäº†è‡ªæˆ‘è¿›åŒ–çš„è½¨è¿¹å†å²å¯¹é½ä¸ç¨€ç–åŠ¨ä½œå¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¼˜åŒ–äº†å†å²æ¨ç†è½¨è¿¹ï¼Œå¹³è¡¡äº†ç¨€ç–ä½†å…³é”®åŠ¨ä½œçš„åˆ†å¸ƒï¼Œä»è€Œå®ç°äº†æ›´è¿è´¯çš„è§„åˆ’ï¼Œå¹¶åœ¨å¤æ‚çš„UIä»»åŠ¡ä¸­å®ç°äº†æ›´å¥½çš„æ³›åŒ–ã€‚æœ¬æ–‡çš„è´¡çŒ®åŒ…æ‹¬å…¬å¸ƒæœ€æ–°çš„å¼€æºUIä»£ç†ã€å…¨é¢çš„æ•°æ®æ¸…ç†åè®®ä»¥åŠä¸€ç§æé«˜å¯¼èˆªæ€§èƒ½çš„è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œè¿™é¼“åŠ±äº†ç¤¾åŒºè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UI-Venusæ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸç”Ÿç”¨æˆ·ç•Œé¢ä»£ç†ï¼Œä»…é€šè¿‡æˆªå›¾ä½œä¸ºè¾“å…¥ã€‚</li>
<li>UI-Venusåœ¨UIæ¥åœ°å’Œå¯¼èˆªä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ï¼Œå…¶7Bå’Œ72Bå˜ä½“åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>UI-Venusåœ¨åœ¨çº¿UIå¯¼èˆªåŒºåŸŸAndroidWorldä¸Šçš„æˆåŠŸç‡è¶…è¿‡äº†ç°æœ‰æ¨¡å‹ã€‚</li>
<li>ä¸ºæé«˜æ€§èƒ½ï¼Œæœ¬æ–‡å¼•å…¥äº†å¥–åŠ±å‡½æ•°ã€æ•°æ®æ¸…ç†ç­–ç•¥å’Œè‡ªæˆ‘è¿›åŒ–çš„è½¨è¿¹å†å²å¯¹é½ä¸ç¨€ç–åŠ¨ä½œå¢å¼ºæ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡å…¬å¸ƒäº†å¼€æºUIä»£ç†ã€æ•°æ®æ¸…ç†åè®®å’Œæé«˜å¯¼èˆªæ€§èƒ½çš„è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ã€‚</li>
<li>UI-Venusçš„ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5a7428b4cacf8cd1db688e5311ce0ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aac5a1f1b87560022fd07ecc6a21a1e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-983480955cc172d16e7c13cb765a9fe3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Transformer-Based-Approach-for-DDoS-Attack-Detection-in-IoT-Networks"><a href="#A-Transformer-Based-Approach-for-DDoS-Attack-Detection-in-IoT-Networks" class="headerlink" title="A Transformer-Based Approach for DDoS Attack Detection in IoT Networks"></a>A Transformer-Based Approach for DDoS Attack Detection in IoT Networks</h2><p><strong>Authors:Sandipan Dey, Payal Santosh Kate, Vatsala Upadhyay, Abhishek Vaish</strong></p>
<p>DDoS attacks have become a major threat to the security of IoT devices and can cause severe damage to the network infrastructure. IoT devices suffer from the inherent problem of resource constraints and are therefore susceptible to such resource-exhausting attacks. Traditional methods for detecting DDoS attacks are not efficient enough to cope with the dynamic nature of IoT networks, as well as the scalability of the attacks, diversity of protocols, high volume of traffic, and variability in device behavior, and variability of protocols like MQTT, CoAP, making it hard to implement security across all the protocols. In this paper, we propose a novel approach, i.e., the use of Transformer models, which have shown remarkable performance in natural language processing tasks, for detecting DDoS attacks on IoT devices. The proposed model extracts features from network traffic data and processes them using a self-attention mechanism. Experiments conducted on a real-world dataset demonstrate that the proposed approach outperforms traditional machine learning techniques, which can be validated by comparing both approachesâ€™ accuracy, precision, recall, and F1-score. The results of this study show that the Transformer models can be an effective solution for detecting DDoS attacks on IoT devices and have the potential to be deployed in real-world IoT environments. </p>
<blockquote>
<p>DDoSæ”»å‡»å·²æˆä¸ºç‰©è”ç½‘è®¾å¤‡å®‰å…¨çš„ä¸»è¦å¨èƒï¼Œå¹¶å¯èƒ½å¯¹ç½‘ç»œåŸºç¡€è®¾æ–½é€ æˆä¸¥é‡æŸå®³ã€‚ç‰©è”ç½‘è®¾å¤‡å­˜åœ¨èµ„æºçº¦æŸçš„å†…åœ¨é—®é¢˜ï¼Œå› æ­¤å®¹æ˜“å—åˆ°æ­¤ç±»è€—å°½èµ„æºçš„æ”»å‡»ã€‚ä¼ ç»Ÿçš„æ£€æµ‹DDoSæ”»å‡»çš„æ–¹æ³•ä¸è¶³ä»¥åº”å¯¹ç‰©è”ç½‘ç½‘ç»œçš„åŠ¨æ€æ€§ï¼Œä»¥åŠæ”»å‡»çš„å¯æ‰©å±•æ€§ã€åè®®çš„å¤šæ ·æ€§ã€æµé‡çš„å¤§é‡æ€§ä»¥åŠè®¾å¤‡è¡Œä¸ºçš„å¯å˜æ€§ï¼Œä»¥åŠMQTTã€CoAPç­‰åè®®çš„å¤šæ ·æ€§ï¼Œä½¿å¾—åœ¨æ‰€æœ‰åè®®ä¸Šå®æ–½å®‰å…¨å˜å¾—å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå³ä½¿ç”¨Transformeræ¨¡å‹æ£€æµ‹ç‰©è”ç½‘è®¾å¤‡ä¸Šçš„DDoSæ”»å‡»ã€‚è¯¥æ¨¡å‹ä»ç½‘ç»œæµé‡æ•°æ®ä¸­æå–ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå¤„ç†ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡æ¯”è¾ƒä¸¤è€…çš„å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°è¿›è¡ŒéªŒè¯ã€‚æœ¬ç ”ç©¶çš„ç»“æœè¡¨æ˜ï¼ŒTransformeræ¨¡å‹æ˜¯æ£€æµ‹ç‰©è”ç½‘è®¾å¤‡ä¸ŠDDoSæ”»å‡»çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¹¶æœ‰å¯èƒ½åœ¨çœŸå®çš„ç‰©è”ç½‘ç¯å¢ƒä¸­éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10636v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç‰©è”ç½‘è®¾å¤‡çš„DDoSæ”»å‡»å·²æˆä¸ºé‡å¤§å®‰å…¨å¨èƒï¼Œå¯èƒ½å¯¼è‡´ç½‘ç»œåŸºç¡€è®¾æ–½ä¸¥é‡å—æŸã€‚ç”±äºç‰©è”ç½‘è®¾å¤‡å­˜åœ¨èµ„æºçº¦æŸé—®é¢˜ï¼Œä¼ ç»Ÿæ£€æµ‹DDoSæ”»å‡»çš„æ–¹æ³•éš¾ä»¥åº”å¯¹ç‰©è”ç½‘ç½‘ç»œçš„åŠ¨æ€æ€§ã€æ”»å‡»çš„è§„æ¨¡åŒ–ã€åè®®å¤šæ ·æ€§å’Œé«˜æµé‡ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå³åˆ©ç”¨åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²çš„Transformeræ¨¡å‹æ£€æµ‹ç‰©è”ç½‘è®¾å¤‡çš„DDoSæ”»å‡»ã€‚è¯¥æ–¹æ³•ä»ç½‘ç»œæµé‡æ•°æ®ä¸­æå–ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå¤„ç†ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå¯ä»¥é€šè¿‡æ¯”è¾ƒä¸¤è€…çš„å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°è¿›è¡ŒéªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DDoSæ”»å‡»å¯¹ç‰©è”ç½‘è®¾å¤‡æ„æˆé‡å¤§å¨èƒï¼Œå¯é€ æˆç½‘ç»œåŸºç¡€è®¾æ–½ä¸¥é‡æŸå®³ã€‚</li>
<li>ç‰©è”ç½‘è®¾å¤‡å› èµ„æºçº¦æŸé—®é¢˜ï¼Œé¢ä¸´DDoSæ”»å‡»æ—¶çš„é˜²æŠ¤é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥åº”å¯¹ç‰©è”ç½‘ç½‘ç»œçš„åŠ¨æ€æ€§ã€æ”»å‡»çš„è§„æ¨¡åŒ–ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºåˆ©ç”¨Transformeræ¨¡å‹æ£€æµ‹ç‰©è”ç½‘è®¾å¤‡çš„DDoSæ”»å‡»ã€‚</li>
<li>Transformeræ¨¡å‹é€šè¿‡æå–ç½‘ç»œæµé‡æ•°æ®ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå¤„ç†ã€‚</li>
<li>åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6e46e23c77b4fb5b9a502df1dbb3cc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b95ef946f4ed1ce1f0f930dedcd01cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f90cf32bab35f0a1d2973492742a5871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-681f604a346c652a2b98e33b5f3fd3f4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Concepts-or-Skills-Rethinking-Instruction-Selection-for-Multi-modal-Models"><a href="#Concepts-or-Skills-Rethinking-Instruction-Selection-for-Multi-modal-Models" class="headerlink" title="Concepts or Skills? Rethinking Instruction Selection for Multi-modal   Models"></a>Concepts or Skills? Rethinking Instruction Selection for Multi-modal   Models</h2><p><strong>Authors:Andrew Bai, Justin Cui, Ruochen Wang, Cho-Jui Hsieh</strong></p>
<p>Vision-language instruction tuning achieves two main purposes: learning visual concepts and learning visual skills. In this paper, we found that vision-language benchmarks fall into the dichotomy of mainly benefiting from training on instructions with similar skills or visual concepts. Inspired by the discovery, we designed a simple targeted training data selection method to optimize the performance of a given benchmark. We first extract the concepts&#x2F;skills from the benchmark, determine whether the benchmark predominantly benefits from similar concepts or skills, and finally select instructions with the most matching concepts&#x2F;skills. Experiments on 10+ benchmarks validate the effectiveness of our targeted data selection method, showing +0.9% over the best existing baseline averaged over all benchmarks and +1.5% on the skill-focused subset. Our findings underscore the importance of recognizing the inherent trade-off within instruction selection, which requires balancing the acquisition of conceptual knowledge against visual skill. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æŒ‡ä»¤è°ƒæ•´å®ç°äº†ä¸¤ä¸ªä¸»è¦ç›®æ ‡ï¼šå­¦ä¹ è§†è§‰æ¦‚å¿µå’Œå­¦ä¹ è§†è§‰æŠ€èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸»è¦å—ç›Šäºå…·æœ‰ç±»ä¼¼æŠ€èƒ½æˆ–è§†è§‰æ¦‚å¿µçš„æŒ‡ä»¤è®­ç»ƒã€‚å—æ­¤å‘ç°å¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®€å•çš„æœ‰é’ˆå¯¹æ€§çš„è®­ç»ƒæ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œä»¥ä¼˜åŒ–ç»™å®šåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚æˆ‘ä»¬é¦–å…ˆä»åŸºå‡†æµ‹è¯•ä¸­æå–æ¦‚å¿µ&#x2F;æŠ€èƒ½ï¼Œç¡®å®šåŸºå‡†æµ‹è¯•ä¸»è¦å—ç›Šäºç±»ä¼¼çš„æ¦‚å¿µè¿˜æ˜¯æŠ€èƒ½ï¼Œå¹¶æœ€ç»ˆé€‰æ‹©æ¦‚å¿µ&#x2F;æŠ€èƒ½æœ€åŒ¹é…çš„æŒ‡ä»¤ã€‚åœ¨10å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒéªŒè¯äº†æˆ‘ä»¬æœ‰é’ˆå¯¹æ€§çš„æ•°æ®é€‰æ‹©æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡è¡¨ç°ä¼˜äºç°æœ‰æœ€ä½³åŸºçº¿+0.9%ï¼Œåœ¨æŠ€èƒ½å¯¼å‘çš„å­é›†ä¸Š+1.5%ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†åœ¨é€‰æ‹©æŒ‡ä»¤æ—¶è®¤è¯†åˆ°å†…åœ¨æƒè¡¡çš„é‡è¦æ€§ï¼Œè¿™éœ€è¦å¹³è¡¡æ¦‚å¿µçŸ¥è¯†çš„è·å–å’Œè§†è§‰æŠ€èƒ½çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10339v1">PDF</a> 11 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æŒ‡ä»¤è°ƒæ•´çš„ä¸¤ä¸ªä¸»è¦ç›®æ ‡ï¼šå­¦ä¹ è§†è§‰æ¦‚å¿µå’Œå­¦ä¹ è§†è§‰æŠ€èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸»è¦å—ç›Šäºç±»ä¼¼æŠ€èƒ½å’Œè§†è§‰æ¦‚å¿µçš„æŒ‡ä»¤è®­ç»ƒã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œè®¾è®¡äº†ä¸€ç§ç®€å•çš„é’ˆå¯¹æ€§è®­ç»ƒæ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œä»¥ä¼˜åŒ–ç»™å®šåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚é€šè¿‡ä»åŸºå‡†æµ‹è¯•ä¸­æå–æ¦‚å¿µ&#x2F;æŠ€èƒ½ï¼Œç¡®å®šä¸»è¦å—ç›Šäºç±»ä¼¼æ¦‚å¿µæˆ–æŠ€èƒ½ï¼Œæœ€ç»ˆé€‰æ‹©åŒ¹é…åº¦æœ€é«˜çš„æŒ‡ä»¤ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥å®šå‘æ•°æ®é€‰æ‹©æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡è¡¨ç°ä¼˜äºæœ€ä½³ç°æœ‰åŸºçº¿+0.9%ï¼Œåœ¨ä¾§é‡æŠ€èƒ½çš„å­é›†ä¸Š+1.5%ã€‚ç ”ç©¶å¼ºè°ƒäº†æŒ‡ä»¤é€‰æ‹©ä¸­å›ºæœ‰æƒè¡¡çš„é‡è¦æ€§ï¼Œéœ€è¦åœ¨è·å–æ¦‚å¿µçŸ¥è¯†å’Œè§†è§‰æŠ€èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æŒ‡ä»¤è°ƒæ•´æ—¨åœ¨å®ç°å­¦ä¹ è§†è§‰æ¦‚å¿µå’Œå­¦ä¹ è§†è§‰æŠ€èƒ½ä¸¤ä¸ªä¸»è¦ç›®æ ‡ã€‚</li>
<li>è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸»è¦å—ç›Šäºç±»ä¼¼æŠ€èƒ½å’Œè§†è§‰æ¦‚å¿µçš„æŒ‡ä»¤è®­ç»ƒã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§ç®€å•çš„é’ˆå¯¹æ€§è®­ç»ƒæ•°æ®é€‰æ‹©æ–¹æ³•æ¥ä¼˜åŒ–ç»™å®šåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ä»åŸºå‡†æµ‹è¯•ä¸­æå–æ¦‚å¿µ&#x2F;æŠ€èƒ½æ¥ç¡®å®šä¸»è¦å—ç›Šç‚¹ã€‚</li>
<li>å®éªŒéªŒè¯äº†å®šå‘æ•°æ®é€‰æ‹©æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æŒ‡ä»¤é€‰æ‹©ä¸­éœ€è¦åœ¨è·å–æ¦‚å¿µçŸ¥è¯†å’Œè§†è§‰æŠ€èƒ½ä¹‹é—´å–å¾—å¹³è¡¡çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10339">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-93fbe9cc715d89b6444a9b746783f38d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d904e1fb5fb0c6e71aab12e53c20b4d7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Pruning-and-Malicious-Injection-A-Retraining-Free-Backdoor-Attack-on-Transformer-Models"><a href="#Pruning-and-Malicious-Injection-A-Retraining-Free-Backdoor-Attack-on-Transformer-Models" class="headerlink" title="Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on   Transformer Models"></a>Pruning and Malicious Injection: A Retraining-Free Backdoor Attack on   Transformer Models</h2><p><strong>Authors:Taibiao Zhao, Mingxuan Sun, Hao Wang, Xiaobing Chen, Xiangwei Zhou</strong></p>
<p>Transformer models have demonstrated exceptional performance and have become indispensable in computer vision (CV) and natural language processing (NLP) tasks. However, recent studies reveal that transformers are susceptible to backdoor attacks. Prior backdoor attack methods typically rely on retraining with clean data or altering the model architecture, both of which can be resource-intensive and intrusive. In this paper, we propose Head-wise Pruning and Malicious Injection (HPMI), a novel retraining-free backdoor attack on transformers that does not alter the modelâ€™s architecture. Our approach requires only a small subset of the original data and basic knowledge of the model architecture, eliminating the need for retraining the target transformer. Technically, HPMI works by pruning the least important head and injecting a pre-trained malicious head to establish the backdoor. We provide a rigorous theoretical justification demonstrating that the implanted backdoor resists detection and removal by state-of-the-art defense techniques, under reasonable assumptions. Experimental evaluations across multiple datasets further validate the effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy loss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four advanced defense mechanisms. Additionally, relative to state-of-the-art retraining-dependent attacks, HPMI achieves greater concealment and robustness against diverse defense strategies, while maintaining minimal impact on clean accuracy. </p>
<blockquote>
<p>Transformeræ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶æˆä¸ºäº†ä¸å¯æˆ–ç¼ºçš„å­˜åœ¨ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒTransformerå®¹æ˜“å—åˆ°åé—¨æ”»å‡»ã€‚ä¹‹å‰çš„åé—¨æ”»å‡»æ–¹æ³•é€šå¸¸ä¾èµ–äºä½¿ç”¨å¹²å‡€æ•°æ®çš„é‡æ–°è®­ç»ƒæˆ–æ”¹å˜æ¨¡å‹æ¶æ„ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½å¯èƒ½æ˜¯èµ„æºå¯†é›†å‹çš„ä¸”å…·ä¾µå…¥æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Head-wise Pruning and Malicious Injectionï¼ˆHPMIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ã€æ— éœ€é‡æ–°è®­ç»ƒçš„Transformeråé—¨æ”»å‡»æ–¹æ³•ï¼Œå®ƒä¸ä¼šæ”¹å˜æ¨¡å‹æ¶æ„ã€‚æˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦åŸå§‹æ•°æ®çš„ä¸€ä¸ªå°å­é›†å’Œæ¨¡å‹æ¶æ„çš„åŸºæœ¬çŸ¥è¯†ï¼Œä»è€Œæ— éœ€å¯¹ç›®æ ‡Transformerè¿›è¡Œé‡æ–°è®­ç»ƒã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼ŒHPMIé€šè¿‡åˆ é™¤æœ€ä¸é‡è¦çš„å¤´éƒ¨å¹¶æ³¨å…¥ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¶æ„å¤´éƒ¨æ¥å»ºç«‹åé—¨ã€‚æˆ‘ä»¬æä¾›äº†ä¸¥æ ¼çš„ç†è®ºè¯æ˜ï¼Œè¯æ˜æ¤å…¥çš„åé—¨èƒ½å¤Ÿåœ¨åˆç†çš„å‡è®¾ä¸‹æŠµæŠ—æœ€å…ˆè¿›çš„é˜²å¾¡æŠ€æœ¯çš„æ£€æµ‹å’Œç§»é™¤ã€‚è·¨å¤šä¸ªæ•°æ®é›†çš„å®éªŒè¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†HPMIçš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å®ƒ1ï¼‰å‡ ä¹ä¸ä¼šé€ æˆæ¸…æ´ç²¾åº¦çš„æŸå¤±ï¼Œ2ï¼‰æ”»å‡»æˆåŠŸç‡è‡³å°‘è¾¾åˆ°99.55%ï¼Œ3ï¼‰ç»•è¿‡äº†å››ç§å…ˆè¿›çš„é˜²å¾¡æœºåˆ¶ã€‚æ­¤å¤–ï¼Œä¸æœ€å…ˆè¿›çš„ä¾èµ–äºé‡æ–°è®­ç»ƒçš„æ”»å‡»ç›¸æ¯”ï¼ŒHPMIåœ¨å¤šç§é˜²å¾¡ç­–ç•¥é¢å‰å…·æœ‰æ›´é«˜çš„éšè”½æ€§å’Œç¨³å¥æ€§ï¼ŒåŒæ—¶å‡ ä¹ä¸å½±å“æ¸…æ´ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10243v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹Transformeræ¨¡å‹çš„æ–°å‹åé—¨æ”»å‡»æ–¹æ³•â€”â€”Head-wise Pruning and Malicious Injection (HPMI)ã€‚è¯¥æ–¹æ³•æ— éœ€é‡è®­æ¨¡å‹ï¼Œä¸”ä¸æ”¹å˜æ¨¡å‹æ¶æ„ï¼Œä»…åˆ©ç”¨åŸå§‹æ•°æ®å­é›†å’Œæ¨¡å‹åŸºç¡€æ¶æ„çŸ¥è¯†å³å¯å®ç°æ”»å‡»ã€‚HPMIé€šè¿‡åˆ é™¤æ¬¡è¦å¤´éƒ¨å¹¶æ³¨å…¥é¢„è®­ç»ƒçš„æ¶æ„å¤´éƒ¨æ¥å»ºç«‹åé—¨ï¼Œå…·æœ‰æŠµå¾¡å…ˆè¿›é˜²å¾¡æŠ€æœ¯æ£€æµ‹å’Œç§»é™¤çš„èƒ½åŠ›ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒHPMIä¸ä»…æ¸…æ´ç²¾åº¦æŸå¤±å°ï¼Œæ”»å‡»æˆåŠŸç‡é«˜è¾¾è‡³å°‘99.55%ï¼Œè€Œä¸”èƒ½ç»•è¿‡å››ç§å…ˆè¿›çš„é˜²å¾¡æœºåˆ¶ã€‚ç›¸è¾ƒäºä¾èµ–é‡è®­çš„æ”»å‡»æ–¹æ³•ï¼ŒHPMIæ›´å…·éšè”½æ€§å’Œå¯¹å¤šç§é˜²å¾¡ç­–ç•¥çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„æ¸…æ´ç²¾åº¦å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†æ˜“å—åé—¨æ”»å‡»ã€‚</li>
<li>ä¼ ç»Ÿçš„åé—¨æ”»å‡»æ–¹æ³•é€šå¸¸éœ€è¦é‡è®­æ¨¡å‹æˆ–æ”¹å˜æ¨¡å‹æ¶æ„ï¼Œèµ„æºæ¶ˆè€—å¤§ä¸”ä¾µå…¥æ€§å¼ºã€‚</li>
<li>HPMIæ˜¯ä¸€ç§æ–°å‹çš„åé—¨æ”»å‡»æ–¹æ³•ï¼Œæ— éœ€é‡è®­æ¨¡å‹ï¼Œä¸æ”¹å˜æ¨¡å‹æ¶æ„ã€‚</li>
<li>HPMIé€šè¿‡åˆ é™¤æ¬¡è¦å¤´éƒ¨å¹¶æ³¨å…¥é¢„è®­ç»ƒçš„æ¶æ„å¤´éƒ¨æ¥å»ºç«‹åé—¨ã€‚</li>
<li>HPMIå…·æœ‰æŠµå¾¡å…ˆè¿›é˜²å¾¡æŠ€æœ¯æ£€æµ‹å’Œç§»é™¤çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒHPMIå…·æœ‰é«˜çš„æ”»å‡»æˆåŠŸç‡å’Œä½çš„æ¸…æ´ç²¾åº¦æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1a6e87664bb3fa0d2ce95738bfda1cc1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13bc53fbfb1430cd31d33fc68d0538e3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Can-Transformers-Break-Encryption-Schemes-via-In-Context-Learning"><a href="#Can-Transformers-Break-Encryption-Schemes-via-In-Context-Learning" class="headerlink" title="Can Transformers Break Encryption Schemes via In-Context Learning?"></a>Can Transformers Break Encryption Schemes via In-Context Learning?</h2><p><strong>Authors:Jathin Korrapati, Patrick Mendoza, Aditya Tomar, Abein Abraham</strong></p>
<p>In-context learning (ICL) has emerged as a powerful capability of transformer-based language models, enabling them to perform tasks by conditioning on a small number of examples presented at inference time, without any parameter updates. Prior work has shown that transformers can generalize over simple function classes like linear functions, decision trees, even neural networks, purely from context, focusing on numerical or symbolic reasoning over underlying well-structured functions. Instead, we propose a novel application of ICL into the domain of cryptographic function learning, specifically focusing on ciphers such as mono-alphabetic substitution and Vigen`ere ciphers, two classes of private-key encryption schemes. These ciphers involve a fixed but hidden bijective mapping between plain text and cipher text characters. Given a small set of (cipher text, plain text) pairs, the goal is for the model to infer the underlying substitution and decode a new cipher text word. This setting poses a structured inference challenge, which is well-suited for evaluating the inductive biases and generalization capabilities of transformers under the ICL paradigm. Code is available at <a target="_blank" rel="noopener" href="https://github.com/adistomar/CS182-project">https://github.com/adistomar/CS182-project</a>. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å·²ç»æˆä¸ºåŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹çš„ä¸€ç§å¼ºå¤§åŠŸèƒ½ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿåœ¨æ¨ç†æ—¶é—´å‘ˆç°å°‘é‡ç¤ºä¾‹çš„æƒ…å†µä¸‹æ‰§è¡Œä»»åŠ¡ï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•å‚æ•°æ›´æ–°ã€‚å…ˆå‰çš„å·¥ä½œå·²ç»è¡¨æ˜ï¼Œè½¬æ¢å™¨å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡æ¦‚æ‹¬ç®€å•çš„å‡½æ•°ç±»åˆ«ï¼Œå¦‚çº¿æ€§å‡½æ•°ã€å†³ç­–æ ‘ï¼Œç”šè‡³æ˜¯ç¥ç»ç½‘ç»œï¼Œä¸“æ³¨äºå¯¹åŸºç¡€ç»“æ„è‰¯å¥½çš„å‡½æ•°çš„æ•°å€¼æˆ–ç¬¦å·æ¨ç†ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºå°†ICLçš„æ–°åº”ç”¨å¼•å…¥åˆ°å¯†ç å‡½æ•°å­¦ä¹ é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯å…³æ³¨å•å­—æ¯æ›¿ä»£å¯†ç å’Œç»´å‰å°¼äºšå¯†ç ç­‰å¯†ç ã€‚è¿™ä¸¤ç±»å¯†ç éƒ½å±äºç§é’¥åŠ å¯†æ–¹æ¡ˆï¼Œæ¶‰åŠæ˜æ–‡å’Œå¯†æ–‡å­—ç¬¦ä¹‹é—´çš„å›ºå®šä½†éšè—çš„åŒå‘æ˜ å°„ã€‚ç»™å®šä¸€ç»„ï¼ˆå¯†æ–‡ï¼Œæ˜æ–‡ï¼‰å¯¹ï¼Œæ¨¡å‹çš„ç›®æ ‡æ˜¯æ¨æ–­å‡ºæ½œåœ¨çš„æ›¿ä»£å…³ç³»å¹¶è§£ç æ–°çš„å¯†æ–‡å•è¯ã€‚æ­¤è®¾ç½®æå‡ºäº†ç»“æ„åŒ–æ¨ç†æŒ‘æˆ˜ï¼Œéå¸¸é€‚åˆäºè¯„ä¼°è½¬æ¢å™¨åœ¨ICLèŒƒå¼ä¸‹çš„å½’çº³åè§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/adistomar/CS182-project%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/adistomar/CS182-projectæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10235v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼ˆICLï¼‰ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†é˜¶æ®µé€šè¿‡å°‘é‡ç¤ºä¾‹è¿›è¡Œä»»åŠ¡æ‰§è¡Œï¼Œæ— éœ€æ›´æ–°ä»»ä½•å‚æ•°ã€‚æœ¬ç ”ç©¶å°†ICLåº”ç”¨äºå¯†ç å­¦å‡½æ•°å­¦ä¹ é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å•å­—æ¯æ›¿ä»£å¯†ç å’ŒVigenÃ¨reå¯†ç ç­‰ç§é’¥åŠ å¯†ç®—æ³•ã€‚ç»™å®šä¸€ç»„ï¼ˆå¯†æ–‡ï¼Œæ˜æ–‡ï¼‰å¯¹ï¼Œæ¨¡å‹çš„ç›®æ ‡æ˜¯æ¨æ–­å‡ºæ½œåœ¨çš„æ›¿ä»£è§„åˆ™å¹¶è§£ç å‡ºæ–°çš„å¯†æ–‡å•è¯ã€‚è¿™ä¸ºè¯„ä¼°è½¬æ¢å™¨çš„å½’çº³åè§å’Œæ³›åŒ–èƒ½åŠ›æä¾›äº†è‰¯å¥½çš„ç»“æ„åŒ–æ¨ç†æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¬æ¢å™¨è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼ˆICLï¼‰ã€‚</li>
<li>ICLèƒ½åœ¨æ¨ç†é˜¶æ®µé€šè¿‡å°‘é‡ç¤ºä¾‹æ‰§è¡Œä»»åŠ¡ï¼Œæ— éœ€æ›´æ–°å‚æ•°ã€‚</li>
<li>ICLè¢«åº”ç”¨äºå¯†ç å­¦å‡½æ•°å­¦ä¹ é¢†åŸŸã€‚</li>
<li>é’ˆå¯¹çš„æ˜¯ç§é’¥åŠ å¯†ç®—æ³•ï¼Œå¦‚å•å­—æ¯æ›¿ä»£å¯†ç å’ŒVigenÃ¨reå¯†ç ã€‚</li>
<li>æ¨¡å‹éœ€ä»ç»™å®šçš„ï¼ˆå¯†æ–‡ï¼Œæ˜æ–‡ï¼‰å¯¹ä¸­æ¨æ–­å‡ºæ½œåœ¨æ›¿ä»£è§„åˆ™ã€‚</li>
<li>æ­¤é¢†åŸŸçš„åº”ç”¨ä¸ºè¯„ä¼°æ¨¡å‹çš„å½’çº³åè§å’Œæ³›åŒ–èƒ½åŠ›æä¾›äº†ç»“æ„åŒ–æ¨ç†æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fac26b29be3af38e874fdd67410b1558.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7dec895c49131143cce35b8e2e8ede2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8fd0eedc9955d63d69b36b8bd2985c0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Bridging-Modality-Gaps-in-e-Commerce-Products-via-Vision-Language-Alignment"><a href="#Bridging-Modality-Gaps-in-e-Commerce-Products-via-Vision-Language-Alignment" class="headerlink" title="Bridging Modality Gaps in e-Commerce Products via Vision-Language   Alignment"></a>Bridging Modality Gaps in e-Commerce Products via Vision-Language   Alignment</h2><p><strong>Authors:Yipeng Zhang, Hongju Yu, Aritra Mandal, Canran Xu, Qunzhi Zhou, Zhe Wu</strong></p>
<p>Item information, such as titles and attributes, is essential for effective user engagement in e-commerce. However, manual or semi-manual entry of structured item specifics often produces inconsistent quality, errors, and slow turnaround, especially for Customer-to-Customer sellers. Generating accurate descriptions directly from item images offers a promising alternative. Existing retrieval-based solutions address some of these issues but often miss fine-grained visual details and struggle with niche or specialized categories.   We propose Optimized Preference-Based AI for Listings (OPAL), a framework for generating schema-compliant, high-quality item descriptions from images using a fine-tuned multimodal large language model (MLLM). OPAL addresses key challenges in multimodal e-commerce applications, including bridging modality gaps and capturing detailed contextual information. It introduces two data refinement methods: MLLM-Assisted Conformity Enhancement, which ensures alignment with structured schema requirements, and LLM-Assisted Contextual Understanding, which improves the capture of nuanced and fine-grained information from visual inputs.   OPAL uses visual instruction tuning combined with direct preference optimization to fine-tune the MLLM, reducing hallucinations and improving robustness across different backbone architectures. We evaluate OPAL on real-world e-commerce datasets, showing that it consistently outperforms baseline methods in both description quality and schema completion rates. These results demonstrate that OPAL effectively bridges the gap between visual and textual modalities, delivering richer, more accurate, and more consistent item descriptions. This work advances automated listing optimization and supports scalable, high-quality content generation in e-commerce platforms. </p>
<blockquote>
<p>å•†å“ä¿¡æ¯ï¼Œå¦‚æ ‡é¢˜å’Œå±æ€§ï¼Œå¯¹äºç”µå­å•†åŠ¡ä¸­çš„æœ‰æ•ˆç”¨æˆ·å‚ä¸è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨æˆ–åŠæ‰‹åŠ¨è¾“å…¥çš„ç»“æ„åŒ–å•†å“è¯¦ç»†ä¿¡æ¯å¾€å¾€ä¼šäº§ç”Ÿè´¨é‡ä¸ç¨³å®šã€é”™è¯¯å’Œå“åº”æ…¢çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå®¢æˆ·å¯¹å®¢æˆ·å–å®¶è€Œè¨€ã€‚ç›´æ¥ä»å•†å“å›¾ç‰‡ç”Ÿæˆå‡†ç¡®çš„æè¿°æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç°æœ‰çš„åŸºäºæ£€ç´¢çš„è§£å†³æ–¹æ¡ˆè§£å†³äº†å…¶ä¸­çš„ä¸€äº›é—®é¢˜ï¼Œä½†å¾€å¾€å¿½ç•¥äº†ç»†å¾®çš„è§†è§‰ç»†èŠ‚ï¼Œå¹¶ä¸”åœ¨å°ä¼—æˆ–ä¸“ä¸šç±»åˆ«ä¸­è¡¨ç°æŒ£æ‰ã€‚æˆ‘ä»¬æå‡ºäº†Optimized Preference-Based AI for Listings (OPAL)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ç»è¿‡å¾®è°ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»å›¾åƒç”Ÿæˆç¬¦åˆæ¶æ„çš„é«˜è´¨é‡å•†å“æè¿°ã€‚OPALè§£å†³äº†å¤šæ¨¡æ€ç”µå­å•†åŠ¡åº”ç”¨ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¼¥åˆæ¨¡æ€å·®è·å’Œæ•è·è¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å®ƒå¼•å…¥äº†ä¸¤ç§æ•°æ®ä¼˜åŒ–æ–¹æ³•ï¼šMLLMè¾…åŠ©ä¸€è‡´æ€§å¢å¼ºï¼Œç¡®ä¿ä¸ç»“æ„åŒ–æ¶æ„è¦æ±‚çš„ä¸€è‡´æ€§ï¼›LLMè¾…åŠ©ä¸Šä¸‹æ–‡ç†è§£ï¼Œæé«˜äº†ä»è§†è§‰è¾“å…¥ä¸­æ•è·ç»†å¾®å’Œç²¾ç»†ä¿¡æ¯çš„èƒ½åŠ›ã€‚OPALä½¿ç”¨è§†è§‰æŒ‡ä»¤è°ƒæ•´ç»“åˆç›´æ¥åå¥½ä¼˜åŒ–æ¥å¾®è°ƒMLLMï¼Œå‡å°‘äº†å¹»è§‰å¹¶æé«˜äº†ä¸åŒä¸»å¹²æ¶æ„çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨çœŸå®çš„ç”µå­å•†åŠ¡æ•°æ®é›†ä¸Šè¯„ä¼°äº†OPALçš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜ï¼Œå®ƒåœ¨æè¿°è´¨é‡å’Œæ¶æ„å®Œæˆç‡æ–¹é¢éƒ½å§‹ç»ˆä¼˜äºåŸºå‡†æ–¹æ³•ã€‚è¿™äº›ç»“æœè¯æ˜äº†OPALåœ¨å¼¥åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å·®è·æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæä¾›äº†æ›´ä¸°å¯Œã€æ›´å‡†ç¡®ã€æ›´ä¸€è‡´çš„å•†å“æè¿°ã€‚è¿™é¡¹å·¥ä½œæ¨åŠ¨äº†è‡ªåŠ¨åˆ—è¡¨ä¼˜åŒ–æ”¯æŒï¼Œå¹¶åœ¨ç”µå­å•†åŠ¡å¹³å°ä¸­å®ç°äº†å¯æ‰©å±•çš„é«˜è´¨é‡å†…å®¹ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10116v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å•†å“ä¿¡æ¯ï¼Œå¦‚æ ‡é¢˜å’Œå±æ€§ï¼Œå¯¹äºç”µå­å•†åŠ¡ä¸­çš„æœ‰æ•ˆç”¨æˆ·å‚ä¸è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨æˆ–åŠè‡ªåŠ¨è¾“å…¥ç»“æ„åŒ–å•†å“è¯¦æƒ…å¾€å¾€äº§ç”Ÿè´¨é‡ä¸ä¸€ã€é”™è¯¯è¾ƒå¤šå’Œå“åº”æ…¢çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å®¢æˆ·å¯¹å®¢æˆ·é”€å”®ä¸­ã€‚ç›´æ¥ä»å•†å“å›¾ç‰‡ç”Ÿæˆå‡†ç¡®æè¿°æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æå‡ºOptimized Preference-Based AI for Listings (OPAL)ï¼Œä¸€ä¸ªä½¿ç”¨å¾®è°ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»å›¾ç‰‡ç”Ÿæˆç¬¦åˆæ¶æ„è¦æ±‚çš„é«˜è´¨é‡å•†å“æè¿°çš„æ¡†æ¶ã€‚OPALè§£å†³äº†å¤šæ¨¡æ€ç”µå­å•†åŠ¡åº”ç”¨çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¼¥åˆæ¨¡æ€å·®è·å’Œæ•æ‰è¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å®ƒå¼•å…¥ä¸¤ç§æ•°æ®ä¼˜åŒ–æ–¹æ³•ï¼šMLLMè¾…åŠ©ä¸€è‡´æ€§å¢å¼ºï¼Œç¡®ä¿ä¸ç»“æ„åŒ–æ¶æ„è¦æ±‚çš„å¯¹é½ï¼›LLMè¾…åŠ©ä¸Šä¸‹æ–‡ç†è§£ï¼Œæé«˜ä»è§†è§‰è¾“å…¥ä¸­æ•æ‰ç»†å¾®å’Œç²¾ç»†ä¿¡æ¯çš„èƒ½åŠ›ã€‚OPALä½¿ç”¨è§†è§‰æŒ‡ä»¤è°ƒæ•´ä¸ç›´æ¥åå¥½ä¼˜åŒ–ç›¸ç»“åˆæ¥å¾®è°ƒMLLMï¼Œå‡å°‘å¹»è§‰å¹¶æé«˜ä¸åŒä¸»å¹²æ¶æ„çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨çœŸå®çš„ç”µå­å•†åŠ¡æ•°æ®é›†ä¸Šè¯„ä¼°OPALï¼Œæ˜¾ç¤ºå…¶åœ¨æè¿°è´¨é‡å’Œæ¶æ„å®Œæˆç‡æ–¹é¢å‡ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚è¿™äº›ç»“æœè¯æ˜OPALæœ‰æ•ˆåœ°å¼¥åˆäº†è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ä¹‹é—´çš„å·®è·ï¼Œæä¾›æ›´åŠ ä¸°å¯Œã€å‡†ç¡®å’Œä¸€è‡´çš„å•†å“æè¿°ã€‚è¿™é¡¹å·¥ä½œæ¨åŠ¨äº†è‡ªåŠ¨åˆ—è¡¨ä¼˜åŒ–ï¼Œå¹¶æ”¯æŒç”µå­å•†åŠ¡å¹³å°ä¸­çš„å¯æ‰©å±•å’Œé«˜å“è´¨å†…å®¹ç”Ÿæˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å•†å“ä¿¡æ¯å¯¹ç”¨æˆ·å‚ä¸åº¦è‡³å…³é‡è¦ï¼Œä½†æ‰‹åŠ¨æˆ–åŠè‡ªåŠ¨è¾“å…¥å­˜åœ¨è´¨é‡ä¸ä¸€ã€é”™è¯¯å’Œå“åº”æ…¢çš„é—®é¢˜ã€‚</li>
<li>ç›´æ¥ä»å•†å“å›¾ç‰‡ç”Ÿæˆæè¿°æä¾›äº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆã€‚</li>
<li>æå‡ºOPALæ¡†æ¶ï¼Œä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä»å›¾ç‰‡ç”Ÿæˆé«˜è´¨é‡å•†å“æè¿°ã€‚</li>
<li>OPALé€šè¿‡ä¸¤ç§æ•°æ®ä¼˜åŒ–æ–¹æ³•è§£å†³å¤šæ¨¡æ€åº”ç”¨ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>OPALä½¿ç”¨è§†è§‰æŒ‡ä»¤è°ƒæ•´ä¸åå¥½ä¼˜åŒ–ç»“åˆæ¥å¾®è°ƒæ¨¡å‹ï¼Œå‡å°‘å¹»è§‰å¹¶æé«˜ç¨³å¥æ€§ã€‚</li>
<li>åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºOPALåœ¨æè¿°è´¨é‡å’Œæ¶æ„å®Œæˆç‡æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10116">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d0d4827714887e3dbd3584f45def6df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b93de4a7015da65c627ad15c45427431.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d7a88c648a33742c3ac4991d2b7bacd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b3cc23d6d6e2f2fd8ace53386ec3e07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c20bd38badf4a0f9906bd008fa68ff6d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Exploring-Superior-Function-Calls-via-Reinforcement-Learning"><a href="#Exploring-Superior-Function-Calls-via-Reinforcement-Learning" class="headerlink" title="Exploring Superior Function Calls via Reinforcement Learning"></a>Exploring Superior Function Calls via Reinforcement Learning</h2><p><strong>Authors:Bingguang Hao, Maolin Wang, Zengzhuang Xu, Yicheng Chen, Cunyin Peng, Jinjie GU, Chenyi Zhuang</strong></p>
<p>Function calling capabilities are crucial for deploying Large Language Models in real-world applications, yet current training approaches fail to develop robust reasoning strategies. Supervised fine-tuning produces models that rely on superficial pattern matching, while standard reinforcement learning methods struggle with the complex action space of structured function calls. We present a novel reinforcement learning framework designed to enhance group relative policy optimization through strategic entropy based exploration specifically tailored for function calling tasks. Our approach addresses three critical challenges in function calling: insufficient exploration during policy learning, lack of structured reasoning in chain-of-thought generation, and inadequate verification of parameter extraction. Our two-stage data preparation pipeline ensures high-quality training samples through iterative LLM evaluation and abstract syntax tree validation. Extensive experiments on the Berkeley Function Calling Leaderboard demonstrate that this framework achieves state-of-the-art performance among open-source models with 86.02% overall accuracy, outperforming standard GRPO by up to 6% on complex multi-function scenarios. Notably, our method shows particularly strong improvements on code-pretrained models, suggesting that structured language generation capabilities provide an advantageous starting point for reinforcement learning in function calling tasks. We will release all the code, models and dataset to benefit the community. </p>
<blockquote>
<p>å‡½æ•°è°ƒç”¨èƒ½åŠ›å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œç„¶è€Œå½“å‰çš„è®­ç»ƒæ–¹æ³•æœªèƒ½å‘å±•å‡ºç¨³å¥çš„æ¨ç†ç­–ç•¥ã€‚ç›‘ç£å¾®è°ƒäº§ç”Ÿçš„æ¨¡å‹ä¾èµ–äºè‚¤æµ…çš„æ¨¡å¼åŒ¹é…ï¼Œè€Œæ ‡å‡†å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚çš„ç»“æ„åŒ–å‡½æ•°è°ƒç”¨åŠ¨ä½œç©ºé—´ä¸­æŒ£æ‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŸºäºç­–ç•¥ç†µçš„æ¢ç´¢å¢å¼ºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œè¯¥æ¡†æ¶ä¸“é—¨é’ˆå¯¹å‡½æ•°è°ƒç”¨ä»»åŠ¡è®¾è®¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†å‡½æ•°è°ƒç”¨ä¸­çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç­–ç•¥å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸è¶³ï¼Œæ€ç»´é“¾ç”Ÿæˆä¸­ç»“æ„åŒ–æ¨ç†çš„ç¼ºä¹ï¼Œä»¥åŠå‚æ•°æå–çš„éªŒè¯ä¸è¶³ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µæ•°æ®å‡†å¤‡ç®¡é“é€šè¿‡è¿­ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å’ŒæŠ½è±¡è¯­æ³•æ ‘éªŒè¯ï¼Œç¡®ä¿é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ã€‚åœ¨Berkeleyå‡½æ•°è°ƒç”¨æ’è¡Œæ¦œä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º86.02%ï¼Œåœ¨å¤æ‚çš„å¤šåŠŸèƒ½åœºæ™¯ä¸‹ï¼Œæ¯”æ ‡å‡†GRPOé«˜å‡º6%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä»£ç é¢„è®­ç»ƒæ¨¡å‹ä¸Šæ˜¾ç¤ºå‡ºç‰¹åˆ«å¼ºå¤§çš„æ”¹è¿›ï¼Œè¿™è¡¨æ˜ç»“æ„åŒ–è¯­è¨€ç”Ÿæˆèƒ½åŠ›ä¸ºå¼ºåŒ–å­¦ä¹ åœ¨å‡½æ•°è°ƒç”¨ä»»åŠ¡ä¸­æä¾›äº†ä¸€ä¸ªæœ‰åˆ©çš„èµ·ç‚¹ã€‚æˆ‘ä»¬å°†å‘å¸ƒæ‰€æœ‰çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ä»¥é€ ç¦ç¤¾åŒºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05118v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­éƒ¨ç½²ç°å®ä¸–ç•Œåº”ç”¨æ—¶çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›çš„é‡è¦æ€§ã€‚å½“å‰è®­ç»ƒæ–¹å¼æ— æ³•å‘å±•å‡ºç¨³å¥çš„æ¨ç†ç­–ç•¥ï¼Œç›‘ç£å¾®è°ƒäº§ç”Ÿçš„æ¨¡å‹ä¾èµ–äºè¡¨é¢æ¨¡å¼åŒ¹é…ï¼Œè€Œä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚çš„å‡½æ•°è°ƒç”¨åŠ¨ä½œç©ºé—´ä¸­è¡¨ç°ä¸ä½³ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åŸºäºæˆ˜ç•¥ç†µçš„æ¢ç´¢å¢å¼ºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å‡½æ•°è°ƒç”¨ä»»åŠ¡ã€‚è¯¥æ–¹æ³•è§£å†³äº†å‡½æ•°è°ƒç”¨ä¸­çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç­–ç•¥å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸è¶³ã€æ€ç»´é“¾ç”Ÿæˆä¸­ç»“æ„åŒ–æ¨ç†çš„ç¼ºä¹ä»¥åŠå‚æ•°æå–éªŒè¯çš„ä¸è¶³ã€‚é€šè¿‡ä¸¤é˜¶æ®µæ•°æ®å‡†å¤‡ç®¡é“ç¡®ä¿é«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ï¼Œé€šè¿‡è¿­ä»£LLMè¯„ä¼°å’ŒæŠ½è±¡è¯­æ³•æ ‘éªŒè¯ã€‚åœ¨Berkeleyå‡½æ•°è°ƒç”¨æ’è¡Œæ¦œä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ€»ä½“å‡†ç¡®åº¦è¾¾åˆ°86.02%ï¼Œåœ¨å¤æ‚çš„å¤šåŠŸèƒ½åœºæ™¯ä¸‹å°†æ ‡å‡†GRPOçš„å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾6%ã€‚ç‰¹åˆ«æ˜¯åœ¨ä»£ç é¢„è®­ç»ƒæ¨¡å‹ä¸Šçš„æ”¹è¿›å°¤ä¸ºæ˜¾è‘—ï¼Œè¡¨æ˜ç»“æ„åŒ–è¯­è¨€ç”Ÿæˆèƒ½åŠ›ä¸ºå‡½æ•°è°ƒç”¨çš„å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ä¸ªæœ‰åˆ©çš„èµ·ç‚¹ã€‚æˆ‘ä»¬å°†å‘å¸ƒæ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†ä»¥é€ ç¦ç¤¾åŒºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰è®­ç»ƒæ–¹å¼åœ¨å¼€å‘LLMçš„ç¨³å¥æ¨ç†ç­–ç•¥æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>ç›‘ç£å¾®è°ƒäº§ç”Ÿçš„æ¨¡å‹ä¾èµ–äºè¡¨é¢æ¨¡å¼åŒ¹é…ã€‚</li>
<li>ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚çš„å‡½æ•°è°ƒç”¨åŠ¨ä½œç©ºé—´ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æ–°å¼ºåŒ–å­¦ä¹ æ¡†æ¶é€šè¿‡æˆ˜ç•¥ç†µæ¢ç´¢å¢å¼ºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œä¸“é—¨ç”¨äºå‡½æ•°è°ƒç”¨ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†å‡½æ•°è°ƒç”¨ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¢ç´¢ä¸è¶³ã€ç»“æ„åŒ–æ¨ç†ç¼ºä¹å’Œå‚æ•°éªŒè¯ä¸è¶³ã€‚</li>
<li>åœ¨Berkeleyå‡½æ•°è°ƒç”¨æ’è¡Œæ¦œä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ€»ä½“å‡†ç¡®åº¦è¾¾åˆ°86.02%ï¼Œå¤æ‚åœºæ™¯ä¸‹è¡¨ç°å°¤å…¶å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4123c5f39102d26a83d95165c011a54f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1253f26143895775e96cfd8eae0001db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33a410925db6fd9f09c5a0eb4ef94157.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-809d9b45f96bf6d6b740e919b9119531.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Diffusion-Beats-Autoregressive-in-Data-Constrained-Settings"><a href="#Diffusion-Beats-Autoregressive-in-Data-Constrained-Settings" class="headerlink" title="Diffusion Beats Autoregressive in Data-Constrained Settings"></a>Diffusion Beats Autoregressive in Data-Constrained Settings</h2><p><strong>Authors:Mihir Prabhudesai, Mengning Wu, Amir Zadeh, Katerina Fragkiadaki, Deepak Pathak</strong></p>
<p>Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings-where training involves repeated passes over limited data and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We find new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. Finally, we explain why diffusion models excel in this regime: their randomized masking objective implicitly trains over a rich distribution of token orderings, acting as an implicit data augmentation that ARâ€™s fixed left-to-right factorization lacks. Our results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. Our code is available at: <a target="_blank" rel="noopener" href="https://diffusion-scaling.github.io/">https://diffusion-scaling.github.io</a>. </p>
<blockquote>
<p>è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹é•¿æœŸä»¥æ¥ä¸€ç›´åœ¨å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ï¼Œåœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸­æ¨åŠ¨è¿›å±•ã€‚æœ€è¿‘ï¼ŒåŸºäºæ‰©æ•£çš„è¯­è¨€æ¨¡å‹ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ³•è€Œå‡ºç°ï¼Œå°½ç®¡å®ƒä»¬ç›¸å¯¹äºARæ¨¡å‹çš„ä¼˜åŠ¿ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†æ•°æ®å—é™ç¯å¢ƒä¸­æ©è”½æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹æ¶‰åŠåœ¨æœ‰é™æ•°æ®ä¸Šå¤šæ¬¡è¿­ä»£ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨ç®—åŠ›å……è¶³ä½†æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬æ˜¾è‘—ä¼˜äºè‡ªå›å½’æ¨¡å‹ã€‚æ‰©æ•£æ¨¡å‹èƒ½æ›´å¥½åœ°åˆ©ç”¨é‡å¤æ•°æ®ï¼Œè¾¾åˆ°æ›´ä½çš„éªŒè¯æŸå¤±å’Œæ›´é«˜çš„ä¸‹æ¸¸æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°äº†æ‰©æ•£æ¨¡å‹çš„æ–°æ‰©å±•å®šå¾‹ï¼Œå¹¶æ¨å¯¼å‡ºä¸´ç•Œè®¡ç®—é˜ˆå€¼çš„é—­å¼è¡¨è¾¾å¼ï¼Œåœ¨è¯¥é˜ˆå€¼ä¸‹ï¼Œæ‰©æ•£å¼€å§‹ä¼˜äºARã€‚æœ€åï¼Œæˆ‘ä»¬è§£é‡Šäº†ä¸ºä»€ä¹ˆåœ¨è¿™ç§ç¯å¢ƒä¸­æ‰©æ•£æ¨¡å‹è¡¨ç°å“è¶Šï¼šå®ƒä»¬çš„éšæœºæ©è”½ç›®æ ‡éšå¼åœ°åœ¨ä¸°å¯Œçš„æ ‡è®°é¡ºåºåˆ†å¸ƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œå……å½“äº†ä¸€ç§éšå¼æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œè€ŒARçš„å›ºå®šä»å·¦åˆ°å³çš„åˆ†è§£åˆ™ç¼ºä¹è¿™ä¸€ç‰¹ç‚¹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“æ•°æ®æˆä¸ºç“¶é¢ˆè€Œä¸æ˜¯ç®—åŠ›æ—¶ï¼Œæ‰©æ•£æ¨¡å‹æ˜¯æ ‡å‡†ARèŒƒå¼çš„æœ‰å¸å¼•åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨ï¼š<a target="_blank" rel="noopener" href="https://diffusion-scaling.github.io/">https://diffusion-scaling.github.io</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15857v6">PDF</a> Project Webpage: <a target="_blank" rel="noopener" href="https://diffusion-scaling.github.io/">https://diffusion-scaling.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ•°æ®å—é™ç¯å¢ƒä¸‹æ©ç æ‰©æ•£æ¨¡å‹ç›¸è¾ƒäºè‡ªå›å½’æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨ç®—åŠ›å……è¶³ä½†æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œæ‰©æ•£æ¨¡å‹æ˜¾è‘—ä¼˜äºè‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åˆ©ç”¨é‡å¤æ•°æ®ï¼Œå®ç°æ›´ä½çš„éªŒè¯æŸå¤±å’Œæ›´ä¼˜çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚æ–‡ç« è¿˜æ­ç¤ºäº†æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½éšç€è®¡ç®—é‡çš„å˜åŒ–éµå¾ªç‰¹å®šçš„è§„å¾‹ï¼Œå¹¶ç»™å‡ºäº†è®¡ç®—é˜ˆå€¼çš„è¡¨è¾¾å¼ã€‚æ­¤å¤–ï¼Œæ‰©æ•£æ¨¡å‹ä¹‹æ‰€ä»¥èƒ½åœ¨è¿™æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œæ˜¯å› ä¸ºå…¶éšæœºæ©ç ç›®æ ‡èƒ½å¤Ÿéšå¼åœ°è®­ç»ƒå„ç§ä¸°å¯Œçš„å•è¯é¡ºåºåˆ†å¸ƒï¼Œè€Œè¿™æ˜¯è‡ªå›å½’æ¨¡å‹ç¼ºä¹çš„å·¦è‡³å³å›ºå®šåˆ†è§£æ‰€æ— æ³•æ¯”æ‹Ÿçš„ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå½“æ•°æ®æˆä¸ºç“¶é¢ˆè€Œéç®—åŠ›æ—¶ï¼Œæ‰©æ•£æ¨¡å‹æä¾›äº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ•°æ®å—é™ç¯å¢ƒä¸‹è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>åœ¨ç®—åŠ›å……è¶³ä½†æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œæ‰©æ•£æ¨¡å‹ä¼˜äºè‡ªå›å½’æ¨¡å‹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½æ›´å¥½åœ°åˆ©ç”¨é‡å¤æ•°æ®ï¼Œå®ç°æ›´ä½çš„éªŒè¯æŸå¤±å’Œæ›´ä¼˜çš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½éšè®¡ç®—é‡çš„å˜åŒ–éµå¾ªç‰¹å®šè§„å¾‹ï¼Œå­˜åœ¨è®¡ç®—é˜ˆå€¼ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„éšæœºæ©ç ç›®æ ‡éšå¼åœ°è®­ç»ƒå„ç§å•è¯é¡ºåºåˆ†å¸ƒï¼Œè¿™æ˜¯è‡ªå›å½’æ¨¡å‹æ‰€ç¼ºä¹çš„ã€‚</li>
<li>æ•°æ®æˆä¸ºç“¶é¢ˆæ—¶ï¼Œæ‰©æ•£æ¨¡å‹æˆä¸ºäº†ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e7a3313d26b4cf7f9a07ca9b4d27a8fd.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Omni-DPO-A-Dual-Perspective-Paradigm-for-Dynamic-Preference-Learning-of-LLMs"><a href="#Omni-DPO-A-Dual-Perspective-Paradigm-for-Dynamic-Preference-Learning-of-LLMs" class="headerlink" title="Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of   LLMs"></a>Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of   LLMs</h2><p><strong>Authors:Shangpin Peng, Weinong Wang, Zhuotao Tian, Senqiao Yang, Xing Wu, Haotian Xu, Chengquan Zhang, Takashi Isobe, Baotian Hu, Min Zhang</strong></p>
<p>Direct Preference Optimization (DPO) has become a cornerstone of reinforcement learning from human feedback (RLHF) due to its simplicity and efficiency. However, existing DPO-based approaches typically treat all preference pairs uniformly, ignoring critical variations in their inherent quality and learning utility, leading to suboptimal data utilization and performance. To address this challenge, we propose Omni-DPO, a dual-perspective optimization framework that jointly accounts for (1) the inherent quality of each preference pair and (2) the modelâ€™s evolving performance on those pairs. By adaptively weighting samples according to both data quality and the modelâ€™s learning dynamics during training, Omni-DPO enables more effective training data utilization and achieves better performance. Experimental results on various models and benchmarks demonstrate the superiority and generalization capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning tasks, Omni-DPO consistently outperforms the baseline methods across all benchmarks, providing strong empirical evidence for the effectiveness and robustness of our approach. Code and models will be available at <a target="_blank" rel="noopener" href="https://github.com/pspdada/Omni-DPO">https://github.com/pspdada/Omni-DPO</a>. </p>
<blockquote>
<p>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å› å…¶ç®€å•æ€§å’Œé«˜æ•ˆæ€§å·²æˆä¸ºäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„æ ¸å¿ƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºDPOçš„æ–¹æ³•é€šå¸¸å¯¹æ‰€æœ‰åå¥½å¯¹ä¸€è§†åŒä»ï¼Œå¿½ç•¥äº†å®ƒä»¬å†…åœ¨è´¨é‡å’Œå­¦ä¹ æ•ˆç”¨ä¸­çš„å…³é”®å·®å¼‚ï¼Œå¯¼è‡´æ•°æ®åˆ©ç”¨å’Œæ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Omni-DPOï¼Œè¿™æ˜¯ä¸€ä¸ªåŒè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼ŒåŒæ—¶è€ƒè™‘ï¼ˆ1ï¼‰æ¯ä¸ªåå¥½å¯¹çš„å†…åœ¨è´¨é‡å’Œï¼ˆ2ï¼‰æ¨¡å‹åœ¨è¿™äº›å¯¹ä¸Šçš„ä¸æ–­å˜åŒ–çš„æ€§èƒ½ã€‚Omni-DPOé€šè¿‡æ ¹æ®æ•°æ®è´¨é‡å’Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ åŠ¨æ€æ¥è‡ªé€‚åº”åœ°åŠ æƒæ ·æœ¬ï¼Œä»è€Œå®ç°äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®åˆ©ç”¨å’Œæ›´å¥½çš„æ€§èƒ½ã€‚åœ¨å„ç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜äº†Omni-DPOçš„ä¼˜è¶Šæ€§å’Œé€šç”¨æ€§ã€‚åœ¨æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨Omni-DPOå¾®è°ƒè¿‡çš„Gemma-2-9b-itåœ¨Arena-HardåŸºå‡†æµ‹è¯•ä¸Šå¤§å¹…é¢†å…ˆé¢†å…ˆLLM Claude 3 Opusï¼Œé¢†å…ˆäº†6.7åˆ†ã€‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ŒOmni-DPOåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šå‡ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œä¸ºæˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§æä¾›äº†æœ‰åŠ›çš„å®è¯è¯æ®ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/pspdada/Omni-DPO%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/pspdada/Omni-DPOä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10054v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¼˜åŒ–ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­çš„æ ¸å¿ƒåœ°ä½ï¼Œå› ä¸ºå®ƒç®€å•é«˜æ•ˆã€‚ç„¶è€Œï¼Œç°æœ‰DPOæ–¹æ³•å¿½ç•¥åå¥½å¯¹æœ¬èº«çš„è´¨é‡å’Œå­¦ä¹ çš„å®ç”¨æ€§å·®å¼‚ï¼Œå¯¼è‡´æ•°æ®åˆ©ç”¨ä¸ä½³å’Œæ€§èƒ½ä¸‹é™ã€‚ä¸ºåº”å¯¹æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºOmni-DPOï¼Œä¸€ä¸ªåŒè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼ŒåŒæ—¶è€ƒè™‘åå¥½å¯¹çš„å†…åœ¨è´¨é‡å’Œæ¨¡å‹åœ¨è¿™äº›åå¥½å¯¹ä¸Šçš„æ€§èƒ½å˜åŒ–ã€‚é€šè¿‡æ ¹æ®æ•°æ®è´¨é‡å’Œæ¨¡å‹å­¦ä¹ åŠ¨æ€åœ¨è®­ç»ƒä¸­è‡ªé€‚åº”åœ°åŠ æƒæ ·æœ¬ï¼ŒOmni-DPOæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®­ç»ƒæ•°æ®å¹¶å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚å®éªŒè¯æ˜Omni-DPOçš„ä¼˜è¶Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨Omni-DPOå¾®è°ƒåçš„Gemma-2-9b-itåœ¨Arena-HardåŸºå‡†æµ‹è¯•ä¸­å¤§å¹…è¶…è¶Šé¢†å…ˆçš„LLM Claude 3 Opusã€‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ŒOmni-DPOåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œä¸ºæˆ‘ä»¬çš„æ–¹æ³•å’Œç¨³å¥æ€§æä¾›äº†å¼ºæœ‰åŠ›çš„å®è¯è¯æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­çš„æ ¸å¿ƒæ–¹æ³•ï¼Œä½†ç°æœ‰æ–¹æ³•å¿½ç•¥åå¥½å¯¹çš„è´¨é‡å’Œå­¦ä¹ çš„å®ç”¨æ€§å·®å¼‚ã€‚</li>
<li>Omni-DPOæ˜¯ä¸€ä¸ªåŒè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼Œè€ƒè™‘åå¥½å¯¹çš„å†…åœ¨è´¨é‡å’Œæ¨¡å‹æ€§èƒ½å˜åŒ–ã€‚</li>
<li>Omni-DPOé€šè¿‡è‡ªé€‚åº”åŠ æƒæ ·æœ¬å®ç°æ›´æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®åˆ©ç”¨ã€‚</li>
<li>å®éªŒè¯æ˜Omni-DPOåœ¨æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸Šè¶…è¶Šé¢†å…ˆçš„LLMã€‚</li>
<li>Omni-DPOåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šä¼˜äºæ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­çš„åŸºå‡†æ–¹æ³•ã€‚</li>
<li>Omni-DPOæ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5f6f80e1e7333c65abbcc512cd4c4be0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92768d16160a9b46a78802d60fce2959.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79ad950d6f31bded15a91cbddd0da112.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6604a16813e11a35aa5efba4ebb3208f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-169f8e5805d16b63539c2e6567304608.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Relationship-Detection-on-Tabular-Data-Using-Statistical-Analysis-and-Large-Language-Models"><a href="#Relationship-Detection-on-Tabular-Data-Using-Statistical-Analysis-and-Large-Language-Models" class="headerlink" title="Relationship Detection on Tabular Data Using Statistical Analysis and   Large Language Models"></a>Relationship Detection on Tabular Data Using Statistical Analysis and   Large Language Models</h2><p><strong>Authors:Panagiotis Koletsis, Christos Panagiotopoulos, Georgios Th. Papadopoulos, Vasilis Efthymiou</strong></p>
<p>Over the past few years, table interpretation tasks have made significant progress due to their importance and the introduction of new technologies and benchmarks in the field. This work experiments with a hybrid approach for detecting relationships among columns of unlabeled tabular data, using a Knowledge Graph (KG) as a reference point, a task known as CPA. This approach leverages large language models (LLMs) while employing statistical analysis to reduce the search space of potential KG relations. The main modules of this approach for reducing the search space are domain and range constraints detection, as well as relation co-appearance analysis. The experimental evaluation on two benchmark datasets provided by the SemTab challenge assesses the influence of each module and the effectiveness of different state-of-the-art LLMs at various levels of quantization. The experiments were performed, as well as at different prompting techniques. The proposed methodology, which is publicly available on github, proved to be competitive with state-of-the-art approaches on these datasets. </p>
<blockquote>
<p>è¿‡å»å‡ å¹´ï¼Œç”±äºè¡¨æ ¼è§£è¯»ä»»åŠ¡çš„é‡è¦æ€§ä»¥åŠè¯¥é¢†åŸŸæ–°æŠ€æœ¯çš„å¼•å…¥å’ŒåŸºå‡†æµ‹è¯•çš„å‘å±•ï¼Œå…¶å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ¬å·¥ä½œå°è¯•ä½¿ç”¨æ··åˆæ–¹æ³•æ£€æµ‹æ— æ ‡ç­¾è¡¨æ ¼æ•°æ®åˆ—ä¹‹é—´çš„å…³ç³»ï¼Œä»¥çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸ºå‚è€ƒç‚¹ï¼Œè¿™é¡¹ä»»åŠ¡è¢«ç§°ä¸ºCPAã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒåŒæ—¶é‡‡ç”¨ç»Ÿè®¡åˆ†ææ¥å‡å°‘çŸ¥è¯†å›¾è°±æ½œåœ¨å…³ç³»çš„æœç´¢ç©ºé—´ã€‚è¯¥æ–¹æ³•å‡å°‘æœç´¢ç©ºé—´çš„ä¸»è¦æ¨¡å—æ˜¯åŸŸå’ŒèŒƒå›´çº¦æŸæ£€æµ‹ï¼Œä»¥åŠå…³ç³»å…±ç°åˆ†æã€‚åœ¨SemTabæŒ‘æˆ˜æä¾›çš„ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯„ä¼°äº†æ¯ä¸ªæ¨¡å—çš„å½±å“ä»¥åŠåœ¨å„çº§é‡åŒ–ä¸‹ä¸åŒæœ€å…ˆè¿›LLMsçš„æœ‰æ•ˆæ€§ã€‚å®éªŒè¿˜é‡‡ç”¨äº†ä¸åŒçš„æç¤ºæŠ€æœ¯ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ï¼Œåœ¨è¿™äº›æ•°æ®é›†ä¸Šè¢«è¯æ˜ä¸æœ€å…ˆè¿›çš„æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06371v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‡å»å‡ å¹´ï¼Œç”±äºè¡¨æ ¼è§£è¯»ä»»åŠ¡çš„é‡è¦æ€§ä»¥åŠæ–°æŠ€å’ŒåŸºå‡†æµ‹è¯•çš„å‡ºç°ï¼Œè¯¥é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ¬æ–‡å°è¯•åˆ©ç”¨çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä½œä¸ºå‚ç…§ç‚¹ï¼Œé‡‡ç”¨ä¸€ç§æ··åˆæ–¹æ³•æ£€æµ‹æ— æ ‡ç­¾è¡¨æ ¼æ•°æ®åˆ—ä¹‹é—´çš„å…³ç³»ï¼Œè¿™è¢«ç§°ä¸ºCPAä»»åŠ¡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¶é‡‡ç”¨ç»Ÿè®¡åˆ†ææ¥ç¼©å°æ½œåœ¨KGå…³ç³»çš„æœç´¢èŒƒå›´ã€‚ç¼©å°æœç´¢èŒƒå›´çš„ä¸»è¦æ¨¡å—åŒ…æ‹¬åŸŸå’ŒèŒƒå›´çº¦æŸæ£€æµ‹ä»¥åŠå…³ç³»å…±ç°åˆ†æã€‚åœ¨SemTabæŒ‘æˆ˜æä¾›çš„ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯„ä¼°äº†æ¯ä¸ªæ¨¡å—çš„å½±å“ä»¥åŠä¸åŒå…ˆè¿›LLMåœ¨ä¸åŒé‡åŒ–æ°´å¹³ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠä¸åŒçš„æç¤ºæŠ€æœ¯ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨å…¬å…±GitHubä¸Šè¯æ˜å…¶åœ¨è¿™äº›æ•°æ®é›†ä¸Šå…·æœ‰ç«äº‰åŠ›ã€‚ </p>
<p><strong>è¦ç‚¹åˆ†æ</strong></p>
<ul>
<li>è¡¨æ ¼è§£è¯»ä»»åŠ¡å› é‡è¦æ€§å’Œæ–°æŠ€æœ¯çš„å¼•å…¥åœ¨è¿‡å»å‡ å¹´å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>æœ¬æ–‡å°è¯•åˆ©ç”¨çŸ¥è¯†å›¾è°±ä½œä¸ºå‚ç…§ç‚¹ï¼Œé‡‡ç”¨æ··åˆæ–¹æ³•æ£€æµ‹æ— æ ‡ç­¾è¡¨æ ¼æ•°æ®åˆ—ä¹‹é—´çš„å…³ç³»ï¼Œè¢«ç§°ä¸ºCPAä»»åŠ¡ã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œç»Ÿè®¡åˆ†æï¼Œæ—¨åœ¨ç¼©å°æ½œåœ¨å…³ç³»çš„æœç´¢èŒƒå›´ã€‚</li>
<li>åŸŸå’ŒèŒƒå›´çº¦æŸæ£€æµ‹ä»¥åŠå…³ç³»å…±ç°åˆ†ææ˜¯è¯¥æ–¹æ³•çš„ä¸»è¦æ¨¡å—ï¼Œç”¨äºç¼©å°æœç´¢èŒƒå›´ã€‚</li>
<li>åœ¨SemTabæŒ‘æˆ˜æä¾›çš„ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé‡åŒ–æ°´å¹³ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b9a6a7035c5422f65206ec1411b7620f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cff71d1bc717cbf39d6415540774f4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67b194b8db6cbdf40ec39a4377287b7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1453e6a9e4cacdec33b3fce9eb8440fd.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Data-Pruning-by-Information-Maximization"><a href="#Data-Pruning-by-Information-Maximization" class="headerlink" title="Data Pruning by Information Maximization"></a>Data Pruning by Information Maximization</h2><p><strong>Authors:Haoru Tan, Sitong Wu, Wei Huang, Shizhen Zhao, Xiaojuan Qi</strong></p>
<p>In this paper, we present InfoMax, a novel data pruning method, also known as coreset selection, designed to maximize the information content of selected samples while minimizing redundancy. By doing so, InfoMax enhances the overall informativeness of the coreset. The information of individual samples is measured by importance scores, which capture their influence or difficulty in model learning. To quantify redundancy, we use pairwise sample similarities, based on the premise that similar samples contribute similarly to the learning process. We formalize the coreset selection problem as a discrete quadratic programming (DQP) task, with the objective of maximizing the total information content, represented as the sum of individual sample contributions minus the redundancies introduced by similar samples within the coreset. To ensure practical scalability, we introduce an efficient gradient-based solver, complemented by sparsification techniques applied to the similarity matrix and dataset partitioning strategies. This enables InfoMax to seamlessly scale to datasets with millions of samples. Extensive experiments demonstrate the superior performance of InfoMax in various data pruning tasks, including image classification, vision-language pre-training, and instruction tuning for large language models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hrtan/InfoMax">https://github.com/hrtan/InfoMax</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ•°æ®ä¿®å‰ªæ–¹æ³•InfoMaxï¼Œä¹Ÿç§°ä¸ºæ ¸å¿ƒé›†é€‰æ‹©ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æœ€å¤§åŒ–æ‰€é€‰æ ·æœ¬çš„ä¿¡æ¯å†…å®¹çš„åŒæ—¶æœ€å°åŒ–å†—ä½™ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒInfoMaxæé«˜äº†æ ¸å¿ƒé›†çš„æ•´ä½“ä¿¡æ¯æ€§ã€‚æˆ‘ä»¬é€šè¿‡é‡è¦æ€§åˆ†æ•°æ¥è¡¡é‡å•ä¸ªæ ·æœ¬çš„ä¿¡æ¯ï¼Œé‡è¦æ€§åˆ†æ•°åæ˜ äº†æ ·æœ¬åœ¨æ¨¡å‹å­¦ä¹ ä¸­çš„å½±å“åŠ›æˆ–éš¾åº¦ã€‚ä¸ºäº†é‡åŒ–å†—ä½™ï¼Œæˆ‘ä»¬åŸºäºç›¸ä¼¼æ ·æœ¬å¯¹å­¦ä¹ è¿‡ç¨‹äº§ç”Ÿç›¸ä¼¼è´¡çŒ®çš„å‡è®¾ï¼Œä½¿ç”¨æˆå¯¹çš„æ ·æœ¬ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬å°†æ ¸å¿ƒé›†é€‰æ‹©é—®é¢˜å½¢å¼åŒ–ä¸ºç¦»æ•£äºŒæ¬¡è§„åˆ’ï¼ˆDQPï¼‰ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–æ€»ä¿¡æ¯å†…å®¹ï¼Œè¡¨ç¤ºä¸ºå•ä¸ªæ ·æœ¬è´¡çŒ®çš„æ€»å’Œå‡å»æ ¸å¿ƒé›†ä¸­ç›¸ä¼¼æ ·æœ¬å¼•èµ·çš„å†—ä½™ã€‚ä¸ºäº†ç¡®ä¿å®é™…çš„å¯æ‰©å±•æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„åŸºäºæ¢¯åº¦çš„æ±‚è§£å™¨ï¼Œå¹¶ç»“åˆç›¸ä¼¼æ€§çŸ©é˜µçš„ç¨€ç–åŒ–æŠ€æœ¯å’Œæ•°æ®é›†åˆ†åŒºç­–ç•¥ã€‚è¿™ä½¿å¾—InfoMaxèƒ½å¤Ÿæ— ç¼åœ°æ‰©å±•åˆ°å…·æœ‰æ•°ç™¾ä¸‡æ ·æœ¬çš„æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒInfoMaxåœ¨å„ç§æ•°æ®ä¿®å‰ªä»»åŠ¡ä¸­çš„æ€§èƒ½å“è¶Šï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€è§†è§‰è¯­è¨€é¢„è®­ç»ƒå’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/hrtan/InfoMax%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hrtan/InfoMaxè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01701v2">PDF</a> Code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/hrtan/InfoMax%7D">https://github.com/hrtan/InfoMax}</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†InfoMaxï¼Œä¸€ç§æ–°å‹æ•°æ®è£å‰ªæ–¹æ³•ï¼ˆä¹Ÿç§°ä¸ºæ ¸å¿ƒé›†é€‰æ‹©ï¼‰ã€‚è¯¥æ–¹æ³•æ—¨åœ¨æœ€å¤§åŒ–é€‰å®šæ ·æœ¬çš„ä¿¡æ¯å†…å®¹ï¼ŒåŒæ—¶æœ€å°åŒ–å†—ä½™ï¼Œä»è€Œæé«˜æ ¸å¿ƒé›†çš„æ•´ä½“ä¿¡æ¯é‡ã€‚é€šè¿‡é‡è¦æ€§åˆ†æ•°æ¥è¡¡é‡å•ä¸ªæ ·æœ¬çš„ä¿¡æ¯é‡ï¼Œè¿™äº›åˆ†æ•°åæ˜ äº†æ¨¡å‹å­¦ä¹ ä¸­æ ·æœ¬çš„å½±å“æˆ–éš¾åº¦ã€‚ä¸ºäº†é‡åŒ–å†—ä½™ï¼Œæ–‡ç« ä½¿ç”¨åŸºäºæ ·æœ¬ç›¸ä¼¼æ€§çš„æˆå¯¹æ ·æœ¬ç›¸ä¼¼æ€§ã€‚æ ¸å¿ƒé›†é€‰æ‹©é—®é¢˜è¢«å½¢å¼åŒ–ä¸ºç¦»æ•£äºŒæ¬¡è§„åˆ’ï¼ˆDQPï¼‰ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–æ€»ä¿¡æ¯å†…å®¹ï¼Œè¡¨ç¤ºä¸ºå•ä¸ªæ ·æœ¬è´¡çŒ®çš„æ€»å’Œå‡å»æ ¸å¿ƒé›†ä¸­ç›¸ä¼¼æ ·æœ¬å¼•å…¥çš„å†—ä½™ã€‚ä¸ºç¡®ä¿å®é™…çš„å¯æ‰©å±•æ€§ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„åŸºäºæ¢¯åº¦çš„æ±‚è§£å™¨ï¼Œå¹¶ç»“åˆç›¸ä¼¼æ€§çŸ©é˜µçš„ç¨€ç–åŒ–æŠ€æœ¯å’Œæ•°æ®é›†åˆ†åŒºç­–ç•¥ã€‚è¿™ä½¿å¾—InfoMaxèƒ½å¤Ÿæ— ç¼åœ°æ‰©å±•åˆ°æ•°ç™¾ä¸‡æ ·æœ¬çš„æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒInfoMaxåœ¨å„ç§æ•°æ®è£å‰ªä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€è§†è§‰è¯­è¨€é¢„è®­ç»ƒå’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>InfoMaxæ˜¯ä¸€ç§æ•°æ®è£å‰ªæ–¹æ³•ï¼Œæ—¨åœ¨æœ€å¤§åŒ–é€‰å®šæ ·æœ¬çš„ä¿¡æ¯å†…å®¹å¹¶æœ€å°åŒ–å†—ä½™ã€‚</li>
<li>é€šè¿‡é‡è¦æ€§åˆ†æ•°è¡¡é‡å•ä¸ªæ ·æœ¬çš„ä¿¡æ¯é‡ï¼Œåæ˜ æ¨¡å‹å­¦ä¹ ä¸­çš„å½±å“æˆ–éš¾åº¦ã€‚</li>
<li>ä½¿ç”¨åŸºäºæ ·æœ¬ç›¸ä¼¼æ€§çš„æˆå¯¹æ ·æœ¬ç›¸ä¼¼æ€§æ¥é‡åŒ–å†—ä½™ã€‚</li>
<li>æ ¸å¿ƒé›†é€‰æ‹©é—®é¢˜è¢«å½¢å¼åŒ–ä¸ºç¦»æ•£äºŒæ¬¡è§„åˆ’ï¼ˆDQPï¼‰ä»»åŠ¡ã€‚</li>
<li>InfoMaxé‡‡ç”¨é«˜æ•ˆçš„åŸºäºæ¢¯åº¦çš„æ±‚è§£å™¨ï¼Œå¯æ‰©å±•åˆ°æ•°ç™¾ä¸‡æ ·æœ¬çš„æ•°æ®é›†ã€‚</li>
<li>å®éªŒä¸­ï¼ŒInfoMaxåœ¨å›¾åƒåˆ†ç±»ã€è§†è§‰è¯­è¨€é¢„è®­ç»ƒå’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒæ•´ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01701">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2907c49f09d21b15361f6acdae21be9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a4d039207b762b2ab12bc0cd721c602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8cda594d137a1c4b1e68fdae1d41222.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b186974a60fadef19bee20652a606c78.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling"><a href="#Curse-of-High-Dimensionality-Issue-in-Transformer-for-Long-context-Modeling" class="headerlink" title="Curse of High Dimensionality Issue in Transformer for Long-context   Modeling"></a>Curse of High Dimensionality Issue in Transformer for Long-context   Modeling</h2><p><strong>Authors:Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan</strong></p>
<p>Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at <a target="_blank" rel="noopener" href="https://github.com/bolixinyu/DynamicGroupAttention">https://github.com/bolixinyu/DynamicGroupAttention</a>. </p>
<blockquote>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”±äºå†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡é¢ä¸´é‡å¤§çš„è®¡ç®—æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼šè™½ç„¶æ³¨æ„åŠ›æƒé‡é€šå¸¸æ˜¯ç¨€ç–çš„ï¼Œä½†æ‰€æœ‰ä»¤ç‰Œéƒ½æ¶ˆè€—ç€å¹³ç­‰çš„è®¡ç®—èµ„æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä¼ ç»Ÿçš„æ¦‚ç‡åºåˆ—å»ºæ¨¡é‡æ–°å®šä¹‰ä¸ºâ€œç›‘ç£å­¦ä¹ ä»»åŠ¡â€ï¼Œè¿™èƒ½å¤Ÿåˆ†ç¦»ç›¸å…³å’Œä¸ç›¸å…³çš„ä»¤ç‰Œï¼Œå¹¶æä¾›å¯¹å†—ä½™æ€§çš„æ›´æ¸…æ™°ç†è§£ã€‚åŸºäºè¿™ç§é‡æ–°è¡¨è¿°ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šåˆ†æäº†æ³¨æ„åŠ›ç¨€ç–æ€§ï¼Œæ­ç¤ºåªæœ‰å°‘æ•°ä»¤ç‰Œå¯¹é¢„æµ‹åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›ä¼˜åŒ–è¡¨è¿°ä¸ºçº¿æ€§ç¼–ç é—®é¢˜ï¼Œå¹¶æå‡ºâ€œåˆ†ç»„ç¼–ç ç­–ç•¥â€ï¼Œä»ç†è®ºä¸Šè¯æ˜äº†å…¶æé«˜å¯¹æŠ—éšæœºå™ªå£°çš„ç¨³å¥æ€§å’Œæé«˜å­¦ä¹ æ•ˆç‡çš„èƒ½åŠ›ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†â€œåŠ¨æ€ç»„æ³¨æ„åŠ›â€ï¼ˆDGAï¼‰ï¼Œå®ƒåˆ©ç”¨åˆ†ç»„ç¼–ç æ¥é€šè¿‡èšåˆä¸å¤ªé‡è¦çš„ä»¤ç‰Œæ¥æ˜ç¡®å‡å°‘å†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DGAåœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bolixinyu/DynamicGroupAttention%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bolixinyu/DynamicGroupAttentionæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22107v4">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”±äºå†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œé•¿ä¸Šä¸‹æ–‡å»ºæ¨¡é¢ä¸´é‡å¤§çš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚æœ¬æ–‡é‡æ–°å®šä¹‰äº†æ¦‚ç‡åºåˆ—å»ºæ¨¡ä¸ºç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼ŒåŒºåˆ†äº†ç›¸å…³å’Œä¸ç›¸å…³çš„ä»¤ç‰Œï¼Œæ˜ç¡®äº†å†—ä½™çš„æ¦‚å¿µã€‚åŸºäºç†è®ºåˆ†æï¼Œæˆ‘ä»¬å‘ç°åªæœ‰å°‘æ•°ä»¤ç‰Œå¯¹é¢„æµ‹æœ‰æ˜¾è‘—è´¡çŒ®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›ä¼˜åŒ–è¡¨è¿°ä¸ºçº¿æ€§ç¼–ç é—®é¢˜ï¼Œå¹¶æå‡ºåˆ†ç»„ç¼–ç ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç†è®ºä¸Šå¯ä»¥æé«˜å¯¹éšæœºå™ªå£°çš„é²æ£’æ€§å’Œå­¦ä¹ æ•ˆç‡ã€‚å—è¿™äº›ç ”ç©¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€åˆ†ç»„æ³¨æ„åŠ›ï¼ˆDGAï¼‰ï¼Œå®ƒé€šè¿‡åˆ†ç»„ç¼–ç æ¥æ˜ç¡®å‡å°‘å†—ä½™ï¼Œåœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶èšåˆä¸å¤ªé‡è¦çš„ä»¤ç‰Œã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒDGAåœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer-based LLMs é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶æ“…é•¿è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä½†é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡å­˜åœ¨è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚</li>
<li>å†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—æ˜¯è®¡ç®—æ•ˆç‡é—®é¢˜çš„ä¸»è¦åŸå› ã€‚</li>
<li>æœ¬æ–‡é‡æ–°å®šä¹‰æ¦‚ç‡åºåˆ—å»ºæ¨¡ä¸ºç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œä»¥åŒºåˆ†ç›¸å…³å’Œä¸ç›¸å…³çš„ä»¤ç‰Œï¼Œæ˜ç¡®å†—ä½™æ¦‚å¿µã€‚</li>
<li>åªæœ‰å°‘æ•°ä»¤ç‰Œå¯¹é¢„æµ‹æœ‰æ˜¾è‘—è´¡çŒ®ã€‚</li>
<li>æå‡ºåŠ¨æ€åˆ†ç»„æ³¨æ„åŠ›ï¼ˆDGAï¼‰æ–¹æ³•ï¼Œé€šè¿‡åˆ†ç»„ç¼–ç ç­–ç•¥å‡å°‘å†—ä½™è®¡ç®—ã€‚</li>
<li>DGAæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ä¸”ä¿æŒç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-262859772e9dbad0b988fa2504696793.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eca6ec960b889a2e81e3ec88cc965ea2.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-28bf8a8110a9746a3695944324442f4d.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  DiCriTest Testing Scenario Generation for Decision-Making Agents   Considering Diversity and Criticality
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b0d785a72dc0d89eb17a05729b6956a1.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  Thyme Think Beyond Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
