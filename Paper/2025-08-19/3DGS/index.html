<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  Remove360 Benchmarking Residuals After Object Removal in 3D Gaussian   Splatting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-3de6c8c965bd3d17eabb4ec183fc9373.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-19-æ›´æ–°"><a href="#2025-08-19-æ›´æ–°" class="headerlink" title="2025-08-19 æ›´æ–°"></a>2025-08-19 æ›´æ–°</h1><h2 id="Remove360-Benchmarking-Residuals-After-Object-Removal-in-3D-Gaussian-Splatting"><a href="#Remove360-Benchmarking-Residuals-After-Object-Removal-in-3D-Gaussian-Splatting" class="headerlink" title="Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian   Splatting"></a>Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian   Splatting</h2><p><strong>Authors:Simona Kocour, Assia Benbihi, Torsten Sattler</strong></p>
<p>Understanding what semantic information persists after object removal is critical for privacy-preserving 3D reconstruction and editable scene representations. In this work, we introduce a novel benchmark and evaluation framework to measure semantic residuals, the unintended semantic traces left behind, after object removal in 3D Gaussian Splatting. We conduct experiments across a diverse set of indoor and outdoor scenes, showing that current methods can preserve semantic information despite the absence of visual geometry. We also release Remove360, a dataset of pre&#x2F;post-removal RGB images and object-level masks captured in real-world environments. While prior datasets have focused on isolated object instances, Remove360 covers a broader and more complex range of indoor and outdoor scenes, enabling evaluation of object removal in the context of full-scene representations. Given ground truth images of a scene before and after object removal, we assess whether we can truly eliminate semantic presence, and if downstream models can still infer what was removed. Our findings reveal critical limitations in current 3D object removal techniques and underscore the need for more robust solutions capable of handling real-world complexity. The evaluation framework is available at github.com&#x2F;spatial-intelligence-ai&#x2F;Remove360.git. Data are available at huggingface.co&#x2F;datasets&#x2F;simkoc&#x2F;Remove360. </p>
<blockquote>
<p>ç†è§£åœ¨ç§»é™¤ç‰©ä½“åä»ç„¶å­˜åœ¨çš„è¯­ä¹‰ä¿¡æ¯å¯¹äºä¿æŠ¤éšç§çš„3Dé‡å»ºå’Œå¯ç¼–è¾‘åœºæ™¯è¡¨ç¤ºè‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ï¼Œæ¥æµ‹é‡3Dé«˜æ–¯æ‹¼è´´ä¸­ç‰©ä½“ç§»é™¤åç•™ä¸‹çš„æ— æ„ä¸­çš„è¯­ä¹‰ç—•è¿¹ã€‚æˆ‘ä»¬åœ¨å®¤å†…å’Œå®¤å¤–åœºæ™¯è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æ²¡æœ‰è§†è§‰å‡ ä½•çš„æƒ…å†µä¸‹ï¼Œå½“å‰çš„æ–¹æ³•ä¹Ÿå¯ä»¥ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†Remove360æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«åœ¨ç°å®ç¯å¢ƒä¸­æ•è·çš„é¢„&#x2F;åç§»é™¤RGBå›¾åƒå’Œå¯¹è±¡çº§æ©è†œã€‚è™½ç„¶å…ˆå‰çš„æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨å­¤ç«‹çš„ç‰©ä½“å®ä¾‹ä¸Šï¼Œä½†Remove360æ¶µç›–äº†æ›´å¹¿æ³›å’Œæ›´å¤æ‚çš„å®¤å†…å’Œå®¤å¤–åœºæ™¯ï¼Œèƒ½å¤Ÿåœ¨å®Œæ•´çš„åœºæ™¯è¡¨ç¤ºä¸­è¯„ä¼°ç‰©ä½“ç§»é™¤ã€‚ç»™å®šç‰©ä½“ç§»é™¤å‰åçš„åœºæ™¯åœ°é¢çœŸå®å›¾åƒï¼Œæˆ‘ä»¬è¯„ä¼°æ˜¯å¦çœŸæ­£æ¶ˆé™¤äº†è¯­ä¹‰å­˜åœ¨ï¼Œä»¥åŠä¸‹æ¸¸æ¨¡å‹æ˜¯å¦ä»ç„¶èƒ½å¤Ÿæ¨æ–­å‡ºå·²ç§»é™¤çš„å†…å®¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†å½“å‰3Dç‰©ä½“ç§»é™¤æŠ€æœ¯çš„å…³é”®å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆæ¥å¤„ç†ç°å®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚è¯„ä¼°æ¡†æ¶å¯åœ¨github.com&#x2F;spatial-intelligence-ai&#x2F;Remove360.gitä¸Šæ‰¾åˆ°ã€‚æ•°æ®å¯åœ¨huggingface.co&#x2F;datasets&#x2F;simkoc&#x2F;Remove360ä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11431v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2503.17574</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨éšç§ä¿æŠ¤çš„ä¸‰ç»´é‡å»ºå’Œå¯ç¼–è¾‘åœºæ™¯è¡¨ç¤ºä¸­ï¼Œç†è§£åœ¨ç‰©ä½“ç§»é™¤åå“ªäº›è¯­ä¹‰ä¿¡æ¯ä¼šæ®‹ç•™ä¸‹æ¥æ˜¯éå¸¸é‡è¦çš„ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶æ¥è¡¡é‡ä¸‰ç»´é«˜æ–¯æ‹¼è´´ä¸­ç‰©ä½“ç§»é™¤åæ‰€ç•™ä¸‹çš„æ— æ„ä¸­çš„è¯­ä¹‰ç—•è¿¹ã€‚å®éªŒæ˜¾ç¤ºï¼Œå³ä¾¿æ²¡æœ‰è§†è§‰å‡ ä½•çš„å­˜åœ¨ï¼Œå½“å‰çš„æ–¹æ³•ä¹Ÿèƒ½ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚åŒæ—¶ï¼Œè¿˜å‘å¸ƒäº†Remove360æ•°æ®é›†ï¼ŒåŒ…å«çœŸå®ç¯å¢ƒä¸­é¢„ç§»é™¤å’Œæœªç§»é™¤çš„RGBå›¾åƒä»¥åŠç‰©ä½“çº§åˆ«çš„æ©è†œã€‚ä¸ä¹‹å‰çš„æ•°æ®é›†ä¸åŒï¼ŒRemove360æ¶µç›–äº†å®¤å†…å’Œå®¤å¤–åœºæ™¯çš„å¹¿æ³›èŒƒå›´ï¼Œèƒ½å¤Ÿåœ¨å®Œæ•´çš„åœºæ™¯è¡¨ç¤ºä¸­è¯„ä¼°ç‰©ä½“çš„ç§»é™¤ã€‚è¯„ä¼°ç»“æœæ­ç¤ºäº†å½“å‰ä¸‰ç»´ç‰©ä½“ç§»é™¤æŠ€æœ¯çš„å…³é”®å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆæ¥å¤„ç†çœŸå®ä¸–ç•Œçš„å¤æ‚æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç†è§£ç‰©ä½“ç§»é™¤åè¯­ä¹‰ä¿¡æ¯çš„ä¿ç•™å¯¹äºéšç§ä¿æŠ¤çš„ä¸‰ç»´é‡å»ºå’Œåœºæ™¯è¡¨ç¤ºè‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶æ¥è¡¡é‡ä¸‰ç»´ç©ºé—´ä¸­ç‰©ä½“ç§»é™¤åçš„è¯­ä¹‰æ®‹ç•™ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå½“å‰çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ²¡æœ‰è§†è§‰å‡ ä½•çš„æƒ…å†µä¸‹ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>å‘å¸ƒäº†Remove360æ•°æ®é›†ï¼ŒåŒ…å«çœŸå®ç¯å¢ƒä¸­é¢„ç§»é™¤å’Œæœªç§»é™¤çš„RGBå›¾åƒå’Œç‰©ä½“çº§åˆ«çš„æ©è†œã€‚</li>
<li>Remove360æ•°æ®é›†ä¸åŒäºä»¥å¾€çš„æ•°æ®é›†ï¼Œæ¶µç›–äº†å®¤å†…å’Œå®¤å¤–æ›´å¹¿æ³›å¤æ‚çš„åœºæ™¯ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºå½“å‰ä¸‰ç»´ç‰©ä½“ç§»é™¤æŠ€æœ¯å­˜åœ¨å…³é”®å±€é™æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dda73ce4bf307171b3c527d62058bafd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebdd180ff5a98bea5ddb945be357bcd0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a9b37d6fe72e083cc1d24f0ad6fc95c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93b7eb6b39b301e19653935a55555719.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Versatile-Video-Tokenization-with-Generative-2D-Gaussian-Splatting"><a href="#Versatile-Video-Tokenization-with-Generative-2D-Gaussian-Splatting" class="headerlink" title="Versatile Video Tokenization with Generative 2D Gaussian Splatting"></a>Versatile Video Tokenization with Generative 2D Gaussian Splatting</h2><p><strong>Authors:Zhenghao Chen, Zicong Chen, Lei Liu, Yiming Wu, Dong Xu</strong></p>
<p>Video tokenization procedure is critical for a wide range of video processing tasks. Most existing approaches directly transform video into fixed-grid and patch-wise tokens, which exhibit limited versatility. Spatially, uniformly allocating a fixed number of tokens often leads to over-encoding in low-information regions. Temporally, reducing redundancy remains challenging without explicitly distinguishing between static and dynamic content. In this work, we propose the Gaussian Video Transformer (GVT), a versatile video tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We first extract latent rigid features from a video clip and represent them with a set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D Gaussians not only enhance spatial adaptability by assigning higher (resp., lower) rendering weights to regions with higher (resp., lower) information content during rasterization, but also improve generalization by avoiding per-video optimization.To enhance the temporal versatility, we introduce a Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into static and dynamic sets, which explicitly model static content shared across different time-steps and dynamic content specific to each time-step, enabling a compact representation.We primarily evaluate GVT on the video reconstruction, while also assessing its performance on action recognition and compression using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments demonstrate that GVT achieves a state-of-the-art video reconstruction quality, outperforms the baseline MAGVIT-v2 in action recognition, and delivers comparable compression performance. </p>
<blockquote>
<p>è§†é¢‘ä»¤ç‰ŒåŒ–ç¨‹åºå¯¹äºå¹¿æ³›çš„è§†é¢‘å¤„ç†ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•ç›´æ¥å°†è§†é¢‘è½¬æ¢ä¸ºå›ºå®šç½‘æ ¼å’Œè¡¥ä¸ä»¤ç‰Œï¼Œè¿™è¡¨ç°å‡ºæœ‰é™çš„çµæ´»æ€§ã€‚åœ¨ç©ºé—´ä¸Šï¼Œå‡åŒ€åˆ†é…å›ºå®šæ•°é‡çš„ä»¤ç‰Œå¾€å¾€ä¼šå¯¼è‡´ä½ä¿¡æ¯åŒºåŸŸçš„è¿‡åº¦ç¼–ç ã€‚åœ¨æ—¶é—´ä¸Šï¼Œå¦‚ä½•åœ¨ä¸æ˜¾å¼åŒºåˆ†é™æ€å’ŒåŠ¨æ€å†…å®¹çš„æƒ…å†µä¸‹å‡å°‘å†—ä½™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ–¯è§†é¢‘è½¬æ¢å™¨ï¼ˆGVTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç”Ÿæˆå¼äºŒç»´é«˜æ–¯æ‹¼è´´ï¼ˆ2DGSï¼‰ç­–ç•¥çš„é€šç”¨è§†é¢‘ä»¤ç‰Œç”Ÿæˆå™¨ã€‚æˆ‘ä»¬é¦–å…ˆä»ä¸€ä¸ªè§†é¢‘å‰ªè¾‘ä¸­æå–æ½œåœ¨åˆšæ€§ç‰¹å¾ï¼Œå¹¶ä»¥å‰é¦ˆæ–¹å¼ä½¿ç”¨æˆ‘ä»¬æå‡ºçš„ç©ºé—´æ—¶é—´é«˜æ–¯åµŒå…¥ï¼ˆSTGEï¼‰æœºåˆ¶æ¥ç”Ÿæˆä¸€ç»„äºŒç»´é«˜æ–¯åˆ†å¸ƒæ¥è¡¨ç¤ºå®ƒä»¬ã€‚è¿™ç§ç”Ÿæˆå¼çš„äºŒç»´é«˜æ–¯åˆ†å¸ƒä¸ä»…é€šè¿‡åœ¨å…‰çº¿æŠ•å°„è¿‡ç¨‹ä¸­ä¸ºä¿¡æ¯å«é‡è¾ƒé«˜ï¼ˆè¾ƒä½ï¼‰çš„åŒºåŸŸåˆ†é…è¾ƒé«˜ï¼ˆè¾ƒä½ï¼‰çš„æ¸²æŸ“æƒé‡æ¥å¢å¼ºç©ºé—´é€‚åº”æ€§ï¼Œè€Œä¸”é€šè¿‡é¿å…é’ˆå¯¹æ¯ä¸ªè§†é¢‘çš„ä¼˜åŒ–æ¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†æé«˜æ—¶é—´çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†é«˜æ–¯é›†åˆ†å‰²ï¼ˆGSPï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å°†äºŒç»´é«˜æ–¯åˆ†å¸ƒåˆ†ä¸ºé™æ€é›†å’ŒåŠ¨æ€é›†ï¼Œæ˜¾å¼åœ°æ¨¡æ‹Ÿä¸åŒæ—¶é—´æ­¥ä¹‹é—´å…±äº«çš„é™æ€å†…å®¹å’Œæ¯ä¸ªæ—¶é—´æ­¥ç‰¹æœ‰çš„åŠ¨æ€å†…å®¹ï¼Œä»è€Œå®ç°ç´§å‡‘è¡¨ç¤ºã€‚æˆ‘ä»¬ä¸»è¦ä½¿ç”¨UCF101ã€Kineticså’ŒDAVISæ•°æ®é›†è¯„ä¼°GVTåœ¨è§†é¢‘é‡å»ºæ–¹é¢çš„è¡¨ç°ï¼ŒåŒæ—¶è¯„ä¼°å…¶åœ¨åŠ¨ä½œè¯†åˆ«å’Œå‹ç¼©æ–¹é¢çš„æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGVTåœ¨è§†é¢‘é‡å»ºæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œåœ¨åŠ¨ä½œè¯†åˆ«æ–¹é¢ä¼˜äºMAGVIT-v2åŸºçº¿æ¨¡å‹ï¼Œå‹ç¼©æ€§èƒ½ä¹Ÿå…·æœ‰å¯æ¯”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11183v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è§†é¢‘ä»¤ç‰ŒåŒ–ç¨‹åºï¼Œè¯¥ç¨‹åºä½¿ç”¨ç”Ÿæˆå¼äºŒç»´é«˜æ–¯å°æ¨¡ç­–ç•¥æ„å»ºäº†é€šç”¨è§†é¢‘ä»¤ç‰Œå™¨ï¼Œé’ˆå¯¹è§†é¢‘å¤„ç†ä»»åŠ¡çš„å¹¿æ³›éœ€æ±‚ï¼Œå®ç°äº†é«˜æ•ˆçš„è§†é¢‘å¤„ç†ã€‚æå‡ºäº†ä¸€ç§åŸºäºäºŒç»´é«˜æ–¯åˆ†å¸ƒçš„åŠ¨æ€ç”Ÿæˆè§†é¢‘ç‰¹å¾åµŒå…¥æŠ€æœ¯ï¼Œå¢å¼ºæ—¶ç©ºè‡ªé€‚åº”èƒ½åŠ›çš„åŒæ—¶é¿å…é’ˆå¯¹è§†é¢‘çš„å•ç‹¬ä¼˜åŒ–ï¼Œä»è€Œåœ¨åœºæ™¯åŒºåŸŸç”Ÿæˆå¯è°ƒæ•´çš„ã€ç»“æ„åŒ–çš„è§†é¢‘ä»¤ç‰Œé›†åˆã€‚ä½¿ç”¨é«˜æ–¯é›†åˆåˆ’åˆ†ç­–ç•¥åˆ†ç¦»äºŒç»´é«˜æ–¯ï¼Œå»ºæ¨¡ä¸åŒæ—¶é—´æ­¥é•¿é—´çš„é™æ€å’ŒåŠ¨æ€å†…å®¹ï¼Œä¸ºè§†é¢‘çš„é‡å»ºã€åŠ¨ä½œè¯†åˆ«å’Œå‹ç¼©æä¾›äº†å…ˆè¿›çš„æ–¹æ³•ã€‚é€šè¿‡UCF101ã€Kineticså’ŒDAVISæ•°æ®é›†çš„å®éªŒéªŒè¯äº†å…¶åœ¨è§†é¢‘é‡å»ºä¸Šçš„ä¼˜è¶Šæ€§ï¼ŒåŒæ—¶è¿˜åœ¨åŠ¨ä½œè¯†åˆ«å’Œå‹ç¼©ä¸Šå±•ç°äº†ç«äº‰åŠ›ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ul>
<li>è§†é¢‘ä»¤ç‰ŒåŒ–å¯¹äºå„ç§è§†é¢‘å¤„ç†ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ç›´æ¥å°†è§†é¢‘è½¬æ¢ä¸ºå›ºå®šç½‘æ ¼å’Œè¡¥ä¸ä»¤ç‰Œï¼Œè¡¨ç°å‡ºæœ‰é™çš„çµæ´»æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼äºŒç»´é«˜æ–¯å°æ¨¡ç­–ç•¥çš„é€šç”¨è§†é¢‘ä»¤ç‰Œå™¨â€”â€”é«˜æ–¯è§†é¢‘è½¬æ¢å™¨ï¼ˆGVTï¼‰ã€‚</li>
<li>ä½¿ç”¨æ—¶ç©ºé«˜æ–¯åµŒå…¥ï¼ˆSTGEï¼‰æœºåˆ¶æå–è§†é¢‘çš„æ½œåœ¨åˆšæ€§ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨ç”Ÿæˆå¼äºŒç»´é«˜æ–¯è¡¨ç¤ºå¢å¼ºæ—¶ç©ºé€‚åº”æ€§ã€‚</li>
<li>é«˜æ–¯é›†åˆåˆ’åˆ†ï¼ˆGSPï¼‰ç­–ç•¥ç”¨äºåˆ†ç¦»é™æ€å’ŒåŠ¨æ€å†…å®¹ï¼Œå®ç°ç´§å‡‘çš„è§†é¢‘è¡¨ç¤ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7510b7c408b48038e360e9d56e134d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8ce7d012741d15160742d048a8c0c09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc90a52361a27e3f048dd4276af08a3b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Vision-Only-Gaussian-Splatting-for-Collaborative-Semantic-Occupancy-Prediction"><a href="#Vision-Only-Gaussian-Splatting-for-Collaborative-Semantic-Occupancy-Prediction" class="headerlink" title="Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy   Prediction"></a>Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy   Prediction</h2><p><strong>Authors:Cheng Chen, Hao Huang, Saurabh Bagchi</strong></p>
<p>Collaborative perception enables connected vehicles to share information, overcoming occlusions and extending the limited sensing range inherent in single-agent (non-collaborative) systems. Existing vision-only methods for 3D semantic occupancy prediction commonly rely on dense 3D voxels, which incur high communication costs, or 2D planar features, which require accurate depth estimation or additional supervision, limiting their applicability to collaborative scenarios. To address these challenges, we propose the first approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D semantic occupancy prediction. By sharing and fusing intermediate Gaussian primitives, our method provides three benefits: a neighborhood-based cross-agent fusion that removes duplicates and suppresses noisy or inconsistent Gaussians; a joint encoding of geometry and semantics in each primitive, which reduces reliance on depth supervision and allows simple rigid alignment; and sparse, object-centric messages that preserve structural information while reducing communication volume. Extensive experiments demonstrate that our approach outperforms single-agent perception and baseline collaborative methods by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU, respectively. When further reducing the number of transmitted Gaussians, our method still achieves a +1.9 improvement in mIoU, using only 34.6% communication volume, highlighting robust performance under limited communication budgets. </p>
<blockquote>
<p>ååŒæ„ŸçŸ¥ä½¿è¿æ¥è½¦è¾†èƒ½å¤Ÿå…±äº«ä¿¡æ¯ï¼Œå…‹æœé®æŒ¡é—®é¢˜å¹¶æ‰©å±•å•ä¸ªä»£ç†ï¼ˆéååŒï¼‰ç³»ç»Ÿå›ºæœ‰çš„æœ‰é™æ„ŸçŸ¥èŒƒå›´ã€‚ç°æœ‰çš„ä»…ä¾èµ–è§†è§‰çš„3Dè¯­ä¹‰å ç”¨é¢„æµ‹æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯†é›†çš„3Dä½“ç´ ï¼Œè¿™ä¼šäº§ç”Ÿè¾ƒé«˜çš„é€šä¿¡æˆæœ¬ï¼Œæˆ–è€…ä¾èµ–äº2Då¹³é¢ç‰¹å¾ï¼Œè¿™éœ€è¦å‡†ç¡®çš„æ·±åº¦ä¼°è®¡æˆ–é¢å¤–çš„ç›‘ç£ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬åœ¨ååŒåœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†ä¸€ç§åˆ©ç”¨ç¨€ç–3Dè¯­ä¹‰é«˜æ–¯æ¶‚æŠ¹æ–¹æ³•è¿›è¡ŒååŒ3Dè¯­ä¹‰å ç”¨é¢„æµ‹çš„æ–¹æ³•ã€‚é€šè¿‡å…±äº«å’Œèåˆä¸­é—´çš„Gaussian primitivesï¼ˆé«˜æ–¯åŸå§‹æ•°æ®ï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä»¥ä¸‹ä¸‰ä¸ªå¥½å¤„ï¼šåŸºäºé‚»åŸŸçš„è·¨ä»£ç†èåˆï¼Œå¯ä»¥æ¶ˆé™¤é‡å¤å¹¶æŠ‘åˆ¶å˜ˆæ‚æˆ–ä¸ä¸€è‡´çš„é«˜æ–¯æ•°æ®ï¼›æ¯ä¸ªåŸå§‹æ•°æ®ä¸­çš„å‡ ä½•å’Œè¯­ä¹‰è”åˆç¼–ç ï¼Œå‡å°‘äº†æ·±åº¦ç›‘ç£çš„ä¾èµ–ï¼Œå¹¶å…è®¸ç®€å•çš„åˆšä½“å¯¹é½ï¼›ä»¥åŠç¨€ç–çš„ã€ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ¶ˆæ¯ï¼Œä¿ç•™ç»“æ„ä¿¡æ¯çš„åŒæ—¶å‡å°‘äº†é€šä¿¡é‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨mIoUå’ŒIoUæŒ‡æ ‡ä¸Šåˆ†åˆ«æ¯”å•ä»£ç†æ„ŸçŸ¥å’ŒåŸºçº¿ååŒæ–¹æ³•é«˜å‡º+8.42å’Œ+3.28ç‚¹ï¼Œä»¥åŠ+5.11å’Œ+22.41ç‚¹ã€‚åœ¨è¿›ä¸€æ­¥å‡å°‘ä¼ è¾“çš„é«˜æ–¯æ•°æ®é‡æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»èƒ½åœ¨mIoUä¸Šå®ç°+1.9çš„æå‡ï¼Œä»…ä½¿ç”¨34.6%çš„é€šä¿¡é‡ï¼Œå‡¸æ˜¾äº†åœ¨æœ‰é™çš„é€šä¿¡é¢„ç®—ä¸‹è¡¨ç°ç¨³å¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10936v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åˆ©ç”¨ç¨€ç–ä¸‰ç»´è¯­ä¹‰é«˜æ–¯å¹³é“ºæŠ€æœ¯å®ç°åä½œä¸‰ç»´è¯­ä¹‰å ç”¨é¢„æµ‹çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å…±äº«å’Œèåˆä¸­é—´é«˜æ–¯åŸºå…ƒï¼Œå®ç°é‚»å±…é—´çš„è·¨æ™ºèƒ½ä½“èåˆï¼Œåˆ é™¤é‡å¤é¡¹å¹¶æŠ‘åˆ¶å™ªå£°æˆ–ä¸ä¸€è‡´çš„é«˜æ–¯åˆ†å¸ƒï¼›åŒæ—¶é‡‡ç”¨å‡ ä½•å’Œè¯­ä¹‰çš„è”åˆç¼–ç ï¼Œå‡å°‘å¯¹æ·±åº¦ç›‘ç£çš„ä¾èµ–ï¼Œå…è®¸ç®€å•çš„åˆšä½“å¯¹é½ï¼›æ­¤å¤–ï¼Œç¨€ç–ã€ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ¶ˆæ¯ä¿ç•™äº†ç»“æ„ä¿¡æ¯ï¼ŒåŒæ—¶å‡å°‘äº†é€šä¿¡é‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•æ™ºèƒ½ä½“æ„ŸçŸ¥å’ŒåŸºå‡†åä½œæ–¹æ³•çš„åŸºç¡€ä¸Šï¼ŒmIoUæé«˜äº†8.42ç‚¹å’Œ3.28ç‚¹ï¼ŒIoUæé«˜äº†5.11ç‚¹å’Œ22.41ç‚¹ã€‚åœ¨å‡å°‘ä¼ è¾“çš„é«˜æ–¯æ•°é‡çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä»èƒ½æé«˜mIoU 1.9ç‚¹ï¼ŒåŒæ—¶ä»…ä½¿ç”¨34.6%çš„é€šä¿¡é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åä½œæ„ŸçŸ¥ä½¿è¿æ¥è½¦è¾†èƒ½å¤Ÿå…±äº«ä¿¡æ¯ï¼Œå…‹æœé®æŒ¡å¹¶æ‰©å±•å•ä¸€æ™ºèƒ½ä½“ç³»ç»Ÿçš„æœ‰é™æ„ŸçŸ¥èŒƒå›´ã€‚</li>
<li>ç°æœ‰ä»…ä¾èµ–è§†è§‰çš„3Dè¯­ä¹‰å ç”¨é¢„æµ‹æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯†é›†çš„3Dä½“ç´ ï¼Œè¿™å¯¼è‡´äº†é«˜é€šä¿¡æˆæœ¬ï¼Œæˆ–ä¾èµ–äº2Då¹³é¢ç‰¹å¾ï¼Œè¿™éœ€è¦å‡†ç¡®çš„æ·±åº¦ä¼°è®¡æˆ–é¢å¤–çš„ç›‘ç£ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨åä½œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åˆ©ç”¨ç¨€ç–ä¸‰ç»´è¯­ä¹‰é«˜æ–¯å¹³é“ºçš„åä½œä¸‰ç»´è¯­ä¹‰å ç”¨é¢„æµ‹æ–¹æ³•ï¼Œé€šè¿‡å…±äº«å’Œèåˆä¸­é—´é«˜æ–¯åŸºå…ƒæ¥å®ç°ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†åŸºäºé‚»å±…çš„è·¨æ™ºèƒ½ä½“èåˆï¼Œå¯ä»¥åˆ é™¤é‡å¤é¡¹å¹¶æŠ‘åˆ¶å™ªå£°æˆ–ä¸ä¸€è‡´çš„é«˜æ–¯åˆ†å¸ƒã€‚</li>
<li>å‡ ä½•å’Œè¯­ä¹‰çš„è”åˆç¼–ç å‡å°‘äº†æ·±åº¦ç›‘ç£çš„ä¾èµ–ï¼Œå…è®¸ç®€å•çš„åˆšä½“å¯¹é½ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨ç¨€ç–ã€ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ¶ˆæ¯ï¼Œä¿ç•™äº†ç»“æ„ä¿¡æ¯å¹¶å‡å°‘äº†é€šä¿¡é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10936">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9d97b745c09382a59942b87f4a73903f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce5325f47e58be698eee9bbac080a6d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-037e196af7e89195359c2e6395c65f68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f75eb2d282c67eb093015690c273701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81902502da1db125d3c7b3b698710fd1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multi-Sample-Anti-Aliasing-and-Constrained-Optimization-for-3D-Gaussian-Splatting"><a href="#Multi-Sample-Anti-Aliasing-and-Constrained-Optimization-for-3D-Gaussian-Splatting" class="headerlink" title="Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian   Splatting"></a>Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian   Splatting</h2><p><strong>Authors:Zheng Zhou, Jia-Chen Zhang, Yu-Jie Xiong, Chun-Ming Xia</strong></p>
<p>Recent advances in 3D Gaussian splatting have significantly improved real-time novel view synthesis, yet insufficient geometric constraints during scene optimization often result in blurred reconstructions of fine-grained details, particularly in regions with high-frequency textures and sharp discontinuities. To address this, we propose a comprehensive optimization framework integrating multisample anti-aliasing (MSAA) with dual geometric constraints. Our system computes pixel colors through adaptive blending of quadruple subsamples, effectively reducing aliasing artifacts in high-frequency components. The framework introduces two constraints: (a) an adaptive weighting strategy that prioritizes under-reconstructed regions through dynamic gradient analysis, and (b) gradient differential constraints enforcing geometric regularization at object boundaries. This targeted optimization enables the model to allocate computational resources preferentially to critical regions requiring refinement while maintaining global consistency. Extensive experimental evaluations across multiple benchmarks demonstrate that our method achieves state-of-the-art performance in detail preservation, particularly in preserving high-frequency textures and sharp discontinuities, while maintaining real-time rendering efficiency. Quantitative metrics and perceptual studies confirm statistically significant improvements over baseline approaches in both structural similarity (SSIM) and perceptual quality (LPIPS). </p>
<blockquote>
<p>åœ¨3Dé«˜æ–¯è´´å›¾æŠ€æœ¯æ–¹é¢ï¼Œæœ€è¿‘çš„è¿›å±•æå¤§åœ°æé«˜äº†å®æ—¶æ–°å‹è§†å›¾åˆæˆçš„è´¨é‡ã€‚ç„¶è€Œï¼Œåœ¨åœºæ™¯ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ ä½•çº¦æŸä¸è¶³å¸¸å¸¸å¯¼è‡´ç²¾ç»†ç»†èŠ‚çš„æ¨¡ç³Šé‡å»ºï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é¢‘çº¹ç†å’Œå°–é”ä¸è¿ç»­åŒºåŸŸã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»¼åˆä¼˜åŒ–æ¡†æ¶ï¼Œå°†å¤šé‡é‡‡æ ·æŠ—é”¯é½¿ï¼ˆMSAAï¼‰ä¸åŒé‡å‡ ä½•çº¦æŸç›¸ç»“åˆã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé€šè¿‡è‡ªé€‚åº”æ··åˆå››ä¸ªå­æ ·æœ¬æ¥è®¡ç®—åƒç´ é¢œè‰²ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†é«˜é¢‘ç»„ä»¶ä¸­çš„é”¯é½¿çŠ¶ä¼ªå½±ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤ä¸ªçº¦æŸï¼šï¼ˆaï¼‰ä¸€ç§è‡ªé€‚åº”åŠ æƒç­–ç•¥ï¼Œé€šè¿‡åŠ¨æ€æ¢¯åº¦åˆ†æä¼˜å…ˆå¤„ç†é‡å»ºä¸è¶³çš„åŒºåŸŸï¼›ï¼ˆbï¼‰æ¢¯åº¦å·®åˆ†çº¦æŸï¼Œåœ¨ç‰©ä½“è¾¹ç•Œå¤„å®æ–½å‡ ä½•æ­£åˆ™åŒ–ã€‚è¿™ç§æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ä½¿æ¨¡å‹èƒ½å¤Ÿä¼˜å…ˆåˆ†é…è®¡ç®—èµ„æºåˆ°éœ€è¦ç²¾ç»†åŒ–çš„å…³é”®åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒå…¨å±€ä¸€è‡´æ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»†èŠ‚ä¿ç•™æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿ç•™é«˜é¢‘çº¹ç†å’Œå°–é”ä¸è¿ç»­æ€§æ–¹é¢ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ¸²æŸ“çš„æ•ˆç‡ã€‚å®šé‡æŒ‡æ ‡å’Œæ„ŸçŸ¥ç ”ç©¶è¯å®ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰å’Œæ„ŸçŸ¥è´¨é‡ï¼ˆLPIPSï¼‰æ–¹é¢éƒ½æœ‰ç»Ÿè®¡ä¸Šçš„æ˜¾è‘—æ”¹å–„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10507v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œä¸‰ç»´é«˜æ–¯ç»˜åˆ¶æŠ€æœ¯çš„æ–°è¿›å±•æå¤§åœ°æ¨åŠ¨äº†å®æ—¶æ–°å‹è§†å›¾åˆæˆçš„å‘å±•ã€‚ä½†åœºæ™¯ä¼˜åŒ–æ—¶å‡ ä½•çº¦æŸä¸è¶³å¸¸å¸¸å¯¼è‡´ç²¾ç»†ç»†èŠ‚é‡å»ºæ¨¡ç³Šï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é¢‘çº¹ç†å’Œæ¸…æ™°è¾¹ç•ŒåŒºåŸŸã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»¼åˆä¼˜åŒ–æ¡†æ¶ï¼Œå°†å¤šé‡é‡‡æ ·æŠ—é”¯é½¿æŠ€æœ¯ä¸åŒé‡å‡ ä½•çº¦æŸç›¸ç»“åˆã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé€šè¿‡è‡ªé€‚åº”æ··åˆå››ä¸ªå­æ ·æœ¬æ¥è®¡ç®—åƒç´ é¢œè‰²ï¼Œæœ‰æ•ˆå‡å°‘é«˜é¢‘æˆåˆ†çš„é”¯é½¿çŠ¶ä¼ªå½±ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤é¡¹çº¦æŸï¼šä¸€æ˜¯è‡ªé€‚åº”åŠ æƒç­–ç•¥ï¼Œé€šè¿‡åŠ¨æ€æ¢¯åº¦åˆ†æä¼˜å…ˆå¤„ç†é‡å»ºä¸è¶³çš„åŒºåŸŸï¼›äºŒæ˜¯æ¢¯åº¦å·®å¼‚çº¦æŸï¼Œåœ¨ç‰©ä½“è¾¹ç•Œå¤„å®æ–½å‡ ä½•æ­£åˆ™åŒ–ã€‚è¿™ç§æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ä½¿æ¨¡å‹èƒ½å¤Ÿä¼˜å…ˆåˆ†é…è®¡ç®—èµ„æºåˆ°éœ€è¦ç²¾ç»†åŒ–çš„å…³é”®åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒå…¨å±€ä¸€è‡´æ€§ã€‚è·¨å¤šä¸ªåŸºå‡†çš„å¹¿æ³›å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»†èŠ‚ä¿ç•™æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒé«˜é¢‘çº¹ç†å’Œæ¸…æ™°è¾¹ç•Œæ–¹é¢ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ¸²æŸ“æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸä¸‰ç»´é«˜æ–¯ç»˜åˆ¶æŠ€æœ¯çš„æ–°è¿›å±•æå‡äº†å®æ—¶æ–°å‹è§†å›¾åˆæˆçš„è´¨é‡ã€‚</li>
<li>åœºæ™¯ä¼˜åŒ–æ—¶å‡ ä½•çº¦æŸä¸è¶³ä¼šå¯¼è‡´ç²¾ç»†ç»†èŠ‚é‡å»ºæ¨¡ç³Šã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»¼åˆä¼˜åŒ–æ¡†æ¶ï¼Œç»“åˆå¤šé‡é‡‡æ ·æŠ—é”¯é½¿ä¸åŒé‡å‡ ä½•çº¦æŸã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”æ··åˆå››ä¸ªå­æ ·æœ¬æ¥è®¡ç®—åƒç´ é¢œè‰²ï¼Œå‡å°‘é«˜é¢‘æˆåˆ†çš„é”¯é½¿çŠ¶ä¼ªå½±ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤é¡¹é‡è¦çº¦æŸï¼šè‡ªé€‚åº”åŠ æƒç­–ç•¥å’Œæ¢¯åº¦å·®å¼‚çº¦æŸã€‚</li>
<li>æ–¹æ³•åœ¨ç»†èŠ‚ä¿ç•™æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é¢‘çº¹ç†å’Œæ¸…æ™°è¾¹ç•Œçš„ä¿ç•™ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f1b6d4178eb7ba087b8ebc5ee2d7f791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12fedcdc640b7f45973a01a9790709d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99f88845727587c1c51de569ea2bf97f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-3D-Gaussian-Splatting-Applications-Segmentation-Editing-and-Generation"><a href="#A-Survey-on-3D-Gaussian-Splatting-Applications-Segmentation-Editing-and-Generation" class="headerlink" title="A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing,   and Generation"></a>A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing,   and Generation</h2><p><strong>Authors:Shuting He, Peilin Ji, Yitong Yang, Changshuo Wang, Jiayi Ji, Yinglin Wang, Henghui Ding</strong></p>
<p>3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative to Neural Radiance Fields (NeRF) for 3D scene representation, offering high-fidelity photorealistic rendering with real-time performance. Beyond novel view synthesis, the explicit and compact nature of 3DGS enables a wide range of downstream applications that require geometric and semantic understanding. This survey provides a comprehensive overview of recent progress in 3DGS applications. It first introduces 2D foundation models that support semantic understanding and control in 3DGS applications, followed by a review of NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS applications into segmentation, editing, generation, and other functional tasks. For each, we summarize representative methods, supervision strategies, and learning paradigms, highlighting shared design principles and emerging trends. Commonly used datasets and evaluation protocols are also summarized, along with comparative analyses of recent methods across public benchmarks. To support ongoing research and development, a continually updated repository of papers, code, and resources is maintained at <a target="_blank" rel="noopener" href="https://github.com/heshuting555/Awesome-3DGS-Applications">https://github.com/heshuting555/Awesome-3DGS-Applications</a>. </p>
<blockquote>
<p>3Dé«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œæœ€è¿‘ä½œä¸ºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æ›¿ä»£æ–¹æ¡ˆåœ¨ä¸‰ç»´åœºæ™¯è¡¨ç¤ºä¸­å´­éœ²å¤´è§’ï¼Œå®ƒä»¥å®æ—¶æ€§èƒ½æä¾›é«˜ä¿çœŸç…§ç‰‡çº§æ¸²æŸ“ã€‚é™¤äº†æ–°é¢–è§†å›¾åˆæˆï¼Œ3DGSçš„æ˜ç¡®å’Œç´§å‡‘æ€§è´¨ä½¿å¾—å®ƒåœ¨éœ€è¦å¤§é‡å‡ ä½•å’Œè¯­ä¹‰ç†è§£çš„ä¸‹æ¸¸åº”ç”¨æ–¹é¢å…·æœ‰å¹¿æ³›çš„å¯èƒ½æ€§ã€‚è¿™ç¯‡ç»¼è¿°å…¨é¢æ¦‚è¿°äº†3DGSåº”ç”¨çš„æœ€æ–°è¿›å±•ã€‚å®ƒé¦–å…ˆä»‹ç»äº†æ”¯æŒ3DGSåº”ç”¨ä¸­è¯­ä¹‰ç†è§£å’Œæ§åˆ¶çš„äºŒç»´åŸºç¡€æ¨¡å‹ï¼Œç„¶åå›é¡¾äº†ä¸º3DGSåŒè¡Œæä¾›ä¿¡æ¯çš„åŸºäºNeRFçš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†3DGSåº”ç”¨åˆ†ç±»ä¸ºåˆ†å‰²ã€ç¼–è¾‘ã€ç”Ÿæˆå’Œå…¶ä»–åŠŸèƒ½ä»»åŠ¡ã€‚å¯¹äºæ¯ä¸€é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬æ€»ç»“äº†ä»£è¡¨æ€§çš„æ–¹æ³•ã€ç›‘ç£ç­–ç•¥å’Œå­¦ä¹ èŒƒå¼ï¼Œå¹¶å¼ºè°ƒäº†å…±äº«è®¾è®¡åŸåˆ™å’Œæ–°å…´è¶‹åŠ¿ã€‚æœ¬æ–‡è¿˜æ€»ç»“äº†å¸¸ç”¨çš„æ•°æ®é›†å’Œè¯„ä¼°åè®®ï¼Œä»¥åŠè¿‘æœŸæ–¹æ³•åœ¨å…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçš„æ¯”è¾ƒåˆ†æã€‚ä¸ºäº†æ”¯æŒæŒç»­çš„ç ”ç©¶å’Œå¼€å‘ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/heshuting555/Awesome-3DGS-Applications%E4%B8%8A%E7%BB%B4%E6%8A%A4%E4%BA%86%E4%B8%80%E4%B8%AA%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E7%9A%84%E8%AE%BA%E6%96%87%E3%80%81%E4%BB%A3%E7%A0%81%E5%92%8C%E8%B5%84%E6%BA%90%E4%BB%93%E5%BA%93%E3%80%82">https://github.com/heshuting555/Awesome-3DGS-Applicationsä¸Šç»´æŠ¤äº†ä¸€ä¸ªæŒç»­æ›´æ–°çš„è®ºæ–‡ã€ä»£ç å’Œèµ„æºä»“åº“ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09977v1">PDF</a> GitHub Repo:   <a target="_blank" rel="noopener" href="https://github.com/heshuting555/Awesome-3DGS-Applications">https://github.com/heshuting555/Awesome-3DGS-Applications</a></p>
<p><strong>Summary</strong></p>
<p>3D Gaussian Splattingï¼ˆ3DGSï¼‰ä½œä¸ºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä»¥å…¶æ˜ç¡®ã€ç´§å‡‘çš„ç‰¹æ€§ï¼Œåœ¨ä¸‰ç»´åœºæ™¯è¡¨ç¤ºä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œæ”¯æŒé«˜è´¨é‡çš„å…‰ç…§çœŸå®æ¸²æŸ“å’Œå®æ—¶æ€§èƒ½ã€‚é™¤äº†æ–°è§†è§’åˆæˆï¼Œ3DGSè¿˜å¹¿æ³›åº”ç”¨äºéœ€è¦å‡ ä½•å’Œè¯­ä¹‰ç†è§£çš„å„ç§ä¸‹æ¸¸åº”ç”¨ã€‚æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†3DGSåº”ç”¨çš„æœ€æ–°è¿›å±•ï¼Œä»‹ç»äº†æ”¯æŒ3DGSåº”ç”¨çš„è¯­ä¹‰ç†è§£å’Œæ§åˆ¶çš„äºŒç»´åŸºç¡€æ¨¡å‹ï¼Œåˆ†ç±»ä»‹ç»äº†3DGSåœ¨åˆ†å‰²ã€ç¼–è¾‘ã€ç”Ÿæˆå’Œå…¶ä»–åŠŸèƒ½ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå¹¶æ€»ç»“äº†å¸¸ç”¨æ•°æ®é›†å’Œè¯„ä¼°åè®®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D Gaussian Splatting (3DGS) æ˜¯ä¸€ç§å¼ºå¤§çš„3Dåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œå¯æ›¿ä»£ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ã€‚</li>
<li>3DGS æä¾›äº†é«˜è´¨é‡çš„å…‰ç…§çœŸå®æ¸²æŸ“å’Œå®æ—¶æ€§èƒ½ã€‚</li>
<li>3DGS å…·æœ‰å¹¿æ³›çš„åº”ç”¨èŒƒå›´ï¼ŒåŒ…æ‹¬åˆ†å‰²ã€ç¼–è¾‘ã€ç”Ÿæˆå’Œå…¶ä»–åŠŸèƒ½ä»»åŠ¡ã€‚</li>
<li>æ–‡ä¸­ä»‹ç»äº†æ”¯æŒ3DGSåº”ç”¨çš„è¯­ä¹‰ç†è§£å’Œæ§åˆ¶çš„äºŒç»´åŸºç¡€æ¨¡å‹ã€‚</li>
<li>æ–‡ç« å¯¹NeRF-basedæ–¹æ³•è¿›è¡Œäº†å›é¡¾ï¼Œè¿™äº›æ–¹æ³•å¯¹3DGSæœ‰å¯ç¤ºä½œç”¨ã€‚</li>
<li>æ–‡ç« æ€»ç»“äº†å¸¸ç”¨æ•°æ®é›†å’Œè¯„ä¼°åè®®ï¼Œå¹¶å¯¹è¿‘æœŸæ–¹æ³•åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d204af30ec8702ea07e350f310da7b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6f0a2164690cbd73b271bc90f366993.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PERSONA-Personalized-Whole-Body-3D-Avatar-with-Pose-Driven-Deformations-from-a-Single-Image"><a href="#PERSONA-Personalized-Whole-Body-3D-Avatar-with-Pose-Driven-Deformations-from-a-Single-Image" class="headerlink" title="PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations   from a Single Image"></a>PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations   from a Single Image</h2><p><strong>Authors:Geonhee Sim, Gyeongsik Moon</strong></p>
<p>Two major approaches exist for creating animatable human avatars. The first, a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a single person, achieving personalization through a disentangled identity representation. However, modeling pose-driven deformations, such as non-rigid cloth deformations, requires numerous pose-rich videos, which are costly and impractical to capture in daily life. The second, a diffusion-based approach, learns pose-driven deformations from large-scale in-the-wild videos but struggles with identity preservation and pose-dependent identity entanglement. We present PERSONA, a framework that combines the strengths of both approaches to obtain a personalized 3D human avatar with pose-driven deformations from a single image. PERSONA leverages a diffusion-based approach to generate pose-rich videos from the input image and optimizes a 3D avatar based on them. To ensure high authenticity and sharp renderings across diverse poses, we introduce balanced sampling and geometry-weighted optimization. Balanced sampling oversamples the input image to mitigate identity shifts in diffusion-generated training videos. Geometry-weighted optimization prioritizes geometry constraints over image loss, preserving rendering quality in diverse poses. </p>
<blockquote>
<p>åˆ›å»ºå¯åŠ¨ç”»äººç±»è™šæ‹Ÿå½¢è±¡ä¸»è¦æœ‰ä¸¤ç§æ–¹æ³•ã€‚ç¬¬ä¸€ç§æ˜¯åŸºäºä¸‰ç»´çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡å•ä¸ªäººçš„è§†é¢‘ä¼˜åŒ–åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æˆ–ä¸‰ç»´å‡ ä½•è¡¨é¢ï¼ˆ3DGSï¼‰çš„è™šæ‹Ÿå½¢è±¡ï¼Œé€šè¿‡åˆ†ç¦»çš„èº«ä»½è¡¨ç¤ºå®ç°ä¸ªæ€§åŒ–ã€‚ç„¶è€Œï¼Œå¯¹å§¿åŠ¿é©±åŠ¨çš„å˜å½¢è¿›è¡Œå»ºæ¨¡ï¼Œå¦‚éåˆšæ€§çš„å¸ƒæ–™å˜å½¢ï¼Œéœ€è¦å¤§é‡çš„ä¸°å¯Œå§¿åŠ¿çš„è§†é¢‘ï¼Œè¿™åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­æ•æ‰èµ·æ¥æˆæœ¬é«˜æ˜‚ä¸”ä¸åˆ‡å®é™…ã€‚ç¬¬äºŒç§æ˜¯åŸºäºæ‰©æ•£çš„æ–¹æ³•ï¼Œå®ƒä»å¤§é‡çš„é‡å¤–è§†é¢‘ä¸­å­¦ä¹ å§¿åŠ¿é©±åŠ¨çš„å˜å½¢ï¼Œä½†åœ¨èº«ä»½ä¿æŒå’Œå§¿åŠ¿ç›¸å…³çš„èº«ä»½çº ç¼ æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†PERSONAï¼Œä¸€ä¸ªç»“åˆä¸¤ç§æ–¹æ³•ä¼˜ç‚¹çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å•å¼ å›¾åƒä¸­è·å¾—å…·æœ‰å§¿åŠ¿é©±åŠ¨å˜å½¢çš„ä¸ªæ€§åŒ–ä¸‰ç»´äººç±»è™šæ‹Ÿå½¢è±¡ã€‚PERSONAåˆ©ç”¨åŸºäºæ‰©æ•£çš„æ–¹æ³•ä»è¾“å…¥å›¾åƒç”Ÿæˆå§¿åŠ¿ä¸°å¯Œçš„è§†é¢‘ï¼Œå¹¶åŸºäºè¿™äº›è§†é¢‘ä¼˜åŒ–ä¸‰ç»´è™šæ‹Ÿå½¢è±¡ã€‚ä¸ºäº†ç¡®ä¿ä¸åŒå§¿åŠ¿çš„é«˜çœŸå®æ€§å’Œæ¸…æ™°æ¸²æŸ“ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¹³è¡¡é‡‡æ ·å’Œå‡ ä½•åŠ æƒä¼˜åŒ–ã€‚å¹³è¡¡é‡‡æ ·å¯¹è¾“å…¥å›¾åƒè¿›è¡Œè¿‡é‡‡æ ·ï¼Œä»¥å‡è½»æ‰©æ•£ç”Ÿæˆè®­ç»ƒè§†é¢‘ä¸­èº«ä»½çš„å˜åŒ–ã€‚å‡ ä½•åŠ æƒä¼˜åŒ–åœ¨å‡ ä½•çº¦æŸå’Œå›¾åƒæŸå¤±ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œä»è€Œåœ¨å¤šç§å§¿åŠ¿ä¸‹ä¿æŒæ¸²æŸ“è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09973v1">PDF</a> Accepted to ICCV 2025. <a target="_blank" rel="noopener" href="https://mks0601.github.io/PERSONA/">https://mks0601.github.io/PERSONA/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åˆ›å»ºåŠ¨ç”»åŒ–äººç±»è§’è‰²ï¼ˆavatarï¼‰çš„ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼Œå¹¶ç»“åˆä¸¤ç§æ–¹æ³•ä¼˜ç‚¹æå‡ºäº†ä¸€ä¸ªåä¸ºPERSONAçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾ç‰‡ç”Ÿæˆä¸ªæ€§åŒ–çš„å¸¦æœ‰åŠ¨ä½œé©±åŠ¨å˜å½¢çš„3Däººç±»è§’è‰²ã€‚é€šè¿‡é‡‡ç”¨åŸºäºæ‰©æ•£çš„æ–¹æ³•ç”Ÿæˆå§¿æ€ä¸°å¯Œçš„è§†é¢‘ï¼Œå¹¶æ ¹æ®è¿™äº›è§†é¢‘ä¼˜åŒ–3Dè§’è‰²ã€‚ä¸ºäº†ç¡®ä¿åœ¨ä¸åŒå§¿æ€ä¸‹çš„é«˜çœŸå®æ€§å’Œæ¸…æ™°æ¸²æŸ“ï¼Œå¼•å…¥äº†å¹³è¡¡é‡‡æ ·å’Œå‡ ä½•åŠ æƒä¼˜åŒ–æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åˆ›å»ºåŠ¨ç”»åŒ–äººç±»è§’è‰²çš„ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼šåŸºäº3Dçš„æ–¹æ³•å’ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚</li>
<li>åŸºäº3Dçš„æ–¹æ³•é€šè¿‡ä¼˜åŒ–NeRFæˆ–3DGSçš„ä¸ªæ€§åŒ–è§’è‰²ä»å•ä¸€è§†é¢‘å®ç°ä¸ªæ€§åŒ–ï¼Œä½†å»ºæ¨¡åŠ¨ä½œé©±åŠ¨å˜å½¢éœ€è¦å¤§é‡å§¿æ€ä¸°å¯Œçš„è§†é¢‘ï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ—¥å¸¸ç”Ÿæ´»ä¸­éš¾ä»¥å®ç°ã€‚</li>
<li>åŸºäºæ‰©æ•£çš„æ–¹æ³•èƒ½ä»å¤§è§„æ¨¡é‡å¤–è§†é¢‘ä¸­å­¦ä¹ åŠ¨ä½œé©±åŠ¨å˜å½¢ï¼Œä½†é¢ä¸´èº«ä»½ä¿ç•™å’Œå§¿æ€ç›¸å…³çš„èº«ä»½çº ç¼ é—®é¢˜ã€‚</li>
<li>æå‡ºäº†PERSONAæ¡†æ¶ï¼Œç»“åˆä¸Šè¿°ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œä»å•å¼ å›¾ç‰‡ç”Ÿæˆä¸ªæ€§åŒ–çš„å¸¦æœ‰åŠ¨ä½œé©±åŠ¨å˜å½¢çš„3Däººç±»è§’è‰²ã€‚</li>
<li>PERSONAåˆ©ç”¨åŸºäºæ‰©æ•£çš„æ–¹æ³•ç”Ÿæˆå§¿æ€ä¸°å¯Œçš„è§†é¢‘ï¼Œå¹¶æ ¹æ®è¿™äº›è§†é¢‘ä¼˜åŒ–3Dè§’è‰²ã€‚</li>
<li>ä¸ºç¡®ä¿ä¸åŒå§¿æ€ä¸‹çš„é«˜çœŸå®æ€§å’Œæ¸…æ™°æ¸²æŸ“ï¼Œå¼•å…¥äº†å¹³è¡¡é‡‡æ ·æŠ€æœ¯æ¥å‡è½»æ‰©æ•£ç”Ÿæˆè®­ç»ƒè§†é¢‘ä¸­èº«ä»½å˜åŒ–çš„é—®é¢˜ï¼Œä»¥åŠå‡ ä½•åŠ æƒä¼˜åŒ–æŠ€æœ¯æ¥ä¼˜å…ˆå¤„ç†å‡ ä½•çº¦æŸï¼Œä¿è¯å›¾åƒæŸå¤±æœ€å°åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba8d4685cb204210e14f12563bab4cd7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7194d1e5939151d7275b87c3268e9862.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5330741114b86a502931db1d156691d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2193ef3653c49226a06ba56b0f23e940.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3de6c8c965bd3d17eabb4ec183fc9373.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-321de461f0e587c11e82d6931cce4d7c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="E-4DGS-High-Fidelity-Dynamic-Reconstruction-from-the-Multi-view-Event-Cameras"><a href="#E-4DGS-High-Fidelity-Dynamic-Reconstruction-from-the-Multi-view-Event-Cameras" class="headerlink" title="E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event   Cameras"></a>E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event   Cameras</h2><p><strong>Authors:Chaoran Feng, Zhenyu Tang, Wangbo Yu, Yatian Pang, Yian Zhao, Jianbin Zhao, Li Yuan, Yonghong Tian</strong></p>
<p>Novel view synthesis and 4D reconstruction techniques predominantly rely on RGB cameras, thereby inheriting inherent limitations such as the dependence on adequate lighting, susceptibility to motion blur, and a limited dynamic range. Event cameras, offering advantages of low power, high temporal resolution and high dynamic range, have brought a new perspective to addressing the scene reconstruction challenges in high-speed motion and low-light scenes. To this end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting approach, for novel view synthesis from multi-view event streams with fast-moving cameras. Specifically, we introduce an event-based initialization scheme to ensure stable training and propose event-adaptive slicing splatting for time-aware reconstruction. Additionally, we employ intensity importance pruning to eliminate floating artifacts and enhance 3D consistency, while incorporating an adaptive contrast threshold for more precise optimization. We design a synthetic multi-view camera setup with six moving event cameras surrounding the object in a 360-degree configuration and provide a benchmark multi-view event stream dataset that captures challenging motion scenarios. Our approach outperforms both event-only and event-RGB fusion baselines and paves the way for the exploration of multi-view event-based reconstruction as a novel approach for rapid scene capture. </p>
<blockquote>
<p>æ–°å‹è§†å›¾åˆæˆå’Œ4Dé‡å»ºæŠ€æœ¯ä¸»è¦ä¾èµ–äºRGBç›¸æœºï¼Œä»è€Œç»§æ‰¿äº†å¦‚ä¾èµ–å……è¶³å…‰çº¿ã€æ˜“å—åˆ°è¿åŠ¨æ¨¡ç³Šå½±å“ä»¥åŠåŠ¨æ€èŒƒå›´æœ‰é™çš„å›ºæœ‰å±€é™æ€§ã€‚äº‹ä»¶ç›¸æœºå…·æœ‰ä½åŠŸè€—ã€é«˜æ—¶é—´åˆ†è¾¨ç‡å’Œé«˜åŠ¨æ€èŒƒå›´ç­‰ä¼˜ç‚¹ï¼Œä¸ºé«˜é€Ÿè¿åŠ¨åœºæ™¯å’Œä½å…‰åœºæ™¯ä¸­çš„åœºæ™¯é‡å»ºæŒ‘æˆ˜æä¾›äº†æ–°çš„è§†è§’ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†E-4DGSï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºäº‹ä»¶é©±åŠ¨çš„åŠ¨æ€é«˜æ–¯æ¶‚æŠ¹æ–¹æ³•ï¼Œç”¨äºä»å¿«é€Ÿç§»åŠ¨çš„ç›¸æœºå¤šè§†å›¾äº‹ä»¶æµä¸­è¿›è¡Œæ–°å‹è§†å›¾åˆæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºäº‹ä»¶çš„åˆå§‹åŒ–æ–¹æ¡ˆï¼Œä»¥ç¡®ä¿ç¨³å®šçš„è®­ç»ƒï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºäº‹ä»¶è‡ªé€‚åº”åˆ‡ç‰‡æ¶‚æŠ¹æ–¹æ³•è¿›è¡Œæ—¶é—´æ„ŸçŸ¥é‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¼ºåº¦é‡è¦æ€§ä¿®å‰ªæ¥æ¶ˆé™¤æ¼‚æµ®çš„ä¼ªå½±ï¼Œå¢å¼º3Dä¸€è‡´æ€§ï¼ŒåŒæ—¶é‡‡ç”¨è‡ªé€‚åº”å¯¹æ¯”åº¦é˜ˆå€¼è¿›è¡Œæ›´ç²¾ç¡®çš„ä¼˜åŒ–ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåˆæˆå¤šè§†è§’ç›¸æœºè®¾ç½®ï¼ŒåŒ…æ‹¬å…­ä¸ªå›´ç»•ç‰©ä½“ä»¥360åº¦é…ç½®ç§»åŠ¨çš„äº‹ä»¶ç›¸æœºï¼Œå¹¶æä¾›äº†ä¸€ä¸ªåŸºå‡†å¤šè§†è§’äº‹ä»¶æµæ•°æ®é›†ï¼Œæ•æ‰å…·æœ‰æŒ‘æˆ˜æ€§çš„è¿åŠ¨åœºæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºä»…ä½¿ç”¨äº‹ä»¶å’Œäº‹ä»¶-RGBèåˆåŸºçº¿ï¼Œä¸ºåŸºäºå¤šè§†è§’äº‹ä»¶é‡å»ºçš„å¿«é€Ÿåœºæ™¯æ•è·æ–°æ–¹æ³•çš„æ¢ç´¢é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09912v1">PDF</a> 16 pages, 10 figures, 5 Tables, accepted by ACMMM 2025</p>
<p><strong>Summary</strong></p>
<p>äº‹ä»¶ç›¸æœºå…·æœ‰ä½åŠŸè€—ã€é«˜æ—¶é—´åˆ†è¾¨ç‡å’Œé«˜åŠ¨æ€èŒƒå›´ç­‰ä¼˜ç‚¹ï¼Œä¸ºé«˜é€Ÿè¿åŠ¨å’Œä½å…‰åœºæ™¯çš„åœºæ™¯é‡å»ºå¸¦æ¥äº†æ–°çš„è§†è§’ã€‚é’ˆå¯¹æ­¤ï¼Œæå‡ºE-4DGSï¼Œé¦–ä¸ªäº‹ä»¶é©±åŠ¨çš„åŠ¨æ€é«˜æ–¯é£æº…æ–¹æ³•ï¼Œç”¨äºä»å¤šè§†è§’äº‹ä»¶æµä¸­è¿›è¡Œæ–°é¢–è§†å›¾åˆæˆã€‚å¼•å…¥äº‹ä»¶åˆå§‹åŒ–æ–¹æ¡ˆä»¥ç¡®ä¿ç¨³å®šè®­ç»ƒï¼Œå¹¶æå‡ºäº‹ä»¶è‡ªé€‚åº”åˆ‡ç‰‡é£æº…è¿›è¡Œæ—¶é—´æ„ŸçŸ¥é‡å»ºã€‚é‡‡ç”¨å¼ºåº¦é‡è¦æ€§ä¿®å‰ªæ¶ˆé™¤æµ®åŠ¨ä¼ªå½±å¹¶å¢å¼º3Dä¸€è‡´æ€§ï¼ŒåŒæ—¶ç»“åˆè‡ªé€‚åº”å¯¹æ¯”åº¦é˜ˆå€¼è¿›è¡Œæ›´ç²¾ç¡®çš„ä¼˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‹ä»¶ç›¸æœºåœ¨åœºæ™¯é‡å»ºä¸­å…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é€Ÿè¿åŠ¨å’Œä½å…‰åœºæ™¯ä¸­ã€‚</li>
<li>E-4DGSæ˜¯é¦–ä¸ªäº‹ä»¶é©±åŠ¨çš„åŠ¨æ€é«˜æ–¯é£æº…æ–¹æ³•ï¼Œç”¨äºä»å¤šè§†è§’äº‹ä»¶æµè¿›è¡Œæ–°é¢–è§†å›¾åˆæˆã€‚</li>
<li>å¼•å…¥äº‹ä»¶åˆå§‹åŒ–æ–¹æ¡ˆç¡®ä¿ç¨³å®šè®­ç»ƒï¼Œå¹¶æå‡ºäº‹ä»¶è‡ªé€‚åº”åˆ‡ç‰‡é£æº…è¿›è¡Œæ—¶é—´æ„ŸçŸ¥é‡å»ºã€‚</li>
<li>é‡‡ç”¨å¼ºåº¦é‡è¦æ€§ä¿®å‰ªæŠ€æœ¯æ¶ˆé™¤æµ®åŠ¨ä¼ªå½±ï¼Œå¢å¼º3Dä¸€è‡´æ€§ã€‚</li>
<li>ç»“åˆè‡ªé€‚åº”å¯¹æ¯”åº¦é˜ˆå€¼è¿›è¡Œæ›´ç²¾ç¡®çš„ä¼˜åŒ–ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªåˆæˆå¤šè§†è§’ç›¸æœºè£…ç½®ï¼Œé‡‡ç”¨å…­ä¸ªå›´ç»•ç‰©ä½“360åº¦é…ç½®çš„è¿åŠ¨äº‹ä»¶ç›¸æœºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09912">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4aaa5f27354aafb066c214496a768653.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47b6ba4325b191fa76b8a30114f4871c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d75c61e1f56c0c9f232bf9571f5c1cac.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HumanGenesis-Agent-Based-Geometric-and-Generative-Modeling-for-Synthetic-Human-Dynamics"><a href="#HumanGenesis-Agent-Based-Geometric-and-Generative-Modeling-for-Synthetic-Human-Dynamics" class="headerlink" title="HumanGenesis: Agent-Based Geometric and Generative Modeling for   Synthetic Human Dynamics"></a>HumanGenesis: Agent-Based Geometric and Generative Modeling for   Synthetic Human Dynamics</h2><p><strong>Authors:Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang</strong></p>
<p>\textbf{Synthetic human dynamics} aims to generate photorealistic videos of human subjects performing expressive, intention-driven motions. However, current approaches face two core challenges: (1) \emph{geometric inconsistency} and \emph{coarse reconstruction}, due to limited 3D modeling and detail preservation; and (2) \emph{motion generalization limitations} and \emph{scene inharmonization}, stemming from weak generative capabilities. To address these, we present \textbf{HumanGenesis}, a framework that integrates geometric and generative modeling through four collaborative agents: (1) \textbf{Reconstructor} builds 3D-consistent human-scene representations from monocular video using 3D Gaussian Splatting and deformation decomposition. (2) \textbf{Critique Agent} enhances reconstruction fidelity by identifying and refining poor regions via multi-round MLLM-based reflection. (3) \textbf{Pose Guider} enables motion generalization by generating expressive pose sequences using time-aware parametric encoders. (4) \textbf{Video Harmonizer} synthesizes photorealistic, coherent video via a hybrid rendering pipeline with diffusion, refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis achieves state-of-the-art performance on tasks including text-guided synthesis, video reenactment, and novel-pose generalization, significantly improving expressiveness, geometric fidelity, and scene integration. </p>
<blockquote>
<p><strong>åˆæˆäººç±»åŠ¨åŠ›å­¦</strong>æ—¨åœ¨ç”Ÿæˆè¡¨è¾¾å¼ºçƒˆã€æ„å›¾é©±åŠ¨åŠ¨ä½œçš„äººç±»ä¸»ä½“çš„é€¼çœŸè§†é¢‘ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼š(1)ç”±äº3Då»ºæ¨¡å’Œç»†èŠ‚ä¿ç•™æœ‰é™è€Œå¯¼è‡´çš„<strong>å‡ ä½•ä¸ä¸€è‡´æ€§å’Œç²—ç³™é‡å»º</strong>ï¼›(2)ç”±äºç”Ÿæˆèƒ½åŠ›è¾ƒå¼±è€Œå¯¼è‡´çš„<strong>è¿åŠ¨æ³›åŒ–é™åˆ¶å’Œåœºæ™¯ä¸å’Œè°</strong>ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>HumanGenesis</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å››ä¸ªåä½œä»£ç†æ•´åˆå‡ ä½•å’Œç”Ÿæˆå»ºæ¨¡çš„æ¡†æ¶ï¼š(1)<strong>é‡å»ºå™¨</strong>ä½¿ç”¨3Dé«˜æ–¯å–·æ¶‚å’Œå˜å½¢åˆ†è§£ä»å•ç›®è§†é¢‘ä¸­æ„å»º3Dä¸€è‡´çš„äºº-åœºæ™¯è¡¨ç¤ºã€‚(2)<strong>è¯„ä»·ä»£ç†</strong>é€šè¿‡å¤šè½®MLLMåŸºäºåå°„æ¥è¯†åˆ«å’Œä¼˜åŒ–ä¸è‰¯åŒºåŸŸï¼Œä»è€Œæé«˜é‡å»ºçš„ä¿çœŸåº¦ã€‚(3)<strong>å§¿åŠ¿å¼•å¯¼è€…</strong>é€šè¿‡ä½¿ç”¨æ—¶é—´æ„ŸçŸ¥å‚æ•°ç¼–ç å™¨ç”Ÿæˆè¡¨è¾¾æ€§å§¿åŠ¿åºåˆ—ï¼Œä»è€Œå®ç°è¿åŠ¨æ³›åŒ–ã€‚(4)<strong>è§†é¢‘åè°ƒå™¨</strong>é€šè¿‡æ··åˆæ¸²æŸ“ç®¡é“è¿›è¡Œæ‰©æ•£ï¼Œé€šè¿‡Back-to-4Dåé¦ˆå¾ªç¯ä¼˜åŒ–é‡å»ºå™¨ï¼Œåˆæˆé€¼çœŸçš„ã€è¿è´¯çš„è§†é¢‘ã€‚HumanGenesisåœ¨æ–‡æœ¬å¼•å¯¼åˆæˆã€è§†é¢‘é‡æ¼”å’Œæ–°é¢–å§¿åŠ¿æ³›åŒ–ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨è¡¨è¾¾æ€§ã€å‡ ä½•ä¿çœŸæ€§å’Œåœºæ™¯é›†æˆæ–¹é¢æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09858v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆæˆäººç±»åŠ¨åŠ›å­¦ï¼ˆSynthetic human dynamicsï¼‰çš„ç›®æ ‡ï¼Œæ—¨åœ¨ç”Ÿæˆå…·æœ‰è¡¨ç°åŠ›ã€æ„å›¾é©±åŠ¨åŠ¨ä½œçš„å…‰å­¦çœŸå®è§†é¢‘ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•é¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šå‡ ä½•ä¸ä¸€è‡´æ€§å’Œç²—ç•¥é‡å»ºä»¥åŠè¿åŠ¨æ³›åŒ–é™åˆ¶å’Œåœºæ™¯ä¸å’Œè°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†HumanGenesisæ¡†æ¶ï¼Œé€šè¿‡å››ä¸ªåä½œä»£ç†æ•´åˆå‡ ä½•å’Œç”Ÿæˆå»ºæ¨¡ï¼ŒåŒ…æ‹¬é‡å»ºå™¨ã€æ‰¹åˆ¤ä»£ç†ã€å§¿æ€å¼•å¯¼å™¨å’Œè§†é¢‘å’Œè°å™¨ã€‚HumanGenesisåœ¨æ–‡æœ¬å¼•å¯¼åˆæˆã€è§†é¢‘é‡æ¼”å’Œæ–°é¢–å§¿æ€æ³›åŒ–ç­‰ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†è¡¨ç°åŠ›ã€å‡ ä½•ä¿çœŸåº¦å’Œåœºæ™¯é›†æˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½“å‰åˆæˆäººç±»åŠ¨åŠ›å­¦é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬å‡ ä½•ä¸ä¸€è‡´æ€§å’Œç²—ç•¥é‡å»ºï¼Œä»¥åŠè¿åŠ¨æ³›åŒ–é™åˆ¶å’Œåœºæ™¯ä¸å’Œè°ã€‚</li>
<li>HumanGenesisæ¡†æ¶é€šè¿‡æ•´åˆå‡ ä½•å’Œç”Ÿæˆå»ºæ¨¡æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>HumanGenesisåŒ…å«å››ä¸ªåä½œä»£ç†ï¼šé‡å»ºå™¨ã€æ‰¹åˆ¤ä»£ç†ã€å§¿æ€å¼•å¯¼å™¨å’Œè§†é¢‘å’Œè°å™¨ã€‚</li>
<li>é‡å»ºå™¨ä½¿ç”¨3Dé«˜æ–¯æ‹¼è´´å’Œå˜å½¢åˆ†è§£æŠ€æœ¯ä»å•ç›®è§†é¢‘ä¸­æ„å»ºä¸€è‡´çš„3Däººç±»åœºæ™¯è¡¨ç¤ºã€‚</li>
<li>æ‰¹åˆ¤ä»£ç†é€šè¿‡å¤šè½®MLLMåé¦ˆå¢å¼ºé‡å»ºçš„ä¿çœŸåº¦ã€‚</li>
<li>å§¿æ€å¼•å¯¼å™¨é€šè¿‡æ—¶é—´æ„ŸçŸ¥ç¼–ç å™¨ç”Ÿæˆè¡¨ç°åŠ›å§¿åŠ¿åºåˆ—ï¼Œä½¿è¿åŠ¨æ³›åŒ–æˆä¸ºå¯èƒ½ã€‚</li>
<li>è§†é¢‘å’Œè°å™¨é€šè¿‡æ··åˆæ¸²æŸ“ç®¡é“å’Œæ‰©æ•£æŠ€æœ¯ï¼Œåˆæˆå…‰å­¦çœŸå®çš„è¿è´¯è§†é¢‘ï¼Œå¹¶é€šè¿‡åé¦ˆå¾ªç¯ä¼˜åŒ–é‡å»ºå™¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bb06d1e0e3fb6ddcc349d6f69ff864b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8dc2ff97445f89f928b14db25ffcef7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a5a313c233c7a7298c0fca946898426.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0acb34337ca1635ffc17bbc026b456d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e24333e3a1d25bab6870e3e5485f23c3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RayletDF-Raylet-Distance-Fields-for-Generalizable-3D-Surface-Reconstruction-from-Point-Clouds-or-Gaussians"><a href="#RayletDF-Raylet-Distance-Fields-for-Generalizable-3D-Surface-Reconstruction-from-Point-Clouds-or-Gaussians" class="headerlink" title="RayletDF: Raylet Distance Fields for Generalizable 3D Surface   Reconstruction from Point Clouds or Gaussians"></a>RayletDF: Raylet Distance Fields for Generalizable 3D Surface   Reconstruction from Point Clouds or Gaussians</h2><p><strong>Authors:Shenxing Wei, Jinxi Li, Yafei Yang, Siyuan Zhou, Bo Yang</strong></p>
<p>In this paper, we present a generalizable method for 3D surface reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from RGB images. Unlike existing coordinate-based methods which are often computationally intensive when rendering explicit surfaces, our proposed method, named RayletDF, introduces a new technique called raylet distance field, which aims to directly predict surface points from query rays. Our pipeline consists of three key modules: a raylet feature extractor, a raylet distance field predictor, and a multi-raylet blender. These components work together to extract fine-grained local geometric features, predict raylet distances, and aggregate multiple predictions to reconstruct precise surface points. We extensively evaluate our method on multiple public real-world datasets, demonstrating superior performance in surface reconstruction from point clouds or 3D Gaussians. Most notably, our method achieves exceptional generalization ability, successfully recovering 3D surfaces in a single-forward pass across unseen datasets in testing. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨RGBå›¾åƒè¿›è¡ŒåŸºäº3DGSçš„åŸå§‹ç‚¹äº‘æˆ–é¢„ä¼°è®¡çš„3Dé«˜æ–¯åˆ†å¸ƒçš„é€šç”¨æ–¹æ³•è¿›è¡Œä¸‰ç»´è¡¨é¢é‡å»ºçš„æ–¹æ³•ã€‚ä¸åŒäºç°æœ‰çš„åæ ‡å›å½’æ–¹æ³•ç»å¸¸éœ€è¦å¤§é‡è®¡ç®—è¿›è¡Œæ˜¾æ€§è¡¨é¢çš„æ¸²æŸ“ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ç§°ä¸ºRayletDFï¼Œå¼•å…¥äº†rayletè·ç¦»åœºè¿™ä¸€æ–°æŠ€æœ¯ï¼Œæ—¨åœ¨ç›´æ¥ä»æŸ¥è¯¢å°„çº¿é¢„æµ‹è¡¨é¢ç‚¹ã€‚æˆ‘ä»¬çš„ç®¡é“åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šrayletç‰¹å¾æå–å™¨ã€rayletè·ç¦»åœºé¢„æµ‹å™¨å’Œå¤šrayletæ··åˆå™¨ã€‚è¿™äº›ç»„ä»¶ååŒå·¥ä½œï¼Œæå–ç²¾ç»†çš„å±€éƒ¨å‡ ä½•ç‰¹å¾ï¼Œé¢„æµ‹rayletè·ç¦»ï¼Œå¹¶èåˆå¤šä¸ªé¢„æµ‹ç»“æœä»¥é‡å»ºç²¾ç¡®çš„è¡¨é¢ç‚¹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå…¬å¼€çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå±•ç¤ºäº†ä»ç‚¹äº‘æˆ–ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒè¿›è¡Œè¡¨é¢é‡å»ºæ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ã€‚æœ€å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æµ‹è¯•æ—¶èƒ½åœ¨å•å¼ æœªæ¥è§¦è¿‡çš„æ•°æ®é›†ä¸Šä¸€æ¬¡æ€§æˆåŠŸåœ°é‡å»ºå‡ºä¸‰ç»´è¡¨é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09830v1">PDF</a> ICCV 2025 Highlight. Shenxing and Jinxi are co-first authors. Code   and data are available at: <a target="_blank" rel="noopener" href="https://github.com/vLAR-group/RayletDF">https://github.com/vLAR-group/RayletDF</a></p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨RGBå›¾åƒé€šè¿‡3DGSè¿›è¡Œä¸‰ç»´è¡¨é¢é‡å»ºçš„é€šç”¨æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»åŸå§‹ç‚¹äº‘æˆ–é¢„å…ˆä¼°è®¡çš„3Dé«˜æ–¯æ•°æ®ä¸­é‡å»ºä¸‰ç»´è¡¨é¢ã€‚ä¸ç°æœ‰çš„åæ ‡åŸºç¡€æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åä¸ºRayletDFï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºå°„çº¿è·ç¦»åœºçš„æ–°æŠ€æœ¯ï¼Œæ—¨åœ¨ç›´æ¥ä»æŸ¥è¯¢å°„çº¿é¢„æµ‹è¡¨é¢ç‚¹ã€‚è¯¥æ–¹æ³•åŒ…å«ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šå°„çº¿ç‰¹å¾æå–å™¨ã€å°„çº¿è·ç¦»åœºé¢„æµ‹å™¨å’Œå¤šå°„çº¿æ··åˆå™¨ã€‚è¿™ä¸‰ä¸ªç»„ä»¶ååŒå·¥ä½œï¼Œç”¨äºæå–ç²¾ç»†å±€éƒ¨å‡ ä½•ç‰¹å¾ã€é¢„æµ‹å°„çº¿è·ç¦»å¹¶èšåˆå¤šä¸ªé¢„æµ‹ç»“æœä»¥é‡å»ºç²¾ç¡®çš„è¡¨é¢ç‚¹ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå…¬å…±çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå±•ç¤ºäº†å¯¹ç‚¹äº‘æˆ–ä¸‰ç»´é«˜æ–¯æ•°æ®çš„è¡¨é¢é‡å»ºçš„å“è¶Šæ€§èƒ½ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æµ‹è¯•ä¸­çš„æœªè§æ•°æ®é›†ä¸Šä¸€æ¬¡æ€§é€šè¿‡æ¢å¤ä¸‰ç»´è¡¨é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºä¸€ç§åä¸ºRayletDFçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»RGBå›¾åƒé€šè¿‡3DGSè¿›è¡Œä¸‰ç»´è¡¨é¢é‡å»ºã€‚</li>
<li>å¼•å…¥å°„çº¿è·ç¦»åœºæŠ€æœ¯ï¼Œç›´æ¥é¢„æµ‹æŸ¥è¯¢å°„çº¿ä¸Šçš„è¡¨é¢ç‚¹ã€‚</li>
<li>æ–¹æ³•åŒ…å«ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šå°„çº¿ç‰¹å¾æå–å™¨ã€å°„çº¿è·ç¦»åœºé¢„æµ‹å™¨å’Œå¤šå°„çº¿æ··åˆå™¨ã€‚</li>
<li>å¹¿æ³›è¯„ä¼°è¡¨æ˜åœ¨å¤šä¸ªå…¬å…±çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨é¢é‡å»ºæ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>æ–¹æ³•å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨æœªè§æ•°æ®é›†ä¸ŠæˆåŠŸæ¢å¤ä¸‰ç»´è¡¨é¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ed876853ad91fbb4c7848eee2c9e3d3f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8264cd59afcc3deac47468b2257a0e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de80164a3715d915b2ff8d76db2d150a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14c8fe98cd1dc178e7a918e2a957010a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6895693131253bbb127eac05a197f304.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-400aee5e8b0b1eee9d46447b20d985ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-633b43f47d406d8dd611e44851ed6c71.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TRACE-Learning-3D-Gaussian-Physical-Dynamics-from-Multi-view-Videos"><a href="#TRACE-Learning-3D-Gaussian-Physical-Dynamics-from-Multi-view-Videos" class="headerlink" title="TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos"></a>TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos</h2><p><strong>Authors:Jinxi Li, Ziyang Song, Bo Yang</strong></p>
<p>In this paper, we aim to model 3D scene geometry, appearance, and physical information just from dynamic multi-view videos in the absence of any human labels. By leveraging physics-informed losses as soft constraints or integrating simple physics models into neural nets, existing works often fail to learn complex motion physics, or doing so requires additional labels such as object types or masks. We propose a new framework named TRACE to model the motion physics of complex dynamic 3D scenes. The key novelty of our method is that, by formulating each 3D point as a rigid particle with size and orientation in space, we directly learn a translation rotation dynamics system for each particle, explicitly estimating a complete set of physical parameters to govern the particleâ€™s motion over time. Extensive experiments on three existing dynamic datasets and one newly created challenging synthetic datasets demonstrate the extraordinary performance of our method over baselines in the task of future frame extrapolation. A nice property of our framework is that multiple objects or parts can be easily segmented just by clustering the learned physical parameters. </p>
<blockquote>
<p>æœ¬æ–‡çš„ç›®æ ‡æ˜¯ä»æ— æ ‡ç­¾çš„åŠ¨æ€å¤šè§†è§’è§†é¢‘ä¸­ï¼Œå¯¹3Dåœºæ™¯è¿›è¡Œå‡ ä½•ã€å¤–è§‚å’Œç‰©ç†ä¿¡æ¯çš„å»ºæ¨¡ã€‚å°½ç®¡ç°æœ‰çš„å·¥ä½œé€šè¿‡åˆ©ç”¨ç‰©ç†ä¿¡æ¯æŸå¤±ä½œä¸ºè½¯çº¦æŸæˆ–å°†ç®€å•çš„ç‰©ç†æ¨¡å‹é›†æˆåˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œä½†å¾€å¾€æ— æ³•å­¦ä¹ åˆ°å¤æ‚çš„è¿åŠ¨ç‰©ç†ï¼Œæˆ–è€…éœ€è¦è¿›è¡Œé¢å¤–çš„æ ‡ç­¾æ ‡æ³¨ï¼Œå¦‚å¯¹è±¡ç±»å‹æˆ–æ©ç ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶TRACEæ¥æ¨¡æ‹Ÿå¤æ‚åŠ¨æ€åœºæ™¯çš„è¿åŠ¨ç‰©ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®æ–°é¢–ä¹‹å¤„åœ¨äºï¼Œé€šè¿‡å°†æ¯ä¸ª3Dç‚¹å®šä¹‰ä¸ºå…·æœ‰å¤§å°å’Œç©ºé—´æ–¹å‘çš„åˆšæ€§ç²’å­ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªç²’å­ç›´æ¥å­¦ä¹ ä¸€ä¸ªç¿»è¯‘æ—‹è½¬åŠ¨åŠ›ç³»ç»Ÿï¼Œå¹¶æ˜ç¡®ä¼°è®¡ä¸€å¥—å®Œæ•´çš„ç‰©ç†å‚æ•°æ¥æ”¯é…ç²’å­éšæ—¶é—´å˜åŒ–çš„è¿åŠ¨ã€‚åœ¨ä¸‰ä¸ªç°æœ‰çš„åŠ¨æ€æ•°æ®é›†å’Œä¸€ä¸ªæ–°åˆ›å»ºçš„æœ‰æŒ‘æˆ˜æ€§çš„åˆæˆæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœªæ¥å¸§å¤–æ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåŸºå‡†çº¿ã€‚æˆ‘ä»¬çš„æ¡†æ¶çš„å¦ä¸€ä¸ªä¼˜ç‚¹æ˜¯ï¼Œå¯ä»¥é€šè¿‡èšç±»å­¦ä¹ çš„ç‰©ç†å‚æ•°è½»æ¾åˆ†å‰²å¤šä¸ªå¯¹è±¡æˆ–éƒ¨åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09811v1">PDF</a> ICCV 2025. Code and data are available at:   <a target="_blank" rel="noopener" href="https://github.com/vLAR-group/TRACE">https://github.com/vLAR-group/TRACE</a></p>
<p><strong>Summary</strong><br>æœ¬æ–‡æ—¨åœ¨ä»åŠ¨æ€å¤šè§†è§’è§†é¢‘ä¸­å»ºæ¨¡3Dåœºæ™¯å‡ ä½•ã€å¤–è§‚å’Œç‰©ç†ä¿¡æ¯ï¼Œæ— éœ€ä»»ä½•äººå·¥æ ‡ç­¾ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºTRACEçš„æ–°æ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿå¤æ‚åŠ¨æ€3Dåœºæ™¯çš„è¿åŠ¨ç‰©ç†ã€‚è¯¥æ–¹æ³•çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºå°†æ¯ä¸ª3Dç‚¹è§†ä¸ºå…·æœ‰ç©ºé—´å’Œæ–¹å‘æ€§çš„åˆšæ€§ç²’å­ï¼Œç›´æ¥å­¦ä¹ æ¯ä¸ªç²’å­çš„å¹³ç§»æ—‹è½¬åŠ¨åŠ›ç³»ç»Ÿï¼Œå¹¶æ˜¾å¼ä¼°è®¡ä¸€ç»„ç‰©ç†å‚æ•°æ¥ç®¡ç†ç²’å­éšæ—¶é—´å˜åŒ–çš„è¿åŠ¨ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æœªæ¥å¸§å¤–æ¨ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œå¹¶å¯é€šè¿‡å­¦ä¹ åˆ°çš„ç‰©ç†å‚æ•°è½»æ¾å®ç°å¯¹å¤šä¸ªå¯¹è±¡æˆ–éƒ¨åˆ†çš„åˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–‡ç« ç›®æ ‡æ˜¯åˆ©ç”¨åŠ¨æ€å¤šè§†è§’è§†é¢‘å»ºæ¨¡3Dåœºæ™¯çš„å‡ ä½•ã€å¤–è§‚å’Œç‰©ç†ä¿¡æ¯ï¼Œæ— éœ€äººå·¥æ ‡ç­¾ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºTRACEçš„æ–°æ¡†æ¶ï¼Œç”¨ä»¥æ¨¡æ‹Ÿå¤æ‚åŠ¨æ€åœºæ™¯çš„è¿åŠ¨ç‰©ç†ã€‚</li>
<li>è¯¥æ–¹æ³•çš„å…³é”®åˆ›æ–°åœ¨äºå°†æ¯ä¸ª3Dç‚¹è§†ä¸ºåˆšæ€§ç²’å­ï¼Œå¹¶å­¦ä¹ å…¶å¹³ç§»æ—‹è½¬åŠ¨åŠ›ç³»ç»Ÿã€‚</li>
<li>æ–¹æ³•é€šè¿‡æ˜¾å¼ä¼°è®¡ç‰©ç†å‚æ•°æ¥ç®¡ç†ç²’å­è¿åŠ¨çš„å¤æ‚æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœªæ¥å¸§å¤–æ¨ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥è½»æ¾é€šè¿‡èšç±»å­¦ä¹ åˆ°çš„ç‰©ç†å‚æ•°å®ç°å¯¹å¤šä¸ªå¯¹è±¡æˆ–éƒ¨åˆ†çš„åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09811">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8b506d82d67235c9723ac88a0036084.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-394a4e7ee3f1e90b2e739988cb5158ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44314d9dae9a7c9913f4c2cbef4076bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-551cd8ff825c6a9499c33f8d265d912d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors"><a href="#GSFixer-Improving-3D-Gaussian-Splatting-with-Reference-Guided-Video-Diffusion-Priors" class="headerlink" title="GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video   Diffusion Priors"></a>GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video   Diffusion Priors</h2><p><strong>Authors:Xingyilang Yin, Qi Zhang, Jiahao Chang, Ying Feng, Qingnan Fan, Xi Yang, Chi-Man Pun, Huaqi Zhang, Xiaodong Cun</strong></p>
<p>Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views is an ill-posed problem due to insufficient information, often resulting in noticeable artifacts. While recent approaches have sought to leverage generative priors to complete information for under-constrained regions, they struggle to generate content that remains consistent with input observations. To address this challenge, we propose GSFixer, a novel framework designed to improve the quality of 3DGS representations reconstructed from sparse inputs. The core of our approach is the reference-guided video restoration model, built upon a DiT-based video diffusion model trained on paired artifact 3DGS renders and clean frames with additional reference-based conditions. Considering the input sparse views as references, our model integrates both 2D semantic features and 3D geometric features of reference views extracted from the visual geometry foundation model, enhancing the semantic coherence and 3D consistency when fixing artifact novel views. Furthermore, considering the lack of suitable benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which contains artifact frames rendered using low-quality 3DGS. Extensive experiments demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS artifact restoration and sparse-view 3D reconstruction. Project page: <a target="_blank" rel="noopener" href="https://github.com/GVCLab/GSFixer">https://github.com/GVCLab/GSFixer</a>. </p>
<blockquote>
<p>ä½¿ç”¨åŸºäºç¨€ç–è§†è§’çš„3Dé«˜æ–¯åˆ†è£‚ï¼ˆ3DGSï¼‰é‡å»ºä¸‰ç»´åœºæ™¯æ˜¯ä¸€ä¸ªç—…æ€é—®é¢˜ï¼Œç”±äºä¿¡æ¯ä¸è¶³ï¼Œå¸¸å¸¸ä¼šäº§ç”Ÿæ˜æ˜¾çš„ä¼ªå½±ã€‚å°½ç®¡è¿‘æœŸçš„æ–¹æ³•è¯•å›¾åˆ©ç”¨ç”Ÿæˆå…ˆéªŒæ¥å¡«è¡¥çº¦æŸä¸è¶³çš„åŒºåŸŸçš„å®Œæ•´æ€§ä¿¡æ¯ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥ç”Ÿæˆä¸è¾“å…¥è§‚æµ‹ä¸€è‡´çš„åƒç´ çº§åˆ«çš„ä¸‰ç»´å‡ ä½•ç»“æ„å’Œå†…å®¹ä¿æŒè¿è´¯æ€§çš„å†…å®¹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GSFixeræ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ä»ç¨€ç–è¾“å…¥é‡å»ºçš„3DGSè¡¨ç¤ºçš„è´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯å‚è€ƒå¼•å¯¼çš„è§†é¢‘æ¢å¤æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºåœ¨é…å¯¹çš„ä¼ªå½±æ¸²æŸ“çš„3DGSå¸§å’Œå¹²å‡€çš„å¸§ä¸Šè®­ç»ƒçš„åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é™„åŠ äº†åŸºäºå‚è€ƒçš„æ¡ä»¶ã€‚æˆ‘ä»¬çš„æ¨¡å‹å°†å‚è€ƒè§†è§’çš„äºŒç»´è¯­ä¹‰ç‰¹å¾å’Œä¸‰ç»´å‡ ä½•ç‰¹å¾èå…¥å…¶ä¸­ï¼Œé€šè¿‡ä»è§†è§‰å‡ ä½•åŸºç¡€æ¨¡å‹ä¸­æå–è¿™äº›ä¿¡æ¯ï¼Œåœ¨ä¿®å¤ä¼ªå½±çš„æ–°è§†è§’æ—¶å¢å¼ºè¯­ä¹‰è¿è´¯æ€§å’Œä¸‰ç»´ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°ç¼ºä¹é€‚åˆç”¨äºè¯„ä¼°3DGSä¼ªå½±æ¢å¤çš„åŸºå‡†æ•°æ®é›†ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DL3DV-Resæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä½¿ç”¨ä½è´¨é‡3DGSæ¸²æŸ“çš„ä¼ªå½±å¸§ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GSFixeråœ¨è§£å†³åŸºäºä¼ªå½±çš„è¯„ä¼°å’Œç¨€ç–è§†è§’ä¸‰ç»´é‡å»ºæ–¹é¢å…·æœ‰ä¼˜äºå½“å‰æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/GVCLab/GSFixer">https://github.com/GVCLab/GSFixer</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¨€ç–è§†å›¾çš„3Dé«˜æ–¯Splattingï¼ˆ3DGSï¼‰é‡å»ºæ˜¯ä¸€ä¸ªä¸é€‚å®šçš„é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´æ˜¾è‘—çš„ä¼ªå½±ã€‚ä¸ºäº†è§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†GSFixeræ¡†æ¶ï¼Œé€šè¿‡å‚è€ƒå¼•å¯¼çš„è§†é¢‘ä¿®å¤æ¨¡å‹æé«˜é‡å»ºè´¨é‡ã€‚è¯¥æ¨¡å‹ç»“åˆäºŒç»´è¯­ä¹‰ç‰¹å¾å’Œä¸‰ç»´å‡ ä½•ç‰¹å¾ï¼Œå¢å¼ºäº†è¯­ä¹‰è¿è´¯æ€§å’Œä¸‰ç»´ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†é’ˆå¯¹ç¼ºä¹é€‚å½“åŸºå‡†çš„é—®é¢˜è€Œè®¾è®¡çš„æ–°æ•°æ®é›†DL3DV-Resç”¨äºè¯„ä¼°ä¿®å¤æ•ˆæœã€‚å®éªŒè¯æ˜ï¼ŒGSFixeråœ¨è§£å†³ä¼ªå½±å’Œç¨€ç–è§†å›¾é‡å»ºæ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/GVCLab/GSFixer">https://github.com/GVCLab/GSFixer</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é‡å»ºåŸºäºç¨€ç–è§†å›¾çš„3Dåœºæ™¯æ˜¯ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ï¼Œå› ä¸ºä¿¡æ¯ä¸è¶³å¯èƒ½å¯¼è‡´æ˜¾è‘—çš„ä¼ªå½±ã€‚</li>
<li>GSFixeræ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé‡‡ç”¨å‚è€ƒå¼•å¯¼çš„è§†é¢‘ä¿®å¤æ¨¡å‹æ¥æé«˜é‡å»ºè´¨é‡ã€‚</li>
<li>GSFixerç»“åˆäº†äºŒç»´è¯­ä¹‰ç‰¹å¾å’Œä¸‰ç»´å‡ ä½•ç‰¹å¾ï¼Œå¢å¼ºè¯­ä¹‰è¿è´¯æ€§å’Œä¸‰ç»´ä¸€è‡´æ€§ã€‚ä¸ºæ­¤å¼•å…¥äº†æ–°çš„æ•°æ®é›†DL3DV-Resç”¨äºè¯„ä¼°ä¼ªå½±ä¿®å¤æ•ˆæœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-964c5666244e5de94a1383ccd953c651.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6611d05d5fc56bbe9395e3444e6affa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c77830b517097a084037acbd5e15f4da.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DualPhys-GS-Dual-Physically-Guided-3D-Gaussian-Splatting-for-Underwater-Scene-Reconstruction"><a href="#DualPhys-GS-Dual-Physically-Guided-3D-Gaussian-Splatting-for-Underwater-Scene-Reconstruction" class="headerlink" title="DualPhys-GS: Dual Physically-Guided 3D Gaussian Splatting for Underwater   Scene Reconstruction"></a>DualPhys-GS: Dual Physically-Guided 3D Gaussian Splatting for Underwater   Scene Reconstruction</h2><p><strong>Authors:Jiachen Li, Guangzhi Han, Jin Wan, Yuan Gao, Delong Han</strong></p>
<p>In 3D reconstruction of underwater scenes, traditional methods based on atmospheric optical models cannot effectively deal with the selective attenuation of light wavelengths and the effect of suspended particle scattering, which are unique to the water medium, and lead to color distortion, geometric artifacts, and collapsing phenomena at long distances. We propose the DualPhys-GS framework to achieve high-quality underwater reconstruction through a dual-path optimization mechanism. Our approach further develops a dual feature-guided attenuation-scattering modeling mechanism, the RGB-guided attenuation optimization model combines RGB features and depth information and can handle edge and structural details. In contrast, the multi-scale depth-aware scattering model captures scattering effects at different scales using a feature pyramid network and an attention mechanism. Meanwhile, we design several special loss functions. The attenuation scattering consistency loss ensures physical consistency. The water body type adaptive loss dynamically adjusts the weighting coefficients. The edge-aware scattering loss is used to maintain the sharpness of structural edges. The multi-scale feature loss helps to capture global and local structural information. In addition, we design a scene adaptive mechanism that can automatically identify the water-body-type characteristics (e.g., clear coral reef waters or turbid coastal waters) and dynamically adjust the scattering and attenuation parameters and optimization strategies. Experimental results show that our method outperforms existing methods in several metrics, especially in suspended matter-dense regions and long-distance scenes, and the reconstruction quality is significantly improved. </p>
<blockquote>
<p>åœ¨æ°´ä¸‹åœºæ™¯çš„3Dé‡å»ºä¸­ï¼ŒåŸºäºå¤§æ°”å…‰å­¦æ¨¡å‹çš„ä¼ ç»Ÿæ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†å…‰æ³¢é•¿çš„é€‰æ‹©æ€§è¡°å‡å’Œæ‚¬æµ®ç²’å­æ•£å°„ç­‰æ°´ä½“ç‹¬æœ‰çš„æ•ˆåº”ï¼Œè¿™ä¼šå¯¼è‡´é¢œè‰²å¤±çœŸã€å‡ ä½•ä¼ªå½±ä»¥åŠè¿œè·ç¦»å¤„çš„å´©æºƒç°è±¡ã€‚æˆ‘ä»¬æå‡ºäº†DualPhys-GSæ¡†æ¶ï¼Œé€šè¿‡åŒè·¯å¾„ä¼˜åŒ–æœºåˆ¶å®ç°é«˜è´¨é‡çš„æ°´ä¸‹é‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§åŒç‰¹å¾å¼•å¯¼è¡°å‡-æ•£å°„å»ºæ¨¡æœºåˆ¶ã€‚RGB-å¼•å¯¼è¡°å‡ä¼˜åŒ–æ¨¡å‹ç»“åˆäº†RGBç‰¹å¾å’Œæ·±åº¦ä¿¡æ¯ï¼Œèƒ½å¤Ÿå¤„ç†è¾¹ç¼˜å’Œç»“æ„ç»†èŠ‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤šå°ºåº¦æ·±åº¦æ„ŸçŸ¥æ•£å°„æ¨¡å‹ä½¿ç”¨ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œå’Œæ³¨æ„åŠ›æœºåˆ¶åœ¨ä¸åŒå°ºåº¦ä¸Šæ•æ‰æ•£å°„æ•ˆåº”ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¤šç§ç‰¹æ®ŠæŸå¤±å‡½æ•°ã€‚è¡°å‡æ•£å°„ä¸€è‡´æ€§æŸå¤±ç¡®ä¿ç‰©ç†ä¸€è‡´æ€§ã€‚æ°´ä½“ç±»å‹è‡ªé€‚åº”æŸå¤±åŠ¨æ€è°ƒæ•´æƒé‡ç³»æ•°ã€‚è¾¹ç¼˜æ„ŸçŸ¥æ•£å°„æŸå¤±ç”¨äºä¿æŒç»“æ„è¾¹ç¼˜çš„æ¸…æ™°åº¦ã€‚å¤šå°ºåº¦ç‰¹å¾æŸå¤±æœ‰åŠ©äºæ•è·å…¨å±€å’Œå±€éƒ¨ç»“æ„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åœºæ™¯è‡ªé€‚åº”æœºåˆ¶ï¼Œå¯ä»¥è‡ªåŠ¨è¯†åˆ«æ°´ä½“ç‰¹å¾ï¼ˆä¾‹å¦‚æ¸…æ¾ˆçš„çŠç‘šç¤æ°´åŸŸæˆ–æµ‘æµŠçš„æ²¿æµ·æ°´åŸŸï¼‰ï¼Œå¹¶åŠ¨æ€è°ƒæ•´æ•£å°„å’Œè¡°å‡å‚æ•°ä»¥åŠä¼˜åŒ–ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨æ‚¬æµ®ç‰©å¯†é›†åŒºåŸŸå’Œè¿œè·ç¦»åœºæ™¯ä¸Šï¼Œé‡å»ºè´¨é‡å¾—åˆ°æ˜¾è‘—æ”¹å–„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09610v1">PDF</a> 12 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æ°´ä¸‹åœºæ™¯ä¸‰ç»´é‡å»ºä¸­ï¼Œä¼ ç»Ÿæ–¹æ³•å­˜åœ¨çš„é—®é¢˜ä»¥åŠæå‡ºçš„DualPhys-GSæ¡†æ¶çš„ä¼˜åŠ¿ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒè·¯å¾„ä¼˜åŒ–æœºåˆ¶å®ç°é«˜è´¨é‡çš„æ°´ä¸‹é‡å»ºï¼Œå¹¶å»ºç«‹äº†åŒç‰¹å¾å¼•å¯¼è¡°å‡æ•£å°„å»ºæ¨¡æœºåˆ¶ã€‚åŒæ—¶ï¼Œè®¾è®¡å¤šç§ç‰¹æ®ŠæŸå¤±å‡½æ•°å’Œåœºæ™¯è‡ªé€‚åº”æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«æ°´ä½“ç‰¹å¾å¹¶è°ƒæ•´å‚æ•°å’Œä¼˜åŒ–ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨æ‚¬æµ®ç‰©å¯†é›†åŒºåŸŸå’Œè¿œè·ç¦»åœºæ™¯ä¸Šé‡å»ºè´¨é‡æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨æ°´ä¸‹åœºæ™¯ä¸‰ç»´é‡å»ºä¸­é¢ä¸´é€‰æ‹©æ€§å…‰æ³¢é•¿è¡°å‡å’Œæ‚¬æµ®é¢—ç²’æ•£å°„é—®é¢˜çš„æŒ‘æˆ˜ã€‚</li>
<li>DualPhys-GSæ¡†æ¶é€šè¿‡åŒè·¯å¾„ä¼˜åŒ–æœºåˆ¶å®ç°é«˜è´¨é‡æ°´ä¸‹é‡å»ºã€‚</li>
<li>å»ºç«‹åŒç‰¹å¾å¼•å¯¼è¡°å‡æ•£å°„å»ºæ¨¡æœºåˆ¶ï¼ŒåŒ…æ‹¬RGBç‰¹å¾å¼•å¯¼çš„è¡°å‡ä¼˜åŒ–æ¨¡å‹å’Œè·¨å°ºåº¦æ·±åº¦æ„ŸçŸ¥æ•£å°„æ¨¡å‹ã€‚</li>
<li>è®¾è®¡å¤šç§ç‰¹æ®ŠæŸå¤±å‡½æ•°ï¼Œç¡®ä¿ç‰©ç†ä¸€è‡´æ€§ã€è‡ªé€‚åº”è°ƒæ•´æƒé‡ç³»æ•°ã€ä¿æŒè¾¹ç¼˜æ¸…æ™°åº¦å¹¶æ•æ‰å…¨å±€å’Œå±€éƒ¨ç»“æ„ä¿¡æ¯ã€‚</li>
<li>åœºæ™¯è‡ªé€‚åº”æœºåˆ¶å¯è‡ªåŠ¨è¯†åˆ«æ°´ä½“ç‰¹å¾å¹¶åŠ¨æ€è°ƒæ•´æ•£å°„å’Œè¡°å‡å‚æ•°åŠä¼˜åŒ–ç­–ç•¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2c35319b7d4fc717cc947d7927bed551.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8ea8e1ef3c622a62ee183430c6d2335.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SVG-Head-Hybrid-Surface-Volumetric-Gaussians-for-High-Fidelity-Head-Reconstruction-and-Real-Time-Editing"><a href="#SVG-Head-Hybrid-Surface-Volumetric-Gaussians-for-High-Fidelity-Head-Reconstruction-and-Real-Time-Editing" class="headerlink" title="SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head   Reconstruction and Real-Time Editing"></a>SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head   Reconstruction and Real-Time Editing</h2><p><strong>Authors:Heyi Sun, Cong Wang, Tian-Xing Xu, Jingwei Huang, Di Kang, Chunchao Guo, Song-Hai Zhang</strong></p>
<p>Creating high-fidelity and editable head avatars is a pivotal challenge in computer vision and graphics, boosting many AR&#x2F;VR applications. While recent advancements have achieved photorealistic renderings and plausible animation, head editing, especially real-time appearance editing, remains challenging due to the implicit representation and entangled modeling of the geometry and global appearance. To address this, we propose Surface-Volumetric Gaussian Head Avatar (SVG-Head), a novel hybrid representation that explicitly models the geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled texture images to capture the global appearance. Technically, it contains two types of Gaussians, in which surface Gaussians explicitly model the appearance of head avatars using learnable texture images, facilitating real-time texture editing, while volumetric Gaussians enhance the reconstruction quality of non-Lambertian regions (e.g., lips and hair). To model the correspondence between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping method, which leverages UV coordinates given by the FLAME mesh to obtain sharp texture images and real-time rendering speed. A hierarchical optimization strategy is further designed to pursue the optimal performance in both reconstruction quality and editing flexibility. Experiments on the NeRSemble dataset show that SVG-Head not only generates high-fidelity rendering results, but also is the first method to obtain explicit texture images for Gaussian head avatars and support real-time appearance editing. </p>
<blockquote>
<p>åˆ›å»ºé«˜ä¿çœŸå’Œå¯ç¼–è¾‘çš„å¤´éƒ¨åŒ–èº«æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢å­¦ä¸­çš„ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œæ¨åŠ¨äº†è®¸å¤šAR&#x2F;VRåº”ç”¨ç¨‹åºçš„å‘å±•ã€‚å°½ç®¡æœ€è¿‘çš„è¿›æ­¥å·²ç»å®ç°äº†é€¼çœŸçš„æ¸²æŸ“å’Œé€¼çœŸçš„åŠ¨ç”»ï¼Œä½†å¤´éƒ¨ç¼–è¾‘ï¼Œå°¤å…¶æ˜¯å®æ—¶å¤–è§‚ç¼–è¾‘ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå‡ ä½•å½¢çŠ¶å’Œå…¨å±€å¤–è§‚çš„éšå¼è¡¨ç¤ºå’Œçº ç¼ å»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Surface-Volumetric Gaussian Head Avatarï¼ˆSVG-Headï¼‰è¿™ä¸€æ–°å‹æ··åˆè¡¨ç¤ºæ–¹æ³•ã€‚å®ƒé€šè¿‡ç»‘å®šåœ¨FLAMEç½‘æ ¼ä¸Šçš„ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒæ˜¾å¼åœ°å»ºæ¨¡å‡ ä½•å½¢çŠ¶ï¼Œå¹¶åˆ©ç”¨è§£è€¦çº¹ç†å›¾åƒæ•æ‰å…¨å±€å¤–è§‚ã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼Œå®ƒåŒ…å«ä¸¤ç§é«˜æ–¯åˆ†å¸ƒï¼Œå…¶ä¸­è¡¨é¢é«˜æ–¯åˆ†å¸ƒä½¿ç”¨å¯å­¦ä¹ çš„çº¹ç†å›¾åƒæ˜¾å¼åœ°æ¨¡æ‹Ÿå¤´éƒ¨åŒ–èº«çš„å¤–è§‚ï¼Œä»è€Œå®ç°å®æ—¶çº¹ç†ç¼–è¾‘ï¼Œè€Œä½“ç§¯é«˜æ–¯åˆ†å¸ƒåˆ™æé«˜äº†éæœ—ä¼¯åœ°åŒºï¼ˆä¾‹å¦‚å˜´å”‡å’Œå¤´å‘ï¼‰çš„é‡å»ºè´¨é‡ã€‚ä¸ºäº†å»ºç«‹ä¸‰ç»´ä¸–ç•Œå’Œçº¹ç†ç©ºé—´ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§ç½‘æ ¼æ„ŸçŸ¥çš„é«˜æ–¯UVæ˜ å°„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨FLAMEç½‘æ ¼æä¾›çš„UVåæ ‡æ¥è·å¾—æ¸…æ™°çš„çº¹ç†å›¾åƒå’Œå®æ—¶æ¸²æŸ“é€Ÿåº¦ã€‚ä¸ºäº†è¿›ä¸€æ­¥è¿½æ±‚åœ¨é‡å»ºè´¨é‡å’Œç¼–è¾‘çµæ´»æ€§æ–¹é¢çš„æœ€ä½³æ€§èƒ½ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§åˆ†å±‚ä¼˜åŒ–ç­–ç•¥ã€‚åœ¨NeRSembleæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSVG-Headä¸ä»…ç”Ÿæˆäº†é«˜ä¿çœŸåº¦çš„æ¸²æŸ“ç»“æœï¼Œè€Œä¸”æ˜¯ç¬¬ä¸€ç§è·å¾—é«˜æ–¯å¤´éƒ¨åŒ–èº«æ˜¾å¼çº¹ç†å›¾åƒå¹¶æ”¯æŒå®æ—¶å¤–è§‚ç¼–è¾‘çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09597v2">PDF</a> Accepted by ICCV 2025. Project page:   <a target="_blank" rel="noopener" href="https://heyy-sun.github.io/SVG-Head/">https://heyy-sun.github.io/SVG-Head/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Surface-Volumetric Gaussian Head Avatarï¼ˆSVG-Headï¼‰æ–¹æ³•ï¼Œé‡‡ç”¨æ··åˆè¡¨ç¤ºæ–¹å¼ï¼Œç”¨3Dé«˜æ–¯æ¨¡å‹æ˜¾å¼å»ºæ¨¡å‡ ä½•å½¢çŠ¶ï¼Œå¹¶ç»“åˆçº¹ç†å›¾åƒæ•æ‰å…¨å±€å¤–è§‚ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„å¤´åƒï¼Œå¹¶æ”¯æŒå®æ—¶ç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SVG-Headæ˜¯ä¸€ç§æ–°å‹çš„å¤´åƒç´ ææ–¹æ³•ï¼Œé‡‡ç”¨æ··åˆè¡¨ç¤ºæ–¹å¼ï¼Œç»“åˆäº†ä¸‰ç»´é«˜æ–¯æ¨¡å‹å’Œçº¹ç†å›¾åƒã€‚</li>
<li>SVG-Headèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„å¤´åƒï¼Œå¹¶æå‡AR&#x2F;VRåº”ç”¨ä½“éªŒã€‚</li>
<li>SVG-Headé€šè¿‡è¡¨é¢é«˜æ–¯æ¨¡å‹æ˜¾å¼å»ºæ¨¡å¤´åƒçš„å¤–è§‚ï¼Œå¹¶ä½¿ç”¨å¯å­¦ä¹ çš„çº¹ç†å›¾åƒè¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå®ç°å®æ—¶çº¹ç†ç¼–è¾‘ã€‚</li>
<li>ä½“ç§¯é«˜æ–¯æ¨¡å‹å¢å¼ºäº†éLambertianåŒºåŸŸï¼ˆå¦‚å˜´å”‡å’Œå¤´å‘ï¼‰çš„é‡å»ºè´¨é‡ã€‚</li>
<li>SVG-Headæä¾›äº†ç½‘æ ¼æ„ŸçŸ¥çš„Gaussian UVæ˜ å°„æ–¹æ³•ï¼Œå®ç°äº†ä»ä¸‰ç»´ä¸–ç•Œåˆ°çº¹ç†ç©ºé—´çš„å¯¹åº”ã€‚</li>
<li>å±‚æ¬¡ä¼˜åŒ–ç­–ç•¥ç”¨äºè¿½æ±‚é‡å»ºè´¨é‡å’Œç¼–è¾‘çµæ´»æ€§çš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2298791d9d0421073e2ce238c6f2c2af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33bd47b08022020e3e8c8402d3e5c2c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5383a750836a33a8c68e752aa32ac0b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d47b1f49bb937b639a46098fcf03749b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SkySplat-Generalizable-3D-Gaussian-Splatting-from-Multi-Temporal-Sparse-Satellite-Images"><a href="#SkySplat-Generalizable-3D-Gaussian-Splatting-from-Multi-Temporal-Sparse-Satellite-Images" class="headerlink" title="SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse   Satellite Images"></a>SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse   Satellite Images</h2><p><strong>Authors:Xuejun Huang, Xinyi Liu, Yi Wan, Zhi Zheng, Bin Zhang, Mingtao Xiong, Yingying Pei, Yongjun Zhang</strong></p>
<p>Three-dimensional scene reconstruction from sparse-view satellite images is a long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its variants have recently attracted attention for its high efficiency, existing methods remain unsuitable for satellite images due to incompatibility with rational polynomial coefficient (RPC) models and limited generalization capability. Recent advances in generalizable 3DGS approaches show potential, but they perform poorly on multi-temporal sparse satellite images due to limited geometric constraints, transient objects, and radiometric inconsistencies. To address these limitations, we propose SkySplat, a novel self-supervised framework that integrates the RPC model into the generalizable 3DGS pipeline, enabling more effective use of sparse geometric cues for improved reconstruction. SkySplat relies only on RGB images and radiometric-robust relative height supervision, thereby eliminating the need for ground-truth height maps. Key components include a Cross-Self Consistency Module (CSCM), which mitigates transient object interference via consistency-based masking, and a multi-view consistency aggregation strategy that refines reconstruction results. Compared to per-scene optimization methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy. It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to 1.80 m on the DFC19 dataset significantly, and demonstrates strong cross-dataset generalization on the MVS3D benchmark. </p>
<blockquote>
<p>ä»ç¨€ç–è§†è§’å«æ˜Ÿå›¾åƒè¿›è¡Œä¸‰ç»´åœºæ™¯é‡å»ºæ˜¯ä¸€é¡¹é•¿æœŸä¸”å¯Œæœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡3Dé«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰åŠå…¶å˜ä½“å› å…¶é«˜æ•ˆç‡è€Œè¿‘æœŸå¤‡å—å…³æ³¨ï¼Œä½†ç°æœ‰æ–¹æ³•ç”±äºä¸æœ‰ç†å¤šé¡¹å¼ç³»æ•°ï¼ˆRPCï¼‰æ¨¡å‹ä¸å…¼å®¹ä»¥åŠæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œä»ä¸é€‚ç”¨äºå«æ˜Ÿå›¾åƒã€‚å¯æ³›åŒ–çš„3DGSæ–¹æ³•çš„æœ€æ–°è¿›å±•æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬ç”±äºå‡ ä½•çº¦æŸæœ‰é™ã€ç¬æ—¶ç‰©ä½“å’Œè¾å°„ä¸ä¸€è‡´æ€§ï¼Œåœ¨å¤šæ—¶é—´ç¨€ç–å«æ˜Ÿå›¾åƒä¸Šçš„è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SkySplatï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è‡ªç›‘ç£æ¡†æ¶ï¼Œå®ƒå°†RPCæ¨¡å‹é›†æˆåˆ°å¯æ³›åŒ–çš„3DGSç®¡é“ä¸­ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç¨€ç–å‡ ä½•çº¿ç´¢æ¥æ”¹å–„é‡å»ºã€‚SkySplatä»…ä¾èµ–äºRGBå›¾åƒå’Œè¾å°„ç¨³å¥çš„ç›¸å¯¹é«˜åº¦ç›‘ç£ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹çœŸå®é«˜åº¦åœ°å›¾çš„éœ€æ±‚ã€‚å…³é”®ç»„ä»¶åŒ…æ‹¬è·¨è‡ªä¸€è‡´æ€§æ¨¡å—ï¼ˆCSCMï¼‰ï¼Œå®ƒé€šè¿‡åŸºäºä¸€è‡´æ€§çš„æ©ç æ¥ç¼“è§£ç¬æ—¶ç‰©ä½“å¹²æ‰°ï¼Œä»¥åŠå¤šè§†è§’ä¸€è‡´æ€§èšåˆç­–ç•¥ï¼Œç”¨äºä¼˜åŒ–é‡å»ºç»“æœã€‚ä¸é€åœºæ™¯ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒSkySplatå®ç°äº†ç›¸å¯¹äºEOGSçš„86å€åŠ é€Ÿå’Œé«˜ç²¾åº¦ã€‚å®ƒåœ¨DFC19æ•°æ®é›†ä¸Šå°†MAEä»13.18ç±³æ˜¾è‘—å‡å°‘åˆ°1.80ç±³ï¼Œå¹¶åœ¨MVS3DåŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09479v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºç¨€ç–è§†è§’å«æ˜Ÿå›¾åƒçš„ä¸‰ç»´åœºæ™¯é‡å»ºæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜çš„ä»»åŠ¡ã€‚å°½ç®¡ä¸‰ç»´é«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰åŠå…¶å˜ä½“å› å…¶é«˜æ•ˆç‡è€Œå¤‡å—å…³æ³¨ï¼Œä½†ç°æœ‰æ–¹æ³•å› æ— æ³•é€‚åº”æœ‰ç†å¤šé¡¹å¼ç³»æ•°ï¼ˆRPCï¼‰æ¨¡å‹ä»¥åŠæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œåœ¨å«æ˜Ÿå›¾åƒä¸Šæ•ˆæœä¸ä½³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºSkySplatï¼Œä¸€ç§æ–°å‹è‡ªç›‘ç£æ¡†æ¶ï¼Œå°†RPCæ¨¡å‹èå…¥å¯æ³›åŒ–çš„ä¸‰ç»´é«˜æ–¯å–·æº…ç®¡é“ä¸­ï¼Œæ›´æœ‰æ•ˆåœ°åˆ©ç”¨ç¨€ç–å‡ ä½•çº¿ç´¢è¿›è¡Œæ”¹è¿›é‡å»ºã€‚SkySplatä»…ä¾èµ–RGBå›¾åƒå’Œé²æ£’çš„ç›¸å¯¹é«˜åº¦ç›‘ç£ï¼Œä»è€Œæ— éœ€çœŸå®é«˜åº¦å›¾ã€‚å…¶å…³é”®ç»„ä»¶åŒ…æ‹¬è·¨è‡ªæˆ‘ä¸€è‡´æ€§æ¨¡å—ï¼ˆCSCMï¼‰ï¼Œé€šè¿‡ä¸€è‡´æ€§æ©ç ç¼“è§£ç¬æ—¶ç‰©ä½“å¹²æ‰°ï¼Œä»¥åŠå¤šè§†è§’ä¸€è‡´æ€§èšåˆç­–ç•¥ï¼Œä¼˜åŒ–é‡å»ºç»“æœã€‚ç›¸è¾ƒäºé’ˆå¯¹åœºæ™¯çš„ä¼˜åŒ–æ–¹æ³•ï¼ŒSkySplatå®ç°äº†å¯¹EOGSçš„86å€åŠ é€Ÿå¹¶æé«˜äº†å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨DFC19æ•°æ®é›†ä¸Šå°†MAEä»13.18ç±³é™è‡³1.80ç±³ï¼Œå¹¶åœ¨MVS3DåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SkySplatæ˜¯ä¸€ç§è‡ªç›‘ç£æ¡†æ¶ï¼Œæ•´åˆRPCæ¨¡å‹åˆ°å¯æ³›åŒ–çš„ä¸‰ç»´é«˜æ–¯å–·æº…ç®¡é“ä¸­ï¼Œæå‡ç¨€ç–å‡ ä½•çº¿ç´¢çš„æœ‰æ•ˆä½¿ç”¨ã€‚</li>
<li>SkySplatä»…ä¾èµ–RGBå›¾åƒå’Œé²æ£’çš„ç›¸å¯¹é«˜åº¦ç›‘ç£ï¼Œæ— éœ€çœŸå®é«˜åº¦å›¾ã€‚</li>
<li>è·¨è‡ªæˆ‘ä¸€è‡´æ€§æ¨¡å—ï¼ˆCSCMï¼‰é€šè¿‡ä¸€è‡´æ€§æ©ç å‡å°‘ç¬æ—¶ç‰©ä½“çš„å¹²æ‰°ã€‚</li>
<li>å¤šè§†è§’ä¸€è‡´æ€§èšåˆç­–ç•¥ä¼˜åŒ–é‡å»ºç»“æœã€‚</li>
<li>SkySplatå®ç°äº†å¯¹åœºæ™¯ä¼˜åŒ–æ–¹æ³•çš„é€Ÿåº¦æå‡ï¼Œå¹¶ä¿æŒäº†é«˜å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨DFC19æ•°æ®é›†ä¸Šï¼ŒSkySplatæ˜¾è‘—é™ä½äº†MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰ã€‚</li>
<li>SkySplatå±•ç°å‡ºå¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨MVS3DåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09479">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-988ab1ecd13844dbc5f7780261e0c50f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-679550490fdfc5774df669ffe65474f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34de5479d3315d3ef00a1c88c610aa29.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1958d0905567a68ab60264c2382bb7e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cab6b8cf657e5942db609f8deda0abf6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Multi-view-Normal-and-Distance-Guidance-Gaussian-Splatting-for-Surface-Reconstruction"><a href="#Multi-view-Normal-and-Distance-Guidance-Gaussian-Splatting-for-Surface-Reconstruction" class="headerlink" title="Multi-view Normal and Distance Guidance Gaussian Splatting for Surface   Reconstruction"></a>Multi-view Normal and Distance Guidance Gaussian Splatting for Surface   Reconstruction</h2><p><strong>Authors:Bo Jia, Yanan Guo, Ying Chang, Benkui Zhang, Ying Xie, Kangning Du, Lin Cao</strong></p>
<p>3D Gaussian Splatting (3DGS) achieves remarkable results in the field of surface reconstruction. However, when Gaussian normal vectors are aligned within the single-view projection plane, while the geometry appears reasonable in the current view, biases may emerge upon switching to nearby views. To address the distance and global matching challenges in multi-view scenes, we design multi-view normal and distance-guided Gaussian splatting. This method achieves geometric depth unification and high-accuracy reconstruction by constraining nearby depth maps and aligning 3D normals. Specifically, for the reconstruction of small indoor and outdoor scenes, we propose a multi-view distance reprojection regularization module that achieves multi-view Gaussian alignment by computing the distance loss between two nearby views and the same Gaussian surface. Additionally, we develop a multi-view normal enhancement module, which ensures consistency across views by matching the normals of pixel points in nearby views and calculating the loss. Extensive experimental results demonstrate that our method outperforms the baseline in both quantitative and qualitative evaluations, significantly enhancing the surface reconstruction capability of 3DGS. Our code will be made publicly available at (<a target="_blank" rel="noopener" href="https://github.com/Bistu3DV/MND-GS/">https://github.com/Bistu3DV/MND-GS/</a>). </p>
<blockquote>
<p>3Dé«˜æ–¯è´´åˆæŠ€æœ¯ï¼ˆ3DGSï¼‰åœ¨è¡¨é¢é‡å»ºé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œå½“é«˜æ–¯æ³•çº¿å‘é‡åœ¨å•è§†æŠ•å½±å¹³é¢å†…å¯¹é½æ—¶ï¼Œè™½ç„¶åœ¨å½“å‰è§†è§’ä¸‹å‡ ä½•å½¢æ€çœ‹èµ·æ¥æ˜¯åˆç†çš„ï¼Œä½†åœ¨åˆ‡æ¢åˆ°é‚»è¿‘è§†è§’æ—¶å¯èƒ½ä¼šå‡ºç°åå·®ã€‚ä¸ºäº†è§£å†³å¤šè§†è§’åœºæ™¯ä¸­çš„è·ç¦»å’Œå…¨å±€åŒ¹é…æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¤šè§†è§’æ³•çº¿å’Œè·ç¦»å¼•å¯¼çš„é«˜æ–¯è´´åˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡çº¦æŸé‚»è¿‘æ·±åº¦å›¾å’Œå¯¹é½3Dæ³•çº¿ï¼Œå®ç°äº†å‡ ä½•æ·±åº¦ç»Ÿä¸€å’Œé«˜ç²¾åº¦é‡å»ºã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºå®¤å†…å¤–å°åœºæ™¯çš„é‡å»ºï¼Œæˆ‘ä»¬æå‡ºäº†å¤šè§†è§’è·ç¦»é‡æŠ•å½±æ­£åˆ™åŒ–æ¨¡å—ï¼Œé€šè¿‡è®¡ç®—ä¸¤ä¸ªé‚»è¿‘è§†è§’å’ŒåŒä¸€é«˜æ–¯è¡¨é¢ä¹‹é—´çš„è·ç¦»æŸå¤±æ¥å®ç°å¤šè§†è§’é«˜æ–¯å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå¤šè§†è§’æ³•çº¿å¢å¼ºæ¨¡å—ï¼Œé€šè¿‡åŒ¹é…é‚»è¿‘è§†è§’ä¸­åƒç´ ç‚¹çš„æ³•çº¿å¹¶è®¡ç®—æŸå¤±ï¼Œç¡®ä¿ä¸åŒè§†è§’ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šéƒ½è¶…è¶Šäº†åŸºçº¿æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†3DGSçš„è¡¨é¢é‡å»ºèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨[<a target="_blank" rel="noopener" href="https://github.com/Bistu3DV/MND-GS/]%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Bistu3DV/MND-GS/]ä¸Šå…¬å¼€æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07701v2">PDF</a> This paper has been accepted by IROS 2025. Code:   <a target="_blank" rel="noopener" href="https://github.com/Bistu3DV/MND-GS/">https://github.com/Bistu3DV/MND-GS/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¤šè§†è§’è·ç¦»å¼•å¯¼çš„é«˜æ–¯å¹³æ»‘æ³•ï¼Œè§£å†³å•ä¸€è§†è§’æŠ•å½±å¹³é¢å†…é«˜æ–¯çŸ¢é‡å¯¹é½åœ¨å¤šä¸ªè§†è§’ä¹‹é—´å­˜åœ¨çš„åå·®é—®é¢˜ã€‚æ­¤æ–¹æ³•é€šè¿‡çº¦æŸé‚»è¿‘æ·±åº¦å›¾å’Œå¯¹é½ä¸‰ç»´æ³•çº¿å®ç°å‡ ä½•æ·±åº¦ç»Ÿä¸€å’Œé«˜ç²¾åº¦é‡å»ºã€‚å¯¹äºå®¤å†…å¤–å°åœºæ™¯çš„é‡å»ºï¼Œæå‡ºå¤šè§†è§’è·ç¦»é‡æŠ•å½±æ­£åˆ™åŒ–æ¨¡å—å’Œå¤šè§†è§’æ³•çº¿å¢å¼ºæ¨¡å—ï¼Œæœ‰æ•ˆæå‡äº†å¤šè§†è§’é«˜æ–¯å¯¹é½æ•ˆæœï¼Œå®ç°äº†ä¼˜å¼‚çš„è¡¨é¢é‡å»ºèƒ½åŠ›ã€‚ç›¸å…³ä»£ç å°†åœ¨ç½‘ç«™ä¸Šå…¬å¼€æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºå¤šè§†è§’è·ç¦»å¼•å¯¼çš„é«˜æ–¯å¹³æ»‘æ³•åœ¨è¡¨é¢é‡å»ºé¢†åŸŸçš„ä¼˜åŠ¿ã€‚</li>
<li>æå‡ºä¸€ç§é’ˆå¯¹é«˜æ–¯çŸ¢é‡çš„å¤šè§†è§’æ–¹æ³•å’Œç­–ç•¥æ¥å¤„ç†åœ¨ä¸åŒè§†è§’ä¸‹çš„é«˜æ–¯åå·®é—®é¢˜ã€‚</li>
<li>é€šè¿‡çº¦æŸé‚»è¿‘æ·±åº¦å›¾å’Œå¯¹é½ä¸‰ç»´æ³•çº¿å®ç°å‡ ä½•æ·±åº¦ç»Ÿä¸€å’Œé«˜ç²¾åº¦é‡å»ºï¼Œæé«˜é‡å»ºè´¨é‡ã€‚</li>
<li>é’ˆå¯¹å®¤å†…å¤–å°åœºæ™¯çš„é‡å»ºéœ€æ±‚ï¼Œè®¾è®¡ç‰¹å®šçš„å¤šè§†è§’è·ç¦»é‡æŠ•å½±æ­£åˆ™åŒ–æ¨¡å—å’Œå¤šè§†è§’æ³•çº¿å¢å¼ºæ¨¡å—ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šå‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†è¡¨é¢é‡å»ºèƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºå¤šè§†è§’åœºæ™¯ä¸‹çš„è¡¨é¢é‡å»ºé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07701">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-23cee6ce8de3bb012d38a272327eaacf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8e8931ee993272e3dcbc60fdfed74fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cbe6f532c58912af6fdbd1cf76b3322.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-21f968f4386427148eeb320d00f436cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fdd1e52decbcdd1c685fb57c9fda9ee.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="3D-Gaussian-Splatting-Driven-Multi-View-Robust-Physical-Adversarial-Camouflage-Generation"><a href="#3D-Gaussian-Splatting-Driven-Multi-View-Robust-Physical-Adversarial-Camouflage-Generation" class="headerlink" title="3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial   Camouflage Generation"></a>3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial   Camouflage Generation</h2><p><strong>Authors:Tianrui Lou, Xiaojun Jia, Siyuan Liang, Jiawei Liang, Ming Zhang, Yanjun Xiao, Xiaochun Cao</strong></p>
<p>Physical adversarial attack methods expose the vulnerabilities of deep neural networks and pose a significant threat to safety-critical scenarios such as autonomous driving. Camouflage-based physical attack is a more promising approach compared to the patch-based attack, offering stronger adversarial effectiveness in complex physical environments. However, most prior work relies on mesh priors of the target object and virtual environments constructed by simulators, which are time-consuming to obtain and inevitably differ from the real world. Moreover, due to the limitations of the backgrounds in training images, previous methods often fail to produce multi-view robust adversarial camouflage and tend to fall into sub-optimal solutions. Due to these reasons, prior work lacks adversarial effectiveness and robustness across diverse viewpoints and physical environments. We propose a physical attack framework based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and precise reconstruction with few images, along with photo-realistic rendering capabilities. Our framework further enhances cross-view robustness and adversarial effectiveness by preventing mutual and self-occlusion among Gaussians and employing a min-max optimization approach that adjusts the imaging background of each viewpoint, helping the algorithm filter out non-robust adversarial features. Extensive experiments validate the effectiveness and superiority of PGA. Our code is available at:<a target="_blank" rel="noopener" href="https://github.com/TRLou/PGA">https://github.com/TRLou/PGA</a>. </p>
<blockquote>
<p>ç‰©ç†å¯¹æŠ—æ”»å‡»æ–¹æ³•æ­ç¤ºäº†æ·±åº¦ç¥ç»ç½‘ç»œçš„è„†å¼±æ€§ï¼Œå¹¶å¯¹è‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®åœºæ™¯æ„æˆäº†é‡å¤§å¨èƒã€‚ä¸åŸºäºè¡¥ä¸çš„æ”»å‡»ç›¸æ¯”ï¼ŒåŸºäºä¼ªè£…æ”»å‡»çš„è§£å†³æ–¹æ¡ˆæ˜¯ä¸€ç§æ›´æœ‰å‰é€”çš„æ–¹æ³•ï¼Œå®ƒåœ¨å¤æ‚çš„ç‰©ç†ç¯å¢ƒä¸­å…·æœ‰æ›´å¼ºçš„å¯¹æŠ—æ•ˆæœã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ—©æœŸçš„å·¥ä½œä¾èµ–äºç›®æ ‡ç‰©ä½“çš„ç½‘æ ¼å…ˆéªŒå’Œç”±æ¨¡æ‹Ÿå™¨æ„å»ºçš„è™šæ‹Ÿç¯å¢ƒï¼Œè¿™äº›éƒ½éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´ä¸”ä¸å¯é¿å…åœ°ä¸çœŸå®ä¸–ç•Œå­˜åœ¨å·®å¼‚ã€‚æ­¤å¤–ï¼Œç”±äºè®­ç»ƒå›¾åƒèƒŒæ™¯çš„é™åˆ¶ï¼Œå…ˆå‰çš„æ–¹æ³•å¾€å¾€æ— æ³•ç”Ÿæˆå¤šè§†è§’ç¨³å¥çš„å¯¹æŠ—ä¼ªè£…ï¼Œå¹¶å®¹æ˜“é™·å…¥æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆã€‚ç”±äºè¿™äº›åŸå› ï¼Œæ—©æœŸå·¥ä½œçš„å¯¹æŠ—æ•ˆæœå’Œåœ¨ä¸åŒè§†è§’å’Œç‰©ç†ç¯å¢ƒä¸­çš„ç¨³å¥æ€§éƒ½ç›¸å¯¹ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¸‰ç»´é«˜æ–¯å±•å¸ƒï¼ˆ3DGSï¼‰çš„ç‰©ç†æ”»å‡»æ¡†æ¶ï¼Œåä¸ºPGAã€‚è¯¥æ¡†æ¶å…·æœ‰å¿«é€Ÿç²¾ç¡®çš„é‡å»ºèƒ½åŠ›å’Œé€¼çœŸçš„æ¸²æŸ“èƒ½åŠ›ï¼Œä»…ä½¿ç”¨å°‘é‡å›¾åƒå³å¯å®ç°ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡é˜²æ­¢é«˜æ–¯ä¹‹é—´çš„ç›¸äº’å’Œè‡ªé®æŒ¡å¹¶é‡‡ç”¨ä¸€ç§æœ€å°æœ€å¤§ä¼˜åŒ–æ–¹æ³•è°ƒæ•´æ¯ä¸ªè§†è§’çš„æˆåƒèƒŒæ™¯ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†è·¨è§†å›¾ç¨³å¥æ€§å’Œå¯¹æŠ—æ•ˆæœï¼Œæœ‰åŠ©äºç®—æ³•è¿‡æ»¤æ‰éç¨³å¥çš„å¯¹æŠ—ç‰¹å¾ã€‚å¤§é‡å®éªŒéªŒè¯äº†PGAçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/TRLou/PGA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/TRLou/PGAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01367v2">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç‰©ç†å¯¹æŠ—æ”»å‡»æ–¹æ³•æ­ç¤ºæ·±åº¦ç¥ç»ç½‘ç»œè„†å¼±æ€§ï¼Œå¹¶åœ¨è‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®åœºæ™¯ä¸­æ„æˆé‡å¤§å¨èƒã€‚ç›¸æ¯”åŸºäºè¡¥ä¸çš„æ”»å‡»ï¼Œä¼ªè£…æ”»å‡»æ˜¯ä¸€ç§æ›´æœ‰å‰é€”çš„æ–¹æ³•ï¼Œåœ¨å¤æ‚çš„ç‰©ç†ç¯å¢ƒä¸­å…·æœ‰æ›´å¼ºçš„å¯¹æŠ—æ•ˆæœã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆå‰çš„å·¥ä½œä¾èµ–äºç›®æ ‡ç‰©ä½“çš„ç½‘æ ¼å…ˆéªŒå’Œæ¨¡æ‹Ÿå™¨æ„å»ºçš„è™šæ‹Ÿç¯å¢ƒï¼Œè¿™äº›ç¯å¢ƒçš„è·å–è€—æ—¶ä¸”ä¸å¯é¿å…åœ°ä¸çœŸå®ä¸–ç•Œå­˜åœ¨å·®å¼‚ã€‚æ­¤å¤–ï¼Œç”±äºè®­ç»ƒå›¾åƒèƒŒæ™¯çš„é™åˆ¶ï¼Œå…ˆå‰çš„æ–¹æ³•å¾€å¾€æ— æ³•äº§ç”Ÿå¤šè§†è§’ç¨³å¥çš„å¯¹æŠ—ä¼ªè£…ï¼Œå¹¶å€¾å‘äºé™·å…¥æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œå…ˆå‰çš„å·¥ä½œåœ¨è·¨è¶Šä¸åŒè§†è§’å’Œç‰©ç†ç¯å¢ƒçš„å¯¹æŠ—æ€§å’Œç¨³å¥æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäº3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰çš„ç‰©ç†æ”»å‡»æ¡†æ¶ï¼Œåä¸ºPGAï¼Œè¯¥æ¡†æ¶å…·æœ‰å¿«é€Ÿç²¾ç¡®é‡å»ºå’Œé€¼çœŸçš„æ¸²æŸ“èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡é˜²æ­¢é«˜æ–¯ä¹‹é—´çš„ç›¸äº’å’Œè‡ªé®æŒ¡ï¼Œå¹¶é‡‡ç”¨è°ƒæ•´æ¯ä¸ªè§†ç‚¹æˆåƒèƒŒæ™¯çš„minmaxä¼˜åŒ–æ–¹æ³•ï¼Œæé«˜äº†è·¨è§†å›¾ç¨³å¥æ€§å’Œå¯¹æŠ—æ€§æ•ˆæœï¼Œè¿‡æ»¤æ‰éç¨³å¥çš„å¯¹æŠ—ç‰¹å¾ã€‚å®éªŒéªŒè¯äº†PGAçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‰©ç†å¯¹æŠ—æ”»å‡»å¨èƒæ·±åº¦ç¥ç»ç½‘ç»œåœ¨è‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®åœºæ™¯çš„åº”ç”¨ã€‚</li>
<li>ä¼ªè£…æ”»å‡»ç›¸æ¯”è¡¥ä¸æ”»å‡»åœ¨å¤æ‚ç‰©ç†ç¯å¢ƒä¸­å…·æœ‰æ›´å¼ºå¯¹æŠ—æ•ˆæœã€‚</li>
<li>å¤§å¤šæ•°å…ˆå‰å·¥ä½œä¾èµ–è™šæ‹Ÿç¯å¢ƒæ„å»ºå’Œæ¨¡æ‹Ÿå™¨è®­ç»ƒå­˜åœ¨ä¸ç°å®å·®å¼‚çš„é—®é¢˜ã€‚</li>
<li>è®­ç»ƒå›¾åƒèƒŒæ™¯é™åˆ¶å¯¼è‡´éš¾ä»¥äº§ç”Ÿå¤šè§†è§’ç¨³å¥çš„å¯¹æŠ—ä¼ªè£…ã€‚</li>
<li>PGAæ¡†æ¶åŸºäº3DGSæŠ€æœ¯å®ç°å¿«é€Ÿç²¾ç¡®é‡å»ºå’Œé€¼çœŸæ¸²æŸ“ã€‚</li>
<li>PGAæ¡†æ¶æé«˜äº†è·¨è§†å›¾ç¨³å¥æ€§å’Œå¯¹æŠ—æ€§æ•ˆæœé€šè¿‡é˜²æ­¢é«˜æ–¯é®æŒ¡å’Œä¼˜åŒ–æˆåƒèƒŒæ™¯æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53954df9d630866ad631b4c3bcee273b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-daa1f8edd6b601fa09ba8314b06ba5b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f74e4e5477d3a8509967dea6f0e494f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d87717a41893ffdac680af5916c0a28.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CCL-LGS-Contrastive-Codebook-Learning-for-3D-Language-Gaussian-Splatting"><a href="#CCL-LGS-Contrastive-Codebook-Learning-for-3D-Language-Gaussian-Splatting" class="headerlink" title="CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian   Splatting"></a>CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian   Splatting</h2><p><strong>Authors:Lei Tian, Xiaomin Li, Liqian Ma, Hao Yin, Zirui Zheng, Hefei Huang, Taiqing Li, Huchuan Lu, Xu Jia</strong></p>
<p>Recent advances in 3D reconstruction techniques and vision-language models have fueled significant progress in 3D semantic understanding, a capability critical to robotics, autonomous driving, and virtual&#x2F;augmented reality. However, methods that rely on 2D priors are prone to a critical challenge: cross-view semantic inconsistencies induced by occlusion, image blur, and view-dependent variations. These inconsistencies, when propagated via projection supervision, deteriorate the quality of 3D Gaussian semantic fields and introduce artifacts in the rendered outputs. To mitigate this limitation, we propose CCL-LGS, a novel framework that enforces view-consistent semantic supervision by integrating multi-view semantic cues. Specifically, our approach first employs a zero-shot tracker to align a set of SAM-generated 2D masks and reliably identify their corresponding categories. Next, we utilize CLIP to extract robust semantic encodings across views. Finally, our Contrastive Codebook Learning (CCL) module distills discriminative semantic features by enforcing intra-class compactness and inter-class distinctiveness. In contrast to previous methods that directly apply CLIP to imperfect masks, our framework explicitly resolves semantic conflicts while preserving category discriminability. Extensive experiments demonstrate that CCL-LGS outperforms previous state-of-the-art methods. Our project page is available at <a target="_blank" rel="noopener" href="https://epsilontl.github.io/CCL-LGS/">https://epsilontl.github.io/CCL-LGS/</a>. </p>
<blockquote>
<p>è¿‘æœŸä¸‰ç»´é‡å»ºæŠ€æœ¯å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æå¤§åœ°æ¨åŠ¨äº†ä¸‰ç»´è¯­ä¹‰ç†è§£çš„å‘å±•ï¼Œè¿™ä¸€èƒ½åŠ›å¯¹äºæœºå™¨äººæŠ€æœ¯ã€è‡ªåŠ¨é©¾é©¶å’Œè™šæ‹Ÿç°å®&#x2F;å¢å¼ºç°å®è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¾èµ–äºŒç»´å…ˆéªŒçš„æ–¹æ³•é¢ä¸´ç€ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ï¼šç”±é®æŒ¡ã€å›¾åƒæ¨¡ç³Šå’Œè§†è§’å˜åŒ–å¼•èµ·çš„è·¨è§†å›¾è¯­ä¹‰ä¸ä¸€è‡´æ€§ã€‚è¿™äº›ä¸ä¸€è‡´æ€§é€šè¿‡æŠ•å½±ç›‘ç£è¿›è¡Œä¼ æ’­ï¼Œä¼šæ¶åŒ–ä¸‰ç»´é«˜æ–¯è¯­ä¹‰åœºçš„å“è´¨ï¼Œå¹¶åœ¨æ¸²æŸ“è¾“å‡ºä¸­å¼•å…¥ä¼ªå½±ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†CCL-LGSè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ•´åˆå¤šè§†å›¾è¯­ä¹‰çº¿ç´¢æ¥å®æ–½è§†å›¾ä¸€è‡´çš„è¯­ä¹‰ç›‘ç£ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé‡‡ç”¨é›¶æ ·æœ¬è¿½è¸ªå™¨æ¥å¯¹é½ä¸€ç»„ç”±SAMç”Ÿæˆçš„äºŒç»´è’™ç‰ˆï¼Œå¹¶å¯é åœ°è¯†åˆ«å‡ºå®ƒä»¬å¯¹åº”çš„ç±»åˆ«ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨CLIPåœ¨ä¸åŒè§†è§’ä¸‹æå–ç¨³å¥çš„è¯­ä¹‰ç¼–ç ã€‚æœ€åï¼Œæˆ‘ä»¬çš„å¯¹æ¯”ä»£ç æœ¬å­¦ä¹ ï¼ˆCCLï¼‰æ¨¡å—é€šè¿‡å¼ºåˆ¶ç±»å†…ç´§å‡‘æ€§å’Œç±»é—´å·®å¼‚æ€§æ¥æç‚¼å‡ºé‰´åˆ«æ€§è¯­ä¹‰ç‰¹å¾ã€‚ä¸ä¹‹å‰ç›´æ¥åº”ç”¨äºä¸å®Œç¾è’™ç‰ˆçš„CLIPæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿæ˜¾å¼è§£å†³è¯­ä¹‰å†²çªï¼ŒåŒæ—¶ä¿ç•™ç±»åˆ«é‰´åˆ«èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCCL-LGSè¶…è¶Šäº†å…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://epsilontl.github.io/CCL-LGS/%E6%B5%8B%E8%AF%88%E3%80%82">https://epsilontl.github.io/CCL-LGS/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20469v2">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†3Dé‡å»ºæŠ€æœ¯å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°è¿›å±•å¯¹3Dè¯­ä¹‰ç†è§£çš„é‡è¦æ€§åŠå…¶åœ¨æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶å’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚ç„¶è€Œï¼Œä¾èµ–äºŒç»´å…ˆéªŒçš„æ–¹æ³•é¢ä¸´ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç”±äºé®æŒ¡ã€å›¾åƒæ¨¡ç³Šå’Œè§†è§’å˜åŒ–å¯¼è‡´çš„è·¨è§†å›¾è¯­ä¹‰ä¸ä¸€è‡´æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†CCL-LGSæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå¤šè§†å›¾è¯­ä¹‰çº¿ç´¢æ¥å®æ–½è§†å›¾ä¸€è‡´è¯­ä¹‰ç›‘ç£ã€‚è¯¥æ¡†æ¶é‡‡ç”¨é›¶æ ·æœ¬è·Ÿè¸ªå™¨å¯¹é½ä¸€ç»„SAMç”Ÿæˆçš„2Dæ©è†œå¹¶å¯é è¯†åˆ«å…¶å¯¹åº”çš„ç±»åˆ«ï¼Œç„¶åä½¿ç”¨CLIPæå–è·¨è§†å›¾çš„ç¨³å¥è¯­ä¹‰ç¼–ç ã€‚æœ€åï¼Œå¯¹æ¯”ä»£ç æœ¬å­¦ä¹ ï¼ˆCCLï¼‰æ¨¡å—é€šè¿‡å®ç°ç±»å†…ç´§å‡‘æ€§å’Œç±»é—´åŒºåˆ†æ€§æ¥æç‚¼åˆ¤åˆ«è¯­ä¹‰ç‰¹å¾ã€‚å®éªŒè¯æ˜ï¼ŒCCL-LGSæ¡†æ¶ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3Dé‡å»ºæŠ€æœ¯å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°è¿›å±•æ¨åŠ¨äº†3Dè¯­ä¹‰ç†è§£é¢†åŸŸçš„æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>ä¾èµ–äºŒç»´å…ˆéªŒçš„æ–¹æ³•é¢ä¸´è·¨è§†å›¾è¯­ä¹‰ä¸ä¸€è‡´æ€§çš„æŒ‘æˆ˜ï¼Œè¿™ä¼šå½±å“3Dé«˜æ–¯è¯­ä¹‰åœºçš„è´¨é‡å’Œæ¸²æŸ“è¾“å‡ºã€‚</li>
<li>CCL-LGSæ¡†æ¶é€šè¿‡æ•´åˆå¤šè§†å›¾è¯­ä¹‰çº¿ç´¢æ¥å®æ–½è§†å›¾ä¸€è‡´è¯­ä¹‰ç›‘ç£ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>CCL-LGSé‡‡ç”¨é›¶æ ·æœ¬è·Ÿè¸ªå™¨å¯¹é½SAMç”Ÿæˆçš„2Dæ©è†œï¼Œå¹¶ä½¿ç”¨CLIPæå–ç¨³å¥çš„è·¨è§†å›¾è¯­ä¹‰ç¼–ç ã€‚</li>
<li>å¯¹æ¯”ä»£ç æœ¬å­¦ä¹ ï¼ˆCCLï¼‰æ¨¡å—æé«˜äº†è¯­ä¹‰ç‰¹å¾çš„åˆ¤åˆ«åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒCCL-LGSæ¡†æ¶åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å…ˆå‰çš„æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2eacf4c6edf19f01e4c27947f1e28a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d262e53769bba5d8d3f8e848b16770a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-748692125e071db6141cffce09d8d35c.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Casual3DHDR-High-Dynamic-Range-3D-Gaussian-Splatting-from-Casually-Captured-Videos"><a href="#Casual3DHDR-High-Dynamic-Range-3D-Gaussian-Splatting-from-Casually-Captured-Videos" class="headerlink" title="Casual3DHDR: High Dynamic Range 3D Gaussian Splatting from Casually   Captured Videos"></a>Casual3DHDR: High Dynamic Range 3D Gaussian Splatting from Casually   Captured Videos</h2><p><strong>Authors:Shucheng Gong, Lingzhe Zhao, Wenpu Li, Hong Xie, Yin Zhang, Shiyu Zhao, Peidong Liu</strong></p>
<p>Photo-realistic novel view synthesis from multi-view images, such as neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), has gained significant attention for its superior performance. However, most existing methods rely on low dynamic range (LDR) images, limiting their ability to capture detailed scenes in high-contrast environments. While some prior works address high dynamic range (HDR) scene reconstruction, they typically require multi-view sharp images with varying exposure times captured at fixed camera positions, which is time-consuming and impractical. To make data acquisition more flexible, we propose \textbf{Casual3DHDR}, a robust one-stage method that reconstructs 3D HDR scenes from casually-captured auto-exposure (AE) videos, even under severe motion blur and unknown, varying exposure times. Our approach integrates a continuous camera trajectory into a unified physical imaging model, jointly optimizing exposure times, camera trajectory, and the camera response function (CRF). Extensive experiments on synthetic and real-world datasets demonstrate that \textbf{Casual3DHDR} outperforms existing methods in robustness and rendering quality. Our source code and dataset will be available at <a target="_blank" rel="noopener" href="https://lingzhezhao.github.io/CasualHDRSplat/">https://lingzhezhao.github.io/CasualHDRSplat/</a> </p>
<blockquote>
<p>åŸºäºå¤šè§†è§’å›¾åƒçš„å…‰ç…§çœŸå®æ„Ÿæ–°å‹è§†å›¾åˆæˆï¼Œä¾‹å¦‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰ï¼Œå› å…¶å“è¶Šæ€§èƒ½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰å›¾åƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨é«˜å¯¹æ¯”åº¦ç¯å¢ƒä¸­æ•æ‰è¯¦ç»†åœºæ™¯çš„èƒ½åŠ›ã€‚è™½ç„¶ä¸€äº›æ—©æœŸçš„å·¥ä½œè§£å†³äº†é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰åœºæ™¯é‡å»ºé—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦å›ºå®šç›¸æœºä½ç½®æ‹æ‘„çš„å¤šè§†è§’æ¸…æ™°å›¾åƒï¼Œè¿™äº›å›¾åƒå…·æœ‰ä¸åŒçš„æ›å…‰æ—¶é—´ï¼Œæ—¢è€—æ—¶åˆä¸å®ç”¨ã€‚ä¸ºäº†ä½¿æ•°æ®é‡‡é›†æ›´åŠ çµæ´»ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>Casual3DHDR</strong>ï¼Œè¿™æ˜¯ä¸€ç§ç¨³å¥çš„ä¸€é˜¶æ®µæ–¹æ³•ï¼Œå¯ä»¥ä»éšæ„æ‹æ‘„çš„è‡ªé€‚åº”æ›å…‰ï¼ˆAEï¼‰è§†é¢‘ä¸­é‡å»º3D HDRåœºæ™¯ï¼Œå³ä½¿åœ¨ä¸¥é‡è¿åŠ¨æ¨¡ç³Šå’ŒæœªçŸ¥ã€å˜åŒ–çš„æ›å…‰æ—¶é—´ä¸‹ä¹Ÿå¯ä»¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è¿ç»­çš„ç›¸æœºè½¨è¿¹é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„ç‰©ç†æˆåƒæ¨¡å‹ä¸­ï¼Œè”åˆä¼˜åŒ–æ›å…‰æ—¶é—´ã€ç›¸æœºè½¨è¿¹å’Œç›¸æœºå“åº”å‡½æ•°ï¼ˆCRFï¼‰ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œ<strong>Casual3DHDR</strong>åœ¨ç¨³å¥æ€§å’Œæ¸²æŸ“è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://lingzhezhao.github.io/CasualHDRSplat/%E6%8F%90%E4%BE%9B%E3%80%82">https://lingzhezhao.github.io/CasualHDRSplat/æä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17728v2">PDF</a> Published in ACM Multimedia 2025. Project page:   <a target="_blank" rel="noopener" href="https://lingzhezhao.github.io/CasualHDRSplat/">https://lingzhezhao.github.io/CasualHDRSplat/</a> (Previously titled   â€œCasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from   Casually Captured Videosâ€)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åŸºäºç¥ç»è¾å°„åœºå’Œ3Dé«˜æ–¯æº…å°„ç­‰æŠ€æœ¯çš„æ–°å‹è§†å›¾åˆæˆæ–¹æ³•ï¼Œåœ¨çœŸå®æ„Ÿæ¸²æŸ“æ–¹é¢å—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºä½åŠ¨æ€èŒƒå›´å›¾åƒï¼Œæ— æ³•æ•æ‰é«˜å¯¹æ¯”åº¦ç¯å¢ƒä¸‹çš„ç²¾ç»†åœºæ™¯ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæ–‡æœ¬æå‡ºäº†ä¸€ç§åä¸ºCasual3DHDRçš„ç¨³å¥å•é˜¶æ®µæ–¹æ³•ï¼Œèƒ½å¤Ÿä»éä¸“ä¸šæ‹æ‘„çš„è‡ªåŠ¨æ›å…‰è§†é¢‘ä¸­é‡å»º3D HDRåœºæ™¯ï¼Œå³ä½¿åœ¨ä¸¥é‡è¿åŠ¨æ¨¡ç³Šå’ŒæœªçŸ¥ã€ä¸åŒæ›å…‰æ—¶é—´ä¸‹ä¹Ÿèƒ½å®ç°ã€‚è¯¥æ–¹æ³•å°†è¿ç»­çš„ç›¸æœºè½¨è¿¹æ•´åˆåˆ°ç»Ÿä¸€çš„ç‰©ç†æˆåƒæ¨¡å‹ä¸­ï¼Œè”åˆä¼˜åŒ–æ›å…‰æ—¶é—´ã€ç›¸æœºè½¨è¿¹å’Œç›¸æœºå“åº”å‡½æ•°ã€‚å®éªŒè¯æ˜ï¼ŒCasual3DHDRåœ¨é²æ£’æ€§å’Œæ¸²æŸ“è´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹è§†å›¾åˆæˆæŠ€æœ¯å¦‚ç¥ç»è¾å°„åœºå’Œ3Dé«˜æ–¯æº…å°„åœ¨çœŸå®æ„Ÿæ¸²æŸ“æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºä½åŠ¨æ€èŒƒå›´å›¾åƒï¼Œéš¾ä»¥æ•æ‰é«˜å¯¹æ¯”åº¦ç¯å¢ƒä¸‹çš„ç²¾ç»†åœºæ™¯ã€‚</li>
<li>Casual3DHDRæ˜¯ä¸€ç§æ–°çš„HDRåœºæ™¯é‡å»ºæ–¹æ³•ï¼Œèƒ½å¤Ÿä»éä¸“ä¸šæ‹æ‘„çš„è‡ªåŠ¨æ›å…‰è§†é¢‘ä¸­é‡å»ºåœºæ™¯ã€‚</li>
<li>Casual3DHDRèƒ½å¤Ÿåœ¨ä¸¥é‡è¿åŠ¨æ¨¡ç³Šå’ŒæœªçŸ¥ã€ä¸åŒçš„æ›å…‰æ—¶é—´ä¸‹å®ç°é‡å»ºã€‚</li>
<li>è¯¥æ–¹æ³•å°†è¿ç»­çš„ç›¸æœºè½¨è¿¹æ•´åˆåˆ°ç‰©ç†æˆåƒæ¨¡å‹ä¸­ï¼Œå¹¶è”åˆä¼˜åŒ–å¤šä¸ªå‚æ•°ã€‚</li>
<li>å®éªŒè¯æ˜Casual3DHDRåœ¨é²æ£’æ€§å’Œæ¸²æŸ“è´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17728">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f43a2d21e715d72be3f72b487d1e22f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa46f4327a0325b05f2a15d45a8d111a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af240a8d155299044775b22ac9f30a93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8051e98ea27320a0fa26c634f7bbad87.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8bf5ab129892cbee6b7b051e90d197c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54ea4593628d745597d43f098551c054.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Introducing-Unbiased-Depth-into-2D-Gaussian-Splatting-for-High-accuracy-Surface-Reconstruction"><a href="#Introducing-Unbiased-Depth-into-2D-Gaussian-Splatting-for-High-accuracy-Surface-Reconstruction" class="headerlink" title="Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy   Surface Reconstruction"></a>Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy   Surface Reconstruction</h2><p><strong>Authors:Yixin Yang, Yang Zhou, Hui Huang</strong></p>
<p>Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry reconstruction quality than the popular 3DGS by using 2D surfels to approximate thin surfaces. However, it falls short when dealing with glossy surfaces, resulting in visible holes in these areas. We find that the reflection discontinuity causes the issue. To fit the jump from diffuse to specular reflection at different viewing angles, depth bias is introduced in the optimized Gaussian primitives. To address that, we first replace the depth distortion loss in 2DGS with a novel depth convergence loss, which imposes a strong constraint on depth continuity. Then, we rectify the depth criterion in determining the actual surface, which fully accounts for all the intersecting Gaussians along the ray. Qualitative and quantitative evaluations across various datasets reveal that our method significantly improves reconstruction quality, with more complete and accurate surfaces than 2DGS. Code is available at <a target="_blank" rel="noopener" href="https://github.com/XiaoXinyyx/Unbiased_Surfel">https://github.com/XiaoXinyyx/Unbiased_Surfel</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼ŒäºŒç»´é«˜æ–¯ç‚¹äº‘ï¼ˆ2DGSï¼‰é€šè¿‡ä½¿ç”¨äºŒç»´é¢å…ƒæ¥è¿‘ä¼¼è–„è¡¨é¢ï¼Œå±•ç°å‡ºäº†æ¯”æµè¡Œçš„ä¸‰ç»´é«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰æ›´ä¼˜è¶Šçš„å‡ ä½•é‡å»ºè´¨é‡ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†å…‰æ»‘è¡¨é¢æ—¶ï¼Œå®ƒè¿˜å­˜åœ¨ç¼ºé™·ï¼Œå¯¼è‡´è¿™äº›åŒºåŸŸå‡ºç°æ˜æ˜¾çš„ç©ºæ´ã€‚æˆ‘ä»¬å‘ç°åå°„ä¸è¿ç»­æ˜¯é€ æˆè¿™ä¸ªé—®é¢˜çš„åŸå› ã€‚ä¸ºäº†æ‹Ÿåˆä¸åŒè§†è§’ä¸‹çš„æ¼«åå°„åˆ°é•œé¢åå°„çš„è·³è·ƒï¼Œåœ¨ä¼˜åŒ–çš„é«˜æ–¯åŸºå…ƒä¸­å¼•å…¥äº†æ·±åº¦åå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆç”¨æ–°å‹æ·±åº¦æ”¶æ•›æŸå¤±æ›¿æ¢äº†äºŒç»´é«˜æ–¯ç‚¹äº‘ä¸­çš„æ·±åº¦å¤±çœŸæŸå¤±ï¼Œå¯¹æ·±åº¦è¿ç»­æ€§æ–½åŠ äº†ä¸¥æ ¼çš„çº¦æŸã€‚ç„¶åï¼Œæˆ‘ä»¬ä¿®æ­£äº†ç¡®å®šå®é™…è¡¨é¢æ—¶çš„æ·±åº¦æ ‡å‡†ï¼Œå……åˆ†è€ƒè™‘æ²¿å…‰çº¿æ–¹å‘çš„æ‰€æœ‰ç›¸äº¤é«˜æ–¯ã€‚è·¨å¤šä¸ªæ•°æ®é›†çš„è´¨é‡å’Œæ•°é‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†é‡å»ºè´¨é‡ï¼Œä¸äºŒç»´é«˜æ–¯ç‚¹äº‘ç›¸æ¯”ï¼Œè¡¨é¢æ›´åŠ å®Œæ•´å’Œå‡†ç¡®ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XiaoXinyyx/Unbiased_Surfel%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/XiaoXinyyx/Unbiased_Surfelæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06587v3">PDF</a> Accepted to the Journal track of Pacific Graphics 2025</p>
<p><strong>Summary</strong><br>     äºŒç»´é«˜æ–¯è´´ç‰‡ï¼ˆ2DGSï¼‰åœ¨å‡ ä½•é‡å»ºä¸Šå…·æœ‰é«˜è´¨é‡çš„è¡¨ç°ï¼Œé€šè¿‡äºŒç»´åŸºæœ¬å‡ ä½•å…ƒç´ æ¥è¿‘ä¼¼è–„è¡¨é¢ï¼Œç›¸è¾ƒäºæµè¡Œçš„ä¸‰ç»´é«˜æ–¯è´´ç‰‡ï¼ˆ3DGSï¼‰æœ‰æ›´ä½³çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†å…·æœ‰å…‰æ³½çš„è¡¨é¢æ—¶ï¼Œå­˜åœ¨æ˜æ˜¾çš„ç¼ºé™·ï¼Œæ˜¾ç¤ºä¸ºç©ºæ´ã€‚é—®é¢˜æºäºåå°„çš„ä¸è¿ç»­æ€§ï¼Œä¼˜åŒ–çš„é«˜æ–¯åŸºæœ¬ä½“åœ¨é€‚åº”æ¼«åå°„åˆ°é•œé¢åå°„çš„è·³è·ƒæ—¶äº§ç”Ÿæ·±åº¦åå·®ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬ä»¥æ·±åº¦æ”¶æ•›æŸå¤±å–ä»£æ·±åº¦å¤±çœŸæŸå¤±ï¼Œå¢å¼ºæ·±åº¦è¿ç»­æ€§çš„çº¦æŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¿®æ­£äº†ç”¨äºç¡®å®šå®é™…è¡¨é¢çš„æ·±åº¦æ ‡å‡†ï¼Œå……åˆ†è€ƒè™‘åˆ°å°„çº¿ä¸Šçš„æ‰€æœ‰ç›¸äº¤é«˜æ–¯ä½“ã€‚ç»è¿‡è·¨å¤šä¸ªæ•°æ®é›†çš„è´¨é‡å’Œæ•°é‡è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†é‡å»ºè´¨é‡ï¼Œè¡¨é¢æ›´åŠ å®Œæ•´å’Œå‡†ç¡®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>2DGSåœ¨å‡ ä½•é‡å»ºä¸Šè¡¨ç°å‡ºä¼˜äº3DGSçš„è´¨é‡ï¼Œå°¤å…¶æ“…é•¿è¿‘ä¼¼è–„è¡¨é¢ã€‚</li>
<li>å¤„ç†å…·æœ‰å…‰æ³½çš„è¡¨é¢æ—¶ï¼Œ2DGSå­˜åœ¨ç©ºæ´é—®é¢˜ã€‚</li>
<li>ç©ºæ´é—®é¢˜æºäºåå°„çš„ä¸è¿ç»­æ€§ä»¥åŠä¼˜åŒ–é«˜æ–¯åŸºæœ¬ä½“æ—¶çš„æ·±åº¦åå·®ã€‚</li>
<li>å¼•å…¥æ·±åº¦æ”¶æ•›æŸå¤±ä»¥æ›¿ä»£æ·±åº¦å¤±çœŸæŸå¤±ï¼Œå¢å¼ºæ·±åº¦è¿ç»­æ€§çš„çº¦æŸã€‚</li>
<li>ä¿®æ­£ç”¨äºç¡®å®šå®é™…è¡¨é¢çš„æ·±åº¦æ ‡å‡†ï¼Œç»¼åˆè€ƒè™‘å°„çº¿ä¸Šçš„æ‰€æœ‰ç›¸äº¤é«˜æ–¯ä½“ã€‚</li>
<li>æ–°æ–¹æ³•æ˜¾è‘—æé«˜äº†é‡å»ºè´¨é‡ï¼Œè¡¨é¢æ›´åŠ å®Œæ•´å’Œå‡†ç¡®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-20b6fac7c0cc9d170481227b1aef6276.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f80ebd41d97ca87d7e796eb8bd96f36d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a467fb770d5e886cd956e5c95c68b0c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e287f46e6d16e23b9c0eabe92ba029c7.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GBR-Generative-Bundle-Refinement-for-High-fidelity-Gaussian-Splatting-with-Enhanced-Mesh-Reconstruction"><a href="#GBR-Generative-Bundle-Refinement-for-High-fidelity-Gaussian-Splatting-with-Enhanced-Mesh-Reconstruction" class="headerlink" title="GBR: Generative Bundle Refinement for High-fidelity Gaussian Splatting   with Enhanced Mesh Reconstruction"></a>GBR: Generative Bundle Refinement for High-fidelity Gaussian Splatting   with Enhanced Mesh Reconstruction</h2><p><strong>Authors:Jianing Zhang, Yuchao Zheng, Ziwei Li, Qionghai Dai, Xiaoyun Yuan</strong></p>
<p>Gaussian splatting has gained attention for its efficient representation and rendering of 3D scenes using continuous Gaussian primitives. However, it struggles with sparse-view inputs due to limited geometric and photometric information, causing ambiguities in depth, shape, and texture.   we propose GBR: Generative Bundle Refinement, a method for high-fidelity Gaussian splatting and meshing using only 4-6 input views. GBR integrates a neural bundle adjustment module to enhance geometry accuracy and a generative depth refinement module to improve geometry fidelity. More specifically, the neural bundle adjustment module integrates a foundation network to produce initial 3D point maps and point matches from unposed images, followed by bundle adjustment optimization to improve multiview consistency and point cloud accuracy. The generative depth refinement module employs a diffusion-based strategy to enhance geometric details and fidelity while preserving the scale. Finally, for Gaussian splatting optimization, we propose a multimodal loss function incorporating depth and normal consistency, geometric regularization, and pseudo-view supervision, providing robust guidance under sparse-view conditions. Experiments on widely used datasets show that GBR significantly outperforms existing methods under sparse-view inputs. Additionally, GBR demonstrates the ability to reconstruct and render large-scale real-world scenes, such as the Pavilion of Prince Teng and the Great Wall, with remarkable details using only 6 views. </p>
<blockquote>
<p>é«˜æ–¯è´´å›¾æŠ€æœ¯å› å…¶ä½¿ç”¨è¿ç»­çš„é«˜æ–¯åŸºæœ¬å…ƒç´ æ¥é«˜æ•ˆè¡¨ç¤ºå’Œæ¸²æŸ“3Dåœºæ™¯è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºå‡ ä½•å’Œå…‰åº¦ä¿¡æ¯æœ‰é™ï¼Œå®ƒåœ¨ç¨€ç–è§†è§’è¾“å…¥æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œå¯¼è‡´æ·±åº¦ã€å½¢çŠ¶å’Œçº¹ç†çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬æå‡ºGBRï¼šç”Ÿæˆæ†è°ƒæ•´ä¼˜åŒ–æ–¹æ³•ï¼ˆGenerative Bundle Refinementï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä»…ä½¿ç”¨4-6ä¸ªè¾“å…¥è§†è§’è¿›è¡Œé«˜ä¿çœŸé«˜æ–¯è´´å›¾å’Œç½‘æ ¼åŒ–çš„æ–¹æ³•ã€‚GBRé›†æˆäº†ä¸€ä¸ªç¥ç»ç½‘ç»œæ†è°ƒæ•´æ¨¡å—ï¼Œä»¥æé«˜å‡ ä½•ç²¾åº¦ï¼Œä»¥åŠä¸€ä¸ªç”Ÿæˆæ·±åº¦ä¼˜åŒ–æ¨¡å—ï¼Œä»¥æé«˜å‡ ä½•ä¿çœŸåº¦ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œç¥ç»ç½‘ç»œæ†è°ƒæ•´æ¨¡å—é¦–å…ˆé›†æˆåŸºç¡€ç½‘ç»œï¼Œä»éå®šä½å›¾åƒç”Ÿæˆåˆå§‹çš„3Dç‚¹å›¾å’Œç‚¹åŒ¹é…ï¼Œç„¶åé€šè¿‡æ†è°ƒæ•´ä¼˜åŒ–æé«˜å¤šè§†è§’çš„ä¸€è‡´æ€§å’Œç‚¹äº‘ç²¾åº¦ã€‚ç”Ÿæˆæ·±åº¦ä¼˜åŒ–æ¨¡å—é‡‡ç”¨åŸºäºæ‰©æ•£çš„ç­–ç•¥ï¼Œä»¥æé«˜å‡ ä½•ç»†èŠ‚å’Œä¿çœŸåº¦ï¼ŒåŒæ—¶ä¿æŒå°ºåº¦ä¸å˜ã€‚æœ€åï¼Œé’ˆå¯¹é«˜æ–¯è´´å›¾ä¼˜åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šæ¨¡æ€æŸå¤±å‡½æ•°ï¼Œç»“åˆäº†æ·±åº¦å’Œæ³•çº¿ä¸€è‡´æ€§ã€å‡ ä½•æ­£åˆ™åŒ–å’Œä¼ªè§†å›¾ç›‘ç£ï¼Œåœ¨ç¨€ç–è§†è§’æ¡ä»¶ä¸‹æä¾›ç¨³å¥çš„å¼•å¯¼ã€‚åœ¨å¸¸ç”¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGBRåœ¨ç¨€ç–è§†è§’è¾“å…¥ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒGBRå±•ç¤ºäº†ä»…ä½¿ç”¨6ä¸ªè§†è§’å°±èƒ½é‡å»ºå’Œæ¸²æŸ“å¤§è§„æ¨¡çœŸå®åœºæ™¯ï¼Œå¦‚å¤ªå­äº­å’Œå¤§é•¿åŸï¼Œå…·æœ‰ä»¤äººæƒŠå¹çš„ç»†èŠ‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05908v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†åŸºäºé«˜æ–¯æ¶‚ç»˜ï¼ˆGaussian splattingï¼‰æŠ€æœ¯çš„æ”¹è¿›æ–¹æ³•GBRï¼ˆGenerative Bundle Refinementï¼‰ï¼Œç”¨äºé«˜æ•ˆè¡¨ç¤ºå’Œæ¸²æŸ“ä¸‰ç»´åœºæ™¯ã€‚é’ˆå¯¹ç¨€ç–è§†è§’è¾“å…¥å¯¼è‡´çš„é«˜æ–¯æ¶‚ç»˜åœ¨æ·±åº¦ã€å½¢çŠ¶å’Œçº¹ç†ä¸Šçš„æ¨¡ç³Šé—®é¢˜ï¼ŒGBRé€šè¿‡å¼•å…¥ç¥ç»æŸè°ƒæ•´æ¨¡å—å’Œç”Ÿæˆæ·±åº¦ç»†åŒ–æ¨¡å—ï¼Œæé«˜äº†å‡ ä½•ç²¾åº¦å’Œä¿çœŸåº¦ã€‚é€šè¿‡ä¼˜åŒ–ç¥ç»ç½‘ç»œå’Œè°ƒæ•´ç­–ç•¥ï¼ŒGBRåœ¨ä»…ä½¿ç”¨4-6ä¸ªè¾“å…¥è§†è§’çš„æƒ…å†µä¸‹å®ç°äº†é«˜ä¿çœŸåº¦çš„é«˜æ–¯æ¶‚ç»˜å’Œç½‘æ ¼åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒGBRåœ¨ç¨€ç–è§†è§’è¾“å…¥ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å¯åœ¨ä»…æœ‰6ä¸ªè§†è§’çš„æƒ…å†µä¸‹é‡å»ºå’Œæ¸²æŸ“å¤§è§„æ¨¡çœŸå®åœºæ™¯ï¼Œå¦‚æ»•ç‹é˜å’Œé•¿åŸç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜æ–¯æ¶‚ç»˜æ˜¯ä¸€ç§ç”¨äºé«˜æ•ˆè¡¨ç¤ºå’Œæ¸²æŸ“ä¸‰ç»´åœºæ™¯çš„æŠ€æœ¯ï¼Œä½†åœ¨ç¨€ç–è§†è§’è¾“å…¥æ—¶å­˜åœ¨æ·±åº¦ã€å½¢çŠ¶å’Œçº¹ç†çš„æ¨¡ç³Šé—®é¢˜ã€‚</li>
<li>GBRé€šè¿‡å¼•å…¥ç¥ç»æŸè°ƒæ•´æ¨¡å—å’Œç”Ÿæˆæ·±åº¦ç»†åŒ–æ¨¡å—è§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œæé«˜äº†å‡ ä½•ç²¾åº¦å’Œä¿çœŸåº¦ã€‚</li>
<li>GBRä½¿ç”¨åŸºç¡€ç½‘ç»œç”Ÿæˆåˆå§‹ä¸‰ç»´ç‚¹å›¾å’Œç‚¹åŒ¹é…ï¼Œå¹¶é€šè¿‡æŸè°ƒæ•´ä¼˜åŒ–æé«˜å¤šè§†è§’ä¸€è‡´æ€§å’Œç‚¹äº‘ç²¾åº¦ã€‚</li>
<li>ç”Ÿæˆæ·±åº¦ç»†åŒ–æ¨¡å—é‡‡ç”¨æ‰©æ•£ç­–ç•¥ï¼Œåœ¨æé«˜å‡ ä½•ç»†èŠ‚å’Œä¿çœŸåº¦çš„åŒæ—¶ä¿æŒå°ºåº¦ä¸å˜ã€‚</li>
<li>ä¸ºä¼˜åŒ–é«˜æ–¯æ¶‚ç»˜ï¼ŒGBRæå‡ºå¤šæ¨¡æ€æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬æ·±åº¦ä¸æ³•çº¿ä¸€è‡´æ€§ã€å‡ ä½•æ­£åˆ™åŒ–å’Œä¼ªè§†è§’ç›‘ç£ï¼Œä¸ºç¨€ç–è§†è§’æ¡ä»¶ä¸‹çš„é‡å»ºæä¾›ç¨³å¥æŒ‡å¯¼ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒGBRåœ¨ç¨€ç–è§†è§’è¾“å…¥ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c93e3cbefa598be41a477109601b94bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cce7bf317066f8cca81094592fec9f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d86265e534128aef3677dff19e51867d.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8e4aa5af078a50a81b2fa7a82fb96f0e.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  GANDiff FR Hybrid GAN Diffusion Synthesis for Causal Bias Attribution   in Face Recognition
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3de41a4cd36242b39102b1fe58b68f1a.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  Gen-AFFECT Generation of Avatar Fine-grained Facial Expressions with   Consistent identiTy
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27197.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
