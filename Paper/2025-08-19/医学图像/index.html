<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  DashCam Video A complementary low-cost data stream for on-demand   forest-infrastructure system monitoring">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-409025a5b77b6e0cd179f75508e39a11.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-19-æ›´æ–°"><a href="#2025-08-19-æ›´æ–°" class="headerlink" title="2025-08-19 æ›´æ–°"></a>2025-08-19 æ›´æ–°</h1><h2 id="DashCam-Video-A-complementary-low-cost-data-stream-for-on-demand-forest-infrastructure-system-monitoring"><a href="#DashCam-Video-A-complementary-low-cost-data-stream-for-on-demand-forest-infrastructure-system-monitoring" class="headerlink" title="DashCam Video: A complementary low-cost data stream for on-demand   forest-infrastructure system monitoring"></a>DashCam Video: A complementary low-cost data stream for on-demand   forest-infrastructure system monitoring</h2><p><strong>Authors:Durga Joshi, Chandi Witharana, Robert Fahey, Thomas Worthley, Zhe Zhu, Diego Cerrai</strong></p>
<p>Our study introduces a novel, low-cost, and reproducible framework for real-time, object-level structural assessment and geolocation of roadside vegetation and infrastructure with commonly available but underutilized dashboard camera (dashcam) video data. We developed an end-to-end pipeline that combines monocular depth estimation, depth error correction, and geometric triangulation to generate accurate spatial and structural data from street-level video streams from vehicle-mounted dashcams. Depth maps were first estimated using a state-of-the-art monocular depth model, then refined via a gradient-boosted regression framework to correct underestimations, particularly for distant objects. The depth correction model achieved strong predictive performance (R2 &#x3D; 0.92, MAE &#x3D; 0.31 on transformed scale), significantly reducing bias beyond 15 m. Further, object locations were estimated using GPS-based triangulation, while object heights were calculated using pin hole camera geometry. Our method was evaluated under varying conditions of camera placement and vehicle speed. Low-speed vehicle with inside camera gave the highest accuracy, with mean geolocation error of 2.83 m, and mean absolute error (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To the best of our knowledge, it is the first framework to combine monocular depth modeling, triangulated GPS-based geolocation, and real-time structural assessment for urban vegetation and infrastructure using consumer-grade video data. Our approach complements conventional RS methods, such as LiDAR and image by offering a fast, real-time, and cost-effective solution for object-level monitoring of vegetation risks and infrastructure exposure, making it especially valuable for utility companies, and urban planners aiming for scalable and frequent assessments in dynamic urban environments. </p>
<blockquote>
<p>æˆ‘ä»¬çš„ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–ã€ä½æˆæœ¬ã€å¯å¤åˆ¶æ€§çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¸¸è§çš„ä½†æœªè¢«å……åˆ†åˆ©ç”¨çš„ä»ªè¡¨æ¿æ‘„åƒå¤´ï¼ˆè¡Œè½¦è®°å½•ä»ªï¼‰è§†é¢‘æ•°æ®ï¼Œè¿›è¡Œå®æ—¶ã€é¢å‘å¯¹è±¡çš„æ¤è¢«å’Œç»“æ„è¯„ä¼°ä»¥åŠåœ°ç†å®šä½ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç®¡é“ï¼Œå®ƒç»“åˆäº†å•ç›®æ·±åº¦ä¼°è®¡ã€æ·±åº¦è¯¯å·®æ ¡æ­£å’Œå‡ ä½•ä¸‰è§’æµ‹é‡æŠ€æœ¯ï¼Œä»è½¦è½½è¡Œè½¦è®°å½•ä»ªçš„è¡—å¤´è§†é¢‘æµç”Ÿæˆå‡†ç¡®çš„ç©ºé—´å’Œç»“æ„æ•°æ®ã€‚é¦–å…ˆï¼Œä½¿ç”¨æœ€å…ˆè¿›çš„å•ç›®æ·±åº¦æ¨¡å‹ä¼°è®¡æ·±åº¦å›¾ï¼Œç„¶åé€šè¿‡æ¢¯åº¦å¢å¼ºå›å½’æ¡†æ¶å¯¹ä½ä¼°è¿›è¡Œä¿®æ­£ï¼Œç‰¹åˆ«æ˜¯è¿œå¤„çš„ç‰©ä½“ã€‚æ·±åº¦æ ¡æ­£æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„é¢„æµ‹æ€§èƒ½ï¼ˆè½¬æ¢å°ºåº¦ä¸Šçš„RÂ²&#x3D; 0.92ï¼ŒMAE &#x3D; 0.31ï¼‰ï¼Œæ˜¾è‘—å‡å°‘äº†è¶…è¿‡15ç±³ä»¥å¤–çš„åå·®ã€‚æ­¤å¤–ï¼Œç‰©ä½“ä½ç½®æ˜¯ä½¿ç”¨åŸºäºGPSçš„ä¸‰è§’æµ‹é‡æŠ€æœ¯ä¼°è®¡çš„ï¼Œè€Œç‰©ä½“é«˜åº¦åˆ™æ˜¯ä½¿ç”¨é’ˆå­”ç›¸æœºå‡ ä½•è®¡ç®—çš„ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„ç›¸æœºæ”¾ç½®ä½ç½®å’Œè½¦è¾†é€Ÿåº¦æ¡ä»¶ä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚è£…æœ‰å†…éƒ¨ç›¸æœºçš„ä½é€Ÿè½¦è¾†è¡¨ç°å‡ºæœ€é«˜çš„å‡†ç¡®æ€§ï¼Œæ ‘æœ¨çš„åœ°ç†å®šä½å¹³å‡è¯¯å·®ä¸º2.83ç±³ï¼Œé«˜åº¦ä¼°è®¡çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä¸º2.09ç±³ï¼Œè€Œæ†çš„å¹³å‡ç»å¯¹è¯¯å·®ä¸º0.88ç±³ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»“åˆå•ç›®æ·±åº¦å»ºæ¨¡ã€åŸºäºä¸‰è§’æµ‹é‡çš„GPSåœ°ç†å®šä½å’Œä½¿ç”¨æ¶ˆè´¹è€…çº§è§†é¢‘æ•°æ®è¿›è¡Œå®æ—¶ç»“æ„è¯„ä¼°çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¥å……äº†ä¼ ç»Ÿçš„é¥æ„Ÿæ–¹æ³•ï¼Œå¦‚æ¿€å…‰é›·è¾¾å’Œå›¾åƒï¼Œæä¾›äº†ä¸€ç§å¿«é€Ÿã€å®æ—¶ã€æˆæœ¬æ•ˆç›Šé«˜çš„é¢å‘å¯¹è±¡çš„æ¤è¢«é£é™©å’Œç»“æ„æš´éœ²ç›‘æµ‹è§£å†³æ–¹æ¡ˆã€‚è¿™ä½¿å¾—å®ƒåœ¨å…¬ç”¨äº‹ä¸šå…¬å¸å’ŒåŸå¸‚è§„åˆ’å¸ˆé’ˆå¯¹åŠ¨æ€åŸå¸‚ç¯å¢ƒè¿›è¡Œå¯æ‰©å±•å’Œé¢‘ç¹çš„è¯„ä¼°æ—¶å…·æœ‰ç‰¹åˆ«çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11591v1">PDF</a> 35 Pages, 15 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–ã€ä½æˆæœ¬ã€å¯å¤åˆ¶æ€§çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯åˆ©ç”¨å¸¸è§çš„ä½†æœªè¢«å……åˆ†åˆ©ç”¨çš„è¡Œè½¦è®°å½•ä»ªè§†é¢‘æ•°æ®ï¼Œè¿›è¡Œå®æ—¶é¢å‘å¯¹è±¡çš„ç»“æ„è¯„ä¼°å’Œè·¯è¾¹æ¤è¢«åŠåŸºç¡€è®¾æ–½çš„åœ°ç†ä½ç½®å®šä½ã€‚ç ”ç©¶å›¢é˜Ÿå»ºç«‹äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç®¡é“ï¼Œç»“åˆå•ç›®æ·±åº¦ä¼°è®¡ã€æ·±åº¦è¯¯å·®ä¿®æ­£å’Œå‡ ä½•ä¸‰è§’æµ‹é‡æŠ€æœ¯ï¼Œä»è¡—é“çº§çš„è§†é¢‘æµä¸­ç”Ÿæˆå‡†ç¡®çš„ç©ºé—´å’Œç»“æ„æ•°æ®ã€‚é¦–å…ˆä½¿ç”¨å…ˆè¿›çš„å•ç›®æ·±åº¦æ¨¡å‹ä¼°è®¡æ·±åº¦å›¾ï¼Œç„¶åé€šè¿‡æ¢¯åº¦å¢å¼ºå›å½’æ¡†æ¶ä¿®æ­£æ·±åº¦å›¾çš„ä½ä¼°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¯¹è¿œå¤„ç‰©ä½“çš„ä½ä¼°ã€‚æ·±åº¦ä¿®æ­£æ¨¡å‹é¢„æµ‹æ€§èƒ½å¼ºå¤§ï¼ˆè½¬æ¢å°ºåº¦ä¸Šçš„RÂ²&#x3D;0.92ï¼ŒMAE&#x3D;0.31ï¼‰ï¼Œæ˜¾è‘—å‡å°‘äº†è¶…è¿‡15ç±³ä»¥å¤–çš„åå·®ã€‚æ­¤å¤–ï¼Œè¿˜åˆ©ç”¨åŸºäºGPSçš„ä¸‰è§’æµ‹é‡æŠ€æœ¯ä¼°è®¡ç‰©ä½“ä½ç½®ï¼Œåˆ©ç”¨é’ˆå­”ç›¸æœºå‡ ä½•è®¡ç®—ç‰©ä½“é«˜åº¦ã€‚è¯¥ç ”ç©¶åœ¨ä¸åŒç›¸æœºæ”¾ç½®ä½ç½®å’Œè½¦é€Ÿæ¡ä»¶ä¸‹å¯¹æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚è½¦å†…ä½é€Ÿè¡Œé©¶ã€ç›¸æœºæœå†…çš„é…ç½®æä¾›äº†æœ€é«˜çš„å‡†ç¡®æ€§ï¼Œæ ‘æœ¨çš„åœ°ç†å®šä½å¹³å‡è¯¯å·®ä¸º2.83ç±³ï¼Œé«˜åº¦ä¼°è®¡çš„å¹³å‡ç»å¯¹è¯¯å·®ä¸º2.09ç±³ï¼Œè€Œæ†çŠ¶ç‰©çš„å¹³å‡ç»å¯¹è¯¯å·®ä¸º0.88ç±³ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªç»“åˆå•ç›®æ·±åº¦å»ºæ¨¡ã€åŸºäºGPSçš„ä¸‰è§’æµ‹é‡åœ°ç†å®šä½å’Œå®æ—¶ç»“æ„è¯„ä¼°ï¼Œåˆ©ç”¨æ¶ˆè´¹çº§è§†é¢‘æ•°æ®å¯¹åŸå¸‚æ¤è¢«å’ŒåŸºç¡€è®¾æ–½è¿›è¡Œåˆ†æçš„æ¡†æ¶ã€‚è¯¥æ–¹æ³•è¡¥å……äº†ä¼ ç»Ÿçš„é¥æ„Ÿæ–¹æ³•ï¼Œå¦‚æ¿€å…‰é›·è¾¾å’Œå›¾åƒæ•°æ®ï¼Œæä¾›äº†ä¸€ç§å¿«é€Ÿã€å®æ—¶ã€æˆæœ¬æ•ˆç›Šé«˜çš„é¢å‘å¯¹è±¡çš„æ¤è¢«é£é™©ç›‘æµ‹å’ŒåŸºç¡€è®¾æ–½æš´éœ²ç›‘æµ‹è§£å†³æ–¹æ¡ˆï¼Œå¯¹äºç”µåŠ›å…¬å¸ã€åŸå¸‚è§„åˆ’å¸ˆç­‰åœ¨åŠ¨æ€åŸå¸‚ç¯å¢ƒä¸­è¿›è¡Œå¯ä¼¸ç¼©å’Œé¢‘ç¹è¯„ä¼°å…·æœ‰ç‰¹åˆ«çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ç»“åˆè¡Œè½¦è®°å½•ä»ªè§†é¢‘æ•°æ®çš„å®æ—¶é¢å‘å¯¹è±¡çš„ç»“æ„è¯„ä¼°å’Œåœ°ç†å®šä½æ¡†æ¶ã€‚</li>
<li>åˆ©ç”¨å•ç›®æ·±åº¦ä¼°è®¡ã€æ·±åº¦è¯¯å·®ä¿®æ­£å’Œå‡ ä½•ä¸‰è§’æµ‹é‡æŠ€æœ¯ç”Ÿæˆå‡†ç¡®çš„ç©ºé—´å’Œç»“æ„æ•°æ®ã€‚</li>
<li>æ·±åº¦ä¿®æ­£æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„é¢„æµ‹æ€§èƒ½ï¼Œæ˜¾è‘—å‡å°‘äº†æ·±åº¦ä¼°è®¡çš„åå·®ã€‚</li>
<li>é€šè¿‡GPS-basedä¸‰è§’æµ‹é‡æŠ€æœ¯ä¼°è®¡ç‰©ä½“ä½ç½®ï¼Œå¹¶åˆ©ç”¨é’ˆå­”ç›¸æœºå‡ ä½•è®¡ç®—ç‰©ä½“é«˜åº¦ã€‚</li>
<li>ä½é€Ÿè¡Œé©¶ã€ç›¸æœºæœå†…çš„é…ç½®æä¾›äº†æœ€é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶åœ¨åŠ¨æ€åŸå¸‚ç¯å¢ƒä¸­è¿›è¡Œæ¤è¢«å’ŒåŸºç¡€è®¾æ–½çš„å®æ—¶ç›‘æµ‹å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a3c6159ed50a4e80b1a9030f102da35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9075900e8d082d3254803c373bb8d351.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d70a853cdc6b05ca420f3196f8daf1ea.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Ultrafast-X-ray-interaction-with-photovoltaic-materials-Thermal-and-nonthermal-responses"><a href="#Ultrafast-X-ray-interaction-with-photovoltaic-materials-Thermal-and-nonthermal-responses" class="headerlink" title="Ultrafast X-ray interaction with photovoltaic materials: Thermal and   nonthermal responses"></a>Ultrafast X-ray interaction with photovoltaic materials: Thermal and   nonthermal responses</h2><p><strong>Authors:Aldo ArtÃ­mez PeÃ±a, Nikita Medvedev</strong></p>
<p>Cadmium telluride (CdTe), lead sulfide (PbS), and indium tin oxide (ITO) are important in various electronic technologies, for which laser irradiation is used to selectively modify and design their unique semiconductor properties. We employ the hybrid multiscale code XTANT-3 to simulate the kinetics of material response to ultrafast X-ray irradiation. The code accounts for nonequilibrium electronic and atomic dynamics, nonadiabatic coupling, nonthermal melting, and bond breaking due to electronic excitation. Among the materials studied, CdTe exhibits the highest radiation resistance, similar to CdS. At the respective threshold doses, the melting is primarily thermal, driven by electron-phonon coupling, which is accompanied by the band gap closure. Additionally, all materials show nonthermal melting at higher doses. Threshold doses increase further if energy sinks and recrystallization are included. In CdTe and PbS, below 1.5 eV&#x2F;atom, the band gap returns to its original value upon recrystallization. As the dose increases, the cooled state becomes more amorphous, reducing the band gap until it stabilizes. Curiously, in a narrow window of deposited doses, ITO exhibit transient superionic behavior, with the liquid oxygen but solid In and Sn sublattices. At 0.6 eV&#x2F;atom in CdTe and 0.4 eV&#x2F;atom in PbS and ITO, material ablation from the surface occurs. The results suggest that femtosecond lasers may be used for tuning the band gap of photovoltaic semiconductors. </p>
<blockquote>
<p>é•‰ç¢²åŒ–ç‰©ï¼ˆCdTeï¼‰ã€é“…ç¡«åŒ–ç‰©ï¼ˆPbSï¼‰å’Œæ°§åŒ–é“Ÿé”¡ï¼ˆITOï¼‰åœ¨å„ç§ç”µå­æŠ€æœ¯ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œæ¿€å…‰è¾å°„è¢«ç”¨äºé€‰æ‹©æ€§åœ°ä¿®æ”¹å’Œè®¾è®¡å®ƒä»¬ç‹¬ç‰¹çš„åŠå¯¼ä½“å±æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨æ··åˆå¤šå°ºåº¦ä»£ç XTANT-3æ¥æ¨¡æ‹Ÿææ–™å¯¹è¶…å¿«Xå°„çº¿è¾å°„çš„åŠ¨åŠ›å­¦ååº”ã€‚è¯¥ä»£ç è€ƒè™‘äº†éå¹³è¡¡ç”µå­å’ŒåŸå­åŠ¨åŠ›å­¦ã€éç»çƒ­è€¦åˆã€éçƒ­ç†”åŒ–å’Œå› ç”µå­æ¿€å‘è€Œå¯¼è‡´çš„é”®æ–­è£‚ã€‚åœ¨æ‰€ç ”ç©¶çš„ææ–™ä¸­ï¼ŒCdTeè¡¨ç°å‡ºæœ€é«˜çš„æŠ—è¾å°„æ€§ï¼Œä¸CdSç›¸ä¼¼ã€‚åœ¨å„è‡ªçš„é˜ˆå€¼å‰‚é‡ä¸‹ï¼Œç†”åŒ–ä¸»è¦æ˜¯çƒ­é©±åŠ¨ï¼Œç”±ç”µå­-å£°å­è€¦åˆä¼´éšå¸¦éš™é—­åˆã€‚æ­¤å¤–ï¼Œæ‰€æœ‰ææ–™åœ¨è¾ƒé«˜å‰‚é‡ä¸‹å‡æ˜¾ç¤ºéçƒ­ç†”åŒ–ã€‚å¦‚æœåŒ…æ‹¬èƒ½é‡å¸æ”¶å’Œå†ç»“æ™¶ï¼Œé˜ˆå€¼å‰‚é‡ä¼šè¿›ä¸€æ­¥å¢åŠ ã€‚åœ¨CdTeå’ŒPbSä¸­ï¼Œä½äº1.5 eV&#x2F;atomçš„æƒ…å†µä¸‹ï¼Œå†ç»“æ™¶åå¸¦éš™ä¼šæ¢å¤åˆ°å…¶åŸå§‹å€¼ã€‚éšç€å‰‚é‡çš„å¢åŠ ï¼Œå†·å´çŠ¶æ€å˜å¾—æ›´åŠ æ— å®šå½¢ï¼Œå‡å°‘å¸¦éš™ç›´è‡³å…¶ç¨³å®šã€‚å¥‡æ€ªçš„æ˜¯ï¼Œåœ¨æ²‰ç§¯å‰‚é‡çš„ç‹­çª„èŒƒå›´å†…ï¼ŒITOè¡¨ç°å‡ºçŸ­æš‚çš„è¶…ç¦»å­è¡Œä¸ºï¼Œå…·æœ‰æ¶²æ€æ°§ä½†Inå’ŒSnå­æ™¶æ ¼ä¸ºå›ºæ€ã€‚åœ¨CdTeçš„0.6 eV&#x2F;atomå’ŒPbSåŠITOçš„0.4 eV&#x2F;atomæ—¶ï¼Œææ–™ä¼šä»è¡¨é¢æ¶ˆèã€‚ç»“æœè¡¨æ˜ï¼Œé£ç§’æ¿€å…‰å¯ç”¨äºè°ƒèŠ‚å…‰ä¼åŠå¯¼ä½“çš„å¸¦éš™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11549v1">PDF</a> 26 pages, 22 figures, 2 tables. arXiv admin note: text overlap with   arXiv:2502.05799</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†Cadmium tellurideï¼ˆCdTeï¼‰ã€lead sulfideï¼ˆPbSï¼‰å’Œindium tin oxideï¼ˆITOï¼‰åœ¨æ¿€å…‰ç…§å°„ä¸‹ææ–™ç‰¹æ€§çš„å˜åŒ–ã€‚é‡‡ç”¨XTANT-3æ··åˆå¤šå°ºåº¦ä»£ç æ¨¡æ‹Ÿäº†è¿™äº›ææ–™å¯¹è¶…å¿«Xå°„çº¿ç…§å°„çš„åŠ¨åŠ›å­¦å“åº”ï¼ŒåŒ…æ‹¬éå¹³è¡¡ç”µå­å’ŒåŸå­åŠ¨åŠ›å­¦ã€éç»çƒ­è€¦åˆã€éçƒ­ç†”åŒ–å’Œé”®æ–­è£‚ç­‰è¿‡ç¨‹ã€‚CdTeè¡¨ç°å‡ºè¾ƒé«˜çš„è¾å°„æŠ—æ€§ï¼Œä¸CdSç›¸ä¼¼ã€‚åœ¨ç‰¹å®šé˜ˆå€¼å‰‚é‡ä¸‹ï¼Œç†”åŒ–ä¸»è¦ç”±ç”µå­-å£°å­è€¦åˆé©±åŠ¨ï¼Œä¼´éšå¸¦éš™é—­åˆã€‚æ›´é«˜å‰‚é‡ä¸‹åˆ™è¡¨ç°å‡ºéçƒ­ç†”è§£ã€‚è€ƒè™‘èƒ½é‡æ²‰ç§¯å’Œå†ç»“æ™¶æ—¶ï¼Œé˜ˆå€¼å‰‚é‡è¿›ä¸€æ­¥å¢åŠ ã€‚åœ¨CdTeå’ŒPbSä¸­ï¼Œä½äº1.5 eV&#x2F;atomçš„æƒ…å†µä¸‹ï¼Œå†ç»“æ™¶åå¸¦éš™å¯æ¢å¤åˆ°åˆå§‹å€¼ã€‚å‰‚é‡å¢åŠ æ—¶ï¼Œå†·å´çŠ¶æ€å˜å¾—æ›´æ— å®šå½¢ï¼Œå¸¦éš™å‡å°ç›´è‡³ç¨³å®šã€‚åœ¨ç‰¹å®šçš„æ²‰ç§¯å‰‚é‡èŒƒå›´å†…ï¼ŒITOè¡¨ç°å‡ºç¬æ€è¶…ç¦»å­è¡Œä¸ºã€‚ç ”ç©¶ç»“æœæš—ç¤ºï¼Œé£ç§’æ¿€å…‰å¯ç”¨äºè°ƒèŠ‚å…‰ä¼åŠå¯¼ä½“çš„å¸¦éš™ã€‚</p>
<p><strong>è¦ç‚¹æ€»ç»“</strong></p>
<ol>
<li>Cadmium telluride (CdTe)ã€lead sulfide (PbS) å’Œ indium tin oxide (ITO) åœ¨ç”µå­ç§‘æŠ€ä¸­æœ‰é‡è¦åº”ç”¨ã€‚</li>
<li>ä½¿ç”¨XTANT-3ä»£ç æ¨¡æ‹Ÿäº†è¿™äº›ææ–™åœ¨è¶…å¿«Xå°„çº¿ç…§å°„ä¸‹çš„ååº”åŠ¨åŠ›å­¦ã€‚</li>
<li>CdTeå±•ç°å‡ºè¾ƒé«˜çš„è¾å°„æŠ—æ€§ï¼Œç±»ä¼¼äºCdSã€‚</li>
<li>åœ¨ç‰¹å®šé˜ˆå€¼å‰‚é‡ä¸‹ï¼Œææ–™ç†”åŒ–ä¸»è¦ç”±ç”µå­-å£°å­è€¦åˆé©±åŠ¨ï¼Œä¼´éšå¸¦éš™é—­åˆï¼›æ›´é«˜å‰‚é‡åˆ™è¡¨ç°ä¸ºéçƒ­ç†”è§£ã€‚</li>
<li>è€ƒè™‘èƒ½é‡æ²‰é›†å’Œå†ç»“æ™¶æ—¶ï¼Œé˜ˆå€¼å‰‚é‡ä¼šå¢åŠ ã€‚</li>
<li>ITOåœ¨ç‰¹å®šæ²‰ç§¯å‰‚é‡èŒƒå›´å†…å±•ç°å‡ºç¬æ€è¶…ç¦»å­è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e17418af22a92b1324a7d9e7a205cfcb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ab1f73b898ba67dc2aa042acf3363f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6ed5a61f07d054255efb332a74fbe08.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a74af4b9c3e8739f88780706771b511.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Subcortical-Masks-Generation-in-CT-Images-via-Ensemble-Based-Cross-Domain-Label-Transfer"><a href="#Subcortical-Masks-Generation-in-CT-Images-via-Ensemble-Based-Cross-Domain-Label-Transfer" class="headerlink" title="Subcortical Masks Generation in CT Images via Ensemble-Based   Cross-Domain Label Transfer"></a>Subcortical Masks Generation in CT Images via Ensemble-Based   Cross-Domain Label Transfer</h2><p><strong>Authors:Augustine X. W. Lee, Pak-Hei Yeung, Jagath C. Rajapakse</strong></p>
<p>Subcortical segmentation in neuroimages plays an important role in understanding brain anatomy and facilitating computer-aided diagnosis of traumatic brain injuries and neurodegenerative disorders. However, training accurate automatic models requires large amounts of labelled data. Despite the availability of publicly available subcortical segmentation datasets for Magnetic Resonance Imaging (MRI), a significant gap exists for Computed Tomography (CT). This paper proposes an automatic ensemble framework to generate high-quality subcortical segmentation labels for CT scans by leveraging existing MRI-based models. We introduce a robust ensembling pipeline to integrate them and apply it to unannotated paired MRI-CT data, resulting in a comprehensive CT subcortical segmentation dataset. Extensive experiments on multiple public datasets demonstrate the superior performance of our proposed framework. Furthermore, using our generated CT dataset, we train segmentation models that achieve improved performance on related segmentation tasks. To facilitate future research, we make our source code, generated dataset, and trained models publicly available at <a target="_blank" rel="noopener" href="https://github.com/SCSE-Biomedical-Computing-Group/CT-Subcortical-Segmentation">https://github.com/SCSE-Biomedical-Computing-Group/CT-Subcortical-Segmentation</a>, marking the first open-source release for CT subcortical segmentation to the best of our knowledge. </p>
<blockquote>
<p>ç¥ç»å½±åƒä¸­çš„çš®è´¨ä¸‹åˆ†å‰²å¯¹äºç†è§£å¤§è„‘ç»“æ„ä»¥åŠä¿ƒè¿›è®¡ç®—æœºè¾…åŠ©çš„è„‘å¤–ä¼¤å’Œç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„è¯Šæ–­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œè®­ç»ƒå‡†ç¡®çš„è‡ªåŠ¨æ¨¡å‹éœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®ã€‚å°½ç®¡å­˜åœ¨ç”¨äºç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„å…¬å¼€çš®è´¨ä¸‹åˆ†å‰²æ•°æ®é›†ï¼Œä½†åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ–¹é¢ä»å­˜åœ¨è¾ƒå¤§ç©ºç™½ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªè‡ªåŠ¨é›†æˆæ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨ç°æœ‰çš„åŸºäºMRIçš„æ¨¡å‹ï¼Œç”Ÿæˆé«˜è´¨é‡çš„CTæ‰«æçš®è´¨ä¸‹åˆ†å‰²æ ‡ç­¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç¨³å¥çš„é›†æˆç®¡é“æ¥æ•´åˆè¿™äº›æ¨¡å‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºæœªé…å¯¹çš„MRI-CTæ•°æ®ï¼Œä»è€Œç”Ÿæˆäº†ä¸€ä¸ªå…¨é¢çš„CTçš®è´¨ä¸‹åˆ†å‰²æ•°æ®é›†ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æå‡ºæ¡†æ¶çš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æˆ‘ä»¬ç”Ÿæˆçš„CTæ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹åœ¨ç›¸å…³åˆ†å‰²ä»»åŠ¡ä¸Šå®ç°äº†æ€§èƒ½æå‡ã€‚ä¸ºäº†ä¾¿äºæœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/SCSE-Biomedical-Computing-Group/CT-Subcortical-Segmentation">https://github.com/SCSE-Biomedical-Computing-Group/CT-Subcortical-Segmentation</a>ä¸Šå…¬å¼€äº†æˆ‘ä»¬çš„æºä»£ç ã€ç”Ÿæˆçš„æ•°æ®é›†å’Œè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯CTçš®è´¨ä¸‹åˆ†å‰²çš„ç¬¬ä¸€ä¸ªå¼€æºå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11450v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé›†æˆå­¦ä¹ çš„æ¡†æ¶ï¼Œåˆ©ç”¨å·²æœ‰çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å­çš®å±‚åˆ†å‰²æ ‡ç­¾æ•°æ®é›†ã€‚è¯¥æ¡†æ¶é€šè¿‡é›†æˆç®¡é“æ•´åˆMRIå’ŒCTæ•°æ®ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå…¨é¢çš„CTå­çš®å±‚åˆ†å‰²æ•°æ®é›†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæé«˜ç›¸å…³åˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œå…¬å¼€æºä»£ç ã€ç”Ÿæˆçš„æ•°æ®é›†å’Œè®­ç»ƒæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­çš®å±‚åˆ†å‰²åœ¨ç¥ç»å›¾åƒä¸­æ‰®æ¼”ç€ç†è§£å¤§è„‘ç»“æ„å’Œä¿ƒè¿›è®¡ç®—æœºè¾…åŠ©è¯Šæ–­è„‘æŸä¼¤å’Œç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„é‡è¦è§’è‰²ã€‚</li>
<li>è®­ç»ƒå‡†ç¡®çš„è‡ªåŠ¨æ¨¡å‹éœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®ï¼Œè€ŒCTå­çš®å±‚åˆ†å‰²æ•°æ®é›†å­˜åœ¨æ˜¾è‘—ç¼ºå£ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé›†æˆå­¦ä¹ çš„æ¡†æ¶ï¼Œåˆ©ç”¨MRIæ¨¡å‹ç”ŸæˆCTå­çš®å±‚åˆ†å‰²æ ‡ç­¾ã€‚</li>
<li>é€šè¿‡é›†æˆç®¡é“æ•´åˆMRIå’ŒCTæ•°æ®ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå…¨é¢çš„CTå­çš®å±‚åˆ†å‰²æ•°æ®é›†ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä½¿ç”¨ç”Ÿæˆçš„CTæ•°æ®é›†è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹åœ¨ç›¸å…³ä»»åŠ¡ä¸Šå–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e3238dc73c5cc22931e7eb4961fa8988.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03ce607958973ec607d94ddf0c1edd6b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-302bc9b4a798738709cf75b48d32d19f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AnatoMaskGAN-GNN-Driven-Slice-Feature-Fusion-and-Noise-Augmentation-for-Medical-Semantic-Image-Synthesis"><a href="#AnatoMaskGAN-GNN-Driven-Slice-Feature-Fusion-and-Noise-Augmentation-for-Medical-Semantic-Image-Synthesis" class="headerlink" title="AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for   Medical Semantic Image Synthesis"></a>AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for   Medical Semantic Image Synthesis</h2><p><strong>Authors:Zonglin Wu, Yule Xue, Qianxiang Hu, Yaoyao Feng, Yuqi Ma, Shanxiong Chen</strong></p>
<p>Medical semantic-mask synthesis boosts data augmentation and analysis, yet most GAN-based approaches still produce one-to-one images and lack spatial consistency in complex scans. To address this, we propose AnatoMaskGAN, a novel synthesis framework that embeds slice-related spatial features to precisely aggregate inter-slice contextual dependencies, introduces diverse image-augmentation strategies, and optimizes deep feature learning to improve performance on complex medical images. Specifically, we design a GNN-based strongly correlated slice-feature fusion module to model spatial relationships between slices and integrate contextual information from neighboring slices, thereby capturing anatomical details more comprehensively; we introduce a three-dimensional spatial noise-injection strategy that weights and fuses spatial features with noise to enhance modeling of structural diversity; and we incorporate a grayscale-texture classifier to optimize grayscale distribution and texture representation during generation. Extensive experiments on the public L2R-OASIS and L2R-Abdomen CT datasets show that AnatoMaskGAN raises PSNR on L2R-OASIS to 26.50 dB (0.43 dB higher than the current state of the art) and achieves an SSIM of 0.8602 on L2R-Abdomen CTâ€“a 0.48 percentage-point gain over the best model, demonstrating its superiority in reconstruction accuracy and perceptual quality. Ablation studies that successively remove the slice-feature fusion module, spatial 3D noise-injection strategy, and grayscale-texture classifier reveal that each component contributes significantly to PSNR, SSIM, and LPIPS, further confirming the independent value of each core design in enhancing reconstruction accuracy and perceptual quality. </p>
<blockquote>
<p>åŒ»å­¦è¯­ä¹‰æ©è†œåˆæˆä¿ƒè¿›äº†æ•°æ®å¢å¼ºå’Œåˆ†æï¼Œä½†å¤§å¤šæ•°åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ–¹æ³•ä»ç„¶åªèƒ½ç”Ÿæˆä¸€å¯¹ä¸€çš„å›¾åƒï¼Œåœ¨å¤æ‚æ‰«æä¸­ç¼ºä¹ç©ºé—´ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AnatoMaskGANï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åˆæˆæ¡†æ¶ï¼Œå®ƒåµŒå…¥åˆ‡ç‰‡ç›¸å…³çš„ç©ºé—´ç‰¹å¾æ¥ç²¾ç¡®èšåˆåˆ‡ç‰‡é—´çš„ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œå¼•å…¥å¤šç§å›¾åƒå¢å¼ºç­–ç•¥ï¼Œå¹¶ä¼˜åŒ–æ·±åº¦ç‰¹å¾å­¦ä¹ ï¼Œä»¥æé«˜åœ¨å¤æ‚åŒ»å­¦å›¾åƒä¸Šçš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„å¼ºç›¸å…³åˆ‡ç‰‡ç‰¹å¾èåˆæ¨¡å—ï¼Œä»¥å»ºæ¨¡åˆ‡ç‰‡ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼Œå¹¶æ•´åˆæ¥è‡ªç›¸é‚»åˆ‡ç‰‡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæ›´å…¨é¢åœ°æ•æ‰è§£å‰–ç»†èŠ‚ï¼›æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸‰ç»´ç©ºé—´å™ªå£°æ³¨å…¥ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡åŠ æƒå’Œèåˆç©ºé—´ç‰¹å¾ä¸å™ªå£°ï¼Œä»¥å¢å¼ºç»“æ„å¤šæ ·æ€§çš„å»ºæ¨¡ï¼›æˆ‘ä»¬è¿˜èå…¥äº†ä¸€ä¸ªç°åº¦çº¹ç†åˆ†ç±»å™¨ï¼Œä»¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç°åº¦åˆ†å¸ƒå’Œçº¹ç†è¡¨ç¤ºã€‚åœ¨å…¬å…±çš„L2R-OASISå’ŒL2R-Abdomen CTæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAnatoMaskGANåœ¨L2R-OASISä¸Šçš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æé«˜åˆ°26.50 dBï¼ˆæ¯”å½“å‰æœ€ä½³æ°´å¹³é«˜å‡º0.43 dBï¼‰ï¼Œåœ¨L2R-Abdomen CTä¸Šçš„ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰è¾¾åˆ°0.8602ï¼ˆæ¯”æœ€ä½³æ¨¡å‹é«˜å‡º0.48ä¸ªç™¾åˆ†ç‚¹ï¼‰ï¼Œè¯æ˜äº†å…¶åœ¨é‡å»ºç²¾åº¦å’Œæ„ŸçŸ¥è´¨é‡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚é€æ­¥ç§»é™¤åˆ‡ç‰‡ç‰¹å¾èåˆæ¨¡å—ã€ç©ºé—´3Då™ªå£°æ³¨å…¥ç­–ç•¥å’Œç°åº¦çº¹ç†åˆ†ç±»å™¨çš„æ¶ˆèç ”ç©¶è¯å®ï¼Œæ¯ä¸ªç»„ä»¶å¯¹PSNRã€SSIMå’ŒLPIPSéƒ½æœ‰æ˜¾è‘—è´¡çŒ®ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†æ¯ä¸ªæ ¸å¿ƒè®¾è®¡åœ¨æé«˜é‡å»ºç²¾åº¦å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢çš„ç‹¬ç«‹ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11375v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong><br>     åŒ»å­¦è¯­ä¹‰é®ç½©åˆæˆæå‡æ•°æ®æ‰©å……ä¸åˆ†æï¼Œé’ˆå¯¹å¤æ‚åŒ»å­¦å›¾åƒï¼ŒAnatoMaskGANæ¡†æ¶åµŒå…¥åˆ‡ç‰‡ç›¸å…³ç©ºé—´ç‰¹å¾ï¼Œç²¾ç¡®èšåˆåˆ‡ç‰‡é—´ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œå¼•å…¥å¤šæ ·åŒ–å›¾åƒå¢å¼ºç­–ç•¥ï¼Œä¼˜åŒ–æ·±åº¦ç‰¹å¾å­¦ä¹ ä»¥æå‡æ€§èƒ½ã€‚è®¾è®¡å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç›¸å…³åˆ‡ç‰‡ç‰¹å¾èåˆæ¨¡å—ï¼Œå»ºæ¨¡åˆ‡ç‰‡é—´ç©ºé—´å…³ç³»ï¼Œé›†æˆé‚»è¿‘åˆ‡ç‰‡ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå…¨é¢æ•æ‰è§£å‰–ç»†èŠ‚ã€‚å¼•å…¥ä¸‰ç»´ç©ºé—´å™ªå£°æ³¨å…¥ç­–ç•¥ï¼Œå¢å¼ºç»“æ„å¤šæ ·æ€§å»ºæ¨¡ã€‚ç»“åˆç°åº¦çº¹ç†åˆ†ç±»å™¨ï¼Œä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ç°åº¦åˆ†å¸ƒå’Œçº¹ç†è¡¨ç¤ºã€‚åœ¨å…¬å¼€æ•°æ®é›†L2R-OASISå’ŒL2R-Abdomen CTä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAnatoMaskGANåœ¨é‡å»ºç²¾åº¦å’Œæ„ŸçŸ¥è´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AnatoMaskGANæ¡†æ¶é€šè¿‡åµŒå…¥åˆ‡ç‰‡ç›¸å…³çš„ç©ºé—´ç‰¹å¾ï¼Œè§£å†³äº†åŒ»å­¦å›¾åƒåˆæˆä¸­ç¼ºä¹ç©ºé—´ä¸€è‡´æ€§çš„é—®é¢˜ã€‚</li>
<li>åˆ‡ç‰‡ç‰¹å¾èåˆæ¨¡å—åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å»ºæ¨¡åˆ‡ç‰‡é—´çš„ç©ºé—´å…³ç³»ï¼Œæé«˜äº†è§£å‰–ç»†èŠ‚çš„æ•æ‰èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼•å…¥ä¸‰ç»´ç©ºé—´å™ªå£°æ³¨å…¥ç­–ç•¥ï¼Œå¢å¼ºäº†ç»“æ„å¤šæ ·æ€§çš„å»ºæ¨¡ã€‚</li>
<li>ç»“åˆç°åº¦çº¹ç†åˆ†ç±»å™¨ï¼Œä¼˜åŒ–äº†ç”Ÿæˆå›¾åƒçš„ç°åº¦åˆ†å¸ƒå’Œçº¹ç†è¡¨ç¤ºã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAnatoMaskGANåœ¨é‡å»ºç²¾åº¦å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢å‡å®ç°äº†æ˜¾è‘—çš„æå‡ã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯å®äº†æ¡†æ¶ä¸­çš„æ¯ä¸ªç»„ä»¶å¯¹æ€§èƒ½çš„æå‡éƒ½èµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-051f86a7f3d02da72bdb11f5f835982c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-149011ca910cf7e8b3943114596ac14e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbe8d80ea5ec337adbdd5cbd34c7db89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b477c7d55dcc2f99ffc8b8f3250e84a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-062c0351cc3c090d6484d7858407109c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d9a511ac032b408434dbcf66c597229.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b12c37edc6c35ccca0126b82c1beac0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77badb071a5acae80b3ddae7029e981c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Does-the-Skeleton-Recall-Loss-Really-Work"><a href="#Does-the-Skeleton-Recall-Loss-Really-Work" class="headerlink" title="Does the Skeleton-Recall Loss Really Work?"></a>Does the Skeleton-Recall Loss Really Work?</h2><p><strong>Authors:Devansh Arora, Nitin Kumar, Sukrit Gupta</strong></p>
<p>Image segmentation is an important and widely performed task in computer vision. Accomplishing effective image segmentation in diverse settings often requires custom model architectures and loss functions. A set of models that specialize in segmenting thin tubular structures are topology preservation-based loss functions. These models often utilize a pixel skeletonization process claimed to generate more precise segmentation masks of thin tubes and better capture the structures that other models often miss. One such model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\cite {kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark tubular datasets. In this work, we performed a theoretical analysis of the gradients for the SRL loss. Upon comparing the performance of the proposed method on some of the tubular datasets (used in the original work, along with some additional datasets), we found that the performance of SRL-based segmentation models did not exceed traditional baseline models. By providing both a theoretical explanation and empirical evidence, this work critically evaluates the limitations of topology-based loss functions, offering valuable insights for researchers aiming to develop more effective segmentation models for complex tubular structures. </p>
<blockquote>
<p>å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­ä¸€ä¸ªé‡è¦ä¸”å¹¿æ³›æ‰§è¡Œçš„ä»»åŠ¡ã€‚åœ¨ä¸åŒçš„åœºæ™¯ä¸‹å®ç°æœ‰æ•ˆçš„å›¾åƒåˆ†å‰²é€šå¸¸éœ€è¦å®šåˆ¶æ¨¡å‹æ¶æ„å’ŒæŸå¤±å‡½æ•°ã€‚ä¸“é—¨ç”¨äºåˆ†å‰²ç»†é•¿ç®¡çŠ¶ç»“æ„çš„æ¨¡å‹é›†æ˜¯åŸºäºæ‹“æ‰‘ä¿ç•™çš„æŸå¤±å‡½æ•°ã€‚è¿™äº›æ¨¡å‹é€šå¸¸åˆ©ç”¨åƒç´ éª¨æ¶åŒ–è¿‡ç¨‹ï¼Œå£°ç§°å¯ä»¥ç”Ÿæˆæ›´ç²¾ç¡®çš„ç»†ç®¡åˆ†å‰²æ©è†œï¼Œå¹¶æ›´å¥½åœ°æ•æ‰å…¶ä»–æ¨¡å‹å¸¸å¿½ç•¥çš„ç»“æ„ã€‚åŸºå°”éœå¤«ç­‰äººæå‡ºçš„Skeleton Recall Lossï¼ˆSRLï¼‰å°±æ˜¯è¿™æ ·ä¸€ç§æ¨¡å‹~\cite {kirchhoff2024srl}ï¼Œæ®ç§°åœ¨åŸºå‡†ç®¡çŠ¶æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹SRLæŸå¤±çš„æ¢¯åº¦è¿›è¡Œäº†ç†è®ºåˆ†æã€‚åœ¨æ¯”è¾ƒæ‰€æå‡ºæ–¹æ³•åœ¨éƒ¨åˆ†ç®¡çŠ¶æ•°æ®é›†ï¼ˆä¸åŸå§‹å·¥ä½œä¸­ä½¿ç”¨çš„æ•°æ®é›†ä¸€èµ·ä½¿ç”¨çš„é™„åŠ æ•°æ®é›†ï¼‰ä¸Šçš„æ€§èƒ½æ—¶ï¼Œæˆ‘ä»¬å‘ç°åŸºäºSRLçš„åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½å¹¶æœªè¶…è¿‡ä¼ ç»ŸåŸºçº¿æ¨¡å‹ã€‚é€šè¿‡æä¾›ç†è®ºè§£é‡Šå’Œå®è¯è¯æ®ï¼Œè¿™é¡¹å·¥ä½œå¯¹åŸºäºæ‹“æ‰‘çš„æŸå¤±å‡½æ•°çš„å±€é™æ€§è¿›è¡Œäº†æ‰¹åˆ¤æ€§è¯„ä»·ï¼Œä¸ºæ—¨åœ¨å¼€å‘é’ˆå¯¹å¤æ‚ç®¡çŠ¶ç»“æ„æ›´æœ‰æ•ˆåˆ†å‰²æ¨¡å‹çš„ç ”ç©¶äººå‘˜æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11374v1">PDF</a> </p>
<p><strong>Summary</strong><br>     éª¨æ¶å›å¿†æŸå¤±ï¼ˆSRLï¼‰æ˜¯é’ˆå¯¹ç®¡çŠ¶ç»“æ„åˆ†å‰²çš„æ‹“æ‰‘ä¿ç•™æŸå¤±å‡½æ•°ä¹‹ä¸€ï¼Œä½†åœ¨ç†è®ºåˆ†æå’Œå®é™…æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°å¹¶æœªè¶…è¿‡ä¼ ç»ŸåŸºçº¿æ¨¡å‹ã€‚æœ¬ç ”ç©¶æ—¢æä¾›äº†ç†è®ºè§£é‡Šï¼Œä¹Ÿç»™å‡ºäº†å®è¯è¯æ®ï¼Œè¯„ä¼°äº†æ‹“æ‰‘åŸºæŸå¤±å‡½æ•°åœ¨å¤æ‚ç®¡çŠ¶ç»“æ„åˆ†å‰²æ¨¡å‹ä¸­çš„å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„é‡è¦ä»»åŠ¡ï¼Œå°¤å…¶åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­éœ€è¦å®šåˆ¶æ¨¡å‹æ¶æ„å’ŒæŸå¤±å‡½æ•°ã€‚</li>
<li>æ‹“æ‰‘ä¿ç•™æŸå¤±å‡½æ•°ä¸“é—¨ç”¨äºåˆ†å‰²ç»†ç®¡çŠ¶ç»“æ„ï¼Œå¸¸ç”¨åƒç´ éª¨æ¶åŒ–è¿‡ç¨‹ç”Ÿæˆæ›´ç²¾ç¡®çš„ç»†ç®¡åˆ†å‰²æ©æ¨¡ã€‚</li>
<li>éª¨æ¶å›å¿†æŸå¤±ï¼ˆSRLï¼‰åœ¨ç®¡çŠ¶æ•°æ®é›†ä¸Šçš„è¡¨ç°è¢«è¯„ä¼°ï¼Œä½†æœªè¶…è¿‡ä¼ ç»ŸåŸºçº¿æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯ç ”ç©¶è¯„ä»·äº†åŸºäºæ‹“æ‰‘çš„æŸå¤±å‡½æ•°åœ¨å¤æ‚ç®¡çŠ¶ç»“æ„åˆ†å‰²æ¨¡å‹ä¸­çš„å±€é™æ€§ã€‚</li>
<li>SRLæŸå¤±çš„ç†è®ºæ¢¯åº¦åˆ†æä¹Ÿè¢«æ‰§è¡Œï¼Œä¸ºæŸå¤±å‡½æ•°çš„è¿›ä¸€æ­¥ä¼˜åŒ–æä¾›äº†æ–¹å‘ã€‚</li>
<li>é™¤äº†åŸå§‹æ•°æ®é›†å¤–ï¼Œç ”ç©¶è¿˜åœ¨å…¶ä»–æ•°æ®é›†ä¸ŠéªŒè¯äº†SRLçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11374">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f6b502c45e501e714eb1affcd08a55bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca005b76bfda806eb7e031239c2ed066.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87f42546bb5094dcb7fc023c94bc3c61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d545b33fe60a3a4ca00d7de884fc873.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TimeMachine-Fine-Grained-Facial-Age-Editing-with-Identity-Preservation"><a href="#TimeMachine-Fine-Grained-Facial-Age-Editing-with-Identity-Preservation" class="headerlink" title="TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation"></a>TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation</h2><p><strong>Authors:Yilin Mi, Qixin Yan, Zheng-Peng Duan, Chunle Guo, Hubery Yin, Hao Liu, Chen Li, Chongyi Li</strong></p>
<p>With the advancement of generative models, facial image editing has made significant progress. However, achieving fine-grained age editing while preserving personal identity remains a challenging task.In this paper, we propose TimeMachine, a novel diffusion-based framework that achieves accurate age editing while keeping identity features unchanged. To enable fine-grained age editing, we inject high-precision age information into the multi-cross attention module, which explicitly separates age-related and identity-related features. This design facilitates more accurate disentanglement of age attributes, thereby allowing precise and controllable manipulation of facial aging.Furthermore, we propose an Age Classifier Guidance (ACG) module that predicts age directly in the latent space, instead of performing denoising image reconstruction during training. By employing a lightweight module to incorporate age constraints, this design enhances age editing accuracy by modest increasing training cost. Additionally, to address the lack of large-scale, high-quality facial age datasets, we construct a HFFA dataset (High-quality Fine-grained Facial-Age dataset) which contains one million high-resolution images labeled with identity and facial attributes. Experimental results demonstrate that TimeMachine achieves state-of-the-art performance in fine-grained age editing while preserving identity consistency. </p>
<blockquote>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„å‘å±•ï¼Œé¢éƒ¨å›¾åƒç¼–è¾‘å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨ä¿æŒä¸ªäººèº«ä»½çš„åŒæ—¶å®ç°ç²¾ç»†å¹´é¾„ç¼–è¾‘ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TimeMachineï¼Œä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒèº«ä»½ç‰¹å¾ä¸å˜çš„æƒ…å†µä¸‹å®ç°ç²¾ç¡®çš„å¹´é¾„ç¼–è¾‘ã€‚ä¸ºäº†å®ç°ç²¾ç»†å¹´é¾„ç¼–è¾‘ï¼Œæˆ‘ä»¬å°†é«˜ç²¾åº¦å¹´é¾„ä¿¡æ¯æ³¨å…¥å¤šäº¤å‰æ³¨æ„æ¨¡å—ï¼Œè¯¥æ¨¡å—æ˜¾å¼åœ°åˆ†ç¦»å¹´é¾„ç›¸å…³å’Œèº«ä»½ç›¸å…³çš„ç‰¹å¾ã€‚è¿™ç§è®¾è®¡æœ‰åŠ©äºæ›´ç²¾ç¡®åœ°åˆ†ç¦»å¹´é¾„å±æ€§ï¼Œä»è€Œå…è®¸ç²¾ç¡®ä¸”å¯æ§çš„é¢éƒ¨è¡°è€æ“ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å¹´é¾„åˆ†ç±»å™¨æŒ‡å¯¼ï¼ˆACGï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­é¢„æµ‹å¹´é¾„ï¼Œè€Œä¸æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œå»å™ªå›¾åƒé‡å»ºã€‚é€šè¿‡é‡‡ç”¨è½»é‡çº§æ¨¡å—æ¥èå…¥å¹´é¾„çº¦æŸï¼Œè¿™ç§è®¾è®¡é€šè¿‡é€‚åº¦å¢åŠ è®­ç»ƒæˆæœ¬æ¥æé«˜å¹´é¾„ç¼–è¾‘çš„å‡†ç¡®æ€§ã€‚å¦å¤–ï¼Œä¸ºäº†è§£å†³ç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡é¢éƒ¨å¹´é¾„æ•°æ®é›†çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªHFFAæ•°æ®é›†ï¼ˆé«˜è´¨é‡ç²¾ç»†é¢éƒ¨å¹´é¾„æ•°æ®é›†ï¼‰ï¼Œå…¶ä¸­åŒ…å«ä¸€åƒä¸‡å¼ å¸¦æœ‰èº«ä»½å’Œé¢éƒ¨å±æ€§æ ‡ç­¾çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeMachineåœ¨ç²¾ç»†å¹´é¾„ç¼–è¾‘æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†èº«ä»½ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11284v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„å‘å±•ï¼Œé¢éƒ¨å›¾åƒç¼–è¾‘å·²å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†åœ¨ä¿æŒä¸ªäººèº«ä»½çš„åŒæ—¶å®ç°ç²¾ç»†å¹´é¾„ç¼–è¾‘ä»å…·æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºTimeMachineï¼Œä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹æ¡†æ¶ï¼Œå¯åœ¨å‡†ç¡®ç¼–è¾‘å¹´é¾„çš„åŒæ—¶ä¿æŒèº«ä»½ç‰¹å¾ä¸å˜ã€‚ä¸ºå®ç°ç²¾ç»†å¹´é¾„ç¼–è¾‘ï¼Œæˆ‘ä»¬åœ¨å¤šäº¤å‰æ³¨æ„æ¨¡å—ä¸­æ³¨å…¥é«˜ç²¾åº¦å¹´é¾„ä¿¡æ¯ï¼Œæ˜ç¡®åˆ†ç¦»å¹´é¾„ç›¸å…³å’Œèº«ä»½ç›¸å…³ç‰¹å¾ï¼Œä¿ƒè¿›å¹´é¾„å±æ€§çš„ç²¾ç¡®åˆ†ç¦»ï¼Œä»è€Œå®ç°é¢éƒ¨è¡°è€çš„ç²¾ç¡®å¯æ§æ“ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºå¹´é¾„åˆ†ç±»å™¨æŒ‡å¯¼ï¼ˆACGï¼‰æ¨¡å—ï¼Œç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­é¢„æµ‹å¹´é¾„ï¼Œè€Œä¸æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œå»å™ªå›¾åƒé‡å»ºã€‚é€šè¿‡é‡‡ç”¨è½»é‡çº§æ¨¡å—æ¥èå…¥å¹´é¾„çº¦æŸï¼Œè¿™ç§è®¾è®¡åœ¨é€‚åº¦å¢åŠ è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹æé«˜äº†å¹´é¾„ç¼–è¾‘çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œä¸ºè§£å†³ç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡é¢éƒ¨å¹´é¾„æ•°æ®é›†çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†HFFAæ•°æ®é›†ï¼ˆé«˜è´¨é‡ç²¾ç»†é¢éƒ¨å¹´é¾„æ•°æ®é›†ï¼‰ï¼ŒåŒ…å«ä¸€åƒä¸‡å¼ å¸¦æœ‰èº«ä»½å’Œé¢éƒ¨å±æ€§æ ‡ç­¾çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTimeMachineåœ¨ç²¾ç»†å¹´é¾„ç¼–è¾‘æ–¹é¢å®ç°äº†å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†èº«ä»½ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TimeMachineæ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ–°å‹é¢éƒ¨å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œå¯å®ç°å‡†ç¡®å¹´é¾„ç¼–è¾‘åŒæ—¶ä¿æŒèº«ä»½ç‰¹å¾ä¸å˜ã€‚</li>
<li>é€šè¿‡æ³¨å…¥é«˜ç²¾åº¦å¹´é¾„ä¿¡æ¯åˆ°å¤šäº¤å‰æ³¨æ„æ¨¡å—ï¼Œå®ç°ç²¾ç»†å¹´é¾„ç¼–è¾‘ã€‚</li>
<li>æå‡ºAge Classifier Guidanceï¼ˆACGï¼‰æ¨¡å—ï¼Œç›´æ¥åœ¨æ½œåœ¨ç©ºé—´é¢„æµ‹å¹´é¾„ï¼Œæé«˜å¹´é¾„ç¼–è¾‘å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨è½»é‡çº§æ¨¡å—èå…¥å¹´é¾„çº¦æŸï¼Œé€‚åº¦å¢åŠ è®­ç»ƒæˆæœ¬ã€‚</li>
<li>æ„å»ºHFFAæ•°æ®é›†ï¼ŒåŒ…å«ä¸€åƒä¸‡å¼ é«˜è´¨é‡ã€ç²¾ç»†é¢éƒ¨å¹´é¾„çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚</li>
<li>TimeMachineåœ¨ä¿æŒèº«ä»½ä¸€è‡´æ€§çš„åŒæ—¶å®ç°äº†ç²¾ç»†å¹´é¾„ç¼–è¾‘çš„å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bdb01c08e6289b88fd5cd4136e75cd35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32ea9c7b09d4b6b4501b99e88fea929c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bebdea02a1b36091dd26db7a47b9c7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e45314c4c0f5782bade086b4dbc73806.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c77e555dbc8477c2532120987686f1b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-Image-to-Image-Schrodinger-Bridge-for-CT-Field-of-View-Extension"><a href="#Efficient-Image-to-Image-Schrodinger-Bridge-for-CT-Field-of-View-Extension" class="headerlink" title="Efficient Image-to-Image SchrÃ¶dinger Bridge for CT Field of View   Extension"></a>Efficient Image-to-Image SchrÃ¶dinger Bridge for CT Field of View   Extension</h2><p><strong>Authors:Zhenhao Li, Long Yang, Xiaojie Yin, Haijun Yu, Jiazhou Wang, Hongbin Han, Weigang Hu, Yixing Huang</strong></p>
<p>Computed tomography (CT) is a cornerstone imaging modality for non-invasive, high-resolution visualization of internal anatomical structures. However, when the scanned object exceeds the scannerâ€™s field of view (FOV), projection data are truncated, resulting in incomplete reconstructions and pronounced artifacts near FOV boundaries. Conventional reconstruction algorithms struggle to recover accurate anatomy from such data, limiting clinical reliability. Deep learning approaches have been explored for FOV extension, with diffusion generative models representing the latest advances in image synthesis. Yet, conventional diffusion models are computationally demanding and slow at inference due to their iterative sampling process. To address these limitations, we propose an efficient CT FOV extension framework based on the image-to-image Schr&quot;odinger Bridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that synthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic mapping between paired limited-FOV and extended-FOV images. This direct correspondence yields a more interpretable and traceable generative process, enhancing anatomical consistency and structural fidelity in reconstructions. I$^2$SB achieves superior quantitative performance, with root-mean-square error (RMSE) values of 49.8,HU on simulated noisy data and 152.0HU on real data, outperforming state-of-the-art diffusion models such as conditional denoising diffusion probabilistic models (cDDPM) and patch-based diffusion methods. Moreover, its one-step inference enables reconstruction in just 0.19s per 2D slice, representing over a 700-fold speedup compared to cDDPM (135s) and surpassing diffusionGAN (0.58s), the second fastest. This combination of accuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical deployment. </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ˜¯ä¸€ç§éä¾µå…¥æ€§çš„é«˜åˆ†è¾¨ç‡æˆåƒæŠ€æœ¯ï¼Œç”¨äºå¯è§†åŒ–å†…éƒ¨è§£å‰–ç»“æ„ã€‚ç„¶è€Œï¼Œå½“æ‰«æå¯¹è±¡è¶…å‡ºæ‰«æä»ªçš„è§†é‡ï¼ˆFOVï¼‰æ—¶ï¼ŒæŠ•å½±æ•°æ®ä¼šè¢«æˆªæ–­ï¼Œå¯¼è‡´é‡å»ºä¸å®Œæ•´ï¼Œå¹¶ä¸”åœ¨FOVè¾¹ç•Œé™„è¿‘å‡ºç°æ˜æ˜¾çš„ä¼ªå½±ã€‚ä¼ ç»Ÿçš„é‡å»ºç®—æ³•éš¾ä»¥ä»è¿™ç§æ•°æ®ä¸­æ¢å¤å‡†ç¡®çš„è§£å‰–ç»“æ„ï¼Œä»è€Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸Šçš„å¯é æ€§ã€‚æ·±åº¦å­¦ä¹ çš„æ–¹æ³•å·²ç»è¢«æ¢ç´¢ç”¨äºæ‰©å±•FOVï¼Œå…¶ä¸­æ‰©æ•£ç”Ÿæˆæ¨¡å‹ä»£è¡¨äº†å›¾åƒåˆæˆçš„æœ€æ–°è¿›å±•ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹è®¡ç®—é‡å¤§ï¼Œç”±äºè¿­ä»£é‡‡æ ·è¿‡ç¨‹ï¼Œæ¨ç†é€Ÿåº¦æ…¢ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒåˆ°å›¾åƒSchrÃ¶dinger Bridgeï¼ˆI$^2$SBï¼‰æ‰©æ•£æ¨¡å‹çš„CT FOVæ‰©å±•æ¡†æ¶ã€‚ä¸åŒäºä¼ ç»Ÿåœ°ä»çº¯é«˜æ–¯å™ªå£°åˆæˆå›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼ŒI$^2$SBå­¦ä¹ é…å¯¹æœ‰é™FOVå’Œæ‰©å±•FOVå›¾åƒä¹‹é—´çš„ç›´æ¥éšæœºæ˜ å°„ã€‚è¿™ç§ç›´æ¥å¯¹åº”å…³ç³»äº§ç”Ÿäº†ä¸€ä¸ªæ›´å¯è§£é‡Šå’Œå¯è¿½è¸ªçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæé«˜äº†é‡å»ºä¸­çš„è§£å‰–ä¸€è‡´æ€§å’Œç»“æ„ä¿çœŸåº¦ã€‚I$^2$SBåœ¨æ¨¡æ‹Ÿå™ªå£°æ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šåˆ†åˆ«è¾¾åˆ°äº†49.8 HUå’Œ152.0 HUçš„å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å€¼ï¼Œè¶…è¶Šäº†æœ€æ–°çš„æ‰©æ•£æ¨¡å‹ï¼Œå¦‚æ¡ä»¶å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆcDDPMï¼‰å’ŒåŸºäºè¡¥ä¸çš„æ‰©æ•£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå…¶ä¸€æ­¥æ¨ç†ä½¿å¾—æ¯ä¸ª2Dåˆ‡ç‰‡çš„é‡å»ºæ—¶é—´ä»…éœ€0.19ç§’ï¼Œä¸cDDPMç›¸æ¯”å®ç°äº†è¶…è¿‡700å€çš„åŠ é€Ÿï¼ˆ135ç§’ï¼‰ï¼Œå¹¶è¶…è¶Šäº†ç¬¬äºŒå¿«çš„DiffusionGANï¼ˆ0.58ç§’ï¼‰ã€‚è¿™ç§ç²¾åº¦å’Œæ•ˆç‡çš„ç»“åˆä½¿å¾—I$^2$SBéå¸¸é€‚åˆå®æ—¶æˆ–ä¸´åºŠéƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11211v1">PDF</a> 10 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>CTæˆåƒä¸­çš„è§†åœºï¼ˆFOVï¼‰æ‰©å±•é—®é¢˜ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„é‡å»ºç®—æ³•æ— æ³•ä»æˆªæ–­çš„æ•°æ®ä¸­æ¢å¤å‡†ç¡®çš„è§£å‰–ç»“æ„ï¼Œè€Œæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒåˆ°å›¾åƒSchrÃ¶dinger Bridgeï¼ˆIÂ²SBï¼‰æ‰©æ•£æ¨¡å‹çš„CT FOVæ‰©å±•æ¡†æ¶ï¼Œå…·æœ‰é«˜æ•ˆã€å‡†ç¡®çš„ç‰¹ç‚¹ã€‚è¯¥æ¨¡å‹èƒ½ç›´æ¥åœ¨æœ‰é™çš„FOVå›¾åƒå’Œæ‰©å±•çš„FOVå›¾åƒä¹‹é—´å»ºç«‹éšæœºæ˜ å°„ï¼Œæé«˜äº†é‡å»ºçš„è§£å‰–ä¸€è‡´æ€§å’Œç»“æ„ä¿çœŸåº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIÂ²SBåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå®ç°äº†å¿«é€Ÿä¸”å‡†ç¡®çš„é‡å»ºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CTæˆåƒä¸­ï¼Œå½“æ‰«æå¯¹è±¡è¶…å‡ºæ‰«æå™¨è§†åœºï¼ˆFOVï¼‰æ—¶ï¼Œä¼šå‡ºç°æ•°æ®æˆªæ–­çš„é—®é¢˜ï¼Œå¯¼è‡´é‡å»ºä¸å®Œæ•´å’Œåœ¨è§†åœºè¾¹ç•Œé™„è¿‘å‡ºç°æ˜æ˜¾çš„ä¼ªå½±ã€‚</li>
<li>ä¼ ç»Ÿçš„é‡å»ºç®—æ³•éš¾ä»¥ä»è¿™ç§æˆªæ–­çš„æ•°æ®ä¸­æ¢å¤å‡†ç¡®çš„è§£å‰–ç»“æ„ï¼Œå½±å“äº†ä¸´åºŠå¯é æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨FOVæ‰©å±•æ–¹é¢è¿›è¡Œäº†æ¢ç´¢ï¼Œæœ€æ–°çš„è¿›å±•æ˜¯å›¾åƒåˆæˆä¸­çš„æ‰©æ•£ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹è®¡ç®—é‡å¤§ï¼Œæ¨ç†é€Ÿåº¦æ…¢ï¼Œå› ä¸ºå…¶è¿­ä»£é‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„CT FOVæ‰©å±•æ¡†æ¶ï¼ŒåŸºäºå›¾åƒåˆ°å›¾åƒSchrÃ¶dinger Bridgeï¼ˆIÂ²SBï¼‰æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>IÂ²SBæ¨¡å‹ç›´æ¥åœ¨æœ‰é™çš„FOVå›¾åƒå’Œæ‰©å±•çš„FOVå›¾åƒä¹‹é—´å»ºç«‹ç›´æ¥å¯¹åº”å…³ç³»ï¼Œæé«˜äº†ç”Ÿæˆçš„è§£å‰–ä¸€è‡´æ€§å’Œç»“æ„ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-047017924f88b3347ff76624dda29dd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f92569f85bc980c94d4e96cb334630e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e34671900878bc4c011604e8d58031b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d5d64fdaa4aa22b8c2ff0e48cf8fe3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4082f0cf62b6fcfb96244c14a6b0918.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-581c6dc2eb18a689e4ea013755599730.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-670b8777e33b37b800bb24dfa990253b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Performance-of-GPT-5-in-Brain-Tumor-MRI-Reasoning"><a href="#Performance-of-GPT-5-in-Brain-Tumor-MRI-Reasoning" class="headerlink" title="Performance of GPT-5 in Brain Tumor MRI Reasoning"></a>Performance of GPT-5 in Brain Tumor MRI Reasoning</h2><p><strong>Authors:Mojtaba Safari, Shansong Wang, Mingzhe Hu, Zach Eidex, Qiang Li, Xiaofeng Yang</strong></p>
<p>Accurate differentiation of brain tumor types on magnetic resonance imaging (MRI) is critical for guiding treatment planning in neuro-oncology. Recent advances in large language models (LLMs) have enabled visual question answering (VQA) approaches that integrate image interpretation with natural language reasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and GPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor Segmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain metastases (MET). Each case included multi-sequence MRI triplanar mosaics and structured clinical features transformed into standardized VQA items. Models were assessed in a zero-shot chain-of-thought setting for accuracy on both visual and reasoning tasks. Results showed that GPT-5-mini achieved the highest macro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%), and GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single model dominating across all cohorts. These findings suggest that GPT-5 family models can achieve moderate accuracy in structured neuro-oncological VQA tasks, but not at a level acceptable for clinical use. </p>
<blockquote>
<p>åœ¨ç¥ç»è‚¿ç˜¤å­¦ä¸­ï¼Œå¯¹ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸Šçš„è„‘è‚¿ç˜¤ç±»å‹è¿›è¡Œå‡†ç¡®åŒºåˆ†å¯¹äºæ²»ç–—è®¡åˆ’çš„åˆ¶å®šè‡³å…³é‡è¦ã€‚æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å¾—è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ–¹æ³•å¾—ä»¥å‘å±•ï¼Œè¯¥æ–¹æ³•å°†å›¾åƒè§£è¯»ä¸è‡ªç„¶è¯­è¨€æ¨ç†ç›¸ç»“åˆã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†GPT-4oã€GPT-5-nanoã€GPT-5-miniå’ŒGPT-5åœ¨ä¸€ç»„ç»è¿‡ç­›é€‰çš„è„‘è‚¿ç˜¤VQAåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œè¯¥æµ‹è¯•æ•°æ®é›†æ¥è‡ªä¸‰ä¸ªè„‘è‚¿ç˜¤åˆ†å‰²ï¼ˆBraTSï¼‰æ•°æ®é›†â€”â€”èƒ¶è´¨æ¯ç»†èƒç˜¤ï¼ˆGLIï¼‰ã€è„‘è†œç˜¤ï¼ˆMENï¼‰å’Œè„‘è½¬ç§»ç˜¤ï¼ˆMETï¼‰ã€‚æ¯ä¸ªæ¡ˆä¾‹éƒ½åŒ…æ‹¬å¤šåºåˆ—MRIä¸‰å¹³é¢é•¶åµŒå›¾å’Œè½¬åŒ–ä¸ºæ ‡å‡†åŒ–VQAé¡¹ç›®çš„ç»“æ„åŒ–ä¸´åºŠç‰¹å¾ã€‚æ¨¡å‹æ˜¯åœ¨é›¶å°„å‡»æ€ç»´é“¾ç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°ï¼Œå¯¹è§†è§‰å’Œæ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§è¿›è¡Œè¯„åˆ¤ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-5-miniçš„å®å¹³å‡å‡†ç¡®ç‡æœ€é«˜ï¼ˆ44.19%ï¼‰ï¼Œå…¶æ¬¡æ˜¯GPT-5ï¼ˆ43.71%ï¼‰ã€GPT-4oï¼ˆ41.49%ï¼‰å’ŒGPT-5-nanoï¼ˆ35.85%ï¼‰ã€‚ä¸åŒè‚¿ç˜¤äºšå‹çš„æ€§èƒ½å„ä¸ç›¸åŒï¼Œæ²¡æœ‰ä»»ä½•å•ä¸€æ¨¡å‹åœ¨æ‰€æœ‰é˜Ÿåˆ—ä¸­éƒ½å æ®ä¸»å¯¼åœ°ä½ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒGPT-5ç³»åˆ—æ¨¡å‹åœ¨ç»“æ„åŒ–çš„ç¥ç»è‚¿ç˜¤å­¦VQAä»»åŠ¡ä¸­å¯ä»¥è¾¾åˆ°ä¸­ç­‰å‡†ç¡®æ€§ï¼Œä½†å°šæœªè¾¾åˆ°ä¸´åºŠä½¿ç”¨è¦æ±‚çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10865v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶è¯„ä¼°äº†GPT-4oã€GPT-5-nanoã€GPT-5-miniå’ŒGPT-5åœ¨è„‘è‚¿ç˜¤VQAé‰´åˆ«æ–¹é¢çš„æ€§èƒ½ï¼Œå‘ç°GPT-5å®¶æ—æ¨¡å‹åœ¨ç»“æ„åŒ–ç¥ç»è‚¿ç˜¤å­¦VQAä»»åŠ¡ä¸­èƒ½è¾¾åˆ°ä¸­ç­‰ç²¾åº¦ï¼Œä½†å°šæœªè¾¾åˆ°ä¸´åºŠä½¿ç”¨è¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹è„‘è‚¿ç˜¤è¿›è¡Œè§†è§‰é—®ç­”ï¼ˆVQAï¼‰è¯„ä¼°ã€‚</li>
<li>ä½¿ç”¨äº†åŒ…æ‹¬èƒ¶è´¨æ¯ç»†èƒç˜¤ï¼ˆGLIï¼‰ã€è„‘è†œç˜¤ï¼ˆMENï¼‰å’Œè„‘è½¬ç§»ç˜¤ï¼ˆMETï¼‰åœ¨å†…çš„ä¸‰ä¸ªæ•°æ®é›†è¿›è¡ŒVQAè¯„ä¼°ã€‚</li>
<li>æ¨¡å‹åœ¨é›¶æ ·æœ¬é“¾æ€ç»´æ¨¡å¼ä¸‹è¿›è¡Œè¯„ä¼°ï¼ŒGPT-5-miniè·å¾—æœ€é«˜å®è§‚å¹³å‡å‡†ç¡®ç‡ï¼ˆ44.19%ï¼‰ã€‚</li>
<li>ä¸åŒè‚¿ç˜¤äºšå‹çš„æ€§èƒ½è¡¨ç°å­˜åœ¨å·®å¼‚ï¼Œæ²¡æœ‰å•ä¸€æ¨¡å‹åœ¨æ‰€æœ‰é˜Ÿåˆ—ä¸­éƒ½å æ®ä¸»å¯¼åœ°ä½ã€‚</li>
<li>GPT-5å®¶æ—æ¨¡å‹åœ¨ç»“æ„åŒ–ç¥ç»è‚¿ç˜¤å­¦VQAä»»åŠ¡ä¸­è¡¨ç°ä¸­ç­‰ç²¾åº¦ã€‚</li>
<li>ç›®å‰å°šæœªè¾¾åˆ°ä¸´åºŠä½¿ç”¨è¦æ±‚çš„æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-65757aa75cba60480bcd4e9914f323f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-887371aa9ee61f1ad137075539cdf0dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0d785a72dc0d89eb17a05729b6956a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12ad61e8baba484d901e5b656f86bd43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f117733e7235ec3fb7883e9cfbbf6345.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mobile-Friendly-Deep-Learning-for-Plant-Disease-Detection-A-Lightweight-CNN-Benchmark-Across-101-Classes-of-33-Crops"><a href="#Mobile-Friendly-Deep-Learning-for-Plant-Disease-Detection-A-Lightweight-CNN-Benchmark-Across-101-Classes-of-33-Crops" class="headerlink" title="Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight   CNN Benchmark Across 101 Classes of 33 Crops"></a>Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight   CNN Benchmark Across 101 Classes of 33 Crops</h2><p><strong>Authors:Anand Kumar, Harminder Pal Monga, Tapasi Brahma, Satyam Kalra, Navas Sherif</strong></p>
<p>Plant diseases are a major threat to food security globally. It is important to develop early detection systems which can accurately detect. The advancement in computer vision techniques has the potential to solve this challenge. We have developed a mobile-friendly solution which can accurately classify 101 plant diseases across 33 crops. We built a comprehensive dataset by combining different datasets, Plant Doc, PlantVillage, and PlantWild, all of which are for the same purpose. We evaluated performance across several lightweight architectures - MobileNetV2, MobileNetV3, MobileNetV3-Large, and EfficientNet-B0, B1 - specifically chosen for their efficiency on resource-constrained devices. The results were promising, with EfficientNet-B1 delivering our best performance at 94.7% classification accuracy. This architecture struck an optimal balance between accuracy and computational efficiency, making it well-suited for real-world deployment on mobile devices. </p>
<blockquote>
<p>æ¤ç‰©ç–¾ç—…å¯¹å…¨çƒé£Ÿå“å®‰å…¨æ„æˆé‡å¤§å¨èƒã€‚å› æ­¤ï¼Œå¼€å‘èƒ½å¤Ÿå‡†ç¡®æ£€æµ‹çš„æ—©æœŸæ£€æµ‹ç³»ç»Ÿéå¸¸é‡è¦ã€‚è®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„è¿›æ­¥ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜æä¾›äº†æ½œåŠ›ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§é€‚ç”¨äºç§»åŠ¨è®¾å¤‡çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥å‡†ç¡®åœ°å¯¹33ç§ä½œç‰©ä¸­çš„101ç§æ¤ç‰©ç–¾ç—…è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬é€šè¿‡åˆå¹¶ä¸åŒçš„æ•°æ®é›†å»ºç«‹äº†ç»¼åˆæ•°æ®é›†ï¼ŒåŒ…æ‹¬ç”¨äºåŒä¸€ç›®çš„çš„Plant Docã€PlantVillageå’ŒPlantWildã€‚æˆ‘ä»¬åœ¨å‡ ä¸ªè½»é‡çº§æ¶æ„ä¸Šè¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ï¼ŒåŒ…æ‹¬ä¸“é—¨ä¸ºäº†åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šæé«˜æ•ˆç‡çš„MobileNetV2ã€MobileNetV3ã€MobileNetV3-Largeå’ŒEfficientNet-B0ã€B1ã€‚ç»“æœä»¤äººé¼“èˆï¼ŒEfficientNet-B1çš„è¡¨ç°æœ€ä½³ï¼Œåˆ†ç±»å‡†ç¡®ç‡è¾¾åˆ°äº†94.7%ã€‚è¯¥æ¶æ„åœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼Œéå¸¸é€‚åˆåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°çœŸå®ä¸–ç•Œéƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10817v1">PDF</a> 15 pages, 5 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æè¿°äº†ä¸€ä¸ªç§»åŠ¨å‹å¥½çš„æ¤ç‰©ç–¾ç—…æ—©æœŸæ£€æµ‹ç³»ç»Ÿï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å‡ºè·¨33ç§ä½œç‰©çš„101ç§æ¤ç‰©ç–¾ç—…ã€‚é€šè¿‡æ•´åˆå¤šä¸ªæ•°æ®é›†å¹¶è¯„ä¼°ä¸åŒè½»é‡çº§æ¶æ„çš„æ€§èƒ½ï¼Œå‘ç°EfficientNet-B1æ¶æ„åœ¨èµ„æºå—é™è®¾å¤‡ä¸Šå®ç°äº†94.7%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œä¸”åœ¨å‡†ç¡®æ€§ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼Œé€‚åˆåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¤ç‰©ç–¾ç—…å¯¹å…¨çƒç²®é£Ÿå®‰å…¨æ„æˆé‡å¤§å¨èƒï¼Œéœ€è¦å¼€å‘æ—©æœŸæ£€æµ‹ç³»ç»Ÿå‡†ç¡®è¯†åˆ«ã€‚</li>
<li>è®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„è¿›æ­¥ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜æä¾›äº†æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ä¸ªç§»åŠ¨å‹å¥½çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å‡†ç¡®è¯†åˆ«101ç§æ¤ç‰©ç–¾ç—…ï¼Œè¦†ç›–33ç§ä½œç‰©ã€‚</li>
<li>é€šè¿‡æ•´åˆPlant Docã€PlantVillageå’ŒPlantWildç­‰å¤šä¸ªæ•°æ®é›†ï¼Œå»ºç«‹äº†ç»¼åˆæ•°æ®é›†ã€‚</li>
<li>è¯„ä¼°äº†å¤šä¸ªè½»é‡çº§æ¶æ„çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬MobileNetV2ã€MobileNetV3ã€MobileNetV3-Largeå’ŒEfficientNet-B0ã€B1ã€‚</li>
<li>EfficientNet-B1æ¶æ„åœ¨å‡†ç¡®æ€§ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ï¼Œå®ç°äº†94.7%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87de3c045adf66a38bffb23bc356be76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8aa8f5be92253b979aa615dc0df280ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36f33eabbb5ca15369c235073b5b6ea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbe06226c116ea8d75dcbc7ccec136ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83f8a322b9efab75383e411a597fb868.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VasoMIM-Vascular-Anatomy-Aware-Masked-Image-Modeling-for-Vessel-Segmentation"><a href="#VasoMIM-Vascular-Anatomy-Aware-Masked-Image-Modeling-for-Vessel-Segmentation" class="headerlink" title="VasoMIM: Vascular Anatomy-Aware Masked Image Modeling for Vessel   Segmentation"></a>VasoMIM: Vascular Anatomy-Aware Masked Image Modeling for Vessel   Segmentation</h2><p><strong>Authors:De-Xing Huang, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Tian-Yu Xiang, Rui-Ze Ma, Nu-Fang Xiao, Zeng-Guang Hou</strong></p>
<p>Accurate vessel segmentation in X-ray angiograms is crucial for numerous clinical applications. However, the scarcity of annotated data presents a significant challenge, which has driven the adoption of self-supervised learning (SSL) methods such as masked image modeling (MIM) to leverage large-scale unlabeled data for learning transferable representations. Unfortunately, conventional MIM often fails to capture vascular anatomy because of the severe class imbalance between vessel and background pixels, leading to weak vascular representations. To address this, we introduce Vascular anatomy-aware Masked Image Modeling (VasoMIM), a novel MIM framework tailored for X-ray angiograms that explicitly integrates anatomical knowledge into the pre-training process. Specifically, it comprises two complementary components: anatomy-guided masking strategy and anatomical consistency loss. The former preferentially masks vessel-containing patches to focus the model on reconstructing vessel-relevant regions. The latter enforces consistency in vascular semantics between the original and reconstructed images, thereby improving the discriminability of vascular representations. Empirically, VasoMIM achieves state-of-the-art performance across three datasets. These findings highlight its potential to facilitate X-ray angiogram analysis. </p>
<blockquote>
<p>åœ¨Xå…‰è¡€ç®¡é€ å½±ä¸­è¿›è¡Œç²¾ç¡®çš„è¡€ç®¡åˆ†å‰²å¯¹äºè®¸å¤šä¸´åºŠåº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™ä¿ƒä½¿é‡‡ç”¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ–¹æ³•ï¼Œå¦‚æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰ï¼Œä»¥åˆ©ç”¨å¤§è§„æ¨¡çš„æ— æ ‡ç­¾æ•°æ®è¿›è¡Œå¯è¿ç§»è¡¨ç¤ºå­¦ä¹ ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„MIMç”±äºè¡€ç®¡å’ŒèƒŒæ™¯åƒç´ ä¹‹é—´çš„ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡ï¼Œå¸¸å¸¸æ— æ³•æ•æ‰åˆ°è¡€ç®¡ç»“æ„ï¼Œå¯¼è‡´è¡€ç®¡è¡¨ç¤ºè¾ƒå¼±ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¡€ç®¡ç»“æ„æ„ŸçŸ¥æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆVasoMIMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹Xå…‰è¡€ç®¡é€ å½±çš„æ–°å‹MIMæ¡†æ¶ï¼Œå®ƒå°†è§£å‰–çŸ¥è¯†æ˜ç¡®èå…¥é¢„è®­ç»ƒè¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåŒ…å«ä¸¤ä¸ªäº’è¡¥çš„ç»„ä»¶ï¼šè§£å‰–å¼•å¯¼æ©è†œç­–ç•¥å’Œè§£å‰–ä¸€è‡´æ€§æŸå¤±ã€‚å‰è€…ä¼˜å…ˆæ©è†œå«è¡€ç®¡çš„æ–‘å—ï¼Œä½¿æ¨¡å‹ä¸“æ³¨äºé‡å»ºä¸è¡€ç®¡ç›¸å…³çš„åŒºåŸŸã€‚åè€…å¼ºåˆ¶åŸå§‹å›¾åƒå’Œé‡å»ºå›¾åƒä¹‹é—´çš„è¡€ç®¡è¯­ä¹‰ä¸€è‡´æ€§ï¼Œä»è€Œæé«˜è¡€ç®¡è¡¨ç¤ºçš„è¾¨åˆ«åŠ›ã€‚ç»éªŒä¸Šï¼ŒVasoMIMåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°çªå‡ºäº†å…¶åœ¨Xå…‰è¡€ç®¡é€ å½±åˆ†æä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10794v1">PDF</a> 14 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹Xå…‰è¡€ç®¡é€ å½±å›¾åƒçš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•â€”â€”è¡€ç®¡ç»“æ„æ„ŸçŸ¥çš„æ©è†œå›¾åƒå»ºæ¨¡ï¼ˆVasoMIMï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆè§£å‰–å­¦çŸ¥è¯†å’Œé¢„è®­ç»ƒè¿‡ç¨‹ï¼Œè§£å†³äº†ä¼ ç»Ÿæ©è†œå›¾åƒå»ºæ¨¡åœ¨è¡€ç®¡é€ å½±å›¾åƒä¸­éš¾ä»¥æ•æ‰è¡€ç®¡ç»“æ„çš„é—®é¢˜ã€‚VasoMIMåŒ…æ‹¬ä¸¤ä¸ªäº’è¡¥çš„ç»„ä»¶ï¼šè§£å‰–å­¦å¼•å¯¼æ©è†œç­–ç•¥å’Œè§£å‰–å­¦ä¸€è‡´æ€§æŸå¤±ï¼Œæ—¨åœ¨æé«˜è¡€ç®¡è¡¨ç¤ºçš„åˆ¤åˆ«åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVasoMIMåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæœ‰æœ›ä¿ƒè¿›Xå…‰è¡€ç®¡é€ å½±å›¾åƒçš„åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Xå…‰è¡€ç®¡é€ å½±ä¸­çš„ç²¾ç¡®è¡€ç®¡åˆ†å‰²å¯¹ä¼—å¤šä¸´åºŠåº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§æ˜¯è¡€ç®¡åˆ†å‰²é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚æ©è†œå›¾åƒå»ºæ¨¡ï¼‰å·²ç”¨äºåˆ©ç”¨å¤§è§„æ¨¡æ— æ ‡ç­¾æ•°æ®è¿›è¡Œå­¦ä¹ ã€‚</li>
<li>ä¼ ç»Ÿæ©è†œå›¾åƒå»ºæ¨¡åœ¨è¡€ç®¡é€ å½±å›¾åƒä¸­å› ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œéš¾ä»¥æ•æ‰è¡€ç®¡ç»“æ„ã€‚</li>
<li>æå‡ºçš„VasoMIMæ–¹æ³•é€šè¿‡ç»“åˆè§£å‰–å­¦çŸ¥è¯†ï¼Œè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå®ç°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>VasoMIMåŒ…æ‹¬è§£å‰–å­¦å¼•å¯¼æ©è†œç­–ç•¥å’Œè§£å‰–å­¦ä¸€è‡´æ€§æŸå¤±ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-20651f053a0ce6434afebdeb8b939dcc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f514d6d6af1d17e79221d3a19007932.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e5b0215ab5f409c0e98ba532126e99f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac92aef219196d4e294511b7d1849997.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3cb5d64fd7547d3eacdc5d11b087d1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44e3afd502a8e1fd61e2acbf1f623a3b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Formation-and-protection-of-an-Eu-Ir-surface-compound-below-hexagonal-boron-nitride"><a href="#Formation-and-protection-of-an-Eu-Ir-surface-compound-below-hexagonal-boron-nitride" class="headerlink" title="Formation and protection of an Eu-Ir surface compound below hexagonal   boron nitride"></a>Formation and protection of an Eu-Ir surface compound below hexagonal   boron nitride</h2><p><strong>Authors:Alaa Mohammed Idris Bakhit, Khadiza Ali, Frederik Schiller</strong></p>
<p>Europium (Eu) intercalation below hexagonal boron nitride (hBN) on an Ir(111) substrate at various Eu coverages is investigated. The structural and electronic properties were examined using low energy electron diffraction (LEED), scanning tunnelling microscopy (STM), x-ray photoelectron spectroscopy (XPS) and angle-resolved photoemission spectroscopy (ARPES). Depending on the deposition temperature, different superstructures, (5 $\times$ $M$), (5 $\times$ 2), and ($ \sqrt{3}$ $\times$ $\sqrt{3})R30^{\circ}$ with respect to the Ir substrate were identified by LEED. The (5 $\times$ $M$) superstructure ($M$ $&gt;$ 2), at 0.1 monolayer (ML), preserved the hBN&#x2F;Ir Moir{&#39;e} pattern and exhibited a unidirectional ordering of Eu atoms. At higher coverage of 0.26 ML, a (5 $\times$ 2) superstructure emerged, where excess Eu atoms diffused into the bulk and were analyzed as Eu in a tri-valent state. At the highest preparation temperature with a one-third ML Eu, the formation of a ($\sqrt{3}$ $\times$ $\sqrt{3})R30^{\circ}$ superstructure indicates the presence of a EuIr$<em>{2}$ surface alloy beneath the hBN layer, with di-valent Eu atoms suggesting potential ferromagnetic properties. Air exposure was used to evaluate the protection of the hBN layer, and the results indicate that the EuIr$</em>{2}$ surface alloy was partially protected. However, the hBN layer remained intact by intercalation and air exposure, as confirmed by ARPES analysis. </p>
<blockquote>
<p>åœ¨é“±ï¼ˆIrï¼ˆ111ï¼‰ï¼‰è¡¬åº•ä¸Šï¼Œå¯¹å…­æ–¹æ°®åŒ–ç¡¼ï¼ˆhBNï¼‰ä¸‹æ–¹ä¸åŒé“•ï¼ˆEuï¼‰è¦†ç›–ç‡çš„é“•æ’å±‚è¿›è¡Œäº†ç ”ç©¶ã€‚åˆ©ç”¨ä½èƒ½ç”µå­è¡å°„ï¼ˆLEEDï¼‰ã€æ‰«æéš§é“æ˜¾å¾®é•œï¼ˆSTMï¼‰ã€Xå°„çº¿å…‰ç”µå­èƒ½è°±ï¼ˆXPSï¼‰å’Œè§’åˆ†è¾¨å…‰ç”µå­èƒ½è°±ï¼ˆARPESï¼‰å¯¹ç»“æ„å’Œç”µå­ç‰¹æ€§è¿›è¡Œäº†æ£€æŸ¥ã€‚æ ¹æ®ä¸åŒçš„æ²‰ç§¯æ¸©åº¦ï¼Œé€šè¿‡LEEDè¯†åˆ«å‡ºä¸é“±è¡¬åº•ç›¸å…³çš„ä¸åŒè¶…ç»“æ„ï¼Œå¦‚ï¼ˆ5å€Ã—Mï¼‰ã€ï¼ˆ5å€Ã—2ï¼‰å’Œï¼ˆ$\sqrt{3}$ Ã— $\sqrt{3}$ï¼‰R30Â°ç»“æ„ã€‚åœ¨0.1å•å±‚ï¼ˆMLï¼‰çš„ï¼ˆ5å€Ã—Mï¼‰è¶…ç»“æ„ï¼ˆM&gt;2ï¼‰ä¸­ï¼Œä¿ç•™äº†hBN &#x2F; Irçš„è«å°”å›¾æ¡ˆï¼Œå¹¶è¡¨ç°å‡ºé“•åŸå­çš„å•å‘æ’åºã€‚åœ¨è¾ƒé«˜çš„0.26 MLè¦†ç›–ä¸‹ï¼Œå‡ºç°ï¼ˆ5å€Ã—2ï¼‰è¶…ç»“æ„ï¼Œè¿‡é‡çš„é“•åŸå­æ‰©æ•£åˆ°åŸºä½“ä¸­ï¼Œè¢«åˆ†æä¸ºä¸‰ä»·é“•ã€‚åœ¨æœ€é«˜åˆ¶å¤‡æ¸©åº¦ä¸‹ï¼Œä¸‰åˆ†ä¹‹ä¸€MLé“•æ—¶å½¢æˆï¼ˆ$\sqrt{3}$ Ã— $\sqrt{3}$ï¼‰R30Â°è¶…ç»“æ„ï¼Œè¡¨æ˜å­˜åœ¨é“•é“±ï¼ˆEuIrâ‚‚ï¼‰è¡¨é¢åˆé‡‘ï¼ŒäºŒä»·é“•åŸå­æš—ç¤ºäº†æ½œåœ¨çš„é“ç£æ€§è´¨ã€‚åˆ©ç”¨ç©ºæ°”æš´éœ²è¯„ä¼°äº†hBNå±‚çš„ä¿æŠ¤æ€§èƒ½ï¼Œç»“æœè¡¨æ˜EuIrâ‚‚è¡¨é¢åˆé‡‘å¾—åˆ°äº†éƒ¨åˆ†ä¿æŠ¤ã€‚ç„¶è€Œï¼ŒhBNå±‚é€šè¿‡æ’å±‚å’Œç©ºæ°”æš´éœ²ä»ç„¶ä¿æŒå®Œæ•´ï¼Œå¦‚ARPESåˆ†ææ‰€è¯å®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10746v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å…³äºEuropiumï¼ˆEuï¼‰åœ¨Irï¼ˆ111ï¼‰åŸºåº•ä¸Šå…­è¾¹å½¢æ°®åŒ–ç¡¼ï¼ˆhBNï¼‰ä¹‹é—´çš„æ’å±‚ç ”ç©¶ï¼Œåœ¨ä¸åŒè¦†ç›–ç‡çš„Euä¸‹è¿›è¡Œäº†è°ƒæŸ¥ã€‚é€šè¿‡ä½èƒ½ç”µå­è¡å°„ï¼ˆLEEDï¼‰ã€æ‰«æéš§é“æ˜¾å¾®é•œï¼ˆSTMï¼‰ã€Xå…‰ç”µå­èƒ½è°±ï¼ˆXPSï¼‰å’Œè§’åº¦åˆ†è¾¨å…‰ç”µå­è°±ï¼ˆARPESï¼‰å¯¹ç»“æ„å’Œç”µå­ç‰¹æ€§è¿›è¡Œäº†ç ”ç©¶ã€‚æ ¹æ®ä¸åŒçš„æ²‰ç§¯æ¸©åº¦ï¼Œå‘ç°äº†ä¸IråŸºåº•æœ‰å…³çš„ï¼ˆ5Ã— $M$ï¼‰ã€ï¼ˆ5Ã— 2ï¼‰å’Œï¼ˆ $\sqrt{3}$ Ã— $\sqrt{3}ï¼‰R30^{\circ}$çš„ä¸åŒè¶…ç»“æ„ã€‚åœ¨0.1å•å±‚ï¼ˆMLï¼‰çš„ï¼ˆ5Ã— $M$ï¼‰ï¼ˆ$M$ï¼2ï¼‰è¶…ç»“æ„ä¸­ï¼Œä¿ç•™äº†hBN &#x2F; Irçš„MoirÃ©å›¾æ¡ˆï¼Œå¹¶è¡¨ç°å‡ºEuåŸå­çš„å•å‘æ’åºã€‚åœ¨è¾ƒé«˜çš„0.26 MLè¦†ç›–ç‡ä¸‹ï¼Œå‡ºç°äº†ï¼ˆ5Ã— 2ï¼‰è¶…ç»“æ„ï¼Œè¿‡é‡çš„EuåŸå­æ‰©æ•£åˆ°ä¸»ä½“ä¸­ï¼Œä»¥ä¸‰ä»·çŠ¶æ€åˆ†æä¸ºEuã€‚åœ¨æœ€é«˜åˆ¶å¤‡æ¸©åº¦ä¸‹ï¼Œä¸‰åˆ†ä¹‹ä¸€MLçš„Euå½¢æˆï¼ˆ$\sqrt{3}$ Ã— $\sqrt{3}ï¼‰R30^{\circ}$è¶…ç»“æ„ï¼Œè¡¨æ˜å­˜åœ¨EuIr_{2}åˆé‡‘è¡¨é¢ï¼ŒäºŒä»·EuåŸå­æš—ç¤ºæ½œåœ¨çš„é“ç£æ€§ã€‚é€šè¿‡ç©ºæ°”æš´éœ²è¯„ä¼°hBNå±‚çš„ä¿æŠ¤æ€§èƒ½ï¼Œç»“æœè¡¨æ˜EuIr_{2}åˆé‡‘è¡¨é¢éƒ¨åˆ†å—ä¿æŠ¤ã€‚ç„¶è€Œï¼Œç”±äºæ’å±‚å’Œç©ºæ°”æš´éœ²ï¼ŒhBNå±‚ä¿æŒå®Œæ•´ï¼Œå¦‚ARPESåˆ†ææ‰€è¯å®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Europium (Eu) åœ¨ä¸åŒè¦†ç›–ç‡ä¸‹äº Ir(111) åŸºåº•ä¸Šçš„ hBN ä¹‹é—´çš„æ’å±‚è¡Œä¸ºè¢«ç ”ç©¶ã€‚</li>
<li>é€šè¿‡å¤šç§æŠ€æœ¯å¦‚ LEEDã€STMã€XPS å’Œ ARPES åˆ†æäº†ç»“æ„å’Œç”µå­ç‰¹æ€§ã€‚</li>
<li>æ²‰ç§¯æ¸©åº¦å½±å“ Eu å½¢æˆçš„è¶…ç»“æ„ï¼Œå¦‚ (5Ã— $M$)ã€(5Ã— 2) å’Œ ï¼ˆ$\sqrt{3}$ Ã— $\sqrt{3})R30^{\circ}$ã€‚</li>
<li>åœ¨è¾ƒä½è¦†ç›–ç‡ä¸‹ï¼ŒEu åŸå­æœ‰åºæ’åˆ—ï¼›åœ¨è¾ƒé«˜è¦†ç›–ç‡ä¸‹ï¼Œå½¢æˆ EuIr_{2} è¡¨é¢åˆé‡‘ã€‚</li>
<li>EuIr_{2} åˆé‡‘å¯èƒ½å…·æœ‰é“ç£æ€§æ€§è´¨ã€‚</li>
<li>hBN å±‚åœ¨æ’å±‚å’Œç©ºæ°”æš´éœ²åä¿æŒå®Œæ•´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-730d9cab100087d550a1f2cea984093e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5261a45acc5bb3ddfc1e05f58fa9166f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3956628eeb159decf0a03b6e79085869.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a57cc6cb976b31920a7b337d8cbefc63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-139cf265c673a27c1e02b263f5d0523f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Physics-Informed-Joint-Multi-TE-Super-Resolution-with-Implicit-Neural-Representation-for-Robust-Fetal-T2-Mapping"><a href="#Physics-Informed-Joint-Multi-TE-Super-Resolution-with-Implicit-Neural-Representation-for-Robust-Fetal-T2-Mapping" class="headerlink" title="Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural   Representation for Robust Fetal T2 Mapping"></a>Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural   Representation for Robust Fetal T2 Mapping</h2><p><strong>Authors:Busra Bulut, Maik Dannecker, Thomas Sanchez, Sara Neves Silva, Vladyslav Zalevskyi, Steven Jia, Jean-Baptiste Ledoux, Guillaume Auzias, FranÃ§ois Rousseau, Jana Hutter, Daniel Rueckert, Meritxell Bach Cuadra</strong></p>
<p>T2 mapping in fetal brain MRI has the potential to improve characterization of the developing brain, especially at mid-field (0.55T), where T2 decay is slower. However, this is challenging as fetal MRI acquisition relies on multiple motion-corrupted stacks of thick slices, requiring slice-to-volume reconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently, T2 mapping involves repeated acquisitions of these stacks at each echo time (TE), leading to long scan times and high sensitivity to motion. We tackle this challenge with a method that jointly reconstructs data across TEs, addressing severe motion. Our approach combines implicit neural representations with a physics-informed regularization that models T2 decay, enabling information sharing across TEs while preserving anatomical and quantitative T2 fidelity. We demonstrate state-of-the-art performance on simulated fetal brain and in vivo adult datasets with fetal-like motion. We also present the first in vivo fetal T2 mapping results at 0.55T. Our study shows potential for reducing the number of stacks per TE in T2 mapping by leveraging anatomical redundancy. </p>
<blockquote>
<p>èƒå„¿è„‘éƒ¨MRIçš„T2æ˜ å°„å…·æœ‰æ”¹å–„å‘è‚²ä¸­å¤§è„‘ç‰¹å¾æè¿°çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­åœºï¼ˆ0.55Tï¼‰ï¼ŒT2è¡°å‡è¾ƒæ…¢ã€‚ç„¶è€Œï¼Œè¿™æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºèƒå„¿MRIé‡‡é›†ä¾èµ–äºå¤šä¸ªå—è¿åŠ¨å½±å“çš„å¤§åšåˆ‡ç‰‡å †å ï¼Œéœ€è¦è¿›è¡Œåˆ‡ç‰‡åˆ°ä½“ç§¯é‡å»ºï¼ˆSVRï¼‰æ¥ä¼°è®¡é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰3Dä½“ç§¯ã€‚ç›®å‰ï¼ŒT2æ˜ å°„æ¶‰åŠåœ¨æ¯ä¸ªå›æ³¢æ—¶é—´ï¼ˆTEï¼‰è¿›è¡Œè¿™äº›å †æ ˆçš„é‡å¤é‡‡é›†ï¼Œå¯¼è‡´æ‰«ææ—¶é—´é•¿ä¸”å¯¹è¿åŠ¨é«˜åº¦æ•æ„Ÿã€‚æˆ‘ä»¬é‡‡ç”¨ä¸€ç§æ–¹æ³•æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•è”åˆé‡å»ºè·¨TEçš„æ•°æ®ï¼Œå¹¶è§£å†³ä¸¥é‡çš„è¿åŠ¨é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†éšå¼ç¥ç»è¡¨å¾å’ŒåŸºäºç‰©ç†çš„æ­£è§„åŒ–ï¼Œå¯¹T2è¡°å‡è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°åœ¨TEä¹‹é—´å…±äº«ä¿¡æ¯çš„åŒæ—¶ä¿æŒè§£å‰–å’Œå®šé‡T2ä¿çœŸã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿçš„èƒå„¿å¤§è„‘å’Œå…·æœ‰èƒå„¿æ ·è¿åŠ¨çš„æ´»ä½“éªŒæ•°æ®é›†ä¸­å±•ç¤ºäº†æœ€æ–°æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†åœ¨0.55Tä¸‹é¦–æ¬¡å®ç°çš„åœ¨æ´»ä½“éªŒä¸­çš„èƒå„¿T2æ˜ å°„ç»“æœã€‚æˆ‘ä»¬çš„ç ”ç©¶æ˜¾ç¤ºäº†é€šè¿‡åˆ©ç”¨è§£å‰–å†—ä½™æ€§å‡å°‘æ¯ä¸ªTEçš„å †æ ˆæ•°é‡åœ¨T2æ˜ å°„ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10680v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>T2æ˜ å°„åœ¨èƒå„¿è„‘éƒ¨MRIä¸­æœ‰æ”¹å–„å¯¹å‘è‚²ä¸­çš„å¤§è„‘ç‰¹å¾è¡¨å¾çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­åœºï¼ˆ0.55Tï¼‰ä¸­ï¼ŒT2è¡°å‡è¾ƒæ…¢æ„æˆæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„èƒå„¿MRIé‡‡é›†ä¾èµ–äºå¤šæ¬¡å—è¿åŠ¨å¹²æ‰°çš„åšåˆ‡ç‰‡å †å ï¼Œéœ€è¦è¿›è¡Œåˆ‡ç‰‡åˆ°ä½“ç§¯é‡å»ºï¼ˆSVRï¼‰ä»¥ä¼°ç®—é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰çš„3Dä½“ç§¯ã€‚å½“å‰T2æ˜ å°„æ¶‰åŠåœ¨æ¯ä¸ªå›æ³¢æ—¶é—´ï¼ˆTEï¼‰è¿›è¡Œè¿™äº›å †å çš„é‡å¤é‡‡é›†ï¼Œå¯¼è‡´æ‰«ææ—¶é—´é•¿ä¸”å¯¹è¿åŠ¨æ•æ„Ÿã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸€ç§è”åˆé‡å»ºè·¨è¶ŠTEæ•°æ®çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†éšå¼ç¥ç»è¡¨å¾å’Œæ¨¡æ‹ŸT2è¡°å‡çš„ç‰©ç†è§„å¾‹çš„æ­£åˆ™åŒ–ï¼Œå®ç°äº†ä¿¡æ¯è·¨TEå…±äº«çš„åŒæ—¶ä¿æŒè§£å‰–ç»“æ„å’ŒT2å€¼çš„å‡†ç¡®æ€§ã€‚åœ¨æ¨¡æ‹Ÿèƒå„¿å¤§è„‘å’Œå…·æœ‰èƒå„¿æ ·è¿åŠ¨çš„ä½“å†…æˆäººæ•°æ®é›†ä¸Šå–å¾—äº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶é¦–æ¬¡å±•ç¤ºäº†åœ¨0.55Tä¸‹çš„ä½“å†…èƒå„¿T2æ˜ å°„ç»“æœã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†é€šè¿‡åˆ©ç”¨è§£å‰–å†—ä½™æ€§å‡å°‘æ¯ä¸ªTEçš„å †å æ•°é‡åœ¨T2æ˜ å°„ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2æ˜ å°„åœ¨èƒå„¿è„‘éƒ¨MRIä¸­æœ‰é‡è¦åº”ç”¨ï¼Œå°¤å…¶åœ¨0.55Tä¸­åœºç¯å¢ƒä¸‹ï¼ŒT2è¡°å‡è¾ƒæ…¢æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿèƒå„¿MRIé‡‡é›†æ–¹æ³•ä¾èµ–äºå¤šæ¬¡å—è¿åŠ¨å¹²æ‰°çš„åšåˆ‡ç‰‡å †å ï¼Œéœ€é€šè¿‡åˆ‡ç‰‡åˆ°ä½“ç§¯é‡å»ºï¼ˆSVRï¼‰è·å–é«˜åˆ†è¾¨ç‡3Dä½“ç§¯ã€‚</li>
<li>å½“å‰T2æ˜ å°„æ–¹æ³•å­˜åœ¨æ‰«ææ—¶é—´é•¿ã€å¯¹è¿åŠ¨æ•æ„Ÿçš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶æå‡ºä¸€ç§è”åˆé‡å»ºè·¨è¶ŠTEæ•°æ®çš„æ–°æ–¹æ³•ï¼Œç»“åˆäº†éšå¼ç¥ç»è¡¨å¾ä¸æ¨¡æ‹ŸT2è¡°å‡çš„ç‰©ç†è§„å¾‹æ­£åˆ™åŒ–ã€‚</li>
<li>æ–°æ–¹æ³•èƒ½å¤Ÿå®ç°ä¿¡æ¯è·¨TEå…±äº«å¹¶ä¿æŒè§£å‰–ç»“æ„å’ŒT2å€¼çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå’Œä½“å†…æ•°æ®é›†ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ï¼Œå±•ç¤ºäº†å‡å°‘æ¯ä¸ªTEçš„å †å æ•°é‡åœ¨T2æ˜ å°„ä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4addd3ccc8d5e00229d1643f3d5b00a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c517a710881e1d2fe8b061dfcb70d8ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f230d6afaa9e1203467738c17413eff2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fba18f01ee4ee44089bcd509a93df3e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PASS-Probabilistic-Agentic-Supernet-Sampling-for-Interpretable-and-Adaptive-Chest-X-Ray-Reasoning"><a href="#PASS-Probabilistic-Agentic-Supernet-Sampling-for-Interpretable-and-Adaptive-Chest-X-Ray-Reasoning" class="headerlink" title="PASS: Probabilistic Agentic Supernet Sampling for Interpretable and   Adaptive Chest X-Ray Reasoning"></a>PASS: Probabilistic Agentic Supernet Sampling for Interpretable and   Adaptive Chest X-Ray Reasoning</h2><p><strong>Authors:Yushi Feng, Junye Du, Yingying Hong, Qifan Wang, Lequan Yu</strong></p>
<p>Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems. </p>
<blockquote>
<p>ç°æœ‰å·¥å…·å¢å¼ºå‹æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼šï¼ˆä¸€ï¼‰é»‘ç®±æ¨ç†æ­¥éª¤æŸå®³äº†å†³ç­–çš„å¯ä¿¡åº¦å¹¶å¸¦æ¥å®‰å…¨é£é™©ï¼›ï¼ˆäºŒï¼‰å¯¹åŒ»ç–—ä»»åŠ¡è‡³å…³é‡è¦çš„å¤šæ¨¡å¼èåˆèƒ½åŠ›è¾ƒå·®ï¼›ï¼ˆä¸‰ï¼‰æ™ºèƒ½ä½“ç®¡é“åˆšæ€§ä¸”è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬å¼•å…¥äº†PASSï¼ˆæ¦‚ç‡æ™ºèƒ½ä½“è¶…ç½‘é‡‡æ ·ï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè§£å†³èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æ¨ç†ä¸Šä¸‹æ–‡ä¸­è¿™äº›æŒ‘æˆ˜çš„å¤šæ¨¡å¼æ¡†æ¶ã€‚PASSè‡ªé€‚åº”åœ°åœ¨å¤šå·¥å…·å›¾ä¸Šé‡‡æ ·æ™ºèƒ½ä½“å·¥ä½œæµç¨‹ï¼Œäº§ç”Ÿå¸¦æœ‰å¯è§£é‡Šæ¦‚ç‡çš„å†³ç­–è·¯å¾„ã€‚é’ˆå¯¹å…·æœ‰å¤šæ¨¡å¼åŒ»ç–—æ•°æ®çš„å¤æ‚CXRæ¨ç†ä»»åŠ¡ï¼ŒPASSåˆ©ç”¨å…¶å­¦ä¹ åˆ°çš„ä»»åŠ¡æ¡ä»¶åˆ†å¸ƒçš„æ™ºèƒ½ä½“è¶…ç½‘ã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥åœ¨æ¯ä¸ªè¶…ç½‘å±‚è‡ªé€‚åº”åœ°é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ï¼Œä¸ºäº‹åå®¡è®¡æä¾›å¸¦æ¦‚ç‡æ³¨é‡Šçš„è½¨è¿¹ï¼Œå¹¶ç›´æ¥æé«˜åŒ»ç–—äººå·¥æ™ºèƒ½çš„å®‰å…¨æ€§ã€‚PASSè¿˜ä¸æ–­å°†é‡è¦å‘ç°å‹ç¼©æˆä¸æ–­å‘å±•çš„ä¸ªæ€§åŒ–è®°å¿†ï¼ŒåŒæ—¶åŠ¨æ€å†³å®šæ˜¯æ·±åŒ–å…¶æ¨ç†è·¯å¾„è¿˜æ˜¯æå‰é€€å‡ºä»¥æé«˜æ•ˆç‡ã€‚ä¸ºäº†ä¼˜åŒ–æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´çš„å¸•ç´¯æ‰˜å‰æ²¿ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹çš„ä¸‰é˜¶æ®µè®­ç»ƒç¨‹åºï¼ŒåŒ…æ‹¬ä¸“å®¶çŸ¥è¯†é¢„çƒ­ã€å¯¹æ¯”è·¯å¾„æ’åå’Œæˆæœ¬æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ã€‚ä¸ºäº†è¿›è¡Œä¸¥æ ¼è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†CAB-Eï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤šæ­¥éª¤ã€å®‰å…¨å…³é”®çš„è‡ªç”±å½¢å¼CXRæ¨ç†çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒéªŒè¯äº†PASSåœ¨å¤šä¸ªæŒ‡æ ‡ï¼ˆä¾‹å¦‚å‡†ç¡®æ€§ã€AUCã€LLM-Jï¼‰ä¸Šæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼ŒåŒæ—¶å¹³è¡¡äº†è®¡ç®—æˆæœ¬ï¼Œæœç€å¯è§£é‡Šã€è‡ªé€‚åº”å’Œå¤šæ¨¡å¼åŒ»ç–—æ™ºèƒ½ä½“ç³»ç»Ÿçš„æ–°èŒƒå¼è½¬å˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10501v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    PASSï¼ˆProbabilistic Agentic Supernet Samplingï¼‰æ˜¯é¦–ä¸ªé’ˆå¯¹èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰æ¨ç†çš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰å·¥å…·å¢å¼ºä»£ç†ç³»ç»Ÿæ‰€é¢ä¸´çš„ä¿¡ä»»ã€å®‰å…¨ã€å¤šæ¨¡å¼é›†æˆã€è®¡ç®—æ•ˆç‡ç­‰æŒ‘æˆ˜ã€‚PASSé€šè¿‡è‡ªé€‚åº”é‡‡æ ·ä»£ç†å·¥ä½œæµç¨‹ï¼Œç”Ÿæˆå¯è§£é‡Šçš„æ¦‚ç‡ä¸ºå†³ç­–è·¯å¾„æ³¨è§£ï¼Œåˆ©ç”¨ä»»åŠ¡æ¡ä»¶åˆ†å¸ƒä»å¤šå·¥å…·å›¾ä¸­é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ï¼Œä¸ºäº‹åå®¡è®¡æä¾›æ¦‚ç‡æ³¨è§£è½¨è¿¹ï¼Œæé«˜åŒ»ç–—äººå·¥æ™ºèƒ½çš„å®‰å…¨æ€§ã€‚PASSè¿˜é€šè¿‡ä¸æ–­å‹ç¼©é‡è¦å‘ç°åˆ°ä¸€ä¸ªä¸ªæ€§åŒ–è®°å¿†ç³»ç»Ÿæ¥å¹³è¡¡æ¨ç†æ•ˆç‡å’Œæ—©æœŸé€€å‡ºç­–ç•¥çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„ç¬¬ä¸‰é˜¶æ®µè®­ç»ƒè¿‡ç¨‹æ¥å®ç°æ€§èƒ½å’Œæˆæœ¬çš„å¹³è¡¡ï¼Œå¹¶é€šè¿‡CAB-EåŸºå‡†æµ‹è¯•éªŒè¯äº†PASSåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>PASSæ˜¯é¦–ä¸ªé’ˆå¯¹åŒ»å­¦å›¾åƒå¤šæ¨¡å¼å¤„ç†çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä»£ç†ç³»ç»Ÿé¢ä¸´çš„å®‰å…¨é£é™©å’Œå¤šæ¨¡å¼é›†æˆé—®é¢˜ã€‚</li>
<li>PASSå¼•å…¥æ¦‚ç‡è§£é‡Šæ€§æ ‡æ³¨ï¼Œé€šè¿‡è‡ªé€‚åº”é‡‡æ ·ä»£ç†å·¥ä½œæµç¨‹ä»¥å¢å¼ºåŒ»ç–—äººå·¥æ™ºèƒ½çš„é€æ˜åº¦å’Œå®‰å…¨æ€§ã€‚</li>
<li>é€šè¿‡åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·å’Œæ—©æœŸé€€å‡ºç­–ç•¥ï¼ŒPASSæé«˜äº†è®¡ç®—æ•ˆç‡å¹¶ä¼˜åŒ–äº†æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>PASSé‡‡ç”¨åˆ›æ–°çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä¸“å®¶çŸ¥è¯†é¢„çƒ­ã€å¯¹æ¯”è·¯å¾„æ’åå’Œæˆæœ¬æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼Œä»¥å®ç°æ€§èƒ½ä¼˜åŒ–ã€‚</li>
<li>CAB-EåŸºå‡†æµ‹è¯•éªŒè¯äº†PASSåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œè¡¨æ˜å…¶åœ¨åŒ»å­¦å›¾åƒæ¨ç†æ–¹é¢å…·æœ‰å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>PASSçš„æŒç»­å­¦ä¹ å’Œä¸ªæ€§åŒ–è®°å¿†ç³»ç»Ÿèƒ½å¤Ÿé€‚åº”å¤æ‚çš„åŒ»å­¦æ•°æ®ç¯å¢ƒå¹¶æå‡é•¿æœŸæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4457419d4262a7e70e362ceab8354d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7df19b92122ecd883f6c19a7903fae2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfbb865d8ba865ab90838d43b5b5da58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a415cd9a418fb96da493a845f3795ef5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc887793f16b3e70b8d00b8c06750a47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dfae07fd00786a3315bec146a6f7018.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-390f20e26797659d0712c552b21b4259.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="From-Images-to-Perception-Emergence-of-Perceptual-Properties-by-Reconstructing-Images"><a href="#From-Images-to-Perception-Emergence-of-Perceptual-Properties-by-Reconstructing-Images" class="headerlink" title="From Images to Perception: Emergence of Perceptual Properties by   Reconstructing Images"></a>From Images to Perception: Emergence of Perceptual Properties by   Reconstructing Images</h2><p><strong>Authors:Pablo HernÃ¡ndez-CÃ¡mara, Jesus Malo, Valero Laparra</strong></p>
<p>A number of scientists suggested that human visual perception may emerge from image statistics, shaping efficient neural representations in early vision. In this work, a bio-inspired architecture that can accommodate several known facts in the retina-V1 cortex, the PerceptNet, has been end-to-end optimized for different tasks related to image reconstruction: autoencoding, denoising, deblurring, and sparsity regularization. Our results show that the encoder stage (V1-like layer) consistently exhibits the highest correlation with human perceptual judgments on image distortion despite not using perceptual information in the initialization or training. This alignment exhibits an optimum for moderate noise, blur and sparsity. These findings suggest that the visual system may be tuned to remove those particular levels of distortion with that level of sparsity and that biologically inspired models can learn perceptual metrics without human supervision. </p>
<blockquote>
<p>è®¸å¤šç§‘å­¦å®¶æå‡ºï¼Œäººç±»è§†è§‰æ„ŸçŸ¥å¯èƒ½æºäºå›¾åƒç»Ÿè®¡ï¼Œåœ¨æ—©æœŸè§†è§‰ä¸­å½¢æˆæœ‰æ•ˆçš„ç¥ç»è¡¨å¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä¸€ç§èƒ½å¤Ÿå®¹çº³è§†ç½‘è†œ-V1çš®å±‚ä¸­å·²çŸ¥äº‹å®çš„ä»¿ç”Ÿæ¶æ„â€”â€”PerceptNetï¼Œå·²ç»é’ˆå¯¹ä¸å›¾åƒé‡å»ºç›¸å…³çš„ä¸åŒä»»åŠ¡è¿›è¡Œäº†ç«¯åˆ°ç«¯çš„ä¼˜åŒ–ï¼šè‡ªåŠ¨ç¼–ç ã€å»å™ªã€å»æ¨¡ç³Šå’Œç¨€ç–æ­£åˆ™åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°½ç®¡åœ¨åˆå§‹åŒ–æˆ–è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰ä½¿ç”¨æ„ŸçŸ¥ä¿¡æ¯ï¼Œä½†ç¼–ç å™¨é˜¶æ®µï¼ˆç±»ä¼¼V1å±‚ï¼‰ä¸äººç±»å¯¹å›¾åƒå¤±çœŸçš„æ„ŸçŸ¥åˆ¤æ–­ä¹‹é—´çš„ç›¸å…³æ€§ä¸€ç›´æœ€é«˜ã€‚è¿™ç§å¯¹é½åœ¨ä¸­ç­‰ç¨‹åº¦çš„å™ªå£°ã€æ¨¡ç³Šå’Œç¨€ç–æ€§æ–¹é¢è¡¨ç°å‡ºæœ€ä½³æ•ˆæœã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œè§†è§‰ç³»ç»Ÿå¯èƒ½è¢«è°ƒæ•´ä¸ºæ¶ˆé™¤ç‰¹å®šç¨‹åº¦çš„å¤±çœŸä¸ç¨€ç–æ€§ç›¸å¯¹åº”çš„ç¨‹åº¦ï¼Œå¹¶ä¸”ç”Ÿç‰©å¯å‘æ¨¡å‹å¯ä»¥åœ¨æ— éœ€äººç±»ç›‘ç£çš„æƒ…å†µä¸‹å­¦ä¹ æ„ŸçŸ¥åº¦é‡æŒ‡æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10450v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç”Ÿç‰©å¯å‘æ¶æ„PerceptNetï¼Œèƒ½èåˆè§†ç½‘è†œè‡³V1çš®å±‚çš„å¤šé¡¹å·²çŸ¥äº‹å®ï¼Œé’ˆå¯¹å›¾åƒé‡å»ºçš„ä¸åŒä»»åŠ¡è¿›è¡Œç«¯åˆ°ç«¯çš„ä¼˜åŒ–ï¼ŒåŒ…æ‹¬è‡ªåŠ¨ç¼–ç ã€å»å™ªã€å»æ¨¡ç³Šå’Œç¨€ç–æ­£åˆ™åŒ–ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå³ä¾¿åœ¨åˆå§‹åŒ–å’Œè®­ç»ƒè¿‡ç¨‹ä¸­æœªä½¿ç”¨æ„ŸçŸ¥ä¿¡æ¯ï¼Œç¼–ç å™¨é˜¶æ®µï¼ˆç±»ä¼¼V1å±‚ï¼‰ä¸äººç±»å¯¹å›¾åƒå¤±çœŸçš„æ„ŸçŸ¥åˆ¤æ–­ä»å…·æœ‰æœ€é«˜ç›¸å…³æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­ç­‰å™ªå£°ã€æ¨¡ç³Šå’Œç¨€ç–æ€§ä¼˜åŒ–æ–¹é¢ã€‚è¿™æš—ç¤ºè§†è§‰ç³»ç»Ÿå¯èƒ½ç»è¿‡è°ƒæ•´ä»¥å»é™¤ç‰¹å®šç¨‹åº¦çš„å¤±çœŸå’Œç›¸åº”ç¨‹åº¦çš„ç¨€ç–æ€§ï¼ŒåŒæ—¶ç”Ÿç‰©å¯å‘æ¨¡å‹å¯åœ¨æ— éœ€äººç±»ç›‘ç£çš„æƒ…å†µä¸‹å­¦ä¹ æ„ŸçŸ¥åº¦é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»è§†è§‰æ„ŸçŸ¥å¯èƒ½æºäºå›¾åƒç»Ÿè®¡ï¼Œå½¢æˆæ—©æœŸè§†è§‰ä¸­çš„é«˜æ•ˆç¥ç»è¡¨å¾ã€‚</li>
<li>PerceptNetæ¶æ„èåˆè§†ç½‘è†œè‡³V1çš®å±‚çš„å¤šä¸ªå·²çŸ¥äº‹å®ã€‚</li>
<li>PerceptNetç»è¿‡ç«¯åˆ°ç«¯çš„ä¼˜åŒ–ï¼Œæ¶‰åŠå›¾åƒé‡å»ºçš„å¤šç§ä»»åŠ¡ã€‚</li>
<li>ç¼–ç å™¨é˜¶æ®µä¸äººç±»çš„æ„ŸçŸ¥åˆ¤æ–­é«˜åº¦ç›¸å…³ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒå¤±çœŸæ–¹é¢ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œè§†è§‰ç³»ç»Ÿå¯èƒ½ç»è¿‡è°ƒæ•´ä»¥å¤„ç†ç‰¹å®šç¨‹åº¦çš„å¤±çœŸå’Œç¨€ç–æ€§ã€‚</li>
<li>ç”Ÿç‰©å¯å‘æ¨¡å‹å¯åœ¨æ— éœ€äººç±»ç›‘ç£çš„æƒ…å†µä¸‹å­¦ä¹ æ„ŸçŸ¥åº¦é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7b1f27277a3c0fadae7b10ce7bad67e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be091c6e9fa207e6d4d4d4db579774a2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DINOMotion-advanced-robust-tissue-motion-tracking-with-DINOv2-in-2D-Cine-MRI-guided-radiotherapy"><a href="#DINOMotion-advanced-robust-tissue-motion-tracking-with-DINOv2-in-2D-Cine-MRI-guided-radiotherapy" class="headerlink" title="DINOMotion: advanced robust tissue motion tracking with DINOv2 in   2D-Cine MRI-guided radiotherapy"></a>DINOMotion: advanced robust tissue motion tracking with DINOv2 in   2D-Cine MRI-guided radiotherapy</h2><p><strong>Authors:Soorena Salari, Catherine Spino, Laurie-Anne Pharand, Fabienne Lathuiliere, Hassan Rivaz, Silvain Beriault, Yiming Xiao</strong></p>
<p>Accurate tissue motion tracking is critical to ensure treatment outcome and safety in 2D-Cine MRI-guided radiotherapy. This is typically achieved by registration of sequential images, but existing methods often face challenges with large misalignments and lack of interpretability. In this paper, we introduce DINOMotion, a novel deep learning framework based on DINOv2 with Low-Rank Adaptation (LoRA) layers for robust, efficient, and interpretable motion tracking. DINOMotion automatically detects corresponding landmarks to derive optimal image registration, enhancing interpretability by providing explicit visual correspondences between sequential images. The integration of LoRA layers reduces trainable parameters, improving training efficiency, while DINOv2â€™s powerful feature representations offer robustness against large misalignments. Unlike iterative optimization-based methods, DINOMotion directly computes image registration at test time. Our experiments on volunteer and patient datasets demonstrate its effectiveness in estimating both linear and nonlinear transformations, achieving Dice scores of 92.07% for the kidney, 90.90% for the liver, and 95.23% for the lung, with corresponding Hausdorff distances of 5.47 mm, 8.31 mm, and 6.72 mm, respectively. DINOMotion processes each scan in approximately 30ms and consistently outperforms state-of-the-art methods, particularly in handling large misalignments. These results highlight its potential as a robust and interpretable solution for real-time motion tracking in 2D-Cine MRI-guided radiotherapy. </p>
<blockquote>
<p>åœ¨äºŒç»´ç”µå½±MRIå¼•å¯¼çš„æ”¾å°„æ²»ç–—è¿‡ç¨‹ä¸­ï¼Œç²¾ç¡®çš„ç»„ç»‡è¿åŠ¨è·Ÿè¸ªå¯¹äºç¡®ä¿æ²»ç–—æ•ˆæœå’Œå®‰å…¨è‡³å…³é‡è¦ã€‚è¿™é€šå¸¸é€šè¿‡è¿ç»­å›¾åƒçš„æ³¨å†Œæ¥å®ç°ï¼Œä½†ç°æœ‰æ–¹æ³•ç»å¸¸é¢ä¸´å¤§é”™ä½å’Œç¼ºä¹è§£é‡Šæ€§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†DINOMotionï¼Œè¿™æ˜¯ä¸€ç§åŸºäºDINOv2å’ŒLow-Rank Adaptationï¼ˆLoRAï¼‰å±‚çš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¯å®ç°ç¨³å¥ã€é«˜æ•ˆã€å¯è§£é‡Šçš„è¿åŠ¨è·Ÿè¸ªã€‚DINOMotionè‡ªåŠ¨æ£€æµ‹ç›¸åº”çš„åœ°æ ‡ä»¥å¾—å‡ºæœ€ä½³å›¾åƒæ³¨å†Œï¼Œé€šè¿‡æä¾›è¿ç»­å›¾åƒä¹‹é—´çš„æ˜ç¡®è§†è§‰å¯¹åº”å…³ç³»æ¥æé«˜è§£é‡Šæ€§ã€‚LoRAå±‚çš„é›†æˆå‡å°‘äº†å¯è®­ç»ƒå‚æ•°ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œè€ŒDINOv2çš„å¼ºå¤§ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›åˆ™æä¾›äº†å¯¹å¤§é”™ä½çš„ç¨³å¥æ€§ã€‚ä¸åŸºäºè¿­ä»£ä¼˜åŒ–çš„æ–¹æ³•ä¸åŒï¼ŒDINOMotionåœ¨æµ‹è¯•æ—¶ç›´æ¥è®¡ç®—å›¾åƒæ³¨å†Œã€‚æˆ‘ä»¬åœ¨å¿—æ„¿è€…å’Œæ‚£è€…æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨ä¼°è®¡çº¿æ€§å’Œéçº¿æ€§è½¬æ¢æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè‚¾è„ã€è‚è„å’Œè‚ºçš„Diceå¾—åˆ†åˆ†åˆ«ä¸º92.07%ã€90.90%å’Œ95.23%ï¼Œç›¸åº”çš„Hausdorffè·ç¦»åˆ†åˆ«ä¸º5.47æ¯«ç±³ã€8.31æ¯«ç±³å’Œ6.72æ¯«ç±³ã€‚DINOMotionå¤„ç†æ¯æ¬¡æ‰«æå¤§çº¦éœ€è¦30æ¯«ç§’ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§é”™ä½æ–¹é¢ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†å…¶åœ¨äºŒç»´ç”µå½±MRIå¼•å¯¼çš„æ”¾å°„æ²»ç–—ä¸­è¿›è¡Œå®æ—¶è¿åŠ¨è·Ÿè¸ªçš„å¼ºå¤§å’Œå¯è§£é‡Šçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10260v1">PDF</a> Accepted to IEEE Transactions on Biomedical Engineering (TMBE), 14   pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è¿åŠ¨è¿½è¸ªæ¡†æ¶DINOMotionï¼Œç”¨äºåœ¨äºŒç»´ç”µå½±MRIå¼•å¯¼ä¸‹å®ç°ç²¾ç¡®çš„ç»„ç»‡è¿åŠ¨è¿½è¸ªï¼Œä»è€Œæé«˜æ”¾å°„æ²»ç–—çš„ç–—æ•ˆå’Œå®‰å…¨æ€§ã€‚DINOMotioné‡‡ç”¨DINOv2ç®—æ³•ç»“åˆä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰å±‚ï¼Œå®ç°ç¨³å¥ã€é«˜æ•ˆã€å¯è§£é‡Šçš„è¿åŠ¨è¿½è¸ªã€‚è¯¥æ–¹æ³•å¯è‡ªåŠ¨æ£€æµ‹å¯¹åº”åœ°æ ‡ä»¥å®ç°æœ€ä½³å›¾åƒé…å‡†ï¼Œé€šè¿‡æä¾›åºåˆ—å›¾åƒä¹‹é—´çš„æ˜ç¡®è§†è§‰å¯¹åº”å…³ç³»å¢å¼ºå¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDINOMotionåœ¨å¤„ç†å¿—æ„¿è€…åŠæ‚£è€…æ•°æ®é›†æ—¶ï¼Œå¯¹çº¿æ€§åŠéçº¿æ€§è½¬æ¢çš„ä¼°è®¡å‡è¡¨ç°ä¼˜å¼‚ï¼Œè‚¾è„ã€è‚è„åŠè‚ºéƒ¨çš„Diceå¾—åˆ†åˆ†åˆ«ä¸º92.07%ã€90.90%åŠ95.23%ï¼Œå¯¹åº”çš„Hausdorffè·ç¦»åˆ†åˆ«ä¸º5.47æ¯«ç±³ã€8.31æ¯«ç±³åŠ6.72æ¯«ç±³ã€‚DINOMotionæ¯ä¸ªæ‰«æå¤„ç†æ—¶é—´çº¦ä¸º30æ¯«ç§’ï¼Œå¹¶èƒ½æŒç»­è¶…è¶Šç°æœ‰çš„é¡¶å°–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§é”™ä½æ–¹é¢è¡¨ç°å‡ºä¼—ã€‚å› æ­¤ï¼ŒDINOMotionå¯èƒ½æˆä¸ºäºŒç»´ç”µå½±MRIå¼•å¯¼æ”¾ç–—ä¸­å®ç°å®æ—¶è¿åŠ¨è¿½è¸ªçš„ç¨³å¥ã€å¯è§£æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DINOMotionæ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„è¿åŠ¨è¿½è¸ªæ¡†æ¶ï¼Œç”¨äºç¡®ä¿äºŒç»´ç”µå½±MRIå¼•å¯¼æ”¾å°„æ²»ç–—çš„ç–—æ•ˆå’Œå®‰å…¨æ€§ã€‚</li>
<li>DINOMotionç»“åˆDINOv2ç®—æ³•å’Œä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰å±‚ï¼Œå®ç°ç¨³å¥ã€é«˜æ•ˆã€å¯è§£é‡Šçš„è¿åŠ¨è¿½è¸ªã€‚</li>
<li>é€šè¿‡è‡ªåŠ¨æ£€æµ‹å¯¹åº”åœ°æ ‡è¿›è¡Œå›¾åƒé…å‡†ï¼Œå¢å¼ºå¯è§£é‡Šæ€§å¹¶æä¾›æ˜ç¡®çš„è§†è§‰å¯¹åº”å…³ç³»ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜DINOMotionåœ¨å¤„ç†å¤šç§æ•°æ®é›†æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§é”™ä½æ–¹é¢ã€‚</li>
<li>DINOMotionå¤„ç†æ¯ä¸ªæ‰«æçš„æ—¶é—´çŸ­ï¼Œçº¦30æ¯«ç§’ï¼Œå¯å®ç°å®æ—¶è¿åŠ¨è¿½è¸ªã€‚</li>
<li>DINOMotionæŒç»­è¶…è¶Šç°æœ‰é¡¶å°–æ–¹æ³•ï¼Œå…·æœ‰æˆä¸ºè¡Œä¸šæ ‡å‡†çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d5e9eb2a3dad25c38a2c48b5e6071797.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb8ac3fe65fe6dfc529899fb4df605d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d73ef1bbb7cf5ced177bd0894766a78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1876658346c98d81b86f447d8da099db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d11007de89ac036148809ee52e255db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11671d479c52709f1fec4a951b52c7f9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Explainable-AI-Technique-in-Lung-Cancer-Detection-Using-Convolutional-Neural-Networks"><a href="#Explainable-AI-Technique-in-Lung-Cancer-Detection-Using-Convolutional-Neural-Networks" class="headerlink" title="Explainable AI Technique in Lung Cancer Detection Using Convolutional   Neural Networks"></a>Explainable AI Technique in Lung Cancer Detection Using Convolutional   Neural Networks</h2><p><strong>Authors:Nishan Rai, Sujan Khatri, Devendra Risal</strong></p>
<p>Early detection of lung cancer is critical to improving survival outcomes. We present a deep learning framework for automated lung cancer screening from chest computed tomography (CT) images with integrated explainability. Using the IQ-OTH&#x2F;NCCD dataset (1,197 scans across Normal, Benign, and Malignant classes), we evaluate a custom convolutional neural network (CNN) and three fine-tuned transfer learning backbones: DenseNet121, ResNet152, and VGG19. Models are trained with cost-sensitive learning to mitigate class imbalance and evaluated via accuracy, precision, recall, F1-score, and ROC-AUC. While ResNet152 achieved the highest accuracy (97.3%), DenseNet121 provided the best overall balance in precision, recall, and F1 (up to 92%, 90%, 91%, respectively). We further apply Shapley Additive Explanations (SHAP) to visualize evidence contributing to predictions, improving clinical transparency. Results indicate that CNN-based approaches augmented with explainability can provide fast, accurate, and interpretable support for lung cancer screening, particularly in resource-limited settings. </p>
<blockquote>
<p>æ—©æœŸå‘ç°è‚ºç™Œå¯¹äºæ”¹å–„ç”Ÿå­˜ç»“æœè‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè‡ªåŠ¨è‚ºç™Œç­›æŸ¥çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºèƒ¸éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒï¼Œå¹¶é›†æˆäº†å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨IQ-OTH&#x2F;NCCDæ•°æ®é›†ï¼ˆåŒ…å«æ­£å¸¸ã€è‰¯æ€§ã€æ¶æ€§ä¸‰ç±»ï¼Œå…±1197å¼ æ‰«æå›¾åƒï¼‰æ¥è¯„ä¼°ä¸€ä¸ªè‡ªå®šä¹‰çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œä¸‰ä¸ªå¾®è°ƒè¿‡çš„è¿ç§»å­¦ä¹ ä¸»å¹²ç½‘ï¼šDenseNet121ã€ResNet152å’ŒVGG19ã€‚æ¨¡å‹é‡‡ç”¨æˆæœ¬æ•æ„Ÿå­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä»¥ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶é€šè¿‡å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’ŒROC-AUCè¿›è¡Œè¯„ä¼°ã€‚è™½ç„¶ResNet152çš„å‡†ç¡®ç‡æœ€é«˜ï¼ˆ97.3%ï¼‰ï¼Œä½†DenseNet121åœ¨ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1æ–¹é¢æä¾›äº†æœ€ä½³çš„æ•´ä½“å¹³è¡¡ï¼ˆåˆ†åˆ«é«˜è¾¾92%ã€90%ã€91%ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åº”ç”¨Shapley Additive Explanationsï¼ˆSHAPï¼‰æ¥å¯è§†åŒ–é¢„æµ‹çš„è¯æ®ï¼Œæé«˜ä¸´åºŠé€æ˜åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œå…·æœ‰å¯è§£é‡Šæ€§çš„CNNæ–¹æ³•å¯ä»¥ä¸ºè‚ºç™Œç­›æŸ¥æä¾›å¿«é€Ÿã€å‡†ç¡®å’Œå¯è§£é‡Šçš„æ”¯æŒï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10196v1">PDF</a> 11 pages, 9 figures, 4 tables. Undergraduate research project report</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆå¯è§£é‡Šæ€§ï¼Œå®ç°ä»èƒ¸éƒ¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒä¸­è‡ªåŠ¨è¿›è¡Œè‚ºç™Œç­›æŸ¥ã€‚ç ”ç©¶ä½¿ç”¨IQ-OTH&#x2F;NCCDæ•°æ®é›†ï¼Œè¯„ä¼°è‡ªå®šä¹‰å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œä¸‰ç§å¾®è°ƒåçš„è¿ç§»å­¦ä¹ ä¸»å¹²ç½‘ç»œï¼ˆDenseNet121ã€ResNet152å’ŒVGG19ï¼‰çš„æ€§èƒ½ã€‚é€šè¿‡æˆæœ¬æ•æ„Ÿå­¦ä¹ å‡è½»ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶é€šè¿‡å‡†ç¡®åº¦ã€ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’ŒROC-AUCè¿›è¡Œè¯„ä»·ã€‚ResNet152å‡†ç¡®ç‡æœ€é«˜ï¼ˆ97.3%ï¼‰ï¼Œè€ŒDenseNet121åœ¨ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢æä¾›æœ€ä½³ç»¼åˆå¹³è¡¡ã€‚ç ”ç©¶è¿˜åº”ç”¨Shapley Additive Explanationsï¼ˆSHAPï¼‰æ¥å¯è§†åŒ–é¢„æµ‹çš„è´¡çŒ®è¯æ®ï¼Œæé«˜ä¸´åºŠé€æ˜åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œç»“åˆè§£é‡Šæ€§çš„CNNæ–¹æ³•å¯ä¸ºè‚ºç™Œç­›æŸ¥æä¾›å¿«é€Ÿã€å‡†ç¡®å’Œå¯è§£é‡Šçš„æ”¯æŒï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶è¿›è¡Œè‚ºç™Œç­›æŸ¥ï¼Œä»èƒ¸éƒ¨CTå›¾åƒä¸­è‡ªåŠ¨æ£€æµ‹ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å¤šç§ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰CNNå’Œä¸‰ç§è¿ç§»å­¦ä¹ ä¸»å¹²ç½‘ç»œã€‚</li>
<li>é€šè¿‡æˆæœ¬æ•æ„Ÿå­¦ä¹ æ¥å‡è½»ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>ResNet152æ¨¡å‹åœ¨å‡†ç¡®ç‡æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>DenseNet121åœ¨ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢æä¾›æœ€ä½³ç»¼åˆè¡¨ç°ã€‚</li>
<li>ç ”ç©¶åº”ç”¨SHAPæ–¹æ³•å¯è§†åŒ–é¢„æµ‹çš„è´¡çŒ®è¯æ®ï¼Œæé«˜ä¸´åºŠé€æ˜åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7bd0bedd1a1efda2a245275c96468630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d3625d0a7b659759fecee4d0b09870d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53136b1b8467363b872e7211dc780251.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-409025a5b77b6e0cd179f75508e39a11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e6f3559ce684e34cf643f409b6a3a63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f733a4e8e12f47e28ae0bbbc8e3840f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22a35bfeb4a69f037301196baf552bce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9869a15bfc778d7c1a59da492360c802.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90621daa481449641af1e550d6a35159.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Optimizing-Convolution-Direction-and-Template-Selection-for-Difference-Image-Analysis"><a href="#Optimizing-Convolution-Direction-and-Template-Selection-for-Difference-Image-Analysis" class="headerlink" title="Optimizing Convolution Direction and Template Selection for Difference   Image Analysis"></a>Optimizing Convolution Direction and Template Selection for Difference   Image Analysis</h2><p><strong>Authors:Rodrigo Angulo, Armin Rest, William P. Blair, Jacob Jencson, David A. Coulter, Qinan Wang, Ryan J. Foley, Charles D. Kilpatrick, Xiaolong Li, Anthony L. Piro</strong></p>
<p>Difference image analysis (DIA) is a powerful tool for studying time-variable phenomena, and has been used by many time-domain surveys. Most DIA algorithms involve matching the spatially-varying PSF shape between science and template images, and then convolving that shape in one image to match the other. The wrong choice of which image to convolve can introduce one of the largest sources of artifacts in the final difference image. We introduce a quantitative metric to determine the optimal convolution direction that depends not only on the sharpness of the images measured by their FWHM, but also on their exposure depths. With this metric, the optimal convolution direction can be determined a priori, depending only on the FWHM and depth of the images. This not only simplifies the process, but also makes it more robust and less prone to creating sub-optimal difference images due to the wrong choice of the convolution direction. As an additional benefit, for a large set of images, we define a Figure-of-Merit based on this metric, which allows us to rank a list of images and determine the ones best suited to be used as templates, thus streamlining and automating the data reduction process. </p>
<blockquote>
<p>å·®å¼‚å›¾åƒåˆ†æï¼ˆDIAï¼‰æ˜¯ç ”ç©¶æ—¶é—´å˜é‡ç°è±¡çš„å¼ºå¤§å·¥å…·ï¼Œå·²è¢«è®¸å¤šæ—¶åŸŸè°ƒæŸ¥æ‰€ä½¿ç”¨ã€‚å¤§å¤šæ•°DIAç®—æ³•æ¶‰åŠåŒ¹é…ç§‘å­¦å’Œæ¨¡æ¿å›¾åƒä¹‹é—´ç©ºé—´å˜åŒ–çš„PSFå½¢çŠ¶ï¼Œç„¶åå°†è¯¥å½¢çŠ¶åœ¨ä¸€ä¸ªå›¾åƒä¸­è¿›è¡Œå·ç§¯ä»¥åŒ¹é…å¦ä¸€ä¸ªå›¾åƒã€‚é€‰æ‹©å“ªä¸ªå›¾åƒè¿›è¡Œå·ç§¯çš„é”™è¯¯å¯èƒ½å¯¼è‡´æœ€ç»ˆå·®å¼‚å›¾åƒä¸­å‡ºç°æœ€å¤§çš„ä¼ªå½±æ¥æºä¹‹ä¸€ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å®šé‡æŒ‡æ ‡ï¼Œä»¥ç¡®å®šæœ€ä½³çš„å·ç§¯æ–¹å‘ï¼Œè¿™ä¸ä»…å–å†³äºé€šè¿‡å®ƒä»¬çš„FWHMæµ‹é‡çš„å›¾åƒæ¸…æ™°åº¦ï¼Œè¿˜å–å†³äºå®ƒä»¬çš„æ›å…‰æ·±åº¦ã€‚ä½¿ç”¨è¯¥æŒ‡æ ‡ï¼Œå¯ä»¥äº‹å…ˆç¡®å®šæœ€ä½³çš„å·ç§¯æ–¹å‘ï¼Œè¿™ä»…å–å†³äºå›¾åƒçš„FWHMå’Œæ·±åº¦ã€‚è¿™ä¸ä»…ç®€åŒ–äº†æµç¨‹ï¼Œè€Œä¸”ä½¿å…¶æ›´åŠ ç¨³å¥ï¼Œå¹¶ä¸”ç”±äºé€‰æ‹©äº†é”™è¯¯çš„å·ç§¯æ–¹å‘è€Œä¸å¤ªå®¹æ˜“äº§ç”Ÿæ¬¡ä¼˜å·®å¼‚å›¾åƒã€‚ä½œä¸ºé™„åŠ å¥½å¤„ï¼Œå¯¹äºå¤§é‡å›¾åƒï¼Œæˆ‘ä»¬åŸºäºè¯¥æŒ‡æ ‡å®šä¹‰äº†ä¸€ä¸ªå“è´¨å› æ•°ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¯¹å›¾åƒåˆ—è¡¨è¿›è¡Œæ’åï¼Œå¹¶ç¡®å®šæœ€é€‚åˆç”¨ä½œæ¨¡æ¿çš„å›¾åƒï¼Œä»è€Œç®€åŒ–å’Œè‡ªåŠ¨åŒ–æ•°æ®ç¼©å‡è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10155v1">PDF</a> 17 pages, 11 figures</p>
<p><strong>Summary</strong><br>     å·®å¼‚å›¾åƒåˆ†æï¼ˆDIAï¼‰æ˜¯ç ”ç©¶æ—¶å˜ç°è±¡çš„æœ‰åŠ›å·¥å…·ï¼Œå·²è¢«è®¸å¤šæ—¶åŸŸè°ƒæŸ¥æ‰€ä½¿ç”¨ã€‚å¤§å¤šæ•°DIAç®—æ³•æ¶‰åŠåŒ¹é…ç§‘å­¦å’Œæ¨¡æ¿å›¾åƒä¹‹é—´ç©ºé—´å˜åŒ–çš„PSFå½¢çŠ¶ï¼Œç„¶åå°†è¯¥å½¢çŠ¶åœ¨ä¸€ä¸ªå›¾åƒä¸­å·ç§¯ä»¥åŒ¹é…å¦ä¸€ä¸ªå›¾åƒã€‚é€‰æ‹©å“ªä¸ªå›¾åƒè¿›è¡Œå·ç§¯ï¼Œå¯èƒ½ä¼šå¯¹æœ€ç»ˆå·®å¼‚å›¾åƒå¼•å…¥æœ€å¤§çš„ä¼ªå½±æ¥æºã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å®šé‡æŒ‡æ ‡ï¼Œä»¥ç¡®å®šæœ€ä½³å·ç§¯æ–¹å‘ï¼Œè¯¥æ–¹å‘ä¸ä»…å–å†³äºé€šè¿‡FWHMæµ‹é‡çš„å›¾åƒæ¸…æ™°åº¦ï¼Œè¿˜å–å†³äºå®ƒä»¬çš„æ›å…‰æ·±åº¦ã€‚æ­¤æŒ‡æ ‡å¯é¢„å…ˆç¡®å®šæœ€ä½³å·ç§¯æ–¹å‘ï¼Œä»…å–å†³äºå›¾åƒçš„FWHMå’Œæ·±åº¦ã€‚è¿™ä¸ä»…ç®€åŒ–äº†æµç¨‹ï¼Œè€Œä¸”ä½¿å…¶æ›´åŠ ç¨³å¥ï¼Œä¸æ˜“å› å·ç§¯æ–¹å‘é€‰æ‹©é”™è¯¯è€Œç”Ÿæˆæ¬¡ä¼˜å·®å¼‚å›¾åƒã€‚å¯¹äºå¤§é‡å›¾åƒï¼Œæˆ‘ä»¬è¿˜åŸºäºè¯¥æŒ‡æ ‡å®šä¹‰äº†ä¸€ä¸ªå“è´¨å› æ•°ï¼Œå¯ä»¥æ’åå›¾åƒåˆ—è¡¨å¹¶ç¡®å®šæœ€é€‚åˆç”¨ä½œæ¨¡æ¿çš„å›¾åƒï¼Œä»è€Œç®€åŒ–å’Œè‡ªåŠ¨åŒ–æ•°æ®ç¼©å‡è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DIAæ˜¯ä¸€ç§ç ”ç©¶æ—¶é—´å˜é‡ç°è±¡çš„æœ‰åŠ›å·¥å…·ï¼Œå¸¸ç”¨äºæ—¶åŸŸè°ƒæŸ¥ã€‚</li>
<li>DIAç®—æ³•çš„æ ¸å¿ƒåœ¨äºåŒ¹é…ç§‘å­¦å’Œæ¨¡æ¿å›¾åƒä¹‹é—´çš„PSFå½¢çŠ¶ï¼Œå¹¶å·ç§¯ä»¥äº§ç”ŸåŒ¹é…ã€‚</li>
<li>é€‰æ‹©å·ç§¯çš„å›¾åƒæ–¹å‘æ˜¯å…³é”®çš„ï¼Œé”™è¯¯çš„é€‰æ‹©å¯èƒ½å¯¼è‡´æœ€ç»ˆå·®å¼‚å›¾åƒä¸­çš„é‡å¤§ä¼ªå½±ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§å®šé‡æŒ‡æ ‡æ¥ç¡®å®šæœ€ä½³å·ç§¯æ–¹å‘ï¼Œè¯¥æŒ‡æ ‡è€ƒè™‘å›¾åƒçš„FWHMå’Œæ›å…‰æ·±åº¦ã€‚</li>
<li>ä½¿ç”¨æ­¤æŒ‡æ ‡å¯ä»¥é¢„å…ˆç¡®å®šæœ€ä½³å·ç§¯æ–¹å‘ï¼Œç®€åŒ–æµç¨‹å¹¶å¢åŠ ç¨³å¥æ€§ã€‚</li>
<li>å¯¹äºå¤§é‡å›¾åƒï¼Œå®šä¹‰äº†ä¸€ä¸ªå“è´¨å› æ•°æ¥æ’åå›¾åƒå¹¶ç¡®å®šæœ€é€‚åˆç”¨ä½œæ¨¡æ¿çš„å›¾åƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0518429756c77937655d3f971e76d225.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-783f138a4baa603e0c1e78044e9b116a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31359a9bb8880d9a71e08d1c1818f722.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8e6a502ac5f3fc7159f9296819ab1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e39c41b0103aefdac112ceed752c2939.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ebe8e536c8bdb247f3a74a12e714dd1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="From-Intent-to-Execution-Multimodal-Chain-of-Thought-Reinforcement-Learning-for-Precise-CAD-Code-Generation"><a href="#From-Intent-to-Execution-Multimodal-Chain-of-Thought-Reinforcement-Learning-for-Precise-CAD-Code-Generation" class="headerlink" title="From Intent to Execution: Multimodal Chain-of-Thought Reinforcement   Learning for Precise CAD Code Generation"></a>From Intent to Execution: Multimodal Chain-of-Thought Reinforcement   Learning for Precise CAD Code Generation</h2><p><strong>Authors:Ke Niu, Haiyang Yu, Zhuofan Chen, Mengyang Zhao, Teng Fu, Bin Li, Xiangyang Xue</strong></p>
<p>Computer-Aided Design (CAD) plays a vital role in engineering and manufacturing, yet current CAD workflows require extensive domain expertise and manual modeling effort. Recent advances in large language models (LLMs) have made it possible to generate code from natural language, opening new opportunities for automating parametric 3D modeling. However, directly translating human design intent into executable CAD code remains highly challenging, due to the need for logical reasoning, syntactic correctness, and numerical precision. In this work, we propose CAD-RL, a multimodal Chain-of-Thought (CoT) guided reinforcement learning post training framework for CAD modeling code generation. Our method combines CoT-based Cold Start with goal-driven reinforcement learning post training using three task-specific rewards: executability reward, geometric accuracy reward, and external evaluation reward. To ensure stable policy learning under sparse and high-variance reward conditions, we introduce three targeted optimization strategies: Trust Region Stretch for improved exploration, Precision Token Loss for enhanced dimensions parameter accuracy, and Overlong Filtering to reduce noisy supervision. To support training and benchmarking, we release ExeCAD, a noval dataset comprising 16,540 real-world CAD examples with paired natural language and structured design language descriptions, executable CADQuery scripts, and rendered 3D models. Experiments demonstrate that CAD-RL achieves significant improvements in reasoning quality, output precision, and code executability over existing VLMs. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰åœ¨å·¥ç¨‹å’Œåˆ¶é€ ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†å½“å‰çš„CADå·¥ä½œæµç¨‹éœ€è¦ä¸°å¯Œçš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’Œæ‰‹åŠ¨å»ºæ¨¡å·¥ä½œã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å¾—ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆä»£ç æˆä¸ºå¯èƒ½ï¼Œä¸ºè‡ªåŠ¨åŒ–å‚æ•°åŒ–3Då»ºæ¨¡æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œç›´æ¥å°†äººç±»çš„è®¾è®¡æ„å›¾ç¿»è¯‘æˆå¯æ‰§è¡Œçš„CADä»£ç ä»ç„¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œè¿™éœ€è¦è¿›è¡Œé€»è¾‘æ¨ç†ã€è¯­æ³•æ­£ç¡®å’Œæ•°å€¼ç²¾ç¡®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CAD-RLï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºCADå»ºæ¨¡ä»£ç ç”Ÿæˆçš„å¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆCoTï¼‰å¼•å¯¼å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åŸºäºCoTçš„å†·å¯åŠ¨å’ŒåŸºäºç›®æ ‡é©±åŠ¨å¼ºåŒ–å­¦ä¹ åè®­ç»ƒï¼Œä½¿ç”¨ä¸‰ç§ç‰¹å®šä»»åŠ¡å¥–åŠ±ï¼šå¯æ‰§è¡Œæ€§å¥–åŠ±ã€å‡ ä½•ç²¾åº¦å¥–åŠ±å’Œå¤–éƒ¨è¯„ä¼°å¥–åŠ±ã€‚ä¸ºäº†ç¡®ä¿åœ¨ç¨€ç–å’Œé«˜æ–¹å·®å¥–åŠ±æ¡ä»¶ä¸‹çš„ç¨³å®šç­–ç•¥å­¦ä¹ ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ç§æœ‰é’ˆå¯¹æ€§çš„ä¼˜åŒ–ç­–ç•¥ï¼šç”¨äºæ”¹è¿›æ¢ç´¢çš„ä¿¡ä»»åŒºåŸŸæ‰©å±•ã€ç”¨äºæé«˜ç»´åº¦å‚æ•°ç²¾åº¦çš„ç²¾ç¡®ä»¤ç‰ŒæŸå¤±ä»¥åŠç”¨äºå‡å°‘å™ªå£°ç›‘ç£çš„è¿‡é•¿è¿‡æ»¤ã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ExeCADæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«16540ä¸ªç°å®ä¸–ç•Œä¸­çš„CADç¤ºä¾‹ï¼Œé…æœ‰è‡ªç„¶è¯­è¨€æè¿°å’Œç»“æ„åŒ–è®¾è®¡è¯­è¨€æè¿°ã€å¯æ‰§è¡Œçš„CADQueryè„šæœ¬å’Œæ¸²æŸ“çš„3Dæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ¨ç†è´¨é‡ã€è¾“å‡ºç²¾åº¦å’Œä»£ç å¯æ‰§è¡Œæ€§æ–¹é¢ï¼ŒCAD-RLç›¸è¾ƒäºç°æœ‰çš„VLMså–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10118v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºChain-of-Thoughtï¼ˆCoTï¼‰å¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„CADå»ºæ¨¡ä»£ç ç”Ÿæˆæ¡†æ¶CAD-RLã€‚è¯¥æ¡†æ¶ç»“åˆäº†CoTçš„å†·å¯åŠ¨æŠ€æœ¯ï¼Œå¹¶åˆ©ç”¨ç›®æ ‡é©±åŠ¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒåçš„ä¼˜åŒ–ï¼Œé€šè¿‡ä¸‰ç§ç‰¹å®šä»»åŠ¡å¥–åŠ±ç¡®ä¿ç”Ÿæˆçš„CADä»£ç å¯æ‰§è¡Œã€å‡ ä½•ç²¾åº¦é«˜å¹¶å¾—åˆ°å¤–éƒ¨è¯„ä»·ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸‰ç§ä¼˜åŒ–ç­–ç•¥æ¥æé«˜ç­–ç•¥å­¦ä¹ çš„ç¨³å®šæ€§ã€‚ä¸ºæ”¯æŒè®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•ï¼Œå‘å¸ƒäº†ExeCADæ•°æ®é›†ï¼ŒåŒ…å«é…å¯¹è‡ªç„¶è¯­è¨€æè¿°å’Œç»“æ„åŒ–è®¾è®¡è¯­è¨€çš„çœŸå®CADæ¨¡å‹å®ä¾‹ã€‚å®éªŒè¯æ˜ï¼ŒCAD-RLåœ¨æ¨ç†è´¨é‡ã€è¾“å‡ºç²¾åº¦å’Œä»£ç å¯æ‰§è¡Œæ€§æ–¹é¢å‡ä¼˜äºç°æœ‰VLMsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CADè®¾è®¡åœ¨å·¥ç¨‹å’Œåˆ¶é€ ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†å½“å‰CADå·¥ä½œæµç¨‹éœ€è¦ä¸“ä¸šçŸ¥è¯†å’Œæ‰‹åŠ¨å»ºæ¨¡åŠªåŠ›ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç”Ÿæˆä»£ç ï¼Œä¸ºè‡ªåŠ¨åŒ–å‚æ•°åŒ–3Då»ºæ¨¡å¸¦æ¥æ–°æœºä¼šã€‚</li>
<li>CAD-RLæ¡†æ¶ç»“åˆäº†CoTæŠ€æœ¯å¹¶å¼•å…¥ä¸‰ç§ç‰¹å®šä»»åŠ¡å¥–åŠ±å’Œä¸‰ç§ä¼˜åŒ–ç­–ç•¥ï¼Œç¡®ä¿ç”ŸæˆCADä»£ç çš„è´¨é‡å’Œç¨³å®šæ€§ã€‚</li>
<li>å‘å¸ƒExeCADæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•CADå»ºæ¨¡æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3b6be24b1d6d02ae65b9fccd68d170f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce13e6f999f7e46b5e92a4a7917722df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edf1b249ffe6b6397d7803fa47ff2769.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f4e6552499ff5ba0a1a1295218a5604.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="In-silico-study-on-the-cytotoxicity-against-Hela-cancer-cells-of-xanthones-bioactive-compounds-from-Garcinia-cowa-QSAR-based-on-Graph-Deep-Learning-Network-Pharmacology-and-Molecular-Docking"><a href="#In-silico-study-on-the-cytotoxicity-against-Hela-cancer-cells-of-xanthones-bioactive-compounds-from-Garcinia-cowa-QSAR-based-on-Graph-Deep-Learning-Network-Pharmacology-and-Molecular-Docking" class="headerlink" title="In silico study on the cytotoxicity against Hela cancer cells of   xanthones bioactive compounds from Garcinia cowa: QSAR based on Graph Deep   Learning, Network Pharmacology, and Molecular Docking"></a>In silico study on the cytotoxicity against Hela cancer cells of   xanthones bioactive compounds from Garcinia cowa: QSAR based on Graph Deep   Learning, Network Pharmacology, and Molecular Docking</h2><p><strong>Authors:Nguyen Manh Son, Pham Huu Vang, Nguyen Thi Dung, Nguyen Manh Ha. Ta Thi Thao, Tran Thi Thu Thuy, Phan Minh Giang</strong></p>
<p>Cancer is recognized as a complex group of diseases, contributing to the highest global mortality rates, with increasing prevalence and a trend toward affecting younger populations. It is characterized by uncontrolled proliferation of abnormal cells, invasion of adjacent tissues, and metastasis to distant organs. Garcinia cowa, a traditional medicinal plant widely used in Southeast Asia, including Vietnam, is employed to treat fever, cough, indigestion, as a laxative, and for parasitic diseases. Numerous xanthone compounds isolated from this species exhibit a broad spectrum of biological activities, with some showing promise as anti cancer and antimalarial agents. Network pharmacology analysis successfully identified key bioactive compounds Rubraxanthone, Garcinone D, Norcowanin, Cowanol, and Cowaxanthone alongside their primary protein targets (TNF, CTNNB1, SRC, NFKB1, and MTOR), providing critical insights into the molecular mechanisms underlying their anti-cancer effects. The Graph Attention Network algorithm demonstrated superior predictive performance, achieving an R2 of 0.98 and an RMSE of 0.02 after data augmentation, highlighting its accuracy in predicting pIC50 values for xanthone based compounds. Additionally, molecular docking revealed MTOR as a potential target for inducing cytotoxicity in HeLa cancer cells from Garcinia cowa. </p>
<blockquote>
<p>ç™Œç—‡è¢«è®¤å®šä¸ºä¸€ç»„å¤æ‚çš„ç–¾ç—…ï¼Œå…·æœ‰å…¨çƒæœ€é«˜çš„æ­»äº¡ç‡ï¼Œä¸”å‘ç—…ç‡ä¸æ–­ä¸Šå‡ï¼Œä¸”æœ‰è¶Šæ¥è¶Šå¹´è½»åŒ–çš„å‘å±•è¶‹åŠ¿ã€‚å…¶ç‰¹ç‚¹ä¸ºå¼‚å¸¸ç»†èƒçš„æ— é™å¢æ®–ã€é‚»è¿‘ç»„ç»‡çš„ä¾µè¢­å’Œå‘è¿œå¤„å™¨å®˜çš„è½¬ç§»ã€‚Garcinia cowaæ˜¯ä¸€ç§åœ¨ä¸œå—äºšï¼ˆåŒ…æ‹¬è¶Šå—ï¼‰å¹¿æ³›ä½¿ç”¨çš„ä¼ ç»Ÿè¯ç”¨æ¤ç‰©ï¼Œç”¨äºæ²»ç–—å‘çƒ­ã€å’³å—½ã€æ¶ˆåŒ–ä¸è‰¯ã€æ³»è¯å’Œå¯„ç”Ÿè™«ç—…ç­‰ã€‚ä»è¿™ç§æ¤ç‰©ä¸­æå–å‡ºçš„ä¼—å¤šxanthoneåŒ–åˆç‰©å…·æœ‰å¹¿æ³›çš„ç”Ÿç‰©æ´»æ€§ï¼Œå…¶ä¸­ä¸€äº›æ˜¾ç¤ºå‡ºæŠ—ç™Œå’ŒæŠ—ç–Ÿç–¾çš„æ½œåŠ›ã€‚ç½‘ç»œè¯ç†å­¦åˆ†ææˆåŠŸç¡®å®šäº†å…³é”®ç”Ÿç‰©æ´»æ€§åŒ–åˆç‰©ï¼Œå¦‚Rubraxanthoneã€Garcinone Dã€Norcowaninã€Cowanolå’ŒCowaxanthoneåŠå…¶ä¸»è¦è›‹ç™½è´¨é¶æ ‡ï¼ˆTNFã€CTNNB1ã€SRCã€NFKB1å’ŒMTORï¼‰ï¼Œä¸ºæˆ‘ä»¬æ·±å…¥äº†è§£äº†è¿™äº›åŒ–åˆç‰©æŠ—ç™Œä½œç”¨çš„åˆ†å­æœºåˆ¶æä¾›äº†é‡è¦è§è§£ã€‚Graph Attention Networkç®—æ³•åœ¨æ•°æ®å¢å¼ºåè¡¨ç°å‡ºä¼˜å¼‚çš„é¢„æµ‹æ€§èƒ½ï¼ŒR2è¾¾åˆ°0.98ï¼ŒRMSEä¸º0.02ï¼Œå‡¸æ˜¾å…¶åœ¨é¢„æµ‹åŸºäºxanthoneåŒ–åˆç‰©çš„pIC50å€¼æ–¹é¢çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œåˆ†å­å¯¹æ¥æ˜¾ç¤ºMTORå¯èƒ½æ˜¯Garcinia cowaè¯±å¯¼HeLaç™Œç»†èƒæ¯’æ€§çš„æ½œåœ¨é¶ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10117v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç™Œç—‡ä½œä¸ºå…¨çƒé«˜æ­»äº¡ç‡ç–¾ç—…çš„ç‰¹ç‚¹å’Œè¶‹åŠ¿ï¼Œä»¥åŠGarcinia cowaè¿™ç§ä¸œå—äºšä¼ ç»Ÿè¯ç”¨æ¤ç‰©åœ¨æ²»ç–—ç™Œç—‡æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶é€šè¿‡ç½‘ç»œè¯ç†å­¦åˆ†æç¡®å®šäº†è¯¥æ¤ç‰©ä¸­çš„å…³é”®ç”Ÿç‰©æ´»æ€§åŒ–åˆç‰©åŠå…¶ä¸»è¦è›‹ç™½è´¨é¶ç‚¹ï¼Œå¹¶å€ŸåŠ©Graph Attention Networkç®—æ³•å’Œåˆ†å­å¯¹æ¥æŠ€æœ¯ï¼Œæ­ç¤ºäº†å…¶åœ¨æŠ—ç™Œä½œç”¨ä¸­çš„åˆ†å­æœºåˆ¶å’Œæ½œåœ¨é¶ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç™Œç—‡ç‰¹ç‚¹åŒ…æ‹¬ç»†èƒå¼‚å¸¸å¢ç”Ÿå¤±æ§ã€ä¾µçŠ¯é‚»è¿‘ç»„ç»‡ä»¥åŠå‘è¿œå¤„å™¨å®˜è½¬ç§»ã€‚</li>
<li>Garcinia cowaæ˜¯ä¸€ç§å¹¿æ³›ç”¨äºæ²»ç–—å¤šç§ç–¾ç—…çš„ä¼ ç»Ÿè¯ç”¨æ¤ç‰©ã€‚</li>
<li>è¯¥æ¤ç‰©ä¸­çš„xanthoneåŒ–åˆç‰©å…·æœ‰å¹¿æ³›çš„ç”Ÿç‰©æ´»æ€§ï¼Œä¸€äº›åŒ–åˆç‰©å…·æœ‰æŠ—ç™Œå’ŒæŠ—ç–Ÿç–¾çš„æ½œåŠ›ã€‚</li>
<li>ç½‘ç»œè¯ç†å­¦åˆ†æç¡®å®šäº†Garcinia cowaä¸­çš„å…³é”®ç”Ÿç‰©æ´»æ€§åŒ–åˆç‰©åŠå…¶ä¸»è¦è›‹ç™½è´¨é¶ç‚¹ã€‚</li>
<li>Graph Attention Networkç®—æ³•åœ¨é¢„æµ‹xanthoneåŸºåŒ–åˆç‰©çš„pIC50å€¼æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>åˆ†å­å¯¹æ¥ç ”ç©¶æ˜¾ç¤ºMTORå¯èƒ½æ˜¯Garcinia cowaè¯±å¯¼HeLaç™Œç»†èƒæ¯’æ€§çš„æ½œåœ¨é¶ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10117">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f32a84696e754fd3b4f3ccf4eb025efa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-223c6721cf0e91d29c28f063f8904498.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-caec3cc69085b5fec4eae4afd6ec6906.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0890c1af07221972ac1a321ff623bfa2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Specialised-or-Generic-Tokenization-Choices-for-Radiology-Language-Models"><a href="#Specialised-or-Generic-Tokenization-Choices-for-Radiology-Language-Models" class="headerlink" title="Specialised or Generic? Tokenization Choices for Radiology Language   Models"></a>Specialised or Generic? Tokenization Choices for Radiology Language   Models</h2><p><strong>Authors:Hermione Warr, Wentian Xu, Harry Anthony, Yasin Ibrahim, Daniel McGowan, Konstantinos Kamnitsas</strong></p>
<p>The vocabulary used by language models (LM) - defined by the tokenizer - plays a key role in text generation quality. However, its impact remains under-explored in radiology. In this work, we address this gap by systematically comparing general, medical, and domain-specific tokenizers on the task of radiology report summarisation across three imaging modalities. We also investigate scenarios with and without LM pre-training on PubMed abstracts. Our findings demonstrate that medical and domain-specific vocabularies outperformed widely used natural language alternatives when models are trained from scratch. Pre-training partially mitigates performance differences between tokenizers, whilst the domain-specific tokenizers achieve the most favourable results. Domain-specific tokenizers also reduce memory requirements due to smaller vocabularies and shorter sequences. These results demonstrate that adapting the vocabulary of LMs to the clinical domain provides practical benefits, including improved performance and reduced computational demands, making such models more accessible and effective for both research and real-world healthcare settings. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰æ‰€ä½¿ç”¨çš„è¯æ±‡ç”±åˆ†è¯å™¨å®šä¹‰ï¼Œè¿™åœ¨æ–‡æœ¬ç”Ÿæˆè´¨é‡ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œå…¶åœ¨æ”¾å°„å­¦é¢†åŸŸçš„å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç³»ç»Ÿåœ°æ¯”è¾ƒä¸‰ç§æˆåƒæ¨¡æ€ä¸‹ç”¨äºæ”¾å°„å­¦æŠ¥å‘Šæ‘˜è¦ä»»åŠ¡çš„é€šç”¨ã€åŒ»ç–—å’Œé¢†åŸŸç‰¹å®šåˆ†è¯å™¨æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†æœ‰æ— åœ¨PubMedæ‘˜è¦ä¸Šè¿›è¡ŒLMé¢„è®­ç»ƒçš„åœºæ™¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“æ¨¡å‹ä»å¤´å¼€å§‹è®­ç»ƒæ—¶ï¼ŒåŒ»ç–—å’Œé¢†åŸŸç‰¹å®šçš„è¯æ±‡è¡¨ç°ä¼˜äºå¹¿æ³›ä½¿ç”¨çš„è‡ªç„¶è¯­è¨€æ›¿ä»£æ–¹æ¡ˆã€‚é¢„è®­ç»ƒåœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡è½»äº†åˆ†è¯å™¨ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œè€Œé¢†åŸŸç‰¹å®šçš„åˆ†è¯å™¨å–å¾—äº†æœ€ç†æƒ³çš„ç»“æœã€‚é¢†åŸŸç‰¹å®šçš„åˆ†è¯å™¨ç”±äºè¯æ±‡é‡è¾ƒå°ã€åºåˆ—è¾ƒçŸ­ï¼Œè¿˜å‡å°‘äº†å†…å­˜éœ€æ±‚ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå°†LMçš„è¯æ±‡é€‚åº”ä¸´åºŠé¢†åŸŸæä¾›äº†å®é™…å¥½å¤„ï¼ŒåŒ…æ‹¬æé«˜æ€§èƒ½å’Œé™ä½è®¡ç®—éœ€æ±‚ï¼Œä½¿å¾—è¿™äº›æ¨¡å‹åœ¨ç ”ç©¶å’Œç°å®ä¸–ç•Œçš„åŒ»ç–—ç¯å¢ƒä¸­éƒ½æ›´å®¹æ˜“è®¿é—®å’Œæ›´æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09952v1">PDF</a> Accepted to ELAMI@MICCAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä¸­ï¼Œä¸åŒçš„åˆ†è¯å™¨ï¼ˆtokenizerï¼‰å¯¹æ”¾å°„å­¦æŠ¥å‘Šæ‘˜è¦ç”Ÿæˆè´¨é‡çš„å½±å“ã€‚é€šè¿‡å¯¹é€šç”¨ã€åŒ»ç–—å’Œé¢†åŸŸç‰¹å®šåˆ†è¯å™¨çš„ç³»ç»Ÿæ¯”è¾ƒï¼Œå‘ç°åŒ»ç–—å’Œé¢†åŸŸç‰¹å®šçš„åˆ†è¯å™¨åœ¨ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹æ—¶è¡¨ç°ä¼˜äºå¹¿æ³›ä½¿ç”¨çš„è‡ªç„¶è¯­è¨€å¤„ç†åˆ†è¯å™¨ã€‚é¢„è®­ç»ƒéƒ¨åˆ†ç¼“è§£äº†åˆ†è¯å™¨ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œè€Œé¢†åŸŸç‰¹å®šçš„åˆ†è¯å™¨å–å¾—äº†æœ€ç†æƒ³çš„ç»“æœã€‚æ­¤å¤–ï¼Œé¢†åŸŸç‰¹å®šçš„åˆ†è¯å™¨ç”±äºè¯æ±‡è¾ƒå°ã€åºåˆ—è¾ƒçŸ­ï¼Œè¿˜é™ä½äº†å†…å­˜éœ€æ±‚ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†è¯­è¨€æ¨¡å‹çš„è¯æ±‡é€‚åº”ä¸´åºŠé¢†åŸŸå…·æœ‰å®é™…æ•ˆç›Šï¼ŒåŒ…æ‹¬æé«˜æ€§èƒ½å’Œé™ä½è®¡ç®—éœ€æ±‚ï¼Œä½¿å¾—è¿™æ ·çš„æ¨¡å‹åœ¨ç ”ç©¶å’Œç°å®åŒ»ç–—ç¯å¢ƒä¸­æ›´åŠ å¯ç”¨å’Œæœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä¸­ä½¿ç”¨çš„åˆ†è¯å™¨ï¼ˆtokenizerï¼‰å¯¹æ”¾å°„å­¦æŠ¥å‘Šæ‘˜è¦ç”Ÿæˆçš„è´¨é‡æœ‰é‡è¦å½±å“ã€‚</li>
<li>åŒ»ç–—å’Œé¢†åŸŸç‰¹å®šçš„åˆ†è¯å™¨åœ¨è®­ç»ƒæ¨¡å‹æ—¶è¡¨ç°ä¼˜äºé€šç”¨åˆ†è¯å™¨ã€‚</li>
<li>é¢„è®­ç»ƒå¯ä»¥éƒ¨åˆ†ç¼“è§£ä¸åŒåˆ†è¯å™¨ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚</li>
<li>é¢†åŸŸç‰¹å®šçš„åˆ†è¯å™¨å–å¾—æœ€ç†æƒ³ç»“æœã€‚</li>
<li>é¢†åŸŸç‰¹å®šçš„åˆ†è¯å™¨èƒ½é™ä½å†…å­˜éœ€æ±‚ï¼Œå› ä¸ºå®ƒä»¬çš„è¯æ±‡è¾ƒå°ï¼Œåºåˆ—è¾ƒçŸ­ã€‚</li>
<li>é€‚åº”ä¸´åºŠé¢†åŸŸçš„è¯­è¨€æ¨¡å‹è¯æ±‡æœ‰åŠ©äºæé«˜æ€§èƒ½å’Œé™ä½è®¡ç®—éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6f6aba9fa3c7b3e3776c16364e12711.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0bec8da0f5a0e1755dca02f50068997.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab8b62641d18768661cd4a15bba685a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21b0cedb0fd52ab57dc04f477204eec0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-971fc087003641bba8c6b2affcb0afbc.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-522dbbaa71aab70b1b338d37f90cc03f.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  MoE-TTS Enhancing Out-of-Domain Text Understanding for   Description-based TTS via Mixture-of-Experts
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4d431081396646a66cd4c1d9560e98ef.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  LoRAtorio An intrinsic approach to LoRA Skill Composition
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
