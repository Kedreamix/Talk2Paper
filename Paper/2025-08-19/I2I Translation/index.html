<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-08-19  StyleMM Stylized 3D Morphable Face Model via Text-Driven Aligned Image   Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-fbf02856ab90883492ec326e1cae83ba.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-19-更新"><a href="#2025-08-19-更新" class="headerlink" title="2025-08-19 更新"></a>2025-08-19 更新</h1><h2 id="StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation"><a href="#StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation" class="headerlink" title="StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image   Translation"></a>StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image   Translation</h2><p><strong>Authors:Seungmi Lee, Kwan Yun, Junyong Noh</strong></p>
<p>We introduce StyleMM, a novel framework that can construct a stylized 3D Morphable Model (3DMM) based on user-defined text descriptions specifying a target style. Building upon a pre-trained mesh deformation network and a texture generator for original 3DMM-based realistic human faces, our approach fine-tunes these models using stylized facial images generated via text-guided image-to-image (i2i) translation with a diffusion model, which serve as stylization targets for the rendered mesh. To prevent undesired changes in identity, facial alignment, or expressions during i2i translation, we introduce a stylization method that explicitly preserves the facial attributes of the source image. By maintaining these critical attributes during image stylization, the proposed approach ensures consistent 3D style transfer across the 3DMM parameter space through image-based training. Once trained, StyleMM enables feed-forward generation of stylized face meshes with explicit control over shape, expression, and texture parameters, producing meshes with consistent vertex connectivity and animatability. Quantitative and qualitative evaluations demonstrate that our approach outperforms state-of-the-art methods in terms of identity-level facial diversity and stylization capability. The code and videos are available at <a href="kwanyun.github.io/stylemm_page">kwanyun.github.io&#x2F;stylemm_page</a>. </p>
<blockquote>
<p>我们介绍了StyleMM，这是一个新型框架，可以根据用户定义的文本描述指定目标风格来构建风格化的3D可变形模型（3DMM）。我们的方法建立在预训练的网格变形网络和原始3DMM现实人脸的纹理生成器之上，使用通过文本引导的图像到图像（i2i）翻译扩散模型生成的风格化面部图像对这些模型进行微调，这些图像作为渲染网格的风格化目标。为了防止在i2i翻译过程中身份、面部对齐或表情发生不必要的变化，我们引入了一种显式保留源图像面部特征的风格化方法。通过保持这些关键特征在图像风格化过程中的一致性，所提出的方法确保了通过基于图像的训练在整个3DMM参数空间进行一致的3D风格转换。一旦训练完成，StyleMM就能够以前馈方式生成具有明确形状、表情和纹理参数控制的风格化面部网格，产生具有一致顶点连接和动画能力的网格。定量和定性评估表明，我们的方法在身份级别的面部多样性和风格化能力方面优于最先进的方法。代码和视频可在<a href="kwanyun.github.io/stylemm_page">kwanyun.github.io&#x2F;stylemm_page</a>查看。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11203v1">PDF</a> Pacific graphics 2025, CGF, 15 pages</p>
<p><strong>Summary</strong></p>
<p>基于用户定义文本描述的目标风格，我们引入了StyleMM这一新型框架，构建了一个风格化的3D可变形模型（3DMM）。该框架在预训练的网格变形网络和原始3DMM逼真人脸纹理生成器的基础上，使用文本引导的图像到图像（i2i）翻译扩散模型生成风格化面部图像，作为渲染网格的风格化目标进行微调。为阻止i2i翻译过程中身份、面部对齐或表情的不必要变化，我们引入了一种显式保留源图像面部特征的风格化方法。通过保持这些关键特征在图像风格化过程中的一致性，所提出的方法确保了在整个3DMM参数空间中的3D风格转换通过基于图像的训练实现一致。训练后的StyleMM可以通过前向生成风格化的面部网格，对形状、表情和纹理参数进行显式控制，产生具有一致顶点连接和动画能力的网格。定量和定性评估表明，我们的方法在身份级别的面部多样性和风格化能力方面优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StyleMM是一个基于用户定义文本描述构建风格化3D可变形模型（3DMM）的框架。</li>
<li>利用预训练模型并结合文本引导的i2i翻译扩散模型生成风格化面部图像。</li>
<li>引入风格化方法，显式保留源图像的面部特征，以保持身份、面部对齐和表情的一致性。</li>
<li>通过图像基训练实现整个3DMM参数空间中的一致3D风格转换。</li>
<li>StyleMM可以生成具有一致顶点连接和动画能力的风格化面部网格，具有显式控制形状、表达和纹理参数的能力。</li>
<li>与现有技术相比，StyleMM在身份级别的面部多样性和风格化能力方面表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11203">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-24c4153a81a2390528acff0042cef047.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3df3c699501d1a68ed0de5fc01b76a00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aaff26d9a31e9a524be9e3ff7806462.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09a9b2ba661fc3a1f6f83a08284c82b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11bd7253b70daf1ebe6589dcfb09b81d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Residual-based-Efficient-Bidirectional-Diffusion-Model-for-Image-Dehazing-and-Haze-Generation"><a href="#Residual-based-Efficient-Bidirectional-Diffusion-Model-for-Image-Dehazing-and-Haze-Generation" class="headerlink" title="Residual-based Efficient Bidirectional Diffusion Model for Image   Dehazing and Haze Generation"></a>Residual-based Efficient Bidirectional Diffusion Model for Image   Dehazing and Haze Generation</h2><p><strong>Authors:Bing Liu, Le Wang, Hao Liu, Mingming Liu</strong></p>
<p>Current deep dehazing methods only focus on removing haze from hazy images, lacking the capability to translate between hazy and haze-free images. To address this issue, we propose a residual-based efficient bidirectional diffusion model (RBDM) that can model the conditional distributions for both dehazing and haze generation. Firstly, we devise dual Markov chains that can effectively shift the residuals and facilitate bidirectional smooth transitions between them. Secondly, the RBDM perturbs the hazy and haze-free images at individual timesteps and predicts the noise in the perturbed data to simultaneously learn the conditional distributions. Finally, to enhance performance on relatively small datasets and reduce computational costs, our method introduces a unified score function learned on image patches instead of entire images. Our RBDM successfully implements size-agnostic bidirectional transitions between haze-free and hazy images with only 15 sampling steps. Extensive experiments demonstrate that the proposed method achieves superior or at least comparable performance to state-of-the-art methods on both synthetic and real-world datasets. </p>
<blockquote>
<p>当前深度去雾方法仅专注于从雾霾图像中去除雾霾，缺乏在雾霾和无雾图像之间进行翻译的能力。为了解决这一问题，我们提出了一种基于残差的高效双向扩散模型（RBDM），该模型可以对去雾和雾霾生成的条件分布进行建模。首先，我们设计了双马尔可夫链，可以有效地转换残差，促进它们之间的双向平滑过渡。其次，RBDM在单个时间步长内扰动无雾和雾霾图像，并预测受扰动数据中的噪声，以同时学习条件分布。最后，为了提高在相对较小数据集上的性能并降低计算成本，我们的方法引入了一个在图像块上学习的统一评分函数，而不是在整个图像上。我们的RBDM仅使用15个采样步骤就成功实现了无雾和雾霾图像之间大小无关的双向过渡。大量实验表明，所提出的方法在合成和真实世界数据集上均达到了或至少与最先进的方法相当的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11134v1">PDF</a> 7 pages, 5 figures, 2025 ICME Accepted</p>
<p><strong>Summary</strong>：</p>
<p>提出一种基于残差的双向扩散模型（RBDM），能建模去雾和生成雾的条件分布。通过双马尔可夫链实现残差转移，双向平滑过渡。对去雾和清晰图像进行扰动，预测扰动数据的噪声，学习条件分布。引入统一评分函数，提高小数据集性能，降低计算成本。成功实现去雾和清晰图像之间的尺寸无关双向转换，仅需15个采样步骤。在合成和真实数据集上实现优异性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>引入基于残差的双向扩散模型（RBDM），能处理去雾和雾生成两种情况的图像转换。</li>
<li>使用双马尔可夫链实现残差转移，实现图像间的双向平滑过渡。</li>
<li>通过扰动图像并预测噪声，学习去雾和清晰图像的条件分布。</li>
<li>引入统一评分函数，提高在小数据集上的性能，并降低计算成本。</li>
<li>模型能在仅15个采样步骤内实现去雾和清晰图像之间的转换。</li>
<li>在合成数据集上的性能优于或至少与现有技术相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11134">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-76a176208cd581cba8c79a197648e07b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e13d2260a752525edf320319bef0d89e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f791d27acc5589854121642b4057152.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eebcf3a19639936ba70b237d2f8d248.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5955921ee0af8be3322721ef52f1947.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f4503a1465caffb7a467a9599a8fa26.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Novel-View-Synthesis-using-DDIM-Inversion"><a href="#Novel-View-Synthesis-using-DDIM-Inversion" class="headerlink" title="Novel View Synthesis using DDIM Inversion"></a>Novel View Synthesis using DDIM Inversion</h2><p><strong>Authors:Sehajdeep SIngh, A V Subramanyam</strong></p>
<p>Synthesizing novel views from a single input image is a challenging task. It requires extrapolating the 3D structure of a scene while inferring details in occluded regions, and maintaining geometric consistency across viewpoints. Many existing methods must fine-tune large diffusion backbones using multiple views or train a diffusion model from scratch, which is extremely expensive. Additionally, they suffer from blurry reconstruction and poor generalization. This gap presents the opportunity to explore an explicit lightweight view translation framework that can directly utilize the high-fidelity generative capabilities of a pretrained diffusion model while reconstructing a scene from a novel view. Given the DDIM-inverted latent of a single input image, we employ a camera pose-conditioned translation U-Net, TUNet, to predict the inverted latent corresponding to the desired target view. However, the image sampled using the predicted latent may result in a blurry reconstruction. To this end, we propose a novel fusion strategy that exploits the inherent noise correlation structure observed in DDIM inversion. The proposed fusion strategy helps preserve the texture and fine-grained details. To synthesize the novel view, we use the fused latent as the initial condition for DDIM sampling, leveraging the generative prior of the pretrained diffusion model. Extensive experiments on MVImgNet demonstrate that our method outperforms existing methods. </p>
<blockquote>
<p>从单一输入图像合成新颖视角是一项具有挑战性的任务。它要求在推断遮挡区域的细节时推断出场景的3D结构，并在不同视角之间保持几何一致性。许多现有方法必须使用多种视角对大型扩散主干进行微调，或者从头开始训练扩散模型，这成本极高。此外，它们还存在模糊重建和泛化能力差的缺点。这一差距为我们探索一个明确的轻量级视图翻译框架提供了机会，该框架能够利用预训练扩散模型的高保真生成能力，从新颖视角重建场景。给定单个输入图像的DDIM反向传播潜在特征，我们采用受相机姿态控制的翻译U-Net（TUNet）来预测对应目标视角的反向传播潜在特征。然而，使用预测潜在特征所采样的图像可能会导致重建模糊。为此，我们提出了一种利用DDIM反演中观察到的固有噪声相关性结构的新型融合策略。该融合策略有助于保留纹理和细粒度细节。为了合成新颖视角，我们使用融合后的潜在特征作为DDIM采样的初始条件，利用预训练扩散模型的生成先验。在MVImgNet上的广泛实验表明，我们的方法优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10688v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对单一输入图像合成新颖视角的任务，现有方法需要微调大型扩散模型或使用多个视角进行训练，成本高昂且存在模糊重建和泛化能力差的问题。为此，我们提出一种显式轻量级视角转换框架，利用预训练扩散模型的高保真生成能力，通过相机姿态调节的翻译U-Net来预测目标视角的倒置潜在对应物。为提高重建图像的清晰度，我们提出了一种利用DDIM反转过程中观察到的固有噪声相关结构的新融合策略。实验证明，该方法在MVImgNet上表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>合成新颖视角是一项挑战任务，要求从单一输入图像中推断出场景的3D结构并在隐藏区域进行细节推断，同时保持不同视角的几何一致性。</li>
<li>现有方法需要昂贵的微调大型扩散模型或使用多个视角进行训练，存在模糊重建和泛化能力差的问题。</li>
<li>提出一种轻量级视角转换框架，利用预训练扩散模型的高保真生成能力。</li>
<li>使用相机姿态调节的翻译U-Net（TUNet）预测目标视角的倒置潜在对应物。</li>
<li>提出一种新型融合策略，利用DDIM反转过程中的噪声相关结构，帮助保留纹理和细节。</li>
<li>使用融合后的潜在条件作为DDIM采样的初始条件，利用预训练扩散模型的生成先验来合成新颖视角。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10688">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f90a471e30ab6f1e73d8bbde4fe994c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d431081396646a66cd4c1d9560e98ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7f35760138c043d0c09ceb8b8d60933.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbf02856ab90883492ec326e1cae83ba.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MANGO-Multimodal-Attention-based-Normalizing-Flow-Approach-to-Fusion-Learning"><a href="#MANGO-Multimodal-Attention-based-Normalizing-Flow-Approach-to-Fusion-Learning" class="headerlink" title="MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion   Learning"></a>MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion   Learning</h2><p><strong>Authors:Thanh-Dat Truong, Christophe Bobda, Nitin Agarwal, Khoa Luu</strong></p>
<p>Multimodal learning has gained much success in recent years. However, current multimodal fusion methods adopt the attention mechanism of Transformers to implicitly learn the underlying correlation of multimodal features. As a result, the multimodal model cannot capture the essential features of each modality, making it difficult to comprehend complex structures and correlations of multimodal inputs. This paper introduces a novel Multimodal Attention-based Normalizing Flow (MANGO) approach\footnote{The source code of this work will be publicly available.} to developing explicit, interpretable, and tractable multimodal fusion learning. In particular, we propose a new Invertible Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for multimodal data. To efficiently capture the complex, underlying correlations in multimodal data in our proposed invertible cross-attention layer, we propose three new cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based Normalizing Flow to enable the scalability of our proposed method to high-dimensional multimodal data. Our experimental results on three different multimodal learning tasks, i.e., semantic segmentation, image-to-image translation, and movie genre classification, have illustrated the state-of-the-art (SoTA) performance of the proposed approach. </p>
<blockquote>
<p>近年来，多模态学习取得了巨大的成功。然而，当前的多模态融合方法采用Transformer的注意力机制来隐式地学习多模态特征之间的底层关联。因此，多模态模型无法捕捉每种模态的基本特征，难以理解多模态输入的复杂结构和关联。本文介绍了一种新颖的多模态注意力基础归一化流（MANGO）方法（该工作的源代码将公开发布）来开发明确、可解释和可行的多模态融合学习。特别是，我们提出了一种新的可逆跨注意力（ICA）层来开发基于归一化流的多模态数据模型。为了在我们提出的可逆跨注意力层中有效地捕捉多模态数据中复杂的底层关联，我们提出了三种新的跨注意力机制：模态间跨注意力（MMCA）、跨模态间注意力（IMCA）和学习跨模态间注意力（LICA）。最后，我们引入了一种新的多模态注意力基础归一化流，使所提出的方法能够扩展到高维多模态数据。我们在三种不同的多模态学习任务（即语义分割、图像到图像的翻译和电影类型分类）上的实验结果证明了所提出方法的最新技术性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10133v1">PDF</a> </p>
<p><strong>Summary</strong><br>多模态学习近年取得了很大进展，但当前的多模态融合方法采用Transformer的注意力机制来隐式学习多模态特征之间的关联。这导致多模态模型无法捕捉每个模态的关键特征，难以理解和分析多模态输入的复杂结构和关联。本文提出一种新颖的多模态注意力基础归一化流（MANGO）方法，开发明确、可解释和可追踪的多模态融合学习。特别是，我们提出了一种新的可逆交叉注意力（ICA）层，用于开发基于归一化流的多模态数据模型。为了在我们的可逆交叉注意力层中有效地捕捉多模态数据的复杂底层关联，我们提出了三种新的交叉注意力机制。最后，我们引入了一种新的多模态注意力基础归一化流，使所提方法能够扩展到高维多模态数据。在三个不同的多模态学习任务上的实验结果证明了所提方法的优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前多模态融合方法存在难以捕捉每个模态关键特征的问题。</li>
<li>论文提出一种新的多模态注意力基础归一化流（MANGO）方法，旨在解决上述问题。</li>
<li>引入可逆交叉注意力（ICA）层，为开发基于归一化流的多模态数据模型提供基础。</li>
<li>提出三种新的交叉注意力机制：模态间交叉注意力（MMCA）、跨模态交叉注意力（IMCA）和可学习跨模态交叉注意力（LICA）。</li>
<li>所提方法能够有效捕捉多模态数据的复杂底层关联。</li>
<li>论文实验验证了所提方法在三种不同多模态学习任务上的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10133">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-cb9f04a6b7cdf765e2ac01b7a1b339a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22389b45c4a020b1d5f50abb844f5468.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RampNet-A-Two-Stage-Pipeline-for-Bootstrapping-Curb-Ramp-Detection-in-Streetscape-Images-from-Open-Government-Metadata"><a href="#RampNet-A-Two-Stage-Pipeline-for-Bootstrapping-Curb-Ramp-Detection-in-Streetscape-Images-from-Open-Government-Metadata" class="headerlink" title="RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in   Streetscape Images from Open Government Metadata"></a>RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in   Streetscape Images from Open Government Metadata</h2><p><strong>Authors:John S. O’Meara, Jared Hwang, Zeyu Wang, Michael Saugstad, Jon E. Froehlich</strong></p>
<p>Curb ramps are critical for urban accessibility, but robustly detecting them in images remains an open problem due to the lack of large-scale, high-quality datasets. While prior work has attempted to improve data availability with crowdsourced or manually labeled data, these efforts often fall short in either quality or scale. In this paper, we introduce and evaluate a two-stage pipeline called RampNet to scale curb ramp detection datasets and improve model performance. In Stage 1, we generate a dataset of more than 210,000 annotated Google Street View (GSV) panoramas by auto-translating government-provided curb ramp location data to pixel coordinates in panoramic images. In Stage 2, we train a curb ramp detection model (modified ConvNeXt V2) from the generated dataset, achieving state-of-the-art performance. To evaluate both stages of our pipeline, we compare to manually labeled panoramas. Our generated dataset achieves 94.0% precision and 92.5% recall, and our detection model reaches 0.9236 AP – far exceeding prior work. Our work contributes the first large-scale, high-quality curb ramp detection dataset, benchmark, and model. </p>
<blockquote>
<p>路缘石坡道对于城市可达性至关重要，但由于缺乏大规模高质量数据集，在图像中稳健地检测它们仍然是一个悬而未决的问题。尽管之前的研究试图通过众包或手动标记的数据来提高数据可用性，但这些努力在质量或规模上往往有所欠缺。在本文中，我们介绍并评估了一个名为RampNet的两阶段流程，以扩大路缘石坡道检测数据集并提高模型性能。在第一阶段，我们通过自动翻译政府提供的路缘石坡道位置数据，生成了超过21万个标注的Google街景（GSV）全景图像数据集。在第二阶段，我们根据生成的数据集训练了路缘石坡道检测模型（修改后的ConvNeXt V2），取得了最先进的性能。为了评估我们流程的两个阶段，我们将结果与手动标记的全景图像进行了比较。我们生成的数据集达到了94.0%的精确度和92.5%的召回率，我们的检测模型达到了0.9236的平均准确率——远远超过以前的工作。我们的工作贡献了一个大规模、高质量的路缘石坡道检测数据集、基准测试和模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09415v1">PDF</a> Accepted to the ICCV’25 Workshop on Vision Foundation Models and   Generative AI for Accessibility: Challenges and Opportunities</p>
<p><strong>Summary</strong></p>
<p>本文介绍了利用RampNet两阶段管道实现城市坡道检测数据集的规模化以及模型性能的提升。第一阶段通过自动翻译政府提供的坡道位置数据生成超过21万张标注的Google街景全景图像数据集。第二阶段使用修改后的ConvNeXt V2模型进行坡道检测，达到业界领先水平。评估结果显示，生成的数据集精确度和召回率分别达到94.0%和92.5%，检测模型的平均精度达到0.9236，远超先前工作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>缺乏大规模高质量数据集是城市坡道检测面临的主要问题。</li>
<li>RampNet两阶段管道解决了坡道检测数据集的规模化问题。</li>
<li>第一阶段通过自动翻译政府数据生成大规模坡道位置数据集。</li>
<li>第二阶段使用修改后的ConvNeXt V2模型进行坡道检测，性能卓越。</li>
<li>生成的数据集精确度和召回率均超过92%。</li>
<li>检测模型的平均精度达到业界最高水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09415">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f17e26b4d1874f7351eb9c4d91aa33ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76021e3e7b2e3ddb6d6087f03286ee4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14b493482e8ade7ab530b5e92f949f40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec17795b775511a77c87c4948de34492.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c28ce468f02d41940ae94cd64ae5759.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b4853bf95f9199ec5f5b6ee3ae1c8df.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Recent-Advances-in-Generative-AI-for-Healthcare-Applications"><a href="#Recent-Advances-in-Generative-AI-for-Healthcare-Applications" class="headerlink" title="Recent Advances in Generative AI for Healthcare Applications"></a>Recent Advances in Generative AI for Healthcare Applications</h2><p><strong>Authors:Yasin Shokrollahi, Jose Colmenarez, Wenxi Liu, Sahar Yarmohammadtoosky, Matthew M. Nikahd, Pengfei Dong, Xianqi Li, Linxia Gu</strong></p>
<p>The rapid advancement of Artificial Intelligence (AI) has catalyzed revolutionary changes across various sectors, notably in healthcare. In particular, generative AI-led by diffusion models and transformer architectures-has enabled significant breakthroughs in medical imaging (including image reconstruction, image-to-image translation, generation, and classification), protein structure prediction, clinical documentation, diagnostic assistance, radiology interpretation, clinical decision support, medical coding, and billing, as well as drug design and molecular representation. These innovations have enhanced clinical diagnosis, data reconstruction, and drug synthesis. This review paper aims to offer a comprehensive synthesis of recent advances in healthcare applications of generative AI, with an emphasis on diffusion and transformer models. Moreover, we discuss current capabilities, highlight existing limitations, and outline promising research directions to address emerging challenges. Serving as both a reference for researchers and a guide for practitioners, this work offers an integrated view of the state of the art, its impact on healthcare, and its future potential. </p>
<blockquote>
<p>人工智能（AI）的快速发展催生了各个领域的革命性变化，特别是在医疗领域。特别是以扩散模型和转换器架构为驱动的生成式人工智能，在医学成像（包括图像重建、图像到图像的翻译、生成和分类）、蛋白质结构预测、临床文档、诊断辅助、放射学解读、临床决策支持、医疗编码和计费以及药物设计和分子表示等方面实现了重大突破。这些创新提高了临床诊断、数据重建和药物合成的能力。这篇综述文章旨在提供关于生成式人工智能在医疗领域应用的最新进展的综合分析，重点介绍扩散模型和转换器模型。此外，我们还讨论了当前的能力，强调了现有的局限性，并概述了解决新兴挑战的有希望的研究方向。这项工作既为研究者提供了参考，也为从业者提供了指南，全面展示了最新技术、其对医疗领域的影响及其未来潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00795v2">PDF</a> 51 pages, 16 figures, 1table</p>
<p><strong>Summary</strong><br>     人工智能（AI）的快速发展催生了各领域革命性的变革，尤其在医疗领域。生成式AI引领的扩散模型和转换器架构在医疗成像、蛋白质结构预测、临床文档、诊断辅助、放射学解读、临床决策支持等方面取得了重大突破。本文旨在全面综述生成式AI在医疗领域的最新进展，重点介绍扩散模型和转换器模型的应用。同时，本文讨论当前的能力，强调现有的局限性，并概述有前景的研究方向以应对新兴挑战。它为研究人员提供了参考，为从业者提供了指南，展现了最新的技术态势及其对医疗领域的影响和未来潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式AI在医疗领域引发重大突破，涵盖医疗成像、蛋白质结构预测、临床文档等方面。</li>
<li>扩散模型和转换器架构是生成式AI的主要技术驱动力。</li>
<li>这些技术增强了临床诊断、数据重建和药物合成的能力。</li>
<li>当前生成式AI在医疗领域的应用已产生显著影响。</li>
<li>尽管有重大进展，但生成式AI在医疗领域仍存在局限性。</li>
<li>有必要进行更多研究以解决新兴挑战，并推动生成式AI在医疗领域的进一步发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.00795">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9d5a960f2db8d45ca023bf3f9a54e9ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16e45f22b94ef7397cdce38d931781dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-556d751dc4875792fb2156ab39933dcd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f03d143c0ef44bc6208cb18864fa415.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Ear-Keeper-A-Cross-Platform-AI-System-for-Rapid-and-Accurate-Ear-Disease-Diagnosis"><a href="#Ear-Keeper-A-Cross-Platform-AI-System-for-Rapid-and-Accurate-Ear-Disease-Diagnosis" class="headerlink" title="Ear-Keeper: A Cross-Platform AI System for Rapid and Accurate Ear   Disease Diagnosis"></a>Ear-Keeper: A Cross-Platform AI System for Rapid and Accurate Ear   Disease Diagnosis</h2><p><strong>Authors:Feiyan Lu, Yubiao Yue, Zhenzhang Li, Meiping Zhang, Wen Luo, Fan Zhang, Tong Liu, Jingyong Shi, Guang Wang, Xinyu Zeng</strong></p>
<p>Early and accurate detection systems for ear diseases, powered by deep learning, are essential for preventing hearing impairment and improving population health. However, the limited diversity of existing otoendoscopy datasets and the poor balance between diagnostic accuracy, computational efficiency, and model size have hindered the translation of artificial intelligence (AI) algorithms into healthcare applications. In this study, we constructed a large-scale, multi-center otoendoscopy dataset covering eight common ear diseases and healthy cases. Building upon this resource, we developed Best-EarNet, an ultrafast and lightweight deep learning architecture integrating a novel Local-Global Spatial Feature Fusion Module with a multi-scale supervision strategy, enabling real-time and accurate classification of ear conditions. Leveraging transfer learning, Best-EarNet, with a model size of only 2.94 MB, achieved diagnostic accuracies of 95.23% on an internal test set (22,581 images) and 92.14% on an external test set (1,652 images), while requiring only 0.0125 seconds (80 frames per second) to process a single image on a standard CPU. Further subgroup analysis by gender and age showed consistently excellent performance of Best-EarNet across all demographic groups. To enhance clinical interpretability and user trust, we incorporated Grad-CAM-based visualization, highlighting the specific abnormal ear regions contributing to AI predictions. Most importantly, we developed Ear-Keeper, a cross-platform intelligent diagnosis system built upon Best-EarNet, deployable on smartphones, tablets, and personal computers. Ear-Keeper enables public users and healthcare providers to perform comprehensive real-time video-based ear canal screening, supporting early detection and timely intervention of ear diseases. </p>
<blockquote>
<p>由深度学习驱动的早期且精确的疾病检测系统对于预防听力受损并改善公众健康至关重要。然而，现有的耳内窥镜数据集多样性有限，诊断准确性、计算效率和模型大小之间的平衡不佳，这些都阻碍了人工智能算法在医疗保健应用中的转化。在这项研究中，我们构建了一个大规模、多中心的耳内窥镜数据集，涵盖了八种常见的耳部疾病和健康状况。基于这一资源，我们开发了Best-EarNet，这是一种超快速且轻量级的深度学习架构，它结合了新颖的全局局部空间特征融合模块和多尺度监督策略，能够实现耳部状况的实时和准确分类。通过迁移学习，仅2.94MB的Best-EarNet在内部测试集（22,581张图像）上实现了95.23%的诊断准确率，在外部测试集（1,652张图像）上实现了92.14%的准确率，同时在标准CPU上处理单张图像仅需0.0125秒（每秒80帧）。按性别和年龄的进一步亚组分析显示，Best-EarNet在所有年龄段的表现均非常出色。为了提高临床解释性和用户信任度，我们结合了基于Grad-CAM的可视化，突出显示对人工智能预测起特定作用的异常耳部区域。最重要的是，我们开发了Ear-Keeper，这是一款基于Best-EarNet的跨平台智能诊断系统，可在智能手机、平板电脑和个人电脑上部署。Ear-Keeper使公众用户和医疗服务提供者能够进行全面实时的基于视频的耳道筛查，支持耳部疾病的早期检测和及时干预。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10610v5">PDF</a> 18 pages,8 figures</p>
<p><strong>Summary</strong></p>
<p>基于深度学习技术的早期准确耳病检测系统对于预防听力受损、提升全民健康水平至关重要。本研究构建了一个大规模、多中心耳内镜数据集，覆盖八种常见耳病及健康案例。在此基础上，研究团队开发出Best-EarNet模型，融合了局部全局空间特征融合模块和多尺度监督策略，实现了耳部状况的实时准确分类。该模型利用迁移学习，仅2.94MB的模型大小便在内部测试集上实现了95.23%的诊断准确率，在外部测试集上达到了92.14%，处理单张图片仅需0.0125秒（每秒处理80帧）。研究还通过性别和年龄分组分析证明了Best-EarNet在所有人群中的卓越表现。为提升临床解释性和用户信任度，研究团队引入了基于Grad-CAM的可视化技术，突出显示导致AI预测异常的特定耳部区域。最重要的是，研究团队开发了跨平台的智能诊断系统Ear-Keeper，可部署于智能手机、平板电脑和个人电脑，支持公众和医疗工作者进行实时视频耳道筛查，实现早期检测和及时干预。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习驱动的耳病早期检测系统对预防听力受损和提高人口健康水平至关重要。</li>
<li>研究构建了一个大规模、多中心的耳内镜数据集，覆盖多种常见耳病。</li>
<li>Best-EarNet模型实现耳部状况的实时准确分类，具备出色的诊断准确率。</li>
<li>Best-EarNet模型具备超快处理速度和轻量级特点，便于实际应用。</li>
<li>模型在多种人口分组中表现优秀，具备广泛的适用性。</li>
<li>引入Grad-CAM可视化技术，提升模型临床解释性和用户信任度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.10610">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-71200bb25fb143804577db89724d25d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4cfca3cd33052e251e5a82a8381aedf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d216a77123fb848e8b4cb938ae3b72a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-010bb2ce72183068ad0222773dd72d43.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b13338eebd805883fd0b237bba96ba97.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-08-19  Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0480971fd3e9425b1778d2a6a9c613cb.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-19  CoFi A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement   Membrane Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
