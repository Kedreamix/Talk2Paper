<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  LoRAtorio An intrinsic approach to LoRA Skill Composition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-4d431081396646a66cd4c1d9560e98ef.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-19-æ›´æ–°"><a href="#2025-08-19-æ›´æ–°" class="headerlink" title="2025-08-19 æ›´æ–°"></a>2025-08-19 æ›´æ–°</h1><h2 id="LoRAtorio-An-intrinsic-approach-to-LoRA-Skill-Composition"><a href="#LoRAtorio-An-intrinsic-approach-to-LoRA-Skill-Composition" class="headerlink" title="LoRAtorio: An intrinsic approach to LoRA Skill Composition"></a>LoRAtorio: An intrinsic approach to LoRA Skill Composition</h2><p><strong>Authors:Niki Foteinopoulou, Ignas Budvytis, Stephan Liwicki</strong></p>
<p>Low-Rank Adaptation (LoRA) has become a widely adopted technique in text-to-image diffusion models, enabling the personalisation of visual concepts such as characters, styles, and objects. However, existing approaches struggle to effectively compose multiple LoRA adapters, particularly in open-ended settings where the number and nature of required skills are not known in advance. In this work, we present LoRAtorio, a novel train-free framework for multi-LoRA composition that leverages intrinsic model behaviour. Our method is motivated by two key observations: (1) LoRA adapters trained on narrow domains produce denoised outputs that diverge from the base model, and (2) when operating out-of-distribution, LoRA outputs show behaviour closer to the base model than when conditioned in distribution. The balance between these two observations allows for exceptional performance in the single LoRA scenario, which nevertheless deteriorates when multiple LoRAs are loaded. Our method operates in the latent space by dividing it into spatial patches and computing cosine similarity between each patchâ€™s predicted noise and that of the base model. These similarities are used to construct a spatially-aware weight matrix, which guides a weighted aggregation of LoRA outputs. To address domain drift, we further propose a modification to classifier-free guidance that incorporates the base modelâ€™s unconditional score into the composition. We extend this formulation to a dynamic module selection setting, enabling inference-time selection of relevant LoRA adapters from a large pool. LoRAtorio achieves state-of-the-art performance, showing up to a 1.3% improvement in ClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises effectively to multiple latent diffusion models. </p>
<blockquote>
<p>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å·²æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ä¸€é¡¹å¹¿æ³›é‡‡ç”¨çš„æŠ€æœ¯ï¼Œä½¿å­—ç¬¦ã€é£æ ¼å’Œå¯¹è±¡ç­‰è§†è§‰æ¦‚å¿µçš„ä¸ªæ€§åŒ–æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåœ°ç»„åˆå¤šä¸ªLoRAé€‚é…å™¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾å¼ç¯å¢ƒä¸­ï¼Œæ‰€éœ€æŠ€èƒ½çš„æ•°é‡å’Œæ€§è´¨æ˜¯æœªçŸ¥çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LoRAtorioï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å¤šLoRAç»„åˆæ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æ¨¡å‹çš„å†…åœ¨è¡Œä¸ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°ä¸¤ä¸ªå…³é”®è§‚å¯Ÿç»“æœçš„å¯å‘ï¼šï¼ˆ1ï¼‰åœ¨ç‹­çª„é¢†åŸŸä¸Šè®­ç»ƒçš„LoRAé€‚é…å™¨ä¼šäº§ç”Ÿå»å™ªè¾“å‡ºï¼Œè¿™äº›è¾“å‡ºä¸åŸºç¡€æ¨¡å‹æœ‰åˆ†æ­§ï¼›ï¼ˆ2ï¼‰åœ¨åˆ†å¸ƒä¹‹å¤–è¿è¡Œæ—¶ï¼ŒLoRAè¾“å‡ºæ˜¾ç¤ºçš„è¡Œä¸ºæ›´æ¥è¿‘åŸºç¡€æ¨¡å‹ï¼Œè€Œä¸æ˜¯åœ¨åˆ†å¸ƒä¸­çš„æ¡ä»¶ã€‚è¿™ä¸¤ç§è§‚å¯Ÿç»“æœä¹‹é—´çš„å¹³è¡¡å…è®¸åœ¨å•ä¸ªLoRAåœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç„¶è€Œå½“åŠ è½½å¤šä¸ªLoRAsæ—¶ï¼Œæ€§èƒ½ä¼šä¸‹é™ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ“ä½œï¼Œå°†å…¶åˆ†ä¸ºç©ºé—´å—å¹¶è®¡ç®—æ¯ä¸ªå—çš„é¢„æµ‹å™ªå£°ä¸åŸºç¡€æ¨¡å‹çš„ä½™å¼¦ç›¸ä¼¼æ€§ã€‚è¿™äº›ç›¸ä¼¼æ€§è¢«ç”¨æ¥æ„å»ºä¸€ä¸ªç©ºé—´æ„ŸçŸ¥æƒé‡çŸ©é˜µï¼Œè¯¥çŸ©é˜µå¼•å¯¼LoRAè¾“å‡ºçš„åŠ æƒèšåˆã€‚ä¸ºäº†è§£å†³é¢†åŸŸæ¼‚ç§»é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†æ— åˆ†ç±»å™¨æŒ‡å¯¼çš„ä¿®æ”¹ï¼Œå°†åŸºç¡€æ¨¡å‹çš„æ— æ¡ä»¶åˆ†æ•°çº³å…¥ç»„åˆã€‚æˆ‘ä»¬å°†è¿™ç§è¡¨è¿°æ‰©å±•åˆ°åŠ¨æ€æ¨¡å—é€‰æ‹©è®¾ç½®ï¼Œä»¥åœ¨æ¨ç†æ—¶ä»å¤§é‡æ± ä¸­é€‰æ‹©ç›¸å…³çš„LoRAé€‚é…å™¨ã€‚LoRAtorioè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨ClipScoreä¸Šå®ç°äº†é«˜è¾¾1.3%çš„æ”¹è¿›ï¼Œåœ¨GPT-4Vé…å¯¹è¯„ä¼°ä¸­çš„èƒœç‡ä¸º72.43%ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°é€‚åº”å¤šç§æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11624v1">PDF</a> 32 pages, 17 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>LoRAtorioæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å¤šLoRAç»„åˆæ–°æ–¹æ³•ï¼Œé€‚ç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ä½ç­‰çº§é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ã€‚è¯¥æ–¹æ³•åŸºäºä¸¤ä¸ªå…³é”®è§‚å¯Ÿç»“æœï¼šLoRAé€‚é…å™¨åœ¨ç‹­çª„é¢†åŸŸä¸Šè®­ç»ƒä¼šäº§ç”Ÿå»å™ªè¾“å‡ºï¼Œè¿™äº›è¾“å‡ºä¸åŸºç¡€æ¨¡å‹æœ‰æ‰€åå·®ï¼›å½“è¶…å‡ºåˆ†å¸ƒæ“ä½œæ—¶ï¼ŒLoRAè¾“å‡ºè¡Œä¸ºä¸åŸºç¡€æ¨¡å‹æ›´æ¥è¿‘ã€‚LoRAtorioé€šè¿‡åˆ†å‰²æ½œåœ¨ç©ºé—´ä¸ºç©ºé—´è¡¥ä¸å¹¶è®¡ç®—æ¯ä¸ªè¡¥ä¸é¢„æµ‹å™ªå£°ä¸åŸºç¡€æ¨¡å‹çš„ä½™å¼¦ç›¸ä¼¼æ€§ï¼Œæ¥æ„å»ºç©ºé—´æ„ŸçŸ¥æƒé‡çŸ©é˜µï¼ŒæŒ‡å¯¼åŠ æƒèšåˆLoRAè¾“å‡ºã€‚ä¸ºè§£å†³é¢†åŸŸæ¼‚ç§»é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºæ— åˆ†ç±»å™¨æŒ‡å¯¼çš„ä¿®æ”¹ï¼Œå°†åŸºç¡€æ¨¡å‹çš„æ— æ¡ä»¶åˆ†æ•°çº³å…¥ç»„åˆä¸­ã€‚è¯¥æ–¹æ³•å¯æ‰©å±•è‡³åŠ¨æ€æ¨¡å—é€‰æ‹©è®¾ç½®ï¼Œå¯åœ¨æ¨ç†æ—¶ä»å¤§é‡æ± ä¸­é€‰æ‹©ç›¸å…³LoRAé€‚é…å™¨ã€‚LoRAtorioå®ç°äº†ä¸€æµçš„æ€§èƒ½ï¼Œåœ¨ClipScoreä¸Šæé«˜äº†1.3%ï¼Œåœ¨GPT-4Væˆå¯¹è¯„ä¼°ä¸­çš„èƒœç‡ä¸º72.43%ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°æ¨å¹¿åˆ°å¤šä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LoRAtorioæ˜¯ä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„å¤šLoRAç»„åˆæ¡†æ¶ï¼Œé€‚ç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ä¸ªæ€§åŒ–è§†è§‰æ¦‚å¿µã€‚</li>
<li>LoRAé€‚é…å™¨åœ¨ç‹­çª„é¢†åŸŸä¸Šè®­ç»ƒä¼šäº§ç”Ÿå»å™ªè¾“å‡ºï¼Œè¿™äº›è¾“å‡ºä¸åŸºç¡€æ¨¡å‹æœ‰æ‰€åå·®ã€‚</li>
<li>å½“è¶…å‡ºåˆ†å¸ƒæ“ä½œæ—¶ï¼ŒLoRAè¾“å‡ºçš„è¡Œä¸ºä¸åŸºç¡€æ¨¡å‹æ›´ä¸ºæ¥è¿‘ã€‚</li>
<li>LoRAtorioé€šè¿‡è®¡ç®—é¢„æµ‹å™ªå£°çš„ä½™å¼¦ç›¸ä¼¼æ€§æ¥æ„å»ºç©ºé—´æ„ŸçŸ¥æƒé‡çŸ©é˜µï¼ŒæŒ‡å¯¼åŠ æƒèšåˆLoRAè¾“å‡ºã€‚</li>
<li>ä¸ºè§£å†³é¢†åŸŸæ¼‚ç§»é—®é¢˜ï¼Œç»“åˆåŸºç¡€æ¨¡å‹çš„æ— æ¡ä»¶åˆ†æ•°è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°åŠ¨æ€æ¨¡å—é€‰æ‹©ï¼Œå¯åœ¨æ¨ç†æ—¶æ ¹æ®éœ€æ±‚é€‰æ‹©ç›¸å…³çš„LoRAé€‚é…å™¨ã€‚</li>
<li>LoRAtorioåœ¨æ€§èƒ½ä¸Šå®ç°äº†ä¸€æµçš„æˆæœï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ¨å¹¿è‡³å¤šä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0d66d1d874d1d9cf71d2e7c93f756c99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3daee1be2cdb06d2893f947ea86c4bb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bb3940b233379db6eab18d51f022e36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddb7d64b0556bd58c32cbbffafc01601.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12b80cfe17320c31d2ad665e3ac0b55f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SPG-Style-Prompting-Guidance-for-Style-Specific-Content-Creation"><a href="#SPG-Style-Prompting-Guidance-for-Style-Specific-Content-Creation" class="headerlink" title="SPG: Style-Prompting Guidance for Style-Specific Content Creation"></a>SPG: Style-Prompting Guidance for Style-Specific Content Creation</h2><p><strong>Authors:Qian Liang, Zichong Chen, Yang Zhou, Hui Huang</strong></p>
<p>Although recent text-to-image (T2I) diffusion models excel at aligning generated images with textual prompts, controlling the visual style of the output remains a challenging task. In this work, we propose Style-Prompting Guidance (SPG), a novel sampling strategy for style-specific image generation. SPG constructs a style noise vector and leverages its directional deviation from unconditional noise to guide the diffusion process toward the target style distribution. By integrating SPG with Classifier-Free Guidance (CFG), our method achieves both semantic fidelity and style consistency. SPG is simple, robust, and compatible with controllable frameworks like ControlNet and IPAdapter, making it practical and widely applicable. Extensive experiments demonstrate the effectiveness and generality of our approach compared to state-of-the-art methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Rumbling281441/SPG">https://github.com/Rumbling281441/SPG</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨å°†ç”Ÿæˆçš„å›¾åƒä¸æ–‡æœ¬æç¤ºå¯¹é½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æ§åˆ¶è¾“å‡ºå›¾åƒçš„å¯è§†é£æ ¼ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Style-Prompting Guidanceï¼ˆSPGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç‰¹å®šé£æ ¼å›¾åƒç”Ÿæˆçš„æ–°å‹é‡‡æ ·ç­–ç•¥ã€‚SPGæ„å»ºäº†ä¸€ä¸ªé£æ ¼å™ªå£°å‘é‡ï¼Œåˆ©ç”¨å…¶æ— æ¡ä»¶å™ªå£°çš„æ–¹å‘åå·®æ¥å¼•å¯¼æ‰©æ•£è¿‡ç¨‹æœå‘ç›®æ ‡é£æ ¼åˆ†å¸ƒã€‚é€šè¿‡å°†SPGä¸æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†è¯­ä¹‰ä¿çœŸå’Œé£æ ¼ä¸€è‡´æ€§ã€‚SPGç®€å•ã€ç¨³å¥ï¼Œä¸”ä¸å¯æ§æ¡†æ¶å¦‚ControlNetå’ŒIPAdapterå…¼å®¹ï¼Œä½¿å…¶å…·æœ‰å®ç”¨æ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æœ‰æ•ˆæ€§å’Œæ™®éæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Rumbling281441/SPG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Rumbling281441/SPGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11476v1">PDF</a> Accepted to the Journal track of Pacific Graphics 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°çš„æ‰©æ•£æ¨¡å‹æå‡ºäº†ä¸€ä¸ªåä¸ºStyle-Prompting Guidanceï¼ˆSPGï¼‰çš„æ–°é‡‡æ ·ç­–ç•¥ï¼Œç”¨äºç‰¹å®šé£æ ¼çš„å›¾åƒç”Ÿæˆã€‚é€šè¿‡æ„å»ºé£æ ¼å™ªå£°å‘é‡å¹¶å¼•å¯¼æ‰©æ•£è¿‡ç¨‹æœç€ç›®æ ‡é£æ ¼åˆ†å¸ƒè¿›è¡Œï¼Œè¯¥ç­–ç•¥å®ç°äº†è¯­ä¹‰ä¿çœŸå’Œé£æ ¼ä¸€è‡´æ€§ã€‚SPGä¸Classifier-Free Guidanceï¼ˆCFGï¼‰ç›¸ç»“åˆï¼Œä½¿æ–¹æ³•æ—¢ç®€å•åˆç¨³å¥ï¼Œä¸”ä¸å¯æ§æ¡†æ¶å¦‚ControlNetå’ŒIPAdapterå…¼å®¹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”å…·æœ‰æœ‰æ•ˆæ€§å’Œæ™®éæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æè¿°çš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä¸æ–‡æœ¬æç¤ºå¯¹é½çš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æ§åˆ¶è¾“å‡ºè§†è§‰é£æ ¼ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºäº†Style-Prompting Guidanceï¼ˆSPGï¼‰é‡‡æ ·ç­–ç•¥ï¼Œç”¨äºç‰¹å®šé£æ ¼çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>SPGé€šè¿‡æ„å»ºé£æ ¼å™ªå£°å‘é‡å¹¶å¼•å¯¼æ‰©æ•£è¿‡ç¨‹æ¥å·¥ä½œï¼Œä»è€Œå®ç°ç›®æ ‡é£æ ¼åˆ†å¸ƒã€‚</li>
<li>SPGä¸Classifier-Free Guidanceï¼ˆCFGï¼‰ç»“åˆï¼Œè¾¾åˆ°è¯­ä¹‰ä¿çœŸå’Œé£æ ¼ä¸€è‡´æ€§ã€‚</li>
<li>SPGæ–¹æ³•ç®€å•ã€ç¨³å¥ï¼Œä¸å¯æ§æ¡†æ¶å…¼å®¹ï¼Œå¦‚ControlNetå’ŒIPAdapterã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æœ‰æ•ˆæ€§å’Œæ™®éæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21c4a07547e9fcf113103927f96985e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-762b6b482a2ddfcb341f8bf6a8f4d236.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-166497678d8c1a11a99f592021fecc67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-425bc78c86dcbfdf7e2832234d3740b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a711904875647a2ab6d12454ba1cd4aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68af2dafb7ad1f87b8a9dd739ba80ae9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models"><a href="#Probing-the-Representational-Power-of-Sparse-Autoencoders-in-Vision-Models" class="headerlink" title="Probing the Representational Power of Sparse Autoencoders in Vision   Models"></a>Probing the Representational Power of Sparse Autoencoders in Vision   Models</h2><p><strong>Authors:Matthew Lyle Olson, Musashi Hinck, Neale Ratzlaff, Changbai Li, Phillip Howard, Vasudev Lal, Shao-Yen Tseng</strong></p>
<p>Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting the hidden states of large language models (LLMs). By learning to reconstruct activations from a sparse bottleneck layer, SAEs discover interpretable features from the high-dimensional internal representations of LLMs. Despite their popularity with language models, SAEs remain understudied in the visual domain. In this work, we provide an extensive evaluation the representational power of SAEs for vision models using a broad range of image-based tasks. Our experimental results demonstrate that SAE features are semantically meaningful, improve out-of-distribution generalization, and enable controllable generation across three vision model architectures: vision embedding models, multi-modal LMMs and diffusion models. In vision embedding models, we find that learned SAE features can be used for OOD detection and provide evidence that they recover the ontological structure of the underlying model. For diffusion models, we demonstrate that SAEs enable semantic steering through text encoder manipulation and develop an automated pipeline for discovering human-interpretable attributes. Finally, we conduct exploratory experiments on multi-modal LLMs, finding evidence that SAE features reveal shared representations across vision and language modalities. Our study provides a foundation for SAE evaluation in vision models, highlighting their strong potential improving interpretability, generalization, and steerability in the visual domain. </p>
<blockquote>
<p>ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSparse Autoencoders, SAEsï¼‰ä½œä¸ºä¸€ç§å·¥å…·ï¼Œå·²ç»è¢«å¹¿æ³›åº”ç”¨äºè§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„éšè—çŠ¶æ€ã€‚é€šè¿‡å­¦ä¼šä»ç¨€ç–ç“¶é¢ˆå±‚é‡å»ºæ¿€æ´»ï¼ŒSAEä»LLMçš„é«˜ç»´å†…éƒ¨è¡¨ç¤ºä¸­å‘ç°å¯è§£é‡Šçš„ç‰¹å¾ã€‚å°½ç®¡SAEåœ¨è¯­è¨€æ¨¡å‹ä¸­å¾ˆå—æ¬¢è¿ï¼Œä½†åœ¨è§†è§‰é¢†åŸŸå®ƒä»¬çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹ä¸€ç³»åˆ—å›¾åƒä»»åŠ¡è¿›è¡Œå¹¿æ³›çš„è¯„ä¼°ï¼Œå…¨é¢è¯„ä¼°äº†SAEåœ¨è§†è§‰æ¨¡å‹ä¸­çš„è¡¨ç¤ºèƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSAEç‰¹å¾æ˜¯è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ï¼Œèƒ½æé«˜è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸‰ç§è§†è§‰æ¨¡å‹æ¶æ„ä¸­å®ç°äº†å¯æ§ç”Ÿæˆï¼šè§†è§‰åµŒå…¥æ¨¡å‹ã€å¤šæ¨¡æ€LLMå’Œæ‰©æ•£æ¨¡å‹ã€‚åœ¨è§†è§‰åµŒå…¥æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°å­¦ä¹ åˆ°çš„SAEç‰¹å¾å¯ç”¨äºOODæ£€æµ‹ï¼Œå¹¶æä¾›è¯æ®è¡¨æ˜å®ƒä»¬æ¢å¤äº†åº•å±‚æ¨¡å‹çš„æœ¬ä½“ç»“æ„ã€‚å¯¹äºæ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬è¯æ˜äº†SAEé€šè¿‡æ–‡æœ¬ç¼–ç å™¨æ“ä½œå®ç°è¯­ä¹‰å¼•å¯¼ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“æ¥å‘ç°äººç±»å¯è§£é‡Šçš„å±æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹å¤šæ¨¡æ€LLMsè¿›è¡Œäº†æ¢ç´¢æ€§å®éªŒï¼Œå‘ç°è¯æ®è¡¨æ˜SAEç‰¹å¾æ­ç¤ºäº†è·¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„å…±äº«è¡¨ç¤ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºè§†è§‰æ¨¡å‹ä¸­SAEçš„è¯„ä¼°å¥ å®šäº†åŸºç¡€ï¼Œçªæ˜¾äº†å®ƒä»¬åœ¨æé«˜è§†è§‰é¢†åŸŸçš„å¯è§£é‡Šæ€§ã€æ³›åŒ–èƒ½åŠ›å’Œå¯æ§æ€§æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11277v1">PDF</a> ICCV 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰åœ¨è§£è¯»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éšè—çŠ¶æ€æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹SAEåœ¨è§†è§‰æ¨¡å‹ä¸­çš„è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºSAEç‰¹å¾å…·æœ‰è¯­ä¹‰æ„ä¹‰ï¼Œèƒ½æé«˜æ¨¡å‹åœ¨åˆ†å¸ƒå¤–çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½æ§åˆ¶ä¸‰ç§è§†è§‰æ¨¡å‹çš„ç”Ÿæˆï¼šè§†è§‰åµŒå…¥æ¨¡å‹ã€å¤šæ¨¡æ€LLMå’Œæ‰©æ•£æ¨¡å‹ã€‚SAEç‰¹å¾å¯ç”¨äºå¼‚å¸¸æ£€æµ‹å¹¶æ­ç¤ºåº•å±‚æ¨¡å‹çš„æœ¬ä½“ç»“æ„ã€‚å¯¹äºæ‰©æ•£æ¨¡å‹ï¼ŒSAEèƒ½å¤Ÿå®ç°è¯­ä¹‰æ§åˆ¶ï¼Œå¹¶å‘ç°äººç±»å¯è§£é‡Šçš„å±æ€§ã€‚åœ¨å¤šæ¨¡æ€LLMä¸Šçš„å®éªŒè¡¨æ˜SAEç‰¹å¾æ­ç¤ºäº†è·¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„å…±äº«è¡¨ç¤ºã€‚ç ”ç©¶ä¸ºSAEåœ¨è§†è§‰æ¨¡å‹ä¸­çš„åº”ç”¨å¥ å®šäº†åšå®çš„åŸºç¡€ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æé«˜å¯è§£é‡Šæ€§ã€æ³›åŒ–èƒ½åŠ›å’Œå¯æ§åˆ¶æ€§æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰åœ¨è§£è¯»å¤§å‹è¯­è¨€æ¨¡å‹éšè—çŠ¶æ€æ–¹é¢å…·æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>åœ¨è§†è§‰æ¨¡å‹ä¸­ï¼ŒSAEçš„ç‰¹å¾å±•ç°å‡ºè¯­ä¹‰æ„ä¹‰ã€‚</li>
<li>SAEèƒ½æé«˜æ¨¡å‹åœ¨åˆ†å¸ƒå¤–çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>SAEèƒ½æ§åˆ¶è§†è§‰æ¨¡å‹çš„ç”Ÿæˆï¼ŒåŒ…æ‹¬è§†è§‰åµŒå…¥æ¨¡å‹ã€å¤šæ¨¡æ€LLMå’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>SAEç‰¹å¾åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶èƒ½æ­ç¤ºåº•å±‚æ¨¡å‹çš„æœ¬ä½“ç»“æ„ã€‚</li>
<li>å¯¹äºæ‰©æ•£æ¨¡å‹ï¼ŒSAEèƒ½å¤Ÿå®ç°è¯­ä¹‰æ§åˆ¶å¹¶å‘ç°äººç±»å¯è§£é‡Šçš„å±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3a0ba3306cd376669226e662340c7001.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-293435352c82ecc17716ef4dc32ee235.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-215665ef1e319f90fe6981ae419d6718.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57ab852d4cc8d86c82ecb3868e4a88f8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Generalized-Decoupled-Learning-for-Enhancing-Open-Vocabulary-Dense-Perception"><a href="#Generalized-Decoupled-Learning-for-Enhancing-Open-Vocabulary-Dense-Perception" class="headerlink" title="Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense   Perception"></a>Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense   Perception</h2><p><strong>Authors:Junjie Wang, Keyu Chen, Yulin Li, Bin Chen, Hengshuang Zhao, Xiaojuan Qi, Zhuotao Tian</strong></p>
<p>Dense visual perception tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense perception often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIPâ€™s image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain <code>content&#39;&#39; and </code>contextâ€™â€™ features respectively. \revise{The context features are enhanced by jointly distilling semantic correlations from Vision Foundation Models (VFMs) and object integrity cues from diffusion models, thereby enhancing spatial consistency. In parallel, the content features are aligned with image crop representations and constrained by region correlations from VFMs to improve local discriminability. Extensive experiments demonstrate that DeCLIP establishes a solid foundation for open-vocabulary dense perception, consistently achieving state-of-the-art performance across a broad spectrum of tasks, including 2D detection and segmentation, 3D instance segmentation, video instance segmentation, and 6D object pose estimation.} Code is available at <a target="_blank" rel="noopener" href="https://github.com/xiaomoguhz/DeCLIP">https://github.com/xiaomoguhz/DeCLIP</a> </p>
<blockquote>
<p>å¯†é›†è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸€ç›´å—åˆ°å…¶ä¾èµ–äºé¢„å®šä¹‰ç±»åˆ«çš„é™åˆ¶ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨è§†è§‰æ¦‚å¿µæ— ç•Œçš„ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚è™½ç„¶åƒCLIPè¿™æ ·çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¼€æ”¾è¯æ±‡ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œä½†å®ƒä»¬ç›´æ¥åº”ç”¨äºå¯†é›†æ„ŸçŸ¥å¾€å¾€ä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬åœ¨å±€éƒ¨ç‰¹å¾è¡¨ç¤ºæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°CLIPçš„å›¾åƒä»¤ç‰Œåœ¨æœ‰æ•ˆåœ°èšåˆç©ºé—´æˆ–è¯­ä¹‰ç›¸å…³åŒºåŸŸçš„ä¿¡æ¯æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´ç‰¹å¾ç¼ºä¹å±€éƒ¨é‰´åˆ«åŠ›å’Œç©ºé—´ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeCLIPï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡è§£è€¦è‡ªæ³¨æ„åŠ›æ¨¡å—æ¥å¢å¼ºCLIPçš„æ–°å‹æ¡†æ¶ï¼Œä»è€Œåˆ†åˆ«è·å¾—â€œå†…å®¹â€å’Œâ€œä¸Šä¸‹æ–‡â€ç‰¹å¾ã€‚é€šè¿‡è”åˆè’¸é¦æ¥è‡ªè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰çš„è¯­ä¹‰å…³è”å’Œæ¥è‡ªæ‰©æ•£æ¨¡å‹çš„å¯¹è±¡å®Œæ•´æ€§çº¿ç´¢ï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡ç‰¹å¾çš„è¡¨ç¤ºï¼Œä»è€Œæé«˜äº†ç©ºé—´ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œå†…å®¹ç‰¹å¾ä¸å›¾åƒè£å‰ªè¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œå¹¶é€šè¿‡VFMsçš„åŒºåŸŸå…³è”è¿›è¡Œçº¦æŸï¼Œä»¥æé«˜å±€éƒ¨é‰´åˆ«åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDeCLIPä¸ºå¼€æ”¾è¯æ±‡å¯†é›†æ„ŸçŸ¥å»ºç«‹äº†åšå®çš„åŸºç¡€ï¼Œåœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬äºŒç»´æ£€æµ‹å’Œåˆ†å‰²ã€ä¸‰ç»´å®ä¾‹åˆ†å‰²ã€è§†é¢‘å®ä¾‹åˆ†å‰²å’Œå…­ç»´å¯¹è±¡å§¿æ€ä¼°è®¡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiaomoguhz/DeCLIP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiaomoguhz/DeCLIPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11256v1">PDF</a> arXiv admin note: text overlap with arXiv:2505.04410</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„åº”ç”¨å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç”±äºä¼ ç»Ÿçš„è§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¾èµ–äºé¢„å®šä¹‰çš„ç±»åˆ«ï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DeCLIPæ¡†æ¶ï¼Œé€šè¿‡è§£è€¦CLIPæ¨¡å‹çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå¢å¼ºç©ºé—´ä¸€è‡´æ€§å’Œå±€éƒ¨è¾¨åˆ«åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒDeCLIPåœ¨å¼€æ”¾è¯æ±‡è¡¨å¯†é›†æ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼ŒåŒ…æ‹¬äºŒç»´æ£€æµ‹ä¸åˆ†å‰²ã€ä¸‰ç»´å®ä¾‹åˆ†å‰²ã€è§†é¢‘å®ä¾‹åˆ†å‰²å’Œå…­è‡ªç”±åº¦ç‰©ä½“å§¿æ€ä¼°è®¡ç­‰ä»»åŠ¡ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿè§†è§‰æ„ŸçŸ¥ä»»åŠ¡å—é™äºé¢„å®šä¹‰ç±»åˆ«ï¼Œéš¾ä»¥é€‚åº”çœŸå®ä¸–ç•Œåœºæ™¯ã€‚</li>
<li>CLIPæ¨¡å‹åœ¨å¼€æ”¾è¯æ±‡è¡¨ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨å¯†é›†æ„ŸçŸ¥ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>CLIPçš„å›¾åƒæ ‡è®°åœ¨èšåˆç©ºé—´æˆ–è¯­ä¹‰ç›¸å…³åŒºåŸŸçš„ä¿¡æ¯æ—¶å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´ç‰¹å¾ç¼ºä¹å±€éƒ¨è¾¨åˆ«åŠ›å’Œç©ºé—´ä¸€è‡´æ€§ã€‚</li>
<li>DeCLIPæ¡†æ¶é€šè¿‡è§£è€¦CLIPæ¨¡å‹çš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ†åˆ«è·å¾—â€œå†…å®¹â€å’Œâ€œä¸Šä¸‹æ–‡â€ç‰¹å¾ï¼Œä»¥å¢å¼ºç©ºé—´ä¸€è‡´æ€§å’Œå±€éƒ¨è¾¨åˆ«åŠ›ã€‚</li>
<li>ä¸Šä¸‹æ–‡ç‰¹å¾é€šè¿‡è”åˆæç‚¼Vision Foundation Modelsï¼ˆVFMsï¼‰çš„è¯­ä¹‰å…³è”å’Œæ‰©æ•£æ¨¡å‹çš„ç‰©ä½“å®Œæ•´æ€§çº¿ç´¢ï¼Œä»è€Œæé«˜ç©ºé—´ä¸€è‡´æ€§ã€‚</li>
<li>å†…å®¹ç‰¹å¾é€šè¿‡ä¸å›¾åƒè£å‰ªè¡¨ç¤ºå¯¹é½ï¼Œå¹¶å—VFMsçš„åŒºåŸŸå…³è”çº¦æŸï¼Œä»¥æé«˜å±€éƒ¨è¾¨åˆ«åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c32fe7766ce0ed33b8d95b6f90bca6c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e2fc63e3cb1c8563cf9e0e7f92f941c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed03ba79bab959383fbd333a346aa371.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2483114acdcc5cb4f09a472d43cc987.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cb66ae415167451204a886f2e78ee14.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Efficient-Image-to-Image-Schrodinger-Bridge-for-CT-Field-of-View-Extension"><a href="#Efficient-Image-to-Image-Schrodinger-Bridge-for-CT-Field-of-View-Extension" class="headerlink" title="Efficient Image-to-Image SchrÃ¶dinger Bridge for CT Field of View   Extension"></a>Efficient Image-to-Image SchrÃ¶dinger Bridge for CT Field of View   Extension</h2><p><strong>Authors:Zhenhao Li, Long Yang, Xiaojie Yin, Haijun Yu, Jiazhou Wang, Hongbin Han, Weigang Hu, Yixing Huang</strong></p>
<p>Computed tomography (CT) is a cornerstone imaging modality for non-invasive, high-resolution visualization of internal anatomical structures. However, when the scanned object exceeds the scannerâ€™s field of view (FOV), projection data are truncated, resulting in incomplete reconstructions and pronounced artifacts near FOV boundaries. Conventional reconstruction algorithms struggle to recover accurate anatomy from such data, limiting clinical reliability. Deep learning approaches have been explored for FOV extension, with diffusion generative models representing the latest advances in image synthesis. Yet, conventional diffusion models are computationally demanding and slow at inference due to their iterative sampling process. To address these limitations, we propose an efficient CT FOV extension framework based on the image-to-image Schr&quot;odinger Bridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that synthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic mapping between paired limited-FOV and extended-FOV images. This direct correspondence yields a more interpretable and traceable generative process, enhancing anatomical consistency and structural fidelity in reconstructions. I$^2$SB achieves superior quantitative performance, with root-mean-square error (RMSE) values of 49.8,HU on simulated noisy data and 152.0HU on real data, outperforming state-of-the-art diffusion models such as conditional denoising diffusion probabilistic models (cDDPM) and patch-based diffusion methods. Moreover, its one-step inference enables reconstruction in just 0.19s per 2D slice, representing over a 700-fold speedup compared to cDDPM (135s) and surpassing diffusionGAN (0.58s), the second fastest. This combination of accuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical deployment. </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ˜¯å¯¹äºæ— åˆ›ã€é«˜åˆ†è¾¨ç‡å¯è§†åŒ–å†…éƒ¨è§£å‰–ç»“æ„çš„é‡è¦æˆåƒæ–¹å¼ã€‚ç„¶è€Œï¼Œå½“æ‰«æå¯¹è±¡è¶…å‡ºæ‰«æä»ªçš„è§†é‡ï¼ˆFOVï¼‰æ—¶ï¼ŒæŠ•å½±æ•°æ®ä¼šè¢«æˆªæ–­ï¼Œå¯¼è‡´é‡å»ºä¸å®Œæ•´ï¼Œå¹¶ä¸”åœ¨FOVè¾¹ç•Œé™„è¿‘å‡ºç°æ˜æ˜¾çš„ä¼ªå½±ã€‚ä¼ ç»Ÿçš„é‡å»ºç®—æ³•å¾ˆéš¾ä»è¿™ç§æ•°æ®ä¸­æ¢å¤å‡†ç¡®çš„è§£å‰–ç»“æ„ï¼Œä»è€Œé™åˆ¶äº†ä¸´åºŠå¯é æ€§ã€‚æ·±åº¦å­¦ä¹ å·²åº”ç”¨äºFOVæ‰©å±•ï¼Œæ‰©æ•£ç”Ÿæˆæ¨¡å‹ä»£è¡¨å›¾åƒåˆæˆçš„æœ€æ–°è¿›å±•ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹è®¡ç®—é‡å¤§ï¼Œç”±äºè¿­ä»£é‡‡æ ·è¿‡ç¨‹ï¼Œæ¨ç†é€Ÿåº¦æ…¢ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒåˆ°å›¾åƒSchrÃ¶dinger Bridgeï¼ˆI$^2$SBï¼‰æ‰©æ•£æ¨¡å‹çš„CT FOVæ‰©å±•æ¡†æ¶ã€‚ä¸åŒäºä»çº¯é«˜æ–¯å™ªå£°åˆæˆå›¾åƒçš„ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼ŒI$^2$SBå­¦ä¹ é…å¯¹æœ‰é™FOVå’Œæ‰©å±•FOVå›¾åƒä¹‹é—´çš„ç›´æ¥éšæœºæ˜ å°„ã€‚è¿™ç§ç›´æ¥å¯¹åº”å…³ç³»äº§ç”Ÿäº†ä¸€ä¸ªæ›´å…·å¯è§£é‡Šæ€§å’Œå¯è¿½æº¯æ€§çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæé«˜äº†é‡å»ºä¸­çš„è§£å‰–ç»“æ„ä¸€è‡´æ€§å’Œç»“æ„ä¿çœŸåº¦ã€‚åœ¨æ¨¡æ‹Ÿå™ªå£°æ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šï¼ŒI$^2$SBå–å¾—äº†ä¼˜å¼‚çš„å®šé‡æ€§èƒ½ï¼Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å€¼åˆ†åˆ«ä¸º49.8 HUå’Œ152.0 HUï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ï¼Œå¦‚æ¡ä»¶å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆcDDPMï¼‰å’ŒåŸºäºè¡¥ä¸çš„æ‰©æ•£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå…¶ä¸€æ­¥æ¨ç†åªéœ€0.19ç§’å³å¯å®Œæˆæ¯ä¸ª2Dåˆ‡ç‰‡çš„é‡å»ºï¼Œä¸cDDPMç›¸æ¯”å®ç°äº†è¶…è¿‡700å€çš„åŠ é€Ÿï¼ˆ135ç§’ï¼‰ï¼Œå¹¶è¶…è¶Šäº†æ‰©æ•£GANï¼ˆ0.58ç§’ï¼‰ï¼Œæˆä¸ºç¬¬äºŒå¿«çš„æ¨¡å‹ã€‚I$^2$SBçš„å‡†ç¡®æ€§å’Œé«˜æ•ˆæ€§ç›¸ç»“åˆï¼Œä½¿å…¶æˆä¸ºå®æ—¶æˆ–ä¸´åºŠéƒ¨ç½²çš„ç†æƒ³é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11211v1">PDF</a> 10 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­ï¼Œå½“æ‰«æå¯¹è±¡è¶…å‡ºæ‰«æä»ªè§†é‡ï¼ˆFOVï¼‰æ—¶ï¼ŒæŠ•å½±æ•°æ®ä¼šè¢«æˆªæ–­ï¼Œå¯¼è‡´é‡å»ºä¸å®Œæ•´å’Œåœ¨è§†é‡è¾¹ç•Œé™„è¿‘å‡ºç°æ˜æ˜¾çš„ä¼ªå½±ã€‚åŸºäºå›¾åƒåˆ°å›¾åƒçš„SchrÃ¶dinger Bridgeï¼ˆI$^2$SBï¼‰æ‰©æ•£æ¨¡å‹çš„CT FOVæ‰©å±•æ¡†æ¶è§£å†³äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹è®¡ç®—é‡å¤§ã€æ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚I$^2$SBç›´æ¥ä»é…å¯¹çš„æœ‰é™è§†é‡å’Œæ‰©å±•è§†é‡å›¾åƒä¸­å­¦ä¹ éšæœºæ˜ å°„ï¼Œæé«˜äº†é‡å»ºçš„è§£å‰–ä¸€è‡´æ€§å’Œç»“æ„ä¿çœŸåº¦ã€‚å…¶åœ¨æ¨¡æ‹Ÿå™ªå£°æ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šçš„å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å€¼åˆ†åˆ«ä¸º49.8 HUå’Œ152.0 HUï¼Œä¼˜äºæ¡ä»¶å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆcDDPMï¼‰å’ŒåŸºäºè¡¥ä¸çš„æ‰©æ•£æ–¹æ³•ã€‚å…¶ä¸€æ­¥æ¨ç†å¯åœ¨0.19ç§’å†…å®Œæˆæ¯ä¸ªäºŒç»´åˆ‡ç‰‡çš„é‡å»ºï¼Œæ¯”cDDPMå¿«700å€ä»¥ä¸Šï¼Œå¹¶è¶…è¶Šäº†ç¬¬äºŒå¿«çš„diffusionGANï¼ˆ0.58ç§’ï¼‰ã€‚ç»“åˆç²¾åº¦å’Œæ•ˆç‡ï¼ŒI$^2$SBéå¸¸é€‚åˆå®æ—¶æˆ–ä¸´åºŠéƒ¨ç½²ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“CTæ‰«æå¯¹è±¡è¶…å‡ºæ‰«æä»ªè§†é‡ï¼ˆFOVï¼‰æ—¶ï¼Œä¼šå‡ºç°æ•°æ®æˆªæ–­é—®é¢˜ï¼Œå¯¼è‡´é‡å»ºä¸å‡†ç¡®ã€‚</li>
<li>ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>I$^2$SBæ‰©æ•£æ¨¡å‹é€šè¿‡ç›´æ¥ä»é…å¯¹çš„æœ‰é™è§†é‡å’Œæ‰©å±•è§†é‡å›¾åƒä¸­å­¦ä¹ éšæœºæ˜ å°„ï¼Œæé«˜äº†é‡å»ºçš„è§£å‰–ä¸€è‡´æ€§å’Œç»“æ„ä¿çœŸåº¦ã€‚</li>
<li>I$^2$SBåœ¨æ¨¡æ‹Ÿå™ªå£°æ•°æ®å’ŒçœŸå®æ•°æ®ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ‰©æ•£æ¨¡å‹ï¼Œå¦‚cDDPMå’ŒåŸºäºè¡¥ä¸çš„æ‰©æ•£æ–¹æ³•ã€‚</li>
<li>I$^2$SBå…·æœ‰å¿«é€Ÿçš„ä¸€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œå¯åœ¨çŸ­æ—¶é—´å†…å®Œæˆé‡å»ºï¼Œæ¯”cDDPMå¿«700å€ä»¥ä¸Šã€‚</li>
<li>I$^2$SBç»“åˆé«˜ç²¾åº¦å’Œé«˜æ•ˆæ€§ï¼Œéå¸¸é€‚åˆå®æ—¶æˆ–ä¸´åºŠéƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-047017924f88b3347ff76624dda29dd8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f92569f85bc980c94d4e96cb334630e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e34671900878bc4c011604e8d58031b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d5d64fdaa4aa22b8c2ff0e48cf8fe3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4082f0cf62b6fcfb96244c14a6b0918.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-581c6dc2eb18a689e4ea013755599730.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-670b8777e33b37b800bb24dfa990253b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation"><a href="#StyleMM-Stylized-3D-Morphable-Face-Model-via-Text-Driven-Aligned-Image-Translation" class="headerlink" title="StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image   Translation"></a>StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image   Translation</h2><p><strong>Authors:Seungmi Lee, Kwan Yun, Junyong Noh</strong></p>
<p>We introduce StyleMM, a novel framework that can construct a stylized 3D Morphable Model (3DMM) based on user-defined text descriptions specifying a target style. Building upon a pre-trained mesh deformation network and a texture generator for original 3DMM-based realistic human faces, our approach fine-tunes these models using stylized facial images generated via text-guided image-to-image (i2i) translation with a diffusion model, which serve as stylization targets for the rendered mesh. To prevent undesired changes in identity, facial alignment, or expressions during i2i translation, we introduce a stylization method that explicitly preserves the facial attributes of the source image. By maintaining these critical attributes during image stylization, the proposed approach ensures consistent 3D style transfer across the 3DMM parameter space through image-based training. Once trained, StyleMM enables feed-forward generation of stylized face meshes with explicit control over shape, expression, and texture parameters, producing meshes with consistent vertex connectivity and animatability. Quantitative and qualitative evaluations demonstrate that our approach outperforms state-of-the-art methods in terms of identity-level facial diversity and stylization capability. The code and videos are available at <a href="kwanyun.github.io/stylemm_page">kwanyun.github.io&#x2F;stylemm_page</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†StyleMMï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·å®šä¹‰çš„æ–‡æœ¬æè¿°æ„å»ºé£æ ¼åŒ–çš„3Då¯å˜å½¢æ¨¡å‹ï¼ˆ3DMMï¼‰ï¼Œè¿™äº›æè¿°æŒ‡å®šäº†ç›®æ ‡é£æ ¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨é¢„è®­ç»ƒçš„ç½‘æ ¼å˜å½¢ç½‘ç»œå’Œç”¨äºåŸå§‹åŸºäº3DMMçš„é€¼çœŸäººè„¸çš„çº¹ç†ç”Ÿæˆå™¨ä¹‹ä¸Šã€‚é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒï¼ˆi2iï¼‰ç¿»è¯‘ç”Ÿæˆé£æ ¼åŒ–çš„é¢éƒ¨å›¾åƒï¼Œå¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¿™äº›å›¾åƒä½œä¸ºæ¸²æŸ“ç½‘æ ¼çš„é£æ ¼åŒ–ç›®æ ‡ã€‚ä¸ºäº†é˜²æ­¢åœ¨i2iç¿»è¯‘è¿‡ç¨‹ä¸­å‡ºç°èº«ä»½ã€é¢éƒ¨å¯¹é½æˆ–è¡¨æƒ…ç­‰ä¸å¿…è¦çš„å˜åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ˜¾å¼ä¿ç•™æºå›¾åƒé¢éƒ¨ç‰¹å¾çš„é£æ ¼åŒ–æ–¹æ³•ã€‚é€šè¿‡åœ¨å›¾åƒé£æ ¼åŒ–è¿‡ç¨‹ä¸­ä¿æŒè¿™äº›å…³é”®ç‰¹å¾ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ç¡®ä¿äº†é€šè¿‡åŸºäºå›¾åƒçš„è®­ç»ƒåœ¨æ•´ä¸ª3DMMå‚æ•°ç©ºé—´å†…è¿›è¡Œä¸€è‡´çš„3Dé£æ ¼è½¬æ¢ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼ŒStyleMMå°±èƒ½å¤Ÿä»¥å‰é¦ˆæ–¹å¼ç”Ÿæˆå…·æœ‰å½¢çŠ¶ã€è¡¨æƒ…å’Œçº¹ç†å‚æ•°æ˜ç¡®æ§åˆ¶çš„é£æ ¼åŒ–é¢éƒ¨ç½‘æ ¼ï¼Œäº§ç”Ÿå…·æœ‰ä¸€è‡´é¡¶ç‚¹è¿æ¥å’ŒåŠ¨ç”»èƒ½åŠ›çš„ç½‘æ ¼ã€‚å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨èº«ä»½çº§é¢éƒ¨å¤šæ ·æ€§å’Œé£æ ¼åŒ–èƒ½åŠ›æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å’Œè§†é¢‘å¯åœ¨<a href="kwanyun.github.io/stylemm_page">kwanyun.github.io&#x2F;stylemm_page</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11203v1">PDF</a> Pacific graphics 2025, CGF, 15 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”¨æˆ·å®šä¹‰æ–‡æœ¬æè¿°çš„ç›®æ ‡é£æ ¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†StyleMMæ¡†æ¶ï¼Œæ„å»ºäº†ä¸€ä¸ªé£æ ¼åŒ–çš„3Då¯å˜å½¢æ¨¡å‹ï¼ˆ3DMMï¼‰ã€‚é€šè¿‡é¢„è®­ç»ƒçš„ç½‘æ ¼å˜å½¢ç½‘ç»œå’ŒåŸå§‹3DMMç°å®äººè„¸çš„çº¹ç†ç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨é€šè¿‡æ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒï¼ˆi2iï¼‰ç¿»è¯‘ä¸æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é£æ ¼åŒ–é¢éƒ¨å›¾åƒå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¿™äº›å›¾åƒä½œä¸ºé£æ ¼åŒ–çš„ç›®æ ‡ç½‘æ ¼ã€‚ä¸ºäº†åœ¨è¿›è¡Œi2iç¿»è¯‘æ—¶é˜²æ­¢èº«ä»½ã€é¢éƒ¨å¯¹é½æˆ–è¡¨æƒ…ç­‰ä¸éœ€è¦çš„æ›´æ”¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é£æ ¼åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯æ˜¾å¼ä¿ç•™æºå›¾åƒçš„é¢éƒ¨å±æ€§ã€‚é€šè¿‡ç»´æŒè¿™äº›å…³é”®å±æ€§è¿›è¡Œå›¾åƒé£æ ¼åŒ–ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ç¡®ä¿äº†é€šè¿‡å›¾åƒè®­ç»ƒåœ¨3DMMå‚æ•°ç©ºé—´ä¸­çš„ä¸€è‡´é£æ ¼è½¬ç§»ã€‚è®­ç»ƒå®Œæˆåï¼ŒStyleMMå¯ä»¥é€šè¿‡å‰é¦ˆç”Ÿæˆå…·æœ‰å½¢çŠ¶ã€è¡¨æƒ…å’Œçº¹ç†å‚æ•°æ˜ç¡®æ§åˆ¶çš„é£æ ¼åŒ–é¢éƒ¨ç½‘æ ¼ï¼Œäº§ç”Ÿå…·æœ‰ä¸€è‡´é¡¶ç‚¹è¿æ¥å’ŒåŠ¨ç”»èƒ½åŠ›çš„ç½‘æ ¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StyleMMæ˜¯ä¸€ä¸ªå¯ä»¥æ ¹æ®ç”¨æˆ·å®šä¹‰çš„æ–‡æœ¬æè¿°æ„å»ºé£æ ¼åŒ–3Då¯å˜å½¢æ¨¡å‹ï¼ˆ3DMMï¼‰çš„æ¡†æ¶ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆé¢„è®­ç»ƒçš„ç½‘æ ¼å˜å½¢ç½‘ç»œå’Œçº¹ç†ç”Ÿæˆå™¨ï¼Œé€šè¿‡æ–‡æœ¬å¼•å¯¼çš„i2iç¿»è¯‘ä¸æ‰©æ•£æ¨¡å‹ç”Ÿæˆé£æ ¼åŒ–é¢éƒ¨å›¾åƒã€‚</li>
<li>ä¸ºäº†ä¿æŒèº«ä»½ä¸€è‡´æ€§ï¼Œæå‡ºäº†ä¸€ç§ä¿ç•™æºå›¾åƒé¢éƒ¨å±æ€§çš„é£æ ¼åŒ–æ–¹æ³•ã€‚</li>
<li>StyleMMèƒ½åœ¨3DMMå‚æ•°ç©ºé—´å†…è¿›è¡Œä¸€è‡´çš„é£æ ¼è½¬ç§»ã€‚</li>
<li>è®­ç»ƒåçš„StyleMMå¯ä»¥ç”Ÿæˆå…·æœ‰å½¢çŠ¶ã€è¡¨è¾¾å’Œçº¹ç†å‚æ•°æ§åˆ¶çš„é£æ ¼åŒ–é¢éƒ¨ç½‘æ ¼ã€‚</li>
<li>è¿™äº›ç”Ÿæˆçš„ç½‘æ ¼å…·æœ‰ä¸€è‡´çš„é¡¶ç‚¹è¿æ¥å’ŒåŠ¨ç”»èƒ½åŠ›ã€‚</li>
<li>å®šé‡å’Œå®šæ€§è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢éƒ¨å¤šæ ·æ€§å’Œé£æ ¼åŒ–èƒ½åŠ›æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-24c4153a81a2390528acff0042cef047.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3df3c699501d1a68ed0de5fc01b76a00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aaff26d9a31e9a524be9e3ff7806462.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09a9b2ba661fc3a1f6f83a08284c82b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11bd7253b70daf1ebe6589dcfb09b81d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Semi-supervised-Image-Dehazing-via-Expectation-Maximization-and-Bidirectional-Brownian-Bridge-Diffusion-Models"><a href="#Semi-supervised-Image-Dehazing-via-Expectation-Maximization-and-Bidirectional-Brownian-Bridge-Diffusion-Models" class="headerlink" title="Semi-supervised Image Dehazing via Expectation-Maximization and   Bidirectional Brownian Bridge Diffusion Models"></a>Semi-supervised Image Dehazing via Expectation-Maximization and   Bidirectional Brownian Bridge Diffusion Models</h2><p><strong>Authors:Bing Liu, Le Wang, Mingming Liu, Hao Liu, Rui Yao, Yong Zhou, Peng Liu, Tongqiang Xia</strong></p>
<p>Existing dehazing methods deal with real-world haze images with difficulty, especially scenes with thick haze. One of the main reasons is the lack of real-world paired data and robust priors. To avoid the costly collection of paired hazy and clear images, we propose an efficient semi-supervised image dehazing method via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models (EM-B3DM) with a two-stage learning scheme. In the first stage, we employ the EM algorithm to decouple the joint distribution of paired hazy and clear images into two conditional distributions, which are then modeled using a unified Brownian Bridge diffusion model to directly capture the structural and content-related correlations between hazy and clear images. In the second stage, we leverage the pre-trained model and large-scale unpaired hazy and clear images to further improve the performance of image dehazing. Additionally, we introduce a detail-enhanced Residual Difference Convolution block (RDC) to capture gradient-level information, significantly enhancing the modelâ€™s representation capability. Extensive experiments demonstrate that our EM-B3DM achieves superior or at least comparable performance to state-of-the-art methods on both synthetic and real-world datasets. </p>
<blockquote>
<p>ç°æœ‰çš„å»é›¾æ–¹æ³•åœ¨åº”å¯¹çœŸå®ä¸–ç•Œçš„é›¾éœ¾å›¾åƒæ—¶é¢ä¸´å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨æµ“åšé›¾éœ¾çš„åœºæ™¯ä¸­ã€‚ä¸»è¦åŸå› ä¹‹ä¸€æ˜¯ç¼ºä¹çœŸå®çš„é…å¯¹æ•°æ®å’Œç¨³å¥çš„å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºäº†é¿å…æ”¶é›†é…å¯¹é›¾å¤©å’Œæ™´æœ—å¤©æ°”çš„å›¾åƒçš„é«˜æˆæœ¬ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åŠç›‘ç£å›¾åƒå»é›¾æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡æœŸæœ›æœ€å¤§åŒ–ä¸åŒå‘å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹ï¼ˆEM-B3DMï¼‰è¿›è¡Œä¸¤é˜¶æ®µå­¦ä¹ æ–¹æ¡ˆã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨EMç®—æ³•å°†é…å¯¹é›¾å¤©å’Œæ™´æœ—å¤©æ°”çš„å›¾åƒçš„è”åˆåˆ†å¸ƒè§£è€¦ä¸ºä¸¤ä¸ªæ¡ä»¶åˆ†å¸ƒï¼Œç„¶åä½¿ç”¨ç»Ÿä¸€çš„å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹å¯¹å®ƒä»¬è¿›è¡Œå»ºæ¨¡ï¼Œä»¥ç›´æ¥æ•è·é›¾å¤©å’Œæ™´æœ—å¤©æ°”çš„å›¾åƒä¹‹é—´çš„ç»“æ„å’Œå†…å®¹ç›¸å…³å…³è”ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å’Œå¤§è§„æ¨¡çš„æœªé…å¯¹é›¾å¤©å’Œæ™´æœ—å¤©æ°”çš„å›¾åƒè¿›ä¸€æ­¥æé«˜å›¾åƒå»é›¾çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç»†èŠ‚å¢å¼ºçš„æ®‹å·®å·®åˆ†å·ç§¯å—ï¼ˆRDCï¼‰ï¼Œä»¥æ•è·æ¢¯åº¦çº§åˆ«çš„ä¿¡æ¯ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„EM-B3DMåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½å‡è¾¾åˆ°æˆ–è‡³å°‘ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11165v1">PDF</a> 10 pages, 4 figures</p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹ç°æœ‰å»é›¾æ–¹æ³•å¤„ç†çœŸå®ä¸–ç•Œé›¾å›¾æ—¶çš„å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœºæ™¯æµ“åšé›¾çš„æƒ…å†µï¼Œæå‡ºä¸€ç§åŸºäºæœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰å’ŒåŒå‘å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹ï¼ˆEM-B3DMï¼‰çš„åŠç›‘ç£å›¾åƒå»é›¾æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨EMç®—æ³•å°†é…å¯¹é›¾å›¾å’Œæ¸…æ™°å›¾çš„è”åˆåˆ†å¸ƒè§£è€¦ä¸ºä¸¤ä¸ªæ¡ä»¶åˆ†å¸ƒï¼Œå¹¶ä½¿ç”¨ç»Ÿä¸€çš„å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹ç›´æ¥æ•æ‰ä¸¤è€…ä¹‹é—´çš„ç»“æ„å’Œå†…å®¹ç›¸å…³æ€§ã€‚å€ŸåŠ©é¢„è®­ç»ƒæ¨¡å‹å’Œå¤§è§„æ¨¡æœªé…å¯¹é›¾å›¾å’Œæ¸…æ™°å›¾åƒï¼Œè¿›ä¸€æ­¥æé«˜å›¾åƒå»é›¾æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¼•å…¥ç»†èŠ‚å¢å¼ºçš„æ®‹å·®å·®åˆ†å·ç§¯å—ï¼ˆRDCï¼‰ï¼Œä»¥æ•æ‰æ¢¯åº¦çº§ä¿¡æ¯ï¼Œæ˜¾è‘—æé«˜æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒEM-B3DMåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½å‡ä¼˜äºæˆ–è‡³å°‘ä¸ç°æœ‰å…ˆè¿›æ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰å»é›¾æ–¹æ³•åœ¨å¤„ç†çœŸå®ä¸–ç•Œï¼Œç‰¹åˆ«æ˜¯åœºæ™¯æµ“åšé›¾çš„æƒ…å†µä¸‹å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç¼ºä¹çœŸå®ä¸–ç•Œé…å¯¹æ•°æ®å’Œç¨³å¥å…ˆéªŒæ˜¯ä¸»è¦åŸå› ä¹‹ä¸€ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºæœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰å’ŒåŒå‘å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹ï¼ˆEM-B3DMï¼‰çš„åŠç›‘ç£å›¾åƒå»é›¾æ–¹æ³•ï¼ŒåŒ…å«ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µä½¿ç”¨EMç®—æ³•è§£è€¦é…å¯¹é›¾å›¾å’Œæ¸…æ™°å›¾çš„è”åˆåˆ†å¸ƒã€‚</li>
<li>ä½¿ç”¨ç»Ÿä¸€çš„å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹æ•æ‰é›¾å›¾å’Œæ¸…æ™°å›¾ä¹‹é—´çš„ç»“æ„å’Œå†…å®¹ç›¸å…³æ€§ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å’Œå¤§è§„æ¨¡æœªé…å¯¹å›¾åƒè¿›ä¸€æ­¥æé«˜å»é›¾æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11165">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49f1832fe4cc2e48a0995a318f12dffa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d27fcc478ac59561a6bc6aa77a17dd88.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-793cc006492ccc52ed9d0bc5e8baf961.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb4f0fa83a118064598910bffd329c96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93b16e64b34c8c36a65e45167269db3d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Residual-based-Efficient-Bidirectional-Diffusion-Model-for-Image-Dehazing-and-Haze-Generation"><a href="#Residual-based-Efficient-Bidirectional-Diffusion-Model-for-Image-Dehazing-and-Haze-Generation" class="headerlink" title="Residual-based Efficient Bidirectional Diffusion Model for Image   Dehazing and Haze Generation"></a>Residual-based Efficient Bidirectional Diffusion Model for Image   Dehazing and Haze Generation</h2><p><strong>Authors:Bing Liu, Le Wang, Hao Liu, Mingming Liu</strong></p>
<p>Current deep dehazing methods only focus on removing haze from hazy images, lacking the capability to translate between hazy and haze-free images. To address this issue, we propose a residual-based efficient bidirectional diffusion model (RBDM) that can model the conditional distributions for both dehazing and haze generation. Firstly, we devise dual Markov chains that can effectively shift the residuals and facilitate bidirectional smooth transitions between them. Secondly, the RBDM perturbs the hazy and haze-free images at individual timesteps and predicts the noise in the perturbed data to simultaneously learn the conditional distributions. Finally, to enhance performance on relatively small datasets and reduce computational costs, our method introduces a unified score function learned on image patches instead of entire images. Our RBDM successfully implements size-agnostic bidirectional transitions between haze-free and hazy images with only 15 sampling steps. Extensive experiments demonstrate that the proposed method achieves superior or at least comparable performance to state-of-the-art methods on both synthetic and real-world datasets. </p>
<blockquote>
<p>å½“å‰æ·±åº¦å»é›¾æ–¹æ³•ä»…ä¸“æ³¨äºä»é›¾éœ¾å›¾åƒä¸­å»é™¤é›¾éœ¾ï¼Œç¼ºä¹åœ¨é›¾éœ¾å›¾åƒå’Œæ— é›¾éœ¾å›¾åƒä¹‹é—´è¿›è¡Œç¿»è¯‘çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ®‹å·®çš„é«˜æ•ˆåŒå‘æ‰©æ•£æ¨¡å‹ï¼ˆRBDMï¼‰ï¼Œè¯¥æ¨¡å‹å¯ä»¥å¯¹å»é›¾å’Œé›¾éœ¾ç”Ÿæˆçš„æ¡ä»¶åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†åŒé©¬å°”å¯å¤«é“¾ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è½¬ç§»æ®‹å·®ï¼Œä¿ƒè¿›å®ƒä»¬ä¹‹é—´çš„åŒå‘å¹³æ»‘è¿‡æ¸¡ã€‚å…¶æ¬¡ï¼ŒRBDMåœ¨å•ä¸ªæ—¶é—´æ­¥é•¿å†…æ‰°åŠ¨æ— é›¾å’Œé›¾éœ¾å›¾åƒï¼Œå¹¶é¢„æµ‹å—æ‰°æ•°æ®ä¸­çš„å™ªå£°ï¼Œä»¥åŒæ—¶å­¦ä¹ æ¡ä»¶åˆ†å¸ƒã€‚æœ€åï¼Œä¸ºäº†æé«˜ç›¸å¯¹è¾ƒå°çš„æ•°æ®é›†çš„æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªåœ¨å›¾åƒå—ä¸Šå­¦ä¹ çš„ç»Ÿä¸€è¯„åˆ†å‡½æ•°ï¼Œè€Œä¸æ˜¯åœ¨æ•´ä¸ªå›¾åƒä¸Šã€‚æˆ‘ä»¬çš„RBDMä»…ä½¿ç”¨15ä¸ªé‡‡æ ·æ­¥éª¤å°±æˆåŠŸå®ç°äº†æ— é›¾å’Œé›¾éœ¾å›¾åƒä¹‹é—´çš„å¤§å°æ— å…³åŒå‘è¿‡æ¸¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æˆ–è‡³å°‘ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11134v1">PDF</a> 7 pages, 5 figures, 2025 ICME Accepted</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ®‹å·®çš„åŒå‘æ‰©æ•£æ¨¡å‹ï¼ˆRBDMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿå»é›¾å’Œç”Ÿæˆé›¾çš„æ¡ä»¶åˆ†å¸ƒã€‚é€šè¿‡æ„å»ºåŒé©¬å°”å¯å¤«é“¾å®ç°æ®‹å·®çš„è½¬ç§»ï¼Œå¹¶é¢„æµ‹æ‰°åŠ¨æ•°æ®çš„å™ªå£°ä»¥å­¦ä¹ æ¡ä»¶åˆ†å¸ƒã€‚ä¸ºæé«˜å°æ•°æ®é›†æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ï¼Œå¼•å…¥ç»Ÿä¸€å¾—åˆ†å‡½æ•°å¯¹å›¾åƒå—è¿›è¡Œå­¦ä¹ è€Œéæ•´ä¸ªå›¾åƒã€‚RBDMä»…éœ€15ä¸ªé‡‡æ ·æ­¥éª¤å³å¯å®ç°å»é›¾ä¸æœ‰é›¾å›¾åƒä¹‹é—´çš„åŒå‘è½¬æ¢ï¼Œä¸”åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„æ€§èƒ½å‡è¾¾åˆ°æˆ–ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ®‹å·®çš„åŒå‘æ‰©æ•£æ¨¡å‹ï¼ˆRBDMï¼‰ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿå»é›¾å’Œç”Ÿæˆé›¾çš„æ¡ä»¶åˆ†å¸ƒã€‚</li>
<li>é€šè¿‡åŒé©¬å°”å¯å¤«é“¾å®ç°æ®‹å·®çš„è½¬ç§»ï¼Œä¿ƒè¿›åŒå‘å¹³æ»‘è¿‡æ¸¡ã€‚</li>
<li>é€šè¿‡é¢„æµ‹æ‰°åŠ¨æ•°æ®çš„å™ªå£°æ¥å­¦ä¹ æ¡ä»¶åˆ†å¸ƒã€‚</li>
<li>å¼•å…¥ç»Ÿä¸€å¾—åˆ†å‡½æ•°ï¼Œå¯¹å›¾åƒå—è¿›è¡Œå­¦ä¹ ä»¥æé«˜æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>å®ç°å»é›¾ä¸æœ‰é›¾å›¾åƒä¹‹é—´çš„åŒå‘è½¬æ¢åªéœ€15ä¸ªé‡‡æ ·æ­¥éª¤ã€‚</li>
<li>åœ¨åˆæˆæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76a176208cd581cba8c79a197648e07b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e13d2260a752525edf320319bef0d89e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f791d27acc5589854121642b4057152.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9eebcf3a19639936ba70b237d2f8d248.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5955921ee0af8be3322721ef52f1947.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f4503a1465caffb7a467a9599a8fa26.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Object-Fidelity-Diffusion-for-Remote-Sensing-Image-Generation"><a href="#Object-Fidelity-Diffusion-for-Remote-Sensing-Image-Generation" class="headerlink" title="Object Fidelity Diffusion for Remote Sensing Image Generation"></a>Object Fidelity Diffusion for Remote Sensing Image Generation</h2><p><strong>Authors:Ziqi Ye, Shuran Ma, Jie Yang, Xiaoyi Yang, Ziyang Gong, Xue Yang, Haipeng Wang</strong></p>
<p>High-precision controllable remote sensing image generation is both meaningful and challenging. Existing diffusion models often produce low-fidelity images due to their inability to adequately capture morphological details, which may affect the robustness and reliability of object detection models. To enhance the accuracy and fidelity of generated objects in remote sensing, this paper proposes Object Fidelity Diffusion (OF-Diff), which effectively improves the fidelity of generated objects. Specifically, we are the first to extract the prior shapes of objects based on the layout for diffusion models in remote sensing. Then, we introduce a dual-branch diffusion model with diffusion consistency loss, which can generate high-fidelity remote sensing images without providing real images during the sampling phase. Furthermore, we introduce DDPO to fine-tune the diffusion process, making the generated remote sensing images more diverse and semantically consistent. Comprehensive experiments demonstrate that OF-Diff outperforms state-of-the-art methods in the remote sensing across key quality metrics. Notably, the performance of several polymorphic and small object classes shows significant improvement. For instance, the mAP increases by 8.3%, 7.7%, and 4.0% for airplanes, ships, and vehicles, respectively. </p>
<blockquote>
<p>é«˜ç²¾åº¦å¯æ§é¥æ„Ÿå›¾åƒç”Ÿæˆæ—¢æœ‰æ„ä¹‰åˆå…·æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„æ‰©æ•£æ¨¡å‹å¾€å¾€ç”±äºæ— æ³•å……åˆ†æ•æ‰å½¢æ€ç»†èŠ‚ï¼Œè€Œäº§ç”Ÿä½ä¿çœŸåº¦çš„å›¾åƒï¼Œè¿™å¯èƒ½ä¼šå½±å“åˆ°ç›®æ ‡æ£€æµ‹æ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚ä¸ºäº†æé«˜é¥æ„Ÿä¸­ç”Ÿæˆç‰©ä½“çš„ç²¾åº¦å’Œä¿çœŸåº¦ï¼Œæœ¬æ–‡æå‡ºäº†å¯¹è±¡ä¿çœŸæ‰©æ•£ï¼ˆOF-Diffï¼‰ï¼Œè¿™æœ‰æ•ˆåœ°æé«˜äº†ç”Ÿæˆç‰©ä½“çš„ä¿çœŸåº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–æ¬¡åŸºäºé¥æ„Ÿçš„æ‰©æ•£æ¨¡å‹å¸ƒå±€æå–äº†ç‰©ä½“çš„å…ˆéªŒå½¢çŠ¶ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…·æœ‰æ‰©æ•£ä¸€è‡´æ€§æŸå¤±çš„åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨é‡‡æ ·é˜¶æ®µä¸æä¾›çœŸå®å›¾åƒçš„æƒ…å†µä¸‹ç”Ÿæˆé«˜ä¿çœŸçš„é¥æ„Ÿå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†DDPOæ¥å¾®è°ƒæ‰©æ•£è¿‡ç¨‹ï¼Œä½¿ç”Ÿæˆçš„é¥æ„Ÿå›¾åƒæ›´åŠ å¤šæ ·åŒ–å’Œè¯­ä¹‰ä¸€è‡´ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒOF-Diffåœ¨é¥æ„Ÿé¢†åŸŸçš„å…³é”®è´¨é‡æŒ‡æ ‡ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤šå½¢æ€å’Œå°ç›®æ ‡ç±»åˆ«çš„æ€§èƒ½è¡¨ç°å‡ºæ˜¾è‘—æå‡ã€‚ä¾‹å¦‚ï¼Œé£æœºã€èˆ¹åªå’Œè½¦è¾†çš„mAPåˆ†åˆ«æé«˜äº†8.3%ã€7.7%å’Œ4.0%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10801v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºObject Fidelity Diffusionï¼ˆOF-Diffï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é¥æ„Ÿå›¾åƒç”Ÿæˆå¯¹è±¡çš„ç²¾åº¦å’Œä¿çœŸåº¦ã€‚å®ƒé€šè¿‡æå–å¯¹è±¡çš„å…ˆéªŒå½¢çŠ¶å¹¶å¼•å…¥åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£ä¸€è‡´æ€§æŸå¤±ï¼Œæœ‰æ•ˆæ”¹å–„ç”Ÿæˆå¯¹è±¡çš„ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡DDPOå¯¹æ‰©æ•£è¿‡ç¨‹è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”Ÿæˆçš„é¥æ„Ÿå›¾åƒæ›´åŠ å¤šæ ·åŒ–å’Œè¯­ä¹‰ä¸€è‡´ã€‚å®éªŒè¡¨æ˜ï¼ŒOF-Diffåœ¨é¥æ„Ÿé¢†åŸŸçš„å…³é”®è´¨é‡æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ€å’Œå°ç›®æ ‡ç±»åˆ«ä¸Šæ€§èƒ½æå‡æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OF-Diffæ–¹æ³•æ—¨åœ¨æé«˜é¥æ„Ÿå›¾åƒç”Ÿæˆå¯¹è±¡çš„ç²¾åº¦å’Œä¿çœŸåº¦ã€‚</li>
<li>é€šè¿‡æå–å¯¹è±¡çš„å…ˆéªŒå½¢çŠ¶ï¼ŒåŸºäºå¸ƒå±€ä¸ºæ‰©æ•£æ¨¡å‹åœ¨é¥æ„Ÿé¢†åŸŸæä¾›æ–°æ€è·¯ã€‚</li>
<li>å¼•å…¥åŒåˆ†æ”¯æ‰©æ•£æ¨¡å‹å’Œæ‰©æ•£ä¸€è‡´æ€§æŸå¤±ï¼Œæœ‰æ•ˆæ”¹å–„ç”Ÿæˆé¥æ„Ÿå›¾åƒçš„è´¨é‡ã€‚</li>
<li>DDPOç”¨äºå¾®è°ƒæ‰©æ•£è¿‡ç¨‹ï¼Œä½¿ç”Ÿæˆçš„é¥æ„Ÿå›¾åƒæ›´åŠ å¤šæ ·åŒ–å’Œè¯­ä¹‰ä¸€è‡´ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒOF-Diffåœ¨é¥æ„Ÿå…³é”®è´¨é‡æŒ‡æ ‡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
<li>å¤šæ€å’Œå°ç›®æ ‡ç±»åˆ«çš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå¦‚é£æœºã€èˆ¹åªå’Œè½¦è¾†çš„mAPåˆ†åˆ«æå‡8.3%ã€7.7%å’Œ4.0%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-027fc6d29001fdd04ede365a56abffb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c46ee0a268f055c2e111cfcdfb3a991.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39cb4609b7d16005c74894c58d59a71e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0905b7432205cad3dcdf0d5ad31a87f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e347fd74f04c853af2cbc33f8230e30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ced7713268a6539886c1f76a4d3bc35f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3473d387efc0effaab78bfe13dae996.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale"><a href="#NextStep-1-Toward-Autoregressive-Image-Generation-with-Continuous-Tokens-at-Scale" class="headerlink" title="NextStep-1: Toward Autoregressive Image Generation with Continuous   Tokens at Scale"></a>NextStep-1: Toward Autoregressive Image Generation with Continuous   Tokens at Scale</h2><p><strong>Authors: NextStep Team, Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, Hongyu Zhou, Kenkun Liu, Ailin Huang, Bin Wang, Changxin Miao, Deshan Sun, En Yu, Fukun Yin, Gang Yu, Hao Nie, Haoran Lv, Hanpeng Hu, Jia Wang, Jian Zhou, Jianjian Sun, Kaijun Tan, Kang An, Kangheng Lin, Liang Zhao, Mei Chen, Peng Xing, Rui Wang, Shiyu Liu, Shutao Xia, Tianhao You, Wei Ji, Xianfang Zeng, Xin Han, Xuelin Zhang, Yana Wei, Yanming Xu, Yimin Jiang, Yingming Wang, Yu Zhou, Yucheng Han, Ziyang Meng, Binxing Jiao, Daxin Jiang, Xiangyu Zhang, Yibo Zhu</strong></p>
<p>Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community. </p>
<blockquote>
<p>å½“å‰æµè¡Œçš„ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ï¼Œè¦ä¹ˆä¾èµ–äºé‡é‡çº§ã€è®¡ç®—å¯†é›†å‹çš„æ‰©æ•£æ¨¡å‹æ¥å¤„ç†è¿ç»­çš„å›¾åƒä»¤ç‰Œï¼Œè¦ä¹ˆé‡‡ç”¨å‘é‡é‡åŒ–ï¼ˆVQï¼‰è·å¾—ç¦»æ•£ä»¤ç‰Œï¼Œä½†ä¼šäº§ç”Ÿé‡åŒ–æŸå¤±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å€ŸåŠ©NextStep-1æ¨åŠ¨è‡ªå›å½’èŒƒå¼çš„å‘å±•ã€‚NextStep-1æ˜¯ä¸€ä¸ªæ‹¥æœ‰14Bå‚æ•°çš„è‡ªå›å½’æ¨¡å‹ï¼Œæ­é…ä¸€ä¸ªè§„æ¨¡ä¸º1.57äº¿çš„æµåŒ¹é…å¤´ï¼Œé€šè¿‡ç¦»æ•£æ–‡æœ¬ä»¤ç‰Œå’Œè¿ç»­å›¾åƒä»¤ç‰Œè¿›è¡Œè®­ç»ƒï¼Œä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œä¸ºç›®æ ‡ã€‚NextStep-1åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„è‡ªå›å½’æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œåœ¨é«˜ä¿çœŸå›¾åƒåˆæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒç¼–è¾‘æ–¹é¢ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬ç»Ÿä¸€æ–¹æ³•çš„å¼ºå¤§å’Œé€šç”¨æ€§ã€‚ä¸ºäº†æ¨åŠ¨å¼€æ”¾ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å‘ç¤¾åŒºå‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10711v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/stepfun-ai/NextStep-1">https://github.com/stepfun-ai/NextStep-1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†NextStep-1æ¨¡å‹ï¼Œå®ƒæ˜¯ä¸€ä¸ªç»“åˆæ–‡æœ¬ç¦»æ•£ä»¤ç‰Œå’Œå›¾åƒè¿ç»­ä»¤ç‰Œçš„14Bå‚æ•°çš„è‡ªå›å½’æ¨¡å‹ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚è¯¥æ¨¡å‹ä½¿ç”¨æµåŒ¹é…å¤´è¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†é«˜ä¿çœŸå›¾åƒåˆæˆçš„å…ˆè¿›æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ã€‚æœ¬æ–‡è¿˜å¼ºè°ƒäº†ç»Ÿä¸€æ–¹æ³•çš„åŠ›é‡å’Œå¤šåŠŸèƒ½æ€§ï¼Œå¹¶å°†ä»£ç å’Œæ¨¡å‹å‘ç¤¾åŒºå¼€æ”¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NextStep-1æ¨¡å‹æ˜¯ä¸€ä¸ªå…ˆè¿›çš„è‡ªå›å½’æ¨¡å‹ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>è¯¥æ¨¡å‹ç»“åˆäº†ç¦»æ•£æ–‡æœ¬ä»¤ç‰Œå’Œè¿ç»­å›¾åƒä»¤ç‰Œè¿›è¡Œè®­ç»ƒã€‚</li>
<li>NextStep-1å®ç°äº†é«˜ä¿çœŸå›¾åƒåˆæˆçš„å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘æ–¹é¢ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å…·å¤‡å¼ºå¤§çš„åŠŸèƒ½å’Œå¤šåŠŸèƒ½æ€§ï¼Œç»Ÿä¸€äº†ä¸åŒçš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>NextStep-1æ¨¡å‹çš„ä»£ç å’Œæ¨¡å‹å·²å‘ç¤¾åŒºå¼€æ”¾ï¼Œä¾¿äºè¿›è¡Œå¼€æ”¾ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10711">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c161feadfad1e52255723fe3e2647a95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-256910c7f4da19f8e289c57cc5cff5cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1a75ac11b738c063e4f11f8662c0efe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9064c3eae467753ffdaf42bdc1989e0c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Novel-View-Synthesis-using-DDIM-Inversion"><a href="#Novel-View-Synthesis-using-DDIM-Inversion" class="headerlink" title="Novel View Synthesis using DDIM Inversion"></a>Novel View Synthesis using DDIM Inversion</h2><p><strong>Authors:Sehajdeep SIngh, A V Subramanyam</strong></p>
<p>Synthesizing novel views from a single input image is a challenging task. It requires extrapolating the 3D structure of a scene while inferring details in occluded regions, and maintaining geometric consistency across viewpoints. Many existing methods must fine-tune large diffusion backbones using multiple views or train a diffusion model from scratch, which is extremely expensive. Additionally, they suffer from blurry reconstruction and poor generalization. This gap presents the opportunity to explore an explicit lightweight view translation framework that can directly utilize the high-fidelity generative capabilities of a pretrained diffusion model while reconstructing a scene from a novel view. Given the DDIM-inverted latent of a single input image, we employ a camera pose-conditioned translation U-Net, TUNet, to predict the inverted latent corresponding to the desired target view. However, the image sampled using the predicted latent may result in a blurry reconstruction. To this end, we propose a novel fusion strategy that exploits the inherent noise correlation structure observed in DDIM inversion. The proposed fusion strategy helps preserve the texture and fine-grained details. To synthesize the novel view, we use the fused latent as the initial condition for DDIM sampling, leveraging the generative prior of the pretrained diffusion model. Extensive experiments on MVImgNet demonstrate that our method outperforms existing methods. </p>
<blockquote>
<p>ä»å•ä¸€è¾“å…¥å›¾åƒåˆæˆæ–°é¢–è§†è§’æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å®ƒéœ€è¦åœ¨æ¨æ–­é®æŒ¡åŒºåŸŸçš„ç»†èŠ‚æ—¶æ¨æ–­åœºæ™¯çš„3Dç»“æ„ï¼Œå¹¶åœ¨ä¸åŒè§‚ç‚¹ä¹‹é—´ä¿æŒå‡ ä½•ä¸€è‡´æ€§ã€‚è®¸å¤šç°æœ‰æ–¹æ³•å¿…é¡»ä½¿ç”¨å¤šç§è§†è§’å¾®è°ƒå¤§å‹æ‰©æ•£ä¸»å¹²æˆ–ä»å¤´å¼€å§‹è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œè¿™æä¸ºæ˜‚è´µã€‚æ­¤å¤–ï¼Œå®ƒä»¬è¿˜å­˜åœ¨æ¨¡ç³Šé‡å»ºå’Œæ³›åŒ–èƒ½åŠ›å·®çš„ç¼ºç‚¹ã€‚è¿™ä¸€å·®è·ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæœºä¼šï¼Œå³æ¢ç´¢ä¸€ä¸ªæ˜ç¡®çš„è½»é‡åŒ–è§†å›¾ç¿»è¯‘æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é«˜ä¿çœŸç”Ÿæˆèƒ½åŠ›ï¼Œä»æ–°é¢–è§†è§’é‡å»ºåœºæ™¯ã€‚ç»™å®šå•ä¸ªè¾“å…¥å›¾åƒçš„DDIMåå‘æŠ•å½±æ½œåœ¨ç‰¹å¾ï¼Œæˆ‘ä»¬é‡‡ç”¨å—ç›¸æœºå§¿æ€æ§åˆ¶çš„ç¿»è¯‘U-Netï¼ˆTUNetï¼‰æ¥é¢„æµ‹å¯¹åº”äºæ‰€éœ€ç›®æ ‡è§†è§’çš„åå‘æŠ•å½±æ½œåœ¨ç‰¹å¾ã€‚ç„¶è€Œï¼Œä½¿ç”¨é¢„æµ‹çš„æ½œåœ¨ç‰¹å¾é‡‡æ ·çš„å›¾åƒå¯èƒ½ä¼šå¯¼è‡´é‡å»ºæ¨¡ç³Šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨DDIMåæ¼”ä¸­è§‚å¯Ÿåˆ°çš„å›ºæœ‰å™ªå£°ç›¸å…³ç»“æ„çš„æ–°èåˆç­–ç•¥ã€‚è¯¥èåˆç­–ç•¥æœ‰åŠ©äºä¿ç•™çº¹ç†å’Œç»†èŠ‚ã€‚ä¸ºäº†åˆæˆæ–°é¢–è§†è§’ï¼Œæˆ‘ä»¬ä½¿ç”¨èåˆåçš„æ½œåœ¨ç‰¹å¾ä½œä¸ºDDIMé‡‡æ ·çš„åˆå§‹æ¡ä»¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒã€‚åœ¨MVImgNetä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10688v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å•ä¸€è¾“å…¥å›¾åƒåˆæˆæ–°é¢–è§†è§’çš„ä»»åŠ¡æŒ‘æˆ˜æ€§å¼ºï¼Œéœ€è¦æ¨æ–­é®æŒ¡åŒºåŸŸçš„ç»†èŠ‚ï¼ŒåŒæ—¶ä¿æŒä¸åŒè§†è§’çš„å‡ ä½•ä¸€è‡´æ€§ã€‚ç°æœ‰æ–¹æ³•éœ€è¦åˆ©ç”¨å¤šè§†è§’å¾®è°ƒå¤§å‹æ‰©æ•£æ¨¡å‹æˆ–ä»å¤´å¼€å§‹è®­ç»ƒï¼Œæˆæœ¬é«˜æ˜‚ä¸”æ¨¡ç³Šé‡å»ºã€æ³›åŒ–èƒ½åŠ›å·®ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†ä¸€ä¸ªæ˜¾å¼çš„è½»é‡çº§è§†è§’è½¬æ¢æ¡†æ¶ï¼Œå¯ç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é«˜ä¿çœŸç”Ÿæˆèƒ½åŠ›ï¼Œä»æ–°é¢–è§†è§’é‡å»ºåœºæ™¯ã€‚é€šè¿‡é‡‡ç”¨å•è¾“å…¥å›¾åƒçš„DDIMåæ¼”æ½œåŠ›å’Œç›¸æœºå§¿æ€è°ƒèŠ‚çš„ç¿»è¯‘U-Netï¼ˆTUNetï¼‰ï¼Œé¢„æµ‹ç›®æ ‡è§†è§’å¯¹åº”çš„åæ¼”æ½œåŠ›ã€‚ä¸ºè§£å†³å›¾åƒé‡‡æ ·å¯èƒ½å¯¼è‡´çš„æ¨¡ç³Šé‡å»ºé—®é¢˜ï¼Œæå‡ºåˆ©ç”¨DDIMåæ¼”ä¸­è§‚å¯Ÿåˆ°çš„å›ºæœ‰å™ªå£°ç›¸å…³ç»“æ„çš„æ–°å‹èåˆç­–ç•¥ï¼Œè¯¥ç­–ç•¥æœ‰åŠ©äºä¿æŒçº¹ç†å’Œç»†èŠ‚ã€‚é€šè¿‡MVImgNetçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆæ–°é¢–è§†è§’æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œéœ€è¦æ¨æ–­é®æŒ¡åŒºåŸŸå¹¶ç»´æŒå‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æˆæœ¬é«˜ï¼Œä¸”å­˜åœ¨æ¨¡ç³Šé‡å»ºå’Œæ³›åŒ–é—®é¢˜ã€‚</li>
<li>ç ”ç©¶æå‡ºä¸€ä¸ªè½»é‡çº§çš„è§†è§’è½¬æ¢æ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é«˜ä¿çœŸç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>é€šè¿‡DDIMåæ¼”æ½œåŠ›å’Œç›¸æœºå§¿æ€è°ƒèŠ‚çš„ç¿»è¯‘U-Netï¼ˆTUNetï¼‰é¢„æµ‹ç›®æ ‡è§†è§’çš„æ½œåŠ›ã€‚</li>
<li>æå‡ºæ–°å‹èåˆç­–ç•¥è§£å†³å›¾åƒé‡‡æ ·çš„æ¨¡ç³Šé‡å»ºé—®é¢˜ï¼Œåˆ©ç”¨DDIMåæ¼”ä¸­çš„å™ªå£°ç›¸å…³ç»“æ„ã€‚</li>
<li>èåˆç­–ç•¥æœ‰åŠ©äºä¿æŒçº¹ç†å’Œç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f90a471e30ab6f1e73d8bbde4fe994c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d431081396646a66cd4c1d9560e98ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7f35760138c043d0c09ceb8b8d60933.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbf02856ab90883492ec326e1cae83ba.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Hybrid-Generative-Fusion-for-Efficient-and-Privacy-Preserving-Face-Recognition-Dataset-Generation"><a href="#Hybrid-Generative-Fusion-for-Efficient-and-Privacy-Preserving-Face-Recognition-Dataset-Generation" class="headerlink" title="Hybrid Generative Fusion for Efficient and Privacy-Preserving Face   Recognition Dataset Generation"></a>Hybrid Generative Fusion for Efficient and Privacy-Preserving Face   Recognition Dataset Generation</h2><p><strong>Authors:Feiran Li, Qianqian Xu, Shilong Bao, Boyu Han, Zhiyong Yang, Qingming Huang</strong></p>
<p>In this paper, we present our approach to the DataCV ICCV Challenge, which centers on building a high-quality face dataset to train a face recognition model. The constructed dataset must not contain identities overlapping with any existing public face datasets. To handle this challenge, we begin with a thorough cleaning of the baseline HSFace dataset, identifying and removing mislabeled or inconsistent identities through a Mixture-of-Experts (MoE) strategy combining face embedding clustering and GPT-4o-assisted verification. We retain the largest consistent identity cluster and apply data augmentation up to a fixed number of images per identity. To further diversify the dataset, we generate synthetic identities using Stable Diffusion with prompt engineering. As diffusion models are computationally intensive, we generate only one reference image per identity and efficiently expand it using Vec2Face, which rapidly produces 49 identity-consistent variants. This hybrid approach fuses GAN-based and diffusion-based samples, enabling efficient construction of a diverse and high-quality dataset. To address the high visual similarity among synthetic identities, we adopt a curriculum learning strategy by placing them early in the training schedule, allowing the model to progress from easier to harder samples. Our final dataset contains 50 images per identity, and all newly generated identities are checked with mainstream face datasets to ensure no identity leakage. Our method achieves \textbf{1st place} in the competition, and experimental results show that our dataset improves model performance across 10K, 20K, and 100K identity scales. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Ferry-Li/datacv_fr">https://github.com/Ferry-Li/datacv_fr</a>. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é’ˆå¯¹DataCV ICCVæŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆçš„é‡ç‚¹æ˜¯æ„å»ºä¸€ä¸ªé«˜è´¨é‡çš„äººè„¸æ•°æ®é›†æ¥è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ã€‚æ„å»ºçš„æ•°æ®é›†ä¸å¾—ä¸ä»»ä½•ç°æœ‰å…¬å…±äººè„¸æ•°æ®é›†å­˜åœ¨èº«ä»½é‡å ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆå½»åº•æ¸…ç†åŸºå‡†HSFaceæ•°æ®é›†ï¼Œé€šè¿‡æ··åˆä¸“å®¶ï¼ˆMoEï¼‰ç­–ç•¥ç»“åˆäººè„¸åµŒå…¥èšç±»å’ŒGPT-4oè¾…åŠ©éªŒè¯æ¥è¯†åˆ«å’Œç§»é™¤é”™è¯¯æ ‡è®°æˆ–èº«ä»½ä¸ä¸€è‡´çš„æƒ…å†µã€‚æˆ‘ä»¬ä¿ç•™æœ€å¤§çš„è¿ç»­èº«ä»½é›†ç¾¤ï¼Œå¹¶å¯¹æ¯ä¸ªèº«ä»½è¿›è¡Œæœ€å¤šå›ºå®šæ•°é‡çš„å›¾åƒæ•°æ®å¢å¼ºã€‚ä¸ºäº†è¿›ä¸€æ­¥å¤šæ ·åŒ–æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨Stable Diffusionå’Œæç¤ºå·¥ç¨‹ç”Ÿæˆåˆæˆèº«ä»½ã€‚ç”±äºæ‰©æ•£æ¨¡å‹è®¡ç®—é‡å¤§ï¼Œæˆ‘ä»¬æ¯ä¸ªèº«ä»½åªç”Ÿæˆä¸€ä¸ªå‚è€ƒå›¾åƒï¼Œå¹¶ä½¿ç”¨Vec2Faceæœ‰æ•ˆåœ°è¿›è¡Œæ‰©å±•ï¼Œè¿…é€Ÿç”Ÿæˆ49ä¸ªèº«ä»½ä¸€è‡´çš„å˜ä½“ã€‚è¿™ç§æ··åˆæ–¹æ³•èåˆäº†åŸºäºGANå’ŒåŸºäºæ‰©æ•£çš„æ ·æœ¬ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°æ„å»ºå¤šæ ·ä¸”é«˜è´¨é‡çš„æ•°æ®é›†ã€‚é’ˆå¯¹åˆæˆèº«ä»½ä¹‹é—´çš„é«˜è§†è§‰ç›¸ä¼¼æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œå°†å®ƒä»¬å°½æ—©çº³å…¥è®­ç»ƒè®¡åˆ’ï¼Œå…è®¸æ¨¡å‹ä»æ˜“åˆ°éš¾é€æ­¥é€‚åº”æ ·æœ¬ã€‚æˆ‘ä»¬çš„æœ€ç»ˆæ•°æ®é›†æ¯ä¸ªèº«ä»½åŒ…å«50å¼ å›¾åƒï¼Œæ‰€æœ‰æ–°ç”Ÿæˆçš„èº«ä»½éƒ½ä¼šä¸ä¸»æµäººè„¸æ•°æ®é›†è¿›è¡Œæ ¸å¯¹ï¼Œä»¥ç¡®ä¿æ²¡æœ‰èº«ä»½æ³„éœ²ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¯”èµ›ä¸­è·å¾—<strong>ç¬¬ä¸€å</strong>ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åœ¨1ä¸‡ã€2ä¸‡å’Œåä¸‡èº«ä»½è§„æ¨¡ä¸Šå‡æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ferry-Li/datacv_fr%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ferry-Li/datacv_fræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10672v1">PDF</a> This paper has been accpeted to ICCV 2025 DataCV Workshop</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥ç ”ç©¶ä¸ºDataCV ICCVæŒ‘æˆ˜èµ›æå‡ºäº†æ„å»ºé«˜è´¨é‡äººè„¸æ•°æ®é›†çš„æ–¹æ³•ï¼Œæ—¨åœ¨è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ã€‚é€šè¿‡å¯¹åŸºå‡†HSFaceæ•°æ®é›†çš„å½»åº•æ¸…æ´—ï¼Œç»“åˆé¢éƒ¨åµŒå…¥èšç±»å’ŒGPT-4oè¾…åŠ©éªŒè¯çš„æ··åˆä¸“å®¶ç­–ç•¥ï¼Œå»é™¤è¯¯æ ‡è®°æˆ–èº«ä»½ä¸ä¸€è‡´çš„æ•°æ®ã€‚ä¿ç•™æœ€å¤§çš„è¿ç»­èº«ä»½é›†ç¾¤ï¼Œå¹¶å¯¹æ¯ä¸ªèº«ä»½è¿›è¡Œæ•°æ®å¢å¼ºã€‚åˆ©ç”¨Stable Diffusionç”Ÿæˆåˆæˆèº«ä»½ï¼Œå¹¶é€šè¿‡Vec2Faceå¿«é€Ÿç”Ÿæˆå¤šä¸ªä¸€è‡´å˜ä½“ã€‚èåˆGANå’Œæ‰©æ•£æ¨¡å‹æ ·æœ¬ï¼Œæ„å»ºå¤šæ ·åŒ–ã€é«˜è´¨é‡æ•°æ®é›†ã€‚é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥è§£å†³åˆæˆèº«ä»½é«˜è§†è§‰ç›¸ä¼¼æ€§é—®é¢˜ã€‚æœ€ç»ˆæ•°æ®é›†åŒ…å«50å¼ å›¾åƒ&#x2F;èº«ä»½ï¼Œæ–°ç”Ÿæˆèº«ä»½ç»ä¸»æµæ•°æ®é›†æ£€æŸ¥æ— æ³„éœ²ã€‚è¯¥æ–¹æ³•åœ¨ç«èµ›ä¸­è£è·ç¬¬ä¸€åï¼Œå®éªŒç»“æœæ˜¾ç¤ºæ•°æ®é›†åœ¨1ä¸‡ã€2ä¸‡ã€åä¸‡èº«ä»½è§„æ¨¡ä¸Šå‡æå‡æ¨¡å‹æ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯¥ç ”ç©¶ä¸ºDataCV ICCVæŒ‘æˆ˜èµ›æå‡ºäº†ä¸€ç§æ„å»ºé«˜è´¨é‡äººè„¸æ•°æ®é›†çš„æ–¹æ³•ï¼Œä¾§é‡äºè®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨åŸºäºMixture-of-Expertsçš„ç­–ç•¥å¯¹åŸºå‡†æ•°æ®é›†è¿›è¡Œæ¸…æ´—ï¼Œå»é™¤è¯¯æ ‡è®°å’Œèº«ä»½ä¸ä¸€è‡´çš„æ•°æ®ã€‚</li>
<li>é€šè¿‡æ•°æ®å¢å¼ºå’Œåˆæˆèº«ä»½ç”ŸæˆæŠ€æœ¯ï¼ˆStable Diffusionï¼‰ä¸°å¯Œæ•°æ®é›†å¤šæ ·æ€§ã€‚</li>
<li>åˆ©ç”¨Vec2Faceå¿«é€Ÿç”Ÿæˆå¤šä¸ªä¸€è‡´å˜ä½“ï¼Œæé«˜æ•°æ®æ•ˆç‡ã€‚</li>
<li>èåˆGANå’Œæ‰©æ•£æ¨¡å‹æ ·æœ¬ï¼Œæ„å»ºå¤šæ ·åŒ–ã€é«˜è´¨é‡æ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥åº”å¯¹åˆæˆèº«ä»½é«˜è§†è§‰ç›¸ä¼¼æ€§çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a8812acd003dffba03c38c577c77101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-532ca8df40131a56bf3a128e51f61cdb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37386e5fb6c9d40cc94977fa6fb5ef09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57a78ccba9560354acd14764709fc6ee.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Unified-Multi-Agent-Framework-for-Universal-Multimodal-Understanding-and-Generation"><a href="#A-Unified-Multi-Agent-Framework-for-Universal-Multimodal-Understanding-and-Generation" class="headerlink" title="A Unified Multi-Agent Framework for Universal Multimodal Understanding   and Generation"></a>A Unified Multi-Agent Framework for Universal Multimodal Understanding   and Generation</h2><p><strong>Authors:Jiulin Li, Ping Huang, Yexin Li, Shuo Chen, Juewen Hu, Ye Tian</strong></p>
<p>Real-world multimodal applications often require any-to-any capabilities, enabling both understanding and generation across modalities including text, image, audio, and video. However, integrating the strengths of autoregressive language models (LLMs) for reasoning and diffusion models for high-fidelity generation remains challenging. Existing approaches rely on rigid pipelines or tightly coupled architectures, limiting flexibility and scalability. We propose MAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that unifies multimodal understanding and generation via two decoupled phases: Cognition and Deliberation. MAGUS enables symbolic multi-agent collaboration within a shared textual workspace. In the Cognition phase, three role-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector - engage in collaborative dialogue to perform structured understanding and planning. The Deliberation phase incorporates a Growth-Aware Search mechanism that orchestrates LLM-based reasoning and diffusion-based generation in a mutually reinforcing manner. MAGUS supports plug-and-play extensibility, scalable any-to-any modality conversion, and semantic alignment - all without the need for joint training. Experiments across multiple benchmarks, including image, video, and audio generation, as well as cross-modal instruction following, demonstrate that MAGUS outperforms strong baselines and state-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the powerful closed-source model GPT-4o. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œä¸­çš„å¤šæ¨¡æ€åº”ç”¨é€šå¸¸éœ€è¦ä»»ä½•åˆ°ä»»ä½•çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€ä¹‹é—´è¿›è¡Œç†è§£å’Œç”Ÿæˆã€‚ç„¶è€Œï¼Œæ•´åˆè‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›å’Œæ‰©æ•£æ¨¡å‹çš„é«˜ä¿çœŸç”Ÿæˆèƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºåƒµåŒ–çš„ç®¡é“æˆ–ç´§å¯†è€¦åˆçš„æ¶æ„ï¼Œè¿™é™åˆ¶äº†çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æå‡ºäº†MAGUSï¼ˆå¤šä»£ç†å¼•å¯¼ç»Ÿä¸€å¤šæ¨¡æ€ç³»ç»Ÿï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªè§£è€¦é˜¶æ®µç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆï¼šè®¤çŸ¥å’Œæ€è€ƒã€‚MAGUSåœ¨å…±äº«æ–‡æœ¬å·¥ä½œç©ºé—´ä¸­å®ç°äº†ç¬¦å·å¤šä»£ç†åä½œã€‚åœ¨è®¤çŸ¥é˜¶æ®µï¼Œä¸‰ä¸ªè§’è‰²æ¡ä»¶çš„å¤šæ¨¡æ€LLMä»£ç†â€”â€”æ„ŸçŸ¥è€…ã€è§„åˆ’è€…å’Œåå°„è€…â€”â€”å‚ä¸åä½œå¯¹è¯ï¼Œæ‰§è¡Œç»“æ„åŒ–ç†è§£å’Œè§„åˆ’ã€‚æ€è€ƒé˜¶æ®µé‡‡ç”¨å¢é•¿æ„ŸçŸ¥æœç´¢æœºåˆ¶ï¼Œä»¥ç›¸äº’åŠ å¼ºçš„æ–¹å¼åè°ƒåŸºäºLLMçš„æ¨ç†å’ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆã€‚MAGUSæ”¯æŒå³æ’å³ç”¨çš„å¯æ‰©å±•æ€§ï¼Œå¯å®ç°ä»»ä½•åˆ°ä»»ä½•çš„æ¨¡æ€è½¬æ¢å’Œè¯­ä¹‰å¯¹é½ï¼Œè€Œæ— éœ€è”åˆè®­ç»ƒã€‚åœ¨å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ç”Ÿæˆä»¥åŠè·¨æ¨¡æ€æŒ‡ä»¤éµå¾ªç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAGUSä¼˜äºå¼ºå¤§çš„åŸºå‡†çº¿å’Œæœ€å…ˆè¿›çš„ç³»ç»Ÿã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨MMEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMAGUSè¶…è¶Šäº†å¼ºå¤§çš„é—­æºæ¨¡å‹GPT-4oã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10494v1">PDF</a> 8 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€åº”ç”¨çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†MAGUSï¼ˆMulti-Agent Guided Unified Multimodal Systemï¼‰æ¡†æ¶ï¼Œå®ç°äº†è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„ç»Ÿä¸€ã€‚è¯¥æ¡†æ¶åŒ…å«è®¤çŸ¥ä¸å†³ç­–ä¸¤ä¸ªè§£è€¦é˜¶æ®µï¼Œé€šè¿‡å¤šæ¨¡æ€LLMä»£ç†çš„ååŒå·¥ä½œå®ç°ç»“æ„åŒ–ç†è§£å’Œè§„åˆ’ã€‚å¢é•¿æ„ŸçŸ¥æœç´¢æœºåˆ¶åè°ƒLLMæ¨ç†å’Œæ‰©æ•£ç”Ÿæˆï¼Œæ”¯æŒæ’ä»¶å¼æ‰©å±•ã€å¯ä¼¸ç¼©çš„ä»»æ„æ¨¡æ€è½¬æ¢å’Œè¯­ä¹‰å¯¹é½ã€‚å®éªŒè¯æ˜ï¼ŒMAGUSåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†å¤šæ¨¡æ€åº”ç”¨ä¸­çš„æŒ‘æˆ˜å’Œéœ€æ±‚ï¼Œå¼ºè°ƒé›†æˆè¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºMAGUSæ¡†æ¶ï¼ŒåŒ…æ‹¬è®¤çŸ¥ä¸å†³ç­–ä¸¤ä¸ªè§£è€¦é˜¶æ®µï¼Œå®ç°è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„ç»Ÿä¸€ã€‚</li>
<li>é‡‡ç”¨å¤šæ¨¡æ€LLMä»£ç†è¿›è¡Œç»“æ„åŒ–ç†è§£å’Œè§„åˆ’ï¼Œé€šè¿‡ç¬¦å·åŒ–çš„å¤šæ™ºèƒ½ä½“åä½œå®ç°æ–‡æœ¬å·¥ä½œç©ºé—´å…±äº«ã€‚</li>
<li>å¢é•¿æ„ŸçŸ¥æœç´¢æœºåˆ¶ç”¨äºåè°ƒLLMæ¨ç†å’Œæ‰©æ•£ç”Ÿæˆï¼Œæé«˜æ¨¡å‹çš„ååŒèƒ½åŠ›ã€‚</li>
<li>MAGUSæ”¯æŒæ’ä»¶å¼æ‰©å±•ã€å¯ä¼¸ç¼©çš„ä»»æ„æ¨¡æ€è½¬æ¢å’Œè¯­ä¹‰å¯¹é½ï¼Œæ— éœ€è”åˆè®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMAGUSåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10494">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c87e95536d14cef003e29474308a6b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-901fdeb2f17a3ff2b360d71e92b368c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3292fa8a68bdec5f9c58c82cb3523c0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e28635dd56692432e04fa56754f41ed5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6dc44c61de9dadd6d8b14f1cdebfebc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1560aefc878334898638b30709f35917.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c3cd9f04a29d44e058e4b39310c49c3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="NanoControl-A-Lightweight-Framework-for-Precise-and-Efficient-Control-in-Diffusion-Transformer"><a href="#NanoControl-A-Lightweight-Framework-for-Precise-and-Efficient-Control-in-Diffusion-Transformer" class="headerlink" title="NanoControl: A Lightweight Framework for Precise and Efficient Control   in Diffusion Transformer"></a>NanoControl: A Lightweight Framework for Precise and Efficient Control   in Diffusion Transformer</h2><p><strong>Authors:Shanyuan Liu, Jian Zhu, Junda Lu, Yue Gong, Liuzhuozheng Li, Bo Cheng, Yuhang Ma, Liebucha Wu, Xiaoyu Wu, Dawei Leng, Yuhui Yin</strong></p>
<p>Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in text-to-image synthesis. However, in the domain of controllable text-to-image generation using DiTs, most existing methods still rely on the ControlNet paradigm originally designed for UNet-based diffusion models. This paradigm introduces significant parameter overhead and increased computational costs. To address these challenges, we propose the Nano Control Diffusion Transformer (NanoControl), which employs Flux as the backbone network. Our model achieves state-of-the-art controllable text-to-image generation performance while incurring only a 0.024% increase in parameter count and a 0.029% increase in GFLOPs, thus enabling highly efficient controllable generation. Specifically, rather than duplicating the DiT backbone for control, we design a LoRA-style (low-rank adaptation) control module that directly learns control signals from raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation mechanism that integrates condition-specific key-value information into the backbone in a simple yet highly effective manner, facilitating deep fusion of conditional features. Extensive benchmark experiments demonstrate that NanoControl significantly reduces computational overhead compared to conventional control approaches, while maintaining superior generation quality and achieving improved controllability. </p>
<blockquote>
<p>Diffusion Transformersï¼ˆDiTsï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆä¸­å±•ç°äº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¯æ§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸï¼Œä½¿ç”¨DiTsçš„å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»ç„¶ä¾èµ–äºæœ€åˆä¸ºåŸºäºUNetçš„æ‰©æ•£æ¨¡å‹è®¾è®¡çš„ControlNetèŒƒå¼ã€‚è¿™ç§èŒƒå¼å¼•å…¥äº†æ˜¾è‘—çš„å‚æ•°å¼€é”€å’Œå¢åŠ çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Nano Control Diffusion Transformerï¼ˆNanoControlï¼‰ï¼Œå®ƒé‡‡ç”¨Fluxä½œä¸ºä¸»å¹²ç½‘ç»œã€‚æˆ‘ä»¬çš„æ¨¡å‹å®ç°äº†æœ€å…ˆè¿›çš„å¯æ§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ€§èƒ½ï¼ŒåŒæ—¶å‚æ•°è®¡æ•°ä»…å¢åŠ 0.024%ï¼ŒGFLOPså¢åŠ 0.029%ï¼Œä»è€Œå®ç°äº†é«˜åº¦æœ‰æ•ˆçš„å¯æ§ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰å¤åˆ¶DiTä¸»å¹²æ¥è¿›è¡Œæ§åˆ¶ï¼Œè€Œæ˜¯è®¾è®¡äº†ä¸€ä¸ªLoRAé£æ ¼çš„ï¼ˆä½ç§©é€‚åº”ï¼‰æ§åˆ¶æ¨¡å—ï¼Œè¯¥æ¨¡å—ç›´æ¥ä»åŸå§‹æ¡ä»¶è¾“å…¥ä¸­å­¦ä¹ æ§åˆ¶ä¿¡å·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§KV-Context Augmentationæœºåˆ¶ï¼Œä»¥ç®€å•è€Œé«˜æ•ˆçš„æ–¹å¼å°†æ¡ä»¶ç‰¹å®šçš„é”®å€¼ä¿¡æ¯é›†æˆåˆ°ä¸»å¹²ä¸­ï¼Œä¿ƒè¿›æ¡ä»¶ç‰¹å¾çš„æ·±åº¦èåˆã€‚å¹¿æ³›çš„åŸºå‡†å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„æ§åˆ¶æ–¹æ³•ç›¸æ¯”ï¼ŒNanoControlæ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†ä¼˜è¶Šçš„ç”Ÿæˆè´¨é‡å’Œæé«˜äº†å¯æ§æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10424v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†Diffusion Transformersåœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚é’ˆå¯¹å¯æ§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§é‡‡ç”¨Fluxä½œä¸ºä¸»å¹²ç½‘ç»œçš„Nano Control Diffusion Transformerï¼ˆNanoControlï¼‰ã€‚è¯¥æ¨¡å‹åœ¨ä¿æŒé«˜æ•ˆå¯æ§ç”Ÿæˆçš„åŒæ—¶ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å¯æ§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ€§èƒ½ï¼Œå¹¶ä¸”åªå¢åŠ äº†0.024%çš„å‚æ•°è®¡æ•°å’Œ0.029%çš„GFLOPsã€‚NanoControlè®¾è®¡äº†ä¸€ä¸ªLoRAé£æ ¼çš„æ§åˆ¶æ¨¡å—ï¼Œç›´æ¥å­¦ä¹ æ¥è‡ªåŸå§‹æ¡ä»¶è¾“å…¥çš„æ§åˆ¶ä¿¡å·ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªKV-Contextå¢å¼ºæœºåˆ¶ï¼Œä»¥ç®€å•é«˜æ•ˆçš„æ–¹å¼å°†æ¡ä»¶ç‰¹å®šçš„é”®å€¼ä¿¡æ¯é›†æˆåˆ°ä¸»å¹²ä¸­ï¼Œä¿ƒè¿›æ¡ä»¶ç‰¹å¾çš„æ·±åº¦èåˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Transformers (DiTs) å·²æˆåŠŸåº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒåˆæˆã€‚</li>
<li>ç°æœ‰å¤§å¤šæ•°å¯æ§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ä»ä¾èµ–äºä¸ºUNetè®¾è®¡çš„ControlNetèŒƒå¼ï¼Œè¿™å¢åŠ äº†å‚æ•°å’Œè®¡ç®—æˆæœ¬ã€‚</li>
<li>NanoControlæ¨¡å‹é‡‡ç”¨Fluxä½œä¸ºä¸»å¹²ç½‘ç»œï¼Œå®ç°äº†é«˜æ•ˆçš„å¯æ§æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚</li>
<li>NanoControlé€šè¿‡è®¾è®¡LoRAé£æ ¼çš„æ§åˆ¶æ¨¡å—ï¼Œç›´æ¥å­¦ä¹ åŸå§‹æ¡ä»¶è¾“å…¥çš„æ§åˆ¶ä¿¡å·ã€‚</li>
<li>NanoControlå¼•å…¥äº†KV-Context Augmentationæœºåˆ¶ï¼Œä»¥é›†æˆæ¡ä»¶ç‰¹å®šçš„é”®å€¼ä¿¡æ¯ï¼Œä¿ƒè¿›æ¡ä»¶ç‰¹å¾çš„æ·±åº¦èåˆã€‚</li>
<li>NanoControlæ¨¡å‹åœ¨ä¿æŒé«˜æ€§èƒ½ç”Ÿæˆçš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e4759fdd59a9612d5bd80bc89414a67e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8b68976a9da7d0946970b1df857c076.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-046439ce65edd8a7f5f686d1b27f3ffb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcc2df525ac139024a68ff479bf81b5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9aaaae3b9a1c3309247ac2aae906852.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Translation-of-Text-Embedding-via-Delta-Vector-to-Suppress-Strongly-Entangled-Content-in-Text-to-Image-Diffusion-Models"><a href="#Translation-of-Text-Embedding-via-Delta-Vector-to-Suppress-Strongly-Entangled-Content-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Translation of Text Embedding via Delta Vector to Suppress Strongly   Entangled Content in Text-to-Image Diffusion Models"></a>Translation of Text Embedding via Delta Vector to Suppress Strongly   Entangled Content in Text-to-Image Diffusion Models</h2><p><strong>Authors:Eunseo Koh, Seunghoo Hong, Tae-Young Kim, Simon S. Woo, Jae-Pil Heo</strong></p>
<p>Text-to-Image (T2I) diffusion models have made significant progress in generating diverse high-quality images from textual prompts. However, these models still face challenges in suppressing content that is strongly entangled with specific words. For example, when generating an image of <code>Charlie Chaplin&quot;, a </code>mustacheâ€ consistently appears even if explicitly instructed not to include it, as the concept of <code>mustache&quot; is strongly entangled with </code>Charlie Chaplinâ€. To address this issue, we propose a novel approach to directly suppress such entangled content within the text embedding space of diffusion models. Our method introduces a delta vector that modifies the text embedding to weaken the influence of undesired content in the generated image, and we further demonstrate that this delta vector can be easily obtained through a zero-shot approach. Furthermore, we propose a Selective Suppression with Delta Vector (SSDV) method to adapt delta vector into the cross-attention mechanism, enabling more effective suppression of unwanted content in regions where it would otherwise be generated. Additionally, we enabled more precise suppression in personalized T2I models by optimizing delta vector, which previous baselines were unable to achieve. Extensive experimental results demonstrate that our approach significantly outperforms existing methods, both in terms of quantitative and qualitative metrics. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå¤šæ ·åŒ–é«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æŠ‘åˆ¶ä¸ç‰¹å®šå•è¯å¼ºçƒˆçº ç¼ çš„å†…å®¹æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œåœ¨ç”Ÿæˆâ€œæŸ¥ç†Â·å“åˆ«æ—â€çš„å›¾åƒæ—¶ï¼Œå³ä½¿æ˜ç¡®æŒ‡ç¤ºä¸è¦åŒ…å«â€œèƒ¡å­â€ï¼Œä½†â€œèƒ¡å­â€ä»ç„¶ä¼šå‡ºç°ï¼Œå› ä¸ºâ€œèƒ¡å­â€çš„æ¦‚å¿µä¸â€œæŸ¥ç†Â·å“åˆ«æ—â€ç´§å¯†ç›¸è¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­ç›´æ¥æŠ‘åˆ¶è¿™ç§çº ç¼ å†…å®¹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªdeltaå‘é‡ï¼Œè¯¥å‘é‡ä¿®æ”¹æ–‡æœ¬åµŒå…¥ä»¥å‡å¼±ç”Ÿæˆå›¾åƒä¸­ä¸éœ€è¦å†…å®¹çš„å½±å“ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜å¯ä»¥é€šè¿‡é›¶æ ·æœ¬æ–¹æ³•è½»æ¾è·å¾—è¿™ä¸ªdeltaå‘é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰Deltaå‘é‡çš„é€‰æ‹©æ€§æŠ‘åˆ¶ï¼ˆSSDVï¼‰æ–¹æ³•ï¼Œå°†deltaå‘é‡é€‚åº”åˆ°äº¤å‰æ³¨æ„æœºåˆ¶ä¸­ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æŠ‘åˆ¶äº†åŸæœ¬ä¼šç”Ÿæˆçš„åŒºåŸŸçš„ä¸æƒ³è¦çš„å†…å®¹ã€‚é€šè¿‡ä¼˜åŒ–deltaå‘é‡ï¼Œæˆ‘ä»¬è¿˜å®ç°äº†ä¸ªæ€§åŒ–T2Iæ¨¡å‹çš„æ›´ç²¾ç¡®æŠ‘åˆ¶ï¼Œè¿™æ˜¯ä»¥å‰çš„åŸºç¡€çº¿æ— æ³•è¾¾åˆ°çš„ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§æŒ‡æ ‡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10407v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ‰©æ•£æ¨¡å‹å·²åœ¨æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æŠ‘åˆ¶ä¸ç‰¹å®šå•è¯å¼ºçƒˆçº ç¼ çš„å†…å®¹æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œåœ¨ç”Ÿæˆâ€œæŸ¥ç†Â·å“åˆ«æ—â€çš„å›¾åƒæ—¶ï¼Œå³ä½¿æ˜ç¡®æŒ‡ç¤ºä¸åŒ…æ‹¬â€œèƒ¡å­â€ï¼Œä½†â€œèƒ¡å­â€ä»ç„¶ä¼šå‡ºç°ï¼Œå› ä¸ºâ€œèƒ¡å­â€çš„æ¦‚å¿µä¸â€œæŸ¥ç†Â·å“åˆ«æ—â€ç´§å¯†ç›¸è¿ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­ç›´æ¥æŠ‘åˆ¶è¿™ç§çº ç¼ å†…å®¹çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªdeltaå‘é‡ï¼Œè¯¥å‘é‡ä¿®æ”¹æ–‡æœ¬åµŒå…¥ä»¥å‡å¼±ç”Ÿæˆå›¾åƒä¸­ä¸éœ€è¦å†…å®¹çš„å½±å“ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜å¯ä»¥é€šè¿‡é›¶æ ·æœ¬æ–¹æ³•è½»æ¾è·å¾—è¿™ä¸ªdeltaå‘é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰Deltaå‘é‡çš„é€‰æ‹©æ€§æŠ‘åˆ¶ï¼ˆSSDVï¼‰æ–¹æ³•ï¼Œå°†deltaå‘é‡é€‚åº”åˆ°äº¤å‰æ³¨æ„æœºåˆ¶ä¸­ï¼Œä»¥åœ¨å¯èƒ½ç”Ÿæˆä¸éœ€è¦å†…å®¹çš„åŒºåŸŸæ›´æœ‰æ•ˆåœ°æŠ‘åˆ¶å®ƒã€‚å¦å¤–ï¼Œé€šè¿‡ä¼˜åŒ–deltaå‘é‡ï¼Œæˆ‘ä»¬åœ¨ä¸ªæ€§åŒ–T2Iæ¨¡å‹ä¸­å®ç°äº†æ›´ç²¾ç¡®çš„æŠ‘åˆ¶ï¼Œè¿™æ˜¯ä»¥å‰çš„æ–¹æ³•æ— æ³•å®ç°çš„ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ— è®ºåœ¨å®šé‡è¿˜æ˜¯å®šæ€§æŒ‡æ ‡ä¸Šéƒ½æ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä¸æ–‡æœ¬æç¤ºç›¸ç¬¦çš„é«˜è´¨é‡å›¾åƒæ–¹é¢å·²å–å¾—è¿›å±•ï¼Œä½†ä»é¢ä¸´æŠ‘åˆ¶ç‰¹å®šçº ç¼ å†…å®¹çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥deltaå‘é‡ä»¥ä¿®æ”¹æ–‡æœ¬åµŒå…¥ï¼Œä»è€Œå‡å¼±ç”Ÿæˆå›¾åƒä¸­ä¸éœ€è¦å†…å®¹çš„å½±å“ã€‚</li>
<li>é€šè¿‡é›¶æ ·æœ¬æ–¹æ³•è½»æ¾è·å¾—deltaå‘é‡ï¼Œå¹¶åº”ç”¨äºäº¤å‰æ³¨æ„æœºåˆ¶ä¸­ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æŠ‘åˆ¶ä¸éœ€è¦çš„å†…å®¹ã€‚</li>
<li>æå‡ºSSDVæ–¹æ³•ï¼Œå°†deltaå‘é‡é€‚åº”åˆ°ä¸ªæ€§åŒ–T2Iæ¨¡å‹ä¸­ï¼Œå®ç°æ›´ç²¾ç¡®çš„æŠ‘åˆ¶ã€‚</li>
<li>ä¼˜åŒ–çš„deltaå‘é‡ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ç‰¹å®šåŒºåŸŸæ›´æœ‰æ•ˆåœ°æŠ‘åˆ¶ä¸éœ€è¦çš„å†…å®¹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºæé«˜T2Iæ‰©æ•£æ¨¡å‹çš„æ€§èƒ½æä¾›äº†æ–°çš„æ€è·¯å’ŒæŠ€æœ¯æ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b468e9cc9767f15282c89dc41195453.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74d9ea769c7ea7657083f2ff56f00cfe.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Cross-view-Generalized-Diffusion-Model-for-Sparse-view-CT-Reconstruction"><a href="#Cross-view-Generalized-Diffusion-Model-for-Sparse-view-CT-Reconstruction" class="headerlink" title="Cross-view Generalized Diffusion Model for Sparse-view CT Reconstruction"></a>Cross-view Generalized Diffusion Model for Sparse-view CT Reconstruction</h2><p><strong>Authors:Jixiang Chen, Yiqun Lin, Yi Qin, Hualiang Wang, Xiaomeng Li</strong></p>
<p>Sparse-view computed tomography (CT) reduces radiation exposure by subsampling projection views, but conventional reconstruction methods produce severe streak artifacts with undersampled data. While deep-learning-based methods enable single-step artifact suppression, they often produce over-smoothed results under significant sparsity. Though diffusion models improve reconstruction via iterative refinement and generative priors, they require hundreds of sampling steps and struggle with stability in highly sparse regimes. To tackle these concerns, we present the Cross-view Generalized Diffusion Model (CvG-Diff), which reformulates sparse-view CT reconstruction as a generalized diffusion process. Unlike existing diffusion approaches that rely on stochastic Gaussian degradation, CvG-Diff explicitly models image-domain artifacts caused by angular subsampling as a deterministic degradation operator, leveraging correlations across sparse-view CT at different sample rates. To address the inherent artifact propagation and inefficiency of sequential sampling in generalized diffusion model, we introduce two innovations: Error-Propagating Composite Training (EPCT), which facilitates identifying error-prone regions and suppresses propagated artifacts, and Semantic-Prioritized Dual-Phase Sampling (SPDPS), an adaptive strategy that prioritizes semantic correctness before detail refinement. Together, these innovations enable CvG-Diff to achieve high-quality reconstructions with minimal iterations, achieving 38.34 dB PSNR and 0.9518 SSIM for 18-view CT using only \textbf{10} steps on AAPM-LDCT dataset. Extensive experiments demonstrate the superiority of CvG-Diff over state-of-the-art sparse-view CT reconstruction methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/CvG-Diff">https://github.com/xmed-lab/CvG-Diff</a>. </p>
<blockquote>
<p>ç¨€ç–è§†å›¾è®¡ç®—å±‚ææˆåƒï¼ˆCTï¼‰é€šè¿‡å­é‡‡æ ·æŠ•å½±è§†å›¾å‡å°‘äº†è¾å°„æš´éœ²ï¼Œä½†ä¼ ç»Ÿé‡å»ºæ–¹æ³•åœ¨å¤„ç†æ¬ é‡‡æ ·æ•°æ®æ—¶ä¼šäº§ç”Ÿä¸¥é‡çš„æ¡çº¹ä¼ªå½±ã€‚è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•èƒ½å¤Ÿå®ç°ä¸€æ­¥ä¼ªå½±æŠ‘åˆ¶ï¼Œä½†å®ƒä»¬å¾€å¾€åœ¨æ˜¾è‘—ç¨€ç–çš„æƒ…å†µä¸‹äº§ç”Ÿè¿‡äºå¹³æ»‘çš„ç»“æœã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹é€šè¿‡è¿­ä»£ä¼˜åŒ–å’Œç”Ÿæˆå…ˆéªŒä¿¡æ¯æ”¹å–„äº†é‡å»ºæ•ˆæœï¼Œä½†å®ƒä»¬éœ€è¦æ•°ç™¾ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œå¹¶ä¸”åœ¨é«˜åº¦ç¨€ç–çš„æƒ…å†µä¸‹ç¨³å®šæ€§è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨è§†å›¾å¹¿ä¹‰æ‰©æ•£æ¨¡å‹ï¼ˆCvG-Diffï¼‰ï¼Œå®ƒå°†ç¨€ç–è§†å›¾CTé‡å»ºé‡æ–°è¡¨è¿°ä¸ºå¹¿ä¹‰æ‰©æ•£è¿‡ç¨‹ã€‚ä¸ç°æœ‰çš„ä¾èµ–éšæœºé«˜æ–¯é€€åŒ–çš„æ‰©æ•£æ–¹æ³•ä¸åŒï¼ŒCvG-Diffæ˜¾å¼åœ°å°†è§’åº¦å­é‡‡æ ·å¼•èµ·çš„å›¾åƒåŸŸä¼ªå½±å»ºæ¨¡ä¸ºç¡®å®šæ€§é€€åŒ–ç®—å­ï¼Œå¹¶åˆ©ç”¨ä¸åŒé‡‡æ ·ç‡çš„ç¨€ç–è§†å›¾CTä¹‹é—´çš„ç›¸å…³æ€§ã€‚ä¸ºäº†è§£å†³å¹¿ä¹‰æ‰©æ•£æ¨¡å‹ä¸­å›ºæœ‰çš„ä¼ªå½±ä¼ æ’­å’Œé¡ºåºé‡‡æ ·çš„ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤é¡¹åˆ›æ–°ï¼šé”™è¯¯ä¼ æ’­å¤åˆè®­ç»ƒï¼ˆEPCTï¼‰ï¼Œå®ƒæœ‰åŠ©äºè¯†åˆ«é”™è¯¯æ˜“å‘åŒºåŸŸå¹¶æŠ‘åˆ¶ä¼ æ’­çš„ä¼ªå½±ï¼›è¯­ä¹‰ä¼˜å…ˆåŒç›¸é‡‡æ ·ï¼ˆSPDPSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”ç­–ç•¥ï¼Œåœ¨ç»†èŠ‚ä¼˜åŒ–ä¹‹å‰ä¼˜å…ˆå…³æ³¨è¯­ä¹‰æ­£ç¡®æ€§ã€‚è¿™äº›åˆ›æ–°çš„ç»“åˆä½¿CvG-Diffèƒ½å¤Ÿåœ¨æœ€å°‘çš„è¿­ä»£æ¬¡æ•°å†…å®ç°é«˜è´¨é‡çš„é‡å»ºï¼Œåœ¨AAPM-LDCTæ•°æ®é›†ä¸Šä½¿ç”¨ä»…æœ‰10ä¸ªæ­¥éª¤çš„18è§†å›¾CTï¼Œè·å¾—38.34 dBçš„PSNRå’Œ0.9518çš„SSIMã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCvG-Diffåœ¨ç¨€ç–è§†å›¾CTé‡å»ºæ–¹æ³•ä¸­è¡¨ç°å“è¶Šã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/CvG-Diff%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/xmed-lab/CvG-Diffè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10313v1">PDF</a> MICCAI 2025 Spotlight</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç¨€ç–è§†å›¾è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰é€šè¿‡å­é‡‡æ ·æŠ•å½±è§†å›¾å‡å°‘è¾å°„æš´éœ²ï¼Œä½†ä¼ ç»Ÿé‡å»ºæ–¹æ³•ä¼šäº§ç”Ÿä¸¥é‡çš„æ¡çº¹ä¼ªå½±ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•èƒ½å¤Ÿä¸€æ­¥æŠ‘åˆ¶ä¼ªå½±ï¼Œä½†åœ¨æ˜¾è‘—ç¨€ç–çš„æƒ…å†µä¸‹ä¼šäº§ç”Ÿè¿‡äºå¹³æ»‘çš„ç»“æœã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹é€šè¿‡è¿­ä»£ä¼˜åŒ–å’Œç”Ÿæˆå…ˆéªŒæ”¹è¿›é‡å»ºï¼Œä½†éœ€è¦æ•°ç™¾ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œå¹¶ä¸”åœ¨é«˜åº¦ç¨€ç–çš„æƒ…å†µä¸‹ç¨³å®šæ€§è¾ƒå·®ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨è§†å›¾å¹¿ä¹‰æ‰©æ•£æ¨¡å‹ï¼ˆCvG-Diffï¼‰ï¼Œå°†ç¨€ç–è§†å›¾CTé‡å»ºé‡æ–°è¡¨è¿°ä¸ºå¹¿ä¹‰æ‰©æ•£è¿‡ç¨‹ã€‚ä¸åŒäºç°æœ‰æ‰©æ•£æ–¹æ³•ä¾èµ–äºéšæœºé«˜æ–¯é€€åŒ–ï¼ŒCvG-Diffæ˜¾å¼åœ°å»ºæ¨¡ç”±è§’åº¦å­é‡‡æ ·å¼•èµ·çš„å›¾åƒåŸŸä¼ªå½±ä½œä¸ºç¡®å®šæ€§é€€åŒ–ç®—å­ï¼Œå¹¶åˆ©ç”¨ä¸åŒé‡‡æ ·ç‡çš„ç¨€ç–è§†å›¾CTä¹‹é—´çš„ç›¸å…³æ€§ã€‚ä¸ºè§£å†³å¹¿ä¹‰æ‰©æ•£æ¨¡å‹ä¸­å›ºæœ‰çš„ä¼ªå½±ä¼ æ’­å’Œé¡ºåºé‡‡æ ·çš„ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤é¡¹åˆ›æ–°ï¼šé”™è¯¯ä¼ æ’­å¤åˆè®­ç»ƒï¼ˆEPCTï¼‰ï¼Œæœ‰åŠ©äºè¯†åˆ«é”™è¯¯åŒºåŸŸå¹¶æŠ‘åˆ¶ä¼ æ’­çš„ä¼ªå½±ï¼›è¯­ä¹‰ä¼˜å…ˆåŒç›¸é‡‡æ ·ï¼ˆSPDPSï¼‰ï¼Œä¸€ç§è‡ªé€‚åº”ç­–ç•¥ï¼Œåœ¨ç»†èŠ‚ä¼˜åŒ–ä¹‹å‰ä¼˜å…ˆè¯­ä¹‰æ­£ç¡®æ€§ã€‚è¿™äº›åˆ›æ–°ä½¿CvG-Diffèƒ½å¤Ÿåœ¨å°‘é‡è¿­ä»£åå®ç°é«˜è´¨é‡é‡å»ºï¼Œåœ¨AAPM-LDCTæ•°æ®é›†ä¸Šä»…ä½¿ç”¨10ä¸ªæ­¥éª¤å°±è¾¾åˆ°38.34 dBçš„PSNRå’Œ0.9518çš„SSIMã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCvG-Diffåœ¨ç¨€ç–è§†å›¾CTé‡å»ºæ–¹æ³•ä¸­è¡¨ç°å“è¶Šã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/CvG-Diff">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CvG-Diffå°†ç¨€ç–è§†å›¾CTé‡å»ºé‡æ–°è¡¨è¿°ä¸ºå¹¿ä¹‰æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥æ”¹è¿›ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>å¼•å…¥ç¡®å®šæ€§é€€åŒ–ç®—å­æ¥æ˜¾å¼å»ºæ¨¡ç”±è§’åº¦å­é‡‡æ ·å¼•èµ·çš„å›¾åƒåŸŸä¼ªå½±ã€‚</li>
<li>æå‡ºé”™è¯¯ä¼ æ’­å¤åˆè®­ç»ƒï¼ˆEPCTï¼‰ä»¥è¯†åˆ«é”™è¯¯åŒºåŸŸå¹¶æŠ‘åˆ¶ä¼ªå½±ä¼ æ’­ã€‚</li>
<li>è¯­ä¹‰ä¼˜å…ˆåŒç›¸é‡‡æ ·ï¼ˆSPDPSï¼‰ç­–ç•¥èƒ½è‡ªé€‚åº”åœ°ä¼˜å…ˆè¯­ä¹‰æ­£ç¡®æ€§ï¼Œå†è¿›è¡Œç»†èŠ‚ä¼˜åŒ–ã€‚</li>
<li>CvG-Diffå®ç°äº†é«˜è´¨é‡é‡å»ºï¼Œä»…ä½¿ç”¨å°‘é‡è¿­ä»£æ­¥éª¤ï¼Œä¸”åœ¨AAPM-LDCTæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å·²åœ¨æŒ‡å®šé“¾æ¥ä¸­å…¬å¼€ï¼Œä¾›å…¬ä¼—æŸ¥é˜…å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6a5dfce931c42b1013d67b6adff3b836.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bff220709efd5ce90a11cd2aaa0cab9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2680f468b64e6565703d16aa9b726f60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0509ca99b6b12d19eadf9bc176d2c999.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="BadBlocks-Low-Cost-and-Stealthy-Backdoor-Attacks-Tailored-for-Text-to-Image-Diffusion-Models"><a href="#BadBlocks-Low-Cost-and-Stealthy-Backdoor-Attacks-Tailored-for-Text-to-Image-Diffusion-Models" class="headerlink" title="BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for   Text-to-Image Diffusion Models"></a>BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for   Text-to-Image Diffusion Models</h2><p><strong>Authors:Yu Pan, Jiahao Chen, Lin Wang, Bingrong Dai, Yi Du</strong></p>
<p>In recent years, Diffusion models have achieved remarkable progress in the field of image generation. However, recent studies have shown that diffusion models are susceptible to backdoor attacks, in which attackers can manipulate the output by injecting covert triggers such as specific visual patterns or textual phrases into the training dataset. Fortunately, with the continuous advancement of defense techniques, defenders have become increasingly capable of identifying and mitigating most backdoor attacks using visual inspection and neural network-based detection methods. However, in this paper, we identify a novel type of backdoor threat that is more lightweight and covert than existing approaches, which we name BadBlocks, requires only about 30 of the computational resources and 20 GPU time typically needed by previous backdoor attacks, yet it successfully injects backdoors and evades the most advanced defense frameworks. BadBlocks enables attackers to selectively contaminate specific blocks within the UNet architecture of diffusion models while maintaining normal functionality in the remaining components. Experimental results demonstrate that BadBlocks achieves a high attack success rate and low perceptual quality loss , even under extremely constrained computational resources and GPU time. Moreover, BadBlocks is able to bypass existing defense frameworks, especially the attention-based backdoor detection method, highlighting it as a novel and noteworthy threat. Ablation studies further demonstrate that effective backdoor injection does not require fine-tuning the entire network and highlight the pivotal role of certain neural network layers in backdoor mapping. Overall, BadBlocks significantly reduces the barrier to conducting backdoor attacks in all aspects. It enables attackers to inject backdoors into large-scale diffusion models even using consumer-grade GPUs. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹å®¹æ˜“å—åˆ°åé—¨æ”»å‡»çš„å½±å“ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡å‘è®­ç»ƒæ•°æ®é›†ä¸­æ³¨å…¥éšè”½çš„è§¦å‘å™¨ï¼ˆä¾‹å¦‚ç‰¹å®šçš„è§†è§‰æ¨¡å¼æˆ–æ–‡æœ¬çŸ­è¯­ï¼‰æ¥æ“çºµè¾“å‡ºã€‚å¹¸è¿çš„æ˜¯ï¼Œéšç€é˜²å¾¡æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œé˜²å¾¡è€…è¶Šæ¥è¶Šèƒ½å¤Ÿä½¿ç”¨è§†è§‰æ£€æŸ¥å’ŒåŸºäºç¥ç»ç½‘ç»œçš„æ£€æµ‹æ–¹æ³•æ¥è¯†åˆ«å’Œç¼“è§£å¤§å¤šæ•°åé—¨æ”»å‡»ã€‚ç„¶è€Œï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯†åˆ«äº†ä¸€ç§æ¯”ç°æœ‰æ–¹æ³•æ›´è½»é‡çº§ã€æ›´éšè”½çš„æ–°å‹åé—¨å¨èƒï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºBadBlocksã€‚BadBlocksä»…éœ€è¦å¤§çº¦30çš„è®¡ç®—èµ„æºå’Œ20çš„GPUæ—¶é—´ï¼Œè€Œä»¥å‰çš„åé—¨æ”»å‡»é€šå¸¸éœ€è¦è¿™äº›èµ„æºï¼Œç„¶è€Œå®ƒå´èƒ½å¤ŸæˆåŠŸæ³¨å…¥åé—¨å¹¶ç»•è¿‡æœ€å…ˆè¿›çš„é˜²å¾¡æ¡†æ¶ã€‚BadBlocksä½¿æ”»å‡»è€…èƒ½å¤Ÿé€‰æ‹©æ€§åœ°æ±¡æŸ“æ‰©æ•£æ¨¡å‹UNetæ¶æ„ä¸­çš„ç‰¹å®šå—ï¼ŒåŒæ—¶ä¿æŒå…¶ä½™ç»„ä»¶çš„æ­£å¸¸åŠŸèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æå…¶æœ‰é™çš„è®¡ç®—èµ„æºå’ŒGPUæ—¶é—´ä¸‹ï¼ŒBadBlocksä¹Ÿèƒ½å®ç°è¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡å’Œè¾ƒä½çš„å¯æ„ŸçŸ¥è´¨é‡æŸå¤±ã€‚è€Œä¸”ï¼ŒBadBlocksèƒ½å¤Ÿç»•è¿‡ç°æœ‰çš„é˜²å¾¡æ¡†æ¶ï¼Œå°¤å…¶æ˜¯åŸºäºæ³¨æ„åŠ›çš„åé—¨æ£€æµ‹æ–¹æ³•ï¼Œå‡¸æ˜¾å‡ºå®ƒæ˜¯ä¸€ç§æ–°å‹ä¸”å€¼å¾—å…³æ³¨çš„å¨èƒã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¡¨æ˜ï¼Œæœ‰æ•ˆçš„åé—¨æ³¨å…¥å¹¶ä¸éœ€è¦å¾®è°ƒæ•´ä¸ªç½‘ç»œï¼Œå¹¶çªå‡ºäº†æŸäº›ç¥ç»ç½‘ç»œå±‚åœ¨åé—¨æ˜ å°„ä¸­çš„å…³é”®ä½œç”¨ã€‚æ€»ä½“è€Œè¨€ï¼ŒBadBlocksåœ¨å„ä¸ªæ–¹é¢éƒ½å¤§å¤§é™ä½äº†è¿›è¡Œåé—¨æ”»å‡»çš„éšœç¢ã€‚å®ƒä½¿æ”»å‡»è€…å³ä½¿ä½¿ç”¨æ¶ˆè´¹çº§GPUä¹Ÿèƒ½å°†åé—¨æ³¨å…¥å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03221v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¿‘æœŸç ”ç©¶æŒ‡å‡ºå…¶æ˜“å—åé—¨æ”»å‡»å½±å“ã€‚æ”»å‡»è€…é€šè¿‡æ³¨å…¥ç‰¹å®šè§†è§‰æ¨¡å¼æˆ–æ–‡æœ¬çŸ­è¯­ç­‰éšè”½è§¦å‘å› ç´ ï¼Œæ“çºµè¾“å‡ºã€‚å°½ç®¡é˜²å¾¡æŠ€æœ¯ä¸æ–­å‘å±•ï¼Œå¤§å¤šæ•°åé—¨æ”»å‡»ä»å¯é€šè¿‡è§†è§‰æ£€æŸ¥å’Œç¥ç»ç½‘ç»œæ£€æµ‹æ–¹æ³•è¿›è¡Œè¯†åˆ«ä¸ç¼“è§£ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æ­ç¤ºäº†ä¸€ç§æ–°å‹åé—¨å¨èƒâ€”â€”BadBlocksã€‚BadBlocksè¾ƒç°æœ‰æ–¹æ³•æ›´ä¸ºè½»ä¾¿ã€éšè”½ï¼Œä»…éœ€çº¦30çš„è®¡ç®—èµ„æºå’Œ20çš„GPUæ—¶é—´ï¼Œä¾¿èƒ½æˆåŠŸæ³¨å…¥åé—¨å¹¶ç»•è¿‡æœ€å…ˆè¿›çš„é˜²å¾¡æ¡†æ¶ã€‚BadBlocksèƒ½é€‰æ‹©æ€§åœ°æ±¡æŸ“æ‰©æ•£æ¨¡å‹UNetæ¶æ„å†…çš„ç‰¹å®šåŒºå—ï¼ŒåŒæ—¶ä¿æŒå…¶ä½™ç»„ä»¶çš„æ­£å¸¸åŠŸèƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBadBlocksåœ¨æä½çš„è®¡ç®—èµ„æºå’ŒGPUæ—¶é—´ä¸‹ï¼Œä»èƒ½å®ç°é«˜æ”»å‡»æˆåŠŸç‡ä¸ä½æ„ŸçŸ¥è´¨é‡æŸå¤±ã€‚æ­¤å¤–ï¼ŒBadBlocksèƒ½ç»•è¿‡ç°æœ‰çš„é˜²å¾¡æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ³¨æ„åŠ›çš„åé—¨æ£€æµ‹æ–¹æ³•ï¼Œå±•ç°å…¶ä½œä¸ºæ–°å‹ä¸”å€¼å¾—å…³æ³¨çš„å¨èƒã€‚ç»¼åˆç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œæœ‰æ•ˆçš„åé—¨æ³¨å…¥æ— éœ€å¾®è°ƒæ•´ä¸ªç½‘ç»œï¼Œå¹¶çªå‡ºäº†æŸäº›ç¥ç»ç½‘ç»œå±‚åœ¨åé—¨æ˜ å°„ä¸­çš„å…³é”®ä½œç”¨ã€‚æ€»ä½“è€Œè¨€ï¼ŒBadBlocksæ˜¾è‘—é™ä½äº†è¿›è¡Œåé—¨æ”»å‡»çš„æ‰€æœ‰æ–¹é¢é—¨æ§›ï¼Œä½¿æ”»å‡»è€…å³ä½¿ä½¿ç”¨æ¶ˆè´¹çº§GPUï¼Œä¹Ÿèƒ½å°†åé—¨æ³¨å…¥å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹ä¸­ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾ç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†æ˜“å—åé—¨æ”»å‡»å½±å“ã€‚</li>
<li>BadBlocksæ˜¯ä¸€ç§æ–°å‹åé—¨å¨èƒï¼Œè¾ƒç°æœ‰æ–¹æ³•æ›´è½»ä¾¿ã€éšè”½ã€‚</li>
<li>BadBlocksèƒ½åœ¨æœ‰é™çš„è®¡ç®—èµ„æºå’ŒGPUæ—¶é—´ä¸‹æˆåŠŸæ³¨å…¥åé—¨ã€‚</li>
<li>BadBlocksèƒ½é€‰æ‹©æ€§åœ°æ±¡æŸ“æ‰©æ•£æ¨¡å‹çš„ç‰¹å®šåŒºå—ï¼ŒåŒæ—¶ä¿æŒå…¶ä»–éƒ¨åˆ†æ­£å¸¸è¿ä½œã€‚</li>
<li>BadBlockså®ç°äº†é«˜æ”»å‡»æˆåŠŸç‡ä¸ä½æ„ŸçŸ¥è´¨é‡æŸå¤±ã€‚</li>
<li>BadBlocksèƒ½ç»•è¿‡ç°æœ‰é˜²å¾¡æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯åŸºäºæ³¨æ„åŠ›çš„åé—¨æ£€æµ‹æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-537993d221bd2f352c4af5aacf50f6c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c9f685408054dc342919d6a74e05e4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75091792281a2f4eaa2ab61b326e672f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a1a57d1e9d54d805dc013b84b38cb5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-954264da8793b3152041455699afc95a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Quantitative-Comparison-of-Fine-Tuning-Techniques-for-Pretrained-Latent-Diffusion-Models-in-the-Generation-of-Unseen-SAR-Images"><a href="#Quantitative-Comparison-of-Fine-Tuning-Techniques-for-Pretrained-Latent-Diffusion-Models-in-the-Generation-of-Unseen-SAR-Images" class="headerlink" title="Quantitative Comparison of Fine-Tuning Techniques for Pretrained Latent   Diffusion Models in the Generation of Unseen SAR Images"></a>Quantitative Comparison of Fine-Tuning Techniques for Pretrained Latent   Diffusion Models in the Generation of Unseen SAR Images</h2><p><strong>Authors:SolÃ¨ne DebuysÃ¨re, Nicolas TrouvÃ©, Nathan Letheule, Olivier LÃ©vÃªque, Elise Colin</strong></p>
<p>We present a framework for adapting a large pretrained latent diffusion model to high-resolution Synthetic Aperture Radar (SAR) image generation. The approach enables controllable synthesis and the creation of rare or out-of-distribution scenes beyond the training set. Rather than training a task-specific small model from scratch, we adapt an open-source text-to-image foundation model to the SAR modality, using its semantic prior to align prompts with SAR imaging physics (side-looking geometry, slant-range projection, and coherent speckle with heavy-tailed statistics). Using a 100k-image SAR dataset, we compare full fine-tuning and parameter-efficient Low-Rank Adaptation (LoRA) across the UNet diffusion backbone, the Variational Autoencoder (VAE), and the text encoders. Evaluation combines (i) statistical distances to real SAR amplitude distributions, (ii) textural similarity via Gray-Level Co-occurrence Matrix (GLCM) descriptors, and (iii) semantic alignment using a SAR-specialized CLIP model. Our results show that a hybrid strategy-full UNet tuning with LoRA on the text encoders and a learned token embedding-best preserves SAR geometry and texture while maintaining prompt fidelity. The framework supports text-based control and multimodal conditioning (e.g., segmentation maps, TerraSAR-X, or optical guidance), opening new paths for large-scale SAR scene data augmentation and unseen scenario simulation in Earth observation. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€‚åº”å¤§å‹é¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹ç”¨äºé«˜åˆ†è¾¨ç‡åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å›¾åƒç”Ÿæˆçš„æ¡†æ¶ã€‚è¯¥æ–¹æ³•å¯å®ç°å¯æ§åˆæˆï¼Œå¹¶åˆ›å»ºè¶…å‡ºè®­ç»ƒé›†èŒƒå›´çš„ç¨€æœ‰æˆ–å¼‚å¸¸åœºæ™¯ã€‚æˆ‘ä»¬å¹¶ä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªç‰¹å®šä»»åŠ¡çš„å°å‹æ¨¡å‹ï¼Œè€Œæ˜¯å°†å¼€æºçš„æ–‡æœ¬åˆ°å›¾åƒåŸºç¡€æ¨¡å‹é€‚åº”åˆ°SARæ¨¡å¼ï¼Œåˆ©ç”¨å…¶è¯­ä¹‰å…ˆéªŒæ¥ä½¿æç¤ºä¸SARæˆåƒç‰©ç†ï¼ˆä¾§è§†å‡ ä½•ã€æ–œè·æŠ•å½±å’Œå…·æœ‰é‡å°¾ç»Ÿè®¡çš„ç›¸å¹²æ–‘ç‚¹ï¼‰å¯¹é½ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªåŒ…å«10ä¸‡å¼ å›¾åƒçš„SARæ•°æ®é›†ï¼Œå…¨é¢å¾®è°ƒäº†UNetæ‰©æ•£ä¸»å¹²ã€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œå¹¶è¿›è¡Œäº†å‚æ•°é«˜æ•ˆçš„ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æ¯”è¾ƒã€‚è¯„ä¼°ç»“åˆäº†ï¼ˆiï¼‰ä¸çœŸå®SARæŒ¯å¹…åˆ†å¸ƒçš„ç»Ÿè®¡è·ç¦»ï¼Œï¼ˆiiï¼‰é€šè¿‡ç°åº¦å…±ç”ŸçŸ©é˜µï¼ˆGLCMï¼‰æè¿°ç¬¦çš„çº¹ç†ç›¸ä¼¼æ€§ï¼Œä»¥åŠï¼ˆiiiï¼‰ä½¿ç”¨SARä¸“ç”¨CLIPæ¨¡å‹çš„è¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ··åˆç­–ç•¥â€”â€”å…¨é¢è°ƒæ•´UNetï¼Œå¹¶åœ¨æ–‡æœ¬ç¼–ç å™¨å’Œå­¦ä¹ çš„ä»¤ç‰ŒåµŒå…¥ä¸Šä½¿ç”¨LoRAâ€”â€”æœ€èƒ½ä¿æŒSARçš„å‡ ä½•å’Œçº¹ç†ï¼ŒåŒæ—¶ä¿æŒæç¤ºä¿çœŸåº¦ã€‚è¯¥æ¡†æ¶æ”¯æŒåŸºäºæ–‡æœ¬çš„æ§åˆ¶å’Œå¤šæ¨¡å¼æ¡ä»¶ï¼ˆä¾‹å¦‚ï¼Œåˆ†å‰²å›¾ã€TerraSAR-Xæˆ–å…‰å­¦æŒ‡å¯¼ï¼‰ï¼Œä¸ºåœ°çƒè§‚æµ‹ä¸­çš„å¤§è§„æ¨¡SARåœºæ™¯æ•°æ®å¢å¼ºå’Œæœªè§åœºæ™¯æ¨¡æ‹Ÿå¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13307v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€‚åº”é«˜åˆ†è¾¨ç‡åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å›¾åƒç”Ÿæˆçš„æ½œæ‰©æ•£æ¨¡å‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•å¯å®ç°å¯æ§åˆæˆï¼Œå¹¶åˆ›å»ºè¶…å‡ºè®­ç»ƒé›†çš„ç¨€æœ‰æˆ–éå¸¸è§åœºæ™¯ã€‚ç ”ç©¶é‡‡ç”¨å¼€æºæ–‡æœ¬åˆ°å›¾åƒåŸºç¡€æ¨¡å‹é€‚åº”SARæ¨¡å¼ï¼Œåˆ©ç”¨è¯­ä¹‰å…ˆéªŒå¯¹é½SARæˆåƒç‰©ç†ï¼ˆæ–œè§†å‡ ä½•ã€æ–œè·æŠ•å½±å’Œå…·æœ‰é‡å°¾ç»Ÿè®¡ç‰¹æ€§çš„ç›¸å¹²æ–‘ç‚¹ï¼‰ã€‚åœ¨10ä¸‡å¼ SARå›¾åƒæ•°æ®é›†ä¸Šï¼Œæ¯”è¾ƒäº†å®Œå…¨å¾®è°ƒä¸ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯åœ¨UNetæ‰©æ•£ä¸»å¹²ã€å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œæ–‡æœ¬ç¼–ç å™¨ä¸­çš„è¡¨ç°ã€‚è¯„ä¼°åŒ…æ‹¬ï¼ˆiï¼‰ä¸çœŸå®SARæŒ¯å¹…åˆ†å¸ƒä¹‹é—´çš„ç»Ÿè®¡è·ç¦»ï¼Œï¼ˆiiï¼‰é€šè¿‡ç°åº¦å…±ç”ŸçŸ©é˜µï¼ˆGLCMï¼‰æè¿°ç¬¦çš„çº¹ç†ç›¸ä¼¼æ€§ï¼Œï¼ˆiiiï¼‰ä½¿ç”¨SARä¸“ç”¨CLIPæ¨¡å‹çš„è¯­ä¹‰å¯¹é½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ··åˆç­–ç•¥â€”â€”å®Œå…¨è°ƒæ•´UNetç»“æ„ï¼ŒåŒæ—¶å¯¹æ–‡æœ¬ç¼–ç å™¨å’Œæ ‡è®°åµŒå…¥ä½¿ç”¨LoRAæŠ€æœ¯â€”â€”æœ€èƒ½ä¿ç•™SARçš„å‡ ä½•å’Œçº¹ç†ç‰¹å¾ï¼ŒåŒæ—¶ä¿æŒæç¤ºä¿çœŸåº¦ã€‚è¯¥æ¡†æ¶æ”¯æŒåŸºäºæ–‡æœ¬çš„æ§åˆ¶å’Œå¤šæ¨¡å¼æ¡ä»¶ï¼ˆå¦‚åˆ†å‰²å›¾ã€TerraSAR-Xæˆ–å…‰å­¦æŒ‡å¯¼ï¼‰ï¼Œä¸ºå¤§è§„æ¨¡SARåœºæ™¯æ•°æ®å¢å¼ºå’Œæœªè§åœºæ™¯æ¨¡æ‹Ÿåœ¨åœ°çƒè§‚æµ‹ä¸­å¼€è¾Ÿæ–°é€”å¾„ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>ä»‹ç»äº†ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¡†æ¶æ¥é€‚åº”é«˜åˆ†è¾¨ç‡SARå›¾åƒç”Ÿæˆçš„æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒé€šè¿‡è¯­ä¹‰å…ˆéªŒå°†æ–‡æœ¬æç¤ºä¸SARæˆåƒç‰©ç†å­¦å¯¹é½çš„é‡è¦æ€§ã€‚</li>
<li>é€šè¿‡å®éªŒå¯¹æ¯”äº†å¾®è°ƒæ•´ä¸ªUNetæ‰©æ•£æ¨¡å‹å’Œä»…ä½¿ç”¨LoRAæŠ€æœ¯è°ƒæ•´éƒ¨åˆ†ç»„ä»¶çš„æ•ˆæœã€‚</li>
<li>è¯„ä¼°æŒ‡æ ‡æ¶µç›–äº†ç»Ÿè®¡è·ç¦»ã€çº¹ç†ç›¸ä¼¼æ€§å’Œè¯­ä¹‰å¯¹é½ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„SARå›¾åƒçœŸå®ä¸”å…·æœ‰æ„ä¹‰ã€‚</li>
<li>ç ”ç©¶ç»“æœæ˜¾ç¤ºæ··åˆç­–ç•¥åœ¨ä¿ç•™SARç‰¹æ€§ä¸ä¿æŒæç¤ºä¿çœŸåº¦æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
<li>æ¡†æ¶æ”¯æŒåŸºäºæ–‡æœ¬çš„æ§åˆ¶å’Œå¤šæ¨¡å¼æ¡ä»¶ï¼Œä¸ºæœªæ¥å¤§è§„æ¨¡SARåœºæ™¯æ•°æ®å¢å¼ºå’Œæ¨¡æ‹Ÿå¼€è¾Ÿæ–°é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4dbd9a1de1c37fe752c8e0f99a13a627.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58fddf029d9162aec0f7cc665230556c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55f91858683bec9a567d492b163e691b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-658e7e1180077c09a4076bba3bc83ac1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a850245ed4061fce9d2c365fc01067cd.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Physics-Guided-Image-Dehazing-Diffusion"><a href="#Physics-Guided-Image-Dehazing-Diffusion" class="headerlink" title="Physics-Guided Image Dehazing Diffusion"></a>Physics-Guided Image Dehazing Diffusion</h2><p><strong>Authors:Shijun Zhou, Baojie Fan, Jiandong Tian</strong></p>
<p>Due to the domain gap between real-world and synthetic hazy images, current data-driven dehazing algorithms trained on synthetic datasets perform well on synthetic data but struggle to generalize to real-world scenarios. To address this challenge, we propose \textbf{I}mage \textbf{D}ehazing \textbf{D}iffusion \textbf{M}odels (IDDM), a novel diffusion process that incorporates the atmospheric scattering model into noise diffusion. IDDM aims to use the gradual haze formation process to help the denoising Unet robustly learn the distribution of clear images from the conditional input hazy images. We design a specialized training strategy centered around IDDM. Diffusion models are leveraged to bridge the domain gap from synthetic to real-world, while the atmospheric scattering model provides physical guidance for haze formation. During the forward process, IDDM simultaneously introduces haze and noise into clear images, and then robustly separates them during the sampling process. By training with physics-guided information, IDDM shows the ability of domain generalization, and effectively restores the real-world hazy images despite being trained on synthetic datasets. Extensive experiments demonstrate the effectiveness of our method through both quantitative and qualitative comparisons with state-of-the-art approaches. </p>
<blockquote>
<p>ç”±äºçœŸå®ä¸–ç•Œå’Œåˆæˆé›¾å›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œå½“å‰åŸºäºåˆæˆæ•°æ®é›†è®­ç»ƒçš„æ•°æ®é©±åŠ¨å»é›¾ç®—æ³•åœ¨åˆæˆæ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨çœŸå®åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å›¾åƒå»é›¾æ‰©æ•£æ¨¡å‹ï¼ˆIDDMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ‰©æ•£è¿‡ç¨‹ï¼Œå°†å¤§æ°”æ•£å°„æ¨¡å‹èå…¥å™ªå£°æ‰©æ•£ä¸­ã€‚IDDMæ—¨åœ¨åˆ©ç”¨é€æ­¥çš„é›¾å½¢æˆè¿‡ç¨‹å¸®åŠ©å»å™ªU-Netä»æ¡ä»¶è¾“å…¥é›¾å›¾åƒä¸­ç¨³å¥åœ°å­¦ä¹ æ¸…æ™°å›¾åƒåˆ†å¸ƒã€‚æˆ‘ä»¬å›´ç»•IDDMè®¾è®¡äº†ä¸€ç§ä¸“é—¨çš„è®­ç»ƒç­–ç•¥ã€‚æ‰©æ•£æ¨¡å‹è¢«ç”¨æ¥ç¼©å°åˆæˆå’ŒçœŸå®ä¸–ç•Œä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œè€Œå¤§æ°”æ•£å°„æ¨¡å‹ä¸ºé›¾çš„å½¢æˆæä¾›äº†ç‰©ç†æŒ‡å¯¼ã€‚åœ¨æ­£å‘è¿‡ç¨‹ä¸­ï¼ŒIDDMåŒæ—¶å°†é›¾å’Œå™ªå£°å¼•å…¥æ¸…æ™°å›¾åƒï¼Œç„¶ååœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ç¨³å¥åœ°åˆ†ç¦»å®ƒä»¬ã€‚é€šè¿‡ç‰©ç†æŒ‡å¯¼ä¿¡æ¯è¿›è¡Œè®­ç»ƒï¼ŒIDDMå±•ç¤ºäº†é¢†åŸŸæ³›åŒ–çš„èƒ½åŠ›ï¼Œå¹¶æœ‰æ•ˆåœ°æ¢å¤äº†çœŸå®ä¸–ç•Œçš„é›¾å›¾åƒï¼Œå°½ç®¡å®ƒæ˜¯åœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸æœ€æ–°æ–¹æ³•çš„å®šé‡å’Œå®šæ€§æ¯”è¾ƒï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21385v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºå›¾åƒå»é›¾æ‰©æ•£æ¨¡å‹ï¼ˆIDDMï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åˆæˆæ•°æ®é›†è®­ç»ƒçš„ç°æœ‰æ•°æ®é©±åŠ¨å»é›¾ç®—æ³•åœ¨çœŸå®åœºæ™¯ä¸‹çš„æ³›åŒ–é—®é¢˜ã€‚IDDMç»“åˆå¤§æ°”æ•£å°„æ¨¡å‹è¿›è¡Œå™ªå£°æ‰©æ•£ï¼Œåˆ©ç”¨é€æ­¥çš„é›¾å½¢æˆè¿‡ç¨‹å¸®åŠ©å»å™ªU-Netä»æ¡ä»¶è¾“å…¥çš„å»é›¾å›¾åƒä¸­å­¦ä¹ æ¸…æ™°å›¾åƒåˆ†å¸ƒã€‚é€šè¿‡ç‰¹å®šçš„è®­ç»ƒç­–ç•¥ï¼ŒIDDMèƒ½å¤Ÿç¼©å°åˆæˆä¸çœŸå®ä¸–ç•Œä¹‹é—´çš„é¢†åŸŸå·®è·ï¼ŒåŒæ—¶å¤§æ°”æ•£å°„æ¨¡å‹ä¸ºé›¾çš„å½¢æˆæä¾›äº†ç‰©ç†æŒ‡å¯¼ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œå»é›¾ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IDDMæ˜¯ä¸€ç§æ–°çš„å›¾åƒå»é›¾æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åˆæˆæ•°æ®é›†è®­ç»ƒçš„ç®—æ³•åœ¨çœŸå®åœºæ™¯ä¸‹çš„æ³›åŒ–é—®é¢˜ã€‚</li>
<li>IDDMç»“åˆå™ªå£°æ‰©æ•£å’Œå¤§æ°”æ•£å°„æ¨¡å‹ï¼Œåˆ©ç”¨é€æ­¥é›¾å½¢æˆè¿‡ç¨‹å¸®åŠ©å­¦ä¹ æ¸…æ™°å›¾åƒåˆ†å¸ƒã€‚</li>
<li>IDDMè®¾è®¡äº†ä¸€ç§ä»¥æ¨¡å‹ä¸ºä¸­å¿ƒçš„è®­ç»ƒç­–ç•¥ï¼Œèƒ½å¤Ÿç¼©å°åˆæˆä¸çœŸå®ä¸–ç•Œä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚</li>
<li>å¤§æ°”æ•£å°„æ¨¡å‹ä¸ºé›¾çš„å½¢æˆæä¾›äº†ç‰©ç†æŒ‡å¯¼ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å»é›¾èƒ½åŠ›ã€‚</li>
<li>IDDMåœ¨çœŸå®ä¸–ç•Œå»é›¾ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œé€šè¿‡å®šé‡å’Œå®šæ€§å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>IDDMé€šè¿‡å¼•å…¥é›¾å’Œå™ªå£°åˆ°æ¸…æ™°å›¾åƒï¼Œç„¶ååœ¨å…¶é‡‡æ ·è¿‡ç¨‹ä¸­ç¨³å¥åœ°åˆ†ç¦»å®ƒä»¬ï¼Œå®ç°äº†å»é›¾æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-151a137f4df40c99783668825efd8e10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fe9bb516ff1ee7ec999e1091cf819af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fc7c3102944b918cd6fdd54d01c6b0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dbc00bd909012b33b0989d28f3c7825.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b10942621b4f4e83b0600ea6fec28e5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MUNBa-Machine-Unlearning-via-Nash-Bargaining"><a href="#MUNBa-Machine-Unlearning-via-Nash-Bargaining" class="headerlink" title="MUNBa: Machine Unlearning via Nash Bargaining"></a>MUNBa: Machine Unlearning via Nash Bargaining</h2><p><strong>Authors:Jing Wu, Mehrtash Harandi</strong></p>
<p>Machine Unlearning (MU) aims to selectively erase harmful behaviors from models while retaining the overall utility of the model. As a multi-task learning problem, MU involves balancing objectives related to forgetting specific concepts&#x2F;data and preserving general performance. A naive integration of these forgetting and preserving objectives can lead to gradient conflicts and dominance, impeding MU algorithms from reaching optimal solutions. To address the gradient conflict and dominance issue, we reformulate MU as a two-player cooperative game, where the two players, namely, the forgetting player and the preservation player, contribute via their gradient proposals to maximize their overall gain and balance their contributions. To this end, inspired by the Nash bargaining theory, we derive a closed-form solution to guide the model toward the Pareto stationary point. Our formulation of MU guarantees an equilibrium solution, where any deviation from the final state would lead to a reduction in the overall objectives for both players, ensuring optimality in each objective. We evaluate our algorithmâ€™s effectiveness on a diverse set of tasks across image classification and image generation. Extensive experiments with ResNet, vision-language model CLIP, and text-to-image diffusion models demonstrate that our method outperforms state-of-the-art MU algorithms, achieving a better trade-off between forgetting and preserving. Our results also highlight improvements in forgetting precision, preservation of generalization, and robustness against adversarial attacks. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ åé—å¿˜ï¼ˆMUï¼‰æ—¨åœ¨ä»æ¨¡å‹ä¸­å‰”é™¤æœ‰å®³è¡Œä¸ºï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„æ•´ä½“å®ç”¨æ€§ã€‚ä½œä¸ºå¤šä»»åŠ¡å­¦ä¹ é—®é¢˜ï¼ŒMUæ¶‰åŠå¹³è¡¡ä¸é—å¿˜ç‰¹å®šæ¦‚å¿µ&#x2F;æ•°æ®ç›¸å…³çš„ç›®æ ‡å’Œä¿æŒæ•´ä½“æ€§èƒ½çš„ç›®æ ‡ã€‚è¿™äº›é—å¿˜å’Œä¿ç•™ç›®æ ‡çš„ç®€å•é›†æˆå¯èƒ½å¯¼è‡´æ¢¯åº¦å†²çªå’Œä¸»å¯¼ï¼Œé˜»ç¢MUç®—æ³•è¾¾åˆ°æœ€ä¼˜è§£ã€‚ä¸ºäº†è§£å†³æ¢¯åº¦å†²çªå’Œä¸»å¯¼é—®é¢˜ï¼Œæˆ‘ä»¬å°†MUé‡æ–°æ„å»ºä¸ºä¸€ä¸ªä¸¤ç©å®¶åˆä½œæ¸¸æˆï¼Œå…¶ä¸­ä¸¤ä¸ªç©å®¶åˆ†åˆ«æ˜¯é—å¿˜ç©å®¶å’Œä¿ç•™ç©å®¶ï¼Œä»–ä»¬é€šè¿‡å„è‡ªçš„æ¢¯åº¦ææ¡ˆæ¥æœ€å¤§åŒ–æ•´ä½“æ”¶ç›Šå¹¶å¹³è¡¡å„è‡ªçš„è´¡çŒ®ã€‚ä¸ºæ­¤ï¼Œå—åˆ°çº³ä»€è°ˆåˆ¤ç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ä¸ªå°é—­å½¢å¼çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥å¼•å¯¼æ¨¡å‹èµ°å‘å¸•ç´¯æ‰˜ç¨³å®šç‚¹ã€‚æˆ‘ä»¬å¯¹MUçš„å…¬å¼åŒ–ä¿è¯äº†å‡è¡¡è§£ï¼Œå³ä»»ä½•åç¦»æœ€ç»ˆçŠ¶æ€çš„è¡Œä¸ºéƒ½ä¼šå¯¼è‡´æ•´ä½“ç›®æ ‡å‡å°‘ï¼Œä»è€Œç¡®ä¿æ¯ä¸ªç›®æ ‡çš„ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨å›¾åƒåˆ†ç±»å’Œå›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ä¸Šè¯„ä¼°äº†ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚ä½¿ç”¨ResNetã€è§†è§‰è¯­è¨€æ¨¡å‹CLIPå’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é—å¿˜å’Œä¿ç•™ä¹‹é—´å–å¾—äº†æ›´å¥½çš„æƒè¡¡ï¼Œä¼˜äºæœ€å…ˆè¿›çš„MUç®—æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜å‡¸æ˜¾äº†é—å¿˜ç²¾ç¡®åº¦ã€ä¿æŒæ³›åŒ–èƒ½åŠ›ä»¥åŠå¯¹æŠ—æ”»å‡»ç¨³å¥æ€§çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15537v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„æœºå™¨é—å¿˜ï¼ˆMUï¼‰æ—¨åœ¨é€‰æ‹©æ€§åˆ é™¤æ¨¡å‹ä¸­çš„æœ‰å®³è¡Œä¸ºï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„æ€»ä½“æ•ˆç”¨ã€‚ä½œä¸ºå¤šä»»åŠ¡å­¦ä¹ é—®é¢˜ï¼ŒMUéœ€è¦å¹³è¡¡å¿˜è®°ç‰¹å®šæ¦‚å¿µå’Œä¿å­˜æ•´ä½“æ€§èƒ½çš„ç›®æ ‡ã€‚ä¸ºäº†è§£å†³æ¢¯åº¦å†²çªå’Œä¸»å¯¼é—®é¢˜ï¼Œæˆ‘ä»¬å°†MUé‡æ–°æ„å»ºä¸ºä¸€ä¸ªåŒäººåˆä½œæ¸¸æˆï¼ŒåŒ…æ‹¬é—å¿˜ç©å®¶å’Œä¿ç•™ç©å®¶ï¼Œä»–ä»¬é€šè¿‡æ¢¯åº¦ææ¡ˆæ¥æœ€å¤§åŒ–æ•´ä½“æ”¶ç›Šå¹¶å¹³è¡¡å„è‡ªè´¡çŒ®ã€‚å—çº³ä»€è°ˆåˆ¤ç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†ä¸€ä¸ªå°é—­å½¢å¼çš„è§£å†³æ–¹æ¡ˆæ¥æŒ‡å¯¼æ¨¡å‹èµ°å‘å¸•ç´¯æ‰˜ç¨³å®šç‚¹ã€‚æˆ‘ä»¬çš„MUå…¬å¼ä¿è¯äº†å‡è¡¡è§£ï¼Œä»»ä½•åç¦»æœ€ç»ˆçŠ¶æ€éƒ½ä¼šå¯¼è‡´åŒæ–¹æ€»ä½“ç›®æ ‡å‡å°‘ï¼Œä»è€Œç¡®ä¿æ¯ä¸ªç›®æ ‡çš„ä¼˜åŒ–ã€‚åœ¨å›¾åƒåˆ†ç±»å’Œå›¾åƒç”Ÿæˆç­‰å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨é—å¿˜å’Œä¿ç•™ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œä¼˜äºæœ€æ–°çš„MUç®—æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜æ˜¾ç¤ºäº†æé«˜é—å¿˜ç²¾åº¦ã€ä¿æŒæ³›åŒ–èƒ½åŠ›å’Œå¢å¼ºå¯¹å¯¹æŠ—æ”»å‡»çš„ç¨³å¥æ€§ç­‰ä¼˜ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨é—å¿˜ï¼ˆMUï¼‰æ—¨åœ¨ä»æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­åˆ é™¤æœ‰å®³è¡Œä¸ºï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ•´ä½“æ•ˆç”¨ã€‚</li>
<li>MUä½œä¸ºå¤šä»»åŠ¡å­¦ä¹ é—®é¢˜ï¼Œéœ€è¦å¹³è¡¡å¿˜è®°ç‰¹å®šæ¦‚å¿µå’Œä¿æŒæ•´ä½“æ€§èƒ½çš„ç›®æ ‡ã€‚</li>
<li>æ¢¯åº¦å†²çªå’Œä¸»å¯¼é—®é¢˜æ˜¯MUé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>å°†MUé‡æ–°æ„å»ºä¸ºåŒäººåˆä½œæ¸¸æˆï¼ŒåŒ…æ‹¬é—å¿˜ç©å®¶å’Œä¿ç•™ç©å®¶ï¼Œä»¥å¹³è¡¡å„è‡ªè´¡çŒ®ã€‚</li>
<li>å—çº³ä»€è°ˆåˆ¤ç†è®ºå¯å‘ï¼Œæ¨å¯¼å‡ºäº†å°é—­å½¢å¼çš„è§£å†³æ–¹æ¡ˆæ¥æŒ‡å¯¼æ¨¡å‹èµ°å‘å¸•ç´¯æ‰˜ç¨³å®šç‚¹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥ç®—æ³•åœ¨é—å¿˜å’Œä¿ç•™ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ï¼Œä¼˜äºå…¶ä»–MUç®—æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-01753026a95b0c80bb4e8d47fb4a4e5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ac2870e24d1cc6241c80e2c95acc807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52e9a49bdb0f38fefeeaa7cf435e49cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-510f7093481ccd57572760d85c6fde36.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-19/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-409025a5b77b6e0cd179f75508e39a11.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  DashCam Video A complementary low-cost data stream for on-demand   forest-infrastructure system monitoring
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-19/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8e4aa5af078a50a81b2fa7a82fb96f0e.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-19  GANDiff FR Hybrid GAN Diffusion Synthesis for Causal Bias Attribution   in Face Recognition
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27768.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
