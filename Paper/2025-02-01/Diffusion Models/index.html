<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-02-01  DiffusionRenderer Neural Inverse and Forward Rendering with Video   Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-96d5591974fd0a745f3e1b0631a054d7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-01-更新"><a href="#2025-02-01-更新" class="headerlink" title="2025-02-01 更新"></a>2025-02-01 更新</h1><h2 id="DiffusionRenderer-Neural-Inverse-and-Forward-Rendering-with-Video-Diffusion-Models"><a href="#DiffusionRenderer-Neural-Inverse-and-Forward-Rendering-with-Video-Diffusion-Models" class="headerlink" title="DiffusionRenderer: Neural Inverse and Forward Rendering with Video   Diffusion Models"></a>DiffusionRenderer: Neural Inverse and Forward Rendering with Video   Diffusion Models</h2><p><strong>Authors:Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, Zian Wang</strong></p>
<p>Understanding and modeling lighting effects are fundamental tasks in computer vision and graphics. Classic physically-based rendering (PBR) accurately simulates the light transport, but relies on precise scene representations–explicit 3D geometry, high-quality material properties, and lighting conditions–that are often impractical to obtain in real-world scenarios. Therefore, we introduce DiffusionRenderer, a neural approach that addresses the dual problem of inverse and forward rendering within a holistic framework. Leveraging powerful video diffusion model priors, the inverse rendering model accurately estimates G-buffers from real-world videos, providing an interface for image editing tasks, and training data for the rendering model. Conversely, our rendering model generates photorealistic images from G-buffers without explicit light transport simulation. Experiments demonstrate that DiffusionRenderer effectively approximates inverse and forwards rendering, consistently outperforming the state-of-the-art. Our model enables practical applications from a single video input–including relighting, material editing, and realistic object insertion. </p>
<blockquote>
<p>理解和建模光照效果是计算机视觉和图形学中的基本任务。经典的基于物理的渲染（PBR）能准确模拟光传输，但依赖于精确的场景表示——明确的3D几何、高质量的材料属性和光照条件，这些在真实世界场景中往往难以获得。因此，我们引入了DiffusionRenderer，这是一种神经方法，在一个整体框架内解决了逆向和正向渲染的双重问题。利用强大的视频扩散模型先验知识，逆向渲染模型能准确地从真实视频估计G缓冲区，为图像编辑任务提供接口，并为渲染模型提供训练数据。相反，我们的渲染模型能够从G缓冲区生成逼真的图像，而无需显式模拟光传输。实验表明，DiffusionRenderer有效地近似了逆向和正向渲染，并始终优于当前最佳水平。我们的模型能够从单个视频输入中实现实际应用，包括重新照明、材料编辑和逼真的对象插入。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18590v1">PDF</a> Project page: research.nvidia.com&#x2F;labs&#x2F;toronto-ai&#x2F;DiffusionRenderer&#x2F;</p>
<p><strong>Summary</strong></p>
<p>神经网络渲染器DiffusionRenderer结合了逆向和正向渲染，借助视频扩散模型先验准确估计G缓冲区，从真实视频生成图像编辑任务的接口和渲染模型的训练数据。渲染模型可直接从G缓冲区生成逼真的图像，无需显式模拟光线传输。实验证明，DiffusionRenderer在逆向和正向渲染方面的表现优于现有技术，并能从单一视频输入实现实用应用，如重新照明、材料编辑和真实对象插入。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffusionRenderer结合了逆向和正向渲染。</li>
<li>利用视频扩散模型准确估计G缓冲区，有助于真实视频的图像编辑任务及渲染模型的训练。</li>
<li>无需模拟显式光线传输就能从G缓冲区生成逼真的图像。</li>
<li>DiffusionRenderer在逆向和正向渲染方面的表现优于现有技术。</li>
<li>该模型可从单一视频输入实现实用应用，如重新照明、材料编辑和真实对象插入等。</li>
<li>该方法能应对复杂场景的渲染问题，提供更高的灵活性和逼真度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18590">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-82a2cad7ab583ef8654768271578a073.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b9aef01fa7e6b97aead43a26d56600a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08d9acd1c63b55302d05e729ea0bb4bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-519896a11b3dc43a386cd7af553205db.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SAeUron-Interpretable-Concept-Unlearning-in-Diffusion-Models-with-Sparse-Autoencoders"><a href="#SAeUron-Interpretable-Concept-Unlearning-in-Diffusion-Models-with-Sparse-Autoencoders" class="headerlink" title="SAeUron: Interpretable Concept Unlearning in Diffusion Models with   Sparse Autoencoders"></a>SAeUron: Interpretable Concept Unlearning in Diffusion Models with   Sparse Autoencoders</h2><p><strong>Authors:Bartosz Cywiński, Kamil Deja</strong></p>
<p>Recent machine unlearning approaches offer promising solution for removing unwanted concepts from diffusion models. However, traditional methods, which largely rely on fine-tuning, provide little insight into the changes they introduce to the base model, making it unclear whether concepts are truly removed or only masked. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to unlearn unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a method of selecting concept-specific features. This enables precise interventions on the model’s activations to block targeted content while preserving the model’s overall performance. Evaluation on the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron’s state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron dismisses the possibility of generating unwanted content, even under adversarial attack. </p>
<blockquote>
<p>最近，机器学习去学习的技术为从扩散模型中移除不需要的概念提供了很有前景的解决方案。然而，传统的方法大多依赖于微调，对于它们给基础模型带来的改变并没有提供太多见解，因此不清楚概念是否真的被移除，还只是被掩盖了。在这项工作中，我们引入了SAeUron，这是一种利用稀疏自编码器（SAE）学习到的特征来去除文本到图像扩散模型中不需要的概念的新方法。首先，我们证明在扩散模型的多个去噪时间步长的激活上接受无监督训练的SAE能够捕获对应特定概念的特征，这些特征稀疏且可解释性强。在此基础上，我们提出了一种选择特定概念特征的方法。这能够对模型的激活进行精确干预，以阻止目标内容，同时保持模型的总体性能。在竞争性的UnlearnCanvas基准测试上进行的对象与风格去学习的评估凸显了SAeUron的卓越性能。此外，我们展示了使用一个单一的SAE可以同时移除多个概念，并且与其他方法相比，SAeUron消除了生成不需要内容的可能性，即使在对抗攻击下也是如此。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18052v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的方法SAeUron，利用稀疏自编码器（SAE）的特性，从文本到图像的扩散模型中去除不需要的概念。该方法能够在不干扰模型整体性能的情况下，精确干预模型的激活，以阻止特定内容的生成。在UnlearnCanvas基准测试上的表现突出，可以同时去除多个概念，且不会生成不需要的内容，具有领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAeUron是一种利用稀疏自编码器（SAE）去除扩散模型中不需要概念的新方法。</li>
<li>SAE在多个去噪时间步长的扩散模型激活上进行无监督训练，能够捕获对应特定概念的特征。</li>
<li>SAeUron可以精确干预模型的激活，以去除特定内容，同时保持模型的总体性能。</li>
<li>在UnlearnCanvas基准测试上，SAeUron表现出卓越的性能，可同时进行对象和风格的去除。</li>
<li>SAeUron可以同时去除多个概念。</li>
<li>与其他方法相比，SAeUron在生成内容方面具有更高的可控性，不会生成不需要的内容。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18052">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4f26be24b03319ff23a555af7940de3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a66012f69e1a8da122b7856c64e66308.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac2793ddbe34ddfb401ccbaaca51d35d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b2e99192360b0df759338ab0c8d2816.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Can-Location-Embeddings-Enhance-Super-Resolution-of-Satellite-Imagery"><a href="#Can-Location-Embeddings-Enhance-Super-Resolution-of-Satellite-Imagery" class="headerlink" title="Can Location Embeddings Enhance Super-Resolution of Satellite Imagery?"></a>Can Location Embeddings Enhance Super-Resolution of Satellite Imagery?</h2><p><strong>Authors:Daniel Panangian, Ksenia Bittner</strong></p>
<p>Publicly available satellite imagery, such as Sentinel- 2, often lacks the spatial resolution required for accurate analysis of remote sensing tasks including urban planning and disaster response. Current super-resolution techniques are typically trained on limited datasets, leading to poor generalization across diverse geographic regions. In this work, we propose a novel super-resolution framework that enhances generalization by incorporating geographic context through location embeddings. Our framework employs Generative Adversarial Networks (GANs) and incorporates techniques from diffusion models to enhance image quality. Furthermore, we address tiling artifacts by integrating information from neighboring images, enabling the generation of seamless, high-resolution outputs. We demonstrate the effectiveness of our method on the building segmentation task, showing significant improvements over state-of-the-art methods and highlighting its potential for real-world applications. </p>
<blockquote>
<p>公开可用的卫星图像，如Sentinel-2，通常缺乏用于准确分析城市规划、灾害应对等遥感任务所需的空间分辨率。当前的超分辨率技术通常只在有限的数据集上进行训练，导致其在不同地理区域的泛化能力较差。在这项工作中，我们提出了一种新型超分辨率框架，它通过引入地理位置嵌入技术来提高地理上下文方面的泛化能力。我们的框架采用生成对抗网络（GANs），并结合扩散模型的技巧来提高图像质量。此外，我们还通过整合邻近图像的信息来解决拼贴痕迹问题，从而生成无缝、高分辨率的输出。我们在建筑分割任务上展示了该方法的有效性，相较于最先进的方法有明显的改进，并突出了其在现实世界应用中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15847v2">PDF</a> Accepted to IEEE&#x2F;CVF Winter Conference on Applications of Computer   Vision (WACV)</p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的超分辨率框架，该框架结合地理上下文通过位置嵌入增强泛化能力。它采用生成对抗网络（GANs）和扩散模型技术提高图像质量，并解决平铺伪影问题，通过整合邻近图像的信息来生成无缝、高分辨率的输出。在建筑物分割任务上，该方法效果显著，相较于现有方法有明显改进，并展现出在真实世界应用中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>公开卫星图像如Sentinel-2的空间分辨率通常不足以进行远程感应任务。</li>
<li>当前超分辨率技术通常在有限数据集上训练，导致在不同地理区域的泛化能力较差。</li>
<li>提出的新型超分辨率框架通过融入地理上下文和位置嵌入增强泛化能力。</li>
<li>框架采用生成对抗网络（GANs）和扩散模型技术提高图像质量。</li>
<li>框架解决了由于图像分割产生的平铺伪影问题，通过整合邻近图像信息生成无缝高分辨率输出。</li>
<li>该方法在建筑物分割任务上表现出优异效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15847">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-45043dcacd79e0d6ba042ecf6f8ef63c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-557fd575c806a577ff9095ae6b519170.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b14916c16d3d32298e69c2323e02fc21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b1919cb1b16c24a870540fda20c0312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b766eac585b1fe425eabd4dfb6a861b6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="EliGen-Entity-Level-Controlled-Image-Generation-with-Regional-Attention"><a href="#EliGen-Entity-Level-Controlled-Image-Generation-with-Regional-Attention" class="headerlink" title="EliGen: Entity-Level Controlled Image Generation with Regional Attention"></a>EliGen: Entity-Level Controlled Image Generation with Regional Attention</h2><p><strong>Authors:Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, Yu Zhang</strong></p>
<p>Recent advancements in diffusion models have significantly advanced text-to-image generation, yet global text prompts alone remain insufficient for achieving fine-grained control over individual entities within an image. To address this limitation, we present EliGen, a novel framework for Entity-level controlled image Generation. Firstly, we put forward regional attention, a mechanism for diffusion transformers that requires no additional parameters, seamlessly integrating entity prompts and arbitrary-shaped spatial masks. By contributing a high-quality dataset with fine-grained spatial and semantic entity-level annotations, we train EliGen to achieve robust and accurate entity-level manipulation, surpassing existing methods in both spatial precision and image quality. Additionally, we propose an inpainting fusion pipeline, extending its capabilities to multi-entity image inpainting tasks. We further demonstrate its flexibility by integrating it with other open-source models such as IP-Adapter, In-Context LoRA and MLLM, unlocking new creative possibilities. The source code, model, and dataset are published at <a target="_blank" rel="noopener" href="https://github.com/modelscope/DiffSynth-Studio.git">https://github.com/modelscope/DiffSynth-Studio.git</a>. </p>
<blockquote>
<p>最近扩散模型方面的进展极大地推动了文本到图像的生成，然而，仅使用全局文本提示仍不足以实现对图像内单个实体的精细控制。为了解决这一局限性，我们提出了EliGen，这是一个用于实体级别控制图像生成的新型框架。首先，我们提出了区域注意力机制，这是一种无需额外参数的扩散变压器机制，无缝集成了实体提示和任意形状的空间掩码。通过提供一个具有精细空间语义和实体级注释的高质量数据集，我们训练EliGen实现稳健准确的实体级操作，在空间和图像质量方面都超越了现有方法。此外，我们提出了一种补全融合管道，将其能力扩展到多实体图像补全任务。我们通过将其与其他开源模型（如IP-Adapter、In-Context LoRA和MLLM）集成，进一步展示了其灵活性，解锁了新的创意可能性。源代码、模型和数据集已发布在<a target="_blank" rel="noopener" href="https://github.com/modelscope/DiffSynth-Studio.git%E3%80%82">https://github.com/modelscope/DiffSynth-Studio.git。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01097v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期扩散模型的新进展极大地推动了文本到图像的生成能力，然而，仅依赖全局文本提示无法实现图像内个体实体的精细控制。为解决这一局限，我们提出了EliGen框架，实现实体级别的图像生成控制。通过引入无需额外参数的区域注意力机制，扩散变压器能够无缝集成实体提示和任意形状的空间掩码。我们使用精细的空间和语义实体级注释的高质量数据集训练EliGen，实现在实体级别的稳健和精确操控，在空间精度和图像质量上超越现有方法。此外，我们提出了扩展其能力的补全融合管道，用于多实体图像补全任务。通过与其他开源模型（如IP-Adapter、In-Context LoRA和MLLM）的集成，展示了其灵活性，开启了新的创意可能性。相关源码、模型和数据集已发布在<a target="_blank" rel="noopener" href="https://github.com/modelscope/DiffSynth-Studio.git%E3%80%82">https://github.com/modelscope/DiffSynth-Studio.git。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的最新进展已显著改善文本到图像的生成能力。</li>
<li>现有方法依赖于全局文本提示，无法实现图像内个体实体的精细控制。</li>
<li>提出了EliGen框架，通过区域注意力机制实现实体级别的图像生成控制。</li>
<li>无需额外参数，扩散变压器能无缝集成实体提示和空间掩码。</li>
<li>使用高质量数据集进行训练，实现稳健和精确的实体级别操控。</li>
<li>在空间精度和图像质量上超越现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01097">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5ad2543709a26b72b8cdacca26cd7396.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-539944f573a718dd36c331e8adcf3332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f01a12ea2b5ccfa91f3e1f4177e0664.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e23638d3b152f1506cbc1ee3b099a7af.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PixelMan-Consistent-Object-Editing-with-Diffusion-Models-via-Pixel-Manipulation-and-Generation"><a href="#PixelMan-Consistent-Object-Editing-with-Diffusion-Models-via-Pixel-Manipulation-and-Generation" class="headerlink" title="PixelMan: Consistent Object Editing with Diffusion Models via Pixel   Manipulation and Generation"></a>PixelMan: Consistent Object Editing with Diffusion Models via Pixel   Manipulation and Generation</h2><p><strong>Authors:Liyao Jiang, Negar Hassanpour, Mohammad Salameh, Mohammadreza Samadi, Jiao He, Fengyu Sun, Di Niu</strong></p>
<p>Recent research explores the potential of Diffusion Models (DMs) for consistent object editing, which aims to modify object position, size, and composition, etc., while preserving the consistency of objects and background without changing their texture and attributes. Current inference-time methods often rely on DDIM inversion, which inherently compromises efficiency and the achievable consistency of edited images. Recent methods also utilize energy guidance which iteratively updates the predicted noise and can drive the latents away from the original image, resulting in distortions. In this paper, we propose PixelMan, an inversion-free and training-free method for achieving consistent object editing via Pixel Manipulation and generation, where we directly create a duplicate copy of the source object at target location in the pixel space, and introduce an efficient sampling approach to iteratively harmonize the manipulated object into the target location and inpaint its original location, while ensuring image consistency by anchoring the edited image to be generated to the pixel-manipulated image as well as by introducing various consistency-preserving optimization techniques during inference. Experimental evaluations based on benchmark datasets as well as extensive visual comparisons show that in as few as 16 inference steps, PixelMan outperforms a range of state-of-the-art training-based and training-free methods (usually requiring 50 steps) on multiple consistent object editing tasks. </p>
<blockquote>
<p>近期研究探讨了Diffusion Models（DMs）在对象一致性编辑方面的潜力。对象一致性编辑旨在修改对象的位置、大小、组成等，同时保持对象的连贯性和背景一致性，而不改变其纹理和属性。当前推理时间的方法通常依赖于DDIM反演，这固有的牺牲了效率和编辑图像的可实现一致性。近期的方法还使用能量引导，通过迭代更新预测噪声并可能驱动潜在空间远离原始图像，从而导致失真。在本文中，我们提出了PixelMan，这是一种无需反演和训练的方法，通过像素操作和生成实现对象的一致性编辑。我们直接在像素空间中创建源对象的目标位置副本，并引入高效的采样方法来迭代将操作对象和谐地融合到目标位置并对其原始位置进行填充，同时通过锚定生成的编辑图像到像素操作图像以及引入各种一致性保持优化技术在推理过程中确保图像的一致性。基于基准数据集的实验评估以及广泛的视觉比较表明，在仅16步推理的情况下，PixelMan在多个对象一致性编辑任务上的表现优于一系列最先进的基于训练和免训练的方法（通常需要50步推理）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14283v2">PDF</a> AAAI 2025; version includes supplementary material; 27 Pages, 15   Figures, 6 Tables</p>
<p><strong>Summary</strong></p>
<p>本文研究了Diffusion Models在物体编辑方面的潜力，提出了一种名为PixelMan的方法，该方法无需反演和训练，即可通过像素操作和生成实现一致的物体编辑。它通过直接在目标位置创建源物体的副本，并采用高效的采样方法来迭代地将操作物体和谐地融入目标位置，同时保证图像的一致性。实验评估显示，PixelMan在少数推理步骤中就能优于一系列最先进的基于训练和非训练的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Models (DMs) 在物体编辑方面具有潜力，能够修改物体位置、大小、组成等，同时保持物体和背景的连贯性。</li>
<li>当前的方法常常依赖DDIM反演，这影响了效率和图像的一致性。</li>
<li>现有方法使用能量引导，但可能导致预测噪声的更新和潜在偏离原始图像，造成失真。</li>
<li>PixelMan方法通过像素操作和生成实现一致的物体编辑，无需反演和训练。</li>
<li>PixelMan通过在目标位置创建源物体的副本，并迭代地将操作物体融入目标位置。</li>
<li>PixelMan采用高效的采样方法来修复和和谐物体在原位置的形象，同时确保图像的一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14283">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-16774f6b7f3bd614b1ec3e3ef2ec68d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6878d114317453c51ba8a7dc633a365.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e1f64cc7b2eccd96e799f9fb373d211.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5e1b1a3d342c4abc3a9157d9ab18d8e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AniDoc-Animation-Creation-Made-Easier"><a href="#AniDoc-Animation-Creation-Made-Easier" class="headerlink" title="AniDoc: Animation Creation Made Easier"></a>AniDoc: Animation Creation Made Easier</h2><p><strong>Authors:Yihao Meng, Hao Ouyang, Hanlin Wang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Zhiheng Liu, Yujun Shen, Huamin Qu</strong></p>
<p>The production of 2D animation follows an industry-standard workflow, encompassing four essential stages: character design, keyframe animation, in-betweening, and coloring. Our research focuses on reducing the labor costs in the above process by harnessing the potential of increasingly powerful generative AI. Using video diffusion models as the foundation, AniDoc emerges as a video line art colorization tool, which automatically converts sketch sequences into colored animations following the reference character specification. Our model exploits correspondence matching as an explicit guidance, yielding strong robustness to the variations (e.g., posture) between the reference character and each line art frame. In addition, our model could even automate the in-betweening process, such that users can easily create a temporally consistent animation by simply providing a character image as well as the start and end sketches. Our code is available at: <a target="_blank" rel="noopener" href="https://yihao-meng.github.io/AniDoc_demo">https://yihao-meng.github.io/AniDoc_demo</a>. </p>
<blockquote>
<p>二维动画的制作遵循行业标准的工作流程，包括四个基本阶段：角色设计、关键帧动画、中间帧生成和上色。我们的研究致力于通过利用日益强大的生成式人工智能的潜力来降低上述过程中的劳动力成本。以视频扩散模型为基础，AniDoc作为一种视频线艺术上色工具应运而生，它会自动将草图序列转换为彩色动画，并遵循参考角色规范。我们的模型利用对应匹配作为明确指导，对参考角色和每个线艺术框架之间的变化（例如姿势）具有很强的稳健性。此外，我们的模型甚至可以自动化中间过程，这样用户只需提供角色图像以及开始和结束的草图，就可以轻松创建时间上连贯的动画。我们的代码在：<a target="_blank" rel="noopener" href="https://yihao-meng.github.io/AniDoc_demo%E3%80%82">https://yihao-meng.github.io/AniDoc_demo。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14173v2">PDF</a> Project page and code: <a target="_blank" rel="noopener" href="https://yihao-meng.github.io/AniDoc_demo">https://yihao-meng.github.io/AniDoc_demo</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一项利用视频扩散模型开发的新型动画生产工具——AniDoc。该工具旨在降低二维动画制作过程中的劳动力成本，通过自动将草图序列转换为彩色动画，从而简化角色设计、关键帧动画、中间帧生成和上色等流程。其核心技术包括利用对应匹配作为明确指导，增强模型对不同角色姿态变化的鲁棒性，并能自动化完成中间帧生成过程。用户只需提供角色图像及起始和结束草图，即可轻松创建出时间连贯的动画。相关代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://yihao-meng.github.io/AniDoc_demo">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AniDoc是一个基于视频扩散模型的二维动画生产工具，旨在降低劳动力成本。</li>
<li>它可以将草图序列自动转换为彩色动画，涵盖角色设计、关键帧动画等流程。</li>
<li>该工具利用对应匹配技术作为明确指导，增强对不同角色姿态变化的鲁棒性。</li>
<li>AniDoc能够自动化完成中间帧生成过程，简化动画创作流程。</li>
<li>用户只需提供角色图像及起始和结束草图，即可轻松创建时间连贯的动画。</li>
<li>该工具的核心技术亮点在于其创新性的对应匹配技术及其在动画生产中的应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14173">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fddf8453905c9d37cb3a71cc7f68e895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8aaa76958c834fd9840cf11cd36be3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d31db743f7a767ab45a653d120968c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fccbeb9b521b4250ae3e1825e3877013.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e982dabbb02cd0d3a1f9e118e5f9243f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="How-to-Backdoor-Consistency-Models"><a href="#How-to-Backdoor-Consistency-Models" class="headerlink" title="How to Backdoor Consistency Models?"></a>How to Backdoor Consistency Models?</h2><p><strong>Authors:Chengen Wang, Murat Kantarcioglu</strong></p>
<p>Consistency models are a new class of models that generate images by directly mapping noise to data, allowing for one-step generation and significantly accelerating the sampling process. However, their robustness against adversarial attacks has not yet been thoroughly investigated. In this work, we conduct the first study on the vulnerability of consistency models to backdoor attacks. While previous research has explored backdoor attacks on diffusion models, those studies have primarily focused on conventional diffusion models, employing a customized backdoor training process and objective, whereas consistency models have distinct training processes and objectives. Our proposed framework demonstrates the vulnerability of consistency models to backdoor attacks. During image generation, poisoned consistency models produce images with a Fr&#39;echet Inception Distance (FID) comparable to that of a clean model when sampling from Gaussian noise. However, once the trigger is activated, they generate backdoor target images. We explore various trigger and target configurations to evaluate the vulnerability of consistency models, including the use of random noise as a trigger. This novel trigger is visually inconspicuous, more challenging to detect, and aligns well with the sampling process of consistency models. Across all configurations, our framework successfully compromises the consistency models while maintaining high utility and specificity. We also examine the stealthiness of our proposed attack, which is attributed to the unique properties of consistency models and the elusive nature of the Gaussian noise trigger. </p>
<blockquote>
<p>一致性模型是一类新的模型，它们通过直接将噪声映射到数据来生成图像，实现了一步生成，并显著加速了采样过程。然而，它们对抗对抗性攻击的鲁棒性尚未得到充分研究。在这项工作中，我们对一致性模型对后门攻击的脆弱性进行了首次研究。虽然之前的研究已经探索了扩散模型上的后门攻击，但这些研究主要集中在传统的扩散模型上，采用定制的后门训练过程和目标，而一致性模型具有不同的训练过程和目标。我们提出的框架展示了一致性模型对后门攻击的脆弱性。在图像生成过程中，中毒的一致性模型在从高斯噪声采样时产生的图像的Fréchet Inception距离（FID）与干净模型的FID相当。然而，一旦触发，它们会生成后门目标图像。我们探索了各种触发器和目标配置来评估一致性模型的脆弱性，包括使用随机噪声作为触发器。这种新型触发器在视觉上并不显眼，更难以检测，并且与一致性模型的采样过程非常契合。在所有配置中，我们的框架成功地攻击了一致性模型，同时保持了高实用性和特异性。我们还检查了所提出的攻击的隐蔽性，这归因于一致性模型的独特属性和高斯噪声触发的不易察觉性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19785v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>一致性模型是一类新的图像生成模型，它们通过直接将噪声映射到数据上来生成图像，实现了一步生成，并显著加速了采样过程。然而，它们对抗对抗性攻击的稳健性尚未得到充分了解。本研究首次探讨了一致性模型对后门攻击的脆弱性。虽然之前的研究已经探索了扩散模型的后门攻击，但这些研究主要集中在传统的扩散模型上，采用定制的后门训练过程和目标，而一致性模型具有不同的训练过程和目标。本研究提出的框架展示了一致性模型对后门攻击的脆弱性。在图像生成过程中，中毒的一致性模型能够从高斯噪声中采样生成具有与干净模型相当的Fréchet Inception Distance (FID)的图像。然而，一旦触发条件被激活，它们就会生成后门目标图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>一致性模型是一种新的图像生成模型，通过噪声映射生成图像，加速采样过程。</li>
<li>目前对于一致性模型对抗攻击的稳健性研究尚不充分。</li>
<li>研究首次探讨了一致性模型对后门攻击的脆弱性。</li>
<li>之前的研究主要关注传统扩散模型的后门攻击，而一致性模型具有不同的训练过程和目标。</li>
<li>提出的框架展示了一致性模型在图像生成过程中易受后门攻击的影响。</li>
<li>中毒的一致性模型在采样过程中生成的图像与干净模型的FID相当，但触发条件激活后会生成后门目标图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19785">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6fcc04cf9ba43eaa9459710333972202.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49885af4493d624f0f14208a76273084.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a92b0e7e5e9d8e0529a2e75b1abeaa94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f3d1bc7a936b69e49e44160b24d72c5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Medical-Phrase-Grounding-with-Off-the-shelf-Diffusion-Models"><a href="#Zero-Shot-Medical-Phrase-Grounding-with-Off-the-shelf-Diffusion-Models" class="headerlink" title="Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models"></a>Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models</h2><p><strong>Authors:Konstantinos Vilouras, Pedro Sanchez, Alison Q. O’Neil, Sotirios A. Tsaftaris</strong></p>
<p>Localizing the exact pathological regions in a given medical scan is an important imaging problem that traditionally requires a large amount of bounding box ground truth annotations to be accurately solved. However, there exist alternative, potentially weaker, forms of supervision, such as accompanying free-text reports, which are readily available. The task of performing localization with textual guidance is commonly referred to as phrase grounding. In this work, we use a publicly available Foundation Model, namely the Latent Diffusion Model, to perform this challenging task. This choice is supported by the fact that the Latent Diffusion Model, despite being generative in nature, contains cross-attention mechanisms that implicitly align visual and textual features, thus leading to intermediate representations that are suitable for the task at hand. In addition, we aim to perform this task in a zero-shot manner, i.e., without any training on the target task, meaning that the model’s weights remain frozen. To this end, we devise strategies to select features and also refine them via post-processing without extra learnable parameters. We compare our proposed method with state-of-the-art approaches which explicitly enforce image-text alignment in a joint embedding space via contrastive learning. Results on a popular chest X-ray benchmark indicate that our method is competitive with SOTA on different types of pathology, and even outperforms them on average in terms of two metrics (mean IoU and AUC-ROC). Source code will be released upon acceptance at <a target="_blank" rel="noopener" href="https://github.com/vios-s">https://github.com/vios-s</a>. </p>
<blockquote>
<p>定位给定医学扫描中的精确病理区域是一个重要的成像问题，传统上需要大量的边界框真实标注来准确解决。然而，存在替代的、潜在的较弱形式的监督，例如伴随的自由文本报告，这些报告很容易获得。使用文本指导进行定位的任务通常被称为短语接地。在这项工作中，我们使用一个公开的Foundation模型，即潜在扩散模型，来执行这项具有挑战性的任务。我们选择这个模型的支持在于，尽管潜在扩散模型本质上是生成式的，但它包含交叉注意机制，能够隐式地对齐视觉和文本特征，从而产生适合当前任务的中间表示。此外，我们旨在以零样本的方式进行此任务，即无需对目标任务进行任何训练，这意味着模型的权重保持冻结。为此，我们制定了选择特征的策略，并通过后处理对它们进行细化，而无需额外的可学习参数。我们将所提出的方法与最新技术进行比较，后者通过对比学习在联合嵌入空间中显式地执行图像文本对齐。在流行的胸部X射线基准测试上的结果表明，我们的方法在不同类型的病理上与最新技术相比具有竞争力，甚至在两个指标（平均IoU和AUC-ROC）上的平均表现优于它们。源代码将在接受后发布在<a target="_blank" rel="noopener" href="https://github.com/vios-s%E3%80%82">https://github.com/vios-s。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.12920v4">PDF</a> 10 pages, 3 figures, IEEE J-BHI Special Issue on Foundation Models in   Medical Imaging</p>
<p><strong>摘要</strong><br>基于公开可用的基础模型——潜在扩散模型，实现无需标注数据的医学图像病变区域定位（称为短语定位任务）。模型无需针对目标任务进行训练，可借助模型内建的跨注意力机制进行视觉与文本特征的隐式对齐，以产生适用于该任务的中介表示。通过策略选择及后处理精炼特征，无需额外学习参数。与通过对比学习在联合嵌入空间中显式执行图像文本对齐的最先进方法相比，我们的方法在公共胸X射线基准测试上表现出竞争力，并在两种指标（平均IoU和AUC-ROC）上平均表现更优。源代码将在接受后发布于GitHub。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>利用公开的基础模型——潜在扩散模型进行医学图像病变区域定位。</li>
<li>模型采用零样本训练方式，无需针对目标任务进行训练。</li>
<li>模型利用跨注意力机制隐式对齐视觉和文本特征。</li>
<li>提出特征选择和后处理策略以精炼特征，无需额外学习参数。</li>
<li>与最先进的方法相比，在公共胸X射线基准测试上具有竞争力。</li>
<li>在两种评估指标上平均表现更优。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.12920">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-52a118dfaa5bdf689cf583ea4b38a6cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67b0e20024063b7da6a0486c12e12d59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96d5591974fd0a745f3e1b0631a054d7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-01/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-01/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3313172443e1fb5a4d8285eaf2c522ef.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-02-01  Diffusion Autoencoders are Scalable Image Tokenizers
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-01/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3bd5ff2abd6a8aef1dfc73fbb8d0838c.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-02-01  VoD-3DGS View-opacity-Dependent 3D Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
