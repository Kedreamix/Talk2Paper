<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-01  Diffusion Autoencoders are Scalable Image Tokenizers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-3313172443e1fb5a4d8285eaf2c522ef.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    63 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-01-æ›´æ–°"><a href="#2025-02-01-æ›´æ–°" class="headerlink" title="2025-02-01 æ›´æ–°"></a>2025-02-01 æ›´æ–°</h1><h2 id="Diffusion-Autoencoders-are-Scalable-Image-Tokenizers"><a href="#Diffusion-Autoencoders-are-Scalable-Image-Tokenizers" class="headerlink" title="Diffusion Autoencoders are Scalable Image Tokenizers"></a>Diffusion Autoencoders are Scalable Image Tokenizers</h2><p><strong>Authors:Yinbo Chen, Rohit Girdhar, Xiaolong Wang, Sai Saketh Rambhatla, Ishan Misra</strong></p>
<p>Tokenizing images into compact visual representations is a key step in learning efficient and high-quality image generative models. We present a simple diffusion tokenizer (DiTo) that learns compact visual representations for image generation models. Our key insight is that a single learning objective, diffusion L2 loss, can be used for training scalable image tokenizers. Since diffusion is already widely used for image generation, our insight greatly simplifies training such tokenizers. In contrast, current state-of-the-art tokenizers rely on an empirically found combination of heuristics and losses, thus requiring a complex training recipe that relies on non-trivially balancing different losses and pretrained supervised models. We show design decisions, along with theoretical grounding, that enable us to scale DiTo for learning competitive image representations. Our results show that DiTo is a simpler, scalable, and self-supervised alternative to the current state-of-the-art image tokenizer which is supervised. DiTo achieves competitive or better quality than state-of-the-art in image reconstruction and downstream image generation tasks. </p>
<blockquote>
<p>å°†å›¾åƒæ ‡è®°åŒ–ä¸ºç´§å‡‘çš„è§†è§‰è¡¨ç¤ºæ˜¯å­¦ä¹ é«˜æ•ˆä¸”é«˜è´¨é‡å›¾åƒç”Ÿæˆæ¨¡å‹çš„å…³é”®æ­¥éª¤ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ‰©æ•£æ ‡è®°å™¨ï¼ˆDiToï¼‰ï¼Œç”¨äºå­¦ä¹ å›¾åƒç”Ÿæˆæ¨¡å‹çš„ç´§å‡‘è§†è§‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå¯ä»¥ä½¿ç”¨å•ä¸ªå­¦ä¹ ç›®æ ‡ï¼Œå³æ‰©æ•£L2æŸå¤±ï¼Œæ¥è®­ç»ƒå¯æ‰©å±•çš„å›¾åƒæ ‡è®°å™¨ã€‚ç”±äºæ‰©æ•£å·²ç»å¹¿æ³›ç”¨äºå›¾åƒç”Ÿæˆï¼Œå› æ­¤æˆ‘ä»¬çš„è§è§£å¤§å¤§ç®€åŒ–äº†æ­¤ç±»æ ‡è®°å™¨çš„è®­ç»ƒã€‚ç›¸åï¼Œå½“å‰æœ€å…ˆè¿›çš„æ ‡è®°å™¨ä¾èµ–äºç»éªŒå‘ç°çš„å¯å‘å¼æ–¹æ³•å’ŒæŸå¤±çš„ç»„åˆï¼Œå› æ­¤éœ€è¦å¤æ‚çš„è®­ç»ƒé…æ–¹ï¼Œè¿™ä¾èµ–äºä¸åŒæŸå¤±çš„å¹³è¡¡å’Œé¢„è®­ç»ƒçš„ç›‘ç£æ¨¡å‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†è®¾è®¡å†³ç­–ä»¥åŠç†è®ºæ”¯æ’‘ï¼Œè¿™äº›ä½¿æˆ‘ä»¬èƒ½æ‰©å±•DiToæ¥å­¦ä¹ æœ‰ç«äº‰åŠ›çš„å›¾åƒè¡¨ç¤ºã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒDiToæ˜¯å½“å‰æœ‰ç›‘ç£çš„å…ˆè¿›å›¾åƒæ ‡è®°å™¨çš„æ›´ç®€å•ã€å¯æ‰©å±•å’Œè‡ªç›‘ç£çš„æ›¿ä»£å“ã€‚DiToåœ¨å›¾åƒé‡å»ºå’Œä¸‹æ¸¸å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›æˆ–æ›´å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18593v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://yinboc.github.io/dito/">https://yinboc.github.io/dito/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„æ‰©æ•£åˆ†è¯å™¨ï¼ˆDiToï¼‰ï¼Œç”¨äºå­¦ä¹ å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„ç´§å‡‘è§†è§‰è¡¨ç¤ºã€‚å…³é”®è§è§£æ˜¯ä½¿ç”¨å•ä¸€çš„æ‰©æ•£L2æŸå¤±ä½œä¸ºè®­ç»ƒå¯æ‰©å±•å›¾åƒåˆ†è¯å™¨çš„å­¦ä¹ ç›®æ ‡ï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚DiToé€šè¿‡ç®€å•çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œå®ç°äº†ä¸å½“å‰æœ€å…ˆè¿›çš„å›¾åƒåˆ†è¯å™¨ç›¸å½“çš„å›¾åƒé‡å»ºå’Œå›¾åƒç”Ÿæˆä»»åŠ¡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ç®€å•çš„æ‰©æ•£åˆ†è¯å™¨ï¼ˆDiToï¼‰ç”¨äºå›¾åƒç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨æ‰©æ•£L2æŸå¤±ä½œä¸ºè®­ç»ƒå›¾åƒåˆ†è¯å™¨çš„å•ä¸€å­¦ä¹ ç›®æ ‡ã€‚</li>
<li>æ‰©æ•£åˆ†è¯å™¨ï¼ˆDiToï¼‰ç®€åŒ–äº†å›¾åƒåˆ†è¯å™¨çš„è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„å›¾åƒåˆ†è¯å™¨éœ€è¦ç»“åˆå¤šç§å¯å‘å¼æ–¹æ³•å’ŒæŸå¤±ï¼Œè®­ç»ƒè¿‡ç¨‹å¤æ‚ã€‚</li>
<li>DiToè®¾è®¡å†³ç­–åŒ…æ‹¬ç†è®ºæ”¯æ’‘ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ ç«äº‰æ€§çš„å›¾åƒè¡¨ç¤ºã€‚</li>
<li>DiToåœ¨å›¾åƒé‡å»ºå’Œå›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†ä¸å½“å‰æœ€å…ˆè¿›çš„å›¾åƒåˆ†è¯å™¨ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18593">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4c9ebbe29b63ce30cf5376eb87d6e47a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9b50e309ea87cabbfa35e83f65025ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-024577c7ed5c4b06ddd51d9e02cc9fd5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2ead2d90d4faf41329e4b6ad1b7076e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25a42531fcf370cbf89d8f6a080328c8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Waveform-Specific-Performance-of-Deep-Learning-Based-Super-Resolution-for-Ultrasound-Contrast-Imaging"><a href="#Waveform-Specific-Performance-of-Deep-Learning-Based-Super-Resolution-for-Ultrasound-Contrast-Imaging" class="headerlink" title="Waveform-Specific Performance of Deep Learning-Based Super-Resolution   for Ultrasound Contrast Imaging"></a>Waveform-Specific Performance of Deep Learning-Based Super-Resolution   for Ultrasound Contrast Imaging</h2><p><strong>Authors:Rienk Zorgdrager, Nathan Blanken, Jelmer M. Wolterink, Michel Versluis, Guillaume Lajoinie</strong></p>
<p>Resolving arterial flows is essential for understanding cardiovascular pathologies, improving diagnosis, and monitoring patient condition. Ultrasound contrast imaging uses microbubbles to enhance the scattering of the blood pool, allowing for real-time visualization of blood flow. Recent developments in vector flow imaging further expand the imaging capabilities of ultrasound by temporally resolving fast arterial flow. The next obstacle to overcome is the lack of spatial resolution. Super-resolved ultrasound images can be obtained by deconvolving radiofrequency (RF) signals before beamforming, breaking the link between resolution and pulse duration. Convolutional neural networks (CNNs) can be trained to locally estimate the deconvolution kernel and consequently super-localize the microbubbles directly within the RF signal. However, microbubble contrast is highly nonlinear, and the potential of CNNs in microbubble localization has not yet been fully exploited. Assessing deep learning-based deconvolution performance for non-trivial imaging pulses is therefore essential for successful translation to a practical setting, where the signal-to-noise ratio is limited, and transmission schemes should comply with safety guidelines. In this study, we train CNNs to deconvolve RF signals and localize the microbubbles driven by harmonic pulses, chirps, or delay-encoded pulse trains. Furthermore, we discuss potential hurdles for in-vitro and in-vivo super-resolution by presenting preliminary experimental results. We find that, whereas the CNNs can accurately localize microbubbles for all pulses, a short imaging pulse offers the best performance in noise-free conditions. However, chirps offer a comparable performance without noise, but are more robust to noise and outperform all other pulses in low-signal-to-noise ratio conditions. </p>
<blockquote>
<p>è§£å†³åŠ¨è„‰è¡€æµé—®é¢˜å¯¹äºç†è§£å¿ƒè¡€ç®¡ç—…ç†ã€æé«˜è¯Šæ–­æ°´å¹³ä»¥åŠç›‘æµ‹ç—…äººçŠ¶å†µè‡³å…³é‡è¦ã€‚è¶…å£°å¯¹æ¯”æˆåƒåˆ©ç”¨å¾®æ°”æ³¡å¢å¼ºè¡€æ¶²æ± çš„æ•£å°„ï¼Œä»è€Œå¯ä»¥å®æ—¶å¯è§†åŒ–è¡€æµã€‚å‘é‡æµæˆåƒçš„è¿‘æœŸå‘å±•è¿›ä¸€æ­¥æ‰©å¤§äº†è¶…å£°çš„æˆåƒèƒ½åŠ›ï¼Œå¯ä»¥é€šè¿‡æ—¶é—´åˆ†è¾¨ç‡è§£æå¿«é€ŸåŠ¨è„‰è¡€æµã€‚ä¸‹ä¸€ä¸ªéœ€è¦å…‹æœçš„éšœç¢æ˜¯ç©ºé—´åˆ†è¾¨ç‡ä¸è¶³ã€‚é€šè¿‡åœ¨æ³¢æŸå½¢æˆä¹‹å‰å¯¹å°„é¢‘ï¼ˆRFï¼‰ä¿¡å·è¿›è¡Œåå·ç§¯ï¼Œå¯ä»¥è·å¾—è¶…çº§åˆ†è¾¨ç‡çš„è¶…å£°å›¾åƒï¼Œæ‰“ç ´äº†åˆ†è¾¨ç‡å’Œè„‰å†²æŒç»­æ—¶é—´ä¹‹é—´çš„è”ç³»ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯ä»¥ç»è¿‡è®­ç»ƒï¼Œå±€éƒ¨ä¼°è®¡åå·ç§¯æ ¸ï¼Œä»è€Œç›´æ¥åœ¨RFä¿¡å·ä¸­è¶…çº§å®šä½å¾®æ°”æ³¡ã€‚ç„¶è€Œï¼Œå¾®æ°”æ³¡å¯¹æ¯”é«˜åº¦éçº¿æ€§ï¼ŒCNNåœ¨å¾®æ°”æ³¡å®šä½æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†å¼€å‘ã€‚å› æ­¤ï¼Œè¯„ä¼°åŸºäºæ·±åº¦å­¦ä¹ çš„åå·ç§¯åœ¨éå¹³å‡¡æˆåƒè„‰å†²ä¸Šçš„è¡¨ç°ï¼Œå¯¹äºæˆåŠŸåº”ç”¨äºå®é™…ç¯å¢ƒè‡³å…³é‡è¦ï¼Œå› ä¸ºåœ¨å®è·µä¸­ä¿¡å™ªæ¯”æœ‰é™ï¼Œä¼ è¾“æ–¹æ¡ˆåº”éµå®ˆå®‰å…¨å‡†åˆ™ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è®­ç»ƒCNNå¯¹RFä¿¡å·è¿›è¡Œåå·ç§¯å¹¶å®šä½ç”±è°æ³¢è„‰å†²ã€å•å•¾æˆ–å»¶è¿Ÿç¼–ç è„‰å†²åˆ—é©±åŠ¨çš„å¾®æ°”æ³¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æä¾›åˆæ­¥å®éªŒç»“æœï¼Œè®¨è®ºäº†ä½“å¤–å’Œä½“å†…è¶…åˆ†è¾¨ç‡çš„æ½œåœ¨éšœç¢ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶CNNå¯ä»¥å‡†ç¡®åœ°å¯¹æ‰€æœ‰è„‰å†²è¿›è¡Œå¾®æ°”æ³¡å®šä½ï¼Œä½†åœ¨æ— å™ªå£°æ¡ä»¶ä¸‹ï¼ŒçŸ­æˆåƒè„‰å†²è¡¨ç°æœ€ä½³ã€‚ç„¶è€Œï¼Œå•å•¾åœ¨æ— å™ªå£°æ¡ä»¶ä¸‹è¡¨ç°ç›¸å½“ï¼Œä½†åœ¨å™ªå£°æ–¹é¢æ›´åŠ ç¨³å¥ï¼Œåœ¨ä½ä¿¡å™ªæ¯”æ¡ä»¶ä¸‹ä¼˜äºæ‰€æœ‰å…¶ä»–è„‰å†²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18375v1">PDF</a> Accepted for publication in IEEE Transactions on Ultrasonics,   Ferroelectrics, and Frequency Control</p>
<p><strong>Summary</strong><br>    è¶…å£°å¯¹æ¯”æˆåƒåˆ©ç”¨å¾®æ°”æ³¡å¢å¼ºè¡€æ¶²æ± æ•£å°„ï¼Œå®ç°è¡€æµå®æ—¶å¯è§†åŒ–ã€‚æ–°è¿‘å‘å±•çš„çŸ¢é‡æµæˆåƒæŠ€æœ¯æé«˜äº†è¶…å£°æˆåƒèƒ½åŠ›ï¼Œå¯å®æ—¶è§£æå¿«é€ŸåŠ¨è„‰è¡€æµã€‚å½“å‰é¢ä¸´çš„æŒ‘æˆ˜åœ¨äºç©ºé—´åˆ†è¾¨ç‡ä¸è¶³ã€‚é€šè¿‡å°„é¢‘ä¿¡å·è§£å·ç§¯é¢„å¤„ç†æ¥è·å¾—è¶…çº§åˆ†è¾¨ç‡è¶…å£°å›¾åƒï¼Œæ‰“ç ´åˆ†è¾¨ç‡ä¸è„‰å†²æŒç»­æ—¶é—´ä¹‹é—´çš„å…³è”ã€‚å·ç§¯ç¥ç»ç½‘ç»œå¯è®­ç»ƒå±€éƒ¨ä¼°è®¡è§£å·ç§¯æ ¸ï¼Œè¿›è€Œç›´æ¥åœ¨å°„é¢‘ä¿¡å·ä¸­è¶…çº§å®šä½å¾®æ°”æ³¡ã€‚æœ¬ç ”ç©¶è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œå¯¹å°„é¢‘ä¿¡å·è¿›è¡Œè§£å·ç§¯å’Œå¾®æ°”æ³¡å®šä½ï¼Œå¹¶è®¨è®ºä½“å¤–å’Œä½“å†…è¶…åˆ†è¾¨ç‡çš„æ½œåœ¨éšœç¢ï¼Œåˆæ­¥å®éªŒç»“æœæŒ‡å‡ºï¼Œå·ç§¯ç¥ç»ç½‘ç»œåœ¨å„ç§è„‰å†²ä¸‹èƒ½å‡†ç¡®å®šä½å¾®æ°”æ³¡ï¼ŒçŸ­æˆåƒè„‰å†²åœ¨æ— å£°å™ªæ¡ä»¶ä¸‹è¡¨ç°æœ€ä½³ï¼Œä½†çŸ­å“Œå“Œå£°åœ¨æ— å™ªå£°æ¡ä»¶ä¸‹è¡¨ç°ç›¸å½“ä¸”æ›´æŠ—å™ªå£°ï¼Œåœ¨ä½ä¿¡å™ªæ¯”æ¡ä»¶ä¸‹è¡¨ç°ä¼˜äºå…¶ä»–æ‰€æœ‰è„‰å†²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°å¯¹æ¯”æˆåƒä½¿ç”¨å¾®æ°”æ³¡å¢å¼ºè¡€æµå¯è§†åŒ–ã€‚</li>
<li>çŸ¢é‡æµæˆåƒæé«˜äº†è¶…å£°æˆåƒèƒ½åŠ›ï¼Œä½†é¢ä¸´ç©ºé—´åˆ†è¾¨ç‡ä¸è¶³çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å°„é¢‘ä¿¡å·è§£å·ç§¯é¢„å¤„ç†å¯è·å¾—è¶…çº§åˆ†è¾¨ç‡è¶…å£°å›¾åƒã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œå¯ç”¨äºå±€éƒ¨ä¼°è®¡è§£å·ç§¯æ ¸å¹¶ç›´æ¥å®šä½å¾®æ°”æ³¡åœ¨å°„é¢‘ä¿¡å·ä¸­çš„ä½ç½®ã€‚</li>
<li>è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œå¯¹å°„é¢‘ä¿¡å·è¿›è¡Œè§£å·ç§¯å’Œå¾®æ°”æ³¡å®šä½çš„å®éªŒç»“æœå±•ç¤ºäº†å¯¹ä¸åŒè„‰å†²çš„é€‚åº”æ€§ã€‚</li>
<li>åœ¨æ— å£°å™ªæ¡ä»¶ä¸‹ï¼ŒçŸ­æˆåƒè„‰å†²è¡¨ç°æœ€ä½³ï¼›è€Œåœ¨ä½ä¿¡å™ªæ¯”æ¡ä»¶ä¸‹ï¼ŒçŸ­å“Œå“Œå£°è¡¨ç°æ›´ç¨³å¥ä¸”ä¼˜äºå…¶ä»–è„‰å†²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7488733bd85d61a4fcececcaa0ecfef7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-076b669bb279c14ce7b80aa9188f5d67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2933a0dccca689efabe227c7e2797215.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b652635a9288a49c28c2c7a82c209bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69e21d2186f13f2d84ee023dee8f7ba.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CodeBrain-Impute-Any-Brain-MRI-via-Instance-specific-Scalar-quantized-Codes"><a href="#CodeBrain-Impute-Any-Brain-MRI-via-Instance-specific-Scalar-quantized-Codes" class="headerlink" title="CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized   Codes"></a>CodeBrain: Impute Any Brain MRI via Instance-specific Scalar-quantized   Codes</h2><p><strong>Authors:Yicheng Wu, Tao Song, Zhonghua Wu, Zongyuan Ge, Zhaolin Chen, Jianfei Cai</strong></p>
<p>MRI imputation aims to synthesize the missing modality from one or more available ones, which is highly desirable since it reduces scanning costs and delivers comprehensive MRI information to enhance clinical diagnosis. In this paper, we propose a unified model, CodeBrain, designed to adapt to various brain MRI imputation scenarios. The core design lies in casting various inter-modality transformations as a full-modality code prediction task. To this end, CodeBrain is trained in two stages: Reconstruction and Code Prediction. First, in the Reconstruction stage, we reconstruct each MRI modality, which is mapped into a shared latent space followed by a scalar quantization. Since such quantization is lossy and the code is low dimensional, another MRI modality belonging to the same subject is randomly selected to generate common features to supplement the code and boost the target reconstruction. In the second stage, we train another encoder by a customized grading loss to predict the full-modality codes from randomly masked MRI samples, supervised by the corresponding quantized codes generated from the first stage. In this way, the inter-modality transformation is achieved by mapping the instance-specific codes in a finite scalar space. We evaluated the proposed CodeBrain model on two public brain MRI datasets (i.e., IXI and BraTS 2023). Extensive experiments demonstrate that our CodeBrain model achieves superior imputation performance compared to four existing methods, establishing a new state of the art for unified brain MRI imputation. Codes will be released. </p>
<blockquote>
<p>MRIè¡¥å…¨æŠ€æœ¯çš„ç›®æ ‡æ˜¯ä»ä¸€ç§æˆ–å¤šç§ç°æœ‰æ¨¡æ€åˆæˆç¼ºå¤±çš„æ¨¡æ€ï¼Œè¿™å¯¹äºå‡å°‘æ‰«ææˆæœ¬å¹¶æä¾›å…¨é¢çš„MRIä¿¡æ¯ä»¥å¢å¼ºä¸´åºŠè¯Šæ–­è€Œè¨€æ˜¯éå¸¸ç†æƒ³çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¨¡å‹â€”â€”CodeBrainï¼Œå®ƒæ—¨åœ¨é€‚åº”å„ç§å¤§è„‘MRIè¡¥å…¨åœºæ™¯ã€‚æ ¸å¿ƒè®¾è®¡åœ¨äºå°†å„ç§è·¨æ¨¡æ€è½¬æ¢ä½œä¸ºå…¨æ¨¡æ€ä»£ç é¢„æµ‹ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼ŒCodeBrainåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼šé‡å»ºå’Œä»£ç é¢„æµ‹ã€‚é¦–å…ˆï¼Œåœ¨é‡å»ºé˜¶æ®µï¼Œæˆ‘ä»¬é‡å»ºæ¯ç§MRIæ¨¡æ€ï¼Œå°†å…¶æ˜ å°„åˆ°å…±äº«æ½œåœ¨ç©ºé—´ç„¶åè¿›è¡Œæ ‡é‡é‡åŒ–ã€‚ç”±äºè¿™ç§é‡åŒ–æ˜¯æœ‰æŸçš„ï¼Œå¹¶ä¸”ä»£ç æ˜¯ä½ç»´çš„ï¼Œå› æ­¤éšæœºé€‰æ‹©å±äºåŒä¸€ä¸»ä½“çš„å¦ä¸€ç§MRIæ¨¡æ€æ¥ç”Ÿæˆå…±åŒç‰¹å¾ï¼Œä»¥è¡¥å……ä»£ç å¹¶ä¿ƒè¿›ç›®æ ‡é‡å»ºã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨å®šåˆ¶çš„åˆ†çº§æŸå¤±æ¥è®­ç»ƒå¦ä¸€ä¸ªç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å¯ä»¥ä»éšæœºé®æŒ¡çš„MRIæ ·æœ¬ä¸­é¢„æµ‹å…¨æ¨¡æ€ä»£ç ï¼Œå¹¶ç”±ç¬¬ä¸€é˜¶æ®µç”Ÿæˆçš„ç›¸åº”é‡åŒ–ä»£ç è¿›è¡Œç›‘ç£ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œé€šè¿‡æ˜ å°„ç‰¹å®šå®ä¾‹çš„ä»£ç åœ¨æœ‰é™çš„æ ‡é‡ç©ºé—´ä¸­å®ç°è·¨æ¨¡æ€è½¬æ¢ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å…±å¤§è„‘MRIæ•°æ®é›†ï¼ˆå³IXIå’ŒBraTS 2023ï¼‰ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„CodeBrainæ¨¡å‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„CodeBrainæ¨¡å‹ä¸å››ç§ç°æœ‰æ–¹æ³•ç›¸æ¯”å®ç°äº†æ›´ä¼˜è¶Šçš„è¡¥å…¨æ€§èƒ½ï¼Œä¸ºç»Ÿä¸€çš„è„‘éƒ¨MRIè¡¥å…¨å»ºç«‹äº†æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚ç›¸å…³ä»£ç å°†ä¼šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18328v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æå‡ºçš„CodeBrainæ¨¡å‹ç”¨äºå¤§è„‘MRIç¼ºå¤±æ¨¡æ€å¡«è¡¥ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒï¼Œå®ç°å¤šç§MRIæ¨¡æ€çš„è½¬æ¢å’Œè¡¥å……ï¼Œè¾¾åˆ°ä¼˜å¼‚çš„å¡«è¡¥æ€§èƒ½ï¼Œå»ºç«‹æ–°çš„ç»Ÿä¸€å¤§è„‘MRIå¡«è¡¥æŠ€æœ¯æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CodeBrainæ¨¡å‹æ—¨åœ¨é€‚åº”å„ç§å¤§è„‘MRIç¼ºå¤±æ¨¡æ€å¡«è¡¥åœºæ™¯ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒï¼Œå³é‡å»ºå’Œä»£ç é¢„æµ‹ï¼Œå®ç°MRIæ¨¡æ€çš„è½¬æ¢ã€‚</li>
<li>åœ¨é‡å»ºé˜¶æ®µï¼Œæ¨¡å‹å°†æ¯ä¸ªMRIæ¨¡æ€æ˜ å°„åˆ°å…±äº«æ½œåœ¨ç©ºé—´ï¼Œç„¶åè¿›è¡Œæ ‡é‡é‡åŒ–ï¼Œä»¥è¡¥å……ä»£ç å¹¶ä¿ƒè¿›ç›®æ ‡é‡å»ºã€‚</li>
<li>åœ¨ä»£ç é¢„æµ‹é˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡å®šåˆ¶çš„åˆ†çº§æŸå¤±è®­ç»ƒå¦ä¸€ä¸ªç¼–ç å™¨ï¼Œä»éšæœºé®æŒ¡çš„MRIæ ·æœ¬ä¸­é¢„æµ‹å…¨æ¨¡æ€ä»£ç ã€‚</li>
<li>CodeBrainæ¨¡å‹é€šè¿‡æ˜ å°„ç‰¹å®šå®ä¾‹ä»£ç åœ¨æœ‰é™æ ‡é‡ç©ºé—´å†…å®ç°æ¨¡æ€é—´è½¬æ¢ã€‚</li>
<li>åœ¨IXIå’ŒBraTS 2023ä¸¤ä¸ªå…¬å¼€å¤§è„‘MRIæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCodeBrainæ¨¡å‹å–å¾—äº†ä¼˜äºå…¶ä»–å››ç§ç°æœ‰æ–¹æ³•çš„å¡«è¡¥æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4c59d2b78a1e9b16df40839db6e6ce56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d43f09fdefe80b7af28289e12ec8f05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3313172443e1fb5a4d8285eaf2c522ef.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Arbitrary-Data-as-Images-Fusion-of-Patient-Data-Across-Modalities-and-Irregular-Intervals-with-Vision-Transformers"><a href="#Arbitrary-Data-as-Images-Fusion-of-Patient-Data-Across-Modalities-and-Irregular-Intervals-with-Vision-Transformers" class="headerlink" title="Arbitrary Data as Images: Fusion of Patient Data Across Modalities and   Irregular Intervals with Vision Transformers"></a>Arbitrary Data as Images: Fusion of Patient Data Across Modalities and   Irregular Intervals with Vision Transformers</h2><p><strong>Authors:Malte TÃ¶lle, Mohamad Scharaf, Samantha Fischer, Christoph Reich, Silav Zeid, Christoph Dieterich, Benjamin Meder, Norbert Frey, Philipp Wild, Sandy Engelhardt</strong></p>
<p>A patient undergoes multiple examinations in each hospital stay, where each provides different facets of the health status. These assessments include temporal data with varying sampling rates, discrete single-point measurements, therapeutic interventions such as medication administration, and images. While physicians are able to process and integrate diverse modalities intuitively, neural networks need specific modeling for each modality complicating the training procedure. We demonstrate that this complexity can be significantly reduced by visualizing all information as images along with unstructured text and subsequently training a conventional vision-text transformer. Our approach, Vision Transformer for irregular sampled Multi-modal Measurements (ViTiMM), not only simplifies data preprocessing and modeling but also outperforms current state-of-the-art methods in predicting in-hospital mortality and phenotyping, as evaluated on 6,175 patients from the MIMIC-IV dataset. The modalities include patientâ€™s clinical measurements, medications, X-ray images, and electrocardiography scans. We hope our work inspires advancements in multi-modal medical AI by reducing the training complexity to (visual) prompt engineering, thus lowering entry barriers and enabling no-code solutions for training. The source code will be made publicly available. </p>
<blockquote>
<p>ç—…æ‚£åœ¨æ¯æ¬¡ä½é™¢æœŸé—´éƒ½ä¼šæ¥å—å¤šæ¬¡æ£€æŸ¥ï¼Œæ¯æ¬¡æ£€æŸ¥æä¾›å¥åº·çŠ¶æ€çš„ä¸åŒæ–¹é¢ã€‚è¿™äº›è¯„ä¼°åŒ…æ‹¬å…·æœ‰ä¸åŒé‡‡æ ·ç‡çš„æ—¶é—´æ•°æ®ã€ç¦»æ•£çš„å•ç‚¹æµ‹é‡ã€è¯ç‰©æ²»ç–—ç­‰æ²»ç–—å¹²é¢„ä»¥åŠå›¾åƒã€‚è™½ç„¶åŒ»ç”Ÿèƒ½å¤Ÿç›´è§‚åœ°å¤„ç†å¹¶æ•´åˆå¤šç§æ¨¡å¼ï¼Œä½†ç¥ç»ç½‘ç»œéœ€è¦ä¸ºæ¯ç§æ¨¡å¼è¿›è¡Œç‰¹å®šå»ºæ¨¡ï¼Œä»è€Œå¤æ‚åŒ–è®­ç»ƒç¨‹åºã€‚æˆ‘ä»¬å±•ç¤ºï¼Œé€šè¿‡å°†æ‰€æœ‰ä¿¡æ¯å¯è§†åŒ–ä¸ºå›¾åƒä»¥åŠéç»“æ„åŒ–æ–‡æœ¬ï¼Œå¯ä»¥å¤§å¹…åº¦å‡å°‘è¿™ç§å¤æ‚æ€§ï¼Œéšåè®­ç»ƒä¼ ç»Ÿçš„è§†è§‰æ–‡æœ¬è½¬æ¢å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•â€”â€”ç”¨äºä¸è§„åˆ™é‡‡æ ·å¤šæ¨¡å¼æµ‹é‡çš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTiMMï¼‰ï¼Œä¸ä»…ç®€åŒ–äº†æ•°æ®é¢„å¤„ç†å’Œå»ºæ¨¡ï¼Œè€Œä¸”åœ¨é¢„æµ‹ä½é™¢æ­»äº¡ç‡å’Œåˆ†å‹æ–¹é¢ï¼Œè¡¨ç°å‡ºä¼˜äºå½“å‰æœ€å…ˆè¿›æ–¹æ³•çš„æ•ˆæœã€‚è¯„ä¼°æ•°æ®æ¥è‡ªMIMIC-IVæ•°æ®é›†çš„6,175åæ‚£è€…ã€‚è¿™äº›æ¨¡å¼åŒ…æ‹¬æ‚£è€…çš„ä¸´åºŠæµ‹é‡ã€è¯ç‰©æ²»ç–—ã€Xå…‰å›¾åƒå’Œå¿ƒç”µå›¾æ‰«æã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½å¤Ÿé€šè¿‡ç®€åŒ–è®­ç»ƒå¤æ‚æ€§ï¼Œå°†ï¼ˆè§†è§‰ï¼‰æç¤ºå·¥ç¨‹åº”ç”¨äºå¤šæ¨¡å¼åŒ»ç–—äººå·¥æ™ºèƒ½ï¼Œä»è€Œé™ä½å…¥é—¨é—¨æ§›å¹¶å®ç°æ— ä»£ç è§£å†³æ–¹æ¡ˆçš„è®­ç»ƒï¼Œä»¥æ­¤æ¿€å‘å¤šæ¨¡å¼åŒ»ç–—äººå·¥æ™ºèƒ½çš„è¿›æ­¥ã€‚æºä»£ç å°†å…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18237v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§å°†å¤šæ¨¡æ€åŒ»ç–—æ•°æ®ï¼ˆåŒ…æ‹¬ä¸´åºŠæµ‹é‡ã€è¯ç‰©ã€Xå…‰å›¾åƒå’Œå¿ƒç”µå›¾æ‰«æç­‰ï¼‰å¯è§†åŒ–æˆå›¾åƒï¼Œå¹¶ç»“åˆéç»“æ„æ–‡æœ¬ï¼Œé€šè¿‡è®­ç»ƒå¸¸è§„è§†è§‰æ–‡æœ¬è½¬æ¢å™¨ï¼ˆVision Transformerï¼‰çš„æ–¹æ³•æ¥å¤„ç†ä¸è§„åˆ™é‡‡æ ·çš„å¤šæ¨¡æ€æµ‹é‡æ•°æ®ã€‚æ–°æ–¹æ³•ç®€åŒ–äº†æ•°æ®é¢„å¤„ç†å’Œå»ºæ¨¡çš„å¤æ‚æ€§ï¼ŒåŒæ—¶é™ä½äº†å¯¹å¤šæ¨¡æ€åŒ»ç–—AIçš„å…¥é—¨é—¨æ§›å¹¶æœ‰æœ›æ¿€å‘æ›´å¤šç›¸å…³ç ”ç©¶çš„å‘å±•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹ä½é™¢æ­»äº¡ç‡å’Œç–¾ç—…åˆ†å‹ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æºä»£ç å°†å…¬å¼€ä¾›å…¬ä¼—ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†å°†å¤šæ¨¡æ€åŒ»ç–—æ•°æ®å¯è§†åŒ–æˆå›¾åƒçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸´åºŠæµ‹é‡ã€è¯ç‰©ã€Xå…‰å›¾åƒå’Œå¿ƒç”µå›¾æ‰«æç­‰ã€‚</li>
<li>é€šè¿‡è®­ç»ƒå¸¸è§„è§†è§‰æ–‡æœ¬è½¬æ¢å™¨ï¼ˆVision Transformerï¼‰å¤„ç†ä¸è§„åˆ™é‡‡æ ·çš„å¤šæ¨¡æ€æµ‹é‡æ•°æ®ã€‚</li>
<li>ç®€åŒ–äº†æ•°æ®é¢„å¤„ç†å’Œå»ºæ¨¡çš„å¤æ‚æ€§ï¼Œé™ä½äº†å¤šæ¨¡æ€åŒ»ç–—AIçš„å…¥é—¨é—¨æ§›ã€‚</li>
<li>æå‡ºäº†å…¨æ–°çš„ViTiMMæ¨¡å‹æ¡†æ¶åœ¨é¢„æµ‹ä½é™¢æ­»äº¡ç‡å’Œç–¾ç—…åˆ†å‹ä¸Šå…·æœ‰å‡ºè‰²è¡¨ç°ã€‚</li>
<li>å®éªŒç»“æœä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æºç å°†å…¬å¼€ä¾›å…¬ä¼—ä½¿ç”¨ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›ä¾¿åˆ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c235d08d35f41b419a74d976f5b650f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e5487d3753c74b2e994a2033c794bd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8ea577701fb0c3471b12cdf0c600a72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b40c93d897be352300d6e2d542b9dd7e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Anatomy-Might-Be-All-You-Need-Forecasting-What-to-Do-During-Surgery"><a href="#Anatomy-Might-Be-All-You-Need-Forecasting-What-to-Do-During-Surgery" class="headerlink" title="Anatomy Might Be All You Need: Forecasting What to Do During Surgery"></a>Anatomy Might Be All You Need: Forecasting What to Do During Surgery</h2><p><strong>Authors:Gary Sarwin, Alessandro Carretta, Victor Staartjes, Matteo Zoli, Diego Mazzatenta, Luca Regli, Carlo Serra, Ender Konukoglu</strong></p>
<p>Surgical guidance can be delivered in various ways. In neurosurgery, spatial guidance and orientation are predominantly achieved through neuronavigation systems that reference pre-operative MRI scans. Recently, there has been growing interest in providing live guidance by analyzing video feeds from tools such as endoscopes. Existing approaches, including anatomy detection, orientation feedback, phase recognition, and visual question-answering, primarily focus on aiding surgeons in assessing the current surgical scene. This work aims to provide guidance on a finer scale, aiming to provide guidance by forecasting the trajectory of the surgical instrument, essentially addressing the question of what to do next. To address this task, we propose a model that not only leverages the historical locations of surgical instruments but also integrates anatomical features. Importantly, our work does not rely on explicit ground truth labels for instrument trajectories. Instead, the ground truth is generated by a detection model trained to detect both anatomical structures and instruments within surgical videos of a comprehensive dataset containing pituitary surgery videos. By analyzing the interaction between anatomy and instrument movements in these videos and forecasting future instrument movements, we show that anatomical features are a valuable asset in addressing this challenging task. To the best of our knowledge, this work is the first attempt to address this task for manually operated surgeries. </p>
<blockquote>
<p>æ‰‹æœ¯æŒ‡å¯¼å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼æä¾›ã€‚åœ¨ç¥ç»å¤–ç§‘ä¸­ï¼Œä¸»è¦é€šè¿‡ç¥ç»å¯¼èˆªç³»ç»Ÿå®ç°ç©ºé—´æŒ‡å¯¼å’Œå®šä½ï¼Œè¯¥ç³»ç»Ÿå‚è€ƒæœ¯å‰MRIæ‰«æã€‚æœ€è¿‘ï¼Œé€šè¿‡åˆ†ææ¥è‡ªå·¥å…·å¦‚å†…çª¥é•œçš„è§†é¢‘æµæä¾›å®æ—¶æŒ‡å¯¼çš„å…´è¶£æ—¥ç›Šæµ“åšã€‚ç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬è§£å‰–æ£€æµ‹ã€æ–¹å‘åé¦ˆã€ç›¸ä½è¯†åˆ«å’Œè§†è§‰é—®ç­”ï¼Œä¸»è¦ä¾§é‡äºå¸®åŠ©å¤–ç§‘åŒ»ç”Ÿè¯„ä¼°å½“å‰æ‰‹æœ¯åœºæ™¯ã€‚æœ¬å·¥ä½œçš„ç›®æ ‡æ˜¯åœ¨æ›´ç²¾ç»†çš„å±‚é¢ä¸Šæä¾›æŒ‡å¯¼ï¼Œæ—¨åœ¨é€šè¿‡é¢„æµ‹æ‰‹æœ¯å™¨æ¢°çš„è½¨è¿¹æ¥æä¾›æŒ‡å¯¼ï¼Œæœ¬è´¨ä¸Šè§£å†³ä¸‹ä¸€æ­¥è¯¥åšä»€ä¹ˆçš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ä»…åˆ©ç”¨æ‰‹æœ¯å™¨æ¢°çš„å†å²ä½ç½®ï¼Œè¿˜æ•´åˆäº†è§£å‰–ç‰¹å¾ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸éœ€è¦ä¾èµ–ä»ªå™¨è½¨è¿¹çš„æ˜ç¡®çœŸå®æ ‡ç­¾ã€‚ç›¸åï¼ŒçœŸå®æ ‡ç­¾æ˜¯ç”±ä¸€ä¸ªæ£€æµ‹æ¨¡å‹ç”Ÿæˆçš„ï¼Œè¯¥æ¨¡å‹ç»è¿‡è®­ç»ƒï¼Œå¯æ£€æµ‹ç»¼åˆæ•°æ®é›†ä¸­æ‰‹æœ¯è§†é¢‘å†…çš„è§£å‰–ç»“æ„å’Œä»ªå™¨ï¼Œè¯¥æ•°æ®é›†åŒ…å«å‚ä½“æ‰‹æœ¯è§†é¢‘ã€‚é€šè¿‡åˆ†æè¿™äº›è§†é¢‘ä¸­è§£å‰–ç»“æ„ä¸å™¨æ¢°è¿åŠ¨çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶é¢„æµ‹æœªæ¥å™¨æ¢°çš„è¿åŠ¨ï¼Œæˆ‘ä»¬è¯æ˜äº†è§£å‰–ç‰¹å¾æ˜¯è§£å†³è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„å®è´µèµ„äº§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•è§£å†³æ‰‹åŠ¨æ‰‹æœ¯çš„è¿™ä¸€ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18011v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†å¤–ç§‘æ‰‹æœ¯æŒ‡å¯¼çš„å¤šç§æ–¹å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¥ç»å¤–ç§‘ä¸­é€šè¿‡ç¥ç»å…ƒå¯¼èˆªç³»ç»Ÿå®ç°ç©ºé—´æŒ‡å¯¼å’Œå®šä½ã€‚æœ€è¿‘ï¼Œé€šè¿‡åˆ†æå¦‚å†…çª¥é•œç­‰å·¥å…·çš„è§†é¢‘æµæä¾›å®æ—¶æŒ‡å¯¼çš„æ–¹æ³•å—åˆ°å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è¯„ä¼°å½“å‰æ‰‹æœ¯åœºæ™¯ï¼Œè€Œæœ¬æ–‡æ—¨åœ¨æä¾›æ›´ç²¾ç»†çš„æŒ‡å¯¼ï¼Œé€šè¿‡é¢„æµ‹æ‰‹æœ¯å™¨æ¢°çš„è½¨è¿¹æ¥è§£ç­”ä¸‹ä¸€æ­¥è¯¥å¦‚ä½•æ“ä½œçš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ä»…åˆ©ç”¨æ‰‹æœ¯å™¨æ¢°çš„å†å²ä½ç½®ï¼Œè¿˜æ•´åˆäº†è§£å‰–ç‰¹å¾ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸éœ€è¦ä»ªå™¨è½¨è¿¹çš„æ˜ç¡®çœŸå®æ ‡ç­¾ï¼Œè€Œæ˜¯é€šè¿‡æ£€æµ‹æ¨¡å‹ç”ŸæˆçœŸå®æ ‡ç­¾ï¼Œè¯¥æ¨¡å‹ç»è¿‡è®­ç»ƒå¯æ£€æµ‹ç»¼åˆæ•°æ®é›†ä¸­çš„æ‰‹æœ¯è§†é¢‘å†…çš„è§£å‰–ç»“æ„å’Œå™¨æ¢°ã€‚é€šè¿‡åˆ†æè¿™äº›è§†é¢‘ä¸­è§£å‰–ç»“æ„å’Œå™¨æ¢°è¿åŠ¨çš„ç›¸äº’ä½œç”¨å¹¶é¢„æµ‹æœªæ¥çš„å™¨æ¢°è¿åŠ¨ï¼Œæˆ‘ä»¬è¯æ˜äº†è§£å‰–ç‰¹å¾æ˜¯è§£å†³è¿™ä¸€å…·æœ‰æŒ‘æˆ˜çš„ä»»åŠ¡çš„é‡è¦èµ„æºã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•é’ˆå¯¹æ‰‹åŠ¨æ‰‹æœ¯è§£å†³æ­¤é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤–ç§‘æ‰‹æœ¯æŒ‡å¯¼æœ‰å¤šç§æ–¹å¼ï¼ŒåŒ…æ‹¬ç¥ç»å…ƒå¯¼èˆªç³»ç»Ÿæä¾›çš„ç©ºé—´æŒ‡å¯¼å’Œå®šä½ã€‚</li>
<li>æœ€è¿‘å¯¹é€šè¿‡è§†é¢‘æµæä¾›å®æ—¶æ‰‹æœ¯æŒ‡å¯¼çš„å…´è¶£æ­£åœ¨å¢é•¿ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è¯„ä¼°å½“å‰æ‰‹æœ¯åœºæ™¯ï¼Œè€Œæœ¬æ–‡æ—¨åœ¨é¢„æµ‹æ‰‹æœ¯å™¨æ¢°çš„è½¨è¿¹ä»¥æä¾›ä¸‹ä¸€æ­¥æ“ä½œæŒ‡å¯¼ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„æ¨¡å‹ç»“åˆäº†æ‰‹æœ¯å™¨æ¢°çš„å†å²ä½ç½®å’Œè§£å‰–ç‰¹å¾ã€‚</li>
<li>è¯¥å·¥ä½œä¸éœ€è¦æ˜ç¡®çš„ä»ªå™¨è½¨è¿¹çœŸå®æ ‡ç­¾ï¼Œè€Œæ˜¯é€šè¿‡æ£€æµ‹æ¨¡å‹ç”ŸæˆçœŸå®æ ‡ç­¾ã€‚</li>
<li>é€šè¿‡åˆ†æè§£å‰–ç»“æ„å’Œå™¨æ¢°è¿åŠ¨çš„ç›¸äº’ä½œç”¨ï¼Œè¯æ˜äº†è§£å‰–ç‰¹å¾å¯¹è§£å†³è¿™ä¸€ä»»åŠ¡çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f19ea5df6437696a9ba350cd745f6000.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3565632fd233d45774d95fb4a73b08bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-637d3e41d928f18535e8a0c73471de81.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Ultraviolet-spectroscopy-of-the-black-hole-X-ray-binary-MAXI-J1820-070-across-a-state-transition"><a href="#Ultraviolet-spectroscopy-of-the-black-hole-X-ray-binary-MAXI-J1820-070-across-a-state-transition" class="headerlink" title="Ultraviolet spectroscopy of the black hole X-ray binary MAXI J1820+070   across a state transition"></a>Ultraviolet spectroscopy of the black hole X-ray binary MAXI J1820+070   across a state transition</h2><p><strong>Authors:Maria Georganti, Christian Knigge, Noel Castro Segura, Knox S. Long, Gulab C. Dewangan, Srimanta Banerjee, Robert I. Hynes, Poshak Gandhi, Diego Altamirano, Joseph Patterson, David R. Zurek</strong></p>
<p>We present ultraviolet (UV) spectroscopic observations covering three distinct accretion states of the low-mass X-ray binary (LMXB) MAXI J1820+070: the luminous hard state, a hard-intermediate state and the soft state. Our observations were obtained during the 2018 eruption of MAXI J1820+070 with the Hubble Space Telescope (HST) and AstroSat observatory. The extinction towards the source turns out to be low - $\rm E_{B-V} &#x3D; 0.2 \pm 0.05$ - making it one of the best UV accretion laboratories among LMXBs. Remarkably, we observe only moderate differences between all three states, with all spectra displaying similar continuum shapes and emission lines. Moreover, the continua are not well-described by physically plausible irradiated disc models. All of this challenges the standard reprocessing picture for UV emission from erupting LMXBs. The UV emission lines are double-peaked, with high-ionization lines displaying higher peak-to-peak velocities. None of the lines display obvious outflow signatures, even though blue-shifted absorption features have been seen in optical and near-infrared lines during the hard state. The emission line ratios are consistent with normal abundances, suggesting that the donor mass at birth was low enough to avoid CNO processing ($\rm M_{2,i} \lesssim 1.0 - 1.5 {\mathrm M_{\odot}}$). Finally, we study the evolution of UV variability in our time-resolved HST observations (hard and hard-intermediate states). All UV power spectra can be modelled with a broken power-law, superposed on which we tentatively detect the $\simeq 18$s quasi-periodic oscillation (QPO) that has been seen in other spectral bands. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹ä½è´¨é‡Xå°„çº¿åŒæ˜Ÿï¼ˆLMXBï¼‰MAXI J1820+070çš„ä¸‰ä¸ªä¸åŒç§¯èšçŠ¶æ€è¿›è¡Œäº†ç´«å¤–çº¿ï¼ˆUVï¼‰å…‰è°±è§‚æµ‹ï¼šå‘å…‰ç¡¬æ€ã€ç¡¬ä¸­é—´æ€å’Œè½¯æ€ã€‚æˆ‘ä»¬çš„è§‚æµ‹æ•°æ®æ˜¯åœ¨MAXI J1820+070äº2018å¹´çˆ†å‘æœŸé—´ä½¿ç”¨å“ˆå‹ƒå¤ªç©ºæœ›è¿œé•œï¼ˆHSTï¼‰å’ŒAstroSatå¤©æ–‡å°è·å¾—çš„ã€‚è¯¥æºçš„æ¶ˆå…‰ç»“æœè¾ƒä½â€”â€”$\rm E_{B-V} &#x3D; 0.2 \pm 0.05$â€”â€”ä½¿å…¶æˆä¸ºLMXBä¸­æœ€å¥½çš„UVç§¯èšå®éªŒå®¤ä¹‹ä¸€ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸‰ç§çŠ¶æ€ä¹‹é—´çš„å·®å¼‚å¹¶ä¸å¤§ï¼Œæ‰€æœ‰å…‰è°±éƒ½æ˜¾ç¤ºå‡ºç›¸ä¼¼çš„è¿ç»­è°±å½¢çŠ¶å’Œå‘å°„çº¿ã€‚æ­¤å¤–ï¼Œè¿ç»­è°±å¹¶ä¸é€‚åˆç”¨ç‰©ç†ä¸Šåˆç†çš„è¾ç…§ç›˜æ¨¡å‹æ¥æè¿°ã€‚æ‰€æœ‰è¿™äº›éƒ½å¯¹æ¥è‡ªçˆ†å‘çš„LMXBçš„UVå‘å°„çš„æ ‡å‡†å†å¤„ç†å›¾åƒæå‡ºäº†æŒ‘æˆ˜ã€‚UVå‘å°„çº¿æ˜¯åŒå³°çš„ï¼Œé«˜ç”µç¦»çº¿çš„å³°å³°å€¼é€Ÿåº¦è¾ƒé«˜ã€‚å°½ç®¡åœ¨ç¡¬æ€æœŸé—´è§‚å¯Ÿåˆ°å…‰å­¦å’Œè¿‘çº¢å¤–çº¿å…·æœ‰è“ç§»å¸æ”¶ç‰¹å¾ï¼Œä½†æ²¡æœ‰ä¸€æ¡çº¿æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æµå‡ºç‰¹å¾ã€‚å‘å°„çº¿æ¯”ç‡ä¸æ­£å¸¸ä¸°åº¦ä¸€è‡´ï¼Œè¿™è¡¨æ˜æèµ è€…çš„åˆå§‹è´¨é‡è¶³å¤Ÿä½ï¼Œé¿å…äº†CNOåŠ å·¥ï¼ˆ$\rm M_{2,i} \leq 1.0 - 1.5 {\mathrm M_{\odot}}$ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ—¶é—´åˆ†è¾¨çš„HSTè§‚æµ‹ä¸­ï¼ˆç¡¬æ€å’Œç¡¬ä¸­é—´æ€ï¼‰UVå¯å˜æ€§çš„æ¼”åŒ–ã€‚æ‰€æœ‰UVåŠŸç‡è°±éƒ½å¯ä»¥ç”¨åˆ†æ®µå¹‚å¾‹æ¨¡å‹æ¥æè¿°ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬åˆæ­¥æ£€æµ‹åˆ°çº¦ä¸º18ç§’çš„å‡†å‘¨æœŸæ€§æŒ¯è¡ï¼ˆQPOï¼‰ï¼Œåœ¨å…¶ä»–å…‰è°±æ³¢æ®µä¹Ÿè§‚å¯Ÿåˆ°è¯¥æŒ¯è¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17935v1">PDF</a> 19 pages, 14 figures, submitted to MNRAS</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æŠ¥å‘Šäº†å¯¹ä½è´¨é‡Xå°„çº¿åŒæ˜ŸMAXI J1820+070ä¸‰ç§ä¸åŒç§¯èšçŠ¶æ€çš„ç´«å¤–çº¿å…‰è°±è§‚æµ‹ç»“æœã€‚è§‚æµ‹å¯¹è±¡æ˜¯åœ¨å…¶2018å¹´çˆ†å‘æœŸé—´çš„MAXI J1820+070ï¼Œä½¿ç”¨å“ˆå‹ƒå¤ªç©ºæœ›è¿œé•œå’ŒAstroSatå¤©æ–‡å°è¿›è¡Œè§‚æµ‹ã€‚æºå¤´çš„æ¶ˆå…‰è¾ƒä½ï¼Œä½¿å…¶æˆä¸ºLMXBä¸­æœ€å¥½çš„ç´«å¤–çº¿ç§¯èšå®éªŒå®¤ä¹‹ä¸€ã€‚å°½ç®¡ä¸‰ä¸ªçŠ¶æ€çš„å·®å¼‚ä¸å¤§ï¼Œä½†å…‰è°±æ˜¾ç¤ºå‡ºç›¸ä¼¼çš„è¿ç»­ä½“å’Œå‘å°„çº¿ã€‚è¿ç»­ä½“ä¸èƒ½ç”¨ç‰©ç†ä¸Šåˆç†çš„è¾ç…§ç›˜æ¨¡å‹å¾ˆå¥½åœ°æè¿°ã€‚è¿™ä¸€åˆ‡å¯¹æ ‡å‡†ç´«å¤–çº¿å‘å°„çš„é‡å¤„ç†å›¾åƒæå‡ºäº†æŒ‘æˆ˜ã€‚ç´«å¤–çº¿å‘å°„çº¿æ˜¯åŒå³°çš„ï¼Œé«˜ç”µç¦»çº¿çš„å³°é—´é€Ÿåº¦è¾ƒé«˜ã€‚å°½ç®¡ç¡¬æ€æœŸé—´è§‚å¯Ÿåˆ°å…‰å­¦å’Œè¿‘çº¢å¤–çº¿æœ‰è“ç§»å¸æ”¶ç‰¹å¾ï¼Œä½†æ²¡æœ‰ä¸€æ¡çº¿æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æµå‡ºè¿¹è±¡ã€‚å‘å°„çº¿æ¯”ç‡ä¸æ­£å¸¸ä¸°åº¦ä¸€è‡´ï¼Œæš—ç¤ºæèµ è€…çš„åˆå§‹è´¨é‡è¶³å¤Ÿä½ï¼Œä»¥é¿å…CNOå¤„ç†ï¼ˆM_{2ï¼Œi} \lesssim 1.0 - 1.5 {\mathrm M_{\odot}}ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç´«å¤–çº¿å¯å˜æ€§çš„æ¼”åŒ–ã€‚æ‰€æœ‰ç´«å¤–çº¿åŠŸç‡è°±éƒ½å¯ä»¥ç”¨æˆªæ–­åŠŸç‡å¾‹æ¨¡å‹è¿›è¡Œå»ºæ¨¡ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šæˆ‘ä»¬æ£€æµ‹åˆ°çº¦ä¸º18ç§’çš„å‡†å‘¨æœŸæ€§æŒ¯è¡ï¼ˆQPOï¼‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æŠ¥å‘Šäº†ä½è´¨é‡Xå°„çº¿åŒæ˜ŸMAXI J1820+070åœ¨ä¸‰ç§ä¸åŒç§¯èšçŠ¶æ€çš„ç´«å¤–çº¿å…‰è°±è§‚æµ‹ç»“æœã€‚</li>
<li>æºå¤´çš„æ¶ˆå…‰è¾ƒä½ï¼Œä½¿å…¶æˆä¸ºLMXBä¸­æœ€ä½³çš„ç´«å¤–çº¿ç§¯èšå®éªŒå®¤ä¹‹ä¸€ã€‚</li>
<li>ä¸‰ä¸ªçŠ¶æ€çš„å…‰è°±æ˜¾ç¤ºå‡ºç›¸ä¼¼çš„è¿ç»­ä½“å’Œå‘å°„çº¿ï¼Œæ˜¾ç¤ºå‡ºé€‚ä¸­çš„å·®å¼‚ã€‚</li>
<li>è¿ç»­ä½“ä¸èƒ½ç”¨ç‰©ç†ç›˜æ¨¡å‹å¾ˆå¥½åœ°æè¿°ï¼Œå¯¹æ ‡å‡†ç´«å¤–çº¿å‘å°„çš„é‡å¤„ç†å›¾åƒæå‡ºäº†æŒ‘æˆ˜ã€‚</li>
<li>ç´«å¤–çº¿å‘å°„çº¿é€šå¸¸å‘ˆç°åŒå³°å½¢æ€ï¼Œé«˜ç”µç¦»çº¿çš„å³°é—´é€Ÿåº¦è¾ƒé«˜ã€‚</li>
<li>æ²¡æœ‰è§‚å¯Ÿåˆ°æ˜æ˜¾çš„æµå‡ºè¿¹è±¡ï¼Œå°½ç®¡åœ¨ç¡¬æ€æœŸé—´è§‚å¯Ÿåˆ°è“ç§»å¸æ”¶ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79d7798f41cbf8ddbba0635b89ac8641.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ad94453d518c45f17bdbda3be6b0b85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b7bcae3cb834224cac9eabcffd73261.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78d00bc2bdb18238fc228d32df603503.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3191b1ed86cd71761244a2b76e5f48fc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Patch-GAN-with-Targeted-Patch-Ranking-for-Fine-Grained-Novelty-Detection-in-Medical-Imaging"><a href="#Unsupervised-Patch-GAN-with-Targeted-Patch-Ranking-for-Fine-Grained-Novelty-Detection-in-Medical-Imaging" class="headerlink" title="Unsupervised Patch-GAN with Targeted Patch Ranking for Fine-Grained   Novelty Detection in Medical Imaging"></a>Unsupervised Patch-GAN with Targeted Patch Ranking for Fine-Grained   Novelty Detection in Medical Imaging</h2><p><strong>Authors:Jingkun Chen, Guang Yang, Xiao Zhang, Jingchao Peng, Tianlu Zhang, Jianguo Zhang, Jungong Han, Vicente Grau</strong></p>
<p>Detecting novel anomalies in medical imaging is challenging due to the limited availability of labeled data for rare abnormalities, which often display high variability and subtlety. This challenge is further compounded when small abnormal regions are embedded within larger normal areas, as whole-image predictions frequently overlook these subtle deviations. To address these issues, we propose an unsupervised Patch-GAN framework designed to detect and localize anomalies by capturing both local detail and global structure. Our framework first reconstructs masked images to learn fine-grained, normal-specific features, allowing for enhanced sensitivity to minor deviations from normality. By dividing these reconstructed images into patches and assessing the authenticity of each patch, our approach identifies anomalies at a more granular level, overcoming the limitations of whole-image evaluation. Additionally, a patch-ranking mechanism prioritizes regions with higher abnormal scores, reinforcing the alignment between local patch discrepancies and the global image context. Experimental results on the ISIC 2016 skin lesion and BraTS 2019 brain tumor datasets validate our frameworkâ€™s effectiveness, achieving AUCs of 95.79% and 96.05%, respectively, and outperforming three state-of-the-art baselines. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒä¸­æ£€æµ‹æ–°å‹å¼‚å¸¸æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºç½•è§å¼‚å¸¸çš„æ ‡æ³¨æ•°æ®æœ‰é™ï¼Œè€Œä¸”è¿™äº›å¼‚å¸¸é€šå¸¸è¡¨ç°å‡ºé«˜åº¦å¯å˜æ€§å’Œç»†å¾®æ€§ã€‚å½“è¾ƒå¤§çš„æ­£å¸¸åŒºåŸŸå†…åµŒæœ‰å°çš„å¼‚å¸¸åŒºåŸŸæ—¶ï¼Œè¿™ä¸ªæŒ‘æˆ˜ä¼šè¿›ä¸€æ­¥åŠ å‰§ï¼Œå› ä¸ºå…¨å›¾é¢„æµ‹é€šå¸¸ä¼šå¿½ç•¥è¿™äº›ç»†å¾®çš„åå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„Patch-GANæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡æ•æ‰å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ç»“æ„æ¥æ£€æµ‹å’Œå®šä½å¼‚å¸¸ã€‚æˆ‘ä»¬çš„æ¡†æ¶é¦–å…ˆé‡å»ºè¢«é®æŒ¡çš„å›¾åƒï¼Œä»¥å­¦ä¹ ç²¾ç»†çš„ã€ç‰¹å®šçš„æ­£å¸¸ç‰¹å¾ï¼Œæé«˜å¯¹æ­£å¸¸ç»†å¾®åå·®çš„æ•æ„Ÿæ€§ã€‚é€šè¿‡å°†è¿™äº›é‡å»ºçš„å›¾åƒåˆ†æˆè¡¥ä¸å¹¶è¯„ä¼°æ¯ä¸ªè¡¥ä¸çš„çœŸå®æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨æ›´ç²¾ç»†çš„å±‚é¢ä¸Šè¯†åˆ«å¼‚å¸¸ï¼Œå…‹æœå…¨å›¾è¯„ä¼°çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œè¡¥ä¸æ’åæœºåˆ¶ä¼šä¼˜å…ˆè€ƒè™‘å¼‚å¸¸å¾—åˆ†è¾ƒé«˜çš„åŒºåŸŸï¼ŒåŠ å¼ºå±€éƒ¨è¡¥ä¸å·®å¼‚ä¸å…¨å±€å›¾åƒä¸Šä¸‹æ–‡ä¹‹é—´çš„å¯¹é½ã€‚åœ¨ISIC 2016çš®è‚¤ç—…å˜å’ŒBraTS 2019è„‘è‚¿ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œåˆ†åˆ«å®ç°äº†95.79%å’Œ96.05%çš„AUCï¼Œå¹¶è¶…è¶Šäº†ä¸‰ç§æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17906v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ— ç›‘ç£å­¦ä¹ çš„Patch-GANæ¡†æ¶ï¼Œç”¨äºè§£å†³åŒ»å­¦å›¾åƒä¸­ç½•è§å¼‚å¸¸æ£€æµ‹çš„éš¾é¢˜ã€‚é€šè¿‡é‡æ„å›¾åƒå­¦ä¹ æ­£å¸¸ç‰¹å¾çš„ç»†èŠ‚ï¼Œå¹¶è¯„ä¼°æ¯ä¸ªåŒºåŸŸçš„çœŸå®æ€§ï¼Œä»¥æ›´ç²¾ç»†åœ°æ£€æµ‹å¼‚å¸¸åŒºåŸŸã€‚æ¡†æ¶é‡‡ç”¨æ’åæœºåˆ¶ï¼Œä¼˜å…ˆå¤„ç†å¼‚å¸¸å¾—åˆ†è¾ƒé«˜çš„åŒºåŸŸï¼ŒåŒæ—¶åœ¨å±€éƒ¨è¡¥ä¸å’Œå…¨å±€å›¾åƒä¹‹é—´ä¿æŒä¸€è‡´æ€§ã€‚åœ¨çš®è‚¤ç—…å˜å’Œè„‘è‚¿ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒä¸­ç½•è§å¼‚å¸¸æ£€æµ‹çš„æŒ‘æˆ˜åœ¨äºç¼ºä¹æ ‡è®°æ•°æ®å’Œå¼‚å¸¸çš„é«˜å˜å¼‚æ€§ä¸å¾®å¦™æ€§ã€‚</li>
<li>æå‡ºçš„Patch-GANæ¡†æ¶æ—¨åœ¨é€šè¿‡æ•è·å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ç»“æ„æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ¡†æ¶é€šè¿‡é‡æ„å›¾åƒå­¦ä¹ æ­£å¸¸ç‰¹å¾çš„ç»†èŠ‚ï¼Œæé«˜å¯¹ç»†å¾®å¼‚å¸¸çš„æ•æ„Ÿæ€§ã€‚</li>
<li>é€šè¿‡è¯„ä¼°æ¯ä¸ªå›¾åƒè¡¥ä¸çš„çœŸå®æ€§ï¼Œåœ¨æ›´ç²¾ç»†çš„å±‚é¢ä¸Šæ£€æµ‹å¼‚å¸¸ã€‚</li>
<li>é‡‡ç”¨æ’åæœºåˆ¶ä¼˜å…ˆå¤„ç†å¼‚å¸¸å¾—åˆ†è¾ƒé«˜çš„åŒºåŸŸã€‚</li>
<li>æ¡†æ¶åœ¨çš®è‚¤ç—…å˜å’Œè„‘è‚¿ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œåˆ†åˆ«å®ç°äº†é«˜AUCå€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ba0e43af528f8c7e4189e54b95ffbaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcda4bce2386961a2e5a5e60d42dd42c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-008d51eba3dad496363d56839a78d8ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bd5ff2abd6a8aef1dfc73fbb8d0838c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f8005cc34f257aadc0142b9fcd725e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c45bdefc2ab9f2accd00456f2839088e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VidSole-A-Multimodal-Dataset-for-Joint-Kinetics-Quantification-and-Disease-Detection-with-Deep-Learning"><a href="#VidSole-A-Multimodal-Dataset-for-Joint-Kinetics-Quantification-and-Disease-Detection-with-Deep-Learning" class="headerlink" title="VidSole: A Multimodal Dataset for Joint Kinetics Quantification and   Disease Detection with Deep Learning"></a>VidSole: A Multimodal Dataset for Joint Kinetics Quantification and   Disease Detection with Deep Learning</h2><p><strong>Authors:Archit Kambhamettu, Samantha Snyder, Maliheh Fakhar, Samuel Audia, Ross Miller, Jae Kun Shim, Aniket Bera</strong></p>
<p>Understanding internal joint loading is critical for diagnosing gait-related diseases such as knee osteoarthritis; however, current methods of measuring joint risk factors are time-consuming, expensive, and restricted to lab settings. In this paper, we enable the large-scale, cost-effective biomechanical analysis of joint loading via three key contributions: the development and deployment of novel instrumented insoles, the creation of a large multimodal biomechanics dataset (VidSole), and a baseline deep learning pipeline to predict internal joint loading factors. Our novel instrumented insole measures the tri-axial forces and moments across five high-pressure points under the foot. VidSole consists of the forces and moments measured by these insoles along with corresponding RGB video from two viewpoints, 3D body motion capture, and force plate data for over 2,600 trials of 52 diverse participants performing four fundamental activities of daily living (sit-to-stand, stand-to-sit, walking, and running). We feed the insole data and kinematic parameters extractable from video (i.e., pose, knee angle) into a deep learning pipeline consisting of an ensemble Gated Recurrent Unit (GRU) activity classifier followed by activity-specific Long Short Term Memory (LSTM) regression networks to estimate knee adduction moment (KAM), a biomechanical risk factor for knee osteoarthritis. The successful classification of activities at an accuracy of 99.02 percent and KAM estimation with mean absolute error (MAE) less than 0.5 percent<em>body weight</em>height, the current threshold for accurately detecting knee osteoarthritis with KAM, illustrates the usefulness of our dataset for future research and clinical settings. </p>
<blockquote>
<p>ç†è§£å…³èŠ‚å†…éƒ¨å—åŠ›å¯¹äºè¯Šæ–­æ­¥æ€ç›¸å…³ç–¾ç—…ï¼ˆå¦‚è†å…³èŠ‚éª¨å…³èŠ‚ç‚ï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰æµ‹é‡å…³èŠ‚é£é™©å› ç´ çš„æ–¹æ³•è€—æ—¶ã€æ˜‚è´µï¼Œä¸”ä»…é™äºå®éªŒå®¤ç¯å¢ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªå…³é”®è´¡çŒ®å®ç°äº†å¤§è§„æ¨¡ã€æ€§ä»·æ¯”é«˜çš„å…³èŠ‚åŠ è½½ç”Ÿç‰©åŠ›å­¦åˆ†æï¼šæ–°å‹ä»ªå™¨åŒ–é‹å«çš„å¼€å‘ä¸éƒ¨ç½²ã€å¤§å‹å¤šæ¨¡å¼ç”Ÿç‰©åŠ›å­¦æ•°æ®é›†ï¼ˆVidSoleï¼‰çš„åˆ›å»ºï¼Œä»¥åŠé¢„æµ‹å†…éƒ¨å…³èŠ‚åŠ è½½å› ç´ çš„æ·±åº¦å­¦ä¹ ç®¡é“åŸºçº¿ã€‚æˆ‘ä»¬æ–°å‹ä»ªå™¨åŒ–é‹å«å¯ä»¥æµ‹é‡äº”ä¸ªé«˜å‹åŠ›ç‚¹ä¸‹çš„ä¸‰è½´åŠ›å’ŒåŠ›çŸ©ã€‚VidSoleç”±è¿™äº›é‹å«æµ‹é‡çš„åŠ›å’ŒåŠ›çŸ©ä»¥åŠæ¥è‡ªä¸¤ä¸ªè§†è§’çš„ç›¸åº”RGBè§†é¢‘ã€3Däººä½“è¿åŠ¨æ•æ‰å’ŒåŠ›é‡æ¿æ•°æ®ç»„æˆï¼Œæ•°æ®æ¥è‡ªè¶…è¿‡2600æ¬¡è¯•éªŒï¼Œæ¶‰åŠ52åä¸åŒå‚ä¸è€…è¿›è¡Œå››é¡¹æ—¥å¸¸åŸºæœ¬æ´»åŠ¨ï¼ˆä»ååˆ°ç«™ã€ä»ç«™åˆ°åã€è¡Œèµ°å’Œè·‘æ­¥ï¼‰ã€‚æˆ‘ä»¬å°†é‹å«æ•°æ®å’Œå¯ä»è§†é¢‘ä¸­æå–çš„è¿åŠ¨å­¦å‚æ•°ï¼ˆå³å§¿åŠ¿ã€è†å…³èŠ‚è§’åº¦ï¼‰è¾“å…¥æ·±åº¦å­¦ä¹ ç®¡é“ï¼Œè¯¥ç®¡é“ç”±é›†æˆé—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰æ´»åŠ¨åˆ†ç±»å™¨ç»„æˆï¼Œå…¶æ¬¡æ˜¯ç‰¹å®šæ´»åŠ¨çš„é•¿çŸ­æ—¶è®°å¿†ï¼ˆLSTMï¼‰å›å½’ç½‘ç»œï¼Œç”¨äºä¼°è®¡è†å…³èŠ‚å†…æ”¶åŠ›çŸ©ï¼ˆKAMï¼‰ï¼Œè¿™æ˜¯è†å…³èŠ‚éª¨å…³èŠ‚ç‚çš„ç”Ÿç‰©åŠ›å­¦é£é™©å› ç´ ã€‚æ´»åŠ¨åˆ†ç±»çš„æˆåŠŸå‡†ç¡®ç‡ä¸º99.02%ï¼Œè†å…³èŠ‚å†…æ”¶åŠ›çŸ©ä¼°è®¡çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä½äºä½“é‡èº«é«˜çš„0.5%ï¼Œè¿™ä¸€æ•°æ®å‡†ç¡®æ£€æµ‹è†å…³èŠ‚éª¨å…³èŠ‚ç‚çš„é˜ˆå€¼è¯´æ˜äº†æˆ‘ä»¬çš„æ•°æ®é›†å¯¹æœªæ¥ç ”ç©¶å’Œä¸´åºŠè®¾ç½®çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.17890v1">PDF</a> Accepted by AAAI 2025 Special Track on AI for Social Impact</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡æ–°å‹ä»ªå™¨åŒ–é‹å«ã€å¤šæ¨¡å¼ç”Ÿç‰©åŠ›å­¦æ•°æ®é›†ï¼ˆVidSoleï¼‰å’Œæ·±åº¦å­¦ä¹ ç®¡é“ï¼Œå®ç°å¯¹å…³èŠ‚è½½è·çš„å¤§è§„æ¨¡ã€æˆæœ¬æ•ˆç›Šé«˜çš„ç”Ÿç‰©åŠ›å­¦åˆ†æã€‚æ–°å‹é‹å«å¯æµ‹é‡äº”ä¸ªé«˜å‹ç‚¹ä¸‹çš„ä¸‰è½´åŠ›å’ŒåŠ›çŸ©ã€‚VidSoleæ•°æ®é›†åŒ…æ‹¬é‹å«æµ‹é‡çš„åŠ›å’ŒåŠ›çŸ©ã€ä¸¤ä¸ªè§†è§’çš„RGBè§†é¢‘ã€3Dèº«ä½“è¿åŠ¨æ•è·ä»¥åŠè¶…è¿‡2600æ¬¡è¯•éªŒçš„æ•°æ®ï¼Œæ¶‰åŠ52åä¸åŒå‚ä¸è€…æ‰§è¡Œå››é¡¹æ—¥å¸¸åŸºæœ¬æ´»åŠ¨ï¼ˆåç«‹ã€ç«™ç«‹ã€è¡Œèµ°å’Œè·‘æ­¥ï¼‰ã€‚ç ”ç©¶é€šè¿‡æ·±åº¦å­¦ä¹ ç®¡é“æˆåŠŸå®ç°æ´»åŠ¨åˆ†ç±»ä¸è†å…³èŠ‚åŠ è½½åŠ›çŸ©çš„ä¼°è®¡ï¼Œå¯¹äºè†å…³èŠ‚éª¨å…³èŠ‚ç‚çš„è¯Šæ–­å…·æœ‰é‡è¦ä»·å€¼ã€‚æ´»åŠ¨åˆ†ç±»å‡†ç¡®ç‡ä¸º99.02%ï¼Œè†å…³èŠ‚åŠ è½½åŠ›çŸ©ä¼°è®¡è¯¯å·®ä½äºä½“é‡ä¹˜ä»¥èº«é«˜çš„ç™¾åˆ†æ¯”å€¼ï¼ˆå°äºæ ‡å‡†è†å…³èŠ‚éª¨å…³èŠ‚ç‚è¯Šæ–­é˜ˆå€¼ï¼‰ã€‚è¯¥æ•°æ®é›†å¯¹åç»­ç ”ç©¶å’Œä¸´åºŠåº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä¸€ã€åˆ©ç”¨æ–°å‹ä»ªå™¨åŒ–é‹å«æµ‹é‡è¶³éƒ¨äº”ä¸ªé«˜å‹ç‚¹çš„ä¸‰è½´åŠ›å’ŒåŠ›çŸ©ï¼Œä¸ºå…³èŠ‚è½½è·åˆ†ææä¾›æ–°æ–¹æ³•ã€‚<br>äºŒã€åˆ›å»ºå¤§å‹å¤šæ¨¡å¼ç”Ÿç‰©åŠ›å­¦æ•°æ®é›†VidSoleï¼Œæ¶µç›–å¤šç§æ´»åŠ¨å’Œå‚ä¸è€…æ•°æ®ï¼Œä¿ƒè¿›æ·±å…¥ç ”ç©¶ã€‚<br>ä¸‰ã€å»ºç«‹æ·±åº¦å­¦ä¹ ç®¡é“ï¼Œä»¥ä¼°è®¡è†å…³èŠ‚éª¨å…³èŠ‚ç‚é£é™©çš„å…³é”®å› ç´ â€”â€”è†å…³èŠ‚åŠ è½½åŠ›çŸ©ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹é‡‡ç”¨æ´»åŠ¨åˆ†ç±»å™¨ç»“åˆé•¿æœŸè®°å¿†å›å½’ç½‘ç»œã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.17890">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-664c14e207d65f74622432937e64c457.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09905a9e3857a07e9f6d990798311da6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd8a2cbb338723222f2e6ee7f73cef50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5eacfae8351194969aef0c7b7e9d8f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f642a0d2f3bcaab3301200779a75001e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8ea6801f742c8ef3d21aaf26ad4a7b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6852e1b79e89041dc9ca73f6c66b052c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Exploring-AI-based-System-Design-for-Pixel-level-Protected-Health-Information-Detection-in-Medical-Images"><a href="#Exploring-AI-based-System-Design-for-Pixel-level-Protected-Health-Information-Detection-in-Medical-Images" class="headerlink" title="Exploring AI-based System Design for Pixel-level Protected Health   Information Detection in Medical Images"></a>Exploring AI-based System Design for Pixel-level Protected Health   Information Detection in Medical Images</h2><p><strong>Authors:Tuan Truong, Ivo M. Baltruschat, Mark Klemens, Grit Werner, Matthias Lenga</strong></p>
<p>Purpose: This study aims to evaluate different setups of an AI-based solution to detect Protected Health Information (PHI) in medical images.   Materials and Methods: Text from eight PHI and eight non-PHI categories are simulated and incorporated into a curated dataset comprising 1,000 medical images across four modalities: CT, X-ray, bone scan, and MRI. The proposed PHI detection pipeline comprises three key components: text localization, extraction, and analysis. Three vision and language models, YOLOv11, EasyOCR, and GPT-4o, are benchmarked in different setups corresponding to three key components. The performance is evaluated with classification metrics, including precision, recall, F1 score, and accuracy.   Results: All four setups demonstrate strong performance in detecting PHI imprints, with all metrics exceeding 0.9. The setup that utilizes YOLOv11 for text localization and GPT-4o for text extraction and analysis achieves the highest performance in PHI detection. However, this setup incurs the highest cost due to the increased number of generated tokens associated with GPT-4o model. Conversely, the setup using solely GPT-4o for the end-to-end pipeline exhibits the lowest performance but showcases the feasibility of multi-modal models in solving complex tasks.   Conclusion: For optimal text localization and extraction, it is recommended to fine-tune an object detection model and utilize built-in Optical Character Recognition (OCR) software. Large language models like GPT-4o can be effectively leveraged to reason about and semantically analyze the PHI content. Although the vision capability of GPT-4o is promising for reading image crops, it remains limited for end-to-end pipeline applications with whole images. </p>
<blockquote>
<p><strong>ç›®çš„</strong>ï¼šæœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°åŸºäºäººå·¥æ™ºèƒ½çš„è§£å†³æ–¹æ¡ˆåœ¨ä¸åŒé…ç½®ä¸‹æ£€æµ‹åŒ»å­¦å›¾åƒä¸­çš„å—ä¿æŠ¤å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>ææ–™ä¸æ–¹æ³•</strong>ï¼šæ¨¡æ‹Ÿäº†æ¥è‡ªå…«ä¸ªPHIå’Œå…«ä¸ªéPHIç±»åˆ«çš„æ–‡æœ¬ï¼Œå¹¶çº³å…¥åŒ…å«1000å¼ åŒ»å­¦å›¾åƒçš„å®šåˆ¶æ•°æ®é›†ï¼Œæ¶‰åŠå››ç§æ¨¡æ€ï¼šCTã€Xå…‰ã€éª¨æ‰«æå’ŒMRIã€‚æå‡ºçš„PHIæ£€æµ‹æµç¨‹åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ–‡æœ¬å®šä½ã€æå–å’Œåˆ†æã€‚ä¸‰ç§è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œå³YOLOv11ã€EasyOCRå’ŒGPT-4oï¼Œåœ¨ä¸åŒçš„é…ç½®ä¸‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œåˆ†åˆ«å¯¹åº”äºä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚æ€§èƒ½è¯„ä¼°é‡‡ç”¨åˆ†ç±»æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’Œå‡†ç¡®ç‡ã€‚</p>
<p><strong>ç»“æœ</strong>ï¼šæ‰€æœ‰å››ç§é…ç½®åœ¨æ£€æµ‹PHIå°è®°æ–¹é¢éƒ½è¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ï¼Œå„é¡¹æŒ‡æ ‡å‡è¶…è¿‡0.9ã€‚ä½¿ç”¨YOLOv11è¿›è¡Œæ–‡æœ¬å®šä½ï¼ŒGPT-4oè¿›è¡Œæ–‡æœ¬æå–å’Œåˆ†æçš„é…ç½®åœ¨PHIæ£€æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ã€‚ç„¶è€Œï¼Œç”±äºGPT-4oæ¨¡å‹äº§ç”Ÿçš„ä»¤ç‰Œæ•°é‡å¢åŠ ï¼Œè¯¥é…ç½®çš„æˆæœ¬ä¹Ÿæœ€é«˜ã€‚ç›¸åï¼Œä»…ä½¿ç”¨GPT-4oè¿›è¡Œç«¯åˆ°ç«¯ç®¡é“çš„é…ç½®æ€§èƒ½æœ€ä½ï¼Œä½†å±•ç¤ºäº†å¤šæ¨¡æ€æ¨¡å‹è§£å†³å¤æ‚ä»»åŠ¡çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09552v2">PDF</a> In progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°åŸºäºAIçš„è§£å†³æ–¹æ¡ˆåœ¨ä¸åŒé…ç½®ä¸‹æ£€æµ‹åŒ»å­¦å›¾åƒä¸­çš„å—ä¿æŠ¤å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä½¿ç”¨æ¨¡æ‹Ÿçš„æ–‡æœ¬æ•°æ®åˆ›å»ºäº†åŒ…å«ä¸€åƒå¼ è·¨å››ç§æ¨¡æ€ï¼ˆCTã€Xå…‰ã€éª¨æ‰«æå’ŒMRIï¼‰çš„åŒ»å­¦å›¾åƒæ•°æ®é›†ã€‚æå‡ºçš„PHIæ£€æµ‹æµç¨‹åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šæ–‡æœ¬å®šä½ã€æå–å’Œåˆ†æã€‚ä½¿ç”¨YOLOv11ã€EasyOCRå’ŒGPT-4oç­‰è§†è§‰å’Œè¯­è¨€æ¨¡å‹åœ¨ä¸åŒé…ç½®ä¸‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æ‰€æœ‰é…ç½®åœ¨æ£€æµ‹PHIå°è®°æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå„é¡¹æŒ‡æ ‡å‡è¶…è¿‡0.9ã€‚ä½¿ç”¨YOLOv11è¿›è¡Œæ–‡æœ¬å®šä½å’ŒGPT-4oè¿›è¡Œæ–‡æœ¬æå–å’Œåˆ†æçš„é…ç½®åœ¨PHIæ£€æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä½†æˆæœ¬æœ€é«˜ã€‚ä»…ä½¿ç”¨GPT-4oè¿›è¡Œç«¯åˆ°ç«¯æµç¨‹çš„é…ç½®æ€§èƒ½æœ€ä½ï¼Œä½†è¯æ˜äº†å¤šæ¨¡æ€æ¨¡å‹åœ¨è§£å†³å¤æ‚ä»»åŠ¡ä¸­çš„å¯è¡Œæ€§ã€‚å»ºè®®å¯¹ç›®æ ‡æ£€æµ‹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åˆ©ç”¨å†…ç½®çš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰è½¯ä»¶è¿›è¡Œæ–‡æœ¬å®šä½å’Œæå–ã€‚å¤§å‹è¯­è¨€æ¨¡å‹å¦‚GPT-4oå¯ä»¥æœ‰æ•ˆåœ°æ¨ç†å’Œè¯­ä¹‰åˆ†æPHIå†…å®¹ã€‚å°½ç®¡GPT-4oåœ¨è¯»å–å›¾åƒè£å‰ªæ–¹é¢çš„è§†è§‰èƒ½åŠ›ä»¤äººé¼“èˆï¼Œä½†å¯¹äºç«¯åˆ°ç«¯ç®¡é“åº”ç”¨æ¥è¯´ä»ç„¶æœ‰é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨è¯„ä¼°ä¸åŒé…ç½®çš„AIè§£å†³æ–¹æ¡ˆåœ¨åŒ»å­¦å›¾åƒä¸­æ£€æµ‹PHIçš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨æ¨¡æ‹Ÿæ–‡æœ¬æ•°æ®åˆ›å»ºäº†åŒ…å«å››ç§æ¨¡æ€çš„åŒ»å­¦å›¾åƒæ•°æ®é›†ã€‚</li>
<li>PHIæ£€æµ‹æµç¨‹åŒ…å«æ–‡æœ¬å®šä½ã€æå–å’Œåˆ†æä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>ä½¿ç”¨YOLOv11å’ŒGPT-4oç­‰æ¨¡å‹è¿›è¡ŒPHIæ£€æµ‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>YOLOv11ä¸GPT-4oç»“åˆåœ¨PHIæ£€æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä½†æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>GPT-4oåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„å¯è¡Œæ€§å¾—åˆ°å±•ç¤ºï¼Œä½†åœ¨å¤„ç†æ•´ä¸ªå›¾åƒæ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e004b3e2266e1d1aa435b96c0237af2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b895074c919a9bc09aada6c68c491a23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df0e7cbf56db148abcd25ab0d0c53589.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b6fdae50c57407d74d07d2236bb0bde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94b605ef4469f3a8895b8258f18327a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3568d1bfbb8c067a63cf873dda8510d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4b1525a57f860fe9d164e805e91ac27.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Quantum-Down-Sampling-Filter-for-Variational-Auto-encoder"><a href="#Quantum-Down-Sampling-Filter-for-Variational-Auto-encoder" class="headerlink" title="Quantum Down Sampling Filter for Variational Auto-encoder"></a>Quantum Down Sampling Filter for Variational Auto-encoder</h2><p><strong>Authors:Farina Riaz, Fakhar Zaman, Hajime Suzuki, Sharif Abuadbba, David Nguyen</strong></p>
<p>Variational Autoencoders (VAEs) are essential tools in generative modeling and image reconstruction, with their performance heavily influenced by the encoder-decoder architecture. This study aims to improve the quality of reconstructed images by enhancing their resolution and preserving finer details, particularly when working with low-resolution inputs (16x16 pixels), where traditional VAEs often yield blurred or in-accurate results. To address this, we propose a hybrid model that combines quantum computing techniques in the VAE encoder with convolutional neural networks (CNNs) in the decoder. By upscaling the resolution from 16x16 to 32x32 during the encoding process, our approach evaluates how the model reconstructs images with enhanced resolution while maintaining key features and structures. This method tests the modelâ€™s robustness in handling image reconstruction and its ability to preserve essential details despite training on lower-resolution data. We evaluate our proposed down sampling filter for Quantum VAE (Q-VAE) on the MNIST and USPS datasets and compare it with classical VAEs and a variant called Classical Direct Passing VAE (CDP-VAE), which uses windowing pooling filters in the encoding process. Performance is assessed using metrics such as the Frechet Inception Distance (FID) and Mean Squared Error (MSE), which measure the fidelity of reconstructed images. Our results demonstrate that the Q-VAE consistently outperforms both the Classical VAE and CDP-VAE, achieving significantly lower FID and MSE scores. Additionally, CDP-VAE yields better performance than C-VAE. These findings highlight the potential of quantum-enhanced VAEs to improve image reconstruction quality by enhancing resolution and preserving essential features, offering a promising direction for future applications in computer vision and synthetic data generation. </p>
<blockquote>
<p>å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰åœ¨ç”Ÿæˆå»ºæ¨¡å’Œå›¾åƒé‡å»ºä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œå…¶æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—åˆ°ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„å½±å“ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æé«˜é‡å»ºå›¾åƒçš„è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä½åˆ†è¾¨ç‡è¾“å…¥ï¼ˆå¦‚16x16åƒç´ ï¼‰æ—¶ï¼Œé€šè¿‡æé«˜åˆ†è¾¨ç‡å¹¶ä¿æŒæ›´ç²¾ç»†çš„ç»†èŠ‚æ¥æå‡å›¾åƒè´¨é‡ï¼Œå› ä¸ºä¼ ç»ŸVAEså¾€å¾€ä¼šäº§ç”Ÿæ¨¡ç³Šæˆ–ä¸å‡†ç¡®çš„ç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å˜åˆ†è‡ªç¼–ç å™¨ç¼–ç å™¨ä¸­çš„é‡å­è®¡ç®—æŠ€æœ¯å’Œè§£ç å™¨ä¸­çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨ç¼–ç è¿‡ç¨‹ä¸­å°†åˆ†è¾¨ç‡ä»16x16æé«˜åˆ°32x32æ¥è¯„ä¼°æ¨¡å‹å¦‚ä½•ä»¥æ›´é«˜çš„åˆ†è¾¨ç‡é‡å»ºå›¾åƒï¼ŒåŒæ—¶ä¿æŒå…³é”®ç‰¹å¾çš„ç»“æ„ã€‚è¿™ç§æ–¹æ³•æµ‹è¯•äº†æ¨¡å‹åœ¨å¤„ç†å›¾åƒé‡å»ºæ–¹é¢çš„ç¨³å¥æ€§ä»¥åŠå…¶åœ¨å¤„ç†ä½åˆ†è¾¨ç‡æ•°æ®æ—¶ä¿ç•™é‡è¦ç»†èŠ‚çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨MNISTå’ŒUSPSæ•°æ®é›†ä¸Šè¯„ä¼°äº†é’ˆå¯¹é‡å­å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆQ-VAEï¼‰æå‡ºçš„ä¸‹é‡‡æ ·æ»¤æ³¢å™¨ï¼Œå¹¶å°†å…¶ä¸ç»å…¸å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆC-VAEï¼‰å’Œä¸€ç§ç§°ä¸ºç»å…¸ç›´æ¥ä¼ é€’å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆCDP-VAEï¼‰çš„å˜ä½“è¿›è¡Œäº†æ¯”è¾ƒã€‚åè€…åœ¨ç¼–ç è¿‡ç¨‹ä¸­ä½¿ç”¨çª—å£æ± åŒ–æ»¤æ³¢å™¨ã€‚æ€§èƒ½è¯„ä¼°ä½¿ç”¨äº†Frechet Inception Distanceï¼ˆFIDï¼‰å’Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ç­‰åº¦é‡æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡è¡¡é‡äº†é‡å»ºå›¾åƒçš„ä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒQ-VAEå§‹ç»ˆä¼˜äºç»å…¸VAEå’ŒCDP-VAEï¼Œè·å¾—äº†æ›´ä½çš„FIDå’ŒMSEåˆ†æ•°ã€‚æ­¤å¤–ï¼ŒCDP-VAEçš„æ€§èƒ½ä¹Ÿæ¯”C-VAEè¦å¥½ã€‚è¿™äº›å‘ç°çªå‡ºäº†é‡å­å¢å¼ºå‹VAEåœ¨æé«˜å›¾åƒé‡å»ºè´¨é‡æ–¹é¢çš„æ½œåŠ›ï¼Œé€šè¿‡æé«˜åˆ†è¾¨ç‡å¹¶ä¿æŒå…³é”®ç‰¹å¾æ¥å®ç°ï¼Œä¸ºè®¡ç®—æœºè§†è§‰å’Œåˆæˆæ•°æ®ç”Ÿæˆé¢†åŸŸçš„åº”ç”¨æä¾›äº†æœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06259v2">PDF</a> 19 pages, 13 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ—¨åœ¨æå‡å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰åœ¨ç”Ÿæˆæ¨¡å‹å’Œå›¾åƒé‡å»ºä¸­çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä½åˆ†è¾¨ç‡è¾“å…¥ï¼ˆ16x16åƒç´ ï¼‰çš„æƒ…å†µã€‚ä¼ ç»ŸVAEåœ¨å¤„ç†æ­¤ç±»è¾“å…¥æ—¶å¾€å¾€äº§ç”Ÿæ¨¡ç³Šæˆ–ä¸å‡†ç¡®çš„ç»“æœã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆé‡å­è®¡ç®—å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ··åˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨VAEç¼–ç å™¨ä¸­ä½¿ç”¨é‡å­è®¡ç®—æŠ€æœ¯ï¼Œå¹¶åœ¨è§£ç å™¨ä¸­ä½¿ç”¨CNNã€‚é€šè¿‡ä»16x16åˆ†è¾¨ç‡æå‡åˆ°32x32åˆ†è¾¨ç‡è¿›è¡Œå›¾åƒé‡å»ºï¼Œè¯„ä¼°æ¨¡å‹åœ¨æå‡åˆ†è¾¨ç‡çš„åŒæ—¶ä¿æŒå…³é”®ç‰¹å¾å’Œç»“æ„çš„èƒ½åŠ›ã€‚å®éªŒåœ¨MNISTå’ŒUSPSæ•°æ®é›†ä¸Šå¯¹æ‰€æå‡ºçš„é‡å­VAEï¼ˆQ-VAEï¼‰è¿›è¡Œä¸‹é‡‡æ ·æ»¤æ³¢å™¨çš„æµ‹è¯•ï¼Œå¹¶ä¸ä¼ ç»ŸVAEå’Œä¸€ç§ç§°ä¸ºç»å…¸ç›´æ¥ä¼ é€’VAEï¼ˆCDP-VAEï¼‰çš„å˜ä½“è¿›è¡Œæ¯”è¾ƒã€‚æ ¹æ®Frechet Inception Distanceï¼ˆFIDï¼‰å’ŒMean Squared Errorï¼ˆMSEï¼‰ç­‰åº¦é‡æŒ‡æ ‡ï¼ŒQ-VAEçš„æ€§èƒ½è¡¨ç°æœ€ä½³ï¼Œå–å¾—äº†æ›´ä½çš„FIDå’ŒMSEåˆ†æ•°ã€‚è¿™äº›ç»“æœçªæ˜¾äº†é‡å­å¢å¼ºVAEåœ¨æé«˜å›¾åƒé‡å»ºè´¨é‡å’Œä¿æŒå…³é”®ç‰¹å¾æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè®¡ç®—æœºè§†è§‰å’Œåˆæˆæ•°æ®ç”Ÿæˆç­‰é¢†åŸŸçš„æœªæ¥åº”ç”¨æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æ—¨åœ¨æé«˜å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰åœ¨ä½åˆ†è¾¨ç‡è¾“å…¥ä¸‹çš„å›¾åƒé‡å»ºè´¨é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆé‡å­è®¡ç®—å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ··åˆæ¨¡å‹ï¼Œå³é‡å­VAEï¼ˆQ-VAEï¼‰ã€‚</li>
<li>Q-VAEåœ¨ç¼–ç è¿‡ç¨‹ä¸­å®ç°äº†ä»16x16åˆ°32x32çš„åˆ†è¾¨ç‡æå‡ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨æå‡åˆ†è¾¨ç‡åŒæ—¶ä¿æŒå…³é”®ç‰¹å¾çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨MNISTå’ŒUSPSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒQ-VAEåœ¨å›¾åƒé‡å»ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºä¼ ç»ŸVAEå’ŒCDP-VAEã€‚</li>
<li>Q-VAEåœ¨Frechet Inception Distanceï¼ˆFIDï¼‰å’ŒMean Squared Errorï¼ˆMSEï¼‰ç­‰åº¦é‡æŒ‡æ ‡ä¸Šå–å¾—äº†æ›´ä½çš„åˆ†æ•°ï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœçªæ˜¾äº†é‡å­å¢å¼ºVAEåœ¨å›¾åƒé‡å»ºå’Œç‰¹å¾ä¿æŒæ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eeee58ae8c992c03617c542ff2947745.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d47b9e3a5d4a0f20784da67f5abc354.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Improved-Analytic-Love-C-Relations-for-Neutron-Stars"><a href="#Improved-Analytic-Love-C-Relations-for-Neutron-Stars" class="headerlink" title="Improved Analytic Love-C Relations for Neutron Stars"></a>Improved Analytic Love-C Relations for Neutron Stars</h2><p><strong>Authors:Tristen Lowrey, Kent Yagi, NicolÃ¡s Yunes</strong></p>
<p>Precise measurements of neutron star observables (such as mass and radius) allow one to constrain the equations of state for supranuclear matter and develop a stronger understanding of nuclear physics. The Neutron star Interior Composition ExploreR (NICER) tracks X-ray hotspots on rotating NSs and is able to infer precise information about the compactness of the star. Gravitational waves carry information about the tidal deformability (related to the tidal Love number) of neutron stars, which has been measured by the LIGO&#x2F;Virgo&#x2F;KAGRA collaboration. These two observables enjoy an approximately universal property between each other that is insensitive to the equations of state (the â€œuniversal Love-C relationâ€). In this paper, we focus on deriving two analytic expressions for the Love-C relations that are ready-to-use and improve upon previous analytic expressions. The first model is inspired by a Newtonian polytrope, whose perturbation to the gravitational potential can be found analytically. We extend this Newtonian model to the relativistic regime by providing a quadratic fit to the gravitational potential perturbation against stellar compactness. The second model makes use of the Tolman VII model and adopts a spectral expansion with Chebyshev polynomials, which converges faster than the Taylor expansions used in previous work. We find that the first model provides a more accurate description of the Love-C relation for realistic neutron stars than the second model, and it provides the best expression among all other analytic relations studied here in terms of describing the averaged numerical Love-C relation. These new models are not only useful in practice, but they also show the power and importance of analytic modeling of neutron stars. </p>
<blockquote>
<p>ç²¾ç¡®æµ‹é‡ä¸­å­æ˜Ÿçš„å¯è§‚æµ‹å€¼ï¼ˆå¦‚è´¨é‡å’ŒåŠå¾„ï¼‰å¯ä»¥é™åˆ¶è¶…æ ¸ç‰©è´¨çš„ç‰©æ€æ–¹ç¨‹ï¼Œå¹¶åŠ å¼ºå¯¹æ ¸ç‰©ç†çš„ç†è§£ã€‚ä¸­å­æ˜Ÿå†…éƒ¨ç»“æ„æ¢æµ‹å™¨ï¼ˆNICERï¼‰è¿½è¸ªæ—‹è½¬ä¸­å­æ˜Ÿä¸Šçš„Xå°„çº¿çƒ­ç‚¹ï¼Œå¹¶èƒ½å¤Ÿæ¨æ–­å‡ºå…³äºæ’æ˜Ÿè‡´å¯†æ€§çš„ç²¾ç¡®ä¿¡æ¯ã€‚å¼•åŠ›æ³¢æºå¸¦æœ‰å…³ä¸­å­æ½®æ¶¨å˜å½¢æ€§çš„ä¿¡æ¯ï¼ˆä¸æ½®æ±æ´›å¤«æ•°æœ‰å…³ï¼‰ï¼Œè¿™æ˜¯ç”±LIGO&#x2F;Virgo&#x2F;KAGRAåˆä½œæµ‹é‡çš„ã€‚è¿™ä¸¤ä¸ªå¯è§‚æµ‹å€¼ä¹‹é—´å…·æœ‰å½¼æ­¤å¤§è‡´é€šç”¨çš„å±æ€§ï¼Œå¯¹ç‰©æ€æ–¹ç¨‹ä¸æ•æ„Ÿï¼ˆâ€œé€šç”¨æ´›å¤«-Cå…³ç³»â€ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå¯¼å‡ºä¸¤ä¸ªæ´›å¤«-Cå…³ç³»çš„è§£æè¡¨è¾¾å¼ï¼Œè¿™äº›è¡¨è¾¾å¼å³å­¦å³ç”¨ï¼Œå¹¶æ”¹è¿›äº†å…ˆå‰çš„è§£æè¡¨è¾¾å¼ã€‚ç¬¬ä¸€ä¸ªæ¨¡å‹å—åˆ°ç‰›é¡¿å¤šè¾¹å½¢çš„å¯å‘ï¼Œå…¶å¼•åŠ›åŠ¿çš„æ‰°åŠ¨å¯ä»¥åˆ†ææ‰¾åˆ°ã€‚æˆ‘ä»¬é€šè¿‡å°†å¼•åŠ›åŠ¿æ‰°åŠ¨ä¸æ’æ˜Ÿè‡´å¯†æ€§è¿›è¡ŒäºŒæ¬¡æ‹Ÿåˆï¼Œå°†è¿™ä¸€ç‰›é¡¿æ¨¡å‹æ‰©å±•åˆ°ç›¸å¯¹è®ºé¢†åŸŸã€‚ç¬¬äºŒä¸ªæ¨¡å‹åˆ©ç”¨Tolman VIIæ¨¡å‹ï¼Œé‡‡ç”¨åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼çš„è°±å±•å¼€ï¼Œå…¶æ”¶æ•›é€Ÿåº¦æ¯”å…ˆå‰å·¥ä½œä¸­ä½¿ç”¨çš„æ³°å‹’å±•å¼€æ›´å¿«ã€‚æˆ‘ä»¬å‘ç°ï¼Œç¬¬ä¸€ä¸ªæ¨¡å‹ä¸ºç°å®çš„ä¸­å­æ˜Ÿæä¾›äº†æ›´å‡†ç¡®çš„æ´›å¤«-Cå…³ç³»æè¿°ï¼Œå¹¶ä¸”åœ¨æè¿°å¹³å‡æ•°å€¼æ´›å¤«-Cå…³ç³»æ–¹é¢ï¼Œå®ƒåœ¨è¿™é‡Œç ”ç©¶çš„æ‰€æœ‰å…¶ä»–è§£æå…³ç³»ä¸­è¡¨ç°æœ€ä½³ã€‚è¿™äº›æ–°æ¨¡å‹ä¸ä»…åœ¨å®è·µä¸­å¾ˆæœ‰ç”¨ï¼Œè€Œä¸”å±•ç¤ºäº†ä¸­å­æ˜Ÿè§£æå»ºæ¨¡çš„åŠ›é‡å’Œé‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06358v2">PDF</a> 11 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>ä¸­å­æ˜Ÿè§‚æµ‹æ•°æ®çš„ç²¾ç¡®æµ‹é‡ï¼ˆå¦‚è´¨é‡å’ŒåŠå¾„ï¼‰å¯¹äºçº¦æŸè¶…æ ¸ç‰©è´¨çš„ç‰©æ€æ–¹ç¨‹ä»¥åŠå¢å¼ºå¯¹æ ¸ç‰©ç†çš„ç†è§£è‡³å…³é‡è¦ã€‚NICERå®éªŒé€šè¿‡è¿½è¸ªæ—‹è½¬ä¸­å­æ˜Ÿä¸Šçš„Xå°„çº¿çƒ­ç‚¹æ¥æ¨æ–­å…³äºæ˜Ÿä½“ç´§è‡´æ€§çš„ç²¾ç¡®ä¿¡æ¯ã€‚å¼•åŠ›æ³¢æºå¸¦æœ‰å…³ä¸­å­æ˜Ÿæ½®æ±å˜å½¢æ€§çš„ä¿¡æ¯ï¼Œè¿™ä¸€ä¿¡æ¯å·²ç»ç”±LIGO&#x2F;Virgo&#x2F;KAGRAåˆä½œæµ‹é‡ã€‚ä¸­å­æ˜Ÿçš„æ½®æ±çˆ±ä¸Cå…³ç³»ï¼ˆLove-C relationï¼‰å…·æœ‰ä¸€ä¸ªå¤§è‡´çš„é€šç”¨å±æ€§ï¼Œè¯¥å±æ€§å¯¹ç‰©æ€æ–¹ç¨‹ä¸æ•æ„Ÿã€‚æœ¬æ–‡é‡ç‚¹æ¨å¯¼äº†ä¸¤ä¸ªç”¨äºLove-Cå…³ç³»çš„è§£æè¡¨è¾¾å¼ï¼Œè¿™äº›è¡¨è¾¾å¼ä½¿ç”¨æ–¹ä¾¿å¹¶åœ¨ä¹‹å‰çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ã€‚ç¬¬ä¸€ä¸ªæ¨¡å‹å—åˆ°ç‰›é¡¿å¤šè¾¹å½¢çš„å¯å‘ï¼Œå…¶å¼•åŠ›åŠ¿æ‰°åŠ¨å¯æ‰¾åˆ°è§£æè§£ã€‚æˆ‘ä»¬é€šè¿‡å°†ç‰›é¡¿æ¨¡å‹æ‰©å±•ä¸ºç›¸å¯¹è®ºæ¨¡å‹ï¼Œæä¾›äº†ä¸€ä¸ªå¼•åŠ›åŠ¿æ‰°åŠ¨ç›¸å¯¹äºæ’æ˜Ÿç´§è‡´æ€§çš„äºŒæ¬¡æ‹Ÿåˆã€‚ç¬¬äºŒä¸ªæ¨¡å‹åˆ©ç”¨Tolman VIIæ¨¡å‹å¹¶é‡‡ç”¨åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼è¿›è¡Œè°±å±•å¼€ï¼Œå…¶æ”¶æ•›é€Ÿåº¦æ¯”ä¹‹å‰çš„æ³°å‹’å±•å¼€æ›´å¿«ã€‚ç ”ç©¶å‘ç°ï¼Œç¬¬ä¸€ä¸ªæ¨¡å‹åœ¨æè¿°ç°å®çš„ä¸­å­æ˜Ÿçš„Love-Cå…³ç³»æ—¶æ¯”ç¬¬äºŒä¸ªæ¨¡å‹æ›´å‡†ç¡®ï¼Œå¹¶ä¸”åœ¨æè¿°å¹³å‡æ•°å€¼Love-Cå…³ç³»æ–¹é¢ï¼Œå®ƒæä¾›äº†æ­¤å¤„ç ”ç©¶çš„æ‰€æœ‰å…¶ä»–è§£æå…³ç³»ä¸­æœ€ä½³çš„è¡¨è¾¾å½¢å¼ã€‚è¿™äº›æ–°æ¨¡å‹ä¸ä»…åœ¨å®è·µä¸­å…·æœ‰å®ç”¨æ€§ï¼Œè€Œä¸”è¿˜å±•ç¤ºäº†ä¸­å­æ˜Ÿè§£æå»ºæ¨¡çš„åŠ›é‡å’Œé‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸­å­æ˜Ÿè§‚æµ‹æ•°æ®çš„ç²¾ç¡®æµ‹é‡å¯¹äºç†è§£æ ¸ç‰©ç†å’Œè¶…æ ¸ç‰©è´¨çš„ç‰©æ€æ–¹ç¨‹è‡³å…³é‡è¦ã€‚</li>
<li>NICERå®éªŒé€šè¿‡è¿½è¸ªæ—‹è½¬ä¸­å­æ˜Ÿä¸Šçš„Xå°„çº¿çƒ­ç‚¹ï¼Œèƒ½å¤Ÿæ¨æ–­å‡ºå…³äºæ˜Ÿä½“ç´§è‡´æ€§çš„ç²¾ç¡®ä¿¡æ¯ã€‚</li>
<li>å¼•åŠ›æ³¢æºå¸¦æœ‰å…³ä¸­å­æ˜Ÿæ½®æ±å˜å½¢æ€§çš„ä¿¡æ¯ï¼Œè¯¥ä¿¡æ¯å·²å¾—åˆ°LIGO&#x2F;Virgo&#x2F;KAGRAåˆä½œçš„æµ‹é‡ã€‚</li>
<li>ä¸­å­æ˜Ÿçš„æ½®æ±çˆ±ä¸Cå…³ç³»ï¼ˆLove-C relationï¼‰åœ¨ç‰©æ€æ–¹ç¨‹ä¸Šå…·æœ‰é€šç”¨æ€§ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸¤ä¸ªæ–°çš„è§£ææ¨¡å‹æ¥æè¿°Love-Cå…³ç³»ï¼Œåˆ†åˆ«æ˜¯åŸºäºç‰›é¡¿å¤šè¾¹å½¢å’ŒTolman VIIæ¨¡å‹çš„æ‰©å±•ã€‚</li>
<li>ç¬¬ä¸€ä¸ªæ¨¡å‹åœ¨æè¿°ç°å®çš„ä¸­å­æ˜Ÿçš„Love-Cå…³ç³»æ—¶è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5605c30048dbba7b001a02e3bd14e780.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d3f075edd4ca5397b576a1e6ef5ef73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7400eb56c09e5cd55ba81921e9d3677b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8baa6906a1b6db98561e8daa289e05f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dc6e19f8156da107b8b28e9325ce55f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Perspectives-Comparison-of-Deep-Learning-Segmentation-Models-on-Biophysical-and-Biomedical-Data"><a href="#Perspectives-Comparison-of-Deep-Learning-Segmentation-Models-on-Biophysical-and-Biomedical-Data" class="headerlink" title="Perspectives: Comparison of Deep Learning Segmentation Models on   Biophysical and Biomedical Data"></a>Perspectives: Comparison of Deep Learning Segmentation Models on   Biophysical and Biomedical Data</h2><p><strong>Authors:J Shepard Bryan IV, Pedro Pessoa, Meyam Tavakoli, Steve Presse</strong></p>
<p>Deep learning based approaches are now widely used across biophysics to help automate a variety of tasks including image segmentation, feature selection, and deconvolution. However, the presence of multiple competing deep learning architectures, each with its own unique advantages and disadvantages, makes it challenging to select an architecture best suited for a specific application. As such, we present a comprehensive comparison of common models. Here, we focus on the task of segmentation assuming the typically small training dataset sizes available from biophysics experiments and compare the following four commonly used architectures: convolutional neural networks, U-Nets, vision transformers, and vision state space models. In doing so, we establish criteria for determining optimal conditions under which each model excels, thereby offering practical guidelines for researchers and practitioners in the field. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨ç”Ÿç‰©ç‰©ç†å­¦ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œç”¨äºè‡ªåŠ¨åŒ–å®ŒæˆåŒ…æ‹¬å›¾åƒåˆ†å‰²ã€ç‰¹å¾é€‰æ‹©å’Œå»å·ç§¯ç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¤šç§ç«äº‰çš„æ·±åº¦å­¦ä¹ æ¶æ„çš„å­˜åœ¨ï¼Œæ¯ç§æ¶æ„éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œä½¿å¾—å¾ˆéš¾é€‰æ‹©å‡ºæœ€é€‚åˆç‰¹å®šåº”ç”¨çš„æ¶æ„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹å¸¸è§çš„æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„æ¯”è¾ƒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸“æ³¨äºåˆ†å‰²ä»»åŠ¡ï¼Œè€ƒè™‘åˆ°ç”Ÿç‰©ç‰©ç†å®éªŒé€šå¸¸å¯ç”¨çš„å°å‹è®­ç»ƒæ•°æ®é›†çš„å¤§å°ï¼Œå¹¶æ¯”è¾ƒäº†ä»¥ä¸‹å››ç§å¸¸ç”¨çš„æ¶æ„ï¼šå·ç§¯ç¥ç»ç½‘ç»œã€U-Netã€è§†è§‰å˜å‹å™¨å’Œè§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬ç¡®å®šäº†æ¯ä¸ªæ¨¡å‹åœ¨ä½•ç§æ¡ä»¶ä¸‹è¡¨ç°æœ€ä½³çš„æ ‡å‡†ï¼Œä»è€Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†å®ç”¨çš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.07786v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ åœ¨ç”Ÿç‰©ç‰©ç†å­¦ä¸­å¹¿æ³›åº”ç”¨äºå›¾åƒåˆ†å‰²ã€ç‰¹å¾é€‰æ‹©å’Œè§£å·ç§¯ç­‰ä»»åŠ¡ã€‚ç”±äºå­˜åœ¨å¤šç§ç«äº‰çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œé€‰æ‹©é€‚åˆç‰¹å®šåº”ç”¨çš„æ¶æ„æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡å…¨é¢æ¯”è¾ƒäº†å¸¸è§çš„æ¨¡å‹ï¼Œé‡ç‚¹å…³æ³¨äº†å‡è®¾ç”Ÿç‰©ç‰©ç†å®éªŒå¯ç”¨è®­ç»ƒæ•°æ®é›†è¾ƒå°çš„åˆ†å‰²ä»»åŠ¡ï¼Œå¹¶æ¯”è¾ƒäº†å·ç§¯ç¥ç»ç½‘ç»œã€U-Netsã€è§†è§‰å˜å‹å™¨å’Œè§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å‹ç­‰å››ç§å¸¸ç”¨æ¶æ„ã€‚é€šè¿‡ç¡®å®šæ¯ä¸ªæ¨¡å‹åœ¨ä½•ç§æ¡ä»¶ä¸‹è¡¨ç°æœ€ä½³ï¼Œä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†å®ç”¨æŒ‡å—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨ç”Ÿç‰©ç‰©ç†å­¦ä¸­æœ‰å¹¿æ³›çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†å‰²ã€ç‰¹å¾é€‰æ‹©å’Œè§£å·ç§¯ç­‰ä»»åŠ¡ã€‚</li>
<li>å­˜åœ¨å¤šç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œæ¯ç§æ¶æ„éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚</li>
<li>é€‰æ‹©é€‚åˆç‰¹å®šåº”ç”¨çš„æœ€ä½³æ¶æ„æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æ¯”è¾ƒäº†å››ç§å¸¸ç”¨æ¶æ„ï¼šå·ç§¯ç¥ç»ç½‘ç»œã€U-Netsã€è§†è§‰å˜å‹å™¨å’Œè§†è§‰çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚</li>
<li>åœ¨å°è®­ç»ƒæ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œä¸åŒæ¶æ„çš„è¡¨ç°ä¼šæœ‰æ‰€ä¸åŒã€‚</li>
<li>é€šè¿‡å®éªŒç¡®å®šäº†æ¯ä¸ªæ¨¡å‹åœ¨ä½•ç§æ¡ä»¶ä¸‹è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.07786">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dca346b4e6b6aef450b3d4d3cce783c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a2b65b4ddff47cc5c13fc2d628f7ca3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8156678443bd28399624eca9832504de.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Medical-Phrase-Grounding-with-Off-the-shelf-Diffusion-Models"><a href="#Zero-Shot-Medical-Phrase-Grounding-with-Off-the-shelf-Diffusion-Models" class="headerlink" title="Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models"></a>Zero-Shot Medical Phrase Grounding with Off-the-shelf Diffusion Models</h2><p><strong>Authors:Konstantinos Vilouras, Pedro Sanchez, Alison Q. Oâ€™Neil, Sotirios A. Tsaftaris</strong></p>
<p>Localizing the exact pathological regions in a given medical scan is an important imaging problem that traditionally requires a large amount of bounding box ground truth annotations to be accurately solved. However, there exist alternative, potentially weaker, forms of supervision, such as accompanying free-text reports, which are readily available. The task of performing localization with textual guidance is commonly referred to as phrase grounding. In this work, we use a publicly available Foundation Model, namely the Latent Diffusion Model, to perform this challenging task. This choice is supported by the fact that the Latent Diffusion Model, despite being generative in nature, contains cross-attention mechanisms that implicitly align visual and textual features, thus leading to intermediate representations that are suitable for the task at hand. In addition, we aim to perform this task in a zero-shot manner, i.e., without any training on the target task, meaning that the modelâ€™s weights remain frozen. To this end, we devise strategies to select features and also refine them via post-processing without extra learnable parameters. We compare our proposed method with state-of-the-art approaches which explicitly enforce image-text alignment in a joint embedding space via contrastive learning. Results on a popular chest X-ray benchmark indicate that our method is competitive with SOTA on different types of pathology, and even outperforms them on average in terms of two metrics (mean IoU and AUC-ROC). Source code will be released upon acceptance at <a target="_blank" rel="noopener" href="https://github.com/vios-s">https://github.com/vios-s</a>. </p>
<blockquote>
<p>åœ¨ç»™å®šåŒ»å­¦æ‰«æä¸­å®šä½ç²¾ç¡®ç—…ç†åŒºåŸŸæ˜¯ä¸€ä¸ªé‡è¦çš„æˆåƒé—®é¢˜ï¼Œä¼ ç»Ÿä¸Šéœ€è¦å¤§é‡çš„è¾¹ç•Œæ¡†çœŸå®æ ‡æ³¨æ¥å‡†ç¡®è§£å†³ã€‚ç„¶è€Œï¼Œå­˜åœ¨æ›¿ä»£çš„ã€æ½œåœ¨çš„è¾ƒå¼±å½¢å¼çš„ç›‘ç£ï¼Œå¦‚ä¼´éšçš„è‡ªç”±æ–‡æœ¬æŠ¥å‘Šï¼Œè¿™äº›æŠ¥å‘Šå¾ˆå®¹æ˜“è·å¾—ã€‚è¿›è¡Œå¸¦æœ‰æ–‡æœ¬æŒ‡å¯¼çš„å®šä½ä»»åŠ¡é€šå¸¸è¢«ç§°ä¸ºçŸ­è¯­å®šä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„åŸºç¡€æ¨¡å‹ï¼Œå³æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelï¼‰ï¼Œæ¥å®Œæˆè¿™é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æˆ‘ä»¬é€‰æ‹©è¯¥æ¨¡å‹æ˜¯å› ä¸ºå°½ç®¡å®ƒæ˜¯ç”Ÿæˆæ€§çš„ï¼Œä½†å®ƒåŒ…å«äº¤å‰æ³¨æ„æœºåˆ¶ï¼Œå¯ä»¥éšå¼åœ°å¯¹é½è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼Œä»è€Œäº§ç”Ÿé€‚åˆå½“å‰ä»»åŠ¡çš„ä¸­é—´è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ—¨åœ¨ä»¥é›¶æ ·æœ¬çš„æ–¹å¼è¿›è¡Œè¿™é¡¹ä»»åŠ¡ï¼Œå³ä¸å¯¹ç›®æ ‡ä»»åŠ¡è¿›è¡Œä»»ä½•è®­ç»ƒï¼Œè¿™æ„å‘³ç€æ¨¡å‹çš„æƒé‡ä¿æŒä¸å˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ¶å®šäº†é€‰æ‹©ç‰¹å¾çš„ç­–ç•¥ï¼Œå¹¶é€šè¿‡åå¤„ç†å¯¹ç‰¹å¾è¿›è¡Œæç‚¼ï¼Œæ— éœ€é¢å¤–çš„å¯å­¦ä¹ å‚æ•°ã€‚æˆ‘ä»¬å°†æ‰€æå‡ºçš„æ–¹æ³•ä¸æœ€æ–°æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒï¼Œæœ€æ–°æŠ€æœ¯é€šè¿‡å¯¹æ¯”å­¦ä¹ åœ¨è”åˆåµŒå…¥ç©ºé—´ä¸­æ˜ç¡®å¼ºåˆ¶æ‰§è¡Œå›¾åƒæ–‡æœ¬å¯¹é½ã€‚åœ¨æµè¡Œçš„èƒ¸éƒ¨Xå°„çº¿åŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒç±»å‹ç—…ç†ä¸Šçš„è¡¨ç°ä¸æœ€æ–°æŠ€æœ¯ç›¸å½“ï¼Œç”šè‡³åœ¨ä¸¤ä¸ªæŒ‡æ ‡ï¼ˆå¹³å‡IoUå’ŒAUC-ROCï¼‰ä¸Šçš„å¹³å‡è¡¨ç°æœ‰æ‰€è¶…è¶Šã€‚æºä»£ç å°†åœ¨è¢«æ¥å—åå‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/vios-s%E3%80%82">https://github.com/vios-sã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.12920v4">PDF</a> 10 pages, 3 figures, IEEE J-BHI Special Issue on Foundation Models in   Medical Imaging</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºåŒ»å­¦æ‰«æä¸­å‡†ç¡®çš„ç—…ç†æ€§åŒºåŸŸå®šä½æ˜¯ä¸€é¡¹é‡è¦çš„åŒ»å­¦å›¾åƒé—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤§é‡çš„è¾¹ç•Œæ¡†æ ‡æ³¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œå­˜åœ¨æ½œåœ¨è¾ƒå¼±çš„æ›¿ä»£æ€§ç›‘ç£æ–¹å¼ï¼Œå¦‚ä¼´éšçš„è‡ªç”±æ–‡æœ¬æŠ¥å‘Šã€‚å€ŸåŠ©æ–‡æœ¬æŒ‡å¯¼è¿›è¡Œå®šä½çš„ä»»åŠ¡è¢«ç§°ä¸ºçŸ­è¯­å®šä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¬å¼€å¯ç”¨çš„åŸºç¡€æ¨¡å‹â€”â€”æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜æ€§é—®é¢˜ã€‚æˆ‘ä»¬é€‰æ‹©è¯¥æ¨¡å‹æ˜¯å› ä¸ºå…¶ç”Ÿæˆæ€§è´¨ä¸­åŒ…å«è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥éšå«åœ°å¯¹é½è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆé€‚åˆå½“å‰ä»»åŠ¡çš„ä¸­é—´è¡¨ç¤ºå½¢å¼ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä»¥é›¶æ ·æœ¬çš„æ–¹å¼è¿›è¡Œæ­¤ä»»åŠ¡ï¼Œå³æ— éœ€å¯¹ç›®æ ‡ä»»åŠ¡è¿›è¡Œä»»ä½•è®­ç»ƒï¼Œè¿™æ„å‘³ç€æ¨¡å‹çš„æƒé‡ä¿æŒå†»ç»“çŠ¶æ€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ¶å®šäº†é€‰æ‹©ç‰¹å¾å¹¶å¯¹å…¶è¿›è¡Œåå¤„ç†çš„ç­–ç•¥ï¼Œè€Œæ— éœ€é¢å¤–çš„å¯å­¦ä¹ å‚æ•°ã€‚æˆ‘ä»¬å°†æ‰€æå‡ºçš„æ–¹æ³•ä¸æœ€å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œåè€…é€šè¿‡å¯¹æ¯”å­¦ä¹ åœ¨è”åˆåµŒå…¥ç©ºé—´ä¸­æ˜ç¡®æ‰§è¡Œå›¾åƒæ–‡æœ¬å¯¹é½ã€‚åœ¨æµè¡Œçš„èƒ¸éƒ¨Xå°„çº¿åŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§ç—…ç†å­¦ä¸Šçš„è¡¨ç°ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œå¹¶ä¸”åœ¨ä¸¤ä¸ªæŒ‡æ ‡ï¼ˆå¹³å‡IoUå’ŒAUC-ROCï¼‰ä¸Šçš„å¹³å‡è¡¨ç°æ›´èƒœä¸€ç­¹ã€‚ä»£ç å°†åœ¨è¢«æ¥å—åå‘å¸ƒäº <a target="_blank" rel="noopener" href="https://github.com/vios-s%E3%80%82">https://github.com/vios-sã€‚</a> </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>å®šä½åŒ»å­¦æ‰«æä¸­çš„ç—…ç†æ€§åŒºåŸŸæ˜¯åŒ»å­¦å›¾åƒé—®é¢˜ä¸­çš„æ ¸å¿ƒéš¾ç‚¹ã€‚</li>
<li>ä¼ ç»Ÿè§£å†³æ–¹å¼éœ€è¦å¤§æ‰¹é‡è¾¹ç•Œæ¡†æ ‡æ³¨æ•°æ®ã€‚</li>
<li>å­˜åœ¨åˆ©ç”¨è‡ªç”±æ–‡æœ¬æŠ¥å‘Šç­‰æ›¿ä»£ç›‘ç£æ–¹å¼çš„å¯èƒ½æ€§ã€‚</li>
<li>é‡‡ç”¨çŸ­è¯­å®šä½ä»»åŠ¡å€ŸåŠ©æ–‡æœ¬æŒ‡å¯¼è¿›è¡Œå®šä½ã€‚</li>
<li>é€‰ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œåˆ©ç”¨å…¶ç”Ÿæˆæ€§è´¨å’Œè·¨æ³¨æ„åŠ›æœºåˆ¶è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨é›¶æ ·æœ¬æ–¹å¼è¿›è¡Œä»»åŠ¡æ‰§è¡Œï¼Œæ¨¡å‹æƒé‡ä¿æŒå†»ç»“çŠ¶æ€ã€‚</li>
<li>é€šè¿‡åå¤„ç†ç­–ç•¥é€‰æ‹©å’Œç²¾ç‚¼ç‰¹å¾ï¼Œæ— éœ€é¢å¤–å¯å­¦ä¹ å‚æ•°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.12920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-52a118dfaa5bdf689cf583ea4b38a6cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67b0e20024063b7da6a0486c12e12d59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96d5591974fd0a745f3e1b0631a054d7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Quantifying-uncertainty-in-lung-cancer-segmentation-with-foundation-models-applied-to-mixed-domain-datasets"><a href="#Quantifying-uncertainty-in-lung-cancer-segmentation-with-foundation-models-applied-to-mixed-domain-datasets" class="headerlink" title="Quantifying uncertainty in lung cancer segmentation with foundation   models applied to mixed-domain datasets"></a>Quantifying uncertainty in lung cancer segmentation with foundation   models applied to mixed-domain datasets</h2><p><strong>Authors:Aneesh Rangnekar, Nishant Nadkarni, Jue Jiang, Harini Veeraraghavan</strong></p>
<p>Medical image foundation models have shown the ability to segment organs and tumors with minimal fine-tuning. These models are typically evaluated on task-specific in-distribution (ID) datasets. However, reliable performance on ID datasets does not guarantee robust generalization on out-of-distribution (OOD) datasets. Importantly, once deployed for clinical use, it is impractical to have &#96;ground truthâ€™ delineations to assess ongoing performance drifts, especially when images fall into the OOD category due to different imaging protocols. Hence, we introduced a comprehensive set of computationally fast metrics to evaluate the performance of multiple foundation models (Swin UNETR, SimMIM, iBOT, SMIT) trained with self-supervised learning (SSL). All models were fine-tuned on identical datasets for lung tumor segmentation from computed tomography (CT) scans. The evaluation was performed on two public lung cancer datasets (LRAD: n &#x3D; 140, 5Rater: n &#x3D; 21) with different image acquisitions and tumor stages compared to training data (n &#x3D; 317 public resource with stage III-IV lung cancers) and a public non-cancer dataset containing volumetric CT scans of patients with pulmonary embolism (n &#x3D; 120). All models produced similarly accurate tumor segmentation on the lung cancer testing datasets. SMIT produced the highest F1-score (LRAD: 0.60, 5Rater: 0.64) and lowest entropy (LRAD: 0.06, 5Rater: 0.12), indicating higher tumor detection rate and confident segmentations. In the OOD dataset, SMIT misdetected the least number of tumors, marked by a median volume occupancy of 5.67 cc compared to the best method SimMIM of 9.97 cc. Our analysis shows that additional metrics such as entropy and volume occupancy may help better understand model performance on mixed domain datasets. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåŸºç¡€æ¨¡å‹åœ¨è½»å¾®å¾®è°ƒçš„æƒ…å†µä¸‹è¡¨ç°å‡ºäº†å¯¹å™¨å®˜å’Œè‚¿ç˜¤çš„åˆ†å‰²èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹é€šå¸¸åœ¨ç‰¹å®šä»»åŠ¡çš„å†…éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ç„¶è€Œï¼Œåœ¨å†…éƒ¨æ•°æ®é›†ä¸Šçš„å¯é æ€§èƒ½å¹¶ä¸èƒ½ä¿è¯åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šçš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚é‡è¦çš„æ˜¯ï¼Œä¸€æ—¦éƒ¨ç½²ç”¨äºä¸´åºŠä½¿ç”¨ï¼Œå¾ˆéš¾è·å¾—â€œçœŸå®å€¼â€çš„è½®å»“æ¥è¯„ä¼°æŒç»­çš„æ€§èƒ½æ¼‚ç§»ï¼Œå°¤å…¶æ˜¯å½“å›¾åƒå› ä¸åŒçš„æˆåƒåè®®è€Œè½å…¥å¼‚å¸¸å€¼ç±»åˆ«æ—¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€å¥—è®¡ç®—å¿«é€Ÿçš„æŒ‡æ ‡æ¥è¯„ä¼°ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒçš„å¤šåŸºç¡€æ¨¡å‹ï¼ˆSwin UNETRã€SimMIMã€iBOTã€SMITï¼‰çš„æ€§èƒ½ã€‚æ‰€æœ‰æ¨¡å‹éƒ½åœ¨ç›¸åŒçš„è‚ºéƒ¨è‚¿ç˜¤åˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¿™äº›æ•°æ®é›†æ¥è‡ªè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ã€‚è¯„ä¼°æ˜¯åœ¨ä¸¤ä¸ªå…¬å…±è‚ºç™Œæ•°æ®é›†ï¼ˆLRADï¼šn&#x3D;140ï¼Œ5Raterï¼šn&#x3D;21ï¼‰ä¸Šè¿›è¡Œçš„ï¼Œè¿™äº›æ•°æ®é›†ä¸è®­ç»ƒæ•°æ®ï¼ˆn&#x3D;317ä¸ªå…¬å…±èµ„æºï¼ŒåŒ…å«III-IVæœŸè‚ºç™Œï¼‰ç›¸æ¯”ï¼Œå›¾åƒé‡‡é›†å’Œè‚¿ç˜¤é˜¶æ®µæœ‰æ‰€ä¸åŒï¼Œè¿˜æœ‰ä¸€ä¸ªåŒ…å«è‚ºæ “å¡æ‚£è€…ä½“ç§¯CTæ‰«æçš„å…¬å…±éç™Œç—‡æ•°æ®é›†ï¼ˆn&#x3D;120ï¼‰ã€‚æ‰€æœ‰æ¨¡å‹åœ¨è‚ºç™Œæµ‹è¯•æ•°æ®é›†ä¸Šçš„è‚¿ç˜¤åˆ†å‰²å‡†ç¡®åº¦ç›¸ä¼¼ã€‚SMITäº§ç”Ÿäº†æœ€é«˜çš„F1åˆ†æ•°ï¼ˆLRADï¼š0.60ï¼Œ5Raterï¼š0.64ï¼‰å’Œæœ€ä½çš„ç†µå€¼ï¼ˆLRADï¼š0.06ï¼Œ5Raterï¼š0.12ï¼‰ï¼Œè¡¨æ˜å…¶å…·æœ‰è¾ƒé«˜çš„è‚¿ç˜¤æ£€æµ‹ç‡å’Œå¯é çš„åˆ†å‰²æ•ˆæœã€‚åœ¨å¼‚å¸¸å€¼æ•°æ®é›†ä¸­ï¼ŒSMITè¯¯æ£€çš„è‚¿ç˜¤æ•°é‡æœ€å°‘ï¼Œä»¥ä½“ç§¯å æœ‰ç‡ä¸­ä½æ•°5.67ccè¡¨ç°æœ€ä½³ï¼Œç›¸æ¯”ä¹‹ä¸‹SimMIMçš„æœ€ä½³æ–¹æ³•ä¸º9.97ccã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œé¢å¤–çš„æŒ‡æ ‡å¦‚ç†µå’Œä½“ç§¯å æœ‰ç‡å¯èƒ½æœ‰åŠ©äºæ›´å¥½åœ°äº†è§£æ··åˆåŸŸæ•°æ®é›†ä¸Šçš„æ¨¡å‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.13113v3">PDF</a> Accepted at SPIE Medical Imaging 2025</p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒåŸºç¡€æ¨¡å‹åœ¨è½»å¾®ç²¾ç»†è°ƒæ•´åèƒ½å¤Ÿå®ç°å¯¹å™¨å®˜å’Œè‚¿ç˜¤çš„åˆ†å‰²ã€‚è¿™äº›æ¨¡å‹é€šå¸¸åœ¨ç‰¹å®šä»»åŠ¡å†…éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œä½†å…¶åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›å°šæœªå¾—åˆ°ä¿è¯ã€‚åœ¨å®é™…ä¸´åºŠåº”ç”¨ä¸­ï¼Œéš¾ä»¥è·å¾—çœŸå®åˆ†å‰²æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒå±äºå¤–éƒ¨æ•°æ®é›†æ—¶ã€‚ç ”ç©¶å¼•å…¥äº†å¿«é€Ÿè®¡ç®—æŒ‡æ ‡é›†æ¥è¯„ä¼°é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒçš„åŸºç¡€æ¨¡å‹æ€§èƒ½ã€‚å¯¹æ‰€æœ‰æ¨¡å‹åœ¨ç›¸åŒçš„è‚ºéƒ¨è‚¿ç˜¤åˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨ä¸¤ä¸ªä¸åŒçš„å…¬å…±è‚ºç™Œæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹åœ¨è‚ºç™Œæµ‹è¯•æ•°æ®é›†ä¸Šçš„è‚¿ç˜¤åˆ†å‰²å‡†ç¡®æ€§ç›¸ä¼¼ã€‚SMITæ¨¡å‹åœ¨F1å¾—åˆ†å’Œç†µå€¼ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¡¨æ˜å…¶å…·æœ‰è¾ƒé«˜çš„è‚¿ç˜¤æ£€æµ‹ç‡å’Œç½®ä¿¡åº¦åˆ†å‰²ã€‚åœ¨å¤–éƒ¨æ•°æ®é›†ä¸­ï¼ŒSMITçš„è¯¯æ£€è‚¿ç˜¤æ•°é‡æœ€å°‘ã€‚åˆ†æè¡¨æ˜ï¼Œé¢å¤–çš„æŒ‡æ ‡å¦‚ç†µå’Œä½“ç§¯å æœ‰ç‡æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£æ¨¡å‹åœ¨æ··åˆåŸŸæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåŸºç¡€æ¨¡å‹å…·å¤‡åœ¨è½»å¾®è°ƒæ•´ååˆ†å‰²å™¨å®˜å’Œè‚¿ç˜¤çš„èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹é€šå¸¸åœ¨ç‰¹å®šä»»åŠ¡å†…éƒ¨æ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œä½†æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>ä¸´åºŠåº”ç”¨ä¸­è¯„ä¼°æ¨¡å‹æ€§èƒ½é¢ä¸´å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹å¤–éƒ¨æ•°æ®é›†æ—¶ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç³»åˆ—å¿«é€Ÿè®¡ç®—æŒ‡æ ‡æ¥è¯„ä¼°è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒçš„åŸºç¡€æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ‰€æœ‰æ¨¡å‹åœ¨è‚ºç™Œæµ‹è¯•æ•°æ®é›†ä¸Šçš„è‚¿ç˜¤åˆ†å‰²å‡†ç¡®æ€§ç›¸ä¼¼ã€‚</li>
<li>SMITæ¨¡å‹åœ¨F1å¾—åˆ†å’Œç†µå€¼ä¸Šè¡¨ç°æœ€ä½³ï¼Œå…·æœ‰é«˜çš„è‚¿ç˜¤æ£€æµ‹ç‡å’Œç½®ä¿¡åº¦åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.13113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f07511d7c1fae6e4344165a78ff814c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f92cb97d67a65dcd3bc4e85f4e111859.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60cab97b127eb365d103c08ed2c507bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-254bbd883ffed063966db8a882b6ad3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfcb68f02bf66ea8a6dec2569742c240.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-01/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7f4317733b7fb50b3b253d0ce7a8d091.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-01  The IACOB project XIV. New clues on the location of the TAMS in the   massive star domain
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-01/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-96d5591974fd0a745f3e1b0631a054d7.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-01  DiffusionRenderer Neural Inverse and Forward Rendering with Video   Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18863.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
