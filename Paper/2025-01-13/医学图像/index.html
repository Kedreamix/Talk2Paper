<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-14  The ultraviolet luminosity function of star-forming galaxies between   redshifts of 0.4 and 0.6">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-fbe9ac1ef0f46cbfd28ac30122713098.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    73 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-14-æ›´æ–°"><a href="#2025-01-14-æ›´æ–°" class="headerlink" title="2025-01-14 æ›´æ–°"></a>2025-01-14 æ›´æ–°</h1><h2 id="The-ultraviolet-luminosity-function-of-star-forming-galaxies-between-redshifts-of-0-4-and-0-6"><a href="#The-ultraviolet-luminosity-function-of-star-forming-galaxies-between-redshifts-of-0-4-and-0-6" class="headerlink" title="The ultraviolet luminosity function of star-forming galaxies between   redshifts of 0.4 and 0.6"></a>The ultraviolet luminosity function of star-forming galaxies between   redshifts of 0.4 and 0.6</h2><p><strong>Authors:M. J. Page, T. Dwelly, I. McHardy, N. Seymour, K. O. Mason, M. Sharma, J. A. Kennea, T. P. Sasseen, A. A. Breeveld, A. E. Matthews</strong></p>
<p>We combine ultraviolet imaging of the 13H survey field, taken with the XMM-Newton Optical Monitor telescope (XMM-OM) and the Neil Gehrels Swift Observatory Ultraviolet and Optical Telescope (UVOT) in the UVM2 band, to measure rest-frame ultraviolet 1500A luminosity functions of star-forming galaxies with redshifts between 0.4 and 0.6. In total the UVM2 imaging covers a sky area of 641 square arcmin, and we detect 273 galaxies in the UVM2 image with 0.4&lt;z&lt;0.6. The luminosity function is fit by a Schechter function with best-fit values for the faint end slope alpha &#x3D; -1.8 +0.4 -0.3 and characteristic absolute magnitude M* &#x3D; -19.1 +0.3 -0.4. In common with XMM-OM based studies at higher redshifts, our best-fitting value for M* is fainter than previous measurements. We argue that the purging of active galactic nuclei from the sample, facilitated by the co-spatial X-ray survey carried out with XMM-Newton is important for the determination of M*. At the brightest absolute magnitudes (M1500&lt;-18.5) the average UV colour of our galaxies is consistent with that of minimal-extinction local analogues, but the average UV colour is redder for galaxies at fainter absolute magnitudes, suggesting that higher levels of dust attenuation enter the sample at absolute magnitudes somewhat fainter than M*. </p>
<blockquote>
<p>æˆ‘ä»¬å°†ç»“åˆäº†ä½¿ç”¨XMM-Newtonå…‰å­¦ç›‘è§†æœ›è¿œé•œï¼ˆXMM-OMï¼‰å’Œå°¼å°”Â·æ ¼é›·å°”æ–¯è¿…æ¸¸å¤©æ–‡å°ç´«å¤–å…‰å­¦æœ›è¿œé•œï¼ˆUVOTï¼‰åœ¨ç´«å¤–å…‰åŒºå¸¦ä¸‹çš„ç´«å¤–çº¿æˆåƒæŠ€æœ¯ï¼Œå¯¹å¤„äºçº¢ç§»èŒƒå›´åœ¨0.4è‡³0.6ä¹‹é—´çš„æ˜Ÿç³»å½¢æˆæ’æ˜Ÿè¿›è¡Œäº†ç´«å¤–æ³¢æ®µçš„å…‰åº¦å‡½æ•°æµ‹é‡ã€‚æ€»çš„æ¥è¯´ï¼ŒUVM2æˆåƒè¦†ç›–äº†å¤©ç©ºåŒºåŸŸä¸º641å¹³æ–¹è§’åˆ†çš„åŒºåŸŸï¼Œå¹¶åœ¨UVM2å›¾åƒä¸­æ£€æµ‹åˆ°çº¢ç§»åœ¨0.4è‡³0.6ä¹‹é—´çš„æ˜Ÿç³»å…±273ä¸ªã€‚å…‰åº¦å‡½æ•°é€šè¿‡è°¢å…‹ç‰¹å‡½æ•°æ‹Ÿåˆå¾—å‡ºæœ€ä½³å€¼ï¼Œå…¶ä¸­æš—ç«¯æ–œç‡Î±&#x3D;-1.8Â±0.4Â±0.3ï¼Œç‰¹å¾ç»å¯¹æ˜Ÿç­‰M*&#x3D; - 19.1 Â± 0.3 Â± 0.4ã€‚ä¸é«˜çº¢ç§»çš„åŸºäºXMM-OMçš„ç ”ç©¶ç±»ä¼¼ï¼Œæˆ‘ä»¬ç¡®å®šçš„Mæ˜Ÿäº®åº¦ä½äºä»¥å‰çš„æµ‹é‡å€¼ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œç”±äºé€šè¿‡ä¸XMM-ç‰›é¡¿åŒæ—¶è¿›è¡Œçš„Xå°„çº¿è§‚æµ‹æ¸…é™¤æ´»è·ƒä¸­çš„é“¶æ²³ç³»æ ¸å¿ƒæ ·æœ¬çš„å­˜åœ¨æœ‰åˆ©äºMçš„ç¡®å®šã€‚åœ¨æœ€äº®çš„ç»å¯¹æ˜Ÿç­‰ï¼ˆMuvç»å¯¹äº®åº¦é«˜äºçº¦å‡æ³¢é•¿å˜å…‰ï¼Œ-äº®åº¦å¤§çº¦ä¸º8mé•¿ä¸ä»¥ä¸‹çš„UVè¾å°„ï¼‰ï¼Œæˆ‘ä»¬çš„æ˜Ÿç³»å¹³å‡ç´«å¤–é¢œè‰²ä¸æœ¬åœ°ç±»ä¼¼çš„æœ€å°å‡å¼±çš„æ˜Ÿç³»ç›¸ç¬¦ï¼Œä½†å¯¹äºæ›´æš—çš„ç»å¯¹æ˜Ÿç­‰çš„æ˜Ÿç³»æ¥è¯´ï¼Œå¹³å‡ç´«å¤–é¢œè‰²æ›´ä¸ºçº¢äº®ï¼Œè¿™è¡¨æ˜åœ¨é«˜ç»å¯¹æ˜Ÿç­‰çš„æ ·æœ¬ä¸­å­˜åœ¨æ›´é«˜æ°´å¹³çš„å°˜åŸƒè¡°å‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06075v1">PDF</a> Published in MNRAS</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ç»“åˆXMM-Newtonå…‰å­¦ç›‘è§†å™¨æœ›è¿œé•œï¼ˆXMM-OMï¼‰å’Œå°¼å°”Â·ç›–å°”ç‘æ–¯Â·æ–¯å¨å¤«ç‰¹å¤©æ–‡å°ç´«å¤–å…‰å­¦æœ›è¿œé•œï¼ˆUVOTï¼‰çš„ç´«å¤–çº¿æˆåƒæ•°æ®ï¼Œæµ‹é‡äº†çº¢ç§»èŒƒå›´åœ¨0.4è‡³0.6ä¹‹é—´çš„æ’æ˜Ÿå½¢æˆæ˜Ÿç³»åœ¨é™æ­¢å¸§ç´«å¤–æ³¢æ®µä¸‹çš„å…‰åº¦å‡½æ•°ã€‚é€šè¿‡å¯¹è°¢å…‹ç‰¹å‡½æ•°è¿›è¡Œæ‹Ÿåˆï¼Œå¾—åˆ°äº†è¾ƒä¸ºå‡†ç¡®çš„ç»“æœã€‚å‰”é™¤æ´»åŠ¨æ˜Ÿç³»æ ¸åæ ·æœ¬çš„ç¡®é‡è¦ï¼Œä¾¿äºç¡®å®šå…‰åº¦å‡½æ•°çš„ç‰¹å¾ç»å¯¹æ˜Ÿç­‰M<em>ï¼ˆ</em>ï¼‰ã€‚åœ¨è¾ƒäº®çš„ç»å¯¹æ˜Ÿç­‰æ¡ä»¶ä¸‹ï¼Œæ˜Ÿç³»å¹³å‡ç´«å¤–é¢œè‰²ä¸æœ¬åœ°ç±»ä¼¼ç‰©ä¸€è‡´ï¼Œä½†åœ¨è¾ƒæš—çš„ç»å¯¹æ˜Ÿç­‰æ¡ä»¶ä¸‹ï¼Œå¹³å‡ç´«å¤–é¢œè‰²æ›´çº¢ï¼Œæš—ç¤ºç€å°˜åŸƒè¡°å‡çš„å½±å“æ›´å¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»“åˆXMM-Newtonå…‰å­¦ç›‘è§†å™¨æœ›è¿œé•œå’ŒUVOTçš„ç´«å¤–çº¿æˆåƒæ•°æ®æµ‹é‡äº†ç‰¹å®šçº¢ç§»èŒƒå›´å†…çš„æ’æ˜Ÿå½¢æˆæ˜Ÿç³»åœ¨é™æ­¢å¸§ç´«å¤–æ³¢æ®µçš„å…‰åº¦å‡½æ•°ã€‚</li>
<li>é€šè¿‡è°¢å…‹ç‰¹å‡½æ•°æ‹Ÿåˆå…‰åº¦å‡½æ•°ï¼Œå¾—åˆ°æœ€ä½³æ‹Ÿåˆå€¼ä¸­çš„æš—ç«¯æ–œç‡Î±å’Œç‰¹å¾ç»å¯¹æ˜Ÿç­‰M<em>ï¼ˆ</em>ï¼‰ã€‚å…¶ä¸­M<em>ï¼ˆ</em>ï¼‰æ¯”ä¹‹å‰çš„ç ”ç©¶ç»“æœæ›´æš—ã€‚</li>
<li>å‰”é™¤æ´»åŠ¨æ˜Ÿç³»æ ¸å¯¹ç¡®å®šç‰¹å¾ç»å¯¹æ˜Ÿç­‰M<em>ï¼ˆ</em>ï¼‰çš„å½±å“ååˆ†é‡è¦ã€‚è¿™æ˜¯å› ä¸ºè¿™äº›æ ¸å¿ƒä¼šå½±å“è§‚æµ‹åˆ°çš„ç´«å¤–å…‰çš„å¼ºåº¦åˆ†å¸ƒã€‚è¿™ä¸€ç‚¹å¯¹åç»­çš„ç ”ç©¶ä¹Ÿæœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06075">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ea7cb24efa018b1159198638044aec2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b87c273337b2f849f9bc57f266e227b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14a1dcaf29b5152346ea23428d4864a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-395ff71e04dc582dd055bc1cfdc56263.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51d1d7a43bebe4ae8ec2c561a94aefc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d32bc037ae78d9d7c420d8f097ab16e9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Swin-X2S-Reconstructing-3D-Shape-from-2D-Biplanar-X-ray-with-Swin-Transformers"><a href="#Swin-X2S-Reconstructing-3D-Shape-from-2D-Biplanar-X-ray-with-Swin-Transformers" class="headerlink" title="Swin-X2S: Reconstructing 3D Shape from 2D Biplanar X-ray with Swin   Transformers"></a>Swin-X2S: Reconstructing 3D Shape from 2D Biplanar X-ray with Swin   Transformers</h2><p><strong>Authors:Kuan Liu, Zongyuan Ying, Jie Jin, Dongyan Li, Ping Huang, Wenjian Wu, Zhe Chen, Jin Qi, Yong Lu, Lianfu Deng, Bo Chen</strong></p>
<p>The conversion from 2D X-ray to 3D shape holds significant potential for improving diagnostic efficiency and safety. However, existing reconstruction methods often rely on hand-crafted features, manual intervention, and prior knowledge, resulting in unstable shape errors and additional processing costs. In this paper, we introduce Swin-X2S, an end-to-end deep learning method for directly reconstructing 3D segmentation and labeling from 2D biplanar orthogonal X-ray images. Swin-X2S employs an encoder-decoder architecture: the encoder leverages 2D Swin Transformer for X-ray information extraction, while the decoder employs 3D convolution with cross-attention to integrate structural features from orthogonal views. A dimension-expanding module is introduced to bridge the encoder and decoder, ensuring a smooth conversion from 2D pixels to 3D voxels. We evaluate proposed method through extensive qualitative and quantitative experiments across nine publicly available datasets covering four anatomies (femur, hip, spine, and rib), with a total of 54 categories. Significant improvements over previous methods have been observed not only in the segmentation and labeling metrics but also in the clinically relevant parameters that are of primary concern in practical applications, which demonstrates the promise of Swin-X2S to provide an effective option for anatomical shape reconstruction in clinical scenarios. Code implementation is available at: \url{<a target="_blank" rel="noopener" href="https://github.com/liukuan5625/Swin-X2S%7D">https://github.com/liukuan5625/Swin-X2S}</a>. </p>
<blockquote>
<p>ä»2D Xå…‰å›¾åƒè½¬æ¢åˆ°3Då½¢çŠ¶åœ¨æå‡è¯Šæ–­æ•ˆç‡å’Œå®‰å…¨æ€§æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é‡å»ºæ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥ç‰¹å¾ã€äººå·¥å¹²é¢„å’Œå…ˆéªŒçŸ¥è¯†ï¼Œå¯¼è‡´å½¢çŠ¶è¯¯å·®ä¸ç¨³å®šå¹¶å¢åŠ äº†å¤„ç†æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Swin-X2Sï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå¯ç›´æ¥ä»2DåŒå¹³é¢æ­£äº¤Xå…‰å›¾åƒé‡å»º3Dåˆ†å‰²å’Œæ ‡è®°ã€‚Swin-X2Sé‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼šç¼–ç å™¨åˆ©ç”¨2D Swin Transformerè¿›è¡ŒXå…‰ä¿¡æ¯æå–ï¼Œè€Œè§£ç å™¨åˆ™é‡‡ç”¨å…·æœ‰è·¨æ³¨æ„åŠ›çš„3Då·ç§¯ï¼Œä»¥æ•´åˆæ¥è‡ªæ­£äº¤è§†å›¾çš„ç»“æ„ç‰¹å¾ã€‚å¼•å…¥äº†ä¸€ä¸ªç»´åº¦æ‰©å±•æ¨¡å—ï¼Œä»¥è¿æ¥ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œç¡®ä¿ä»2Dåƒç´ åˆ°3Dä½“ç´ çš„å¹³æ»‘è½¬æ¢ã€‚æˆ‘ä»¬åœ¨ä¹ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®šæ€§å’Œå®šé‡å®éªŒï¼Œæ¶µç›–äº†å››ä¸ªéƒ¨ä½ï¼ˆè‚¡éª¨ã€é«‹å…³èŠ‚ã€è„Šæ¤å’Œè‚‹éª¨ï¼‰ï¼Œå…±54ä¸ªç±»åˆ«ï¼Œå¯¹æ‰€æå‡ºçš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚ç›¸è¾ƒäºä»¥å‰çš„æ–¹æ³•ï¼Œä¸ä»…åœ¨åˆ†å‰²å’Œæ ‡è®°æŒ‡æ ‡ä¸Šè§‚å¯Ÿåˆ°æ˜¾è‘—æ”¹è¿›ï¼Œè€Œä¸”åœ¨ä¸´åºŠç›¸å…³å‚æ•°æ–¹é¢ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œè¿™äº›å‚æ•°æ˜¯å®é™…åº”ç”¨ä¸­çš„ä¸»è¦å…³æ³¨ç‚¹ã€‚è¿™è¯æ˜äº†Swin-X2Såœ¨ä¸´åºŠåœºæ™¯ä¸­ä¸ºè§£å‰–å½¢çŠ¶é‡å»ºæä¾›æœ‰æ•ˆé€‰é¡¹çš„æ½œåŠ›ã€‚ä»£ç å®ç°å¯è®¿é—®äºï¼š\url{<a target="_blank" rel="noopener" href="https://github.com/liukuan5625/Swin-X2S%7D%E3%80%82">https://github.com/liukuan5625/Swin-X2S}ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05961v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºæ·±åº¦å­¦ä¹ çš„Swin-X2Sæ–¹æ³•å¯ç›´æ¥ä»äºŒç»´åŒæ­£äº¤Xå°„çº¿å›¾åƒé‡å»ºä¸‰ç»´åˆ†å‰²å’Œæ ‡ç­¾ï¼Œæé«˜äº†è¯Šæ–­æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ‰‹å·¥ç‰¹å¾ã€äººå·¥å¹²é¢„å’Œå…ˆéªŒçŸ¥è¯†çš„ä¾èµ–ï¼Œé™ä½äº†å½¢çŠ¶è¯¯å·®å’Œé¢å¤–å¤„ç†æˆæœ¬ã€‚ä»£ç å®ç°å·²å‘å¸ƒäºGitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä»äºŒç»´Xå°„çº¿è½¬æ¢åˆ°ä¸‰ç»´å½¢æ€å¯¹æå‡è¯Šæ–­æ•ˆç‡å’Œå®‰å…¨æ€§å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>å½“å‰é‡å»ºæ–¹æ³•ä¾èµ–äºæ‰‹å·¥ç‰¹å¾ã€äººå·¥å¹²é¢„å’Œå…ˆéªŒçŸ¥è¯†ï¼Œå­˜åœ¨ä¸ç¨³å®šæ€§å’Œé¢å¤–æˆæœ¬ã€‚</li>
<li>Swin-X2Sæ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œå¯ç›´æ¥ä»äºŒç»´åŒæ­£äº¤Xå°„çº¿å›¾åƒé‡å»ºä¸‰ç»´åˆ†å‰²å’Œæ ‡ç­¾ã€‚</li>
<li>Swin-X2Sä½¿ç”¨ç¼–ç -è§£ç æ¶æ„ï¼Œç¼–ç é˜¶æ®µåˆ©ç”¨äºŒç»´Swin Transformeræå–Xå°„çº¿ä¿¡æ¯ï¼Œè§£ç é˜¶æ®µé‡‡ç”¨ä¸‰ç»´å·ç§¯ä¸äº¤å‰æ³¨æ„åŠ›æœºåˆ¶æ•´åˆæ­£äº¤è§†å›¾çš„ç»“æ„ç‰¹å¾ã€‚</li>
<li>å¼•å…¥ç»´åº¦æ‰©å±•æ¨¡å—ï¼Œç¡®ä¿ä»äºŒç»´åƒç´ åˆ°ä¸‰ç»´ä½“ç´ çš„å¹³æ»‘è½¬æ¢ã€‚</li>
<li>åœ¨æ¶µç›–å››ä¸ªè§£å‰–éƒ¨ä½ï¼ˆè‚¡éª¨ã€é«‹å…³èŠ‚ã€è„Šæ¤å’Œè‚‹éª¨ï¼‰çš„ä¹ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç›¸è¾ƒäºå…ˆå‰æ–¹æ³•æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6e513711194aced710d28d441f5d3a4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce9ac87d56a38f0559973d917310c08c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a703e266cc0cc92b5154263408131e98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-119a720e015e3424429aba58ba2bcd58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c715edb395b06299122fd65d49e5db0d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MRI-Patterns-of-the-Hippocampus-and-Amygdala-for-Predicting-Stages-of-Alzheimerâ€™s-Progression-A-Minimal-Feature-Machine-Learning-Framework"><a href="#MRI-Patterns-of-the-Hippocampus-and-Amygdala-for-Predicting-Stages-of-Alzheimerâ€™s-Progression-A-Minimal-Feature-Machine-Learning-Framework" class="headerlink" title="MRI Patterns of the Hippocampus and Amygdala for Predicting Stages of   Alzheimerâ€™s Progression: A Minimal Feature Machine Learning Framework"></a>MRI Patterns of the Hippocampus and Amygdala for Predicting Stages of   Alzheimerâ€™s Progression: A Minimal Feature Machine Learning Framework</h2><p><strong>Authors:Aswini Kumar Patra, Soraisham Elizabeth Devi, Tejashwini Gajurel</strong></p>
<p>Alzheimerâ€™s disease (AD) progresses through distinct stages, from early mild cognitive impairment (EMCI) to late mild cognitive impairment (LMCI) and eventually to AD. Accurate identification of these stages, especially distinguishing LMCI from EMCI, is crucial for developing pre-dementia treatments but remains challenging due to subtle and overlapping imaging features. This study proposes a minimal-feature machine learning framework that leverages structural MRI data, focusing on the hippocampus and amygdala as regions of interest. The framework addresses the curse of dimensionality through feature selection, utilizes region-specific voxel information, and implements innovative data organization to enhance classification performance by reducing noise. The methodology integrates dimensionality reduction techniques such as PCA and t-SNE with state-of-the-art classifiers, achieving the highest accuracy of 88.46%. This framework demonstrates the potential for efficient and accurate staging of AD progression while providing valuable insights for clinical applications. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰è¿›å±•ç»è¿‡ä¸åŒçš„é˜¶æ®µï¼Œä»æ—©æœŸè½»åº¦è®¤çŸ¥éšœç¢ï¼ˆEMCIï¼‰åˆ°æ™šæœŸè½»åº¦è®¤çŸ¥éšœç¢ï¼ˆLMCIï¼‰ï¼Œæœ€ç»ˆå‘å±•åˆ°ADã€‚å‡†ç¡®è¯†åˆ«è¿™äº›é˜¶æ®µï¼Œç‰¹åˆ«æ˜¯åŒºåˆ†LMCIå’ŒEMCIï¼Œå¯¹äºå¼€å‘ç—´å‘†å‰æ²»ç–—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæˆåƒç‰¹å¾çš„ç»†å¾®å’Œé‡å ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç»“æ„MRIæ•°æ®çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œé‡ç‚¹å…³æ³¨æµ·é©¬ä½“å’Œæä»æ ¸ä½œä¸ºæ„Ÿå…´è¶£åŒºåŸŸã€‚è¯¥æ¡†æ¶é€šè¿‡ç‰¹å¾é€‰æ‹©è§£å†³äº†ç»´æ•°è¯…å’’é—®é¢˜ï¼Œåˆ©ç”¨ç‰¹å®šåŒºåŸŸçš„ä½“ç´ ä¿¡æ¯ï¼Œå¹¶é€šè¿‡åˆ›æ–°çš„æ•°æ®ç»„ç»‡æé«˜åˆ†ç±»æ€§èƒ½ï¼Œå‡å°‘å™ªå£°å¹²æ‰°ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä¸»æˆåˆ†åˆ†æå’Œt-SNEç­‰é™ç»´æŠ€æœ¯ä¸æœ€å…ˆè¿›çš„åˆ†ç±»å™¨ï¼Œå–å¾—äº†æœ€é«˜çš„88.46%å‡†ç¡®ç‡ã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†åœ¨ADè¿›å±•åˆ†æœŸä¸­é«˜æ•ˆå‡†ç¡®çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¸ºä¸´åºŠåº”ç”¨æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05852v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç»“æ„MRIæ•°æ®çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå‡†ç¡®è¯†åˆ«é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„ä¸åŒé˜¶æ®µï¼Œç‰¹åˆ«æ˜¯æ—©æœŸè½»åº¦è®¤çŸ¥éšœç¢ï¼ˆEMCIï¼‰å’Œæ™šæœŸè½»åº¦è®¤çŸ¥éšœç¢ï¼ˆLMCIï¼‰ã€‚è¯¥æ¡†æ¶å…³æ³¨æµ·é©¬ä½“å’Œæä»æ ¸è¿™ä¸¤ä¸ªå…³é”®åŒºåŸŸï¼Œé€šè¿‡ç‰¹å¾é€‰æ‹©è§£å†³ç»´åº¦ç¾éš¾é—®é¢˜ï¼Œåˆ©ç”¨åŒºåŸŸç‰¹å®šä½“ç´ ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨åˆ›æ–°çš„æ•°æ®ç»„ç»‡æ–¹å¼æé«˜åˆ†ç±»æ€§èƒ½ã€‚ç»“åˆPCAå’Œt-SNEç­‰é™ç»´æŠ€æœ¯ä¸æœ€å…ˆè¿›çš„åˆ†ç±»å™¨ï¼Œæœ€é«˜å‡†ç¡®ç‡å¯è¾¾88.46%ï¼Œä¸ºADè¿›å±•çš„æœ‰æ•ˆåˆ†æœŸæä¾›äº†æ½œåŠ›ï¼Œå¹¶ä¸ºä¸´åºŠåº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„è¿›å±•åŒ…æ‹¬ä»æ—©æœŸè½»åº¦è®¤çŸ¥éšœç¢ï¼ˆEMCIï¼‰åˆ°æ™šæœŸè½»åº¦è®¤çŸ¥éšœç¢ï¼ˆLMCIï¼‰å†åˆ°ADçš„å¤šä¸ªé˜¶æ®µã€‚</li>
<li>å‡†ç¡®è¯†åˆ«è¿™äº›é˜¶æ®µå¯¹äºå¼€å‘é¢„ç—´å‘†æ²»ç–—è‡³å…³é‡è¦ï¼Œä½†ç”±äºæˆåƒç‰¹å¾çš„ç»†å¾®å’Œé‡å ï¼Œä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç»“æ„MRIæ•°æ®çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œå…³æ³¨æµ·é©¬ä½“å’Œæä»æ ¸è¿™ä¸¤ä¸ªå…³é”®åŒºåŸŸè¿›è¡Œåˆ†ç±»ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ç‰¹å¾é€‰æ‹©è§£å†³ç»´åº¦ç¾éš¾é—®é¢˜ï¼Œæé«˜åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†é™ç»´æŠ€æœ¯å’Œæœ€å…ˆè¿›çš„åˆ†ç±»å™¨ï¼Œå®ç°æœ€é«˜å‡†ç¡®ç‡88.46%ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºADè¿›å±•çš„æœ‰æ•ˆåˆ†æœŸæä¾›äº†æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0e8ea0851169fd0da0940d9bfe8496f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3adf8e1da41f23796c73d60b165bf2e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94113d9b41369101acad60edffd6b67d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-922e0f1f5a23118bdfbc252cdcf95fb3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="High-order-strong-coupling-expansion-for-X-ray-absorption-on-a-dynamically-screened-impurity"><a href="#High-order-strong-coupling-expansion-for-X-ray-absorption-on-a-dynamically-screened-impurity" class="headerlink" title="High order strong-coupling expansion for X-ray absorption on a   dynamically screened impurity"></a>High order strong-coupling expansion for X-ray absorption on a   dynamically screened impurity</h2><p><strong>Authors:Eva Paprotzki, Martin Eckstein</strong></p>
<p>Time-resolved X-ray absorption can reveal the dynamical screening of the local Coulomb interaction in strongly correlated photo-excited materials. Here, we focus on the theoretical prediction of X-ray absorption in the presence of dynamical screening using the strong coupling expansion, i.e., an expansion around the isolated absorption site in terms of the retarded interaction. The evaluation of higher order diagrams is made numerically feasible by an approach based on the decomposition of the retarded interaction into complex exponentials. With this, we evaluate the strong coupling series to third order on an electron-boson model of Holstein type. We demonstrate that in relevant coupling regimes, even low orders of the strong coupling expansion can give a significant correction over the previously used lowest order approximation. </p>
<blockquote>
<p>æ—¶é—´åˆ†è¾¨çš„Xå°„çº¿å¸æ”¶èƒ½å¤Ÿæ­ç¤ºå¼ºç›¸å…³å…‰æ¿€å‘ææ–™ä¸­å±€éƒ¨åº“ä»‘ç›¸äº’ä½œç”¨çš„åŠ¨åŠ›å­¦ç­›é€‰ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨åœ¨å­˜åœ¨åŠ¨åŠ›å­¦ç­›é€‰çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¼ºè€¦åˆå±•å¼€æ³•å¯¹Xå°„çº¿å¸æ”¶çš„ç†è®ºé¢„æµ‹ã€‚è¿™æ˜¯ä¸€ç§å›´ç»•å­¤ç«‹å¸æ”¶ä½ç‚¹è¿›è¡Œå»¶è¿Ÿç›¸äº’ä½œç”¨å±•å¼€çš„å±•å¼€æ–¹æ³•ã€‚é€šè¿‡å°†å»¶è¿Ÿç›¸äº’ä½œç”¨åˆ†è§£ä¸ºå¤æ•°æŒ‡æ•°çš„æ–¹æ³•ï¼Œä½¿å¾—é«˜é˜¶å›¾çš„è¯„ä¼°åœ¨æ•°å€¼ä¸Šæ˜¯å¯è¡Œçš„ã€‚åˆ©ç”¨æ­¤æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨Holsteinå‹ç”µå­-ç»è‰²å­æ¨¡å‹ä¸Šå¯¹å¼ºè€¦åˆç³»åˆ—è¿›è¡Œäº†ä¸‰é˜¶è¯„ä¼°ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨ç›¸å…³çš„è€¦åˆçŠ¶æ€ä¸‹ï¼Œå³ä½¿æ˜¯å¼ºè€¦åˆå±•å¼€çš„è¾ƒä½é˜¶ä¹Ÿå¯ä»¥å¯¹ä¹‹å‰ä½¿ç”¨çš„æœ€ä½é˜¶è¿‘ä¼¼è¿›è¡Œé‡å¤§ä¿®æ­£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05825v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ç ”ç©¶äº†åŸºäºæ—¶é—´è§£æçš„Xå°„çº¿å¸æ”¶æŠ€æœ¯æ­ç¤ºå¼ºå…³è”å…‰æ¿€å‘ææ–™ä¸­å±€éƒ¨åº“ä»‘ç›¸äº’ä½œç”¨çš„åŠ¨åŠ›å­¦ç­›é€‰æœºåˆ¶ã€‚æ–‡ç« é‡ç‚¹å…³æ³¨äº†å­˜åœ¨åŠ¨åŠ›å­¦ç­›é€‰ä½œç”¨ä¸‹çš„Xå°„çº¿å¸æ”¶çš„ç†è®ºé¢„æµ‹ï¼Œé‡‡ç”¨äº†å¼ºè€¦åˆå±•å¼€æ³•ï¼Œå›´ç»•å­¤ç«‹å¸æ”¶ä½ç‚¹å±•å¼€ï¼Œç”¨å»¶è¿Ÿç›¸äº’ä½œç”¨è¡¨ç¤ºã€‚é€šè¿‡å¯¹é«˜é˜¶å›¾è¿›è¡Œæ•°å€¼åˆ†è§£çš„æ–¹æ³•ï¼Œå®ç°äº†å»¶è¿Ÿç›¸äº’ä½œç”¨çš„åˆ†è§£ä¸ºå¤æ•°æŒ‡æ•°å½¢å¼çš„è®¡ç®—ã€‚é€šè¿‡è¯„ä¼°éœå°”æ–¯å¦ç”µå­-ç»è‰²æ¨¡å‹çš„å¼ºè€¦åˆçº§æ•°æ‰©å±•è‡³ä¸‰é˜¶æ¥æ¼”ç¤ºå…¶åœ¨ç›¸å…³è€¦åˆç¯å¢ƒä¸‹çš„åº”ç”¨ä»·å€¼ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨ä½é˜¶è¿‘ä¼¼çš„åŸºç¡€ä¸Šå¼•å…¥é«˜é˜¶ä¿®æ­£å¯ä»¥æ˜¾è‘—æé«˜é¢„æµ‹ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´è§£æçš„Xå°„çº¿å¸æ”¶æŠ€æœ¯å¯ç”¨äºæ­ç¤ºå¼ºå…³è”å…‰æ¿€å‘ææ–™ä¸­çš„åŠ¨åŠ›å­¦ç­›é€‰æœºåˆ¶ã€‚</li>
<li>é‡‡ç”¨å¼ºè€¦åˆå±•å¼€æ³•é¢„æµ‹å­˜åœ¨åŠ¨åŠ›å­¦ç­›é€‰ä½œç”¨ä¸‹çš„Xå°„çº¿å¸æ”¶ã€‚</li>
<li>é€šè¿‡å»¶è¿Ÿç›¸äº’ä½œç”¨å°†å¤æ‚æ¨¡å‹æ•°å€¼åŒ–å¤„ç†æˆä¸ºå¯èƒ½ã€‚</li>
<li>ä½¿ç”¨åŸºäºå¤æ‚æŒ‡æ•°å½¢å¼çš„è®¡ç®—å®ç°å¯¹é«˜é˜¶å›¾çš„æ•°å€¼åˆ†è§£ã€‚</li>
<li>åœ¨éœå°”æ–¯å¦ç”µå­-ç»è‰²æ¨¡å‹ä¸­è¯„ä¼°å¼ºè€¦åˆç³»åˆ—çš„æ‰©å±•æ•ˆæœè‡³ä¸‰é˜¶ã€‚</li>
<li>é«˜é˜¶ä¿®æ­£å¯æ˜¾è‘—æ”¹è¿›æœ€ä½é˜¶è¿‘ä¼¼ä¸‹çš„é¢„æµ‹ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-38c5c17f3abeee4276f36813ae3a79c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9879c23bc6270147c86e440e60fecc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ed612f71e3558ba23528e92dc706aaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85b8790e495a531b61bb2fcc1f97035e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-265fbc7371691faf6fcd00c73451f9ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a128b9a9920d22c3f9c1387215a08255.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learned-Discrepancy-Reconstruction-and-Benchmark-Dataset-for-Magnetic-Particle-Imaging"><a href="#Learned-Discrepancy-Reconstruction-and-Benchmark-Dataset-for-Magnetic-Particle-Imaging" class="headerlink" title="Learned Discrepancy Reconstruction and Benchmark Dataset for Magnetic   Particle Imaging"></a>Learned Discrepancy Reconstruction and Benchmark Dataset for Magnetic   Particle Imaging</h2><p><strong>Authors:Meira Iske, Hannes Albers, Tobias Knopp, Tobias Kluth</strong></p>
<p>Magnetic Particle Imaging (MPI) is an emerging imaging modality based on the magnetic response of superparamagnetic iron oxide nanoparticles to achieve high-resolution and real-time imaging without harmful radiation. One key challenge in the MPI image reconstruction task arises from its underlying noise model, which does not fulfill the implicit Gaussian assumptions that are made when applying traditional reconstruction approaches. To address this challenge, we introduce the Learned Discrepancy Approach, a novel learning-based reconstruction method for inverse problems that includes a learned discrepancy function. It enhances traditional techniques by incorporating an invertible neural network to explicitly model problem-specific noise distributions. This approach does not rely on implicit Gaussian noise assumptions, making it especially suited to handle the sophisticated noise model in MPI and also applicable to other inverse problems. To further advance MPI reconstruction techniques, we introduce the MPI-MNIST dataset - a large collection of simulated MPI measurements derived from the MNIST dataset of handwritten digits. The dataset includes noise-perturbed measurements generated from state-of-the-art model-based system matrices and measurements of a preclinical MPI scanner device. This provides a realistic and flexible environment for algorithm testing. Validated against the MPI-MNIST dataset, our method demonstrates significant improvements in reconstruction quality in terms of structural similarity when compared to classical reconstruction techniques. </p>
<blockquote>
<p>ç£å…±æŒ¯é¢—ç²’æˆåƒï¼ˆMPIï¼‰æ˜¯ä¸€ç§æ–°å…´çš„æˆåƒæŠ€æœ¯ï¼ŒåŸºäºè¶…é¡ºç£æ€§æ°§åŒ–é“çº³ç±³ç²’å­çš„ç£å“åº”å®ç°é«˜åˆ†è¾¨ç‡å’Œå®æ—¶æˆåƒï¼Œæ— éœ€æœ‰å®³è¾å°„ã€‚MPIå›¾åƒé‡å»ºä»»åŠ¡ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜åœ¨äºå…¶å›ºæœ‰çš„å™ªå£°æ¨¡å‹ï¼Œå®ƒå¹¶ä¸æ»¡è¶³åœ¨åº”ç”¨ä¼ ç»Ÿé‡å»ºæ–¹æ³•æ—¶æ‰€åšçš„éšå«é«˜æ–¯å‡è®¾ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­¦ä¹ å·®å¼‚æ³•ï¼ˆLearned Discrepancy Approachï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹é€†é—®é¢˜çš„æ–°å‹å­¦ä¹ é‡å»ºæ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå­¦ä¹ åˆ°çš„å·®å¼‚å‡½æ•°ã€‚å®ƒé€šè¿‡èå…¥å¯é€†ç¥ç»ç½‘ç»œæ¥æ˜¾å¼å»ºæ¨¡ç‰¹å®šé—®é¢˜çš„å™ªå£°åˆ†å¸ƒï¼Œå¢å¼ºäº†ä¼ ç»ŸæŠ€æœ¯ã€‚è¿™ç§æ–¹æ³•ä¸ä¾èµ–äºéšå«çš„é«˜æ–¯å™ªå£°å‡è®¾ï¼Œä½¿å…¶ç‰¹åˆ«é€‚åˆå¤„ç†MPIä¸­çš„å¤æ‚å™ªå£°æ¨¡å‹ï¼Œä¹Ÿå¯åº”ç”¨äºå…¶ä»–é€†é—®é¢˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ¨è¿›MPIé‡å»ºæŠ€æœ¯ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MPI-MNISTæ•°æ®é›†â€”â€”è¿™æ˜¯ä¸€ä¸ªåŸºäºMNISTæ‰‹å†™æ•°å­—æ•°æ®é›†æ¨¡æ‹Ÿçš„å¤§é‡MPIæµ‹é‡å€¼çš„å¤§å‹é›†åˆã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬ç”±å›½å®¶å…ˆè¿›æŠ€æœ¯æ¨¡å‹çŸ©é˜µç”Ÿæˆå¹¶ç»è¿‡å™ªå£°å¹²æ‰°çš„æµ‹é‡å€¼ä»¥åŠæ¥è‡ªä¸´åºŠå‰MPIæ‰«æè®¾å¤‡çš„æµ‹é‡å€¼ã€‚è¿™ä¸ºç®—æ³•æµ‹è¯•æä¾›äº†ä¸€ä¸ªç°å®ä¸”çµæ´»çš„ç¯å¢ƒã€‚ä¸MPI-MNISTæ•°æ®é›†éªŒè¯ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»“æ„ç›¸ä¼¼æ€§æ–¹é¢é‡å»ºè´¨é‡æ˜¾è‘—æé«˜ï¼Œä¸ä¼ ç»Ÿé‡å»ºæŠ€æœ¯ç›¸æ¯”ä¼˜åŠ¿æ˜æ˜¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05583v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¶…é¡ºç£æ€§æ°§åŒ–é“çº³ç±³ç²’å­çš„ç£æ€§å“åº”ï¼Œç£æ€§ç²’å­æˆåƒï¼ˆMPIï¼‰æ˜¯ä¸€ç§æ–°å…´çš„æ— è¾å°„ã€å®æ—¶ã€é«˜åˆ†è¾¨ç‡æˆåƒæŠ€æœ¯ã€‚MPIå›¾åƒé‡å»ºä»»åŠ¡é¢ä¸´çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜åœ¨äºå…¶å™ªå£°æ¨¡å‹ä¸æ»¡è¶³ä¼ ç»Ÿé‡å»ºæ–¹æ³•éšå«çš„é«˜æ–¯å‡è®¾ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å­¦ä¹ å·®å¼‚æ³•ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹é€†é—®é¢˜çš„æ–°å‹å­¦ä¹ é‡å»ºæ–¹æ³•ï¼ŒåŒ…æ‹¬å­¦ä¹ å·®å¼‚å‡½æ•°ã€‚å®ƒé€šè¿‡å¼•å…¥å¯é€†ç¥ç»ç½‘ç»œæ¥æ˜¾å¼å»ºæ¨¡é—®é¢˜ç‰¹å®šçš„å™ªå£°åˆ†å¸ƒï¼Œå¢å¼ºä¼ ç»ŸæŠ€æœ¯ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºéšå«çš„é«˜æ–¯å™ªå£°å‡è®¾ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤„ç†MPIä¸­çš„å¤æ‚å™ªå£°æ¨¡å‹ï¼Œä¹Ÿå¯åº”ç”¨äºå…¶ä»–é€†é—®é¢˜ã€‚ä¸ºè¿›ä¸€æ­¥æ¨è¿›MPIé‡å»ºæŠ€æœ¯ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†MPI-MNISTæ•°æ®é›†â€”â€”ä»MNISTæ‰‹å†™æ•°å­—æ•°æ®é›†ä¸­æ¨¡æ‹Ÿçš„MPIæµ‹é‡ç»“æœçš„é›†åˆã€‚è¯¥æ•°æ®é›†åŒ…å«ç”±æœ€æ–°æ¨¡å‹ç³»ç»ŸçŸ©é˜µç”Ÿæˆçš„å™ªå£°å¹²æ‰°æµ‹é‡å€¼å’Œé¢„ä¸´åºŠMPIæ‰«æè®¾å¤‡çš„æµ‹é‡å€¼ï¼Œä¸ºç®—æ³•æµ‹è¯•æä¾›äº†ç°å®ä¸”çµæ´»çš„ç¯å¢ƒã€‚éªŒè¯ç»“æœè¡¨æ˜ï¼Œä¸ç»å…¸é‡å»ºæŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ç»“æ„ç›¸ä¼¼æ€§æ–¹é¢é‡å»ºè´¨é‡æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MPIæ˜¯ä¸€ç§æ–°å…´æˆåƒæŠ€æœ¯ï¼ŒåŸºäºè¶…é¡ºç£æ€§æ°§åŒ–é“çº³ç±³ç²’å­çš„ç£æ€§å“åº”ï¼Œå¯å®ç°æ— è¾å°„ã€å®æ—¶ã€é«˜åˆ†è¾¨ç‡æˆåƒã€‚</li>
<li>MPIå›¾åƒé‡å»ºé¢ä¸´çš„å…³é”®æŒ‘æˆ˜åœ¨äºå…¶å™ªå£°æ¨¡å‹ä¸æ»¡è¶³ä¼ ç»Ÿé‡å»ºæ–¹æ³•çš„é«˜æ–¯å‡è®¾ã€‚</li>
<li>å­¦ä¹ å·®å¼‚æ³•æ˜¯ä¸€ç§æ–°å‹çš„é’ˆå¯¹é€†é—®é¢˜çš„å­¦ä¹ é‡å»ºæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å¯é€†ç¥ç»ç½‘ç»œæ˜¾å¼å»ºæ¨¡é—®é¢˜ç‰¹å®šçš„å™ªå£°åˆ†å¸ƒã€‚</li>
<li>å­¦ä¹ å·®å¼‚æ³•ä¸ä¾èµ–éšå«çš„é«˜æ–¯å™ªå£°å‡è®¾ï¼Œé€‚ç”¨äºå¤„ç†MPIä¸­çš„å¤æ‚å™ªå£°æ¨¡å‹å’Œå…¶ä»–é€†é—®é¢˜ã€‚</li>
<li>æ¨å‡ºäº†MPI-MNISTæ•°æ®é›†ï¼Œä¸ºMPIé‡å»ºæŠ€æœ¯çš„ç®—æ³•æµ‹è¯•æä¾›äº†ç°å®ä¸”çµæ´»çš„ç¯å¢ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b1d8eda1e633d41ceb9aaae17fdba046.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Patch-GAN-Transfer-Learning-with-Reconstructive-Models-for-Cloud-Removal"><a href="#Patch-GAN-Transfer-Learning-with-Reconstructive-Models-for-Cloud-Removal" class="headerlink" title="Patch-GAN Transfer Learning with Reconstructive Models for Cloud Removal"></a>Patch-GAN Transfer Learning with Reconstructive Models for Cloud Removal</h2><p><strong>Authors:Wanli Ma, Oktay Karakus, Paul L. Rosin</strong></p>
<p>Cloud removal plays a crucial role in enhancing remote sensing image analysis, yet accurately reconstructing cloud-obscured regions remains a significant challenge. Recent advancements in generative models have made the generation of realistic images increasingly accessible, offering new opportunities for this task. Given the conceptual alignment between image generation and cloud removal tasks, generative models present a promising approach for addressing cloud removal in remote sensing. In this work, we propose a deep transfer learning approach built on a generative adversarial network (GAN) framework to explore the potential of the novel masked autoencoder (MAE) image reconstruction model in cloud removal. Due to the complexity of remote sensing imagery, we further propose using a patch-wise discriminator to determine whether each patch of the image is real or not. The proposed reconstructive transfer learning approach demonstrates significant improvements in cloud removal performance compared to other GAN-based methods. Additionally, whilst direct comparisons with some of the state-of-the-art cloud removal techniques are limited due to unclear details regarding their train&#x2F;test data splits, the proposed model achieves competitive results based on available benchmarks. </p>
<blockquote>
<p>äº‘å»é™¤åœ¨å¢å¼ºé¥æ„Ÿå›¾åƒåˆ†ææ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç„¶è€Œï¼Œå‡†ç¡®åœ°é‡å»ºè¢«äº‘å±‚é®è”½çš„åŒºåŸŸä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„å‘å±•ä½¿å¾—ç”ŸæˆçœŸå®å›¾åƒå˜å¾—è¶Šæ¥è¶Šå®¹æ˜“ï¼Œè¿™ä¸ºè¿™ä¸€ä»»åŠ¡æä¾›äº†æ–°çš„æœºä¼šã€‚é‰´äºå›¾åƒç”Ÿæˆå’Œäº‘å»é™¤ä»»åŠ¡ä¹‹é—´çš„æ¦‚å¿µä¸€è‡´æ€§ï¼Œç”Ÿæˆæ¨¡å‹ä¸ºè§£å†³é¥æ„Ÿä¸­çš„äº‘å»é™¤é—®é¢˜æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¡†æ¶çš„æ·±åº¦è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æ¢ç´¢æ–°å‹æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰å›¾åƒé‡å»ºæ¨¡å‹åœ¨äº‘å»é™¤ä¸­çš„æ½œåŠ›ã€‚ç”±äºé¥æ„Ÿå›¾åƒçš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å»ºè®®ä½¿ç”¨åŸºäºè¡¥ä¸çš„é‰´åˆ«å™¨æ¥ç¡®å®šå›¾åƒçš„æ¯ä¸ªè¡¥ä¸æ˜¯å¦çœŸå®ã€‚æ‰€æå‡ºçš„é‡å»ºè¿ç§»å­¦ä¹ æ–¹æ³•åœ¨äº‘å»é™¤æ€§èƒ½ä¸Šæ˜¾ç¤ºå‡ºä¸å…¶ä»–åŸºäºGANçš„æ–¹æ³•ç›¸æ¯”çš„æ˜¾è‘—æ”¹å–„ã€‚æ­¤å¤–ï¼Œå°½ç®¡ç”±äºå¯¹å…¶è®­ç»ƒ&#x2F;æµ‹è¯•æ•°æ®åˆ†å‰²çš„ç»†èŠ‚ä¸æ˜ç¡®ï¼Œæ— æ³•ç›´æ¥ä¸ä¸€äº›æœ€å…ˆè¿›çš„äº‘å»é™¤æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒï¼Œä½†åŸºäºç°æœ‰åŸºå‡†æµ‹è¯•ï¼Œæ‰€æå‡ºæ¨¡å‹å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05265v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äº‘å»é™¤åœ¨å¢å¼ºé¥æ„Ÿå›¾åƒåˆ†ææ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†å‡†ç¡®é‡å»ºäº‘é®æŒ¡åŒºåŸŸä»å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•ä½¿å¾—ç”ŸæˆçœŸå®å›¾åƒå˜å¾—æ›´å®¹æ˜“ï¼Œä¸ºè¯¥ä»»åŠ¡æä¾›äº†æ–°çš„æœºä¼šã€‚é‰´äºå›¾åƒç”Ÿæˆå’Œäº‘å»é™¤ä»»åŠ¡ä¹‹é—´çš„æ¦‚å¿µä¸€è‡´æ€§ï¼Œç”Ÿæˆæ¨¡å‹åœ¨è§£å†³é¥æ„Ÿä¸­çš„äº‘å»é™¤æ–¹é¢å‘ˆç°å‡ºå……æ»¡å¸Œæœ›çš„æ–¹æ³•ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¡†æ¶çš„æ·±åº¦è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æ¢ç´¢æ–°å‹æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰å›¾åƒé‡å»ºæ¨¡å‹åœ¨äº‘å»é™¤æ–¹é¢çš„æ½œåŠ›ã€‚ç”±äºé¥æ„Ÿå›¾åƒçš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºä½¿ç”¨åŸºäºè¡¥ä¸çš„é‰´åˆ«å™¨æ¥åˆ¤æ–­å›¾åƒä¸­çš„æ¯ä¸ªè¡¥ä¸æ˜¯å¦çœŸå®ã€‚æ‰€æå‡ºçš„é‡å»ºè¿ç§»å­¦ä¹ æ–¹æ³•åœ¨äº‘å»é™¤æ€§èƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä¸å…¶ä»–åŸºäºGANçš„æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚å°½ç®¡ç”±äºå¯¹å…¶è®­ç»ƒ&#x2F;æµ‹è¯•æ•°æ®åˆ†å‰²çš„ç»†èŠ‚ä¸æ˜ç¡®ï¼Œæ— æ³•ä¸æŸäº›æœ€å…ˆè¿›çš„äº‘å»é™¤æŠ€æœ¯ç›´æ¥è¿›è¡Œæ¯”è¾ƒï¼Œä½†åŸºäºå¯ç”¨çš„åŸºå‡†æµ‹è¯•ï¼Œæ‰€æå‡ºçš„æ¨¡å‹å–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‘å»é™¤åœ¨é¥æ„Ÿå›¾åƒåˆ†æä¸­è‡³å…³é‡è¦ï¼Œä½†é‡å»ºäº‘é®æŒ¡åŒºåŸŸå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹çš„å‘å±•ä¸ºäº‘å»é™¤ä»»åŠ¡æä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºGANæ¡†æ¶çš„æ·±åº¦è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨æ–°å‹æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰è¿›è¡Œäº‘å»é™¤ã€‚</li>
<li>ä¸ºåº”å¯¹é¥æ„Ÿå›¾åƒçš„å¤æ‚æ€§ï¼Œå¼•å…¥äº†åŸºäºè¡¥ä¸çš„é‰´åˆ«å™¨ã€‚</li>
<li>æ‰€æå‡ºçš„é‡å»ºè¿ç§»å­¦ä¹ æ–¹æ³•åœ¨äº‘å»é™¤æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œç›¸è¾ƒäºå…¶ä»–GANæ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>æ¨¡å‹åœ¨å¯ç”¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1098818dd68b7094b0750901ce2d16c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ce54d8960ac706467474ce84fa59520.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87c9273a84356e435e7b48b3bafa5a90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acf13eefa831e99f2059978a97a7f9ee.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RadioTransformer-Accurate-Radio-Map-Construction-and-Coverage-Prediction"><a href="#RadioTransformer-Accurate-Radio-Map-Construction-and-Coverage-Prediction" class="headerlink" title="RadioTransformer: Accurate Radio Map Construction and Coverage   Prediction"></a>RadioTransformer: Accurate Radio Map Construction and Coverage   Prediction</h2><p><strong>Authors:Yuxuan Li, Cheng Zhang, Wen Wang, Yongming Huang</strong></p>
<p>Radio map, or pathloss map prediction, is a crucial method for wireless network modeling and management. By leveraging deep learning to construct pathloss patterns from geographical maps, an accurate digital replica of the transmission environment could be established with less computational overhead and lower prediction error compared to traditional model-driven techniques. While existing state-of-the-art (SOTA) methods predominantly rely on convolutional architectures, this paper introduces a hybrid transformer-convolution model, termed RadioTransformer, to enhance the accuracy of radio map prediction. The proposed model features a multi-scale transformer-based encoder for efficient feature extraction and a convolution-based decoder for precise pixel-level image reconstruction. Simulation results demonstrate that the proposed scheme significantly improves prediction accuracy, and over a 30% reduction in root mean square error (RMSE) is achieved compared to typical SOTA approaches. </p>
<blockquote>
<p>æ— çº¿ç”µåœ°å›¾ï¼ˆæˆ–è·¯å¾„æŸè€—åœ°å›¾é¢„æµ‹ï¼‰æ˜¯æ— çº¿ç½‘ç»œå»ºæ¨¡å’Œç®¡ç†çš„é‡è¦æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨æ·±åº¦å­¦ä¹ ä»åœ°ç†åœ°å›¾æ„å»ºè·¯å¾„æŸè€—æ¨¡å¼ï¼Œå¯ä»¥å»ºç«‹ä¸€ä¸ªå…·æœ‰è¾ƒå°‘è®¡ç®—å¼€é”€å’Œè¾ƒä½é¢„æµ‹è¯¯å·®çš„ä¼ è¾“ç¯å¢ƒçš„ç²¾ç¡®æ•°å­—å‰¯æœ¬ï¼Œä¸ä¼ ç»Ÿæ¨¡å‹é©±åŠ¨æŠ€æœ¯ç›¸æ¯”ã€‚å°½ç®¡ç°æœ‰çš„æœ€æ–°æ–¹æ³•ä¸»è¦ä¾èµ–äºå·ç§¯æ¶æ„ï¼Œä½†æœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ··åˆçš„Transformerå·ç§¯æ¨¡å‹ï¼Œç§°ä¸ºRadioTransformerï¼Œä»¥æé«˜æ— çº¿ç”µåœ°å›¾é¢„æµ‹çš„ç²¾åº¦ã€‚æ‰€æå‡ºçš„æ¨¡å‹å…·æœ‰åŸºäºå¤šå°ºåº¦Transformerçš„ç¼–ç å™¨ï¼Œç”¨äºæœ‰æ•ˆçš„ç‰¹å¾æå–å’ŒåŸºäºå·ç§¯çš„è§£ç å™¨ï¼Œç”¨äºç²¾ç¡®çš„åƒç´ çº§å›¾åƒé‡å»ºã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—æé«˜é¢„æµ‹ç²¾åº¦ï¼Œä¸å…¸å‹çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰é™ä½äº†30%ä»¥ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05190v1">PDF</a> Submitted to IEEE VTC 2025 Spring</p>
<p><strong>æ€»ç»“</strong><br>    åˆ©ç”¨æ·±åº¦å­¦ä¹ ç»“åˆåœ°ç†åœ°å›¾æ„å»ºè·¯å¾„æŸè€—æ¨¡å¼ï¼Œå»ºç«‹ä¼ è¾“ç¯å¢ƒçš„ç²¾å‡†æ•°å­—å‰¯æœ¬ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¨¡å‹é©±åŠ¨æŠ€æœ¯ï¼Œæ­¤æ–¹æ³•è®¡ç®—å¼€é”€æ›´å°ï¼Œé¢„æµ‹è¯¯å·®æ›´ä½ã€‚ç°æœ‰å…ˆè¿›æŠ€æœ¯ä¸»è¦ä¾èµ–å·ç§¯æ¶æ„ï¼Œè€Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆTransformerå·ç§¯æ¨¡å‹â€”â€”RadioTransformerï¼Œä»¥æé«˜è·¯å¾„æŸè€—åœ°å›¾é¢„æµ‹çš„å‡†ç¡®åº¦ã€‚è¯¥æ¨¡å‹å…·æœ‰åŸºäºå¤šå°ºåº¦Transformerçš„ç¼–ç å™¨è¿›è¡Œé«˜æ•ˆç‰¹å¾æå–å’ŒåŸºäºå·ç§¯çš„è§£ç å™¨è¿›è¡Œç²¾ç¡®çš„åƒç´ çº§å›¾åƒé‡å»ºã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—æé«˜é¢„æµ‹ç²¾åº¦ï¼Œç›¸è¾ƒäºå…¸å‹å…ˆè¿›æŠ€æœ¯ï¼Œå‡æ–¹æ ¹è¯¯å·®é™ä½äº†è¶…è¿‡30%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ— çº¿ç”µåœ°å›¾é¢„æµ‹åœ¨æ— çº¿ç½‘ç»œå»ºæ¨¡å’Œç®¡ç†ä¸­æ˜¯å…³é”®æ–¹æ³•ã€‚</li>
<li>æ·±åº¦å­¦ä¹ è¢«ç”¨äºä»åœ°ç†åœ°å›¾æ„å»ºè·¯å¾„æŸè€—æ¨¡å¼ï¼Œåˆ›å»ºä¼ è¾“ç¯å¢ƒçš„æ•°å­—å‰¯æœ¬ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ¨¡å‹é©±åŠ¨æŠ€æœ¯ç›¸æ¯”ï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ çš„æ–¹æ³•è®¡ç®—å¼€é”€æ›´å°ï¼Œé¢„æµ‹è¯¯å·®æ›´ä½ã€‚</li>
<li>å½“å‰å…ˆè¿›æŠ€æœ¯ä¸»è¦ä¾èµ–å·ç§¯æ¶æ„è¿›è¡Œè·¯å¾„æŸè€—åœ°å›¾é¢„æµ‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆTransformerå·ç§¯æ¨¡å‹â€”â€”RadioTransformerã€‚</li>
<li>RadioTransformeræ¨¡å‹åŒ…æ‹¬åŸºäºå¤šå°ºåº¦Transformerçš„ç¼–ç å™¨å’ŒåŸºäºå·ç§¯çš„è§£ç å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-384f77e7ebb86d04dc0094397d0b0939.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb00088284d177df02611145988f5766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9da63e3410a78e1ee3ce051406cca596.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-952b0d240eed02c1bccc300f90bc0edd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Improving-the-U-Net-Configuration-for-Automated-Delineation-of-Head-and-Neck-Cancer-on-MRI"><a href="#Improving-the-U-Net-Configuration-for-Automated-Delineation-of-Head-and-Neck-Cancer-on-MRI" class="headerlink" title="Improving the U-Net Configuration for Automated Delineation of Head and   Neck Cancer on MRI"></a>Improving the U-Net Configuration for Automated Delineation of Head and   Neck Cancer on MRI</h2><p><strong>Authors:Andrei Iantsen</strong></p>
<p>Tumor volume segmentation on MRI is a challenging and time-consuming process that is performed manually in typical clinical settings. This work presents an approach to automated delineation of head and neck tumors on MRI scans, developed in the context of the MICCAI Head and Neck Tumor Segmentation for MR-Guided Applications (HNTS-MRG) 2024 Challenge. Rather than designing a new, task-specific convolutional neural network, the focus of this research was to propose improvements to the configuration commonly used in medical segmentation tasks, relying solely on the traditional U-Net architecture. The empirical results presented in this article suggest the superiority of patch-wise normalization used for both training and sliding window inference. They also indicate that the performance of segmentation models can be enhanced by applying a scheduled data augmentation policy during training. Finally, it is shown that a small improvement in quality can be achieved by using Gaussian weighting to combine predictions for individual patches during sliding window inference. The model with the best configuration obtained an aggregated Dice Similarity Coefficient (DSCagg) of 0.749 in Task 1 and 0.710 in Task 2 on five cross-validation folds. The ensemble of five models (one best model per validation fold) showed consistent results on a private test set of 50 patients with an DSCagg of 0.752 in Task 1 and 0.718 in Task 2 (team name: andrei.iantsen). The source code and model weights are freely available at <a target="_blank" rel="noopener" href="http://www.github.com/iantsen/hntsmrg">www.github.com/iantsen/hntsmrg</a>. </p>
<blockquote>
<p>è‚¿ç˜¤ä½“ç§¯åœ¨MRIä¸Šçš„åˆ†å‰²æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œè€—æ—¶çš„è¿‡ç¨‹ï¼Œé€šå¸¸åœ¨å…¸å‹çš„ä¸´åºŠç¯å¢ƒä¸­æ‰‹åŠ¨æ‰§è¡Œã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åœ¨MRIæ‰«æä¸Šè‡ªåŠ¨æç»˜å¤´éƒ¨å’Œé¢ˆéƒ¨è‚¿ç˜¤çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¯åœ¨MICCAIå¤´éƒ¨å’Œé¢ˆéƒ¨è‚¿ç˜¤åˆ†å‰²ç”¨äºMRå¼•å¯¼åº”ç”¨ï¼ˆHNTS-MRGï¼‰2024æŒ‘æˆ˜èµ›çš„èƒŒæ™¯ä¸‹å¼€å‘çš„ã€‚æœ¬æ–‡çš„é‡ç‚¹ä¸æ˜¯è®¾è®¡æ–°çš„é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œè€Œæ˜¯å¯¹åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­é€šå¸¸ä½¿ç”¨çš„é…ç½®è¿›è¡Œæ”¹è¿›ï¼Œä»…ä¾èµ–äºä¼ ç»Ÿçš„U-Netæ¶æ„ã€‚æœ¬æ–‡çš„å®è¯ç»“æœè¡¨æ˜ï¼Œç”¨äºè®­ç»ƒå’Œæ»‘åŠ¨çª—å£æ¨æ–­çš„å—çº§å½’ä¸€åŒ–å…·æœ‰ä¼˜è¶Šæ€§ã€‚ä»–ä»¬è¿˜è¡¨æ˜ï¼Œé€šè¿‡è®­ç»ƒæœŸé—´å®æ–½è®¡åˆ’æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¯ä»¥æé«˜åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚æœ€åï¼Œç»“æœæ˜¾ç¤ºï¼Œåœ¨ä½¿ç”¨é«˜æ–¯æƒé‡åˆå¹¶æ»‘åŠ¨çª—å£æ¨æ–­æœŸé—´ä¸ªåˆ«è¡¥ä¸çš„é¢„æµ‹æ—¶ï¼Œå¯ä»¥å®ç°è´¨é‡çš„å°å¹…æå‡ã€‚æœ€ä½³é…ç½®æ¨¡å‹åœ¨äº”ä¸ªäº¤å‰éªŒè¯æŠ˜å ä¸­çš„ä»»åŠ¡1å’Œä»»åŠ¡2ä¸Šåˆ†åˆ«è·å¾—äº†0.749å’Œ0.710çš„Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCaggï¼‰ã€‚äº”ä¸ªæ¨¡å‹ï¼ˆæ¯ä¸ªéªŒè¯æŠ˜å çš„æœ€ä½³æ¨¡å‹ï¼‰åœ¨åŒ…å«50åæ‚£è€…çš„ç§æœ‰æµ‹è¯•é›†ä¸Šçš„ç»“æœè¡¨ç°ä¸€è‡´ï¼Œä»»åŠ¡1å’Œä»»åŠ¡2çš„DSCaggåˆ†åˆ«ä¸º0.752å’Œ0.718ï¼ˆå›¢é˜Ÿåç§°ï¼šandrei.iantsenï¼‰ã€‚æºä»£ç å’Œæ¨¡å‹æƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="http://www.github.com/iantsen/hntsmrg%E5%85%8D%E8%B4%B9%E8%8E%B7%E5%8F%96%E3%80%82">www.github.com/iantsen/hntsmrgå…è´¹è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05120v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹MRIæ‰«æä¸­å¤´éƒ¨å’Œé¢ˆéƒ¨è‚¿ç˜¤çš„è‡ªåŠ¨åˆ†å‰²æ–¹æ³•ï¼Œé‡ç‚¹æ˜¯å¯¹å¸¸ç”¨åŒ»å­¦åˆ†å‰²ä»»åŠ¡é…ç½®çš„æ”¹è¿›ï¼Œè€Œéè®¾è®¡æ–°çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚é€šè¿‡é‡‡ç”¨patch-wiseå½’ä¸€åŒ–è¿›è¡Œè®­ç»ƒå’Œæ»‘åŠ¨çª—å£æ¨æ–­ï¼Œä»¥åŠè®¡åˆ’æ•°æ®å¢å¼ºç­–ç•¥ï¼Œæé«˜äº†åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚æœ€ä½³é…ç½®çš„æ¨¡å‹åœ¨äº”ä¸ªäº¤å‰éªŒè¯é›†ä¸Šè·å¾—äº†è¾ƒé«˜çš„Diceç›¸ä¼¼ç³»æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶å…³æ³¨äºæ”¹è¿›å¸¸ç”¨äºåŒ»å­¦åˆ†å‰²ä»»åŠ¡çš„é…ç½®ï¼Œè€Œéè®¾è®¡æ–°çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚</li>
<li>é‡‡ç”¨patch-wiseå½’ä¸€åŒ–è¿›è¡Œè®­ç»ƒå’Œæ»‘åŠ¨çª—å£æ¨æ–­ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡è®¡åˆ’æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¢å¼ºäº†åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æœ€ä½³é…ç½®çš„æ¨¡å‹åœ¨äº”ä¸ªäº¤å‰éªŒè¯é›†ä¸Šçš„Diceç›¸ä¼¼ç³»æ•°è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¨¡å‹åœ¨ç§äººæµ‹è¯•é›†ä¸Šçš„ç»“æœä¹Ÿè¡¨ç°ç¨³å®šã€‚</li>
<li>æºä»£ç å’Œæ¨¡å‹æƒé‡å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05120">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b7e23d82847a3bbd02d58479e29714f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60fac089c9b23b3af96aecd791e280b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b4b8f5dd03e972fbbabcb066ffa35b0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="D3RM-A-Discrete-Denoising-Diffusion-Refinement-Model-for-Piano-Transcription"><a href="#D3RM-A-Discrete-Denoising-Diffusion-Refinement-Model-for-Piano-Transcription" class="headerlink" title="D3RM: A Discrete Denoising Diffusion Refinement Model for Piano   Transcription"></a>D3RM: A Discrete Denoising Diffusion Refinement Model for Piano   Transcription</h2><p><strong>Authors:Hounsu Kim, Taegyun Kwon, Juhan Nam</strong></p>
<p>Diffusion models have been widely used in the generative domain due to their convincing performance in modeling complex data distributions. Moreover, they have shown competitive results on discriminative tasks, such as image segmentation. While diffusion models have also been explored for automatic music transcription, their performance has yet to reach a competitive level. In this paper, we focus on discrete diffusion modelâ€™s refinement capabilities and present a novel architecture for piano transcription. Our model utilizes Neighborhood Attention layers as the denoising module, gradually predicting the target high-resolution piano roll, conditioned on the finetuned features of a pretrained acoustic model. To further enhance refinement, we devise a novel strategy which applies distinct transition states during training and inference stage of discrete diffusion models. Experiments on the MAESTRO dataset show that our approach outperforms previous diffusion-based piano transcription models and the baseline model in terms of F1 score. Our code is available in <a target="_blank" rel="noopener" href="https://github.com/hanshounsu/d3rm">https://github.com/hanshounsu/d3rm</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ç”±äºå…¶åœ¨å¯¹å¤æ‚æ•°æ®åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡æ—¶çš„å‡ºè‰²è¡¨ç°ï¼Œåœ¨ç”Ÿæˆé¢†åŸŸå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚æ­¤å¤–ï¼Œå®ƒä»¬åœ¨åˆ¤åˆ«ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†å‰²ï¼‰ä¸Šä¹Ÿå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹ä¹Ÿå·²è¢«æ¢ç´¢ç”¨äºè‡ªåŠ¨éŸ³ä¹è½¬å½•ï¼Œä½†å…¶æ€§èƒ½å°šæœªè¾¾åˆ°ç«äº‰æ°´å¹³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„ç²¾ç»†åŒ–èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç”¨äºé’¢ç´è½¬å½•çš„æ–°å‹æ¶æ„ã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨é‚»åŸŸæ³¨æ„åŠ›å±‚ä½œä¸ºå»å™ªæ¨¡å—ï¼ŒåŸºäºé¢„è®­ç»ƒå£°å­¦æ¨¡å‹çš„å¾®è°ƒç‰¹å¾ï¼Œé€æ­¥é¢„æµ‹ç›®æ ‡é«˜åˆ†è¾¨ç‡é’¢ç´ä¹è°±ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ç²¾ç»†åŒ–ç¨‹åº¦ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹ç­–ç•¥ï¼Œåœ¨ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†é˜¶æ®µåº”ç”¨ä¸åŒçš„è¿‡æ¸¡çŠ¶æ€ã€‚åœ¨MAESTROæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨F1åˆ†æ•°æ–¹é¢ä¼˜äºä¹‹å‰çš„åŸºäºæ‰©æ•£çš„é’¢ç´è½¬å½•æ¨¡å‹å’ŒåŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/hanshounsu/d3rm%E3%80%82">https://github.com/hanshounsu/d3rmã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05068v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨é’¢ç´éŸ³ä¹è½¬å½•æ–¹é¢çš„åº”ç”¨ã€‚é€šè¿‡é‡‡ç”¨æ–°é¢–çš„æ¶æ„å’Œç­–ç•¥ï¼Œè¯¥æ¨¡å‹åœ¨MAESTROæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ä¹‹å‰çš„æ‰©æ•£æ¨¡å‹åŠåŸºçº¿æ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„ç²¾ç»†åŒ–èƒ½åŠ›ã€‚å…·ä½“é‡‡ç”¨äº†é‚»å±…æ³¨æ„åŠ›å±‚ä½œä¸ºå»å™ªæ¨¡å—ï¼Œé€æ­¥é¢„æµ‹ç›®æ ‡é«˜åˆ†è¾¨ç‡é’¢ç´è°±å·ï¼Œå¹¶åœ¨è®­ç»ƒåŠæ¨ç†é˜¶æ®µå®æ–½ç‹¬ç‰¹çš„è¿‡æ¸¡çŠ¶æ€ç­–ç•¥æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨é’¢ç´éŸ³ä¹è½¬å½•é¢†åŸŸå±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¶æ„ç”¨äºé’¢ç´è½¬å½•çš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹ä½¿ç”¨é‚»å±…æ³¨æ„åŠ›å±‚ä½œä¸ºå»å™ªæ¨¡å—ä»¥æé«˜ç²¾ç»†åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ç‹¬ç‰¹çš„è¿‡æ¸¡çŠ¶æ€ç­–ç•¥ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨MAESTROæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ‰©æ•£æ¨¡å‹åŠåŸºçº¿æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šä¾›ç ”ç©¶äººå‘˜å‚è€ƒå’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0964bffbf1c1d6192cc6180e46eb1f87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44a494d972de7b54258131ca8a6c6ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92d3dd22fcd9d11ebc0f706a7dfe86a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2016f224be32580eb20bd95e37c89a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7425f69833b2da851f65418d3858aedf.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-CT-Image-Classification-Network-Framework-for-Lung-Tumors-Based-on-Pre-trained-MobileNetV2-Model-and-Transfer-learning-And-Its-Application-and-Market-Analysis-in-the-Medical-field"><a href="#A-CT-Image-Classification-Network-Framework-for-Lung-Tumors-Based-on-Pre-trained-MobileNetV2-Model-and-Transfer-learning-And-Its-Application-and-Market-Analysis-in-the-Medical-field" class="headerlink" title="A CT Image Classification Network Framework for Lung Tumors Based on   Pre-trained MobileNetV2 Model and Transfer learning, And Its Application and   Market Analysis in the Medical field"></a>A CT Image Classification Network Framework for Lung Tumors Based on   Pre-trained MobileNetV2 Model and Transfer learning, And Its Application and   Market Analysis in the Medical field</h2><p><strong>Authors:Ziyang Gao, Yong Tian, Shih-Chi Lin, Junghua Lin</strong></p>
<p>In the medical field, accurate diagnosis of lung cancer is crucial for treatment. Traditional manual analysis methods have significant limitations in terms of accuracy and efficiency. To address this issue, this paper proposes a deep learning network framework based on the pre-trained MobileNetV2 model, initialized with weights from the ImageNet-1K dataset (version 2). The last layer of the model (the fully connected layer) is replaced with a new fully connected layer, and a softmax activation function is added to efficiently classify three types of lung cancer CT scan images. Experimental results show that the model achieves an accuracy of 99.6% on the test set, with significant improvements in feature extraction compared to traditional models.With the rapid development of artificial intelligence technologies, deep learning applications in medical image processing are bringing revolutionary changes to the healthcare industry. AI-based lung cancer detection systems can significantly improve diagnostic efficiency, reduce the workload of doctors, and occupy an important position in the global healthcare market. The potential of AI to improve diagnostic accuracy, reduce medical costs, and promote precision medicine will have a profound impact on the future development of the healthcare industry. </p>
<blockquote>
<p>åœ¨åŒ»å­¦é¢†åŸŸï¼Œè‚ºç™Œçš„å‡†ç¡®è¯Šæ–­å¯¹æ²»ç–—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„åˆ†ææ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å­˜åœ¨é‡å¤§å±€é™æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒMobileNetV2æ¨¡å‹çš„æ·±åº¦å­¦ä¹ ç½‘ç»œæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ImageNet-1Kæ•°æ®é›†ï¼ˆç¬¬äºŒç‰ˆï¼‰çš„æƒé‡è¿›è¡Œåˆå§‹åŒ–ã€‚è¯¥æ¨¡å‹çš„æœ€åä¸€å±‚ï¼ˆå…¨è¿æ¥å±‚ï¼‰è¢«æ›¿æ¢ä¸ºæ–°çš„å…¨è¿æ¥å±‚ï¼Œå¹¶æ·»åŠ äº†ä¸€ä¸ªsoftmaxæ¿€æ´»å‡½æ•°ï¼Œä»¥æœ‰æ•ˆåœ°å¯¹ä¸‰ç§è‚ºç™ŒCTæ‰«æå›¾åƒè¿›è¡Œåˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†99.6%çš„å‡†ç¡®ç‡ï¼Œåœ¨ç‰¹å¾æå–æ–¹é¢ç›¸æ¯”ä¼ ç»Ÿæ¨¡å‹æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œæ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒå¤„ç†ä¸­çš„åº”ç”¨ç»™åŒ»ç–—ä¿å¥è¡Œä¸šå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ã€‚åŸºäºäººå·¥æ™ºèƒ½çš„è‚ºç™Œæ£€æµ‹ç³»ç»Ÿå¯ä»¥æ˜¾è‘—æé«˜è¯Šæ–­æ•ˆç‡ï¼Œå‡è½»åŒ»ç”Ÿçš„å·¥ä½œé‡ï¼Œåœ¨å…¨çƒåŒ»ç–—å¸‚åœºä¸­å æ®é‡è¦åœ°ä½ã€‚äººå·¥æ™ºèƒ½æé«˜è¯Šæ–­å‡†ç¡®ç‡ã€é™ä½åŒ»ç–—æˆæœ¬ã€ä¿ƒè¿›ç²¾å‡†åŒ»ç–—çš„æ½œåŠ›å°†å¯¹åŒ»ç–—å«ç”Ÿè¡Œä¸šçš„æœªæ¥å‘å±•äº§ç”Ÿæ·±è¿œå½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04996v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ ç½‘ç»œæ¡†æ¶çš„è‚ºç™Œè¯Šæ–­ç ”ç©¶é‡‡ç”¨é¢„è®­ç»ƒçš„MobileNetV2æ¨¡å‹ï¼Œåˆ©ç”¨ImageNet-1Kæ•°æ®é›†è¿›è¡Œæƒé‡åˆå§‹åŒ–ï¼Œæå‡ºæ–°æ–¹æ³•ä»¥æå‡æ¨¡å‹åœ¨CTæ‰«æå›¾åƒä¸Šçš„è‚ºç™Œåˆ†ç±»å‡†ç¡®æ€§ã€‚æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šå‡†ç¡®ç‡é«˜è¾¾99.6%ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›æ”¹å˜åŒ»ç–—è¡Œä¸šçš„è¯Šæ–­æ•ˆç‡å’Œç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶é‡‡ç”¨æ·±åº¦å­¦ä¹ ç½‘ç»œæ¡†æ¶ï¼ŒåŸºäºé¢„è®­ç»ƒçš„MobileNetV2æ¨¡å‹è¿›è¡Œè‚ºç™Œè¯Šæ–­ã€‚</li>
<li>ä½¿ç”¨ImageNet-1Kæ•°æ®é›†è¿›è¡Œæƒé‡åˆå§‹åŒ–ï¼Œä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹é€šè¿‡æ›¿æ¢åŸæœ‰å…¨è¿æ¥å±‚å¹¶æ·»åŠ softmaxæ¿€æ´»å‡½æ•°ï¼Œå®ç°å¯¹ä¸‰ç§è‚ºç™ŒCTæ‰«æå›¾åƒçš„é«˜æ•ˆåˆ†ç±»ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡é«˜è¾¾99.6%ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨ç‰¹å¾æå–æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>AIåœ¨åŒ»ç–—å›¾åƒå¤„ç†æ–¹é¢çš„åº”ç”¨ä¸ºåŒ»ç–—è¡Œä¸šå¸¦æ¥é©å‘½æ€§å˜åŒ–ã€‚</li>
<li>AIè‚ºç™Œæ£€æµ‹ç³»ç»Ÿèƒ½æ˜¾è‘—æé«˜è¯Šæ–­æ•ˆç‡ï¼Œå‡è½»åŒ»ç”Ÿå·¥ä½œé‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04996">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2c3ffad0f25e976019714893832e2625.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac59f5ce779aaf23b1830d7032fc7272.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e66d647befc63dc1dd754c8fd72e6390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01498356fcac75f46649ed7802b1c00d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40f6f0168d841f65eb9f6c259156cbae.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Steerable-Deep-Network-for-Model-Free-Diffusion-MRI-Registration"><a href="#A-Steerable-Deep-Network-for-Model-Free-Diffusion-MRI-Registration" class="headerlink" title="A Steerable Deep Network for Model-Free Diffusion MRI Registration"></a>A Steerable Deep Network for Model-Free Diffusion MRI Registration</h2><p><strong>Authors:Gianfranco Cortes, Xiaoda Qu, Baba C. Vemuri</strong></p>
<p>Nonrigid registration is vital to medical image analysis but remains challenging for diffusion MRI (dMRI) due to its high-dimensional, orientation-dependent nature. While classical methods are accurate, they are computationally demanding, and deep neural networks, though efficient, have been underexplored for nonrigid dMRI registration compared to structural imaging. We present a novel, deep learning framework for model-free, nonrigid registration of raw diffusion MRI data that does not require explicit reorientation. Unlike previous methods relying on derived representations such as diffusion tensors or fiber orientation distribution functions, in our approach, we formulate the registration as an equivariant diffeomorphism of position-and-orientation space. Central to our method is an $\mathsf{SE}(3)$-equivariant UNet that generates velocity fields while preserving the geometric properties of a raw dMRIâ€™s domain. We introduce a new loss function based on the maximum mean discrepancy in Fourier space, implicitly matching ensemble average propagators across images. Experimental results on Human Connectome Project dMRI data demonstrate competitive performance compared to state-of-the-art approaches, with the added advantage of bypassing the overhead for estimating derived representations. This work establishes a foundation for data-driven, geometry-aware dMRI registration directly in the acquisition space. </p>
<blockquote>
<p>éåˆšæ€§é…å‡†å¯¹åŒ»å­¦å›¾åƒåˆ†æè‡³å…³é‡è¦ï¼Œä½†ç”±äºæ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰çš„é«˜ç»´ã€æ–¹å‘ä¾èµ–ç‰¹æ€§ï¼Œå®ƒä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è™½ç„¶ç»å…¸æ–¹æ³•å‡†ç¡®ï¼Œä½†è®¡ç®—é‡å¤§ï¼Œè€Œæ·±åº¦ç¥ç»ç½‘ç»œè™½ç„¶æ•ˆç‡é«˜ï¼Œä½†ä¸ç»“æ„æˆåƒç›¸æ¯”ï¼Œåœ¨éåˆšæ€§dMRIé…å‡†æ–¹é¢çš„æ¢ç´¢è¾ƒå°‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ— æ¨¡å‹éåˆšæ€§é…å‡†åŸå§‹æ‰©æ•£MRIæ•°æ®ï¼Œæ— éœ€æ˜ç¡®è°ƒæ•´æ–¹å‘ã€‚ä¸åŒäºä¾èµ–è¡ç”Ÿè¡¨ç¤ºï¼ˆå¦‚æ‰©æ•£å¼ é‡æˆ–çº¤ç»´æ–¹å‘åˆ†å¸ƒå‡½æ•°ï¼‰çš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†é…å‡†å…¬å¼åŒ–ä¸ºä½ç½®å’Œæ–¹å‘çš„ç­‰è·åŒæ„ç©ºé—´ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªSE(3)ç­‰è·UNetï¼Œå®ƒç”Ÿæˆé€Ÿåº¦åœºçš„åŒæ—¶ä¿ç•™äº†åŸå§‹dMRIåŸŸçš„å‡ ä½•å±æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æŸå¤±å‡½æ•°ï¼ŒåŸºäºå‚…é‡Œå¶ç©ºé—´ä¸­çš„æœ€å¤§å‡å€¼å·®å¼‚ï¼Œéšå¼åŒ¹é…å›¾åƒä¹‹é—´çš„æ•´ä½“å¹³å‡ä¼ æ’­å™¨ã€‚åœ¨äººç±»è¿æ¥ç»„é¡¹ç›®dMRIæ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰ç«äº‰æ€§èƒ½ï¼Œå¹¶é¢å¤–é¿å…äº†ä¼°è®¡è¡ç”Ÿè¡¨ç¤ºçš„å¼€é”€ã€‚è¿™é¡¹å·¥ä½œä¸ºç›´æ¥åœ¨é‡‡é›†ç©ºé—´ä¸­è¿›è¡Œæ•°æ®é©±åŠ¨ã€å‡ ä½•æ„ŸçŸ¥çš„dMRIé…å‡†å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04794v2">PDF</a> Coauthor was inadvertently left out. This is now corrected</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ‘˜è¦æå‡ºäº†ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¯¹åŸå§‹æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰æ•°æ®è¿›è¡Œéåˆšæ€§æ³¨å†Œï¼Œæ— éœ€æ˜ç¡®è°ƒæ•´æ–¹å‘ã€‚è¯¥æ¡†æ¶é‡‡ç”¨SE(3)ç­‰ä»·æ€§UNetç”Ÿæˆé€Ÿåº¦åœºï¼ŒåŒæ—¶ä¿ç•™åŸå§‹dMRIåŸŸçš„å‡ ä½•ç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Human Connectome Project dMRIæ•°æ®ä¸Šçš„æ€§èƒ½ä¸æœ€æ–°æ–¹æ³•ç›¸å½“ï¼Œä¸”å…·æœ‰æ— éœ€ä¼°è®¡æ´¾ç”Ÿè¡¨ç¤ºçš„é¢å¤–ä¼˜åŠ¿ã€‚è¿™ä¸ºç›´æ¥åœ¨é‡‡é›†ç©ºé—´è¿›è¡Œæ•°æ®é©±åŠ¨ã€å‡ ä½•æ„ŸçŸ¥çš„dMRIæ³¨å†Œå¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éåˆšæ€§æ³¨å†Œå¯¹åŒ»å­¦å›¾åƒåˆ†æè‡³å…³é‡è¦ï¼Œä½†å¯¹äºé«˜ç»´ã€æ–¹å‘ä¾èµ–çš„æ‰©æ•£ç£å…±æŒ¯æˆåƒï¼ˆdMRIï¼‰å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç»å…¸æ–¹æ³•è™½ç„¶å‡†ç¡®ä½†è®¡ç®—é‡å¤§ï¼Œè€Œæ·±åº¦ç¥ç»ç½‘ç»œè™½ç„¶æ•ˆç‡é«˜ä½†åœ¨éåˆšæ€§dMRIæ³¨å†Œæ–¹é¢çš„åº”ç”¨ç›¸è¾ƒäºç»“æ„æˆåƒè¢«æ¢ç´¢å¾—è¾ƒå°‘ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¯¹åŸå§‹dMRIæ•°æ®è¿›è¡Œéåˆšæ€§æ³¨å†Œï¼Œæ— éœ€æ˜ç¡®è°ƒæ•´æ–¹å‘ã€‚</li>
<li>é‡‡ç”¨SE(3)-equivariant UNetç”Ÿæˆé€Ÿåº¦åœºï¼ŒåŒæ—¶ä¿ç•™åŸå§‹dMRIåŸŸçš„å‡ ä½•ç‰¹æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºå‚…é‡Œå¶ç©ºé—´ä¸­æœ€å¤§å‡å€¼å·®å¼‚çš„æ–°æŸå¤±å‡½æ•°ï¼Œéšå¼åŒ¹é…å›¾åƒé—´çš„æ•´ä½“å¹³å‡ä¼ æ’­å™¨ã€‚</li>
<li>åœ¨Human Connectome Project dMRIæ•°æ®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-860b3988b9e39d9ee539c4c52b8d50ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5f92b72a9b36945d634769088da15f7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Rethinking-domain-generalization-in-medical-image-segmentation-One-image-as-one-domain"><a href="#Rethinking-domain-generalization-in-medical-image-segmentation-One-image-as-one-domain" class="headerlink" title="Rethinking domain generalization in medical image segmentation: One   image as one domain"></a>Rethinking domain generalization in medical image segmentation: One   image as one domain</h2><p><strong>Authors:Jin Hong, Bo Liu, Guoli Long</strong></p>
<p>Domain shifts in medical image segmentation, particularly when data comes from different centers, pose significant challenges. Intra-center variability, such as differences in scanner models or imaging protocols, can cause domain shifts as large as, or even larger than, those between centers. To address this, we propose the â€œone image as one domainâ€ (OIOD) hypothesis, which treats each image as a unique domain, enabling flexible and robust domain generalization. Based on this hypothesis, we develop a unified disentanglement-based domain generalization (UniDDG) framework, which simultaneously handles both multi-source and single-source domain generalization without requiring explicit domain labels. This approach simplifies training with a fixed architecture, independent of the number of source domains, reducing complexity and enhancing scalability. We decouple each input image into content representation and style code, then exchange and combine these within the batch for segmentation, reconstruction, and further disentanglement. By maintaining distinct style codes for each image, our model ensures thorough decoupling of content representations and style codes, improving domain invariance of the content representations. Additionally, we enhance generalization with expansion mask attention (EMA) for boundary preservation and style augmentation (SA) to simulate diverse image styles, improving robustness to domain shifts. Extensive experiments show that our method achieves Dice scores of 84.43% and 88.91% for multi-source to single-center and single-center generalization in optic disc and optic cup segmentation, respectively, and 86.96% and 88.56% for prostate segmentation, outperforming current state-of-the-art domain generalization methods, offering superior performance and adaptability across clinical settings. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸæ¼‚ç§»ï¼Œç‰¹åˆ«æ˜¯å½“æ•°æ®æ¥è‡ªä¸åŒä¸­å¿ƒæ—¶ï¼Œä¼šå¸¦æ¥é‡å¤§æŒ‘æˆ˜ã€‚æ¥è‡ªåŒä¸€ä¸­å¿ƒçš„å†…éƒ¨å·®å¼‚æ€§ï¼Œå¦‚æ‰«æä»ªå‹å·æˆ–æˆåƒåè®®çš„ä¸åŒï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸è·¨ä¸­å¿ƒé¢†åŸŸæ¼‚ç§»ä¸€æ ·ç”šè‡³æ›´å¤§çš„é¢†åŸŸæ¼‚ç§»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä¸€å›¾ä¸€ä¸ªé¢†åŸŸâ€ï¼ˆOIODï¼‰å‡è®¾ï¼Œè¯¥å‡è®¾å°†æ¯å¼ å›¾åƒè§†ä¸ºä¸€ä¸ªç‹¬ç‰¹çš„é¢†åŸŸï¼Œä»¥å®ç°çµæ´»å’Œç¨³å¥çš„é¢†åŸŸæ³›åŒ–ã€‚åŸºäºè¿™ä¸€å‡è®¾ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºåˆ†ç¦»ç†è®ºçš„é¢†åŸŸæ³›åŒ–ï¼ˆUniDDGï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥åŒæ—¶å¤„ç†å¤šæºå’Œå•æºé¢†åŸŸæ³›åŒ–ï¼Œæ— éœ€æ˜ç¡®çš„é¢†åŸŸæ ‡ç­¾ã€‚è¿™ç§æ–¹æ³•ç®€åŒ–äº†å›ºå®šæ¶æ„çš„è®­ç»ƒè¿‡ç¨‹ï¼Œæ— è®ºæºé¢†åŸŸçš„æ•°é‡å¦‚ä½•éƒ½æ˜¯å¦‚ä¸€çš„ã€‚æˆ‘ä»¬é€šè¿‡å°†æ¯ä¸ªè¾“å…¥å›¾åƒè§£è€¦ä¸ºå†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç ï¼Œç„¶ååœ¨æ‰¹æ¬¡å†…è¿›è¡Œäº¤æ¢å’Œç»„åˆä»¥è¿›è¡Œåˆ†å‰²ã€é‡å»ºå’Œè¿›ä¸€æ­¥çš„è§£è€¦ã€‚é€šè¿‡ä¸ºæ¯ä¸ªå›¾åƒä¿ç•™ç‹¬ç‰¹çš„é£æ ¼ä»£ç ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç¡®ä¿äº†å†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç çš„å½»åº•è§£è€¦ï¼Œæé«˜äº†å†…å®¹è¡¨ç¤ºçš„é¢†åŸŸä¸å˜æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ‰©å±•æ©è†œæ³¨æ„åŠ›ï¼ˆEMAï¼‰ä¿æŒè¾¹ç•Œï¼Œæ¨¡æ‹Ÿå„ç§å›¾åƒé£æ ¼ä»¥å¢å¼ºé£æ ¼æ‰©å……ï¼ˆSAï¼‰ï¼Œæé«˜äº†å¯¹é¢†åŸŸæ¼‚ç§»çš„ç¨³å¥æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†ç›˜å’Œè§†æ¯åˆ†å‰²çš„å¤šæºåˆ°å•ä¸€ä¸­å¿ƒå’Œå•ä¸€ä¸­å¿ƒæ³›åŒ–ä¸­åˆ†åˆ«å®ç°äº†84.43%å’Œ88.91%çš„Diceå¾—åˆ†ï¼Œåœ¨å‰åˆ—è…ºåˆ†å‰²ä¸­åˆ†åˆ«å®ç°äº†86.96%å’Œ88.56%ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„é¢†åŸŸæ³›åŒ–æ–¹æ³•ï¼Œåœ¨ä¸´åºŠç¯å¢ƒä¸­æä¾›äº†å“è¶Šçš„æ€§èƒ½å’Œé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04741v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„è·¨ä¸­å¿ƒæ•°æ®é¢†åŸŸæ¼‚ç§»é—®é¢˜ï¼Œæå‡ºâ€œä¸€å›¾ä¸€åŸŸâ€ï¼ˆOIODï¼‰å‡è®¾å’Œç»Ÿä¸€è§£è€¦çš„åŸŸæ³›åŒ–ï¼ˆUniDDGï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ— éœ€æ˜ç¡®çš„åŸŸæ ‡ç­¾å³å¯åŒæ—¶å¤„ç†å¤šæºå’Œå•æºåŸŸæ³›åŒ–é—®é¢˜ï¼Œç®€åŒ–äº†å›ºå®šæ¶æ„çš„è®­ç»ƒï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ‰©å±•æ€§ã€‚é€šè¿‡ç»´æŠ¤æ¯ä¸ªå›¾åƒç‰¹æœ‰çš„é£æ ¼ä»£ç ï¼Œæ¨¡å‹ç¡®ä¿å†…å®¹å’Œé£æ ¼ä»£ç çš„å½»åº•è§£è€¦ï¼Œæé«˜å†…å®¹è¡¨ç¤ºçš„åŸŸä¸å˜æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ‰©å±•æ©è†œæ³¨æ„åŠ›ï¼ˆEMAï¼‰å’Œé£æ ¼å¢å¼ºï¼ˆSAï¼‰æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…‰å­¦åœ†ç›˜ã€å…‰å­¦æ¯å’Œå‰åˆ—è…ºåˆ†å‰²ä¸­å‡å–å¾—äº†ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„åŸŸæ³›åŒ–æ–¹æ³•çš„ç»“æœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„è·¨ä¸­å¿ƒå’Œè·¨ä»ªå™¨å·®å¼‚é€ æˆäº†é¢†åŸŸæ¼‚ç§»çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºâ€œä¸€å›¾ä¸€åŸŸâ€ï¼ˆOIODï¼‰å‡è®¾ï¼Œå°†æ¯å¹…å›¾åƒè§†ä¸ºç‹¬ç‰¹çš„åŸŸï¼Œä¿ƒè¿›çµæ´»å’Œç¨³å¥çš„åŸŸæ³›åŒ–ã€‚</li>
<li>å‘å±•äº†ç»Ÿä¸€è§£è€¦çš„åŸŸæ³›åŒ–ï¼ˆUniDDGï¼‰æ¡†æ¶ï¼Œæ— éœ€æ˜ç¡®çš„åŸŸæ ‡ç­¾å³å¯å¤„ç†å¤šæºå’Œå•æºåŸŸæ³›åŒ–é—®é¢˜ã€‚</li>
<li>é€šè¿‡è§£è€¦è¾“å…¥å›¾åƒä¸ºå†…å®¹è¡¨ç¤ºå’Œé£æ ¼ä»£ç ï¼Œå¢å¼ºäº†æ¨¡å‹çš„åŸŸä¸å˜æ€§ã€‚</li>
<li>é€šè¿‡æ‰©å±•æ©è†œæ³¨æ„åŠ›ï¼ˆEMAï¼‰å’Œé£æ ¼å¢å¼ºï¼ˆSAï¼‰æé«˜äº†æ¨¡å‹çš„è¾¹ç•Œä¿ç•™èƒ½åŠ›å’Œå¯¹å„ç§å›¾åƒé£æ ¼çš„ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šå‡å–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8045c617d54947e3a9f78534db6df996.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a40bc7f9fcedd92fde70f67a5b947788.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Generative-Style-Transfer-for-MRI-Image-Segmentation-A-Case-of-Glioma-Segmentation-in-Sub-Saharan-Africa"><a href="#Generative-Style-Transfer-for-MRI-Image-Segmentation-A-Case-of-Glioma-Segmentation-in-Sub-Saharan-Africa" class="headerlink" title="Generative Style Transfer for MRI Image Segmentation: A Case of Glioma   Segmentation in Sub-Saharan Africa"></a>Generative Style Transfer for MRI Image Segmentation: A Case of Glioma   Segmentation in Sub-Saharan Africa</h2><p><strong>Authors:Rancy Chepchirchir, Jill Sunday, Raymond Confidence, Dong Zhang, Talha Chaudhry, Udunna C. Anazodo, Kendi Muchungi, Yujing Zou</strong></p>
<p>In Sub-Saharan Africa (SSA), the utilization of lower-quality Magnetic Resonance Imaging (MRI) technology raises questions about the applicability of machine learning methods for clinical tasks. This study aims to provide a robust deep learning-based brain tumor segmentation (BraTS) method tailored for the SSA population using a threefold approach. Firstly, the impact of domain shift from the SSA training data on model efficacy was examined, revealing no significant effect. Secondly, a comparative analysis of 3D and 2D full-resolution models using the nnU-Net framework indicates similar performance of both the models trained for 300 epochs achieving a five-fold cross-validation score of 0.93. Lastly, addressing the performance gap observed in SSA validation as opposed to the relatively larger BraTS glioma (GLI) validation set, two strategies are proposed: fine-tuning SSA cases using the GLI+SSA best-pretrained 2D fullres model at 300 epochs, and introducing a novel neural style transfer-based data augmentation technique for the SSA cases. This investigation underscores the potential of enhancing brain tumor prediction within SSAâ€™s unique healthcare landscape. </p>
<blockquote>
<p>åœ¨æ’’å“ˆæ‹‰ä»¥å—éæ´²ï¼ˆSSAï¼‰ï¼Œç”±äºä½¿ç”¨è´¨é‡è¾ƒä½çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æŠ€æœ¯ï¼Œæœºå™¨å­¦ä¹ åœ¨ä¸´åºŠä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§å¼•å‘äº†è´¨ç–‘ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é‡‡ç”¨ä¸‰é‡æ–¹æ³•ï¼Œä¸ºSSAäººç¾¤æä¾›ä¸€ç§ç¨³å¥çš„æ·±åº¦å­¦ä¹ çš„è„‘è‚¿ç˜¤åˆ†å‰²ï¼ˆBraTSï¼‰æ–¹æ³•ã€‚é¦–å…ˆï¼Œç ”ç©¶äº†SSAè®­ç»ƒæ•°æ®å¯¹æ¨¡å‹åŠŸæ•ˆçš„åŸŸåç§»å½±å“ï¼Œç»“æœæ˜¾ç¤ºæ— æ˜¾è‘—å½±å“ã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨nnU-Netæ¡†æ¶å¯¹3Då’Œ2Då…¨åˆ†è¾¨ç‡æ¨¡å‹è¿›è¡Œçš„æ¯”è¾ƒåˆ†æè¡¨æ˜ï¼Œä¸¤è€…æ€§èƒ½ç›¸ä¼¼ï¼Œç»è¿‡300è½®è®­ç»ƒçš„æ¨¡å‹åœ¨äº”å€äº¤å‰éªŒè¯ä¸­å¾—åˆ†è¾¾åˆ°0.93ã€‚æœ€åï¼Œé’ˆå¯¹åœ¨SSAéªŒè¯é›†ä¸­è§‚å¯Ÿåˆ°çš„æ€§èƒ½å·®è·ä¸ç›¸å¯¹è¾ƒå¤§çš„BraTSèƒ¶è´¨ç˜¤ï¼ˆGLIï¼‰éªŒè¯é›†ä¹‹é—´çš„å¯¹æ¯”ï¼Œæå‡ºäº†ä¸¤ç§ç­–ç•¥ï¼šä½¿ç”¨GLI+SSAæœ€ä½³é¢„è®­ç»ƒçš„2Då…¨åˆ†è¾¨ç‡æ¨¡å‹å¯¹SSAç—…ä¾‹è¿›è¡Œå¾®è°ƒï¼ˆè®­ç»ƒå‘¨æœŸä¸º300è½®ï¼‰ï¼Œå¹¶ä¸ºSSAç—…ä¾‹å¼•å…¥ä¸€ç§æ–°å‹çš„åŸºäºç¥ç»é£æ ¼è½¬ç§»çš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†æé«˜SSAç‹¬ç‰¹åŒ»ç–—ç¯å¢ƒä¸‹è„‘è‚¿ç˜¤é¢„æµ‹æ½œåŠ›çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04734v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>     æœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ä¸ºæ’’å“ˆæ‹‰ä»¥å—éæ´²ï¼ˆSSAï¼‰åœ°åŒºå¼€å‘ä¸€ç§ç¨³å¥çš„è„‘è‚¿ç˜¤åˆ†å‰²ï¼ˆBraTSï¼‰æ–¹æ³•ã€‚ç ”ç©¶é€šè¿‡ä¸‰æ–¹é¢å±•å¼€ï¼šé¦–å…ˆï¼Œç ”ç©¶äº†SSAè®­ç»ƒæ•°æ®å¯¹æ¨¡å‹æœ‰æ•ˆæ€§çš„å½±å“ï¼Œå‘ç°æ— æ˜æ˜¾å½±å“ï¼›å…¶æ¬¡ï¼Œå¯¹æ¯”åˆ†æäº†åŸºäºnnU-Netæ¡†æ¶çš„3Då’Œ2Då…¨åˆ†è¾¨ç‡æ¨¡å‹ï¼ŒäºŒè€…æ€§èƒ½ç›¸ä¼¼ï¼Œè®­ç»ƒæ¨¡å‹è¾¾åˆ°äº”å€äº¤å‰éªŒè¯å¾—åˆ†0.93ï¼›æœ€åï¼Œé’ˆå¯¹SSAéªŒè¯é›†ä¸è¾ƒå¤§çš„BraTSèƒ¶è´¨ç˜¤ï¼ˆGLIï¼‰éªŒè¯é›†ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œæå‡ºäº†ä¸¤ç§ç­–ç•¥ï¼šä½¿ç”¨æœ€ä½³é¢„è®­ç»ƒçš„2D fullresæ¨¡å‹å¯¹SSAæ¡ˆä¾‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶å¼•å…¥ä¸€ç§æ–°å‹çš„åŸºäºç¥ç»é£æ ¼è½¬ç§»çš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚ç ”ç©¶çªå‡ºäº†æé«˜SSAåœ°åŒºè„‘è‚¿ç˜¤é¢„æµ‹æ½œåŠ›çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>SSAåœ°åŒºä½¿ç”¨ä½è´¨é‡MRIæŠ€æœ¯å¼•å‘å¯¹æœºå™¨å­¦ä¹ æ–¹æ³•ä¸´åºŠé€‚ç”¨æ€§çš„è´¨ç–‘ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ä¸‰æ–¹é¢ä¸ºSSAåœ°åŒºå¼€å‘äº†ä¸€ç§ç¨³å¥çš„è„‘è‚¿ç˜¤åˆ†å‰²ï¼ˆBraTSï¼‰æ–¹æ³•ã€‚</li>
<li>SSAè®­ç»ƒæ•°æ®å¯¹æ¨¡å‹æœ‰æ•ˆæ€§çš„å½±å“ç ”ç©¶æœªå‘ç°æ˜¾è‘—æ•ˆæœã€‚</li>
<li>3Då’Œ2Då…¨åˆ†è¾¨ç‡æ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”åˆ†ææ˜¾ç¤ºäºŒè€…ç›¸ä¼¼ï¼Œä¸”å‡è¡¨ç°è‰¯å¥½ã€‚</li>
<li>é’ˆå¯¹SSAéªŒè¯é›†ä¸GLIéªŒè¯é›†é—´çš„æ€§èƒ½å·®è·ï¼Œæå‡ºäº†ä¸¤ç§ç­–ç•¥æ¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„2D fullresæ¨¡å‹å¯¹SSAæ¡ˆä¾‹è¿›è¡Œå¾®è°ƒæ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64e89ce3dd998f05b39a4d608633f23b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66a5dc1b8dd202a6dae34725f7979891.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8911fb7dea7cefb6ff789af56a1159a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbe9ac1ef0f46cbfd28ac30122713098.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e12bbef616fab9c5772cb8bc216cec2a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="K-space-Diffusion-Model-Based-MR-Reconstruction-Method-for-Simultaneous-Multislice-Imaging"><a href="#K-space-Diffusion-Model-Based-MR-Reconstruction-Method-for-Simultaneous-Multislice-Imaging" class="headerlink" title="K-space Diffusion Model Based MR Reconstruction Method for Simultaneous   Multislice Imaging"></a>K-space Diffusion Model Based MR Reconstruction Method for Simultaneous   Multislice Imaging</h2><p><strong>Authors:Ting Zhao, Zhuoxu Cui, Congcong Liu, Xingyang Wu, Yihang Zhou, Dong Liang, Haifeng Wang</strong></p>
<p>Simultaneous Multi-Slice(SMS) is a magnetic resonance imaging (MRI) technique which excites several slices concurrently using multiband radiofrequency pulses to reduce scanning time. However, due to its variable data structure and difficulty in acquisition, it is challenging to integrate SMS data as training data into deep learning frameworks.This study proposed a novel k-space diffusion model of SMS reconstruction that does not utilize SMS data for training. Instead, it incorporates Slice GRAPPA during the sampling process to reconstruct SMS data from different acquisition modes.Our results demonstrated that this method outperforms traditional SMS reconstruction methods and can achieve higher acceleration factors without in-plane aliasing. </p>
<blockquote>
<p>åŒæ­¥å¤šå±‚ï¼ˆSimultaneous Multi-Sliceï¼Œç®€ç§°SMSï¼‰æ˜¯ä¸€ç§ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æŠ€æœ¯ã€‚å®ƒé€šè¿‡å¤šé¢‘å¸¦å°„é¢‘è„‰å†²åŒæ—¶æ¿€å‘å¤šä¸ªå±‚é¢ï¼Œä»è€Œå‡å°‘æ‰«ææ—¶é—´ã€‚ç„¶è€Œï¼Œç”±äºå…¶æ•°æ®ç»“æ„å¤šå˜ä¸”é‡‡é›†éš¾åº¦é«˜ï¼Œå°†SMSæ•°æ®ä½œä¸ºè®­ç»ƒæ•°æ®é›†æˆåˆ°æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„SMSé‡å»ºkç©ºé—´æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸ä½¿ç”¨SMSæ•°æ®è¿›è¡Œè®­ç»ƒã€‚ç›¸åï¼Œå®ƒåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ç»“åˆäº†Slice GRAPPAï¼Œä»ä¸åŒé‡‡é›†æ¨¡å¼é‡å»ºSMSæ•°æ®ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»ŸSMSé‡å»ºæ–¹æ³•ï¼Œä¸”åœ¨ä¸å‡ºç°å¹³é¢æ··å çš„æƒ…å†µä¸‹ï¼Œå¯å®ç°æ›´é«˜çš„åŠ é€Ÿå› å­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03293v2">PDF</a> Accepted at the 2025 IEEE 22nd International Symposium on Biomedical   Imaging (ISBI)</p>
<p><strong>Summary</strong><br>     è¯¥æ–‡æœ¬ä»‹ç»äº†Simultaneous Multi-Sliceï¼ˆSMSï¼‰ç£å…±æŒ¯æˆåƒæŠ€æœ¯åŠå…¶åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„é›†æˆæŒ‘æˆ˜ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„kç©ºé—´æ‰©æ•£æ¨¡å‹ç”¨äºSMSé‡å»ºï¼Œè¯¥æ¨¡å‹ä¸åˆ©ç”¨SMSæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè€Œæ˜¯åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­èå…¥Slice GRAPPAæŠ€æœ¯ï¼Œä»ä¸åŒé‡‡é›†æ¨¡å¼é‡å»ºSMSæ•°æ®ã€‚æ­¤æ–¹æ³•ä¼˜äºä¼ ç»ŸSMSé‡å»ºæ–¹æ³•ï¼Œå¯ä»¥å®ç°æ›´é«˜çš„åŠ é€Ÿå› å­ä¸”æ²¡æœ‰å¹³é¢æ··å é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMSæ˜¯ä¸€ç§ç£å…±æŒ¯æˆåƒæŠ€æœ¯ï¼Œå¯å¹¶è¡Œæ¿€å‘å¤šä¸ªåˆ‡ç‰‡ä»¥å‡å°‘æ‰«ææ—¶é—´ã€‚</li>
<li>SMSæ•°æ®ç”±äºå…¶æ•°æ®ç»“æ„çš„å˜åŒ–å’Œé‡‡é›†éš¾åº¦ï¼Œéš¾ä»¥ä½œä¸ºè®­ç»ƒæ•°æ®é›†æˆåˆ°æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„kç©ºé—´æ‰©æ•£æ¨¡å‹ç”¨äºSMSé‡å»ºï¼Œè¯¥æ¨¡å‹ä¸éœ€è¦ä½¿ç”¨SMSæ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ­¤æ¨¡å‹é€šè¿‡èå…¥Slice GRAPPAæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­é‡å»ºä¸åŒé‡‡é›†æ¨¡å¼çš„SMSæ•°æ®ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»ŸSMSé‡å»ºæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥å®ç°æ›´é«˜çš„åŠ é€Ÿå› å­ï¼Œæé«˜æˆåƒæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03293">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5946111ea1f95d72734c516345b2f0db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22aa40318d9f0fa2b883f57d7e45a9f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c3eefe8af4e777ca1cde3b1570943d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b85ce8e7cd121bc67c268889b6930d21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f65a0f70d67dfb12ff1a59d439836b1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="tCURLoRA-Tensor-CUR-Decomposition-Based-Low-Rank-Parameter-Adaptation-and-Its-Application-in-Medical-Image-Segmentation"><a href="#tCURLoRA-Tensor-CUR-Decomposition-Based-Low-Rank-Parameter-Adaptation-and-Its-Application-in-Medical-Image-Segmentation" class="headerlink" title="tCURLoRA: Tensor CUR Decomposition Based Low-Rank Parameter Adaptation   and Its Application in Medical Image Segmentation"></a>tCURLoRA: Tensor CUR Decomposition Based Low-Rank Parameter Adaptation   and Its Application in Medical Image Segmentation</h2><p><strong>Authors:Guanghua He, Wangang Cheng, Hancan Zhu, Xiaohao Cai, Gaohang Yu</strong></p>
<p>Transfer learning, by leveraging knowledge from pre-trained models, has significantly enhanced the performance of target tasks. However, as deep neural networks scale up, full fine-tuning introduces substantial computational and storage challenges in resource-constrained environments, limiting its widespread adoption. To address this, parameter-efficient fine-tuning (PEFT) methods have been developed to reduce computational complexity and storage requirements by minimizing the number of updated parameters. While matrix decomposition-based PEFT methods, such as LoRA, show promise, they struggle to fully capture the high-dimensional structural characteristics of model weights. In contrast, high-dimensional tensors offer a more natural representation of neural network weights, allowing for a more comprehensive capture of higher-order features and multi-dimensional interactions. In this paper, we propose tCURLoRA, a novel fine-tuning method based on tensor CUR decomposition. By concatenating pre-trained weight matrices into a three-dimensional tensor and applying tensor CUR decomposition, we update only the lower-order tensor components during fine-tuning, effectively reducing computational and storage overhead. Experimental results demonstrate that tCURLoRA outperforms existing PEFT methods in medical image segmentation tasks. </p>
<blockquote>
<p>åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†è¿›è¡Œè¿ç§»å­¦ä¹ å·²æ˜¾è‘—æé«˜ç›®æ ‡ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€æ·±åº¦ç¥ç»ç½‘ç»œè§„æ¨¡çš„æ‰©å¤§ï¼Œå®Œå…¨å¾®è°ƒåœ¨èµ„æºå—é™ç¯å¢ƒä¸­å¸¦æ¥äº†å¤§é‡çš„è®¡ç®—å’Œå­˜å‚¨æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶å¹¿æ³›é‡‡ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼€å‘äº†å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–æ›´æ–°çš„å‚æ•°æ•°é‡æ¥é™ä½è®¡ç®—å¤æ‚æ€§å’Œå­˜å‚¨è¦æ±‚ã€‚è™½ç„¶åŸºäºçŸ©é˜µåˆ†è§£çš„PEFTæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬éš¾ä»¥å®Œå…¨æ•è·æ¨¡å‹æƒé‡çš„é«˜ç»´ç»“æ„ç‰¹å¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé«˜ç»´å¼ é‡æä¾›äº†ç¥ç»ç½‘ç»œæƒé‡çš„æ›´è‡ªç„¶è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°æ•è·é«˜é˜¶ç‰¹å¾å’Œå¤šç»´äº¤äº’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼ é‡CURåˆ†è§£çš„æ–°å‹å¾®è°ƒæ–¹æ³•tCURLoRAã€‚æˆ‘ä»¬å°†é¢„è®­ç»ƒçš„æƒé‡çŸ©é˜µåˆå¹¶æˆä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼Œå¹¶åº”ç”¨å¼ é‡CURåˆ†è§£ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åªæ›´æ–°ä½é˜¶å¼ é‡ç»„ä»¶ï¼Œæœ‰æ•ˆé™ä½è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒtCURLoRAä¼˜äºç°æœ‰çš„PEFTæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.02227v2">PDF</a> </p>
<p><strong>Summary</strong><br>     åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çŸ¥è¯†è¿›è¡Œçš„è¿ç§»å­¦ä¹ å·²æ˜¾è‘—æå‡ç›®æ ‡ä»»åŠ¡æ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€æ·±åº¦ç¥ç»ç½‘ç»œè§„æ¨¡çš„æ‰©å¤§ï¼Œå®Œå…¨å¾®è°ƒåœ¨èµ„æºå—é™ç¯å¢ƒä¸­å¸¦æ¥å·¨å¤§è®¡ç®—å’Œå­˜å‚¨æŒ‘æˆ˜ï¼Œé™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼€å‘äº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œä»¥å‡å°‘è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ï¼Œæœ€å°åŒ–æ›´æ–°å‚æ•°æ•°é‡ã€‚è™½ç„¶åŸºäºçŸ©é˜µåˆ†è§£çš„PEFTæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬éš¾ä»¥å®Œå…¨æ•æ‰æ¨¡å‹æƒé‡çš„é«˜ç»´ç»“æ„ç‰¹å¾ã€‚ç›¸åï¼Œé«˜ç»´å¼ é‡æä¾›æ›´è‡ªç„¶çš„ç¥ç»ç½‘ç»œæƒé‡è¡¨ç¤ºï¼Œèƒ½æ›´å…¨é¢åœ°æ•è·é«˜é˜¶ç‰¹å¾å’Œå¤šç»´äº¤äº’ã€‚æœ¬æ–‡æå‡ºåŸºäºå¼ é‡CURåˆ†è§£çš„æ–°å‹å¾®è°ƒæ–¹æ³•tCURLoRAã€‚é€šè¿‡å°†é¢„è®­ç»ƒæƒé‡çŸ©é˜µæ‹¼æ¥æˆä¸‰ç»´å¼ é‡å¹¶åº”ç”¨å¼ é‡CURåˆ†è§£ï¼Œä»…åœ¨å¾®è°ƒæ—¶æ›´æ–°ä½é˜¶å¼ é‡ç»„ä»¶ï¼Œæœ‰æ•ˆå‡å°‘è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚å®éªŒç»“æœè¯æ˜tCURLoRAåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰PEFTæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿ç§»å­¦ä¹ å€ŸåŠ©é¢„è®­ç»ƒæ¨¡å‹çŸ¥è¯†æå‡äº†ç›®æ ‡ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æ·±åº¦ç¥ç»ç½‘ç»œè§„æ¨¡æ‰©å¤§å¯¼è‡´å®Œå…¨å¾®è°ƒé¢ä¸´è®¡ç®—å’Œå­˜å‚¨æŒ‘æˆ˜ã€‚</li>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•æ—¨åœ¨å‡å°‘è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ã€‚</li>
<li>ç°æœ‰çŸ©é˜µåˆ†è§£æ–¹æ³•å¦‚LoRAéš¾ä»¥æ•æ‰æ¨¡å‹æƒé‡çš„é«˜ç»´ç»“æ„ç‰¹å¾ã€‚</li>
<li>é«˜ç»´å¼ é‡ä¸ºç¥ç»ç½‘ç»œæƒé‡æä¾›æ›´è‡ªç„¶çš„è¡¨ç¤ºï¼Œæœ‰åŠ©äºæ•è·é«˜é˜¶ç‰¹å¾å’Œå¤šç»´äº¤äº’ã€‚</li>
<li>tCURLoRAæ–¹æ³•åŸºäºå¼ é‡CURåˆ†è§£ï¼Œä»…åœ¨å¾®è°ƒæ—¶æ›´æ–°ä½é˜¶å¼ é‡ç»„ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.02227">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92f34ffd9dd87621c54912d089eb0948.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a0aa9dd65aa7434422a9f628f85204f.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ViPOcc-Leveraging-Visual-Priors-from-Vision-Foundation-Models-for-Single-View-3D-Occupancy-Prediction"><a href="#ViPOcc-Leveraging-Visual-Priors-from-Vision-Foundation-Models-for-Single-View-3D-Occupancy-Prediction" class="headerlink" title="ViPOcc: Leveraging Visual Priors from Vision Foundation Models for   Single-View 3D Occupancy Prediction"></a>ViPOcc: Leveraging Visual Priors from Vision Foundation Models for   Single-View 3D Occupancy Prediction</h2><p><strong>Authors:Yi Feng, Yu Han, Xijing Zhang, Tanghui Li, Yanting Zhang, Rui Fan</strong></p>
<p>Inferring the 3D structure of a scene from a single image is an ill-posed and challenging problem in the field of vision-centric autonomous driving. Existing methods usually employ neural radiance fields to produce voxelized 3D occupancy, lacking instance-level semantic reasoning and temporal photometric consistency. In this paper, we propose ViPOcc, which leverages the visual priors from vision foundation models (VFMs) for fine-grained 3D occupancy prediction. Unlike previous works that solely employ volume rendering for RGB and depth image reconstruction, we introduce a metric depth estimation branch, in which an inverse depth alignment module is proposed to bridge the domain gap in depth distribution between VFM predictions and the ground truth. The recovered metric depth is then utilized in temporal photometric alignment and spatial geometric alignment to ensure accurate and consistent 3D occupancy prediction. Additionally, we also propose a semantic-guided non-overlapping Gaussian mixture sampler for efficient, instance-aware ray sampling, which addresses the redundant and imbalanced sampling issue that still exists in previous state-of-the-art methods. Extensive experiments demonstrate the superior performance of ViPOcc in both 3D occupancy prediction and depth estimation tasks on the KITTI-360 and KITTI Raw datasets. Our code is available at: \url{<a target="_blank" rel="noopener" href="https://mias.group/ViPOcc%7D">https://mias.group/ViPOcc}</a>. </p>
<blockquote>
<p>ä»å•å¹…å›¾åƒæ¨æ–­åœºæ™¯çš„ä¸‰ç»´ç»“æ„æ˜¯è§†è§‰è‡ªä¸»é©¾é©¶é¢†åŸŸä¸­ä¸€ä¸ªä¸é€‚å®šä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ç¥ç»è¾å°„åœºæ¥ç”Ÿæˆä½“ç´ åŒ–çš„ä¸‰ç»´å ç”¨ä¿¡æ¯ï¼Œä½†ç¼ºä¹å®ä¾‹çº§åˆ«çš„è¯­ä¹‰æ¨ç†å’Œæš‚æ—¶å…‰åº¦ä¸€è‡´æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ViPOccï¼Œå®ƒåˆ©ç”¨è§†è§‰å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰è¿›è¡Œç²¾ç»†çš„ä¸‰ç»´å ç”¨é¢„æµ‹ã€‚ä¸åŒäºä¹‹å‰ä»…ä½¿ç”¨ä½“ç§¯æ¸²æŸ“è¿›è¡ŒRGBå’Œæ·±åº¦å›¾åƒé‡å»ºçš„å·¥ä½œï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåº¦é‡æ·±åº¦ä¼°è®¡åˆ†æ”¯ï¼Œå¹¶æå‡ºä¸€ä¸ªé€†æ·±åº¦å¯¹é½æ¨¡å—ï¼Œä»¥ç¼©å°VFMé¢„æµ‹å’ŒçœŸå®å€¼ä¹‹é—´æ·±åº¦åˆ†å¸ƒçš„åŸŸå·®è·ã€‚ç„¶åï¼Œæ¢å¤çš„åº¦é‡æ·±åº¦è¢«ç”¨äºæš‚æ—¶å…‰åº¦å¯¹é½å’Œç©ºé—´å‡ ä½•å¯¹é½ï¼Œä»¥ç¡®ä¿å‡†ç¡®ä¸”ä¸€è‡´çš„ä¸‰ç»´å ç”¨é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è¯­ä¹‰å¼•å¯¼çš„éé‡å é«˜æ–¯æ··åˆé‡‡æ ·å™¨ï¼Œç”¨äºé«˜æ•ˆã€å®ä¾‹æ„ŸçŸ¥çš„å°„çº¿é‡‡æ ·ï¼Œè§£å†³äº†å…ˆå‰æœ€å…ˆè¿›æ–¹æ³•ä¸­ä»ç„¶å­˜åœ¨çš„å†—ä½™å’Œä¸å¹³è¡¡çš„é‡‡æ ·é—®é¢˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒViPOccåœ¨KITTI-360å’ŒKITTI Rawæ•°æ®é›†ä¸Šçš„ä¸‰ç»´å ç”¨é¢„æµ‹å’Œæ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š[<a target="_blank" rel="noopener" href="https://mias.group/ViPOcc]%E8%8E%B7%E5%8F%96%E3%80%82">https://mias.group/ViPOcc]è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11210v2">PDF</a> accepted to AAAI25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè§†è§‰å…ˆéªŒçš„ç²¾ç»†ç²’åº¦ä¸‰ç»´å ç”¨é¢„æµ‹æ–¹æ³•ViPOccï¼Œè§£å†³äº†å•ä¸€å›¾åƒæ¨æ–­ä¸‰ç»´åœºæ™¯ç»“æ„çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å¼•å…¥è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰æ¥æé«˜æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ï¼Œé€šè¿‡å¼•å…¥é€†æ·±åº¦å¯¹é½æ¨¡å—æ¥ç¼©å°VFMé¢„æµ‹ä¸çœŸå®æ·±åº¦ä¹‹é—´çš„åŸŸå·®è·ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†è¯­ä¹‰å¼•å¯¼çš„éé‡å é«˜æ–¯æ··åˆé‡‡æ ·å™¨ï¼Œè§£å†³äº†ä¹‹å‰çš„å†—ä½™å’Œä¸å¹³è¡¡é‡‡æ ·é—®é¢˜ã€‚åœ¨KITTI-360å’ŒKITTI Rawæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒViPOccåœ¨ä¸‰ç»´å ç”¨é¢„æµ‹å’Œæ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViPOccè§£å†³äº†ä»å•ä¸€å›¾åƒæ¨æ–­ä¸‰ç»´åœºæ™¯ç»“æ„çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>æ–¹æ³•åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰æé«˜æ·±åº¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥é€†æ·±åº¦å¯¹é½æ¨¡å—æ¥ç¼©å°VFMé¢„æµ‹ä¸çœŸå®æ·±åº¦ä¹‹é—´çš„åŸŸå·®è·ã€‚</li>
<li>æå‡ºäº†è¯­ä¹‰å¼•å¯¼çš„éé‡å é«˜æ–¯æ··åˆé‡‡æ ·å™¨ï¼Œè§£å†³å†—ä½™å’Œä¸å¹³è¡¡é‡‡æ ·é—®é¢˜ã€‚</li>
<li>ViPOccåœ¨KITTI-360å’ŒKITTI Rawæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•ç¡®ä¿äº†å‡†ç¡®ä¸”ä¸€è‡´çš„ä¸‰ç»´å ç”¨é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-be951ad7fdae83c121ebe4a00d034b9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cb1161d5887d3e30e2b4b7e296f11c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a05f7cf90808059f4cb7561eb4baab6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c44885d55730f0c014ad8c9d9a78af7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Efficient-Progressive-Image-Compression-with-Variance-aware-Masking"><a href="#Efficient-Progressive-Image-Compression-with-Variance-aware-Masking" class="headerlink" title="Efficient Progressive Image Compression with Variance-aware Masking"></a>Efficient Progressive Image Compression with Variance-aware Masking</h2><p><strong>Authors:Alberto Presta, Enzo Tartaglione, Attilio Fiandrotti, Marco Grangetto, Pamela Cosman</strong></p>
<p>Learned progressive image compression is gaining momentum as it allows improved image reconstruction as more bits are decoded at the receiver. We propose a progressive image compression method in which an image is first represented as a pair of base-quality and top-quality latent representations. Next, a residual latent representation is encoded as the element-wise difference between the top and base representations. Our scheme enables progressive image compression with element-wise granularity by introducing a masking system that ranks each element of the residual latent representation from most to least important, dividing it into complementary components, which can be transmitted separately to the decoder in order to obtain different reconstruction quality. The masking system does not add further parameters nor complexity. At the receiver, any elements of the top latent representation excluded from the transmitted components can be independently replaced with the mean predicted by the hyperprior architecture, ensuring reliable reconstructions at any intermediate quality level. We also introduced Rate Enhancement Modules (REMs), which refine the estimation of entropy parameters using already decoded components. We obtain results competitive with state-of-the-art competitors, while significantly reducing computational complexity, decoding time, and number of parameters. </p>
<blockquote>
<p>æ¸è¿›å¼åŒ»å­¦å›¾åƒå‹ç¼©æŠ€æœ¯æ­£é€æ¸å—åˆ°å…³æ³¨ï¼Œå› ä¸ºå®ƒåœ¨æ¥æ”¶ç«¯è§£ç æ›´å¤šä½æ—¶ï¼Œèƒ½å¤Ÿå®ç°æ›´å¥½çš„å›¾åƒé‡å»ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›å¼å›¾åƒå‹ç¼©æ–¹æ³•ï¼Œé¦–å…ˆï¼Œå°†å›¾åƒè¡¨ç¤ºä¸ºä¸€å¯¹åŸºç¡€è´¨é‡å’Œé¡¶çº§è´¨é‡çš„æ½œåœ¨è¡¨ç¤ºã€‚ç„¶åï¼Œå°†æ®‹å·®æ½œåœ¨è¡¨ç¤ºç¼–ç ä¸ºé¡¶çº§å’ŒåŸºç¡€è¡¨ç¤ºä¹‹é—´çš„å…ƒç´ å·®å¼‚ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªå¯¹æ®‹å·®æ½œåœ¨è¡¨ç¤ºä¸­çš„æ¯ä¸ªå…ƒç´ è¿›è¡Œé‡è¦æ€§æ’åºçš„æ©ç ç³»ç»Ÿï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆå®ç°äº†å…·æœ‰å…ƒç´ çº§ç²’åº¦çš„æ¸è¿›å¼å›¾åƒå‹ç¼©ã€‚æ©ç ç³»ç»Ÿåˆ†ä¸ºä¸åŒçš„è¡¥å……ç»„ä»¶è¿›è¡Œä¼ è¾“ï¼Œè¿™äº›ç»„ä»¶å¯ä»¥å•ç‹¬ä¼ è¾“åˆ°è§£ç å™¨ä»¥è·å¾—ä¸åŒçš„é‡å»ºè´¨é‡ã€‚æ©ç ç³»ç»Ÿä¸ä¼šå¢åŠ é¢å¤–çš„å‚æ•°æˆ–å¤æ‚æ€§ã€‚åœ¨æ¥æ”¶ç«¯ï¼Œé¡¶çº§æ½œåœ¨è¡¨ç¤ºä¸­è¢«æ’é™¤çš„ä»»ä½•å…ƒç´ éƒ½å¯ä»¥ç‹¬ç«‹æ›¿æ¢ä¸ºè¶…å…ˆéªŒæ¶æ„é¢„æµ‹çš„å‡å€¼ï¼Œç¡®ä¿åœ¨ä»»ä½•ä¸­é—´è´¨é‡æ°´å¹³ä¸‹éƒ½èƒ½å®ç°å¯é çš„é‡å»ºã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†é€Ÿç‡å¢å¼ºæ¨¡å—ï¼ˆREMï¼‰ï¼Œå®ƒä½¿ç”¨å·²è§£ç çš„ç»„ä»¶æ¥æ”¹è¿›ç†µå‚æ•°çš„ä¼°è®¡ã€‚æˆ‘ä»¬çš„ç»“æœä¸ç°ä»£ç«äº‰å¯¹æ‰‹ç›¸å½“ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€è§£ç æ—¶é—´å’Œå‚æ•°æ•°é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10185v3">PDF</a> 9 pages. Accepted at WACV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ¸è¿›å¼å›¾åƒå‹ç¼©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å›¾åƒè¡¨ç¤ºä¸ºåŸºç¡€è´¨é‡å±‚å’Œé¡¶çº§è´¨é‡å±‚çš„æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶ç¼–ç å‰©ä½™æ½œåœ¨è¡¨ç¤ºä¸ºä¸¤è€…ä¹‹é—´çš„å·®å¼‚ã€‚å¼•å…¥äº†ä¸€ä¸ªæ©ç ç³»ç»Ÿï¼ŒæŒ‰é‡è¦æ€§å¯¹å‰©ä½™æ½œåœ¨è¡¨ç¤ºä¸­çš„æ¯ä¸ªå…ƒç´ è¿›è¡Œæ’åï¼Œå°†å…¶åˆ†è§£ä¸ºå¯å•ç‹¬ä¼ è¾“çš„äº’è¡¥ç»„ä»¶ï¼Œä»¥è·å¾—ä¸åŒçš„é‡å»ºè´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ç‡å¢å¼ºæ¨¡å—(REMs)æ”¹è¿›äº†ç†µå‚æ•°çš„ä¼°è®¡ï¼Œè·å¾—äº†ä¸æœ€æ–°ç«äº‰å¯¹æ‰‹ç›¸å½“çš„ç»“æœï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€è§£ç æ—¶é—´å’Œå‚æ•°æ•°é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¸è¿›å¼å›¾åƒå‹ç¼©æ–¹æ³•å…è®¸åœ¨æ¥æ”¶ç«¯è§£ç æ›´å¤šä½æ—¶æ”¹è¿›å›¾åƒé‡å»ºã€‚</li>
<li>å›¾åƒè¢«è¡¨ç¤ºä¸ºåŸºç¡€è´¨é‡å±‚å’Œé¡¶çº§è´¨é‡å±‚çš„æ½œåœ¨è¡¨ç¤ºï¼Œå‰©ä½™æ½œåœ¨è¡¨ç¤ºè¢«ç¼–ç ä¸ºä¸¤è€…ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>å¼•å…¥æ©ç ç³»ç»Ÿï¼ŒæŒ‰é‡è¦æ€§å¯¹å‰©ä½™æ½œåœ¨è¡¨ç¤ºä¸­çš„å…ƒç´ è¿›è¡Œæ’åï¼Œå¹¶åˆ†è§£ä¸ºå¯å•ç‹¬ä¼ è¾“çš„äº’è¡¥ç»„ä»¶ã€‚</li>
<li>æ©ç ç³»ç»Ÿä¸ä¼šå¢åŠ é¢å¤–çš„å‚æ•°æˆ–å¤æ‚æ€§ã€‚</li>
<li>ä½¿ç”¨è¶…å…ˆéªŒæ¶æ„é¢„æµ‹çš„å‡å€¼ç‹¬ç«‹æ›¿æ¢é¡¶çº§æ½œåœ¨è¡¨ç¤ºä¸­æœªä¼ è¾“çš„ç»„ä»¶å…ƒç´ ï¼Œç¡®ä¿ä»»ä½•ä¸­é—´è´¨é‡æ°´å¹³çš„å¯é é‡å»ºã€‚</li>
<li>å¼•å…¥ç‡å¢å¼ºæ¨¡å—(REMs)æ”¹è¿›äº†ç†µå‚æ•°çš„ä¼°è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-62db592c8738f3f836c9f8f0d5932809.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ccd3795f106840bada607d32a399ca55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed11f591773a3634c9eaa5305ffe554c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-699f44cdf67e97086183447701075869.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f82332a5ebf3d7e3f1768166a384ada8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4e7b1a161721dcf1f5a5ffb83f10acf.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Foundations-of-Adaptive-High-Level-Tight-Control-of-Prostate-Cancer-A-Path-from-From-Terminal-Disease-to-Chronic-Condition"><a href="#Foundations-of-Adaptive-High-Level-Tight-Control-of-Prostate-Cancer-A-Path-from-From-Terminal-Disease-to-Chronic-Condition" class="headerlink" title="Foundations of Adaptive High-Level Tight Control of Prostate Cancer: A   Path from From Terminal Disease to Chronic Condition"></a>Foundations of Adaptive High-Level Tight Control of Prostate Cancer: A   Path from From Terminal Disease to Chronic Condition</h2><p><strong>Authors:Trung V. Phan, Shengkai Li, Benjamin Howe, Sarah R. Amend, Kenneth J. Pienta, Joel S. Brown, Robert A. Gatenby, Constantine Frangakis, Robert H. Austin, Ioannis G. Keverkidis</strong></p>
<p>Metastatic prostate cancer is one of the leading causes of cancer-related morbidity and mortality worldwide. It is characterized by a high mortality rate and a poor prognosis. In this work, we explore how a clinical oncologist can apply a Stackelberg game-theoretic framework to prolong metastatic prostate cancer survival, or even make it chronic in duration. We utilize a Bayesian optimization approach to identify the optimal adaptive chemotherapeutic treatment policy for a single drug (Abiraterone) to maximize the time before the patient begins to show symptoms. We show that, with precise adaptive optimization of drug delivery, it is possible to significantly prolong the cancer suppression period, potentially converting metastatic prostate cancer from a terminal disease to a chronic disease for most patients, as supported by clinical and analytical evidence. We suggest that clinicians might explore the possibility of implementing a high-level tight control (HLTC) treatment, in which the trigger signals (i.e. biomarker levels) for drug administration and cessation are both high and close together, typically yield the best outcomes, as demonstrated through both computation and theoretical analysis. This simple insight could serve as a valuable guide for improving current adaptive chemotherapy treatments in other hormone-sensitive cancers. </p>
<blockquote>
<p>è½¬ç§»æ€§å‰åˆ—è…ºç™Œæ˜¯å…¨çƒèŒƒå›´å†…å¯¼è‡´ç™Œç—‡ç›¸å…³å‘ç—…ç‡å’Œæ­»äº¡ç‡çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚å…¶ç‰¹ç‚¹æ˜¯æ­»äº¡ç‡é«˜ï¼Œé¢„åä¸è‰¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä¸´åºŠè‚¿ç˜¤ç§‘åŒ»ç”Ÿå¦‚ä½•è¿ç”¨æ–¯å¡”å…‹å°”ä¼¯æ ¼åšå¼ˆç†è®ºæ¡†æ¶æ¥å»¶é•¿è½¬ç§»æ€§å‰åˆ—è…ºç™Œæ‚£è€…çš„ç”Ÿå­˜æœŸï¼Œç”šè‡³ä½¿å…¶è½¬å˜ä¸ºæ…¢æ€§ç—…ã€‚æˆ‘ä»¬é‡‡ç”¨è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•æ¥ç¡®å®šé’ˆå¯¹å•ä¸€è¯ç‰©ï¼ˆé˜¿æ¯”ç‰¹é¾™ï¼‰çš„æœ€ä½³è‡ªé€‚åº”åŒ–ç–—æ²»ç–—æ–¹æ¡ˆï¼Œä»¥æœ€å¤§ç¨‹åº¦åœ°å»¶é•¿æ‚£è€…å¼€å§‹è¡¨ç°å‡ºç—‡çŠ¶ä¹‹å‰çš„æ—¶é—´ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡ç²¾ç¡®çš„è‡ªé€‚åº”è¯ç‰©ä¼˜åŒ–ï¼Œå¯ä»¥æ˜¾è‘—å»¶é•¿ç™Œç—‡æŠ‘åˆ¶æœŸï¼Œä»è€Œå°†å¤§å¤šæ•°æ‚£è€…çš„è½¬ç§»æ€§å‰åˆ—è…ºç™Œä»ä¸€ç§è‡´å‘½ç–¾ç—…è½¬å˜ä¸ºä¸€ç§æ…¢æ€§ç–¾ç—…ï¼Œè¿™å¾—åˆ°äº†ä¸´åºŠå’Œåˆ†æè¯æ®çš„æ”¯æŒã€‚æˆ‘ä»¬å»ºè®®ä¸´åºŠåŒ»ç”Ÿå¯èƒ½æ¢ç´¢å®æ–½é«˜çº§ç´§å¯†æ§åˆ¶ï¼ˆHLTCï¼‰æ²»ç–—çš„å¯èƒ½æ€§ï¼Œå…¶ä¸­è¯ç‰©æ²»ç–—å’Œåœè¯çš„è§¦å‘ä¿¡å·ï¼ˆå³ç”Ÿç‰©æ ‡å¿—ç‰©æ°´å¹³ï¼‰æ—¢é«˜åˆç´§å¯†ç›¸å…³ï¼Œé€šå¸¸ä¼šäº§ç”Ÿæœ€ä½³æ•ˆæœï¼Œè¿™å·²é€šè¿‡è®¡ç®—å’Œç†è®ºåˆ†æå¾—åˆ°è¯æ˜ã€‚è¿™ä¸€ç®€å•è§è§£å¯ä»¥ä¸ºæ”¹è¿›å…¶ä»–æ¿€ç´ æ•æ„Ÿç™Œç—‡çš„å½“å‰è‡ªé€‚åº”åŒ–ç–—æ²»ç–—æä¾›æœ‰ä»·å€¼çš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16005v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†ä¸´åºŠè‚¿ç˜¤å­¦å®¶å¦‚ä½•è¿ç”¨æ–¯å¡”å…‹å°”ä¼¯æ ¼åšå¼ˆç†è®ºæ¡†æ¶æ¥å»¶é•¿è½¬ç§»æ€§å‰åˆ—è…ºç™Œæ‚£è€…çš„ç”Ÿå­˜æœŸï¼Œç”šè‡³ä½¿å…¶æˆä¸ºä¸€ç§æ…¢æ€§ç—…ã€‚ç ”ç©¶é‡‡ç”¨è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•ï¼Œé’ˆå¯¹å•ä¸€è¯ç‰©é˜¿æ¯”ç‰¹é¾™ï¼ˆAbirateroneï¼‰åˆ¶å®šæœ€ä½³é€‚åº”æ€§åŒ–ç–—æ–¹æ¡ˆï¼Œä»¥å»¶é•¿æ‚£è€…æ— ç—‡çŠ¶æœŸæ—¶é—´ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç²¾ç¡®çš„è¯ç‰©è¾“é€é€‚åº”æ€§ä¼˜åŒ–ï¼Œå¯ä»¥æ˜¾è‘—å»¶é•¿ç™Œç—‡æŠ‘åˆ¶æœŸï¼Œå°†è½¬ç§»æ€§å‰åˆ—è…ºç™Œä»è‡´å‘½ç–¾ç—…è½¬å˜ä¸ºå¤§å¤šæ•°æ‚£è€…çš„æ…¢æ€§ç–¾ç—…ã€‚å»ºè®®ä¸´åºŠåŒ»å¸ˆè€ƒè™‘å®æ–½é«˜çº§ç´§å¯†æ§åˆ¶ï¼ˆHLTCï¼‰æ²»ç–—ï¼Œå…¶ä¸­è¯ç‰©ç»™è¯å’Œåœè¯çš„è§¦å‘ä¿¡å·æ¥è¿‘ï¼Œé€šå¸¸èƒ½å–å¾—æœ€ä½³æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¬ç§»æ€§å‰åˆ—è…ºç™Œæ˜¯å…¨çƒç™Œç—‡å‘ç—…ç‡å’Œæ­»äº¡ç‡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå…·æœ‰é«˜åº¦çš„è‡´æ­»ç‡å’Œä¸è‰¯çš„é¢„åã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ–¯å¡”å…‹å°”ä¼¯æ ¼åšå¼ˆç†è®ºæ¡†æ¶å’Œè´å¶æ–¯ä¼˜åŒ–æ–¹æ³•ï¼Œæ¢ç´¢å¦‚ä½•å»¶é•¿è½¬ç§»æ€§å‰åˆ—è…ºç™Œæ‚£è€…çš„ç”Ÿå­˜æœŸã€‚</li>
<li>é€šè¿‡ç²¾ç¡®çš„è¯ç‰©è¾“é€é€‚åº”æ€§ä¼˜åŒ–ï¼Œä½¿ç”¨å•ä¸€è¯ç‰©é˜¿æ¯”ç‰¹é¾™ï¼ˆAbirateroneï¼‰å¯ä»¥æ˜¾è‘—å»¶é•¿ç™Œç—‡æŠ‘åˆ¶æœŸã€‚</li>
<li>å°†è½¬ç§»æ€§å‰åˆ—è…ºç™Œä»è‡´å‘½ç–¾ç—…è½¬å˜ä¸ºæ…¢æ€§ç–¾ç—…æ˜¯å¯èƒ½çš„ã€‚</li>
<li>å®æ–½é«˜çº§ç´§å¯†æ§åˆ¶ï¼ˆHLTCï¼‰æ²»ç–—å¯èƒ½å–å¾—æœ€ä½³æ²»ç–—æ•ˆæœï¼Œå…¶ä¸­è¯ç‰©ç»™è¯å’Œåœè¯çš„è§¦å‘ä¿¡å·æ¥è¿‘ã€‚</li>
<li>è¿™ç§æ²»ç–—æ–¹æ³•å¯èƒ½å¯¹å…¶ä»–æ¿€ç´ æ•æ„Ÿçš„ç™Œç—‡çš„é€‚åº”æ€§åŒ–ç–—æ²»ç–—æœ‰æ”¹è¿›ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9a09ab7105aca2e903f1a5752592624.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0aed79244dc8160aa965b62513bf15d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MiM-Mask-in-Mask-Self-Supervised-Pre-Training-for-3D-Medical-Image-Analysis"><a href="#MiM-Mask-in-Mask-Self-Supervised-Pre-Training-for-3D-Medical-Image-Analysis" class="headerlink" title="MiM: Mask in Mask Self-Supervised Pre-Training for 3D Medical Image   Analysis"></a>MiM: Mask in Mask Self-Supervised Pre-Training for 3D Medical Image   Analysis</h2><p><strong>Authors:Jiaxin Zhuang, Linshan Wu, Qiong Wang, Peng Fei, Varut Vardhanabhuti, Lin Luo, Hao Chen</strong></p>
<p>The Vision Transformer (ViT) has demonstrated remarkable performance in Self-Supervised Learning (SSL) for 3D medical image analysis. Masked AutoEncoder (MAE) for feature pre-training can further unleash the potential of ViT on various medical vision tasks. However, due to large spatial sizes with much higher dimensions of 3D medical images, the lack of hierarchical design for MAE may hinder the performance of downstream tasks. In this paper, we propose a novel \textit{Mask in Mask (MiM)} pre-training framework for 3D medical images, which aims to advance MAE by learning discriminative representation from hierarchical visual tokens across varying scales. We introduce multiple levels of granularity for masked inputs from the volume, which are then reconstructed simultaneously ranging at both fine and coarse levels. Additionally, a cross-level alignment mechanism is applied to adjacent level volumes to enforce anatomical similarity hierarchically. Furthermore, we adopt a hybrid backbone to enhance the hierarchical representation learning efficiently during the pre-training. MiM was pre-trained on a large scale of available 3D volumetric images, \textit{i.e.,} Computed Tomography (CT) images containing various body parts. Extensive experiments on thirteen public datasets demonstrate the superiority of MiM over other SSL methods in organ&#x2F;lesion&#x2F;tumor segmentation and disease classification. We further scale up the MiM to large pre-training datasets with more than 10k volumes, showing that large-scale pre-training can further enhance the performance of downstream tasks. The improvement also concluded that the research community should pay more attention to the scale of the pre-training dataset towards the healthcare foundation model for 3D medical images. </p>
<blockquote>
<p>Vision Transformerï¼ˆViTï¼‰åœ¨ç”¨äº3DåŒ»å­¦å›¾åƒåˆ†æçš„Self-Supervised Learningï¼ˆSSLï¼‰ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ä½¿ç”¨Masked AutoEncoderï¼ˆMAEï¼‰è¿›è¡Œç‰¹å¾é¢„è®­ç»ƒå¯ä»¥è¿›ä¸€æ­¥é‡Šæ”¾ViTåœ¨å„ç§åŒ»å­¦è§†è§‰ä»»åŠ¡ä¸Šçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äº3DåŒ»å­¦å›¾åƒå…·æœ‰è¾ƒå¤§çš„ç©ºé—´å°ºå¯¸å’Œæ›´é«˜çš„ç»´åº¦ï¼ŒMAEç¼ºä¹å±‚æ¬¡è®¾è®¡å¯èƒ½ä¼šé˜»ç¢ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹3DåŒ»å­¦å›¾åƒæå‡ºäº†ä¸€ç§æ–°å‹çš„\textit{Mask in Maskï¼ˆMiMï¼‰é¢„è®­ç»ƒæ¡†æ¶}ï¼Œæ—¨åœ¨é€šè¿‡ä»å„ç§å°ºåº¦çš„åˆ†å±‚è§†è§‰æ ‡è®°ä¸­å­¦ä¹ åˆ¤åˆ«è¡¨ç¤ºæ¥æ”¹è¿›MAEã€‚æˆ‘ä»¬ä»ä½“ç§¯ä¸­å¼•å…¥äº†å¤šä¸ªç²’åº¦çš„é®æŒ¡è¾“å…¥ï¼Œç„¶ååœ¨ç²¾ç»†å’Œç²—ç•¥çº§åˆ«ä¸ŠåŒæ—¶è¿›è¡Œé‡å»ºã€‚æ­¤å¤–ï¼Œè¿˜åº”ç”¨äº†è·¨çº§å¯¹é½æœºåˆ¶æ¥å¯¹ç›¸é‚»çº§åˆ«ä½“ç§¯è¿›è¡Œåˆ†å±‚è§£å‰–ç›¸ä¼¼æ€§å¼ºåˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨æ··åˆéª¨å¹²ç½‘åœ¨é¢„è®­ç»ƒæœŸé—´æœ‰æ•ˆåœ°å¢å¼ºåˆ†å±‚è¡¨ç¤ºå­¦ä¹ ã€‚MiMåœ¨å¤§é‡å¯ç”¨çš„3Dä½“ç§¯å›¾åƒä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå³åŒ…å«å„ç§èº«ä½“éƒ¨ä½çš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒã€‚åœ¨åä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMiMåœ¨å™¨å®˜&#x2F;ç—…ç¶&#x2F;è‚¿ç˜¤åˆ†å‰²å’Œç–¾ç—…åˆ†ç±»æ–¹é¢ä¼˜äºå…¶ä»–SSLæ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†MiMæ‰©å±•åˆ°å…·æœ‰è¶…è¿‡10kä½“ç§¯çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ï¼Œç»“æœè¡¨æ˜å¤§è§„æ¨¡é¢„è®­ç»ƒå¯ä»¥è¿›ä¸€æ­¥æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ”¹è¿›ç»“æœä¹Ÿè¡¨æ˜ï¼Œç ”ç©¶ç•Œåº”æ›´åŠ å…³æ³¨é¢„è®­ç»ƒæ•°æ®é›†è§„æ¨¡å¯¹äºæ„å»ºé¢å‘3DåŒ»å­¦å›¾åƒçš„å«ç”Ÿå¥åº·åŸºç¡€æ¨¡å‹çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15580v2">PDF</a> submitted to a journal, updated v2</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹3DåŒ»å­¦å›¾åƒè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„Mask in Maskï¼ˆMiMï¼‰é¢„è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡ä»å¤šå±‚æ¬¡è§†è§‰æ ‡è®°ä¸­å­¦ä¹ åˆ¤åˆ«è¡¨ç¤ºæ¥æ”¹è¿›Mask AutoEncoderï¼ˆMAEï¼‰åœ¨å¤šç§åŒ»å­¦è§†è§‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚MiMå¼•å…¥å¤šä¸ªçº§åˆ«çš„ç²’åº¦å¯¹ä½“ç§¯è¿›è¡Œæ©ç è¾“å…¥ï¼Œå¹¶åœ¨ç²¾ç»†å’Œç²—ç•¥çº§åˆ«ä¸Šè¿›è¡ŒåŒæ—¶é‡å»ºã€‚æ­¤å¤–ï¼Œè¿˜åº”ç”¨äº†è·¨çº§åˆ«å¯¹é½æœºåˆ¶æ¥å¢å¼ºç›¸é‚»çº§åˆ«ä½“ç§¯çš„è§£å‰–ç›¸ä¼¼æ€§ã€‚MiMåœ¨å¤§é‡å¯ç”¨çš„3Dä½“ç§¯å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨å™¨å®˜&#x2F;ç—…å˜&#x2F;è‚¿ç˜¤åˆ†å‰²å’Œç–¾ç—…åˆ†ç±»ç­‰åä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜äºå…¶ä»–SSLæ–¹æ³•çš„æ€§èƒ½ã€‚å¤§è§„æ¨¡é¢„è®­ç»ƒå¯è¿›ä¸€æ­¥æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer (ViT)åœ¨è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä¸­å±•ç°å‡ºå¯¹3DåŒ»å­¦å›¾åƒåˆ†æçš„å‡ºè‰²æ€§èƒ½ã€‚</li>
<li>Masked AutoEncoder (MAE)èƒ½å¤Ÿè¿›ä¸€æ­¥é‡Šæ”¾ViTåœ¨å„ç§åŒ»å­¦è§†è§‰ä»»åŠ¡ä¸Šçš„æ½œåŠ›ã€‚</li>
<li>ç”±äº3DåŒ»å­¦å›¾åƒçš„ç©ºé—´å°ºå¯¸å¤§ä¸”ç»´åº¦é«˜ï¼ŒMAEç¼ºä¹å±‚æ¬¡è®¾è®¡å¯èƒ½é˜»ç¢ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†Mask in Mask (MiM)é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä»å¤šå±‚æ¬¡è§†è§‰æ ‡è®°ä¸­å­¦ä¹ æ¥æé«˜MAEçš„æ€§èƒ½ã€‚</li>
<li>MiMå¼•å…¥å¤šä¸ªçº§åˆ«çš„ç²’åº¦å¯¹ä½“ç§¯è¿›è¡Œæ©ç è¾“å…¥ï¼Œå¹¶åœ¨ç²¾ç»†å’Œç²—ç•¥çº§åˆ«ä¸Šè¿›è¡Œé‡å»ºã€‚</li>
<li>MiMé‡‡ç”¨è·¨çº§åˆ«å¯¹é½æœºåˆ¶æ¥å¢å¼ºç›¸é‚»çº§åˆ«ä½“ç§¯çš„è§£å‰–ç›¸ä¼¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a5281ff02d56e77715c61624e4c1f79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b93c51c0bab868e4a205ccce9cad853.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72624bbf664723822721eab31e4513c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3e9bcec7279e1b9d213b22d9e7d6d92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-148b151ed41b88d0cb7838ded114877f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c9ac97b1459d1a576c9a911a8d6098e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a5dd8467341555d03e377166d4f53cfd.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-14  Low-Resource Text-to-Speech Synthesis Using Noise-Augmented Training of   ForwardTacotron
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-14/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cc41d6a4994aa580d50294bdd66e41dc.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-14  AgroGPT Efficient Agricultural Vision-Language Model with Expert Tuning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
