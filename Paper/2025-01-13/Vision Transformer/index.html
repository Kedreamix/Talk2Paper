<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-13  Language-Inspired Relation Transfer for Few-shot Class-Incremental   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e7b68b4b581daa9a71e1ee5ccfcd19cd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    35 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-13-æ›´æ–°"><a href="#2025-01-13-æ›´æ–°" class="headerlink" title="2025-01-13 æ›´æ–°"></a>2025-01-13 æ›´æ–°</h1><h2 id="Language-Inspired-Relation-Transfer-for-Few-shot-Class-Incremental-Learning"><a href="#Language-Inspired-Relation-Transfer-for-Few-shot-Class-Incremental-Learning" class="headerlink" title="Language-Inspired Relation Transfer for Few-shot Class-Incremental   Learning"></a>Language-Inspired Relation Transfer for Few-shot Class-Incremental   Learning</h2><p><strong>Authors:Yifan Zhao, Jia Li, Zeyin Song, Yonghong Tian</strong></p>
<p>Depicting novel classes with language descriptions by observing few-shot samples is inherent in human-learning systems. This lifelong learning capability helps to distinguish new knowledge from old ones through the increase of open-world learning, namely Few-Shot Class-Incremental Learning (FSCIL). Existing works to solve this problem mainly rely on the careful tuning of visual encoders, which shows an evident trade-off between the base knowledge and incremental ones. Motivated by human learning systems, we propose a new Language-inspired Relation Transfer (LRT) paradigm to understand objects by joint visual clues and text depictions, composed of two major steps. We first transfer the pretrained text knowledge to the visual domains by proposing a graph relation transformation module and then fuse the visual and language embedding by a text-vision prototypical fusion module. Second, to mitigate the domain gap caused by visual finetuning, we propose context prompt learning for fast domain alignment and imagined contrastive learning to alleviate the insufficient text data during alignment. With collaborative learning of domain alignments and text-image transfer, our proposed LRT outperforms the state-of-the-art models by over $13%$ and $7%$ on the final session of mini-ImageNet and CIFAR-100 FSCIL benchmarks. </p>
<blockquote>
<p>é€šè¿‡è§‚å¯Ÿå°‘æ•°æ ·æœ¬å¹¶ç”¨è¯­è¨€æè¿°æ¥æç»˜æ–°å‹ç±»åˆ«æ˜¯äººç±»å­¦ä¹ ç³»ç»Ÿçš„å›ºæœ‰èƒ½åŠ›ã€‚è¿™ç§ç»ˆèº«å­¦ä¹ èƒ½åŠ›æœ‰åŠ©äºé€šè¿‡å¼€æ”¾ä¸–ç•Œå­¦ä¹ çš„å¢åŠ æ¥åŒºåˆ†æ–°æ—§çŸ¥è¯†ï¼Œå³æ‰€è°“çš„å°‘æ•°ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„ç°æœ‰å·¥ä½œä¸»è¦ä¾èµ–äºè§†è§‰ç¼–ç å™¨çš„ç²¾ç»†è°ƒæ•´ï¼Œè¿™æ˜¾ç¤ºå‡ºåŸºç¡€çŸ¥è¯†å’Œå¢é‡çŸ¥è¯†ä¹‹é—´çš„æ˜æ˜¾æƒè¡¡ã€‚å—äººç±»å­¦ä¹ ç³»ç»Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€å¯å‘å…³ç³»è½¬ç§»ï¼ˆLRTï¼‰èŒƒå¼ï¼Œé€šè¿‡è”åˆè§†è§‰çº¿ç´¢å’Œæ–‡æœ¬æè¿°æ¥ç†è§£å¯¹è±¡ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ­¥éª¤ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºå›¾å…³ç³»è½¬æ¢æ¨¡å—å°†é¢„è®­ç»ƒçš„æ–‡æœ¬çŸ¥è¯†è½¬ç§»åˆ°è§†è§‰é¢†åŸŸï¼Œç„¶åé€šè¿‡æ–‡æœ¬è§†è§‰åŸå‹èåˆæ¨¡å—èåˆè§†è§‰å’Œè¯­è¨€åµŒå…¥ã€‚å…¶æ¬¡ï¼Œä¸ºäº†ç¼“è§£å› è§†è§‰å¾®è°ƒè€Œäº§ç”Ÿçš„é¢†åŸŸå·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡æç¤ºå­¦ä¹ æ¥è¿›è¡Œå¿«é€Ÿé¢†åŸŸå¯¹é½å’Œæƒ³è±¡ä¸­çš„å¯¹æ¯”å­¦ä¹ ï¼Œä»¥ç¼“è§£å¯¹é½è¿‡ç¨‹ä¸­çš„æ–‡æœ¬æ•°æ®ä¸è¶³ã€‚é€šè¿‡é¢†åŸŸå¯¹é½å’Œæ–‡æœ¬å›¾åƒè½¬ç§»çš„åä½œå­¦ä¹ ï¼Œæˆ‘ä»¬æå‡ºçš„LRTåœ¨mini-ImageNetå’ŒCIFAR-100 FSCILåŸºå‡†æµ‹è¯•çš„æœ€åä¸€æœŸåˆ†åˆ«ä¼˜äºæœ€æ–°æ¨¡å‹è¶…è¿‡13%å’Œ7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05862v1">PDF</a> Accepted by IEEE TPAMI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†äººç±»å­¦ä¹ ç³»ç»Ÿä¸­çš„ä¸€ç§èƒ½åŠ›ï¼šé€šè¿‡è§‚å¯Ÿå°‘é‡æ ·æœ¬è¿›è¡Œè¯­è¨€æè¿°æ¥æç»˜æ–°ç±»åˆ«ã€‚è¿™ç§ç»ˆèº«å­¦ä¹ èƒ½åŠ›æœ‰åŠ©äºé€šè¿‡å¼€æ”¾ä¸–ç•Œå­¦ä¹ çš„å¢åŠ æ¥åŒºåˆ†æ–°æ—§çŸ¥è¯†ï¼Œå³å°æ ·æœ¬ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç°æœ‰å·¥ä½œä¸»è¦ä¾èµ–äºè§†è§‰ç¼–ç å™¨çš„ç²¾ç»†è°ƒæ•´ï¼Œè¿™æ˜¾ç¤ºå‡ºåŸºç¡€çŸ¥è¯†å’Œå¢é‡çŸ¥è¯†ä¹‹é—´çš„æƒè¡¡ã€‚å—äººç±»å­¦ä¹ ç³»ç»Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€å¯å‘å…³ç³»è½¬ç§»ï¼ˆLRTï¼‰èŒƒå¼ï¼Œé€šè¿‡è”åˆè§†è§‰çº¿ç´¢å’Œæ–‡æœ¬æè¿°æ¥ç†è§£å¯¹è±¡ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼šé¦–å…ˆå°†é¢„è®­ç»ƒçš„æ–‡æœ¬çŸ¥è¯†è½¬ç§»åˆ°è§†è§‰é¢†åŸŸï¼Œç„¶åèåˆè§†è§‰å’Œè¯­è¨€åµŒå…¥ã€‚ä¸ºç¼“è§£è§†è§‰å¾®è°ƒå¸¦æ¥çš„é¢†åŸŸå·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡æç¤ºå­¦ä¹ å’Œæƒ³è±¡å¯¹æ¯”å­¦ä¹ ï¼Œä»¥å‡è½»å¯¹é½è¿‡ç¨‹ä¸­çš„æ–‡æœ¬æ•°æ®ä¸è¶³é—®é¢˜ã€‚é€šè¿‡é¢†åŸŸå¯¹é½å’Œæ–‡æœ¬å›¾åƒè½¬ç§»çš„åä½œå­¦ä¹ ï¼Œæˆ‘ä»¬æå‡ºçš„LRTåœ¨mini-ImageNetå’ŒCIFAR-100 FSCILåŸºå‡†æµ‹è¯•çš„æœ€ç»ˆé˜¶æ®µè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œåˆ†åˆ«æé«˜äº†13%å’Œ7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»å­¦ä¹ ç³»ç»Ÿèƒ½å¤Ÿé€šè¿‡è§‚å¯Ÿå°‘é‡æ ·æœ¬å¹¶ç”¨è¯­è¨€è¿›è¡Œæè¿°æ¥æç»˜æ–°ç±»åˆ«ï¼Œè¿™è¢«ç§°ä¸ºå°æ ·ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡ç²¾ç»†è°ƒæ•´è§†è§‰ç¼–ç å™¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†å­˜åœ¨åŸºç¡€çŸ¥è¯†å’Œå¢é‡çŸ¥è¯†ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>å¼•å…¥è¯­è¨€å¯å‘å…³ç³»è½¬ç§»ï¼ˆLRTï¼‰èŒƒå¼ï¼Œç»“åˆè§†è§‰çº¿ç´¢å’Œæ–‡æœ¬æè¿°æ¥ç†è§£å¯¹è±¡ã€‚</li>
<li>LRTåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼šå°†æ–‡æœ¬çŸ¥è¯†è½¬ç§»åˆ°è§†è§‰é¢†åŸŸï¼Œç„¶åèåˆè§†è§‰å’Œè¯­è¨€åµŒå…¥ã€‚</li>
<li>ä¸ºè§£å†³è§†è§‰å¾®è°ƒå¸¦æ¥çš„é¢†åŸŸå·®è·é—®é¢˜ï¼Œæå‡ºäº†ä¸Šä¸‹æ–‡æç¤ºå­¦ä¹ å’Œæƒ³è±¡å¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>LRTåœ¨FSCILåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†è¯†åˆ«å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b6b20cd92253b45161bafd41232cdfe0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6aa2a0cccc223bd24a7e4f6b7003855b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c359e615a84c8e09a9042acb95b8f26d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a70759992b52099d3f430af09de0e99.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Zero-shot-Shark-Tracking-and-Biometrics-from-Aerial-Imagery"><a href="#Zero-shot-Shark-Tracking-and-Biometrics-from-Aerial-Imagery" class="headerlink" title="Zero-shot Shark Tracking and Biometrics from Aerial Imagery"></a>Zero-shot Shark Tracking and Biometrics from Aerial Imagery</h2><p><strong>Authors:Chinmay K Lalgudi, Mark E Leone, Jaden V Clark, Sergio Madrigal-Mora, Mario Espinoza</strong></p>
<p>The recent widespread adoption of drones for studying marine animals provides opportunities for deriving biological information from aerial imagery. The large scale of imagery data acquired from drones is well suited for machine learning (ML) analysis. Development of ML models for analyzing marine animal aerial imagery has followed the classical paradigm of training, testing, and deploying a new model for each dataset, requiring significant time, human effort, and ML expertise. We introduce Frame Level ALIgment and tRacking (FLAIR), which leverages the video understanding of Segment Anything Model 2 (SAM2) and the vision-language capabilities of Contrastive Language-Image Pre-training (CLIP). FLAIR takes a drone video as input and outputs segmentation masks of the species of interest across the video. Notably, FLAIR leverages a zero-shot approach, eliminating the need for labeled data, training a new model, or fine-tuning an existing model to generalize to other species. With a dataset of 18,000 drone images of Pacific nurse sharks, we trained state-of-the-art object detection models to compare against FLAIR. We show that FLAIR massively outperforms these object detectors and performs competitively against two human-in-the-loop methods for prompting SAM2, achieving a Dice score of 0.81. FLAIR readily generalizes to other shark species without additional human effort and can be combined with novel heuristics to automatically extract relevant information including length and tailbeat frequency. FLAIR has significant potential to accelerate aerial imagery analysis workflows, requiring markedly less human effort and expertise than traditional machine learning workflows, while achieving superior accuracy. By reducing the effort required for aerial imagery analysis, FLAIR allows scientists to spend more time interpreting results and deriving insights about marine ecosystems. </p>
<blockquote>
<p>æ— äººæœºå¹¿æ³›ç”¨äºç ”ç©¶æµ·æ´‹ç”Ÿç‰©ï¼Œè¿™ä¸ºä»ç©ºä¸­å›¾åƒä¸­æå–ç”Ÿç‰©ä¿¡æ¯æä¾›äº†æœºä¼šã€‚ä»æ— äººæœºè·å–çš„å›¾åƒæ•°æ®è§„æ¨¡åºå¤§ï¼Œéå¸¸é€‚åˆè¿›è¡Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åˆ†æã€‚é’ˆå¯¹æµ·æ´‹ç”Ÿç‰©ç©ºä¸­å›¾åƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹å¼€å‘éµå¾ªäº†ä¸ºæ¯ä¸ªæ•°æ®é›†è®­ç»ƒã€æµ‹è¯•å’Œéƒ¨ç½²æ–°æ¨¡å‹çš„ç»å…¸èŒƒå¼ï¼Œè¿™éœ€è¦å¤§é‡æ—¶é—´ã€äººåŠ›å’Œæœºå™¨å­¦ä¹ ä¸“ä¸šçŸ¥è¯†ã€‚æˆ‘ä»¬å¼•å…¥äº†Frame Level ALIgmentå’ŒtRackingï¼ˆFLAIRï¼‰ï¼Œå®ƒåˆ©ç”¨Segment Anything Model 2ï¼ˆSAM2ï¼‰çš„è§†é¢‘ç†è§£èƒ½åŠ›å’ŒContrastive Language-Image Pre-trainingï¼ˆCLIPï¼‰çš„è§†å¬è¯­è¨€èƒ½åŠ›ã€‚FLAIRä»¥æ— äººæœºè§†é¢‘ä¸ºè¾“å…¥ï¼Œè¾“å‡ºè§†é¢‘ä¸­æ„Ÿå…´è¶£ç‰©ç§çš„åˆ†å‰²æ©è†œã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒFLAIRé‡‡ç”¨é›¶æ ·æœ¬æ–¹æ³•ï¼Œæ— éœ€æ ‡è®°æ•°æ®ï¼Œæ— éœ€è®­ç»ƒæ–°æ¨¡å‹æˆ–å¾®è°ƒç°æœ‰æ¨¡å‹å³å¯æ¨å¹¿åˆ°å…¶ä»–ç‰©ç§ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«å¤ªå¹³æ´‹æŠ¤å£«é²¨çš„æ— äººæœºå›¾åƒæ•°æ®é›†è®­ç»ƒäº†æœ€å…ˆè¿›çš„å¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼Œä»¥ä¸FLAIRè¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬è¯æ˜FLAIRå¤§å¤§ä¼˜äºè¿™äº›å¯¹è±¡æ£€æµ‹å™¨ï¼Œå¹¶ä¸”ä¸ä¸¤ç§äººç±»å¾ªç¯æç¤ºSAM2çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå®ç°äº†0.81çš„Diceå¾—åˆ†ã€‚FLAIRå¾ˆå®¹æ˜“æ¨å¹¿åˆ°å…¶ä»–é²¨é±¼ç‰©ç§ï¼Œæ— éœ€é¢å¤–çš„äººåŠ›æŠ•å…¥ï¼Œå¹¶ä¸”å¯ä»¥ç»“åˆæ–°é¢–å¯å‘å¼ç®—æ³•è‡ªåŠ¨æå–ç›¸å…³ä¿¡æ¯ï¼ŒåŒ…æ‹¬é•¿åº¦å’Œå°¾æ‹é¢‘ç‡ã€‚FLAIRåœ¨åŠ é€Ÿç©ºä¸­å›¾åƒåˆ†æå·¥ä½œæµç¨‹æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œç›¸æ¯”ä¼ ç»Ÿæœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ï¼Œå®ƒå¤§å¤§å‡å°‘äº†äººåŠ›å’Œä¸“ä¸šçŸ¥è¯†éœ€æ±‚ï¼ŒåŒæ—¶å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å‡å°‘ç©ºä¸­å›¾åƒåˆ†ææ‰€éœ€çš„å·¥ä½œé‡ï¼ŒFLAIRè®©ç§‘å­¦å®¶èƒ½å¤ŸèŠ±æ›´å¤šæ—¶é—´è§£é‡Šç»“æœå¹¶å¾—å‡ºå…³äºæµ·æ´‹ç”Ÿæ€ç³»ç»Ÿçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05717v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ— äººæœºå¹¿æ³›ç”¨äºæµ·æ´‹åŠ¨ç‰©ç ”ç©¶ï¼Œæä¾›ä»é«˜ç©ºå½±åƒè·å–ç”Ÿç‰©ä¿¡æ¯çš„æœºé‡ã€‚å¤§è§„æ¨¡å½±åƒæ•°æ®é€‚åˆä½¿ç”¨æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰è¿›è¡Œåˆ†æã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•FLAIRï¼Œå®ƒåˆ©ç”¨SAM2çš„è§†é¢‘ç†è§£èƒ½åŠ›å’ŒCLIPçš„è§†è¯­è¨€é¢„è®­ç»ƒèƒ½åŠ›ï¼Œæ¥å—æ— äººæœºè§†é¢‘è¾“å…¥å¹¶è¾“å‡ºç‰©ç§åˆ†å‰²æ©è†œã€‚FLAIRé‡‡ç”¨é›¶æ ·æœ¬æ–¹æ³•ï¼Œæ— éœ€æ ‡æ³¨æ•°æ®ã€è®­ç»ƒæ–°æ¨¡å‹æˆ–å¾®è°ƒç°æœ‰æ¨¡å‹å³å¯æ¨å¹¿åˆ°å…¶ä»–ç‰©ç§ã€‚åœ¨å¤ªå¹³æ´‹æŠ¤å£«é²¨çš„1.8ä¸‡å¼ æ— äººæœºå›¾åƒæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬è®­ç»ƒäº†æœ€å…ˆè¿›çš„ç‰©ä½“æ£€æµ‹æ¨¡å‹ä¸FLAIRè¿›è¡Œæ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼ŒFLAIRå¤§ä¸¾è¶…è¶Šäº†è¿™äº›ç‰©ä½“æ£€æµ‹å™¨ï¼Œå¹¶ä¸ä¸¤ç§äººç±»å¾ªç¯æç¤ºSAM2çš„æ–¹æ³•ç«äº‰ï¼Œå®ç°äº†Diceè¯„åˆ†0.81ã€‚FLAIRæ˜“äºæ¨å¹¿åˆ°å…¶ä»–é²¨é±¼ç‰©ç§ï¼Œæ— éœ€é¢å¤–çš„äººåŠ›æŠ•å…¥ï¼Œå¹¶èƒ½ä¸æ–°é¢–ç®—æ³•ç»“åˆè‡ªåŠ¨æå–ç›¸å…³ä¿¡æ¯ï¼Œå¦‚é•¿åº¦å’Œå°¾æ³¢é¢‘ç‡ã€‚FLAIRå…·æœ‰åŠ é€Ÿé«˜ç©ºå½±åƒåˆ†æå·¥ä½œæµç¨‹çš„å·¨å¤§æ½œåŠ›ï¼Œç›¸æ¯”ä¼ ç»Ÿæœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹å¤§å¤§å‡å°‘äººåŠ›å’Œä¸“å®¶éœ€æ±‚ï¼ŒåŒæ—¶å®ç°æ›´é«˜çš„ç²¾åº¦ã€‚é€šè¿‡é™ä½é«˜ç©ºå½±åƒåˆ†ææ‰€éœ€çš„åŠªåŠ›ï¼ŒFLAIRä½¿ç§‘å­¦å®¶æœ‰æ›´å¤šæ—¶é—´è§£è¯»ç»“æœå¹¶æŒ–æ˜æµ·æ´‹ç”Ÿæ€ç³»ç»Ÿçš„è§è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ— äººæœºå¹¿æ³›åº”ç”¨äºæµ·æ´‹åŠ¨ç‰©ç ”ç©¶ï¼Œä¸ºä»é«˜ç©ºå½±åƒè·å–ç”Ÿç‰©ä¿¡æ¯æä¾›æœºé‡ã€‚</li>
<li>å¤§è§„æ¨¡å½±åƒæ•°æ®é€‚åˆä½¿ç”¨æœºå™¨å­¦ä¹ è¿›è¡Œåˆ†æã€‚</li>
<li>FLAIRæ–¹æ³•åˆ©ç”¨SAM2çš„è§†é¢‘ç†è§£èƒ½åŠ›å’ŒCLIPçš„è§†è¯­è¨€é¢„è®­ç»ƒèƒ½åŠ›ï¼Œæ¥å—æ— äººæœºè§†é¢‘è¾“å…¥å¹¶è¾“å‡ºç‰©ç§åˆ†å‰²æ©è†œã€‚</li>
<li>FLAIRé‡‡ç”¨é›¶æ ·æœ¬æ–¹æ³•ï¼Œæ— éœ€æ ‡æ³¨æ•°æ®ã€è®­ç»ƒæ–°æ¨¡å‹æˆ–å¾®è°ƒç°æœ‰æ¨¡å‹å³å¯æ¨å¹¿åˆ°å…¶ä»–ç‰©ç§ã€‚</li>
<li>åœ¨å¤ªå¹³æ´‹æŠ¤å£«é²¨çš„æ— äººæœºå›¾åƒæ•°æ®é›†ä¸Šï¼ŒFLAIRæ˜¾è‘—ä¼˜äºç‰©ä½“æ£€æµ‹å™¨ã€‚</li>
<li>FLAIRæ˜“äºæ¨å¹¿åˆ°å…¶ä»–é²¨é±¼ç‰©ç§ï¼Œå¹¶èƒ½ç»“åˆæ–°é¢–ç®—æ³•è‡ªåŠ¨æå–ç›¸å…³ä¿¡æ¯ã€‚</li>
<li>FLAIRå…·æœ‰åŠ é€Ÿé«˜ç©ºå½±åƒåˆ†æå·¥ä½œæµç¨‹çš„æ½œåŠ›ï¼Œå‡å°‘äººåŠ›å’Œä¸“å®¶éœ€æ±‚ï¼ŒåŒæ—¶æé«˜åˆ†æç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6cea70281f34a5c281a624592950ad77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79f396e72a565907ea524e1040cbef85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97470dc6bdcc5d4d50074b7af8bc2cca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d391e744b8e56b8978d817077f255662.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EndoDINO-A-Foundation-Model-for-GI-Endoscopy"><a href="#EndoDINO-A-Foundation-Model-for-GI-Endoscopy" class="headerlink" title="EndoDINO: A Foundation Model for GI Endoscopy"></a>EndoDINO: A Foundation Model for GI Endoscopy</h2><p><strong>Authors:Patrick Dermyer, Angad Kalra, Matt Schwartz</strong></p>
<p>In this work, we present EndoDINO, a foundation model for GI endoscopy tasks that achieves strong generalizability by pre-training on a well-curated image dataset sampled from the largest known GI endoscopy video dataset in the literature. Specifically, we pre-trained ViT models with 1B, 307M, and 86M parameters using datasets ranging from 100K to 10M curated images. Using EndoDINO as a frozen feature encoder, we achieved state-of-the-art performance in anatomical landmark classification, polyp segmentation, and Mayo endoscopic scoring (MES) for ulcerative colitis with only simple decoder heads. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EndoDINOï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹èƒƒè‚ é“å†…çª¥é•œä»»åŠ¡çš„åŸºç¡€æ¨¡å‹ã€‚å®ƒé€šè¿‡é¢„è®­ç»ƒåœ¨æ–‡çŒ®ä¸­å·²çŸ¥æœ€å¤§çš„èƒƒè‚ é“å†…çª¥é•œè§†é¢‘æ•°æ®é›†ä¸­ç²¾å¿ƒæŒ‘é€‰çš„å›¾åƒæ•°æ®é›†ï¼Œå®ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨åŒ…å«ä»10ä¸‡åˆ°1äº¿å¼ ç²¾é€‰å›¾åƒçš„å¤šä¸ªæ•°æ®é›†ï¼Œå¯¹æ‹¥æœ‰1äº¿ã€3äº¿å’Œ8äº¿å‚æ•°çš„ViTæ¨¡å‹è¿›è¡Œäº†é¢„è®­ç»ƒã€‚ä»¥EndoDINOä½œä¸ºå†»ç»“çš„ç‰¹å¾ç¼–ç å™¨ï¼Œæˆ‘ä»¬ä»…åœ¨ç®€å•çš„è§£ç å™¨å¤´ä¸Šå®ç°äº†æºƒç–¡æ€§ç»“è‚ ç‚è§£å‰–å­¦åœ°æ ‡åˆ†ç±»ã€æ¯è‚‰åˆ†å‰²å’Œæ¢…å¥¥å†…çª¥é•œè¯„åˆ†ï¼ˆMESï¼‰çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05488v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>æœ¬æ–‡ä»‹ç»äº†EndoDINOï¼Œä¸€ç§ç”¨äºèƒƒè‚ é“å†…é•œä»»åŠ¡çš„é€šç”¨æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨ä¸€ä¸ªå¤§å‹çš„èƒƒè‚ é“å†…é•œè§†é¢‘æ•°æ®é›†ä¸Šé¢„è®­ç»ƒå¾—åˆ°ï¼Œå…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨ä¸åŒå‚æ•°çš„ViTæ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶ä½¿ç”¨ç®€å•çš„è§£ç å™¨å®ç°äº†åœ¨è§£å‰–åœ°æ ‡åˆ†ç±»ã€æ¯è‚‰åˆ†å‰²å’Œæºƒç–¡æ€§ç»“è‚ ç‚çš„Mayoå†…é•œè¯„åˆ†ä¸Šçš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>EndoDINOæ˜¯ä¸€ä¸ªç”¨äºèƒƒè‚ é“å†…é•œä»»åŠ¡çš„é€šç”¨æ¨¡å‹ï¼Œé€šè¿‡é¢„è®­ç»ƒåœ¨å¤§å‹æ•°æ®é›†ä¸Šå®ç°å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨äº†ViTæ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå‚æ•°è§„æ¨¡ä»åƒä¸‡çº§åˆ°ç™¾ä¸‡çº§ä¸ç­‰ã€‚</li>
<li>EndoDINOé€šè¿‡é¢„è®­ç»ƒåœ¨å¤§é‡ç²¾é€‰å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>ä½¿ç”¨EndoDINOä½œä¸ºç‰¹å¾ç¼–ç å™¨ï¼Œå®ç°äº†åœ¨è§£å‰–åœ°æ ‡åˆ†ç±»ã€æ¯è‚‰åˆ†å‰²å’Œæºƒç–¡æ€§ç»“è‚ ç‚è¯„åˆ†ä¸Šçš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨ç®€å•çš„è§£ç å™¨å¤´è¿›è¡Œä»»åŠ¡ç‰¹å®šè§£ç ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†ä½¿ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥ç›¸å…³ä»»åŠ¡æä¾›äº†å€Ÿé‰´å’Œå‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05488">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e7b68b4b581daa9a71e1ee5ccfcd19cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83a02271ac99ddfc3fc718ee3c040bb7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-534be693cf1894ea564e278ac1009906.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfeda6c0d7e3ff39b42b989d7ef4f04b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-216af44cfce16cb17ffe0ee6439bc43f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66b1043e58bbaa7bd778fda2f168cdf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e0b62a439111be3cfb9f01ea73922f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d64cce1ea5e206f3bd401fd3435b93ba.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Leveraging-Registers-in-Vision-Transformers-for-Robust-Adaptation"><a href="#Leveraging-Registers-in-Vision-Transformers-for-Robust-Adaptation" class="headerlink" title="Leveraging Registers in Vision Transformers for Robust Adaptation"></a>Leveraging Registers in Vision Transformers for Robust Adaptation</h2><p><strong>Authors:Srikar Yellapragada, Kowshik Thopalli, Vivek Narayanaswamy, Wesam Sakla, Yang Liu, Yamen Mubarka, Dimitris Samaras, Jayaraman J. Thiagarajan</strong></p>
<p>Vision Transformers (ViTs) have shown success across a variety of tasks due to their ability to capture global image representations. Recent studies have identified the existence of high-norm tokens in ViTs, which can interfere with unsupervised object discovery. To address this, the use of â€œregistersâ€ which are additional tokens that isolate high norm patch tokens while capturing global image-level information has been proposed. While registers have been studied extensively for object discovery, their generalization properties particularly in out-of-distribution (OOD) scenarios, remains underexplored. In this paper, we examine the utility of register token embeddings in providing additional features for improving generalization and anomaly rejection. To that end, we propose a simple method that combines the special CLS token embedding commonly employed in ViTs with the average-pooled register embeddings to create feature representations which are subsequently used for training a downstream classifier. We find that this enhances OOD generalization and anomaly rejection, while maintaining in-distribution (ID) performance. Extensive experiments across multiple ViT backbones trained with and without registers reveal consistent improvements of 2-4% in top-1 OOD accuracy and a 2-3% reduction in false positive rates for anomaly detection. Importantly, these gains are achieved without additional computational overhead. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTsï¼‰ç”±äºå…¶æ•æ‰å…¨å±€å›¾åƒè¡¨å¾çš„èƒ½åŠ›ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸã€‚æœ€è¿‘çš„ç ”ç©¶å‘ç°ViTsä¸­å­˜åœ¨é«˜èŒƒæ•°ä»¤ç‰Œï¼Œè¿™å¯èƒ½ä¼šå¹²æ‰°æ— ç›‘ç£å¯¹è±¡å‘ç°ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä½¿ç”¨â€œå¯„å­˜å™¨â€çš„æ–¹æ³•ï¼Œè¿™äº›å¯„å­˜å™¨æ˜¯é¢å¤–çš„ä»¤ç‰Œï¼Œå¯ä»¥éš”ç¦»é«˜èŒƒæ•°è¡¥ä¸ä»¤ç‰ŒåŒæ—¶æ•è·å…¨å±€å›¾åƒçº§ä¿¡æ¯ã€‚è™½ç„¶å¯„å­˜å™¨åœ¨å¯¹è±¡å‘ç°æ–¹é¢å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†åœ¨è¶…å‡ºåˆ†é…èŒƒå›´çš„æƒ…å¢ƒä¸‹ï¼ˆOODï¼‰å…¶é€šç”¨å±æ€§ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¯„å­˜å™¨ä»¤ç‰ŒåµŒå…¥åœ¨æé«˜é€šç”¨æ€§å’Œå¼‚å¸¸æ‹’ç»æ–¹é¢çš„ä½œç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†è§†è§‰Transformerä¸­å¸¸ç”¨çš„ç‰¹æ®ŠCLSä»¤ç‰ŒåµŒå…¥å’Œå¹³å‡æ± åŒ–çš„å¯„å­˜å™¨åµŒå…¥ï¼Œä»¥åˆ›å»ºç‰¹å¾è¡¨ç¤ºï¼Œéšåç”¨äºè®­ç»ƒä¸‹æ¸¸åˆ†ç±»å™¨ã€‚æˆ‘ä»¬å‘ç°è¿™å¢å¼ºäº†OODçš„é€šç”¨æ€§å’Œå¼‚å¸¸æ‹’ç»èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†å†…éƒ¨åˆ†å¸ƒï¼ˆIDï¼‰æ€§èƒ½ã€‚ä½¿ç”¨å’Œä¸ä½¿ç”¨å¯„å­˜å™¨çš„å¤šä¸ªViTä¸»å¹²çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨é¡¶çº§OODå‡†ç¡®ç‡ä¸ŠæŒç»­æé«˜äº†2-4%ï¼Œå¼‚å¸¸æ£€æµ‹çš„è¯¯æŠ¥ç‡é™ä½äº†2-3%ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™äº›æ”¶ç›Šçš„å®ç°æ²¡æœ‰é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04784v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>ViTsä¸­çš„é«˜èŒƒæ•°ä»¤ç‰Œä¼šå¹²æ‰°æ— ç›‘ç£å¯¹è±¡å‘ç°ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä½¿ç”¨â€œå¯„å­˜å™¨â€æ¥æ•è·å…¨å±€å›¾åƒçº§ä¿¡æ¯å¹¶éš”ç¦»é«˜èŒƒæ•°æ–‘å—ä»¤ç‰Œã€‚æœ¬æ–‡æ¢è®¨äº†å¯„å­˜å™¨ä»¤ç‰ŒåµŒå…¥åœ¨æé«˜é€šç”¨æ€§å’Œå¼‚å¸¸æ‹’ç»æ–¹é¢çš„ä½œç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œå°†ViTsä¸­å¸¸ç”¨çš„ç‰¹æ®ŠCLSä»¤ç‰ŒåµŒå…¥ä¸å¹³å‡æ± åŒ–çš„å¯„å­˜å™¨åµŒå…¥ç›¸ç»“åˆï¼Œåˆ›å»ºç”¨äºè®­ç»ƒä¸‹æ¸¸åˆ†ç±»å™¨çš„ç‰¹å¾è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œè¿™æé«˜äº†OODçš„é€šç”¨æ€§å’Œå¼‚å¸¸æ‹’ç»èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†IDæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) ä¸­çš„é«˜èŒƒæ•°ä»¤ç‰Œä¼šå¹²æ‰°æ— ç›‘ç£å¯¹è±¡å‘ç°ã€‚</li>
<li>â€œå¯„å­˜å™¨â€æ˜¯ä¸€ç§é¢å¤–çš„ä»¤ç‰Œï¼Œæ—¨åœ¨éš”ç¦»é«˜èŒƒæ•°æ–‘å—ä»¤ç‰Œï¼ŒåŒæ—¶æ•è·å…¨å±€å›¾åƒçº§ä¿¡æ¯ã€‚</li>
<li>å¯„å­˜å™¨åœ¨å¯¹è±¡å‘ç°æ–¹é¢çš„åº”ç”¨å·²ç»å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ï¼Œä½†åœ¨OODåœºæ™¯ä¸­çš„é€šç”¨å±æ€§ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚</li>
<li>ç»“åˆCLSä»¤ç‰ŒåµŒå…¥å’Œå¹³å‡æ± åŒ–çš„å¯„å­˜å™¨åµŒå…¥åˆ›å»ºçš„ç‰¹å¾è¡¨ç¤ºæœ‰åŠ©äºæ”¹å–„é€šç”¨æ€§å’Œå¼‚å¸¸æ‹’ç»ã€‚</li>
<li>è¿™ç§æ–¹æ³•åœ¨OODåœºæ™¯ä¸‹æé«˜äº†é¡¶çº§å‡†ç¡®ç‡2-4%ï¼Œåœ¨å¼‚å¸¸æ£€æµ‹ä¸­é™ä½äº†è¯¯æŠ¥ç‡2-3%ã€‚</li>
<li>è¿™äº›æ”¹è¿›æ˜¯åœ¨ä¸å¢åŠ è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹å®ç°çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04784">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9df3e957d240416285b00289f76e7629.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9418a08f9a0e59e0106d74188672ce6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90b3335a818e14b115ffacf1546261ca.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-a-Multimodal-Large-Language-Model-with-Pixel-Level-Insight-for-Biomedicine"><a href="#Towards-a-Multimodal-Large-Language-Model-with-Pixel-Level-Insight-for-Biomedicine" class="headerlink" title="Towards a Multimodal Large Language Model with Pixel-Level Insight for   Biomedicine"></a>Towards a Multimodal Large Language Model with Pixel-Level Insight for   Biomedicine</h2><p><strong>Authors:Xiaoshuang Huang, Lingdong Shen, Jia Liu, Fangxin Shang, Hongxiang Li, Haifeng Huang, Yehui Yang</strong></p>
<p>In recent years, Multimodal Large Language Models (MLLM) have achieved notable advancements, demonstrating the feasibility of developing an intelligent biomedical assistant. However, current biomedical MLLMs predominantly focus on image-level understanding and restrict interactions to textual commands, thus limiting their capability boundaries and the flexibility of usage. In this paper, we introduce a novel end-to-end multimodal large language model for the biomedical domain, named MedPLIB, which possesses pixel-level understanding. Excitingly, it supports visual question answering (VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form shapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE) multi-stage training strategy, which divides MoE into separate training phases for a visual-language expert model and a pixel-grounding expert model, followed by fine-tuning using MoE. This strategy effectively coordinates multitask learning while maintaining the computational cost at inference equivalent to that of a single expert model. To advance the research of biomedical MLLMs, we introduce the Medical Complex Vision Question Answering Dataset (MeCoVQA), which comprises an array of 8 modalities for complex medical imaging question answering and image region understanding. Experimental results indicate that MedPLIB has achieved state-of-the-art outcomes across multiple medical visual language tasks. More importantly, in zero-shot evaluations for the pixel grounding task, MedPLIB leads the best small and large models by margins of 19.7 and 15.6 respectively on the mDice metric. The codes, data, and model checkpoints will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/ShawnHuang497/MedPLIB">https://github.com/ShawnHuang497/MedPLIB</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œè¯æ˜äº†å¼€å‘æ™ºèƒ½ç”Ÿç‰©åŒ»å­¦åŠ©ç†çš„å¯è¡Œæ€§ã€‚ç„¶è€Œï¼Œå½“å‰ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„MLLMä¸»è¦ä¾§é‡äºå›¾åƒçº§åˆ«çš„ç†è§£ï¼Œå¹¶å°†äº’åŠ¨é™åˆ¶åœ¨æ–‡æœ¬å‘½ä»¤ä¸Šï¼Œä»è€Œé™åˆ¶äº†å…¶èƒ½åŠ›è¾¹ç•Œå’Œä½¿ç”¨çµæ´»æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç”¨äºç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„æ–°å‹ç«¯åˆ°ç«¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåä¸ºMedPLIBï¼Œå®ƒæ‹¥æœ‰åƒç´ çº§çš„ç†è§£ã€‚ä»¤äººå…´å¥‹çš„æ˜¯ï¼Œå®ƒæ”¯æŒè§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€ä»»æ„åƒç´ çº§æç¤ºï¼ˆç‚¹ã€è¾¹ç•Œæ¡†å’Œè‡ªç”±å½¢å¼å½¢çŠ¶ï¼‰å’Œåƒç´ çº§å®šä½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥å°†MoEåˆ†ä¸ºè§†è§‰è¯­è¨€ä¸“å®¶æ¨¡å‹å’Œåƒç´ å®šä½ä¸“å®¶æ¨¡å‹çš„å•ç‹¬è®­ç»ƒé˜¶æ®µï¼Œç„¶åä½¿ç”¨MoEè¿›è¡Œå¾®è°ƒã€‚è¿™ç§ç­–ç•¥æœ‰æ•ˆåœ°åè°ƒäº†å¤šä»»åŠ¡å­¦ä¹ ï¼ŒåŒæ—¶ä¿æŒæ¨ç†çš„è®¡ç®—æˆæœ¬ç›¸å½“äºå•ä¸ªä¸“å®¶æ¨¡å‹çš„æˆæœ¬ã€‚ä¸ºäº†æ¨åŠ¨ç”Ÿç‰©åŒ»å­¦MLLMçš„ç ”ç©¶è¿›å±•ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ»ç–—å¤æ‚è§†è§‰é—®ç­”æ•°æ®é›†ï¼ˆMeCoVQAï¼‰ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”¨äºå¤æ‚åŒ»å­¦å›¾åƒé—®ç­”å’Œå›¾åƒåŒºåŸŸç†è§£çš„8ç§æ¨¡æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedPLIBåœ¨å¤šä¸ªåŒ»å­¦è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨åƒç´ å®šä½ä»»åŠ¡çš„é›¶æ ·æœ¬è¯„ä¼°ä¸­ï¼ŒMedPLIBåœ¨mDiceæŒ‡æ ‡ä¸Šé¢†å…ˆæœ€ä½³å°å‹å’Œå¤§å‹æ¨¡å‹çš„å·®è·åˆ†åˆ«ä¸º19.7å’Œ15.6ã€‚ä»£ç ã€æ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ShawnHuang497/MedPLIB">https://github.com/ShawnHuang497/MedPLIB</a>ä¸Šå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09278v2">PDF</a> Accepted by AAAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„æ–°å‹ç«¯åˆ°ç«¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹â€”â€”MedPLIBï¼Œå®ƒå…·å¤‡åƒç´ çº§ç†è§£èƒ½åŠ›ï¼Œæ”¯æŒè§†è§‰é—®ç­”ã€ä»»æ„åƒç´ çº§æç¤ºå’Œåƒç´ çº§å®šä½ã€‚é€šè¿‡é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå®ç°äº†å¤šä»»åŠ¡å­¦ä¹ çš„æœ‰æ•ˆåè°ƒï¼ŒåŒæ—¶ä¿æŒäº†æ¨ç†é˜¶æ®µçš„è®¡ç®—æˆæœ¬ä¸å•ä¸€ä¸“å®¶æ¨¡å‹ç›¸å½“ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å¼•å…¥äº†åŒ»ç–—å¤æ‚è§†è§‰é—®ç­”æ•°æ®é›†MeCoVQAï¼Œç”¨äºå¤æ‚åŒ»ç–—å½±åƒé—®ç­”å’Œå›¾åƒåŒºåŸŸç†è§£ç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMedPLIBåœ¨å¤šä¸ªåŒ»ç–—è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨åƒç´ å®šä½ä»»åŠ¡çš„é›¶æ ·æœ¬è¯„ä¼°ä¸­ï¼Œé¢†å…ˆå…¶ä»–æœ€ä½³å°å‹å’Œå¤§å‹æ¨¡å‹çš„mDiceæŒ‡æ ‡åˆ†åˆ«æé«˜äº†19.7å’Œ15.6ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedPLIBæ˜¯ä¸€ä¸ªå…·å¤‡åƒç´ çº§ç†è§£èƒ½åŠ›çš„ç”Ÿç‰©åŒ»å­¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>MedPLIBæ”¯æŒè§†è§‰é—®ç­”ã€ä»»æ„åƒç´ çº§æç¤ºå’Œåƒç´ çº§å®šä½ã€‚</li>
<li>é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå®ç°å¤šä»»åŠ¡å­¦ä¹ çš„æœ‰æ•ˆåè°ƒã€‚</li>
<li>å¼•å…¥äº†åŒ»ç–—å¤æ‚è§†è§‰é—®ç­”æ•°æ®é›†MeCoVQAï¼Œç”¨äºåŒ»ç–—å½±åƒç†è§£å’Œé—®ç­”ã€‚</li>
<li>MedPLIBåœ¨å¤šä¸ªåŒ»ç–—è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—æœ€æ–°æˆæœã€‚</li>
<li>åœ¨åƒç´ å®šä½ä»»åŠ¡çš„é›¶æ ·æœ¬è¯„ä¼°ä¸­ï¼ŒMedPLIBè¡¨ç°ä¼˜å¼‚ï¼Œé¢†å…ˆå…¶ä»–æ¨¡å‹ã€‚</li>
<li>MedPLIBçš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å°†å…¬å¼€æä¾›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be83b556b828710f8139e15fdf999671.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2135b3e74c831daee8e8a4df25c8b87d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db4b720337c2a3d3775f585ccf7c629b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e5ff05a03344a5c3f71d490bdd71d4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b6ea87ed3969940b1c60dd28b40e1ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-036cdd1ae131467f2dfedc3e26e2f65c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UniMatch-V2-Pushing-the-Limit-of-Semi-Supervised-Semantic-Segmentation"><a href="#UniMatch-V2-Pushing-the-Limit-of-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="UniMatch V2: Pushing the Limit of Semi-Supervised Semantic Segmentation"></a>UniMatch V2: Pushing the Limit of Semi-Supervised Semantic Segmentation</h2><p><strong>Authors:Lihe Yang, Zhen Zhao, Hengshuang Zhao</strong></p>
<p>Semi-supervised semantic segmentation (SSS) aims at learning rich visual knowledge from cheap unlabeled images to enhance semantic segmentation capability. Among recent works, UniMatch improves its precedents tremendously by amplifying the practice of weak-to-strong consistency regularization. Subsequent works typically follow similar pipelines and propose various delicate designs. Despite the achieved progress, strangely, even in this flourishing era of numerous powerful vision models, almost all SSS works are still sticking to 1) using outdated ResNet encoders with small-scale ImageNet-1K pre-training, and 2) evaluation on simple Pascal and Cityscapes datasets. In this work, we argue that, it is necessary to switch the baseline of SSS from ResNet-based encoders to more capable ViT-based encoders (e.g., DINOv2) that are pre-trained on massive data. A simple update on the encoder (even using 2x fewer parameters) can bring more significant improvement than careful method designs. Built on this competitive baseline, we present our upgraded and simplified UniMatch V2, inheriting the core spirit of weak-to-strong consistency from V1, but requiring less training cost and providing consistently better results. Additionally, witnessing the gradually saturated performance on Pascal and Cityscapes, we appeal that we should focus on more challenging benchmarks with complex taxonomy, such as ADE20K and COCO datasets. Code, models, and logs of all reported values, are available at <a target="_blank" rel="noopener" href="https://github.com/LiheYoung/UniMatch-V2">https://github.com/LiheYoung/UniMatch-V2</a>. </p>
<blockquote>
<p>åŠç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆSSSï¼‰æ—¨åœ¨ä»å»‰ä»·çš„æœªæ ‡è®°å›¾åƒä¸­å­¦ä¹ ä¸°å¯Œçš„è§†è§‰çŸ¥è¯†ï¼Œä»¥æé«˜è¯­ä¹‰åˆ†å‰²èƒ½åŠ›ã€‚åœ¨è¿‘æœŸçš„å·¥ä½œä¸­ï¼ŒUniMatché€šè¿‡åŠ å¼ºå¼±åˆ°å¼ºçš„ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œæå¤§åœ°æ”¹è¿›äº†å‰äººçš„ç ”ç©¶ã€‚åç»­å·¥ä½œé€šå¸¸éµå¾ªç±»ä¼¼çš„æµç¨‹ï¼Œå¹¶æå‡ºäº†å„ç§ç²¾è‡´çš„è®¾è®¡ã€‚å°½ç®¡å·²ç»å–å¾—äº†è¿›å±•ï¼Œä½†å¥‡æ€ªçš„æ˜¯ï¼Œå³ä½¿åœ¨ä¼—å¤šå¼ºå¤§çš„è§†è§‰æ¨¡å‹çš„ç¹è£æ—¶ä»£ï¼Œå‡ ä¹æ‰€æœ‰çš„SSSå·¥ä½œä»ç„¶åšæŒä½¿ç”¨1)è¿‡æ—¶çš„ResNetç¼–ç å™¨è¿›è¡Œå°è§„æ¨¡ImageNet-1Ké¢„è®­ç»ƒï¼Œä»¥åŠ2)åœ¨ç®€å•çš„Pascalå’ŒCityscapesæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10777v2">PDF</a> Accepted by TPAMI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŠç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆSSSï¼‰é¢†åŸŸçš„ç ”ç©¶ç°çŠ¶ã€‚å°½ç®¡å·²æœ‰è®¸å¤šä¼˜ç§€çš„æ¨¡å‹å’Œæ–¹æ³•ï¼Œä½†å¤§å¤šæ•°SSSå·¥ä½œä»ä½¿ç”¨è¿‡æ—¶çš„ResNetç¼–ç å™¨å’Œå°è§„æ¨¡ImageNet-1Ké¢„è®­ç»ƒï¼Œå¹¶åœ¨ç®€å•çš„Pascalå’ŒCityscapesæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æœ¬æ–‡ä¸»å¼ å°†SSSçš„åŸºçº¿ä»åŸºäºResNetçš„ç¼–ç å™¨åˆ‡æ¢åˆ°èƒ½åŠ›æ›´å¼ºçš„åŸºäºViTçš„ç¼–ç å™¨ï¼ˆä¾‹å¦‚ï¼Œé¢„è®­ç»ƒåœ¨å¤§é‡æ•°æ®ä¸Šçš„DINOv2ï¼‰ã€‚ç®€å•çš„ç¼–ç å™¨æ›´æ–°ï¼ˆå³ä½¿ä½¿ç”¨æ›´å°‘çš„å‚æ•°ï¼‰å¯ä»¥å¸¦æ¥æ¯”ç²¾å¿ƒè®¾è®¡çš„æ–¹æ³•æ›´å¤§çš„æ”¹è¿›ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æå‡ºäº†å‡çº§å’Œç®€åŒ–çš„UniMatch V2ï¼Œç»§æ‰¿äº†V1çš„å¼±åˆ°å¼ºçš„ä¸€è‡´æ€§æ ¸å¿ƒç²¾ç¥ï¼Œä½†å‡å°‘äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶æä¾›äº†æ›´ä¸€è‡´çš„ç»“æœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å‘¼åå…³æ³¨å…·æœ‰å¤æ‚åˆ†ç±»çš„æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œå¦‚ADE20Kå’ŒCOCOæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆSSSï¼‰æ—¨åœ¨ä»å¤§é‡çš„æ— æ ‡ç­¾å›¾åƒä¸­å­¦ä¹ ä¸°å¯Œçš„è§†è§‰çŸ¥è¯†ï¼Œä»¥æé«˜è¯­ä¹‰åˆ†å‰²çš„èƒ½åŠ›ã€‚</li>
<li>UniMatché€šè¿‡åŠ å¼ºå¼±åˆ°å¼ºçš„ä¸€è‡´æ€§æ­£åˆ™åŒ–æ¥æé«˜å…ˆå‰çš„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å½“å‰SSSå·¥ä½œä»æ™®éé‡‡ç”¨è¿‡æ—¶çš„ResNetç¼–ç å™¨å’Œå°è§„æ¨¡ImageNet-1Ké¢„è®­ç»ƒï¼Œå¹¶åœ¨ç®€å•çš„æ•°æ®é›†ä¸Šè¯„ä¼°ã€‚</li>
<li>åˆ‡æ¢åˆ°èƒ½åŠ›æ›´å¼ºçš„ViTç¼–ç å™¨ï¼ˆå¦‚é¢„è®­ç»ƒåœ¨å¤§é‡æ•°æ®ä¸Šçš„DINOv2ï¼‰æ˜¯å¿…è¦çš„ã€‚</li>
<li>ç®€å•çš„ç¼–ç å™¨æ›´æ–°å¯ä»¥å¸¦æ¥æ˜¾è‘—çš„æ”¹è¿›ï¼Œç”šè‡³è¶…è¿‡å¤æ‚æ–¹æ³•çš„è®¾è®¡ã€‚</li>
<li>UniMatch V2æ˜¯å‡çº§å’Œç®€åŒ–çš„ç‰ˆæœ¬ï¼Œç»§æ‰¿äº†V1çš„æ ¸å¿ƒç²¾ç¥ï¼Œä½†å‡å°‘äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶æä¾›äº†æ›´å¥½çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76051a686712d5e3ec4a6de949334a05.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eeb11082cf0534de0fd6d7bc4496e824.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4711c09f9a572de06681087290b9b7db.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Static-for-Dynamic-Towards-a-Deeper-Understanding-of-Dynamic-Facial-Expressions-Using-Static-Expression-Data"><a href="#Static-for-Dynamic-Towards-a-Deeper-Understanding-of-Dynamic-Facial-Expressions-Using-Static-Expression-Data" class="headerlink" title="Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial   Expressions Using Static Expression Data"></a>Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial   Expressions Using Static Expression Data</h2><p><strong>Authors:Yin Chen, Jia Li, Yu Zhang, Zhenzhen Hu, Shiguang Shan, Meng Wang, Richang Hong</strong></p>
<p>Dynamic facial expression recognition (DFER) infers emotions from the temporal evolution of expressions, unlike static facial expression recognition (SFER), which relies solely on a single snapshot. This temporal analysis provides richer information and promises greater recognition capability. However, current DFER methods often exhibit unsatisfied performance largely due to fewer training samples compared to SFER. Given the inherent correlation between static and dynamic expressions, we hypothesize that leveraging the abundant SFER data can enhance DFER. To this end, we propose Static-for-Dynamic (S4D), a unified dual-modal learning framework that integrates SFER data as a complementary resource for DFER. Specifically, S4D employs dual-modal self-supervised pre-training on facial images and videos using a shared Vision Transformer (ViT) encoder-decoder architecture, yielding improved spatiotemporal representations. The pre-trained encoder is then fine-tuned on static and dynamic expression datasets in a multi-task learning setup to facilitate emotional information interaction. Unfortunately, vanilla multi-task learning in our study results in negative transfer. To address this, we propose an innovative Mixture of Adapter Experts (MoAE) module that facilitates task-specific knowledge acquisition while effectively extracting shared knowledge from both static and dynamic expression data. Extensive experiments demonstrate that S4D achieves a deeper understanding of DFER, setting new state-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with weighted average recall (WAR) of 53.65%, 58.44%, and 76.68%, respectively. Additionally, a systematic correlation analysis between SFER and DFER tasks is presented, which further elucidates the potential benefits of leveraging SFER. </p>
<blockquote>
<p>åŠ¨æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆDFERï¼‰æ˜¯ä»è¡¨æƒ…çš„æ—¶ç©ºæ¼”å˜ä¸­æ¨æ–­æƒ…æ„Ÿï¼Œä¸ä»…ä¾èµ–äºå•ä¸€é™æ€å›¾åƒçš„é™æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆSFERï¼‰ä¸åŒã€‚è¿™ç§æ—¶é—´åˆ†ææä¾›äº†æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œå¹¶æ‰¿è¯ºå…·æœ‰æ›´é«˜çš„è¯†åˆ«èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºä¸SFERç›¸æ¯”è®­ç»ƒæ ·æœ¬è¾ƒå°‘ï¼Œå½“å‰çš„DFERæ–¹æ³•å¾€å¾€è¡¨ç°å‡ºä¸å°½å¦‚äººæ„çš„æ€§èƒ½ã€‚è€ƒè™‘åˆ°é™æ€å’ŒåŠ¨æ€è¡¨æƒ…ä¹‹é—´çš„å†…åœ¨ç›¸å…³æ€§ï¼Œæˆ‘ä»¬å‡è®¾åˆ©ç”¨ä¸°å¯Œçš„SFERæ•°æ®å¯ä»¥å¢å¼ºDFERã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Static-for-Dynamicï¼ˆS4Dï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŒæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†SFERæ•°æ®ä½œä¸ºDFERçš„è¡¥å……èµ„æºã€‚å…·ä½“æ¥è¯´ï¼ŒS4Dé‡‡ç”¨é¢éƒ¨å›¾åƒå’Œè§†é¢‘çš„åŒé‡æ¨¡æ€è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œä½¿ç”¨å…±äº«çš„Vision Transformerï¼ˆViTï¼‰ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œäº§ç”Ÿæ”¹è¿›çš„æ—¶ç©ºè¡¨ç¤ºã€‚ç„¶åï¼Œåœ¨å¤šä»»åŠ¡å­¦ä¹ è®¾ç½®ä¸­ï¼Œå¯¹é™æ€å’ŒåŠ¨æ€è¡¨æƒ…æ•°æ®é›†è¿›è¡Œå¾®è°ƒé¢„è®­ç»ƒç¼–ç å™¨ï¼Œä»¥ä¿ƒè¿›æƒ…æ„Ÿä¿¡æ¯äº¤äº’ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ç ”ç©¶ä¸­çš„æ™®é€šå¤šä»»åŠ¡å­¦ä¹ å¯¼è‡´äº†è´Ÿè¿ç§»ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ›æ–°çš„Mixture of Adapter Expertsï¼ˆMoAEï¼‰æ¨¡å—ï¼Œå®ƒæœ‰åŠ©äºè·å–ç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ä»é™æ€å’ŒåŠ¨æ€è¡¨æƒ…æ•°æ®ä¸­æå–å…±äº«çŸ¥è¯†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒS4Då¯¹DFERæœ‰äº†æ›´æ·±å…¥çš„ç†è§£ï¼Œåœ¨FERV39Kã€MAFWå’ŒDFEWåŸºå‡†æµ‹è¯•ä¸Šåˆ›é€ äº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼ŒåŠ æƒå¹³å‡å¬å›ç‡åˆ†åˆ«ä¸º53.65%ã€58.44%å’Œ76.68%ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†SFERå’ŒDFERä»»åŠ¡ä¹‹é—´çš„ç³»ç»Ÿç›¸å…³æ€§åˆ†æï¼Œè¿›ä¸€æ­¥é˜æ˜äº†åˆ©ç”¨SFERçš„æ½œåœ¨ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06154v2">PDF</a> The code and model are publicly available here   <a target="_blank" rel="noopener" href="https://github.com/MSA-LMC/S4D">https://github.com/MSA-LMC/S4D</a></p>
<p><strong>Summary</strong><br>     åŠ¨æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆDFERï¼‰é€šè¿‡è¡¨æƒ…çš„æ—¶é—´æ¼”å˜æ¥æ¨æ–­æƒ…ç»ªï¼Œä¸ä»…ä¾èµ–äºå•ä¸€é™æ€å›¾åƒçš„é™æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆSFERï¼‰ä¸åŒã€‚ä¸ºæå‡DFERçš„æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºStatic-for-Dynamicï¼ˆS4Dï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä¸°å¯Œçš„SFERæ•°æ®ä½œä¸ºDFERçš„è¡¥å……èµ„æºã€‚S4Dé‡‡ç”¨åŒæ¨¡æ€è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œä½¿ç”¨å…±äº«çš„Vision Transformerï¼ˆViTï¼‰ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œåœ¨é¢éƒ¨å›¾åƒå’Œè§†é¢‘ä¸Šç”Ÿæˆæ”¹è¿›çš„æ—¶ç©ºè¡¨ç¤ºã€‚ç„¶åï¼Œå¯¹é¢„è®­ç»ƒçš„ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œä»¥åœ¨å¤šä»»åŠ¡å­¦ä¹ è®¾ç½®ä¸­é€‚åº”é™æ€å’ŒåŠ¨æ€è¡¨æƒ…æ•°æ®é›†ï¼Œä¿ƒè¿›æƒ…æ„Ÿä¿¡æ¯äº¤äº’ã€‚æˆ‘ä»¬æå‡ºMixture of Adapter Expertsï¼ˆMoAEï¼‰æ¨¡å—æ¥è§£å†³å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„è´Ÿè¿ç§»é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒS4Dåœ¨FERV39Kã€MAFWå’ŒDFEWåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†çªç ´æ€§æ€§èƒ½ï¼ŒåŠ æƒå¹³å‡å¬å›ç‡åˆ†åˆ«ä¸º53.65ï¼…ã€58.44ï¼…å’Œ76.68ï¼…ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿›è¡Œäº†SFERå’ŒDFERä»»åŠ¡ä¹‹é—´çš„ç³»ç»Ÿç›¸å…³æ€§åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆDFERï¼‰é€šè¿‡è¡¨æƒ…çš„æ—¶é—´æ¼”å˜æ¨æ–­æƒ…ç»ªï¼Œè¾ƒé™æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆSFERï¼‰å…·æœ‰æ›´å¥½çš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>å½“å‰çš„DFERæ–¹æ³•ç”±äºè®­ç»ƒæ ·æœ¬è¾ƒå°‘ï¼Œæ€§èƒ½å¸¸ä¸æ»¡è¶³è¦æ±‚ã€‚</li>
<li>æå‡ºStatic-for-Dynamicï¼ˆS4Dï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨ä¸°å¯Œçš„SFERæ•°æ®å¢å¼ºDFERæ€§èƒ½ã€‚</li>
<li>S4Dé‡‡ç”¨åŒæ¨¡æ€è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œä½¿ç”¨å…±äº«çš„Vision Transformeræ¶æ„ï¼Œç”Ÿæˆæ”¹è¿›çš„æ—¶ç©ºè¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ å¾®è°ƒæ¨¡å‹ï¼Œä¿ƒè¿›æƒ…æ„Ÿä¿¡æ¯äº¤äº’ã€‚</li>
<li>æå‡ºMixture of Adapter Expertsï¼ˆMoAEï¼‰æ¨¡å—è§£å†³å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„è´Ÿè¿ç§»é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06154">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-53deb3513a6b0f573072cce1aea7e016.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af90fe0dfe7baa335d9a7a77feef33bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ed4ba1e204756542881809dce6673bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4cf7afc0afc676a1ea00f377e981d54.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MiM-Mask-in-Mask-Self-Supervised-Pre-Training-for-3D-Medical-Image-Analysis"><a href="#MiM-Mask-in-Mask-Self-Supervised-Pre-Training-for-3D-Medical-Image-Analysis" class="headerlink" title="MiM: Mask in Mask Self-Supervised Pre-Training for 3D Medical Image   Analysis"></a>MiM: Mask in Mask Self-Supervised Pre-Training for 3D Medical Image   Analysis</h2><p><strong>Authors:Jiaxin Zhuang, Linshan Wu, Qiong Wang, Peng Fei, Varut Vardhanabhuti, Lin Luo, Hao Chen</strong></p>
<p>The Vision Transformer (ViT) has demonstrated remarkable performance in Self-Supervised Learning (SSL) for 3D medical image analysis. Masked AutoEncoder (MAE) for feature pre-training can further unleash the potential of ViT on various medical vision tasks. However, due to large spatial sizes with much higher dimensions of 3D medical images, the lack of hierarchical design for MAE may hinder the performance of downstream tasks. In this paper, we propose a novel \textit{Mask in Mask (MiM)} pre-training framework for 3D medical images, which aims to advance MAE by learning discriminative representation from hierarchical visual tokens across varying scales. We introduce multiple levels of granularity for masked inputs from the volume, which are then reconstructed simultaneously ranging at both fine and coarse levels. Additionally, a cross-level alignment mechanism is applied to adjacent level volumes to enforce anatomical similarity hierarchically. Furthermore, we adopt a hybrid backbone to enhance the hierarchical representation learning efficiently during the pre-training. MiM was pre-trained on a large scale of available 3D volumetric images, \textit{i.e.,} Computed Tomography (CT) images containing various body parts. Extensive experiments on thirteen public datasets demonstrate the superiority of MiM over other SSL methods in organ&#x2F;lesion&#x2F;tumor segmentation and disease classification. We further scale up the MiM to large pre-training datasets with more than 10k volumes, showing that large-scale pre-training can further enhance the performance of downstream tasks. The improvement also concluded that the research community should pay more attention to the scale of the pre-training dataset towards the healthcare foundation model for 3D medical images. </p>
<blockquote>
<p>Vision Transformerï¼ˆViTï¼‰åœ¨ç”¨äº3DåŒ»å­¦å›¾åƒåˆ†æçš„Self-Supervised Learningï¼ˆSSLï¼‰ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ä½¿ç”¨Masked AutoEncoderï¼ˆMAEï¼‰è¿›è¡Œç‰¹å¾é¢„è®­ç»ƒå¯ä»¥è¿›ä¸€æ­¥é‡Šæ”¾ViTåœ¨å„ç§åŒ»å­¦è§†è§‰ä»»åŠ¡ä¸Šçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äº3DåŒ»å­¦å›¾åƒå…·æœ‰è¾ƒå¤§çš„ç©ºé—´å°ºå¯¸å’Œæ›´é«˜çš„ç»´åº¦ï¼ŒMAEç¼ºä¹å±‚æ¬¡è®¾è®¡å¯èƒ½ä¼šé˜»ç¢ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äº3DåŒ»å­¦å›¾åƒçš„å…¨æ–°é¢„è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºâ€œMask in Maskï¼ˆMiMï¼‰â€ï¼Œæ—¨åœ¨é€šè¿‡ä»ä¸åŒå°ºåº¦çš„å±‚æ¬¡è§†è§‰æ ‡è®°ä¸­å­¦ä¹ è¾¨åˆ«æ€§è¡¨ç¤ºæ¥æ”¹è¿›MAEã€‚æˆ‘ä»¬ä»ä½“ç§¯ä¸­å¼•å…¥äº†å¤šä¸ªç²’åº¦çš„é®æŒ¡è¾“å…¥ï¼Œç„¶ååœ¨ç²¾ç»†å’Œç²—ç•¥çº§åˆ«ä¸ŠåŒæ—¶è¿›è¡Œé‡å»ºã€‚æ­¤å¤–ï¼Œè¿˜å¯¹ç›¸é‚»çº§åˆ«ä½“ç§¯åº”ç”¨äº†è·¨çº§åˆ«å¯¹é½æœºåˆ¶ï¼Œä»¥æŒ‰å±‚æ¬¡å¼ºåˆ¶æ‰§è¡Œè§£å‰–ç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨æ··åˆéª¨å¹²ç½‘ä»¥å¢å¼ºé¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„å±‚æ¬¡è¡¨ç¤ºå­¦ä¹ ã€‚MiMåœ¨å¤§é‡å¯ç”¨çš„3Dä½“ç§¯å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå³åŒ…å«å„ç§èº«ä½“éƒ¨ä½çš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒã€‚åœ¨13ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMiMåœ¨å™¨å®˜&#x2F;ç—…ç¶&#x2F;è‚¿ç˜¤åˆ†å‰²å’Œç–¾ç—…åˆ†ç±»æ–¹é¢çš„è¡¨ç°ä¼˜äºå…¶ä»–SSLæ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†MiMæ‰©å±•åˆ°å…·æœ‰è¶…è¿‡10kä½“ç§¯çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ä¸Šï¼Œç»“æœè¡¨æ˜å¤§è§„æ¨¡é¢„è®­ç»ƒå¯ä»¥è¿›ä¸€æ­¥æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ”¹è¿›çš„ç»“æœè¿˜è¡¨æ˜ï¼Œç ”ç©¶ç•Œåº”æ›´åŠ å…³æ³¨é¢å‘3DåŒ»å­¦å›¾åƒåŒ»ç–—ä¿å¥åŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®é›†è§„æ¨¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.15580v2">PDF</a> submitted to a journal, updated v2</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Vision Transformerï¼ˆViTï¼‰åœ¨3DåŒ»å­¦å›¾åƒåˆ†æçš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚Masked AutoEncoderï¼ˆMAEï¼‰çš„ç‰¹å¾é¢„è®­ç»ƒå¯ä»¥è¿›ä¸€æ­¥é‡Šæ”¾ViTåœ¨å„ç§åŒ»å­¦è§†è§‰ä»»åŠ¡ä¸Šçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äº3DåŒ»å­¦å›¾åƒçš„ç©ºé—´å°ºå¯¸å¤§ã€ç»´åº¦é«˜ï¼ŒMAEç¼ºä¹å±‚æ¬¡è®¾è®¡å¯èƒ½ä¼šé˜»ç¢ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„é’ˆå¯¹3DåŒ»å­¦å›¾åƒçš„â€œMask in Maskï¼ˆMiMï¼‰â€é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä»å„ç§å°ºåº¦çš„å±‚æ¬¡è§†è§‰æ ‡è®°ä¸­å­¦ä¹ è¾¨åˆ«æ€§è¡¨ç¤ºæ¥æ”¹è¿›MAEã€‚æˆ‘ä»¬å¼•å…¥äº†å¤šç²’åº¦çš„æ©è†œè¾“å…¥ï¼Œè¿™äº›è¾“å…¥åœ¨ç²¾ç»†å’Œç²—ç³™çº§åˆ«ä¸ŠåŒæ—¶è¿›è¡Œé‡å»ºã€‚æ­¤å¤–ï¼Œè¿˜åº”ç”¨äº†è·¨çº§åˆ«å¯¹é½æœºåˆ¶ï¼Œä»¥å¼ºåˆ¶ç›¸é‚»çº§åˆ«ä½“ç§¯åœ¨è§£å‰–ç»“æ„ä¸Šçš„ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨æ··åˆéª¨å¹²ç½‘ï¼Œä»¥åœ¨é¢„è®­ç»ƒæœŸé—´æ›´æœ‰æ•ˆåœ°è¿›è¡Œå±‚æ¬¡è¡¨ç¤ºå­¦ä¹ ã€‚MiMåœ¨å¤§é‡å¯ç”¨çš„3Dä½“ç§¯å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä¾‹å¦‚åŒ…å«å„ç§èº«ä½“éƒ¨ä½çš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒã€‚åœ¨13ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMiMåœ¨å™¨å®˜&#x2F;ç—…å˜&#x2F;è‚¿ç˜¤åˆ†å‰²å’Œç–¾ç—…åˆ†ç±»æ–¹é¢ä¼˜äºå…¶ä»–SSLæ–¹æ³•ã€‚æˆ‘ä»¬å°†MiMæ‰©å±•åˆ°å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10kä¸ªä½“ç§¯çš„æ•°æ®ï¼Œæ˜¾ç¤ºå¤§è§„æ¨¡é¢„è®­ç»ƒå¯ä»¥è¿›ä¸€æ­¥æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æ”¹è¿›çš„ç»“è®ºè¿˜è¡¨æ˜ï¼Œç ”ç©¶ç•Œåº”æ›´åŠ å…³æ³¨é¢å‘3DåŒ»å­¦å›¾åƒçš„å¥åº·æŠ¤ç†åŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®é›†è§„æ¨¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Vision Transformerï¼ˆViTï¼‰åœ¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„3DåŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</li>
<li>Masked AutoEncoderï¼ˆMAEï¼‰å¯ç”¨äºç‰¹å¾é¢„è®­ç»ƒï¼Œé‡Šæ”¾ViTåœ¨åŒ»å­¦è§†è§‰ä»»åŠ¡ä¸Šçš„æ½œåŠ›ã€‚</li>
<li>ç”±äº3DåŒ»å­¦å›¾åƒçš„ç©ºé—´å°ºå¯¸å¤§ã€ç»´åº¦é«˜ï¼ŒMAEçš„ç¼ºä¹å±‚æ¬¡è®¾è®¡å¯èƒ½å½±å“ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„Mask in Maskï¼ˆMiMï¼‰é¢„è®­ç»ƒæ¡†æ¶æ—¨åœ¨é€šè¿‡å±‚æ¬¡è§†è§‰æ ‡è®°å­¦ä¹ æ”¹è¿›MAEã€‚</li>
<li>MiMå¼•å…¥å¤šç²’åº¦æ©è†œè¾“å…¥ï¼Œå¹¶åœ¨ä¸åŒçº§åˆ«ä¸Šè¿›è¡Œé‡å»ºã€‚</li>
<li>MiMé‡‡ç”¨è·¨çº§åˆ«å¯¹é½æœºåˆ¶ï¼Œç¡®ä¿ç›¸é‚»ä½“ç§¯çš„è§£å‰–ç»“æ„ç›¸ä¼¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.15580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a5281ff02d56e77715c61624e4c1f79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b93c51c0bab868e4a205ccce9cad853.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72624bbf664723822721eab31e4513c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3e9bcec7279e1b9d213b22d9e7d6d92.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-148b151ed41b88d0cb7838ded114877f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c9ac97b1459d1a576c9a911a8d6098e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-13/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-13/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b62557dcde66a30a690cd794a36c8369.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-13  Improving Zero-Shot Object-Level Change Detection by Incorporating   Visual Correspondence
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b168b67e881c0c2348a414513c82a38a.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-13  OVO-Bench How Far is Your Video-LLMs from Real-World Online Video   Understanding?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17663.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
