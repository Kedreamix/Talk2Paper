<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-13  LlamaV-o1 Rethinking Step-by-step Visual Reasoning in LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-cdc6367ae46853359c51f0747fd831ca.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-13-æ›´æ–°"><a href="#2025-01-13-æ›´æ–°" class="headerlink" title="2025-01-13 æ›´æ–°"></a>2025-01-13 æ›´æ–°</h1><h2 id="LlamaV-o1-Rethinking-Step-by-step-Visual-Reasoning-in-LLMs"><a href="#LlamaV-o1-Rethinking-Step-by-step-Visual-Reasoning-in-LLMs" class="headerlink" title="LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs"></a>LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs</h2><p><strong>Authors:Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, Hisham Cholakkal, Ivan Laptev, Mubarak Shah, Fahad Shahbaz Khan, Salman Khan</strong></p>
<p>Reasoning is a fundamental capability for solving complex multi-step problems, particularly in visual contexts where sequential step-wise understanding is essential. Existing approaches lack a comprehensive framework for evaluating visual reasoning and do not emphasize step-wise problem-solving. To this end, we propose a comprehensive framework for advancing step-by-step visual reasoning in large language models (LMMs) through three key contributions. First, we introduce a visual reasoning benchmark specifically designed to evaluate multi-step reasoning tasks. The benchmark presents a diverse set of challenges with eight different categories ranging from complex visual perception to scientific reasoning with over 4k reasoning steps in total, enabling robust evaluation of LLMsâ€™ abilities to perform accurate and interpretable visual reasoning across multiple steps. Second, we propose a novel metric that assesses visual reasoning quality at the granularity of individual steps, emphasizing both correctness and logical coherence. The proposed metric offers deeper insights into reasoning performance compared to traditional end-task accuracy metrics. Third, we present a new multimodal visual reasoning model, named LlamaV-o1, trained using a multi-step curriculum learning approach, where tasks are progressively organized to facilitate incremental skill acquisition and problem-solving. The proposed LlamaV-o1 is designed for multi-step reasoning and learns step-by-step through a structured training paradigm. Extensive experiments show that our LlamaV-o1 outperforms existing open-source models and performs favorably against close-source proprietary models. Compared to the recent Llava-CoT, our LlamaV-o1 achieves an average score of 67.3 with an absolute gain of 3.8% across six benchmarks while being 5 times faster during inference scaling. Our benchmark, model, and code are publicly available. </p>
<blockquote>
<p>æ¨ç†æ˜¯è§£å†³å¤æ‚å¤šæ­¥éª¤é—®é¢˜çš„åŸºæœ¬èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦è¿ç»­æ­¥éª¤ç†è§£çš„è§†è§‰ç¯å¢ƒä¸­å°¤ä¸ºé‡è¦ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹è¯„ä¼°è§†è§‰æ¨ç†çš„ç»¼åˆæ¡†æ¶ï¼Œå¹¶ä¸å¼ºè°ƒé€æ­¥è§£å†³é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é€šè¿‡ä¸‰é¡¹å…³é”®è´¡çŒ®ï¼Œæå‡ºäº†æ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€æ­¥è§†è§‰æ¨ç†çš„ç»¼åˆæ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡çš„è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†ä¸€ç³»åˆ—ä»å¤æ‚è§†è§‰æ„ŸçŸ¥åˆ°ç§‘å­¦æ¨ç†çš„å…«ä¸ªä¸åŒç±»åˆ«çš„æŒ‘æˆ˜ï¼Œæ€»å…±åŒ…å«è¶…è¿‡4000ä¸ªæ¨ç†æ­¥éª¤ï¼Œèƒ½å¤Ÿç¨³å¥åœ°è¯„ä¼°LLMåœ¨å¤šä¸ªæ­¥éª¤ä¸­è¿›è¡Œå‡†ç¡®å’Œå¯è§£é‡Šçš„è§†è§‰æ¨ç†çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡åœ¨å•ä¸ªæ­¥éª¤çš„ç²’åº¦ä¸Šè¯„ä¼°è§†è§‰æ¨ç†çš„è´¨é‡ï¼Œå¼ºè°ƒæ­£ç¡®æ€§å’Œé€»è¾‘è¿è´¯æ€§ã€‚ä¸ä¼ ç»Ÿçš„ä»»åŠ¡ç»“æŸå‡†ç¡®æ€§æŒ‡æ ‡ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æŒ‡æ ‡æä¾›äº†å¯¹æ¨ç†æ€§èƒ½çš„æ›´æ·±å…¥è§è§£ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºLlamaV-o1çš„æ–°å‹å¤šæ¨¡æ€è§†è§‰æ¨ç†æ¨¡å‹ï¼Œé‡‡ç”¨å¤šæ­¥éª¤è¯¾ç¨‹å­¦ä¹ æ³•è¿›è¡Œè®­ç»ƒï¼Œä»»åŠ¡æŒ‰è¿›åº¦ç»„ç»‡ï¼Œä¾¿äºé€æ­¥æŒæ¡æŠ€èƒ½å’Œè§£å†³é—®é¢˜ã€‚æå‡ºçš„LlamaV-o1æ—¨åœ¨è¿›è¡Œå¤šæ­¥éª¤æ¨ç†ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–è®­ç»ƒèŒƒå¼è¿›è¡Œé€æ­¥å­¦ä¹ ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LlamaV-o1æ€§èƒ½ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ã€‚ä¸æœ€è¿‘çš„Llava-CoTç›¸æ¯”ï¼Œæˆ‘ä»¬çš„LlamaV-o1åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡å¾—åˆ†67.3ï¼Œç»å¯¹æå‡3.8%ï¼ŒåŒæ—¶æ¨ç†ç¼©æ”¾é€Ÿåº¦æé«˜5å€ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ã€æ¨¡å‹å’Œä»£ç å‡å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06186v1">PDF</a> 15 pages, 5 Figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œä»¥æ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMMsï¼‰çš„é€æ­¥è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä¸»è¦é€šè¿‡ä¸‰ä¸ªå…³é”®è´¡çŒ®å®ç°ã€‚é¦–å…ˆï¼Œå¼•å…¥ä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡çš„è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«ä»å¤æ‚è§†è§‰æ„ŸçŸ¥åˆ°ç§‘å­¦æ¨ç†çš„å…«ä¸ªä¸åŒç±»åˆ«ï¼ŒåŒ…å«è¶…è¿‡4000ä¸ªæ¨ç†æ­¥éª¤ï¼Œèƒ½å¤Ÿç¨³å¥åœ°è¯„ä¼°LLMsåœ¨å¤šæ­¥éª¤å†…çš„å‡†ç¡®å’Œå¯è§£é‡Šè§†è§‰æ¨ç†èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæå‡ºä¸€ç§æ–°å‹è¯„ä¼°æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡åœ¨å•ä¸ªæ­¥éª¤çš„ç²’åº¦ä¸Šè¯„ä¼°è§†è§‰æ¨ç†è´¨é‡ï¼Œå¼ºè°ƒæ­£ç¡®æ€§å’Œé€»è¾‘è¿è´¯æ€§ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ä»»åŠ¡å®Œæˆåº¦è¯„ä¼°æŒ‡æ ‡ï¼Œæä¾›æ›´æ·±å…¥çš„æ¨ç†æ€§èƒ½æ´å¯Ÿã€‚æœ€åï¼Œæå‡ºä¸€ç§åä¸ºLlamaV-o1çš„æ–°å‹å¤šæ¨¡æ€è§†è§‰æ¨ç†æ¨¡å‹ï¼Œé‡‡ç”¨å¤šæ­¥éª¤è¯¾ç¨‹å­¦ä¹ æ³•è¿›è¡Œè®­ç»ƒï¼Œä»»åŠ¡æŒ‰è¿›åº¦ç»„ç»‡ï¼Œä¿ƒè¿›é€æ­¥æŠ€èƒ½è·å–å’Œé—®é¢˜è§£å†³ã€‚LlamaV-o1ä¸“ä¸ºå¤šæ­¥éª¤æ¨ç†è®¾è®¡ï¼Œé€šè¿‡ç»“æ„åŒ–è®­ç»ƒèŒƒå¼è¿›è¡Œé€æ­¥å­¦ä¹ ã€‚å®éªŒæ˜¾ç¤ºï¼ŒLlamaV-o1åœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸å¯¹äºæœ€è¿‘çš„Llava-CoTæ¨¡å‹å–å¾—å¹³å‡å¾—åˆ†67.3çš„ä¼˜å¼‚æˆç»©ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æå‡5å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMMsï¼‰çš„æ–°çš„ç»¼åˆæ¡†æ¶ï¼Œæ—¨åœ¨æ¨è¿›é€æ­¥è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡æ€§èƒ½ï¼ŒåŒ…æ‹¬å¤šä¸ªç±»åˆ«å’Œè¶…è¿‡4000ä¸ªæ¨ç†æ­¥éª¤ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹çš„è§†è§‰æ¨ç†è´¨é‡è¯„ä¼°æŒ‡æ ‡ï¼Œå…³æ³¨äºæ­¥éª¤çº§åˆ«çš„æ­£ç¡®æ€§å’Œé€»è¾‘è¿è´¯æ€§ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°å‹å¤šæ¨¡æ€è§†è§‰æ¨ç†æ¨¡å‹LlamaV-o1ï¼Œç”¨äºå¤šæ­¥éª¤æ¨ç†ï¼Œå¹¶é‡‡ç”¨ç»“æ„åŒ–è®­ç»ƒèŒƒå¼è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å®éªŒæ˜¾ç¤ºLlamaV-o1æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æœ‰æ˜æ˜¾çš„æ€§èƒ½æå‡ã€‚</li>
<li>LlamaV-o1æ¨¡å‹å…·æœ‰é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦ï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æå‡äº†5å€ã€‚</li>
<li>å…¬å¼€æä¾›äº†åŸºå‡†æµ‹è¯•ã€æ¨¡å‹å’Œä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06186">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45f304408bd58316319ae2ed41c1ffcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae166ab75134aa3ef1476e711c9e1b94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dce15076051c4841274c4cf0ba9b0f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4524fa7745d98ccced57bae6f377af25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3642bc7d44d75488106bd57d58bf1190.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multilingual-Performance-of-a-Multimodal-Artificial-Intelligence-System-on-Multisubject-Physics-Concept-Inventories"><a href="#Multilingual-Performance-of-a-Multimodal-Artificial-Intelligence-System-on-Multisubject-Physics-Concept-Inventories" class="headerlink" title="Multilingual Performance of a Multimodal Artificial Intelligence System   on Multisubject Physics Concept Inventories"></a>Multilingual Performance of a Multimodal Artificial Intelligence System   on Multisubject Physics Concept Inventories</h2><p><strong>Authors:Gerd Kortemeyer, Marina Babayeva, Giulia Polverini, Bor Gregorcic, Ralf Widenhorn</strong></p>
<p>We investigate the multilingual and multimodal performance of a large language model-based artificial intelligence (AI) system, GPT-4o, on a diverse set of physics concept inventories spanning multiple languages and subject areas. The inventories taken from the PhysPort website cover the classical physics topics of mechanics, electromagnetism, optics, and thermodynamics as well as relativity, quantum mechanics, astronomy, mathematics, and laboratory skills. Unlike previous text-only studies, we uploaded the inventories as images mirroring what a student would see on paper, assessing the systemâ€™s multimodal functionality. The AI is prompted in English and autonomously chooses the language of its response - either remaining in the nominal language of the test, switching entirely to English, or mixing languages - revealing adaptive behavior dependent on linguistic complexity and data availability. Our results indicate some variation in performance across subject areas, with laboratory skills standing out as the area of poorest performance. Furthermore, the AIâ€™s performance on questions that require visual interpretation of images is worse than on purely text-based questions. Questions that are difficult for the AI tend to be that way invariably of the inventory language. We also find large variations in performance across languages, with some appearing to benefit substantially from language switching, a phenomenon similar to code-switching ofhuman speakers. Overall, comparing the obtained AI results to the existing literature, we find that the AI system outperforms average undergraduate students post-instruction in all subject areas but laboratory skills. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„äººå·¥æ™ºèƒ½ç³»ç»ŸGPT-4oåœ¨å¤šè¯­è¨€ã€å¤šæ¨¡å¼ç¯å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚è¯¥ç³»ç»Ÿçš„æµ‹è¯•èŒƒå›´æ¶µç›–å¤šä¸ªè¯­è¨€å’Œå­¦ç§‘é¢†åŸŸçš„ç‰©ç†æ¦‚å¿µæ¸…å•ï¼Œè¿™äº›æ¸…å•æ¥è‡ªPhysPortç½‘ç«™ï¼Œæ¶µç›–äº†ç»å…¸ç‰©ç†å­¦çš„åŠ›å­¦ã€ç”µç£å­¦ã€å…‰å­¦ã€çƒ­åŠ›å­¦è¯é¢˜ï¼Œè¿˜åŒ…æ‹¬ç›¸å¯¹è®ºã€é‡å­åŠ›å­¦ã€å¤©æ–‡å­¦ã€æ•°å­¦å’Œå®éªŒæŠ€èƒ½ã€‚ä¸ä»¥å¾€ä»…é’ˆå¯¹æ–‡æœ¬çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬å°†æ¸…å•ä»¥å›¾åƒçš„å½¢å¼ä¸Šä¼ ï¼Œæ¨¡æ‹Ÿå­¦ç”Ÿåœ¨çº¸ä¸Šçœ‹åˆ°çš„å†…å®¹ï¼Œè¯„ä¼°ç³»ç»Ÿçš„å¤šæ¨¡å¼åŠŸèƒ½ã€‚è¯¥ç³»ç»Ÿçš„æç¤ºè¯­ä¸ºè‹±è¯­ï¼Œå¹¶ä¸”èƒ½å¤Ÿç‹¬ç«‹é€‰æ‹©å›åº”çš„è¯­è¨€ï¼Œå¯ä»¥é€‰æ‹©ä¿æŒåœ¨æµ‹è¯•çš„æŒ‡å®šè¯­è¨€ã€å®Œå…¨åˆ‡æ¢åˆ°è‹±è¯­æˆ–æ··åˆä½¿ç”¨å¤šç§è¯­è¨€ï¼Œæ ¹æ®è¯­è¨€å¤æ‚æ€§å’Œæ•°æ®å¯ç”¨æ€§å±•ç°å‡ºé€‚åº”æ€§çš„è¡Œä¸ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸åŒå­¦ç§‘é¢†åŸŸçš„è¡¨ç°å­˜åœ¨ä¸€å®šå·®å¼‚ï¼Œå®éªŒæŠ€èƒ½æ˜¯è¡¨ç°è¾ƒå·®çš„é¢†åŸŸã€‚æ­¤å¤–ï¼Œå¯¹äºéœ€è¦å›¾åƒè§†è§‰è§£é‡Šçš„é—®é¢˜ï¼ŒAIçš„è¡¨ç°ä¸å¦‚åŸºäºçº¯æ–‡æœ¬çš„é—®é¢˜ã€‚å¯¹äºAIè€Œè¨€ï¼Œéš¾ä»¥å›ç­”çš„é—®é¢˜å¾€å¾€ä¸æ¸…å•çš„è¯­è¨€æœ‰å…³ã€‚æˆ‘ä»¬è¿˜å‘ç°ä¸åŒè¯­è¨€ä¹‹é—´çš„æ€§èƒ½å·®å¼‚å¾ˆå¤§ï¼Œæœ‰äº›è¯­è¨€åœ¨åˆ‡æ¢æ—¶ä¼šå¤§å¤§å—ç›Šï¼Œè¿™ç§ç°è±¡ç±»ä¼¼äºäººç±»è¯´è¯è€…çš„ä»£ç è½¬æ¢ã€‚æ€»ä½“è€Œè¨€ï¼Œå°†äººå·¥æ™ºèƒ½çš„ç»“æœä¸ç°æœ‰æ–‡çŒ®è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å‘ç°è¯¥AIç³»ç»Ÿåœ¨æ‰€æœ‰å­¦ç§‘é¢†åŸŸä¸­çš„è¡¨ç°è¶…è¿‡äº†å¹³å‡æœ¬ç§‘ç”Ÿçš„è¡¨ç°ï¼Œä½†åœ¨å®éªŒæŠ€èƒ½æ–¹é¢é™¤å¤–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06143v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç ”ç©¶å›¢é˜Ÿå¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„AIç³»ç»ŸGPT-4oåœ¨å¤šè¯­è¨€ã€å¤šæ¨¡æ€ç¯å¢ƒä¸‹çš„ç‰©ç†æ¦‚å¿µåº“è¡¨ç°è¿›è¡Œäº†è°ƒæŸ¥ã€‚æµ‹è¯•æ¶µç›–äº†å¤šä¸ªè¯­è¨€å’Œä¸»é¢˜çš„ç‰©ç†æ¦‚å¿µåº“ï¼ŒåŒ…æ‹¬ç»å…¸ç‰©ç†ã€ç›¸å¯¹è®ºã€é‡å­åŠ›å­¦ã€å¤©æ–‡å­¦ã€æ•°å­¦å’Œå®éªŒæŠ€èƒ½ç­‰é¢†åŸŸã€‚ç ”ç©¶é€šè¿‡ä¸Šä¼ å›¾åƒå½¢å¼çš„é¢˜åº“æ¥è¯„ä¼°AIçš„å¤šæ¨¡æ€åŠŸèƒ½ï¼Œæ¨¡æ‹Ÿå­¦ç”Ÿçº¸ä¸Šä½œç­”çš„åœºæ™¯ã€‚AIåœ¨è‹±è¯­æç¤ºä¸‹å¯è‡ªä¸»é€‰æ‹©å›ç­”è¯­è¨€ï¼Œå±•ç°å‡ºäº†æ ¹æ®è¯­è¨€å¤æ‚åº¦å’Œæ•°æ®å¯ç”¨æ€§çš„è‡ªé€‚åº”è¡Œä¸ºã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸åŒä¸»é¢˜é¢†åŸŸçš„è¡¨ç°æœ‰æ‰€å·®å¼‚ï¼Œå®éªŒæŠ€èƒ½é¢†åŸŸè¡¨ç°æœ€å·®ã€‚æ­¤å¤–ï¼Œå¯¹äºéœ€è¦å›¾åƒè§£è¯»çš„é—®é¢˜ï¼ŒAIçš„è¡¨ç°è¾ƒçº¯æ–‡æœ¬é—®é¢˜æ›´å·®ã€‚å›°éš¾é—®é¢˜çš„å…±æ€§åœ¨äºé¢˜åº“è¯­è¨€ã€‚ä¸åŒè¯­è¨€é—´çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œéƒ¨åˆ†è¯­è¨€åˆ‡æ¢æœ‰åŠ©äºæå‡è¡¨ç°ï¼Œç±»ä¼¼äºäººç±»çš„è¯­è¨€åˆ‡æ¢ç°è±¡ã€‚æ€»ä½“è€Œè¨€ï¼Œç›¸è¾ƒäºç°æœ‰æ–‡çŒ®ä¸­çš„å¤§å­¦ç”Ÿï¼ŒAIç³»ç»Ÿåœ¨å¤§éƒ¨åˆ†é¢†åŸŸè¡¨ç°æ›´ä½³ï¼Œå”¯ç‹¬åœ¨å®éªŒæŠ€èƒ½æ–¹é¢è¿˜æœ‰å¾…æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oåœ¨å¤šè¯­è¨€å’Œå¤šæ¨¡æ€ç¯å¢ƒä¸‹çš„ç‰©ç†æ¦‚å¿µåº“è¡¨ç°è¢«ç ”ç©¶ã€‚</li>
<li>æµ‹è¯•æ¶µç›–äº†å¤šä¸ªè¯­è¨€å’Œä¸»é¢˜çš„ç‰©ç†æ¦‚å¿µï¼ŒåŒ…æ‹¬ç»å…¸ç‰©ç†å’Œå…¶ä»–å¤šä¸ªé¢†åŸŸã€‚</li>
<li>AIåœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸‹è¿›è¡Œè¯„ä¼°ï¼Œå¯ä»¥è‡ªä¸»é€‰æ‹©å›ç­”è¯­è¨€ï¼Œå±•ç¤ºå‡ºè‡ªé€‚åº”è¡Œä¸ºã€‚</li>
<li>AIåœ¨ä¸åŒä¸»é¢˜é¢†åŸŸçš„è¡¨ç°å­˜åœ¨å·®å¼‚ï¼Œå®éªŒæŠ€èƒ½é¢†åŸŸè¡¨ç°æœ€å·®ã€‚</li>
<li>AIåœ¨è§£è¯»å›¾åƒç±»é—®é¢˜ä¸Šçš„è¡¨ç°è¾ƒçº¯æ–‡æœ¬é—®é¢˜æ›´å·®ã€‚</li>
<li>å›°éš¾é—®é¢˜çš„å…±æ€§åœ¨äºé¢˜åº“è¯­è¨€ã€‚</li>
<li>AIåœ¨è¯­è¨€åˆ‡æ¢æ–¹é¢çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œéƒ¨åˆ†è¯­è¨€åˆ‡æ¢æœ‰åŠ©äºæé«˜è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f98eff55ae772b1cb6e7204c38f57d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80103412d3d2f18227e59b49df63ce96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0980e4c689ca66396d2637e849bdc419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93aa0f6035b70ee43fa050001c220236.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e3d5231f93a57c905d4426fe89ee5e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e60690ef94dcb04f14075b653cf72675.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Supervision-policies-can-shape-long-term-risk-management-in-general-purpose-AI-models"><a href="#Supervision-policies-can-shape-long-term-risk-management-in-general-purpose-AI-models" class="headerlink" title="Supervision policies can shape long-term risk management in   general-purpose AI models"></a>Supervision policies can shape long-term risk management in   general-purpose AI models</h2><p><strong>Authors:Manuel Cebrian, Emilia Gomez, David Fernandez Llorca</strong></p>
<p>The rapid proliferation and deployment of General-Purpose AI (GPAI) models, including large language models (LLMs), present unprecedented challenges for AI supervisory entities. We hypothesize that these entities will need to navigate an emergent ecosystem of risk and incident reporting, likely to exceed their supervision capacity. To investigate this, we develop a simulation framework parameterized by features extracted from the diverse landscape of risk, incident, or hazard reporting ecosystems, including community-driven platforms, crowdsourcing initiatives, and expert assessments. We evaluate four supervision policies: non-prioritized (first-come, first-served), random selection, priority-based (addressing the highest-priority risks first), and diversity-prioritized (balancing high-priority risks with comprehensive coverage across risk types). Our results indicate that while priority-based and diversity-prioritized policies are more effective at mitigating high-impact risks, particularly those identified by experts, they may inadvertently neglect systemic issues reported by the broader community. This oversight can create feedback loops that amplify certain types of reporting while discouraging others, leading to a skewed perception of the overall risk landscape. We validate our simulation results with several real-world datasets, including one with over a million ChatGPT interactions, of which more than 150,000 conversations were identified as risky. This validation underscores the complex trade-offs inherent in AI risk supervision and highlights how the choice of risk management policies can shape the future landscape of AI risks across diverse GPAI models used in society. </p>
<blockquote>
<p>é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆGPAIï¼‰æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿæ‰©æ•£å’Œéƒ¨ç½²ï¼Œç»™äººå·¥æ™ºèƒ½ç›‘ç®¡å®ä½“å¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‡è®¾è¿™äº›å®ä½“éœ€è¦åœ¨ä¸€ä¸ªæ–°å…´çš„å……æ»¡é£é™©å’Œäº‹æ•…æŠ¥å‘Šçš„ç¯å¢ƒä¸­å¯¼èˆªï¼Œè¿™ä¸ªç¯å¢ƒå¯èƒ½ä¼šè¶…å‡ºå…¶ç›‘ç®¡èƒ½åŠ›ã€‚ä¸ºäº†è°ƒæŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¨¡æ‹Ÿæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä»å¤šæ ·åŒ–çš„é£é™©ã€äº‹æ•…æˆ–å±å®³æŠ¥å‘Šç”Ÿæ€ç³»ç»Ÿæå–çš„ç‰¹å¾è¿›è¡Œå‚æ•°åŒ–è®¾ç½®ï¼Œè¿™äº›ç”Ÿæ€ç³»ç»ŸåŒ…æ‹¬ç¤¾åŒºé©±åŠ¨çš„å¹³å°ã€ä¼—åŒ…å€¡è®®å’Œä¸“å®¶è¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°äº†å››ç§ç›‘ç®¡æ”¿ç­–ï¼šéä¼˜å…ˆï¼ˆå…ˆåˆ°å…ˆå¾—ï¼‰ã€éšæœºé€‰æ‹©ã€åŸºäºä¼˜å…ˆçº§çš„ï¼ˆå…ˆè§£å†³æœ€é«˜é£é™©çš„é—®é¢˜ï¼‰å’Œä¼˜å…ˆè€ƒè™‘å¤šæ ·æ€§çš„ï¼ˆå¹³è¡¡é«˜é£é™©ä¸ä¸åŒç±»å‹é£é™©çš„å…¨é¢è¦†ç›–ï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶åŸºäºä¼˜å…ˆçº§å’Œä¼˜å…ˆè€ƒè™‘å¤šæ ·æ€§çš„æ”¿ç­–åœ¨ç¼“è§£é«˜é£é™©æ–¹é¢æ›´ä¸ºæœ‰æ•ˆï¼Œå°¤å…¶æ˜¯ä¸“å®¶æ‰€è¯†åˆ«çš„é«˜é£é™©ï¼Œä½†å®ƒä»¬å¯èƒ½ä¼šæ— æ„ä¸­å¿½è§†ç¤¾åŒºæŠ¥å‘Šçš„æ›´ä¸ºå¹¿æ³›çš„é—®é¢˜ã€‚è¿™ç§ç›‘ç£ç–å¿½å¯èƒ½ä¼šäº§ç”Ÿåé¦ˆå¾ªç¯ï¼Œæ”¾å¤§æŸäº›ç±»å‹çš„æŠ¥å‘Šï¼ŒåŒæ—¶éåˆ¶å…¶ä»–ç±»å‹çš„æŠ¥å‘Šï¼Œä»è€Œå¯¼è‡´å¯¹æ•´ä½“é£é™©æ™¯è§‚çš„åé¢‡è®¤è¯†ã€‚æˆ‘ä»¬é€šè¿‡å¤šä¸ªç°å®ä¸–ç•Œçš„æ•°æ®é›†éªŒè¯äº†æ¨¡æ‹Ÿç»“æœçš„æœ‰æ•ˆæ€§ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªåŒ…å«è¶…è¿‡ä¸€ç™¾ä¸‡æ¬¡çš„ChatGPTäº¤äº’çš„æ•°æ®é›†ï¼Œå…¶ä¸­è¶…è¿‡15ä¸‡æ¬¡å¯¹è¯è¢«è¯†åˆ«ä¸ºå…·æœ‰é£é™©ã€‚è¿™ä¸€éªŒè¯å¼ºè°ƒäº†äººå·¥æ™ºèƒ½é£é™©ç›‘ç®¡å›ºæœ‰çš„å¤æ‚æƒè¡¡ï¼Œå¹¶çªå‡ºäº†é£é™©ç®¡ç†æ”¿ç­–çš„é€‰æ‹©å¦‚ä½•å¡‘é€ ç¤¾ä¼šä½¿ç”¨çš„å„ç§GPAIæ¨¡å‹çš„äººå·¥æ™ºèƒ½é£é™©æœªæ¥æ ¼å±€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06137v1">PDF</a> 24 pages, 14 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆGPAIï¼‰æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿…é€Ÿå‘å±•å’Œéƒ¨ç½²ç»™äººå·¥æ™ºèƒ½ç›‘ç®¡æœºæ„å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªæ¨¡æ‹Ÿæ¡†æ¶ï¼Œè¯„ä¼°äº†å››ç§ç›‘ç®¡æ”¿ç­–çš„æ•ˆæœï¼Œå‘ç°ä¼˜å…ˆå¤„ç†é«˜é£é™©é—®é¢˜å¹¶å…¼é¡¾é£é™©ç±»å‹å¤šæ ·æ€§çš„æ”¿ç­–èƒ½æœ‰æ•ˆç¼“è§£é«˜é£é™©å½±å“ï¼Œä½†å¯èƒ½å¿½è§†ç¤¾åŒºåæ˜ çš„ç³»ç»Ÿæ€§é—®é¢˜ï¼Œé€ æˆåé¦ˆå¾ªç¯å¹¶æ‰­æ›²æ•´ä½“é£é™©è®¤çŸ¥ã€‚ç ”ç©¶é€šè¿‡åŒ…æ‹¬ç™¾ä¸‡æ¬¡ChatGPTäº¤äº’åœ¨å†…çš„çœŸå®æ•°æ®é›†éªŒè¯äº†æ¨¡æ‹Ÿç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šç”¨äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆGPAIï¼‰çš„å¿«é€Ÿéƒ¨ç½²ç»™AIç›‘ç®¡æœºæ„å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>éœ€è¦å»ºç«‹ä¸€ä¸ªåº”å¯¹é£é™©çš„æ¨¡æ‹Ÿæ¡†æ¶ã€‚</li>
<li>å››ç§ç›‘ç®¡æ”¿ç­–æ¨¡æ‹Ÿè¯„ä¼°å‘ç°ä¼˜å…ˆå¤„ç†é«˜é£é™©å¹¶å…¼é¡¾é£é™©å¤šæ ·æ€§çš„æ”¿ç­–æ›´æœ‰æ•ˆã€‚</li>
<li>ä¼˜å…ˆå¤„ç†é«˜é£é™©çš„æ”¿ç­–å¯èƒ½å¿½è§†ç¤¾åŒºåæ˜ çš„ç³»ç»Ÿæ€§é—®é¢˜ã€‚</li>
<li>å¿½è§†ç¤¾åŒºåé¦ˆå¯èƒ½å½¢æˆåé¦ˆå¾ªç¯å¹¶æ‰­æ›²æ•´ä½“é£é™©è®¤çŸ¥ã€‚</li>
<li>æ¨¡æ‹Ÿç»“æœé€šè¿‡çœŸå®æ•°æ®é›†éªŒè¯ï¼ŒåŒ…æ‹¬ç™¾ä¸‡æ¬¡ChatGPTäº¤äº’æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b043833b98bc47ac4bfa7672490b5217.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdc6367ae46853359c51f0747fd831ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8181ef46672ea374a2a93b5420ecdd33.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Contextual-ASR-Error-Handling-with-LLMs-Augmentation-for-Goal-Oriented-Conversational-AI"><a href="#Contextual-ASR-Error-Handling-with-LLMs-Augmentation-for-Goal-Oriented-Conversational-AI" class="headerlink" title="Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented   Conversational AI"></a>Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented   Conversational AI</h2><p><strong>Authors:Yuya Asano, Sabit Hassan, Paras Sharma, Anthony Sicilia, Katherine Atwell, Diane Litman, Malihe Alikhani</strong></p>
<p>General-purpose automatic speech recognition (ASR) systems do not always perform well in goal-oriented dialogue. Existing ASR correction methods rely on prior user data or named entities. We extend correction to tasks that have no prior user data and exhibit linguistic flexibility such as lexical and syntactic variations. We propose a novel context augmentation with a large language model and a ranking strategy that incorporates contextual information from the dialogue states of a goal-oriented conversational AI and its tasks. Our method ranks (1) n-best ASR hypotheses by their lexical and semantic similarity with context and (2) context by phonetic correspondence with ASR hypotheses. Evaluated in home improvement and cooking domains with real-world users, our method improves recall and F1 of correction by 34% and 16%, respectively, while maintaining precision and false positive rate. Users rated .8-1 point (out of 5) higher when our correction method worked properly, with no decrease due to false positives. </p>
<blockquote>
<p>é€šç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨é¢å‘ç›®æ ‡çš„å¯¹è¯ä¸­å¹¶ä¸æ€»æ˜¯è¡¨ç°è‰¯å¥½ã€‚ç°æœ‰çš„ASRæ ¡æ­£æ–¹æ³•ä¾èµ–äºç”¨æˆ·å…ˆå‰æ•°æ®æˆ–å‘½åå®ä½“ã€‚æˆ‘ä»¬å°†æ ¡æ­£æ‰©å±•åˆ°æ²¡æœ‰ç”¨æˆ·å…ˆå‰æ•°æ®ä¸”è¡¨ç°å‡ºè¯­è¨€çµæ´»æ€§çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è¯æ±‡å’Œå¥æ³•å˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ä¸Šä¸‹æ–‡æ‰©å……æ–¹æ³•ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ’åç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç»“åˆäº†é¢å‘ç›®æ ‡å¯¹è¯AIçš„ä¼šè¯çŠ¶æ€åŠå…¶ä»»åŠ¡ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ï¼ˆ1ï¼‰è¯æ±‡å’Œè¯­ä¹‰ä¸ä¸Šä¸‹æ–‡çš„ç›¸ä¼¼åº¦å¯¹æœ€ä½³ASRå‡è®¾è¿›è¡Œæ’åï¼Œï¼ˆ2ï¼‰é€šè¿‡éŸ³éŸµä¸ASRå‡è®¾çš„å¯¹åº”æ¥æ’åä¸Šä¸‹æ–‡ã€‚åœ¨å®¶åº­æ”¹å–„å’Œçƒ¹é¥ªé¢†åŸŸè¿›è¡Œç°å®ä¸–ç•Œç”¨æˆ·è¯„ä¼°æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†å¬å›ç‡å’Œæ ¡æ­£F1å€¼åˆ†åˆ«æé«˜äº†34%å’Œ16%ï¼ŒåŒæ—¶ä¿æŒäº†ç²¾ç¡®åº¦å’Œè¯¯æŠ¥ç‡ã€‚ç”¨æˆ·è¯„ä»·ï¼ˆåœ¨æ­£å¸¸å·¥ä½œæƒ…å†µä¸‹ï¼‰æ¯”åŸå…ˆæé«˜äº†0.8-1åˆ†ï¼ˆæ»¡åˆ†5åˆ†ï¼‰ï¼Œå¹¶ä¸”æ²¡æœ‰å› ä¸ºè¯¯æŠ¥è€Œé™ä½è¯„åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06129v1">PDF</a> Accepted to COLING 2025 Industry Track</p>
<p><strong>Summary</strong></p>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨ç›®æ ‡å¯¼å‘å¯¹è¯ä¸­çš„è¡¨ç°å¹¶ä¸æ€»æ˜¯ç†æƒ³ã€‚ç°æœ‰ASRæ ¡æ­£æ–¹æ³•ä¾èµ–äºç”¨æˆ·å…ˆéªŒæ•°æ®æˆ–å‘½åå®ä½“ã€‚æˆ‘ä»¬å°†å…¶æ‰©å±•åˆ°æ²¡æœ‰ç”¨æˆ·å…ˆéªŒæ•°æ®ä¸”è¡¨ç°å‡ºè¯­è¨€çµæ´»æ€§çš„ä»»åŠ¡ä¸­ï¼Œå¦‚è¯æ±‡å’Œå¥æ³•å˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸Šä¸‹æ–‡æ‰©å……æ–¹æ³•ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ’åç­–ç•¥ï¼Œè¯¥ç­–ç•¥èå…¥äº†ç›®æ ‡å¯¼å‘å¯¹è¯AIçš„å¯¹è¯çŠ¶æ€å’Œä»»åŠ¡ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ï¼ˆ1ï¼‰è¯æ³•å’Œè¯­ä¹‰ç›¸ä¼¼æ€§å¯¹ASRå‡è®¾è¿›è¡Œæ’åï¼Œå¹¶ä¸ä¸Šä¸‹æ–‡å¯¹æ¯”ï¼›ï¼ˆ2ï¼‰é€šè¿‡éŸ³ç´ å¯¹åº”ä¸ASRå‡è®¾è¿›è¡Œæ’åã€‚åœ¨å®¶åº­æ”¹å–„å’Œçƒ¹é¥ªé¢†åŸŸè¿›è¡ŒçœŸå®ç”¨æˆ·è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†æ ¡æ­£çš„å¬å›ç‡å’ŒF1å€¼åˆ†åˆ«æé«˜äº†34%å’Œ16%ï¼ŒåŒæ—¶ä¿æŒç²¾ç¡®åº¦å’Œè¯¯æŠ¥ç‡ã€‚ç”¨æˆ·å¯¹æˆ‘ä»¬çš„æ ¡æ­£æ–¹æ³•ç»™äºˆäº†æ›´é«˜çš„è¯„ä»·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASRç³»ç»Ÿåœ¨ç›®æ ‡å¯¼å‘å¯¹è¯ä¸­çš„è¡¨ç°æœ‰å¾…æé«˜ã€‚</li>
<li>ç°æœ‰ASRæ ¡æ­£æ–¹æ³•ä¸»è¦ä¾èµ–äºç”¨æˆ·å…ˆéªŒæ•°æ®æˆ–å‘½åå®ä½“ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¸Šä¸‹æ–‡æ‰©å……æ–¹æ³•ï¼Œç”¨äºæ‰©å±•ASRæ ¡æ­£åˆ°æ²¡æœ‰ç”¨æˆ·å…ˆéªŒæ•°æ®çš„ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ’åç­–ç•¥ï¼Œè€ƒè™‘äº†å¯¹è¯çŠ¶æ€å’Œä»»åŠ¡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æ–¹æ³•é€šè¿‡è¯æ³•å’Œè¯­ä¹‰ç›¸ä¼¼æ€§å¯¹ASRå‡è®¾è¿›è¡Œæ’åï¼ŒåŒæ—¶è€ƒè™‘äº†ä¸ä¸Šä¸‹æ–‡çš„å¯¹æ¯”ä»¥åŠéŸ³ç´ å¯¹åº”ã€‚</li>
<li>åœ¨çœŸå®ç”¨æˆ·è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ ¡æ­£çš„å¬å›ç‡å’ŒF1å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1123fb1a419f1b14f5e4763606ae767a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27a9e69a88beb6f1bc6251cafa66b150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70287212c397852caf7e1cc7a3806de6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7de331c4d9268a4a2c9b7dc6adbd524a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbadfa42a9edf06c53fb8824e5e29b03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0657d63283fe94440f120d90d962afd.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="From-Conversation-to-Automation-Leveraging-Large-Language-Models-to-Analyze-Strategies-in-Problem-Solving-Therapy"><a href="#From-Conversation-to-Automation-Leveraging-Large-Language-Models-to-Analyze-Strategies-in-Problem-Solving-Therapy" class="headerlink" title="From Conversation to Automation: Leveraging Large Language Models to   Analyze Strategies in Problem Solving Therapy"></a>From Conversation to Automation: Leveraging Large Language Models to   Analyze Strategies in Problem Solving Therapy</h2><p><strong>Authors:Elham Aghakhani, Lu Wang, Karla T. Washington, George Demiris, Jina Huh-Yoo, Rezvaneh Rezapour</strong></p>
<p>Problem-solving therapy (PST) is a structured psychological approach that helps individuals manage stress and resolve personal issues by guiding them through problem identification, solution brainstorming, decision-making, and outcome evaluation. As mental health care increasingly integrates technologies like chatbots and large language models (LLMs), understanding how PST can be effectively automated is important. This study leverages anonymized therapy transcripts to analyze and classify therapeutic interventions using various LLMs and transformer-based models. Our results show that GPT-4o achieved the highest accuracy (0.76) in identifying PST strategies, outperforming other models. Additionally, we introduced a new dimension of communication strategies that enhances the current PST framework, offering deeper insights into therapist-client interactions. This research demonstrates the potential of LLMs to automate complex therapeutic dialogue analysis, providing a scalable, efficient tool for mental health interventions. Our annotation framework can enhance the accessibility, effectiveness, and personalization of PST, supporting therapists in real-time with more precise, targeted interventions. </p>
<blockquote>
<p>é—®é¢˜è§£å†³ç–—æ³•ï¼ˆPSTï¼‰æ˜¯ä¸€ç§ç»“æ„åŒ–å¿ƒç†æ–¹æ³•ï¼Œé€šè¿‡å¼•å¯¼ä¸ªäººè¯†åˆ«é—®é¢˜ã€é›†æ€å¹¿ç›Šæ‰¾è§£å†³æ–¹æ¡ˆã€åšå‡ºå†³å®šå’Œè¯„ä¼°ç»“æœï¼Œå¸®åŠ©ä»–ä»¬ç®¡ç†å‹åŠ›å¹¶è§£å†³ä¸ªäººé—®é¢˜ã€‚éšç€å¿ƒç†å¥åº·æŠ¤ç†è¶Šæ¥è¶Šå¤šåœ°æ•´åˆèŠå¤©æœºå™¨äººå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç­‰æŠ€æœ¯ï¼Œäº†è§£å¦‚ä½•æœ‰æ•ˆåœ°è‡ªåŠ¨åŒ–PSTéå¸¸é‡è¦ã€‚æœ¬ç ”ç©¶ä½¿ç”¨åŒ¿åæ²»ç–—è®°å½•æ¥åˆ†æå¹¶åˆ©ç”¨å„ç§LLMå’ŒåŸºäºè½¬æ¢å™¨çš„æ¨¡å‹å¯¹æ²»ç–—å¹²é¢„è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒGPT-4oåœ¨è¯†åˆ«PSTç­–ç•¥æ–¹é¢è¾¾åˆ°äº†æœ€é«˜å‡†ç¡®ç‡ï¼ˆ0.76ï¼‰ï¼Œä¼˜äºå…¶ä»–æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¢å¼ºå½“å‰PSTæ¡†æ¶çš„æ²Ÿé€šç­–ç•¥æ–°ç»´åº¦ï¼Œä¸ºæ²»ç–—å¸ˆä¸å®¢æˆ·çš„äº’åŠ¨æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†LLMè‡ªåŠ¨åŒ–å¤æ‚æ²»ç–—å¯¹è¯åˆ†æçš„æ½œåŠ›ï¼Œä¸ºå¿ƒç†å¥åº·å¹²é¢„æä¾›äº†å¯æ‰©å±•ã€é«˜æ•ˆçš„å·¥å…·ã€‚æˆ‘ä»¬çš„æ³¨é‡Šæ¡†æ¶å¯ä»¥æé«˜PSTçš„æ™®åŠæ€§ã€æœ‰æ•ˆæ€§å’Œä¸ªæ€§åŒ–ç¨‹åº¦ï¼Œä¸ºæ²»ç–—å¸ˆæä¾›å®æ—¶æ”¯æŒï¼Œè¿›è¡Œæ›´ç²¾ç¡®ã€æœ‰é’ˆå¯¹æ€§çš„å¹²é¢„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06101v1">PDF</a> 16 pages</p>
<p><strong>Summary</strong><br>PSTï¼ˆé—®é¢˜è§£å†³ç–—æ³•ï¼‰æ˜¯ä¸€ç§ç»“æ„åŒ–å¿ƒç†æ–¹æ³•ï¼Œé€šè¿‡å¼•å¯¼ä¸ªäººè¯†åˆ«é—®é¢˜ã€è§£å†³éš¾é¢˜ã€è¿›è¡Œå†³ç­–è¯„ä¼°å’Œæˆæœè¯„ä¼°æ¥å¸®åŠ©ä¸ªäººåº”å¯¹å‹åŠ›å’Œè§£å†³ä¸ªäººé—®é¢˜ã€‚éšç€ç²¾ç¥å«ç”ŸæŠ¤ç†è¶Šæ¥è¶Šå¤šåœ°èå…¥èŠå¤©æœºå™¨äººç­‰å¤§å‹è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œç†è§£å¦‚ä½•æœ‰æ•ˆåœ°è‡ªåŠ¨åŒ–PSTå˜å¾—è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶ä½¿ç”¨åŒ¿åæ²»ç–—è®°å½•åˆ†æå¹¶åˆ†ç±»æ²»ç–—å¹²é¢„æªæ–½ï¼Œæ¶‰åŠå¤šç§å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ã€‚ç»“æœè¡¨æ˜GPT-4oåœ¨è¯†åˆ«PSTç­–ç•¥æ–¹é¢å–å¾—äº†æœ€é«˜ç²¾ç¡®åº¦ï¼ˆå‡†ç¡®ç‡ä¸º0.76ï¼‰ï¼Œè¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æ–°çš„æ²Ÿé€šç­–ç•¥ç»´åº¦ï¼Œå¢å¼ºäº†å½“å‰çš„PSTæ¡†æ¶ï¼Œæä¾›äº†å¯¹æ²»ç–—å¸ˆä¸æ‚£è€…äº’åŠ¨çš„æ·±å…¥äº†è§£ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–å¤æ‚çš„æ²»ç–—å¯¹è¯åˆ†ææ–¹é¢çš„æ½œåŠ›ï¼Œæä¾›äº†ä¸€ç§ç”¨äºç²¾ç¥å¥åº·å¹²é¢„çš„å¯æ‰©å±•é«˜æ•ˆå·¥å…·ã€‚å…¶æ³¨é‡Šæ¡†æ¶å¯ä»¥æå‡PSTçš„ä¾¿æ·æ€§ã€æ•ˆæœå’Œä¸ªæ€§åŒ–ç¨‹åº¦ï¼Œæ”¯æŒæ²»ç–—å¸ˆè¿›è¡Œæ›´ç²¾ç¡®ã€æœ‰é’ˆå¯¹æ€§çš„å¹²é¢„æªæ–½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é—®é¢˜è§£å†³ç–—æ³•ï¼ˆPSTï¼‰æ˜¯ä¸€ç§ç»“æ„åŒ–çš„å¿ƒç†æ–¹æ³•ï¼Œç”¨ä»¥å¼•å¯¼ä¸ªäººé¢å¯¹å’Œè§£å†³è‡ªå·±çš„é—®é¢˜ã€‚</li>
<li>éšç€ç§‘æŠ€çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èå…¥ï¼ŒPSTçš„è‡ªåŠ¨åŒ–æˆä¸ºç ”ç©¶ç„¦ç‚¹ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨åŒ¿åæ²»ç–—è®°å½•æ¥åˆ†æå’Œåˆ†ç±»æ²»ç–—å¹²é¢„æªæ–½ï¼ŒåŒ…æ‹¬ä½¿ç”¨å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ã€‚</li>
<li>GPT-4oæ¨¡å‹åœ¨è¯†åˆ«PSTç­–ç•¥æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸º0.76ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†æ–°çš„æ²Ÿé€šç­–ç•¥ç»´åº¦ï¼Œå¢å¼ºäº†æˆ‘ä»¬å¯¹æ²»ç–—å¸ˆä¸æ‚£è€…äº¤æµçš„ç†è§£ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–æ²»ç–—å¯¹è¯åˆ†ææ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œå¯ä¸ºç²¾ç¥å¥åº·å¹²é¢„æä¾›é«˜æ•ˆå·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b6d094197cf47299b3b7c2a98c336fa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ef30c15baa94fef60726bf9f4bcf330.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e5bda69624a362260f2fc81c049f916.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hermit-Kingdom-Through-the-Lens-of-Multiple-Perspectives-A-Case-Study-of-LLM-Hallucination-on-North-Korea"><a href="#Hermit-Kingdom-Through-the-Lens-of-Multiple-Perspectives-A-Case-Study-of-LLM-Hallucination-on-North-Korea" class="headerlink" title="Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study   of LLM Hallucination on North Korea"></a>Hermit Kingdom Through the Lens of Multiple Perspectives: A Case Study   of LLM Hallucination on North Korea</h2><p><strong>Authors:Eunjung Cho, Won Ik Cho, Soomin Seo</strong></p>
<p>Hallucination in large language models (LLMs) remains a significant challenge for their safe deployment, particularly due to its potential to spread misinformation. Most existing solutions address this challenge by focusing on aligning the models with credible sources or by improving how models communicate their confidence (or lack thereof) in their outputs. While these measures may be effective in most contexts, they may fall short in scenarios requiring more nuanced approaches, especially in situations where access to accurate data is limited or determining credible sources is challenging. In this study, we take North Korea - a country characterised by an extreme lack of reliable sources and the prevalence of sensationalist falsehoods - as a case study. We explore and evaluate how some of the best-performing multilingual LLMs and specific language-based models generate information about North Korea in three languages spoken in countries with significant geo-political interests: English (United States, United Kingdom), Korean (South Korea), and Mandarin Chinese (China). Our findings reveal significant differences, suggesting that the choice of model and language can lead to vastly different understandings of North Korea, which has important implications given the global security challenges the country poses. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰ä»æ˜¯å…¶å®‰å…¨éƒ¨ç½²çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å› ä¸ºå…¶ä¼ æ’­è¯¯å¯¼ä¿¡æ¯çš„å¯èƒ½æ€§ã€‚ç°æœ‰çš„è§£å†³æ–¹æ¡ˆå¤§å¤šä¾§é‡äºé€šè¿‡å°†æ¨¡å‹ä¸å¯é æ¥æºå¯¹é½æˆ–æ”¹è¿›æ¨¡å‹å¦‚ä½•ä¼ è¾¾å…¶è¾“å‡ºä¸­çš„ä¿¡å¿ƒï¼ˆæˆ–ç¼ºä¹ä¿¡å¿ƒï¼‰æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚è™½ç„¶è¿™äº›æªæ–½åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å¯èƒ½æœ‰æ•ˆï¼Œä½†åœ¨éœ€è¦æ›´å¾®å¦™æ–¹æ³•çš„æƒ…å†µä¸‹å¯èƒ½ä¼šå¤±æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ— æ³•è·å¾—å‡†ç¡®æ•°æ®æˆ–ç¡®å®šå¯é æ¥æºå…·æœ‰æŒ‘æˆ˜æ€§çš„æƒ…å†µä¸‹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»¥æœé²œä¸ºä¾‹â€”â€”ä¸€ä¸ªä»¥æåº¦ç¼ºä¹å¯é ä¿¡æ¯å’Œæµè¡Œè€¸äººå¬é—»çš„è™šå‡ä¿¡æ¯ä¸ºç‰¹ç‚¹çš„å›½å®¶ã€‚æˆ‘ä»¬æ¢ç´¢å¹¶è¯„ä¼°äº†ä¸€äº›è¡¨ç°æœ€ä½³çš„å¤šè¯­è¨€LLMå’ŒåŸºäºç‰¹å®šè¯­è¨€çš„æ¨¡å‹å¦‚ä½•ç”Ÿæˆå…³äºæœé²œçš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸‰ç§åœ¨å…·æœ‰é‡å¤§åœ°ç¼˜æ”¿æ²»åˆ©ç›Šçš„å›½å®¶ä¸­ä½¿ç”¨çš„è¯­è¨€ï¼šè‹±è¯­ï¼ˆç¾å›½ã€è‹±å›½ï¼‰ã€éŸ©è¯­ï¼ˆéŸ©å›½ï¼‰å’Œä¸­æ–‡æ™®é€šè¯ï¼ˆä¸­å›½ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºäº†æ˜¾è‘—å·®å¼‚ï¼Œè¿™è¡¨æ˜æ‰€é€‰æ¨¡å‹å’Œè¯­è¨€çš„ä¸åŒå¯èƒ½å¯¼è‡´å¯¹æœé²œçš„ç†è§£å¤§ç›¸å¾„åº­ï¼Œè€ƒè™‘åˆ°æœé²œå¯¹å…¨çƒå®‰å…¨å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¿™ä¸€å‘ç°å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05981v1">PDF</a> Accepted at COLING 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰ç°è±¡å¯¹æ¨¡å‹çš„å®‰å…¨éƒ¨ç½²æ„æˆé‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å…¶ä¼ æ’­é”™è¯¯ä¿¡æ¯çš„æ½œåŠ›ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆä¸»è¦é€šè¿‡ä¸å¯é æ•°æ®æºå¯¹é½æ¨¡å‹æˆ–æ”¹è¿›æ¨¡å‹å¯¹è¾“å‡ºçš„ä¿¡å¿ƒæ²Ÿé€šæ¥è§£å†³æ­¤é—®é¢˜ã€‚è¿™äº›æªæ–½è™½ç„¶é€‚ç”¨äºå¤§å¤šæ•°æƒ…å†µï¼Œä½†åœ¨éœ€è¦æ›´ç²¾ç»†å¤„ç†çš„æƒ…å†µä¸‹å¯èƒ½ä¸è¶³å¤Ÿæœ‰æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ— æ³•è·å¾—å‡†ç¡®æ•°æ®æˆ–éš¾ä»¥ç¡®å®šå¯é æ•°æ®æºçš„æƒ…å†µä¸‹ã€‚æœ¬ç ”ç©¶ä»¥æœé²œä¸ºä¾‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç¼ºä¹å¯é ä¿¡æ¯æºä¸”æµè¡Œè€¸äººå¬é—»çš„è™šå‡ä¿¡æ¯çš„å›½å®¶ã€‚æˆ‘ä»¬æ¢ç´¢å¹¶è¯„ä¼°äº†ä¸€äº›è¡¨ç°æœ€å¥½çš„å¤šè¯­è¨€LLMå’ŒåŸºäºç‰¹å®šè¯­è¨€çš„æ¨¡å‹å¦‚ä½•ç”¨è‹±è¯­ï¼ˆç¾å›½ã€è‹±å›½ï¼‰ã€éŸ©è¯­ï¼ˆéŸ©å›½ï¼‰å’Œä¸­æ–‡æ™®é€šè¯ï¼ˆä¸­å›½ï¼‰ä¸‰ç§è¯­è¨€ç”Ÿæˆæœ‰å…³æœé²œçš„ä¿¡æ¯ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºæ˜¾è‘—çš„è¯­è¨€å·®å¼‚ï¼Œæ˜¾ç¤ºæ‰€é€‰æ¨¡å‹å’Œè¯­è¨€çš„ä¸åŒå¯èƒ½å¯¼è‡´å¯¹æœé²œçš„ç†è§£å­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œè¿™å¯¹å…¨çƒå®‰å…¨æŒ‘æˆ˜å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹»è§‰ç°è±¡å¯èƒ½å¯¹æ¨¡å‹çš„éƒ¨ç½²äº§ç”Ÿå®‰å…¨éšæ‚£ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¼ æ’­é”™è¯¯ä¿¡æ¯æ–¹é¢ã€‚</li>
<li>å½“å‰è§£å†³æ–¹æ¡ˆä¸»è¦é€šè¿‡ä¸å¯é æ•°æ®æºå¯¹é½æ¨¡å‹æˆ–æé«˜æ¨¡å‹å¯¹è¾“å‡ºä¿¡å¿ƒçš„å±•ç°æ¥åº”å¯¹æŒ‘æˆ˜ã€‚</li>
<li>åœ¨ç¼ºä¹å‡†ç¡®æ•°æ®æˆ–éš¾ä»¥ç¡®å®šå¯é ä¿¡æ¯æºçš„æƒ…å¢ƒä¸­ï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆå¯èƒ½æ•ˆæœæœ‰é™ã€‚</li>
<li>ç ”ç©¶ä»¥æœé²œä¸ºæ¡ˆä¾‹ï¼Œæ­ç¤ºè¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå…³äºè¯¥å›½çš„ä¿¡æ¯æ—¶å­˜åœ¨çš„å·®å¼‚ã€‚</li>
<li>ä¸åŒè¯­è¨€å’Œæ¨¡å‹çš„é€‰æ‹©å¯èƒ½å¯¼è‡´å¯¹æœé²œç†è§£çš„ä¸åŒï¼Œè¿™åœ¨å…¨çƒå®‰å…¨é—®é¢˜ä¸Šå…·æœ‰é‡è¦å½±å“ã€‚</li>
<li>è·¨è¯­è¨€å’Œæ¨¡å‹çš„å·®å¼‚åœ¨ç‰¹å®šåœ°åŒºçš„è¯¯è§£å’Œè¯¯åˆ¤ä¸­å¯èƒ½èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7d6055080b4e5a3c8fb5bbdd275220bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a3ac3bca9a879482e5605c9109db046.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Model-Inversion-in-Split-Learning-for-Personalized-LLMs-New-Insights-from-Information-Bottleneck-Theory"><a href="#Model-Inversion-in-Split-Learning-for-Personalized-LLMs-New-Insights-from-Information-Bottleneck-Theory" class="headerlink" title="Model Inversion in Split Learning for Personalized LLMs: New Insights   from Information Bottleneck Theory"></a>Model Inversion in Split Learning for Personalized LLMs: New Insights   from Information Bottleneck Theory</h2><p><strong>Authors:Yunmeng Shu, Shaofeng Li, Tian Dong, Yan Meng, Haojin Zhu</strong></p>
<p>Personalized Large Language Models (LLMs) have become increasingly prevalent, showcasing the impressive capabilities of models like GPT-4. This trend has also catalyzed extensive research on deploying LLMs on mobile devices. Feasible approaches for such edge-cloud deployment include using split learning. However, previous research has largely overlooked the privacy leakage associated with intermediate representations transmitted from devices to servers. This work is the first to identify model inversion attacks in the split learning framework for LLMs, emphasizing the necessity of secure defense. For the first time, we introduce mutual information entropy to understand the information propagation of Transformer-based LLMs and assess privacy attack performance for LLM blocks. To address the issue of representations being sparser and containing less information than embeddings, we propose a two-stage attack system in which the first part projects representations into the embedding space, and the second part uses a generative model to recover text from these embeddings. This design breaks down the complexity and achieves attack scores of 38%-75% in various scenarios, with an over 60% improvement over the SOTA. This work comprehensively highlights the potential privacy risks during the deployment of personalized LLMs on the edge side. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šæ™®éï¼ŒGPT-4ç­‰æ¨¡å‹çš„å‡ºè‰²è¡¨ç°å¼•äººæ³¨ç›®ã€‚è¿™ä¸€è¶‹åŠ¿ä¹Ÿæ¨åŠ¨äº†åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šéƒ¨ç½²LLMçš„å¹¿æ³›ç ”ç©¶ã€‚å¯¹äºè¿™ç§è¾¹ç¼˜äº‘éƒ¨ç½²çš„å¯è¡Œæ–¹æ³•åŒ…æ‹¬ä½¿ç”¨åˆ†å‰²å­¦ä¹ ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†ä»è®¾å¤‡åˆ°æœåŠ¡å™¨ä¼ è¾“çš„ä¸­é—´è¡¨ç¤ºå½¢å¼æ‰€å…³è”çš„éšç§æ³„éœ²é—®é¢˜ã€‚æœ¬å·¥ä½œæ˜¯é¦–æ¬¡åœ¨åˆ†å‰²å­¦ä¹ æ¡†æ¶ä¸­è¯†åˆ«LLMçš„æ¨¡å‹åè½¬æ”»å‡»ï¼Œå¹¶å¼ºè°ƒå®‰å…¨é˜²å¾¡çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬é¦–æ¬¡å¼•å…¥äº’ä¿¡æ¯ç†µæ¥äº†è§£åŸºäºTransformerçš„LLMçš„ä¿¡æ¯ä¼ æ’­æƒ…å†µï¼Œå¹¶è¯„ä¼°LLMå—çš„éšç§æ”»å‡»æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¡¨ç¤ºç¨€ç–ä¸”åŒ…å«çš„ä¿¡æ¯å°‘äºåµŒå…¥çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ”»å‡»ç³»ç»Ÿï¼Œå…¶ä¸­ç¬¬ä¸€é˜¶æ®µå°†è¡¨ç¤ºæŠ•å½±åˆ°åµŒå…¥ç©ºé—´ï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨ç”Ÿæˆæ¨¡å‹ä»è¿™äº›åµŒå…¥ä¸­æ¢å¤æ–‡æœ¬ã€‚è¿™ç§è®¾è®¡é™ä½äº†å¤æ‚æ€§ï¼Œåœ¨å„ç§åœºæ™¯ä¸‹å®ç°äº†38%-75%çš„æ”»å‡»å¾—åˆ†ï¼Œæ¯”ç°æœ‰æŠ€æœ¯æé«˜äº†60%ä»¥ä¸Šã€‚æœ¬å·¥ä½œå…¨é¢çªå‡ºäº†åœ¨è¾¹ç¼˜ä¾§éƒ¨ç½²ä¸ªæ€§åŒ–LLMæœŸé—´å¯èƒ½å­˜åœ¨çš„æ½œåœ¨éšç§é£é™©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05965v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ç ”ç©¶äº†åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šéƒ¨ç½²ä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶çš„éšç§é—®é¢˜ã€‚ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„åŸºäºäº’ä¿¡æ¯ç†µçš„æ”»å‡»æ–¹å¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ”»å‡»ç³»ç»Ÿä»¥è§£å†³ç°æœ‰ç ”ç©¶ä¸­å¿½è§†çš„ä¸­é—´è¡¨ç¤ºä¼ è¾“éšç§é—®é¢˜ã€‚è¯¥è®¾è®¡èƒ½å¤Ÿçªç ´å¤æ‚åœºæ™¯ï¼Œå®ç°åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ”»å‡»å¾—åˆ†è¾¾åˆ°38%-75%ï¼Œæ¯”ç°æœ‰æŠ€æœ¯é«˜å‡ºè¶…è¿‡60%ã€‚è¿™é¡¹ç ”ç©¶å…¨é¢å¼ºè°ƒäº†è¾¹ç¼˜ä¾§éƒ¨ç½²ä¸ªæ€§åŒ–LLMsæ—¶çš„æ½œåœ¨éšç§é£é™©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šçš„éƒ¨ç½²è¶Šæ¥è¶Šæ™®éï¼Œä½†ä¸­é—´è¡¨ç¤ºä¼ è¾“éšç§é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†å…³æ³¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨¡å‹é€†æ”»å‡»æ–¹å¼ï¼Œå¼ºè°ƒäº†åœ¨éƒ¨ç½²ä¸ªæ€§åŒ–LLMsæ—¶çš„éšç§æ³„éœ²é£é™©ã€‚</li>
<li>ä½¿ç”¨äº’ä¿¡æ¯ç†µæ¥ç†è§£å’Œè¯„ä¼°Transformer-based LLMsçš„ä¿¡æ¯ä¼ æ’­ä»¥åŠéšç§æ”»å‡»æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†åˆ›æ–°çš„æ”»å‡»ç³»ç»Ÿï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œå°†ä¸­é—´è¡¨ç¤ºæŠ•å½±åˆ°åµŒå…¥ç©ºé—´ï¼Œå¹¶ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ä»è¿™äº›åµŒå…¥ä¸­æ¢å¤æ–‡æœ¬ã€‚</li>
<li>è¯¥æ”»å‡»ç³»ç»Ÿçš„è®¾è®¡èƒ½å¤Ÿé™ä½å¤æ‚æ€§ï¼Œå¹¶åœ¨ä¸åŒåœºæ™¯ä¸‹å®ç°è¾ƒé«˜çš„æ”»å‡»å¾—åˆ†ã€‚è¿™ä¸€è®¾è®¡ä¸ç°æœ‰æŠ€æœ¯çš„æ”¹è¿›å¹…åº¦è¶…è¿‡äº†60%ã€‚</li>
<li>æœ¬ç ”ç©¶çš„é‡è¦æ€§åœ¨äºå…¶æŒ‡å‡ºä¸ªæ€§åŒ–LLMsåœ¨è¾¹ç¼˜ä¾§éƒ¨ç½²æ—¶é¢ä¸´çš„æ½œåœ¨éšç§é£é™©ï¼Œå¹¶æä¾›äº†ç›¸åº”çš„å®‰å…¨é˜²å¾¡æ–¹æ³•ã€‚è¿™ä¸ºæœªæ¥ç›¸å…³ç ”ç©¶çš„æ¨è¿›æŒ‡æ˜äº†æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1a1618b0e79b9576ce7ff9012d34dc0f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a20c23383768ffad4fd3d2660887205.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1755e5dd192caa9a6e792119dd9739e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d576f228a2ffe3dfee84a178c00f87a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc91ee9892cb06efbfe6b524392da7e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a05582c43ba69525760facfa46bcba36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1500fdf20ce27572fcba9d7c215a1b06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1e71d884d685a1fac2f4629dd37d9b1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Valley2-Exploring-Multimodal-Models-with-Scalable-Vision-Language-Design"><a href="#Valley2-Exploring-Multimodal-Models-with-Scalable-Vision-Language-Design" class="headerlink" title="Valley2: Exploring Multimodal Models with Scalable Vision-Language   Design"></a>Valley2: Exploring Multimodal Models with Scalable Vision-Language   Design</h2><p><strong>Authors:Ziheng Wu, Zhenghao Chen, Ruipu Luo, Can Zhang, Yuan Gao, Zhentao He, Xian Wang, Haoran Lin, Minghui Qiu</strong></p>
<p>Recently, vision-language models have made remarkable progress, demonstrating outstanding capabilities in various tasks such as image captioning and video understanding. We introduce Valley2, a novel multimodal large language model designed to enhance performance across all domains and extend the boundaries of practical applications in e-commerce and short video scenarios. Notably, Valley2 achieves state-of-the-art (SOTA) performance on e-commerce benchmarks, surpassing open-source models of similar size by a large margin (79.66 vs. 72.76). Additionally, Valley2 ranks second on the OpenCompass leaderboard among models with fewer than 10B parameters, with an impressive average score of 67.4. The code and model weights are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/bytedance/Valley">https://github.com/bytedance/Valley</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾åƒæ ‡æ³¨å’Œè§†é¢‘ç†è§£ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œå¹¶å±•ç¤ºäº†å‡ºè‰²çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ¨å‡ºäº†Valley2ï¼Œè¿™æ˜¯ä¸€æ¬¾æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ‰€æœ‰é¢†åŸŸçš„æ€§èƒ½ï¼Œå¹¶æ‰©å±•åœ¨ç”µå­å•†åŠ¡å’ŒçŸ­è§†é¢‘åœºæ™¯ä¸­çš„å®é™…åº”ç”¨è¾¹ç•Œã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒValley2åœ¨ç”µå­å•†åŠ¡åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»¥è¾ƒå¤§å¹…åº¦è¶…è¶Šäº†ç±»ä¼¼è§„æ¨¡çš„å¼€æºæ¨¡å‹ï¼ˆ79.66å¯¹72.76ï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨å°‘äº10Bå‚æ•°çš„æ¨¡å‹ä¸­ï¼ŒValley2åœ¨OpenCompassæ’è¡Œæ¦œä¸Šååˆ—ç¬¬äºŒï¼Œå¹³å‡å¾—åˆ†ä»¤äººå°è±¡æ·±åˆ»ï¼Œè¾¾åˆ°67.4åˆ†ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/bytedance/Valley%E3%80%82">https://github.com/bytedance/Valleyã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05901v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å±±è°·æ¨¡å‹2ï¼ˆValley2ï¼‰æ˜¯ä¸€æ¬¾å…¨æ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€‚ç”¨äºç”µå­å•†åŠ¡å’ŒçŸ­è§†é¢‘åœºæ™¯ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚åœ¨ç”µå­å•†åŠ¡é¢†åŸŸçš„æ ‡æ†æµ‹è¯•ä¸­ï¼Œå±±è°·æ¨¡å‹2å·²è¾¾åˆ°æœ€å‰æ²¿çš„æŠ€æœ¯æ€§èƒ½è¡¨ç°ï¼Œè¿œè¶…åŒé‡çº§å¼€æºæ¨¡å‹ï¼›å¹¶ä¸”åœ¨å‚æ•°æ•°é‡å°‘äºåäº¿çš„å¤§å‹æ¨¡å‹ç«èµ›ä¸­ååˆ—ç¬¬äºŒã€‚æ¨¡å‹æƒé‡ä¸ä»£ç å·²åœ¨GitHubä¸Šå¼€æºã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Valley2æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€‚ç”¨äºç”µå­å•†åŠ¡å’ŒçŸ­è§†é¢‘åœºæ™¯ã€‚</li>
<li>åœ¨ç”µå­å•†åŠ¡é¢†åŸŸçš„æ ‡æ†æµ‹è¯•ä¸­ï¼Œå…¶è¡¨ç°å·²è¾¾åˆ°äº†å‰æ²¿æŠ€æœ¯ï¼ˆState-of-the-artï¼‰çš„æ°´å¹³ã€‚ </li>
<li>å¯¹æ¯”äºç›¸ä¼¼çš„å¼€æºæ¨¡å‹ï¼Œå±±è°·æ¨¡å‹2çš„æ€§èƒ½å¤§å¹…è¶…è¶Šç°æœ‰æ¨¡å‹ï¼ˆ79.66 vs. 72.76ï¼‰ã€‚ </li>
<li>åœ¨å‚æ•°æ•°é‡å°‘äºåäº¿çš„æ¨¡å‹ä¸­ï¼Œå±±è°·æ¨¡å‹æ’åç¬¬äºŒã€‚å…¶å¹³å‡å¾—åˆ†è¾¾åˆ°äº†ä»¤äººå°è±¡æ·±åˆ»çš„67.4åˆ†ã€‚ </li>
<li>Valleyæ¨¡å‹çš„ä»£ç å’Œæƒé‡å·²åœ¨GitHubä¸Šå¼€æºï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf1136016c5b5e8f89f5190219a6775c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-591a92272b6ed8c454850d2e45e727dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d9357e12f8b76d48e0654dc37586810.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ff4e501051493fa591745d7f333f9cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f7dfb92496394963ad36c0d6473cbbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f312064c987a6d44d994f8fd743e2c4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ConSim-Measuring-Concept-Based-Explanationsâ€™-Effectiveness-with-Automated-Simulatability"><a href="#ConSim-Measuring-Concept-Based-Explanationsâ€™-Effectiveness-with-Automated-Simulatability" class="headerlink" title="ConSim: Measuring Concept-Based Explanationsâ€™ Effectiveness with   Automated Simulatability"></a>ConSim: Measuring Concept-Based Explanationsâ€™ Effectiveness with   Automated Simulatability</h2><p><strong>Authors:Antonin PochÃ©, Alon Jacovi, Agustin Martin Picard, Victor Boutin, Fanny Jourdan</strong></p>
<p>Concept-based explanations work by mapping complex model computations to human-understandable concepts. Evaluating such explanations is very difficult, as it includes not only the quality of the induced space of possible concepts but also how effectively the chosen concepts are communicated to users. Existing evaluation metrics often focus solely on the former, neglecting the latter. We introduce an evaluation framework for measuring concept explanations via automated simulatability: a simulatorâ€™s ability to predict the explained modelâ€™s outputs based on the provided explanations. This approach accounts for both the concept space and its interpretation in an end-to-end evaluation. Human studies for simulatability are notoriously difficult to enact, particularly at the scale of a wide, comprehensive empirical evaluation (which is the subject of this work). We propose using large language models (LLMs) as simulators to approximate the evaluation and report various analyses to make such approximations reliable. Our method allows for scalable and consistent evaluation across various models and datasets. We report a comprehensive empirical evaluation using this framework and show that LLMs provide consistent rankings of explanation methods. Code available at <a target="_blank" rel="noopener" href="https://github.com/AnonymousConSim/ConSim">https://github.com/AnonymousConSim/ConSim</a> </p>
<blockquote>
<p>åŸºäºæ¦‚å¿µçš„è§£é‡Šæ˜¯é€šè¿‡å°†å¤æ‚çš„æ¨¡å‹è®¡ç®—æ˜ å°„åˆ°äººç±»å¯ç†è§£çš„æ¦‚å¿µæ¥å·¥ä½œçš„ã€‚è¯„ä¼°è¿™ç§è§£é‡Šæ˜¯éå¸¸å›°éš¾çš„ï¼Œå› ä¸ºå®ƒä¸ä»…åŒ…æ‹¬å¯èƒ½çš„æ¦‚å¿µç©ºé—´çš„è´¨é‡ï¼Œè¿˜åŒ…æ‹¬æ‰€é€‰æ‹©çš„æ¦‚å¿µå¦‚ä½•æœ‰æ•ˆåœ°ä¼ è¾¾ç»™ç”¨æˆ·ã€‚ç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡å¾€å¾€åªå…³æ³¨å‰è€…ï¼Œè€Œå¿½è§†äº†åè€…ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ¨¡æ‹Ÿæ€§æ¥è¡¡é‡æ¦‚å¿µè§£é‡Šï¼šæ¨¡æ‹Ÿå™¨æ ¹æ®æä¾›çš„è§£é‡Šé¢„æµ‹è§£é‡Šæ¨¡å‹çš„è¾“å‡ºçš„èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•æ—¢è€ƒè™‘æ¦‚å¿µç©ºé—´ï¼Œåˆè€ƒè™‘å…¶ç«¯åˆ°ç«¯çš„æœ€ç»ˆè§£é‡Šã€‚è¿›è¡Œæ¨¡æ‹Ÿæ€§ç ”ç©¶å¯¹äººç±»æ¥è¯´å°¤å…¶å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹¿æ³›è€Œå…¨é¢çš„ç»éªŒè¯„ä¼°çš„å°ºåº¦ä¸Šï¼ˆè¿™æ˜¯æœ¬æ–‡çš„ä¸»é¢˜ï¼‰ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ¨¡æ‹Ÿå™¨æ¥è¿‘ä¼¼è¯„ä¼°ï¼Œå¹¶æŠ¥å‘Šå„ç§åˆ†æä½¿è¿™äº›è¿‘ä¼¼å€¼å¯é ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸åœ¨å¤šç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¿›è¡Œå¯æ‰©å±•å’Œä¸€è‡´æ€§çš„è¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨è¯¥æ¡†æ¶æŠ¥å‘Šäº†å…¨é¢çš„ç»éªŒè¯„ä¼°ï¼Œå¹¶è¯æ˜äº†LLMå¯¹è§£é‡Šæ–¹æ³•æä¾›äº†ä¸€è‡´çš„æ’åã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AnonymousConSim/ConSim%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AnonymousConSim/ConSimæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05855v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¦‚å¿µè§£é‡Šé€šè¿‡æ˜ å°„å¤æ‚æ¨¡å‹è®¡ç®—åˆ°äººç±»å¯ç†è§£çš„æ¦‚å¿µæ¥å®ç°ã€‚è¯„ä¼°æ­¤ç±»è§£é‡Šéå¸¸å›°éš¾ï¼Œå› ä¸ºå®ƒä¸ä»…åŒ…æ‹¬å¯èƒ½æ¦‚å¿µç©ºé—´çš„è´¨é‡ï¼Œè¿˜åŒ…æ‹¬æ‰€é€‰æ‹©æ¦‚å¿µå‘ç”¨æˆ·ä¼ è¾¾çš„æœ‰æ•ˆæ€§ã€‚ç°æœ‰è¯„ä¼°æŒ‡æ ‡å¾€å¾€åªå…³æ³¨å‰è€…è€Œå¿½ç•¥äº†åè€…ã€‚æˆ‘ä»¬é€šè¿‡è‡ªåŠ¨æ¨¡æ‹Ÿèƒ½åŠ›å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œå³æ¨¡æ‹Ÿå™¨æ ¹æ®æä¾›çš„è§£é‡Šé¢„æµ‹è§£é‡Šæ¨¡å‹è¾“å‡ºçš„èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•åŒæ—¶è€ƒè™‘äº†æ¦‚å¿µç©ºé—´åŠå…¶è§£é‡Šï¼Œè¿›è¡Œäº†ç«¯åˆ°ç«¯çš„è¯„ä¼°ã€‚è¿›è¡Œæ¨¡æ‹Ÿèƒ½åŠ›çš„äººç±»ç ”ç©¶æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹¿æ³›è€Œå…¨é¢çš„ç»éªŒè¯„ä¼°ä¸­æ›´æ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬æè®®ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œè¿‘ä¼¼è¯„ä¼°ï¼Œå¹¶æŠ¥å‘Šäº†å„ç§åˆ†æä»¥ç¡®ä¿æ­¤ç±»è¿‘ä¼¼çš„å¯é æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸å¯¹å„ç§æ¨¡å‹å’Œæ•°æ®é›†è¿›è¡Œå¯æ‰©å±•å’Œä¸€è‡´çš„è¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨è¯¥æ¡†æ¶è¿›è¡Œäº†å…¨é¢çš„ç»éªŒè¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†LLMå¯¹è§£é‡Šæ–¹æ³•çš„ä¸€è‡´æ’åã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚å¿µè§£é‡Šé€šè¿‡å°†å¤æ‚æ¨¡å‹è®¡ç®—æ˜ å°„åˆ°äººç±»å¯ç†è§£çš„æ¦‚å¿µæ¥å·¥ä½œã€‚</li>
<li>è¯„ä¼°æ¦‚å¿µè§£é‡Šä¸ä»…æ¶‰åŠå¯èƒ½æ¦‚å¿µç©ºé—´çš„è´¨é‡ï¼Œè¿˜åŒ…æ‹¬å¦‚ä½•æœ‰æ•ˆåœ°å‘ç”¨æˆ·ä¼ è¾¾æ‰€é€‰æ¦‚å¿µã€‚</li>
<li>ç°æœ‰è¯„ä¼°æŒ‡æ ‡ä¸»è¦å…³æ³¨æ¦‚å¿µç©ºé—´çš„è´¨é‡ï¼Œå¿½ç•¥äº†å‘ç”¨æˆ·ä¼ è¾¾æ¦‚å¿µçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåŸºäºè‡ªåŠ¨æ¨¡æ‹Ÿèƒ½åŠ›çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥å…¨é¢è€ƒè™‘æ¦‚å¿µç©ºé—´å’Œå…¶è§£é‡Šã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œè¿‘ä¼¼è¯„ä¼°ã€‚</li>
<li>LLMåœ¨è¯„ä¼°ä¸åŒè§£é‡Šæ–¹æ³•æ—¶æä¾›äº†ä¸€è‡´çš„æ’åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-00afc647f566aaf5e8c6f38d5122bb8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f5d2a5521cda36541a09f5d764a5297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4a4cc152c4baba105b0b5080b7a2650.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f452ba0731ad243c01c036ad145d3c2a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Super-class-guided-Transformer-for-Zero-Shot-Attribute-Classification"><a href="#Super-class-guided-Transformer-for-Zero-Shot-Attribute-Classification" class="headerlink" title="Super-class guided Transformer for Zero-Shot Attribute Classification"></a>Super-class guided Transformer for Zero-Shot Attribute Classification</h2><p><strong>Authors:Sehyung Kim, Chanhyeong Yang, Jihwan Park, Taehoon Song, Hyunwoo J. Kim</strong></p>
<p>Attribute classification is crucial for identifying specific characteristics within image regions. Vision-Language Models (VLMs) have been effective in zero-shot tasks by leveraging their general knowledge from large-scale datasets. Recent studies demonstrate that transformer-based models with class-wise queries can effectively address zero-shot multi-label classification. However, poor utilization of the relationship between seen and unseen attributes makes the model lack generalizability. Additionally, attribute classification generally involves many attributes, making maintaining the modelâ€™s scalability difficult. To address these issues, we propose Super-class guided transFormer (SugaFormer), a novel framework that leverages super-classes to enhance scalability and generalizability for zero-shot attribute classification. SugaFormer employs Super-class Query Initialization (SQI) to reduce the number of queries, utilizing common semantic information from super-classes, and incorporates Multi-context Decoding (MD) to handle diverse visual cues. To strengthen generalizability, we introduce two knowledge transfer strategies that utilize VLMs. During training, Super-class guided Consistency Regularization (SCR) aligns SugaFormerâ€™s features with VLMs using region-specific prompts, and during inference, Zero-shot Retrieval-based Score Enhancement (ZRSE) refines predictions for unseen attributes. Extensive experiments demonstrate that SugaFormer achieves state-of-the-art performance across three widely-used attribute classification benchmarks under zero-shot, and cross-dataset transfer settings. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/mlvlab/SugaFormer">https://github.com/mlvlab/SugaFormer</a>. </p>
<blockquote>
<p>å±æ€§åˆ†ç±»å¯¹äºè¯†åˆ«å›¾åƒåŒºåŸŸå†…çš„ç‰¹å®šç‰¹å¾è‡³å…³é‡è¦ã€‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®é›†çš„ä¸€èˆ¬çŸ¥è¯†ï¼Œåœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æœ‰æ•ˆæ€§ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºå¸¦æœ‰ç±»åˆ«æŸ¥è¯¢çš„transformeræ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°è§£å†³é›¶æ ·æœ¬å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜ã€‚ç„¶è€Œï¼Œå¯¹å¯è§å’Œä¸å¯è§å±æ€§ä¹‹é—´å…³ç³»çš„åˆ©ç”¨ä¸è¶³ä½¿å¾—æ¨¡å‹çš„é€šç”¨æ€§è¾ƒå·®ã€‚æ­¤å¤–ï¼Œå±æ€§åˆ†ç±»é€šå¸¸æ¶‰åŠè®¸å¤šå±æ€§ï¼Œä½¿å¾—ä¿æŒæ¨¡å‹çš„å¯æ‰©å±•æ€§å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Super-class guided transFormerï¼ˆSugaFormerï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è¶…ç±»æ¥å¢å¼ºé›¶æ ·æœ¬å±æ€§åˆ†ç±»çš„å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ã€‚SugaFormeré‡‡ç”¨Super-class Query Initializationï¼ˆSQIï¼‰æ¥å‡å°‘æŸ¥è¯¢æ•°é‡ï¼Œåˆ©ç”¨è¶…ç±»çš„é€šç”¨è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ç»“åˆMulti-context Decodingï¼ˆMDï¼‰æ¥å¤„ç†å„ç§è§†è§‰çº¿ç´¢ã€‚ä¸ºäº†åŠ å¼ºé€šç”¨æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§åˆ©ç”¨VLMsçš„çŸ¥è¯†è½¬ç§»ç­–ç•¥ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒSuper-class guided Consistency Regularizationï¼ˆSCRï¼‰é€šè¿‡åŒºåŸŸç‰¹å®šçš„æç¤ºä½¿SugaFormerçš„ç‰¹å¾ä¸VLMså¯¹é½ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒZero-shot Retrieval-based Score Enhancementï¼ˆZRSEï¼‰å®Œå–„äº†æœªè§å±æ€§çš„é¢„æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSugaFormeråœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„å±æ€§åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é›¶æ ·æœ¬å’Œè·¨æ•°æ®é›†è¿ç§»è®¾ç½®çš„æœ€æ–°æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/mlvlab/SugaFormer">https://github.com/mlvlab/SugaFormer</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05728v1">PDF</a> AAAI25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSuper-class guided transFormerï¼ˆSugaFormerï¼‰çš„æ–°æ¡†æ¶ï¼Œç”¨äºè§£å†³é›¶æ ·æœ¬å±æ€§åˆ†ç±»ä¸­çš„å¯ä¼¸ç¼©æ€§å’Œæ³›åŒ–æ€§é—®é¢˜ã€‚å®ƒé€šè¿‡åˆ©ç”¨è¶…ç±»ä¿¡æ¯ï¼Œé‡‡ç”¨Super-class Query Initializationï¼ˆSQIï¼‰å‡å°‘æŸ¥è¯¢æ•°é‡ï¼Œå¹¶ç»“åˆMulti-context Decodingï¼ˆMDï¼‰å¤„ç†å¤šæ ·åŒ–çš„è§†è§‰çº¿ç´¢ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥ä¸¤ç§åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„çŸ¥è¯†è½¬ç§»ç­–ç•¥ï¼Œå³Super-class guided Consistency Regularizationï¼ˆSCRï¼‰å’ŒZero-shot Retrieval-based Score Enhancementï¼ˆZRSEï¼‰ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„å±æ€§åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šï¼ŒSugaFormeråœ¨é›¶æ ·æœ¬å’Œè·¨æ•°æ®é›†ä¼ è¾“è®¾ç½®ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å±æ€§åˆ†ç±»åœ¨è¯†åˆ«å›¾åƒåŒºåŸŸç‰¹å®šç‰¹å¾ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡æ•°æ®é›†çš„ä¸€èˆ¬çŸ¥è¯†å·²ç»å±•ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºç±»æŸ¥è¯¢çš„transformeræ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°è§£å†³é›¶æ ·æœ¬å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜ã€‚</li>
<li>SugaFormeræ¡†æ¶é€šè¿‡åˆ©ç”¨è¶…ç±»ä¿¡æ¯æ¥è§£å†³å±æ€§åˆ†ç±»ä¸­çš„å¯ä¼¸ç¼©æ€§å’Œæ³›åŒ–æ€§é—®é¢˜ã€‚</li>
<li>Super-class Query Initializationï¼ˆSQIï¼‰å‡å°‘äº†æŸ¥è¯¢æ•°é‡ï¼Œè€ŒMulti-context Decodingï¼ˆMDï¼‰å¤„ç†å¤šæ ·åŒ–çš„è§†è§‰çº¿ç´¢ã€‚</li>
<li>SugaFormerå¼•å…¥ä¸¤ç§çŸ¥è¯†è½¬ç§»ç­–ç•¥æ¥åˆ©ç”¨VLMsï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>SugaFormeråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œè·¨æ•°æ®é›†ä¼ è¾“è®¾ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05728">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-339e86467a2b93a964fe386aedf97f6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce1ec599dd84fdfc626d4a840a458d31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a253ebcf799571b261cbe3fa9594210.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e74b5349d2ab95debfe9e4d33c8e888b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="The-Impact-of-Model-Scaling-on-Seen-and-Unseen-Language-Performance"><a href="#The-Impact-of-Model-Scaling-on-Seen-and-Unseen-Language-Performance" class="headerlink" title="The Impact of Model Scaling on Seen and Unseen Language Performance"></a>The Impact of Model Scaling on Seen and Unseen Language Performance</h2><p><strong>Authors:Rhitabrat Pokharel, Sina Bagheri Nezhad, Ameeta Agrawal, Suresh Singh</strong></p>
<p>The rapid advancement of Large Language Models (LLMs), particularly those trained on multilingual corpora, has intensified the need for a deeper understanding of their performance across a diverse range of languages and model sizes. Our research addresses this critical need by studying the performance and scaling behavior of multilingual LLMs in text classification and machine translation tasks across 204 languages. We systematically examine both seen and unseen languages across three model families of varying sizes in zero-shot and few-shot settings. Our findings show significant differences in scaling behavior between zero-shot and two-shot scenarios, with striking disparities in performance between seen and unseen languages. Model scale has little effect on zero-shot performance, which remains mostly flat. However, in two-shot settings, larger models show clear linear improvements in multilingual text classification. For translation tasks, however, only the instruction-tuned model showed clear benefits from scaling. Our analysis also suggests that overall resource levels, not just the proportions of pretraining languages, are better predictors of model performance, shedding light on what drives multilingual LLM effectiveness. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå°¤å…¶æ˜¯é‚£äº›åœ¨å¤šè¯­ç§è¯­æ–™åº“ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œå¯¹è·¨å¤šç§è¯­è¨€å’Œä¸åŒæ¨¡å‹è§„æ¨¡çš„æ€§èƒ½è¿›è¡Œæ›´æ·±å…¥ç†è§£çš„å¿…è¦æ€§æ—¥ç›Šå‡¸æ˜¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡ç ”ç©¶å’Œè¯„ä¼°å¤šè¯­ç§LLMåœ¨æ–‡æœ¬åˆ†ç±»å’Œæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­åœ¨204ç§è¯­è¨€ä¸­çš„æ€§èƒ½å’Œæ‰©å±•è¡Œä¸ºæ¥åº”å¯¹è¿™ä¸€è¿«åˆ‡éœ€æ±‚ã€‚æˆ‘ä»¬åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸‹ï¼Œç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†ä¸‰ä¸ªä¸åŒè§„æ¨¡çš„æ¨¡å‹å®¶æ—åœ¨å·²çŸ¥å’ŒæœªçŸ¥è¯­è¨€ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé›¶æ ·æœ¬å’Œä¸¤æ ·æœ¬åœºæ™¯ä¸­çš„æ‰©å±•è¡Œä¸ºå­˜åœ¨æ˜æ˜¾å·®å¼‚ï¼Œå·²çŸ¥å’ŒæœªçŸ¥è¯­è¨€ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ä¹Ÿå¾ˆæ˜¾è‘—ã€‚åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹ï¼Œæ¨¡å‹è§„æ¨¡å¯¹æ€§èƒ½çš„å½±å“å¾®ä¹å…¶å¾®ï¼ŒåŸºæœ¬ä¿æŒå¹³ç¨³ã€‚ç„¶è€Œï¼Œåœ¨ä¸¤æ ·æœ¬è®¾ç½®ä¸­ï¼Œè¾ƒå¤§çš„æ¨¡å‹åœ¨å¤šè¯­ç§æ–‡æœ¬åˆ†ç±»ä¸­æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„çº¿æ€§æ”¹è¿›ã€‚ç„¶è€Œï¼Œå¯¹äºç¿»è¯‘ä»»åŠ¡ï¼Œåªæœ‰ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„æ¨¡å‹æ‰èƒ½ä»æ‰©å±•ä¸­å—ç›Šæ˜æ˜¾ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜è¡¨æ˜ï¼Œæ€»ä½“è€Œè¨€ï¼Œèµ„æºæ°´å¹³ï¼Œè€Œä¸ä»…ä»…æ˜¯é¢„è®­ç»ƒè¯­è¨€çš„æ¯”ä¾‹ï¼Œæ›´èƒ½é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿™ä¸ºæˆ‘ä»¬äº†è§£å¤šè¯­ç§LLMæ•ˆæœçš„åŸå› æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05629v1">PDF</a> Accepted at SEAS Workshop at AAAI25</p>
<p><strong>Summary</strong></p>
<p>å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ†ç±»å’Œæœºå™¨ç¿»è¯‘æ–¹é¢çš„æ€§èƒ½ç ”ç©¶è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°ç ”ç©¶äº†ä¸åŒè§„æ¨¡æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„è¡¨ç°ä¸æ‰©å±•æ€§ï¼Œæ¶µç›–äº†204ç§è¯­è¨€ï¼Œæ¶‰åŠé›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬åœºæ™¯ã€‚ç ”ç©¶å‘ç°ï¼Œé›¶æ ·æœ¬å’Œä¸¤æ ·æœ¬åœºæ™¯ä¸‹çš„æ‰©å±•æ€§è¡Œä¸ºå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”åœ¨å¯è§è¯­è¨€å’Œä¸å¯è§è¯­è¨€é—´çš„æ€§èƒ½è¡¨ç°æœ‰æ˜¾è‘—å·®å¼‚ã€‚æ¨¡å‹è§„æ¨¡å¯¹é›¶æ ·æœ¬æ€§èƒ½å½±å“è¾ƒå°ï¼Œè€Œåœ¨ä¸¤æ ·æœ¬ç¯å¢ƒä¸­ï¼Œå¤§å‹æ¨¡å‹åœ¨è·¨è¯­è¨€æ–‡æœ¬åˆ†ç±»ä¸Šè¡¨ç°å‡ºæ¸…æ™°çš„çº¿æ€§æ”¹è¿›ã€‚ç„¶è€Œï¼Œå¯¹äºç¿»è¯‘ä»»åŠ¡ï¼Œåªæœ‰ç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹åœ¨æ‰©å±•æ—¶æ˜¾ç¤ºå‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚æ€»ä½“è€Œè¨€ï¼Œèµ„æºæ°´å¹³è€Œéé¢„è®­ç»ƒè¯­è¨€çš„æ¯”ä¾‹èƒ½æ›´å¥½åœ°é¢„æµ‹æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­è¨€LLMåœ¨æ–‡æœ¬åˆ†ç±»å’Œæœºå™¨ç¿»è¯‘æ–¹é¢çš„æ€§èƒ½ç ”ç©¶éå¸¸é‡è¦ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠå¤šç§è¯­è¨€ç¯å¢ƒå’Œä¸åŒè§„æ¨¡çš„æ¨¡å‹ã€‚</li>
<li>é›¶æ ·æœ¬å’Œä¸¤æ ·æœ¬åœºæ™¯ä¸‹çš„æ¨¡å‹æ‰©å±•æ€§è¡Œä¸ºå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡å¯¹é›¶æ ·æœ¬æ€§èƒ½å½±å“è¾ƒå°ï¼Œè€Œåœ¨ä¸¤æ ·æœ¬ç¯å¢ƒä¸­ï¼Œå¤§å‹æ¨¡å‹åœ¨è·¨è¯­è¨€æ–‡æœ¬åˆ†ç±»ä¸Šè¡¨ç°æ›´å¥½ã€‚</li>
<li>å¯¹äºç¿»è¯‘ä»»åŠ¡ï¼Œåªæœ‰ç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹åœ¨æ‰©å±•æ—¶æ˜¾ç¤ºå‡ºä¼˜åŠ¿ã€‚</li>
<li>èµ„æºæ°´å¹³è€Œéé¢„è®­ç»ƒè¯­è¨€çš„æ¯”ä¾‹æ˜¯æ¨¡å‹æ€§èƒ½æ›´å¥½çš„é¢„æµ‹æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-697ad8f1427eec64e2ef2669de2148fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78e98036ca38a594a5a6ca142d51a33c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f5cd3ea48359374f3e4f5cafa13de35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-372cf91c6ce058cc7b0e6a83002b3639.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd4e035062a33c430adb2227b79856f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ea90ffe8a1c8b43065d3059b07e0914.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb8dbaa20c4d8f29c02e6b6c1926a301.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Centurio-On-Drivers-of-Multilingual-Ability-of-Large-Vision-Language-Model"><a href="#Centurio-On-Drivers-of-Multilingual-Ability-of-Large-Vision-Language-Model" class="headerlink" title="Centurio: On Drivers of Multilingual Ability of Large Vision-Language   Model"></a>Centurio: On Drivers of Multilingual Ability of Large Vision-Language   Model</h2><p><strong>Authors:Gregor Geigle, Florian Schneider, Carolin Holtermann, Chris Biemann, Radu Timofte, Anne Lauscher, Goran GlavaÅ¡</strong></p>
<p>Most Large Vision-Language Models (LVLMs) to date are trained predominantly on English data, which makes them struggle to understand non-English input and fail to generate output in the desired target language. Existing efforts mitigate these issues by adding multilingual training data, but do so in a largely ad-hoc manner, lacking insight into how different training mixes tip the scale for different groups of languages. In this work, we present a comprehensive investigation into the training strategies for massively multilingual LVLMs. First, we conduct a series of multi-stage experiments spanning 13 downstream vision-language tasks and 43 languages, systematically examining: (1) the number of training languages that can be included without degrading English performance and (2) optimal language distributions of pre-training as well as (3) instruction-tuning data. Further, we (4) investigate how to improve multilingual text-in-image understanding, and introduce a new benchmark for the task. Surprisingly, our analysis reveals that one can (i) include as many as 100 training languages simultaneously (ii) with as little as 25-50% of non-English data, to greatly improve multilingual performance while retaining strong English performance. We further find that (iii) including non-English OCR data in pre-training and instruction-tuning is paramount for improving multilingual text-in-image understanding. Finally, we put all our findings together and train Centurio, a 100-language LVLM, offering state-of-the-art performance in an evaluation covering 14 tasks and 56 languages. </p>
<blockquote>
<p>è¿„ä»Šä¸ºæ­¢ï¼Œå¤§å¤šæ•°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸»è¦æ¥å—è‹±è¯­æ•°æ®çš„è®­ç»ƒï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥ç†è§£å’Œå¤„ç†éè‹±è¯­è¾“å…¥ï¼Œå¹¶ä¸”æ— æ³•ç”Ÿæˆç›®æ ‡è¯­è¨€çš„è¾“å‡ºã€‚ç°æœ‰çš„åŠªåŠ›é€šè¿‡æ·»åŠ å¤šè¯­è¨€è®­ç»ƒæ•°æ®æ¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†è¿™æ ·åšåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ä¸´æ—¶æ€§çš„ï¼Œç¼ºä¹å¯¹ä¸åŒè®­ç»ƒç»„åˆå¦‚ä½•å½±å“ä¸åŒè¯­è¨€ç¾¤ä½“çš„è®¤è¯†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹å¤§è§„æ¨¡å¤šè¯­è¨€LVLMçš„è®­ç»ƒç­–ç•¥è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€ç³»åˆ—è·¨13ä¸ªä¸‹æ¸¸è§†è§‰è¯­è¨€ä»»åŠ¡å’Œ43ç§è¯­è¨€çš„å¤šé˜¶æ®µå®éªŒï¼Œç³»ç»Ÿåœ°ç ”ç©¶äº†ï¼šï¼ˆ1ï¼‰åœ¨ä¸å½±å“è‹±è¯­æ€§èƒ½çš„æƒ…å†µä¸‹å¯ä»¥åŒ…å«å¤šå°‘ç§è®­ç»ƒè¯­è¨€ï¼›ï¼ˆ2ï¼‰é¢„è®­ç»ƒçš„æœ€ä¼˜è¯­è¨€åˆ†å¸ƒä»¥åŠï¼ˆ3ï¼‰æŒ‡ä»¤å¾®è°ƒæ•°æ®çš„ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ï¼ˆ4ï¼‰ç ”ç©¶äº†å¦‚ä½•æ”¹è¿›å¤šè¯­è¨€æ–‡æœ¬å›¾åƒç†è§£ï¼Œå¹¶å¼•å…¥äº†è¯¥ä»»åŠ¡çš„æ–°çš„åŸºå‡†æµ‹è¯•ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œäººä»¬å¯ä»¥åŒæ—¶åŒ…å«å¤šè¾¾100ç§è®­ç»ƒè¯­è¨€ï¼ˆiiï¼‰åªéœ€ä½¿ç”¨25-50ï¼…çš„éè‹±è¯­æ•°æ®ï¼Œå°±å¯ä»¥åœ¨ä¿æŒå¼ºå¤§çš„è‹±è¯­æ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¤§æé«˜å¤šè¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å‘ç°ï¼Œï¼ˆiiiï¼‰åœ¨é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒä¸­åŒ…å«éè‹±è¯­OCRæ•°æ®å¯¹äºæé«˜å¤šè¯­è¨€æ–‡æœ¬å›¾åƒç†è§£è‡³å…³é‡è¦ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å‘ç°ç»“åˆèµ·æ¥ï¼Œè®­ç»ƒäº†Centurioï¼Œè¿™æ˜¯ä¸€ä¸ª100ç§è¯­è¨€çš„LVLMï¼Œåœ¨æ¶µç›–14é¡¹ä»»åŠ¡å’Œ56ç§è¯­è¨€çš„è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05122v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§è§„æ¨¡å¤šè¯­è¨€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è®­ç»ƒç­–ç•¥ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œæ¢è®¨äº†åŒæ—¶è®­ç»ƒå¤šç§è¯­è¨€æ—¶å¦‚ä½•å¹³è¡¡è‹±è¯­æ€§èƒ½ä¸å…¶ä»–è¯­è¨€æ€§èƒ½çš„é—®é¢˜ï¼Œå¹¶å‘ç°å¯ä»¥åŒæ—¶åŒ…å«å¤šè¾¾100ç§è®­ç»ƒè¯­è¨€ï¼Œåªéœ€ä½¿ç”¨25%~50%çš„éè‹±è¯­æ•°æ®å³å¯åœ¨ä¿æŒè‹±è¯­æ€§èƒ½çš„åŒæ—¶å¤§å¹…æå‡å¤šè¯­è¨€èƒ½åŠ›ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜å‘ç°åŒ…å«éè‹±è¯­OCRæ•°æ®çš„é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒå¯¹äºæé«˜å¤šè¯­è¨€æ–‡æœ¬å›¾åƒç†è§£è‡³å…³é‡è¦ã€‚æœ€ç»ˆï¼ŒåŸºäºè¿™äº›å‘ç°ï¼Œè®­ç»ƒå‡ºäº†ä¸€ä¸ªæ”¯æŒ100ç§è¯­è¨€çš„LVLMæ¨¡å‹Centurioï¼Œåœ¨æ¶µç›–å¤šç§ä»»åŠ¡å’Œè¯­è¨€çš„è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨åº”å¯¹éè‹±è¯­è¾“å…¥å’Œç”Ÿæˆç›®æ ‡è¯­è¨€è¾“å‡ºæ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œç³»ç»Ÿç ”ç©¶äº†å¤šè¯­è¨€è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬è®­ç»ƒè¯­è¨€æ•°é‡ã€é¢„è®­ç»ƒåŠæŒ‡ä»¤å¾®è°ƒæ•°æ®çš„è¯­è¨€åˆ†å¸ƒã€‚</li>
<li>ç ”ç©¶å‘ç°å¯ä»¥åŒæ—¶åŒ…å«å¤šè¾¾100ç§è®­ç»ƒè¯­è¨€ï¼Œä¸”åªéœ€ä½¿ç”¨éƒ¨åˆ†éè‹±è¯­æ•°æ®å³å¯åœ¨ä¿æŒè‹±è¯­æ€§èƒ½çš„åŒæ—¶æå‡å¤šè¯­è¨€èƒ½åŠ›ã€‚</li>
<li>åŒ…å«éè‹±è¯­OCRæ•°æ®çš„é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒå¯¹äºæé«˜å¤šè¯­è¨€æ–‡æœ¬å›¾åƒç†è§£è‡³å…³é‡è¦ã€‚</li>
<li>åŸºäºä¸Šè¿°å‘ç°ï¼Œå¼€å‘å‡ºä¸€ç§æ”¯æŒ100ç§è¯­è¨€çš„LVLMæ¨¡å‹Centurioã€‚</li>
<li>Centurioåœ¨æ¶µç›–å¤šç§ä»»åŠ¡å’Œè¯­è¨€çš„è¯„ä¼°ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-338786764e1b5a59de065727a372ccf6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da19c567e8580ce49effa14c7b13300b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d50e885af6ada6fb63d60670f8b0e466.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e94e55d0b831ca26596fb19dcd29377b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5c3cb7ce3e1da8672c567198f3b4f85.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="On-the-Generalizability-of-Transformer-Models-to-Code-Completions-of-Different-Lengths"><a href="#On-the-Generalizability-of-Transformer-Models-to-Code-Completions-of-Different-Lengths" class="headerlink" title="On the Generalizability of Transformer Models to Code Completions of   Different Lengths"></a>On the Generalizability of Transformer Models to Code Completions of   Different Lengths</h2><p><strong>Authors:Nathan Cooper, Rosalia Tufano, Gabriele Bavota, Denys Poshyvanyk</strong></p>
<p>The programming landscape is nowadays being reshaped by the advent of Large Language Models (LLMs) able to automate code-related tasks related to code implementation (e.g., code completion) and comprehension (e.g., code summarization). Such a paradigm shift comes with a number of implications related to how software will be written, maintained, and evolved. Also, these LLMs are extremely expensive to train, posing questions on their sustainability over time. Given their training cost, their ability to generalize, namely their ability to work on task instances different from those on which they have been trained, is an aspect worth being investigated. Previous work already showed that transformer models can successfully support code completion in a cross-project setting. However, it is unclear whether LLM are able to generalize to inputs having lengths not seen during training. For example, it is known that training a model on short instances allows to substantially reduce the training cost. However, the extent to which such a model would provide good performance on sequences having lengths not seen during training is not known. Many recent works in Natural Language Processing (NLP) tackled this problem in the context of decoder-only LLMs, i.e., xPOS and ALiBi. To assess if these solutions extend to encoder-decoder LLMs usually adopted in the code-related tasks, we present a large empirical study evaluating this generalization property of these and other encoding schemes proposed in the literature, namely Sinusoidal, xPOS, ALiBi, and T5. We found that none of these solutions successfully generalize to unseen lengths and that the only safe solution is to ensure the representativeness in the training set of all lengths likely to be encountered at inference time. </p>
<blockquote>
<p>å½“å‰ï¼Œç¼–ç¨‹é¢†åŸŸæ­£åœ¨è¢«å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡å¡‘ã€‚è¿™äº›æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨åŒ–ä¸ä»£ç å®ç°ï¼ˆä¾‹å¦‚ä»£ç è¡¥å…¨ï¼‰å’Œç†è§£ï¼ˆä¾‹å¦‚ä»£ç æ‘˜è¦ï¼‰ç›¸å…³çš„ä»»åŠ¡ã€‚è¿™ç§èŒƒå¼è½¬å˜å¸¦æ¥äº†ä¸€ç³»åˆ—å…³äºè½¯ä»¶å¦‚ä½•ç¼–å†™ã€ç»´æŠ¤å’Œæ¼”è¿›çš„å¯ç¤ºã€‚æ­¤å¤–ï¼Œè®­ç»ƒè¿™äº›LLMéœ€è¦æé«˜çš„æˆæœ¬ï¼Œè¿™å¼•å‘äº†å®ƒä»¬æ˜¯å¦å¯æŒç»­å‘å±•çš„è´¨ç–‘ã€‚è€ƒè™‘åˆ°å®ƒä»¬çš„è®­ç»ƒæˆæœ¬ï¼Œå®ƒä»¬åœ¨ä»»åŠ¡å®ä¾‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›â€”â€”å°¤å…¶æ˜¯å¤„ç†ä¸åŒäºè®­ç»ƒå®ä¾‹çš„ä»»åŠ¡â€”â€”æ˜¯ä¸€ä¸ªå€¼å¾—è°ƒæŸ¥çš„æ–¹é¢ã€‚ä¹‹å‰çš„å·¥ä½œå·²ç»è¡¨æ˜ï¼Œtransformeræ¨¡å‹å¯ä»¥åœ¨è·¨é¡¹ç›®ç¯å¢ƒä¸­æˆåŠŸæ”¯æŒä»£ç è¡¥å…¨ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šLLMæ˜¯å¦èƒ½å¤Ÿæ³›åŒ–åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§é•¿åº¦çš„è¾“å…¥ã€‚ä¾‹å¦‚ï¼Œå·²çŸ¥åœ¨çŸ­å®ä¾‹ä¸Šè®­ç»ƒæ¨¡å‹å¯ä»¥å¤§å¤§å‡å°‘è®­ç»ƒæˆæœ¬ã€‚ä½†æ˜¯ï¼Œè¿™ç§æ¨¡å‹åœ¨å¤„ç†è®­ç»ƒæœŸé—´æœªè§é•¿åº¦çš„åºåˆ—æ–¹é¢çš„è¡¨ç°ç¨‹åº¦å°šä¸æ¸…æ¥šã€‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„è®¸å¤šæœ€æ–°å·¥ä½œéƒ½åœ¨é’ˆå¯¹ä»…è§£ç å™¨LLMçš„æƒ…å¢ƒè§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚xPOSå’ŒALiBiã€‚ä¸ºäº†è¯„ä¼°è¿™äº›è§£å†³æ–¹æ¡ˆæ˜¯å¦é€‚ç”¨äºä»£ç ä¸­é€šå¸¸é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨LLMçš„ä»»åŠ¡ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å¤§å‹å®è¯ç ”ç©¶ï¼Œè¯„ä¼°äº†è¿™äº›è§£å†³æ–¹æ¡ˆä»¥åŠæ–‡çŒ®ä¸­æå‡ºçš„å…¶ä»–ç¼–ç æ–¹æ¡ˆçš„æ³›åŒ–å±æ€§ï¼ŒåŒ…æ‹¬æ­£å¼¦ã€xPOSã€ALiBiå’ŒT5ã€‚æˆ‘ä»¬å‘ç°è¿™äº›è§£å†³æ–¹æ¡ˆéƒ½ä¸é€‚ç”¨äºæœªè§é•¿åº¦çš„æ³›åŒ–ï¼Œå”¯ä¸€å®‰å…¨çš„è§£å†³æ–¹æ¡ˆæ˜¯ç¡®ä¿è®­ç»ƒé›†ä¸­çš„ä»£è¡¨æ€§èƒ½å¤Ÿæ¶µç›–æ‰€æœ‰å¯èƒ½åœ¨æ¨ç†æ—¶é‡åˆ°çš„é•¿åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05051v1">PDF</a> Accepted for publication at ICSME 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨é‡å¡‘ç¼–ç¨‹é¢†åŸŸï¼Œèƒ½å¤Ÿè‡ªåŠ¨åŒ–ä¸ä»£ç å®ç°å’Œç†è§£ç›¸å…³çš„ä»»åŠ¡ï¼Œå¦‚ä»£ç è¡¥å…¨å’Œä»£ç æ‘˜è¦ã€‚ç„¶è€Œï¼ŒLLMçš„è®­ç»ƒæˆæœ¬æé«˜ï¼Œå…¶å¯æŒç»­æ€§é—®é¢˜å¤‡å—å…³æ³¨ã€‚LLMçš„æ³›åŒ–èƒ½åŠ›ï¼Œå³åœ¨æ–°ä»»åŠ¡ä¸Šè¡¨ç°çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ—¶æœªè§è¿‡çš„åºåˆ—é•¿åº¦ä¸Šï¼Œæ˜¯ä¸€ä¸ªå€¼å¾—ç ”ç©¶çš„é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®­ç»ƒæ¨¡å‹åœ¨çŸ­å®ä¾‹ä¸Šå¯èƒ½é™ä½è®­ç»ƒæˆæœ¬ï¼Œä½†å¯¹äºæœªè§è¿‡é•¿åº¦çš„åºåˆ—æ€§èƒ½è¡¨ç°å°šä¸æ¸…æ¥šã€‚æœ¬ç ”ç©¶å¯¹ç¼–ç æ–¹æ¡ˆè¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼ŒåŒ…æ‹¬æ­£å¼¦ã€xPOSã€ALiBiå’ŒT5ç­‰æ–¹æ¡ˆï¼Œå‘ç°è¿™äº›æ–¹æ¡ˆå‡ä¸èƒ½æˆåŠŸæ³›åŒ–åˆ°æœªè§è¿‡çš„é•¿åº¦ï¼Œå”¯ä¸€å®‰å…¨çš„è§£å†³æ–¹æ¡ˆæ˜¯ç¡®ä¿è®­ç»ƒé›†ä¸­æ¶µç›–æ‰€æœ‰å¯èƒ½åœ¨æ¨ç†æ—¶é‡åˆ°çš„é•¿åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨æ”¹å˜ç¼–ç¨‹é¢†åŸŸï¼Œå®ç°è‡ªåŠ¨åŒ–ä»£ç ä»»åŠ¡ã€‚</li>
<li>LLMçš„è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œå…¶å¯æŒç»­æ€§å—åˆ°å…³æ³¨ã€‚</li>
<li>LLMçš„æ³›åŒ–èƒ½åŠ›ï¼Œå³åœ¨æ–°çš„ã€ä¸è®­ç»ƒæ•°æ®ä¸åŒçš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæ˜¯ä¸€ä¸ªé‡è¦ç ”ç©¶é¢†åŸŸã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨è§£ç å™¨LLMçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨ç¼–ç æ–¹æ¡ˆçš„ç ”ç©¶ä¸­ï¼ŒåŒ…æ‹¬æ­£å¼¦ã€xPOSã€ALiBiå’ŒT5ç­‰æ–¹æ¡ˆåœ¨ä»£ç ç›¸å…³ä»»åŠ¡ä¸­çš„æ³›åŒ–æ€§èƒ½è¡¨ç°ä¸ä¸€ã€‚</li>
<li>ç›®å‰æ²¡æœ‰è§£å†³æ–¹æ¡ˆèƒ½å¤ŸæˆåŠŸæ³›åŒ–åˆ°æœªè§è¿‡çš„é•¿åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-980f0ce69b7501297ab65ec93359ed7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab7c1f33ec1dcc3f545ee4c071512d91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1033a5fc0cb327ff4093c8b45c2fa547.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Demystifying-Domain-adaptive-Post-training-for-Financial-LLMs"><a href="#Demystifying-Domain-adaptive-Post-training-for-Financial-LLMs" class="headerlink" title="Demystifying Domain-adaptive Post-training for Financial LLMs"></a>Demystifying Domain-adaptive Post-training for Financial LLMs</h2><p><strong>Authors:Zixuan Ke, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty</strong></p>
<p>Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations. To address these challenges, we introduce FINDAP, a systematic and fine-grained investigation into domain-adaptive post-training of LLMs for the finance domain. Our approach begins by identifying the core capabilities required for the target domain and designing a comprehensive evaluation suite aligned with these needs. We then analyze the effectiveness of key post-training stages, including continual pretraining, instruction tuning, and preference alignment. Building on these insights, we propose an effective training recipe centered on a novel preference data distillation method, which leverages process signals from a generative reward model. The resulting model, Llama-Fin, achieves state-of-the-art performance across a wide range of financial tasks. Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs. Project page: <a target="_blank" rel="noopener" href="https://github.com/SalesforceAIResearch/FinDap">https://github.com/SalesforceAIResearch/FinDap</a> </p>
<blockquote>
<p>é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åè®­ç»ƒåœ¨å½“ä»Šçš„åŒ»å­¦ã€é‡‘èç­‰ä¸“ä¸šåŒ–é¢†åŸŸå±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨ä¸åŒæ•°æ®å’Œæ¨¡å‹é…ç½®ä¸‹ç¡®å®šæœ€ä½³é€‚åº”æ ‡å‡†å’Œè®­ç»ƒç­–ç•¥ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FINDAPï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é‡‘èé¢†åŸŸLLMé¢†åŸŸè‡ªé€‚åº”åè®­ç»ƒçš„å…¨é¢è€Œç²¾ç»†çš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆç¡®å®šç›®æ ‡é¢†åŸŸæ‰€éœ€çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶è®¾è®¡ä¸€å¥—ç¬¦åˆè¿™äº›éœ€æ±‚çš„ç»¼åˆè¯„ä¼°å¥—ä»¶ã€‚ç„¶ååˆ†æå…³é”®åè®­ç»ƒé˜¶æ®µçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬è¿ç»­é¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒæ•´å’Œåå¥½å¯¹é½ç­‰ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥æ–°å‹åå¥½æ•°æ®è’¸é¦æ–¹æ³•ä¸ºä¸­å¿ƒçš„æœ‰æ•ˆè®­ç»ƒé…æ–¹ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ¥è‡ªç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„è¿‡ç¨‹ä¿¡å·ã€‚å› æ­¤å¾—åˆ°çš„æ¨¡å‹Llama-Finåœ¨å¹¿æ³›çš„é‡‘èä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜å¼ºè°ƒäº†æ¯ä¸ªåè®­ç»ƒé˜¶æ®µå¯¹ç‹¬ç‰¹èƒ½åŠ›çš„è´¡çŒ®ï¼Œæ­ç¤ºäº†ç‰¹å®šçš„æŒ‘æˆ˜å’Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºLLMçš„é¢†åŸŸé€‚åº”æä¾›äº†å®è´µçš„è§è§£ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/SalesforceAIResearch/FinDap">https://github.com/SalesforceAIResearch/FinDap</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04961v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢†åŸŸè‡ªé€‚åº”çš„åæœŸè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»å­¦å’Œé‡‘èç­‰ç‰¹å®šé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•FINDAPï¼Œé€šè¿‡ç³»ç»Ÿåœ°ç²¾ç»†ç ”ç©¶LLMçš„é¢†åŸŸè‡ªé€‚åº”åæœŸè®­ç»ƒï¼Œç‰¹åˆ«æ˜¯åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨ã€‚é€šè¿‡è¯†åˆ«ç›®æ ‡é¢†åŸŸæ‰€éœ€çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶è®¾è®¡ä¸ä¹‹ç›¸ç¬¦çš„ç»¼åˆè¯„ä¼°å¥—ä»¶ï¼Œåˆ†æå…³é”®åæœŸè®­ç»ƒé˜¶æ®µçš„æ•ˆæœï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒæ•´å’Œåå¥½å¯¹é½ç­‰ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæå‡ºäº†ä¸€ç§ä»¥æ–°å‹åå¥½æ•°æ®è’¸é¦æ–¹æ³•ä¸ºä¸­å¿ƒçš„æœ‰æ•ˆè®­ç»ƒé…æ–¹ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„æµç¨‹ä¿¡å·ã€‚æœ€ç»ˆæ¨¡å‹Llama-Finåœ¨å¹¿æ³›çš„é‡‘èä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æœ¬æ–‡åˆ†æè¿˜æŒ‡å‡ºäº†æ¯ä¸ªåæœŸè®­ç»ƒé˜¶æ®µå¯¹ä¸åŒèƒ½åŠ›çš„è´¡çŒ®ï¼Œæ­ç¤ºäº†ç‰¹å®šæŒ‘æˆ˜å’Œæœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œä¸ºLLMçš„é¢†åŸŸè‡ªé€‚åº”æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢†åŸŸè‡ªé€‚åº”çš„åæœŸè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šé¢†åŸŸå¦‚åŒ»å­¦å’Œé‡‘èä¸­æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</li>
<li>FINDAPæ–¹æ³•é€šè¿‡ç³»ç»Ÿåœ°ç ”ç©¶LLMçš„é¢†åŸŸè‡ªé€‚åº”åæœŸè®­ç»ƒï¼Œä¸ºé‡‘èé¢†åŸŸæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>é€šè¿‡è¯†åˆ«ç›®æ ‡é¢†åŸŸæ‰€éœ€çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶è®¾è®¡ç›¸åº”çš„ç»¼åˆè¯„ä¼°å¥—ä»¶ï¼Œä»¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åˆ†æäº†å…³é”®åæœŸè®­ç»ƒé˜¶æ®µçš„æ•ˆæœï¼ŒåŒ…æ‹¬æŒç»­é¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒæ•´å’Œåå¥½å¯¹é½ç­‰ã€‚</li>
<li>æå‡ºäº†ä»¥åå¥½æ•°æ®è’¸é¦æ–¹æ³•ä¸ºä¸­å¿ƒçš„æœ‰æ•ˆè®­ç»ƒé…æ–¹ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„æµç¨‹ä¿¡å·ã€‚</li>
<li>æœ€ç»ˆæ¨¡å‹Llama-Finåœ¨å¹¿æ³›çš„é‡‘èä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cbd7642570961932132d82dbc6d80fde.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f89441d376d780b2d79a00c4c664e691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-324ebafe976f22984c7e080cdd16b007.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beca488feaa3739b1f011f081549d2a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a963045eb356337bf0fb505c64387c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c64bbda5020c8cc55c99becfe3db1d4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3de675f323bd6630046d0ca0b115e33.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AI-Driven-Reinvention-of-Hydrological-Modeling-for-Accurate-Predictions-and-Interpretation-to-Transform-Earth-System-Modeling"><a href="#AI-Driven-Reinvention-of-Hydrological-Modeling-for-Accurate-Predictions-and-Interpretation-to-Transform-Earth-System-Modeling" class="headerlink" title="AI-Driven Reinvention of Hydrological Modeling for Accurate Predictions   and Interpretation to Transform Earth System Modeling"></a>AI-Driven Reinvention of Hydrological Modeling for Accurate Predictions   and Interpretation to Transform Earth System Modeling</h2><p><strong>Authors:Cuihui Xia, Lei Yue, Deliang Chen, Yuyang Li, Hongqiang Yang, Ancheng Xue, Zhiqiang Li, Qing He, Guoqing Zhang, Dambaru Ballab Kattel, Lei Lei, Ming Zhou</strong></p>
<p>Traditional equation-driven hydrological models often struggle to accurately predict streamflow in challenging regional Earth systems like the Tibetan Plateau, while hybrid and existing algorithm-driven models face difficulties in interpreting hydrological behaviors. This work introduces HydroTrace, an algorithm-driven, data-agnostic model that substantially outperforms these approaches, achieving a Nash-Sutcliffe Efficiency of 98% and demonstrating strong generalization on unseen data. Moreover, HydroTrace leverages advanced attention mechanisms to capture spatial-temporal variations and feature-specific impacts, enabling the quantification and spatial resolution of streamflow partitioning as well as the interpretation of hydrological behaviors such as glacier-snow-streamflow interactions and monsoon dynamics. Additionally, a large language model (LLM)-based application allows users to easily understand and apply HydroTraceâ€™s insights for practical purposes. These advancements position HydroTrace as a transformative tool in hydrological and broader Earth system modeling, offering enhanced prediction accuracy and interpretability. </p>
<blockquote>
<p>ä¼ ç»ŸåŸºäºæ–¹ç¨‹çš„æ°´æ–‡æ¨¡å‹åœ¨é¢ä¸´å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒºåŸŸåœ°çƒç³»ç»Ÿï¼ˆå¦‚é’è—é«˜åŸï¼‰æ—¶ï¼Œå¾€å¾€éš¾ä»¥å‡†ç¡®é¢„æµ‹å¾„æµã€‚è€Œæ··åˆå’Œç°æœ‰ç®—æ³•é©±åŠ¨æ¨¡å‹åœ¨è§£é‡Šæ°´æ–‡è¡Œä¸ºæ–¹é¢åˆ™é¢ä¸´å›°éš¾ã€‚æœ¬æ–‡ä»‹ç»äº†HydroTraceï¼Œè¿™æ˜¯ä¸€ç§ç®—æ³•é©±åŠ¨ã€æ•°æ®è‡ªä¸»æ¨¡å‹ï¼Œç›¸è¾ƒäºè¿™äº›æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜è¾¾98%çš„Nash-Sutcliffeæ•ˆç‡ï¼Œå¹¶åœ¨æœªè§æ•°æ®ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒHydroTraceåˆ©ç”¨å…ˆè¿›çš„æ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰æ—¶ç©ºå˜åŒ–å’Œç‰¹å¾ç‰¹å®šå½±å“ï¼Œå®ç°äº†å¾„æµåˆ†é…çš„å®šé‡åŒ–å’Œç©ºé—´åˆ†è¾¨ç‡ï¼Œå¹¶è§£é‡Šäº†æ°´æ–‡è¡Œä¸ºï¼Œå¦‚å†°å·-é›ª-å¾„æµç›¸äº’ä½œç”¨å’Œå­£é£åŠ¨åŠ›å­¦ã€‚å¦å¤–ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åº”ç”¨ç¨‹åºå…è®¸ç”¨æˆ·è½»æ¾ç†è§£å’Œåº”ç”¨HydroTraceçš„è§è§£ä»¥è§£å†³å®é™…é—®é¢˜ã€‚è¿™äº›è¿›æ­¥ä½¿HydroTraceåœ¨æ°´æ–‡å’Œæ›´å¹¿æ³›çš„åœ°çƒç³»ç»Ÿå»ºæ¨¡ä¸­æˆä¸ºä¸€ç§å˜é©æ€§å·¥å…·ï¼Œæé«˜äº†é¢„æµ‹ç²¾åº¦å’Œå¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04733v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†HydroTraceæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç®—æ³•é©±åŠ¨ã€æ•°æ®æ— å…³çš„æ°´æ–‡æ¨¡å‹ï¼Œé€‚ç”¨äºåƒé’è—é«˜åŸè¿™æ ·çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŒºåŸŸåœ°çƒç³»ç»Ÿã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ•æ‰æ—¶ç©ºå˜åŒ–å’Œç‰¹å¾ç‰¹å®šå½±å“ï¼Œå…·æœ‰é«˜é¢„æµ‹å‡†ç¡®ç‡å’Œå‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒHydroTraceè¿˜åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿å…¬ä¼—æ›´å®¹æ˜“ç†è§£å’Œåº”ç”¨å…¶è§è§£ï¼Œä¸ºæ°´æ–‡å’Œæ›´å¹¿æ³›çš„åœ°çƒç³»ç»Ÿå»ºæ¨¡æä¾›äº†å…ˆè¿›å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HydroTraceæ˜¯ä¸€ç§ç®—æ³•é©±åŠ¨ã€æ•°æ®æ— å…³çš„æ°´æ–‡æ¨¡å‹ï¼Œé€‚ç”¨äºå…·æœ‰æŒ‘æˆ˜æ€§çš„åŒºåŸŸåœ°çƒç³»ç»Ÿã€‚</li>
<li>HydroTraceé€šè¿‡åˆ©ç”¨å…ˆè¿›çš„æ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰æ—¶ç©ºå˜åŒ–å’Œç‰¹å¾ç‰¹å®šå½±å“ï¼Œä»è€Œå®ç°é«˜é¢„æµ‹å‡†ç¡®ç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>HydroTraceå®ç°äº†é«˜è¾¾98%çš„Nash-Sutcliffeæ•ˆç‡ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå’Œæ··åˆæ¨¡å‹ã€‚</li>
<li>HydroTraceèƒ½å¤Ÿé‡åŒ–å¹¶è§£æå¾„æµçš„æ—¶ç©ºåˆ†å¸ƒå’Œç©ºé—´åˆ†è¾¨ç‡ï¼Œè§£é‡Šå¦‚å†°å·-é›ª-å¾„æµç›¸äº’ä½œç”¨å’Œå­£é£åŠ¨æ€ç­‰æ°´æ–‡è¡Œä¸ºã€‚</li>
<li>HydroTraceåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿å¾—å…¬ä¼—æ›´å®¹æ˜“ç†è§£å’Œåº”ç”¨å…¶è§è§£ã€‚</li>
<li>HydroTraceåœ¨å®ç”¨æ€§å’Œå‡†ç¡®æ€§æ–¹é¢ä¸ºæ°´æ–‡æ¨¡å‹å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ï¼Œå¹¶å¯¹å¹¿æ³›çš„åœ°çƒç³»ç»Ÿå»ºæ¨¡å…·æœ‰é‡å¤§æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c7fec4cf5f469bf55f35fb2e5b685f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0e163e48e31fafbf48a146f56005f3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fa12cd0c10b1e27d05bb11951feb4c5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MoColl-Agent-Based-Specific-and-General-Model-Collaboration-for-Image-Captioning"><a href="#MoColl-Agent-Based-Specific-and-General-Model-Collaboration-for-Image-Captioning" class="headerlink" title="MoColl: Agent-Based Specific and General Model Collaboration for Image   Captioning"></a>MoColl: Agent-Based Specific and General Model Collaboration for Image   Captioning</h2><p><strong>Authors:Pu Yang, Bin Dong</strong></p>
<p>Image captioning is a critical task at the intersection of computer vision and natural language processing, with wide-ranging applications across various domains. For complex tasks such as diagnostic report generation, deep learning models require not only domain-specific image-caption datasets but also the incorporation of relevant general knowledge to provide contextual accuracy. Existing approaches exhibit inherent limitations: specialized models excel in capturing domain-specific details but lack generalization, while vision-language models (VLMs) built on large language models (LLMs) leverage general knowledge but struggle with domain-specific adaptation. To address these limitations, this paper proposes a novel agent-enhanced model collaboration framework, which we call MoColl, designed to effectively integrate domain-specific and general knowledge. Specifically, our approach is to decompose complex image captioning tasks into a series of interconnected question-answer subtasks. A trainable visual question answering (VQA) model is employed as a specialized tool to focus on domain-specific visual analysis, answering task-specific questions based on image content. Concurrently, an LLM-based agent with general knowledge formulates these questions and synthesizes the resulting question-answer pairs into coherent captions. Beyond its role in leveraging the VQA model, the agent further guides its training to enhance its domain-specific capabilities. Experimental results on radiology report generation validate the effectiveness of the proposed framework, demonstrating significant improvements in the quality of generated reports. </p>
<blockquote>
<p>å›¾åƒæ ‡æ³¨æ˜¯è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„äº¤å‰ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨èŒƒå›´ï¼Œæ¶µç›–å„ä¸ªé¢†åŸŸã€‚å¯¹äºç”Ÿæˆè¯Šæ–­æŠ¥å‘Šç­‰å¤æ‚ä»»åŠ¡ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ä»…éœ€è¦ç‰¹å®šé¢†åŸŸçš„å›¾åƒæ ‡æ³¨æ•°æ®é›†ï¼Œè¿˜éœ€è¦èå…¥ç›¸å…³çš„é€šç”¨çŸ¥è¯†æ¥æä¾›ä¸Šä¸‹æ–‡å‡†ç¡®æ€§ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ï¼šç‰¹å®šé¢†åŸŸçš„æ¨¡å‹æ“…é•¿æ•æ‰ç‰¹å®šç»†èŠ‚ï¼Œä½†ç¼ºä¹æ³›åŒ–èƒ½åŠ›ï¼›è€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åˆ©ç”¨é€šç”¨çŸ¥è¯†ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§æ–¹é¢å´é‡åˆ°å›°éš¾ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ä»£ç†å¢å¼ºæ¨¡å‹åä½œæ¡†æ¶ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºMoCollï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°æ•´åˆç‰¹å®šé¢†åŸŸçŸ¥è¯†å’Œé€šç”¨çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯å°†å¤æ‚çš„å›¾åƒæ ‡æ³¨ä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—ç›¸äº’å…³è”çš„é—®ç­”å­ä»»åŠ¡ã€‚é‡‡ç”¨å¯è®­ç»ƒçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹ä½œä¸ºä¸“ç”¨å·¥å…·ï¼Œä¸“æ³¨äºç‰¹å®šé¢†åŸŸçš„è§†è§‰åˆ†æï¼Œæ ¹æ®å›¾åƒå†…å®¹å›ç­”ç‰¹å®šä»»åŠ¡çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†åˆ©ç”¨é€šç”¨çŸ¥è¯†æ¥åˆ¶å®šè¿™äº›é—®é¢˜ï¼Œå¹¶å°†å¾—åˆ°çš„é—®é¢˜ç­”æ¡ˆå¯¹åˆæˆè¿è´¯çš„æ ‡æ³¨ã€‚é™¤äº†å‘æŒ¥å¯¹VQAæ¨¡å‹çš„åˆ©ç”¨ä½œç”¨å¤–ï¼Œè¯¥ä»£ç†è¿˜è¿›ä¸€æ­¥æŒ‡å¯¼å…¶è®­ç»ƒï¼Œä»¥å¢å¼ºå…¶ç‰¹å®šé¢†åŸŸçš„èƒ½åŠ›ã€‚åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹é¢çš„å®éªŒç»“æœéªŒè¯äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºç”Ÿæˆçš„æŠ¥å‘Šè´¨é‡æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01834v2">PDF</a> </p>
<p><strong>Summary</strong><br>     å›¾åƒæè¿°æ˜¯è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„é‡è¦ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸã€‚é’ˆå¯¹è¯Šæ–­æŠ¥å‘Šç”Ÿæˆç­‰å¤æ‚ä»»åŠ¡ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ä»…éœ€è¦ç‰¹å®šçš„å›¾åƒæè¿°æ•°æ®é›†ï¼Œè¿˜éœ€è¦èå…¥ç›¸å…³çš„é€šç”¨çŸ¥è¯†ä»¥æé«˜ä¸Šä¸‹æ–‡å‡†ç¡®æ€§ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼šç‰¹å®šæ¨¡å‹æ“…é•¿æ•æ‰ç‰¹å®šé¢†åŸŸçš„ç»†èŠ‚ï¼Œä½†ç¼ºä¹æ³›åŒ–èƒ½åŠ›ï¼›è€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è™½ç„¶å¯ä»¥åˆ©ç”¨é€šç”¨çŸ¥è¯†ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§æ–¹é¢å´é‡åˆ°å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»£ç†å¢å¼ºæ¨¡å‹åä½œæ¡†æ¶ï¼ˆMoCollï¼‰ï¼Œæ—¨åœ¨æœ‰æ•ˆæ•´åˆç‰¹å®šé¢†åŸŸå’Œé€šç”¨çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡æ–¹æ³•æ˜¯å°†å¤æ‚çš„å›¾åƒæè¿°ä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—ç›¸äº’å…³è”çš„é—®ç­”å­ä»»åŠ¡ã€‚é‡‡ç”¨å¯è®­ç»ƒçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹ä½œä¸ºç‰¹å®šå·¥å…·ï¼Œä¸“æ³¨äºç‰¹å®šé¢†åŸŸçš„è§†è§‰åˆ†æï¼Œæ ¹æ®å›¾åƒå†…å®¹å›ç­”ç‰¹å®šä»»åŠ¡é—®é¢˜ã€‚åŒæ—¶ï¼Œå…·æœ‰é€šç”¨çŸ¥è¯†çš„LLMåŸºäºä»£ç†åˆ¶å®šè¿™äº›é—®é¢˜ï¼Œå¹¶å°†ç»“æœé—®é¢˜ç­”æ¡ˆå¯¹åˆæˆè¿è´¯çš„æè¿°ã€‚é™¤äº†å‘æŒ¥åœ¨VQAæ¨¡å‹ä¸­çš„ä½œç”¨å¤–ï¼Œä»£ç†è¿˜è¿›ä¸€æ­¥æŒ‡å¯¼å…¶è®­ç»ƒï¼Œå¢å¼ºå…¶åœ¨ç‰¹å®šé¢†åŸŸçš„èƒ½åŠ›ã€‚åœ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹é¢çš„å®éªŒéªŒè¯äº†æ‰€ææ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºç”Ÿæˆçš„æŠ¥å‘Šè´¨é‡æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæè¿°æ˜¯è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„é‡è¦äº¤å‰ä»»åŠ¡ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨é¢†åŸŸã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å›¾åƒæè¿°ä»»åŠ¡ä¸­éœ€è¦ç‰¹å®šçš„å›¾åƒæè¿°æ•°æ®é›†å’Œé€šç”¨çŸ¥è¯†ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼šç‰¹å®šæ¨¡å‹ç¼ºä¹æ³›åŒ–èƒ½åŠ›ï¼Œè€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„é€‚åº”æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä»£ç†å¢å¼ºæ¨¡å‹åä½œæ¡†æ¶ï¼ˆMoCollï¼‰ï¼Œä»¥æ•´åˆç‰¹å®šé¢†åŸŸå’Œé€šç”¨çŸ¥è¯†ã€‚</li>
<li>MoCollæ¡†æ¶å°†å›¾åƒæè¿°ä»»åŠ¡åˆ†è§£ä¸ºä¸€ç³»åˆ—ç›¸äº’å…³è”çš„é—®ç­”å­ä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨å¯è®­ç»ƒçš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¨¡å‹å›ç­”ç‰¹å®šä»»åŠ¡é—®é¢˜ï¼ŒåŒæ—¶åˆ©ç”¨å…·æœ‰é€šç”¨çŸ¥è¯†çš„LLMä»£ç†åˆ¶å®šé—®é¢˜å’Œåˆæˆæè¿°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-05e91cfbc9ff29aa0b8cea1f238fa79c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00fb4fbb3c0821e5fe6748f616af58b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0283a42a9aa48959a6aa7894357d615.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04a640bc94b9f14871940594caf37f26.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GeoX-Geometric-Problem-Solving-Through-Unified-Formalized-Vision-Language-Pre-training"><a href="#GeoX-Geometric-Problem-Solving-Through-Unified-Formalized-Vision-Language-Pre-training" class="headerlink" title="GeoX: Geometric Problem Solving Through Unified Formalized   Vision-Language Pre-training"></a>GeoX: Geometric Problem Solving Through Unified Formalized   Vision-Language Pre-training</h2><p><strong>Authors:Renqiu Xia, Mingsheng Li, Hancheng Ye, Wenjie Wu, Hongbin Zhou, Jiakang Yuan, Tianshuo Peng, Xinyu Cai, Xiangchao Yan, Bin Wang, Conghui He, Botian Shi, Tao Chen, Junchi Yan, Bo Zhang</strong></p>
<p>Despite their proficiency in general tasks, Multi-modal Large Language Models (MLLMs) struggle with automatic Geometry Problem Solving (GPS), which demands understanding diagrams, interpreting symbols, and performing complex reasoning. This limitation arises from their pre-training on natural images and texts, along with the lack of automated verification in the problem-solving process. Besides, current geometric specialists are limited by their task-specific designs, making them less effective for broader geometric problems. To this end, we present GeoX, a multi-modal large model focusing on geometric understanding and reasoning tasks. Given the significant differences between geometric diagram-symbol and natural image-text, we introduce unimodal pre-training to develop a diagram encoder and symbol decoder, enhancing the understanding of geometric images and corpora. Furthermore, we introduce geometry-language alignment, an effective pre-training paradigm that bridges the modality gap between unimodal geometric experts. We propose a Generator-And-Sampler Transformer (GS-Former) to generate discriminative queries and eliminate uninformative representations from unevenly distributed geometric signals. Finally, GeoX benefits from visual instruction tuning, empowering it to take geometric images and questions as input and generate verifiable solutions. Experiments show that GeoX outperforms both generalists and geometric specialists on publicly recognized benchmarks, such as GeoQA, UniGeo, Geometry3K, and PGPS9k. </p>
<blockquote>
<p>å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šè¡¨ç°ç†Ÿç»ƒï¼Œä½†åœ¨è‡ªåŠ¨å‡ ä½•é—®é¢˜æ±‚è§£ï¼ˆGPSï¼‰æ–¹é¢å´é¢ä¸´æŒ‘æˆ˜ã€‚GPSéœ€è¦ç†è§£å›¾è¡¨ã€è§£é‡Šç¬¦å·å’Œè¿›è¡Œå¤æ‚æ¨ç†ã€‚è¿™ä¸€å±€é™æ€§æºäºå®ƒä»¬å¯¹è‡ªç„¶å›¾åƒå’Œæ–‡æœ¬çš„é¢„è®­ç»ƒï¼Œä»¥åŠé—®é¢˜è§£å†³è¿‡ç¨‹ä¸­ç¼ºä¹è‡ªåŠ¨éªŒè¯ã€‚æ­¤å¤–ï¼Œå½“å‰çš„å‡ ä½•ä¸“å®¶å—é™äºå…¶ç‰¹å®šä»»åŠ¡çš„è®¾è®¡ï¼Œä½¿å¾—ä»–ä»¬å¯¹æ›´å¹¿æ³›çš„å‡ ä½•é—®é¢˜å¤„ç†æ•ˆç‡è¾ƒä½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸“æ³¨äºå‡ ä½•ç†è§£å’Œæ¨ç†ä»»åŠ¡çš„GeoXå¤šæ¨¡æ€å¤§å‹æ¨¡å‹ã€‚è€ƒè™‘åˆ°å‡ ä½•å›¾å½¢ç¬¦å·ä¸å‡ ä½•å›¾åƒæ–‡æœ¬ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†å•æ¨¡æ€é¢„è®­ç»ƒæ¥å¼€å‘å›¾è¡¨ç¼–ç å™¨å’Œç¬¦å·è§£ç å™¨ï¼Œæé«˜å¯¹å‡ ä½•å›¾åƒå’Œè¯­æ–™çš„ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å‡ ä½•è¯­è¨€å¯¹é½çš„æœ‰æ•ˆé¢„è®­ç»ƒæ¨¡å¼ï¼Œå®ƒå¼¥åˆäº†å•æ¨¡æ€å‡ ä½•ä¸“å®¶ä¹‹é—´çš„æ¨¡æ€å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”Ÿæˆå™¨å’Œé‡‡æ ·å™¨Transformerï¼ˆGS-Formerï¼‰ï¼Œä»¥ç”ŸæˆåŒºåˆ†æ€§æŸ¥è¯¢å¹¶æ¶ˆé™¤ä¸å‡åŒ€åˆ†å¸ƒçš„å‡ ä½•ä¿¡å·ä¸­çš„æ— ç”¨è¡¨ç¤ºã€‚æœ€åï¼ŒGeoXå¾—ç›Šäºè§†è§‰æŒ‡ä»¤å¾®è°ƒï¼Œèƒ½å¤Ÿæ¥æ”¶å‡ ä½•å›¾åƒå’Œé—®é¢˜ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆå¯éªŒè¯çš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼ŒGeoXåœ¨å…¬è®¤çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºé€šç”¨æ¨¡å‹å’Œå‡ ä½•ä¸“å®¶æ¨¡å‹ï¼Œå¦‚GeoQAã€UniGeoã€Geometry3Kå’ŒPGPS9kç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11863v2">PDF</a> Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Alpha-Innovator/GeoX">https://github.com/Alpha-Innovator/GeoX</a></p>
<p><strong>Summary</strong>ï¼šå°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è‡ªåŠ¨å‡ ä½•é—®é¢˜æ±‚è§£ï¼ˆGPSï¼‰æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™è¦æ±‚ç†è§£å›¾è¡¨ã€è§£é‡Šç¬¦å·å’Œè¿›è¡Œå¤æ‚æ¨ç†ã€‚å…¶å±€é™æ€§æºäºå¯¹è‡ªç„¶å›¾åƒå’Œæ–‡æœ¬çš„é¢„è®­ç»ƒï¼Œä»¥åŠé—®é¢˜æ±‚è§£è¿‡ç¨‹ä¸­ç¼ºä¹è‡ªåŠ¨éªŒè¯ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºGeoXæ¨¡å‹ï¼Œä¸“æ³¨äºå‡ ä½•ç†è§£å’Œæ¨ç†ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥å•æ¨¡æ€é¢„è®­ç»ƒã€å‡ ä½•è¯­è¨€å¯¹é½å’Œç”Ÿæˆé‡‡æ ·å˜å‹å™¨ï¼ˆGS-Formerï¼‰ç­‰æ–¹æ³•ï¼Œæé«˜å‡ ä½•å›¾åƒå’Œè¯­æ–™çš„ç†è§£èƒ½åŠ›ï¼Œç¼©å°æ¨¡æ€é—´çš„å·®è·ã€‚å®éªŒè¡¨æ˜ï¼ŒGeoXåœ¨å…¬è®¤çš„åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºé€šç”¨æ¨¡å‹å’Œå‡ ä½•ä¸“å®¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªåŠ¨å‡ ä½•é—®é¢˜æ±‚è§£ï¼ˆGPSï¼‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>å±€é™æ€§çš„åŸå› æ˜¯æ¨¡å‹é¢„è®­ç»ƒä¸»è¦åŸºäºè‡ªç„¶å›¾åƒå’Œæ–‡æœ¬ï¼Œç¼ºä¹é’ˆå¯¹å‡ ä½•é—®é¢˜çš„ä¸“é—¨è®­ç»ƒã€‚</li>
<li>GeoXæ¨¡å‹æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡å¼•å…¥å•æ¨¡æ€é¢„è®­ç»ƒã€å‡ ä½•è¯­è¨€å¯¹é½ç­‰æ–¹æ³•æå‡å¯¹å‡ ä½•å›¾åƒå’Œè¯­æ–™çš„ç†è§£ã€‚</li>
<li>GeoXæ¨¡å‹é‡‡ç”¨ç”Ÿæˆé‡‡æ ·å˜å‹å™¨ï¼ˆGS-Formerï¼‰æ¥ç”Ÿæˆåˆ¤åˆ«æ€§æŸ¥è¯¢å¹¶æ¶ˆé™¤ä¸å‡åŒ€åˆ†å¸ƒçš„å‡ ä½•ä¿¡å·ä¸­çš„éä¿¡æ¯è¡¨ç¤ºã€‚</li>
<li>GeoXæ¨¡å‹é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼Œå¯ä»¥æ¥æ”¶å‡ ä½•å›¾åƒå’Œé—®é¢˜ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆå¯éªŒè¯çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGeoXåœ¨å¤šä¸ªå…¬è®¤çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºé€šç”¨æ¨¡å‹å’Œå‡ ä½•ä¸“å®¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ed163b9fa52b4d3c1af08308558ede1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9073f51406a4face60b735adc2d839dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26d4b89e6980cf38e191a469e2c205a8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Filter-then-Generate-Large-Language-Models-with-Structure-Text-Adapter-for-Knowledge-Graph-Completion"><a href="#Filter-then-Generate-Large-Language-Models-with-Structure-Text-Adapter-for-Knowledge-Graph-Completion" class="headerlink" title="Filter-then-Generate: Large Language Models with Structure-Text Adapter   for Knowledge Graph Completion"></a>Filter-then-Generate: Large Language Models with Structure-Text Adapter   for Knowledge Graph Completion</h2><p><strong>Authors:Ben Liu, Jihai Zhang, Fangquan Lin, Cheng Yang, Min Peng</strong></p>
<p>Large Language Models (LLMs) present massive inherent knowledge and superior semantic comprehension capability, which have revolutionized various tasks in natural language processing. Despite their success, a critical gap remains in enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence suggests that LLMs consistently perform worse than conventional KGC approaches, even through sophisticated prompt design or tailored instruction-tuning. Fundamentally, applying LLMs on KGC introduces several critical challenges, including a vast set of entity candidates, hallucination issue of LLMs, and under-exploitation of the graph structure. To address these challenges, we propose a novel instruction-tuning-based method, namely FtG. Specifically, we present a \textit{filter-then-generate} paradigm and formulate the KGC task into a multiple-choice question format. In this way, we can harness the capability of LLMs while mitigating the issue casused by hallucinations. Moreover, we devise a flexible ego-graph serialization prompt and employ a structure-text adapter to couple structure and text information in a contextualized manner. Experimental results demonstrate that FtG achieves substantial performance gain compared to existing state-of-the-art methods. The instruction dataset and code are available at \url{<a target="_blank" rel="noopener" href="https://github.com/LB0828/FtG%7D">https://github.com/LB0828/FtG}</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹¥æœ‰åºå¤§çš„å†…åœ¨çŸ¥è¯†å’Œå“è¶Šè¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„å„é¡¹ä»»åŠ¡ã€‚å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ–¹é¢ï¼ŒLLMä»å­˜åœ¨æ˜æ˜¾å·®è·ã€‚ç»éªŒè¯æ®è¡¨æ˜ï¼Œå³ä½¿é€šè¿‡å¤æ‚æç¤ºè®¾è®¡æˆ–é’ˆå¯¹æŒ‡ä»¤è°ƒæ•´ï¼ŒLLMçš„è¡¨ç°å§‹ç»ˆé€Šäºä¼ ç»ŸKGCæ–¹æ³•ã€‚æ ¹æœ¬åŸå› åœ¨äºï¼Œå°†LLMåº”ç”¨äºKGCé¢ä¸´å‡ ä¸ªå…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤§é‡çš„å®ä½“å€™é€‰å¯¹è±¡ã€LLMçš„å¹»è§‰é—®é¢˜ä»¥åŠå¯¹å›¾ç»“æ„çš„åˆ©ç”¨ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæŒ‡ä»¤è°ƒæ•´çš„æ–°æ–¹æ³•ï¼Œç§°ä¸ºFtGã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªâ€œè¿‡æ»¤ç„¶åç”Ÿæˆâ€çš„æ¨¡å¼ï¼Œå¹¶å°†KGCä»»åŠ¡åˆ¶å®šä¸ºå¤šé€‰é—®é¢˜æ ¼å¼ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å‡è½»å¹»è§‰é—®é¢˜å½±å“çš„åŒæ—¶ï¼Œåˆ©ç”¨LLMçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªçµæ´»çš„è‡ªæˆ‘å›¾åºåˆ—åŒ–æç¤ºï¼Œå¹¶é‡‡ç”¨äº†ç»“æ„æ–‡æœ¬é€‚é…å™¨ï¼Œä»¥è¯­å¢ƒåŒ–çš„æ–¹å¼å°†ç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯è¿›è¡Œç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒFtGå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æŒ‡ä»¤æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/LB0b28/FtG%E3%80%82">https://github.com/LB0b28/FtGã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09094v2">PDF</a> COLING 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å·¨å¤§çš„å†…åœ¨çŸ¥è¯†å’Œé«˜çº§çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„å„é¡¹ä»»åŠ¡ã€‚ç„¶è€Œï¼Œåœ¨çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ–¹é¢ï¼ŒLLMçš„åº”ç”¨ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚å°½ç®¡é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºæˆ–æŒ‡ä»¤è°ƒæ•´ï¼ŒLLMçš„è¡¨ç°ä»ç„¶ä¸åŠä¼ ç»Ÿçš„KGCæ–¹æ³•ã€‚åœ¨KGCä»»åŠ¡ä¸­åº”ç”¨LLMé¢ä¸´å®ä½“å€™é€‰é›†è§„æ¨¡å¤§ã€LLMçš„å¹»è§‰é—®é¢˜ä»¥åŠå›¾å½¢ç»“æ„åˆ©ç”¨ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæŒ‡ä»¤è°ƒæ•´çš„æ–¹æ³•FtGï¼Œå¹¶é‡‡ç”¨äº†â€œè¿‡æ»¤-ç”Ÿæˆâ€çš„æ¨¡å¼ï¼Œå°†KGCä»»åŠ¡è½¬åŒ–ä¸ºå¤šé€‰é—®é¢˜å½¢å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†çµæ´»çš„è‡ªæˆ‘å›¾åºåˆ—åŒ–æç¤ºï¼Œå¹¶é‡‡ç”¨ç»“æ„æ–‡æœ¬é€‚é…å™¨ä»¥è¯­å¢ƒåŒ–çš„æ–¹å¼ç»“åˆç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFtGç›¸è¾ƒäºç°æœ‰å…ˆè¿›æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå…·æœ‰å¼ºå¤§çš„å†…åœ¨çŸ¥è¯†å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä½†å…¶åœ¨çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ–¹é¢çš„åº”ç”¨ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>LLMåœ¨KGCæ–¹é¢çš„æŒ‘æˆ˜åŒ…æ‹¬å®ä½“å€™é€‰é›†è§„æ¨¡å¤§ã€LLMçš„å¹»è§‰é—®é¢˜ä»¥åŠå›¾å½¢ç»“æ„åˆ©ç”¨ä¸è¶³ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºæŒ‡ä»¤è°ƒæ•´çš„æ–¹æ³•FtGã€‚</li>
<li>FtGé‡‡ç”¨â€œè¿‡æ»¤-ç”Ÿæˆâ€æ¨¡å¼ï¼Œå°†KGCä»»åŠ¡è½¬åŒ–ä¸ºå¤šé€‰é—®é¢˜å½¢å¼ä»¥åˆ©ç”¨LLMçš„ä¼˜åŠ¿å¹¶ç¼“è§£å¹»è§‰é—®é¢˜ã€‚</li>
<li>FtGè®¾è®¡äº†çµæ´»çš„è‡ªæˆ‘å›¾åºåˆ—åŒ–æç¤ºï¼Œä»¥æ›´å¥½åœ°é€‚åº”KGCä»»åŠ¡ã€‚</li>
<li>FtGé‡‡ç”¨ç»“æ„æ–‡æœ¬é€‚é…å™¨ç»“åˆç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c0f0caca43fe55a9f9b62902d34260d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20db14047254ea12ab9ec15db2f7cf0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b686d8c1e237c0bf5890f9c311242e35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-114a1c92cda64b924c3a35a44380c477.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LUMIA-Linear-probing-for-Unimodal-and-MultiModal-Membership-Inference-Attacks-leveraging-internal-LLM-states"><a href="#LUMIA-Linear-probing-for-Unimodal-and-MultiModal-Membership-Inference-Attacks-leveraging-internal-LLM-states" class="headerlink" title="LUMIA: Linear probing for Unimodal and MultiModal Membership Inference   Attacks leveraging internal LLM states"></a>LUMIA: Linear probing for Unimodal and MultiModal Membership Inference   Attacks leveraging internal LLM states</h2><p><strong>Authors:Luis Ibanez-Lissen, Lorena Gonzalez-Manzano, Jose Maria de Fuentes, Nicolas Anciaux, Joaquin Garcia-Alfaro</strong></p>
<p>Large Language Models (LLMs) are increasingly used in a variety of applications, but concerns around membership inference have grown in parallel. Previous efforts focus on black-to-grey-box models, thus neglecting the potential benefit from internal LLM information. To address this, we propose the use of Linear Probes (LPs) as a method to detect Membership Inference Attacks (MIAs) by examining internal activations of LLMs. Our approach, dubbed LUMIA, applies LPs layer-by-layer to get fine-grained data on the model inner workings. We test this method across several model architectures, sizes and datasets, including unimodal and multimodal tasks. In unimodal MIA, LUMIA achieves an average gain of 15.71 % in Area Under the Curve (AUC) over previous techniques. Remarkably, LUMIA reaches AUC&gt;60% in 65.33% of cases â€“ an increment of 46.80% against the state of the art. Furthermore, our approach reveals key insights, such as the model layers where MIAs are most detectable. In multimodal models, LPs indicate that visual inputs can significantly contribute to detect MIAs â€“ AUC&gt;60% is reached in 85.90% of experiments. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§åº”ç”¨ä¸­è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†å…³äºæˆå‘˜æ¨ç†çš„æ‹…å¿§ä¹Ÿåœ¨æ—¥ç›Šå¢é•¿ã€‚ä¹‹å‰çš„åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨é»‘ç®±åˆ°ç°ç®±æ¨¡å‹ä¸Šï¼Œä»è€Œå¿½è§†äº†å†…éƒ¨LLMä¿¡æ¯çš„æ½œåœ¨ä»·å€¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨çº¿æ€§æ¢é’ˆï¼ˆLPsï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡æ£€æŸ¥LLMçš„å†…éƒ¨æ¿€æ´»æ¥æ£€æµ‹æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºLUMIAï¼Œå®ƒé€šè¿‡é€å±‚åº”ç”¨LPsæ¥è·å¾—å…³äºæ¨¡å‹å†…éƒ¨å·¥ä½œæœºåˆ¶çš„ç»†ç²’åº¦æ•°æ®ã€‚æˆ‘ä»¬åœ¨å¤šç§æ¨¡å‹æ¶æ„ã€è§„æ¨¡å’Œæ•°æ®é›†ä¸Šæµ‹è¯•äº†æ­¤æ–¹æ³•ï¼ŒåŒ…æ‹¬å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚åœ¨å•æ¨¡æ€MIAä¸­ï¼Œä¸ä¼ ç»Ÿçš„æŠ€æœ¯ç›¸æ¯”ï¼Œå¹³å‡å¢ç›Šæé«˜äº†AUCï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰çš„ç™¾åˆ†æ¯”è¾¾åˆ°å¹³å‡å¢åŠ ç™¾åˆ†æ¯”ä¸ºç™¾åˆ†ä¹‹äº”ç‚¹ä¸ƒåä¸€ï¼ˆAUCæé«˜äº†ç™¾åˆ†ä¹‹äº”ç‚¹ä¸ƒåä¸€ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨ç™¾åˆ†ä¹‹å…­åäº”ç‚¹ä¸‰ä¸‰çš„æ¡ˆä¾‹ä¸­ï¼ŒLumiaçš„AUCè¶…è¿‡äº†ç™¾åˆ†ä¹‹å…­åï¼ˆAUC&gt; 60%ï¼‰ï¼Œç›¸æ¯”ä¹‹å‰æœ€é«˜æ°´å¹³çš„æ¡ˆä¾‹æœ‰äº†å››åå…­ç‚¹å…«çš„æé«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ­ç¤ºäº†ä¸€äº›é‡è¦è§è§£ï¼Œæ¯”å¦‚æœ€èƒ½æ£€æµ‹åˆ°MIAçš„æ¨¡å‹å±‚ã€‚åœ¨å¤šæ¨¡æ€æ¨¡å‹ä¸­ï¼ŒLPsè¡¨æ˜è§†è§‰è¾“å…¥å¯ä»¥å¤§å¤§æœ‰åŠ©äºæ£€æµ‹MIAâ€”â€”åœ¨ç™¾åˆ†ä¹‹å…«åäº”ç‚¹ä¹çš„å®éªŒä¸­è¾¾åˆ°AUC&gt; 60%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19876v3">PDF</a> </p>
<p><strong>Summary</strong><br>LLMå†…éƒ¨ä¿¡æ¯ç”¨äºæ£€æµ‹æˆå‘˜æ¨ç†æ”»å‡»çš„æ–°æ–¹æ³•â€”â€”LUMIAæŠ€æœ¯æå‡ºåˆ©ç”¨çº¿æ€§æ¢å¤´ï¼ˆLPsï¼‰è¿›è¡Œæ¢ç©¶å†…éƒ¨æ´»åŠ¨å®ç°æ›´ç²¾ç»†çš„æ£€æµ‹ã€‚è¯¥æŠ€æœ¯èƒ½é€‚ç”¨äºä¸åŒæ¨¡å‹æ¶æ„ã€å¤§å°å’Œæ•°æ®é›†ï¼ŒåŒ…æ‹¬å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ï¼Œå¯æå‡æˆå‘˜æ¨ç†æ”»å‡»æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsåœ¨å¤šç§åº”ç”¨ä¸­çš„ä½¿ç”¨æ—¥ç›Šæ™®åŠï¼Œæˆå‘˜æ¨ç†æ”»å‡»çš„æ£€æµ‹å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚</li>
<li>LUMIAæŠ€æœ¯åˆ©ç”¨çº¿æ€§æ¢å¤´ï¼ˆLPsï¼‰é€šè¿‡æ£€æŸ¥LLMçš„å†…éƒ¨æ´»åŠ¨æ¥æ£€æµ‹æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAsï¼‰ã€‚</li>
<li>LUMIAèƒ½å¤Ÿåœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„ã€å¤§å°å’Œå•æ¨¡æ€ä»»åŠ¡ä¸Šå®ç°æ›´ç²¾ç»†çš„æ£€æµ‹ï¼Œå¹³å‡æå‡AUCå€¼è¾¾15.71%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19876">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d6608075f019d36321b2ac4fc2bb25c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bbd5c9815287a52534f31fedcbd3521.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Separating-Tongue-from-Thought-Activation-Patching-Reveals-Language-Agnostic-Concept-Representations-in-Transformers"><a href="#Separating-Tongue-from-Thought-Activation-Patching-Reveals-Language-Agnostic-Concept-Representations-in-Transformers" class="headerlink" title="Separating Tongue from Thought: Activation Patching Reveals   Language-Agnostic Concept Representations in Transformers"></a>Separating Tongue from Thought: Activation Patching Reveals   Language-Agnostic Concept Representations in Transformers</h2><p><strong>Authors:ClÃ©ment Dumas, Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West</strong></p>
<p>A central question in multilingual language modeling is whether large language models (LLMs) develop a universal concept representation, disentangled from specific languages. In this paper, we address this question by analyzing latent representations (latents) during a word translation task in transformer-based LLMs. We strategically extract latents from a source translation prompt and insert them into the forward pass on a target translation prompt. By doing so, we find that the output language is encoded in the latent at an earlier layer than the concept to be translated. Building on this insight, we conduct two key experiments. First, we demonstrate that we can change the concept without changing the language and vice versa through activation patching alone. Second, we show that patching with the mean over latents across different languages does not impair and instead improves the modelsâ€™ performance in translating the concept. Our results provide evidence for the existence of language-agnostic concept representations within the investigated models. </p>
<blockquote>
<p>åœ¨å¤šè¯­è¨€è¯­è¨€å»ºæ¨¡ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦è„±ç¦»äº†ç‰¹å®šè¯­è¨€ï¼Œå‘å±•å‡ºäº†ä¸€ç§é€šç”¨çš„æ¦‚å¿µè¡¨ç¤ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æåŸºäºtransformerçš„LLMåœ¨å•è¯ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ½œåœ¨è¡¨ç¤ºï¼ˆlatentsï¼‰ï¼Œæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬ç­–ç•¥æ€§åœ°ä»æºç¿»è¯‘æç¤ºä¸­æå–æ½œåœ¨å› ç´ ï¼Œå¹¶å°†å…¶æ’å…¥åˆ°ç›®æ ‡ç¿»è¯‘æç¤ºçš„å‰å‘ä¼ é€’ä¸­ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å‘ç°è¾“å‡ºè¯­è¨€æ¯”è¦ç¿»è¯‘çš„æ¦‚å¿µæ›´æ—©åœ°ç¼–ç åœ¨æ½œåœ¨å› ç´ ä¸­ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸¤é¡¹å…³é”®å®éªŒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜å¯ä»¥é€šè¿‡ä»…ä½¿ç”¨æ¿€æ´»è¡¥ä¸æ¥æ”¹å˜æ¦‚å¿µè€Œä¸æ”¹å˜è¯­è¨€ï¼Œåä¹‹äº¦ç„¶ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡ä¸åŒè¯­è¨€çš„æ½œåœ¨å› ç´ å¹³å‡å€¼è¿›è¡Œè¡¥ä¸å¹¶ä¸ä¼šæŸå®³æ¨¡å‹çš„æ€§èƒ½ï¼Œåè€Œèƒ½æé«˜å…¶åœ¨ç¿»è¯‘æ¦‚å¿µæ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æœæä¾›äº†è¯æ®ï¼Œè¯æ˜åœ¨æ‰€ç ”ç©¶çš„æ¨¡å‹ä¸­å­˜åœ¨ä¸è¯­è¨€æ— å…³çš„æ¦‚å¿µè¡¨ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.08745v3">PDF</a> 18 pages, 14 figures, previous version published under the title â€œHow   Do Llamas Process Multilingual Text? A Latent Exploration through Activation   Patchingâ€ at the ICML 2024 mechanistic interpretability workshop at   <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=0ku2hIm4BS">https://openreview.net/forum?id=0ku2hIm4BS</a></p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ç ”ç©¶äº†åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶å‘ç°è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨ä¸ç‰¹å®šè¯­è¨€æ— å…³çš„æ¦‚å¿µè¡¨ç¤ºã€‚é€šè¿‡åˆ†æå’Œä¿®æ”¹è¿™äº›æ½œåœ¨è¡¨ç¤ºï¼Œå®éªŒè¯æ˜å¯ä»¥åœ¨ä¸æ”¹å˜æ¦‚å¿µçš„æƒ…å†µä¸‹æ”¹å˜è¾“å‡ºè¯­è¨€ï¼ŒåŒæ—¶ä¹Ÿè¡¨æ˜ç”¨ä¸åŒè¯­è¨€çš„å¹³å‡æ½œåœ¨è¡¨ç¤ºè¿›è¡Œä¿®è¡¥å¯ä»¥æé«˜æ¨¡å‹çš„ç¿»è¯‘æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ½œåœ¨è¡¨ç¤ºæ˜¯ç ”ç©¶çš„é‡ç‚¹ã€‚</li>
<li>é€šè¿‡åˆ†æå‘ç°ï¼Œè¾“å‡ºè¯­è¨€è¢«ç¼–ç åœ¨ç¿»è¯‘è¦ç¿»è¯‘çš„æ¦‚å¿µçš„æ—©æœŸå±‚ä¸­ã€‚</li>
<li>é€šè¿‡æ¿€æ´»è¡¥ä¸æŠ€æœ¯ï¼Œå¯ä»¥åœ¨ä¸æ”¹å˜æ¦‚å¿µçš„æƒ…å†µä¸‹æ”¹å˜è¾“å‡ºè¯­è¨€ï¼Œåä¹‹äº¦ç„¶ã€‚</li>
<li>ä½¿ç”¨ä¸åŒè¯­è¨€çš„å¹³å‡æ½œåœ¨è¡¨ç¤ºè¿›è¡Œè¡¥ä¸å¯ä»¥æé«˜æ¨¡å‹çš„ç¿»è¯‘æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜è¯­è¨€æ¨¡å‹ä¸­å­˜åœ¨ä¸ç‰¹å®šè¯­è¨€æ— å…³çš„æ¦‚å¿µè¡¨ç¤ºã€‚</li>
<li>è¯­è¨€æ¨¡å‹çš„æ½œä»£è¡¨ç»“æ„æ­ç¤ºäº†ä¸€ç§æ½œåœ¨çš„æ™®éæ¦‚å¿µè¡¨ç¤ºæ–¹å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.08745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a6f0dcf2135a0d466cab5d526af4fed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db9ddc4cb6411eca1d6e0c851b33b4f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-993bdd7b4ea74eef66e4581405d419fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6d7caaa16edb2720fbf8f0efc3e090f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-13/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-13/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c55bb0419401785f71826165ed212ef5.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-13  A Mixed-Integer Conic Program for the Multi-Agent Moving-Target   Traveling Salesman Problem
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-10/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e6c154ef655b72c0d2b4c6aae0a3a143.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-10  Identity-Preserving Video Dubbing Using Motion Warping
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17204.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
