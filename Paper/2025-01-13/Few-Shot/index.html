<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-13  Low-Resource Text-to-Speech Synthesis Using Noise-Augmented Training of   ForwardTacotron">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-697ad8f1427eec64e2ef2669de2148fc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    45 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-13-æ›´æ–°"><a href="#2025-01-13-æ›´æ–°" class="headerlink" title="2025-01-13 æ›´æ–°"></a>2025-01-13 æ›´æ–°</h1><h2 id="Low-Resource-Text-to-Speech-Synthesis-Using-Noise-Augmented-Training-of-ForwardTacotron"><a href="#Low-Resource-Text-to-Speech-Synthesis-Using-Noise-Augmented-Training-of-ForwardTacotron" class="headerlink" title="Low-Resource Text-to-Speech Synthesis Using Noise-Augmented Training of   ForwardTacotron"></a>Low-Resource Text-to-Speech Synthesis Using Noise-Augmented Training of   ForwardTacotron</h2><p><strong>Authors:Kishor Kayyar Lakshminarayana, Frank Zalkow, Christian Dittmar, Nicola Pia, Emanuel A. P. Habets</strong></p>
<p>In recent years, several text-to-speech systems have been proposed to synthesize natural speech in zero-shot, few-shot, and low-resource scenarios. However, these methods typically require training with data from many different speakers. The speech quality across the speaker set typically is diverse and imposes an upper limit on the quality achievable for the low-resource speaker. In the current work, we achieve high-quality speech synthesis using as little as five minutes of speech from the desired speaker by augmenting the low-resource speaker data with noise and employing multiple sampling techniques during training. Our method requires only four high-quality, high-resource speakers, which are easy to obtain and use in practice. Our low-complexity method achieves improved speaker similarity compared to the state-of-the-art zero-shot method HierSpeech++ and the recent low-resource method AdapterMix while maintaining comparable naturalness. Our proposed approach can also reduce the data requirements for speech synthesis for new speakers and languages. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå·²æœ‰å¤šç§æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿè¢«æå‡ºï¼Œå¯åœ¨é›¶æ ·æœ¬ã€å°æ ·æœ¬æ–‡æœ¬èµ„æºç¨€ç¼ºçš„åœºæ™¯ä¸‹åˆæˆè‡ªç„¶è¯­éŸ³ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦ä½¿ç”¨æ¥è‡ªè®¸å¤šä¸åŒå‘è¨€äººçš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å‘è¨€äººé›†ä¸­çš„è¯­éŸ³è´¨é‡å„ä¸ç›¸åŒï¼Œå¹¶å¯¹ä½èµ„æºå‘è¨€äººçš„è¯­éŸ³è´¨é‡è®¾å®šäº†ä¸Šé™ã€‚åœ¨ç›®å‰çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç›®æ ‡å‘è¨€äººçš„äº”åˆ†é’Ÿçš„è¯­éŸ³æ¥å®ç°é«˜è´¨é‡çš„è¯­éŸ³åˆæˆï¼Œé€šè¿‡ä½¿ç”¨å™ªå£°æ¥å¢å¼ºä½èµ„æºè¯­éŸ³æ•°æ®å¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨å¤šç§é‡‡æ ·æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦å››ä¸ªé«˜è´¨é‡ã€é«˜èµ„æºçš„å‘è¨€äººï¼Œåœ¨å®è·µä¸­æ˜“äºè·å–å’Œä½¿ç”¨ã€‚æˆ‘ä»¬çš„ä½å¤æ‚åº¦æ–¹æ³•åœ¨æé«˜è¯´è¯äººç›¸ä¼¼åº¦æ–¹é¢ä¼˜äºæœ€æ–°çš„é›¶æ ·æœ¬æ–¹æ³•HierSpeech++å’Œæœ€è¿‘çš„ä½èµ„æºæ–¹æ³•AdapterMixï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„è‡ªç„¶åº¦ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•è¿˜å¯ä»¥é™ä½æ–°å‘è¨€äººå’Œæ–°è¯­è¨€çš„è¯­éŸ³åˆæˆå¯¹æ•°æ®çš„è¦æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05976v1">PDF</a> Accepted for publication at the 2025 IEEE International Conference on   Acoustics, Speech, and Signal Processing (ICASSP 2025) to be held at   Hyderabad, India</p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼Œé’ˆå¯¹é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œä½èµ„æºåœºæ™¯çš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿè¢«ç›¸ç»§æå‡ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡ä¸åŒè¯´è¯äººçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¯­éŸ³è´¨é‡å› è¯´è¯äººçš„ä¸åŒè€Œå„å¼‚ï¼Œå¹¶å¯¹ä½èµ„æºè¯´è¯äººçš„è¯­éŸ³è´¨é‡è®¾ç½®äº†ä¸Šé™ã€‚å½“å‰å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†ä½èµ„æºè¯´è¯äººçš„æ•°æ®å¢åŠ å™ªå£°å¹¶é‡‡ç”¨å¤šé‡é‡‡æ ·æŠ€æœ¯è¿›è¡Œè®­ç»ƒï¼Œä»…ä½¿ç”¨äº”åˆ†é’Ÿçš„ç›®æ ‡è¯´è¯äººçš„è¯­éŸ³å³å¯å®ç°é«˜è´¨é‡çš„è¯­éŸ³åˆæˆã€‚è¯¥æ–¹æ³•ä»…éœ€å››ä¸ªé«˜è´¨é‡ã€é«˜èµ„æºçš„è¯´è¯äººï¼Œæ˜“äºåœ¨å®é™…æ“ä½œä¸­è·å¾—å’Œä½¿ç”¨ã€‚ç›¸æ¯”æœ€æ–°çš„é›¶æ ·æœ¬æ–¹æ³•HierSpeech++å’Œè¿‘æœŸçš„ä½èµ„æºæ–¹æ³•AdapterMixï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒè‡ªç„¶æ€§çš„åŒæ—¶ï¼Œæé«˜äº†è¯´è¯äººç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯é™ä½æ–°è¯´è¯äººå’Œæ–°è¯­è¨€åˆæˆæ‰€éœ€çš„æ•°æ®è¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘å¹´æå‡ºçš„æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿä¸»è¦åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œä½èµ„æºåœºæ™¯ä¸‹è¿›è¡Œåˆæˆï¼Œä½†å­˜åœ¨å¯¹å¤§é‡ä¸åŒè¯´è¯äººæ•°æ®çš„éœ€æ±‚ã€‚</li>
<li>è¯­éŸ³è´¨é‡å› è¯´è¯äººçš„ä¸åŒè€Œå­˜åœ¨å·®å¼‚ï¼Œå¯¹ä½èµ„æºè¯´è¯äººçš„è¯­éŸ³è´¨é‡è®¾ç½®äº†ä¸Šé™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä»…ä½¿ç”¨äº”åˆ†é’Ÿç›®æ ‡è¯´è¯äººçš„è¯­éŸ³å’Œé€šè¿‡å¢åŠ å™ªå£°åŠå¤šé‡é‡‡æ ·æŠ€æœ¯å³å¯å®ç°é«˜è´¨é‡è¯­éŸ³åˆæˆã€‚</li>
<li>æ–¹æ³•ä»…éœ€å››ä¸ªé«˜è´¨é‡ã€é«˜èµ„æºçš„è¯´è¯äººï¼Œæ˜“äºè·å–å¹¶åœ¨å®è·µä¸­ä½¿ç”¨ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè‡ªç„¶æ€§çš„åŒæ—¶ï¼Œæé«˜äº†è¯´è¯äººç›¸ä¼¼æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é™ä½äº†æ–°è¯´è¯äººå’Œæ–°è¯­è¨€åˆæˆæ‰€éœ€çš„æ•°æ®è¦æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8627bb3acb34ae21e0051676b4a63d76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cdc86350e7df716c4eef9db8fb102d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b276c7df10d0c0ff301145f43106594.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dea2c4271cb0ac387f48852700a8b93.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Language-Inspired-Relation-Transfer-for-Few-shot-Class-Incremental-Learning"><a href="#Language-Inspired-Relation-Transfer-for-Few-shot-Class-Incremental-Learning" class="headerlink" title="Language-Inspired Relation Transfer for Few-shot Class-Incremental   Learning"></a>Language-Inspired Relation Transfer for Few-shot Class-Incremental   Learning</h2><p><strong>Authors:Yifan Zhao, Jia Li, Zeyin Song, Yonghong Tian</strong></p>
<p>Depicting novel classes with language descriptions by observing few-shot samples is inherent in human-learning systems. This lifelong learning capability helps to distinguish new knowledge from old ones through the increase of open-world learning, namely Few-Shot Class-Incremental Learning (FSCIL). Existing works to solve this problem mainly rely on the careful tuning of visual encoders, which shows an evident trade-off between the base knowledge and incremental ones. Motivated by human learning systems, we propose a new Language-inspired Relation Transfer (LRT) paradigm to understand objects by joint visual clues and text depictions, composed of two major steps. We first transfer the pretrained text knowledge to the visual domains by proposing a graph relation transformation module and then fuse the visual and language embedding by a text-vision prototypical fusion module. Second, to mitigate the domain gap caused by visual finetuning, we propose context prompt learning for fast domain alignment and imagined contrastive learning to alleviate the insufficient text data during alignment. With collaborative learning of domain alignments and text-image transfer, our proposed LRT outperforms the state-of-the-art models by over $13%$ and $7%$ on the final session of mini-ImageNet and CIFAR-100 FSCIL benchmarks. </p>
<blockquote>
<p>é€šè¿‡è§‚å¯Ÿå°‘æ•°æ ·æœ¬å¹¶ç”¨è¯­è¨€æè¿°æ¥æç»˜æ–°å‹ç±»åˆ«ï¼Œæ˜¯äººç±»å­¦ä¹ ç³»ç»Ÿçš„å›ºæœ‰ç‰¹æ€§ã€‚è¿™ç§ç»ˆèº«å­¦ä¹ èƒ½åŠ›æœ‰åŠ©äºé€šè¿‡å¼€æ”¾ä¸–ç•Œå­¦ä¹ çš„å¢åŠ æ¥åŒºåˆ†æ–°æ—§çŸ¥è¯†ï¼Œå³æ‰€è°“çš„Few-Shot Class-Incremental Learningï¼ˆFSCILï¼‰ã€‚ç°æœ‰è§£å†³æ­¤é—®é¢˜çš„ç ”ç©¶ä¸»è¦ä¾èµ–äºè§†è§‰ç¼–ç å™¨çš„ä»”ç»†è°ƒæ•´ï¼Œè¿™æ˜¾ç¤ºå‡ºåŸºç¡€çŸ¥è¯†å’Œå¢é‡çŸ¥è¯†ä¹‹é—´çš„æ˜æ˜¾æƒè¡¡ã€‚å—äººç±»å­¦ä¹ ç³»ç»Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€å¯å‘å…³ç³»è½¬ç§»ï¼ˆLRTï¼‰èŒƒå¼ï¼Œé€šè¿‡è”åˆè§†è§‰çº¿ç´¢å’Œæ–‡æœ¬æè¿°æ¥ç†è§£å¯¹è±¡ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ­¥éª¤ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºå›¾å…³ç³»è½¬æ¢æ¨¡å—å°†é¢„è®­ç»ƒçš„æ–‡æœ¬çŸ¥è¯†è½¬ç§»åˆ°è§†è§‰é¢†åŸŸï¼Œç„¶åé€šè¿‡æ–‡æœ¬è§†è§‰åŸå‹èåˆæ¨¡å—èåˆè§†è§‰å’Œè¯­è¨€åµŒå…¥ã€‚å…¶æ¬¡ï¼Œä¸ºäº†ç¼“è§£ç”±äºè§†è§‰å¾®è°ƒé€ æˆçš„é¢†åŸŸå·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡æç¤ºå­¦ä¹ è¿›è¡Œå¿«é€Ÿé¢†åŸŸå¯¹é½å’Œæƒ³è±¡å¯¹æ¯”å­¦ä¹ ï¼Œä»¥ç¼“è§£å¯¹é½è¿‡ç¨‹ä¸­çš„æ–‡æœ¬æ•°æ®ä¸è¶³ã€‚é€šè¿‡é¢†åŸŸå¯¹é½å’Œæ–‡æœ¬å›¾åƒä¼ è¾“çš„åä½œå­¦ä¹ ï¼Œæˆ‘ä»¬æå‡ºçš„LRTåœ¨mini-ImageNetå’ŒCIFAR-100 FSCILåŸºå‡†æµ‹è¯•çš„æœ€åä¸€æœŸæ¯”æœ€å…ˆè¿›çš„æ¨¡å‹é«˜å‡ºè¶…è¿‡13%å’Œ7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05862v1">PDF</a> Accepted by IEEE TPAMI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºäººç±»å­¦ä¹ ç³»ç»Ÿçš„å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è§†è§‰ç¼–ç å™¨çš„è°ƒæ•´ï¼Œå­˜åœ¨åŸºç¡€çŸ¥è¯†ä¸å¢é‡çŸ¥è¯†ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œæå‡ºä¸€ç§æ–°çš„è¯­è¨€å¯å‘å…³ç³»è½¬ç§»ï¼ˆLRTï¼‰èŒƒå¼ã€‚è¯¥èŒƒå¼é€šè¿‡ä¸¤ä¸ªä¸»è¦æ­¥éª¤ç†è§£å’Œè¯†åˆ«å¯¹è±¡ï¼šä¸€æ˜¯é€šè¿‡å›¾å…³ç³»è½¬æ¢æ¨¡å—å°†é¢„è®­ç»ƒçš„æ–‡æœ¬çŸ¥è¯†è½¬ç§»åˆ°è§†è§‰é¢†åŸŸï¼Œç„¶åé€šè¿‡æ–‡æœ¬-è§†è§‰åŸå‹èåˆæ¨¡å—èåˆè§†è§‰å’Œæ–‡æœ¬åµŒå…¥ã€‚äºŒæ˜¯ä¸ºäº†å‡å°‘å› è§†è§‰å¾®è°ƒå¯¼è‡´çš„é¢†åŸŸå·®è·ï¼Œæå‡ºäº†ä¸Šä¸‹æ–‡æç¤ºå­¦ä¹ å’Œæƒ³è±¡å¯¹æ¯”å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„LRTåœ¨mini-ImageNetå’ŒCIFAR-100çš„FSCILåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºç°æœ‰æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»å­¦ä¹ ç³»ç»Ÿèƒ½å¤Ÿé€šè¿‡å¯¹å°‘æ ·æœ¬çš„è§‚å¯Ÿï¼Œé€šè¿‡è¯­è¨€æè¿°æ¥æç»˜æ–°å‹ç±»åˆ«ï¼Œè¿™ä¸ºFew-Shot Class-Incremental Learningï¼ˆFSCILï¼‰æä¾›äº†å¯ç¤ºã€‚</li>
<li>ç°æœ‰è§£å†³FSCILé—®é¢˜çš„æ–¹æ³•ä¸»è¦ä¾èµ–è§†è§‰ç¼–ç å™¨çš„è°ƒæ•´ï¼Œå­˜åœ¨åŸºç¡€çŸ¥è¯†å’Œå¢é‡çŸ¥è¯†çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>æå‡ºçš„Language-inspired Relation Transferï¼ˆLRTï¼‰èŒƒå¼é€šè¿‡ç»“åˆè§†è§‰çº¿ç´¢å’Œæ–‡æœ¬æè¿°æ¥ç†è§£å¯¹è±¡ã€‚</li>
<li>LRTèŒƒå¼åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼šå°†é¢„è®­ç»ƒçš„æ–‡æœ¬çŸ¥è¯†è½¬ç§»åˆ°è§†è§‰é¢†åŸŸï¼Œç„¶åèåˆè§†è§‰å’Œæ–‡æœ¬åµŒå…¥ã€‚</li>
<li>ä¸ºå‡å°‘å› è§†è§‰å¾®è°ƒå¯¼è‡´çš„é¢†åŸŸå·®è·ï¼Œæå‡ºäº†ä¸Šä¸‹æ–‡æç¤ºå­¦ä¹ å’Œæƒ³è±¡å¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLRTåœ¨FSCILä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡ç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b6b20cd92253b45161bafd41232cdfe0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6aa2a0cccc223bd24a7e4f6b7003855b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c359e615a84c8e09a9042acb95b8f26d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a70759992b52099d3f430af09de0e99.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-Unsupervised-Graph-Few-shot-Learning-via-Set-Functions-and-Optimal-Transport"><a href="#Enhancing-Unsupervised-Graph-Few-shot-Learning-via-Set-Functions-and-Optimal-Transport" class="headerlink" title="Enhancing Unsupervised Graph Few-shot Learning via Set Functions and   Optimal Transport"></a>Enhancing Unsupervised Graph Few-shot Learning via Set Functions and   Optimal Transport</h2><p><strong>Authors:Yonghao Liu, Fausto Giunchiglia, Ximing Li, Lan Huang, Xiaoyue Feng, Renchu Guan</strong></p>
<p>Graph few-shot learning has garnered significant attention for its ability to rapidly adapt to downstream tasks with limited labeled data, sparking considerable interest among researchers. Recent advancements in graph few-shot learning models have exhibited superior performance across diverse applications. Despite their successes, several limitations still exist. First, existing models in the meta-training phase predominantly focus on instance-level features within tasks, neglecting crucial set-level features essential for distinguishing between different categories. Second, these models often utilize query sets directly on classifiers trained with support sets containing only a few labeled examples, overlooking potential distribution shifts between these sets and leading to suboptimal performance. Finally, previous models typically require necessitate abundant labeled data from base classes to extract transferable knowledge, which is typically infeasible in real-world scenarios. To address these issues, we propose a novel model named STAR, which leverages Set funcTions and optimAl tRansport for enhancing unsupervised graph few-shot learning. Specifically, STAR utilizes expressive set functions to obtain set-level features in an unsupervised manner and employs optimal transport principles to align the distributions of support and query sets, thereby mitigating distribution shift effects. Theoretical analysis demonstrates that STAR can capture more task-relevant information and enhance generalization capabilities. Empirically, extensive experiments across multiple datasets validate the effectiveness of STAR. Our code can be found here. </p>
<blockquote>
<p>å›¾å°‘é‡å­¦ä¹ ï¼ˆGraph Few-Shot Learningï¼‰å› å…¶èƒ½å¤Ÿåœ¨æœ‰é™æ ‡è®°æ•°æ®ä¸‹è¿…é€Ÿé€‚åº”ä¸‹æ¸¸ä»»åŠ¡çš„èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå¼•å‘äº†ç ”ç©¶äººå‘˜çš„æå¤§å…´è¶£ã€‚æœ€è¿‘çš„å›¾å°‘é‡å­¦ä¹ æ¨¡å‹çš„è¿›å±•åœ¨å„ç§åº”ç”¨ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†ä»å­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚é¦–å…ˆï¼Œç°æœ‰æ¨¡å‹åœ¨å…ƒè®­ç»ƒé˜¶æ®µä¸»è¦å…³æ³¨ä»»åŠ¡å†…çš„å®ä¾‹çº§ç‰¹å¾ï¼Œè€Œå¿½ç•¥äº†å¯¹äºåŒºåˆ†ä¸åŒç±»åˆ«è‡³å…³é‡è¦çš„é›†åˆçº§ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ç›´æ¥åœ¨ç”±æ”¯æŒé›†ï¼ˆä»…åŒ…å«å°‘é‡æ ‡è®°ç¤ºä¾‹ï¼‰è®­ç»ƒçš„åˆ†ç±»å™¨ä¸Šä½¿ç”¨æŸ¥è¯¢é›†ï¼Œå¿½è§†äº†è¿™äº›é›†åˆä¹‹é—´æ½œåœ¨çš„åˆ†å¸ƒåç§»ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æœ€åï¼Œä»¥å‰çš„æ¨¡å‹é€šå¸¸éœ€è¦æ¥è‡ªåŸºç±»çš„ä¸°å¯Œæ ‡è®°æ•°æ®æ¥æå–å¯è½¬ç§»çŸ¥è¯†ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­é€šå¸¸ä¸å¯è¡Œã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹STARï¼Œå®ƒåˆ©ç”¨é›†åˆå‡½æ•°å’Œæœ€ä¼˜ä¼ è¾“ï¼ˆSet funcTions and optimAl tRansportï¼‰æ¥æé«˜æ— ç›‘ç£å›¾å°‘é‡å­¦ä¹ çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒSTARä½¿ç”¨è¡¨è¾¾æ€§å¼ºçš„é›†åˆå‡½æ•°ä»¥æ— ç›‘ç£çš„æ–¹å¼è·å–é›†åˆçº§ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨æœ€ä¼˜ä¼ è¾“åŸç†æ¥å¯¹é½æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†çš„åˆ†å¸ƒï¼Œä»è€Œå‡è½»åˆ†å¸ƒåç§»çš„å½±å“ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒSTARèƒ½å¤Ÿæ•è·æ›´å¤šä»»åŠ¡ç›¸å…³ä¿¡æ¯å¹¶æé«˜æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†STARçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05635v1">PDF</a> KDD2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å›¾å°‘æ ·æœ¬å­¦ä¹ çš„é‡è¦æ€§åŠå…¶åœ¨é¢å¯¹ä¸‹æ¸¸ä»»åŠ¡æ—¶çš„ä¼˜åŠ¿ã€‚å°½ç®¡ç°æœ‰æ¨¡å‹åœ¨å¤šä¸ªåº”ç”¨é¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å®ƒä»¬ä»é¢ä¸´ä¸€äº›å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSTARçš„æ–°å‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨é›†åˆå‡½æ•°å’Œæœ€ä½³ä¼ è¾“æŠ€æœ¯ï¼Œä»¥æ— ç›‘ç£çš„æ–¹å¼è§£å†³å›¾å°‘æ ·æœ¬å­¦ä¹ é—®é¢˜ã€‚é€šè¿‡ç†è®ºåˆ†æéªŒè¯ï¼ŒSTARèƒ½æ›´å¥½åœ°æ•æ‰ä»»åŠ¡ç›¸å…³ä¿¡æ¯å¹¶æå‡æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†STARçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾å°‘æ ·æœ¬å­¦ä¹ èƒ½è¿…é€Ÿé€‚åº”ä¸‹æ¸¸ä»»åŠ¡ï¼Œå…·æœ‰æœ‰é™æ ‡è®°æ•°æ®çš„ä¼˜åŠ¿ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä¸»è¦å…³æ³¨å®ä¾‹çº§ç‰¹å¾ï¼Œå¿½è§†äº†é›†åˆçº§ç‰¹å¾çš„é‡è¦æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨å…ƒè®­ç»ƒé˜¶æ®µç›´æ¥ä½¿ç”¨æŸ¥è¯¢é›†è¿›è¡Œè®­ç»ƒå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æ¨¡å‹é€šå¸¸ä¾èµ–äºå¤§é‡çš„åŸºæœ¬ç±»åˆ«çš„æ ‡è®°æ•°æ®æ¥æå–å¯è½¬ç§»çŸ¥è¯†ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­é€šå¸¸ä¸å¯è¡Œã€‚</li>
<li>æå‡ºçš„STARæ¨¡å‹åˆ©ç”¨é›†åˆå‡½æ•°ä»¥æ— ç›‘ç£æ–¹å¼è·å–é›†åˆçº§ç‰¹å¾ã€‚</li>
<li>STARé‡‡ç”¨æœ€ä½³ä¼ è¾“åŸåˆ™æ¥å¯¹é½æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†åˆ†å¸ƒï¼Œå‡å°‘åˆ†å¸ƒåç§»çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-04a6f67322a1f465680b0a4f95979b16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d37a9d3098f21fedd2d73b0d2b43971.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a13133522a9ddff8674ca1c5e7ec666.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff1e2cdb8c45e0b8a53a9515d30f64f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b57ca75b05f9d40190f2f712b8e9c8a7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-Impact-of-Model-Scaling-on-Seen-and-Unseen-Language-Performance"><a href="#The-Impact-of-Model-Scaling-on-Seen-and-Unseen-Language-Performance" class="headerlink" title="The Impact of Model Scaling on Seen and Unseen Language Performance"></a>The Impact of Model Scaling on Seen and Unseen Language Performance</h2><p><strong>Authors:Rhitabrat Pokharel, Sina Bagheri Nezhad, Ameeta Agrawal, Suresh Singh</strong></p>
<p>The rapid advancement of Large Language Models (LLMs), particularly those trained on multilingual corpora, has intensified the need for a deeper understanding of their performance across a diverse range of languages and model sizes. Our research addresses this critical need by studying the performance and scaling behavior of multilingual LLMs in text classification and machine translation tasks across 204 languages. We systematically examine both seen and unseen languages across three model families of varying sizes in zero-shot and few-shot settings. Our findings show significant differences in scaling behavior between zero-shot and two-shot scenarios, with striking disparities in performance between seen and unseen languages. Model scale has little effect on zero-shot performance, which remains mostly flat. However, in two-shot settings, larger models show clear linear improvements in multilingual text classification. For translation tasks, however, only the instruction-tuned model showed clear benefits from scaling. Our analysis also suggests that overall resource levels, not just the proportions of pretraining languages, are better predictors of model performance, shedding light on what drives multilingual LLM effectiveness. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šè¯­è¨€è¯­æ–™åº“ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦æ›´æ·±å…¥åœ°ç†è§£å…¶åœ¨å¤šç§è¯­è¨€å’Œä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡ç ”ç©¶å’Œæ¯”è¾ƒä¸åŒå¤§å°æ¨¡å‹å®¶æ—åœ¨204ç§è¯­è¨€çš„æ–‡æœ¬åˆ†ç±»å’Œæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œæ‰©å±•è¡Œä¸ºæ¥åº”å¯¹è¿™ä¸€è¿«åˆ‡éœ€æ±‚ã€‚æˆ‘ä»¬åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸­ï¼Œç³»ç»Ÿç ”ç©¶äº†å¯è§è¯­è¨€å’Œæœªè§è¯­è¨€åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°é›¶æ ·æœ¬å’Œä¸¤æ ·æœ¬åœºæ™¯ä¸­çš„æ‰©å±•è¡Œä¸ºå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¹¶ä¸”åœ¨å¯è§è¯­è¨€å’Œæœªè§è¯­è¨€ä¹‹é—´è¡¨ç°å‡ºæƒŠäººçš„è¡¨ç°å·®å¼‚ã€‚æ¨¡å‹è§„æ¨¡å¯¹é›¶æ ·æœ¬è¡¨ç°å‡ ä¹æ²¡æœ‰å½±å“ï¼ŒåŸºæœ¬ä¸Šä¿æŒç¨³å®šã€‚ç„¶è€Œï¼Œåœ¨ä¸¤æ ·æœ¬ç¯å¢ƒä¸­ï¼Œå¤§å‹æ¨¡å‹åœ¨è·¨è¯­è¨€æ–‡æœ¬åˆ†ç±»ä¸­æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„çº¿æ€§æ”¹è¿›ã€‚ä½†å¯¹äºç¿»è¯‘ä»»åŠ¡æ¥è¯´ï¼Œåªæœ‰æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹æ‰èƒ½ä»æ‰©å±•ä¸­æ˜æ˜¾å—ç›Šã€‚æˆ‘ä»¬çš„åˆ†æè¿˜è¡¨æ˜ï¼Œæ•´ä½“èµ„æºæ°´å¹³ï¼Œè€Œéé¢„è®­ç»ƒè¯­è¨€çš„æ¯”ä¾‹æ›´èƒ½é¢„æµ‹æ¨¡å‹æ€§èƒ½ï¼Œè¿™æ­ç¤ºäº†æ¨åŠ¨å¤šè¯­è¨€LLMæœ‰æ•ˆæ€§çš„å› ç´ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05629v1">PDF</a> Accepted at SEAS Workshop at AAAI25</p>
<p><strong>Summary</strong></p>
<p>å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ†ç±»å’Œæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œæ‰©å±•è¡Œä¸ºç ”ç©¶ã€‚ç ”ç©¶æ¶‰åŠä¸‰ä¸ªä¸åŒè§„æ¨¡çš„æ¨¡å‹å®¶æ—ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹å¯¹204ç§è¯­è¨€çš„æ€§èƒ½è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å‘ç°é›¶æ ·æœ¬å’Œä¸¤æ ·æœ¬åœºæ™¯ä¸‹çš„æ‰©å±•è¡Œä¸ºå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸åŒè¯­è¨€å’Œæ¨¡å‹è§„æ¨¡ä¸‹çš„æ€§èƒ½å·®å¼‚æ˜æ˜¾ã€‚æ¨¡å‹è§„æ¨¡å¯¹é›¶æ ·æœ¬æ€§èƒ½å½±å“è¾ƒå°ï¼Œè€Œåœ¨ä¸¤æ ·æœ¬åœºæ™¯ä¸‹ï¼Œå¤§å‹æ¨¡å‹åœ¨è·¨è¯­è¨€æ–‡æœ¬åˆ†ç±»ä¸­çš„è¡¨ç°å‘ˆç°çº¿æ€§æå‡ã€‚å¯¹äºç¿»è¯‘ä»»åŠ¡ï¼Œåªæœ‰ç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹æ‰èƒ½ä»æ‰©å±•ä¸­è·å¾—æ˜æ˜¾ä¼˜åŠ¿ã€‚æ•´ä½“èµ„æºæ°´å¹³è€Œéé¢„è®­ç»ƒè¯­è¨€çš„æ¯”ä¾‹èƒ½æ›´å¥½åœ°é¢„æµ‹æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ†ç±»å’Œæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­çš„æ€§èƒ½éœ€æ·±å…¥ç ”ç©¶ã€‚</li>
<li>é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹çš„æ¨¡å‹è¡¨ç°å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡å¯¹é›¶æ ·æœ¬æ€§èƒ½å½±å“è¾ƒå°ï¼Œä½†åœ¨ä¸¤æ ·æœ¬åœºæ™¯ä¸‹ï¼Œå¤§å‹æ¨¡å‹åœ¨è·¨è¯­è¨€æ–‡æœ¬åˆ†ç±»ä¸­æœ‰çº¿æ€§æå‡ã€‚</li>
<li>å¯¹äºæœºå™¨ç¿»è¯‘ä»»åŠ¡ï¼Œåªæœ‰ç»è¿‡æŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹èƒ½ä»æ‰©å±•ä¸­è·å¾—ä¼˜åŠ¿ã€‚</li>
<li>æ•´ä½“èµ„æºæ°´å¹³æ˜¯é¢„æµ‹æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ã€‚</li>
<li>ä¸åŒè¯­è¨€å’Œæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚æ˜æ˜¾ï¼Œéœ€è¦é’ˆå¯¹ä¸åŒè¯­è¨€å’Œä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-697ad8f1427eec64e2ef2669de2148fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78e98036ca38a594a5a6ca142d51a33c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f5cd3ea48359374f3e4f5cafa13de35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-372cf91c6ce058cc7b0e6a83002b3639.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd4e035062a33c430adb2227b79856f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ea90ffe8a1c8b43065d3059b07e0914.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb8dbaa20c4d8f29c02e6b6c1926a301.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TimeDP-Learning-to-Generate-Multi-Domain-Time-Series-with-Domain-Prompts"><a href="#TimeDP-Learning-to-Generate-Multi-Domain-Time-Series-with-Domain-Prompts" class="headerlink" title="TimeDP: Learning to Generate Multi-Domain Time Series with Domain   Prompts"></a>TimeDP: Learning to Generate Multi-Domain Time Series with Domain   Prompts</h2><p><strong>Authors:Yu-Hao Huang, Chang Xu, Yueying Wu, Wu-Jun Li, Jiang Bian</strong></p>
<p>Time series generation models are crucial for applications like data augmentation and privacy preservation. Most existing time series generation models are typically designed to generate data from one specified domain. While leveraging data from other domain for better generalization is proved to work in other application areas, this approach remains challenging for time series modeling due to the large divergence in patterns among different real world time series categories. In this paper, we propose a multi-domain time series diffusion model with domain prompts, named TimeDP. In TimeDP, we utilize a time series semantic prototype module which defines time series prototypes to represent time series basis, each prototype vector serving as â€œwordâ€ representing some elementary time series feature. A prototype assignment module is applied to extract the extract domain specific prototype weights, for learning domain prompts as generation condition. During sampling, we extract â€œdomain promptâ€ with few-shot samples from the target domain and use the domain prompts as condition to generate time series samples. Experiments demonstrate that our method outperforms baselines to provide the state-of-the-art in-domain generation quality and strong unseen domain generation capability. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—ç”Ÿæˆæ¨¡å‹åœ¨æ•°æ®å¢å¼ºå’Œéšç§ä¿æŠ¤ç­‰åº”ç”¨ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚å¤§å¤šæ•°ç°æœ‰çš„æ—¶é—´åºåˆ—ç”Ÿæˆæ¨¡å‹é€šå¸¸è¢«è®¾è®¡ç”¨æ¥ä»ç‰¹å®šé¢†åŸŸç”Ÿæˆæ•°æ®ã€‚è™½ç„¶åˆ©ç”¨å…¶ä»–é¢†åŸŸçš„æ•°æ®ä»¥å®ç°æ›´å¥½çš„æ³›åŒ–åœ¨å…¶ä»–åº”ç”¨é¢†åŸŸå·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ç”±äºä¸åŒç°å®ä¸–ç•Œæ—¶é—´åºåˆ—ç±»åˆ«ä¹‹é—´æ¨¡å¼å­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œè¿™ç§æ–¹æ³•åœ¨æ—¶é—´åºåˆ—å»ºæ¨¡ä¸­ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰é¢†åŸŸæç¤ºçš„å¤šé¢†åŸŸæ—¶é—´åºåˆ—æ‰©æ•£æ¨¡å‹ï¼Œåä¸ºTimeDPã€‚åœ¨TimeDPä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨æ—¶é—´åºåˆ—è¯­ä¹‰åŸå‹æ¨¡å—æ¥å®šä¹‰æ—¶é—´åºåˆ—åŸå‹ä»¥è¡¨ç¤ºæ—¶é—´åºåˆ—åŸºç¡€ï¼Œæ¯ä¸ªåŸå‹å‘é‡éƒ½ä½œä¸ºä»£è¡¨æŸäº›åŸºæœ¬æ—¶é—´åºåˆ—ç‰¹å¾çš„â€œå•è¯â€ã€‚åº”ç”¨åŸå‹åˆ†é…æ¨¡å—æ¥æå–é¢†åŸŸç‰¹å®šçš„åŸå‹æƒé‡ï¼Œä½œä¸ºç”Ÿæˆæ¡ä»¶çš„é¢†åŸŸæç¤ºè¿›è¡Œå­¦ä¹ ã€‚åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»ç›®æ ‡é¢†åŸŸæå–å°‘é‡æ ·æœ¬çš„â€œé¢†åŸŸæç¤ºâ€ï¼Œå¹¶å°†é¢†åŸŸæç¤ºä½œä¸ºæ¡ä»¶æ¥ç”Ÿæˆæ—¶é—´åºåˆ—æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢†åŸŸå†…ç”Ÿæˆè´¨é‡å’Œæœªè§é¢†åŸŸçš„ç”Ÿæˆèƒ½åŠ›æ–¹é¢å‡è¶…è¿‡äº†åŸºçº¿æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05403v1">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTimeDPçš„å¤šåŸŸæ—¶é—´åºåˆ—æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥æ—¶é—´åºåˆ—è¯­ä¹‰åŸå‹æ¨¡å—å’ŒåŸå‹åˆ†é…æ¨¡å—ï¼Œå®ç°è·¨åŸŸç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæå–ç‰¹å®šåŸŸçš„æ—¶é—´åºåˆ—åŸå‹æƒé‡ï¼Œå°†å…¶ä½œä¸ºç”Ÿæˆæ¡ä»¶è¿›è¡Œå­¦ä¹ ï¼Œä»è€Œåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­åˆ©ç”¨æ¥è‡ªç›®æ ‡åŸŸçš„å°‘é‡æ ·æœ¬æå–â€œåŸŸæç¤ºâ€ï¼Œå¹¶ä½œä¸ºæ¡ä»¶ç”Ÿæˆæ—¶é—´åºåˆ—æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢†åŸŸå†…ç”Ÿæˆè´¨é‡å’Œæœªè§é¢†åŸŸçš„ç”Ÿæˆèƒ½åŠ›ä¸Šå…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—ç”Ÿæˆæ¨¡å‹åœ¨æ•°æ®å¢å¼ºå’Œéšç§ä¿æŠ¤ç­‰åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ç°æœ‰æ—¶é—´åºåˆ—ç”Ÿæˆæ¨¡å‹é€šå¸¸å±€é™äºç‰¹å®šé¢†åŸŸçš„æ•°æ®ç”Ÿæˆã€‚</li>
<li>è·¨åŸŸç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºä¸åŒç°å®ä¸–ç•Œæ—¶é—´åºåˆ—ç±»åˆ«ä¹‹é—´å­˜åœ¨æ¨¡å¼å·®å¼‚ã€‚</li>
<li>TimeDPæ¨¡å‹é€šè¿‡å¼•å…¥æ—¶é—´åºåˆ—è¯­ä¹‰åŸå‹æ¨¡å—å’ŒåŸå‹åˆ†é…æ¨¡å—å®ç°å¤šåŸŸæ—¶é—´åºåˆ—ç”Ÿæˆã€‚</li>
<li>TimeDPæ¨¡å‹èƒ½å¤Ÿæå–ç‰¹å®šåŸŸçš„æ—¶é—´åºåˆ—åŸå‹æƒé‡ï¼Œä½œä¸ºç”Ÿæˆæ¡ä»¶è¿›è¡Œå­¦ä¹ ã€‚</li>
<li>åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼ŒTimeDPæ¨¡å‹åˆ©ç”¨æ¥è‡ªç›®æ ‡åŸŸçš„å°‘é‡æ ·æœ¬æå–â€œåŸŸæç¤ºâ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8307d6f762d9380e06594f8dcac84cf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c3b5545b9d0493ccbcfad2070232a0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-951e9816ddc5b8b26608145c915c6972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6a05595f65dda64a1de927738664819.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Harnessing-Large-Language-and-Vision-Language-Models-for-Robust-Out-of-Distribution-Detection"><a href="#Harnessing-Large-Language-and-Vision-Language-Models-for-Robust-Out-of-Distribution-Detection" class="headerlink" title="Harnessing Large Language and Vision-Language Models for Robust   Out-of-Distribution Detection"></a>Harnessing Large Language and Vision-Language Models for Robust   Out-of-Distribution Detection</h2><p><strong>Authors:Pei-Kang Lee, Jun-Cheng Chen, Ja-Ling Wu</strong></p>
<p>Out-of-distribution (OOD) detection has seen significant advancements with zero-shot approaches by leveraging the powerful Vision-Language Models (VLMs) such as CLIP. However, prior research works have predominantly focused on enhancing Far-OOD performance, while potentially compromising Near-OOD efficacy, as observed from our pilot study. To address this issue, we propose a novel strategy to enhance zero-shot OOD detection performances for both Far-OOD and Near-OOD scenarios by innovatively harnessing Large Language Models (LLMs) and VLMs. Our approach first exploit an LLM to generate superclasses of the ID labels and their corresponding background descriptions followed by feature extraction using CLIP. We then isolate the core semantic features for ID data by subtracting background features from the superclass features. The refined representation facilitates the selection of more appropriate negative labels for OOD data from a comprehensive candidate label set of WordNet, thereby enhancing the performance of zero-shot OOD detection in both scenarios. Furthermore, we introduce novel few-shot prompt tuning and visual prompt tuning to adapt the proposed framework to better align with the target distribution. Experimental results demonstrate that the proposed approach consistently outperforms current state-of-the-art methods across multiple benchmarks, with an improvement of up to 2.9% in AUROC and a reduction of up to 12.6% in FPR95. Additionally, our method exhibits superior robustness against covariate shift across different domains, further highlighting its effectiveness in real-world scenarios. </p>
<blockquote>
<p>éåˆ†å¸ƒå†…ï¼ˆOODï¼‰æ£€æµ‹åˆ©ç”¨å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚CLIPçš„é›¶æ ·æœ¬æ–¹æ³•å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„ç ”ç©¶å·¥ä½œä¸»è¦é›†ä¸­åœ¨æé«˜è¿œOODæ€§èƒ½ä¸Šï¼Œå¯èƒ½ä¼šæŸå®³è¿‘OODçš„æ•ˆæœï¼Œä»æˆ‘ä»¬çš„åˆæ­¥ç ”ç©¶ä¸­å¯ä»¥è§‚å¯Ÿåˆ°è¿™ä¸€ç‚¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œé€šè¿‡åˆ›æ–°æ€§åœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒVLMsï¼Œæé«˜é›¶æ ·æœ¬çš„è¿œOODå’Œè¿‘OODåœºæ™¯çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåˆ©ç”¨LLMç”ŸæˆIDæ ‡ç­¾çš„è¶…ç±»åŠå…¶ç›¸åº”çš„èƒŒæ™¯æè¿°ï¼Œç„¶åä½¿ç”¨CLIPè¿›è¡Œç‰¹å¾æå–ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡ä»è¶…ç±»ç‰¹å¾ä¸­å‡å»èƒŒæ™¯ç‰¹å¾æ¥åˆ†ç¦»IDæ•°æ®çš„æ ¸å¿ƒè¯­ä¹‰ç‰¹å¾ã€‚è¿™ç§ç²¾ç‚¼çš„è¡¨ç¤ºæœ‰åŠ©äºä»WordNetçš„ç»¼åˆå€™é€‰æ ‡ç­¾é›†ä¸­ä¸ºOODæ•°æ®é€‰æ‹©æ›´åˆé€‚çš„è´Ÿæ ‡ç­¾ï¼Œä»è€Œæé«˜ä¸¤ç§åœºæ™¯ä¸‹çš„é›¶æ ·æœ¬OODæ£€æµ‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°å‹çš„å°æ ·æœ¬æç¤ºè°ƒæ•´å’Œè§†è§‰æç¤ºè°ƒæ•´ï¼Œä»¥é€‚åº”æ¡†æ¶ä»¥æ›´å¥½åœ°ä¸ç›®æ ‡åˆ†å¸ƒå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨AUROCä¸Šæé«˜äº†é«˜è¾¾2.9%ï¼Œåœ¨FPR9udä¸‹å‡å°‘äº†é«˜è¾¾12.6%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åº”å¯¹ä¸åŒé¢†åŸŸçš„åå˜é‡å˜åŒ–æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ï¼Œè¿›ä¸€æ­¥çªæ˜¾å…¶åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05228v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºCLIPçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæå‡ºä¸€ç§æ”¹è¿›ç­–ç•¥ä»¥æé«˜é›¶æ ·æœ¬æ¨¡å¼ä¸‹çš„ç¦»ç¾¤å€¼æ£€æµ‹æ€§èƒ½ï¼Œé’ˆå¯¹è¿œè¿‘ä¸¤ç±»ç¦»ç¾¤æ•°æ®éƒ½è¿›è¡Œäº†æ”¹è¿›ã€‚è¯¥ç ”ç©¶ä½¿ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆæ ‡ç­¾å’ŒèƒŒæ™¯æè¿°ï¼Œæå–ç‰¹å¾åï¼Œé€šè¿‡å‡å»èƒŒæ™¯ç‰¹å¾å¾—åˆ°ç²¾ç‚¼è¡¨ç¤ºï¼Œé€‰æ‹©æ›´åˆé€‚çš„è´Ÿæ ‡ç­¾ç”¨äºç¦»ç¾¤å€¼æ•°æ®ã€‚æ­¤å¤–ï¼Œå¼•å…¥å°‘æ ·æœ¬æç¤ºè°ƒæ•´å’Œè§†è§‰æç¤ºè°ƒæ•´ä»¥é€‚åº”ç›®æ ‡åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äºå½“å‰å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨AUROCä¸Šæé«˜äº†é«˜è¾¾2.9%ï¼Œåœ¨FPR95ä¸Šé™ä½äº†é«˜è¾¾12.6%ã€‚æ­¤æ–¹æ³•å¯¹ä¸åŒåŸŸçš„åå˜é‡åç§»å…·æœ‰è‰¯å¥½çš„ç¨³å¥æ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨çœŸå®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä¸€ã€åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆçš„ç­–ç•¥æ¥æé«˜é›¶æ ·æœ¬æ¨¡å¼ä¸‹çš„ç¦»ç¾¤å€¼æ£€æµ‹æ€§èƒ½ã€‚<br>äºŒã€ç”Ÿæˆæ ‡ç­¾å’ŒèƒŒæ™¯æè¿°å¹¶ä½¿ç”¨ç‰¹å¾æå–ï¼Œé€šè¿‡ç²¾ç‚¼è¡¨ç¤ºé€‰æ‹©æ›´åˆé€‚çš„è´Ÿæ ‡ç­¾ç”¨äºç¦»ç¾¤å€¼æ•°æ®ã€‚<br>ä¸‰ã€å¼•å…¥å°‘æ ·æœ¬æç¤ºè°ƒæ•´å’Œè§†è§‰æç¤ºè°ƒæ•´ä»¥é€‚åº”ç›®æ ‡åˆ†å¸ƒï¼Œæé«˜æ£€æµ‹æ€§èƒ½ã€‚<br>å››ã€å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬AUROCå’ŒFPR95ç­‰æŒ‡æ ‡ã€‚<br>äº”ã€è¯¥æ–¹æ³•å¯¹ä¸åŒçš„åå˜é‡åç§»å…·æœ‰è‰¯å¥½çš„ç¨³å¥æ€§ï¼Œé€‚ç”¨äºçœŸå®åœºæ™¯ã€‚<br>å…­ã€ç ”ç©¶å¼ºè°ƒäº†ç»“åˆè¯­è¨€æ¨¡å‹å’Œè§†è§‰æ¨¡å‹çš„ç­–ç•¥å¯¹äºè§£å†³ç¦»ç¾¤å€¼æ£€æµ‹é—®é¢˜çš„é‡è¦æ€§ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a4800b4afac9d67c8ba9ee8760e5de1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-116f607afe04a5ae7c90c7dbab6ed4a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ef9b9939963977b8481b81c34512fe0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cde997ab75bfebf9a3f0f4500b54f62f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Text-Based-Knowledge-Embedded-Soft-Sensing-Modeling-Approach-for-General-Industrial-Process-Tasks-Based-on-Large-Language-Model"><a href="#A-Text-Based-Knowledge-Embedded-Soft-Sensing-Modeling-Approach-for-General-Industrial-Process-Tasks-Based-on-Large-Language-Model" class="headerlink" title="A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for   General Industrial Process Tasks Based on Large Language Model"></a>A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for   General Industrial Process Tasks Based on Large Language Model</h2><p><strong>Authors:Shuo Tong, Han Liu, Runyuan Guo, Xueqiong Tian, Wenqing Wang, Ding Liu, Youmin Zhang</strong></p>
<p>Data-driven soft sensors (DDSS) have become mainstream methods for predicting key performance indicators in process industries. However, DDSS development requires complex and costly customized designs tailored to various tasks during the modeling process. Moreover, DDSS are constrained to a single structured data modality, limiting their ability to incorporate additional contextual knowledge. Furthermore, DDSSsâ€™ limited representation learning leads to weak predictive performance with scarce data. To address these challenges, we propose a general framework named LLM-TKESS (large language model for text-based knowledge-embedded soft sensing), harnessing the powerful general problem-solving capabilities, cross-modal knowledge transfer abilities, and few-shot capabilities of LLM for enhanced soft sensing modeling. Specifically, an auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLMâ€™s potential for capturing temporal relationships within series and spatial semantic relationships among auxiliary variables. Then, we propose a two-stage fine-tuning alignment strategy: in the first stage, employing parameter-efficient fine-tuning through autoregressive training adjusts LLM to rapidly accommodate process variable data, resulting in a soft sensing foundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM to various downstream tasks without modifying its architecture. Then, we propose two text-based knowledge-embedded soft sensors, integrating new natural language modalities to overcome the limitations of pure structured data models. Furthermore, benefiting from LLMâ€™s pre-existing world knowledge, our model demonstrates outstanding predictive capabilities in small sample conditions. Using the thermal deformation of air preheater rotor as a case study, we validate through extensive experiments that LLM-TKESS exhibits outstanding performance. </p>
<blockquote>
<p>æ•°æ®é©±åŠ¨è½¯ä¼ æ„Ÿå™¨ï¼ˆDDSSï¼‰å·²æˆä¸ºæµç¨‹å·¥ä¸šé¢„æµ‹å…³é”®ç»©æ•ˆæŒ‡æ ‡çš„ä¸»æµæ–¹æ³•ã€‚ç„¶è€Œï¼ŒDDSSå¼€å‘éœ€è¦åœ¨å»ºæ¨¡è¿‡ç¨‹ä¸­å¯¹å¤šç§ä»»åŠ¡è¿›è¡Œå®šåˆ¶åŒ–çš„å¤æ‚ä¸”æ˜‚è´µçš„è®¾è®¡ã€‚æ­¤å¤–ï¼ŒDDSSå—é™äºå•ä¸€çš„ç»“æ„åŒ–æ•°æ®æ¨¡æ€ï¼Œæ— æ³•èå…¥é¢å¤–çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ã€‚è€Œä¸”ï¼ŒDDSSçš„æœ‰é™è¡¨ç¤ºå­¦ä¹ å¯¼è‡´åœ¨æ•°æ®ç¨€ç¼ºæ—¶çš„é¢„æµ‹æ€§èƒ½è¾ƒå¼±ã€‚</p>
</blockquote>
<p>ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºLLM-TKESSï¼ˆåŸºäºæ–‡æœ¬çŸ¥è¯†åµŒå…¥è½¯æ„ŸçŸ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„é€šç”¨æ¡†æ¶ï¼Œåˆ©ç”¨LLMçš„å¼ºå¤§é€šç”¨é—®é¢˜è§£å†³èƒ½åŠ›ã€è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»èƒ½åŠ›å’Œå°æ ·æœ¬èƒ½åŠ›ï¼Œå¢å¼ºè½¯æ„ŸçŸ¥å»ºæ¨¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¾…åŠ©å˜é‡åºåˆ—ç¼–ç å™¨ï¼ˆAVSç¼–ç å™¨ï¼‰ï¼Œä»¥é‡Šæ”¾LLMæ•æ‰åºåˆ—å†…çš„æ—¶é—´å…³ç³»å’Œè¾…åŠ©å˜é‡ä¹‹é—´çš„ç©ºé—´è¯­ä¹‰å…³ç³»çš„æ½œåŠ›ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05075v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ•°æ®é©±åŠ¨è½¯ä¼ æ„Ÿå™¨ï¼ˆDDSSï¼‰å·²æˆä¸ºæµç¨‹å·¥ä¸šä¸­é¢„æµ‹å…³é”®æ€§èƒ½æŒ‡æ ‡çš„ä¸»æµæ–¹æ³•ï¼Œä½†å…¶å¼€å‘è¿‡ç¨‹ä¸­éœ€è¦é’ˆå¯¹å„ç§ä»»åŠ¡è¿›è¡Œå¤æ‚ä¸”æˆæœ¬é«˜æ˜‚çš„å®šåˆ¶è®¾è®¡ã€‚é’ˆå¯¹DDSSåœ¨æ•°æ®ç¨€ç¼ºã€å•ä¸€æ•°æ®æ¨¡æ€å’Œæœ‰é™è¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºä¸€ç§é€šç”¨æ¡†æ¶LLM-TKESSï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹æ–‡æœ¬çŸ¥è¯†åµŒå…¥è½¯ä¼ æ„Ÿï¼‰ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é€šç”¨é—®é¢˜è§£å†³èƒ½åŠ›ã€è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»èƒ½åŠ›å’Œå°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œå¢å¼ºè½¯ä¼ æ„Ÿå»ºæ¨¡ã€‚é€šè¿‡è¾…åŠ©å˜é‡åºåˆ—ç¼–ç å™¨å’Œä¸¤é˜¶æ®µç²¾ç»†è°ƒæ•´å¯¹é½ç­–ç•¥ï¼Œç»“åˆè‡ªç„¶è¯­è¨€æ¨¡æ€æ•°æ®ï¼ŒLLM-TKESSå±•ç°å‡ºå“è¶Šçš„é¢„æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é©±åŠ¨è½¯ä¼ æ„Ÿå™¨ï¼ˆDDSSï¼‰æ˜¯æµç¨‹å·¥ä¸šä¸­é¢„æµ‹å…³é”®æ€§èƒ½æŒ‡æ ‡çš„ä¸»æµæ–¹æ³•ï¼Œä½†å­˜åœ¨å¼€å‘å¤æ‚ã€æˆæœ¬é«˜å’Œå±€é™æ€§ã€‚</li>
<li>LLM-TKESSæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é€šç”¨é—®é¢˜è§£å†³èƒ½åŠ›å’Œè·¨æ¨¡æ€çŸ¥è¯†è¿ç§»èƒ½åŠ›ï¼Œä»¥æ”¹è¿›DDSSçš„å¼±ç‚¹ã€‚</li>
<li>LLM-TKESSé€šè¿‡è¾…åŠ©å˜é‡åºåˆ—ç¼–ç å™¨ï¼ˆAVS Encoderï¼‰æ•æ‰æ—¶é—´åºåˆ—å’Œç©ºé—´è¯­ä¹‰å…³ç³»ã€‚</li>
<li>ä¸¤é˜¶æ®µç²¾ç»†è°ƒæ•´å¯¹é½ç­–ç•¥ï¼Œä½¿LLMé€‚åº”è¿‡ç¨‹å˜é‡æ•°æ®ï¼Œå¹¶å¿«é€Ÿå»ºç«‹è½¯ä¼ æ„ŸåŸºç¡€æ¨¡å‹ï¼ˆSSFMï¼‰ã€‚</li>
<li>LLM-TKESSé€šè¿‡è®­ç»ƒé€‚é…å™¨é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹ç»“æ„ä¸å˜ã€‚</li>
<li>ç»“åˆè‡ªç„¶è¯­è¨€æ¨¡æ€æ•°æ®ï¼Œå…‹æœçº¯ç»“æ„åŒ–æ•°æ®æ¨¡å‹çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05075">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-57037798706e02268b462fde155a5485.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f767c1c5b887d2bfebde927b2ec3447f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1ea3ca29100b6b7b9da59d909968570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c027b263bed802051c8eb4559956b2e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b2b915e3ad730af09aa766bbc92f602.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf158bcdf2197d04389ab112e6539fdb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Continuous-Knowledge-Preserving-Decomposition-for-Few-Shot-Continual-Learning"><a href="#Continuous-Knowledge-Preserving-Decomposition-for-Few-Shot-Continual-Learning" class="headerlink" title="Continuous Knowledge-Preserving Decomposition for Few-Shot Continual   Learning"></a>Continuous Knowledge-Preserving Decomposition for Few-Shot Continual   Learning</h2><p><strong>Authors:Xiaojie Li, Yibo Yang, Jianlong Wu, David A. Clifton, Yue Yu, Bernard Ghanem, Min Zhang</strong></p>
<p>Few-shot class-incremental learning (FSCIL) involves learning new classes from limited data while retaining prior knowledge, and often results in catastrophic forgetting. Existing methods either freeze backbone networks to preserve knowledge, which limits adaptability, or rely on additional modules or prompts, introducing inference overhead. To this end, we propose Continuous Knowledge-Preserving Decomposition for FSCIL (CKPD-FSCIL), a framework that decomposes a modelâ€™s weights into two parts: one that compacts existing knowledge (knowledge-sensitive components) and another that carries redundant capacity to accommodate new abilities (redundant-capacity components). The decomposition is guided by a covariance matrix from replay samples, ensuring principal components align with classification abilities. During adaptation, we freeze the knowledge-sensitive components and only adapt the redundant-capacity components, fostering plasticity while minimizing interference without changing the architecture or increasing overhead. Additionally, CKPD introduces an adaptive layer selection strategy to identify layers with redundant capacity, dynamically allocating adapters. Experiments on multiple benchmarks show that CKPD-FSCIL outperforms state-of-the-art methods. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ¶‰åŠä»æœ‰é™æ•°æ®ä¸­å­¦ä¹ æ–°ç±»åˆ«çš„åŒæ—¶ä¿ç•™å…ˆå‰çŸ¥è¯†ï¼Œè¿™å¸¸å¸¸å¯¼è‡´ç¾éš¾æ€§é—å¿˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆå†»ç»“ä¸»å¹²ç½‘ç»œä»¥ä¿ç•™çŸ¥è¯†ï¼Œä»è€Œé™åˆ¶äº†é€‚åº”æ€§ï¼Œè¦ä¹ˆä¾èµ–äºé™„åŠ æ¨¡å—æˆ–æç¤ºï¼Œå¢åŠ äº†æ¨ç†å¼€é”€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘FSCILçš„æŒç»­çŸ¥è¯†ä¿ç•™åˆ†è§£ï¼ˆCKPD-FSCILï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ¨¡å‹çš„æƒé‡åˆ†è§£ä¸ºä¸¤éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†æ˜¯å‹ç¼©ç°æœ‰çŸ¥è¯†ï¼ˆçŸ¥è¯†æ•æ„Ÿç»„ä»¶ï¼‰ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯å…·æœ‰å®¹çº³æ–°èƒ½åŠ›å‰©ä½™å®¹é‡ï¼ˆå‰©ä½™å®¹é‡ç»„ä»¶ï¼‰ã€‚åˆ†è§£ç”±å›æ”¾æ ·æœ¬çš„åæ–¹å·®çŸ©é˜µå¼•å¯¼ï¼Œç¡®ä¿ä¸»æˆåˆ†ä¸åˆ†ç±»èƒ½åŠ›å¯¹é½ã€‚åœ¨é€‚åº”è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å†»ç»“çŸ¥è¯†æ•æ„Ÿç»„ä»¶ï¼Œåªé€‚åº”å‰©ä½™å®¹é‡ç»„ä»¶ï¼Œä¿ƒè¿›å¯å¡‘æ€§ï¼ŒåŒæ—¶æœ€å°åŒ–å¹²æ‰°ï¼Œè€Œä¸æ”¹å˜æ¶æ„æˆ–å¢åŠ å¼€é”€ã€‚æ­¤å¤–ï¼ŒCKPDå¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”å±‚é€‰æ‹©ç­–ç•¥ï¼Œä»¥è¯†åˆ«å…·æœ‰å‰©ä½™å®¹é‡çš„å±‚ï¼Œå¹¶åŠ¨æ€åˆ†é…é€‚é…å™¨ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCKPD-FSCILä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05017v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/xiaojieli0903/CKPD-FSCIL">https://github.com/xiaojieli0903/CKPD-FSCIL</a></p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹æƒé‡åˆ†è§£æ–¹æ³•CKPD-FSCILè§£å†³å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚é€šè¿‡åˆ†è§£æ¨¡å‹æƒé‡ä¸ºçŸ¥è¯†æ•æ„Ÿç»„ä»¶å’Œå†—ä½™å®¹é‡ç»„ä»¶ï¼Œç¡®ä¿åœ¨é€‚åº”æ–°ç±»åˆ«æ—¶ä¿ç•™ç°æœ‰çŸ¥è¯†ã€‚é‡‡ç”¨ç”±å›æ”¾æ ·æœ¬çš„åæ–¹å·®çŸ©é˜µå¼•å¯¼çš„åˆ†è§£æ–¹æ³•ï¼Œç¡®ä¿ä¸»æˆåˆ†ä¸åˆ†ç±»èƒ½åŠ›å¯¹é½ã€‚å†»ç»“çŸ¥è¯†æ•æ„Ÿç»„ä»¶ï¼Œä»…é€‚åº”å†—ä½™å®¹é‡ç»„ä»¶ï¼Œä¿ƒè¿›å¯å¡‘æ€§å¹¶æœ€å°åŒ–å¹²æ‰°ï¼Œä¸”æ— éœ€æ”¹å˜æ¶æ„æˆ–å¢åŠ é¢å¤–å¼€é”€ã€‚CKPDè¿˜å¼•å…¥äº†è‡ªé€‚åº”å±‚é€‰æ‹©ç­–ç•¥æ¥è¯†åˆ«å…·æœ‰å†—ä½™å®¹é‡çš„å±‚ï¼Œå¹¶åŠ¨æ€åˆ†é…é€‚é…å™¨ã€‚CKPD-FSCILåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CKPD-FSCILè§£å†³äº†å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>é€šè¿‡å°†æ¨¡å‹æƒé‡åˆ†è§£ä¸ºçŸ¥è¯†æ•æ„Ÿç»„ä»¶å’Œå†—ä½™å®¹é‡ç»„ä»¶ï¼ŒCKPD-FSCILèƒ½å¤Ÿåœ¨é€‚åº”æ–°ç±»åˆ«æ—¶ä¿ç•™ç°æœ‰çŸ¥è¯†ã€‚</li>
<li>åˆ†è§£æ–¹æ³•ç”±å›æ”¾æ ·æœ¬çš„åæ–¹å·®çŸ©é˜µå¼•å¯¼ï¼Œç¡®ä¿ä¸»æˆåˆ†ä¸åˆ†ç±»èƒ½åŠ›å¯¹é½ã€‚</li>
<li>åœ¨é€‚åº”è¿‡ç¨‹ä¸­ï¼ŒCKPD-FSCILé€šè¿‡å†»ç»“çŸ¥è¯†æ•æ„Ÿç»„ä»¶å¹¶ä»…é€‚åº”å†—ä½™å®¹é‡ç»„ä»¶ï¼Œä»¥ä¿ƒè¿›å¯å¡‘æ€§å¹¶æœ€å°åŒ–å¹²æ‰°ã€‚</li>
<li>CKPD-FSCILæ–¹æ³•ä¸éœ€è¦æ”¹å˜æ¨¡å‹æ¶æ„æˆ–å¢åŠ é¢å¤–å¼€é”€ã€‚</li>
<li>CKPDå¼•å…¥äº†è‡ªé€‚åº”å±‚é€‰æ‹©ç­–ç•¥ï¼Œèƒ½å¤Ÿè¯†åˆ«å…·æœ‰å†—ä½™å®¹é‡çš„å±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce201257b4dd560cfee13f6a8c4de7bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cd9df9ec2e824fd71f794f1a474af2e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="More-is-not-always-better-Enhancing-Many-Shot-In-Context-Learning-with-Differentiated-and-Reweighting-Objectives"><a href="#More-is-not-always-better-Enhancing-Many-Shot-In-Context-Learning-with-Differentiated-and-Reweighting-Objectives" class="headerlink" title="More is not always better? Enhancing Many-Shot In-Context Learning with   Differentiated and Reweighting Objectives"></a>More is not always better? Enhancing Many-Shot In-Context Learning with   Differentiated and Reweighting Objectives</h2><p><strong>Authors:Xiaoqing Zhang, Ang Lv, Yuhan Liu, Flood Sung, Wei Liu, Shuo Shang, Xiuying Chen, Rui Yan</strong></p>
<p>Large language models (LLMs) excel at few-shot in-context learning (ICL) without requiring parameter updates. However, as the number of ICL demonstrations increases from a few to many, performance tends to plateau and eventually decline. We identify two primary causes for this trend: the suboptimal negative log-likelihood (NLL) optimization objective and the incremental data noise. To address these issues, we introduce DrICL, a novel optimization method that enhances model performance through Differentiated Learning and advantage-based Reweighting objectives. Globally, DrICL utilizes differentiated learning to optimize the NLL objective, ensuring that many-shot performance surpasses zero-shot levels. Locally, it dynamically adjusts the weighting of many-shot demonstrations by leveraging cumulative advantages inspired by reinforcement learning, thereby improving generalization. This approach allows the model to handle varying numbers of shots effectively, mitigating the impact of noisy data. Recognizing the lack of multi-task datasets with diverse many-shot distributions, we develop the Many-Shot ICL Benchmark (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers from 1 to 350 within sequences of up to 8,000 tokens-for fine-tuning purposes. ICL-50 facilitates the evaluation of many-shot ICL strategies across seven prominent NLP tasks and 50 distinct datasets. Experimental results demonstrate that LLMs enhanced with DrICL achieve significant improvements in many-shot setups across various tasks, including both in-domain and out-of-domain scenarios. We release the code and benchmark dataset hoping to facilitate further research in many-shot ICL. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸éœ€æ›´æ–°å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ“…é•¿äºå°‘é‡æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚ç„¶è€Œï¼Œéšç€ICLæ¼”ç¤ºæ ·æœ¬çš„æ•°é‡ä»å°‘æ•°å¢åŠ åˆ°å¤šæ•°ï¼Œæ€§èƒ½å¾€å¾€è¾¾åˆ°å¹³å°æœŸå¹¶æœ€ç»ˆä¸‹é™ã€‚æˆ‘ä»¬ç¡®å®šäº†å¯¼è‡´è¿™ä¸€è¶‹åŠ¿çš„ä¸¤ä¸ªä¸»è¦åŸå› ï¼šæ¬¡ä¼˜çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰ä¼˜åŒ–ç›®æ ‡å’Œå¢é‡æ•°æ®å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DrICLï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡å·®å¼‚åŒ–å­¦ä¹ å’ŒåŸºäºä¼˜åŠ¿çš„é‡åŠ æƒç›®æ ‡æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚å…¨å±€ä¸Šï¼ŒDrICLé€šè¿‡å·®å¼‚åŒ–å­¦ä¹ æ¥ä¼˜åŒ–NLLç›®æ ‡ï¼Œç¡®ä¿å¤šæ¬¡å°„å‡»çš„æ€§èƒ½è¶…è¿‡é›¶æ¬¡å°„å‡»æ°´å¹³ã€‚å±€éƒ¨ä¸Šï¼Œå®ƒå—å¼ºåŒ–å­¦ä¹ çš„å¯å‘ï¼Œåˆ©ç”¨ç´¯ç§¯ä¼˜åŠ¿åŠ¨æ€è°ƒæ•´å¤šæ¬¡å°„å‡»æ¼”ç¤ºçš„æƒé‡ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†ä¸åŒæ•°é‡çš„æ ·æœ¬ï¼Œå‡è½»å™ªå£°æ•°æ®çš„å½±å“ã€‚è®¤è¯†åˆ°ç¼ºä¹å…·æœ‰å¤šç§å¤šæ ·å¤šæ¬¡å°„å‡»åˆ†å¸ƒçš„å¤šä»»åŠ¡æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†å¤šæ¬¡å°„å‡»ICLåŸºå‡†æµ‹è¯•ï¼ˆICL-50ï¼‰â€”â€”ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«50ä¸ªä»»åŠ¡ï¼Œæ¶µç›–ä»1åˆ°350çš„å°„å‡»æ¬¡æ•°ï¼Œåºåˆ—ä¸­çš„ä»¤ç‰Œé«˜è¾¾8000ä¸ªï¼Œç”¨äºå¾®è°ƒç›®çš„ã€‚ICL-50æœ‰åŠ©äºè¯„ä¼°è·¨ä¸ƒä¸ªçªå‡ºNLPä»»åŠ¡å’Œ50ä¸ªä¸åŒæ•°æ®é›†çš„å¤šæ¬¡å°„å‡»ICLç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨DrICLå¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡çš„å¤šé•œå¤´è®¾ç½®ä¸­å®ç°äº†æ˜¾ç€æ”¹è¿›ï¼ŒåŒ…æ‹¬åŸŸå†…å’ŒåŸŸå¤–åœºæ™¯ã€‚æˆ‘ä»¬å‘å¸ƒä»£ç å’ŒåŸºå‡†æ•°æ®é›†ï¼Œå¸Œæœ›ä¿ƒè¿›å¤šé•œå¤´ICLçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04070v2">PDF</a> 13 pages, 8 figures, 11 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ— éœ€å‚æ•°æ›´æ–°çš„å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†éšç€ä¸Šä¸‹æ–‡å­¦ä¹ ç¤ºä¾‹ä»å°‘æ•°å¢åŠ åˆ°å¤šæ•°ï¼Œæ€§èƒ½å¾€å¾€è¾¾åˆ°å¹³å°æœŸå¹¶æœ€ç»ˆä¸‹é™ã€‚ç ”ç©¶æŒ‡å‡ºè¿™ä¸€ç°è±¡ä¸»è¦ç”±æ¬¡ä¼˜çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ä¼˜åŒ–ç›®æ ‡å’Œé€’å¢çš„æ•°æ®å™ªå£°å¼•èµ·ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºDrICLè¿™ä¸€æ–°å‹ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡å·®å¼‚åŒ–å­¦ä¹ å’ŒåŸºäºä¼˜åŠ¿çš„é‡åŠ æƒç›®æ ‡æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚DrICLå…¨å±€ä¼˜åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ç›®æ ‡ï¼Œç¡®ä¿å¤šç¤ºä¾‹æ€§èƒ½è¶…è¶Šé›¶ç¤ºä¾‹æ°´å¹³ï¼›å±€éƒ¨åˆ™é€šè¿‡å€Ÿé‰´å¼ºåŒ–å­¦ä¹ çš„ç´¯ç§¯ä¼˜åŠ¿åŠ¨æ€è°ƒæ•´å¤šç¤ºä¾‹æ¼”ç¤ºçš„æƒé‡ï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤æ–¹æ³•ä½¿æ¨¡å‹æœ‰æ•ˆåº”å¯¹ä¸åŒæ ·æœ¬é‡ï¼Œç¼“è§£æ•°æ®å™ªå£°å½±å“ã€‚ç ”ç©¶è®¤è¯†åˆ°ç¼ºä¹å¤šæ ·å¤šç¤ºä¾‹åˆ†å¸ƒçš„å¤šä»»åŠ¡æ•°æ®é›†ï¼Œäºæ˜¯å¼€å‘å‡ºå¤§å‹åŸºå‡†æµ‹è¯•ICL-50ï¼ŒåŒ…å«50ä¸ªä»»åŠ¡ã€æ¶µç›–1è‡³350ä¸ªç¤ºä¾‹ã€åºåˆ—é•¿è¾¾8000ä¸ªç¬¦å·ï¼Œç”¨äºå¾®è°ƒç›®çš„ã€‚ICL-50èƒ½åœ¨ä¸ƒä¸ªæ˜¾è‘—çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡å’Œ50ä¸ªä¸åŒæ•°æ®é›†ä¸Šè¯„ä¼°å¤šç¤ºä¾‹ICLç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨DrICLå¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡çš„å¤šç¤ºä¾‹è®¾ç½®ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒ…æ‹¬åŸŸå†…å’ŒåŸŸå¤–åœºæ™¯ã€‚ç ”ç©¶å…¬å¸ƒäº†ä»£ç å’ŒåŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼Œå¸Œæœ›èƒ½è¿›ä¸€æ­¥æ¨åŠ¨å¤šç¤ºä¾‹ICLçš„ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†éšç€ç¤ºä¾‹æ•°é‡å¢åŠ ï¼Œæ€§èƒ½å¯èƒ½ä¸‹é™ã€‚</li>
<li>æ€§èƒ½ä¸‹é™çš„ä¸»è¦åŸå› åŒ…æ‹¬æ¬¡ä¼˜çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ä¼˜åŒ–ç›®æ ‡å’Œæ•°æ®å™ªå£°çš„é€’å¢ã€‚</li>
<li>å¼•å…¥DrICLæ–¹æ³•ï¼Œé€šè¿‡å·®å¼‚åŒ–å­¦ä¹ å’ŒåŸºäºä¼˜åŠ¿çš„é‡åŠ æƒæ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>DrICLèƒ½åœ¨å¤šç¤ºä¾‹åœºæ™¯ä¸‹æå‡æ¨¡å‹æ€§èƒ½ï¼Œå¹¶è¶…è¶Šé›¶ç¤ºä¾‹æ°´å¹³ã€‚</li>
<li>å¼€å‘å‡ºå¤§å‹åŸºå‡†æµ‹è¯•ICL-50ï¼Œç”¨äºè¯„ä¼°å¤šç¤ºä¾‹ICLç­–ç•¥åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨DrICLçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç¤ºä¾‹è®¾ç½®ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒ…æ‹¬åœ¨åŸŸå†…å’ŒåŸŸå¤–åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e86bd6a0b46ff64e68bf20f4e592a04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c272ef986b8b10591e903d057584ff63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da544742c78d322f4d2c8f78ca2d918d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afc85a7ba35b104cfcc5aeb268448e69.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Literature-Meets-Data-A-Synergistic-Approach-to-Hypothesis-Generation"><a href="#Literature-Meets-Data-A-Synergistic-Approach-to-Hypothesis-Generation" class="headerlink" title="Literature Meets Data: A Synergistic Approach to Hypothesis Generation"></a>Literature Meets Data: A Synergistic Approach to Hypothesis Generation</h2><p><strong>Authors:Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan</strong></p>
<p>AI holds promise for transforming scientific processes, including hypothesis generation. Prior work on hypothesis generation can be broadly categorized into theory-driven and data-driven approaches. While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other. To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation. We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97% over few-shot, 15.75% over literature-based alone, and 3.37% over data-driven alone). Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection. Our results show that human accuracy improves significantly by 7.44% and 14.19% on these tasks, respectively. These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æœ‰æœ›æ”¹å˜ç§‘å­¦è¿‡ç¨‹ï¼ŒåŒ…æ‹¬å‡è®¾ç”Ÿæˆã€‚å…³äºå‡è®¾ç”Ÿæˆçš„å‰æœŸå·¥ä½œå¯ä»¥å¤§è‡´åˆ†ä¸ºç†è®ºé©±åŠ¨å’Œæ•°æ®é©±åŠ¨çš„æ–¹æ³•ã€‚è™½ç„¶è¿™ä¸¤ç§æ–¹æ³•åœ¨ç”Ÿæˆæ–°é¢–ä¸”åˆç†çš„å‡è®¾æ–¹é¢éƒ½è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å®ƒä»¬æ˜¯å¦èƒ½ç›¸äº’è¡¥å……ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç»“åˆæ–‡çŒ®è§è§£å’Œæ•°æ®çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå‡è®¾ç”Ÿæˆã€‚æˆ‘ä»¬åœ¨äº”ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šåº”ç”¨äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ï¼Œå¹¶è¯æ˜æ•´åˆæ–‡çŒ®å’Œæ•°æ®çš„æ–¹æ³•ä¼˜äºå…¶ä»–åŸºå‡†æµ‹è¯•ï¼ˆåœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹é«˜å‡º8.97%ï¼Œåœ¨ä»…åŸºäºæ–‡çŒ®çš„æƒ…å†µä¸‹é«˜å‡º15.75%ï¼Œåœ¨ä»…æ•°æ®é©±åŠ¨çš„æƒ…å†µä¸‹é«˜å‡º3.37%ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†é¦–æ¬¡äººç±»è¯„ä¼°ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å‡è®¾åœ¨ååŠ©äººç±»è¿›è¡Œä¸¤é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡æ—¶çš„æ•ˆç”¨ï¼šæ¬ºéª—æ£€æµ‹å’ŒAIç”Ÿæˆå†…å®¹æ£€æµ‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸Šï¼Œäººç±»å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†7.44%å’Œ14.19%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ•´åˆåŸºäºæ–‡çŒ®å’ŒåŸºäºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ä¸ºå‡è®¾ç”Ÿæˆæä¾›äº†ä¸€ä¸ªå…¨é¢è€Œå¾®å¦™çš„æ¡†æ¶ï¼Œå¹¶å¯èƒ½ä¸ºç§‘å­¦æ¢ç´¢å¼€è¾Ÿæ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17309v3">PDF</a> 37 pages, 9 figures, code link:   <a target="_blank" rel="noopener" href="https://github.com/ChicagoHAI/hypothesis-generation">https://github.com/ChicagoHAI/hypothesis-generation</a></p>
<p><strong>Summary</strong><br>åŸºäºæ–‡æœ¬çš„ä¿¡æ¯ï¼Œäººå·¥æ™ºèƒ½åœ¨èåˆæ–‡çŒ®å’Œå¤§æ•°æ®çš„åŸºç¡€ä¸Šï¼Œå¯ä»¥é€šè¿‡LLMé©±åŠ¨çš„æ–¹æ³•ç”Ÿæˆå‡è®¾ï¼Œæ—¢æå‡äº†å‡è®¾ç”Ÿæˆçš„æ•ˆæœï¼Œåˆæœ‰åŠ©äºæ”¹å–„äººç±»åœ¨æ¬ºéª—æ£€æµ‹å’ŒAIç”Ÿæˆå†…å®¹æ£€æµ‹ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•çš„å‡ºç°ä¸ºç§‘å­¦è¿‡ç¨‹å¸¦æ¥äº†å˜é©çš„æ‰¿è¯ºï¼Œç‰¹åˆ«æ˜¯åœ¨å‡è®¾ç”Ÿæˆæ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIå…·æœ‰æ”¹å˜ç§‘å­¦è¿‡ç¨‹çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡è®¾ç”Ÿæˆæ–¹é¢ã€‚</li>
<li>å‡è®¾ç”Ÿæˆæ–¹æ³•å¯åˆ†ä¸ºç†è®ºé©±åŠ¨å’Œæ•°æ®é©±åŠ¨ä¸¤å¤§ç±»ï¼Œä½†ä¸¤è€…äº’è¡¥æ€§å°šå¾…ç ”ç©¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆæ–‡çŒ®å’Œæ•°æ®çš„LLMé©±åŠ¨å‡è®¾ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨äº”ç»„ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡è¶…è¿‡å…¶ä»–åŸºå‡†æµ‹è¯•ã€‚</li>
<li>äººç±»åœ¨æ¬ºéª—æ£€æµ‹å’ŒAIç”Ÿæˆå†…å®¹æ£€æµ‹ä»»åŠ¡ä¸Šï¼Œå€ŸåŠ©LLMç”Ÿæˆçš„å‡è®¾ï¼Œå‡†ç¡®æ€§å¾—åˆ°æ˜¾è‘—æé«˜ã€‚</li>
<li>ç»¼åˆæ–‡çŒ®å’Œå¤§æ•°æ®çš„æ–¹æ³•ä¸ºå‡è®¾ç”Ÿæˆæä¾›äº†å…¨é¢è€Œç»†è‡´çš„æ¡†æ¶ã€‚</li>
<li>è¯¥æ–¹æ³•å¯èƒ½å¼€å¯æ–°çš„ç§‘å­¦æ¢ç´¢é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17309">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a2dfaab60ceefd4438b8c0fa72b42a63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32fa866a82f68872fe86f7fabcd997d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ed6ac10d21beb49a95aa29b546815e0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SEA-SQL-Semantic-Enhanced-Text-to-SQL-with-Adaptive-Refinement"><a href="#SEA-SQL-Semantic-Enhanced-Text-to-SQL-with-Adaptive-Refinement" class="headerlink" title="SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement"></a>SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement</h2><p><strong>Authors:Chaofan Li, Yingxia Shao, Yawen Li, Zheng Liu</strong></p>
<p>Recent advancements in large language models (LLMs) have significantly contributed to the progress of the Text-to-SQL task. A common requirement in many of these works is the post-correction of SQL queries. However, the majority of this process entails analyzing error cases to develop prompts with rules that eliminate model bias. And there is an absence of execution verification for SQL queries. In addition, the prevalent techniques primarily depend on GPT-4 and few-shot prompts, resulting in expensive costs. To investigate the effective methods for SQL refinement in a cost-efficient manner, we introduce Semantic-Enhanced Text-to-SQL with Adaptive Refinement (SEA-SQL), which includes Adaptive Bias Elimination and Dynamic Execution Adjustment, aims to improve performance while minimizing resource expenditure with zero-shot prompts. Specifically, SEA-SQL employs a semantic-enhanced schema to augment database information and optimize SQL queries. During the SQL query generation, a fine-tuned adaptive bias eliminator is applied to mitigate inherent biases caused by the LLM. The dynamic execution adjustment is utilized to guarantee the executability of the bias eliminated SQL query. We conduct experiments on the Spider and BIRD datasets to demonstrate the effectiveness of this framework. The results demonstrate that SEA-SQL achieves state-of-the-art performance in the GPT3.5 scenario with 9%-58% of the generation cost. Furthermore, SEA-SQL is comparable to GPT-4 with only 0.9%-5.3% of the generation cost. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å¯¹Text-to-SQLä»»åŠ¡äº§ç”Ÿäº†é‡å¤§æ¨åŠ¨ã€‚è®¸å¤šç ”ç©¶ä¸­çš„ä¸€é¡¹å¸¸è§è¦æ±‚æ˜¯SQLæŸ¥è¯¢çš„åä¿®æ­£ã€‚ç„¶è€Œï¼Œè¿™ä¸ªè¿‡ç¨‹çš„å¤§éƒ¨åˆ†æ¶‰åŠåˆ†æé”™è¯¯æƒ…å†µï¼Œä»¥åˆ¶å®šè§„åˆ™æç¤ºæ¥æ¶ˆé™¤æ¨¡å‹åè§ã€‚æ­¤å¤–ï¼Œç¼ºä¹å¯¹SQLæŸ¥è¯¢çš„æ‰§è¡ŒéªŒè¯ã€‚æ­¤å¤–ï¼Œä¸»æµæŠ€æœ¯ä¸»è¦ä¾èµ–äºGPT-4å’Œå°‘é‡æç¤ºï¼Œå¯¼è‡´æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†ä»¥æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹å¼ç ”ç©¶SQLç²¾ç‚¼çš„æœ‰æ•ˆæ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¸¦æœ‰è‡ªé€‚åº”ç²¾ç‚¼çš„è¯­ä¹‰å¢å¼ºæ–‡æœ¬åˆ°SQLï¼ˆSEA-SQLï¼‰ï¼ŒåŒ…æ‹¬è‡ªé€‚åº”åå·®æ¶ˆé™¤å’ŒåŠ¨æ€æ‰§è¡Œè°ƒæ•´ï¼Œæ—¨åœ¨æé«˜æ€§èƒ½ï¼ŒåŒæ—¶ä»¥é›¶æç¤ºçš„æ–¹å¼æœ€å°åŒ–èµ„æºæ”¯å‡ºã€‚å…·ä½“æ¥è¯´ï¼ŒSEA-SQLé‡‡ç”¨è¯­ä¹‰å¢å¼ºæ¶æ„æ¥å¢å¼ºæ•°æ®åº“ä¿¡æ¯å¹¶ä¼˜åŒ–SQLæŸ¥è¯¢ã€‚åœ¨ç”ŸæˆSQLæŸ¥è¯¢æ—¶ï¼Œåº”ç”¨äº†å¾®è°ƒçš„è‡ªé€‚åº”åå·®æ¶ˆé™¤å™¨ï¼Œä»¥å‡è½»LLMå¼•èµ·çš„å›ºæœ‰åå·®ã€‚åŠ¨æ€æ‰§è¡Œè°ƒæ•´ç”¨äºä¿è¯åå·®æ¶ˆé™¤åçš„SQLæŸ¥è¯¢çš„å¯æ‰§è¡Œæ€§ã€‚æˆ‘ä»¬åœ¨Spiderå’ŒBIRDæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œä»¥è¯æ˜è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨GPT3.5åœºæ™¯ä¸­ï¼ŒSEA-SQLè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”Ÿæˆæˆæœ¬é™ä½äº†9%~58%ã€‚æ­¤å¤–ï¼ŒSEA-SQLä¸GPT-4ç›¸æ¯”ï¼Œç”Ÿæˆæˆæœ¬ä»…å¢åŠ äº†0.9%~5.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04919v2">PDF</a> The article has been accepted by Frontiers of Computer Science (FCS),   with the DOI: {10.1007&#x2F;s11704-025-41136-3}</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ°SQLä»»åŠ¡ä¸­çš„æœ€æ–°è¿›å±•å·²ç»å–å¾—äº†æ˜¾è‘—è´¡çŒ®ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•éƒ½éœ€è¦å¯¹SQLæŸ¥è¯¢è¿›è¡Œåä¿®æ­£ï¼Œè¿™ä¸»è¦æ¶‰åŠåˆ°åˆ†æé”™è¯¯æƒ…å†µå¹¶åˆ¶å®šè§„åˆ™æç¤ºä»¥æ¶ˆé™¤æ¨¡å‹åè§ã€‚æ­¤å¤–ï¼Œç¼ºä¹SQLæŸ¥è¯¢çš„æ‰§è¡ŒéªŒè¯ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„SQLç²¾ç‚¼æœ‰æ•ˆæ–¹æ³•â€”â€”è¯­ä¹‰å¢å¼ºæ–‡æœ¬åˆ°SQLè‡ªé€‚åº”ç²¾ç‚¼ï¼ˆSEA-SQLï¼‰ï¼Œæ—¨åœ¨æé«˜æ€§èƒ½çš„åŒæ—¶æœ€å°åŒ–èµ„æºæ¶ˆè€—ï¼Œé‡‡ç”¨é›¶é•œå¤´æç¤ºã€‚SEA-SQLé‡‡ç”¨è¯­ä¹‰å¢å¼ºæ¨¡å¼æ¥å¢å¼ºæ•°æ®åº“ä¿¡æ¯å¹¶ä¼˜åŒ–SQLæŸ¥è¯¢ã€‚åœ¨SQLæŸ¥è¯¢ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œåº”ç”¨äº†ç²¾ç»†è°ƒæ•´çš„è‡ªé€‚åº”åè§æ¶ˆé™¤å™¨æ¥å‡è½»LLMå¼•èµ·çš„å›ºæœ‰åè§ã€‚åŠ¨æ€æ‰§è¡Œè°ƒæ•´å¯ç¡®ä¿æ¶ˆé™¤åè§çš„SQLæŸ¥è¯¢çš„å¯æ‰§è¡Œæ€§ã€‚åœ¨Spiderå’ŒBIRDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSEA-SQLåœ¨GPT3.5åœºæ™¯ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”Ÿæˆæˆæœ¬é™ä½äº†9%-58%ã€‚æ­¤å¤–ï¼ŒSEA-SQLä¸GPT-4ç›¸æ¯”ï¼Œç”Ÿæˆæˆæœ¬ä»…å¢åŠ äº†0.9%-5.3%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æè¿°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ°SQLä»»åŠ¡ä¸­çš„åº”ç”¨è¿›å±•åŠæŒ‘æˆ˜ã€‚</li>
<li>ä»‹ç»äº†SEA-SQLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨é€šè¿‡è‡ªé€‚åº”ç²¾ç‚¼æŠ€æœ¯æé«˜SQLæŸ¥è¯¢çš„æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šã€‚</li>
<li>SEA-SQLé‡‡ç”¨è¯­ä¹‰å¢å¼ºæ¨¡å¼æ¥ä¼˜åŒ–æ•°æ®åº“ä¿¡æ¯å’ŒSQLæŸ¥è¯¢ã€‚</li>
<li>è‡ªé€‚åº”åè§æ¶ˆé™¤å™¨çš„åº”ç”¨æ˜¯SEA-SQLçš„ä¸€ä¸ªé‡è¦ç‰¹ç‚¹ï¼Œèƒ½å¤Ÿå‡è½»LLMçš„å›ºæœ‰åè§ã€‚</li>
<li>åŠ¨æ€æ‰§è¡Œè°ƒæ•´ç¡®ä¿æ¶ˆé™¤åè§çš„SQLæŸ¥è¯¢çš„å¯æ‰§è¡Œæ€§ã€‚</li>
<li>åœ¨Spiderå’ŒBIRDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSEA-SQLåœ¨GPT3.5åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸GPT-4ç›¸æ¯”å…·æœ‰è¾ƒä½çš„æˆæœ¬å¢åŠ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.04919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-049869cb50af810b69d417155d2d0b91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6a327f153f8ca93e324fa3c96eb5668.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88a1f73bb57d7f1111e4322c637bb935.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8872f24f7bb1e61c73bc0dda0d7186a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4e9fe1824e5de6863543e2644ceb699.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-13/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-13/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-57c5227adaa957949cfe1103c5f28961.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-13  HipyrNet Hypernet-Guided Feature Pyramid network for mixed-exposure   correction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c55bb0419401785f71826165ed212ef5.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-13  A Mixed-Integer Conic Program for the Multi-Agent Moving-Target   Traveling Salesman Problem
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32271.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
