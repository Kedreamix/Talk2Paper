<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-01-13  Low-Resource Text-to-Speech Synthesis Using Noise-Augmented Training of   ForwardTacotron">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-697ad8f1427eec64e2ef2669de2148fc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    45 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-13-更新"><a href="#2025-01-13-更新" class="headerlink" title="2025-01-13 更新"></a>2025-01-13 更新</h1><h2 id="Low-Resource-Text-to-Speech-Synthesis-Using-Noise-Augmented-Training-of-ForwardTacotron"><a href="#Low-Resource-Text-to-Speech-Synthesis-Using-Noise-Augmented-Training-of-ForwardTacotron" class="headerlink" title="Low-Resource Text-to-Speech Synthesis Using Noise-Augmented Training of   ForwardTacotron"></a>Low-Resource Text-to-Speech Synthesis Using Noise-Augmented Training of   ForwardTacotron</h2><p><strong>Authors:Kishor Kayyar Lakshminarayana, Frank Zalkow, Christian Dittmar, Nicola Pia, Emanuel A. P. Habets</strong></p>
<p>In recent years, several text-to-speech systems have been proposed to synthesize natural speech in zero-shot, few-shot, and low-resource scenarios. However, these methods typically require training with data from many different speakers. The speech quality across the speaker set typically is diverse and imposes an upper limit on the quality achievable for the low-resource speaker. In the current work, we achieve high-quality speech synthesis using as little as five minutes of speech from the desired speaker by augmenting the low-resource speaker data with noise and employing multiple sampling techniques during training. Our method requires only four high-quality, high-resource speakers, which are easy to obtain and use in practice. Our low-complexity method achieves improved speaker similarity compared to the state-of-the-art zero-shot method HierSpeech++ and the recent low-resource method AdapterMix while maintaining comparable naturalness. Our proposed approach can also reduce the data requirements for speech synthesis for new speakers and languages. </p>
<blockquote>
<p>近年来，已有多种文本到语音系统被提出，可在零样本、小样本文本资源稀缺的场景下合成自然语音。然而，这些方法通常需要使用来自许多不同发言人的数据进行训练。发言人集中的语音质量各不相同，并对低资源发言人的语音质量设定了上限。在目前的工作中，我们通过使用目标发言人的五分钟的语音来实现高质量的语音合成，通过使用噪声来增强低资源语音数据并在训练过程中采用多种采样技术。我们的方法只需要四个高质量、高资源的发言人，在实践中易于获取和使用。我们的低复杂度方法在提高说话人相似度方面优于最新的零样本方法HierSpeech++和最近的低资源方法AdapterMix，同时保持相当的自然度。我们提出的方法还可以降低新发言人和新语言的语音合成对数据的要求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05976v1">PDF</a> Accepted for publication at the 2025 IEEE International Conference on   Acoustics, Speech, and Signal Processing (ICASSP 2025) to be held at   Hyderabad, India</p>
<p><strong>Summary</strong></p>
<p>近年来，针对零样本、少样本和低资源场景的文本到语音系统被相继提出。然而，这些方法通常需要大量不同说话人的数据进行训练，语音质量因说话人的不同而各异，并对低资源说话人的语音质量设置了上限。当前工作中，我们通过将低资源说话人的数据增加噪声并采用多重采样技术进行训练，仅使用五分钟的目标说话人的语音即可实现高质量的语音合成。该方法仅需四个高质量、高资源的说话人，易于在实际操作中获得和使用。相比最新的零样本方法HierSpeech++和近期的低资源方法AdapterMix，我们的方法在保持自然性的同时，提高了说话人相似性。此外，该方法还可降低新说话人和新语言合成所需的数据要求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近年提出的文本到语音系统主要在零样本、少样本和低资源场景下进行合成，但存在对大量不同说话人数据的需求。</li>
<li>语音质量因说话人的不同而存在差异，对低资源说话人的语音质量设置了上限。</li>
<li>提出了一种新方法，仅使用五分钟目标说话人的语音和通过增加噪声及多重采样技术即可实现高质量语音合成。</li>
<li>方法仅需四个高质量、高资源的说话人，易于获取并在实践中使用。</li>
<li>与现有方法相比，该方法在保持自然性的同时，提高了说话人相似性。</li>
<li>该方法降低了新说话人和新语言合成所需的数据要求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05976">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8627bb3acb34ae21e0051676b4a63d76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cdc86350e7df716c4eef9db8fb102d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b276c7df10d0c0ff301145f43106594.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dea2c4271cb0ac387f48852700a8b93.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Language-Inspired-Relation-Transfer-for-Few-shot-Class-Incremental-Learning"><a href="#Language-Inspired-Relation-Transfer-for-Few-shot-Class-Incremental-Learning" class="headerlink" title="Language-Inspired Relation Transfer for Few-shot Class-Incremental   Learning"></a>Language-Inspired Relation Transfer for Few-shot Class-Incremental   Learning</h2><p><strong>Authors:Yifan Zhao, Jia Li, Zeyin Song, Yonghong Tian</strong></p>
<p>Depicting novel classes with language descriptions by observing few-shot samples is inherent in human-learning systems. This lifelong learning capability helps to distinguish new knowledge from old ones through the increase of open-world learning, namely Few-Shot Class-Incremental Learning (FSCIL). Existing works to solve this problem mainly rely on the careful tuning of visual encoders, which shows an evident trade-off between the base knowledge and incremental ones. Motivated by human learning systems, we propose a new Language-inspired Relation Transfer (LRT) paradigm to understand objects by joint visual clues and text depictions, composed of two major steps. We first transfer the pretrained text knowledge to the visual domains by proposing a graph relation transformation module and then fuse the visual and language embedding by a text-vision prototypical fusion module. Second, to mitigate the domain gap caused by visual finetuning, we propose context prompt learning for fast domain alignment and imagined contrastive learning to alleviate the insufficient text data during alignment. With collaborative learning of domain alignments and text-image transfer, our proposed LRT outperforms the state-of-the-art models by over $13%$ and $7%$ on the final session of mini-ImageNet and CIFAR-100 FSCIL benchmarks. </p>
<blockquote>
<p>通过观察少数样本并用语言描述来描绘新型类别，是人类学习系统的固有特性。这种终身学习能力有助于通过开放世界学习的增加来区分新旧知识，即所谓的Few-Shot Class-Incremental Learning（FSCIL）。现有解决此问题的研究主要依赖于视觉编码器的仔细调整，这显示出基础知识和增量知识之间的明显权衡。受人类学习系统的启发，我们提出了一种新的语言启发关系转移（LRT）范式，通过联合视觉线索和文本描述来理解对象，包括两个主要步骤。首先，我们通过提出图关系转换模块将预训练的文本知识转移到视觉领域，然后通过文本视觉原型融合模块融合视觉和语言嵌入。其次，为了缓解由于视觉微调造成的领域差距，我们提出了上下文提示学习进行快速领域对齐和想象对比学习，以缓解对齐过程中的文本数据不足。通过领域对齐和文本图像传输的协作学习，我们提出的LRT在mini-ImageNet和CIFAR-100 FSCIL基准测试的最后一期比最先进的模型高出超过13%和7%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05862v1">PDF</a> Accepted by IEEE TPAMI</p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于人类学习系统的少样本类增量学习（FSCIL）问题。针对现有方法主要依赖视觉编码器的调整，存在基础知识与增量知识之间的权衡问题，提出一种新的语言启发关系转移（LRT）范式。该范式通过两个主要步骤理解和识别对象：一是通过图关系转换模块将预训练的文本知识转移到视觉领域，然后通过文本-视觉原型融合模块融合视觉和文本嵌入。二是为了减少因视觉微调导致的领域差距，提出了上下文提示学习和想象对比学习。实验结果表明，所提出的LRT在mini-ImageNet和CIFAR-100的FSCIL基准测试中，相较于现有模型有显著提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类学习系统能够通过对少样本的观察，通过语言描述来描绘新型类别，这为Few-Shot Class-Incremental Learning（FSCIL）提供了启示。</li>
<li>现有解决FSCIL问题的方法主要依赖视觉编码器的调整，存在基础知识和增量知识的权衡问题。</li>
<li>提出的Language-inspired Relation Transfer（LRT）范式通过结合视觉线索和文本描述来理解对象。</li>
<li>LRT范式包括两个主要步骤：将预训练的文本知识转移到视觉领域，然后融合视觉和文本嵌入。</li>
<li>为减少因视觉微调导致的领域差距，提出了上下文提示学习和想象对比学习。</li>
<li>实验结果表明，LRT在FSCIL任务上的性能超过现有模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05862">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b6b20cd92253b45161bafd41232cdfe0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6aa2a0cccc223bd24a7e4f6b7003855b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c359e615a84c8e09a9042acb95b8f26d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a70759992b52099d3f430af09de0e99.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-Unsupervised-Graph-Few-shot-Learning-via-Set-Functions-and-Optimal-Transport"><a href="#Enhancing-Unsupervised-Graph-Few-shot-Learning-via-Set-Functions-and-Optimal-Transport" class="headerlink" title="Enhancing Unsupervised Graph Few-shot Learning via Set Functions and   Optimal Transport"></a>Enhancing Unsupervised Graph Few-shot Learning via Set Functions and   Optimal Transport</h2><p><strong>Authors:Yonghao Liu, Fausto Giunchiglia, Ximing Li, Lan Huang, Xiaoyue Feng, Renchu Guan</strong></p>
<p>Graph few-shot learning has garnered significant attention for its ability to rapidly adapt to downstream tasks with limited labeled data, sparking considerable interest among researchers. Recent advancements in graph few-shot learning models have exhibited superior performance across diverse applications. Despite their successes, several limitations still exist. First, existing models in the meta-training phase predominantly focus on instance-level features within tasks, neglecting crucial set-level features essential for distinguishing between different categories. Second, these models often utilize query sets directly on classifiers trained with support sets containing only a few labeled examples, overlooking potential distribution shifts between these sets and leading to suboptimal performance. Finally, previous models typically require necessitate abundant labeled data from base classes to extract transferable knowledge, which is typically infeasible in real-world scenarios. To address these issues, we propose a novel model named STAR, which leverages Set funcTions and optimAl tRansport for enhancing unsupervised graph few-shot learning. Specifically, STAR utilizes expressive set functions to obtain set-level features in an unsupervised manner and employs optimal transport principles to align the distributions of support and query sets, thereby mitigating distribution shift effects. Theoretical analysis demonstrates that STAR can capture more task-relevant information and enhance generalization capabilities. Empirically, extensive experiments across multiple datasets validate the effectiveness of STAR. Our code can be found here. </p>
<blockquote>
<p>图少量学习（Graph Few-Shot Learning）因其能够在有限标记数据下迅速适应下游任务的能力而受到广泛关注，引发了研究人员的极大兴趣。最近的图少量学习模型的进展在各种应用中表现出了卓越的性能。尽管取得了成功，但仍存在一些局限性。首先，现有模型在元训练阶段主要关注任务内的实例级特征，而忽略了对于区分不同类别至关重要的集合级特征。其次，这些模型通常直接在由支持集（仅包含少量标记示例）训练的分类器上使用查询集，忽视了这些集合之间潜在的分布偏移，导致性能不佳。最后，以前的模型通常需要来自基类的丰富标记数据来提取可转移知识，这在现实场景中通常不可行。为了解决这些问题，我们提出了一种新型模型STAR，它利用集合函数和最优传输（Set funcTions and optimAl tRansport）来提高无监督图少量学习的性能。具体来说，STAR使用表达性强的集合函数以无监督的方式获取集合级特征，并采用最优传输原理来对齐支持集和查询集的分布，从而减轻分布偏移的影响。理论分析表明，STAR能够捕获更多任务相关信息并提高泛化能力。在多个数据集上的大量实验验证了STAR的有效性。我们的代码可以在这里找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05635v1">PDF</a> KDD2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了图少样本学习的重要性及其在面对下游任务时的优势。尽管现有模型在多个应用领域表现出卓越性能，但它们仍面临一些局限性。为此，提出了一种名为STAR的新型模型，该模型利用集合函数和最佳传输技术，以无监督的方式解决图少样本学习问题。通过理论分析验证，STAR能更好地捕捉任务相关信息并提升泛化能力。在多个数据集上的实验证明了STAR的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图少样本学习能迅速适应下游任务，具有有限标记数据的优势。</li>
<li>现有模型主要关注实例级特征，忽视了集合级特征的重要性。</li>
<li>模型在元训练阶段直接使用查询集进行训练可能导致性能下降。</li>
<li>模型通常依赖于大量的基本类别的标记数据来提取可转移知识，这在现实世界中通常不可行。</li>
<li>提出的STAR模型利用集合函数以无监督方式获取集合级特征。</li>
<li>STAR采用最佳传输原则来对齐支持集和查询集分布，减少分布偏移的影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05635">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-04a6f67322a1f465680b0a4f95979b16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d37a9d3098f21fedd2d73b0d2b43971.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a13133522a9ddff8674ca1c5e7ec666.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff1e2cdb8c45e0b8a53a9515d30f64f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b57ca75b05f9d40190f2f712b8e9c8a7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-Impact-of-Model-Scaling-on-Seen-and-Unseen-Language-Performance"><a href="#The-Impact-of-Model-Scaling-on-Seen-and-Unseen-Language-Performance" class="headerlink" title="The Impact of Model Scaling on Seen and Unseen Language Performance"></a>The Impact of Model Scaling on Seen and Unseen Language Performance</h2><p><strong>Authors:Rhitabrat Pokharel, Sina Bagheri Nezhad, Ameeta Agrawal, Suresh Singh</strong></p>
<p>The rapid advancement of Large Language Models (LLMs), particularly those trained on multilingual corpora, has intensified the need for a deeper understanding of their performance across a diverse range of languages and model sizes. Our research addresses this critical need by studying the performance and scaling behavior of multilingual LLMs in text classification and machine translation tasks across 204 languages. We systematically examine both seen and unseen languages across three model families of varying sizes in zero-shot and few-shot settings. Our findings show significant differences in scaling behavior between zero-shot and two-shot scenarios, with striking disparities in performance between seen and unseen languages. Model scale has little effect on zero-shot performance, which remains mostly flat. However, in two-shot settings, larger models show clear linear improvements in multilingual text classification. For translation tasks, however, only the instruction-tuned model showed clear benefits from scaling. Our analysis also suggests that overall resource levels, not just the proportions of pretraining languages, are better predictors of model performance, shedding light on what drives multilingual LLM effectiveness. </p>
<blockquote>
<p>随着大型语言模型（LLM）的快速发展，特别是在多语言语料库上训练的模型，我们需要更深入地理解其在多种语言和不同模型规模上的性能。我们的研究通过研究和比较不同大小模型家族在204种语言的文本分类和机器翻译任务中的性能和扩展行为来应对这一迫切需求。我们在零样本和少样本环境中，系统研究了可见语言和未见语言在不同模型上的表现。我们发现零样本和两样本场景中的扩展行为存在显著差异，并且在可见语言和未见语言之间表现出惊人的表现差异。模型规模对零样本表现几乎没有影响，基本上保持稳定。然而，在两样本环境中，大型模型在跨语言文本分类中显示出明显的线性改进。但对于翻译任务来说，只有指令调优模型才能从扩展中明显受益。我们的分析还表明，整体资源水平，而非预训练语言的比例更能预测模型性能，这揭示了推动多语言LLM有效性的因素。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05629v1">PDF</a> Accepted at SEAS Workshop at AAAI25</p>
<p><strong>Summary</strong></p>
<p>多语言大型语言模型（LLM）在文本分类和机器翻译任务中的性能和扩展行为研究。研究涉及三个不同规模的模型家族，在零样本和少样本场景下对204种语言的性能进行了系统评估。发现零样本和两样本场景下的扩展行为存在显著差异，不同语言和模型规模下的性能差异明显。模型规模对零样本性能影响较小，而在两样本场景下，大型模型在跨语言文本分类中的表现呈现线性提升。对于翻译任务，只有经过指令调优的模型才能从扩展中获得明显优势。整体资源水平而非预训练语言的比例能更好地预测模型性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多语言大型语言模型（LLM）在文本分类和机器翻译任务中的性能需深入研究。</li>
<li>零样本和少样本场景下的模型表现存在显著差距。</li>
<li>模型规模对零样本性能影响较小，但在两样本场景下，大型模型在跨语言文本分类中有线性提升。</li>
<li>对于机器翻译任务，只有经过指令调优的模型能从扩展中获得优势。</li>
<li>整体资源水平是预测模型性能的关键因素。</li>
<li>不同语言和模型之间的性能差异明显，需要针对不同语言和任务进行优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-697ad8f1427eec64e2ef2669de2148fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78e98036ca38a594a5a6ca142d51a33c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f5cd3ea48359374f3e4f5cafa13de35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-372cf91c6ce058cc7b0e6a83002b3639.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd4e035062a33c430adb2227b79856f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ea90ffe8a1c8b43065d3059b07e0914.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb8dbaa20c4d8f29c02e6b6c1926a301.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TimeDP-Learning-to-Generate-Multi-Domain-Time-Series-with-Domain-Prompts"><a href="#TimeDP-Learning-to-Generate-Multi-Domain-Time-Series-with-Domain-Prompts" class="headerlink" title="TimeDP: Learning to Generate Multi-Domain Time Series with Domain   Prompts"></a>TimeDP: Learning to Generate Multi-Domain Time Series with Domain   Prompts</h2><p><strong>Authors:Yu-Hao Huang, Chang Xu, Yueying Wu, Wu-Jun Li, Jiang Bian</strong></p>
<p>Time series generation models are crucial for applications like data augmentation and privacy preservation. Most existing time series generation models are typically designed to generate data from one specified domain. While leveraging data from other domain for better generalization is proved to work in other application areas, this approach remains challenging for time series modeling due to the large divergence in patterns among different real world time series categories. In this paper, we propose a multi-domain time series diffusion model with domain prompts, named TimeDP. In TimeDP, we utilize a time series semantic prototype module which defines time series prototypes to represent time series basis, each prototype vector serving as “word” representing some elementary time series feature. A prototype assignment module is applied to extract the extract domain specific prototype weights, for learning domain prompts as generation condition. During sampling, we extract “domain prompt” with few-shot samples from the target domain and use the domain prompts as condition to generate time series samples. Experiments demonstrate that our method outperforms baselines to provide the state-of-the-art in-domain generation quality and strong unseen domain generation capability. </p>
<blockquote>
<p>时间序列生成模型在数据增强和隐私保护等应用中发挥着重要作用。大多数现有的时间序列生成模型通常被设计用来从特定领域生成数据。虽然利用其他领域的数据以实现更好的泛化在其他应用领域已被证明是有效的，但由于不同现实世界时间序列类别之间模式存在巨大差异，这种方法在时间序列建模中仍然具有挑战性。在本文中，我们提出了一种带有领域提示的多领域时间序列扩散模型，名为TimeDP。在TimeDP中，我们利用时间序列语义原型模块来定义时间序列原型以表示时间序列基础，每个原型向量都作为代表某些基本时间序列特征的“单词”。应用原型分配模块来提取领域特定的原型权重，作为生成条件的领域提示进行学习。在采样过程中，我们从目标领域提取少量样本的“领域提示”，并将领域提示作为条件来生成时间序列样本。实验表明，我们的方法在领域内生成质量和未见领域的生成能力方面均超过了基线方法，达到了最新水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05403v1">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为TimeDP的多域时间序列扩散模型，通过引入时间序列语义原型模块和原型分配模块，实现跨域生成时间序列数据。该模型能够提取特定域的时间序列原型权重，将其作为生成条件进行学习，从而在采样过程中利用来自目标域的少量样本提取“域提示”，并作为条件生成时间序列样本。实验表明，该方法在领域内生成质量和未见领域的生成能力上具有优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时间序列生成模型在数据增强和隐私保护等应用中具有重要意义。</li>
<li>现有时间序列生成模型通常局限于特定领域的数据生成。</li>
<li>跨域生成时间序列数据具有挑战性，因为不同现实世界时间序列类别之间存在模式差异。</li>
<li>TimeDP模型通过引入时间序列语义原型模块和原型分配模块实现多域时间序列生成。</li>
<li>TimeDP模型能够提取特定域的时间序列原型权重，作为生成条件进行学习。</li>
<li>在采样过程中，TimeDP模型利用来自目标域的少量样本提取“域提示”。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05403">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8307d6f762d9380e06594f8dcac84cf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c3b5545b9d0493ccbcfad2070232a0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-951e9816ddc5b8b26608145c915c6972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6a05595f65dda64a1de927738664819.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Harnessing-Large-Language-and-Vision-Language-Models-for-Robust-Out-of-Distribution-Detection"><a href="#Harnessing-Large-Language-and-Vision-Language-Models-for-Robust-Out-of-Distribution-Detection" class="headerlink" title="Harnessing Large Language and Vision-Language Models for Robust   Out-of-Distribution Detection"></a>Harnessing Large Language and Vision-Language Models for Robust   Out-of-Distribution Detection</h2><p><strong>Authors:Pei-Kang Lee, Jun-Cheng Chen, Ja-Ling Wu</strong></p>
<p>Out-of-distribution (OOD) detection has seen significant advancements with zero-shot approaches by leveraging the powerful Vision-Language Models (VLMs) such as CLIP. However, prior research works have predominantly focused on enhancing Far-OOD performance, while potentially compromising Near-OOD efficacy, as observed from our pilot study. To address this issue, we propose a novel strategy to enhance zero-shot OOD detection performances for both Far-OOD and Near-OOD scenarios by innovatively harnessing Large Language Models (LLMs) and VLMs. Our approach first exploit an LLM to generate superclasses of the ID labels and their corresponding background descriptions followed by feature extraction using CLIP. We then isolate the core semantic features for ID data by subtracting background features from the superclass features. The refined representation facilitates the selection of more appropriate negative labels for OOD data from a comprehensive candidate label set of WordNet, thereby enhancing the performance of zero-shot OOD detection in both scenarios. Furthermore, we introduce novel few-shot prompt tuning and visual prompt tuning to adapt the proposed framework to better align with the target distribution. Experimental results demonstrate that the proposed approach consistently outperforms current state-of-the-art methods across multiple benchmarks, with an improvement of up to 2.9% in AUROC and a reduction of up to 12.6% in FPR95. Additionally, our method exhibits superior robustness against covariate shift across different domains, further highlighting its effectiveness in real-world scenarios. </p>
<blockquote>
<p>非分布内（OOD）检测利用强大的视觉语言模型（VLMs）如CLIP的零样本方法取得了重大进展。然而，先前的研究工作主要集中在提高远OOD性能上，可能会损害近OOD的效果，从我们的初步研究中可以观察到这一点。为了解决这个问题，我们提出了一种新的策略，通过创新性地利用大型语言模型（LLMs）和VLMs，提高零样本的远OOD和近OOD场景的性能。我们的方法首先利用LLM生成ID标签的超类及其相应的背景描述，然后使用CLIP进行特征提取。接下来，我们通过从超类特征中减去背景特征来分离ID数据的核心语义特征。这种精炼的表示有助于从WordNet的综合候选标签集中为OOD数据选择更合适的负标签，从而提高两种场景下的零样本OOD检测性能。此外，我们引入了新型的小样本提示调整和视觉提示调整，以适应框架以更好地与目标分布对齐。实验结果表明，所提出的方法在多基准测试中始终优于当前最先进的方法，在AUROC上提高了高达2.9%，在FPR9ud下减少了高达12.6%。此外，我们的方法在应对不同领域的协变量变化方面表现出更强的稳健性，进一步突显其在现实世界场景中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05228v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>摘要</strong><br>基于CLIP的视觉语言模型和大型语言模型，提出一种改进策略以提高零样本模式下的离群值检测性能，针对远近两类离群数据都进行了改进。该研究使用语言模型生成标签和背景描述，提取特征后，通过减去背景特征得到精炼表示，选择更合适的负标签用于离群值数据。此外，引入少样本提示调整和视觉提示调整以适应目标分布。实验结果表明，该方法在多个基准测试中均优于当前先进技术，在AUROC上提高了高达2.9%，在FPR95上降低了高达12.6%。此方法对不同域的协变量偏移具有良好的稳健性，显示出其在真实场景中的有效性。</p>
<p><strong>关键见解</strong></p>
<p>一、利用视觉语言模型和大型语言模型结合的策略来提高零样本模式下的离群值检测性能。<br>二、生成标签和背景描述并使用特征提取，通过精炼表示选择更合适的负标签用于离群值数据。<br>三、引入少样本提示调整和视觉提示调整以适应目标分布，提高检测性能。<br>四、实验结果表明该方法在多个基准测试中表现优异，包括AUROC和FPR95等指标。<br>五、该方法对不同的协变量偏移具有良好的稳健性，适用于真实场景。<br>六、研究强调了结合语言模型和视觉模型的策略对于解决离群值检测问题的重要性。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05228">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1a4800b4afac9d67c8ba9ee8760e5de1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-116f607afe04a5ae7c90c7dbab6ed4a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ef9b9939963977b8481b81c34512fe0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cde997ab75bfebf9a3f0f4500b54f62f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Text-Based-Knowledge-Embedded-Soft-Sensing-Modeling-Approach-for-General-Industrial-Process-Tasks-Based-on-Large-Language-Model"><a href="#A-Text-Based-Knowledge-Embedded-Soft-Sensing-Modeling-Approach-for-General-Industrial-Process-Tasks-Based-on-Large-Language-Model" class="headerlink" title="A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for   General Industrial Process Tasks Based on Large Language Model"></a>A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for   General Industrial Process Tasks Based on Large Language Model</h2><p><strong>Authors:Shuo Tong, Han Liu, Runyuan Guo, Xueqiong Tian, Wenqing Wang, Ding Liu, Youmin Zhang</strong></p>
<p>Data-driven soft sensors (DDSS) have become mainstream methods for predicting key performance indicators in process industries. However, DDSS development requires complex and costly customized designs tailored to various tasks during the modeling process. Moreover, DDSS are constrained to a single structured data modality, limiting their ability to incorporate additional contextual knowledge. Furthermore, DDSSs’ limited representation learning leads to weak predictive performance with scarce data. To address these challenges, we propose a general framework named LLM-TKESS (large language model for text-based knowledge-embedded soft sensing), harnessing the powerful general problem-solving capabilities, cross-modal knowledge transfer abilities, and few-shot capabilities of LLM for enhanced soft sensing modeling. Specifically, an auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM’s potential for capturing temporal relationships within series and spatial semantic relationships among auxiliary variables. Then, we propose a two-stage fine-tuning alignment strategy: in the first stage, employing parameter-efficient fine-tuning through autoregressive training adjusts LLM to rapidly accommodate process variable data, resulting in a soft sensing foundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM to various downstream tasks without modifying its architecture. Then, we propose two text-based knowledge-embedded soft sensors, integrating new natural language modalities to overcome the limitations of pure structured data models. Furthermore, benefiting from LLM’s pre-existing world knowledge, our model demonstrates outstanding predictive capabilities in small sample conditions. Using the thermal deformation of air preheater rotor as a case study, we validate through extensive experiments that LLM-TKESS exhibits outstanding performance. </p>
<blockquote>
<p>数据驱动软传感器（DDSS）已成为流程工业预测关键绩效指标的主流方法。然而，DDSS开发需要在建模过程中对多种任务进行定制化的复杂且昂贵的设计。此外，DDSS受限于单一的结构化数据模态，无法融入额外的上下文知识。而且，DDSS的有限表示学习导致在数据稀缺时的预测性能较弱。</p>
</blockquote>
<p>为了解决这些挑战，我们提出了一种名为LLM-TKESS（基于文本知识嵌入软感知的大型语言模型）的通用框架，利用LLM的强大通用问题解决能力、跨模态知识迁移能力和小样本能力，增强软感知建模。具体来说，我们提出了一种辅助变量序列编码器（AVS编码器），以释放LLM捕捉序列内的时间关系和辅助变量之间的空间语义关系的潜力。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05075v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>数据驱动软传感器（DDSS）已成为流程工业中预测关键性能指标的主流方法，但其开发过程中需要针对各种任务进行复杂且成本高昂的定制设计。针对DDSS在数据稀缺、单一数据模态和有限表示学习方面的局限性，提出一种通用框架LLM-TKESS（大型语言模型文本知识嵌入软传感），利用大型语言模型的通用问题解决能力、跨模态知识迁移能力和小样本学习能力，增强软传感建模。通过辅助变量序列编码器和两阶段精细调整对齐策略，结合自然语言模态数据，LLM-TKESS展现出卓越的预测性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据驱动软传感器（DDSS）是流程工业中预测关键性能指标的主流方法，但存在开发复杂、成本高和局限性。</li>
<li>LLM-TKESS框架利用大型语言模型的通用问题解决能力和跨模态知识迁移能力，以改进DDSS的弱点。</li>
<li>LLM-TKESS通过辅助变量序列编码器（AVS Encoder）捕捉时间序列和空间语义关系。</li>
<li>两阶段精细调整对齐策略，使LLM适应过程变量数据，并快速建立软传感基础模型（SSFM）。</li>
<li>LLM-TKESS通过训练适配器适应各种下游任务，同时保持模型结构不变。</li>
<li>结合自然语言模态数据，克服纯结构化数据模型的局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05075">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-57037798706e02268b462fde155a5485.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f767c1c5b887d2bfebde927b2ec3447f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1ea3ca29100b6b7b9da59d909968570.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c027b263bed802051c8eb4559956b2e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b2b915e3ad730af09aa766bbc92f602.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf158bcdf2197d04389ab112e6539fdb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Continuous-Knowledge-Preserving-Decomposition-for-Few-Shot-Continual-Learning"><a href="#Continuous-Knowledge-Preserving-Decomposition-for-Few-Shot-Continual-Learning" class="headerlink" title="Continuous Knowledge-Preserving Decomposition for Few-Shot Continual   Learning"></a>Continuous Knowledge-Preserving Decomposition for Few-Shot Continual   Learning</h2><p><strong>Authors:Xiaojie Li, Yibo Yang, Jianlong Wu, David A. Clifton, Yue Yu, Bernard Ghanem, Min Zhang</strong></p>
<p>Few-shot class-incremental learning (FSCIL) involves learning new classes from limited data while retaining prior knowledge, and often results in catastrophic forgetting. Existing methods either freeze backbone networks to preserve knowledge, which limits adaptability, or rely on additional modules or prompts, introducing inference overhead. To this end, we propose Continuous Knowledge-Preserving Decomposition for FSCIL (CKPD-FSCIL), a framework that decomposes a model’s weights into two parts: one that compacts existing knowledge (knowledge-sensitive components) and another that carries redundant capacity to accommodate new abilities (redundant-capacity components). The decomposition is guided by a covariance matrix from replay samples, ensuring principal components align with classification abilities. During adaptation, we freeze the knowledge-sensitive components and only adapt the redundant-capacity components, fostering plasticity while minimizing interference without changing the architecture or increasing overhead. Additionally, CKPD introduces an adaptive layer selection strategy to identify layers with redundant capacity, dynamically allocating adapters. Experiments on multiple benchmarks show that CKPD-FSCIL outperforms state-of-the-art methods. </p>
<blockquote>
<p>少量样本类增量学习（FSCIL）涉及从有限数据中学习新类别的同时保留先前知识，这常常导致灾难性遗忘。现有方法要么冻结主干网络以保留知识，从而限制了适应性，要么依赖于附加模块或提示，增加了推理开销。为此，我们提出了面向FSCIL的持续知识保留分解（CKPD-FSCIL）框架，该框架将模型的权重分解为两部分：一部分是压缩现有知识（知识敏感组件），另一部分是具有容纳新能力剩余容量（剩余容量组件）。分解由回放样本的协方差矩阵引导，确保主成分与分类能力对齐。在适应过程中，我们冻结知识敏感组件，只适应剩余容量组件，促进可塑性，同时最小化干扰，而不改变架构或增加开销。此外，CKPD引入了一种自适应层选择策略，以识别具有剩余容量的层，并动态分配适配器。在多个基准测试上的实验表明，CKPD-FSCIL优于最新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05017v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/xiaojieli0903/CKPD-FSCIL">https://github.com/xiaojieli0903/CKPD-FSCIL</a></p>
<p><strong>Summary</strong></p>
<p>模型权重分解方法CKPD-FSCIL解决少量类别增量学习中的灾难性遗忘问题。通过分解模型权重为知识敏感组件和冗余容量组件，确保在适应新类别时保留现有知识。采用由回放样本的协方差矩阵引导的分解方法，确保主成分与分类能力对齐。冻结知识敏感组件，仅适应冗余容量组件，促进可塑性并最小化干扰，且无需改变架构或增加额外开销。CKPD还引入了自适应层选择策略来识别具有冗余容量的层，并动态分配适配器。CKPD-FSCIL在多个基准测试上的表现优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CKPD-FSCIL解决了少量类别增量学习中的灾难性遗忘问题。</li>
<li>通过将模型权重分解为知识敏感组件和冗余容量组件，CKPD-FSCIL能够在适应新类别时保留现有知识。</li>
<li>分解方法由回放样本的协方差矩阵引导，确保主成分与分类能力对齐。</li>
<li>在适应过程中，CKPD-FSCIL通过冻结知识敏感组件并仅适应冗余容量组件，以促进可塑性并最小化干扰。</li>
<li>CKPD-FSCIL方法不需要改变模型架构或增加额外开销。</li>
<li>CKPD引入了自适应层选择策略，能够识别具有冗余容量的层。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05017">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ce201257b4dd560cfee13f6a8c4de7bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cd9df9ec2e824fd71f794f1a474af2e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="More-is-not-always-better-Enhancing-Many-Shot-In-Context-Learning-with-Differentiated-and-Reweighting-Objectives"><a href="#More-is-not-always-better-Enhancing-Many-Shot-In-Context-Learning-with-Differentiated-and-Reweighting-Objectives" class="headerlink" title="More is not always better? Enhancing Many-Shot In-Context Learning with   Differentiated and Reweighting Objectives"></a>More is not always better? Enhancing Many-Shot In-Context Learning with   Differentiated and Reweighting Objectives</h2><p><strong>Authors:Xiaoqing Zhang, Ang Lv, Yuhan Liu, Flood Sung, Wei Liu, Shuo Shang, Xiuying Chen, Rui Yan</strong></p>
<p>Large language models (LLMs) excel at few-shot in-context learning (ICL) without requiring parameter updates. However, as the number of ICL demonstrations increases from a few to many, performance tends to plateau and eventually decline. We identify two primary causes for this trend: the suboptimal negative log-likelihood (NLL) optimization objective and the incremental data noise. To address these issues, we introduce DrICL, a novel optimization method that enhances model performance through Differentiated Learning and advantage-based Reweighting objectives. Globally, DrICL utilizes differentiated learning to optimize the NLL objective, ensuring that many-shot performance surpasses zero-shot levels. Locally, it dynamically adjusts the weighting of many-shot demonstrations by leveraging cumulative advantages inspired by reinforcement learning, thereby improving generalization. This approach allows the model to handle varying numbers of shots effectively, mitigating the impact of noisy data. Recognizing the lack of multi-task datasets with diverse many-shot distributions, we develop the Many-Shot ICL Benchmark (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers from 1 to 350 within sequences of up to 8,000 tokens-for fine-tuning purposes. ICL-50 facilitates the evaluation of many-shot ICL strategies across seven prominent NLP tasks and 50 distinct datasets. Experimental results demonstrate that LLMs enhanced with DrICL achieve significant improvements in many-shot setups across various tasks, including both in-domain and out-of-domain scenarios. We release the code and benchmark dataset hoping to facilitate further research in many-shot ICL. </p>
<blockquote>
<p>大型语言模型（LLM）在不需更新参数的情况下，擅长于少量样本上下文学习（ICL）。然而，随着ICL演示样本的数量从少数增加到多数，性能往往达到平台期并最终下降。我们确定了导致这一趋势的两个主要原因：次优的负对数似然（NLL）优化目标和增量数据噪声。为了解决这些问题，我们引入了DrICL，这是一种新型优化方法，通过差异化学习和基于优势的重加权目标来提高模型性能。全局上，DrICL通过差异化学习来优化NLL目标，确保多次射击的性能超过零次射击水平。局部上，它受强化学习的启发，利用累积优势动态调整多次射击演示的权重，从而提高泛化能力。这种方法使模型能够有效地处理不同数量的样本，减轻噪声数据的影响。认识到缺乏具有多种多样多次射击分布的多任务数据集，我们开发了多次射击ICL基准测试（ICL-50）——一个大规模基准测试，包含50个任务，涵盖从1到350的射击次数，序列中的令牌高达8000个，用于微调目的。ICL-50有助于评估跨七个突出NLP任务和50个不同数据集的多次射击ICL策略。实验结果表明，使用DrICL增强的大型语言模型在各种任务的多镜头设置中实现了显着改进，包括域内和域外场景。我们发布代码和基准数据集，希望促进多镜头ICL的进一步研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04070v2">PDF</a> 13 pages, 8 figures, 11 tables</p>
<p><strong>摘要</strong></p>
<p>大型语言模型在无需参数更新的少样本上下文学习（ICL）中表现出色，但随着上下文学习示例从少数增加到多数，性能往往达到平台期并最终下降。研究指出这一现象主要由次优的负对数似然优化目标和递增的数据噪声引起。为解决这些问题，提出DrICL这一新型优化方法，通过差异化学习和基于优势的重加权目标来提升模型性能。DrICL全局优化负对数似然目标，确保多示例性能超越零示例水平；局部则通过借鉴强化学习的累积优势动态调整多示例演示的权重，提升模型的泛化能力。此方法使模型有效应对不同样本量，缓解数据噪声影响。研究认识到缺乏多样多示例分布的多任务数据集，于是开发出大型基准测试ICL-50，包含50个任务、涵盖1至350个示例、序列长达8000个符号，用于微调目的。ICL-50能在七个显著的自然语言处理任务和50个不同数据集上评估多示例ICL策略。实验结果显示，采用DrICL增强的大型语言模型在各种任务的多示例设置中取得了显著改进，包括域内和域外场景。研究公布了代码和基准测试数据集，希望能进一步推动多示例ICL的研究。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型在少样本上下文学习中表现优异，但随着示例数量增加，性能可能下降。</li>
<li>性能下降的主要原因包括次优的负对数似然优化目标和数据噪声的递增。</li>
<li>引入DrICL方法，通过差异化学习和基于优势的重加权来优化模型性能。</li>
<li>DrICL能在多示例场景下提升模型性能，并超越零示例水平。</li>
<li>开发出大型基准测试ICL-50，用于评估多示例ICL策略在多个自然语言处理任务和数据集上的性能。</li>
<li>采用DrICL的大型语言模型在多示例设置中取得了显著改进，包括在域内和域外场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04070">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5e86bd6a0b46ff64e68bf20f4e592a04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c272ef986b8b10591e903d057584ff63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da544742c78d322f4d2c8f78ca2d918d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afc85a7ba35b104cfcc5aeb268448e69.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Literature-Meets-Data-A-Synergistic-Approach-to-Hypothesis-Generation"><a href="#Literature-Meets-Data-A-Synergistic-Approach-to-Hypothesis-Generation" class="headerlink" title="Literature Meets Data: A Synergistic Approach to Hypothesis Generation"></a>Literature Meets Data: A Synergistic Approach to Hypothesis Generation</h2><p><strong>Authors:Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan</strong></p>
<p>AI holds promise for transforming scientific processes, including hypothesis generation. Prior work on hypothesis generation can be broadly categorized into theory-driven and data-driven approaches. While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other. To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation. We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97% over few-shot, 15.75% over literature-based alone, and 3.37% over data-driven alone). Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection. Our results show that human accuracy improves significantly by 7.44% and 14.19% on these tasks, respectively. These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry. </p>
<blockquote>
<p>人工智能（AI）有望改变科学过程，包括假设生成。关于假设生成的前期工作可以大致分为理论驱动和数据驱动的方法。虽然这两种方法在生成新颖且合理的假设方面都被证明是有效的，但它们是否能相互补充仍然是一个悬而未决的问题。为了解决这个问题，我们开发了一种结合文献见解和数据的方法，利用大型语言模型（LLM）进行假设生成。我们在五个不同的数据集上应用了我们提出的方法，并证明整合文献和数据的方法优于其他基准测试（在少样本情况下高出8.97%，在仅基于文献的情况下高出15.75%，在仅数据驱动的情况下高出3.37%）。此外，我们还进行了首次人类评估，以评估大型语言模型生成的假设在协助人类进行两项具有挑战性的任务时的效用：欺骗检测和AI生成内容检测。我们的结果表明，在这些任务上，人类准确率分别提高了7.44%和14.19%。这些发现表明，整合基于文献和基于数据驱动的方法为假设生成提供了一个全面而微妙的框架，并可能为科学探索开辟新的途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17309v3">PDF</a> 37 pages, 9 figures, code link:   <a target="_blank" rel="noopener" href="https://github.com/ChicagoHAI/hypothesis-generation">https://github.com/ChicagoHAI/hypothesis-generation</a></p>
<p><strong>Summary</strong><br>基于文本的信息，人工智能在融合文献和大数据的基础上，可以通过LLM驱动的方法生成假设，既提升了假设生成的效果，又有助于改善人类在欺骗检测和AI生成内容检测任务上的准确性。该方法的出现为科学过程带来了变革的承诺，特别是在假设生成方面。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI具有改变科学过程的潜力，特别是在假设生成方面。</li>
<li>假设生成方法可分为理论驱动和数据驱动两大类，但两者互补性尚待研究。</li>
<li>提出了一种结合文献和数据的LLM驱动假设生成方法，该方法在五组不同数据集上的表现均超过其他基准测试。</li>
<li>人类在欺骗检测和AI生成内容检测任务上，借助LLM生成的假设，准确性得到显著提高。</li>
<li>综合文献和大数据的方法为假设生成提供了全面而细致的框架。</li>
<li>该方法可能开启新的科学探索途径。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17309">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a2dfaab60ceefd4438b8c0fa72b42a63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32fa866a82f68872fe86f7fabcd997d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ed6ac10d21beb49a95aa29b546815e0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SEA-SQL-Semantic-Enhanced-Text-to-SQL-with-Adaptive-Refinement"><a href="#SEA-SQL-Semantic-Enhanced-Text-to-SQL-with-Adaptive-Refinement" class="headerlink" title="SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement"></a>SEA-SQL: Semantic-Enhanced Text-to-SQL with Adaptive Refinement</h2><p><strong>Authors:Chaofan Li, Yingxia Shao, Yawen Li, Zheng Liu</strong></p>
<p>Recent advancements in large language models (LLMs) have significantly contributed to the progress of the Text-to-SQL task. A common requirement in many of these works is the post-correction of SQL queries. However, the majority of this process entails analyzing error cases to develop prompts with rules that eliminate model bias. And there is an absence of execution verification for SQL queries. In addition, the prevalent techniques primarily depend on GPT-4 and few-shot prompts, resulting in expensive costs. To investigate the effective methods for SQL refinement in a cost-efficient manner, we introduce Semantic-Enhanced Text-to-SQL with Adaptive Refinement (SEA-SQL), which includes Adaptive Bias Elimination and Dynamic Execution Adjustment, aims to improve performance while minimizing resource expenditure with zero-shot prompts. Specifically, SEA-SQL employs a semantic-enhanced schema to augment database information and optimize SQL queries. During the SQL query generation, a fine-tuned adaptive bias eliminator is applied to mitigate inherent biases caused by the LLM. The dynamic execution adjustment is utilized to guarantee the executability of the bias eliminated SQL query. We conduct experiments on the Spider and BIRD datasets to demonstrate the effectiveness of this framework. The results demonstrate that SEA-SQL achieves state-of-the-art performance in the GPT3.5 scenario with 9%-58% of the generation cost. Furthermore, SEA-SQL is comparable to GPT-4 with only 0.9%-5.3% of the generation cost. </p>
<blockquote>
<p>最近大型语言模型（LLM）的进展对Text-to-SQL任务产生了重大推动。许多研究中的一项常见要求是SQL查询的后修正。然而，这个过程的大部分涉及分析错误情况，以制定规则提示来消除模型偏见。此外，缺乏对SQL查询的执行验证。此外，主流技术主要依赖于GPT-4和少量提示，导致成本高昂。为了以成本效益高的方式研究SQL精炼的有效方法，我们引入了带有自适应精炼的语义增强文本到SQL（SEA-SQL），包括自适应偏差消除和动态执行调整，旨在提高性能，同时以零提示的方式最小化资源支出。具体来说，SEA-SQL采用语义增强架构来增强数据库信息并优化SQL查询。在生成SQL查询时，应用了微调的自适应偏差消除器，以减轻LLM引起的固有偏差。动态执行调整用于保证偏差消除后的SQL查询的可执行性。我们在Spider和BIRD数据集上进行了实验，以证明该框架的有效性。结果表明，在GPT3.5场景中，SEA-SQL达到了最先进的性能，生成成本降低了9%~58%。此外，SEA-SQL与GPT-4相比，生成成本仅增加了0.9%~5.3%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04919v2">PDF</a> The article has been accepted by Frontiers of Computer Science (FCS),   with the DOI: {10.1007&#x2F;s11704-025-41136-3}</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在文本到SQL任务中的最新进展已经取得了显著贡献。然而，大多数方法都需要对SQL查询进行后修正，这主要涉及到分析错误情况并制定规则提示以消除模型偏见。此外，缺乏SQL查询的执行验证。本文介绍了一种成本效益高的SQL精炼有效方法——语义增强文本到SQL自适应精炼（SEA-SQL），旨在提高性能的同时最小化资源消耗，采用零镜头提示。SEA-SQL采用语义增强模式来增强数据库信息并优化SQL查询。在SQL查询生成过程中，应用了精细调整的自适应偏见消除器来减轻LLM引起的固有偏见。动态执行调整可确保消除偏见的SQL查询的可执行性。在Spider和BIRD数据集上的实验表明，SEA-SQL在GPT3.5场景中实现了最先进的性能，生成成本降低了9%-58%。此外，SEA-SQL与GPT-4相比，生成成本仅增加了0.9%-5.3%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本描述了大型语言模型（LLM）在文本到SQL任务中的应用进展及挑战。</li>
<li>介绍了SEA-SQL方法，该方法旨在通过自适应精炼技术提高SQL查询的性能和成本效益。</li>
<li>SEA-SQL采用语义增强模式来优化数据库信息和SQL查询。</li>
<li>自适应偏见消除器的应用是SEA-SQL的一个重要特点，能够减轻LLM的固有偏见。</li>
<li>动态执行调整确保消除偏见的SQL查询的可执行性。</li>
<li>在Spider和BIRD数据集上的实验表明，SEA-SQL在GPT3.5场景中表现优异，与GPT-4相比具有较低的成本增加。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.04919">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-049869cb50af810b69d417155d2d0b91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6a327f153f8ca93e324fa3c96eb5668.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88a1f73bb57d7f1111e4322c637bb935.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8872f24f7bb1e61c73bc0dda0d7186a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4e9fe1824e5de6863543e2644ceb699.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-13/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-13/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-57c5227adaa957949cfe1103c5f28961.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-01-13  HipyrNet Hypernet-Guided Feature Pyramid network for mixed-exposure   correction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c55bb0419401785f71826165ed212ef5.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-01-13  A Mixed-Integer Conic Program for the Multi-Agent Moving-Target   Traveling Salesman Problem
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
