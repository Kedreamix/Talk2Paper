<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-13  xLSTM-SENet xLSTM for Single-Channel Speech Enhancement">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6564fc8e4fa3ed8b34b51dccc7a1f7aa.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    64 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-13-æ›´æ–°"><a href="#2025-01-13-æ›´æ–°" class="headerlink" title="2025-01-13 æ›´æ–°"></a>2025-01-13 æ›´æ–°</h1><h2 id="xLSTM-SENet-xLSTM-for-Single-Channel-Speech-Enhancement"><a href="#xLSTM-SENet-xLSTM-for-Single-Channel-Speech-Enhancement" class="headerlink" title="xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement"></a>xLSTM-SENet: xLSTM for Single-Channel Speech Enhancement</h2><p><strong>Authors:Nikolai Lund KÃ¼hne, Jan Ã˜stergaard, Jesper Jensen, Zheng-Hua Tan</strong></p>
<p>While attention-based architectures, such as Conformers, excel in speech enhancement, they face challenges such as scalability with respect to input sequence length. In contrast, the recently proposed Extended Long Short-Term Memory (xLSTM) architecture offers linear scalability. However, xLSTM-based models remain unexplored for speech enhancement. This paper introduces xLSTM-SENet, the first xLSTM-based single-channel speech enhancement system. A comparative analysis reveals that xLSTM-and notably, even LSTM-can match or outperform state-of-the-art Mamba- and Conformer-based systems across various model sizes in speech enhancement on the VoiceBank+Demand dataset. Through ablation studies, we identify key architectural design choices such as exponential gating and bidirectionality contributing to its effectiveness. Our best xLSTM-based model, xLSTM-SENet2, outperforms state-of-the-art Mamba- and Conformer-based systems on the Voicebank+DEMAND dataset. </p>
<blockquote>
<p>åŸºäºæ³¨æ„åŠ›çš„æ¶æ„ï¼Œå¦‚Conformersï¼Œåœ¨è¯­éŸ³å¢å¼ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬é¢ä¸´ç€è¾“å…¥åºåˆ—é•¿åº¦æ–¹é¢çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€è¿‘æå‡ºçš„æ‰©å±•é•¿çŸ­æœŸè®°å¿†ï¼ˆxLSTMï¼‰æ¶æ„æä¾›äº†çº¿æ€§å¯æ‰©å±•æ€§ã€‚ç„¶è€Œï¼Œå°šæœªé’ˆå¯¹è¯­éŸ³å¢å¼ºæ¢ç´¢åŸºäºxLSTMçš„æ¨¡å‹ã€‚æœ¬æ–‡ä»‹ç»äº†åŸºäºxLSTMçš„å•é€šé“è¯­éŸ³å¢å¼ºç³»ç»ŸxLSTM-SENetã€‚å¯¹æ¯”åˆ†æè¡¨æ˜ï¼ŒxLSTMï¼ˆå°¤å…¶æ˜¯LSTMï¼‰åœ¨VoiceBank+Demandæ•°æ®é›†ä¸Šçš„è¯­éŸ³å¢å¼ºæ–¹é¢ï¼Œå¯ä»¥ä¸æœ€æ–°çš„Mambaå’ŒConformerç³»ç»Ÿç›¸åŒ¹é…æˆ–è¡¨ç°æ›´ä¼˜ç§€ï¼Œä¸”è¿™ä¸€ä¼˜åŠ¿åœ¨ä¸åŒæ¨¡å‹å¤§å°ä¸­éƒ½å­˜åœ¨ã€‚é€šè¿‡æ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬ç¡®å®šäº†å…³é”®æ¶æ„è®¾è®¡é€‰æ‹©ï¼Œå¦‚æŒ‡æ•°é—¨æ§å’ŒåŒå‘æ€§å¯¹å…¶æœ‰æ•ˆæ€§æœ‰æ‰€è´¡çŒ®ã€‚æˆ‘ä»¬æœ€å¥½çš„åŸºäºxLSTMçš„æ¨¡å‹xLSTM-SENet2åœ¨Voicebank+DEMANDæ•°æ®é›†ä¸Šè¶…è¶Šäº†æœ€æ–°çš„Mambaå’ŒConformerç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06146v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>xLSTM-SENetæ˜¯ç¬¬ä¸€ç¯‡ä½¿ç”¨æ‰©å±•é•¿çŸ­æœŸè®°å¿†ï¼ˆxLSTMï¼‰çš„å•é€šé“è¯­éŸ³å¢å¼ºç³»ç»Ÿã€‚å®ƒè§£å†³äº†åŸºäºæ³¨æ„åŠ›æ¶æ„å¦‚Conformersåœ¨è¯­éŸ³å¢å¼ºä¸Šé‡åˆ°çš„çº¿æ€§æ‰©å±•æ€§é—®é¢˜ã€‚ç›¸æ¯”ç°æœ‰çš„Mambaå’ŒConformerç³»ç»Ÿï¼ŒxLSTM-SENetåœ¨å„ç§æ¨¡å‹å¤§å°ä¸‹çš„è¯­éŸ³å¢å¼ºæ€§èƒ½ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨VoiceBank+Demandæ•°æ®é›†ä¸Šã€‚å…¶å…³é”®è®¾è®¡é€‰æ‹©ï¼Œå¦‚æŒ‡æ•°é—¨æ§å’ŒåŒå‘æ€§ï¼Œä¸ºå…¶æœ‰æ•ˆæ€§åšå‡ºäº†è´¡çŒ®ã€‚æœ€ä½³æ¨¡å‹xLSTM-SENet2åœ¨Voicebank+DEMANDæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>xLSTM-SENetæ˜¯åŸºäºæ‰©å±•é•¿çŸ­æœŸè®°å¿†ï¼ˆxLSTMï¼‰çš„é¦–ä¸ªå•é€šé“è¯­éŸ³å¢å¼ºç³»ç»Ÿã€‚</li>
<li>è¯¥ç³»ç»Ÿè§£å†³äº†åŸºäºæ³¨æ„åŠ›æ¶æ„åœ¨è¯­éŸ³å¢å¼ºæ–¹é¢çš„çº¿æ€§æ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>åœ¨VoiceBank+Demandæ•°æ®é›†ä¸Šï¼ŒxLSTM-SENetçš„è¯­éŸ³å¢å¼ºæ€§èƒ½ä¸ç°æœ‰çš„Mambaå’ŒConformerç³»ç»Ÿç›¸æ¯”è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>xLSTM-SENetçš„æœ€ä½³æ¨¡å‹xLSTM-SENet2çš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æŒ‡æ•°é—¨æ§å’ŒåŒå‘æ€§æ˜¯è¯¥ç³»ç»Ÿå…³é”®çš„è®¾è®¡é€‰æ‹©ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡æ¯”è¾ƒåˆ†æå’Œæ¶ˆèç ”ç©¶éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2feb103770040c44ae74f1945d12f39b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40823c755071f1ff5cfc69ce50c998c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f0e0fd52fd5de6c9eb1677e20236c84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fc246ee2cff3379901aa51573b11a92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b592a0a3913ce2b601d774da7e0fa482.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Contextual-ASR-Error-Handling-with-LLMs-Augmentation-for-Goal-Oriented-Conversational-AI"><a href="#Contextual-ASR-Error-Handling-with-LLMs-Augmentation-for-Goal-Oriented-Conversational-AI" class="headerlink" title="Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented   Conversational AI"></a>Contextual ASR Error Handling with LLMs Augmentation for Goal-Oriented   Conversational AI</h2><p><strong>Authors:Yuya Asano, Sabit Hassan, Paras Sharma, Anthony Sicilia, Katherine Atwell, Diane Litman, Malihe Alikhani</strong></p>
<p>General-purpose automatic speech recognition (ASR) systems do not always perform well in goal-oriented dialogue. Existing ASR correction methods rely on prior user data or named entities. We extend correction to tasks that have no prior user data and exhibit linguistic flexibility such as lexical and syntactic variations. We propose a novel context augmentation with a large language model and a ranking strategy that incorporates contextual information from the dialogue states of a goal-oriented conversational AI and its tasks. Our method ranks (1) n-best ASR hypotheses by their lexical and semantic similarity with context and (2) context by phonetic correspondence with ASR hypotheses. Evaluated in home improvement and cooking domains with real-world users, our method improves recall and F1 of correction by 34% and 16%, respectively, while maintaining precision and false positive rate. Users rated .8-1 point (out of 5) higher when our correction method worked properly, with no decrease due to false positives. </p>
<blockquote>
<p>é€šç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨é¢å‘ç›®æ ‡çš„å¯¹è¯ä¸­å¹¶ä¸æ€»æ˜¯è¡¨ç°è‰¯å¥½ã€‚ç°æœ‰çš„ASRæ ¡æ­£æ–¹æ³•ä¾èµ–äºç”¨æˆ·å…ˆéªŒæ•°æ®æˆ–å‘½åå®ä½“ã€‚æˆ‘ä»¬å°†æ ¡æ­£æ‰©å±•åˆ°æ²¡æœ‰ç”¨æˆ·å…ˆéªŒæ•°æ®ä¸”è¡¨ç°å‡ºè¯­è¨€çµæ´»æ€§çš„ä»»åŠ¡ï¼Œä¾‹å¦‚è¯æ±‡å’Œå¥æ³•å˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸Šä¸‹æ–‡å¢å¼ºæ–¹æ³•ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ’åç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç»“åˆäº†é¢å‘ç›®æ ‡çš„å¯¹è¯å¼äººå·¥æ™ºèƒ½åŠå…¶ä»»åŠ¡çš„å¯¹è¯çŠ¶æ€ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ï¼ˆ1ï¼‰è¯æ³•å’Œè¯­ä¹‰ä¸ä¸Šä¸‹æ–‡çš„ç›¸ä¼¼åº¦å¯¹æœ€ä½³çš„ASRå‡è®¾è¿›è¡Œæ’åï¼Œï¼ˆ2ï¼‰é€šè¿‡éŸ³ç´ å¯¹åº”ä¸ASRå‡è®¾çš„ä¸Šä¸‹æ–‡è¿›è¡Œæ’åã€‚åœ¨å®¶åº­æ”¹å–„å’Œçƒ¹é¥ªé¢†åŸŸè¿›è¡ŒçœŸå®ç”¨æˆ·è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†ä¿®æ­£çš„å¬å›ç‡å’ŒF1åˆ†æ•°ï¼Œåˆ†åˆ«ä¸º34%å’Œ16%ï¼ŒåŒæ—¶ä¿æŒäº†ç²¾ç¡®åº¦å’Œè¯¯æŠ¥ç‡ã€‚ç”¨æˆ·åœ¨æ²¡æœ‰è¯¯æŠ¥çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ ¡æ­£æ–¹æ³•æ­£å¸¸å·¥ä½œæ—¶çš„è¯„åˆ†æ¯”åŸæ¥é«˜å‡º0.8-1åˆ†ï¼ˆæ»¡åˆ†5åˆ†ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06129v1">PDF</a> Accepted to COLING 2025 Industry Track</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹é€šç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨ç›®æ ‡å¯¼å‘å¯¹è¯ä¸­çš„æ€§èƒ½ä¸ä½³é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ’åç­–ç•¥çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹è¯çŠ¶æ€å’Œä»»åŠ¡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¹ASRå‡è®¾è¿›è¡Œæ’åï¼Œå¹¶è€ƒè™‘è¯æ±‡å’Œè¯­ä¹‰ç›¸ä¼¼æ€§ä»¥åŠä¸è¯­éŸ³å‡è®¾çš„è¯­éŸ³å¯¹åº”å…³ç³»ã€‚åœ¨å®¶å±…æ”¹å–„å’Œçƒ¹é¥ªé¢†åŸŸè¿›è¡ŒçœŸå®ç”¨æˆ·è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†ä¿®æ­£çš„å¬å›ç‡å’ŒF1åˆ†æ•°ï¼Œåˆ†åˆ«ä¸º34%å’Œ16%ï¼ŒåŒæ—¶ä¿æŒç²¾åº¦å’Œè¯¯æŠ¥ç‡ã€‚ç”¨æˆ·å¯¹æˆ‘ä»¬çš„ä¿®æ­£æ–¹æ³•è¯„ä»·æ›´é«˜ï¼Œä¸”æ²¡æœ‰å› è¯¯æŠ¥è€Œé™ä½è¯„ä»·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é€šç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨ç›®æ ‡å¯¼å‘å¯¹è¯ä¸­çš„æ€§èƒ½æœ‰å¾…æé«˜ã€‚</li>
<li>ç°æœ‰ASRä¿®æ­£æ–¹æ³•ä¾èµ–ç”¨æˆ·å…ˆéªŒæ•°æ®æˆ–å‘½åå®ä½“ï¼Œè€Œæœ¬æ–‡æ‰©å±•äº†æ— å…ˆéªŒç”¨æˆ·æ•°æ®çš„ä»»åŠ¡ä¿®æ­£ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ’åç­–ç•¥çš„æ–¹æ³•ï¼Œè€ƒè™‘è¯æ±‡å’Œè¯­ä¹‰ç›¸ä¼¼æ€§ä»¥åŠä¸ASRå‡è®¾çš„è¯­éŸ³å¯¹åº”å…³ç³»ã€‚</li>
<li>æ–¹æ³•é€šè¿‡å¯¹è¯çŠ¶æ€å’Œä»»åŠ¡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹ASRå‡è®¾è¿›è¡Œæ’åã€‚</li>
<li>åœ¨å®¶å±…æ”¹å–„å’Œçƒ¹é¥ªé¢†åŸŸçš„çœŸå®ç”¨æˆ·è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜ä¿®æ­£çš„å¬å›ç‡å’ŒF1åˆ†æ•°ã€‚</li>
<li>ç”¨æˆ·å¯¹ä¿®æ­£æ–¹æ³•çš„è¯„ä»·æ›´é«˜ï¼Œä¸”åœ¨æ— è¯¯æŠ¥æƒ…å†µä¸‹è¯„ä»·æœªé™ä½ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºå…·æœ‰è¯­è¨€çµæ´»æ€§çš„ä»»åŠ¡ï¼Œå¦‚è¯æ±‡å’Œå¥æ³•å˜åŒ–ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1123fb1a419f1b14f5e4763606ae767a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27a9e69a88beb6f1bc6251cafa66b150.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70287212c397852caf7e1cc7a3806de6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7de331c4d9268a4a2c9b7dc6adbd524a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbadfa42a9edf06c53fb8824e5e29b03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0657d63283fe94440f120d90d962afd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Comparing-Self-Supervised-Learning-Models-Pre-Trained-on-Human-Speech-and-Animal-Vocalizations-for-Bioacoustics-Processing"><a href="#Comparing-Self-Supervised-Learning-Models-Pre-Trained-on-Human-Speech-and-Animal-Vocalizations-for-Bioacoustics-Processing" class="headerlink" title="Comparing Self-Supervised Learning Models Pre-Trained on Human Speech   and Animal Vocalizations for Bioacoustics Processing"></a>Comparing Self-Supervised Learning Models Pre-Trained on Human Speech   and Animal Vocalizations for Bioacoustics Processing</h2><p><strong>Authors:Eklavya Sarkar, Mathew Magimai. -Doss</strong></p>
<p>Self-supervised learning (SSL) foundation models have emerged as powerful, domain-agnostic, general-purpose feature extractors applicable to a wide range of tasks. Such models pre-trained on human speech have demonstrated high transferability for bioacoustic processing. This paper investigates (i) whether SSL models pre-trained directly on animal vocalizations offer a significant advantage over those pre-trained on speech, and (ii) whether fine-tuning speech-pretrained models on automatic speech recognition (ASR) tasks can enhance bioacoustic classification. We conduct a comparative analysis using three diverse bioacoustic datasets and two different bioacoustic tasks. Results indicate that pre-training on bioacoustic data provides only marginal improvements over speech-pretrained models, with comparable performance in most scenarios. Fine-tuning on ASR tasks yields mixed outcomes, suggesting that the general-purpose representations learned during SSL pre-training are already well-suited for bioacoustic tasks. These findings highlight the robustness of speech-pretrained SSL models for bioacoustics and imply that extensive fine-tuning may not be necessary for optimal performance. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åŸºç¡€æ¨¡å‹å·²ç»ä½œä¸ºå¼ºå¤§ã€é¢†åŸŸæ— å…³ã€é€šç”¨çš„ç‰¹å¾æå–å™¨å‡ºç°ï¼Œé€‚ç”¨äºå¤šç§ä»»åŠ¡ã€‚é¢„è®­ç»ƒåœ¨äººç±»è¯­éŸ³ä¸Šçš„æ­¤ç±»æ¨¡å‹å·²æ˜¾ç¤ºå‡ºåœ¨é«˜è½¬ç§»æ€§ç”Ÿç‰©å£°å¤„ç†ä¸­çš„é«˜å¯ç”¨æ€§ã€‚æœ¬æ–‡è°ƒæŸ¥äº†ï¼ˆiï¼‰ç›´æ¥é¢„è®­ç»ƒåœ¨åŠ¨ç‰©é¸£å«ä¸Šçš„SSLæ¨¡å‹æ˜¯å¦æ¯”é‚£äº›é¢„è®­ç»ƒåœ¨è¯­éŸ³ä¸Šçš„æ¨¡å‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä»¥åŠï¼ˆiiï¼‰æ˜¯å¦åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡ä¸Šå¾®è°ƒè¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥å¢å¼ºç”Ÿç‰©å£°éŸ³åˆ†ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ä¸ªä¸åŒçš„ç”Ÿç‰©å£°æ•°æ®é›†å’Œä¸¤ä¸ªä¸åŒçš„ç”Ÿç‰©å£°ä»»åŠ¡è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ç”Ÿç‰©å£°éŸ³æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒåªç•¥å¾®æ”¹è¿›äº†è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ€§èƒ½ç›¸å½“ã€‚åœ¨ASRä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒä¼šäº§ç”Ÿæ··åˆçš„ç»“æœï¼Œè¿™è¡¨æ˜åœ¨SSLé¢„è®­ç»ƒæœŸé—´å­¦ä¹ çš„é€šç”¨è¡¨ç¤ºå·²ç»éå¸¸é€‚åˆç”Ÿç‰©å£°éŸ³ä»»åŠ¡ã€‚è¿™äº›å‘ç°çªå‡ºäº†è¯­éŸ³é¢„è®­ç»ƒSSLæ¨¡å‹åœ¨ç”Ÿç‰©å£°å­¦ä¸­çš„ç¨³å¥æ€§ï¼Œå¹¶ä¸”æš—ç¤ºå¯èƒ½ä¸éœ€è¦å¤§é‡å¾®è°ƒå°±èƒ½è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05987v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹åœ¨åŠ¨ç‰©å£°éŸ³å¤„ç†æ–¹é¢çš„åº”ç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œç›¸è¾ƒäºåœ¨è¯­éŸ³ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œç›´æ¥å¯¹åŠ¨ç‰©é¸£å«å£°è¿›è¡Œé¢„è®­ç»ƒçš„SSLæ¨¡å‹ä¼˜åŠ¿å¹¶ä¸æ˜¾è‘—ã€‚æ­¤å¤–ï¼Œå¯¹è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡çš„å¾®è°ƒï¼Œå¯¹äºç”Ÿç‰©å£°éŸ³åˆ†ç±»çš„å¢å¼ºä½œç”¨ä¹Ÿè¡¨ç°ä¸ä¸€ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ç”Ÿç‰©å£°éŸ³æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒåªå¸¦æ¥äº†è½»å¾®çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”åœ¨å¤§å¤šæ•°åœºæ™¯ä¸­ä¸è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹è¡¨ç°ç›¸å½“ã€‚è¿™äº›å‘ç°çªæ˜¾äº†è¯­éŸ³é¢„è®­ç»ƒçš„SSLæ¨¡å‹åœ¨ç”Ÿç‰©å£°éŸ³å­¦ä¸­çš„ç¨³å¥æ€§ï¼Œå¹¶æš—ç¤ºå¯èƒ½ä¸éœ€è¦å¤§é‡å¾®è°ƒå³å¯è·å¾—æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SSLæ¨¡å‹å¯ä»¥ç›´æ¥åº”ç”¨äºåŠ¨ç‰©å£°éŸ³å¤„ç†ï¼Œä½†ç›¸è¾ƒäºè¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹ï¼Œå…¶ä¼˜åŠ¿å¹¶ä¸æ˜¾è‘—ã€‚</li>
<li>åœ¨ç”Ÿç‰©å£°éŸ³æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒç›¸è¾ƒäºè¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹åªå¸¦æ¥äº†è½»å¾®çš„æ€§èƒ½æå‡ã€‚</li>
<li>åœ¨å¤§å¤šæ•°ç”Ÿç‰©å£°éŸ³åˆ†ç±»åœºæ™¯ä¸­ï¼Œè¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹çš„è¡¨ç°å·²ç»è¶³å¤Ÿå¥½ã€‚</li>
<li>å¯¹è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒASRä»»åŠ¡çš„å¾®è°ƒå¯¹äºç”Ÿç‰©å£°éŸ³åˆ†ç±»çš„å¢å¼ºä½œç”¨è¡¨ç°ä¸ä¸€ã€‚</li>
<li>è¯­éŸ³é¢„è®­ç»ƒçš„SSLæ¨¡å‹åœ¨ç”Ÿç‰©å£°éŸ³å­¦ä¸­å…·æœ‰ç¨³å¥æ€§ã€‚</li>
<li>æ— éœ€å¤§é‡å¾®è°ƒå³å¯è·å¾—SSLæ¨¡å‹çš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05987">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-069f65c81fba81350f61aa9661655a36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-415451f2d0c1e922f0ae44348958e598.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34ee1d8f1fb3095aeee384a72b9267da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fd990b3840480ab0de5f59dbab94305.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07c027ae6bd45f3fda2fedb209b08512.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f995ef8a60b1589c3341405f4d99417.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69d11f4db0bbe5739100fc7980de58b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6133af7c0f3dccbba1aea75669da4ca6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Estimation-and-Restoration-of-Unknown-Nonlinear-Distortion-using-Diffusion"><a href="#Estimation-and-Restoration-of-Unknown-Nonlinear-Distortion-using-Diffusion" class="headerlink" title="Estimation and Restoration of Unknown Nonlinear Distortion using   Diffusion"></a>Estimation and Restoration of Unknown Nonlinear Distortion using   Diffusion</h2><p><strong>Authors:Michal Å vento, Eloi Moliner, Lauri Juvela, Alec Wright, Vesa VÃ¤limÃ¤ki</strong></p>
<p>The restoration of nonlinearly distorted audio signals, alongside the identification of the applied memoryless nonlinear operation, is studied. The paper focuses on the difficult but practically important case in which both the nonlinearity and the original input signal are unknown. The proposed method uses a generative diffusion model trained unconditionally on guitar or speech signals to jointly model and invert the nonlinear system at inference time. Both the memoryless nonlinear function model and the restored audio signal are obtained as output. Successful example case studies are presented including inversion of hard and soft clipping, digital quantization, half-wave rectification, and wavefolding nonlinearities. Our results suggest that, out of the nonlinear functions tested here, the cubic Catmull-Rom spline is best suited to approximating these nonlinearities. In the case of guitar recordings, comparisons with informed and supervised methods show that the proposed blind method is at least as good as they are in terms of objective metrics. Experiments on distorted speech show that the proposed blind method outperforms general-purpose speech enhancement techniques and restores the original voice quality. The proposed method can be applied to audio effects modeling, restoration of music and speech recordings, and characterization of analog recording media. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†éçº¿æ€§å¤±çœŸéŸ³é¢‘ä¿¡å·çš„æ¢å¤ä»¥åŠæ‰€åº”ç”¨çš„æ— è®°å¿†éçº¿æ€§æ“ä½œè¯†åˆ«ã€‚æ–‡ç« é‡ç‚¹å…³æ³¨åœ¨å®é™…åº”ç”¨ä¸­éå¸¸é‡è¦ä½†éš¾åº¦è¾ƒå¤§çš„æƒ…å†µï¼Œå³éçº¿æ€§å’ŒåŸå§‹è¾“å…¥ä¿¡å·å‡æœªçŸ¥ã€‚æå‡ºçš„æ–¹æ³•ä½¿ç”¨äº†ä¸€ä¸ªåœ¨å‰ä»–æˆ–è¯­éŸ³ä¿¡å·ä¸Šæ— æ¡ä»¶è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œåœ¨æ¨ç†æ—¶é—´è”åˆå»ºæ¨¡å¹¶åè½¬éçº¿æ€§ç³»ç»Ÿã€‚è¾“å‡ºä¸­åŒ…å«æ— è®°å¿†éçº¿æ€§å‡½æ•°æ¨¡å‹å’Œæ¢å¤çš„éŸ³é¢‘ä¿¡å·ã€‚æˆåŠŸçš„æ¡ˆä¾‹ç ”ç©¶åŒ…æ‹¬ç¡¬è£å‰ªã€è½¯è£å‰ªã€æ•°å­—é‡åŒ–ã€åŠæ³¢æ•´æµå’Œæ³¢å½¢æŠ˜å éçº¿æ€§çš„åè½¬ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨æ‰€æµ‹è¯•çš„éçº¿æ€§å‡½æ•°ä¸­ï¼Œç«‹æ–¹Catmull-Romæ ·æ¡æœ€é€‚åˆé€¼è¿‘è¿™äº›éçº¿æ€§ã€‚åœ¨å‰ä»–å½•éŸ³çš„æƒ…å†µä¸‹ï¼Œä¸æœ‰ç›‘ç£æ–¹æ³•çš„æ¯”è¾ƒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç›²æ–¹æ³•åœ¨å®¢è§‚æŒ‡æ ‡æ–¹é¢è‡³å°‘ä¸è¿™äº›æ–¹æ³•ä¸€æ ·å¥½ã€‚å¯¹å¤±çœŸè¯­éŸ³çš„å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„ç›²æ–¹æ³•ä¼˜äºé€šç”¨è¯­éŸ³å¢å¼ºæŠ€æœ¯ï¼Œå¹¶èƒ½æ¢å¤åŸå§‹è¯­éŸ³è´¨é‡ã€‚è¯¥æ–¹æ³•å¯åº”ç”¨äºéŸ³é¢‘æ•ˆæœå»ºæ¨¡ã€éŸ³ä¹å’Œè¯­éŸ³è®°å½•çš„æ¢å¤ä»¥åŠæ¨¡æ‹Ÿå½•éŸ³ä»‹è´¨çš„è¡¨å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05959v1">PDF</a> Submitted to the Journal of Audio Engineering Society, special issue   â€œThe Sound of Digital Audio Effectsâ€</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ç ”ç©¶çš„æ˜¯éçº¿æ€§å¤±çœŸéŸ³é¢‘ä¿¡å·çš„æ¢å¤ï¼Œä»¥åŠè®°å¿†æ€§éçº¿æ€§æ“ä½œçš„åº”ç”¨è¯†åˆ«ã€‚æ–‡ç« å…³æ³¨åœ¨æœªçŸ¥éçº¿æ€§å’ŒåŸå§‹è¾“å…¥ä¿¡å·çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨æ— æ¡ä»¶è®­ç»ƒåœ¨å‰ä»–æˆ–è¯­éŸ³ä¿¡å·ä¸Šçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼Œå¯¹éçº¿æ€§ç³»ç»Ÿè¿›è¡Œè”åˆå»ºæ¨¡å’Œåå‘è¿ç®—ã€‚æ­¤æ–¹æ³•å¯å¾—åˆ°è®°å¿†æ€§éçº¿æ€§å‡½æ•°æ¨¡å‹å’Œæ¢å¤çš„éŸ³é¢‘ä¿¡å·ã€‚æˆåŠŸåº”ç”¨æ¡ˆä¾‹åŒ…æ‹¬ç¡¬è£å‰ªã€è½¯è£å‰ªã€æ•°å­—é‡åŒ–ã€åŠæ³¢æ•´æµå’Œæ³¢æŠ˜å éçº¿æ€§çš„åå‘è¿ç®—ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ‰€æµ‹è¯•çš„éçº¿æ€§å‡½æ•°ä¸­ï¼ŒCatmull-Romä¸‰æ¬¡æ ·æ¡æ›²çº¿æœ€é€‚åˆè¿‘ä¼¼è¿™äº›éçº¿æ€§ã€‚åœ¨å‰ä»–å½•éŸ³æ¡ˆä¾‹ä¸­ï¼Œä¸æœ‰ç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼Œæ­¤ç›²æ–¹æ³•åœ¨å®¢è§‚æŒ‡æ ‡ä¸Šè¡¨ç°ç›¸å½“ã€‚åœ¨å¤±çœŸè¯­éŸ³å®éªŒä¸­ï¼Œæ­¤ç›²æ–¹æ³•ä¼˜äºé€šç”¨è¯­éŸ³å¢å¼ºæŠ€æœ¯ï¼Œå¹¶èƒ½æ¢å¤åŸå§‹è¯­éŸ³è´¨é‡ã€‚æ­¤æ–¹æ³•å¯åº”ç”¨äºéŸ³é¢‘æ•ˆæœå»ºæ¨¡ã€éŸ³ä¹å’Œè¯­éŸ³è®°å½•çš„æ¢å¤ä»¥åŠæ¨¡æ‹Ÿå½•éŸ³åª’ä½“çš„ç‰¹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ç ”ç©¶äº†éçº¿æ€§å¤±çœŸéŸ³é¢‘ä¿¡å·çš„æ¢å¤å’Œè®°å¿†æ€§éçº¿æ€§æ“ä½œçš„åº”ç”¨è¯†åˆ«ã€‚</li>
<li>åœ¨æœªçŸ¥éçº¿æ€§å’ŒåŸå§‹è¾“å…¥ä¿¡å·çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹è¿›è¡Œå»ºæ¨¡å’Œåå‘è¿ç®—ã€‚</li>
<li>æˆåŠŸåº”ç”¨æ¡ˆä¾‹åŒ…æ‹¬å¤šç§éçº¿æ€§æ“ä½œçš„åå‘è¿ç®—ï¼Œå¦‚ç¡¬è£å‰ªã€è½¯è£å‰ªç­‰ã€‚</li>
<li>Catmull-Romä¸‰æ¬¡æ ·æ¡æ›²çº¿æœ€é€‚åˆè¿‘ä¼¼è¿™äº›éçº¿æ€§ã€‚</li>
<li>åœ¨å‰ä»–å½•éŸ³æ¡ˆä¾‹ä¸­ï¼Œç›²æ–¹æ³•ä¸æœ‰ç›‘ç£æ–¹æ³•è¡¨ç°ç›¸å½“ã€‚</li>
<li>åœ¨å¤±çœŸè¯­éŸ³å®éªŒä¸­ï¼Œç›²æ–¹æ³•ä¼˜äºé€šç”¨è¯­éŸ³å¢å¼ºæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a06c63ed451b278c018a126d326fce34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a7d2affaee98185802554607a99d2d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b8c318610eb36b9f34511e1e4c843e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85b19dcb620133df56aa85f7fa707f33.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Unmasking-Deepfakes-Leveraging-Augmentations-and-Features-Variability-for-Deepfake-Speech-Detection"><a href="#Unmasking-Deepfakes-Leveraging-Augmentations-and-Features-Variability-for-Deepfake-Speech-Detection" class="headerlink" title="Unmasking Deepfakes: Leveraging Augmentations and Features Variability   for Deepfake Speech Detection"></a>Unmasking Deepfakes: Leveraging Augmentations and Features Variability   for Deepfake Speech Detection</h2><p><strong>Authors:Inbal Rimon, Oren Gal, Haim Permuter</strong></p>
<p>The detection of deepfake speech has become increasingly challenging with the rapid evolution of deepfake technologies. In this paper, we propose a hybrid architecture for deepfake speech detection, combining a self-supervised learning framework for feature extraction with a classifier head to form an end-to-end model. Our approach incorporates both audio-level and feature-level augmentation techniques. Specifically, we introduce and analyze various masking strategies for augmenting raw audio spectrograms and for enhancing feature representations during training. We incorporate compression augmentations during the pretraining phase of the feature extractor to address the limitations of small, single-language datasets. We evaluate the model on the ASVSpoof5 (ASVSpoof 2024) challenge, achieving state-of-the-art results in Track 1 under closed conditions with an Equal Error Rate of 4.37%. By employing different pretrained feature extractors, the model achieves an enhanced EER of 3.39%. Our model demonstrates robust performance against unseen deepfake attacks and exhibits strong generalization across different codecs. </p>
<blockquote>
<p>éšç€æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œæ·±åº¦ä¼ªé€ è¯­éŸ³çš„æ£€æµ‹è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹çš„æ··åˆæ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶è¿›è¡Œç‰¹å¾æå–å’Œåˆ†ç±»å™¨å¤´ä»¥å½¢æˆç«¯åˆ°ç«¯æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†éŸ³é¢‘çº§åˆ«å’Œç‰¹å¾çº§åˆ«çš„å¢å¼ºæŠ€æœ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥å¹¶åˆ†æäº†å„ç§æ©è”½ç­–ç•¥ï¼Œä»¥å¢å¼ºåŸå§‹éŸ³é¢‘é¢‘è°±å›¾å¹¶åœ¨è®­ç»ƒæœŸé—´ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨ç‰¹å¾æå–å™¨çš„é¢„è®­ç»ƒé˜¶æ®µå¼•å…¥äº†å‹ç¼©å¢å¼ºæŠ€æœ¯ï¼Œä»¥è§£å†³å°å‹å•ä¸€è¯­è¨€æ•°æ®é›†çš„é™åˆ¶ã€‚æˆ‘ä»¬åœ¨ASVSpoof5ï¼ˆASVSpoof 2024ï¼‰æŒ‘æˆ˜ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œåœ¨å°é—­æ¡ä»¶ä¸‹çš„è½¨é“1ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œç­‰è¯¯ç‡ï¼ˆEqual Error Rateï¼‰ä¸º4.37%ã€‚é€šè¿‡é‡‡ç”¨ä¸åŒçš„é¢„è®­ç»ƒç‰¹å¾æå–å™¨ï¼Œæ¨¡å‹çš„EERæå‡è‡³äº†3.39%ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯¹æœªè§è¿‡çš„æ·±åº¦ä¼ªé€ æ”»å‡»è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„ç¼–è§£ç å™¨ä¹‹é—´å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05545v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œæ·±åº¦ä¼ªé€ è¯­éŸ³çš„æ£€æµ‹é¢ä¸´è¶Šæ¥è¶Šå¤§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆæ¶æ„çš„æ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆç»“åˆè‡ªç›‘ç£å­¦ä¹ æ¡†æ¶è¿›è¡Œç‰¹å¾æå–å’Œåˆ†ç±»å™¨å¤´éƒ¨ä»¥å½¢æˆç«¯åˆ°ç«¯æ¨¡å‹ã€‚è¯¥æ–¹æ³•ç»“åˆäº†éŸ³é¢‘çº§åˆ«å’Œç‰¹å¾çº§åˆ«çš„å¢å¼ºæŠ€æœ¯ï¼Œå¼•å…¥å¹¶åˆ†æäº†å„ç§æ©ç ç­–ç•¥æ¥å¢å¼ºåŸå§‹éŸ³é¢‘é¢‘è°±å›¾ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ”¹å–„ç‰¹å¾è¡¨ç¤ºã€‚é’ˆå¯¹å°è¯­ç§å•ä¸€æ•°æ®é›†çš„é™åˆ¶ï¼Œæˆ‘ä»¬åœ¨ç‰¹å¾æå–å™¨çš„é¢„è®­ç»ƒé˜¶æ®µå¼•å…¥äº†å‹ç¼©å¢å¼ºæŠ€æœ¯ã€‚åœ¨ASVSpoof5ï¼ˆASVSpoof 2024ï¼‰æŒ‘æˆ˜ä¸Šè¯„ä¼°è¯¥æ¨¡å‹ï¼Œåœ¨å°é—­æ¡ä»¶ä¸‹è½¨è¿¹1ä¸­å–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœï¼Œç­‰è¯¯ç ç‡ä¸º4.37%ã€‚é€šè¿‡ä½¿ç”¨ä¸åŒçš„é¢„è®­ç»ƒç‰¹å¾æå–å™¨ï¼Œè¯¥æ¨¡å‹çš„ç­‰è¯¯ç ç‡è¿›ä¸€æ­¥æé«˜è‡³3.39%ã€‚è¯¥æ¨¡å‹å¯¹æœªçŸ¥çš„æ·±åº¦ä¼ªé€ æ”»å‡»è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä¸åŒçš„ç¼–è§£ç å™¨ä¹‹é—´å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ä¼ªé€ è¯­éŸ³æ£€æµ‹é¢ä¸´æŒ‘æˆ˜ï¼šéšç€æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œç°æœ‰çš„è¯­éŸ³æ£€æµ‹æ–¹æ¡ˆå·²æ— æ³•æ»¡è¶³éœ€æ±‚ã€‚</li>
<li>æ··åˆæ¶æ„æ–¹æ¡ˆï¼šç»“åˆè‡ªç›‘ç£å­¦ä¹ æ¡†æ¶è¿›è¡Œç‰¹å¾æå–å’Œåˆ†ç±»å™¨å¤´éƒ¨ï¼Œå½¢æˆç«¯åˆ°ç«¯æ¨¡å‹ã€‚</li>
<li>éŸ³é¢‘å’Œç‰¹å¾çº§åˆ«å¢å¼ºï¼šå¼•å…¥éŸ³é¢‘çº§åˆ«å’Œç‰¹å¾çº§åˆ«çš„å¢å¼ºæŠ€æœ¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ©ç ç­–ç•¥çš„åº”ç”¨ï¼šé€šè¿‡æ©ç ç­–ç•¥å¢å¼ºåŸå§‹éŸ³é¢‘é¢‘è°±å›¾ï¼Œæ”¹å–„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>é¢„è®­ç»ƒé˜¶æ®µçš„å‹ç¼©å¢å¼ºï¼šé’ˆå¯¹å°è¯­ç§å•ä¸€æ•°æ®é›†çš„å±€é™æ€§ï¼Œå¼•å…¥å‹ç¼©å¢å¼ºæŠ€æœ¯ã€‚</li>
<li>æ¨¡å‹åœ¨ASVSpoof5æŒ‘æˆ˜ä¸Šè¡¨ç°ä¼˜å¼‚ï¼šåœ¨å°é—­æ¡ä»¶ä¸‹è½¨è¿¹1çš„ç­‰è¯¯ç ç‡è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-71412a311f31d28bf7b26212521158c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-199d18f2078e408b0ab34dc1993792d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6564fc8e4fa3ed8b34b51dccc7a1f7aa.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ZipEnhancer-Dual-Path-Down-Up-Sampling-based-Zipformer-for-Monaural-Speech-Enhancement"><a href="#ZipEnhancer-Dual-Path-Down-Up-Sampling-based-Zipformer-for-Monaural-Speech-Enhancement" class="headerlink" title="ZipEnhancer: Dual-Path Down-Up Sampling-based Zipformer for Monaural   Speech Enhancement"></a>ZipEnhancer: Dual-Path Down-Up Sampling-based Zipformer for Monaural   Speech Enhancement</h2><p><strong>Authors:Haoxu Wang, Biao Tian</strong></p>
<p>In contrast to other sequence tasks modeling hidden layer features with three axes, Dual-Path time and time-frequency domain speech enhancement models are effective and have low parameters but are computationally demanding due to their hidden layer features with four axes. We propose ZipEnhancer, which is Dual-Path Down-Up Sampling-based Zipformer for Monaural Speech Enhancement, incorporating time and frequency domain Down-Up sampling to reduce computational costs. We introduce the ZipformerBlock as the core block and propose the design of the Dual-Path DownSampleStacks that symmetrically scale down and scale up. Also, we introduce the ScaleAdam optimizer and Eden learning rate scheduler to improve the performance further. Our model achieves new state-of-the-art results on the DNS 2020 Challenge and Voicebank+DEMAND datasets, with a perceptual evaluation of speech quality (PESQ) of 3.69 and 3.63, using 2.04M parameters and 62.41G FLOPS, outperforming other methods with similar complexity levels. </p>
<blockquote>
<p>ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…¶ä»–åºåˆ—ä»»åŠ¡æ¨¡å‹ä½¿ç”¨å…·æœ‰ä¸‰ä¸ªè½´çš„ç‰¹å¾è¿›è¡Œéšè—å±‚å»ºæ¨¡ï¼Œè€ŒåŒè·¯å¾„æ—¶é—´å’Œæ—¶é¢‘åŸŸè¯­éŸ³å¢å¼ºæ¨¡å‹è™½ç„¶æœ‰æ•ˆä¸”å‚æ•°è¾ƒå°‘ï¼Œä½†ç”±äºå…¶å…·æœ‰å››ä¸ªè½´çš„éšè—å±‚ç‰¹å¾è€Œè®¡ç®—é‡è¾ƒå¤§ã€‚æˆ‘ä»¬æå‡ºäº†ZipEnhancerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåŒè·¯å¾„å‡é™é‡‡æ ·çš„Zipformerå•å£°é“è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œå®ƒç»“åˆäº†æ—¶é—´å’Œé¢‘åŸŸçš„å‡é™é‡‡æ ·ä»¥é™ä½è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å¼•å…¥äº†ZipformerBlockä½œä¸ºæ ¸å¿ƒæ¨¡å—ï¼Œå¹¶æå‡ºäº†åŒè·¯å¾„DownSampleStacksçš„è®¾è®¡ï¼Œå®ç°äº†å¯¹ç§°çš„ç¼©å°å’Œæ”¾å¤§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ScaleAdamä¼˜åŒ–å™¨å’ŒEdenå­¦ä¹ ç‡è°ƒåº¦å™¨ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨DNS 2020æŒ‘æˆ˜èµ›å’ŒVoicebank+DEMANDæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œä½¿ç”¨204ä¸‡å‚æ•°å’Œ62.41G FLOPSçš„æ„ŸçŸ¥è¯­éŸ³è´¨é‡è¯„ä¼°ï¼ˆPESQï¼‰ä¸º3.69å’Œ3.63ï¼Œä¼˜äºå…¶ä»–å…·æœ‰ç›¸ä¼¼å¤æ‚åº¦çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05183v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Dual-Pathæ—¶åŸŸå’Œæ—¶é¢‘åŸŸè¯­éŸ³å¢å¼ºæ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œå…¶éšè—å±‚ç‰¹å¾å…·æœ‰å››è½´ï¼Œä½†å‚æ•°è¾ƒå°‘ä¸”è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚ä¸ºè§£å†³è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ZipEnhanceræ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºåŒè·¯å¾„ä¸Šä¸‹é‡‡æ ·æŠ€æœ¯çš„Zipformerè®¾è®¡ï¼Œç”¨äºå•å£°è¯­éŸ³å¢å¼ºã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†ZipformerBlockä½œä¸ºæ ¸å¿ƒç»„ä»¶å’ŒåŒè·¯å¾„ä¸‹é‡‡æ ·å †æ ˆçš„å¯¹ç§°æ‰©å±•ä¸ç¼©å°è®¾è®¡ã€‚é€šè¿‡å¼•å…¥ScaleAdamä¼˜åŒ–å™¨å’ŒEdenå­¦ä¹ ç‡è°ƒåº¦å™¨è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨DNS 2020æŒ‘æˆ˜å’ŒVoicebank+DEMANDæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°çš„æœ€å¥½ç»“æœï¼Œå®ç°äº†è‰¯å¥½çš„è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä»·ï¼ˆPESQï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Dual-Pathæ—¶åŸŸå’Œæ—¶é¢‘åŸŸè¯­éŸ³å¢å¼ºæ¨¡å‹å…·æœ‰å››è½´éšè—å±‚ç‰¹å¾ï¼Œç›¸è¾ƒäºå…¶ä»–åºåˆ—ä»»åŠ¡å»ºæ¨¡æ–¹æ³•æ›´æœ‰æ•ˆä¸”å‚æ•°æ›´å°‘ã€‚</li>
<li>ZipEnhanceræ¨¡å‹é€šè¿‡å¼•å…¥åŒè·¯å¾„ä¸Šä¸‹é‡‡æ ·æŠ€æœ¯çš„Zipformerè®¾è®¡ï¼Œè§£å†³äº†è®¡ç®—æˆæœ¬è¾ƒé«˜çš„é—®é¢˜ã€‚</li>
<li>ZipformerBlockè¢«å¼•å…¥ä½œä¸ºæ ¸å¿ƒç»„ä»¶ï¼ŒåŒè·¯å¾„ä¸‹é‡‡æ ·å †æ ˆè®¾è®¡å®ç°äº†å¯¹ç§°çš„ç¼©å°å’Œæ‰©å±•ã€‚</li>
<li>ScaleAdamä¼˜åŒ–å™¨å’ŒEdenå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å¼•å…¥è¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨DNS 2020æŒ‘æˆ˜å’ŒVoicebank+DEMANDæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°çš„æœ€å¥½ç»“æœã€‚</li>
<li>è¯¥æ¨¡å‹çš„è¯­éŸ³è´¨é‡æ„ŸçŸ¥è¯„ä»·ï¼ˆPESQï¼‰è¾¾åˆ°äº†è¾ƒé«˜çš„æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0c1a2faa867f2867e09132afb6e62c73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-efc249082eea92db8bdfc91994b881af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5bd51e75d050dc76d4ff2f5c874b5a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cde91761bc5274790dc8eace74d01e20.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="JELLY-Joint-Emotion-Recognition-and-Context-Reasoning-with-LLMs-for-Conversational-Speech-Synthesis"><a href="#JELLY-Joint-Emotion-Recognition-and-Context-Reasoning-with-LLMs-for-Conversational-Speech-Synthesis" class="headerlink" title="JELLY: Joint Emotion Recognition and Context Reasoning with LLMs for   Conversational Speech Synthesis"></a>JELLY: Joint Emotion Recognition and Context Reasoning with LLMs for   Conversational Speech Synthesis</h2><p><strong>Authors:Jun-Hyeok Cha, Seung-Bin Kim, Hyung-Seok Oh, Seong-Whan Lee</strong></p>
<p>Recently, there has been a growing demand for conversational speech synthesis (CSS) that generates more natural speech by considering the conversational context. To address this, we introduce JELLY, a novel CSS framework that integrates emotion recognition and context reasoning for generating appropriate speech in conversation by fine-tuning a large language model (LLM) with multiple partial LoRA modules. We propose an Emotion-aware Q-former encoder, which enables the LLM to perceive emotions in speech. The encoder is trained to align speech emotions with text, utilizing datasets of emotional speech. The entire model is then fine-tuned with conversational speech data to infer emotional context for generating emotionally appropriate speech in conversation. Our experimental results demonstrate that JELLY excels in emotional context modeling, synthesizing speech that naturally aligns with conversation, while mitigating the scarcity of emotional conversational speech datasets. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¯¹è¯å¼è¯­éŸ³åˆæˆï¼ˆCSSï¼‰çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œå®ƒå¯ä»¥é€šè¿‡è€ƒè™‘å¯¹è¯ä¸Šä¸‹æ–‡æ¥ç”Ÿæˆæ›´è‡ªç„¶çš„è¯­éŸ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†JELLYï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„CSSæ¡†æ¶ï¼Œé€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¶é›†æˆå¤šä¸ªå±€éƒ¨LoRAæ¨¡å—ï¼Œç”¨äºåœ¨å¯¹è¯ä¸­ç”Ÿæˆé€‚å½“çš„è¯­éŸ³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æƒ…æ„Ÿæ„ŸçŸ¥Q-formerç¼–ç å™¨ï¼Œä½¿LLMèƒ½å¤Ÿæ„ŸçŸ¥è¯­éŸ³ä¸­çš„æƒ…æ„Ÿã€‚è¯¥ç¼–ç å™¨ç»è¿‡è®­ç»ƒï¼Œèƒ½å¤Ÿå°†è¯­éŸ³æƒ…æ„Ÿä¸æ–‡æœ¬å¯¹é½ï¼Œåˆ©ç”¨æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†ã€‚ç„¶åå¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¯¹è¯è¯­éŸ³æ•°æ®çš„å¾®è°ƒï¼Œä»¥æ¨æ–­æƒ…æ„Ÿä¸Šä¸‹æ–‡ï¼Œä»è€Œåœ¨å¯¹è¯ä¸­ç”Ÿæˆæƒ…æ„Ÿä¸Šé€‚å½“çš„è¯­éŸ³ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒJELLYåœ¨æƒ…æ„Ÿä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåˆæˆä¸å¯¹è¯è‡ªç„¶å¯¹é½çš„è¯­éŸ³ï¼ŒåŒæ—¶ç¼“è§£äº†æƒ…æ„Ÿå¯¹è¯è¯­éŸ³æ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04904v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¯¹è¯å¼è¯­éŸ³åˆæˆï¼ˆCSSï¼‰éœ€æ±‚å¢é•¿ï¼Œè€ƒè™‘å¯¹è¯è¯­å¢ƒç”Ÿæˆæ›´è‡ªç„¶çš„è¯­éŸ³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºJELLYï¼Œä¸€ç§æ–°å‹CSSæ¡†æ¶ï¼Œé€šè¿‡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¶ç»“åˆå¤šä¸ªå±€éƒ¨å¯è°ƒæ•´ï¼ˆLoRAï¼‰æ¨¡å—ï¼Œç”Ÿæˆå¯¹è¯ä¸­é€‚å½“çš„è¯­éŸ³ã€‚æˆ‘ä»¬æå‡ºæƒ…æ„Ÿæ„ŸçŸ¥Q-formerç¼–ç å™¨ï¼Œä½¿LLMèƒ½å¤Ÿæ„ŸçŸ¥è¯­éŸ³ä¸­çš„æƒ…æ„Ÿã€‚ç¼–ç å™¨ç»è¿‡è®­ç»ƒï¼Œä½¿è¯­éŸ³æƒ…æ„Ÿä¸æ–‡æœ¬å¯¹é½ï¼Œå¹¶åˆ©ç”¨æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†ã€‚æ•´ä¸ªæ¨¡å‹è¿›ä¸€æ­¥ç”¨å¯¹è¯è¯­éŸ³æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»¥æ¨æ–­æƒ…æ„Ÿè¯­å¢ƒï¼Œç”Ÿæˆå¯¹è¯ä¸­æƒ…æ„Ÿä¸Šé€‚å½“çš„è¯­éŸ³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJELLYåœ¨æƒ…æ„Ÿè¯­å¢ƒå»ºæ¨¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåˆæˆä¸å¯¹è¯è‡ªç„¶å¯¹é½çš„è¯­éŸ³ï¼ŒåŒæ—¶ç¼“è§£æƒ…æ„Ÿå¯¹è¯è¯­éŸ³æ•°æ®é›†ç¼ºä¹çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>JELLYæ˜¯ä¸€ä¸ªæ–°å‹çš„å¯¹è¯å¼è¯­éŸ³åˆæˆï¼ˆCSSï¼‰æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆæ›´è‡ªç„¶çš„è¯­éŸ³ã€‚</li>
<li>å®ƒé€šè¿‡æ•´åˆæƒ…æ„Ÿè¯†åˆ«å’Œè¯­å¢ƒæ¨ç†æ¥é€‚åº”å¯¹è¯è¯­å¢ƒã€‚</li>
<li>JELLYä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¶ç»“åˆå¤šä¸ªå±€éƒ¨å¯è°ƒæ•´ï¼ˆLoRAï¼‰æ¨¡å—æ¥å®ç°ã€‚</li>
<li>æå‡ºäº†æƒ…æ„Ÿæ„ŸçŸ¥Q-formerç¼–ç å™¨ï¼Œä½¿LLMèƒ½æ„ŸçŸ¥è¯­éŸ³ä¸­çš„æƒ…æ„Ÿã€‚</li>
<li>ç¼–ç å™¨ç»è¿‡è®­ç»ƒï¼Œå°†è¯­éŸ³æƒ…æ„Ÿä¸æ–‡æœ¬å¯¹é½ï¼Œåˆ©ç”¨æƒ…æ„Ÿè¯­éŸ³æ•°æ®é›†ã€‚</li>
<li>æ•´ä¸ªæ¨¡å‹ç”¨å¯¹è¯è¯­éŸ³æ•°æ®å¾®è°ƒï¼Œä»¥æ¨æ–­æƒ…æ„Ÿè¯­å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04904">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a5dd8467341555d03e377166d4f53cfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-505768a7af8b9a4f17eba5b2df8da603.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f0a7b437f5523ad6388d50483a3e85f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-314c4671b2d44201482c293b8593039e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-Listened-Speech-Decoding-from-EEG-via-Parallel-Phoneme-Sequence-Prediction"><a href="#Enhancing-Listened-Speech-Decoding-from-EEG-via-Parallel-Phoneme-Sequence-Prediction" class="headerlink" title="Enhancing Listened Speech Decoding from EEG via Parallel Phoneme   Sequence Prediction"></a>Enhancing Listened Speech Decoding from EEG via Parallel Phoneme   Sequence Prediction</h2><p><strong>Authors:Jihwan Lee, Tiantian Feng, Aditya Kommineni, Sudarsana Reddy Kadiri, Shrikanth Narayanan</strong></p>
<p>Brain-computer interfaces (BCI) offer numerous human-centered application possibilities, particularly affecting people with neurological disorders. Text or speech decoding from brain activities is a relevant domain that could augment the quality of life for people with impaired speech perception. We propose a novel approach to enhance listened speech decoding from electroencephalography (EEG) signals by utilizing an auxiliary phoneme predictor that simultaneously decodes textual phoneme sequences. The proposed model architecture consists of three main parts: EEG module, speech module, and phoneme predictor. The EEG module learns to properly represent EEG signals into EEG embeddings. The speech module generates speech waveforms from the EEG embeddings. The phoneme predictor outputs the decoded phoneme sequences in text modality. Our proposed approach allows users to obtain decoded listened speech from EEG signals in both modalities (speech waveforms and textual phoneme sequences) simultaneously, eliminating the need for a concatenated sequential pipeline for each modality. The proposed approach also outperforms previous methods in both modalities. The source code and speech samples are publicly available. </p>
<blockquote>
<p>è„‘æœºæ¥å£ï¼ˆBCIï¼‰æä¾›äº†è®¸å¤šä»¥äººç±»ä¸ºä¸­å¿ƒçš„åº”ç”¨å¯èƒ½æ€§ï¼Œå°¤å…¶å¯¹ç¥ç»éšœç¢æ‚£è€…äº§ç”Ÿäº†å½±å“ã€‚ä»è„‘æ´»åŠ¨ä¸­è§£ç æ–‡æœ¬æˆ–è¯­éŸ³æ˜¯ä¸€ä¸ªç›¸å…³åŸŸï¼Œè¿™å¯èƒ½æé«˜è¯­éŸ³æ„ŸçŸ¥å—æŸæ‚£è€…çš„ç”Ÿæ´»è´¨é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è¾…åŠ©éŸ³ç´ é¢„æµ‹å™¨åŒæ—¶è§£ç æ–‡æœ¬éŸ³ç´ åºåˆ—çš„æ–°æ–¹æ³•ï¼Œä»¥å¢å¼ºä»è„‘ç”µå›¾ï¼ˆEEGï¼‰ä¿¡å·ä¸­å¬å–çš„è¯­éŸ³è§£ç æ•ˆæœã€‚æ‰€æè®®çš„æ¨¡å‹æ¶æ„ç”±ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šEEGæ¨¡å—ã€è¯­éŸ³æ¨¡å—å’ŒéŸ³ç´ é¢„æµ‹å™¨ã€‚EEGæ¨¡å—å­¦ä¹ é€‚å½“åœ°è¡¨ç¤ºEEGä¿¡å·ä¸ºEEGåµŒå…¥ã€‚è¯­éŸ³æ¨¡å—ä»EEGåµŒå…¥ç”Ÿæˆè¯­éŸ³æ³¢å½¢ã€‚éŸ³ç´ é¢„æµ‹å™¨è¾“å‡ºè§£ç çš„éŸ³ç´ åºåˆ—åœ¨æ–‡æœ¬æ¨¡å¼ä¸‹ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å…è®¸ç”¨æˆ·åŒæ—¶ä»EEGä¿¡å·ä¸­è·å¾—ä¸¤ç§æ¨¡å¼çš„è§£ç è¯­éŸ³ï¼ˆè¯­éŸ³æ³¢å½¢å’Œæ–‡æœ¬éŸ³ç´ åºåˆ—ï¼‰ï¼Œè€Œæ— éœ€ä¸ºæ¯ä¸ªæ¨¡å¼ä½¿ç”¨ä¸²è”çš„åºè´¯ç®¡é“ã€‚è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæ¨¡æ€ä¸Šçš„è¡¨ç°å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚æºä»£ç å’Œè¯­éŸ³æ ·æœ¬å¯å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04844v1">PDF</a> ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨è„‘æœºæ¥å£ï¼ˆBCIï¼‰æŠ€æœ¯å¢å¼ºè†å¬è¯­éŸ³è§£ç çš„æ–°æ–¹æ³•ã€‚æ–°æ–¹æ³•åˆ©ç”¨è„‘ç”µå›¾ï¼ˆEEGï¼‰ä¿¡å·ï¼Œé€šè¿‡è¾…åŠ©éŸ³ç´ é¢„æµ‹å™¨åŒæ—¶è§£ç æ–‡æœ¬éŸ³ç´ åºåˆ—ï¼Œä»è€Œæé«˜è¯­éŸ³è§£ç çš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬EEGæ¨¡å—ã€è¯­éŸ³æ¨¡å—å’ŒéŸ³ç´ é¢„æµ‹å™¨ä¸‰ä¸ªéƒ¨åˆ†ï¼Œå¯åŒæ—¶è¾“å‡ºè¯­éŸ³æ³¢å½¢å’Œæ–‡æœ¬éŸ³ç´ åºåˆ—ä¸¤ç§æ¨¡æ€çš„è§£ç è¯­éŸ³ã€‚ç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæ¨¡æ€ä¸Šéƒ½è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BCIæŠ€æœ¯åœ¨äººç±»ä¸ºä¸­å¿ƒçš„åº”ç”¨ä¸­å…·æœ‰å¹¿é˜”çš„å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¸®åŠ©å…·æœ‰ç¥ç»éšœç¢çš„äººç¾¤æ–¹é¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨EEGä¿¡å·å’Œè¾…åŠ©éŸ³ç´ é¢„æµ‹å™¨å¢å¼ºè¯­éŸ³è§£ç ã€‚</li>
<li>æ–°æ–¹æ³•åŒ…æ‹¬EEGæ¨¡å—ã€è¯­éŸ³æ¨¡å—å’ŒéŸ³ç´ é¢„æµ‹å™¨ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥åŒæ—¶åœ¨ä¸¤ç§æ¨¡æ€ï¼ˆè¯­éŸ³æ³¢å½¢å’Œæ–‡æœ¬éŸ³ç´ åºåˆ—ï¼‰ä¸Šè¾“å‡ºè§£ç è¯­éŸ³ã€‚</li>
<li>ä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ–°æ–¹æ³•åœ¨ä¸¤ç§æ¨¡æ€ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•çš„æºä»£ç å’Œè¯­éŸ³æ ·æœ¬å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-475b8868ff70324a00472d1b6ca6d1cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-427c6567680bffc6fc3c03b6a5eb5009.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-632d77e8bc96a0d6a692f920b331bb5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c8644822c6e720c0eb9b35b8a41dad5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce1aad9599fccbb5107bb20bd0e3a2a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e69e6811343eb91b48b84f35e850e497.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Building-Foundations-for-Natural-Language-Processing-of-Historical-Turkish-Resources-and-Models"><a href="#Building-Foundations-for-Natural-Language-Processing-of-Historical-Turkish-Resources-and-Models" class="headerlink" title="Building Foundations for Natural Language Processing of Historical   Turkish: Resources and Models"></a>Building Foundations for Natural Language Processing of Historical   Turkish: Resources and Models</h2><p><strong>Authors:Åaziye BetÃ¼l Ã–zateÅŸ, TarÄ±k Emre TÄ±raÅŸ, Ece Elif Adak, Berat DoÄŸan, Fatih Burak KaragÃ¶z, Efe Eren GenÃ§, Esma F. Bilgin TaÅŸdemir</strong></p>
<p>This paper introduces foundational resources and models for natural language processing (NLP) of historical Turkish, a domain that has remained underexplored in computational linguistics. We present the first named entity recognition (NER) dataset, HisTR and the first Universal Dependencies treebank, OTA-BOUN for a historical form of the Turkish language along with transformer-based models trained using these datasets for named entity recognition, dependency parsing, and part-of-speech tagging tasks. Additionally, we introduce Ottoman Text Corpus (OTC), a clean corpus of transliterated historical Turkish texts that spans a wide range of historical periods. Our experimental results show significant improvements in the computational analysis of historical Turkish, achieving promising results in tasks that require understanding of historical linguistic structures. They also highlight existing challenges, such as domain adaptation and language variations across time periods. All of the presented resources and models are made available at <a target="_blank" rel="noopener" href="https://huggingface.co/bucolin">https://huggingface.co/bucolin</a> to serve as a benchmark for future progress in historical Turkish NLP. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹è®¡ç®—è¯­è¨€å­¦ä¸­å°šæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸâ€”â€”å†å²åœŸè€³å…¶è¯­çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„åŸºç¡€èµ„æºå’Œæ¨¡å‹ã€‚æˆ‘ä»¬é¦–æ¬¡æ¨å‡ºäº†å†å²åœŸè€³å…¶è¯­å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ•°æ®é›†HisTRå’Œé€šç”¨ä¾å­˜å…³ç³»æ ‘åº“OTA-BOUNï¼Œä»¥åŠä½¿ç”¨è¿™äº›æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œç”¨äºå‘½åå®ä½“è¯†åˆ«ã€ä¾å­˜å…³ç³»è§£æå’Œè¯æ€§æ ‡æ³¨ä»»åŠ¡çš„åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†å¥¥æ–¯æ›¼æ–‡æœ¬è¯­æ–™åº“ï¼ˆOTCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨è¶Šå¹¿æ³›å†å²æ—¶æœŸçš„è½¬å†™å†å²åœŸè€³å…¶è¯­æ–‡æœ¬çš„å¹²å‡€è¯­æ–™åº“ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹å†å²åœŸè€³å…¶è¯­çš„è®¡ç®—åˆ†ææœ‰äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨éœ€è¦ç†è§£å†å²è¯­è¨€ç»“æ„çš„ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ä»–ä»¬è¿˜å¼ºè°ƒäº†ç°æœ‰çš„æŒ‘æˆ˜ï¼Œå¦‚é¢†åŸŸé€‚åº”å’Œæ—¶é—´è·¨åº¦ä¸Šçš„è¯­è¨€å˜åŒ–ã€‚æ‰€æœ‰æä¾›çš„èµ„æºå’Œæ¨¡å‹éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/bucolin%E6%89%BE%E5%88%B0%EF%BC%8C%E4%B8%BA%E5%8E%86%E5%8F%B2%E5%9C%9F%E8%80%B3%E5%85%B6%E8%AF%ADNLP%E7%9A%84%E6%9C%AA%E6%9D%A5%E5%8F%91%E5%B1%95%E6%8F%90%E4%BE%9B%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E3%80%82">https://huggingface.co/bucolinæ‰¾åˆ°ï¼Œä¸ºå†å²åœŸè€³å…¶è¯­NLPçš„æœªæ¥å‘å±•æä¾›åŸºå‡†æµ‹è¯•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04828v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å†å²åœŸè€³å…¶è¯­çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„åŸºç¡€èµ„æºå’Œæ¨¡å‹ã€‚æ–‡ç« é¦–æ¬¡æ¨å‡ºäº†HisTRå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ•°æ®é›†å’ŒOTA-BOUNå†å²åœŸè€³å…¶è¯­çš„é€šç”¨ä¾å­˜å…³ç³»æ ‘åº“ï¼Œä»¥åŠä½¿ç”¨è¿™äº›æ•°æ®é›†è®­ç»ƒçš„ç”¨äºå‘½åå®ä½“è¯†åˆ«ã€ä¾å­˜å…³ç³»è§£æå’Œè¯æ€§æ ‡æ³¨ä»»åŠ¡çš„åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†æ¨ªè·¨å¹¿æ³›å†å²æ—¶æœŸçš„æ¸…æ´å†å²åœŸè€³å…¶è¯­æ–‡æœ¬è¯­æ–™åº“â€”â€”å¥¥æ–¯æ›¼æ–‡æœ¬è¯­æ–™åº“ï¼ˆOTCï¼‰ã€‚å®éªŒç»“æœæ˜¾è‘—æ”¹å–„äº†å†å²åœŸè€³å…¶è¯­çš„è®¡ç®—åˆ†æï¼Œåœ¨éœ€è¦ç†è§£å†å²è¯­è¨€ç»“æ„çš„ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœï¼ŒåŒæ—¶ä¹Ÿçªæ˜¾äº†é¢†åŸŸé€‚åº”æ€§å’Œæ—¶é—´è·¨åº¦ä¸­çš„è¯­è¨€å˜åŒ–ç­‰ç°æœ‰æŒ‘æˆ˜ã€‚æ‰€æœ‰èµ„æºå’Œæ¨¡å‹å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/bucolin%E4%B8%8A%E6%89%BE%E5%88%B0%EF%BC%8C%E7%9B%AE%E7%9A%84%E4%B8%BA%E6%9C%AC%E5%B9%B4%E5%8F%AF%E5%A4%96%E7%9A%84%E5%8F%A4%E4%BB%BD%E5%A3%AB%E8%AF%ADNLP%E5%B1%A5%E5%B7%A5%E7%BB%BF%E6%BA%90">https://huggingface.co/bucolinä¸Šæ‰¾åˆ°ï¼Œæ—¨åœ¨ä¸ºæœªæ¥çš„å†å²åœŸè€³å…¶è¯­NLPè¿›å±•æä¾›åŸºå‡†æµ‹è¯•ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä»‹ç»äº†é’ˆå¯¹å†å²åœŸè€³å…¶è¯­è‡ªç„¶è¯­è¨€å¤„ç†çš„åŸºç¡€èµ„æºå’Œæ¨¡å‹ï¼Œå¡«è¡¥äº†è®¡ç®—è¯­è¨€å­¦é¢†åŸŸå¯¹è¯¥è¯­è¨€çš„æ¢ç´¢ç©ºç™½ã€‚</li>
<li>æ–‡ç« é¦–æ¬¡å‘å¸ƒäº†HisTRå‘½åå®ä½“è¯†åˆ«æ•°æ®é›†å’ŒOTA-BOUNå†å²åœŸè€³å…¶è¯­é€šç”¨ä¾å­˜å…³ç³»æ ‘åº“ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨è¿™äº›æ•°æ®é›†è®­ç»ƒçš„åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ï¼Œåœ¨å‘½åå®ä½“è¯†åˆ«ã€ä¾å­˜å…³ç³»è§£æå’Œè¯æ€§æ ‡æ³¨ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
<li>æ–‡ç« è¿˜ä»‹ç»äº†å¥¥æ–¯æ›¼æ–‡æœ¬è¯­æ–™åº“ï¼ˆOTCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¸…æ´çš„å†å²åœŸè€³å…¶è¯­æ–‡æœ¬è¯­æ–™åº“ï¼Œæ¶µç›–å¹¿æ³›çš„å†å²æ—¶æœŸã€‚</li>
<li>å®éªŒç»“æœæ˜¾è‘—æ”¹å–„äº†å†å²åœŸè€³å…¶è¯­çš„è®¡ç®—åˆ†æï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç†è§£å†å²è¯­è¨€ç»“æ„çš„ä»»åŠ¡ä¸­ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†é¢†åŸŸé€‚åº”æ€§å’Œæ—¶é—´è·¨åº¦ä¸­çš„è¯­è¨€å˜åŒ–ç­‰ç°æœ‰æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4f534784bd3911f4763ff5dcfb885357.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75a0b8de605239bd2c6973f116714539.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d997919625260dd9b3a40aa0590419d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d3dfbd5b8282968e283b70f4102e4c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-782697730b953817dc680c8904244df0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Right-Label-Context-in-End-to-End-Training-of-Time-Synchronous-ASR-Models"><a href="#Right-Label-Context-in-End-to-End-Training-of-Time-Synchronous-ASR-Models" class="headerlink" title="Right Label Context in End-to-End Training of Time-Synchronous ASR   Models"></a>Right Label Context in End-to-End Training of Time-Synchronous ASR   Models</h2><p><strong>Authors:Tina Raissi, Ralf SchlÃ¼ter, Hermann Ney</strong></p>
<p>Current time-synchronous sequence-to-sequence automatic speech recognition (ASR) models are trained by using sequence level cross-entropy that sums over all alignments. Due to the discriminative formulation, incorporating the right label context into the training criterionâ€™s gradient causes normalization problems and is not mathematically well-defined. The classic hybrid neural network hidden Markov model (NN-HMM) with its inherent generative formulation enables conditioning on the right label context. However, due to the HMM state-tying the identity of the right label context is never modeled explicitly. In this work, we propose a factored loss with auxiliary left and right label contexts that sums over all alignments. We show that the inclusion of the right label context is particularly beneficial when training data resources are limited. Moreover, we also show that it is possible to build a factored hybrid HMM system by relying exclusively on the full-sum criterion. Experiments were conducted on Switchboard 300h and LibriSpeech 960h. </p>
<blockquote>
<p>å½“å‰çš„æ—¶é—´åŒæ­¥åºåˆ—åˆ°åºåˆ—è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹æ˜¯é€šè¿‡ä½¿ç”¨åºåˆ—çº§åˆ«çš„äº¤å‰ç†µï¼ˆå¯¹æ‰€æœ‰å¯¹é½è¿›è¡Œæ±‚å’Œï¼‰æ¥è®­ç»ƒçš„ã€‚ç”±äºé‡‡ç”¨äº†åˆ¤åˆ«å¼å…¬å¼ï¼Œå°†æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡çº³å…¥è®­ç»ƒæ ‡å‡†çš„æ¢¯åº¦ä¼šå¯¼è‡´å½’ä¸€åŒ–é—®é¢˜ï¼Œå¹¶ä¸”ä»æ•°å­¦ä¸Šå®šä¹‰ä¸å¤Ÿæ˜ç¡®ã€‚ç»å…¸çš„æ··åˆç¥ç»ç½‘ç»œéšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆNN-HMMï¼‰å‡­å€Ÿå…¶å†…åœ¨çš„ç”Ÿæˆå¼å…¬å¼ï¼Œèƒ½å¤Ÿå®ç°ä»¥æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡è¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚ç„¶è€Œï¼Œç”±äºéšé©¬å°”å¯å¤«æ¨¡å‹çš„çŠ¶æ€ç»‘å®šï¼Œå› æ­¤ç»ä¸ä¼šæ˜ç¡®åœ°å»ºæ¨¡æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡çš„èº«ä»½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰è¾…åŠ©å·¦å’Œå³æ ‡ç­¾ä¸Šä¸‹æ–‡çš„åˆ†è§£æŸå¤±ï¼Œè¯¥æŸå¤±å¯¹æ‰€æœ‰å¯¹é½è¿›è¡Œäº†æ±‚å’Œã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨è®­ç»ƒæ•°æ®èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼ŒåŒ…å«æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡ç‰¹åˆ«æœ‰ç›Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä»…ä¾é å…¨å’Œå‡†åˆ™æ¥æ„å»ºåˆ†è§£æ··åˆHMMç³»ç»Ÿæ˜¯å¯èƒ½çš„ã€‚å®éªŒåœ¨Switchboard 300hå’ŒLibriSpeech 960hä¸Šè¿›è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04521v2">PDF</a> Accepted for presentation at ICASSP 2025</p>
<p><strong>Summary</strong>ï¼š<br>å½“å‰æ—¶åºåŒæ­¥çš„åºåˆ—åˆ°åºåˆ—è¯­éŸ³è¯†åˆ«æ¨¡å‹é‡‡ç”¨åºåˆ—çº§åˆ«çš„äº¤å‰ç†µè¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡å¯¹é½æ±‚å’Œå®ç°ã€‚ç”±äºåˆ¤åˆ«å¼å…¬å¼çš„é—®é¢˜ï¼Œå°†æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡çº³å…¥è®­ç»ƒæ ‡å‡†çš„æ¢¯åº¦ä¼šå¯¼è‡´å½’ä¸€åŒ–é—®é¢˜å¹¶ä¸”æ•°å­¦ä¸Šæœªæ˜ç¡®å®šä¹‰ã€‚ç»å…¸çš„æ··åˆç¥ç»ç½‘ç»œéšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆNN-HMMï¼‰å…·æœ‰å…¶å›ºæœ‰çš„ç”Ÿæˆå¼å…¬å¼ï¼Œå¯å®ç°æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡æ¡ä»¶è®¾ç½®ã€‚ç„¶è€Œï¼Œç”±äºHMMçš„çŠ¶æ€ç»‘å®šï¼Œæ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡çš„èº«ä»½ä»æœªè¢«æ˜ç¡®å»ºæ¨¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰è¾…åŠ©å·¦å³æ ‡ç­¾ä¸Šä¸‹æ–‡çš„åˆ†è§£æŸå¤±ï¼Œé€šè¿‡å¯¹é½æ±‚å’Œå®ç°ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨è®­ç»ƒæ•°æ®èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¼•å…¥æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡ç‰¹åˆ«æœ‰ç›Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†ä»…ä¾é å…¨å’Œå‡†åˆ™å»ºç«‹åˆ†è§£æ··åˆHMMç³»ç»Ÿçš„å¯èƒ½æ€§ã€‚å®éªŒåœ¨Switchboard 300hå’ŒLibriSpeech 960hä¸Šè¿›è¡Œã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰ASRæ¨¡å‹ä½¿ç”¨åºåˆ—çº§åˆ«çš„äº¤å‰ç†µè¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡å¯¹é½æ±‚å’Œå®ç°ã€‚</li>
<li>åˆ¤åˆ«å¼å…¬å¼åœ¨çº³å…¥æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡æ—¶ä¼šå¯¼è‡´è®­ç»ƒé—®é¢˜ã€‚</li>
<li>NN-HMMæ¨¡å‹èƒ½å¤ŸåŸºäºç”Ÿæˆå¼å…¬å¼å®ç°æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡æ¡ä»¶è®¾ç½®ã€‚</li>
<li>HMMçš„çŠ¶æ€ç»‘å®šå¯¼è‡´æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡æœªè¢«æ˜ç¡®å»ºæ¨¡ã€‚</li>
<li>æå‡ºçš„åˆ†è§£æŸå¤±æ¨¡å‹å¸¦æœ‰è¾…åŠ©çš„å·¦å³æ ‡ç­¾ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡å¯¹é½æ±‚å’Œå®ç°ã€‚</li>
<li>åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¼•å…¥æ­£ç¡®çš„æ ‡ç­¾ä¸Šä¸‹æ–‡ç‰¹åˆ«æœ‰ç›Šã€‚</li>
<li>ä»…é å…¨å’Œå‡†åˆ™å»ºç«‹åˆ†è§£æ··åˆHMMç³»ç»Ÿæ˜¯å¯èƒ½çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04521">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-750334e28e89f8a9c285ac04dab4363f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d200467c6c9ab9497e1faffd6671107.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CAMEL-Cross-Attention-Enhanced-Mixture-of-Experts-and-Language-Bias-for-Code-Switching-Speech-Recognition"><a href="#CAMEL-Cross-Attention-Enhanced-Mixture-of-Experts-and-Language-Bias-for-Code-Switching-Speech-Recognition" class="headerlink" title="CAMEL: Cross-Attention Enhanced Mixture-of-Experts and Language Bias for   Code-Switching Speech Recognition"></a>CAMEL: Cross-Attention Enhanced Mixture-of-Experts and Language Bias for   Code-Switching Speech Recognition</h2><p><strong>Authors:He Wang, Xucheng Wan, Naijun Zheng, Kai Liu, Huan Zhou, Guojian Li, Lei Xie</strong></p>
<p>Code-switching automatic speech recognition (ASR) aims to transcribe speech that contains two or more languages accurately. To better capture language-specific speech representations and address language confusion in code-switching ASR, the mixture-of-experts (MoE) architecture and an additional language diarization (LD) decoder are commonly employed. However, most researches remain stagnant in simple operations like weighted summation or concatenation to fuse languagespecific speech representations, leaving significant opportunities to explore the enhancement of integrating language bias information. In this paper, we introduce CAMEL, a cross-attention-based MoE and language bias approach for code-switching ASR. Specifically, after each MoE layer, we fuse language-specific speech representations with cross-attention, leveraging its strong contextual modeling abilities. Additionally, we design a source attention-based mechanism to incorporate the language information from the LD decoder output into text embeddings. Experimental results demonstrate that our approach achieves state-of-the-art performance on the SEAME, ASRU200, and ASRU700+LibriSpeech460 Mandarin-English code-switching ASR datasets. </p>
<blockquote>
<p>ä»£ç åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ—¨åœ¨å‡†ç¡®è½¬å½•åŒ…å«ä¸¤ç§æˆ–å¤šç§è¯­è¨€çš„è¯­éŸ³ã€‚ä¸ºäº†æ›´å¥½åœ°æ•è·ç‰¹å®šè¯­è¨€çš„è¯­éŸ³è¡¨ç¤ºå¹¶è§£å†³ä»£ç åˆ‡æ¢ASRä¸­çš„è¯­è¨€æ··æ·†é—®é¢˜ï¼Œé€šå¸¸é‡‡ç”¨åŸºäºä¸“å®¶çš„æ··åˆï¼ˆMoEï¼‰æ¶æ„å’Œé¢å¤–çš„è¯­è¨€åˆ†é¦ï¼ˆLDï¼‰è§£ç å™¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶ä»åœç•™åœ¨åŠ æƒæ±‚å’Œæˆ–æ‹¼æ¥ç­‰ç®€å•æ“ä½œæ¥èåˆç‰¹å®šè¯­è¨€çš„è¯­éŸ³è¡¨ç¤ºï¼Œç•™ä¸‹äº†æ¢ç´¢æ•´åˆè¯­è¨€åè§ä¿¡æ¯å¢å¼ºçš„æ˜¾è‘—æœºä¼šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CAMELï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè·¨æ³¨æ„åŠ›çš„MoEå’Œè¯­è¨€åè§æ–¹æ³•çš„ä»£ç åˆ‡æ¢ASRã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯ä¸ªMoEå±‚ä¹‹åï¼Œæˆ‘ä»¬åˆ©ç”¨å¼ºå¤§çš„ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›èåˆç‰¹å®šè¯­è¨€çš„è¯­éŸ³è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæºæ³¨æ„åŠ›çš„æœºåˆ¶ï¼Œå°†LDè§£ç å™¨çš„è¯­è¨€ä¿¡æ¯å¹¶å…¥æ–‡æœ¬åµŒå…¥ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨SEAMEã€ASRU200ã€ASRU700å’ŒLibriSpeech460çš„æ™®é€šè¯è‹±è¯­ä»£ç åˆ‡æ¢ASRæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12760v2">PDF</a> Accepted by ICASSP 2025. 5 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒè¯­æˆ–å¤šè¯­æ··åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯çš„æœ€æ–°ç ”ç©¶ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºäº¤å‰æ³¨æ„åŠ›å’Œè¯­è¨€åå¥½çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„æ–¹æ³•CAMELï¼Œæ—¨åœ¨æ›´å‡†ç¡®åœ°è½¬å½•åŒ…å«ä¸¤ç§æˆ–å¤šç§è¯­è¨€çš„è¯­éŸ³ã€‚è¯¥æ–¹æ³•é€šè¿‡èåˆè¯­è¨€ç‰¹å®šè¯­éŸ³è¡¨ç¤ºå’Œå¼•å…¥è¯­è¨€ä¿¡æ¯åµŒå…¥æœºåˆ¶ï¼Œæé«˜äº†ä»£ç åˆ‡æ¢ASRçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAMELåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç åˆ‡æ¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„ç›®æ ‡ï¼šå‡†ç¡®è½¬å½•åŒ…å«ä¸¤ç§æˆ–å¤šç§è¯­è¨€çš„è¯­éŸ³ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼šå¤šæ•°ç ”ç©¶ä»…é€šè¿‡ç®€å•æ“ä½œï¼ˆå¦‚åŠ æƒæ±‚å’Œæˆ–æ‹¼æ¥ï¼‰èåˆè¯­è¨€ç‰¹å®šçš„è¯­éŸ³è¡¨ç¤ºï¼Œæœªèƒ½å……åˆ†æ¢ç´¢æ•´åˆè¯­è¨€åè§ä¿¡æ¯çš„å¢å¼ºæ–¹å¼ã€‚</li>
<li>CAMELæ–¹æ³•ä»‹ç»ï¼šé‡‡ç”¨åŸºäºäº¤å‰æ³¨æ„åŠ›çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„å’Œè¯­è¨€åè§æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>MoEæ¶æ„ä¸­çš„åˆ›æ–°ç‚¹ï¼šåœ¨æ¯ä¸ªMoEå±‚åï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›èåˆè¯­è¨€ç‰¹å®šçš„è¯­éŸ³è¡¨ç¤ºï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„ä¸Šä¸‹æ–‡å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>è¯­è¨€ä¿¡æ¯åµŒå…¥æœºåˆ¶ï¼šè®¾è®¡äº†ä¸€ç§åŸºäºæºæ³¨æ„åŠ›çš„æœºåˆ¶ï¼Œå°†LDè§£ç å™¨çš„è¯­è¨€ä¿¡æ¯èå…¥æ–‡æœ¬åµŒå…¥ä¸­ã€‚</li>
<li>å®éªŒç»“æœï¼šCAMELåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„è¡¨ç°ï¼ŒåŒ…æ‹¬SEAMEã€ASRU200ã€ASRU700å’ŒLibriSpeechçš„Mandarin-Englishä»£ç åˆ‡æ¢ASRæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-32e24547596b7e17cec44e1e78197fec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5f54c07922fb6776db501b8b998cc01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9168efb5e9f2ee8f7cb40f1b54af5099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a948c134d02d66d48bb96fb34a1a0fc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12e849cc5d79f2ac41655d31403ee173.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PFML-Self-Supervised-Learning-of-Time-Series-Data-Without-Representation-Collapse"><a href="#PFML-Self-Supervised-Learning-of-Time-Series-Data-Without-Representation-Collapse" class="headerlink" title="PFML: Self-Supervised Learning of Time-Series Data Without   Representation Collapse"></a>PFML: Self-Supervised Learning of Time-Series Data Without   Representation Collapse</h2><p><strong>Authors:Einari Vaaras, Manu Airaksinen, Okko RÃ¤sÃ¤nen</strong></p>
<p>Self-supervised learning (SSL) is a data-driven learning approach that utilizes the innate structure of the data to guide the learning process. In contrast to supervised learning, which depends on external labels, SSL utilizes the inherent characteristics of the data to produce its own supervisory signal. However, one frequent issue with SSL methods is representation collapse, where the model outputs a constant input-invariant feature representation. This issue hinders the potential application of SSL methods to new data modalities, as trying to avoid representation collapse wastes researchersâ€™ time and effort. This paper introduces a novel SSL algorithm for time-series data called Prediction of Functionals from Masked Latents (PFML). Instead of predicting masked input signals or their latent representations directly, PFML operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. The algorithm is designed to avoid representation collapse, rendering it straightforwardly applicable to different time-series data domains, such as novel sensor modalities in clinical data. We demonstrate the effectiveness of PFML through complex, real-life classification tasks across three different data modalities: infant posture and movement classification from multi-sensor inertial measurement unit data, emotion recognition from speech data, and sleep stage classification from EEG data. The results show that PFML is superior to a conceptually similar SSL method and a contrastive learning-based SSL method. Additionally, PFML is on par with the current state-of-the-art SSL method, while also being conceptually simpler and without suffering from representation collapse. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ˜¯ä¸€ç§æ•°æ®é©±åŠ¨çš„å­¦ä¹ æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ•°æ®çš„å›ºæœ‰ç»“æ„æ¥æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚ä¸ä¾èµ–äºå¤–éƒ¨æ ‡ç­¾çš„ç›‘ç£å­¦ä¹ ç›¸æ¯”ï¼ŒSSLåˆ©ç”¨æ•°æ®çš„å†…åœ¨ç‰¹æ€§æ¥äº§ç”Ÿè‡ªå·±çš„ç›‘ç£ä¿¡å·ã€‚ç„¶è€Œï¼ŒSSLæ–¹æ³•çš„ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯è¡¨ç¤ºå´©æºƒï¼Œå³æ¨¡å‹è¾“å‡ºä¸€ä¸ªæ’å®šçš„è¾“å…¥ä¸å˜çš„ç‰¹å¾è¡¨ç¤ºã€‚è¿™ä¸ªé—®é¢˜é˜»ç¢äº†SSLæ–¹æ³•åœ¨æ–°æ•°æ®æ¨¡æ€çš„æ½œåœ¨åº”ç”¨ï¼Œå› ä¸ºè¯•å›¾é¿å…è¡¨ç¤ºå´©æºƒä¼šæµªè´¹ç ”ç©¶äººå‘˜çš„æ—¶é—´å’Œç²¾åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºæ—¶é—´åºåˆ—æ•°æ®çš„æ–°å‹SSLç®—æ³•ï¼Œç§°ä¸ºâ€œä»æ©ç æ½œåœ¨ç‰¹å¾é¢„æµ‹åŠŸèƒ½â€ï¼ˆPFMLï¼‰ã€‚PFMLä¸æ˜¯ç›´æ¥é¢„æµ‹æ©ç è¾“å…¥ä¿¡å·æˆ–å…¶æ½œåœ¨è¡¨ç¤ºï¼Œè€Œæ˜¯ç»™å®šä¸€ç³»åˆ—æœªæ©ç åµŒå…¥ï¼Œé€šè¿‡é¢„æµ‹ä¸æ©ç åµŒå…¥ç›¸å¯¹åº”çš„è¾“å…¥ä¿¡å·ç»Ÿè®¡åŠŸèƒ½æ¥æ“ä½œã€‚è¯¥ç®—æ³•æ—¨åœ¨é¿å…è¡¨ç¤ºå´©æºƒï¼Œä½¿å…¶èƒ½å¤Ÿç›´æ¥åº”ç”¨äºä¸åŒçš„æ—¶é—´åºåˆ—æ•°æ®é¢†åŸŸï¼Œå¦‚ä¸´åºŠæ•°æ®ä¸­çš„æ–°å‹ä¼ æ„Ÿå™¨æ¨¡æ€ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªä¸åŒæ•°æ®æ¨¡æ€çš„å¤æ‚ç°å®åˆ†ç±»ä»»åŠ¡å±•ç¤ºäº†PFMLçš„æœ‰æ•ˆæ€§ï¼šä»å¤šä¼ æ„Ÿå™¨æƒ¯æ€§æµ‹é‡å•å…ƒæ•°æ®è¿›è¡Œå©´å„¿å§¿åŠ¿å’Œè¿åŠ¨åˆ†ç±»ï¼Œä»è¯­éŸ³æ•°æ®è¿›è¡Œæƒ…æ„Ÿè¯†åˆ«ï¼Œä»è„‘ç”µå›¾æ•°æ®è¿›è¡Œç¡çœ é˜¶æ®µåˆ†ç±»ã€‚ç»“æœè¡¨æ˜ï¼ŒPFMLä¼˜äºä¸€ä¸ªæ¦‚å¿µç›¸ä¼¼çš„SSLæ–¹æ³•å’Œä¸€ä¸ªåŸºäºå¯¹æ¯”å­¦ä¹ çš„SSLæ–¹æ³•ã€‚æ­¤å¤–ï¼ŒPFMLä¸å½“å‰æœ€å…ˆè¿›çš„SSLæ–¹æ³•ä¸ç›¸ä¸Šä¸‹ï¼ŒåŒæ—¶æ¦‚å¿µä¸Šæ›´ç®€å•ï¼Œå¹¶ä¸”æ²¡æœ‰é­å—è¡¨ç¤ºå´©æºƒçš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10087v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹æ—¶é—´åºåˆ—æ•°æ®çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ–°ç®—æ³•â€”â€”åŸºäºæ©ç éšå±‚çš„åŠŸèƒ½é¢„æµ‹ï¼ˆPFMLï¼‰ã€‚PFMLé€šè¿‡é¢„æµ‹å¯¹åº”æ©ç åµŒå…¥çš„ç»Ÿè®¡åŠŸèƒ½ï¼Œè€Œä¸æ˜¯ç›´æ¥é¢„æµ‹æ©ç è¾“å…¥ä¿¡å·æˆ–å…¶éšå±‚è¡¨ç¤ºï¼Œä»è€Œé¿å…äº†è¡¨ç¤ºå´©æºƒé—®é¢˜ã€‚è¿™ä½¿å¾—PFMLå¯ç›´æ¥åº”ç”¨äºä¸åŒçš„æ—¶é—´åºåˆ—æ•°æ®é¢†åŸŸï¼Œå¦‚ä¸´åºŠæ•°æ®ä¸­çš„æ–°å‹ä¼ æ„Ÿå™¨æ¨¡æ€ã€‚é€šè¿‡è·¨ä¸‰ç§ä¸åŒæ•°æ®æ¨¡æ€çš„å¤æ‚ç°å®åˆ†ç±»ä»»åŠ¡éªŒè¯ï¼ŒPFMLåœ¨å©´å„¿å§¿åŠ¿å’Œè¿åŠ¨åˆ†ç±»ã€æƒ…æ„Ÿè¯†åˆ«ä»¥åŠç¡çœ é˜¶æ®µåˆ†ç±»ç­‰åº”ç”¨ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºä¸€ç§æ¦‚å¿µç›¸ä¼¼çš„SSLæ–¹æ³•å’Œä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„SSLæ–¹æ³•ï¼Œå¹¶ä¸å½“å‰æœ€å…ˆè¿›çš„SSLæ–¹æ³•è¡¨ç°ç›¸å½“ï¼ŒåŒæ—¶æ¦‚å¿µæ›´ç®€å•ï¼Œä¸”ä¸å­˜åœ¨è¡¨ç¤ºå´©æºƒé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ˜¯ä¸€ç§åˆ©ç”¨æ•°æ®å†…åœ¨ç»“æ„æ¥æŒ‡å¯¼å­¦ä¹ è¿‡ç¨‹çš„æ•°æ®é©±åŠ¨å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>SSLé¢ä¸´çš„ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯è¡¨ç¤ºå´©æºƒï¼Œå³æ¨¡å‹è¾“å‡ºæ’å®šçš„è¾“å…¥ä¸å˜ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>PFMLæ˜¯ä¸€ç§é’ˆå¯¹æ—¶é—´åºåˆ—æ•°æ®çš„SSLæ–°ç®—æ³•ï¼Œé€šè¿‡é¢„æµ‹æ©ç åµŒå…¥çš„ç»Ÿè®¡åŠŸèƒ½æ¥é¿å…è¡¨ç¤ºå´©æºƒé—®é¢˜ã€‚</li>
<li>PFMLå¯ç›´æ¥åº”ç”¨äºä¸åŒçš„æ—¶é—´åºåˆ—æ•°æ®é¢†åŸŸï¼Œå¦‚æ–°å‹ä¼ æ„Ÿå™¨æ¨¡æ€çš„ä¸´åºŠæ•°æ®ã€‚</li>
<li>PFMLåœ¨å¤šç§å¤æ‚ç°å®åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬å©´å„¿å§¿åŠ¿å’Œè¿åŠ¨åˆ†ç±»ã€æƒ…æ„Ÿè¯†åˆ«ä»¥åŠç¡çœ é˜¶æ®µåˆ†ç±»ã€‚</li>
<li>PFMLä¼˜äºå…¶ä»–SSLæ–¹æ³•ï¼Œä¸”ä¸å½“å‰æœ€å…ˆè¿›çš„SSLæ–¹æ³•è¡¨ç°ç›¸å½“ï¼ŒåŒæ—¶æ¦‚å¿µæ›´ç®€å•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9abf989222a5e681c5a4cbe783dc6cf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95dd8d9cd263cacd39e91d653ac55632.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Mask-Weighted-Spatial-Likelihood-Coding-for-Speaker-Independent-Joint-Localization-and-Mask-Estimation"><a href="#Mask-Weighted-Spatial-Likelihood-Coding-for-Speaker-Independent-Joint-Localization-and-Mask-Estimation" class="headerlink" title="Mask-Weighted Spatial Likelihood Coding for Speaker-Independent Joint   Localization and Mask Estimation"></a>Mask-Weighted Spatial Likelihood Coding for Speaker-Independent Joint   Localization and Mask Estimation</h2><p><strong>Authors:Jakob Kienegger, Alina Mannanova, Timo Gerkmann</strong></p>
<p>Due to their robustness and flexibility, neural-driven beamformers are a popular choice for speech separation in challenging environments with a varying amount of simultaneous speakers alongside noise and reverberation. Time-frequency masks and relative directions of the speakers regarding a fixed spatial grid can be used to estimate the beamformerâ€™s parameters. To some degree, speaker-independence is achieved by ensuring a greater amount of spatial partitions than speech sources. In this work, we analyze how to encode both mask and positioning into such a grid to enable joint estimation of both quantities. We propose mask-weighted spatial likelihood coding and show that it achieves considerable performance in both tasks compared to baseline encodings optimized for either localization or mask estimation. In the same setup, we demonstrate superiority for joint estimation of both quantities. Conclusively, we propose a universal approach which can replace an upstream sound source localization system solely by adapting the training framework, making it highly relevant in performance-critical scenarios. </p>
<blockquote>
<p>ç”±äºç¥ç»é©±åŠ¨æ³¢æŸå½¢æˆå™¨çš„ç¨³å¥æ€§å’Œçµæ´»æ€§ï¼Œå®ƒæˆä¸ºåœ¨å……æ»¡æŒ‘æˆ˜çš„ç¯å¢ƒä¸­å®ç°è¯­éŸ³åˆ†ç¦»çš„æµè¡Œé€‰æ‹©ã€‚åœ¨è¿™ç§ç¯å¢ƒä¸­ï¼Œå­˜åœ¨æ•°é‡ä¸ä¸€çš„åŒæ—¶è¯´è¯äººï¼Œä»¥åŠå™ªå£°å’Œå›å£°ã€‚å¯ä»¥ä½¿ç”¨æ—¶é—´-é¢‘ç‡æ©æ¨¡å’Œç›¸å¯¹äºå›ºå®šç©ºé—´ç½‘æ ¼çš„è¯´è¯äººæ–¹å‘æ¥ä¼°è®¡æ³¢æŸå½¢æˆå™¨çš„å‚æ•°ã€‚åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œé€šè¿‡ç¡®ä¿ç©ºé—´åˆ†åŒºæ•°é‡å¤šäºè¯­éŸ³æºï¼Œå¯ä»¥å®ç°è¯´è¯äººçš„ç‹¬ç«‹æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†å¦‚ä½•å°†æ©æ¨¡å’Œå®šä½ç¼–ç åˆ°è¿™æ ·çš„ç½‘æ ¼ä¸­ï¼Œä»¥å®ç°ä¸¤ç§æ•°é‡çš„è”åˆä¼°è®¡ã€‚æˆ‘ä»¬æå‡ºäº†æ©æ¨¡åŠ æƒç©ºé—´å¯èƒ½æ€§ç¼–ç ï¼Œå¹¶è¯æ˜ä¸é’ˆå¯¹å®šä½æˆ–æ©æ¨¡ä¼°è®¡ä¼˜åŒ–çš„åŸºçº¿ç¼–ç ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤é¡¹ä»»åŠ¡ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚åœ¨åŒä¸€è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸¤ç§æ•°é‡è”åˆä¼°è®¡çš„ä¼˜è¶Šæ€§ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œä»…é€šè¿‡è°ƒæ•´è®­ç»ƒæ¡†æ¶å°±å¯ä»¥æ›¿ä»£ä¸Šæ¸¸å£°æºå®šä½ç³»ç»Ÿï¼Œä½¿å…¶åœ¨æ€§èƒ½è‡³å…³é‡è¦çš„åœºæ™¯ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19595v2">PDF</a> \copyright 2025 IEEE. Personal use of this material is permitted.   Permission from IEEE must be obtained for all other uses, in any current or   future media, including reprinting&#x2F;republishing this material for advertising   or promotional purposes, creating new collective works, for resale or   redistribution to servers or lists, or reuse of any copyrighted component of   this work in other works</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºç¥ç»ç½‘ç»œé©±åŠ¨çš„æ³¢æŸå½¢æˆå™¨å› å…¶ç¨³å¥æ€§å’Œçµæ´»æ€§ï¼Œåœ¨å……æ»¡æŒ‘æˆ˜çš„ç¯å¢ƒä¸­ï¼Œå¦‚å­˜åœ¨å¤šä¸ªåŒæ—¶å‘å£°çš„å£°æºã€å™ªå£°å’Œå›å£°ç­‰æƒ…å†µä¸‹ï¼Œè¢«å¹¿æ³›åº”ç”¨äºè¯­éŸ³åˆ†ç¦»ã€‚é€šè¿‡æ—¶é—´é¢‘ç‡æ©è†œå’Œç›¸å¯¹äºå›ºå®šç©ºé—´ç½‘æ ¼çš„å£°æºæ–¹å‘ï¼Œå¯ä»¥ä¼°ç®—æ³¢æŸå½¢æˆå™¨çš„å‚æ•°ã€‚é€šè¿‡å¢åŠ ç©ºé—´åˆ†åŒºæ•°é‡ä»¥å®ç°ç›¸å¯¹å¤šçš„å£°æºæ•°é‡ï¼Œä»è€Œå®ç°æŸç§ç¨‹åº¦çš„æ‰¬å£°å™¨ç‹¬ç«‹æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†å¦‚ä½•å°†æ©è†œå’Œå®šä½ä¿¡æ¯ç¼–ç åˆ°ç½‘æ ¼ä¸­ï¼Œä»¥å®ç°ä¸¤ç§æ•°é‡çš„è”åˆä¼°è®¡ã€‚æˆ‘ä»¬æå‡ºäº†æ©è†œåŠ æƒç©ºé—´å¯èƒ½æ€§ç¼–ç ï¼Œå¹¶è¯æ˜ä¸é’ˆå¯¹å®šä½æˆ–æ©è†œä¼°è®¡ä¼˜åŒ–çš„åŸºçº¿ç¼–ç ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨ä¸¤é¡¹ä»»åŠ¡ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨ç›¸åŒçš„è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸¤ç§æ•°é‡è”åˆä¼°è®¡çš„ä¼˜è¶Šæ€§ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œä»…é€šè¿‡è°ƒæ•´è®­ç»ƒæ¡†æ¶å°±èƒ½æ›¿ä»£ä¸Šæ¸¸å£°æºå®šä½ç³»ç»Ÿï¼Œä½¿å…¶åœ¨æ€§èƒ½å…³é”®åœºæ™¯ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œé©±åŠ¨çš„æ³¢æŸå½¢æˆå™¨åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­è¿›è¡Œè¯­éŸ³åˆ†ç¦»æ—¶è¡¨ç°å‡ºç¨³å¥æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>æ—¶é—´é¢‘ç‡æ©è†œå’Œç›¸å¯¹äºå›ºå®šç©ºé—´ç½‘æ ¼çš„å£°æºæ–¹å‘å¯ç”¨äºä¼°ç®—æ³¢æŸå½¢æˆå™¨çš„å‚æ•°ã€‚</li>
<li>é€šè¿‡ç¡®ä¿æ›´å¤šçš„ç©ºé—´åˆ†åŒºæ¥å®ç°ç›¸å¯¹å¤šçš„å£°æºæ•°é‡ï¼Œä»è€Œè¾¾åˆ°æŸç§ç¨‹åº¦çš„æ‰¬å£°å™¨ç‹¬ç«‹æ€§ã€‚</li>
<li>æå‡ºäº†æ©è†œåŠ æƒç©ºé—´å¯èƒ½æ€§ç¼–ç ï¼Œå®ç°æ©è†œå’Œå®šä½çš„è”åˆä¼°è®¡ã€‚</li>
<li>ä¸åŸºçº¿ç¼–ç ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å®šä½å’Œæ©è†œä¼°è®¡ä»»åŠ¡ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è°ƒæ•´è®­ç»ƒæ¡†æ¶å¯æ›¿ä»£ä¸Šæ¸¸å£°æºå®šä½ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19595">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-db76c877d6ca1e8900b414d5a0ac4582.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-217dad0393db9e0eac0f60772df1fbc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7c3eb61353dd84ed9179754f3676453.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d4ab129ffce0f79c75eb144e6db6a2e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Takin-VC-Expressive-Zero-Shot-Voice-Conversion-via-Adaptive-Hybrid-Content-Encoding-and-Enhanced-Timbre-Modeling"><a href="#Takin-VC-Expressive-Zero-Shot-Voice-Conversion-via-Adaptive-Hybrid-Content-Encoding-and-Enhanced-Timbre-Modeling" class="headerlink" title="Takin-VC: Expressive Zero-Shot Voice Conversion via Adaptive Hybrid   Content Encoding and Enhanced Timbre Modeling"></a>Takin-VC: Expressive Zero-Shot Voice Conversion via Adaptive Hybrid   Content Encoding and Enhanced Timbre Modeling</h2><p><strong>Authors:Yuguang Yang, Yu Pan, Jixun Yao, Xiang Zhang, Jianhao Ye, Hongbin Zhou, Lei Xie, Lei Ma, Jianjun Zhao</strong></p>
<p>Expressive zero-shot voice conversion (VC) is a critical and challenging task that aims to transform the source timbre into an arbitrary unseen speaker while preserving the original content and expressive qualities. Despite recent progress in zero-shot VC, there remains considerable potential for improvements in speaker similarity and speech naturalness. Moreover, existing zero-shot VC systems struggle to fully reproduce paralinguistic information in highly expressive speech, such as breathing, crying, and emotional nuances, limiting their practical applicability. To address these issues, we propose Takin-VC, a novel expressive zero-shot VC framework via adaptive hybrid content encoding and memory-augmented context-aware timbre modeling. Specifically, we introduce an innovative hybrid content encoder that incorporates an adaptive fusion module, capable of effectively integrating quantized features of the pre-trained WavLM and HybridFormer in an implicit manner, so as to extract precise linguistic features while enriching paralinguistic elements. For timbre modeling, we propose advanced memory-augmented and context-aware modules to generate high-quality target timbre features and fused representations that seamlessly align source content with target timbre. To enhance real-time performance, we advocate a conditional flow matching model to reconstruct the Mel-spectrogram of the source speech. Experimental results show that our Takin-VC consistently surpasses state-of-the-art VC systems, achieving notable improvements in terms of speech naturalness, speech expressiveness, and speaker similarity, while offering enhanced inference speed. </p>
<blockquote>
<p>è¡¨è¾¾æ€§é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰æ˜¯ä¸€é¡¹å…³é”®ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œæ—¨åœ¨å°†æºéŸ³è‰²è½¬æ¢ä¸ºä»»æ„æœªè§è¿‡çš„ç›®æ ‡è¯´è¯äººï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å†…å®¹å’Œè¡¨è¾¾å“è´¨ã€‚å°½ç®¡è¿‘æœŸåœ¨é›¶æ ·æœ¬VCæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨è¯´è¯äººç›¸ä¼¼æ€§å’Œè¯­éŸ³è‡ªç„¶æ€§æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›æ½œåŠ›ã€‚æ­¤å¤–ï¼Œç°æœ‰é›¶æ ·æœ¬VCç³»ç»Ÿéš¾ä»¥å®Œå…¨å†ç°é«˜åº¦è¡¨è¾¾æ€§è¯­éŸ³ä¸­çš„å‰¯è¯­è¨€ä¿¡æ¯ï¼Œå¦‚å‘¼å¸ã€å“­æ³£å’Œæƒ…æ„Ÿç»†å¾®å·®åˆ«ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Takin-VCï¼Œä¸€ç§é€šè¿‡è‡ªé€‚åº”æ··åˆå†…å®¹ç¼–ç å’Œå¢å¼ºè®°å¿†ä¸Šä¸‹æ–‡æ„ŸçŸ¥éŸ³è‰²å»ºæ¨¡çš„æ–°å‹è¡¨è¾¾æ€§é›¶æ ·æœ¬VCæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„æ··åˆå†…å®¹ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨åŒ…å«ä¸€ä¸ªè‡ªé€‚åº”èåˆæ¨¡å—ï¼Œèƒ½å¤Ÿä»¥éšå¼æ–¹å¼æœ‰æ•ˆæ•´åˆé¢„è®­ç»ƒWavLMå’ŒHybridFormerçš„é‡åŒ–ç‰¹å¾ï¼Œä»è€Œç²¾ç¡®æå–è¯­è¨€ç‰¹å¾å¹¶ä¸°å¯Œå‰¯è¯­è¨€å…ƒç´ ã€‚å¯¹äºéŸ³è‰²å»ºæ¨¡ï¼Œæˆ‘ä»¬æå‡ºäº†å…ˆè¿›çš„å¢å¼ºè®°å¿†å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å—ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„ç›®æ ‡éŸ³è‰²ç‰¹å¾å’Œèåˆè¡¨ç¤ºï¼Œæ— ç¼å¯¹é½æºå†…å®¹ä¸ç›®æ ‡éŸ³è‰²ã€‚ä¸ºäº†æé«˜å®æ—¶æ€§èƒ½ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨æ¡ä»¶æµåŒ¹é…æ¨¡å‹æ¥é‡å»ºæºè¯­éŸ³çš„æ¢…å°”é¢‘è°±å›¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Takin-VCç³»ç»ŸæŒç»­è¶…è¶Šæœ€å…ˆè¿›çš„VCç³»ç»Ÿï¼Œåœ¨è¯­éŸ³è‡ªç„¶æ€§ã€è¯­éŸ³è¡¨è¾¾æ€§å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒæ—¶æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01350v2">PDF</a> Work in Progress; Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è¡¨è¾¾å¼é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢æ¡†æ¶Takin-VCï¼Œé€šè¿‡è‡ªé€‚åº”æ··åˆå†…å®¹ç¼–ç å’Œå¢å¼ºå‹è®°å¿†ä¸Šä¸‹æ–‡æ„ŸçŸ¥éŸ³è‰²å»ºæ¨¡ï¼Œæ—¨åœ¨è§£å†³é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢ä¸­çš„è¡¨è¾¾åŠ›ã€éŸ³è‰²ç›¸ä¼¼æ€§ã€è‡ªç„¶åº¦ä»¥åŠå®æ—¶æ€§èƒ½çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡¨è¾¾é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰æ—¨åœ¨å°†æºéŸ³è‰²è½¬æ¢ä¸ºä»»æ„æœªè§è¿‡çš„ç›®æ ‡éŸ³è‰²ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹å†…å®¹å’Œè¡¨è¾¾è´¨é‡ã€‚</li>
<li>ç°æœ‰é›¶æ ·æœ¬VCç³»ç»Ÿåœ¨å¤„ç†é«˜åº¦è¡¨è¾¾æ€§çš„è¯­éŸ³æ—¶ï¼Œéš¾ä»¥å®Œå…¨å¤åˆ¶å‰¯è¯­è¨€ä¿¡æ¯ï¼ˆå¦‚å‘¼å¸ã€å“­æ³£å’Œæƒ…æ„Ÿç»†å¾®å·®åˆ«ï¼‰ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚</li>
<li>Takin-VCæ¡†æ¶å¼•å…¥äº†ä¸€ç§åˆ›æ–°æ€§çš„æ··åˆå†…å®¹ç¼–ç å™¨ï¼Œé€šè¿‡è‡ªé€‚åº”èåˆæ¨¡å—æœ‰æ•ˆæ•´åˆé¢„è®­ç»ƒWavLMå’ŒHybridFormerçš„é‡åŒ–ç‰¹å¾ï¼Œä»¥æå–ç²¾ç¡®çš„è¯­è¨€ç‰¹å¾å¹¶ä¸°å¯Œå‰¯è¯­è¨€å…ƒç´ ã€‚</li>
<li>é’ˆå¯¹éŸ³è‰²å»ºæ¨¡ï¼Œæå‡ºäº†å…ˆè¿›çš„è®°å¿†å¢å¼ºå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å—ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„ç›®æ ‡éŸ³è‰²ç‰¹å¾å’Œèåˆè¡¨ç¤ºï¼Œæ— ç¼å¯¹é½æºå†…å®¹ä¸ç›®æ ‡éŸ³è‰²ã€‚</li>
<li>ä¸ºäº†æé«˜å®æ—¶æ€§èƒ½ï¼ŒTakin-VCé‡‡ç”¨æ¡ä»¶æµåŒ¹é…æ¨¡å‹é‡å»ºæºè¯­éŸ³çš„Melå…‰è°±å›¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTakin-VCåœ¨è¯­éŸ³è‡ªç„¶åº¦ã€è¡¨è¾¾åŠ›å’ŒéŸ³è‰²ç›¸ä¼¼æ€§æ–¹é¢å‡è¶…è¶Šç°æœ‰VCç³»ç»Ÿï¼ŒåŒæ—¶æé«˜æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01350">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e18dd42bf14bcc06c8209ded21173be6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d19df096172da9b4e2c63789a578041.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7a5838b6a25bb9a9bb0fbc5466803f1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MultiMed-Multilingual-Medical-Speech-Recognition-via-Attention-Encoder-Decoder"><a href="#MultiMed-Multilingual-Medical-Speech-Recognition-via-Attention-Encoder-Decoder" class="headerlink" title="MultiMed: Multilingual Medical Speech Recognition via Attention Encoder   Decoder"></a>MultiMed: Multilingual Medical Speech Recognition via Attention Encoder   Decoder</h2><p><strong>Authors:Khai Le-Duc, Phuc Phan, Tan-Hanh Pham, Bach Phan Tat, Minh-Huong Ngo, Truong-Son Hy</strong></p>
<p>Multilingual automatic speech recognition (ASR) in the medical domain serves as a foundational task for various downstream applications such as speech translation, spoken language understanding, and voice-activated assistants. This technology enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we introduce MultiMed, the first multilingual medical ASR dataset, along with the first collection of small-to-large end-to-end medical ASR models, spanning five languages: Vietnamese, English, German, French, and Mandarin Chinese. To our best knowledge, MultiMed stands as the worldâ€™s largest medical ASR dataset across all major benchmarks: total duration, number of recording conditions, number of accents, and number of speaking roles. Furthermore, we present the first multilinguality study for medical ASR, which includes reproducible empirical baselines, a monolinguality-multilinguality analysis, Attention Encoder Decoder (AED) vs Hybrid comparative study, a layer-wise ablation study for the AED, and a linguistic analysis for multilingual medical ASR. All code, data, and models are available online: <a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed/tree/master/MultiMed">https://github.com/leduckhai/MultiMed/tree/master/MultiMed</a> </p>
<blockquote>
<p>åœ¨åŒ»ç–—é¢†åŸŸä¸­ï¼Œå¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯ä½œä¸ºå¤šç§ä¸‹æ¸¸åº”ç”¨çš„åŸºç¡€ä»»åŠ¡ï¼Œå¦‚è¯­éŸ³ç¿»è¯‘ã€å£è¯­ç†è§£å’Œè¯­éŸ³åŠ©æ‰‹ç­‰ã€‚è¯¥æŠ€æœ¯é€šè¿‡å…‹æœè¯­è¨€éšœç¢å®ç°é«˜æ•ˆæ²Ÿé€šã€ç¼“è§£ä¸“ä¸šåŠ³åŠ¨åŠ›çŸ­ç¼ºä»¥åŠä¿ƒè¿›è¯Šæ–­å’Œæ²»ç–—çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç–«æƒ…æœŸé—´ä¸ºæ‚£è€…æŠ¤ç†å¸¦æ¥æå‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MultiMedï¼Œå®ƒæ˜¯é¦–ä¸ªå¤šè¯­è¨€åŒ»ç–—ASRæ•°æ®é›†ï¼Œä»¥åŠä¸è¦†ç›–äº”ç§è¯­è¨€ï¼ˆè¶Šå—è¯­ã€è‹±è¯­ã€å¾·è¯­ã€æ³•è¯­å’Œæ™®é€šè¯ï¼‰çš„å°åˆ°å¤§çš„ç«¯åˆ°ç«¯åŒ»ç–—ASRæ¨¡å‹é›†åˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒMultiMedåœ¨æ‰€æœ‰ä¸»è¦åŸºå‡†æµ‹è¯•ä¸­å‡ä¸ºä¸–ç•Œä¸Šæœ€å¤§çš„åŒ»ç–—ASRæ•°æ®é›†ï¼šæ€»æ—¶é•¿ã€å½•éŸ³æ¡ä»¶æ•°é‡ã€å£éŸ³æ•°é‡å’Œè¯´è¯è§’è‰²æ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡è¿›è¡Œäº†åŒ»ç–—ASRçš„å¤šè¯­è¨€æ€§ç ”ç©¶ï¼ŒåŒ…æ‹¬å¯å¤åˆ¶çš„å®è¯åŸºå‡†æµ‹è¯•ã€å•è¯­è¨€ä¸å¤šè¯­è¨€åˆ†æã€æ³¨æ„åŠ›ç¼–ç å™¨è§£ç å™¨ï¼ˆAEDï¼‰ä¸æ··åˆæŠ€æœ¯çš„æ¯”è¾ƒç ”ç©¶ã€AEDçš„é€å±‚æ¶ˆèç ”ç©¶ä»¥åŠå¤šè¯­è¨€åŒ»ç–—ASRçš„è¯­è¨€åˆ†æã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å‡å¯åœ¨ç½‘ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/leduckhai/MultiMed/tree/master/MultiMed%E3%80%82">https://github.com/leduckhai/MultiMed/tree/master/MultiMedã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14074v2">PDF</a> Preprint, 38 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†Multilingualè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„é‡è¦æ€§ï¼Œå¹¶æ¨å‡ºäº†MultiMedå¤šè¯­è¨€åŒ»ç–—ASRæ•°æ®é›†ä»¥åŠä¸€ç³»åˆ—æ¶µç›–äº”ç§è¯­è¨€çš„å°åˆ°å¤§çš„ç«¯åˆ°ç«¯åŒ»ç–—ASRæ¨¡å‹ã€‚MultiMedæ˜¯ç›®å‰å…¨çƒæœ€å¤§çš„åŒ»ç–—ASRæ•°æ®é›†ï¼ŒåŒ…å«å¯å¤ç°çš„å®è¯åŸºå‡†æµ‹è¯•ã€å•è¯­-å¤šè¯­åˆ†æã€æ³¨æ„åŠ›ç¼–ç å™¨è§£ç å™¨ï¼ˆAEDï¼‰ä¸æ··åˆæ¨¡å‹çš„æ¯”è¾ƒï¼Œä»¥åŠAEDçš„é€å±‚æ¶ˆèåˆ†æå’Œå¤šè¯­è¨€åŒ»ç–—ASRçš„è¯­è¨€å­¦åˆ†æã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å‡å¯åœ¨ç½‘ä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Multilingual ASRåœ¨åŒ»ç–—é¢†åŸŸæœ‰é‡è¦åº”ç”¨ï¼Œå¦‚è¯­è¨€ç¿»è¯‘ã€å£è¯­ç†è§£å’Œè¯­éŸ³åŠ©æ‰‹ç­‰ã€‚</li>
<li>MultiMedæ•°æ®é›†æ˜¯å…¨çƒæœ€å¤§çš„åŒ»ç–—ASRæ•°æ®é›†ï¼ŒåŒ…å«å¤šç§è¯­è¨€å’Œå¤§é‡çš„å½•éŸ³ã€å£éŸ³å’Œè¯´è¯äººè§’è‰²ã€‚</li>
<li>æ¨å‡ºäº†æ¶µç›–äº”ç§è¯­è¨€çš„ç«¯åˆ°ç«¯åŒ»ç–—ASRæ¨¡å‹ã€‚</li>
<li>æä¾›äº†å¯å¤ç°çš„å®è¯åŸºå‡†æµ‹è¯•ã€å•è¯­-å¤šè¯­åˆ†æä»¥åŠAEDä¸Hybridæ¨¡å‹çš„æ¯”è¾ƒã€‚</li>
<li>è¿›è¡Œäº†AEDçš„é€å±‚æ¶ˆèåˆ†æã€‚</li>
<li>è¿›è¡Œäº†å¤šè¯­è¨€åŒ»ç–—ASRçš„è¯­è¨€å­¦åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ad0f5e00f1d5b65f62bd19ef746fd1f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-507c39eb998a3135daf673ff5962c0c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-073bb1c62c083326da5fc5b592653b7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b374ef1dd2943af3b084e1b04816a48.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="EffectiveASR-A-Single-Step-Non-Autoregressive-Mandarin-Speech-Recognition-Architecture-with-High-Accuracy-and-Inference-Speed"><a href="#EffectiveASR-A-Single-Step-Non-Autoregressive-Mandarin-Speech-Recognition-Architecture-with-High-Accuracy-and-Inference-Speed" class="headerlink" title="EffectiveASR: A Single-Step Non-Autoregressive Mandarin Speech   Recognition Architecture with High Accuracy and Inference Speed"></a>EffectiveASR: A Single-Step Non-Autoregressive Mandarin Speech   Recognition Architecture with High Accuracy and Inference Speed</h2><p><strong>Authors:Ziyang Zhuang, Chenfeng Miao, Kun Zou, Ming Fang, Tao Wei, Zijian Li, Ning Cheng, Wei Hu, Shaojun Wang, Jing Xiao</strong></p>
<p>Non-autoregressive (NAR) automatic speech recognition (ASR) models predict tokens independently and simultaneously, bringing high inference speed. However, there is still a gap in the accuracy of the NAR models compared to the autoregressive (AR) models. In this paper, we propose a single-step NAR ASR architecture with high accuracy and inference speed, called EffectiveASR. It uses an Index Mapping Vector (IMV) based alignment generator to generate alignments during training, and an alignment predictor to learn the alignments for inference. It can be trained end-to-end (E2E) with cross-entropy loss combined with alignment loss. The proposed EffectiveASR achieves competitive results on the AISHELL-1 and AISHELL-2 Mandarin benchmarks compared to the leading models. Specifically, it achieves character error rates (CER) of 4.26%&#x2F;4.62% on the AISHELL-1 dev&#x2F;test dataset, which outperforms the AR Conformer with about 30x inference speedup. </p>
<blockquote>
<p>éè‡ªå›å½’ï¼ˆNARï¼‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹å¯ä»¥ç‹¬ç«‹ä¸”åŒæ—¶è¿›è¡Œä»¤ç‰Œé¢„æµ‹ï¼Œä»è€Œå®ç°é«˜é€Ÿæ¨æ–­ã€‚ç„¶è€Œï¼Œä¸è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ç›¸æ¯”ï¼ŒNARæ¨¡å‹çš„å‡†ç¡®æ€§ä»å­˜åœ¨å·®è·ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…·æœ‰é«˜ç²¾åº¦å’Œæ¨æ–­é€Ÿåº¦çš„å•æ­¥NAR ASRæ¶æ„ï¼Œç§°ä¸ºEffectiveASRã€‚å®ƒä½¿ç”¨åŸºäºç´¢å¼•æ˜ å°„å‘é‡ï¼ˆIMVï¼‰çš„å¯¹é½ç”Ÿæˆå™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆå¯¹é½ï¼Œå¹¶å¯¹é½é¢„æµ‹å™¨æ¥å­¦ä¹ æ¨æ–­çš„å¯¹é½ã€‚å®ƒå¯ä»¥ç»“åˆäº¤å‰ç†µæŸå¤±å’Œå¯¹é½æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰è®­ç»ƒã€‚ä¸é¢†å…ˆæ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„EffectiveASRåœ¨AISHELL-1å’ŒAISHELL-2æ™®é€šè¯åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒåœ¨AISHELL-1å¼€å‘&#x2F;æµ‹è¯•æ•°æ®é›†ä¸Šå®ç°äº†4.26%&#x2F;4.62%çš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ï¼Œè¿™ä¼˜äºå…·æœ‰çº¦30å€æ¨æ–­é€Ÿåº¦æå‡çš„AR Conformeræ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.08835v4">PDF</a> Accepted by IEEE International Conference on Acoustics, Speech, and   Signal Processing (ICASSP) 2025</p>
<p><strong>Summary</strong><br>     éè‡ªå›å½’ï¼ˆNARï¼‰è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹é€šè¿‡ç‹¬ç«‹ä¸”å¹¶è¡Œé¢„æµ‹è¯­éŸ³ç¬¦å·ï¼Œå®ç°äº†é«˜é€Ÿæ¨æ–­ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…·æœ‰é«˜ç²¾åº¦å’Œé«˜é€Ÿæ¨æ–­çš„å•æ­¥NAR ASRæ¶æ„ï¼Œåä¸ºEffectiveASRã€‚å®ƒä½¿ç”¨åŸºäºç´¢å¼•æ˜ å°„å‘é‡ï¼ˆIMVï¼‰çš„å¯¹é½ç”Ÿæˆå™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆå¯¹é½ï¼Œå¹¶å¯¹é½é¢„æµ‹å™¨ä»¥å­¦ä¹ æ¨æ–­æ—¶çš„å¯¹é½ã€‚å®ƒå¯ä»¥ä¸äº¤å‰ç†µæŸå¤±å’Œå¯¹é½æŸå¤±ç›¸ç»“åˆè¿›è¡Œç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰è®­ç»ƒã€‚åœ¨AISHELL-1å’ŒAISHELL-2æ±‰è¯­åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEffectiveASRä¸é¢†å…ˆæ¨¡å‹ç›¸æ¯”å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåœ¨AISHELL-1å¼€å‘&#x2F;æµ‹è¯•æ•°æ®é›†ä¸Šå®ç°äº†å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸º4.26%&#x2F;4.62%ï¼Œç›¸è¾ƒäºè‡ªå›å½’Conformeræ¨¡å‹ï¼Œåœ¨æ¨æ–­é€Ÿåº¦ä¸ŠåŠ å¿«äº†çº¦30å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NAR ASRæ¨¡å‹é€šè¿‡ç‹¬ç«‹é¢„æµ‹è¯­éŸ³ç¬¦å·å®ç°é«˜é€Ÿæ¨æ–­ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºEffectiveASRçš„å•æ­¥NAR ASRæ¶æ„ï¼Œå…·æœ‰é«˜ç²¾åº¦å’Œé«˜é€Ÿæ¨æ–­çš„ç‰¹ç‚¹ã€‚</li>
<li>EffectiveASRä½¿ç”¨IMVåŸºäºå¯¹é½ç”Ÿæˆå™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆå¯¹é½ã€‚</li>
<li>EffectiveASRå¯ä»¥é€šè¿‡ç»“åˆäº¤å‰ç†µæŸå¤±å’Œå¯¹é½æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰è®­ç»ƒã€‚</li>
<li>EffectiveASRåœ¨AISHELL-1å’ŒAISHELL-2æ±‰è¯­åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</li>
<li>EffectiveASRåœ¨AISHELL-1æ•°æ®é›†ä¸Šçš„å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸º4.26%å’Œ4.62%ï¼Œæ€§èƒ½ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.08835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b61846478130d3f1e7a5aa82922c0380.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a440c5d734079a31b0769957bf3790c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5baa6488d7adc795e1b015da01110215.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Separate-and-Reconstruct-Asymmetric-Encoder-Decoder-for-Speech-Separation"><a href="#Separate-and-Reconstruct-Asymmetric-Encoder-Decoder-for-Speech-Separation" class="headerlink" title="Separate and Reconstruct: Asymmetric Encoder-Decoder for Speech   Separation"></a>Separate and Reconstruct: Asymmetric Encoder-Decoder for Speech   Separation</h2><p><strong>Authors:Ui-Hyeop Shin, Sangyoun Lee, Taehan Kim, Hyung-Min Park</strong></p>
<p>In speech separation, time-domain approaches have successfully replaced the time-frequency domain with latent sequence feature from a learnable encoder. Conventionally, the feature is separated into speaker-specific ones at the final stage of the network. Instead, we propose a more intuitive strategy that separates features earlier by expanding the feature sequence to the number of speakers as an extra dimension. To achieve this, an asymmetric strategy is presented in which the encoder and decoder are partitioned to perform distinct processing in separation tasks. The encoder analyzes features, and the output of the encoder is split into the number of speakers to be separated. The separated sequences are then reconstructed by the weight-shared decoder, which also performs cross-speaker processing. Without relying on speaker information, the weight-shared network in the decoder directly learns to discriminate features using a separation objective. In addition, to improve performance, traditional methods have extended the sequence length, leading to the adoption of dual-path models, which handle the much longer sequence effectively by segmenting it into chunks. To address this, we introduce global and local Transformer blocks that can directly handle long sequences more efficiently without chunking and dual-path processing. The experimental results demonstrated that this asymmetric structure is effective and that the combination of proposed global and local Transformer can sufficiently replace the role of inter- and intra-chunk processing in dual-path structure. Finally, the presented model combining both of these achieved state-of-the-art performance with much less computation in various benchmark datasets. </p>
<blockquote>
<p>åœ¨è¯­éŸ³åˆ†ç¦»é¢†åŸŸï¼Œæ—¶åŸŸæ–¹æ³•å·²ç»æˆåŠŸåœ°ç”¨å¯å­¦ä¹ çš„ç¼–ç å™¨çš„æ½œåœ¨åºåˆ—ç‰¹å¾æ›¿æ¢äº†æ—¶é¢‘åŸŸã€‚ä¼ ç»Ÿä¸Šï¼Œç‰¹å¾åœ¨ç½‘ç»œçš„æœ€åé˜¶æ®µè¢«åˆ†ç¦»æˆé’ˆå¯¹è¯´è¯è€…çš„ç‰¹å¾ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›´ç›´è§‚çš„ç­–ç•¥ï¼Œé€šè¿‡æ‰©å¤§ç‰¹å¾åºåˆ—åˆ°è¯´è¯è€…çš„æ•°é‡ä½œä¸ºä¸€ä¸ªé¢å¤–çš„ç»´åº¦æ¥æ›´æ—©åœ°åˆ†ç¦»ç‰¹å¾ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæå‡ºäº†ä¸€ç§ä¸å¯¹ç§°çš„ç­–ç•¥ï¼Œå°†ç¼–ç å™¨å’Œè§£ç å™¨åˆ’åˆ†å¼€æ¥æ‰§è¡Œåˆ†ç¦»ä»»åŠ¡ä¸­çš„ä¸åŒå¤„ç†ã€‚ç¼–ç å™¨åˆ†æç‰¹å¾ï¼Œç¼–ç å™¨çš„è¾“å‡ºè¢«åˆ†å‰²æˆè¦åˆ†ç¦»çš„è¯´è¯è€…æ•°é‡ã€‚ç„¶åï¼Œåˆ†ç¦»çš„åºåˆ—è¢«æƒé‡å…±äº«çš„è§£ç å™¨é‡å»ºï¼Œè¯¥è§£ç å™¨ä¹Ÿæ‰§è¡Œè·¨è¯´è¯è€…çš„å¤„ç†ã€‚ä¸ä¾èµ–è¯´è¯è€…ä¿¡æ¯ï¼Œè§£ç å™¨ä¸­çš„æƒé‡å…±äº«ç½‘ç»œç›´æ¥é€šè¿‡åˆ†ç¦»ç›®æ ‡å­¦ä¹ è¾¨åˆ«ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜æ€§èƒ½ï¼Œä¼ ç»Ÿæ–¹æ³•å·²ç»å»¶é•¿äº†åºåˆ—é•¿åº¦ï¼Œå¯¼è‡´é‡‡ç”¨åŒè·¯å¾„æ¨¡å‹ï¼Œé€šè¿‡å°†å…¶åˆ†å‰²æˆå—æ¥æœ‰æ•ˆå¤„ç†æ›´é•¿çš„åºåˆ—ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨å±€å’Œå±€éƒ¨Transformerå—ï¼Œèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ç›´æ¥å¤„ç†é•¿åºåˆ—ï¼Œè€Œæ— éœ€åˆ†å—å’ŒåŒè·¯å¾„å¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ä¸å¯¹ç§°ç»“æ„æ˜¯æœ‰æ•ˆçš„ï¼Œæ‰€æå‡ºçš„å…¨å±€å’Œå±€éƒ¨Transformerçš„ç»„åˆå¯ä»¥å……åˆ†æ›¿ä»£åŒè·¯å¾„ç»“æ„ä¸­å—å†…å’Œå—é—´çš„å¤„ç†è§’è‰²ã€‚æœ€åï¼Œç»“åˆè¿™ä¸¤ç§æ–¹æ³•çš„æ¨¡å‹åœ¨å„ç§åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å¤§å¤§å‡å°‘äº†è®¡ç®—é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.05983v4">PDF</a> In NeurIPS 2024; Project Page:   <a target="_blank" rel="noopener" href="https://dmlguq456.github.io/SepReformer_Demo">https://dmlguq456.github.io/SepReformer_Demo</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³åˆ†ç¦»é¢†åŸŸçš„ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨å¯å­¦ä¹ ç¼–ç å™¨çš„æ½œåœ¨åºåˆ—ç‰¹å¾æ›¿æ¢æ—¶é¢‘åŸŸç‰¹å¾ï¼Œå¹¶æå‰è¿›è¡Œç‰¹å¾åˆ†ç¦»ã€‚é€šè¿‡æ‰©å±•ç‰¹å¾åºåˆ—ä»¥åŒ¹é…è¯´è¯äººæ•°æ¥å®ç°æ­¤ç›®çš„ã€‚æ–‡ç« æå‡ºäº†ä¸å¯¹ç§°ç­–ç•¥ï¼Œä½¿ç¼–ç å™¨å’Œè§£ç å™¨åœ¨åˆ†ç¦»ä»»åŠ¡ä¸­è¿›è¡Œä¸åŒçš„å¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ä¸å¯¹ç§°ç»“æ„ç»“åˆå…¨å±€å’Œå±€éƒ¨Transformerèƒ½å¤Ÿæœ‰æ•ˆåœ°å®ç°è¯­éŸ³åˆ†ç¦»ï¼Œå¹¶è¾¾åˆ°äº†è¾ƒé«˜çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨å¯å­¦ä¹ ç¼–ç å™¨çš„æ½œåœ¨åºåˆ—ç‰¹å¾æ›¿ä»£ä¼ ç»Ÿçš„æ—¶é—´é¢‘åŸŸæ–¹æ³•æ¥è¿›è¡Œè¯­éŸ³åˆ†ç¦»ã€‚</li>
<li>æå‡ºä¸€ç§æå‰ç‰¹å¾åˆ†ç¦»çš„ç­–ç•¥ï¼Œé€šè¿‡æ‰©å±•ç‰¹å¾åºåˆ—ä¸è¯´è¯äººæ•°åŒ¹é…å®ç°ã€‚</li>
<li>å¼•å…¥ä¸å¯¹ç§°ç­–ç•¥ï¼Œä½¿ç¼–ç å™¨å’Œè§£ç å™¨åœ¨è¯­éŸ³åˆ†ç¦»ä»»åŠ¡ä¸­æ‰¿æ‹…ä¸åŒçš„è§’è‰²ã€‚</li>
<li>è§£ç å™¨æ— éœ€ä¾èµ–è¯´è¯è€…ä¿¡æ¯å³å¯ç›´æ¥å­¦ä¹ åŒºåˆ†ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨å…¨å±€å’Œå±€éƒ¨Transformerä»¥æ›´æœ‰æ•ˆåœ°å¤„ç†é•¿åºåˆ—ï¼Œæ— éœ€åˆ†æ®µå’ŒåŒè·¯å¾„å¤„ç†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ä¸å¯¹ç§°ç»“æ„çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠç»“åˆå…¨å±€å’Œå±€éƒ¨Transformeråœ¨åŒè·¯å¾„ç»“æ„ä¸­çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.05983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8912104f7c6c38f3733fd8d1786bdb71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19914c3f33806758908cae3d8b4da101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db63b77d4b18bfa90ed5e29c1fa51021.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96d88178aecfeda80f145388b6015e17.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-13/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-13/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-52c9bd86dbe9864d1588f070bec47a0a.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-13  The GAN is dead; long live the GAN! A Modern GAN Baseline
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b62557dcde66a30a690cd794a36c8369.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-13  Improving Zero-Shot Object-Level Change Detection by Incorporating   Visual Correspondence
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15534.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
