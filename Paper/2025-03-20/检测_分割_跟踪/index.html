<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-03-20  LEGNet Lightweight Edge-Gaussian Driven Network for Low-Quality Remote   Sensing Image Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9a2d3ac6f0083779454a51cd45ab1b8a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    26 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-20-更新"><a href="#2025-03-20-更新" class="headerlink" title="2025-03-20 更新"></a>2025-03-20 更新</h1><h2 id="LEGNet-Lightweight-Edge-Gaussian-Driven-Network-for-Low-Quality-Remote-Sensing-Image-Object-Detection"><a href="#LEGNet-Lightweight-Edge-Gaussian-Driven-Network-for-Low-Quality-Remote-Sensing-Image-Object-Detection" class="headerlink" title="LEGNet: Lightweight Edge-Gaussian Driven Network for Low-Quality Remote   Sensing Image Object Detection"></a>LEGNet: Lightweight Edge-Gaussian Driven Network for Low-Quality Remote   Sensing Image Object Detection</h2><p><strong>Authors:Wei Lu, Si-Bao Chen, Hui-Dong Li, Qing-Ling Shu, Chris H. Q. Ding, Jin Tang, Bin Luo</strong></p>
<p>Remote sensing object detection (RSOD) faces formidable challenges in complex visual environments. Aerial and satellite images inherently suffer from limitations such as low spatial resolution, sensor noise, blurred objects, low-light degradation, and partial occlusions. These degradation factors collectively compromise the feature discriminability in detection models, resulting in three key issues: (1) reduced contrast that hampers foreground-background separation, (2) structural discontinuities in edge representations, and (3) ambiguous feature responses caused by variations in illumination. These collectively weaken model robustness and deployment feasibility. To address these challenges, we propose LEGNet, a lightweight network that incorporates a novel edge-Gaussian aggregation (EGA) module specifically designed for low-quality remote sensing images. Our key innovation lies in the synergistic integration of Scharr operator-based edge priors with uncertainty-aware Gaussian modeling: (a) The orientation-aware Scharr filters preserve high-frequency edge details with rotational invariance; (b) The uncertainty-aware Gaussian layers probabilistically refine low-confidence features through variance estimation. This design enables precision enhancement while maintaining architectural simplicity. Comprehensive evaluations across four RSOD benchmarks (DOTA-v1.0, v1.5, DIOR-R, FAIR1M-v1.0) and a UAV-view dataset (VisDrone2019) demonstrate significant improvements. LEGNet achieves state-of-the-art performance across five benchmark datasets while ensuring computational efficiency, making it well-suited for deployment on resource-constrained edge devices in real-world remote sensing applications. The code is available at <a target="_blank" rel="noopener" href="https://github.com/lwCVer/LEGNet">https://github.com/lwCVer/LEGNet</a>. </p>
<blockquote>
<p>遥感目标检测（RSOD）在复杂的视觉环境中面临着巨大的挑战。航空和卫星图像本质上存在诸如空间分辨率低、传感器噪声、目标模糊、低光退化和部分遮挡等局限性。这些退化因素集体影响了检测模型中的特征辨别力，导致三个关键问题：（1）对比度降低，妨碍前景背景分离；（2）边缘表示中的结构不连续；（3）由光照变化引起的特征响应模糊。这些共同削弱了模型的稳健性和部署的可行性。为了解决这些挑战，我们提出了LEGNet，这是一个轻量级网络，它融入了一个新型的边缘高斯聚合（EGA）模块，专门针对低质量的遥感图像设计。我们的关键创新在于将基于Scharr算子的边缘先验与不确定性感知高斯建模协同集成：（a）方向感知的Scharr滤波器以旋转不变性保留高频边缘细节；（b）不确定性感知的高斯层通过方差估计概率地优化低置信特征。这一设计既提高了精度，又保持了架构的简单性。在四个RSOD基准（DOTA-v1.0、v1.5、DIOR-R、FAIR1M-v1.0）和一个无人机视角数据集（VisDrone2019）上的全面评估表明，LEGNet取得了显著改进，在五个基准数据集上实现了最先进的性能，同时确保了计算效率，非常适合在资源受限的边缘设备上用于现实世界遥感应用部署。代码可通过<a target="_blank" rel="noopener" href="https://github.com/lwCVer/LEGNet%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lwCVer/LEGNet获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14012v1">PDF</a> 12 pages, 5 figures. Remote Sensing Image Object Detection</p>
<p><strong>Summary</strong></p>
<p>在复杂的视觉环境中，遥感目标检测面临严峻挑战。远程图像因低空间分辨率、传感器噪声、目标模糊、低光照退化和部分遮挡等固有缺陷，导致检测模型的特征辨别能力下降。这些问题集体造成对比减少、边缘断裂以及因光照变化产生的特征模糊，降低模型稳健性和部署可行性。为解决此问题，我们提出LEGNet网络，它结合了新颖的边缘高斯聚合模块，专为低质量遥感图像设计。主要创新在于结合基于Scharr算子的边缘先验与概率化高斯建模，既能保留边缘细节又能通过方差估计优化低置信度特征。在四个遥感目标检测基准测试中表现出卓越性能，同时保证计算效率，适用于资源受限的边缘设备进行遥感应用部署。代码已上传至GitHub供公众查阅。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>遥感目标检测面临诸多挑战，包括图像低空间分辨率和模糊对象等问题。</li>
<li>上述缺陷降低检测模型的性能及识别特征的可靠性。</li>
<li>LEGNet网络设计针对低质量遥感图像提出，包含新颖的边缘高斯聚合模块。</li>
<li>主要创新在于结合基于Scharr算子的边缘先验与概率化高斯建模。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14012">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1768f338648abe5981cd8665c707c068.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-791ec0128e7d69293864fb2eab55cd5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3c64494da2f182f23c2a7aec3a06387.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-901839f9dd68f104edf4325f23b40d31.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FrustumFusionNets-A-Three-Dimensional-Object-Detection-Network-Based-on-Tractor-Road-Scene"><a href="#FrustumFusionNets-A-Three-Dimensional-Object-Detection-Network-Based-on-Tractor-Road-Scene" class="headerlink" title="FrustumFusionNets: A Three-Dimensional Object Detection Network Based on   Tractor Road Scene"></a>FrustumFusionNets: A Three-Dimensional Object Detection Network Based on   Tractor Road Scene</h2><p><strong>Authors:Lili Yang, Mengshuai Chang, Xiao Guo, Yuxin Feng, Yiwen Mei, Caicong Wu</strong></p>
<p>To address the issues of the existing frustum-based methods’ underutilization of image information in road three-dimensional object detection as well as the lack of research on agricultural scenes, we constructed an object detection dataset using an 80-line Light Detection And Ranging (LiDAR) and a camera in a complex tractor road scene and proposed a new network called FrustumFusionNets (FFNets). Initially, we utilize the results of image-based two-dimensional object detection to narrow down the search region in the three-dimensional space of the point cloud. Next, we introduce a Gaussian mask to enhance the point cloud information. Then, we extract the features from the frustum point cloud and the crop image using the point cloud feature extraction pipeline and the image feature extraction pipeline, respectively. Finally, we concatenate and fuse the data features from both modalities to achieve three-dimensional object detection. Experiments demonstrate that on the constructed test set of tractor road data, the FrustumFusionNetv2 achieves 82.28% and 95.68% accuracy in the three-dimensional object detection of the two main road objects, cars and people, respectively. This performance is 1.83% and 2.33% better than the original model. It offers a hybrid fusion-based multi-object, high-precision, real-time three-dimensional object detection technique for unmanned agricultural machines in tractor road scenarios. On the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) Benchmark Suite validation set, the FrustumFusionNetv2 also demonstrates significant superiority in detecting road pedestrian objects compared with other frustum-based three-dimensional object detection methods. </p>
<blockquote>
<p>为了解决现有基于截锥体方法在道路三维目标检测中对图像信息利用不足的问题以及在农业场景研究方面的缺乏，我们构建了一个目标检测数据集，该数据集使用拖拉机道路场景中一台复杂的80线激光雷达和摄像机。在此基础上，我们提出了一种新的网络结构，称为FrustumFusionNets（FFNets）。首先，我们利用基于图像的两维目标检测结果来缩小点云的三维搜索区域。接下来，我们引入高斯掩膜以增强点云信息。然后，我们从截锥点云和作物图像中提取特征，分别使用点云特征提取管道和图像特征提取管道。最后，我们将两种模态的数据特征进行拼接和融合，以实现三维目标检测。实验表明，在构建的拖拉机道路测试集上，FrustumFusionNetv2针对主要道路目标车辆和行人，在三维目标检测方面的准确率分别达到82.28%和95.68%。这一性能比原始模型提高了1.83%和2.33%。它为无人驾驶农业机器在拖拉机道路上的场景中提供了一种基于混合融合的多目标、高精度、实时的三维目标检测技术。在KITTI基准套件验证集上，与其他的基于截锥体的三维目标检测方法相比，FrustumFusionNetv2在检测道路行人目标方面表现出显著的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13951v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对现有基于Frustum的方法对图像信息利用不足的问题，以及农业场景研究缺乏的问题，我们构建了一个使用激光雷达和拖拉机道路场景相机的对象检测数据集，并提出了一个新的网络——FrustumFusionNets（FFNets）。首先，利用基于图像的二维对象检测结果缩小点云的三维搜索区域。接着，引入高斯掩膜增强点云信息。然后，分别从点云和农作物图像中提取特征，并通过拼接融合两种模态的数据特征来实现三维对象检测。实验表明，在构建的拖拉机道路测试集上，FrustumFusionNetv2对主要道路对象汽车和行人的三维对象检测准确率分别达到82.28%和95.68%，较原模型提升1.83%和2.33%。该模型为无人驾驶农业机械在拖拉机道路上的多对象、高精度、实时三维对象检测提供了混合融合技术。在KITTI基准套件验证集上，与其他的基于Frustum的三维对象检测方法相比，该模型在检测道路行人对象方面具有显著优势。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>针对现有方法图像信息利用不足的问题，构建了基于激光雷达和相机的拖拉机道路场景对象检测数据集。</li>
<li>提出了一种新的网络结构——FrustumFusionNets（FFNets）。</li>
<li>利用二维图像检测结果缩小三维点云的搜索区域。</li>
<li>通过引入高斯掩膜增强点云信息。</li>
<li>实现了从点云和农作物图像中提取特征并进行三维对象检测。</li>
<li>在拖拉机道路测试集上，新模型较原模型有显著提升。</li>
<li>模型为无人驾驶农业机械提供了混合融合技术的多对象、高精度、实时三维对象检测方案。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13951">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-204845204a87dc53f24c90eeb70f0d2f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HSOD-BIT-V2-A-New-Challenging-Benchmarkfor-Hyperspectral-Salient-Object-Detection"><a href="#HSOD-BIT-V2-A-New-Challenging-Benchmarkfor-Hyperspectral-Salient-Object-Detection" class="headerlink" title="HSOD-BIT-V2: A New Challenging Benchmarkfor Hyperspectral Salient Object   Detection"></a>HSOD-BIT-V2: A New Challenging Benchmarkfor Hyperspectral Salient Object   Detection</h2><p><strong>Authors:Yuhao Qiu, Shuyan Bai, Tingfa Xu, Peifu Liu, Haolin Qin, Jianan Li</strong></p>
<p>Salient Object Detection (SOD) is crucial in computer vision, yet RGB-based methods face limitations in challenging scenes, such as small objects and similar color features. Hyperspectral images provide a promising solution for more accurate Hyperspectral Salient Object Detection (HSOD) by abundant spectral information, while HSOD methods are hindered by the lack of extensive and available datasets. In this context, we introduce HSOD-BIT-V2, the largest and most challenging HSOD benchmark dataset to date. Five distinct challenges focusing on small objects and foreground-background similarity are designed to emphasize spectral advantages and real-world complexity. To tackle these challenges, we propose Hyper-HRNet, a high-resolution HSOD network. Hyper-HRNet effectively extracts, integrates, and preserves effective spectral information while reducing dimensionality by capturing the self-similar spectral features. Additionally, it conveys fine details and precisely locates object contours by incorporating comprehensive global information and detailed object saliency representations. Experimental analysis demonstrates that Hyper-HRNet outperforms existing models, especially in challenging scenarios. </p>
<blockquote>
<p>显著性目标检测（SOD）在计算机视觉中至关重要，然而基于RGB的方法在具有挑战性的场景中，如小物体和相似颜色特征方面存在局限性。高光谱图像通过丰富的光谱信息为解决更准确的高光谱显著性目标检测（HSOD）问题提供了有前景的解决方案。然而，由于缺乏广泛可用的数据集，HSOD方法受到阻碍。在此背景下，我们介绍了迄今为止最大且最具挑战性的高光谱显著性目标检测数据集HSOD-BIT-V2。我们设计了五个独特挑战，重点关注小物体和前景背景相似性，以突出光谱优势和现实世界复杂性。为了解决这些挑战，我们提出了高分辨率高光谱显著性目标检测网络Hyper-HRNet。Hyper-HRNet能够有效地提取、集成和保留有效的光谱信息，通过捕获自相似光谱特征来降低维度。此外，它结合全面的全局信息和详细的对象显著性表示，传递了精细的细节并准确地对对象轮廓进行了定位。实验分析表明，Hyper-HRNet在具有挑战性的场景中表现优于现有模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13906v1">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于计算机视觉的显著性目标检测（SOD）的重要性，指出RGB方法在某些复杂场景中存在局限性。为解决这一问题，研究者引入超光谱图像作为解决方案，并推出迄今为止最大的超光谱显著性目标检测（HSOD）数据集HSOD-BIT-V2。针对超光谱图像的特点和实际应用场景中的复杂性，设计五个具有挑战性的小目标识别场景及五个背景识别挑战场景。此外，提出了基于Hyper-HRNet的超光谱高分辨率目标检测网络。该网络能有效提取、整合和保留有效的光谱信息，通过捕捉自相似光谱特征降低维度，同时结合全局信息和详细的对象显著性表示，精细地描绘出物体的轮廓。实验表明，Hyper-HRNet相较于现有模型性能更优，特别是在复杂场景下。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RGB方法在处理某些复杂场景的显著性目标检测（SOD）时存在局限性。</li>
<li>超光谱图像提供了解决此问题的有前景的解决方案，拥有丰富的光谱信息以提高准确性。</li>
<li>HSOD数据集HSOD-BIT-V2是目前最大的超光谱显著性目标检测数据集，包含五个具有挑战性的小目标识别场景和五个背景识别挑战场景。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13906">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d698f8db3bb5fd7d9ce28cbf562c2882.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a25afaa0aed6520389ca10e34df815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bbd2f5d68a19e30032d0009a3d98964.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45596f5fcdfe9bf187d00da7e4ff879b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4a51350df28c87fca51337d1ea88627.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-034ca91c43af03854e4fdfca10364d09.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Biologically-inspired-Semi-supervised-Semantic-Segmentation-for-Biomedical-Imaging"><a href="#Biologically-inspired-Semi-supervised-Semantic-Segmentation-for-Biomedical-Imaging" class="headerlink" title="Biologically-inspired Semi-supervised Semantic Segmentation for   Biomedical Imaging"></a>Biologically-inspired Semi-supervised Semantic Segmentation for   Biomedical Imaging</h2><p><strong>Authors:Luca Ciampi, Gabriele Lagani, Giuseppe Amato, Fabrizio Falchi</strong></p>
<p>We propose a novel bio-inspired semi-supervised learning approach for training downsampling-upsampling semantic segmentation architectures. The first stage does not use backpropagation. Rather, it exploits the Hebbian principle &#96;&#96;fire together, wire together’’ as a local learning rule for updating the weights of both convolutional and transpose-convolutional layers, allowing unsupervised discovery of data features. In the second stage, the model is fine-tuned with standard backpropagation on a small subset of labeled data. We evaluate our methodology through experiments conducted on several widely used biomedical datasets, deeming that this domain is paramount in computer vision and is notably impacted by data scarcity. Results show that our proposed method outperforms SOTA approaches across different levels of label availability. Furthermore, we show that using our unsupervised stage to initialize the SOTA approaches leads to performance improvements. The code to replicate our experiments can be found at <a target="_blank" rel="noopener" href="https://github.com/ciampluca/hebbian-bootstraping-semi-supervised-medical-imaging">https://github.com/ciampluca/hebbian-bootstraping-semi-supervised-medical-imaging</a> </p>
<blockquote>
<p>我们提出了一种新型的生物启发半监督学习方法，用于训练下采样-上采样语义分割架构。第一阶段不使用反向传播。相反，它利用赫布原则“一起发射，一起连接”作为局部学习规则，以更新卷积层和转置卷积层的权重，从而实现数据特征的无监督发现。在第二阶段，模型使用标准反向传播对小部分标记数据进行微调。我们通过实验评估了我们的方法，实验是在几个广泛使用的生物医学数据集上进行的，我们认为这个领域在计算机视觉领域中至关重要，并且受到数据稀缺的显著影响。结果表明，我们的方法在标签可用性的不同层次上都优于最新技术方法。此外，我们还表明使用我们的无监督阶段来初始化最新技术方法可以提高性能。复制我们实验的代码可以在 <a target="_blank" rel="noopener" href="https://github.com/ciampluca/hebbian-bootstraping-semi-supervised-medical-imaging">https://github.com/ciampluca/hebbian-bootstraping-semi-supervised-medical-imaging</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03192v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的生物启发半监督学习方法，用于训练下采样-上采样语义分割架构。该方法分为两个阶段，第一阶段不采用反向传播，而是利用赫布原理（一起放电，一起连接）作为局部学习规则来更新卷积和转置卷积层的权重，实现数据特征的无监督发现。第二阶段使用标准反向传播对模型进行微调，仅使用少量标记数据进行训练。在多个广泛使用的生物医学数据集上进行的实验表明，该方法在不同标签可用性级别上均优于现有技术。此外，使用我们的无监督阶段初始化现有技术还可以提高性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的半监督学习方法，用于训练语义分割架构。</li>
<li>方法分为两个阶段：第一阶段利用赫布原理进行无监督学习，更新权重。</li>
<li>第二阶段使用标准反向传播进行微调，使用少量标记数据。</li>
<li>实验在多个生物医学数据集上进行，显示方法在不同标签可用性级别上优于现有技术。</li>
<li>该方法有助于提高现有技术的性能，通过无监督阶段进行初始化。</li>
<li>该方法对于数据稀缺领域（如生物医学图像）特别有影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03192">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-33892d7ae6a885c9c8a3d5b83905fb3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f10b4932e9fd6d658fd7949a9958b97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0961590f89164df6b22d0ab8cf5b68c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6220ccdad717656c44be42e7167a1eb5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f15608aad095c3f68039a3af86063684.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Review-of-Human-Object-Interaction-Detection"><a href="#A-Review-of-Human-Object-Interaction-Detection" class="headerlink" title="A Review of Human-Object Interaction Detection"></a>A Review of Human-Object Interaction Detection</h2><p><strong>Authors:Yuxiao Wang, Yu Lei, Li Cui, Weiying Xue, Qi Liu, Zhenao Wei</strong></p>
<p>Human-object interaction (HOI) detection plays a key role in high-level visual understanding, facilitating a deep comprehension of human activities. Specifically, HOI detection aims to locate the humans and objects involved in interactions within images or videos and classify the specific interactions between them. The success of this task is influenced by several key factors, including the accurate localization of human and object instances, as well as the correct classification of object categories and interaction relationships. This paper systematically summarizes and discusses the recent work in image-based HOI detection. First, the mainstream datasets involved in HOI relationship detection are introduced. Furthermore, starting with two-stage methods and end-to-end one-stage detection approaches, this paper comprehensively discusses the current developments in image-based HOI detection, analyzing the strengths and weaknesses of these two methods. Additionally, the advancements of zero-shot learning, weakly supervised learning, and the application of large-scale language models in HOI detection are discussed. Finally, the current challenges in HOI detection are outlined, and potential research directions and future trends are explored. </p>
<blockquote>
<p>人机交互（HOI）检测在高层次视觉理解中扮演着关键角色，有助于深入理解人类活动。具体而言，HOI检测旨在定位图像或视频中涉及交互的人类和物体，并分类它们之间的特定交互。该任务的成功受到几个关键因素的影响，包括人类和物体实例的准确定位，以及物体类别和交互关系的正确分类。本文系统地总结和讨论了基于图像的HOI检测的最新工作。首先，介绍了HOI关系检测涉及的主流数据集。此外，本文综合讨论了基于图像的HOI检测的当前发展，包括两阶段方法和端到端的一阶段检测方案，分析了这两种方法的优缺点。另外，还讨论了零样本学习、弱监督学习以及大规模语言模型在HOI检测中的应用。最后，概述了HOI检测当前的挑战，并探讨了潜在的研究方向和未来趋势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10641v3">PDF</a> Accepted by 2024 2nd International Conference on Computer, Vision and   Intelligent Technology (ICCVIT)</p>
<p><strong>Summary</strong></p>
<p>本文综述了基于图像的HOI检测（人机交互检测）的最新工作。介绍了主流数据集，并详细讨论了当前的两阶段方法和端到端的一阶段检测方法的进展，分析了它们的优缺点。此外，还讨论了零样本学习、弱监督学习以及大规模语言模型在HOI检测中的应用，指出了当前挑战并探讨了未来研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HOI检测是高级视觉理解中的关键任务，旨在识别图像或视频中的人类与物体的交互，并分类它们之间的交互。</li>
<li>主要数据集介绍。</li>
<li>当前的两阶段方法和一阶段检测方法的进展，以及它们的优缺点。</li>
<li>零样本学习、弱监督学习在HOI检测中的应用。</li>
<li>大规模语言模型在HOI检测中的使用正在增长。</li>
<li>HOI检测当前面临的挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10641">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9a2d3ac6f0083779454a51cd45ab1b8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-521833083ce03915fa9881a48e46f716.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61bd2816199f5d3c2f217225352df434.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e56ed6c302e330bb24da1b64f5575e84.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Semi-Supervised-Semantic-Segmentation-Based-on-Pseudo-Labels-A-Survey"><a href="#Semi-Supervised-Semantic-Segmentation-Based-on-Pseudo-Labels-A-Survey" class="headerlink" title="Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey"></a>Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey</h2><p><strong>Authors:Lingyan Ran, Yali Li, Guoqiang Liang, Yanning Zhang</strong></p>
<p>Semantic segmentation is an important and popular research area in computer vision that focuses on classifying pixels in an image based on their semantics. However, supervised deep learning requires large amounts of data to train models and the process of labeling images pixel by pixel is time-consuming and laborious. This review aims to provide a first comprehensive and organized overview of the state-of-the-art research results on pseudo-label methods in the field of semi-supervised semantic segmentation, which we categorize from different perspectives and present specific methods for specific application areas. In addition, we explore the application of pseudo-label technology in medical and remote-sensing image segmentation. Finally, we also propose some feasible future research directions to address the existing challenges. </p>
<blockquote>
<p>语义分割是计算机视觉中一个重要且热门的研究领域，主要关注基于图像语义对像素进行分类。然而，深度学习的监督学习需要大量的数据来训练模型，逐个像素地标注图像既耗时又费力。这篇综述旨在全面、系统地介绍半监督语义分割领域伪标签方法的最新研究成果，从不同的角度对它们进行分类，并针对特定应用领域介绍具体方法。此外，我们还探讨了伪标签技术在医疗和遥感图像分割中的应用。最后，我们还提出了一些可行的未来研究方向，以应对现有挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.01909v3">PDF</a> Accepted by IEEE Transactions on Circuits and Systems for Video   Technology(TCSVT)</p>
<p><strong>Summary</strong><br>语义分割是计算机视觉中一个重要且热门的研究领域，主要对图像中的像素进行分类。然而，监督深度学习需要大量数据进行模型训练，逐像素标注图像的过程耗时费力。本文旨在提供伪标签方法在半监督语义分割领域的最新研究成果的首次全面概述，从不同角度分类并呈现特定应用领域的具体方法。此外，本文还探讨了伪标签技术在医疗和遥感图像分割中的应用，并提出解决现有挑战的可行的未来研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义分割是计算机视觉中的重要研究领域，专注于基于像素语义对图像进行分类。</li>
<li>监督深度学习需要大量数据进行模型训练，标注过程耗时费力。</li>
<li>伪标签方法在半监督语义分割领域具有广泛的应用前景。</li>
<li>本文提供了伪标签方法的全面概述，包括从不同角度的分类以及特定领域的应用方法。</li>
<li>伪标签技术在医疗和遥感图像分割领域的应用得到了探讨。</li>
<li>当前存在挑战和问题需要解决，需要进一步的研究和探索。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.01909">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e61ae1cc5d887d26086ba20ed50b1d36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aad6bbbd8d18cc7d69a2aff258df9a2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50c235727093679ffd70b362dce66667.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c80a069b21c7cb3e3e897e7994498a1d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Co-Learning-Semantic-aware-Unsupervised-Segmentation-for-Pathological-Image-Registration"><a href="#Co-Learning-Semantic-aware-Unsupervised-Segmentation-for-Pathological-Image-Registration" class="headerlink" title="Co-Learning Semantic-aware Unsupervised Segmentation for Pathological   Image Registration"></a>Co-Learning Semantic-aware Unsupervised Segmentation for Pathological   Image Registration</h2><p><strong>Authors:Yang Liu, Shi Gu</strong></p>
<p>The registration of pathological images plays an important role in medical applications. Despite its significance, most researchers in this field primarily focus on the registration of normal tissue into normal tissue. The negative impact of focal tissue, such as the loss of spatial correspondence information and the abnormal distortion of tissue, are rarely considered. In this paper, we propose GIRNet, a novel unsupervised approach for pathological image registration by incorporating segmentation and inpainting through the principles of Generation, Inpainting, and Registration (GIR). The registration, segmentation, and inpainting modules are trained simultaneously in a co-learning manner so that the segmentation of the focal area and the registration of inpainted pairs can improve collaboratively. Overall, the registration of pathological images is achieved in a completely unsupervised learning framework. Experimental results on multiple datasets, including Magnetic Resonance Imaging (MRI) of T1 sequences, demonstrate the efficacy of our proposed method. Our results show that our method can accurately achieve the registration of pathological images and identify lesions even in challenging imaging modalities. Our unsupervised approach offers a promising solution for the efficient and cost-effective registration of pathological images. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/brain-intelligence-lab/GIRNet">https://github.com/brain-intelligence-lab/GIRNet</a>. </p>
<blockquote>
<p>病理图像的配准在医疗应用中扮演着重要角色。尽管其意义重大，但该领域的大多数研究者主要关注正常组织的配准到正常组织。很少考虑焦点组织的负面影响，如空间对应关系信息的丢失和组织异常扭曲。在本文中，我们提出了GIRNet，这是一种通过结合分割和修复的原则（生成、修复和配准（GIR））用于病理图像配准的新型无监督方法。配准、分割和修复模块以协同学习的方式进行训练，使得焦点区域的分割和修复对的配准可以协同改进。总体而言，病理图像的配准是在一个完全无监督的学习框架中实现的。在多个数据集上的实验结果，包括T1序列的磁共振成像（MRI），证明了我们提出的方法的有效性。我们的结果表明，我们的方法可以准确地实现病理图像的配准，即使在具有挑战性的成像模式下也能识别病变。我们的无监督方法为病理图像的有效和经济的配准提供了有前景的解决方案。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/brain-intelligence-lab/GIRNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/brain-intelligence-lab/GIRNet找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11040v3">PDF</a> 13 pages, 7 figures, published in Medical Image Computing and   Computer Assisted Intervention (MICCAI) 2023</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于生成、修复和注册（GIR）原则的新型无监督病理图像注册方法GIRNet。该方法通过同时训练注册、分割和修复模块，实现了对病变区域的分割和对修复后图像的注册，提高了两者的协同性能。实验结果表明，该方法在多个数据集上可有效实现病理图像的注册和病变识别，为高效、经济的病理图像注册提供了有前途的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>病理图像注册在医疗应用中具有重要意义，但现有研究大多局限于正常组织的注册。</li>
<li>文章提出了一个名为GIRNet的新型无监督方法用于病理图像注册。</li>
<li>GIRNet结合了分割和修复技术，通过生成、修复和注册的原则实现病理图像的精准注册。</li>
<li>该方法能够在无监督学习框架内实现对病变区域的分割以及对修复后图像的注册协同进步。</li>
<li>实验结果显示GIRNet在多个数据集上的表现良好，能有效实现病理图像的注册并识别病变。</li>
<li>所提出的代码在GitHub上开放供研究使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.11040">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d0b752b2690a964f4ff37d51c64f21fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19c4512bf5070012829b402135127eb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a2becdd260ae923525ea4e9ed4ba656.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-11e7e797ea8fd5c561d87275120b3d33.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-03-20  Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems The   Role of Datasets, Architectures, and Loss Functions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e2e4ba0692f50c329b5560452c3ae0f8.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-03-20  Towards Scalable Foundation Model for Multi-modal and Hyperspectral   Geospatial Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17259.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
