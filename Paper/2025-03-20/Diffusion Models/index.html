<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  The Power of Context How Multimodality Improves Image Super-Resolution">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-c0a55271a8689b65d4a8c4ec0da92bdd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-20-æ›´æ–°"><a href="#2025-03-20-æ›´æ–°" class="headerlink" title="2025-03-20 æ›´æ–°"></a>2025-03-20 æ›´æ–°</h1><h2 id="The-Power-of-Context-How-Multimodality-Improves-Image-Super-Resolution"><a href="#The-Power-of-Context-How-Multimodality-Improves-Image-Super-Resolution" class="headerlink" title="The Power of Context: How Multimodality Improves Image Super-Resolution"></a>The Power of Context: How Multimodality Improves Image Super-Resolution</h2><p><strong>Authors:Kangfu Mei, Hossein Talebi, Mojtaba Ardakani, Vishal M. Patel, Peyman Milanfar, Mauricio Delbracio</strong></p>
<p>Single-image super-resolution (SISR) remains challenging due to the inherent difficulty of recovering fine-grained details and preserving perceptual quality from low-resolution inputs. Existing methods often rely on limited image priors, leading to suboptimal results. We propose a novel approach that leverages the rich contextual information available in multiple modalities â€“ including depth, segmentation, edges, and text prompts â€“ to learn a powerful generative prior for SISR within a diffusion model framework. We introduce a flexible network architecture that effectively fuses multimodal information, accommodating an arbitrary number of input modalities without requiring significant modifications to the diffusion process. Crucially, we mitigate hallucinations, often introduced by text prompts, by using spatial information from other modalities to guide regional text-based conditioning. Each modalityâ€™s guidance strength can also be controlled independently, allowing steering outputs toward different directions, such as increasing bokeh through depth or adjusting object prominence via segmentation. Extensive experiments demonstrate that our model surpasses state-of-the-art generative SISR methods, achieving superior visual quality and fidelity. See project page at <a target="_blank" rel="noopener" href="https://mmsr.kfmei.com/">https://mmsr.kfmei.com/</a>. </p>
<blockquote>
<p>å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºä»ä½åˆ†è¾¨ç‡è¾“å…¥ä¸­æ¢å¤ç²¾ç»†ç»†èŠ‚å¹¶ä¿æŒæ„ŸçŸ¥è´¨é‡å…·æœ‰å›ºæœ‰çš„å›°éš¾ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºæœ‰é™çš„å›¾åƒå…ˆéªŒï¼Œå¯¼è‡´ç»“æœä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å¤šç§æ¨¡å¼ï¼ˆåŒ…æ‹¬æ·±åº¦ã€åˆ†å‰²ã€è¾¹ç¼˜å’Œæ–‡æœ¬æç¤ºï¼‰ä¸­å¯ç”¨çš„ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œåœ¨æ‰©æ•£æ¨¡å‹æ¡†æ¶å†…å­¦ä¹ å¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒï¼Œç”¨äºSISRã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çµæ´»çš„ç½‘ç»œæ¶æ„ï¼Œæœ‰æ•ˆåœ°èåˆäº†å¤šæ¨¡å¼ä¿¡æ¯ï¼Œé€‚åº”ä»»æ„æ•°é‡çš„è¾“å…¥æ¨¡å¼ï¼Œæ— éœ€å¯¹æ‰©æ•£è¿‡ç¨‹è¿›è¡Œé‡å¤§ä¿®æ”¹ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨å…¶ä»–æ¨¡æ€çš„ç©ºé—´ä¿¡æ¯æ¥æŒ‡å¯¼åŸºäºåŒºåŸŸçš„æ–‡æœ¬æ¡ä»¶åŒ–ï¼Œå‡è½»äº†ç”±æ–‡æœ¬æç¤ºå¼•èµ·çš„å¹»è§‰ã€‚æ¯ç§æ¨¡æ€çš„å¼•å¯¼å¼ºåº¦ä¹Ÿå¯ä»¥ç‹¬ç«‹æ§åˆ¶ï¼Œä»è€Œå¼•å¯¼è¾“å‡ºæœå‘ä¸åŒæ–¹å‘ï¼Œä¾‹å¦‚é€šè¿‡æ·±åº¦å¢åŠ æ•£æ™¯æˆ–é€šè¿‡åˆ†æ®µè°ƒæ•´å¯¹è±¡çªå‡ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¶…è¶Šäº†æœ€å…ˆè¿›çš„ç”ŸæˆSISRæ–¹æ³•ï¼Œå®ç°äº†å“è¶Šçš„å¯è§†è´¨é‡å’Œä¿çœŸåº¦ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://mmsr.kfmei.com/%E3%80%82">https://mmsr.kfmei.com/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14503v1">PDF</a> accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ¡†æ¶ä¸­çš„ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ·±åº¦ã€åˆ†å‰²ã€è¾¹ç¼˜å’Œæ–‡æœ¬æç¤ºï¼Œå­¦ä¹ å¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒã€‚é€šè¿‡çµæ´»çš„ç½‘ç»œæ¶æ„æœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œé€‚åº”ä»»æ„æ•°é‡çš„è¾“å…¥æ¨¡æ€ï¼Œæ— éœ€å¯¹æ‰©æ•£è¿‡ç¨‹è¿›è¡Œé‡å¤§ä¿®æ”¹ã€‚åˆ©ç”¨å…¶ä»–æ¨¡æ€çš„ç©ºé—´ä¿¡æ¯æ¥æŒ‡å¯¼åŸºäºæ–‡æœ¬çš„åŒºåŸŸæ¡ä»¶ï¼Œç¼“è§£ç”±æ–‡æœ¬æç¤ºå¼•èµ·çš„å¹»è§‰ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹è¶…è¶Šäº†æœ€å…ˆè¿›çš„ç”ŸæˆSISRæ–¹æ³•ï¼Œå®ç°äº†ä¼˜è¶Šçš„è§†è§‰è´¨é‡å’Œä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆæ·±åº¦ã€åˆ†å‰²ã€è¾¹ç¼˜å’Œæ–‡æœ¬æç¤ºï¼‰åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œæé«˜å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰çš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>æå‡ºçµæ´»ç½‘ç»œæ¶æ„ï¼Œæœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œé€‚åº”å¤šç§è¾“å…¥æ¨¡æ€ã€‚</li>
<li>åˆ©ç”¨å…¶ä»–æ¨¡æ€çš„ç©ºé—´ä¿¡æ¯æ¥æŒ‡å¯¼åŸºäºæ–‡æœ¬çš„åŒºåŸŸæ¡ä»¶ï¼Œå‡å°‘æ–‡æœ¬æç¤ºå¼•èµ·çš„å¹»è§‰ã€‚</li>
<li>æ¯ç§æ¨¡æ€çš„å¼•å¯¼å¼ºåº¦å¯ä»¥ç‹¬ç«‹æ§åˆ¶ï¼Œä½¿è¾“å‡ºå¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´ï¼Œå¦‚é€šè¿‡æ·±åº¦å¢åŠ æ•£æ™¯æˆ–é€šè¿‡åˆ†æ®µè°ƒæ•´å¯¹è±¡çªå‡ºç¨‹åº¦ã€‚</li>
<li>æ–¹æ³•çš„ä¼˜è¶Šæ€§é€šè¿‡å¹¿æ³›çš„å®éªŒå¾—åˆ°éªŒè¯ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„ç”ŸæˆSISRæ–¹æ³•ã€‚</li>
<li>æ¨¡å‹å®ç°çš„é«˜è§†è§‰è´¨é‡å’Œä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a51b43f536819da75f00d84647d57e46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17320c6dc9fc8542455d7edb48612f79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97e3b936385b4d0a6f09320cb478a585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c782d62545c4e7389a05ea1a776609ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4374694ea8ce9880138ac2c7cf82d3f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef3d964982a61a5764e8d04c5af1d46d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DiffMoE-Dynamic-Token-Selection-for-Scalable-Diffusion-Transformers"><a href="#DiffMoE-Dynamic-Token-Selection-for-Scalable-Diffusion-Transformers" class="headerlink" title="DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers"></a>DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers</h2><p><strong>Authors:Minglei Shi, Ziyang Yuan, Haotian Yang, Xintao Wang, Mingwu Zheng, Xin Tao, Wenliang Zhao, Wenzhao Zheng, Jie Zhou, Jiwen Lu, Pengfei Wan, Di Zhang, Kun Gai</strong></p>
<p>Diffusion models have demonstrated remarkable success in various image generation tasks, but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. To address this limitation, we propose a novel approach that leverages the inherent heterogeneity of the diffusion process. Our method, DiffMoE, introduces a batch-level global token pool that enables experts to access global token distributions during training, promoting specialized expert behavior. To unleash the full potential of the diffusion process, DiffMoE incorporates a capacity predictor that dynamically allocates computational resources based on noise levels and sample complexity. Through comprehensive evaluation, DiffMoE achieves state-of-the-art performance among diffusion models on ImageNet benchmark, substantially outperforming both dense architectures with 3x activated parameters and existing MoE approaches while maintaining 1x activated parameters. The effectiveness of our approach extends beyond class-conditional generation to more challenging tasks such as text-to-image generation, demonstrating its broad applicability across different diffusion model applications. Project Page: <a target="_blank" rel="noopener" href="https://shiml20.github.io/DiffMoE/">https://shiml20.github.io/DiffMoE/</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å„ç§å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶æ€§èƒ½å¾€å¾€å—åˆ°ä¸åŒæ¡ä»¶å’Œå™ªå£°æ°´å¹³ä¸‹è¾“å…¥ç»Ÿä¸€å¤„ç†æ–¹å¼çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£è¿‡ç¨‹å›ºæœ‰å¼‚è´¨æ€§çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•DiffMoEå¼•å…¥äº†ä¸€ä¸ªæ‰¹æ¬¡çº§å…¨å±€ä»¤ç‰Œæ± ï¼Œä½¿ä¸“å®¶èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¿é—®å…¨å±€ä»¤ç‰Œåˆ†å¸ƒï¼Œä»è€Œä¿ƒè¿›ä¸“ä¸šä¸“å®¶è¡Œä¸ºã€‚ä¸ºäº†é‡Šæ”¾æ‰©æ•£è¿‡ç¨‹çš„å…¨éƒ¨æ½œåŠ›ï¼ŒDiffMoEèå…¥äº†ä¸€ä¸ªå®¹é‡é¢„æµ‹å™¨ï¼Œè¯¥é¢„æµ‹å™¨èƒ½å¤Ÿæ ¹æ®å™ªå£°æ°´å¹³å’Œæ ·æœ¬å¤æ‚æ€§åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºã€‚é€šè¿‡å…¨é¢è¯„ä¼°ï¼ŒDiffMoEåœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ‰©æ•£æ¨¡å‹ä¸­çš„æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œåœ¨ä¿æŒ1å€æ¿€æ´»å‚æ•°çš„åŒæ—¶ï¼Œå¤§å¹…è¶…è¶Šäº†å¯†é›†æ¶æ„çš„æ¨¡å‹ï¼ˆå…·æœ‰ä¸‰å€æ¿€æ´»å‚æ•°ï¼‰å’Œç°æœ‰çš„MoEæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ä¸ä»…é™äºç±»åˆ«æ¡ä»¶ç”Ÿæˆï¼Œè€Œä¸”é€‚ç”¨äºæ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒæ‰©æ•£æ¨¡å‹åº”ç”¨ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://shiml20.github.io/DiffMoE/%E3%80%82">https://shiml20.github.io/DiffMoE/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14487v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://shiml20.github.io/DiffMoE/">https://shiml20.github.io/DiffMoE/</a></p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å„ç§å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶æ€§èƒ½å¾€å¾€å—é™äºä¸åŒæ¡ä»¶å’Œå™ªå£°æ°´å¹³ä¸‹è¾“å…¥çš„å‡åŒ€å¤„ç†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£è¿‡ç¨‹å›ºæœ‰å¼‚è´¨æ€§çš„æ–°æ–¹æ³•DiffMoEã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªæ‰¹å¤„ç†çº§åˆ«çš„å…¨å±€ä»¤ç‰Œæ± ï¼Œä½¿ä¸“å®¶èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¿é—®å…¨å±€ä»¤ç‰Œåˆ†å¸ƒï¼Œä»è€Œä¿ƒè¿›ä¸“ä¸šä¸“å®¶è¡Œä¸ºã€‚ä¸ºäº†é‡Šæ”¾æ‰©æ•£è¿‡ç¨‹çš„æ½œåŠ›ï¼ŒDiffMoEç»“åˆäº†èƒ½åŠ›é¢„æµ‹å™¨ï¼Œæ ¹æ®å™ªå£°æ°´å¹³å’Œæ ·æœ¬å¤æ‚æ€§åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºã€‚ç»è¿‡å…¨é¢è¯„ä¼°ï¼ŒDiffMoEåœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ‰©æ•£æ¨¡å‹çš„æœ€ä½³æ€§èƒ½ï¼Œåœ¨ä¿æŒ1å€æ¿€æ´»å‚æ•°çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¼˜äºå¯†é›†æ¶æ„å’Œç°æœ‰MoEæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…é€‚ç”¨äºç±»åˆ«æ¡ä»¶ç”Ÿæˆï¼Œè¿˜é€‚ç”¨äºæ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒæ‰©æ•£æ¨¡å‹åº”ç”¨ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†å—é™äºä¸åŒæ¡ä»¶å’Œå™ªå£°æ°´å¹³ä¸‹çš„è¾“å…¥å¤„ç†ã€‚</li>
<li>DiffMoEæ–¹æ³•åˆ©ç”¨æ‰©æ•£è¿‡ç¨‹çš„å¼‚è´¨æ€§ï¼Œå¼•å…¥æ‰¹å¤„ç†çº§åˆ«çš„å…¨å±€ä»¤ç‰Œæ± ï¼Œä¿ƒè¿›ä¸“å®¶è¡Œä¸ºã€‚</li>
<li>DiffMoEç»“åˆèƒ½åŠ›é¢„æµ‹å™¨ï¼Œæ ¹æ®å™ªå£°æ°´å¹³å’Œæ ·æœ¬å¤æ‚æ€§åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºã€‚</li>
<li>DiffMoEåœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ‰©æ•£æ¨¡å‹çš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>DiffMoEåœ¨ä¿æŒ1å€æ¿€æ´»å‚æ•°çš„åŒæ—¶ï¼Œä¼˜äºå¯†é›†æ¶æ„å’Œå…¶ä»–MoEæ–¹æ³•ã€‚</li>
<li>DiffMoEä¸ä»…é€‚ç”¨äºç±»åˆ«æ¡ä»¶ç”Ÿæˆï¼Œè¿˜é€‚ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç­‰æ›´å¤æ‚çš„ä»»åŠ¡ã€‚</li>
<li>DiffMoEæ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œå¯åº”ç”¨äºä¸åŒçš„æ‰©æ•£æ¨¡å‹åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3971bb77e440822a6e77cfbdcdffae07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5be073f326511ea1e85538238d7b3985.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7eec6e3f175853e962248e6b3c1a49ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33990026d011208f3a519261a1055302.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8884f961b2943cb27b6a6c8869b91986.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SIR-DIFF-Sparse-Image-Sets-Restoration-with-Multi-View-Diffusion-Model"><a href="#SIR-DIFF-Sparse-Image-Sets-Restoration-with-Multi-View-Diffusion-Model" class="headerlink" title="SIR-DIFF: Sparse Image Sets Restoration with Multi-View Diffusion Model"></a>SIR-DIFF: Sparse Image Sets Restoration with Multi-View Diffusion Model</h2><p><strong>Authors:Yucheng Mao, Boyang Wang, Nilesh Kulkarni, Jeong Joon Park</strong></p>
<p>The computer vision community has developed numerous techniques for digitally restoring true scene information from single-view degraded photographs, an important yet extremely ill-posed task. In this work, we tackle image restoration from a different perspective by jointly denoising multiple photographs of the same scene. Our core hypothesis is that degraded images capturing a shared scene contain complementary information that, when combined, better constrains the restoration problem. To this end, we implement a powerful multi-view diffusion model that jointly generates uncorrupted views by extracting rich information from multi-view relationships. Our experiments show that our multi-view approach outperforms existing single-view image and even video-based methods on image deblurring and super-resolution tasks. Critically, our model is trained to output 3D consistent images, making it a promising tool for applications requiring robust multi-view integration, such as 3D reconstruction or pose estimation. </p>
<blockquote>
<p>è®¡ç®—æœºè§†è§‰é¢†åŸŸå·²ç»å¼€å‘äº†è®¸å¤šä»å•è§†å›¾é€€åŒ–ç…§ç‰‡ä¸­æ•°å­—æ¢å¤çœŸå®åœºæ™¯ä¿¡æ¯çš„æŠ€å·§ï¼Œè¿™æ˜¯ä¸€é¡¹é‡è¦ä½†æåº¦ä¸é€‚çš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è”åˆå»å™ªåŒä¸€åœºæ™¯çš„å¤šå¼ ç…§ç‰‡ï¼Œä»ä¸åŒçš„è§’åº¦è§£å†³äº†å›¾åƒæ¢å¤é—®é¢˜ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒå‡è®¾æ˜¯ï¼Œæ•æ‰åŒä¸€åœºæ™¯çš„é€€åŒ–å›¾åƒåŒ…å«äº’è¡¥ä¿¡æ¯ï¼Œå½“è¿™äº›ä¿¡æ¯ç»“åˆæ—¶ï¼Œå¯ä»¥æ›´å¥½åœ°çº¦æŸæ¢å¤é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªå¼ºå¤§çš„å¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ä»å¤šè§†å›¾å…³ç³»ä¸­æå–ä¸°å¯Œä¿¡æ¯ï¼Œè”åˆç”ŸæˆæœªæŸåçš„è§†å›¾ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šè§†å›¾æ–¹æ³•åœ¨å›¾åƒå»æ¨¡ç³Šå’Œè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„å•è§†å›¾å›¾åƒç”šè‡³åŸºäºè§†é¢‘çš„æ–¹æ³•ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¢«è®­ç»ƒè¾“å‡ºä¸‰ç»´ä¸€è‡´çš„å›¾åƒï¼Œä½¿å…¶æˆä¸ºé€‚ç”¨äºéœ€è¦ç¨³å¥å¤šè§†å›¾é›†æˆçš„åº”ç”¨çš„æœ‰å‰é€”çš„å·¥å…·ï¼Œä¾‹å¦‚ä¸‰ç»´é‡å»ºæˆ–å§¿æ€ä¼°è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14463v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šè§†è§’æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œç”¨äºä»å¤šä¸ªåŒä¸€åœºæ™¯çš„é€€åŒ–å›¾åƒä¸­æå–ä¸°å¯Œä¿¡æ¯å¹¶è”åˆå»å™ªï¼Œä»è€Œæ¢å¤çœŸå®åœºæ™¯ä¿¡æ¯ã€‚è¯¥æ–¹æ³•å‡è®¾åŒä¸€åœºæ™¯çš„å¤šä¸ªé€€åŒ–å›¾åƒåŒ…å«äº’è¡¥ä¿¡æ¯ï¼Œå½“è¿™äº›ä¿¡æ¯ç»“åˆæ—¶èƒ½æ›´å¥½åœ°çº¦æŸæ¢å¤é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå»æ¨¡ç³Šå’Œè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„å•è§†å›¾å›¾åƒå’Œè§†é¢‘æ–¹æ³•ï¼Œä¸”æ¨¡å‹è®­ç»ƒå¯è¾“å‡ºä¸‰ç»´ä¸€è‡´çš„å›¾åƒï¼Œä¸ºä¸‰ç»´é‡å»ºæˆ–å§¿æ€ä¼°è®¡ç­‰åº”ç”¨æä¾›äº†æœ‰åŠ›å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šè§†è§’æ‰©æ•£æ¨¡å‹çš„å›¾åƒæ¢å¤æ–¹æ³•ï¼Œè”åˆå¤„ç†å¤šä¸ªåŒä¸€åœºæ™¯çš„å›¾åƒä»¥æé«˜æ¢å¤æ•ˆæœã€‚</li>
<li>æ–¹æ³•çš„æ ¸å¿ƒå‡è®¾æ˜¯åŒä¸€åœºæ™¯çš„å¤šä¸ªé€€åŒ–å›¾åƒåŒ…å«äº’è¡¥ä¿¡æ¯ï¼Œæœ‰åŠ©äºæ›´å¥½åœ°è§£å†³å›¾åƒæ¢å¤è¿™ä¸€ç—…æ€é—®é¢˜ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå»æ¨¡ç³Šå’Œè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šç°æœ‰çš„å•è§†å›¾åŠè§†é¢‘æ–¹æ³•ã€‚</li>
<li>æ¨¡å‹å¯ä»¥è®­ç»ƒè¾“å‡ºä¸‰ç»´ä¸€è‡´çš„å›¾åƒï¼Œè¿™å¯¹äºéœ€è¦å¤šè§†è§’æ•´åˆçš„åº”ç”¨ï¼ˆå¦‚ä¸‰ç»´é‡å»ºã€å§¿æ€ä¼°è®¡ç­‰ï¼‰å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>è¯¥æ–¹æ³•å¼ºè°ƒäº†å¤šè§†è§’ä¿¡æ¯åœ¨å›¾åƒæ¢å¤ä¸­çš„ä»·å€¼ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</li>
<li>æå‡ºçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæå–å’Œæ•´åˆå¤šè§†è§’å…³ç³»ä¸­çš„ä¸°å¯Œä¿¡æ¯ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„æ¢å¤å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14463">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8bae5f40a4e91e144be01443fa4398bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfeb52b77fa1950d15e6cccefcf12e60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0eacadeffc44593e17c100a8d7c49e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7178b493eb48fa2f067119805e9c925.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-784d71e627494ac4c4f5b520d3c3b096.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Bolt3D-Generating-3D-Scenes-in-Seconds"><a href="#Bolt3D-Generating-3D-Scenes-in-Seconds" class="headerlink" title="Bolt3D: Generating 3D Scenes in Seconds"></a>Bolt3D: Generating 3D Scenes in Seconds</h2><p><strong>Authors:Stanislaw Szymanowicz, Jason Y. Zhang, Pratul Srinivasan, Ruiqi Gao, Arthur Brussee, Aleksander Holynski, Ricardo Martin-Brualla, Jonathan T. Barron, Philipp Henzler</strong></p>
<p>We present a latent diffusion model for fast feed-forward 3D scene generation. Given one or more images, our model Bolt3D directly samples a 3D scene representation in less than seven seconds on a single GPU. We achieve this by leveraging powerful and scalable existing 2D diffusion network architectures to produce consistent high-fidelity 3D scene representations. To train this model, we create a large-scale multiview-consistent dataset of 3D geometry and appearance by applying state-of-the-art dense 3D reconstruction techniques to existing multiview image datasets. Compared to prior multiview generative models that require per-scene optimization for 3D reconstruction, Bolt3D reduces the inference cost by a factor of up to 300 times. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå¿«é€Ÿå‰é¦ˆ3Dåœºæ™¯ç”Ÿæˆçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚ç»™å®šä¸€å¼ æˆ–å¤šå¼ å›¾åƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹Bolt3Dèƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šä¸åˆ°ä¸ƒç§’å†…ç›´æ¥é‡‡æ ·3Dåœºæ™¯è¡¨ç¤ºã€‚æˆ‘ä»¬åˆ©ç”¨å¼ºå¤§ä¸”å¯æ‰©å±•çš„ç°æœ‰2Dæ‰©æ•£ç½‘ç»œæ¶æ„æ¥ç”Ÿæˆä¸€è‡´çš„é«˜ä¿çœŸ3Dåœºæ™¯è¡¨ç¤ºï¼Œä»è€Œå®ç°äº†è¿™ä¸€ç›®æ ‡ã€‚ä¸ºäº†è®­ç»ƒæ­¤æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡å°†æœ€æ–°å…ˆè¿›çš„å¯†é›†3Dé‡å»ºæŠ€æœ¯åº”ç”¨äºç°æœ‰çš„å¤šè§†è§’å›¾åƒæ•°æ®é›†ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè§†è§’ä¸€è‡´æ€§çš„3Då‡ ä½•å’Œå¤–è§‚æ•°æ®é›†ã€‚ä¸ä»¥å‰éœ€è¦é’ˆå¯¹3Dé‡å»ºè¿›è¡Œåœºæ™¯ä¼˜åŒ–çš„å¤šè§†è§’ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒBolt3Då°†æ¨ç†æˆæœ¬é™ä½äº†é«˜è¾¾30ã€ã€ã€‘å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14445v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://szymanowiczs.github.io/bolt3d">https://szymanowiczs.github.io/bolt3d</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºç»™å®šçš„ä¸€å¹…æˆ–å¤šå¹…å›¾åƒï¼ŒBolt3Dæ¨¡å‹èƒ½åœ¨å•ä¸ªGPUä¸Šå®ç°å¿«é€Ÿçš„é¦ˆå‰ä¸‰ç»´åœºæ™¯ç”Ÿæˆã€‚å®ƒå€Ÿé‰´ç°æœ‰çš„å¼ºå¤§çš„äºŒç»´æ‰©æ•£ç½‘ç»œæ¶æ„ï¼Œç”Ÿæˆä¸€è‡´çš„é«˜ä¿çœŸä¸‰ç»´åœºæ™¯è¡¨ç¤ºã€‚é€šè¿‡åº”ç”¨æœ€æ–°çš„å¯†é›†ä¸‰ç»´é‡å»ºæŠ€æœ¯åˆ°ç°æœ‰çš„å¤šè§†è§’å›¾åƒæ•°æ®é›†ï¼Œåˆ›å»ºäº†å¤§è§„æ¨¡çš„å¤šè§†è§’ä¸€è‡´çš„ä¸‰ç»´å‡ ä½•å’Œå¤–è§‚æ•°æ®é›†ä»¥è®­ç»ƒæ¨¡å‹ã€‚ç›¸æ¯”äºéœ€è¦é’ˆå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œä¼˜åŒ–çš„å…ˆå‰çš„å¤šè§†è§’ç”Ÿæˆæ¨¡å‹ï¼ŒBolt3Dæ¨¡å‹å°†æ¨ç†æˆæœ¬é™ä½äº†é«˜è¾¾300å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Bolt3Dæ¨¡å‹å®ç°äº†åŸºäºå›¾åƒçš„ä¸‰ç»´åœºæ™¯å¿«é€Ÿç”Ÿæˆã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šå®Œæˆåœ¨ä¸ƒç§’å†…ç›´æ¥é‡‡æ ·ä¸‰ç»´åœºæ™¯è¡¨ç¤ºã€‚</li>
<li>Bolt3Dåˆ©ç”¨å¼ºå¤§çš„äºŒç»´æ‰©æ•£ç½‘ç»œæ¶æ„å®ç°é«˜ä¿çœŸä¸‰ç»´åœºæ™¯ç”Ÿæˆã€‚</li>
<li>åˆ›å»ºå¤§è§„æ¨¡çš„å¤šè§†è§’ä¸€è‡´çš„ä¸‰ç»´å‡ ä½•å’Œå¤–è§‚æ•°æ®é›†ç”¨äºè®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æœ€æ–°çš„å¯†é›†ä¸‰ç»´é‡å»ºæŠ€æœ¯ã€‚</li>
<li>Bolt3Dæ¨¡å‹å°†æ¨ç†æˆæœ¬é™ä½äº†é«˜è¾¾300å€ç›¸æ¯”äºå…¶ä»–å¤šè§†è§’ç”Ÿæˆæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-840f768f1790dcfc89db0c27d0fb6353.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5114d12fe5c8eddaffd511c629abe648.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7154ae10528a5ec0f440c7516e8c8a0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcc133dd7d51c940a7a837dff85beb8b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RFMI-Estimating-Mutual-Information-on-Rectified-Flow-for-Text-to-Image-Alignment"><a href="#RFMI-Estimating-Mutual-Information-on-Rectified-Flow-for-Text-to-Image-Alignment" class="headerlink" title="RFMI: Estimating Mutual Information on Rectified Flow for Text-to-Image   Alignment"></a>RFMI: Estimating Mutual Information on Rectified Flow for Text-to-Image   Alignment</h2><p><strong>Authors:Chao Wang, Giulio Franzese, Alessandro Finamore, Pietro Michiardi</strong></p>
<p>Rectified Flow (RF) models trained with a Flow matching framework have achieved state-of-the-art performance on Text-to-Image (T2I) conditional generation. Yet, multiple benchmarks show that synthetic images can still suffer from poor alignment with the prompt, i.e., images show wrong attribute binding, subject positioning, numeracy, etc. While the literature offers many methods to improve T2I alignment, they all consider only Diffusion Models, and require auxiliary datasets, scoring models, and linguistic analysis of the prompt. In this paper we aim to address these gaps. First, we introduce RFMI, a novel Mutual Information (MI) estimator for RF models that uses the pre-trained model itself for the MI estimation. Then, we investigate a self-supervised fine-tuning approach for T2I alignment based on RFMI that does not require auxiliary information other than the pre-trained model itself. Specifically, a fine-tuning set is constructed by selecting synthetic images generated from the pre-trained RF model and having high point-wise MI between images and prompts. Our experiments on MI estimation benchmarks demonstrate the validity of RFMI, and empirical fine-tuning on SD3.5-Medium confirms the effectiveness of RFMI for improving T2I alignment while maintaining image quality. </p>
<blockquote>
<p>ä½¿ç”¨æµåŒ¹é…æ¡†æ¶è®­ç»ƒçš„Rectified Flowï¼ˆRFï¼‰æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¡ä»¶ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ç„¶è€Œï¼Œå¤šä¸ªåŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œåˆæˆå›¾åƒä»ç„¶å¯èƒ½å› æç¤ºå¯¹é½ä¸è‰¯è€Œå‡ºç°å±æ€§ç»‘å®šé”™è¯¯ã€ä¸»é¢˜å®šä½é”™è¯¯ã€æ•°é‡è®¡ç®—ç­‰é—®é¢˜ã€‚è™½ç„¶æ–‡çŒ®æä¾›äº†è®¸å¤šæ”¹è¿›T2Iå¯¹é½çš„æ–¹æ³•ï¼Œä½†å®ƒä»¬éƒ½åªè€ƒè™‘æ‰©æ•£æ¨¡å‹ï¼Œå¹¶éœ€è¦è¾…åŠ©æ•°æ®é›†ã€è¯„åˆ†æ¨¡å‹å’Œæç¤ºçš„è¯­è¨€åˆ†æã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³è¿™äº›ç©ºç™½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†RFMIï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºRFæ¨¡å‹çš„æ–°å‹äº’ä¿¡æ¯ï¼ˆMIï¼‰ä¼°è®¡å™¨ï¼Œå®ƒä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æœ¬èº«è¿›è¡ŒMIä¼°è®¡ã€‚ç„¶åï¼Œæˆ‘ä»¬åŸºäºRFMIç ”ç©¶äº†ä¸€ç§æ— éœ€é™¤é¢„è®­ç»ƒæ¨¡å‹ä»¥å¤–çš„è¾…åŠ©ä¿¡æ¯çš„è‡ªæˆ‘ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œç”¨äºæ”¹å–„T2Iå¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡é€‰æ‹©ä»é¢„è®­ç»ƒçš„RFæ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒï¼Œå¹¶å…·æœ‰è¾ƒé«˜çš„å›¾åƒä¸æç¤ºä¹‹é—´çš„ç‚¹å¯¹ç‚¹äº’ä¿¡æ¯æ¥æ„å»ºå¾®è°ƒé›†ã€‚æˆ‘ä»¬åœ¨äº’ä¿¡æ¯ä¼°è®¡åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒéªŒè¯äº†RFMIçš„æœ‰æ•ˆæ€§ï¼Œåœ¨SD3.5-Mediumä¸Šçš„ç»éªŒå¾®è°ƒè¯å®äº†RFMIåœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶æé«˜T2Iå¯¹é½çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14358v1">PDF</a> to appear at ICLR 2025 Workshop on Deep Generative Model in Machine   Learning: Theory, Principle and Efficacy</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„å¯¹é½é—®é¢˜ã€‚æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºäº’ä¿¡æ¯çš„è¯„ä¼°å™¨RFMIï¼Œç”¨äºè¯„ä¼°Rectified Flowæ¨¡å‹çš„äº’ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨æ­¤è¯„ä¼°å™¨è¿›è¡Œè‡ªç›‘ç£å¾®è°ƒï¼Œä»¥æé«˜å›¾åƒä¸æ–‡æœ¬æç¤ºçš„å¯¹é½åº¦ã€‚è¯¥æ–¹æ³•ä»…ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹æœ¬èº«ï¼Œæ— éœ€é¢å¤–çš„æ•°æ®é›†ã€è¯„åˆ†æ¨¡å‹æˆ–è¯­è¨€åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rectified Flow (RF) models åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨ä¸æç¤ºå¯¹é½ä¸è‰¯çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šä¸“æ³¨äºä½¿ç”¨æ‰©æ•£æ¨¡å‹æ”¹å–„å¯¹é½ï¼Œå¹¶éœ€è¦é¢å¤–çš„æ•°æ®é›†ã€è¯„åˆ†æ¨¡å‹å’Œè¯­è¨€åˆ†æã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„äº’ä¿¡æ¯è¯„ä¼°å™¨RFMIï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹è‡ªèº«è¿›è¡Œäº’ä¿¡æ¯ä¼°è®¡ã€‚</li>
<li>ä½¿ç”¨RFMIè¿›è¡Œè‡ªç›‘ç£å¾®è°ƒï¼Œæ— éœ€é¢å¤–çš„è¾…åŠ©ä¿¡æ¯ï¼Œä»…ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹æœ¬èº«ã€‚</li>
<li>é€šè¿‡é€‰æ‹©é«˜å›¾åƒä¸æç¤ºç‚¹äº’ä¿¡æ¯çš„åˆæˆå›¾åƒæ„å»ºå¾®è°ƒé›†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜RFMIçš„æœ‰æ•ˆæ€§ï¼Œåœ¨MIä¼°è®¡åŸºå‡†æµ‹è¯•å’Œå®é™…å¾®è°ƒä¸­å‡éªŒè¯äº†å…¶æ”¹å–„T2Iå¯¹é½åŒæ—¶ä¿æŒå›¾åƒè´¨é‡çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f3ea0e995432d0490a38d2d317f21d63.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VEGGIE-Instructional-Editing-and-Reasoning-Video-Concepts-with-Grounded-Generation"><a href="#VEGGIE-Instructional-Editing-and-Reasoning-Video-Concepts-with-Grounded-Generation" class="headerlink" title="VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded   Generation"></a>VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded   Generation</h2><p><strong>Authors:Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan, Joyce Chai, Mohit Bansal</strong></p>
<p>Recent video diffusion models have enhanced video editing, but it remains challenging to handle instructional editing and diverse tasks (e.g., adding, removing, changing) within a unified framework. In this paper, we introduce VEGGIE, a Video Editor with Grounded Generation from Instructions, a simple end-to-end framework that unifies video concept editing, grounding, and reasoning based on diverse user instructions. Specifically, given a video and text query, VEGGIE first utilizes an MLLM to interpret user intentions in instructions and ground them to the video contexts, generating frame-specific grounded task queries for pixel-space responses. A diffusion model then renders these plans and generates edited videos that align with user intent. To support diverse tasks and complex instructions, we employ a curriculum learning strategy: first aligning the MLLM and video diffusion model with large-scale instructional image editing data, followed by end-to-end fine-tuning on high-quality multitask video data. Additionally, we introduce a novel data synthesis pipeline to generate paired instructional video editing data for model training. It transforms static image data into diverse, high-quality video editing samples by leveraging Image-to-Video models to inject dynamics. VEGGIE shows strong performance in instructional video editing with different editing skills, outperforming the best instructional baseline as a versatile model, while other models struggle with multi-tasking. VEGGIE also excels in video object grounding and reasoning segmentation, where other baselines fail. We further reveal how the multiple tasks help each other and highlight promising applications like zero-shot multimodal instructional and in-context video editing. </p>
<blockquote>
<p>è¿‘æœŸï¼Œè§†é¢‘æ‰©æ•£æ¨¡å‹å·²ç»æå‡äº†è§†é¢‘ç¼–è¾‘çš„èƒ½åŠ›ï¼Œä½†åœ¨ç»Ÿä¸€æ¡†æ¶å†…å¤„ç†æŒ‡ä»¤ç¼–è¾‘å’Œå¤šæ ·åŒ–ä»»åŠ¡ï¼ˆä¾‹å¦‚æ·»åŠ ã€åˆ é™¤ã€æ›´æ”¹ï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VEGGIEï¼Œä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„æ¥åœ°ç”Ÿæˆè§†é¢‘ç¼–è¾‘å™¨ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œèåˆäº†è§†é¢‘æ¦‚å¿µç¼–è¾‘ã€æ¥åœ°å’ŒåŸºäºå„ç§ç”¨æˆ·æŒ‡ä»¤çš„æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªè§†é¢‘å’Œæ–‡æœ¬æŸ¥è¯¢ï¼ŒVEGGIEé¦–å…ˆåˆ©ç”¨MLLMæ¥è§£é‡Šç”¨æˆ·æ„å›¾çš„æŒ‡ä»¤ï¼Œå¹¶å°†å®ƒä»¬æ¥åœ°åˆ°è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œä¸ºåƒç´ ç©ºé—´å“åº”ç”Ÿæˆç‰¹å®šå¸§çš„æ¥åœ°ä»»åŠ¡æŸ¥è¯¢ã€‚ç„¶åï¼Œæ‰©æ•£æ¨¡å‹ä¼šå‘ˆç°è¿™äº›è®¡åˆ’å¹¶ç”Ÿæˆç¬¦åˆç”¨æˆ·æ„å›¾çš„ç¼–è¾‘è§†é¢‘ã€‚ä¸ºäº†æ”¯æŒå¤šæ ·åŒ–çš„ä»»åŠ¡å’Œå¤æ‚çš„æŒ‡ä»¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼šé¦–å…ˆä½¿ç”¨å¤§è§„æ¨¡çš„æŒ‡ä»¤å›¾åƒç¼–è¾‘æ•°æ®æ¥å¯¹é½MLLMå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç„¶ååœ¨é«˜è´¨é‡çš„å¤šä»»åŠ¡è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„å¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°æ®åˆæˆç®¡é“ï¼Œä»¥ç”Ÿæˆç”¨äºæ¨¡å‹è®­ç»ƒçš„ä¸€å¯¹ä¸€æŒ‡ä»¤è§†é¢‘ç¼–è¾‘æ•°æ®ã€‚å®ƒé€šè¿‡åˆ©ç”¨å›¾åƒåˆ°è§†é¢‘çš„æ¨¡å‹æ³¨å…¥åŠ¨æ€æ€§ï¼Œå°†é™æ€å›¾åƒæ•°æ®è½¬åŒ–ä¸ºå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è§†é¢‘ç¼–è¾‘æ ·æœ¬ã€‚VEGGIEåœ¨å…·æœ‰ä¸åŒç¼–è¾‘æŠ€èƒ½çš„æŒ‡ä»¤è§†é¢‘ç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½œä¸ºä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œå®ƒè¶…è¶Šäº†æœ€ä½³æŒ‡ä»¤åŸºçº¿ï¼Œè€Œå…¶ä»–æ¨¡å‹åœ¨å¤šä»»åŠ¡å¤„ç†æ–¹é¢åˆ™è¡¨ç°æŒ£æ‰ã€‚VEGGIEåœ¨è§†é¢‘å¯¹è±¡æ¥åœ°å’Œæ¨ç†åˆ†å‰²æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè€Œå…¶ä»–åŸºçº¿åˆ™æœªèƒ½åšåˆ°ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ­ç¤ºäº†å¤šä¸ªä»»åŠ¡æ˜¯å¦‚ä½•ç›¸äº’å¸®åŠ©çš„ï¼Œå¹¶å¼ºè°ƒäº†æœ‰å‰æ™¯çš„åº”ç”¨ï¼Œå¦‚é›¶å°„å‡»å¤šæ¨¡å¼æŒ‡ä»¤å’Œåœ¨ä¸Šä¸‹æ–‡ä¸­çš„è§†é¢‘ç¼–è¾‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14350v1">PDF</a> First three authors contributed equally. Project page:   <a target="_blank" rel="noopener" href="https://veggie-gen.github.io/">https://veggie-gen.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VEGGIEï¼Œä¸€æ¬¾åŸºäºæŒ‡ä»¤çš„é€šç”¨è§†é¢‘ç¼–è¾‘æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½è§£æç”¨æˆ·æŒ‡ä»¤å¹¶å…³è”åˆ°è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆç‰¹å®šå¸§çš„ä»»åŠ¡æŸ¥è¯¢ï¼Œå†é€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆç¬¦åˆç”¨æˆ·æ„å›¾çš„è§†é¢‘ã€‚ä¸ºæ”¯æŒå¤šæ ·ä»»åŠ¡å’Œå¤æ‚æŒ‡ä»¤ï¼Œé‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œå…ˆåœ¨å¤§è§„æ¨¡æŒ‡ä»¤å›¾åƒç¼–è¾‘æ•°æ®ä¸Šå¯¹é½MLLMå’Œæ‰©æ•£æ¨¡å‹ï¼Œå†åœ¨é«˜è´¨å¤šä»»åŠ¡è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„å¾®è°ƒã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†æ–°å‹æ•°æ®åˆæˆæµç¨‹æ¥ç”Ÿæˆé…å¯¹æŒ‡ä»¤è§†é¢‘ç¼–è¾‘æ•°æ®ç”¨äºæ¨¡å‹è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒVEGGIEåœ¨æŒ‡ä»¤è§†é¢‘ç¼–è¾‘ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå…¶ä»–æ¨¡å‹éš¾ä»¥èƒœä»»å¤šä»»åŠ¡å¤„ç†æ—¶ï¼ŒVEGGIEå±•ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§ã€‚åŒæ—¶ï¼Œå®ƒåœ¨è§†é¢‘å¯¹è±¡å…³è”å’Œæ¨ç†åˆ†å‰²æ–¹é¢ä¹Ÿè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VEGGIEæ˜¯ä¸€ä¸ªç»Ÿä¸€è§†é¢‘ç¼–è¾‘ã€å…³è”å’ŒåŸºäºæŒ‡ä»¤çš„æ¨ç†çš„æ¡†æ¶ã€‚</li>
<li>é€šè¿‡MLLMè§£é‡Šç”¨æˆ·æŒ‡ä»¤å¹¶å°†å…¶å…³è”åˆ°è§†é¢‘ä¸Šä¸‹æ–‡ã€‚</li>
<li>ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆç¬¦åˆç”¨æˆ·æ„å›¾çš„è§†é¢‘ç¼–è¾‘ã€‚</li>
<li>é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥æ”¯æŒå¤šæ ·ä»»åŠ¡å’Œå¤æ‚æŒ‡ä»¤çš„è®­ç»ƒã€‚</li>
<li>å¼•å…¥æ•°æ®åˆæˆæµç¨‹ç”Ÿæˆé…å¯¹æŒ‡ä»¤è§†é¢‘ç¼–è¾‘æ•°æ®ã€‚</li>
<li>VEGGIEåœ¨æŒ‡ä»¤è§†é¢‘ç¼–è¾‘æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œå°¤å…¶æ˜¯å¤šä»»åŠ¡å¤„ç†å’Œè§†é¢‘å¯¹è±¡å…³è”æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14350">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a63235a8dedb827a42d8917edfe5313.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b1e9cf510817cc414098bcbf8e2b667.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0b107c4552a56b5cb3d171cd65e6210a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e900a0068210883ca8d7217aaba69d7b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Free-Lunch-Color-Texture-Disentanglement-for-Stylized-Image-Generation"><a href="#Free-Lunch-Color-Texture-Disentanglement-for-Stylized-Image-Generation" class="headerlink" title="Free-Lunch Color-Texture Disentanglement for Stylized Image Generation"></a>Free-Lunch Color-Texture Disentanglement for Stylized Image Generation</h2><p><strong>Authors:Jiang Qin, Senmao Li, Alexandra Gomez-Villa, Shiqi Yang, Yaxing Wang, Kai Wang, Joost van de Weijer</strong></p>
<p>Recent advances in Text-to-Image (T2I) diffusion models have transformed image generation, enabling significant progress in stylized generation using only a few style reference images. However, current diffusion-based methods struggle with fine-grained style customization due to challenges in controlling multiple style attributes, such as color and texture. This paper introduces the first tuning-free approach to achieve free-lunch color-texture disentanglement in stylized T2I generation, addressing the need for independently controlled style elements for the Disentangled Stylized Image Generation (DisIG) problem. Our approach leverages the Image-Prompt Additivity property in the CLIP image embedding space to develop techniques for separating and extracting Color-Texture Embeddings (CTE) from individual color and texture reference images. To ensure that the color palette of the generated image aligns closely with the color reference, we apply a whitening and coloring transformation to enhance color consistency. Additionally, to prevent texture loss due to the signal-leak bias inherent in diffusion training, we introduce a noise term that preserves textural fidelity during the Regularized Whitening and Coloring Transformation (RegWCT). Through these methods, our Style Attributes Disentanglement approach (SADis) delivers a more precise and customizable solution for stylized image generation. Experiments on images from the WikiArt and StyleDrop datasets demonstrate that, both qualitatively and quantitatively, SADis surpasses state-of-the-art stylization methods in the DisIG task. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•å·²ç»æ”¹å˜äº†å›¾åƒç”Ÿæˆé¢†åŸŸï¼Œä½¿å¾—ä»…ä½¿ç”¨å°‘æ•°é£æ ¼å‚è€ƒå›¾åƒå°±èƒ½å®ç°é£æ ¼åŒ–ç”Ÿæˆæ–¹é¢çš„æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå½“å‰åŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨æ§åˆ¶å¤šç§é£æ ¼å±æ€§ï¼ˆå¦‚é¢œè‰²å’Œçº¹ç†ï¼‰æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå› æ­¤åœ¨ç²¾ç»†é£æ ¼å®šåˆ¶æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡å¼•å…¥äº†æ— éœ€è°ƒæ•´å³å¯å®ç°é£æ ¼åŒ–T2Iç”Ÿæˆä¸­çš„è‡ªç”±åˆé¤é¢œè‰²çº¹ç†åˆ†ç¦»çš„æ–¹æ³•ï¼Œè§£å†³äº†ç¦»æ•£é£æ ¼åŒ–å›¾åƒç”Ÿæˆï¼ˆDisIGï¼‰é—®é¢˜ä¸­å¯¹ç‹¬ç«‹æ§åˆ¶é£æ ¼å…ƒç´ çš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨CLIPå›¾åƒåµŒå…¥ç©ºé—´ä¸­çš„Image-Prompt Additivityå±æ€§ï¼Œå¼€å‘å‡ºäº†ä»å•ä¸ªé¢œè‰²å’Œçº¹ç†å‚è€ƒå›¾åƒä¸­åˆ†ç¦»å’Œæå–é¢œè‰²çº¹ç†åµŒå…¥ï¼ˆCTEï¼‰çš„æŠ€æœ¯ã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆå›¾åƒçš„é¢œè‰²è°ƒè‰²æ¿ä¸é¢œè‰²å‚è€ƒç´§å¯†å¯¹é½ï¼Œæˆ‘ä»¬åº”ç”¨äº†ä¸€ç§å¢ç™½å’Œç€è‰²å˜æ¢ä»¥å¢å¼ºé¢œè‰²ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†é˜²æ­¢ç”±äºæ‰©æ•£è®­ç»ƒä¸­çš„ä¿¡å·æ³„æ¼åå·®è€Œå¯¼è‡´çš„çº¹ç†æŸå¤±ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹å™ªå£°é¡¹ï¼Œåœ¨æ­£åˆ™åŒ–å¢ç™½å’Œç€è‰²å˜æ¢ï¼ˆRegWCTï¼‰è¿‡ç¨‹ä¸­ä¿æŒçº¹ç†ä¿çœŸåº¦ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬çš„é£æ ¼å±æ€§åˆ†ç¦»æ–¹æ³•ï¼ˆSADisï¼‰ä¸ºé£æ ¼åŒ–å›¾åƒç”Ÿæˆæä¾›äº†æ›´ç²¾ç¡®å’Œå¯å®šåˆ¶åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨WikiArtå’ŒStyleDropæ•°æ®é›†ä¸Šçš„å›¾åƒå®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨å®šæ€§è¿˜æ˜¯å®šé‡ä¸Šï¼ŒSADisåœ¨DisIGä»»åŠ¡ä¸­éƒ½è¶…è¶Šäº†æœ€å…ˆè¿›çš„é£æ ¼åŒ–æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14275v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è°ƒæ•´çš„æ–¹æ³•æ¥å®ç°é£æ ¼åŒ–å›¾åƒç”Ÿæˆä¸­çš„è‰²å½©çº¹ç†è§£çº ç¼ ã€‚è¯¥æ–¹æ³•åˆ©ç”¨CLIPå›¾åƒåµŒå…¥ç©ºé—´ä¸­çš„å›¾åƒæç¤ºæ·»åŠ æ€§å±æ€§ï¼Œä»å•ä¸ªè‰²å½©å’Œçº¹ç†å‚è€ƒå›¾åƒä¸­åˆ†ç¦»å’Œæå–è‰²å½©çº¹ç†åµŒå…¥ï¼ˆCTEï¼‰ã€‚é€šè¿‡ç™½åŒ–å’Œå½©è‰²è½¬æ¢å¢å¼ºé¢œè‰²ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡å¼•å…¥å™ªå£°é¡¹åœ¨è§„åˆ™åŒ–ç™½åŒ–å’Œå½©è‰²è½¬æ¢ï¼ˆRegWCTï¼‰è¿‡ç¨‹ä¸­ä¿æŒçº¹ç†ä¿çœŸåº¦ï¼Œä»¥é˜²æ­¢ç”±äºæ‰©æ•£è®­ç»ƒä¸­çš„ä¿¡å·æ³„æ¼åå·®è€Œå¯¼è‡´çš„çº¹ç†æŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼ŒStyle Attributes Disentanglementï¼ˆSADisï¼‰æ–¹æ³•åœ¨WikiArtå’ŒStyleDropæ•°æ®é›†ä¸Šçš„é£æ ¼åŒ–å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ— è®ºåœ¨å®šæ€§è¿˜æ˜¯å®šé‡æ–¹é¢éƒ½è¶…è¶Šäº†æœ€æ–°é£æ ¼åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•å®ç°äº†åŸºäºå°‘é‡é£æ ¼å‚è€ƒå›¾åƒçš„æ˜¾è‘—é£æ ¼åŒ–ç”Ÿæˆã€‚</li>
<li>å½“å‰æ‰©æ•£æ–¹æ³•é¢ä¸´ç²¾ç»†é£æ ¼å®šåˆ¶çš„æŒ‘æˆ˜ï¼Œéš¾ä»¥æ§åˆ¶å¤šç§é£æ ¼å±æ€§ï¼Œå¦‚è‰²å½©å’Œçº¹ç†ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡æå‡ºäº†æ— éœ€è°ƒæ•´çš„è‰²å½©çº¹ç†è§£çº ç¼ æ–¹æ³•ï¼Œè§£å†³äº†ç‹¬ç«‹æ§åˆ¶é£æ ¼å…ƒç´ çš„éœ€æ±‚ã€‚</li>
<li>åˆ©ç”¨CLIPå›¾åƒåµŒå…¥ç©ºé—´çš„å›¾åƒæç¤ºæ·»åŠ æ€§å±æ€§ï¼Œå®ç°è‰²å½©çº¹ç†åµŒå…¥ï¼ˆCTEï¼‰çš„åˆ†ç¦»å’Œæå–ã€‚</li>
<li>é€šè¿‡ç™½åŒ–å’Œå½©è‰²è½¬æ¢å¢å¼ºé¢œè‰²ä¸€è‡´æ€§ï¼Œç¡®ä¿ç”Ÿæˆå›¾åƒçš„é¢œè‰²è°ƒè‰²æ¿ä¸é¢œè‰²å‚è€ƒå¯¹é½ã€‚</li>
<li>å¼•å…¥å™ªå£°é¡¹ä»¥ä¿æŒçº¹ç†ä¿çœŸåº¦ï¼Œé˜²æ­¢å› æ‰©æ•£è®­ç»ƒä¸­çš„ä¿¡å·æ³„æ¼åå·®è€Œå¯¼è‡´çš„çº¹ç†æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-280096814a7e6eaf4f2c2b425183cc5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-463d97b7c4a1021a07daf8446c9fece1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe732d1aea1c35bb65c8aa5063198c41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e96c1c7be3d4cca8694e1faaa64c9eef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-042c7447d4c97623da6eacf567efd287.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CTSR-Controllable-Fidelity-Realness-Trade-off-Distillation-for-Real-World-Image-Super-Resolution"><a href="#CTSR-Controllable-Fidelity-Realness-Trade-off-Distillation-for-Real-World-Image-Super-Resolution" class="headerlink" title="CTSR: Controllable Fidelity-Realness Trade-off Distillation for   Real-World Image Super Resolution"></a>CTSR: Controllable Fidelity-Realness Trade-off Distillation for   Real-World Image Super Resolution</h2><p><strong>Authors:Runyi Li, Bin Chen, Jian Zhang, Radu Timofte</strong></p>
<p>Real-world image super-resolution is a critical image processing task, where two key evaluation criteria are the fidelity to the original image and the visual realness of the generated results. Although existing methods based on diffusion models excel in visual realness by leveraging strong priors, they often struggle to achieve an effective balance between fidelity and realness. In our preliminary experiments, we observe that a linear combination of multiple models outperforms individual models, motivating us to harness the strengths of different models for a more effective trade-off. Based on this insight, we propose a distillation-based approach that leverages the geometric decomposition of both fidelity and realness, alongside the performance advantages of multiple teacher models, to strike a more balanced trade-off. Furthermore, we explore the controllability of this trade-off, enabling a flexible and adjustable super-resolution process, which we call CTSR (Controllable Trade-off Super-Resolution). Experiments conducted on several real-world image super-resolution benchmarks demonstrate that our method surpasses existing state-of-the-art approaches, achieving superior performance across both fidelity and realness metrics. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡å¤„ç†æ˜¯ä¸€é¡¹å…³é”®çš„å›¾åƒå¤„ç†ä»»åŠ¡ï¼Œå…¶ä¸¤ä¸ªå…³é”®çš„è¯„ä¼°æ ‡å‡†æ˜¯å¯¹äºåŸå§‹å›¾åƒçš„ä¿çœŸåº¦å’Œç”Ÿæˆç»“æœçš„è§†è§‰çœŸå®åº¦ã€‚å°½ç®¡åŸºäºæ‰©æ•£æ¨¡å‹çš„ç°æœ‰æ–¹æ³•åœ¨åˆ©ç”¨å¼ºå¤§å…ˆéªŒçŸ¥è¯†æ–¹é¢åœ¨è§†è§‰çœŸå®åº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥åœ¨ä¿çœŸåº¦å’ŒçœŸå®åº¦ä¹‹é—´å–å¾—æœ‰æ•ˆå¹³è¡¡ã€‚åœ¨æˆ‘ä»¬çš„åˆæ­¥å®éªŒä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å¤šç§æ¨¡å‹çš„çº¿æ€§ç»„åˆè¡¨ç°ä¼˜äºå•ä¸ªæ¨¡å‹ï¼Œè¿™æ¿€åŠ±æˆ‘ä»¬åˆ©ç”¨ä¸åŒæ¨¡å‹çš„ä¼˜åŠ¿æ¥å®ç°æ›´æœ‰æ•ˆçš„æƒè¡¡ã€‚åŸºäºæ­¤è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè’¸é¦çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä¿çœŸåº¦å’ŒçœŸå®åº¦çš„å‡ ä½•åˆ†è§£ï¼Œä»¥åŠå¤šä¸ªæ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä»¥å®ç°æ›´å¹³è¡¡çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†è¿™ç§æƒè¡¡çš„å¯æ§æ€§ï¼Œå®ç°äº†ä¸€ä¸ªçµæ´»å¯è°ƒçš„è¶…çº§åˆ†è¾¨ç‡è¿‡ç¨‹ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºå¯æ§æƒè¡¡è¶…çº§åˆ†è¾¨ç‡ï¼ˆCTSRï¼‰ã€‚åœ¨å‡ ä¸ªç°å®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨ä¿çœŸåº¦å’ŒçœŸå®åº¦æŒ‡æ ‡ä¸Šå‡å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14272v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„ç°å®å›¾åƒè¶…åˆ†è¾¨ç‡å¤„ç†æŠ€æœ¯é¢ä¸´ä¿æŒåŸå§‹å›¾åƒä¿çœŸåº¦å’Œç”Ÿæˆç»“æœçœŸå®æ„Ÿçš„åŒé‡è¯„ä»·å‡†åˆ™æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶æ“…é•¿åˆ©ç”¨å¼ºå…ˆéªŒä¿¡æ¯æé«˜çœŸå®æ„Ÿï¼Œä½†åœ¨ä¿æŒä¿çœŸå’ŒçœŸå®ä¹‹é—´çš„å¹³è¡¡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ¬ç ”ç©¶è§‚å¯Ÿåˆ°å¤šç§æ¨¡å‹çš„çº¿æ€§ç»„åˆè¡¨ç°ä¼˜äºå•ä¸€æ¨¡å‹ï¼Œå› æ­¤æå‡ºä¸€ç§åŸºäºè’¸é¦çš„æ–¹æ³•ï¼Œé€šè¿‡å‡ ä½•åˆ†è§£ä¿çœŸåº¦å’ŒçœŸå®æ„Ÿï¼Œå¹¶ç»“åˆå¤šä¸ªæ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œå®ç°æ›´å¹³è¡¡çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æ¢ç´¢äº†è¿™ç§æƒè¡¡çš„å¯æ§æ€§ï¼Œæå‡ºäº†ä¸€ç§çµæ´»å¯è°ƒçš„è¶…çº§åˆ†è¾¨ç‡è¿‡ç¨‹ï¼Œç§°ä¸ºå¯æ§æƒè¡¡è¶…çº§åˆ†è¾¨ç‡ï¼ˆCTSRï¼‰ã€‚åœ¨å¤šä¸ªç°å®å›¾åƒè¶…åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨ä¿çœŸåº¦å’ŒçœŸå®æ„ŸæŒ‡æ ‡ä¸Šå‡å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡å¤„ç†ä¸­é¢ä¸´ä¿æŒä¿çœŸåº¦å’ŒçœŸå®æ„Ÿçš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è™½ç„¶èƒ½æé«˜çœŸå®æ„Ÿï¼Œä½†åœ¨ä¿æŒä¸¤è€…å¹³è¡¡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>é€šè¿‡è§‚å¯Ÿå‘ç°å¤šç§æ¨¡å‹çš„çº¿æ€§ç»„åˆé€šå¸¸è¡¨ç°æ›´å¥½ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºè’¸é¦çš„æ–¹æ³•ï¼Œé€šè¿‡å‡ ä½•åˆ†è§£å’Œç»“åˆå¤šä¸ªæ•™å¸ˆæ¨¡å‹å®ç°æ›´å¹³è¡¡çš„æƒè¡¡ã€‚</li>
<li>å¼•å…¥äº†å¯æ§æƒè¡¡çš„æ¦‚å¿µï¼Œä½¿å¾—è¶…çº§åˆ†è¾¨ç‡è¿‡ç¨‹æ›´åŠ çµæ´»å¯è°ƒã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªç°å®å›¾åƒè¶…åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf87d6fec016e60098b98b6050892ed0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05eb09180310ba4fdd44856e10249caf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da70224350850b71e1deaf6bc073009f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e6ab0f45148391164f06af6258a587f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CRCE-Coreference-Retention-Concept-Erasure-in-Text-to-Image-Diffusion-Models"><a href="#CRCE-Coreference-Retention-Concept-Erasure-in-Text-to-Image-Diffusion-Models" class="headerlink" title="CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion   Models"></a>CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion   Models</h2><p><strong>Authors:Yuyang Xue, Edward Moroshko, Feng Chen, Steven McDonagh, Sotirios A. Tsaftaris</strong></p>
<p>Text-to-Image diffusion models can produce undesirable content that necessitates concept erasure techniques. However, existing methods struggle with under-erasure, leaving residual traces of targeted concepts, or over-erasure, mistakenly eliminating unrelated but visually similar concepts. To address these limitations, we introduce CRCE, a novel concept erasure framework that leverages Large Language Models to identify both semantically related concepts that should be erased alongside the target and distinct concepts that should be preserved. By explicitly modeling coreferential and retained concepts semantically, CRCE enables more precise concept removal, without unintended erasure. Experiments demonstrate that CRCE outperforms existing methods on diverse erasure tasks. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¼šäº§ç”Ÿä¸éœ€è¦çš„å†…å®¹ï¼Œè¿™éœ€è¦ä½¿ç”¨æ¦‚å¿µæ¶ˆé™¤æŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨æ¶ˆé™¤æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¦ä¹ˆæ— æ³•å®Œå…¨æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µè€Œç•™ä¸‹æ®‹ç•™ç—•è¿¹ï¼Œè¦ä¹ˆè¯¯åˆ é™¤ä¸ç‰¹å®šæ¦‚å¿µæ— å…³ä½†è§†è§‰ä¸Šç›¸ä¼¼çš„æ¦‚å¿µã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†CRCEï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¦‚å¿µæ¶ˆé™¤æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯†åˆ«ä¸ç›®æ ‡æ¦‚å¿µåŒæ—¶åº”è¯¥æ¶ˆé™¤çš„è¯­ä¹‰ç›¸å…³æ¦‚å¿µï¼Œä»¥åŠåº”è¯¥ä¿ç•™çš„ç‰¹æœ‰æ¦‚å¿µã€‚é€šè¿‡æ˜ç¡®åœ°å»ºç«‹æ ¸å¿ƒå‚ç…§å’Œä¿ç•™æ¦‚å¿µçš„è¯­ä¹‰æ¨¡å‹ï¼ŒCRCEèƒ½å¤Ÿå®ç°æ›´ç²¾ç¡®çš„æ¦‚å¿µå»é™¤ï¼Œé¿å…ä¸å¿…è¦çš„è¯¯åˆ é™¤ã€‚å®éªŒè¡¨æ˜ï¼ŒCRCEåœ¨å¤šç§æ¶ˆé™¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14232v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„ä¸ç†æƒ³å†…å®¹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºCRCEçš„æ–°æ¦‚å¿µæ¶ˆé™¤æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯†åˆ«åº”æ¶ˆé™¤çš„ç›®æ ‡æ¦‚å¿µä»¥åŠä¸ç›®æ ‡è¯­ä¹‰ç›¸å…³çš„æ¦‚å¿µï¼Œä»¥åŠåº”ä¿æŒçš„ç‹¬ç‰¹æ¦‚å¿µã€‚CRCEèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°ç§»é™¤æ¦‚å¿µï¼Œè€Œä¸ä¼šé€ æˆæ„å¤–æ¶ˆé™¤ã€‚å®éªŒè¯æ˜ï¼ŒCRCEåœ¨å¤šç§æ¶ˆé™¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¼šç”Ÿæˆä¸ç†æƒ³çš„å†…å®¹ï¼Œéœ€è¦æ¦‚å¿µæ¶ˆé™¤æŠ€æœ¯æ¥è§£å†³ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼Œå¦‚ä¸å®Œå…¨æ¶ˆé™¤æˆ–è¿‡åº¦æ¶ˆé™¤ã€‚</li>
<li>CRCEæ¡†æ¶è¢«å¼•å…¥ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯†åˆ«åº”æ¶ˆé™¤å’Œä¿ç•™çš„æ¦‚å¿µã€‚</li>
<li>CRCEèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°ç§»é™¤æ¦‚å¿µï¼Œé¿å…æ„å¤–æ¶ˆé™¤ã€‚</li>
<li>CRCEåœ¨å¤šç§æ¶ˆé™¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>CRCEæ¡†æ¶é€šè¿‡æ˜ç¡®å»ºæ¨¡æ ¸å¿ƒæ¦‚å¿µå’Œä¿ç•™æ¦‚å¿µçš„è¯­ä¹‰æ¥å®ç°å…¶ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7719807530de50f8f423632590331182.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28a807bd46cbf1bc8969eebf75967a6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f40199f40f15efa43a5f92e379f5d03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-619e6d5bfdf3f6a1a0f13a78667fd828.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DefectFill-Realistic-Defect-Generation-with-Inpainting-Diffusion-Model-for-Visual-Inspection"><a href="#DefectFill-Realistic-Defect-Generation-with-Inpainting-Diffusion-Model-for-Visual-Inspection" class="headerlink" title="DefectFill: Realistic Defect Generation with Inpainting Diffusion Model   for Visual Inspection"></a>DefectFill: Realistic Defect Generation with Inpainting Diffusion Model   for Visual Inspection</h2><p><strong>Authors:Jaewoo Song, Daemin Park, Kanghyun Baek, Sangyub Lee, Jooyoung Choi, Eunji Kim, Sungroh Yoon</strong></p>
<p>Developing effective visual inspection models remains challenging due to the scarcity of defect data. While image generation models have been used to synthesize defect images, producing highly realistic defects remains difficult. We propose DefectFill, a novel method for realistic defect generation that requires only a few reference defect images. It leverages a fine-tuned inpainting diffusion model, optimized with our custom loss functions incorporating defect, object, and attention terms. It enables precise capture of detailed, localized defect features and their seamless integration into defect-free objects. Additionally, our Low-Fidelity Selection method further enhances the defect sample quality. Experiments show that DefectFill generates high-quality defect images, enabling visual inspection models to achieve state-of-the-art performance on the MVTec AD dataset. </p>
<blockquote>
<p>å¼€å‘æœ‰æ•ˆçš„è§†è§‰æ£€æµ‹æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç¼ºé™·æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚è™½ç„¶å›¾åƒç”Ÿæˆæ¨¡å‹å·²è¢«ç”¨äºåˆæˆç¼ºé™·å›¾åƒï¼Œä½†äº§ç”Ÿé«˜åº¦é€¼çœŸçš„ç¼ºé™·ä»ç„¶å¾ˆå›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†DefectFillï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç°å®ç¼ºé™·ç”Ÿæˆæ–¹æ³•ï¼Œåªéœ€è¦å°‘é‡çš„å‚è€ƒç¼ºé™·å›¾åƒã€‚å®ƒåˆ©ç”¨ç»è¿‡å¾®è°ƒçš„å†…å¡«æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆç¼ºé™·ã€å¯¹è±¡å’Œæ³¨æ„åŠ›æœ¯è¯­ï¼Œå¯¹æˆ‘ä»¬çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚å®ƒèƒ½å¤Ÿç²¾ç¡®æ•æ‰è¯¦ç»†çš„å±€éƒ¨ç¼ºé™·ç‰¹å¾ï¼Œå¹¶å°†å…¶æ— ç¼é›†æˆåˆ°æ— ç¼ºé™·çš„å¯¹è±¡ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ä½è´¨é‡é€‰æ‹©æ–¹æ³•è¿›ä¸€æ­¥æé«˜äº†ç¼ºé™·æ ·æœ¬çš„è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒDefectFillèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ç¼ºé™·å›¾åƒï¼Œä½¿è§†è§‰æ£€æµ‹æ¨¡å‹åœ¨MVTec ADæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13985v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¼ºé™·æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œå‘å±•æœ‰æ•ˆçš„è§†è§‰æ£€æµ‹æ¨¡å‹ä¾ç„¶é¢‡å…·éš¾åº¦ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDefectFillçš„æ–°å‹ç¼ºé™·ç”Ÿæˆæ–¹æ³•ï¼Œä»…éœ€å°‘é‡ç¼ºé™·å›¾åƒå‚è€ƒå³å¯ç”Ÿæˆé€¼çœŸçš„ç¼ºé™·ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¾®è°ƒåçš„å¡«å……æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆå®šåˆ¶çš„æŸå¤±å‡½æ•°å’Œç¼ºé™·ã€ç‰©ä½“ä»¥åŠæ³¨æ„åŠ›æœ¯è¯­è¿›è¡Œä¼˜åŒ–ï¼Œèƒ½ç²¾å‡†æ•æ‰è¯¦ç»†çš„å±€éƒ¨ç¼ºé™·ç‰¹å¾ï¼Œå¹¶å°†å…¶æ— ç¼é›†æˆåˆ°æ— ç¼ºé™·ç‰©ä½“ä¸­ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ä½ä¿çœŸé€‰æ‹©æ–¹æ³•è¿›ä¸€æ­¥æå‡äº†ç¼ºé™·æ ·æœ¬è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒDefectFillç”Ÿæˆçš„ç¼ºé™·å›¾åƒè´¨é‡é«˜ï¼Œèƒ½ä½¿è§†è§‰æ£€æµ‹æ¨¡å‹åœ¨MVTec ADæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†DefectFillæ–¹æ³•ï¼Œç”¨äºåŸºäºå°‘é‡å‚è€ƒç¼ºé™·å›¾åƒç”Ÿæˆé€¼çœŸçš„ç¼ºé™·ã€‚</li>
<li>åˆ©ç”¨å¾®è°ƒåçš„å¡«å……æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆå®šåˆ¶çš„æŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>èåˆäº†ç¼ºé™·ã€ç‰©ä½“å’Œæ³¨æ„åŠ›æœ¯è¯­ï¼Œèƒ½å¤Ÿç²¾å‡†æ•æ‰å±€éƒ¨ç¼ºé™·ç‰¹å¾ã€‚</li>
<li>æå‡ºä½ä¿çœŸé€‰æ‹©æ–¹æ³•ï¼Œæå‡äº†ç¼ºé™·æ ·æœ¬è´¨é‡ã€‚</li>
<li>DefectFillç”Ÿæˆçš„ç¼ºé™·å›¾åƒè´¨é‡é«˜ã€‚</li>
<li>è§†è§‰æ£€æµ‹æ¨¡å‹åœ¨MVTec ADæ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13985">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-141535b4558ad415ccf68b9b107dd58c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c17782eab1cc39607449de4df33dd5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-641a2ca98211d5ea5ce31d4de39f99a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04c71ec2016c9e4f825b391d5482921b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-148259f9547e4032f12fc3e79ab9f340.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DIFFVSGG-Diffusion-Driven-Online-Video-Scene-Graph-Generation"><a href="#DIFFVSGG-Diffusion-Driven-Online-Video-Scene-Graph-Generation" class="headerlink" title="DIFFVSGG: Diffusion-Driven Online Video Scene Graph Generation"></a>DIFFVSGG: Diffusion-Driven Online Video Scene Graph Generation</h2><p><strong>Authors:Mu Chen, Liulei Li, Wenguan Wang, Yi Yang</strong></p>
<p>Top-leading solutions for Video Scene Graph Generation (VSGG) typically adopt an offline pipeline. Though demonstrating promising performance, they remain unable to handle real-time video streams and consume large GPU memory. Moreover, these approaches fall short in temporal reasoning, merely aggregating frame-level predictions over a temporal context. In response, we introduce DIFFVSGG, an online VSGG solution that frames this task as an iterative scene graph update problem. Drawing inspiration from Latent Diffusion Models (LDMs) which generate images via denoising a latent feature embedding, we unify the decoding of object classification, bounding box regression, and graph generation three tasks using one shared feature embedding. Then, given an embedding containing unified features of object pairs, we conduct a step-wise Denoising on it within LDMs, so as to deliver a clean embedding which clearly indicates the relationships between objects. This embedding then serves as the input to task-specific heads for object classification, scene graph generation, etc. DIFFVSGG further facilitates continuous temporal reasoning, where predictions for subsequent frames leverage results of past frames as the conditional inputs of LDMs, to guide the reverse diffusion process for current frames. Extensive experiments on three setups of Action Genome demonstrate the superiority of DIFFVSGG. </p>
<blockquote>
<p>é’ˆå¯¹è§†é¢‘åœºæ™¯å›¾ç”Ÿæˆï¼ˆVSGGï¼‰çš„é¡¶å°–è§£å†³æ–¹æ¡ˆé€šå¸¸é‡‡ç”¨ç¦»çº¿ç®¡é“ã€‚å°½ç®¡è¡¨ç°æœ‰å‰æ™¯ï¼Œä½†å®ƒä»¬ä»æ— æ³•å¤„ç†å®æ—¶è§†é¢‘æµå¹¶æ¶ˆè€—å¤§é‡çš„GPUå†…å­˜ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•åœ¨æ—¶åºæ¨ç†æ–¹é¢ä¸è¶³ï¼Œä»…ä»…åœ¨æ—¶åºä¸Šä¸‹æ–‡ä¸­å¯¹å¸§çº§é¢„æµ‹è¿›è¡Œèšåˆã€‚ä¸ºäº†åº”å¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DIFFVSGGï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨çº¿VSGGè§£å†³æ–¹æ¡ˆï¼Œå°†æ­¤é¡¹ä»»åŠ¡æ„å»ºä¸ºè¿­ä»£åœºæ™¯å›¾æ›´æ–°é—®é¢˜ã€‚æˆ‘ä»¬ä»æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä¸­æ±²å–çµæ„Ÿï¼Œè¯¥æ¨¡å‹é€šè¿‡å»å™ªæ½œåœ¨ç‰¹å¾åµŒå…¥æ¥ç”Ÿæˆå›¾åƒã€‚æˆ‘ä»¬ç»Ÿä¸€äº†ç›®æ ‡åˆ†ç±»ã€è¾¹ç•Œæ¡†å›å½’å’Œå›¾å½¢ç”Ÿæˆè¿™ä¸‰ä¸ªä»»åŠ¡çš„è§£ç å·¥ä½œï¼Œä½¿ç”¨å•ä¸€å…±äº«ç‰¹å¾åµŒå…¥ã€‚ç»™å®šåŒ…å«å¯¹è±¡å¯¹ç»Ÿä¸€ç‰¹å¾çš„åµŒå…¥åï¼Œæˆ‘ä»¬åœ¨LDMä¸­å¯¹å…¶è¿›è¡Œåˆ†æ­¥å»å™ªå¤„ç†ï¼Œä»¥è·å–æ¸…æ™°æŒ‡ç¤ºå¯¹è±¡é—´å…³ç³»çš„å¹²å‡€åµŒå…¥ã€‚ç„¶åï¼Œæ­¤åµŒå…¥ä½œä¸ºç›®æ ‡åˆ†ç±»ã€åœºæ™¯å›¾ç”Ÿæˆç­‰ä»»åŠ¡ç‰¹å®šå¤´çš„è¾“å…¥ã€‚DIFFVSGGè¿›ä¸€æ­¥ä¿ƒè¿›äº†è¿ç»­çš„æ—¶åºæ¨ç†ï¼Œåç»­å¸§çš„é¢„æµ‹åˆ©ç”¨è¿‡å»å¸§çš„ç»“æœä½œä¸ºLDMçš„æ¡ä»¶è¾“å…¥ï¼Œä»¥æŒ‡å¯¼å½“å‰å¸§çš„åå‘æ‰©æ•£è¿‡ç¨‹ã€‚åœ¨Action Genomeçš„ä¸‰ä¸ªè®¾ç½®ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†DIFFVSGGçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13957v1">PDF</a> CVPR 2025, Code: <a target="_blank" rel="noopener" href="https://github.com/kagawa588/DiffVsgg">https://github.com/kagawa588/DiffVsgg</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„åœ¨çº¿è§†é¢‘åœºæ™¯å›¾ç”Ÿæˆæ–¹æ¡ˆDIFFVSGGè§£å†³äº†ä¼ ç»Ÿç¦»çº¿ç®¡é“åœ¨å®æ—¶è§†é¢‘æµå¤„ç†æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡ç»Ÿä¸€ç‰¹å¾åµŒå…¥è§£ç ç‰©ä½“åˆ†ç±»ã€è¾¹ç•Œæ¡†å›å½’å’Œå›¾ç”Ÿæˆä¸‰é¡¹ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å»å™ªæ­¥éª¤ï¼ŒDIFFVSGGèƒ½æ¸…æ™°è¡¨è¾¾ç‰©ä½“é—´å…³ç³»ã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æŒè¿ç»­æ—¶é—´æ¨ç†ï¼Œåˆ©ç”¨è¿‡å»å¸§çš„é¢„æµ‹ç»“æœæŒ‡å¯¼å½“å‰å¸§çš„åæ‰©æ•£è¿‡ç¨‹ã€‚åœ¨Action Genomeçš„ä¸‰ä¸ªè®¾ç½®ä¸Šçš„å®éªŒè¯æ˜äº†DIFFVSGGçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DIFFVSGGæ˜¯ä¸€ç§åœ¨çº¿è§†é¢‘åœºæ™¯å›¾ç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†ä¼ ç»Ÿç¦»çº¿ç®¡é“æ— æ³•å¤„ç†å®æ—¶è§†é¢‘æµçš„é—®é¢˜ã€‚</li>
<li>å®ƒé€šè¿‡ç»Ÿä¸€ç‰¹å¾åµŒå…¥è§£ç ç‰©ä½“åˆ†ç±»ã€è¾¹ç•Œæ¡†å›å½’å’Œå›¾ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>DIFFVSGGé‡‡ç”¨åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å»å™ªæ­¥éª¤ï¼Œä»¥è¡¨è¾¾ç‰©ä½“é—´çš„æ¸…æ™°å…³ç³»ã€‚</li>
<li>è¯¥æ–¹æ¡ˆæ”¯æŒè¿ç»­æ—¶é—´æ¨ç†ï¼Œåˆ©ç”¨è¿‡å»å¸§çš„é¢„æµ‹ç»“æœæŒ‡å¯¼å½“å‰å¸§çš„å¤„ç†ã€‚</li>
<li>DIFFVSGGåœ¨Action Genomeçš„ä¸‰ä¸ªè®¾ç½®ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†DIFFVSGGåœ¨è§†é¢‘åœºæ™¯å›¾ç”Ÿæˆä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c99003cab15e2a434b03a2674ea7508d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00bf1211107b94d3816de831b092648b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-831f0d388e86c1d1715f26a4d0c0414e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b41d9950a874103296214ebb968bfa2f.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Sparse-Autoencoder-as-a-Zero-Shot-Classifier-for-Concept-Erasing-in-Text-to-Image-Diffusion-Models"><a href="#Sparse-Autoencoder-as-a-Zero-Shot-Classifier-for-Concept-Erasing-in-Text-to-Image-Diffusion-Models" class="headerlink" title="Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in   Text-to-Image Diffusion Models"></a>Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in   Text-to-Image Diffusion Models</h2><p><strong>Authors:Zhihua Tian, Sirun Nan, Ming Xu, Shengfang Zhai, Wenjie Qu, Jian Liu, Kui Ren, Ruoxi Jia, Jiaheng Zhang</strong></p>
<p>Text-to-image (T2I) diffusion models have achieved remarkable progress in generating high-quality images but also raise peopleâ€™s concerns about generating harmful or misleading content. While extensive approaches have been proposed to erase unwanted concepts without requiring retraining from scratch, they inadvertently degrade performance on normal generation tasks. In this work, we propose Interpret then Deactivate (ItD), a novel framework to enable precise concept removal in T2I diffusion models while preserving overall performance. ItD first employs a sparse autoencoder (SAE) to interpret each concept as a combination of multiple features. By permanently deactivating the specific features associated with target concepts, we repurpose SAE as a zero-shot classifier that identifies whether the input prompt includes target concepts, allowing selective concept erasure in diffusion models. Moreover, we demonstrate that ItD can be easily extended to erase multiple concepts without requiring further training. Comprehensive experiments across celebrity identities, artistic styles, and explicit content demonstrate ItDâ€™s effectiveness in eliminating targeted concepts without interfering with normal concept generation. Additionally, ItD is also robust against adversarial prompts designed to circumvent content filters. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/NANSirun/Interpret-then-deactivate">https://github.com/NANSirun/Interpret-then-deactivate</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ä¹Ÿå¼•å‘äº†äººä»¬å¯¹ç”Ÿæˆæœ‰å®³æˆ–è¯¯å¯¼æ€§å†…å®¹çš„æ‹…å¿§ã€‚è™½ç„¶å·²æœ‰è®¸å¤šæ–¹æ³•è¢«æå‡ºæ¥æ¶ˆé™¤ä¸éœ€è¦çš„æ¦‚å¿µï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒï¼Œä½†å®ƒä»¬æ— æ„ä¸­ä¼šé™ä½æ­£å¸¸ç”Ÿæˆä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè§£é‡Šç„¶ååœç”¨â€ï¼ˆItDï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨T2Iæ‰©æ•£æ¨¡å‹ä¸­å®ç°ç²¾ç¡®çš„æ¦‚å¿µç§»é™¤ï¼ŒåŒæ—¶ä¿ç•™æ•´ä½“æ€§èƒ½ã€‚ItDé¦–å…ˆé‡‡ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰æ¥è§£é‡Šæ¯ä¸ªæ¦‚å¿µæ˜¯å¤šä¸ªç‰¹å¾çš„ç»„åˆã€‚é€šè¿‡æ°¸ä¹…åœç”¨ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„ç‰¹å®šç‰¹å¾ï¼Œæˆ‘ä»¬å°†SAEé‡æ–°å®šä½ä¸ºä¸€ç§é›¶å°„å‡»åˆ†ç±»å™¨ï¼Œå¯ä»¥è¯†åˆ«è¾“å…¥æç¤ºæ˜¯å¦åŒ…å«ç›®æ ‡æ¦‚å¿µï¼Œä»è€Œåœ¨æ‰©æ•£æ¨¡å‹ä¸­å®ç°é€‰æ‹©æ€§æ¦‚å¿µåˆ é™¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ItDå¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•åˆ°åˆ é™¤å¤šä¸ªæ¦‚å¿µï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒã€‚åœ¨åäººèº«ä»½ã€è‰ºæœ¯é£æ ¼å’Œæ˜ç¡®å†…å®¹æ–¹é¢çš„ç»¼åˆå®éªŒè¯æ˜äº†ItDåœ¨æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸ä¼šå¹²æ‰°æ­£å¸¸çš„æ¦‚å¿µç”Ÿæˆã€‚æ­¤å¤–ï¼ŒItDå¯¹äºè®¾è®¡ç”¨äºè§„é¿å†…å®¹è¿‡æ»¤å™¨çš„å¯¹æŠ—æ€§æç¤ºä¹Ÿå¾ˆç¨³å¥ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/NANSirun/Interpret-then-deactivate%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NANSirun/Interpret-then-deactivateæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09446v2">PDF</a> 25 pages</p>
<p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¹Ÿå¼•å‘äº†äººä»¬å¯¹ç”Ÿæˆæœ‰å®³æˆ–è¯¯å¯¼æ€§å†…å®¹çš„æ‹…å¿§ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šæ–¹æ³•æ¥æ¶ˆé™¤ä¸éœ€è¦çš„æ¦‚å¿µï¼Œè€Œæ— éœ€ä»å¤´è¿›è¡Œå†è®­ç»ƒï¼Œä½†å®ƒä»¬ä¼šæ— æ„ä¸­é™ä½æ­£å¸¸ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè§£é‡Šååœç”¨â€ï¼ˆItDï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨T2Iæ‰©æ•£æ¨¡å‹ä¸­å®ç°ç²¾ç¡®çš„æ¦‚å¿µç§»é™¤ï¼ŒåŒæ—¶ä¿ç•™æ•´ä½“æ€§èƒ½ã€‚ItDé¦–å…ˆä½¿ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰æ¥è§£é‡Šæ¯ä¸ªæ¦‚å¿µæ˜¯å¤šä¸ªç‰¹å¾çš„ç»„åˆã€‚é€šè¿‡æ°¸ä¹…åœç”¨ä¸ç›®æ ‡æ¦‚å¿µç›¸å…³çš„ç‰¹å®šç‰¹å¾ï¼Œæˆ‘ä»¬é‡æ–°å°†SAEç”¨ä½œé›¶æ ·æœ¬åˆ†ç±»å™¨ï¼Œä»¥è¯†åˆ«è¾“å…¥æç¤ºæ˜¯å¦åŒ…å«ç›®æ ‡æ¦‚å¿µï¼Œä»è€Œåœ¨æ‰©æ•£æ¨¡å‹ä¸­å®ç°é€‰æ‹©æ€§æ¦‚å¿µåˆ é™¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ItDå¯ä»¥è½»æ¾åœ°æ‰©å±•åˆ°åˆ é™¤å¤šä¸ªæ¦‚å¿µï¼Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒã€‚åœ¨åäººèº«ä»½ã€è‰ºæœ¯é£æ ¼å’Œæ˜ç¡®å†…å®¹æ–¹é¢çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒItDåœ¨æ¶ˆé™¤ç›®æ ‡æ¦‚å¿µæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œè€Œä¸ä¼šå¹²æ‰°æ­£å¸¸çš„æ¦‚å¿µç”Ÿæˆã€‚æ­¤å¤–ï¼ŒItDè¿˜èƒ½å¯¹æŠ—æ—¨åœ¨ç»•è¿‡å†…å®¹è¿‡æ»¤å™¨çš„å¯¹æŠ—æ€§æç¤ºã€‚ç›¸å…³ä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/NANSirun/Interpret-then-deactivate">ç½‘å€é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹è™½èƒ½ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†å­˜åœ¨ç”Ÿæˆæœ‰å®³æˆ–è¯¯å¯¼å†…å®¹çš„é£é™©ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨å»é™¤éå¿…è¦æ¦‚å¿µæ—¶å¯èƒ½å½±å“æ¨¡å‹åœ¨å¸¸è§„ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æå‡ºäº†â€œè§£é‡Šååœç”¨â€ï¼ˆItDï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ç²¾ç¡®ç§»é™¤æœ‰å®³æ¦‚å¿µçš„åŒæ—¶ä¿æŒæ•´ä½“æ€§èƒ½ã€‚</li>
<li>ItDåˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰è§£é‡Šæ¦‚å¿µå¹¶åœç”¨ç‰¹å®šç‰¹å¾ï¼Œå®ç°é€‰æ‹©æ€§æ¦‚å¿µåˆ é™¤ã€‚</li>
<li>ItDå¯è½»æ¾æ‰©å±•åˆ°åˆ é™¤å¤šä¸ªæ¦‚å¿µï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>å®éªŒè¯æ˜ItDåœ¨æ¶ˆé™¤ç‰¹å®šæ¦‚å¿µæ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œä¸å½±å“æ­£å¸¸æ¦‚å¿µç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3dce8405617cec30bd53c1b7fe9ec266.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-652a771469b19328d85285631c4affb0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3cae802f1211e999084eb8ef93bb18c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2dd786f3a40be6b3e0cc15428d46751f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3281c0fbb3653ba7686ca80bb34ba1dd.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="One-for-More-Continual-Diffusion-Model-for-Anomaly-Detection"><a href="#One-for-More-Continual-Diffusion-Model-for-Anomaly-Detection" class="headerlink" title="One-for-More: Continual Diffusion Model for Anomaly Detection"></a>One-for-More: Continual Diffusion Model for Anomaly Detection</h2><p><strong>Authors:Xiaofan Li, Xin Tan, Zhuo Chen, Zhizhong Zhang, Ruixin Zhang, Rizen Guo, Guanna Jiang, Yulong Chen, Yanyun Qu, Lizhuang Ma, Yuan Xie</strong></p>
<p>With the rise of generative models, there is a growing interest in unifying all tasks within a generative framework. Anomaly detection methods also fall into this scope and utilize diffusion models to generate or reconstruct normal samples when given arbitrary anomaly images. However, our study found that the diffusion model suffers from severe <code>faithfulness hallucination&#39;&#39; and </code>catastrophic forgettingâ€™â€™, which canâ€™t meet the unpredictable pattern increments. To mitigate the above problems, we propose a continual diffusion model that uses gradient projection to achieve stable continual learning. Gradient projection deploys a regularization on the model updating by modifying the gradient towards the direction protecting the learned knowledge. But as a double-edged sword, it also requires huge memory costs brought by the Markov process. Hence, we propose an iterative singular value decomposition method based on the transitive property of linear representation, which consumes tiny memory and incurs almost no performance loss. Finally, considering the risk of &#96;&#96;over-fittingâ€™â€™ to normal images of the diffusion model, we propose an anomaly-masked network to enhance the condition mechanism of the diffusion model. For continual anomaly detection, ours achieves first place in 17&#x2F;18 settings on MVTec and VisA. Code is available at <a target="_blank" rel="noopener" href="https://github.com/FuNz-0/One-for-More">https://github.com/FuNz-0/One-for-More</a> </p>
<blockquote>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„å‘å±•ï¼Œè¶Šæ¥è¶Šæœ‰å…´è¶£åœ¨ç”Ÿæˆæ¡†æ¶å†…ç»Ÿä¸€æ‰€æœ‰ä»»åŠ¡ã€‚å¼‚å¸¸æ£€æµ‹æ–¹æ³•æ˜¯å…¶ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œåœ¨ç»™å®šçš„ä»»æ„å¼‚å¸¸å›¾åƒä¸­åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæˆ–é‡å»ºæ­£å¸¸æ ·æœ¬ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶å‘ç°æ‰©æ•£æ¨¡å‹å­˜åœ¨ä¸¥é‡çš„â€œå¿ å®å¹»è§‰â€å’Œâ€œç¾éš¾æ€§é—å¿˜â€ï¼Œæ— æ³•æ»¡è¶³ä¸å¯é¢„æµ‹çš„æ¨¡å¼å¢é‡ã€‚ä¸ºäº†ç¼“è§£ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æŒç»­æ‰©æ•£æ¨¡å‹ï¼Œé‡‡ç”¨æ¢¯åº¦æŠ•å½±å®ç°ç¨³å®šçš„æŒç»­å­¦ä¹ ã€‚æ¢¯åº¦æŠ•å½±é€šè¿‡åœ¨æ¨¡å‹æ›´æ–°è¿‡ç¨‹ä¸­éƒ¨ç½²æ­£åˆ™åŒ–ï¼Œä¿®æ”¹æ¢¯åº¦ä»¥ä¿æŠ¤æ‰€å­¦çŸ¥è¯†æ–¹å‘ã€‚ç„¶è€Œï¼Œå®ƒå°±åƒä¸€æŠŠåŒåˆƒå‰‘ï¼Œè¿˜éœ€è¦é©¬å°”å¯å¤«è¿‡ç¨‹å¸¦æ¥çš„å·¨å¤§å†…å­˜æˆæœ¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºçº¿æ€§è¡¨ç¤ºä¼ é€’å±æ€§çš„è¿­ä»£å¥‡å¼‚å€¼åˆ†è§£æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•æ¶ˆè€—å†…å­˜å°ä¸”å‡ ä¹ä¸ä¼šé€ æˆæ€§èƒ½æŸå¤±ã€‚æœ€åï¼Œè€ƒè™‘åˆ°æ‰©æ•£æ¨¡å‹å¯¹æ­£å¸¸å›¾åƒçš„â€œè¿‡åº¦æ‹Ÿåˆâ€é£é™©ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼‚å¸¸æ©ç ç½‘ç»œæ¥å¢å¼ºæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶æœºåˆ¶ã€‚å¯¹äºæŒç»­çš„å¼‚å¸¸æ£€æµ‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨MVTecå’ŒVisAçš„17&#x2F;18è®¾ç½®ä¸Šæ’åç¬¬ä¸€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/FuNz-0/One-for-More%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/FuNz-0/One-for-Moreæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19848v2">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹ã€‚é’ˆå¯¹æ‰©æ•£æ¨¡å‹å­˜åœ¨çš„â€œå¿ å®åº¦å¹»è§‰â€å’Œâ€œç¾éš¾æ€§é—å¿˜â€é—®é¢˜ï¼Œæå‡ºäº†æŒç»­æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡æ¢¯åº¦æŠ•å½±å®ç°ç¨³å®šæŒç»­å­¦ä¹ ï¼Œå¹¶é‡‡ç”¨åŸºäºçº¿æ€§è¡¨ç¤ºä¼ é€’å±æ€§çš„è¿­ä»£å¥‡å¼‚å€¼åˆ†è§£æ–¹æ³•é™ä½å†…å­˜æ¶ˆè€—ã€‚ä¸ºå¢å¼ºæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶æœºåˆ¶ï¼Œè¿˜æå‡ºäº†å¼‚å¸¸æ©è†œç½‘ç»œã€‚åœ¨MVTecå’ŒVisAçš„17&#x2F;18è®¾ç½®ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨æŒç»­å¼‚å¸¸æ£€æµ‹ä¸­å–å¾—ç¬¬ä¸€åã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹ä¸­å—åˆ°â€œå¿ å®åº¦å¹»è§‰â€å’Œâ€œç¾éš¾æ€§é—å¿˜â€çš„æŒ‘æˆ˜ã€‚</li>
<li>æŒç»­æ‰©æ•£æ¨¡å‹é€šè¿‡æ¢¯åº¦æŠ•å½±å®ç°ç¨³å®šæŒç»­å­¦ä¹ ï¼Œç¼“è§£ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>æ¢¯åº¦æŠ•å½±é€šè¿‡ä¿®æ”¹æ¨¡å‹æ›´æ–°æ—¶çš„æ¢¯åº¦æ¥ä¿æŠ¤å·²å­¦çŸ¥è¯†ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å­˜åœ¨å¤§é‡å†…å­˜æ¶ˆè€—ï¼Œä¸ºæ­¤æå‡ºäº†åŸºäºçº¿æ€§è¡¨ç¤ºä¼ é€’å±æ€§çš„è¿­ä»£å¥‡å¼‚å€¼åˆ†è§£æ–¹æ³•é™ä½å†…å­˜æ¶ˆè€—ã€‚</li>
<li>å¼‚å¸¸æ©è†œç½‘ç»œç”¨äºå¢å¼ºæ‰©æ•£æ¨¡å‹çš„æ¡ä»¶æœºåˆ¶ã€‚</li>
<li>æ–¹æ³•åœ¨MVTecå’ŒVisAçš„å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-06ecc30bebc29d61e5ee0bd183c2da36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea5c787bdb07edbeb736e50c54f24a0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb5dd7436d5dd7100a57441806cfd7ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2715aa9155cbca8c63d968679b518b6b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-24154a95c36a91700ccf264be90a0795.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a033812a787c90980add9c0033f911c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Appearance-Matching-Adapter-for-Exemplar-based-Semantic-Image-Synthesis-in-the-Wild"><a href="#Appearance-Matching-Adapter-for-Exemplar-based-Semantic-Image-Synthesis-in-the-Wild" class="headerlink" title="Appearance Matching Adapter for Exemplar-based Semantic Image Synthesis   in-the-Wild"></a>Appearance Matching Adapter for Exemplar-based Semantic Image Synthesis   in-the-Wild</h2><p><strong>Authors:Siyoon Jin, Jisu Nam, Jiyoung Kim, Dahyun Chung, Yeong-Seok Kim, Joonhyung Park, Heonjeong Chu, Seungryong Kim</strong></p>
<p>Exemplar-based semantic image synthesis generates images aligned with semantic content while preserving the appearance of an exemplar. Conventional structure-guidance models like ControlNet, are limited as they rely solely on text prompts to control appearance and cannot utilize exemplar images as input. Recent tuning-free approaches address this by transferring local appearance via implicit cross-image matching in the augmented self-attention mechanism of pre-trained diffusion models. However, prior works are often restricted to single-object cases or foreground object appearance transfer, struggling with complex scenes involving multiple objects. To overcome this, we propose AM-Adapter (Appearance Matching Adapter) to address exemplar-based semantic image synthesis in-the-wild, enabling multi-object appearance transfer from a single scene-level image. AM-Adapter automatically transfers local appearances from the scene-level input. AM-Adapter alternatively provides controllability to map user-defined object details to specific locations in the synthesized images. Our learnable framework enhances cross-image matching within augmented self-attention by integrating semantic information from segmentation maps. To disentangle generation and matching, we adopt stage-wise training. We first train the structure-guidance and generation networks, followed by training the matching adapter while keeping the others frozen. During inference, we introduce an automated exemplar retrieval method for selecting exemplar image-segmentation pairs efficiently. Despite utilizing minimal learnable parameters, AM-Adapter achieves state-of-the-art performance, excelling in both semantic alignment and local appearance fidelity. Extensive ablations validate our design choices. Code and weights will be released.: <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/AM-Adapter/">https://cvlab-kaist.github.io/AM-Adapter/</a> </p>
<blockquote>
<p>åŸºäºèŒƒä¾‹çš„è¯­ä¹‰å›¾åƒåˆæˆèƒ½å¤Ÿç”Ÿæˆä¸è¯­ä¹‰å†…å®¹å¯¹é½çš„å›¾åƒï¼ŒåŒæ—¶ä¿ç•™èŒƒä¾‹çš„å¤–è§‚ã€‚ä¼ ç»Ÿçš„ç»“æ„å¼•å¯¼æ¨¡å‹ï¼Œå¦‚ControlNetï¼Œå—é™äºä»…ä¾èµ–æ–‡æœ¬æç¤ºæ¥æ§åˆ¶å¤–è§‚ï¼Œæ— æ³•åˆ©ç”¨èŒƒä¾‹å›¾åƒä½œä¸ºè¾“å…¥ã€‚æœ€è¿‘çš„æ— éœ€å¾®è°ƒçš„æ–¹æ³•é€šè¿‡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¢å¼ºè‡ªæ³¨æ„æœºåˆ¶ä¸­çš„éšå¼è·¨å›¾åƒåŒ¹é…æ¥ä¼ è¾“å±€éƒ¨å¤–è§‚ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œé€šå¸¸ä»…é™äºå•å¯¹è±¡æƒ…å†µæˆ–å‰æ™¯å¯¹è±¡å¤–è§‚ä¼ è¾“ï¼Œå¯¹äºæ¶‰åŠå¤šä¸ªå¯¹è±¡çš„å¤æ‚åœºæ™¯æ„Ÿåˆ°å›°æ‰°ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºAM-Adapterï¼ˆå¤–è§‚åŒ¹é…é€‚é…å™¨ï¼‰ï¼Œè§£å†³åŸºäºèŒƒä¾‹çš„è¯­ä¹‰å›¾åƒåˆæˆåœ¨é‡å¤–çš„é—®é¢˜ï¼Œæ”¯æŒä»å•ä¸ªåœºæ™¯çº§å›¾åƒè¿›è¡Œå¤šå¯¹è±¡å¤–è§‚ä¼ è¾“ã€‚AM-Adapterè‡ªåŠ¨ä»åœºæ™¯çº§è¾“å…¥è½¬ç§»å±€éƒ¨å¤–è§‚ã€‚AM-Adapterè¿˜æä¾›å¯æ§æ€§ï¼Œå°†ç”¨æˆ·å®šä¹‰çš„å¯¹è±¡ç»†èŠ‚æ˜ å°„åˆ°åˆæˆå›¾åƒä¸­çš„ç‰¹å®šä½ç½®ã€‚æˆ‘ä»¬çš„å¯å­¦ä¹ æ¡†æ¶é€šè¿‡æ•´åˆåˆ†å‰²å›¾çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¢å¼ºäº†å¢å¼ºè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è·¨å›¾åƒåŒ¹é…ã€‚ä¸ºäº†åˆ†ç¦»ç”Ÿæˆå’ŒåŒ¹é…ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒã€‚æˆ‘ä»¬é¦–å…ˆè®­ç»ƒç»“æ„å¼•å¯¼å’Œç”Ÿæˆç½‘ç»œï¼Œç„¶åå†»ç»“å…¶ä»–éƒ¨åˆ†ï¼Œåªè®­ç»ƒåŒ¹é…é€‚é…å™¨ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªåŠ¨èŒƒä¾‹æ£€ç´¢æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåœ°é€‰æ‹©èŒƒä¾‹å›¾åƒ-åˆ†å‰²å¯¹ã€‚å°½ç®¡ä½¿ç”¨çš„å¯å­¦ä¹ å‚æ•°æœ€å°‘ï¼Œä½†AM-Adapterä»è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è¯­ä¹‰å¯¹é½å’Œå±€éƒ¨å¤–è§‚ä¿çœŸåº¦æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚å¤§é‡çš„æ¶ˆèå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„è®¾è®¡é€‰æ‹©ã€‚[ä»£ç å’Œæƒé‡å°†å‘å¸ƒäº]ï¼š<a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/AM-Adapter/">https://cvlab-kaist.github.io/AM-Adapter/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03150v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¤ºä¾‹çš„è¯­ä¹‰å›¾åƒåˆæˆå¯ä»¥ç”Ÿæˆä¸è¯­ä¹‰å†…å®¹å¯¹é½çš„å›¾åƒï¼ŒåŒæ—¶ä¿ç•™ç¤ºä¾‹çš„å¤–è§‚ã€‚ä¼ ç»Ÿçš„ç»“æ„æŒ‡å¯¼æ¨¡å‹å¦‚ControlNetå­˜åœ¨å±€é™æ€§ï¼Œä»…ä¾èµ–äºæ–‡æœ¬æç¤ºæ¥æ§åˆ¶å¤–è§‚ï¼Œæ— æ³•åˆ©ç”¨ç¤ºä¾‹å›¾åƒä½œä¸ºè¾“å…¥ã€‚è¿‘æœŸæ— éœ€å¾®è°ƒçš„æ–¹æ³•é€šè¿‡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¢å¼ºè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è·¨å›¾åƒéšå¼åŒ¹é…æ¥è½¬ç§»å±€éƒ¨å¤–è§‚ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œé€šå¸¸å±€é™äºå•å¯¹è±¡çš„æƒ…å†µæˆ–å‰æ™¯å¯¹è±¡å¤–è§‚è½¬ç§»ï¼Œé¢ä¸´æ¶‰åŠå¤šä¸ªå¯¹è±¡çš„å¤æ‚åœºæ™¯æ—¶æ„Ÿåˆ°å›°éš¾ã€‚ä¸ºå…‹æœæ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºAM-Adapterï¼ˆå¤–è§‚åŒ¹é…é€‚é…å™¨ï¼‰ï¼Œè§£å†³é‡ç”Ÿç¯å¢ƒä¸­çš„åŸºäºç¤ºä¾‹çš„è¯­ä¹‰å›¾åƒåˆæˆé—®é¢˜ï¼Œå®ç°ä»å•ä¸ªåœºæ™¯çº§å›¾åƒçš„å¤šå¯¹è±¡å¤–è§‚è½¬ç§»ã€‚AM-Adapterè‡ªåŠ¨ä»åœºæ™¯çº§è¾“å…¥è½¬ç§»å±€éƒ¨å¤–è§‚ï¼Œå¹¶æä¾›å¯æ§æ€§ï¼Œå°†ç”¨æˆ·å®šä¹‰çš„å¯¹è±¡ç»†èŠ‚æ˜ å°„åˆ°åˆæˆå›¾åƒä¸­çš„ç‰¹å®šä½ç½®ã€‚æˆ‘ä»¬çš„å¯å­¦ä¹ æ¡†æ¶é€šè¿‡é›†æˆæ¥è‡ªåˆ†å‰²å›¾çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¢å¼ºå¢å¼ºè‡ªæ³¨æ„åŠ›ä¸­çš„è·¨å›¾åƒåŒ¹é…ã€‚ä¸ºäº†åˆ†ç¦»ç”Ÿæˆå’ŒåŒ¹é…ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒã€‚é¦–å…ˆè®­ç»ƒç»“æ„æŒ‡å¯¼å’Œç”Ÿæˆç½‘ç»œï¼Œç„¶åå†»ç»“å…¶ä»–éƒ¨åˆ†ï¼Œä»…è®­ç»ƒåŒ¹é…é€‚é…å™¨ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥è‡ªåŠ¨ç¤ºä¾‹æ£€ç´¢æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåœ°é€‰æ‹©ç¤ºä¾‹å›¾åƒ-åˆ†å‰²å¯¹ã€‚å°½ç®¡ä½¿ç”¨çš„å¯å­¦ä¹ å‚æ•°æœ€å°‘ï¼Œä½†AM-Adapterä»è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è¯­ä¹‰å¯¹é½å’Œå±€éƒ¨å¤–è§‚ä¿çœŸåº¦æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºç¤ºä¾‹çš„è¯­ä¹‰å›¾åƒåˆæˆç»“åˆäº†è¯­ä¹‰å†…å®¹ä¸ç¤ºä¾‹çš„å¤–è§‚ã€‚</li>
<li>ä¼ ç»Ÿç»“æ„æŒ‡å¯¼æ¨¡å‹å—é™äºä»…ä½¿ç”¨æ–‡æœ¬æç¤ºï¼Œæ— æ³•åˆ©ç”¨ç¤ºä¾‹å›¾åƒã€‚</li>
<li>è¿‘æœŸæ–¹æ³•é€šè¿‡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¢å¼ºè‡ªæ³¨æ„åŠ›æ¥è½¬ç§»å±€éƒ¨å¤–è§‚ã€‚</li>
<li>AM-Adapterè§£å†³äº†æ¶‰åŠå¤šä¸ªå¯¹è±¡çš„å¤æ‚åœºæ™¯ä¸­çš„è¯­ä¹‰å›¾åƒåˆæˆé—®é¢˜ã€‚</li>
<li>AM-Adapterå®ç°äº†ä»å•ä¸ªåœºæ™¯çº§å›¾åƒçš„å¤šå¯¹è±¡å¤–è§‚è½¬ç§»ã€‚</li>
<li>æ¡†æ¶é€šè¿‡é›†æˆåˆ†å‰²å›¾çš„è¯­ä¹‰ä¿¡æ¯å¢å¼ºäº†è·¨å›¾åƒåŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fe5868eab06f2569eb348c75c33c0aa8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7777c9010b59e354269fbb35ab67ace9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42ec79f73c6e5928e0e7200f386170df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8023ae680108831a9cd97b2341827ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c9103d8f6c5d4ea6094ee778dd91086.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Posterior-Sampling-Ability-of-Plug-Play-Diffusion-Methods-in-Sparse-View-CT"><a href="#Evaluating-the-Posterior-Sampling-Ability-of-Plug-Play-Diffusion-Methods-in-Sparse-View-CT" class="headerlink" title="Evaluating the Posterior Sampling Ability of Plug&amp;Play Diffusion Methods   in Sparse-View CT"></a>Evaluating the Posterior Sampling Ability of Plug&amp;Play Diffusion Methods   in Sparse-View CT</h2><p><strong>Authors:Liam Moroy, Guillaume Bourmaud, FrÃ©dÃ©ric Champagnat, Jean-FranÃ§ois Giovannelli</strong></p>
<p>Plug&amp;Play (PnP) diffusion models are state-of-the-art methods in computed tomography (CT) reconstruction. Such methods usually consider applications where the sinogram contains a sufficient amount of information for the posterior distribution to be concentrated around a single mode, and consequently are evaluated using image-to-image metrics such as PSNR&#x2F;SSIM. Instead, we are interested in reconstructing compressible flow images from sinograms having a small number of projections, which results in a posterior distribution no longer concentrated or even multimodal. Thus, in this paper, we aim at evaluating the approximate posterior of PnP diffusion models and introduce two posterior evaluation properties. We quantitatively evaluate three PnP diffusion methods on three different datasets for several numbers of projections. We surprisingly find that, for each method, the approximate posterior deviates from the true posterior when the number of projections decreases. </p>
<blockquote>
<p>Plug&amp;Playï¼ˆPnPï¼‰æ‰©æ•£æ¨¡å‹æ˜¯è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰é‡å»ºé¢†åŸŸçš„æœ€å‰æ²¿æ–¹æ³•ã€‚è¿™ç±»æ–¹æ³•é€šå¸¸åº”ç”¨äºè¾›æ ¼å°”å›¾ï¼ˆsinogramï¼‰åŒ…å«è¶³å¤Ÿä¿¡æ¯ï¼Œä½¿å¾—åéªŒåˆ†å¸ƒé›†ä¸­åœ¨å•ä¸€æ¨¡å¼å‘¨å›´çš„æƒ…å†µã€‚å› æ­¤ï¼Œå®ƒä»¬é€šå¸¸ä½¿ç”¨å›¾åƒåˆ°å›¾åƒçš„æŒ‡æ ‡ï¼ˆå¦‚PSNR&#x2F;SSIMï¼‰è¿›è¡Œè¯„ä¼°ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯¹ä»è¾›æ ¼å°”å›¾é‡å»ºå¯å‹ç¼©æµå›¾åƒæ„Ÿå…´è¶£ï¼Œè¯¥è¾›æ ¼å°”å›¾å…·æœ‰è¾ƒå°‘çš„æŠ•å½±æ•°ï¼Œå¯¼è‡´åéªŒåˆ†å¸ƒä¸å†é›†ä¸­ï¼Œç”šè‡³æ˜¯å¤šæ¨¡æ€çš„ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨è¯„ä¼°PnPæ‰©æ•£æ¨¡å‹çš„è¿‘ä¼¼åéªŒæ¦‚ç‡ï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªåéªŒè¯„ä¼°å±æ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šå®šé‡è¯„ä¼°äº†ä¸‰ç§PnPæ‰©æ•£æ–¹æ³•çš„ä¸åŒæŠ•å½±æ•°é‡ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ï¼Œå¯¹äºæ¯ç§æ–¹æ³•ï¼Œå½“æŠ•å½±æ•°é‡å‡å°‘æ—¶ï¼Œè¿‘ä¼¼åéªŒåˆ†å¸ƒéƒ½ä¼šåç¦»çœŸå®åéªŒåˆ†å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21301v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Plug&amp;Playï¼ˆPnPï¼‰æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—å±‚ææˆåƒï¼ˆCTï¼‰é‡å»ºä¸­çš„æœ€æ–°åº”ç”¨ã€‚æ–‡ç« é‡ç‚¹å…³æ³¨äºä»æŠ•å½±æ•°é‡è¾ƒå°‘çš„è¾›å…‹å›¾ï¼ˆsinogramï¼‰é‡å»ºå¯å‹ç¼©æµåŠ¨å›¾åƒçš„é—®é¢˜ã€‚ç”±äºæŠ•å½±æ•°é‡å‡å°‘ï¼ŒåéªŒåˆ†å¸ƒä¸å†é›†ä¸­ç”šè‡³å‘ˆç°å¤šæ¨¡æ€ã€‚æ–‡ç« æ—¨åœ¨è¯„ä¼°PnPæ‰©æ•£æ¨¡å‹çš„è¿‘ä¼¼åéªŒï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªåéªŒè¯„ä¼°å±æ€§ã€‚é€šè¿‡å¯¹ä¸‰ç§PnPæ‰©æ•£æ–¹æ³•åœ¨ä¸‰ä¸ªä¸åŒæ•°æ®é›†ä¸Šè¿›è¡Œå®šé‡è¯„ä¼°ï¼Œå‘ç°éšç€æŠ•å½±æ•°é‡çš„å‡å°‘ï¼Œæ¯ç§æ–¹æ³•çš„è¿‘ä¼¼åéªŒéƒ½ä¼šåç¦»çœŸå®åéªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PnPæ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—å±‚ææˆåƒï¼ˆCTï¼‰é‡å»ºä¸­å¤„äºå‰æ²¿åœ°ä½ã€‚</li>
<li>æ–‡ç« å…³æ³¨ä»æŠ•å½±æ•°é‡è¾ƒå°‘çš„è¾›å…‹å›¾é‡å»ºå¯å‹ç¼©æµåŠ¨å›¾åƒçš„é—®é¢˜ã€‚</li>
<li>ç”±äºæŠ•å½±æ•°é‡å‡å°‘ï¼ŒåéªŒåˆ†å¸ƒç‰¹æ€§å‘ç”Ÿå˜åŒ–ï¼Œä¸å†é›†ä¸­ç”šè‡³å‘ˆç°å¤šæ¨¡æ€ã€‚</li>
<li>å¼•å…¥ä¸¤ä¸ªåéªŒè¯„ä¼°å±æ€§ä»¥è¯„ä¼°PnPæ‰©æ•£æ¨¡å‹çš„è¿‘ä¼¼åéªŒã€‚</li>
<li>å¯¹ä¸‰ç§PnPæ‰©æ•£æ–¹æ³•åœ¨ä¸‰ä¸ªä¸åŒæ•°æ®é›†ä¸Šè¿›è¡Œå®šé‡è¯„ä¼°ã€‚</li>
<li>å‘ç°éšç€æŠ•å½±æ•°é‡çš„å‡å°‘ï¼ŒPnPæ‰©æ•£æ¨¡å‹çš„è¿‘ä¼¼åéªŒä¼šåç¦»çœŸå®åéªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fc9cee43fc4a04579430e9026bf5c7f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6babed71ed2bfee288ad9237327b50a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ea902f14e5b979f6ab39ced661e5941.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00cc0a19dbf08f8be713d992c56c0ec9.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="PQPP-A-Joint-Benchmark-for-Text-to-Image-Prompt-and-Query-Performance-Prediction"><a href="#PQPP-A-Joint-Benchmark-for-Text-to-Image-Prompt-and-Query-Performance-Prediction" class="headerlink" title="PQPP: A Joint Benchmark for Text-to-Image Prompt and Query Performance   Prediction"></a>PQPP: A Joint Benchmark for Text-to-Image Prompt and Query Performance   Prediction</h2><p><strong>Authors:Eduard Poesina, Adriana Valentina Costache, Adrian-Gabriel Chifu, Josiane Mothe, Radu Tudor Ionescu</strong></p>
<p>Text-to-image generation has recently emerged as a viable alternative to text-to-image retrieval, driven by the visually impressive results of generative diffusion models. Although query performance prediction is an active research topic in information retrieval, to the best of our knowledge, there is no prior study that analyzes the difficulty of queries (referred to as prompts) in text-to-image generation, based on human judgments. To this end, we introduce the first dataset of prompts which are manually annotated in terms of image generation performance. Additionally, we extend these evaluations to text-to-image retrieval by collecting manual annotations that represent retrieval performance. We thus establish the first joint benchmark for prompt and query performance prediction (PQPP) across both tasks, comprising over 10K queries. Our benchmark enables (i) the comparative assessment of prompt&#x2F;query difficulty in both image generation and image retrieval, and (ii) the evaluation of prompt&#x2F;query performance predictors addressing both generation and retrieval. We evaluate several pre- and post-generation&#x2F;retrieval performance predictors, thus providing competitive baselines for future research. Our benchmark and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Eduard6421/PQPP">https://github.com/Eduard6421/PQPP</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä½œä¸ºä¸€ç§å¯è¡Œçš„æ–¹æ³•ï¼Œå·²ç»é€æ¸å´­éœ²å¤´è§’ï¼Œæˆä¸ºæ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢çš„ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œå…¶èƒŒåé©±åŠ¨çš„æ˜¯ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„è§†è§‰ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚å°½ç®¡æŸ¥è¯¢æ€§èƒ½é¢„æµ‹åœ¨ä¿¡æ¯æ£€ç´¢é¢†åŸŸæ˜¯ä¸€ä¸ªçƒ­é—¨çš„ç ”ç©¶è¯¾é¢˜ï¼Œä½†æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç›®å‰å°šæ²¡æœ‰åŸºäºäººç±»åˆ¤æ–­æ¥åˆ†ææ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„æŸ¥è¯¢ï¼ˆç§°ä¸ºæç¤ºï¼‰éš¾åº¦çš„ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¦–ä¸ªå…³äºæç¤ºçš„æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™äº›æç¤ºæ˜¯åŸºäºå›¾åƒç”Ÿæˆæ€§èƒ½çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹è¿™äº›è¯„ä¼°è¿›è¡Œäº†æ‰©å±•ï¼Œå¯¹æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢è¿›è¡Œäº†æ‰‹åŠ¨æ ‡æ³¨ï¼Œä»¥ä»£è¡¨æ£€ç´¢æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å»ºç«‹äº†ç¬¬ä¸€ä¸ªé’ˆå¯¹å›¾åƒç”Ÿæˆå’Œå›¾åƒæ£€ç´¢ä»»åŠ¡çš„æç¤ºå’ŒæŸ¥è¯¢æ€§èƒ½é¢„æµ‹ï¼ˆPQPPï¼‰çš„è”åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¶…è¿‡10ä¸‡ä¸ªæŸ¥è¯¢ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å¯ä»¥è¿›è¡Œï¼šï¼ˆiï¼‰å¯¹å›¾åƒç”Ÿæˆå’Œå›¾åƒæ£€ç´¢ä¸­çš„æç¤º&#x2F;æŸ¥è¯¢éš¾åº¦çš„æ¯”è¾ƒè¯„ä¼°ï¼›ï¼ˆiiï¼‰å¯¹åŒæ—¶å¤„ç†ç”Ÿæˆå’Œæ£€ç´¢çš„æç¤º&#x2F;æŸ¥è¯¢æ€§èƒ½é¢„æµ‹çš„è¯„ä»·ã€‚æˆ‘ä»¬å¯¹å¤šä¸ªé¢„ç”Ÿæˆå’Œç”Ÿæˆåçš„æ€§èƒ½é¢„æµ‹å™¨è¿›è¡Œäº†è¯„ä¼°ï¼Œä»è€Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ç«äº‰æ€§çš„åŸºå‡†çº¿ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Eduard6421/PQPP%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Eduard6421/PQPPå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.04746v2">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†åŸºäºäººç±»åˆ¤æ–­çš„é¦–ä¸ªé’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢ä»»åŠ¡çš„æŸ¥è¯¢æ€§èƒ½é¢„æµ‹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡10Kä¸ªæŸ¥è¯¢ï¼Œæ—¨åœ¨æ¯”è¾ƒè¯„ä¼°å›¾åƒç”Ÿæˆå’Œå›¾åƒæ£€ç´¢ä¸­çš„æŸ¥è¯¢éš¾åº¦ï¼Œå¹¶è¯„ä¼°é’ˆå¯¹è¿™ä¸¤ä¸ªä»»åŠ¡çš„æŸ¥è¯¢æ€§èƒ½é¢„æµ‹å™¨ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æä¾›äº†ä¸€äº›é¢„è®­ç»ƒå’Œåè®­ç»ƒçš„æŸ¥è¯¢æ€§èƒ½é¢„æµ‹å™¨çš„ç«äº‰åŸºçº¿ã€‚è¯¥åŸºå‡†æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬ä»‹ç»äº†åŸºäºäººç±»åˆ¤æ–­çš„é¦–ä¸ªé’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œæ£€ç´¢çš„æŸ¥è¯¢æ€§èƒ½é¢„æµ‹æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«è¶…è¿‡10Kä¸ªæŸ¥è¯¢ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ¯”è¾ƒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œæ£€ç´¢ä¸­çš„æŸ¥è¯¢éš¾åº¦ã€‚</li>
<li>æ•°æ®é›†é¦–æ¬¡å»ºç«‹äº†ä¸€ä¸ªè”åˆåŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œæ£€ç´¢ä¸­çš„æŸ¥è¯¢æ€§èƒ½é¢„æµ‹å™¨ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é¢„è®­ç»ƒå’Œåè®­ç»ƒçš„æŸ¥è¯¢æ€§èƒ½é¢„æµ‹å™¨çš„ç«äº‰åŸºçº¿ã€‚</li>
<li>æ•°æ®é›†å¯å…¬å¼€è®¿é—®å¹¶æä¾›ä»£ç ç¤ºä¾‹ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯æŸ¥è¯¢æ€§èƒ½çš„é¢„æµ‹å’Œåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.04746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8334367c58402fb1a95038a37b745dc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7038f20babee4b7c38cfccd0cc6fa2b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcbef9abf1e716f6fbd6b6f6d9c8146a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0a55271a8689b65d4a8c4ec0da92bdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30aba3250213727d37b78b016305a600.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Unlearning-Concepts-in-Diffusion-Model-via-Concept-Domain-Correction-and-Concept-Preserving-Gradient"><a href="#Unlearning-Concepts-in-Diffusion-Model-via-Concept-Domain-Correction-and-Concept-Preserving-Gradient" class="headerlink" title="Unlearning Concepts in Diffusion Model via Concept Domain Correction and   Concept Preserving Gradient"></a>Unlearning Concepts in Diffusion Model via Concept Domain Correction and   Concept Preserving Gradient</h2><p><strong>Authors:Yongliang Wu, Shiji Zhou, Mingzhuo Yang, Lianzhe Wang, Heng Chang, Wenbo Zhu, Xinting Hu, Xiao Zhou, Xu Yang</strong></p>
<p>Text-to-image diffusion models have achieved remarkable success in generating photorealistic images. However, the inclusion of sensitive information during pre-training poses significant risks. Machine Unlearning (MU) offers a promising solution to eliminate sensitive concepts from these models. Despite its potential, existing MU methods face two main challenges: 1) limited generalization, where concept erasure is effective only within the unlearned set, failing to prevent sensitive concept generation from out-of-set prompts; and 2) utility degradation, where removing target concepts significantly impacts the modelâ€™s overall performance. To address these issues, we propose a novel concept domain correction framework named \textbf{DoCo} (\textbf{Do}main \textbf{Co}rrection). By aligning the output domains of sensitive and anchor concepts through adversarial training, our approach ensures comprehensive unlearning of target concepts. Additionally, we introduce a concept-preserving gradient surgery technique that mitigates conflicting gradient components, thereby preserving the modelâ€™s utility while unlearning specific concepts. Extensive experiments across various instances, styles, and offensive concepts demonstrate the effectiveness of our method in unlearning targeted concepts with minimal impact on related concepts, outperforming previous approaches even for out-of-distribution prompts. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸå›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ•æ„Ÿä¿¡æ¯çš„åŒ…å«å¸¦æ¥äº†å¾ˆå¤§çš„é£é™©ã€‚æœºå™¨é—å¿˜ï¼ˆMUï¼‰ä¸ºä»è¿™äº›æ¨¡å‹ä¸­æ¶ˆé™¤æ•æ„Ÿæ¦‚å¿µæä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡æ½œåŠ›å·¨å¤§ï¼Œä½†ç°æœ‰çš„MUæ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š1ï¼‰æœ‰é™çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ¦‚å¿µæ¶ˆé™¤åªåœ¨æœªå­¦ä¹ çš„é›†åˆä¸­æœ‰æ•ˆï¼Œæœªèƒ½é˜²æ­¢æ¥è‡ªé›†åˆå¤–æç¤ºçš„æ•æ„Ÿæ¦‚å¿µç”Ÿæˆï¼›2ï¼‰æ•ˆç”¨é™ä½ï¼Œç§»é™¤ç›®æ ‡æ¦‚å¿µæ˜¾è‘—å½±å“æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºDoCoï¼ˆé¢†åŸŸä¿®æ­£ï¼‰çš„æ–°å‹æ¦‚å¿µåŸŸä¿®æ­£æ¡†æ¶ã€‚é€šè¿‡å¯¹æŠ—è®­ç»ƒå¯¹é½æ•æ„Ÿæ¦‚å¿µå’Œé”šæ¦‚å¿µçš„è¾“å‡ºåŸŸï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†ç›®æ ‡æ¦‚å¿µçš„å…¨é¢é—å¿˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ¦‚å¿µä¿ç•™æ¢¯åº¦æ‰‹æœ¯æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å‡è½»äº†å†²çªçš„æ¢¯åº¦åˆ†é‡ï¼Œä»è€Œåœ¨é—å¿˜ç‰¹å®šæ¦‚å¿µçš„åŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚åœ¨å„ç§å®ä¾‹ã€é£æ ¼å’Œå†’çŠ¯æ¦‚å¿µæ–¹é¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é—å¿˜ç›®æ ‡æ¦‚å¿µæ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œå¯¹ç›¸å…³æ¦‚å¿µçš„å½±å“æœ€å°ï¼Œå³ä½¿åœ¨ç¦»ç¾¤æç¤ºçš„æƒ…å†µä¸‹ä¹Ÿä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15304v3">PDF</a> AAAI 2025 camera-ready version</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸå›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥æ•æ„Ÿä¿¡æ¯å­˜åœ¨é‡å¤§é£é™©ã€‚æœºå™¨é—å¿˜ï¼ˆMUï¼‰ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„MUæ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯é€šç”¨æ€§æœ‰é™ï¼Œä»…åœ¨æœªå­¦ä¹ çš„é›†åˆå†…æœ‰æ•ˆï¼Œæ— æ³•é˜²æ­¢æ•æ„Ÿæ¦‚å¿µä»å¤–éƒ¨æç¤ºç”Ÿæˆï¼›äºŒæ˜¯æ•ˆç”¨é™ä½ï¼Œç§»é™¤ç›®æ ‡æ¦‚å¿µä¼šå½±å“æ¨¡å‹çš„æ€»ä½“æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºDoCoï¼ˆé¢†åŸŸä¿®æ­£ï¼‰çš„æ–°æ¦‚å¿µä¿®æ­£æ¡†æ¶ã€‚é€šè¿‡å¯¹æ•æ„Ÿæ¦‚å¿µå’Œé”šæ¦‚å¿µè¾“å‡ºé¢†åŸŸè¿›è¡Œå¯¹é½ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†ç›®æ ‡æ¦‚å¿µçš„å…¨é¢é—å¿˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¦‚å¿µä¿ç•™æ¢¯åº¦æ‰‹æœ¯æŠ€æœ¯ï¼Œç¼“è§£å†²çªæ¢¯åº¦æˆåˆ†ï¼Œåœ¨é—å¿˜ç‰¹å®šæ¦‚å¿µçš„åŒæ—¶ä¿ç•™æ¨¡å‹çš„å®ç”¨æ€§ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é—å¿˜ç›®æ ‡æ¦‚å¿µæ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå¯¹å…³è”æ¦‚å¿µçš„å½±å“æœ€å°ï¼Œå³ä½¿åœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æç¤ºä¸­ä¹Ÿè¶…è¿‡äº†ä»¥å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé€¼çœŸå›¾åƒä¸Šè¡¨ç°å‡ºå“è¶Šçš„æˆåŠŸï¼Œä½†é¢„è®­ç»ƒä¸­çš„æ•æ„Ÿä¿¡æ¯å¼•å…¥å¸¦æ¥é£é™©ã€‚</li>
<li>ç°æœ‰çš„æœºå™¨é—å¿˜ï¼ˆMUï¼‰æ–¹æ³•å­˜åœ¨ä¸¤å¤§æŒ‘æˆ˜ï¼šæœ‰é™é€šç”¨æ€§å’Œæ•ˆç”¨é™ä½ã€‚</li>
<li>æå‡ºåä¸ºDoCoçš„æ–°æ¦‚å¿µä¿®æ­£æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ•æ„Ÿæ¦‚å¿µå’Œé”šæ¦‚å¿µè¾“å‡ºé¢†åŸŸè¿›è¡Œå¯¹é½ï¼Œç¡®ä¿ç›®æ ‡æ¦‚å¿µçš„å…¨é¢é—å¿˜ã€‚</li>
<li>å¼•å…¥æ¦‚å¿µä¿ç•™æ¢¯åº¦æ‰‹æœ¯æŠ€æœ¯ï¼Œä»¥ç¼“è§£å†²çªæ¢¯åº¦æˆåˆ†ï¼Œä»è€Œåœ¨é—å¿˜ç‰¹å®šæ¦‚å¿µçš„åŒæ—¶ä¿ç•™æ¨¡å‹çš„å®ç”¨æ€§ã€‚</li>
<li>DoCoæ¡†æ¶é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œè¡¨ç°ä¼˜è¶Šäºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>DoCoæ¡†æ¶èƒ½å¤„ç†å„ç§å®ä¾‹ã€é£æ ¼å’Œå†’çŠ¯æ€§æ¦‚å¿µï¼Œå¯¹ç›®æ ‡æ¦‚å¿µçš„é—å¿˜æ•ˆæœæ˜¾è‘—ï¼Œå¯¹å…³è”æ¦‚å¿µå½±å“æœ€å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15304">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24b40787292bc37464a1365df11aa285.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffb61b4c200ae3e1bd70274f0a2e487d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-684dc5863bca88f8c816563852e56b7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7f4a9145fd71d95e411395b5ff4d7f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97f387e0027a2abda4090077805da2fe.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Bracket-Diffusion-HDR-Image-Generation-by-Consistent-LDR-Denoising"><a href="#Bracket-Diffusion-HDR-Image-Generation-by-Consistent-LDR-Denoising" class="headerlink" title="Bracket Diffusion: HDR Image Generation by Consistent LDR Denoising"></a>Bracket Diffusion: HDR Image Generation by Consistent LDR Denoising</h2><p><strong>Authors:Mojtaba Bemana, Thomas LeimkÃ¼hler, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel</strong></p>
<p>We demonstrate generating HDR images using the concerted action of multiple black-box, pre-trained LDR image diffusion models. Relying on a pre-trained LDR generative diffusion models is vital as, first, there is no sufficiently large HDR image dataset available to re-train them, and, second, even if it was, re-training such models is impossible for most compute budgets. Instead, we seek inspiration from the HDR image capture literature that traditionally fuses sets of LDR images, called â€œexposure bracketsâ€™â€™, to produce a single HDR image. We operate multiple denoising processes to generate multiple LDR brackets that together form a valid HDR result. The key to making this work is to introduce a consistency term into the diffusion process to couple the brackets such that they agree across the exposure range they share while accounting for possible differences due to the quantization error. We demonstrate state-of-the-art unconditional and conditional or restoration-type (LDR2HDR) generative modeling results, yet in HDR. </p>
<blockquote>
<p>æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨å¤šä¸ªé¢„è®­ç»ƒçš„LDRå›¾åƒæ‰©æ•£æ¨¡å‹çš„ååŒä½œç”¨ç”ŸæˆHDRå›¾åƒã€‚ä¾èµ–é¢„è®­ç»ƒçš„LDRç”Ÿæˆæ‰©æ•£æ¨¡å‹è‡³å…³é‡è¦ï¼Œé¦–å…ˆï¼Œç›®å‰æ²¡æœ‰è¶³å¤Ÿå¤§çš„HDRå›¾åƒæ•°æ®é›†å¯ä¾›é‡æ–°è®­ç»ƒï¼Œå…¶æ¬¡ï¼Œå³ä½¿æœ‰ï¼Œå¤§å¤šæ•°è®¡ç®—é¢„ç®—ä¹Ÿæ— æ³•é‡æ–°è®­ç»ƒè¿™æ ·çš„æ¨¡å‹ã€‚ç›¸åï¼Œæˆ‘ä»¬ä»HDRå›¾åƒæ•è·æ–‡çŒ®ä¸­å¯»æ‰¾çµæ„Ÿï¼Œè¯¥æ–‡çŒ®ä¼ ç»Ÿä¸Šå°†ä¸€ç»„LDRå›¾åƒï¼ˆç§°ä¸ºâ€œæ›å…‰æ‹¬å·â€ï¼‰èåˆåœ¨ä¸€èµ·ï¼Œä»¥äº§ç”Ÿå•ä¸ªHDRå›¾åƒã€‚æˆ‘ä»¬æ‰§è¡Œå¤šæ¬¡å»å™ªè¿‡ç¨‹ä»¥ç”Ÿæˆå¤šä¸ªLDRæ‹¬å·ï¼Œè¿™äº›æ‹¬å·å…±åŒå½¢æˆä¸€ä¸ªæœ‰æ•ˆçš„HDRç»“æœã€‚ä½¿è¿™é¡¹å·¥ä½œæˆåŠŸçš„å…³é”®æ˜¯å‘æ‰©æ•£è¿‡ç¨‹å¼•å…¥ä¸€è‡´æ€§é¡¹ï¼Œä»¥è€¦åˆæ‹¬å·ï¼Œä½¿å®ƒä»¬åœ¨å…±äº«çš„æ›å…‰èŒƒå›´å†…ä¿æŒä¸€è‡´ï¼ŒåŒæ—¶è€ƒè™‘åˆ°ç”±äºé‡åŒ–è¯¯å·®å¯èƒ½å¯¼è‡´çš„ä¸åŒã€‚æˆ‘ä»¬å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ— æ¡ä»¶å’Œæœ‰æ¡ä»¶çš„æˆ–æ¢å¤ç±»å‹ï¼ˆLDR2HDRï¼‰ç”Ÿæˆå»ºæ¨¡ç»“æœï¼Œä½†ä»ç„¶æ˜¯HDRçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14304v2">PDF</a> 11 pages, 14 figures, Accepted to Eurographics 2025, see   <a target="_blank" rel="noopener" href="https://bracketdiffusion.mpi-inf.mpg.de/">https://bracketdiffusion.mpi-inf.mpg.de</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨å¤šä¸ªé¢„è®­ç»ƒçš„LDRå›¾åƒæ‰©æ•£æ¨¡å‹ååŒå·¥ä½œæ¥ç”ŸæˆHDRå›¾åƒã€‚ç”±äºç›®å‰å°šæ²¡æœ‰è¶³å¤Ÿå¤§çš„HDRå›¾åƒæ•°æ®é›†å¯ç”¨äºé‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œä¸”å³ä½¿å­˜åœ¨ä¹Ÿéš¾ä»¥æ‰¿å—å¤§å¤šæ•°è®¡ç®—é¢„ç®—è¿›è¡Œé‡è®­ç»ƒï¼Œå› æ­¤ä¾èµ–é¢„è®­ç»ƒçš„LDRç”Ÿæˆæ‰©æ•£æ¨¡å‹è‡³å…³é‡è¦ã€‚æˆ‘ä»¬ä»HDRå›¾åƒæ•è·çš„æ–‡çŒ®ä¸­è·å¾—çµæ„Ÿï¼Œè¿™äº›æ–‡çŒ®é€šè¿‡èåˆä¸€ç³»åˆ—çš„LDRå›¾åƒç”ŸæˆHDRå›¾åƒã€‚æˆ‘ä»¬é€šè¿‡æ‰§è¡Œå¤šæ¬¡å»å™ªè¿‡ç¨‹ç”Ÿæˆå¤šä¸ªLDR bracketsæ¥å½¢æˆæœ‰æ•ˆçš„HDRç»“æœã€‚è¯¥è¿‡ç¨‹çš„å…³é”®åœ¨äºåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å…¥ä¸€è‡´æ€§æœ¯è¯­ï¼Œå°†ä¸åŒæ›å…‰èŒƒå›´å†…çš„å„ä¸ªbracketsç»„åˆèµ·æ¥ï¼ŒåŒæ—¶è€ƒè™‘å¯èƒ½çš„é‡åŒ–è¯¯å·®å¼•èµ·çš„å·®å¼‚ã€‚æˆ‘ä»¬åœ¨HDRä¸Šå±•ç¤ºäº†æ— æ¡ä»¶å’Œæ¡ä»¶ç”Ÿæˆå»ºæ¨¡ç»“æœä»¥åŠæœ€å…ˆè¿›çš„æ¢å¤å‹æˆ–é‡å»ºå‹ï¼ˆLDR2HDRï¼‰ç»“æœã€‚  â€‹â€‹</p>
<p>â€‹â€‹ <strong>å…³é”®è§è§£</strong></p>
<p>â€‹â€‹ 1. åˆ©ç”¨å¤šä¸ªé¢„è®­ç»ƒçš„LDRå›¾åƒæ‰©æ•£æ¨¡å‹ååŒç”ŸæˆHDRå›¾åƒï¼Œæ— éœ€å¤§é‡æ•°æ®é‡è®­æ¨¡å‹ã€‚<br>â€‹â€‹ 2. HDRå›¾åƒç”Ÿæˆçµæ„Ÿæ¥æºäºä¼ ç»ŸHDRå›¾åƒæ•è·æŠ€æœ¯ä¸­çš„èåˆå¤šä¸ªLDRå›¾åƒçš„æ–¹æ³•ã€‚<br>â€‹â€‹ 3. é€šè¿‡æ‰§è¡Œå¤šæ¬¡å»å™ªè¿‡ç¨‹ç”Ÿæˆå¤šä¸ªLDR bracketså½¢æˆHDRç»“æœã€‚<br>â€‹â€‹ 4. åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å…¥ä¸€è‡´æ€§æœ¯è¯­æ¥ç»„åˆä¸åŒæ›å…‰èŒƒå›´çš„bracketsã€‚è€ƒè™‘é‡åŒ–è¯¯å·®å¼•èµ·çš„å·®å¼‚ã€‚<br>â€‹â€‹ 5. HDRå›¾åƒç”ŸæˆæŠ€æœ¯å®ç°äº†æ— æ¡ä»¶ç”Ÿæˆå»ºæ¨¡å’Œæ¡ä»¶ç”Ÿæˆå»ºæ¨¡ï¼ˆå¦‚æ¢å¤æˆ–é‡å»ºå‹ï¼‰ã€‚<br>â€‹â€‹ 6. è¯¥æ–¹æ³•å¯¹äºè§£å†³å½“å‰ç¼ºä¹è¶³å¤Ÿå¤§çš„HDRå›¾åƒæ•°æ®é›†çš„é—®é¢˜å…·æœ‰å…³é”®ä½œç”¨ã€‚å³ä½¿å­˜åœ¨æ•°æ®é›†ä¹Ÿéš¾ä»¥æ‰¿å—å¤§å¤šæ•°è®¡ç®—é¢„ç®—è¿›è¡Œé‡è®­ç»ƒçš„é—®é¢˜ã€‚<br>â€‹â€‹ 7. æ­¤æ–¹æ³•æé«˜äº†ä»LDRå›¾åƒç”ŸæˆHDRå›¾åƒçš„å»ºæ¨¡æ€§èƒ½å’Œè´¨é‡ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14304">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1ad4a6f9733ce1a05c8c047d0ef6c1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf3994879dd9e48ceb82cb2bed8bae7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-058b323f3c9a016d428dbd92fe352dc4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13b04154cb0fdf379d59f798311d4450.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a161f08649b752bbf72113bbf4b6358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9b067d041a504b2aa6a4b65a0c75451.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e5e2cba310243a748f096befda46f02.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Text-Guided-Texturing-by-Synchronized-Multi-View-Diffusion"><a href="#Text-Guided-Texturing-by-Synchronized-Multi-View-Diffusion" class="headerlink" title="Text-Guided Texturing by Synchronized Multi-View Diffusion"></a>Text-Guided Texturing by Synchronized Multi-View Diffusion</h2><p><strong>Authors:Yuxin Liu, Minshan Xie, Hanyuan Liu, Tien-Tsin Wong</strong></p>
<p>This paper introduces a novel approach to synthesize texture to dress up a given 3D object, given a text prompt. Based on the pretrained text-to-image (T2I) diffusion model, existing methods usually employ a project-and-inpaint approach, in which a view of the given object is first generated and warped to another view for inpainting. But it tends to generate inconsistent texture due to the asynchronous diffusion of multiple views. We believe such asynchronous diffusion and insufficient information sharing among views are the root causes of the inconsistent artifact. In this paper, we propose a synchronized multi-view diffusion approach that allows the diffusion processes from different views to reach a consensus of the generated content early in the process, and hence ensures the texture consistency. To synchronize the diffusion, we share the denoised content among different views in each denoising step, specifically blending the latent content in the texture domain from views with overlap. Our method demonstrates superior performance in generating consistent, seamless, highly detailed textures, comparing to state-of-the-art methods. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ ¹æ®æ–‡æœ¬æç¤ºå¯¹ç»™å®š3Då¯¹è±¡è¿›è¡Œçº¹ç†åˆæˆçš„æ–°æ–¹æ³•ã€‚åŸºäºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æŠ•å½±å’Œå¡«å……çš„æ–¹æ³•ï¼Œé¦–å…ˆç”Ÿæˆç»™å®šå¯¹è±¡çš„è§†å›¾ï¼Œå¹¶å°†å…¶æ‰­æ›²åˆ°å¦ä¸€ä¸ªè§†å›¾è¿›è¡Œå¡«å……ã€‚ä½†ç”±äºå¤šè§†å›¾çš„å¼‚æ­¥æ‰©æ•£ï¼Œè¿™ç§æ–¹æ³•å¾€å¾€ä¼šäº§ç”Ÿä¸ä¸€è‡´çš„çº¹ç†ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ç§å¼‚æ­¥æ‰©æ•£ä»¥åŠè§†å›¾ä¹‹é—´ä¿¡æ¯åˆ†äº«ä¸è¶³æ˜¯ä¸ä¸€è‡´çº¹ç†çš„æ ¹æºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒæ­¥å¤šè§†å›¾æ‰©æ•£æ–¹æ³•ï¼Œå…è®¸ä¸åŒè§†å›¾çš„æ‰©æ•£è¿‡ç¨‹åœ¨æ—©æœŸå°±è¾¾æˆå…±è¯†ï¼Œä»è€Œç¡®ä¿çº¹ç†çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†åŒæ­¥æ‰©æ•£ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­åˆ†äº«ä¸åŒè§†å›¾çš„å»å™ªå†…å®¹ï¼Œç‰¹åˆ«æ˜¯æ··åˆæ¥è‡ªé‡å è§†å›¾çš„çº¹ç†åŸŸä¸­çš„æ½œåœ¨å†…å®¹ã€‚ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆä¸€è‡´ã€æ— ç¼ã€é«˜ç»†èŠ‚çš„çº¹ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12891v2">PDF</a> 11 pages, 11 figures, technical papers, â€œText, Texturing, and   Stylizationâ€@SIGGRAPH Asia 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºæ ¹æ®æ–‡æœ¬æç¤ºåˆæˆçº¹ç†ä»¥è£…é¥°ç»™å®šçš„3Då¯¹è±¡ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œå¦‚å¼‚æ­¥æ‰©æ•£å’Œä¿¡æ¯å…±äº«ä¸è¶³å¯¼è‡´çš„çº¹ç†ä¸ä¸€è‡´é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŒæ­¥å¤šè§†å›¾æ‰©æ•£æ–¹æ³•ï¼Œé€šè¿‡åœ¨ä¸åŒè§†å›¾çš„æ‰©æ•£è¿‡ç¨‹ä¸­å®ç°æ—©æœŸå†…å®¹å…±è¯†ï¼Œç¡®ä¿çº¹ç†ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­å…±äº«å»å™ªå†…å®¹ï¼Œå¹¶ç‰¹åˆ«èåˆæ¥è‡ªé‡å è§†å›¾çš„çº¹ç†åŸŸæ½œåœ¨å†…å®¹ï¼Œå±•ç¤ºäº†ç”Ÿæˆä¸€è‡´ã€æ— ç¼ã€é«˜ç»†èŠ‚çº¹ç†çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºæ–‡æœ¬æç¤ºåˆæˆçº¹ç†ä»¥è£…é¥°3Då¯¹è±¡çš„æ–°æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é‡‡ç”¨æŠ•å½±å’Œä¿®å¤ï¼ˆproject-and-inpaintï¼‰æ–¹å¼ï¼Œä½†å­˜åœ¨çº¹ç†ä¸ä¸€è‡´é—®é¢˜ã€‚</li>
<li>çº¹ç†ä¸ä¸€è‡´çš„æ ¹æœ¬åŸå› æ˜¯å¼‚æ­¥æ‰©æ•£å’Œè§†å›¾é—´ä¿¡æ¯åˆ†äº«ä¸è¶³ã€‚</li>
<li>æå‡ºäº†åŒæ­¥å¤šè§†å›¾æ‰©æ•£æ–¹æ³•ï¼Œç¡®ä¿ä¸åŒè§†å›¾åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­æ—©æœŸè¾¾æˆå†…å®¹å…±è¯†ã€‚</li>
<li>é€šè¿‡åœ¨æ¯ä¸€æ­¥å»å™ªè¿‡ç¨‹ä¸­å…±äº«å»å™ªå†…å®¹ï¼Œå¹¶èåˆé‡å è§†å›¾çš„æ½œåœ¨å†…å®¹æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆäº†æ›´ä¸€è‡´ã€æ— ç¼ã€é«˜ç»†èŠ‚çš„çº¹ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.12891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0780e425ecd92b30e65bcd0f5d34c816.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d97d1118e97120082afa6b9259b13b90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed798c2b691864b10bbbc0cbd8e75884.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5ecb79a3898b35a7b48481869ff9b83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ba491c0e5ad84cf07e6884c486ba445.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-269f22fed228fdef8433d3ba3be92c1d.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  Periodontal Bone Loss Analysis via Keypoint Detection With Heuristic   Post-Processing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f87187633247999b654758645b746bb5.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  Segmentation-Guided Neural Radiance Fields for Novel Street View   Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
