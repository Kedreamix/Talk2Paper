<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-03-20  PC-Talk Precise Facial Animation Control for Audio-Driven Talking Face   Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f9262304a8a2a125837231f0f70d031f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-20-更新"><a href="#2025-03-20-更新" class="headerlink" title="2025-03-20 更新"></a>2025-03-20 更新</h1><h2 id="PC-Talk-Precise-Facial-Animation-Control-for-Audio-Driven-Talking-Face-Generation"><a href="#PC-Talk-Precise-Facial-Animation-Control-for-Audio-Driven-Talking-Face-Generation" class="headerlink" title="PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face   Generation"></a>PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face   Generation</h2><p><strong>Authors:Baiqin Wang, Xiangyu Zhu, Fan Shen, Hao Xu, Zhen Lei</strong></p>
<p>Recent advancements in audio-driven talking face generation have made great progress in lip synchronization. However, current methods often lack sufficient control over facial animation such as speaking style and emotional expression, resulting in uniform outputs. In this paper, we focus on improving two key factors: lip-audio alignment and emotion control, to enhance the diversity and user-friendliness of talking videos. Lip-audio alignment control focuses on elements like speaking style and the scale of lip movements, whereas emotion control is centered on generating realistic emotional expressions, allowing for modifications in multiple attributes such as intensity. To achieve precise control of facial animation, we propose a novel framework, PC-Talk, which enables lip-audio alignment and emotion control through implicit keypoint deformations. First, our lip-audio alignment control module facilitates precise editing of speaking styles at the word level and adjusts lip movement scales to simulate varying vocal loudness levels, maintaining lip synchronization with the audio. Second, our emotion control module generates vivid emotional facial features with pure emotional deformation. This module also enables the fine modification of intensity and the combination of multiple emotions across different facial regions. Our method demonstrates outstanding control capabilities and achieves state-of-the-art performance on both HDTF and MEAD datasets in extensive experiments. </p>
<blockquote>
<p>近期音频驱动说话人脸生成技术的进展在嘴唇同步方面取得了很大的进步。然而，当前的方法往往对面部动画的控制不足，如演讲风格和情感表达，导致输出单一。在本文中，我们专注于改进两个关键因素：唇音频对齐和情感控制，以提高对话视频的多样性和用户友好性。唇音频对齐控制专注于演讲风格和嘴唇运动规模等元素，而情感控制则侧重于生成逼真的情感表达，允许强度等多个属性的修改。为了实现面部动画的精确控制，我们提出了一种新型框架PC-Talk，它通过隐式关键点变形实现唇音频对齐和情感控制。首先，我们的唇音频对齐控制模块便于精确编辑单词级别的演讲风格，并调整嘴唇运动规模以模拟不同的音量水平，同时保持与音频的嘴唇同步。其次，我们的情感控制模块通过纯情感变形生成生动的情感面部特征。该模块还实现了强度的精细修改和不同面部区域多种情感的组合。我们的方法展示了出色的控制能力，并在HDTF和MEAD数据集上的广泛实验中达到了最新性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14295v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了音频驱动的说话面部生成技术的最新进展，重点解决了唇音对齐和情绪控制两个关键问题，以提高说话视频的多样性和用户友好性。文章提出了一种新的框架PC-Talk，通过隐性关键点变形实现面部动画的精确控制。该框架包括唇音对齐控制模块和情绪控制模块，前者可以在词级精确编辑说话风格并调整唇部运动幅度以模拟不同的音量水平，后者可以生成逼真的情感面部表情并精细调整情感强度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期音频驱动的说话面部生成技术在唇同步方面取得显著进展，但对面部动画的控制如说话风格和情感表达方面仍有不足。</li>
<li>论文主要解决唇音对齐和情绪控制两个关键问题，旨在提高说话视频的多样性和用户友好性。</li>
<li>提出的PC-Talk框架通过隐性关键点变形实现面部动画的精确控制。</li>
<li>唇音对齐控制模块可以精确编辑说话风格，调整唇部运动幅度以模拟不同的音量水平。</li>
<li>情绪控制模块可以生成逼真的情感面部表情，并允许对情感强度进行微调。</li>
<li>PC-Talk框架在HDTF和MEAD数据集上的实验表现优异，展现了出色的控制能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14295">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0dea4682391aea1e14e875c46d2f39a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3154734d3aa3d378f235a6e8001265a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b6fd1e3fad00bf64b745ae760052ccd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db2d75adbbe2a300522365c7a04e510e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ead007beafb3e49e90d0d544358ec5a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5183ebdaf5036076cade30390574de47.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SyncDiff-Diffusion-based-Talking-Head-Synthesis-with-Bottlenecked-Temporal-Visual-Prior-for-Improved-Synchronization"><a href="#SyncDiff-Diffusion-based-Talking-Head-Synthesis-with-Bottlenecked-Temporal-Visual-Prior-for-Improved-Synchronization" class="headerlink" title="SyncDiff: Diffusion-based Talking Head Synthesis with Bottlenecked   Temporal Visual Prior for Improved Synchronization"></a>SyncDiff: Diffusion-based Talking Head Synthesis with Bottlenecked   Temporal Visual Prior for Improved Synchronization</h2><p><strong>Authors:Xulin Fan, Heting Gao, Ziyi Chen, Peng Chang, Mei Han, Mark Hasegawa-Johnson</strong></p>
<p>Talking head synthesis, also known as speech-to-lip synthesis, reconstructs the facial motions that align with the given audio tracks. The synthesized videos are evaluated on mainly two aspects, lip-speech synchronization and image fidelity. Recent studies demonstrate that GAN-based and diffusion-based models achieve state-of-the-art (SOTA) performance on this task, with diffusion-based models achieving superior image fidelity but experiencing lower synchronization compared to their GAN-based counterparts. To this end, we propose SyncDiff, a simple yet effective approach to improve diffusion-based models using a temporal pose frame with information bottleneck and facial-informative audio features extracted from AVHuBERT, as conditioning input into the diffusion process. We evaluate SyncDiff on two canonical talking head datasets, LRS2 and LRS3 for direct comparison with other SOTA models. Experiments on LRS2&#x2F;LRS3 datasets show that SyncDiff achieves a synchronization score 27.7%&#x2F;62.3% relatively higher than previous diffusion-based methods, while preserving their high-fidelity characteristics. </p>
<blockquote>
<p>说话人头部合成，也被称为语音对口型合成，会重建与给定音频轨迹相对应的面部动作。对合成视频的评估主要集中在两个方面：口型语音同步和图像保真度。最近的研究表明，基于生成对抗网络（GAN）和扩散模型的方法在这项任务上达到了最新技术水平（SOTA），其中扩散模型在图像保真度方面表现更佳，但与基于GAN的方法相比同步性较低。为此，我们提出了SyncDiff，这是一种简单有效的方法，通过采用包含信息瓶颈的临时姿态帧和从AVHuBERT中提取的面部信息音频特征作为扩散过程的条件输入，改进了基于扩散的模型。我们在两个标准的说话人头部数据集LRS2和LRS3上对SyncDiff进行了评估，以便与其他最新技术模型进行直接比较。在LRS2&#x2F;LRS3数据集上的实验表明，SyncDiff相对于之前的扩散方法实现了更高的同步得分，提高了27.7%&#x2F;62.3%，同时保持了其高保真特性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13371v1">PDF</a> Accepted to WACV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了说话人头部合成技术，重点讨论了基于扩散模型的改进方法SyncDiff。SyncDiff利用时间姿势帧和面部信息音频特征，提高了扩散模型的性能，实现了更准确的唇音同步和高保真图像。在LRS2和LRS3数据集上的实验表明，SyncDiff相较于其他先进模型有更好的同步性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>说话人头部合成技术包括语音到唇部的合成，主要评估两个方面：唇语音同步和图像保真度。</li>
<li>GAN模型和扩散模型是目前该任务的最先进模型。</li>
<li>扩散模型在图像保真度上表现优越，但在同步方面相对较差。</li>
<li>SyncDiff方法是一种简单有效的改进扩散模型的方法，通过使用时间姿势帧和面部信息音频特征作为条件输入到扩散过程中。</li>
<li>SyncDiff在LRS2和LRS3数据集上的实验结果表明，其同步得分相较于其他先进模型有显著提高。</li>
<li>SyncDiff方法能够在保持高保真特性的同时，提高唇语音同步的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13371">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-04cd02607912ce44d044088f63bf2ed2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-598cdbf1892330f99a556135b5920cae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7f6ef84b4251bd688f1f479c3887a61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70406ef38b0eeaafa7b28d980054ce5a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Unlock-Pose-Diversity-Accurate-and-Efficient-Implicit-Keypoint-based-Spatiotemporal-Diffusion-for-Audio-driven-Talking-Portrait"><a href="#Unlock-Pose-Diversity-Accurate-and-Efficient-Implicit-Keypoint-based-Spatiotemporal-Diffusion-for-Audio-driven-Talking-Portrait" class="headerlink" title="Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based   Spatiotemporal Diffusion for Audio-driven Talking Portrait"></a>Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based   Spatiotemporal Diffusion for Audio-driven Talking Portrait</h2><p><strong>Authors:Chaolong Yang, Kai Yao, Yuyao Yan, Chenru Jiang, Weiguang Zhao, Jie Sun, Guangliang Cheng, Yifei Zhang, Bin Dong, Kaizhu Huang</strong></p>
<p>Audio-driven single-image talking portrait generation plays a crucial role in virtual reality, digital human creation, and filmmaking. Existing approaches are generally categorized into keypoint-based and image-based methods. Keypoint-based methods effectively preserve character identity but struggle to capture fine facial details due to the fixed points limitation of the 3D Morphable Model. Moreover, traditional generative networks face challenges in establishing causality between audio and keypoints on limited datasets, resulting in low pose diversity. In contrast, image-based approaches produce high-quality portraits with diverse details using the diffusion network but incur identity distortion and expensive computational costs. In this work, we propose KDTalker, the first framework to combine unsupervised implicit 3D keypoint with a spatiotemporal diffusion model. Leveraging unsupervised implicit 3D keypoints, KDTalker adapts facial information densities, allowing the diffusion process to model diverse head poses and capture fine facial details flexibly. The custom-designed spatiotemporal attention mechanism ensures accurate lip synchronization, producing temporally consistent, high-quality animations while enhancing computational efficiency. Experimental results demonstrate that KDTalker achieves state-of-the-art performance regarding lip synchronization accuracy, head pose diversity, and execution efficiency.Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/chaolongy/KDTalker">https://github.com/chaolongy/KDTalker</a>. </p>
<blockquote>
<p>音频驱动的单图像说话肖像生成在虚拟现实、数字人类创建和电影制作中扮演着至关重要的角色。现有方法通常分为基于关键点的方法和基于图像的方法。基于关键点的方法能够有效地保留人物身份，但由于3D可变形模型的固定点限制，很难捕捉面部细节。此外，传统的生成网络在有限数据集上建立音频和关键点之间的因果关系时面临挑战，导致姿势多样性较低。相比之下，基于图像的方法使用扩散网络生成具有各种细节的高质量肖像，但会产生身份失真和昂贵的计算成本。在这项工作中，我们提出了KDTalker，这是第一个结合无监督隐式3D关键点和时空扩散模型的框架。利用无监督隐式3D关键点，KDTalker适应面部信息密度，使扩散过程能够灵活地模拟各种头部姿势并捕捉面部细节。定制设计的时空注意力机制确保准确的唇部同步，产生时间一致的高质量动画，同时提高计算效率。实验结果表明，KDTalker在唇部同步准确性、头部姿势多样性和执行效率方面达到最新技术水平。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/chaolongy/KDTalker%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/chaolongy/KDTalker找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12963v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于音频驱动的单图像说话肖像生成在虚拟现实、数字人类创建和电影制作中起着至关重要的作用。现有方法主要分为基于关键点的方法和基于图像的方法。本文提出了一种结合无监督隐式三维关键点与时空扩散模型的框架KDTalker。它采用无监督隐式三维关键点，自适应面部信息密度，使扩散过程能够灵活建模多种头部姿态并捕捉面部细节。定制设计的时空注意力机制确保了准确的唇同步，产生时间连贯的高质量动画，同时提高计算效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频驱动的单图像说话肖像生成在多个领域有重要应用。</li>
<li>现有方法分为基于关键点和基于图像两大类，各有优缺点。</li>
<li>基于关键点的方法虽能保留角色身份，但难以捕捉面部细节。</li>
<li>传统生成网络在建立音频与关键点之间的因果关系时面临挑战。</li>
<li>基于图像的方法能产生高质量肖像，但可能产生身份扭曲和计算成本高的问题。</li>
<li>KDTalker框架结合了无监督隐式三维关键点和时空扩散模型，提高了面部信息捕捉的灵活性和准确性。</li>
<li>KDTalker实现了先进的唇同步精度、头部姿态多样性和执行效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12963">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a889d7e910f61352d3fa643e52b58e4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5203dc0be2d3da64f568e4872272ea2f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49aaa97d719087b1bfb1a2ae82312e86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9262304a8a2a125837231f0f70d031f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Prosody-Enhanced-Acoustic-Pre-training-and-Acoustic-Disentangled-Prosody-Adapting-for-Movie-Dubbing"><a href="#Prosody-Enhanced-Acoustic-Pre-training-and-Acoustic-Disentangled-Prosody-Adapting-for-Movie-Dubbing" class="headerlink" title="Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody   Adapting for Movie Dubbing"></a>Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody   Adapting for Movie Dubbing</h2><p><strong>Authors:Zhedong Zhang, Liang Li, Chenggang Yan, Chunshan Liu, Anton van den Hengel, Yuankai Qi</strong></p>
<p>Movie dubbing describes the process of transforming a script into speech that aligns temporally and emotionally with a given movie clip while exemplifying the speaker’s voice demonstrated in a short reference audio clip. This task demands the model bridge character performances and complicated prosody structures to build a high-quality video-synchronized dubbing track. The limited scale of movie dubbing datasets, along with the background noise inherent in audio data, hinder the acoustic modeling performance of trained models. To address these issues, we propose an acoustic-prosody disentangled two-stage method to achieve high-quality dubbing generation with precise prosody alignment. First, we propose a prosody-enhanced acoustic pre-training to develop robust acoustic modeling capabilities. Then, we freeze the pre-trained acoustic system and design a disentangled framework to model prosodic text features and dubbing style while maintaining acoustic quality. Additionally, we incorporate an in-domain emotion analysis module to reduce the impact of visual domain shifts across different movies, thereby enhancing emotion-prosody alignment. Extensive experiments show that our method performs favorably against the state-of-the-art models on two primary benchmarks. The demos are available at <a target="_blank" rel="noopener" href="https://zzdoog.github.io/ProDubber/">https://zzdoog.github.io/ProDubber/</a>. </p>
<blockquote>
<p>电影配音是将剧本转化为与给定电影片段在时间和情感上对齐的台词的过程，同时以简短参考音频片段中的演示者的声音为榜样。这项任务要求模型将角色表演和复杂的韵律结构结合起来，以构建高质量的与视频同步的配音轨迹。电影配音数据集规模的有限性，以及音频数据中的背景噪音，阻碍了训练模型的声学建模性能。为了解决这些问题，我们提出了一种声学韵律解耦的两阶段方法来实现高质量的配音生成，具有精确韵律对齐。首先，我们提出了一种韵律增强的声学预训练，以发展稳健的声学建模能力。然后，我们冻结预训练的声学系统，设计一个解耦框架来建模文本特征和配音风格，同时保持声学质量。此外，我们加入了一个领域内的情感分析模块，以减少不同电影之间视觉领域变化的影响，从而增强情感韵律对齐。大量实验表明，我们的方法在两个主要基准测试上的表现优于最先进的模型。演示地址是：<a target="_blank" rel="noopener" href="https://zzdoog.github.io/ProDubber/%E3%80%82">https://zzdoog.github.io/ProDubber/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12042v2">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了电影配音的过程，并指出了现有数据集和音频数据中的背景噪声所带来的挑战。为解决这些问题，提出了一种基于声学-语调分离的两阶段方法来实现高质量的配音生成和精确的语调对齐。首先通过提出一种增强语调的声学预训练方法，增强声学建模能力。然后冻结预训练的声学系统，设计一个分离的框架来模拟语音文本特征和配音风格，同时保持声学质量。此外，还引入了一个领域的情感分析模块，以减少不同电影之间视觉域变化的冲击，从而提高情感语调的对齐效果。实验证明，该方法在两个主要基准测试上的表现优于现有模型。相关演示可通过<a target="_blank" rel="noopener" href="https://zzdoog.github.io/ProDubber/">链接</a>查看。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>电影配音是暂时和情感上与给定电影片段相符的语音转换过程，需要模型桥接角色表演和复杂的语调结构来构建高质量的视频同步配音轨道。</li>
<li>当前面临的主要挑战是电影配音数据集规模的限制以及音频数据中的背景噪声问题。</li>
<li>提出了一种基于声学-语调分离的两阶段方法来实现高质量的配音生成和精确的语调对齐。</li>
<li>通过声学预训练提升声学建模能力，通过冻结预训练声学系统并建立分离框架处理文本特征和配音风格。</li>
<li>引入了领域的情感分析模块，减少不同电影间视觉域变化对情感语调对齐的影响。</li>
<li>实验证明该方法在主要基准测试上的表现优于现有模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12042">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f668efb3ad1254019a82b00bfcbdd188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53311922658be8ead1ffbba037090584.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3614debffb60ad043d0ef7e07834a70.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RASA-Replace-Anyone-Say-Anything-–-A-Training-Free-Framework-for-Audio-Driven-and-Universal-Portrait-Video-Editing"><a href="#RASA-Replace-Anyone-Say-Anything-–-A-Training-Free-Framework-for-Audio-Driven-and-Universal-Portrait-Video-Editing" class="headerlink" title="RASA: Replace Anyone, Say Anything – A Training-Free Framework for   Audio-Driven and Universal Portrait Video Editing"></a>RASA: Replace Anyone, Say Anything – A Training-Free Framework for   Audio-Driven and Universal Portrait Video Editing</h2><p><strong>Authors:Tianrui Pan, Lin Liu, Jie Liu, Xiaopeng Zhang, Jie Tang, Gangshan Wu, Qi Tian</strong></p>
<p>Portrait video editing focuses on modifying specific attributes of portrait videos, guided by audio or video streams. Previous methods typically either concentrate on lip-region reenactment or require training specialized models to extract keypoints for motion transfer to a new identity. In this paper, we introduce a training-free universal portrait video editing framework that provides a versatile and adaptable editing strategy. This framework supports portrait appearance editing conditioned on the changed first reference frame, as well as lip editing conditioned on varied speech, or a combination of both. It is based on a Unified Animation Control (UAC) mechanism with source inversion latents to edit the entire portrait, including visual-driven shape control, audio-driven speaking control, and inter-frame temporal control. Furthermore, our method can be adapted to different scenarios by adjusting the initial reference frame, enabling detailed editing of portrait videos with specific head rotations and facial expressions. This comprehensive approach ensures a holistic and flexible solution for portrait video editing. The experimental results show that our model can achieve more accurate and synchronized lip movements for the lip editing task, as well as more flexible motion transfer for the appearance editing task. Demo is available at <a target="_blank" rel="noopener" href="https://alice01010101.github.io/RASA/">https://alice01010101.github.io/RASA/</a>. </p>
<blockquote>
<p>肖像视频编辑主要关注根据音频或视频流修改肖像视频的具体属性。之前的方法通常集中在唇部区域的再现，或者需要训练专门模型以提取关键点，用于将动作转移到新身份。在本文中，我们介绍了一个无训练通用的肖像视频编辑框架，该框架提供了一种通用和可适应的编辑策略。此框架支持根据更改的第一帧进行肖像外观编辑，以及根据各种语音进行唇部编辑，或两者的组合。它基于统一动画控制（UAC）机制，使用源反转潜在变量来编辑整个肖像，包括视觉驱动的形状控制、音频驱动的说话控制以及帧间时间控制。此外，通过调整初始参考帧，我们的方法能够适应不同场景，实现对具有特定头部旋转和面部表情的肖像视频的详细编辑。这种综合方法确保了肖像视频编辑的全面和灵活解决方案。实验结果表明，我们的模型在唇部编辑任务上可以实现更准确、更同步的唇部运动，以及在外观编辑任务上实现更灵活的动态转移。演示请访问：<a target="_blank" rel="noopener" href="https://alice01010101.github.io/RASA/%E3%80%82">https://alice01010101.github.io/RASA/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11571v1">PDF</a> Demo is available at <a target="_blank" rel="noopener" href="https://alice01010101.github.io/RASA/">https://alice01010101.github.io/RASA/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种无需训练即可应用的通用肖像视频编辑框架，该框架支持基于更改的第一帧的肖像外观编辑和基于不同语音的唇部编辑，或两者的组合。它基于统一动画控制（UAC）机制，通过源反转潜码来编辑整个肖像，包括视觉驱动的形状控制、音频驱动的说话控制和帧间时间控制。此外，通过调整初始帧，该方法可适应不同的场景，实现对肖像视频特定头部旋转和面部表情的详细编辑。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种新型的肖像视频编辑框架，该框架无需训练即可应用，具有广泛适应性和灵活性。</li>
<li>框架支持基于更改的第一帧的肖像外观编辑和基于不同语音的唇部编辑，或两者的组合。</li>
<li>该框架基于统一动画控制（UAC）机制，该机制通过源反转潜码编辑整个肖像。</li>
<li>实现了视觉驱动的形状控制、音频驱动的说话控制和帧间时间控制。</li>
<li>通过调整初始帧，该框架可适应不同的编辑场景，实现详细编辑。</li>
<li>实验结果表明，该框架在唇编辑任务上能实现更准确、更同步的唇部运动，在外观编辑任务上能实现更灵活的动态转移。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11571">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-494b5351c6dca9bcb5633a94bb18f63d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19ed25f2fa2b012b588b574e4b650201.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-689210928e643e3f8fe57b994cfeaf22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33ea69f8f4131a693b15aedf596686ba.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EmoDiffusion-Enhancing-Emotional-3D-Facial-Animation-with-Latent-Diffusion-Models"><a href="#EmoDiffusion-Enhancing-Emotional-3D-Facial-Animation-with-Latent-Diffusion-Models" class="headerlink" title="EmoDiffusion: Enhancing Emotional 3D Facial Animation with Latent   Diffusion Models"></a>EmoDiffusion: Enhancing Emotional 3D Facial Animation with Latent   Diffusion Models</h2><p><strong>Authors:Yixuan Zhang, Qing Chang, Yuxi Wang, Guang Chen, Zhaoxiang Zhang, Junran Peng</strong></p>
<p>Speech-driven 3D facial animation seeks to produce lifelike facial expressions that are synchronized with the speech content and its emotional nuances, finding applications in various multimedia fields. However, previous methods often overlook emotional facial expressions or fail to disentangle them effectively from the speech content. To address these challenges, we present EmoDiffusion, a novel approach that disentangles different emotions in speech to generate rich 3D emotional facial expressions. Specifically, our method employs two Variational Autoencoders (VAEs) to separately generate the upper face region and mouth region, thereby learning a more refined representation of the facial sequence. Unlike traditional methods that use diffusion models to connect facial expression sequences with audio inputs, we perform the diffusion process in the latent space. Furthermore, we introduce an Emotion Adapter to evaluate upper face movements accurately. Given the paucity of 3D emotional talking face data in the animation industry, we capture facial expressions under the guidance of animation experts using LiveLinkFace on an iPhone. This effort results in the creation of an innovative 3D blendshape emotional talking face dataset (3D-BEF) used to train our network. Extensive experiments and perceptual evaluations validate the effectiveness of our approach, confirming its superiority in generating realistic and emotionally rich facial animations. </p>
<blockquote>
<p>语音驱动的三维面部动画旨在产生与语音内容及其情感细微差别同步的逼真面部表情，并广泛应用于各种多媒体领域。然而，之前的方法常常忽视情感面部表情，或者无法有效地从语音内容中将其分辨出来。为了应对这些挑战，我们提出了EmoDiffusion这一新方法，该方法能够分辨语音中的不同情感，生成丰富的三维情感面部表情。具体来说，我们的方法采用两个变分自编码器（VAEs）分别生成上半脸区域和嘴巴区域，从而学习更精细的面部序列表示。不同于传统方法使用扩散模型将面部表情序列与音频输入相连，我们在潜在空间执行扩散过程。此外，我们引入情感适配器来准确评估上半脸的运动。鉴于动画行业缺乏三维情感对话面部数据，我们在动画专家的指导下使用iPhone上的LiveLinkFace工具捕捉面部表情，从而创建了创新的三维情感混合对话面部数据集（3D-BEF），用于训练我们的网络。大量实验和感知评估验证了我们的方法的有效性，证实其在生成真实且情感丰富的面部动画方面的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11028v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了一种名为EmoDiffusion的新方法，该方法旨在解决语音驱动的3D面部动画中的情感表达问题。通过采用两个变分自编码器（VAEs）分别生成面部上半部分和嘴巴区域，并在潜在空间中进行扩散过程，该方法能够精细地表示面部序列。此外，还引入了一个情感适配器来准确评估面部上半部分运动。为解决动画行业中缺乏3D情感对话面部数据的问题，该研究还使用iPhone上的LiveLinkFace工具在动画专家指导下捕获面部表情，创建了一个创新的3D情感谈话面部数据集（3D-BEF），用于训练网络。实验和感知评估验证了该方法的真实性和情感丰富性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>EmoDiffusion方法能够解决语音驱动的3D面部动画中情感表达的问题。</li>
<li>通过变分自编码器（VAEs）分别生成面部上半部分和嘴巴区域，提高面部动画的精细度。</li>
<li>在潜在空间中进行扩散过程，实现面部表情与语音内容的更紧密关联。</li>
<li>引入情感适配器准确评估面部上半部分运动。</li>
<li>创建了一个创新的3D情感谈话面部数据集（3D-BEF），用于训练网络。</li>
<li>该方法通过广泛实验验证其有效性，并表现出生成真实和情感丰富面部动画的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11028">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-58e853a905eceb5428ab9a863737421b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f254f40f8a1ce87ba5cea036d281732.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Versatile-Multimodal-Controls-for-Whole-Body-Talking-Human-Animation"><a href="#Versatile-Multimodal-Controls-for-Whole-Body-Talking-Human-Animation" class="headerlink" title="Versatile Multimodal Controls for Whole-Body Talking Human Animation"></a>Versatile Multimodal Controls for Whole-Body Talking Human Animation</h2><p><strong>Authors:Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Zixin Zhu, Minghui Yang, Ming Yang, Le Wang</strong></p>
<p>Human animation from a single reference image shall be flexible to synthesize whole-body motion for either a headshot or whole-body portrait, where the motions are readily controlled by audio signal and text prompts. This is hard for most existing methods as they only support producing pre-specified head or half-body motion aligned with audio inputs. In this paper, we propose a versatile human animation method, i.e., VersaAnimator, which generates whole-body talking human from arbitrary portrait images, not only driven by audio signal but also flexibly controlled by text prompts. Specifically, we design a text-controlled, audio-driven motion generator that produces whole-body motion representations in 3D synchronized with audio inputs while following textual motion descriptions. To promote natural smooth motion, we propose a code-pose translation module to link VAE codebooks with 2D DWposes extracted from template videos. Moreover, we introduce a multi-modal video diffusion that generates photorealistic human animation from a reference image according to both audio inputs and whole-body motion representations. Extensive experiments show that VersaAnimator outperforms existing methods in visual quality, identity preservation, and audio-lip synchronization. </p>
<blockquote>
<p>从单一参考图像生成的人脸动画应该能够灵活地合成头部特写或全身肖像的全身运动，这些运动可以通过音频信号和文字提示轻松控制。对于大多数现有方法而言，这很难实现，因为它们仅支持生成与音频输入对齐的预设头部或半身运动。在本文中，我们提出了一种通用的人脸动画方法，即VersaAnimator，它可以从任意的肖像图像生成全身说话的人脸动画，不仅由音频信号驱动，还通过文字提示进行灵活控制。具体来说，我们设计了一个文本控制、音频驱动的运动生成器，它产生与音频输入同步的全身运动表示（在3D中），同时遵循文本运动描述。为了促进自然流畅的运动，我们提出了一个编码姿势转换模块，将VAE代码本与从模板视频中提取的2DDW姿势相关联。此外，我们引入了一种多模式视频扩散方法，根据参考图像、音频输入和全身运动表示生成写实风格的人脸动画。大量实验表明，VersaAnimator在视觉质量、身份保留和音频唇形同步方面优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08714v2">PDF</a> </p>
<p><strong>Summary</strong><br>动画技术可以从单一参考图像生成全身动态人像，该技术灵活性强，可以根据音频信号和文本提示进行操控。以往方法大多只支持预设定的头部或半身运动与音频对齐，而此方法不仅能从任意肖像图像生成全身动态人像，更能通过文本描述灵活控制，音频驱动产生与音频输入同步的全身运动表现。此方法设计了一个文本控制的运动生成器，通过代码姿势翻译模块连接VAE编码本与从模板视频提取的2DDW姿势，促进自然流畅的运动。同时引入多模态视频扩散技术，根据参考图像、音频输入和全身运动表现生成逼真的动画效果。实验结果证明此方法在视觉质量、身份保留和音频同步方面表现优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>人像动画技术能从单一参考图像生成全身动态人像。</li>
<li>技术结合音频信号和文本提示进行操控，具有灵活性。</li>
<li>提出一种文本控制的运动生成器，能同步生成全身运动表现。</li>
<li>通过代码姿势翻译模块和多模态视频扩散技术促进自然流畅的运动并提升动画效果。</li>
<li>该方法在视觉质量、身份保留和音频同步方面表现优越。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08714">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a4be268b951eb464599915f093168534.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b16248ac9cc86d5b6803d40037667cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b111d2208a9e49f013d5550bffdaa200.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28fcbcf16b6a3381317a45ec3ee5082f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dfa8bd672ff361a77bf3c1459b8296e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-82b742bc479adb9e7b33d83aad6fef07.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-03-20  Less is More Improving Motion Diffusion Models with Sparse Keyframes
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-10a3540cf65985bc59d402abe493a471.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-03-20  MusicInfuser Making Video Diffusion Listen and Dance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18863.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
