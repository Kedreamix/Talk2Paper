<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  Aligning Multimodal LLM with Human Preference A Survey">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-96f22e362929099cfc1528b65e5c71a8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-20-æ›´æ–°"><a href="#2025-03-20-æ›´æ–°" class="headerlink" title="2025-03-20 æ›´æ–°"></a>2025-03-20 æ›´æ–°</h1><h2 id="Aligning-Multimodal-LLM-with-Human-Preference-A-Survey"><a href="#Aligning-Multimodal-LLM-with-Human-Preference-A-Survey" class="headerlink" title="Aligning Multimodal LLM with Human Preference: A Survey"></a>Aligning Multimodal LLM with Human Preference: A Survey</h2><p><strong>Authors:Tao Yu, Yi-Fan Zhang, Chaoyou Fu, Junkang Wu, Jinda Lu, Kun Wang, Xingyu Lu, Yunhang Shen, Guibin Zhang, Dingjie Song, Yibo Yan, Tianlong Xu, Qingsong Wen, Zhang Zhang, Yan Huang, Liang Wang, Tieniu Tan</strong></p>
<p>Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at <a target="_blank" rel="noopener" href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment">https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿé€šè¿‡ç®€å•çš„æç¤ºå¤„ç†å¤šç§ä¸€èˆ¬ä»»åŠ¡ï¼Œè€Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚åŸºäºLLMçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤„ç†æ¶‰åŠè§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬æ•°æ®çš„å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¸çœŸå®æ€§ã€å®‰å…¨æ€§ã€ç±»ä¼¼äººç±»çš„æ¨ç†å’Œäººç±»åå¥½å¯¹é½ç­‰ç›¸å…³çš„å…³é”®é—®é¢˜ä»æœªå¾—åˆ°è¶³å¤Ÿé‡è§†ã€‚è¿™ä¸€å·®è·ä¿ƒä½¿äº†å„ç§å¯¹é½ç®—æ³•çš„æ¶Œç°ï¼Œæ¯ç§ç®—æ³•éƒ½é’ˆå¯¹ä¸åŒçš„åº”ç”¨åœºæ™¯å’Œä¼˜åŒ–ç›®æ ‡ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¯¹é½ç®—æ³•æ˜¯è§£å†³ä¸Šè¿°æŒ‘æˆ˜çš„æœ‰åŠ›æ–¹æ³•ã€‚æœ¬æ–‡æ—¨åœ¨å…¨é¢ç³»ç»Ÿåœ°ç»¼è¿°MLLMçš„å¯¹é½ç®—æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä»¥ä¸‹å››ä¸ªæ–¹é¢ï¼šï¼ˆ1ï¼‰å¯¹é½ç®—æ³•æ‰€æ¶µç›–çš„åº”ç”¨åœºæ™¯ï¼ŒåŒ…æ‹¬é€šç”¨å›¾åƒç†è§£ã€å¤šå›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ï¼Œä»¥åŠæ‰©å±•çš„å¤šæ¨¡æ€åº”ç”¨ï¼›ï¼ˆ2ï¼‰æ„å»ºå¯¹é½æ•°æ®é›†çš„æ ¸å¿ƒå› ç´ ï¼ŒåŒ…æ‹¬æ•°æ®æ¥æºã€æ¨¡å‹å“åº”å’Œåå¥½æ³¨é‡Šï¼›ï¼ˆ3ï¼‰ç”¨äºè¯„ä¼°å¯¹é½ç®—æ³•çš„åŸºå‡†æµ‹è¯•ï¼›ï¼ˆ4ï¼‰å¯¹é½ç®—æ³•æœªæ¥å‘å±•æ–¹å‘çš„æ¢è®¨ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨å¸®åŠ©ç ”ç©¶äººå‘˜æ•´ç†è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•å¹¶æ¿€å‘æ›´å¥½çš„å¯¹é½æ–¹æ³•ã€‚æœ¬æ–‡çš„é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignmentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14504v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment">https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¤„ç†å„ç§é€šç”¨ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚åŸºäºLLMçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å¤„ç†æ¶‰åŠè§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬æ•°æ®çš„å¤æ‚ä»»åŠ¡æ—¶å±•ç°å‡ºæƒŠäººçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…³äºçœŸå®æ€§ã€å®‰å…¨æ€§ã€ç±»ä¼¼äººç±»çš„æ¨ç†ä»¥åŠä¸äººç±»åå¥½çš„å¯¹é½ç­‰é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†è§£å†³ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå„ç§å¯¹é½ç®—æ³•åº”è¿è€Œç”Ÿï¼Œæ¯ç§ç®—æ³•éƒ½é’ˆå¯¹ä¸åŒçš„åº”ç”¨åœºæ™¯å’Œä¼˜åŒ–ç›®æ ‡ã€‚æœ¬æ–‡æ—¨åœ¨å…¨é¢ç³»ç»Ÿåœ°ç»¼è¿°MLLMçš„å¯¹é½ç®—æ³•ï¼ŒåŒ…æ‹¬åº”ç”¨æƒ…æ™¯ã€æ„å»ºå¯¹é½æ•°æ®é›†çš„æ ¸å¿ƒå› ç´ ã€è¯„ä¼°å¯¹é½ç®—æ³•çš„åŸºå‡†æµ‹è¯•ä»¥åŠæœªæ¥å‘å±•æ–¹å‘çš„è®¨è®ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså…·å¤‡å¤„ç†å¤šç§é€šç”¨ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚</li>
<li>MLLMsåœ¨å¤„ç†å¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚è§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬ï¼‰æ—¶å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>LLMså’ŒMLLMsé¢ä¸´çœŸå®æ€§ã€å®‰å…¨æ€§å’Œä¸äººç±»åå¥½å¯¹é½ç­‰å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>å¯¹é½ç®—æ³•ç”¨äºè§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œå¹¶é’ˆå¯¹ä¸åŒåº”ç”¨åœºæ™¯å’Œä¼˜åŒ–ç›®æ ‡è¿›è¡Œå‘å±•ã€‚</li>
<li>æœ¬æ–‡ç»¼è¿°äº†MLLMå¯¹é½ç®—æ³•çš„åº”ç”¨æƒ…æ™¯ï¼ŒåŒ…æ‹¬å›¾åƒç†è§£ã€å¤šå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ä»¥åŠæ‰©å±•çš„å¤šæ¨¡æ€åº”ç”¨ã€‚</li>
<li>æ„å»ºå¯¹é½æ•°æ®é›†çš„æ ¸å¿ƒå› ç´ åŒ…æ‹¬æ•°æ®æ¥æºã€æ¨¡å‹å“åº”å’Œåå¥½æ ‡æ³¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4fc5dc3948d69434759e74de5d237218.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25ae4c3b47d48bfb1dc35d7091d44515.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4b799c74e6a36f8833d618c66aef1c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d767e9db0e846be6d7843e1cdabfddd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5109879b845d39412d62665b69048b6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Temporal-Consistency-for-LLM-Reasoning-Process-Error-Identification"><a href="#Temporal-Consistency-for-LLM-Reasoning-Process-Error-Identification" class="headerlink" title="Temporal Consistency for LLM Reasoning Process Error Identification"></a>Temporal Consistency for LLM Reasoning Process Error Identification</h2><p><strong>Authors:Jiacheng Guo, Yue Wu, Jiahao Qiu, Kaixuan Huang, Xinzhe Juan, Ling Yang, Mengdi Wang</strong></p>
<p>Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B&#x2F;8B distilled models to outperform all 70B&#x2F;72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/jcguo123/Temporal-Consistency">https://github.com/jcguo123/Temporal-Consistency</a> </p>
<blockquote>
<p>éªŒè¯å¯¹äºæœ‰æ•ˆçš„æ•°å­¦æ¨ç†è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ—¶åºä¸€è‡´æ€§æ–¹æ³•ï¼ŒéªŒè¯è€…å¯ä»¥æ ¹æ®ä¹‹å‰çš„è¯„ä¼°ç»“æœåå¤ä¿®æ­£ä»–ä»¬çš„åˆ¤æ–­ã€‚ä¸ä¸€è½®éªŒè¯æˆ–å¤šæ¨¡å‹è¾©è®ºæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä¸€ç³»åˆ—è‡ªæˆ‘åæ€è¡Œä¸ºçš„è¿è´¯æ€§æ¥æé«˜éªŒè¯çš„å‡†ç¡®æ€§ã€‚åœ¨å¤šç§æ•°å­¦è¿‡ç¨‹è¯¯å·®è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆMathcheckã€ProcessBenchå’ŒPRM800Kï¼‰ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºå‡†æ–¹æ³•ä¸Šå®ç°äº†æ€§èƒ½æ”¹è¿›ã€‚å½“åº”ç”¨äºæœ€æ–°çš„DeepSeek R1è’¸é¦æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½¿7B&#x2F;8Bè’¸é¦æ¨¡å‹åœ¨ProcessBenchä¸Šä¼˜äºæ‰€æœ‰70B&#x2F;72Bæ¨¡å‹å’ŒGPT-4oã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨æˆ‘ä»¬æ–¹æ³•çš„14Bè’¸é¦æ¨¡å‹å®ç°äº†ä¸Deepseek-R1ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/jcguo123/Temporal-Consistency">https://github.com/jcguo123/Temporal-Consistency</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14495v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ—¶åºä¸€è‡´æ€§éªŒè¯æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡éªŒè¯è€…åŸºäºä¹‹å‰çš„è¯„ä¼°è¿›è¡Œè¿­ä»£åˆ¤æ–­æ¥æé«˜éªŒè¯çš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§æ•°å­¦è¿‡ç¨‹è¯¯å·®è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½æ˜¾è‘—æé«˜è’¸é¦æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ—¶åºä¸€è‡´æ€§çš„æ–°éªŒè¯æ–¹æ³•ã€‚</li>
<li>éªŒè¯è€…é€šè¿‡è¿­ä»£åˆ¤æ–­æ¥å®Œå–„è‡ªå·±çš„è¯„ä¼°ã€‚</li>
<li>ä¸å…¶ä»–éªŒè¯æ–¹æ³•ç›¸æ¯”ï¼Œå¦‚ä¸€è½®éªŒè¯æˆ–å¤šæ¨¡å‹è¾©è®ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä¸€ç³»åˆ—è‡ªæˆ‘åæ€è¡ŒåŠ¨çš„ä¸€è‡´æ€§æ¥æé«˜éªŒè¯å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°å­¦è¿‡ç¨‹è¯¯å·®è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å½“åº”ç”¨äºDeepSeek R1è’¸é¦æ¨¡å‹æ—¶ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½¿è’¸é¦æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†å¤§å‹æ¨¡å‹å¦‚GPT-4oã€‚</li>
<li>ä½¿ç”¨è¯¥æ–¹æ³•çš„è’¸é¦14Bæ¨¡å‹æ€§èƒ½ä¸Deepseek-R1ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-864435f65ba915c9c421c72f02dcaca9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9f347cea6136be5e6d5647dfe7bd089.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6edba0c74726ea6a8f69b51e0be5c298.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02ad2c1bead1643cdbe57dd22a2df711.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49904da6b786b168eef1f756be2bbffd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6fa6a0677d295b614a48c6f59f27860f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03907bb3d4660fd29148acbe611707e7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Creation-MMBench-Assessing-Context-Aware-Creative-Intelligence-in-MLLM"><a href="#Creation-MMBench-Assessing-Context-Aware-Creative-Intelligence-in-MLLM" class="headerlink" title="Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM"></a>Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM</h2><p><strong>Authors:Xinyu Fang, Zhijian Chen, Kai Lan, Shengyuan Ding, Yingji Liang, Xiangyu Zhao, Farong Wen, Zicheng Zhang, Guofeng Zhang, Haodong Duan, Kai Chen, Dahua Lin</strong></p>
<p>Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLMâ€™s creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on <a target="_blank" rel="noopener" href="https://github.com/open-compass/Creation-MMBench">https://github.com/open-compass/Creation-MMBench</a>. </p>
<blockquote>
<p>åˆ›é€ åŠ›æ˜¯æ™ºèƒ½çš„ä¸€ä¸ªåŸºæœ¬æ–¹é¢ï¼Œæ¶‰åŠåœ¨ä¸åŒæƒ…å¢ƒä¸­äº§ç”Ÿæ–°é¢–ä¸”é€‚å½“è§£å†³æ–¹æ¡ˆçš„èƒ½åŠ›ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å¹¿æ³›è¯„ä¼°äº†å…¶åˆ›é€ åŠ›ï¼Œä½†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨è¯¥é¢†åŸŸçš„è¯„ä¼°ä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†Creation-MMBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­åŸºäºå›¾åƒçš„ä»»åŠ¡çš„åˆ›é€ åŠ›èƒ½åŠ›çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«765ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œæ¶µç›–51ä¸ªç²¾ç»†ä»»åŠ¡ã€‚ä¸ºäº†ç¡®ä¿ä¸¥æ ¼çš„è¯„ä¼°ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªæµ‹è¯•ç”¨ä¾‹å®šä¹‰äº†ç‰¹å®šçš„è¯„ä¼°æ ‡å‡†ï¼ŒæŒ‡å¯¼å¯¹é€šç”¨å“åº”è´¨é‡å’Œä¸è§†è§‰è¾“å…¥çš„äº‹å®ä¸€è‡´æ€§çš„è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¸“æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œå½“å‰å¼€æºçš„MLLMåœ¨åˆ›é€ æ€§ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜æ˜¾è¾ƒå·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè§†è§‰å¾®è°ƒå¯èƒ½ä¼šè´Ÿé¢å½±å“åŸºç¡€LLMçš„åˆ›é€ åŠ›ã€‚Creation-MMBenchä¸ºæ¨è¿›MLLMçš„åˆ›é€ åŠ›æä¾›äº†å®è´µçš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥æ”¹è¿›å¤šæ¨¡æ€ç”Ÿæˆæ™ºèƒ½å¥ å®šäº†åŸºç¡€ã€‚å®Œæ•´çš„æ•°æ®å’Œè¯„ä¼°ä»£ç å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/open-compass/Creation-MMBench%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/open-compass/Creation-MMBenchä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14478v1">PDF</a> Evaluation Code and dataset see   <a target="_blank" rel="noopener" href="https://github.com/open-compass/Creation-MMBench">https://github.com/open-compass/Creation-MMBench</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨çœŸå®ä¸–ç•Œå›¾åƒä»»åŠ¡ä¸­çš„åˆ›é€ æ€§èƒ½åŠ›è¯„ä¼°çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•Creation-MMBenchã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«765ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œæ¶µç›–51ä¸ªç²¾ç»†ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsçš„åˆ›é€ æ€§èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åˆ›é€ æ€§ä»»åŠ¡æ–¹é¢ï¼Œå½“å‰å¼€æºçš„MLLMsä¸ä¸“æœ‰æ¨¡å‹ç›¸æ¯”è¡¨ç°æ˜¾è‘—è¾ƒå·®ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æŒ‡å‡ºè§†è§‰å¾®è°ƒå¯èƒ½ä¼šå¯¹åŸºç¡€LLMçš„åˆ›é€ æ€§èƒ½åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚Creation-MMBenchä¸ºæ¨è¿›MLLMçš„åˆ›é€ åŠ›æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥å¤šæ¨¡æ€ç”Ÿæˆæ™ºèƒ½çš„æ”¹è¿›å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„åˆ›é€ æ€§èƒ½åŠ›è¯„ä¼°æ˜¯ä¸€ä¸ªæ–°å…´é¢†åŸŸã€‚</li>
<li>Creation-MMBenchæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°MLLMsåœ¨çœŸå®ä¸–ç•Œå›¾åƒä»»åŠ¡ä¸­åˆ›é€ æ€§èƒ½åŠ›çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…å«765ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œæ¶µç›–51ä¸ªç²¾ç»†ä»»åŠ¡ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°MLLMsçš„è¡¨ç°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰å¼€æºçš„MLLMsåœ¨åˆ›é€ æ€§ä»»åŠ¡æ–¹é¢è¡¨ç°è¾ƒå·®ï¼Œä¸ä¸“æœ‰æ¨¡å‹ç›¸æ¯”å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>è§†è§‰å¾®è°ƒå¯èƒ½ä¼šå¯¹åŸºç¡€LLMçš„åˆ›é€ æ€§èƒ½åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚</li>
<li>Creation-MMBenchä¸ºæ¨è¿›MLLMçš„åˆ›é€ åŠ›ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c8a7011b2a23d17e5f5d73994b107850.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09efbe47a8f3ceea96b742e454f6c65c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e67453f000ddf69c1ca3786e6c2eb249.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2dc92eb5e8bc35ceb2ca6a9e4ebd994f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-92cd3363a2eb929bbdac909e11ee8524.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d2dcea6246fd2c8eba09b110c3016c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f68c504d10bf168f36fe462f35e6d09.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DAPO-An-Open-Source-LLM-Reinforcement-Learning-System-at-Scale"><a href="#DAPO-An-Open-Source-LLM-Reinforcement-Learning-System-at-Scale" class="headerlink" title="DAPO: An Open-Source LLM Reinforcement Learning System at Scale"></a>DAPO: An Open-Source LLM Reinforcement Learning System at Scale</h2><p><strong>Authors:Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, Mingxuan Wang</strong></p>
<p>Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the $\textbf{D}$ecoupled Clip and $\textbf{D}$ynamic s$\textbf{A}$mpling $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{DAPO}$) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL. </p>
<blockquote>
<p>æ¨ç†æ‰©å±•èµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹å‰æ‰€æœªæœ‰çš„æ¨ç†èƒ½åŠ›ï¼Œå¼ºåŒ–å­¦ä¹ æ˜¯æ¿€å‘å¤æ‚æ¨ç†çš„æ ¸å¿ƒæŠ€æœ¯ã€‚ç„¶è€Œï¼Œæœ€å‰æ²¿æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®æŠ€æœ¯ç»†èŠ‚è¢«éšè—ï¼ˆä¾‹å¦‚åœ¨OpenAI o1åšå®¢å’ŒDeepSeek R1æŠ€æœ¯æŠ¥å‘Šä¸­ï¼‰ï¼Œå› æ­¤ç¤¾åŒºä»éš¾ä»¥å¤åˆ¶å…¶RLè®­ç»ƒç»“æœã€‚æˆ‘ä»¬æå‡ºäº†è§£è€¦çš„Clipå’ŒåŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ç®—æ³•ï¼Œå¹¶å®Œå…¨å¼€æºä¸€ä¸ªå…ˆè¿›çš„å¤§å‹RLç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨Qwen2.5-32BåŸºç¡€æ¨¡å‹åœ¨AIME 2024ä¸Šå®ç°äº†50åˆ†ã€‚ä¸åŒäºä¹‹å‰éšç’è®­ç»ƒç»†èŠ‚çš„å·¥ä½œï¼Œæˆ‘ä»¬ä»‹ç»äº†ç®—æ³•ä¸­çš„å››ä¸ªå…³é”®æŠ€æœ¯ï¼Œä½¿å¤§è§„æ¨¡å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ å–å¾—äº†æˆåŠŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨verlæ¡†æ¶ä¸Šå¼€æºäº†æˆ‘ä»¬çš„è®­ç»ƒä»£ç ï¼Œä»¥åŠç²¾å¿ƒç­–åˆ’å’Œå¤„ç†è¿‡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å¼€æºç³»ç»Ÿçš„è¿™äº›ç»„ä»¶æé«˜äº†å¯é‡å¤æ€§ï¼Œå¹¶æ”¯æŒæœªæ¥åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14476v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://dapo-sia.github.io/">https://dapo-sia.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ¨æ–­æ‰©å±•å’Œå¼ºåŒ–å­¦ä¹ çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å‰æ‰€æœªæœ‰çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡å­˜åœ¨ä¸€äº›éšè—çš„æŠ€æœ¯ç»†èŠ‚ï¼Œä½¿å¾—ç¤¾åŒºéš¾ä»¥é‡ç°å…¶å¼ºåŒ–å­¦ä¹ è®­ç»ƒç»“æœï¼Œä½†ä»å­˜åœ¨ä¸€äº›é—®é¢˜å€¼å¾—å…³æ³¨å’Œæ¢ç´¢ã€‚æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ç®—æ³•â€œDAPOâ€ï¼Œå¹¶ä¸”å®Œå…¨å¼€æºäº†ä¸€ä¸ªæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿã€‚ä¸å…ˆå‰çš„ç ”ç©¶ä¸åŒï¼Œè¯¥ç³»ç»Ÿå¯¹AIME 2024è¾¾åˆ°è¿‘äº”åç‚¹åˆ†çš„æ•ˆæœé‡‡ç”¨å¼€æºå¤„ç†æ¨¡å¼ã€‚æ­¤è®ºæ–‡å¼•å…¥å››é¡¹å…³é”®æŠ€æœ¯ä½¿å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ å–å¾—æˆåŠŸï¼ŒåŒæ—¶å¼€æºäº†è®­ç»ƒä»£ç å’Œæ•°æ®å¤„ç†æ•°æ®é›†ã€‚è¿™å°†æœ‰åŠ©äºæå‡ç³»ç»Ÿçš„å¯é‡å¤æ€§å¹¶æ”¯æŒæœªæ¥çš„å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ èµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹å‰æ‰€æœªæœ‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€äº›å…³é”®æŠ€æœ¯ç»†èŠ‚å°šæœªå…¬å¼€ï¼Œå½±å“äº†æŠ€æœ¯çš„é‡ç°æ€§ã€‚éœ€è¦é‡ç‚¹å…³æ³¨è¿™äº›éšè—çš„è¯¦ç»†ä¿¡æ¯ä»¥å¢å¼ºç ”ç©¶è¿›å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a5f5362e301bf903157b5c79231bd4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db16af696ffdcd2c5766dcd2187d7ed1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c055e4050baf073fb00ea05bd12951f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-764ab49e819faaf000443b4d359989c8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EnvBench-A-Benchmark-for-Automated-Environment-Setup"><a href="#EnvBench-A-Benchmark-for-Automated-Environment-Setup" class="headerlink" title="EnvBench: A Benchmark for Automated Environment Setup"></a>EnvBench: A Benchmark for Automated Environment Setup</h2><p><strong>Authors:Aleksandra Eliseeva, Alexander Kovrigin, Ilia Kholkin, Egor Bogomolov, Yaroslav Zharov</strong></p>
<p>Recent advances in Large Language Models (LLMs) have enabled researchers to focus on practical repository-level tasks in software engineering domain. In this work, we consider a cornerstone task for automating work with software repositories-environment setup, i.e., a task of configuring a repository-specific development environment on a system. Existing studies on environment setup introduce innovative agentic strategies, but their evaluation is often based on small datasets that may not capture the full range of configuration challenges encountered in practice. To address this gap, we introduce a comprehensive environment setup benchmark EnvBench. It encompasses 329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on repositories that present genuine configuration challenges, excluding projects that can be fully configured by simple deterministic scripts. To enable further benchmark extension and usage for model tuning, we implement two automatic metrics: a static analysis check for missing imports in Python and a compilation check for JVM languages. We demonstrate the applicability of our benchmark by evaluating three environment setup approaches, including a simple zero-shot baseline and two agentic workflows, that we test with two powerful LLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to successfully configure 6.69% repositories for Python and 29.47% repositories for JVM, suggesting that EnvBench remains challenging for current approaches. Our benchmark suite is publicly available at <a target="_blank" rel="noopener" href="https://github.com/JetBrains-Research/EnvBench">https://github.com/JetBrains-Research/EnvBench</a>. The dataset and experiment trajectories are available at <a target="_blank" rel="noopener" href="https://jb.gg/envbench">https://jb.gg/envbench</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å¾—ç ”ç©¶äººå‘˜èƒ½å¤Ÿå…³æ³¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸä¸­çš„å®é™…ä»“åº“çº§ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘è½¯ä»¶ä»“åº“è‡ªåŠ¨åŒ–å·¥ä½œä¸­çš„ä¸€ä¸ªåŸºçŸ³ä»»åŠ¡â€”â€”ç¯å¢ƒè®¾ç½®ï¼Œå³åœ¨ç³»ç»Ÿä¸Šé…ç½®ç‰¹å®šäºä»“åº“çš„å¼€å‘ç¯å¢ƒã€‚å…³äºç¯å¢ƒè®¾ç½®çš„ç ”ç©¶å·²ç»å¼•å…¥äº†åˆ›æ–°çš„ä¸»åŠ¨ç­–ç•¥ï¼Œä½†å®ƒä»¬çš„è¯„ä¼°é€šå¸¸åŸºäºå°å‹æ•°æ®é›†ï¼Œå¯èƒ½æ— æ³•æ•æ‰åˆ°å®è·µä¸­é‡åˆ°çš„é…ç½®æŒ‘æˆ˜çš„å…¨èŒƒå›´ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨é¢çš„ç¯å¢ƒè®¾ç½®åŸºå‡†æµ‹è¯•EnvBenchã€‚å®ƒæ¶µç›–äº†329ä¸ªPythonå’Œ665ä¸ªåŸºäºJVMï¼ˆJavaã€Kotlinï¼‰çš„ä»“åº“ï¼Œé‡ç‚¹å…³æ³¨é‚£äº›å­˜åœ¨çœŸå®é…ç½®æŒ‘æˆ˜çš„ä»“åº“ï¼Œæ’é™¤é‚£äº›å¯ä»¥é€šè¿‡ç®€å•çš„ç¡®å®šæ€§è„šæœ¬å®Œå…¨é…ç½®çš„é¡¹ç›®ã€‚ä¸ºäº†å®ç°åŸºå‡†æµ‹è¯•çš„è¿›ä¸€æ­¥æ‰©å±•å’Œæ¨¡å‹è°ƒæ•´çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬å®ç°äº†ä¸¤é¡¹è‡ªåŠ¨æŒ‡æ ‡ï¼šPythonä¸­ç¼ºå¤±å¯¼å…¥çš„é™æ€åˆ†ææ£€æŸ¥ä»¥åŠJVMè¯­è¨€çš„ç¼–è¯‘æ£€æŸ¥ã€‚æˆ‘ä»¬é€šè¿‡è¯„ä¼°ä¸‰ç§ç¯å¢ƒè®¾ç½®æ–¹æ³•æ¥éªŒè¯æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•çš„é€‚ç”¨æ€§ï¼ŒåŒ…æ‹¬ä¸€ä¸ªç®€å•çš„é›¶æ ·æœ¬åŸºå‡†å’Œä¸¤ç§ä¸»åŠ¨å·¥ä½œæµç¨‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªå¼ºå¤§çš„LLMä¸»å¹²ï¼ˆGPT-4oå’ŒGPT-4o-miniï¼‰è¿›è¡Œæµ‹è¯•ã€‚æœ€ä½³æ–¹æ³•æˆåŠŸé…ç½®äº†6.69%çš„Pythonä»“åº“å’Œ29.47%çš„JVMä»“åº“ï¼Œè¿™è¡¨æ˜EnvBenchå¯¹å½“å‰æ–¹æ³•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å¥—ä»¶å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JetBrains-Research/EnvBench">https://github.com/JetBrains-Research/EnvBench</a>å…¬å¼€è®¿é—®ã€‚æ•°æ®é›†å’Œå®éªŒè½¨è¿¹å¯åœ¨<a target="_blank" rel="noopener" href="https://jb.gg/envbench">https://jb.gg/envbench</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14443v1">PDF</a> Accepted at the DL4Code workshop at ICLRâ€™25</p>
<p><strong>Summary</strong>ï¼š<br>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å¾—è½¯ä»¶å·¥ç¨‹ä¸­ä»“åº“çº§åˆ«çš„ä»»åŠ¡å¾—ä»¥å®è·µã€‚æœ¬æ–‡å…³æ³¨è½¯ä»¶ä»“åº“ç¯å¢ƒè®¾ç½®çš„è‡ªåŠ¨åŒ–ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†å…¨é¢çš„ç¯å¢ƒè®¾ç½®åŸºå‡†æµ‹è¯•EnvBenchã€‚è¯¥æµ‹è¯•åŒ…å«å¤šç§Pythonå’ŒJVMä»“åº“ï¼Œæ¶µç›–çœŸå®é…ç½®æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å®ç°äº†ä¸¤ä¸ªè‡ªåŠ¨åº¦é‡æŒ‡æ ‡ä»¥æ”¯æŒåŸºå‡†æµ‹è¯•çš„æ‰©å±•å’Œæ¨¡å‹è°ƒæ•´ã€‚é€šè¿‡å¯¹ä¸‰ç§ç¯å¢ƒè®¾ç½®æ–¹æ³•çš„è¯„ä¼°ï¼Œå‘ç°å½“å‰æ¨¡å‹ä»éœ€æ”¹è¿›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å¥—ä»¶å…¬å¼€å¯ç”¨ï¼Œå¹¶æä¾›æ•°æ®é›†å’Œå®éªŒè½¨è¿¹ä»¥ä¾›ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†è½¯ä»¶å·¥ç¨‹ä¸­ä»“åº“çº§åˆ«ä»»åŠ¡çš„å®è·µã€‚</li>
<li>ç¯å¢ƒè®¾ç½®æ˜¯è½¯ä»¶ä»“åº“è‡ªåŠ¨åŒ–çš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€ã€‚</li>
<li>ç°æœ‰ç ”ç©¶è™½æå‡ºåˆ›æ–°æ€§çš„ä»£ç†ç­–ç•¥ï¼Œä½†å…¶è¯„ä¼°é€šå¸¸åŸºäºå°æ•°æ®é›†ï¼Œæ— æ³•å…¨é¢åæ˜ å®è·µä¸­çš„é…ç½®æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥å…¨é¢çš„ç¯å¢ƒè®¾ç½®åŸºå‡†æµ‹è¯•EnvBenchï¼ŒåŒ…å«å¤šç§Pythonå’ŒJVMä»“åº“ã€‚</li>
<li>EnvBenchä¸“æ³¨äºå…·æœ‰çœŸå®é…ç½®æŒ‘æˆ˜çš„ä»“åº“ï¼Œæ’é™¤å¯é€šè¿‡ç®€å•ç¡®å®šæ€§è„šæœ¬å®Œå…¨é…ç½®çš„é¡¹ç›®ã€‚</li>
<li>å®ç°ä¸¤ä¸ªè‡ªåŠ¨åº¦é‡æŒ‡æ ‡ä»¥æ”¯æŒåŸºå‡†æµ‹è¯•çš„æ‰©å±•å’Œæ¨¡å‹è°ƒæ•´ï¼šPythonçš„é™æ€åˆ†ææ£€æŸ¥ç¼ºå¤±å¯¼å…¥å’ŒJVMè¯­è¨€çš„ç¼–è¯‘æ£€æŸ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14443">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c090c14b0c85993368ae2bba5d70d95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d8d0c6c3414172bc53c4eaab1807b50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d4012c597f5f8c644bf21216555514e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLM-FE-Automated-Feature-Engineering-for-Tabular-Data-with-LLMs-as-Evolutionary-Optimizers"><a href="#LLM-FE-Automated-Feature-Engineering-for-Tabular-Data-with-LLMs-as-Evolutionary-Optimizers" class="headerlink" title="LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as   Evolutionary Optimizers"></a>LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as   Evolutionary Optimizers</h2><p><strong>Authors:Nikhil Abhyankar, Parshin Shojaee, Chandan K. Reddy</strong></p>
<p>Automated feature engineering plays a critical role in improving predictive model performance for tabular learning tasks. Traditional automated feature engineering methods are limited by their reliance on pre-defined transformations within fixed, manually designed search spaces, often neglecting domain knowledge. Recent advances using Large Language Models (LLMs) have enabled the integration of domain knowledge into the feature engineering process. However, existing LLM-based approaches use direct prompting or rely solely on validation scores for feature selection, failing to leverage insights from prior feature discovery experiments or establish meaningful reasoning between feature generation and data-driven performance. To address these challenges, we propose LLM-FE, a novel framework that combines evolutionary search with the domain knowledge and reasoning capabilities of LLMs to automatically discover effective features for tabular learning tasks. LLM-FE formulates feature engineering as a program search problem, where LLMs propose new feature transformation programs iteratively, and data-driven feedback guides the search process. Our results demonstrate that LLM-FE consistently outperforms state-of-the-art baselines, significantly enhancing the performance of tabular prediction models across diverse classification and regression benchmarks. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹åœ¨æ”¹è¿›è¡¨æ ¼å­¦ä¹ ä»»åŠ¡çš„é¢„æµ‹æ¨¡å‹æ€§èƒ½ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¼ ç»Ÿçš„è‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹æ–¹æ³•å—é™äºåœ¨å›ºå®šã€æ‰‹åŠ¨è®¾è®¡çš„æœç´¢ç©ºé—´å†…é¢„å…ˆå®šä¹‰çš„è½¬æ¢ï¼Œå¾€å¾€å¿½ç•¥äº†é¢†åŸŸçŸ¥è¯†ã€‚æœ€è¿‘ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å·²ç»å°†é¢†åŸŸçŸ¥è¯†é›†æˆåˆ°ç‰¹å¾å·¥ç¨‹è¿‡ç¨‹ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMçš„æ–¹æ³•ä½¿ç”¨ç›´æ¥æç¤ºæˆ–ä»…ä¾èµ–éªŒè¯åˆ†æ•°è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œæœªèƒ½åˆ©ç”¨å…ˆå‰ç‰¹å¾å‘ç°å®éªŒä¸­çš„è§è§£æˆ–åœ¨ç‰¹å¾ç”Ÿæˆå’Œæ•°æ®é©±åŠ¨æ€§èƒ½ä¹‹é—´å»ºç«‹æœ‰æ„ä¹‰çš„æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLM-FEï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆè¿›åŒ–æœç´¢å’ŒLLMçš„é¢†åŸŸçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œè‡ªåŠ¨å‘ç°è¡¨æ ¼å­¦ä¹ ä»»åŠ¡çš„æœ‰æ•ˆç‰¹å¾çš„æ–°å‹æ¡†æ¶ã€‚LLM-FEå°†ç‰¹å¾å·¥ç¨‹åˆ¶å®šä¸ºç¨‹åºæœç´¢é—®é¢˜ï¼Œå…¶ä¸­LLMæå‡ºæ–°çš„ç‰¹å¾è½¬æ¢ç¨‹åºè¿›è¡Œè¿­ä»£ï¼Œæ•°æ®é©±åŠ¨çš„åé¦ˆå¼•å¯¼æœç´¢è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLLM-FEæŒç»­è¶…è¶Šæœ€å…ˆè¿›çš„åŸºå‡†æµ‹è¯•ï¼Œæ˜¾è‘—æé«˜äº†å„ç§åˆ†ç±»å’Œå›å½’åŸºå‡†æµ‹è¯•è¡¨çš„é¢„æµ‹æ¨¡å‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14434v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹åœ¨æ”¹è¿›è¡¨æ ¼å­¦ä¹ ä»»åŠ¡çš„é¢„æµ‹æ¨¡å‹æ€§èƒ½ä¸­æ‰®æ¼”å…³é”®è§’è‰²ã€‚ä¼ ç»Ÿçš„è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹æ–¹æ³•å—é™äºå›ºå®šçš„æ‰‹åŠ¨è®¾è®¡æœç´¢ç©ºé—´å†…çš„é¢„å®šä¹‰è½¬æ¢ï¼Œå¸¸å¸¸å¿½è§†é¢†åŸŸçŸ¥è¯†ã€‚æœ€è¿‘ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å¾—é¢†åŸŸçŸ¥è¯†å¯ä»¥èå…¥ç‰¹å¾å·¥ç¨‹è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLMæ–¹æ³•é€šè¿‡ç›´æ¥æç¤ºæˆ–ä»…ä¾èµ–éªŒè¯åˆ†æ•°è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œæœªèƒ½åˆ©ç”¨å…ˆå‰çš„ç‰¹å¾å‘ç°å®éªŒä¸­çš„è§è§£æˆ–å»ºç«‹ç‰¹å¾ç”Ÿæˆä¸æ•°æ®é©±åŠ¨æ€§èƒ½ä¹‹é—´çš„æœ‰æ„ä¹‰çš„å…³ç³»ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLM-FEè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒç»“åˆäº†è¿›åŒ–æœç´¢ã€é¢†åŸŸçŸ¥è¯†å’ŒLLMsçš„æ¨ç†èƒ½åŠ›ï¼Œè‡ªåŠ¨ä¸ºè¡¨æ ¼å­¦ä¹ ä»»åŠ¡å‘ç°æœ‰æ•ˆç‰¹å¾ã€‚LLM-FEå°†ç‰¹å¾å·¥ç¨‹åˆ¶å®šä¸ºç¨‹åºæœç´¢é—®é¢˜ï¼Œå…¶ä¸­LLMsè¿­ä»£åœ°æå‡ºæ–°çš„ç‰¹å¾è½¬æ¢ç¨‹åºï¼Œæ•°æ®é©±åŠ¨çš„åé¦ˆå¼•å¯¼æœç´¢è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLLM-FEæŒç»­ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œæ˜¾è‘—æé«˜äº†ä¸åŒåˆ†ç±»å’Œå›å½’åŸºå‡†æµ‹è¯•çš„è¡¨æ ¼é¢„æµ‹æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹å¯¹äºæ”¹è¿›è¡¨æ ¼å­¦ä¹ ä»»åŠ¡çš„é¢„æµ‹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å› ä¾èµ–å›ºå®šçš„æ‰‹åŠ¨è®¾è®¡æœç´¢ç©ºé—´å’Œé¢„å®šä¹‰è½¬æ¢è€Œå—é™ã€‚</li>
<li>è¿‘æœŸLLMçš„è¿›æ­¥ä½¿å¾—é¢†åŸŸçŸ¥è¯†èå…¥ç‰¹å¾å·¥ç¨‹æˆä¸ºå¯èƒ½ã€‚</li>
<li>ç°æœ‰LLMæ–¹æ³•ä¸»è¦ä¾èµ–ç›´æ¥æç¤ºæˆ–éªŒè¯åˆ†æ•°ï¼Œå¿½è§†å…ˆå‰å®éªŒä¸­çš„è§è§£å’Œç‰¹å¾ç”Ÿæˆä¸æ•°æ®é©±åŠ¨æ€§èƒ½çš„å…³ç³»ã€‚</li>
<li>LLM-FEæ¡†æ¶ç»“åˆè¿›åŒ–æœç´¢ã€é¢†åŸŸçŸ¥è¯†å’ŒLLMsçš„æ¨ç†èƒ½åŠ›ï¼Œè‡ªåŠ¨å‘ç°è¡¨æ ¼å­¦ä¹ çš„æœ‰æ•ˆç‰¹å¾ã€‚</li>
<li>LLM-FEå°†ç‰¹å¾å·¥ç¨‹å®šä¹‰ä¸ºç¨‹åºæœç´¢é—®é¢˜ï¼ŒLLMsæå‡ºè¿­ä»£ç‰¹å¾è½¬æ¢ç¨‹åºï¼Œå¹¶ç”±æ•°æ®é©±åŠ¨çš„åé¦ˆå¼•å¯¼æœç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8383153ff18664c4a80211f4cd21d2bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe0190a0e99cbfee091a736abd936424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b2e151961f8f12570962bdf87e53879.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a000b736b29dff853efcfb30afc5e193.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75a5eb10db4accd311a71497b0b74a8a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unifying-Text-Semantics-and-Graph-Structures-for-Temporal-Text-attributed-Graphs-with-Large-Language-Models"><a href="#Unifying-Text-Semantics-and-Graph-Structures-for-Temporal-Text-attributed-Graphs-with-Large-Language-Models" class="headerlink" title="Unifying Text Semantics and Graph Structures for Temporal   Text-attributed Graphs with Large Language Models"></a>Unifying Text Semantics and Graph Structures for Temporal   Text-attributed Graphs with Large Language Models</h2><p><strong>Authors:Siwei Zhang, Yun Xiong, Yateng Tang, Xi Chen, Zian Jia, Zehao Gu, Jiarong Xu, Jiawei Zhang</strong></p>
<p>Temporal graph neural networks (TGNNs) have shown remarkable performance in temporal graph modeling. However, real-world temporal graphs often possess rich textual information, giving rise to temporal text-attributed graphs (TTAGs). Such combination of dynamic text semantics and evolving graph structures introduces heightened complexity. Existing TGNNs embed texts statically and rely heavily on encoding mechanisms that biasedly prioritize structural information, overlooking the temporal evolution of text semantics and the essential interplay between semantics and structures for synergistic reinforcement. To tackle these issues, we present \textbf, a novel framework that seamlessly extends existing TGNNs for TTAG modeling. The key idea is to employ the advanced large language models (LLMs) to extract the dynamic semantics in text space and then generate expressive representations unifying both semantics and structures. Specifically, we propose a Temporal Semantics Extractor in the {Cross} framework, which empowers the LLM to offer the temporal semantic understanding of nodeâ€™s evolving contexts of textual neighborhoods, facilitating semantic dynamics. Subsequently, we introduce the Semantic-structural Co-encoder, which collaborates with the above Extractor for synthesizing illuminating representations by jointly considering both semantic and structural information while encouraging their mutual reinforcement. Extensive experimental results on four public datasets and one practical industrial dataset demonstrate {Cross}â€™s significant effectiveness and robustness. </p>
<blockquote>
<p>æ—¶åºå›¾ç¥ç»ç½‘ç»œï¼ˆTGNNï¼‰åœ¨æ—¶åºå›¾å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„æ—¶åºå›¾é€šå¸¸å…·æœ‰ä¸°å¯Œçš„æ–‡æœ¬ä¿¡æ¯ï¼Œä»è€Œäº§ç”Ÿäº†æ—¶åºæ–‡æœ¬å±æ€§å›¾ï¼ˆTTAGï¼‰ã€‚åŠ¨æ€æ–‡æœ¬è¯­ä¹‰å’Œä¸æ–­æ¼”å˜çš„å›¾å½¢ç»“æ„çš„ç»“åˆå¢åŠ äº†å¤æ‚æ€§ã€‚ç°æœ‰çš„TGNNsé™æ€åµŒå…¥æ–‡æœ¬ï¼Œå¹¶ä¸¥é‡ä¾èµ–äºç¼–ç æœºåˆ¶ï¼Œè¿™äº›æœºåˆ¶åå‘äºä¼˜å…ˆå¤„ç†ç»“æ„ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†æ–‡æœ¬è¯­ä¹‰çš„æ—¶ç©ºæ¼”å˜ä»¥åŠè¯­ä¹‰å’Œç»“æ„ä¹‹é—´çš„åŸºæœ¬ç›¸äº’ä½œç”¨ï¼Œä»¥å®ç°ååŒå¢å¼ºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œCrossâ€çš„æ–°æ¡†æ¶ï¼Œæ— ç¼æ‰©å±•ç°æœ‰çš„TGNNç”¨äºTTAGå»ºæ¨¡ã€‚ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå–æ–‡æœ¬ç©ºé—´ä¸­çš„åŠ¨æ€è¯­ä¹‰ï¼Œç„¶åç”Ÿæˆç»Ÿä¸€è¯­ä¹‰å’Œç»“æ„çš„è¡¨è¾¾æ€§è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨â€œCrossâ€æ¡†æ¶ä¸­æå‡ºäº†ä¸€ä¸ªæ—¶åºè¯­ä¹‰æå–å™¨ï¼Œå®ƒèµ‹äºˆLLMå¯¹èŠ‚ç‚¹æ–‡æœ¬é‚»åŸŸä¸Šä¸‹æ–‡æ¼”å˜çš„ç†è§£èƒ½åŠ›ï¼Œä¿ƒè¿›è¯­ä¹‰åŠ¨æ€ã€‚éšåï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰ç»“æ„ååŒç¼–ç å™¨ï¼Œå®ƒä¸ä¸Šè¿°æå–å™¨åˆä½œï¼Œé€šè¿‡è”åˆè€ƒè™‘è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯æ¥åˆæˆè¯´æ˜æ€§è¡¨ç¤ºï¼ŒåŒæ—¶é¼“åŠ±å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä¿ƒè¿›ã€‚åœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†å’Œä¸€ä¸ªå®é™…å·¥ä¸šæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¯æ˜äº†â€œCrossâ€çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14411v1">PDF</a> Submit to ICML2025</p>
<p><strong>Summary</strong></p>
<p>TGNNåœ¨å¤„ç†å¸¦æœ‰ä¸°å¯Œæ–‡æœ¬ä¿¡æ¯çš„åŠ¨æ€æ—¶åºå›¾æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºCrossçš„æ–°æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå–æ–‡æœ¬ä¸­çš„åŠ¨æ€è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆç»Ÿä¸€è¯­ä¹‰å’Œç»“æ„çš„è¡¨è¾¾æ€§è¡¨ç¤ºã€‚é€šè¿‡Temporal Semantics Extractorå’ŒSemantic-structural Co-encoderä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå®ç°è¯­ä¹‰å’Œç»“æ„çš„ååŒå¼ºåŒ–ã€‚å®éªŒè¯æ˜ï¼ŒCrossåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TGNNåœ¨å¤„ç†å¸¦æœ‰ä¸°å¯Œæ–‡æœ¬ä¿¡æ¯çš„æ—¶åºå›¾æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>Crossæ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå–æ–‡æœ¬ä¸­çš„åŠ¨æ€è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>Temporal Semantics Extractoræ˜¯Crossæ¡†æ¶çš„å…³é”®ç»„ä»¶ï¼Œä½¿LLMèƒ½å¤Ÿç†è§£èŠ‚ç‚¹æ–‡æœ¬é‚»åŸŸçš„è¯­ä¹‰çš„æ¼”å˜ã€‚</li>
<li>Semantic-structural Co-encoderä¸Temporal Semantics Extractoråä½œï¼Œé€šè¿‡è”åˆè€ƒè™‘è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯æ¥ç”Ÿæˆè¡¨è¾¾æ€§è¡¨ç¤ºã€‚</li>
<li>Crossé€šè¿‡ååŒå¼ºåŒ–è¯­ä¹‰å’Œç»“æ„ä¿¡æ¯ï¼Œæé«˜äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCrosså…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58b3118d85de0323d18ca76b664f19d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f43d55d5865206b7f48479ca9da12bb9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1dad39ce21d38fbd1eeb0c54370015b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91d8e2b89b8fc81542b927e0d0a47024.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DualToken-Towards-Unifying-Visual-Understanding-and-Generation-with-Dual-Visual-Vocabularies"><a href="#DualToken-Towards-Unifying-Visual-Understanding-and-Generation-with-Dual-Visual-Vocabularies" class="headerlink" title="DualToken: Towards Unifying Visual Understanding and Generation with   Dual Visual Vocabularies"></a>DualToken: Towards Unifying Visual Understanding and Generation with   Dual Visual Vocabularies</h2><p><strong>Authors:Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen, Zenan Zhou, Jianhua Xu, Jiaqi Wang, Kaicheng Yu</strong></p>
<p>The differing representation spaces required for visual understanding and generation pose a challenge in unifying them within the autoregressive paradigm of large language models. A vision tokenizer trained for reconstruction excels at capturing low-level perceptual details, making it well-suited for visual generation but lacking high-level semantic representations for understanding tasks. Conversely, a vision encoder trained via contrastive learning aligns well with language but struggles to decode back into the pixel space for generation tasks. To bridge this gap, we propose DualToken, a method that unifies representations for both understanding and generation within a single tokenizer. However, directly integrating reconstruction and semantic objectives in a single tokenizer creates conflicts, leading to degraded performance in both reconstruction quality and semantic performance. Instead of forcing a single codebook to handle both semantic and perceptual information, DualToken disentangles them by introducing separate codebooks for high and low-level features, effectively transforming their inherent conflict into a synergistic relationship. As a result, DualToken achieves state-of-the-art performance in both reconstruction and semantic tasks while demonstrating remarkable effectiveness in downstream MLLM understanding and generation tasks. Notably, we also show that DualToken, as a unified tokenizer, surpasses the naive combination of two distinct types vision encoders, providing superior performance within a unified MLLM. </p>
<blockquote>
<p>å¯¹äºè§†è§‰ç†è§£å’Œç”Ÿæˆæ‰€éœ€è¦çš„ä¸åŒè¡¨ç¤ºç©ºé—´ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’èŒƒå¼ä¸­ç»Ÿä¸€å®ƒä»¬æ„æˆäº†ä¸€é¡¹æŒ‘æˆ˜ã€‚ç”¨äºé‡å»ºçš„æ„¿æ™¯åˆ†è¯å™¨æ“…é•¿æ•æ‰ä½çº§åˆ«çš„æ„ŸçŸ¥ç»†èŠ‚ï¼Œéå¸¸é€‚åˆäºè§†è§‰ç”Ÿæˆï¼Œä½†ç¼ºä¹é«˜çº§è¯­ä¹‰è¡¨ç¤ºï¼Œä¸é€‚ç”¨äºç†è§£ä»»åŠ¡ã€‚ç›¸åï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä¸è¯­è¨€å¯¹é½è‰¯å¥½ï¼Œä½†åœ¨ç”Ÿæˆåƒç´ ç©ºé—´æ—¶éš¾ä»¥è§£ç ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†DualTokenæ–¹æ³•ï¼Œå®ƒåœ¨ä¸€ä¸ªå•ä¸€çš„åˆ†è¯å™¨ä¸­ç»Ÿä¸€äº†ç†è§£å’Œç”Ÿæˆçš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç›´æ¥åœ¨åˆ†è¯å™¨ä¸­æ•´åˆé‡å»ºå’Œè¯­ä¹‰ç›®æ ‡ä¼šäº§ç”Ÿå†²çªï¼Œå¯¼è‡´é‡å»ºè´¨é‡å’Œè¯­ä¹‰æ€§èƒ½çš„ä¸‹é™ã€‚DualTokenå¹¶æ²¡æœ‰å¼ºåˆ¶å•ä¸ªä»£ç æœ¬å¤„ç†è¯­ä¹‰å’Œæ„ŸçŸ¥ä¿¡æ¯ï¼Œè€Œæ˜¯é€šè¿‡å¼•å…¥é«˜çº§å’Œä½çº§ç‰¹å¾çš„ä¸åŒä»£ç æœ¬æ¥è§£å†³å®ƒä»¬ä¹‹é—´çš„å†²çªï¼Œå°†å…¶å›ºæœ‰çš„å†²çªè½¬åŒ–ä¸ºååŒå…³ç³»ã€‚å› æ­¤ï¼ŒDualTokenåœ¨é‡å»ºå’Œè¯­ä¹‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸å¤§å‹è¯­è¨€æ¨¡å‹ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„åˆ†è¯å™¨ï¼ŒDualTokenè¶…è¶Šäº†ä¸¤ç§ä¸åŒç±»å‹è§†è§‰ç¼–ç å™¨çš„ç®€å•ç»„åˆï¼Œåœ¨ç»Ÿä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æä¾›äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14324v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†è§‰ç†è§£å’Œç”Ÿæˆæ‰€éœ€çš„ä¸åŒè¡¨ç¤ºç©ºé—´ç»™åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’èŒƒå¼ä¸­ç»Ÿä¸€å®ƒä»¬å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸€ç§ç”¨äºé‡å»ºçš„è§†è§‰åˆ†è¯å™¨æ“…é•¿æ•æ‰ä½çº§åˆ«çš„æ„ŸçŸ¥ç»†èŠ‚ï¼Œå¾ˆé€‚åˆäºè§†è§‰ç”Ÿæˆï¼Œä½†ç¼ºä¹é«˜çº§è¯­ä¹‰è¡¨ç¤ºï¼Œä¸è¶³ä»¥å®Œæˆç†è§£ä»»åŠ¡ã€‚ç›¸åï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä¸è¯­è¨€å¯¹é½è‰¯å¥½ï¼Œä½†åœ¨ç”Ÿæˆä»»åŠ¡ä¸­è§£ç å›åƒç´ ç©ºé—´æ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†DualTokenæ–¹æ³•ï¼Œæ—¨åœ¨åœ¨ä¸€ä¸ªåˆ†è¯å™¨ä¸­ç»Ÿä¸€ç†è§£å’Œè¡¨ç¤ºã€‚ç„¶è€Œï¼Œåœ¨å•ä¸€åˆ†è¯å™¨ä¸­ç›´æ¥æ•´åˆé‡å»ºå’Œè¯­ä¹‰ç›®æ ‡ä¼šäº§ç”Ÿå†²çªï¼Œå¯¼è‡´é‡å»ºè´¨é‡å’Œè¯­ä¹‰æ€§èƒ½çš„ä¸‹é™ã€‚DualTokené€šè¿‡å¼•å…¥ç”¨äºé«˜çº§å’Œä½çº§ç‰¹å¾çš„ä¸åŒä»£ç æœ¬ï¼Œè§£å†³è¿™ä¸€å†²çªï¼Œå°†å…¶è½¬åŒ–ä¸ºååŒå…³ç³»ã€‚å› æ­¤ï¼ŒDualTokenåœ¨é‡å»ºå’Œè¯­ä¹‰ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä¸‹æ¸¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½œä¸ºç»Ÿä¸€çš„åˆ†è¯å™¨ï¼ŒDualTokençš„è¡¨ç°è¶…è¶Šäº†ä¸¤ç§ä¸åŒç±»å‹è§†è§‰ç¼–ç å™¨çš„ç®€å•ç»„åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰ç†è§£å’Œç”Ÿæˆéœ€è¦ä¸åŒçš„è¡¨ç¤ºç©ºé—´ï¼Œç»™ç»Ÿä¸€å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>é‡å»ºçš„è§†è§‰åˆ†è¯å™¨æ“…é•¿æ•æ‰ä½çº§åˆ«æ„ŸçŸ¥ç»†èŠ‚ï¼Œé€‚åˆè§†è§‰ç”Ÿæˆï¼Œä½†ç¼ºä¹é«˜çº§è¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä¸è¯­è¨€å¯¹é½è‰¯å¥½ï¼Œä½†åœ¨ç”Ÿæˆä»»åŠ¡ä¸­è§£ç å›åƒç´ ç©ºé—´æœ‰å›°éš¾ã€‚</li>
<li>DualTokenæ–¹æ³•åœ¨ä¸€ä¸ªåˆ†è¯å™¨ä¸­ç»Ÿä¸€ç†è§£å’Œç”Ÿæˆè¡¨ç¤ºï¼Œå®ç°å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>ç›´æ¥æ•´åˆé‡å»ºå’Œè¯­ä¹‰ç›®æ ‡åœ¨å•ä¸€åˆ†è¯å™¨ä¸­ä¼šäº§ç”Ÿå†²çªï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>DualTokené€šè¿‡å¼•å…¥ä¸åŒä»£ç æœ¬è§£å†³å†²çªï¼Œå®ç°ååŒå…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14324">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1ddd75ca0c3cc3d9f5b743ac2ea6b2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-659a932e0f31c12e91af555ac37b69ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae66ad5a23909fd6398295d8b6e818be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ee69955c7819839fbb9dbdfe0c0a173.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f0f5185f3e857c787b90269e06ae2d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ca96d4dc5685f765e10020e45353c47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90805619900331187541159c8d293702.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d06771420225eaa66be86e3b8ec120d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Showdown-of-ChatGPT-vs-DeepSeek-in-Solving-Programming-Tasks"><a href="#A-Showdown-of-ChatGPT-vs-DeepSeek-in-Solving-Programming-Tasks" class="headerlink" title="A Showdown of ChatGPT vs DeepSeek in Solving Programming Tasks"></a>A Showdown of ChatGPT vs DeepSeek in Solving Programming Tasks</h2><p><strong>Authors:Ronas Shakya, Farhad Vadiee, Mohammad Khalil</strong></p>
<p>The advancement of large language models (LLMs) has created a competitive landscape for AI-assisted programming tools. This study evaluates two leading models: ChatGPT 03-mini and DeepSeek-R1 on their ability to solve competitive programming tasks from Codeforces. Using 29 programming tasks of three levels of easy, medium, and hard difficulty, we assessed the outcome of both models by their accepted solutions, memory efficiency, and runtime performance. Our results indicate that while both models perform similarly on easy tasks, ChatGPT outperforms DeepSeek-R1 on medium-difficulty tasks, achieving a 54.5% success rate compared to DeepSeek 18.1%. Both models struggled with hard tasks, thus highlighting some ongoing challenges LLMs face in handling highly complex programming problems. These findings highlight key differences in both model capabilities and their computational power, offering valuable insights for developers and researchers working to advance AI-driven programming tools. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºAIè¾…åŠ©ç¼–ç¨‹å·¥å…·é¢†åŸŸåˆ›é€ äº†ç«äº‰ç¯å¢ƒã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸¤æ¬¾é¢†å…ˆæ¨¡å‹ï¼šChatGPT 03-miniå’ŒDeepSeek-R1ï¼Œå®ƒä»¬åœ¨è§£å†³Codeforcesç«èµ›ç¼–ç¨‹ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨29ä¸ªåˆ†ä¸ºä¸‰ä¸ªéš¾åº¦çº§åˆ«çš„ç¼–ç¨‹ä»»åŠ¡ï¼Œé€šè¿‡æ¥å—çš„è§£å†³æ–¹æ¡ˆã€å†…å­˜æ•ˆç‡å’Œè¿è¡Œæ—¶æ€§èƒ½æ¥è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä¸¤ä¸ªæ¨¡å‹åœ¨ç®€å•ä»»åŠ¡ä¸Šçš„è¡¨ç°ç›¸ä¼¼ï¼Œä½†åœ¨ä¸­ç­‰éš¾åº¦ä»»åŠ¡ä¸Šï¼ŒChatGPTçš„è¡¨ç°ä¼˜äºDeepSeek-R1ï¼ŒæˆåŠŸç‡ä¸º54.5%ï¼Œè€ŒDeepSeekçš„æˆåŠŸç‡ä¸º18.1%ã€‚ä¸¤ä¸ªæ¨¡å‹åœ¨å›°éš¾ä»»åŠ¡ä¸Šéƒ½é‡åˆ°äº†æŒ‘æˆ˜ï¼Œè¿™çªå‡ºäº†LLMåœ¨å¤„ç†é«˜åº¦å¤æ‚çš„ç¼–ç¨‹é—®é¢˜æ—¶é¢ä¸´çš„ä¸€äº›æŒç»­æŒ‘æˆ˜ã€‚è¿™äº›å‘ç°çªå‡ºäº†ä¸¤ç§æ¨¡å‹åœ¨èƒ½åŠ›å’Œè®¡ç®—èƒ½åŠ›æ–¹é¢çš„å…³é”®å·®å¼‚ï¼Œä¸ºè‡´åŠ›äºå¼€å‘AIé©±åŠ¨çš„ç¼–ç¨‹å·¥å…·çš„å¼€å‘è€…å’Œç ”ç©¶äººå‘˜æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13549v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æ¨åŠ¨äº†AIè¾…åŠ©ç¼–ç¨‹å·¥å…·ä¹‹é—´çš„ç«äº‰ã€‚æœ¬ç ”ç©¶å¯¹æ¯”äº†ä¸¤æ¬¾é¢†å…ˆæ¨¡å‹ï¼šChatGPT 03-miniå’ŒDeepSeek-R1ï¼Œé€šè¿‡Codeforcesçš„ç¼–ç¨‹ä»»åŠ¡è¯„ä¼°å®ƒä»¬çš„æ€§èƒ½ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œåœ¨ç®€å•ä»»åŠ¡ä¸Šä¸¤è€…è¡¨ç°ç›¸è¿‘ï¼Œä½†åœ¨ä¸­ç­‰éš¾åº¦ä»»åŠ¡ä¸Šï¼ŒChatGPTè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼ŒæˆåŠŸç‡ä¸º54.5%ï¼Œè€ŒDeepSeekçš„æˆåŠŸç‡ä¸ºä»…ä¸º18.1%ã€‚ä¸¤è€…åœ¨å¤æ‚ä»»åŠ¡ä¸Šéƒ½é¢ä¸´æŒ‘æˆ˜ï¼Œå‡¸æ˜¾å‡ºLLMåœ¨å¤„ç†é«˜åº¦å¤æ‚ç¼–ç¨‹é—®é¢˜ä¸Šçš„å±€é™ã€‚æ­¤ç ”ç©¶ä¸ºå¼€å‘è€…å’Œç ”ç©¶è€…æä¾›äº†å®è´µçš„ä¿¡æ¯ï¼Œä»¥æ¨åŠ¨AIé©±åŠ¨çš„ç¼–ç¨‹å·¥å…·çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨AIè¾…åŠ©ç¼–ç¨‹å·¥å…·é¢†åŸŸå…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>ChatGPTå’ŒDeepSeek-R1æ˜¯ä¸¤æ¬¾é¢†å…ˆçš„LLMæ¨¡å‹ã€‚</li>
<li>åœ¨ç®€å•ç¼–ç¨‹ä»»åŠ¡ä¸Šï¼ŒChatGPTå’ŒDeepSeek-R1è¡¨ç°ç›¸è¿‘ã€‚</li>
<li>åœ¨ä¸­ç­‰éš¾åº¦ä»»åŠ¡ä¸Šï¼ŒChatGPTä¼˜äºDeepSeek-R1ã€‚</li>
<li>åœ¨å¤æ‚ç¼–ç¨‹ä»»åŠ¡ä¸Šï¼Œä¸¤è€…éƒ½é¢ä¸´æŒ‘æˆ˜ï¼Œæ˜¾ç¤ºå‡ºLLMçš„å±€é™ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›äº†å…³äºä¸¤ä¸ªæ¨¡å‹çš„èƒ½åŠ›å’Œè®¡ç®—èƒ½åŠ›çš„å…³é”®å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e00af8b47715f6b17e7d6f2a45b9725.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3f7c04485f138b0c7201da49b0b5857.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96f22e362929099cfc1528b65e5c71a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99d5d4a87da2ca6af76ab4009105ef7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-57815d4fc535f25b3aec9b49a91d36a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad6cf107827165df15b5d65af5039c2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8915bc890d917ed159c438fd57bceb03.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Falcon-A-Remote-Sensing-Vision-Language-Foundation-Model"><a href="#Falcon-A-Remote-Sensing-Vision-Language-Foundation-Model" class="headerlink" title="Falcon: A Remote Sensing Vision-Language Foundation Model"></a>Falcon: A Remote Sensing Vision-Language Foundation Model</h2><p><strong>Authors:Kelu Yao, Nuo Xu, Rong Yang, Yingying Xu, Zhuoyan Gao, Titinunt Kitrungrotsakul, Yi Ren, Pu Zhang, Jin Wang, Ning Wei, Chao Li</strong></p>
<p>This paper introduces a holistic vision-language foundation model tailored for remote sensing, named Falcon. Falcon offers a unified, prompt-based paradigm that effectively executes comprehensive and complex remote sensing tasks. Falcon demonstrates powerful understanding and reasoning abilities at the image, region, and pixel levels. Specifically, given simple natural language instructions and remote sensing images, Falcon can produce impressive results in text form across 14 distinct tasks, i.e., image classification, object detection, segmentation, image captioning, and etc. To facilitate Falconâ€™s training and empower its representation capacity to encode rich spatial and semantic information, we developed Falcon_SFT, a large-scale, multi-task, instruction-tuning dataset in the field of remote sensing. The Falcon_SFT dataset consists of approximately 78 million high-quality data samples, covering 5.6 million multi-spatial resolution and multi-view remote sensing images with diverse instructions. It features hierarchical annotations and undergoes manual sampling verification to ensure high data quality and reliability. Extensive comparative experiments are conducted, which verify that Falcon achieves remarkable performance over 67 datasets and 14 tasks, despite having only 0.7B parameters. We release the complete dataset, code, and model weights at <a target="_blank" rel="noopener" href="https://github.com/TianHuiLab/Falcon">https://github.com/TianHuiLab/Falcon</a>, hoping to help further develop the open-source community. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªä¸“ä¸ºé¥æ„Ÿé¢†åŸŸå®šåˆ¶çš„å…¨è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œåä¸ºFalconã€‚Falconæä¾›äº†ä¸€ä¸ªåŸºäºæç¤ºçš„ç»Ÿä¸€èŒƒå¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ‰§è¡Œå…¨é¢å¤æ‚çš„é¥æ„Ÿä»»åŠ¡ã€‚Falconåœ¨å›¾åƒã€åŒºåŸŸå’Œåƒç´ çº§åˆ«å±•ç°å‡ºå¼ºå¤§çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œç»™å®šç®€å•çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œé¥æ„Ÿå›¾åƒï¼ŒFalconå¯ä»¥åœ¨14ä¸ªä¸åŒçš„ä»»åŠ¡ä¸­äº§ç”Ÿä»¤äººå°è±¡æ·±åˆ»çš„æ–‡æœ¬å½¢å¼çš„ç»“æœï¼Œå¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€åˆ†å‰²ã€å›¾åƒæè¿°ç­‰ã€‚ä¸ºäº†è®­ç»ƒFalconå¹¶å¢å¼ºå…¶è¡¨ç¤ºèƒ½åŠ›ä»¥ç¼–ç ä¸°å¯Œçš„ç©ºé—´è¯­ä¹‰ä¿¡æ¯ï¼Œæˆ‘ä»¬å¼€å‘äº†é¥æ„Ÿé¢†åŸŸçš„å¤§è§„æ¨¡å¤šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Falcon_SFTã€‚Falcon_SFTæ•°æ®é›†åŒ…å«çº¦7800ä¸‡é«˜è´¨é‡æ•°æ®æ ·æœ¬ï¼Œè¦†ç›–560ä¸‡å…·æœ‰å¤šç§æŒ‡ä»¤çš„å¤šç©ºé—´åˆ†è¾¨ç‡å’Œå¤šè§†è§’é¥æ„Ÿå›¾åƒã€‚å®ƒé‡‡ç”¨åˆ†å±‚æ³¨é‡Šå¹¶ç»è¿‡æ‰‹åŠ¨é‡‡æ ·éªŒè¯ï¼Œä»¥ç¡®ä¿æ•°æ®çš„é«˜è´¨é‡å’Œå¯é æ€§ã€‚è¿›è¡Œäº†å¹¿æ³›çš„å¯¹æ¯”å®éªŒï¼ŒéªŒè¯äº†åœ¨67ä¸ªæ•°æ®é›†å’Œ14ä¸ªä»»åŠ¡ä¸­ï¼Œå°½ç®¡åªæœ‰0.7Bå‚æ•°ï¼Œä½†Falconå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/TianHuiLab/Falcon%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E5%AE%8C%E6%95%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%81%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%EF%BC%8C%E5%B8%8C%E6%9C%9B%E6%9C%89%E5%8A%A9%E4%BA%8E%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%BC%80%E5%8F%91%E5%BC%80%E6%BA%90%E7%A4%BE%E5%8C%BA%E3%80%82">https://github.com/TianHuiLab/Falconä¸Šå‘å¸ƒäº†å®Œæ•´çš„æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹æƒé‡ï¼Œå¸Œæœ›æœ‰åŠ©äºè¿›ä¸€æ­¥å¼€å‘å¼€æºç¤¾åŒºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11070v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾é’ˆå¯¹é¥æ„Ÿé¢†åŸŸçš„å…¨æ–°è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹â€”â€”Falconã€‚å®ƒé‡‡ç”¨ç»Ÿä¸€çš„æç¤ºå¼æ¡†æ¶ï¼Œèƒ½æ‰§è¡Œå¤æ‚å¤šæ ·çš„é¥æ„Ÿä»»åŠ¡ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„å›¾åƒã€åŒºåŸŸå’Œåƒç´ çº§åˆ«çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç®€å•çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œé¥æ„Ÿå›¾åƒï¼ŒFalconèƒ½åœ¨14ç§ä¸åŒä»»åŠ¡ä¸­ä»¥æ–‡æœ¬å½¢å¼ç”Ÿæˆä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ä¸ºè®­ç»ƒFalconå¹¶å¢å¼ºå…¶è¡¨ç¤ºèƒ½åŠ›ä»¥ç¼–ç ä¸°å¯Œçš„ç©ºé—´è¯­ä¹‰ä¿¡æ¯ï¼Œå›¢é˜Ÿå¼€å‘äº†Falcon_SFTæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«çº¦7800ä¸‡é«˜è´¨é‡æ•°æ®æ ·æœ¬ï¼Œè¦†ç›–å¤šç©ºé—´åˆ†è¾¨ç‡å’Œå¤šè§†è§’çš„é¥æ„Ÿå›¾åƒï¼Œå¹¶æä¾›å¤šæ ·åŒ–çš„æŒ‡ä»¤ã€‚Falcon_SFTæ•°æ®é›†å…·æœ‰å±‚æ¬¡åŒ–æ³¨é‡Šï¼Œå¹¶ç»è¿‡äººå·¥é‡‡æ ·éªŒè¯ä»¥ç¡®ä¿æ•°æ®è´¨é‡å’Œå¯é æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒFalconåœ¨67ä¸ªæ•°æ®é›†å’Œ14ä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå®ƒçš„å‚æ•°è§„æ¨¡ä»…æœ‰0.7Bã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Falconæ˜¯ä¸€ä¸ªé’ˆå¯¹é¥æ„Ÿé¢†åŸŸçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œå…·å¤‡å¼ºå¤§çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>Falconé‡‡ç”¨æç¤ºå¼æ¡†æ¶ï¼Œèƒ½æ‰§è¡Œå¤æ‚å¤šæ ·çš„é¥æ„Ÿä»»åŠ¡ã€‚</li>
<li>Falconèƒ½é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œé¥æ„Ÿå›¾åƒï¼Œåœ¨å¤šç§ä»»åŠ¡ä¸­ç”Ÿæˆæ–‡æœ¬ç»“æœã€‚</li>
<li>ä¸ºè®­ç»ƒFalconï¼Œå¼€å‘äº†å¤§å‹å¤šä»»åŠ¡çš„Falcon_SFTæ•°æ®é›†ã€‚</li>
<li>Falcon_SFTæ•°æ®é›†åŒ…å«é«˜è´¨é‡æ•°æ®æ ·æœ¬ï¼Œè¦†ç›–å¤šç§é¥æ„Ÿå›¾åƒï¼Œå…·æœ‰å±‚æ¬¡åŒ–æ³¨é‡Šå’Œäººå·¥éªŒè¯ã€‚</li>
<li>Falconåœ¨å¤šä¸ªæ•°æ®é›†å’Œä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå‚æ•°è§„æ¨¡è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-33504beacbb170eda88331525efcb1da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e0b96d8e08091fcd3732edca7d18dcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c6faffe783053ca95b7c40a4853c74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98d73e3842ecc217667f058374b8ad99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3362166d5b933dcc990b7626d73a8265.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6476b8bf9370aebbe30cc8efb899b905.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Predicting-Stock-Movement-with-BERTweet-and-Transformers"><a href="#Predicting-Stock-Movement-with-BERTweet-and-Transformers" class="headerlink" title="Predicting Stock Movement with BERTweet and Transformers"></a>Predicting Stock Movement with BERTweet and Transformers</h2><p><strong>Authors:Michael Charles Albada, Mojolaoluwa Joshua Sonola</strong></p>
<p>Applying deep learning and computational intelligence to finance has been a popular area of applied research, both within academia and industry, and continues to attract active attention. The inherently high volatility and non-stationary of the data pose substantial challenges to machine learning models, especially so for todayâ€™s expressive and highly-parameterized deep learning models. Recent work has combined natural language processing on data from social media to augment models based purely on historic price data to improve performance has received particular attention. Previous work has achieved state-of-the-art performance on this task by combining techniques such as bidirectional GRUs, variational autoencoders, word and document embeddings, self-attention, graph attention, and adversarial training. In this paper, we demonstrated the efficacy of BERTweet, a variant of BERT pre-trained specifically on a Twitter corpus, and the transformer architecture by achieving competitive performance with the existing literature and setting a new baseline for Matthews Correlation Coefficient on the Stocknet dataset without auxiliary data sources. </p>
<blockquote>
<p>å°†æ·±åº¦å­¦ä¹ è®¡ç®—æ™ºèƒ½åº”ç”¨äºé‡‘èé¢†åŸŸä¸€ç›´æ˜¯å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œåº”ç”¨ç ”ç©¶çš„çƒ­é—¨é¢†åŸŸï¼Œå¹¶ä¸”ä¸æ–­å¼•èµ·å…³æ³¨ã€‚æ•°æ®çš„é«˜å†…åœ¨æ³¢åŠ¨æ€§å’Œéå¹³ç¨³æ€§ç»™æœºå™¨å­¦ä¹ æ¨¡å‹å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å½“ä»Šå¤æ‚ä¸”é«˜åº¦å‚æ•°åŒ–çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¿‘æœŸçš„ç ”ç©¶ç»“åˆäº†ç¤¾äº¤åª’ä½“æ•°æ®çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œä»¥æ‰©å……ä»…åŸºäºå†å²ä»·æ ¼æ•°æ®çš„æ¨¡å‹ï¼Œä»¥æé«˜æ€§èƒ½ï¼Œè¿™ä¸€ç ”ç©¶å¼•èµ·äº†ç‰¹åˆ«çš„å…³æ³¨ã€‚å…ˆå‰çš„ç ”ç©¶é€šè¿‡ç»“åˆè¯¸å¦‚åŒå‘GRUã€å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ã€å•è¯å’Œæ–‡æ¡£åµŒå…¥ã€è‡ªæ³¨æ„åŠ›ã€å›¾æ³¨æ„åŠ›å’Œå¯¹æŠ—è®­ç»ƒç­‰æŠ€æœ¯ï¼Œåœ¨æ­¤ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†BERTweetï¼ˆä¸€ç§åœ¨Twitterè¯­æ–™åº“ä¸Šä¸“é—¨è¿›è¡Œé¢„è®­ç»ƒçš„BERTå˜ä½“ï¼‰å’ŒåŸºäºTransformeræ¶æ„çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡å®ç°ä¸ç°æœ‰æ–‡çŒ®å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ä»¥åŠåœ¨ä¸ä½¿ç”¨è¾…åŠ©æ•°æ®æºçš„æƒ…å†µä¸‹åœ¨Stocknetæ•°æ®é›†ä¸Šä¸º Matthews Correlation Coefficientè®¾ç«‹æ–°çš„åŸºå‡†çº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10957v1">PDF</a> 9 pages, 4 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ åŠè®¡ç®—æ™ºèƒ½åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨ç ”ç©¶å¤‡å—å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œã€‚é‡‘èæ•°æ®çš„é«˜æ³¢åŠ¨æ€§å’Œéå¹³ç¨³æ€§ç»™æœºå™¨å­¦ä¹ æ¨¡å‹å¸¦æ¥æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç ”ç©¶ç»“åˆäº†ç¤¾äº¤åª’ä½“æ•°æ®è‡ªç„¶è¯­è¨€å¤„ç†å¢å¼ºæ¨¡å‹æ€§èƒ½ï¼Œåœ¨çº¯å†å²ä»·æ ¼æ•°æ®åŸºç¡€ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚æœ¬æ–‡å±•ç¤ºäº†BERTweetæ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œè¯¥æ¨¡å‹åœ¨Twitterè¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡å˜å‹å™¨æ¶æ„åœ¨Stocknetæ•°æ®é›†ä¸Šå®ç°äº†ä¸ç°æœ‰æ–‡çŒ®ç›¸å½“çš„ç«äº‰åŠ›è¡¨ç°ï¼Œæ— éœ€è¾…åŠ©æ•°æ®æºå³å¯è®¾ç½®æ–°çš„Matthewsç›¸å…³ç³»æ•°åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åŠè®¡ç®—æ™ºèƒ½åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨æ˜¯å½“å‰çƒ­é—¨ç ”ç©¶é¢†åŸŸã€‚</li>
<li>é‡‘èæ•°æ®çš„é«˜æ³¢åŠ¨æ€§å’Œéå¹³ç¨³æ€§ç»™æœºå™¨å­¦ä¹ æ¨¡å‹å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>ç»“åˆç¤¾äº¤åª’ä½“æ•°æ®çš„è‡ªç„¶è¯­è¨€å¤„ç†å¯å¢å¼ºé‡‘èé¢†åŸŸæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>BERTweetæ¨¡å‹åœ¨Twitterè¯­æ–™åº“ä¸Šçš„é¢„è®­ç»ƒå¯¹äºé‡‘èæ–‡æœ¬æ•°æ®æ˜¯æœ‰æ•ˆçš„ã€‚</li>
<li>å˜å‹å™¨æ¶æ„åœ¨é‡‘èæ–‡æœ¬æ•°æ®å¤„ç†ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>æ— éœ€è¾…åŠ©æ•°æ®æºï¼ŒBERTweetæ¨¡å‹åœ¨Stocknetæ•°æ®é›†ä¸Šå®ç°äº†æ–°çš„Matthewsç›¸å…³ç³»æ•°åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be897fe391d8f2a3e0584511a08939d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ffe532c30800af6dbcb4c2011a209da6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-063b4edd3656e4dfa98a1ecf2e11ace6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e1596862de4719d5ecb9b744a9bb8c3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ChatGPT-Encounters-Morphing-Attack-Detection-Zero-Shot-MAD-with-Multi-Modal-Large-Language-Models-and-General-Vision-Models"><a href="#ChatGPT-Encounters-Morphing-Attack-Detection-Zero-Shot-MAD-with-Multi-Modal-Large-Language-Models-and-General-Vision-Models" class="headerlink" title="ChatGPT Encounters Morphing Attack Detection: Zero-Shot MAD with   Multi-Modal Large Language Models and General Vision Models"></a>ChatGPT Encounters Morphing Attack Detection: Zero-Shot MAD with   Multi-Modal Large Language Models and General Vision Models</h2><p><strong>Authors:Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, Christoph Busch</strong></p>
<p>Face Recognition Systems (FRS) are increasingly vulnerable to face-morphing attacks, prompting the development of Morphing Attack Detection (MAD) algorithms. However, a key challenge in MAD lies in its limited generalizability to unseen data and its lack of explainability-critical for practical application environments such as enrolment stations and automated border control systems. Recognizing that most existing MAD algorithms rely on supervised learning paradigms, this work explores a novel approach to MAD using zero-shot learning leveraged on Large Language Models (LLMs). We propose two types of zero-shot MAD algorithms: one leveraging general vision models and the other utilizing multimodal LLMs. For general vision models, we address the MAD task by computing the mean support embedding of an independent support set without using morphed images. For the LLM-based approach, we employ the state-of-the-art GPT-4 Turbo API with carefully crafted prompts. To evaluate the feasibility of zero-shot MAD and the effectiveness of the proposed methods, we constructed a print-scan morph dataset featuring various unseen morphing algorithms, simulating challenging real-world application scenarios. Experimental results demonstrated notable detection accuracy, validating the applicability of zero-shot learning for MAD tasks. Additionally, our investigation into LLM-based MAD revealed that multimodal LLMs, such as ChatGPT, exhibit remarkable generalizability to untrained MAD tasks. Furthermore, they possess a unique ability to provide explanations and guidance, which can enhance transparency and usability for end-users in practical applications. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«ç³»ç»Ÿï¼ˆFRSï¼‰è¶Šæ¥è¶Šå®¹æ˜“å—åˆ°äººè„¸å˜å½¢æ”»å‡»çš„å½±å“ï¼Œè¿™ä¿ƒä½¿äº†å˜å½¢æ”»å‡»æ£€æµ‹ï¼ˆMADï¼‰ç®—æ³•çš„å‘å±•ã€‚ç„¶è€Œï¼ŒMADçš„å…³é”®æŒ‘æˆ˜åœ¨äºå…¶å¯¹æœªè§æ•°æ®çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œä»¥åŠåœ¨å®é™…åº”ç”¨ç¯å¢ƒï¼ˆå¦‚æ³¨å†Œç«™å’Œè‡ªåŠ¨è¾¹å¢ƒæ§åˆ¶ç³»ç»Ÿï¼‰ä¸­ç¼ºä¹å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p>æ„è¯†åˆ°å¤§å¤šæ•°ç°æœ‰çš„MADç®—æ³•éƒ½ä¾èµ–äºç›‘ç£å­¦ä¹ æ¨¡å¼ï¼Œè¿™é¡¹å·¥ä½œæ¢ç´¢äº†ä¸€ç§ä½¿ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é›¶å°„å‡»å­¦ä¹ çš„æ–°æ–¹æ³•æ¥è¿›è¡ŒMADã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç±»å‹çš„é›¶å°„å‡»MADç®—æ³•ï¼šä¸€ç§åˆ©ç”¨é€šç”¨è§†è§‰æ¨¡å‹ï¼Œå¦ä¸€ç§åˆ©ç”¨å¤šæ¨¡æ€LLMã€‚å¯¹äºé€šç”¨è§†è§‰æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—ç‹¬ç«‹æ”¯æŒé›†çš„å‡å€¼æ”¯æŒåµŒå…¥æ¥è§£å†³MADä»»åŠ¡ï¼Œè€Œä¸ä½¿ç”¨åˆæˆçš„å›¾åƒã€‚å¯¹äºåŸºäºLLMçš„æ–¹æ³•ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æœ€å…ˆè¿›çš„GPT-4 Turbo APIå’Œç²¾å¿ƒè®¾è®¡çš„æç¤ºã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10937v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>äººè„¸è¯†åˆ«ç³»ç»Ÿï¼ˆFRSï¼‰æ˜“å—é¢å­”å˜å½¢æ”»å‡»å½±å“ï¼Œä¿ƒä½¿ç ”å‘é¢å‘å˜å½¢æ”»å‡»æ£€æµ‹çš„ç®—æ³•ã€‚ä½†å…³é”®åœ¨äºè¯¥æ£€æµ‹æ–¹æ³•çš„æœ‰é™æ³›åŒ–èƒ½åŠ›ä¸è§£é‡Šæ€§çš„ç¼ºä¹ï¼Œå¯¹å®æˆ˜ç¯å¢ƒä¸‹å¦‚æŠ¥åˆ°ç«™ã€è‡ªåŠ¨åŒ–è¾¹å¢ƒæ§åˆ¶ç³»ç»Ÿç­‰å®é™…åº”ç”¨åœºæ™¯é€ æˆå½±å“ã€‚æœ¬ç ”ç©¶æ„è¯†åˆ°å¤šæ•°ç°æœ‰çš„å˜å½¢æ”»å‡»æ£€æµ‹ç®—æ³•ä¾èµ–ç›‘ç£å­¦ä¹ æ¨¡å¼ï¼Œäºæ˜¯æ¢ç´¢é‡‡ç”¨åŸºäºé›¶å°„å‡»å­¦ä¹ åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°æ–¹æ³•ã€‚æœ¬ç ”ç©¶æå‡ºä¸¤ç§é›¶å°„å‡»å˜å½¢æ”»å‡»æ£€æµ‹ç®—æ³•ï¼šä¸€ç§åˆ©ç”¨é€šç”¨è§†è§‰æ¨¡å‹ï¼Œå¦ä¸€ç§åˆ™é‡‡ç”¨å¤šæ¨¡æ€LLMã€‚å¯¹äºé€šç”¨è§†è§‰æ¨¡å‹ï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—ç‹¬ç«‹æ”¯æŒé›†çš„å‡å€¼æ”¯æŒåµŒå…¥æ¥è§£å†³å˜å½¢æ”»å‡»æ£€æµ‹ä»»åŠ¡ï¼Œè€Œä¸ä½¿ç”¨å˜å½¢å›¾åƒã€‚å¯¹äºåŸºäºLLMçš„æ–¹æ³•ï¼Œæˆ‘ä»¬é‡‡ç”¨æœ€å…ˆè¿›çš„GPT-4 Turbo APIä¸ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯ã€‚ä¸ºäº†è¯„ä¼°é›¶å°„å‡»å˜å½¢æ”»å‡»æ£€æµ‹çš„å¯è¡Œæ€§ä»¥åŠæ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«å„ç§æœªè§è¿‡çš„å˜å½¢ç®—æ³•çš„é¢æ‰«ææ‰“å°å˜å½¢æ•°æ®é›†ï¼Œæ¨¡æ‹Ÿå…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®åº”ç”¨åœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºæ˜¾è‘—çš„æ£€æµ‹å‡†ç¡®æ€§ï¼ŒéªŒè¯äº†é›¶å°„å‡»å­¦ä¹ åœ¨å˜å½¢æ”»å‡»æ£€æµ‹ä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹åŸºäºLLMçš„å˜å½¢æ”»å‡»æ£€æµ‹çš„ç ”ç©¶è¡¨æ˜ï¼Œå¦‚ChatGPTç­‰å¤šæ¨¡æ€LLMåœ¨æœªç»è®­ç»ƒçš„å˜å½¢æ”»å‡»æ£€æµ‹ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºéå‡¡çš„æ³›åŒ–èƒ½åŠ›ã€‚è€Œä¸”ï¼Œå®ƒä»¬è¿˜å…·æœ‰æä¾›è§£é‡Šå’ŒæŒ‡å¯¼çš„ç‹¬ç‰¹èƒ½åŠ›ï¼Œå¯ä»¥æé«˜å®é™…åº”ç”¨ä¸­å¯¹æœ€ç»ˆç”¨æˆ·çš„é€æ˜åº¦å’Œå¯ç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>äººè„¸è¯†åˆ«ç³»ç»Ÿé¢ä¸´é¢å­”å˜å½¢æ”»å‡»çš„é£é™©ï¼Œéœ€è¦å¼€å‘æœ‰æ•ˆçš„å˜å½¢æ”»å‡»æ£€æµ‹ç®—æ³•ã€‚</li>
<li>å½“å‰å¤§å¤šæ•°å˜å½¢æ”»å‡»æ£€æµ‹ç®—æ³•ä¾èµ–äºç›‘ç£å­¦ä¹ æ¨¡å¼ï¼Œå­˜åœ¨æ³›åŒ–èƒ½åŠ›å’Œè§£é‡Šæ€§çš„å±€é™æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†åŸºäºé›¶å°„å‡»å­¦ä¹ çš„å˜å½¢æ”»å‡»æ£€æµ‹æ–°æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯†åˆ«ã€‚</li>
<li>æå‡ºä¸¤ç§é›¶å°„å‡»æ£€æµ‹ç®—æ³•ï¼šåŸºäºé€šç”¨è§†è§‰æ¨¡å‹çš„æ–¹æ³•å’ŒåŸºäºå¤šæ¨¡æ€LLMçš„æ–¹æ³•ã€‚</li>
<li>é€šç”¨è§†è§‰æ¨¡å‹é€šè¿‡è®¡ç®—æ”¯æŒé›†çš„å‡å€¼æ”¯æŒåµŒå…¥æ¥æ£€æµ‹å˜å½¢æ”»å‡»ï¼Œæ— éœ€ä½¿ç”¨å˜å½¢å›¾åƒã€‚</li>
<li>ä½¿ç”¨GPT-4 Turbo APIå’Œç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯è¿›è¡ŒLLMå®éªŒï¼Œè¯æ˜äº†é›¶å°„å‡»æ£€æµ‹çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b1862bd2ff514a07c63406dafe8c71e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ec86cdb6d80085054128327967c72c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b187e68033a45f1b3fdeedadac4eb76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ea6fea4cfcd52470cf84b3d4ae9dc56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccaef5bf8d0da14a152c68ed29fa3e8d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Chat-TS-Enhancing-Multi-Modal-Reasoning-Over-Time-Series-and-Natural-Language-Data"><a href="#Chat-TS-Enhancing-Multi-Modal-Reasoning-Over-Time-Series-and-Natural-Language-Data" class="headerlink" title="Chat-TS: Enhancing Multi-Modal Reasoning Over Time-Series and Natural   Language Data"></a>Chat-TS: Enhancing Multi-Modal Reasoning Over Time-Series and Natural   Language Data</h2><p><strong>Authors:Paul Quinlan, Qingguo Li, Xiaodan Zhu</strong></p>
<p>Time-series analysis is critical for a wide range of fields such as healthcare, finance, transportation, and energy, among many others. The practical applications often involve analyzing time-series data alongside contextual information in the form of natural language to support informed decisions. However, current time-series models are limited in their ability to perform reasoning that involves both time-series and their textual content. In this work, we address this gap by introducing \textit{Chat-TS}, a large language model (LLM) based framework, designed to support reasoning over time series and textual data. Unlike traditional models, Chat-TS integrates time-series tokens into LLMsâ€™ vocabulary, enhancing its reasoning ability over both modalities without compromising the core natural language capabilities, enabling practical analysis and reasoning across modalities. To support learning and evaluation in this setup, we contribute new datasets: the \textit{TS Instruct Training Dataset} which pairs diverse time-series data with relevant text instructions and responses for instruction tuning, the \textit{TS Instruct Question and Answer (QA) Gold Dataset} which provides multiple-choice questions designed to evaluate multimodal reasoning, and a \textit{TS Instruct Quantitative Probing Set} which contains a small subset of the TS Instruct QA tasks alongside math and decision-making questions for LLM evaluation. We designed a training strategy to preserve the inherent reasoning capabilities of LLMs while augmenting them for time-series reasoning. Experiments show that Chat-TS achieves state-of-the-art performance in multi-modal reasoning tasks by maintaining strong natural language proficiency while improving time-series reasoning. ~\footnote{To ensure replicability and facilitate future research, all models, datasets, and code will be available at [\texttt{Github-URL}].} </p>
<blockquote>
<p>æ—¶é—´åºåˆ—åˆ†æåœ¨åŒ»ç–—ä¿å¥ã€é‡‘èã€äº¤é€šã€èƒ½æºç­‰ä¼—å¤šé¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨ã€‚å®é™…åº”ç”¨ä¸­ï¼Œå¸¸ä¸è‡ªç„¶è¯­è¨€å½¢å¼çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸€èµ·åˆ†ææ—¶é—´åºåˆ—æ•°æ®ï¼Œä»¥æ”¯æŒå†³ç­–ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ—¶é—´åºåˆ—æ¨¡å‹åœ¨æ¶‰åŠæ—¶é—´åºåˆ—åŠå…¶æ–‡æœ¬å†…å®¹çš„æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„Chat-TSæ¡†æ¶æ¥è§£å†³è¿™ä¸€å·®è·ã€‚Chat-TSæ—¨åœ¨æ”¯æŒæ—¶é—´åºåˆ—å’Œæ–‡æœ¬æ•°æ®çš„æ¨ç†ã€‚ä¸ä¼ ç»Ÿæ¨¡å‹ä¸åŒï¼ŒChat-TSå°†æ—¶é—´åºåˆ—æ ‡è®°é›†æˆåˆ°LLMè¯æ±‡è¡¨ä¸­ï¼Œåœ¨ä¸æŸå®³æ ¸å¿ƒè‡ªç„¶è¯­è¨€åŠŸèƒ½çš„æƒ…å†µä¸‹ï¼Œå¢å¼ºå…¶åœ¨ä¸¤ç§æ¨¡å¼ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œä»è€Œå®ç°è·¨æ¨¡å¼çš„å®ç”¨åˆ†æå’Œæ¨ç†ã€‚ä¸ºäº†æ”¯æŒåœ¨æ­¤è®¾ç½®ä¸­çš„å­¦ä¹ å’Œè¯„ä¼°ï¼Œæˆ‘ä»¬æä¾›äº†æ–°æ•°æ®é›†ï¼šå°†å¤šæ ·åŒ–æ—¶é—´åºåˆ—æ•°æ®ä¸ç›¸å…³æ–‡æœ¬æŒ‡ä»¤å’Œå“åº”ç›¸åŒ¹é…çš„TS Instructè®­ç»ƒæ•°æ®é›†ï¼Œç”¨äºæŒ‡ä»¤å¾®è°ƒï¼›æ—¨åœ¨è¯„ä¼°å¤šæ¨¡å¼æ¨ç†çš„TS Instructé—®ç­”é»„é‡‘æ•°æ®é›†ï¼›ä»¥åŠåŒ…å«ä¸€å°éƒ¨åˆ†TS Instruct QAä»»åŠ¡çš„TS Instructå®šé‡æ¢æµ‹é›†ä»¥åŠç”¨äºLLMè¯„ä¼°çš„æ•°å­¦å’Œå†³ç­–é—®é¢˜ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œä»¥ä¿ç•™LLMçš„å›ºæœ‰æ¨ç†èƒ½åŠ›å¹¶å¢å¼ºæ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒChat-TSåœ¨å¤šæ¨¡å¼æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢ä¿æŒäº†å¼ºå¤§èƒ½åŠ›çš„åŒæ—¶æé«˜äº†æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›ã€‚ä¸ºç¡®ä¿å¯å¤åˆ¶æ€§å’Œä¿ƒè¿›æœªæ¥ç ”ç©¶ï¼Œæ‰€æœ‰æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç éƒ½å°†åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10883v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ—¶é—´åºåˆ—ä¸æ–‡æœ¬æ•°æ®æ¨ç†æ¡†æ¶Chat-TSã€‚é€šè¿‡æ•´åˆæ—¶é—´åºåˆ—æ ‡è®°ç¬¦å·è¿›LLMçš„è¯æ±‡è¡¨ä¸­ï¼ŒChat-TSå¢å¼ºäº†å…¶åœ¨ä¸¤ç§æ¨¡æ€æ•°æ®ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¸æŸå®³å…¶æ ¸å¿ƒçš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚ä¸ºæ”¯æŒåœ¨è¿™ç§è®¾ç½®ä¸‹çš„å­¦ä¹ å’Œè¯„ä¼°ï¼Œæœ¬æ–‡è´¡çŒ®äº†æ–°çš„æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨ä¿ç•™LLMçš„å†…åœ¨æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå¢å¼ºå®ƒä»¬çš„æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒChat-TSåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—åˆ†æåœ¨å¤šä¸ªé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ä¿å¥ã€é‡‘èã€äº¤é€šå’Œèƒ½æºç­‰ï¼‰å…·æœ‰å¹¿æ³›åº”ç”¨ï¼Œæ¶‰åŠæ—¶é—´åºåˆ—æ•°æ®ä¸è‡ªç„¶è¯­è¨€å½¢å¼ä¸Šä¸‹æ–‡ä¿¡æ¯çš„åˆ†æï¼Œä»¥æ”¯æŒå†³ç­–åˆ¶å®šã€‚</li>
<li>å½“å‰çš„æ—¶é—´åºåˆ—æ¨¡å‹åœ¨åŒæ—¶å¤„ç†æ—¶é—´åºåˆ—å’Œæ–‡æœ¬å†…å®¹çš„æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Chat-TSæ¡†æ¶æ—¨åœ¨æ”¯æŒæ—¶é—´åºåˆ—å’Œæ–‡æœ¬æ•°æ®çš„æ¨ç†ï¼Œé€šè¿‡æ•´åˆæ—¶é—´åºåˆ—æ ‡è®°è¿›LLMçš„è¯æ±‡è¡¨ä¸­ï¼Œå¢å¼ºäº†å…¶åœ¨ä¸¤ç§æ¨¡æ€æ•°æ®ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†æ–°çš„æ•°æ®é›†ä»¥æ”¯æŒChat-TSæ¡†æ¶çš„å­¦ä¹ ä¸è¯„ä¼°ï¼ŒåŒ…æ‹¬TS Instructè®­ç»ƒæ•°æ®é›†ã€TS Instructé—®ç­”ï¼ˆQAï¼‰é»„é‡‘æ•°æ®é›†ä»¥åŠTS Instructå®šé‡æ¢æµ‹é›†ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨ä¿ç•™LLMçš„å†…åœ¨æ¨ç†èƒ½åŠ›çš„åŒæ—¶å¢å¼ºå…¶æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChat-TSåœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›çš„åŒæ—¶æé«˜æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-04bd58c51633b226dfcc75ebf6f2bb16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc6c05ba14a7f603a032caf69ef98f85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-281d0a97a4734e18fc83efcc46a60a01.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8165d394feadcee13c682e3d8cb560b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-491ec84055dc938c15282347e4c5bc6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c866f5701a23d5ba62c9a701382983a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed26452a47bc1b4b743a68209301eeed.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="On-the-Limitations-of-Vision-Language-Models-in-Understanding-Image-Transforms"><a href="#On-the-Limitations-of-Vision-Language-Models-in-Understanding-Image-Transforms" class="headerlink" title="On the Limitations of Vision-Language Models in Understanding Image   Transforms"></a>On the Limitations of Vision-Language Models in Understanding Image   Transforms</h2><p><strong>Authors:Ahmad Mustafa Anis, Hasnain Ali, Saquib Sarfraz</strong></p>
<p>Vision Language Models (VLMs) have demonstrated significant potential in various downstream tasks, including Image&#x2F;Video Generation, Visual Question Answering, Multimodal Chatbots, and Video Understanding. However, these models often struggle with basic image transformations. This paper investigates the image-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by Google. Our findings reveal that these models lack comprehension of multiple image-level augmentations. To facilitate this study, we created an augmented version of the Flickr8k dataset, pairing each image with a detailed description of the applied transformation. We further explore how this deficiency impacts downstream tasks, particularly in image editing, and evaluate the performance of state-of-the-art Image2Image models on simple transformations. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼ŒåŒ…æ‹¬å›¾åƒ&#x2F;è§†é¢‘ç”Ÿæˆã€è§†è§‰é—®ç­”ã€å¤šæ¨¡æ€èŠå¤©æœºå™¨äººå’Œè§†é¢‘ç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åŸºæœ¬çš„å›¾åƒè½¬æ¢æ–¹é¢ç»å¸¸é‡åˆ°å›°éš¾ã€‚æœ¬æ–‡ç ”ç©¶äº†VLMsçš„å›¾åƒçº§åˆ«ç†è§£ï¼Œç‰¹åˆ«æ˜¯OpenAIçš„CLIPå’ŒGoogleçš„SigLIPã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹ç¼ºä¹å¯¹å›¾åƒçº§åˆ«å¢å¼ºçš„ç†è§£ã€‚ä¸ºäº†æ¨åŠ¨è¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬åˆ›å»ºäº†Flickr8kæ•°æ®é›†çš„å¢å¼ºç‰ˆæœ¬ï¼Œä¸ºæ¯ä¸ªå›¾åƒæä¾›äº†æ‰€åº”ç”¨è½¬æ¢çš„è¯¦ç»†æè¿°ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ¢è®¨äº†è¿™ç§ç¼ºé™·å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒç¼–è¾‘æ–¹é¢ï¼Œå¹¶è¯„ä¼°äº†æœ€æ–°Image2Imageæ¨¡å‹åœ¨ç®€å•è½¬æ¢ä¸Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09837v2">PDF</a> 8 pages, 15 images</p>
<p><strong>Summary</strong><br>     è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼ŒåŒ…æ‹¬å›¾åƒ&#x2F;è§†é¢‘ç”Ÿæˆã€è§†è§‰é—®ç­”ã€å¤šæ¨¡æ€èŠå¤©æœºå™¨äººå’Œè§†é¢‘ç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨åŸºæœ¬å›¾åƒè½¬æ¢æ–¹é¢å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚æœ¬æ–‡ç ”ç©¶äº†VLMsçš„å›¾åƒçº§åˆ«ç†è§£ï¼Œç‰¹åˆ«æ˜¯OpenAIçš„CLIPå’ŒGoogleçš„SigLIPã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹ç¼ºä¹å¯¹å›¾åƒçº§åˆ«å¢å¼ºçš„ç†è§£ã€‚ä¸ºäº†æ¨åŠ¨è¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬åˆ›å»ºäº†Flickr8kæ•°æ®é›†çš„å¢å¼ºç‰ˆæœ¬ï¼Œä¸ºæ¯ä¸ªå›¾åƒé…å¤‡è¯¦ç»†çš„è½¬æ¢æè¿°ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†è¿™ç§ç¼ºé™·å¯¹ä¸‹æ¸¸ä»»åŠ¡ã€å°¤å…¶æ˜¯å›¾åƒç¼–è¾‘çš„å½±å“ï¼Œå¹¶è¯„ä¼°äº†æœ€å…ˆè¿›çš„Image2Imageæ¨¡å‹åœ¨ç®€å•è½¬æ¢æ–¹é¢çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>VLMsåœ¨åŸºæœ¬å›¾åƒè½¬æ¢æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶äº†VLMsçš„å›¾åƒçº§åˆ«ç†è§£ï¼Œç‰¹åˆ«æ˜¯CLIPå’ŒSigLIPæ¨¡å‹ã€‚</li>
<li>è¿™äº›æ¨¡å‹ç¼ºä¹å¯¹å›¾åƒçº§åˆ«å¢å¼ºçš„ç†è§£ã€‚</li>
<li>ä¸ºäº†æ¨åŠ¨ç ”ç©¶ï¼Œåˆ›å»ºäº†Flickr8kæ•°æ®é›†çš„å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«å›¾åƒè½¬æ¢æè¿°ã€‚</li>
<li>è¿™ç§ç¼ºé™·å½±å“ä¸‹æ¸¸ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯å›¾åƒç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3deba1a6af4d3157274bdf0405194cba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c454940760913645b686754cbcd6a46d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa6ecc18507cccac1927dc33660f3a38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87db6d2273c54cbdeae08981d6ed6c15.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b9856e4640fd98b698ce3751e3546eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2731ce35c28573ac02b6438fa41a7d5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b49fd19c024995dbc350d070f825249b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Implicit-Reasoning-in-Transformers-is-Reasoning-through-Shortcuts"><a href="#Implicit-Reasoning-in-Transformers-is-Reasoning-through-Shortcuts" class="headerlink" title="Implicit Reasoning in Transformers is Reasoning through Shortcuts"></a>Implicit Reasoning in Transformers is Reasoning through Shortcuts</h2><p><strong>Authors:Tianhe Lin, Jian Xie, Siyu Yuan, Deqing Yang</strong></p>
<p>Test-time compute is emerging as a new paradigm for enhancing language modelsâ€™ complex multi-step reasoning capabilities, as demonstrated by the success of OpenAIâ€™s o1 and o3, as well as DeepSeekâ€™s R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on unfixed-pattern data tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization. </p>
<blockquote>
<p>æµ‹è¯•æ—¶çš„è®¡ç®—æ­£æˆä¸ºå¢å¼ºè¯­è¨€æ¨¡å‹å¤æ‚å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›çš„æ–°èŒƒå¼ï¼ŒOpenAIçš„o1å’Œo3ä»¥åŠDeepSeekçš„R1çš„æˆåŠŸæ¼”ç¤ºäº†è¿™ä¸€ç‚¹ã€‚ä¸æµ‹è¯•æ—¶è®¡ç®—ä¸­çš„æ˜¾å¼æ¨ç†ç›¸æ¯”ï¼Œéšå¼æ¨ç†çš„æ¨ç†æ•ˆç‡æ›´é«˜ï¼Œç”Ÿæˆçš„æ ‡è®°æ›´å°‘ã€‚ç„¶è€Œï¼Œä¸ºä»€ä¹ˆå…ˆè¿›çš„æ¨ç†èƒ½åŠ›æ²¡æœ‰åœ¨éšå¼æ¨ç†é£æ ¼ä¸­å‡ºç°ï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»é›¶å¼€å§‹è®­ç»ƒGPT-2ï¼Œåœ¨ä¸€ä¸ªç²¾é€‰çš„å¤šæ­¥æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒåˆ†æï¼Œä»¥ç ”ç©¶è¯­è¨€æ¨¡å‹å¦‚ä½•åœ¨å¤šæ­¥ä»»åŠ¡ä¸­è¿›è¡Œéšå¼æ¨ç†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†ä»¥ä¸‹å‡ ç‚¹ï¼š1ï¼‰è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡éšå¼æ¨ç†è¿›è¡Œé€æ­¥æ¨ç†ï¼Œå¹¶åœ¨é¢†åŸŸå†…å¤–æµ‹è¯•ä¸­å®ç°é«˜å‡†ç¡®æ€§ã€‚ä½†è¿™ç§èƒ½åŠ›ä»…åœ¨è®­ç»ƒå›ºå®šæ¨¡å¼æ•°æ®æ—¶å‡ºç°ã€‚2ï¼‰ç›¸åï¼Œä»è®­ç»ƒéå›ºå®šæ¨¡å¼æ•°æ®ä¸­æ¶Œç°çš„éšå¼æ¨ç†èƒ½åŠ›å¾€å¾€å€¾å‘äºè¿‡åº¦é€‚åº”ç‰¹å®šæ¨¡å¼ï¼Œè€Œæ— æ³•è¿›ä¸€æ­¥æ¨å¹¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸€å±€é™æ€§ä¹Ÿè¢«è§‚å¯Ÿåˆ°åœ¨æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œè¯­è¨€æ¨¡å‹é€šè¿‡æ·å¾„å­¦ä¹ è·å¾—éšå¼æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å…·æœ‰ç›¸ä¼¼æ¨¡å¼çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07604v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶è®¡ç®—ï¼ˆtest-time computeï¼‰è¿™ä¸€æ–°å…´èŒƒå¼åœ¨å¢å¼ºè¯­è¨€æ¨¡å‹å¤æ‚å¤šæ­¥æ¨ç†èƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚é€šè¿‡å¯¹GPT-2è¿›è¡Œè®­ç»ƒå’Œå®éªŒåˆ†æï¼Œå‘ç°è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡éšå¼æ¨ç†å®Œæˆå¤šæ­¥ä»»åŠ¡ï¼Œå¹¶åœ¨å›ºå®šæ¨¡å¼æ•°æ®ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œå½“è®­ç»ƒæ•°æ®æ¨¡å¼ä¸å›ºå®šæ—¶ï¼Œéšå¼æ¨ç†èƒ½åŠ›å®¹æ˜“è¿‡åº¦æ‹Ÿåˆç‰¹å®šæ¨¡å¼ï¼Œç¼ºä¹è¿›ä¸€æ­¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™è¡¨æ˜è¯­è¨€æ¨¡å‹çš„éšå¼æ¨ç†æ˜¯é€šè¿‡æ·å¾„å­¦ä¹ è·å¾—çš„ï¼Œèƒ½å¤Ÿåœ¨ç±»ä¼¼çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶è®¡ç®—æ˜¯å¢å¼ºè¯­è¨€æ¨¡å‹å¤šæ­¥æ¨ç†èƒ½åŠ›çš„æ–°å…´èŒƒå¼ã€‚</li>
<li>éšå¼æ¨ç†ç›¸æ¯”æ˜¾å¼æ¨ç†æ›´æ¨ç†é«˜æ•ˆï¼Œéœ€è¦ç”Ÿæˆçš„æ ‡è®°æ›´å°‘ã€‚</li>
<li>è¯­è¨€æ¨¡å‹å¯ä»¥é€šè¿‡éšå¼æ¨ç†å®Œæˆå¤šæ­¥ä»»åŠ¡ï¼Œå¹¶åœ¨å›ºå®šæ¨¡å¼æ•°æ®ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ã€‚</li>
<li>åœ¨è®­ç»ƒæ•°æ®æ¨¡å¼ä¸å›ºå®šçš„æƒ…å†µä¸‹ï¼Œè¯­è¨€æ¨¡å‹çš„éšå¼æ¨ç†èƒ½åŠ›å®¹æ˜“è¿‡åº¦æ‹Ÿåˆç‰¹å®šæ¨¡å¼ã€‚</li>
<li>éšå¼æ¨ç†èƒ½åŠ›çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œå³ä½¿åœ¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</li>
<li>è¯­è¨€æ¨¡å‹çš„éšå¼æ¨ç†èƒ½åŠ›æ˜¯é€šè¿‡æ·å¾„å­¦ä¹ è·å¾—çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdb334ba62f5e99826ad4064f8999ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-507474786ef57422c2b87a7dce1ab786.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa7bf03854d50d9b43fcb9df5f15e3bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95c10904308ffea12e992ee445cdce34.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da4ae7ec347e475a6ab350797ce3cbef.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Zero-Shot-Anomaly-Detection-and-Reasoning-with-Multimodal-Large-Language-Models"><a href="#Towards-Zero-Shot-Anomaly-Detection-and-Reasoning-with-Multimodal-Large-Language-Models" class="headerlink" title="Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large   Language Models"></a>Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large   Language Models</h2><p><strong>Authors:Jiacong Xu, Shao-Yuan Lo, Bardia Safaei, Vishal M. Patel, Isht Dwivedi</strong></p>
<p>Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the traditional unsupervised AD setting that requires a large number of normal samples to train a model, ZSAD is more practical for handling data-restricted real-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have shown revolutionary reasoning capabilities in various vision tasks. However, the reasoning of image abnormalities remains underexplored due to the lack of corresponding datasets and benchmarks. To facilitate research in AD &amp; reasoning, we establish the first visual instruction tuning dataset, Anomaly-Instruct-125k, and the evaluation benchmark, VisA-D&amp;R. Through investigation with our benchmark, we reveal that current MLLMs like GPT-4o cannot accurately detect and describe fine-grained anomalous details in images. To address this, we propose Anomaly-OneVision (Anomaly-OV), the first specialist visual assistant for ZSAD and reasoning. Inspired by human behavior in visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM) mechanism to adaptively select and emphasize abnormal visual tokens. Extensive experiments demonstrate that Anomaly-OV achieves significant improvements over advanced generalist models in both detection and reasoning. Extensions to medical and 3D AD are provided for future study. The link to our project page: <a target="_blank" rel="noopener" href="https://xujiacong.github.io/Anomaly-OV/">https://xujiacong.github.io/Anomaly-OV/</a> </p>
<blockquote>
<p>é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰æ˜¯ä¸€ç§æ–°å…´çš„å¼‚å¸¸æ£€æµ‹èŒƒå¼ã€‚ä¸åŒäºä¼ ç»Ÿæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹éœ€è¦å¤§é‡æ­£å¸¸æ ·æœ¬è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼ŒZSADåœ¨æ•°æ®å—é™çš„ç°å®åœºæ™¯ä¸­å…·æœ‰æ›´å®é™…çš„é€‚ç”¨æ€§ã€‚æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºé©å‘½æ€§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ç›¸åº”çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œå›¾åƒå¼‚å¸¸çš„æ¨ç†ä»ç„¶é²œæœ‰ç ”ç©¶ã€‚ä¸ºäº†ä¿ƒè¿›å¼‚å¸¸æ£€æµ‹å’Œæ¨ç†çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å»ºç«‹äº†é¦–ä¸ªè§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Anomaly-Instruct-125kå’Œè¯„ä¼°åŸºå‡†VisA-D&amp;Rã€‚é€šè¿‡æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„MLLMæ¨¡å‹å¦‚GPT-4oæ— æ³•å‡†ç¡®æ£€æµ‹å’Œæè¿°å›¾åƒä¸­çš„ç²¾ç»†å¼‚å¸¸ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Anomaly-OneVisionï¼ˆAnomaly-OVï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºZSADå’Œæ¨ç†çš„ä¸“å®¶è§†è§‰åŠ©æ‰‹ã€‚Anomaly-OVå€Ÿé‰´äº†äººç±»åœ¨è§†è§‰æ£€æŸ¥ä¸­çš„è¡Œä¸ºï¼Œåˆ©ç”¨äºŒæ¬¡ç‰¹å¾åŒ¹é…ï¼ˆLTFMï¼‰æœºåˆ¶è‡ªé€‚åº”é€‰æ‹©å’Œå¼ºè°ƒå¼‚å¸¸çš„è§†è§‰æ ‡è®°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAnomaly-OVåœ¨æ£€æµ‹å’Œæ¨ç†æ–¹é¢éƒ½å®ç°äº†å¯¹å…ˆè¿›é€šç”¨æ¨¡å‹çš„æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¿˜ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†åŒ»å­¦å’Œä¸‰ç»´å¼‚å¸¸æ£€æµ‹çš„æ‰©å±•ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢é“¾æ¥ä¸ºï¼š<a target="_blank" rel="noopener" href="https://xujiacong.github.io/Anomaly-OV/">é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07601v2">PDF</a> 19 pages, 10 figures, accepted by CVPR 2025</p>
<p><strong>Summary</strong><br>åŸºäºé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰çš„æ–°å…´å‘å±•è¶‹åŠ¿ï¼Œå½“å‰é¢ä¸´ç¼ºä¹è®­ç»ƒæ•°æ®å’Œå®é™…åº”ç”¨åœºæ™¯çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡å»ºç«‹é¦–ä¸ªè§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†Anomaly-Instruct-125kå’Œè¯„ä¼°åŸºå‡†VisA-D&amp;Rï¼Œæ¨åŠ¨äº†å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹å’Œæ¨ç†æ–¹é¢çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰MLLMsåœ¨å›¾åƒå¼‚å¸¸æ£€æµ‹æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå› æ­¤æå‡ºAnomaly-OneVisionï¼ˆAnomaly-OVï¼‰ä½œä¸ºé¦–ä¸ªé’ˆå¯¹ZSADå’Œæ¨ç†çš„ä¸“å®¶è§†è§‰åŠ©æ‰‹ã€‚Anomaly-OVé‡‡ç”¨çœ‹ä¸¤æ¬¡ç‰¹å¾åŒ¹é…ï¼ˆLTFMï¼‰æœºåˆ¶ï¼Œè‡ªé€‚åº”é€‰æ‹©å’Œå¼ºè°ƒå¼‚å¸¸è§†è§‰æ ‡è®°ï¼Œå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æœªæ¥ç ”ç©¶å°†æ‰©å±•åˆ°åŒ»å­¦å’Œä¸‰ç»´å¼‚å¸¸æ£€æµ‹é¢†åŸŸã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®é¡¹ç›®é¡µé¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ZSADä½œä¸ºä¸€ç§æ–°å…´å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰èŒƒå¼ï¼Œæ›´é€‚ç”¨äºå¤„ç†æ•°æ®å—é™çš„å®é™…æƒ…å†µã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹å’Œæ¨ç†æ–¹é¢å…·æœ‰é©å‘½æ€§æ½œåŠ›ã€‚</li>
<li>å½“å‰å›¾åƒå¼‚å¸¸æ£€æµ‹ç ”ç©¶ä¸­ç¼ºä¹ç›¸åº”æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•é›†ï¼Œé™åˆ¶äº†ç ”ç©¶å‘å±•ã€‚</li>
<li>Anomaly-Instruct-125kæ•°æ®é›†å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œä¿ƒè¿›äº†ADå’Œæ¨ç†ç ”ç©¶çš„è¿›æ­¥ã€‚</li>
<li>å½“å‰MLLMsåœ¨å›¾åƒå¼‚å¸¸æ£€æµ‹æ–¹é¢å­˜åœ¨å‡†ç¡®æ€§é—®é¢˜ï¼Œæ— æ³•å‡†ç¡®æ•æ‰ç»†å¾®çš„å¼‚å¸¸ç»†èŠ‚ã€‚</li>
<li>Anomaly-OneVisionï¼ˆAnomaly-OVï¼‰é€šè¿‡è‡ªé€‚åº”é€‰æ‹©å’Œå¼ºè°ƒå¼‚å¸¸è§†è§‰æ ‡è®°æ¥æé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>Anomaly-OVä½¿ç”¨çœ‹ä¸¤æ¬¡ç‰¹å¾åŒ¹é…ï¼ˆLTFMï¼‰æœºåˆ¶æ¥æ¨¡æ‹Ÿäººç±»è§†è§‰æ£€æŸ¥çš„è¡Œä¸ºæ¨¡å¼ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a66d7de2088908dd3a867da7f055785.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ccea24de69adea98ef180bb29ffe6e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6e01cacb627a2b67b799a98ea163b17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb102e6ee688632fa83304364091f085.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22bf350f7a49c94cf29fdd159937bdb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c8b27acfed861f4829743ca05ed04ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-473fa97609bcb3f7bbebd7e5c65a2810.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="To-Retrieve-or-Not-to-Retrieve-Uncertainty-Detection-for-Dynamic-Retrieval-Augmented-Generation"><a href="#To-Retrieve-or-Not-to-Retrieve-Uncertainty-Detection-for-Dynamic-Retrieval-Augmented-Generation" class="headerlink" title="To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic   Retrieval Augmented Generation"></a>To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic   Retrieval Augmented Generation</h2><p><strong>Authors:Kaustubh D. Dhole</strong></p>
<p>Retrieval-Augmented Generation equips large language models with the capability to retrieve external knowledge, thereby mitigating hallucinations by incorporating information beyond the modelâ€™s intrinsic abilities. However, most prior works have focused on invoking retrieval deterministically, which makes it unsuitable for tasks such as long-form question answering. Instead, dynamically performing retrieval by invoking it only when the underlying LLM lacks the required knowledge can be more efficient. In this context, we delve deeper into the question, â€œTo Retrieve or Not to Retrieve?â€ by exploring multiple uncertainty detection methods. We evaluate these methods for the task of long-form question answering, employing dynamic retrieval, and present our comparisons. Our findings suggest that uncertainty detection metrics, such as Degree Matrix Jaccard and Eccentricity, can reduce the number of retrieval calls by almost half, with only a slight reduction in question-answering accuracy. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹é…å¤‡äº†æ£€ç´¢å¤–éƒ¨çŸ¥è¯†çš„èƒ½åŠ›ï¼Œä»è€Œé€šè¿‡èå…¥æ¨¡å‹è‡ªèº«èƒ½åŠ›ä¹‹å¤–çš„ä¿¡æ¯æ¥æŠ‘åˆ¶å¹»è§‰ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ—©æœŸå·¥ä½œéƒ½é›†ä¸­åœ¨ç¡®å®šæ€§åœ°è°ƒç”¨æ£€ç´¢ä¸Šï¼Œè¿™ä½¿å¾—å®ƒä¸é€‚ç”¨äºé•¿å½¢å¼é—®ç­”ç­‰ä»»åŠ¡ã€‚ç›¸åï¼Œä»…åœ¨åŸºç¡€LLMç¼ºä¹æ‰€éœ€çŸ¥è¯†æ—¶æ‰åŠ¨æ€æ‰§è¡Œæ£€ç´¢ä¼šæ›´åŠ é«˜æ•ˆã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é€šè¿‡æ¢ç´¢å¤šç§ä¸ç¡®å®šæ€§æ£€æµ‹æ–¹æ³•ï¼Œæ·±å…¥æ¢è®¨â€œæ˜¯å¦è¿›è¡Œæ£€ç´¢ï¼Ÿâ€çš„é—®é¢˜ã€‚æˆ‘ä»¬å¯¹é•¿å½¢å¼é—®ç­”ä»»åŠ¡è¯„ä¼°è¿™äº›æ–¹æ³•ï¼Œé‡‡ç”¨åŠ¨æ€æ£€ç´¢ï¼Œå¹¶å±•ç¤ºæˆ‘ä»¬çš„æ¯”è¾ƒç»“æœã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸ç¡®å®šæ€§æ£€æµ‹æŒ‡æ ‡ï¼ˆå¦‚åº¦çŸ©é˜µé›…å¡å°”å’Œç¦»å¿ƒç‡ï¼‰å¯ä»¥å°†æ£€ç´¢è°ƒç”¨æ¬¡æ•°å‡å°‘è¿‘ä¸€åŠï¼Œè€Œé—®ç­”å‡†ç¡®ç‡åªæœ‰è½»å¾®ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09292v3">PDF</a> 1st workshop of â€œQuantify Uncertainty and Hallucination in Foundation   Models: The Next Frontier in Reliable AIâ€ at ICLR 2025</p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿæ£€ç´¢å¤–éƒ¨çŸ¥è¯†ï¼Œä»è€Œèå…¥æ¨¡å‹æœ¬èº«ä¸å…·å¤‡çš„ä¿¡æ¯ï¼Œå‡å°‘è™šæ„å†…å®¹ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶å¤§å¤šé‡‡ç”¨ç¡®å®šæ€§æ£€ç´¢æ–¹å¼ï¼Œè¿™ä¸é€‚ç”¨äºé•¿æ–‡æœ¬é—®ç­”ä»»åŠ¡ã€‚æœ¬æ–‡é€šè¿‡æ¢ç´¢å¤šç§ä¸ç¡®å®šæ€§æ£€æµ‹æ–¹æ³•æ¥åŠ¨æ€æ‰§è¡Œæ£€ç´¢ï¼Œåªåœ¨è¯­è¨€æ¨¡å‹ç¼ºä¹æ‰€éœ€çŸ¥è¯†æ—¶è°ƒç”¨æ£€ç´¢åŠŸèƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ä¸ç¡®å®šæ€§æ£€æµ‹æŒ‡æ ‡ï¼ˆå¦‚åº¦æ•°çŸ©é˜µJaccardå’Œç¦»å¿ƒç‡ï¼‰å¯ä»¥å‡å°‘è¿‘ä¸€åŠçš„æ£€ç´¢è°ƒç”¨æ¬¡æ•°ï¼ŒåŒæ—¶ä»…ç•¥å¾®é™ä½é—®ç­”å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿèå…¥å¤–éƒ¨çŸ¥è¯†ï¼Œå‡å°‘è™šæ„å†…å®¹ã€‚</li>
<li>ä¼ ç»Ÿçš„ç¡®å®šæ€§æ£€ç´¢æ–¹å¼ä¸é€‚ç”¨äºé•¿æ–‡æœ¬é—®ç­”ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡åŠ¨æ€æ‰§è¡Œæ£€ç´¢å¯ä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>ä¸ç¡®å®šæ€§æ£€æµ‹æ–¹æ³•æ˜¯å®ç°åŠ¨æ€æ£€ç´¢çš„å…³é”®ã€‚</li>
<li>å®éªŒä¸­ä½¿ç”¨çš„ä¸ç¡®å®šæ€§æ£€æµ‹æŒ‡æ ‡å¦‚åº¦æ•°çŸ©é˜µJaccardå’Œç¦»å¿ƒç‡å¯ä»¥æœ‰æ•ˆå‡å°‘æ£€ç´¢è°ƒç”¨æ¬¡æ•°ã€‚</li>
<li>åœ¨ä½¿ç”¨ä¸ç¡®å®šæ€§æ£€æµ‹æŒ‡æ ‡å‡å°‘æ£€ç´¢è°ƒç”¨çš„åŒæ—¶ï¼Œåªç•¥å¾®å½±å“é—®ç­”å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a851a547f2938fc4438d15f73872404.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2387096cab7e11b8269b30ae5ab86dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0cf8141c4641e8cb50667003a2814d3b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Transformers-Struggle-to-Learn-to-Search"><a href="#Transformers-Struggle-to-Learn-to-Search" class="headerlink" title="Transformers Struggle to Learn to Search"></a>Transformers Struggle to Learn to Search</h2><p><strong>Authors:Abulhair Saparov, Srushti Pawar, Shreyas Pimpalgaonkar, Nitish Joshi, Richard Yuanzhe Pang, Vishakh Padmakumar, Seyed Mehran Kazemi, Najoung Kim, He He</strong></p>
<p>Search is an ability foundational in many important tasks, and recent studies have shown that large language models (LLMs) struggle to perform search robustly. It is unknown whether this inability is due to a lack of data, insufficient model parameters, or fundamental limitations of the transformer architecture. In this work, we use the foundational graph connectivity problem as a testbed to generate effectively limitless high-coverage data to train small transformers and test whether they can learn to perform search. We find that, when given the right training distribution, the transformer is able to learn to search.   We analyze the algorithm that the transformer has learned through a novel mechanistic interpretability technique that enables us to extract the computation graph from the trained model. We find that transformers perform search at every vertex in parallel: For each vertex in the input graph, transformers compute the set of vertices reachable from that vertex. Each layer then progressively expands these sets, allowing the model to search over a number of vertices exponential in $n_{\text{layers}}$.   However, we find that as the input graph size increases, the transformer has greater difficulty in learning the task. This difficulty is not resolved even as the number of parameters is increased, suggesting that increasing model scale will not lead to robust search abilities. We also find that performing search in-context (i.e., chain-of-thought) does not resolve this inability to learn to search on larger graphs. </p>
<blockquote>
<p>æœç´¢æ˜¯è®¸å¤šé‡è¦ä»»åŠ¡ä¸­çš„åŸºç¡€èƒ½åŠ›ï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ‰§è¡Œç¨³å¥æœç´¢æ—¶é‡åˆ°å›°éš¾ã€‚ç›®å‰å°šä¸æ¸…æ¥šè¿™ç§æ— èƒ½æ˜¯ç”±äºæ•°æ®ä¸è¶³ã€æ¨¡å‹å‚æ•°ä¸è¶³ï¼Œè¿˜æ˜¯ç”±äºTransformeræ¶æ„çš„æ ¹æœ¬é™åˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»¥åŸºç¡€å›¾è¿æ¥é—®é¢˜ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œç”Ÿæˆæœ‰æ•ˆæ— é™çš„é«˜è¦†ç›–ç‡æ•°æ®æ¥è®­ç»ƒå°å‹Transformerï¼Œå¹¶æµ‹è¯•å®ƒä»¬æ˜¯å¦èƒ½å­¦ä¼šæœç´¢ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“ç»™äºˆæ­£ç¡®çš„è®­ç»ƒåˆ†å¸ƒæ—¶ï¼ŒTransformerèƒ½å¤Ÿå­¦ä¼šæœç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04703v2">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœç´¢ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶æŒ‡å‡ºå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶é€šè¿‡åˆ©ç”¨åŸºç¡€å›¾è¿æ¥é—®é¢˜ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œç”Ÿæˆå¤§é‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå‘ç°ç»™äºˆé€‚å½“çš„è®­ç»ƒåˆ†å¸ƒï¼Œå˜å‹å™¨æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ è¿›è¡Œæœç´¢ã€‚é€šè¿‡æ–°å‹æœºæ¢°è§£é‡ŠæŠ€æœ¯ï¼Œç ”ç©¶äººå‘˜åˆ†æäº†æ¨¡å‹ç®—æ³•ï¼Œå‘ç°å˜å‹å™¨åœ¨æ¯ä¸ªé¡¶ç‚¹å¹¶è¡Œæ‰§è¡Œæœç´¢ä»»åŠ¡ã€‚ç„¶è€Œï¼Œéšç€è¾“å…¥å›¾å½¢å¤§å°çš„å¢åŠ ï¼Œæ¨¡å‹å­¦ä¹ ä»»åŠ¡çš„éš¾åº¦ä¹Ÿéšä¹‹å¢åŠ ï¼Œå³ä½¿å¢åŠ å‚æ•°æ•°é‡ä¹Ÿæ— æ³•è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¿™è¡¨æ˜æ‰©å¤§æ¨¡å‹è§„æ¨¡å¹¶ä¸ä¸€å®šèƒ½æé«˜æ¨¡å‹çš„æœç´¢èƒ½åŠ›ã€‚åŒæ—¶ï¼Œä¸Šä¸‹æ–‡æœç´¢ï¼ˆå³æ€ç»´é“¾ï¼‰ä¹Ÿæ— æ³•è§£å†³å¤§å‹å›¾å½¢ä¸Šçš„æœç´¢å­¦ä¹ éš¾é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœç´¢ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³çš„åŸå› å°šä¸æ¸…æ¥šï¼Œå¯èƒ½æ¶‰åŠæ•°æ®ç¼ºä¹ã€æ¨¡å‹å‚æ•°ä¸è¶³æˆ–è½¬æ¢å™¨æ¶æ„çš„æ ¹æœ¬é™åˆ¶ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨åŸºç¡€å›¾è¿æ¥é—®é¢˜ä½œä¸ºæµ‹è¯•å¹³å°ï¼Œå¯ä»¥æœ‰æ•ˆç”Ÿæˆå¤§é‡æ•°æ®æ¥è®­ç»ƒå°å‹è½¬æ¢å™¨ï¼Œä½¿å…¶å­¦ä¹ æœç´¢ä»»åŠ¡ã€‚</li>
<li>ç»™äºˆé€‚å½“çš„è®­ç»ƒåˆ†å¸ƒï¼Œå˜å‹å™¨æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ è¿›è¡Œæœç´¢ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡æœºæ¢°è§£é‡ŠæŠ€æœ¯ï¼Œç ”ç©¶äººå‘˜å‘ç°å˜å‹å™¨æ¨¡å‹åœ¨æ¯ä¸ªé¡¶ç‚¹å¹¶è¡Œæ‰§è¡Œæœç´¢ä»»åŠ¡ã€‚</li>
<li>éšç€è¾“å…¥å›¾å½¢å¤§å°çš„å¢åŠ ï¼Œæ¨¡å‹å­¦ä¹ ä»»åŠ¡çš„éš¾åº¦å¢åŠ ï¼Œå¢åŠ å‚æ•°æ•°é‡ä¹Ÿæ— æ³•è§£å†³è¿™ä¸€éš¾é¢˜ã€‚</li>
<li>æ‰©å¤§æ¨¡å‹è§„æ¨¡å¹¶ä¸ä¸€å®šèƒ½æé«˜æ¨¡å‹çš„æœç´¢èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8dab0ca5ce49ee2b1d49f313038f7233.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69e52a212a0d61d99920ad7dacf1e320.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbad3f2ff79518a6079302aef25e0477.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc2f7b5e304d6a0b1df3d8425f27ee4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bf6305f3176cce475c9ae6ec71c0cb3.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CreatiLayout-Siamese-Multimodal-Diffusion-Transformer-for-Creative-Layout-to-Image-Generation"><a href="#CreatiLayout-Siamese-Multimodal-Diffusion-Transformer-for-Creative-Layout-to-Image-Generation" class="headerlink" title="CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative   Layout-to-Image Generation"></a>CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative   Layout-to-Image Generation</h2><p><strong>Authors:Hui Zhang, Dexiang Hong, Yitong Wang, Jie Shao, Xinglong Wu, Zuxuan Wu, Yu-Gang Jiang</strong></p>
<p>Diffusion models have been recognized for their ability to generate images that are not only visually appealing but also of high artistic quality. As a result, Layout-to-Image (L2I) generation has been proposed to leverage region-specific positions and descriptions to enable more precise and controllable generation. However, previous methods primarily focus on UNet-based models (e.g., SD1.5 and SDXL), and limited effort has explored Multimodal Diffusion Transformers (MM-DiTs), which have demonstrated powerful image generation capabilities. Enabling MM-DiT for layout-to-image generation seems straightforward but is challenging due to the complexity of how layout is introduced, integrated, and balanced among multiple modalities. To this end, we explore various network variants to efficiently incorporate layout guidance into MM-DiT, and ultimately present SiamLayout. To Inherit the advantages of MM-DiT, we use a separate set of network weights to process the layout, treating it as equally important as the image and text modalities. Meanwhile, to alleviate the competition among modalities, we decouple the image-layout interaction into a siamese branch alongside the image-text one and fuse them in the later stage. Moreover, we contribute a large-scale layout dataset, named LayoutSAM, which includes 2.7 million image-text pairs and 10.7 million entities. Each entity is annotated with a bounding box and a detailed description. We further construct the LayoutSAM-Eval benchmark as a comprehensive tool for evaluating the L2I generation quality. Finally, we introduce the Layout Designer, which taps into the potential of large language models in layout planning, transforming them into experts in layout generation and optimization. Our code, model, and dataset will be available at <a target="_blank" rel="noopener" href="https://creatilayout.github.io/">https://creatilayout.github.io</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å› å…¶èƒ½å¤Ÿç”Ÿæˆä¸ä»…è§†è§‰å¸å¼•åŠ›å¼ºè€Œä¸”è‰ºæœ¯æ€§é«˜çš„å›¾åƒè€Œå¤‡å—å…³æ³¨ã€‚å› æ­¤ï¼Œæå‡ºäº†Layout-to-Imageï¼ˆL2Iï¼‰ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨ç‰¹å®šåŒºåŸŸçš„ä½ç½®å’Œæè¿°æ¥å®ç°æ›´ç²¾ç¡®å’Œå¯æ§çš„ç”Ÿæˆã€‚ç„¶è€Œï¼Œä¹‹å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åŸºäºUNetçš„æ¨¡å‹ï¼ˆä¾‹å¦‚SD1.5å’ŒSDXLï¼‰ï¼Œå¯¹å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆMM-DiTï¼‰çš„æ¢ç´¢æœ‰é™ï¼Œè€ŒMM-DiTå·²æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚å°½ç®¡ä½¿MM-DiTç”¨äºå¸ƒå±€åˆ°å›¾åƒç”Ÿæˆçœ‹ä¼¼ç®€å•ï¼Œä½†ç”±äºå¼•å…¥ã€é›†æˆå’Œå¹³è¡¡å¸ƒå±€äºå¤šç§æ¨¡æ€ä¹‹é—´çš„å¤æ‚æ€§ï¼Œå®é™…ä¸Šé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å„ç§ç½‘ç»œå˜ä½“ï¼Œä»¥æœ‰æ•ˆåœ°å°†å¸ƒå±€æŒ‡å¯¼èå…¥MM-DiTï¼Œå¹¶æœ€ç»ˆæ¨å‡ºSiamLayoutã€‚ä¸ºäº†ç»§æ‰¿MM-DiTçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç»„ç‹¬ç«‹çš„ç½‘ç»œæƒé‡æ¥å¤„ç†å¸ƒå±€ï¼Œå°†å…¶è§†ä¸ºä¸å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€åŒç­‰é‡è¦ã€‚åŒæ—¶ï¼Œä¸ºäº†å‡è½»æ¨¡æ€ä¹‹é—´çš„ç«äº‰ï¼Œæˆ‘ä»¬å°†å›¾åƒå¸ƒå±€äº¤äº’è§£è€¦ä¸ºä¸å›¾åƒæ–‡æœ¬åˆ†æ”¯å¹¶åˆ—çš„å­ªç”Ÿåˆ†æ”¯ï¼Œå¹¶åœ¨åæœŸè¿›è¡Œèåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªå¤§è§„æ¨¡å¸ƒå±€æ•°æ®é›†LayoutSAMï¼Œå…¶ä¸­åŒ…æ‹¬270ä¸‡å¼ å›¾åƒæ–‡æœ¬å¯¹å’Œ1070ä¸‡ä¸ªå®ä½“ã€‚æ¯ä¸ªå®ä½“éƒ½å¸¦æœ‰è¾¹ç•Œæ¡†å’Œè¯¦ç»†æè¿°ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†LayoutSAM-EvalåŸºå‡†æµ‹è¯•ï¼Œä½œä¸ºè¯„ä¼°L2Iç”Ÿæˆè´¨é‡çš„ç»¼åˆå·¥å…·ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†Layout Designerï¼Œå®ƒæŒ–æ˜äº†å¤§è¯­è¨€æ¨¡å‹åœ¨å¸ƒå±€è§„åˆ’ä¸­çš„æ½œåŠ›ï¼Œå°†å…¶è½¬å˜ä¸ºå¸ƒå±€ç”Ÿæˆå’Œä¼˜åŒ–çš„ä¸“å®¶ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://creatilayout.github.ioä¸Šæä¾›./">https://creatilayout.github.ioä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03859v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Layout-to-Imageï¼ˆL2Iï¼‰ç”ŸæˆæŠ€æœ¯åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„åº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡ç°æœ‰çš„æ‰©æ•£æ¨¡å‹å·²ç»èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†åœ¨å¼•å…¥å¸ƒå±€æŒ‡å¯¼æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æ–‡ç« ä»‹ç»äº†å¦‚ä½•åˆ©ç”¨å¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆMM-DiTï¼‰è¿›è¡Œå¸ƒå±€åˆ°å›¾åƒçš„ç”Ÿæˆï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•å°†å¸ƒå±€æŒ‡å¯¼æœ‰æ•ˆåœ°èå…¥MM-DiTä¸­çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜è´¡çŒ®äº†ä¸€ä¸ªå¤§è§„æ¨¡å¸ƒå±€æ•°æ®é›†LayoutSAMï¼ŒåŒ…å«å¤§é‡å¸¦æ ‡æ³¨çš„å›¾åƒæ–‡æœ¬å¯¹å®ä½“æ•°æ®ã€‚åŒæ—¶æ¨å‡ºäº†LayoutSAM-Evalè¯„ä¼°å·¥å…·ä»¥è¯„ä»·L2Iç”Ÿæˆè´¨é‡ï¼Œå¹¶å¼•å…¥äº†Layout Designeråˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›åœ¨å¸ƒå±€è§„åˆ’æ–¹é¢çš„ä¸“å®¶ç³»ç»Ÿã€‚æœ€ç»ˆçš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å°†å…¬å¼€æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ä¸”å…·æœ‰è‰ºæœ¯æ„Ÿçš„å›¾åƒã€‚</li>
<li>Layout-to-Imageï¼ˆL2Iï¼‰ç”ŸæˆæŠ€æœ¯åˆ©ç”¨åŒºåŸŸç‰¹å®šä½ç½®å’Œæè¿°æ¥å®ç°æ›´ç²¾ç¡®å’Œå¯æ§çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>ç°æœ‰çš„æ–¹æ³•ä¸»è¦å…³æ³¨UNet-basedæ¨¡å‹ï¼Œè€Œå¯¹Multimodal Diffusion Transformersï¼ˆMM-DiTï¼‰çš„ç ”ç©¶æœ‰é™ã€‚</li>
<li>MM-DiTèå…¥å¸ƒå±€æŒ‡å¯¼é¢ä¸´å¤æ‚æ€§æŒ‘æˆ˜ï¼Œéœ€è¦æ¢ç´¢æœ‰æ•ˆçš„ç½‘ç»œå˜ä½“ã€‚</li>
<li>SiamLayouté€šè¿‡é‡‡ç”¨åˆ†ç¦»çš„ç½‘ç»œæƒé‡å¤„ç†å¸ƒå±€ä¿¡æ¯ï¼Œå®ç°äº†MM-DiTçš„ä¼˜åŠ¿ï¼ŒåŒæ—¶å¹³è¡¡äº†å„æ¨¡æ€ä¹‹é—´çš„ç«äº‰ã€‚</li>
<li>æ¨å‡ºäº†å¤§è§„æ¨¡å¸ƒå±€æ•°æ®é›†LayoutSAMï¼ŒåŒ…å«å¤§é‡å¸¦æ ‡æ³¨çš„å›¾åƒæ–‡æœ¬å¯¹å®ä½“æ•°æ®ï¼Œç”¨äºè¯„ä¼°å’Œæ”¹è¿›L2Iç”ŸæˆæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2186aa6968aa156c8fbd5ff1718b5eed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0ed95b355a2515fd90089decc74a881.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8f984f91d2b5674ea0537a014c7211e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d4dd2d17a78056913095869443933be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfd40eb89e8f7e1118eed3ae85625d62.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Collaborative-Instance-Object-Navigation-Leveraging-Uncertainty-Awareness-to-Minimize-Human-Agent-Dialogues"><a href="#Collaborative-Instance-Object-Navigation-Leveraging-Uncertainty-Awareness-to-Minimize-Human-Agent-Dialogues" class="headerlink" title="Collaborative Instance Object Navigation: Leveraging   Uncertainty-Awareness to Minimize Human-Agent Dialogues"></a>Collaborative Instance Object Navigation: Leveraging   Uncertainty-Awareness to Minimize Human-Agent Dialogues</h2><p><strong>Authors:Francesco Taioli, Edoardo Zorzi, Gianni Franchi, Alberto Castellini, Alessandro Farinelli, Marco Cristani, Yiming Wang</strong></p>
<p>Language-driven instance object navigation assumes that human users initiate the task by providing a detailed description of the target instance to the embodied agent. While this description is crucial for distinguishing the target from visually similar instances in a scene, providing it prior to navigation can be demanding for human. To bridge this gap, we introduce Collaborative Instance object Navigation (CoIN), a new task setting where the agent actively resolve uncertainties about the target instance during navigation in natural, template-free, open-ended dialogues with human. We propose a novel training-free method, Agent-user Interaction with UncerTainty Awareness (AIUTA), which operates independently from the navigation policy, and focuses on the human-agent interaction reasoning with Vision-Language Models (VLMs) and Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue within the agent to obtain a complete and accurate observation description with a novel uncertainty estimation technique. Then, an Interaction Trigger module determines whether to ask a question to the human, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, with a curated dataset designed for challenging multi-instance scenarios. CoIN-Bench supports both online evaluation with humans and reproducible experiments with simulated user-agent interactions. On CoIN-Bench, we show that AIUTA serves as a competitive baseline, while existing language-driven instance navigation methods struggle in complex multi-instance scenes. Code and benchmark will be available upon acceptance at <a target="_blank" rel="noopener" href="https://intelligolabs.github.io/CoIN/">https://intelligolabs.github.io/CoIN/</a> </p>
<blockquote>
<p>è¯­è¨€é©±åŠ¨çš„å®ä¾‹å¯¹è±¡å¯¼èˆªå‡è®¾äººç±»ç”¨æˆ·é€šè¿‡å‘å®ä½“ä»£ç†æä¾›ç›®æ ‡å®ä¾‹çš„è¯¦ç»†æè¿°æ¥å¯åŠ¨ä»»åŠ¡ã€‚è™½ç„¶è¿™ä¸ªæè¿°å¯¹äºä»åœºæ™¯ä¸­çš„è§†è§‰ç›¸ä¼¼å®ä¾‹ä¸­åŒºåˆ†ç›®æ ‡è‡³å…³é‡è¦ï¼Œä½†åœ¨å¯¼èˆªä¹‹å‰æä¾›å®ƒå¯èƒ½å¯¹äººç±»æ¥è¯´æ˜¯ä¸€ç§æŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä½œå®ä¾‹å¯¹è±¡å¯¼èˆªï¼ˆCoINï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ä»»åŠ¡è®¾ç½®ï¼Œå…¶ä¸­ä»£ç†åœ¨ä¸äººç±»è¿›è¡Œæ— æ¨¡æ¿ã€å¼€æ”¾å¼çš„è‡ªç„¶å¯¹è¯è¿‡ç¨‹ä¸­ï¼Œç§¯æè§£å†³å…³äºç›®æ ‡å®ä¾‹çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„æ–¹æ³•â€”â€”å…·æœ‰ä¸ç¡®å®šæ€§æ„è¯†çš„ä»£ç†ç”¨æˆ·äº¤äº’ï¼ˆAIUTAï¼‰ï¼Œå®ƒç‹¬ç«‹äºå¯¼èˆªç­–ç•¥ï¼Œä¸“æ³¨äºåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äººç±»ä»£ç†äº¤äº’æ¨ç†ã€‚é¦–å…ˆï¼Œåœ¨å¯¹è±¡æ£€æµ‹åï¼Œè‡ªæˆ‘æé—®æ¨¡å‹ä¼šåœ¨ä»£ç†å†…éƒ¨å‘èµ·ä¸€æ¬¡è‡ªæˆ‘å¯¹è¯ï¼Œåˆ©ç”¨ä¸€ç§æ–°å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡æŠ€æœ¯æ¥è·å¾—å®Œæ•´å‡†ç¡®çš„è§‚å¯Ÿæè¿°ã€‚ç„¶åï¼Œäº¤äº’è§¦å‘æ¨¡å—ç¡®å®šæ˜¯å¦å‘äººç±»æé—®ã€ç»§ç»­æˆ–åœæ­¢å¯¼èˆªï¼Œä»¥æœ€å°åŒ–ç”¨æˆ·è¾“å…¥ã€‚ä¸ºäº†è¯„ä¼°æ€§èƒ½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CoIN-Benchï¼Œå®ƒåŒ…å«ä¸“ä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šå®ä¾‹åœºæ™¯è®¾è®¡çš„ç²¾é€‰æ•°æ®é›†ã€‚CoIN-Benchæ”¯æŒä¸äººç±»åœ¨çº¿è¯„ä¼°ä»¥åŠå¯é‡å¤çš„æ¨¡æ‹Ÿç”¨æˆ·ä»£ç†äº¤äº’å®éªŒã€‚åœ¨CoIN-Benchä¸Šï¼Œæˆ‘ä»¬å±•ç¤ºäº†AIUTAä½œä¸ºä¸€ä¸ªæœ‰ç«äº‰åŠ›çš„åŸºå‡†çº¿ï¼Œè€Œç°æœ‰çš„è¯­è¨€é©±åŠ¨å®ä¾‹å¯¼èˆªæ–¹æ³•åœ¨å¤æ‚çš„å¤šå®ä¾‹åœºæ™¯ä¸­è¡¨ç°æŒ£æ‰ã€‚æ¥å—åï¼Œä»£ç å’ŒåŸºå‡†çº¿å°†åœ¨<a target="_blank" rel="noopener" href="https://intelligolabs.github.io/CoIN/%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://intelligolabs.github.io/CoIN/ä¸Šå¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01250v3">PDF</a> <a target="_blank" rel="noopener" href="https://intelligolabs.github.io/CoIN/">https://intelligolabs.github.io/CoIN/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ä»»åŠ¡è®¾ç½®â€”â€”åä½œå¼å®ä¾‹å¯¹è±¡å¯¼èˆªï¼ˆCoINï¼‰ï¼Œå…¶ä¸­ä»£ç†åœ¨å¯¼èˆªè¿‡ç¨‹ä¸­é€šè¿‡ä¸ç”¨æˆ·çš„å¼€æ”¾å¯¹è¯ä¸»åŠ¨è§£å†³ç›®æ ‡å®ä¾‹çš„ä¸ç¡®å®šæ€§é—®é¢˜ã€‚æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•â€”â€”å…·æœ‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ä»£ç†ç”¨æˆ·äº¤äº’ï¼ˆAIUTAï¼‰ï¼Œè¯¥æ–¹æ³•ç‹¬ç«‹äºå¯¼èˆªç­–ç•¥ï¼Œä¸“æ³¨äºåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œäººæœºäº¤äº’æ¨ç†ã€‚é€šè¿‡è‡ªæˆ‘æé—®æ¨¡å‹è·å¾—å®Œæ•´å‡†ç¡®çš„è§‚å¯Ÿæè¿°ï¼Œå¹¶å¼•å…¥äº¤äº’è§¦å‘æ¨¡å—æ¥å†³å®šæ˜¯å¦å‘ç”¨æˆ·æé—®ã€ç»§ç»­æˆ–åœæ­¢å¯¼èˆªã€‚è¯„ä¼°ä¸­å¼•å…¥äº†CoIN-Benchæ•°æ®é›†ï¼Œæ”¯æŒåœ¨çº¿äººç±»è¯„ä¼°å’Œå¯é‡å¤çš„æ¨¡æ‹Ÿç”¨æˆ·-ä»£ç†äº¤äº’å®éªŒã€‚AIUTAåœ¨CoIN-Benchä¸Šè¡¨ç°æœ‰ç«äº‰åŠ›ï¼Œè€Œç°æœ‰çš„è¯­è¨€é©±åŠ¨å®ä¾‹å¯¼èˆªæ–¹æ³•åœ¨å¤æ‚å¤šå®ä¾‹åœºæ™¯ä¸­è¡¨ç°æŒ£æ‰ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¼•å…¥æ–°çš„ä»»åŠ¡è®¾ç½®ï¼šåä½œå¼å®ä¾‹å¯¹è±¡å¯¼èˆªï¼ˆCoINï¼‰ã€‚</li>
<li>æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼šAIUTAï¼Œç”¨äºè§£å†³å¯¼èˆªè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§é—®é¢˜ã€‚</li>
<li>AIUTAåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œäººæœºäº¤äº’æ¨ç†ã€‚</li>
<li>é€šè¿‡è‡ªæˆ‘æé—®æ¨¡å‹è·å¾—å¯¹ç›®æ ‡å®ä¾‹çš„å®Œæ•´å‡†ç¡®è§‚å¯Ÿæè¿°ã€‚</li>
<li>å¼•å…¥äº¤äº’è§¦å‘æ¨¡å—ä»¥æœ€å°åŒ–ç”¨æˆ·è¾“å…¥ï¼Œå†³å®šå¯¼èˆªå†³ç­–ã€‚</li>
<li>è¯„ä¼°ä¸­ä½¿ç”¨äº†æ–°çš„æ•°æ®é›†CoIN-Benchï¼Œæ”¯æŒåœ¨çº¿äººç±»è¯„ä¼°å’Œæ¨¡æ‹Ÿå®éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01250">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17ed04fcd8938cfbc674d14347d929a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7f9b9f5e2f6f944860dbd31097cb3ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4add5e176edf665f26820eaacf06cd5f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1bfcf16d2f1dcb31142c9a8c39dd0b14.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  DARS Dynamic Action Re-Sampling to Enhance Coding Agent Performance by   Adaptive Tree Traversal
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5d6a422a9a5048b5eea40aafa607f945.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  MusicInfuser Making Video Diffusion Listen and Dance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16905.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
