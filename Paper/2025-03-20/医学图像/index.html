<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  Surface phonons in MoS2">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-cd0d4aeb6ae44ddd5fd8bd0e4044017d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-20-æ›´æ–°"><a href="#2025-03-20-æ›´æ–°" class="headerlink" title="2025-03-20 æ›´æ–°"></a>2025-03-20 æ›´æ–°</h1><h2 id="Surface-phonons-in-MoS2"><a href="#Surface-phonons-in-MoS2" class="headerlink" title="Surface phonons in MoS2"></a>Surface phonons in MoS2</h2><p><strong>Authors:Aleksandar Radic, Boyao Liu, Andrew Jardine, Akshay Rao, Sam Lambrick</strong></p>
<p>The thermal and electronic performance of atomically thin semiconductors is underpinned by their vibrational dynamics, yet surface phonons in layered materials remain poorly understood due to limitations in conventional experimental techniques. We employ helium-3 spin-echo (HeSE) spectroscopy to resolve the lowest energy (&lt;10 meV) dispersions of surface phonons on bulk molybdenum disulfide (MoS\textsubscript{2}) with sub-meV energy resolution near $\Gamma$. We identify two low-energy optical modes, E_2g and A_1g, exhibiting unexpected quasi-acoustic dispersion, and crucially do not find evidence of an acoustic mode at the surface. A_1g follows a purely quadratic dispersion ((\omega_{A_{1g}} \propto q^2)), while (E_{2g}) displays quartic behaviour ((\omega_{E_{2g}} \propto q^4)), indicative of strong anharmonicity. These modes, absent in inelastic x-ray scattering (IXS) measurements and theoretical predictions of the bulk, exhibit finite-layer confinement equivalent to 4.5 and 6 layers, respectively. Their rapid dispersion yields substantial group velocities at small wavevectors, suggesting a dominant role in surface and few-layer thermal transport. This work establishes optical surface phonons as key drivers of thermal management in 2D materials and highlights the necessity of understanding surface phononics for designing next-generation optoelectronic devices. </p>
<blockquote>
<p>åŸå­çº§è¶…è–„åŠå¯¼ä½“çš„çƒ­å­¦å’Œç”µå­æ€§èƒ½ç”±å…¶æŒ¯åŠ¨åŠ¨åŠ›å­¦æ”¯æ’‘ã€‚ç„¶è€Œï¼Œç”±äºä¼ ç»Ÿå®éªŒæŠ€æœ¯çš„å±€é™æ€§ï¼Œå±‚çŠ¶ææ–™ä¸­çš„è¡¨é¢å£°å­ä»äº†è§£ä¸è¶³ã€‚æˆ‘ä»¬é‡‡ç”¨æ°¦-3è‡ªæ—‹å›æ³¢ï¼ˆHeSEï¼‰å…‰è°±æ³•ï¼Œä»¥äºšæ¯«ç”µå­ä¼ç‰¹çš„èƒ½é‡åˆ†è¾¨ç‡ï¼Œè§£å†³å¤§å—äºŒç¡«åŒ–é’¼ï¼ˆMoS2ï¼‰è¡¨é¢å£°å­çš„æœ€ä½èƒ½é‡ï¼ˆ&lt;10 meVï¼‰åˆ†æ•£é—®é¢˜ï¼Œæ¥è¿‘äºÎ“ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¸¤ä¸ªä½èƒ½å…‰å­¦æ¨¡å¼ï¼Œå³E_2gå’ŒA_1gï¼Œå®ƒä»¬è¡¨ç°å‡ºæ„å¤–çš„å‡†å£°å­¦åˆ†æ•£ç‰¹å¾ï¼Œå¹¶ä¸”å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬æ²¡æœ‰å‘ç°è¡¨é¢å¤„çš„å£°å­¦æ¨¡å¼çš„è¯æ®ã€‚A_1géµå¾ªçº¯äºŒæ¬¡åˆ†æ•£ï¼ˆ(\omega_{A_{1g}} \propto q^2)ï¼‰ï¼Œè€ŒE_2gè¡¨ç°å‡ºå››æ¬¡è¡Œä¸ºï¼ˆ(\omega_{E_{2g}} \propto q^4)ï¼‰ï¼Œè¡¨æ˜å…·æœ‰å¼ºçƒˆçš„éè°æ€§ã€‚è¿™äº›æ¨¡å¼åœ¨å¼¹æ€§Xå°„çº¿æ•£å°„ï¼ˆIXSï¼‰æµ‹é‡å’Œå¤§ç†è®ºé¢„æµ‹ä¸­å‡æœªå‘ç°ï¼Œåˆ†åˆ«ç›¸å½“äº4.5å±‚å’Œ6å±‚çš„æœ‰é™å±‚çº¦æŸã€‚å®ƒä»¬å¿«é€Ÿçš„åˆ†æ•£åœ¨å°æ³¢çŸ¢ä¸‹äº§ç”Ÿäº†å¯è§‚çš„ç¾¤é€Ÿåº¦ï¼Œè¡¨æ˜åœ¨è¡¨é¢å’Œå°‘å±‚çƒ­è¾“è¿ä¸­èµ·ç€ä¸»å¯¼ä½œç”¨ã€‚è¿™é¡¹å·¥ä½œç¡®ç«‹äº†å…‰å­¦è¡¨é¢å£°å­åœ¨äºŒç»´ææ–™çƒ­ç®¡ç†ä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶å¼ºè°ƒäº†äº†è§£è¡¨é¢å£°å­å­¦å¯¹äºè®¾è®¡ä¸‹ä¸€ä»£å…‰ç”µå­å™¨ä»¶çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14464v1">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶åˆ©ç”¨æ°¦-3è‡ªæ—‹å›æ³¢å…‰è°±æŠ€æœ¯ï¼Œè§£æäº†è–„å±‚äºŒç¡«åŒ–é’¼è¡¨é¢å£°å­çš„æœ€ä½èƒ½é‡å¼¥æ•£å…³ç³»ã€‚å‘ç°ä¸¤ç§ä½èƒ½å…‰å­¦æ¨¡å¼E_2gå’ŒA_1gå…·æœ‰æ„å¤–çš„å‡†å£°å­¦å¼¥æ•£ç‰¹æ€§ï¼Œå¹¶ç¡®å®šå®ƒä»¬å…·æœ‰å¼ºçƒˆçš„éçº¿æ€§è¡Œä¸ºã€‚è¿™äº›æ¨¡å¼åœ¨å…‰å­¦ç‰¹æ€§ä¸Šå±•ç°å‡ºæœ‰é™çš„å±‚é™åˆ¶ï¼Œå¯¹è¡¨é¢å’Œå°‘å±‚çƒ­ä¼ è¾“èµ·åˆ°é‡è¦ä½œç”¨ã€‚ç ”ç©¶å¼ºè°ƒäº†å…‰å­¦è¡¨é¢å£°å­åœ¨äºŒç»´ææ–™çƒ­ç®¡ç†ä¸­çš„é‡è¦æ€§ï¼Œä¸ºä¸‹ä¸€ä»£å…‰ç”µå­å™¨ä»¶çš„è®¾è®¡æä¾›äº†å…³é”®ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ©ç”¨æ°¦-3è‡ªæ—‹å›æ³¢å…‰è°±æŠ€æœ¯è§£æäº†è–„å±‚äºŒç¡«åŒ–é’¼çš„è¡¨é¢å£°å­å¼¥æ•£å…³ç³»ã€‚</li>
<li>å‘ç°äº†ä¸¤ç§ä½èƒ½å…‰å­¦æ¨¡å¼E_2gå’ŒA_1gï¼Œå…·æœ‰å‡†å£°å­¦å¼¥æ•£ç‰¹æ€§ã€‚</li>
<li>A_1gæ¨¡å¼éµå¾ªçº¯ç²¹çš„äºŒæ¬¡å¼¥æ•£ï¼Œè€ŒE_2gæ¨¡å¼æ˜¾ç¤ºå››æ¬¡è¡Œä¸ºï¼Œè¡¨æ˜å¼ºçƒˆçš„éè°æ€§ã€‚</li>
<li>è¿™äº›æ¨¡å¼åœ¨å¼¹æ€§Xå°„çº¿æ•£å°„æµ‹é‡ä¸­æœªè¢«è§‚å¯Ÿåˆ°ï¼Œå¹¶ä¸”ä¸ç†è®ºé¢„æµ‹ä¸åŒã€‚</li>
<li>è§‚å¯Ÿåˆ°è¿™äº›æ¨¡å¼çš„æœ‰é™å±‚é™åˆ¶ï¼Œç›¸å½“äº4.5å’Œ6å±‚ã€‚</li>
<li>å®ƒä»¬å¿«é€Ÿå¼¥æ•£äº§ç”Ÿè¾ƒå¤§çš„ç¾¤é€Ÿåº¦ï¼Œå¯¹è¡¨é¢å’Œå°‘å±‚çƒ­ä¼ è¾“èµ·åˆ°é‡è¦ä½œç”¨ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†å…‰å­¦è¡¨é¢å£°å­åœ¨äºŒç»´ææ–™çƒ­ç®¡ç†ä¸­çš„é‡è¦æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a4b081ddaee08ccfff15151941d2944.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-356f2ef3238a82c334e1255dd125d34b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44dd2cbb84b86e4c5e17fb7054bd70a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2274456d865d369afdf40fbcf660cfb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b42a211a92e3d3ba38eaf54d136ddb12.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MAST-Pro-Dynamic-Mixture-of-Experts-for-Adaptive-Segmentation-of-Pan-Tumors-with-Knowledge-Driven-Prompts"><a href="#MAST-Pro-Dynamic-Mixture-of-Experts-for-Adaptive-Segmentation-of-Pan-Tumors-with-Knowledge-Driven-Prompts" class="headerlink" title="MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of   Pan-Tumors with Knowledge-Driven Prompts"></a>MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of   Pan-Tumors with Knowledge-Driven Prompts</h2><p><strong>Authors:Runqi Meng, Sifan Song, Pengfei Jin, Yujin Oh, Lin Teng, Yulin Wang, Yiqun Sun, Ling Chen, Xiang Li, Quanzheng Li, Ning Guo, Dinggang Shen</strong></p>
<p>Accurate tumor segmentation is crucial for cancer diagnosis and treatment. While foundation models have advanced general-purpose segmentation, existing methods still struggle with: (1) limited incorporation of medical priors, (2) imbalance between generic and tumor-specific features, and (3) high computational costs for clinical adaptation. To address these challenges, we propose MAST-Pro (Mixture-of-experts for Adaptive Segmentation of pan-Tumors with knowledge-driven Prompts), a novel framework that integrates dynamic Mixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor segmentation. Specifically, text and anatomical prompts provide domain-specific priors, guiding tumor representation learning, while D-MoE dynamically selects experts to balance generic and tumor-specific feature learning, improving segmentation accuracy across diverse tumor types. To enhance efficiency, we employ Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with significantly reduced computational overhead. Experiments on multi-anatomical tumor datasets demonstrate that MAST-Pro outperforms state-of-the-art approaches, achieving up to a 5.20% improvement in average DSC while reducing trainable parameters by 91.04%, without compromising accuracy. </p>
<blockquote>
<p>å‡†ç¡®çš„è‚¿ç˜¤åˆ†å‰²å¯¹äºç™Œç—‡è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚è™½ç„¶åŸºç¡€æ¨¡å‹å·²ç»æ¨åŠ¨äº†é€šç”¨åˆ†å‰²çš„è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼š(1)åŒ»ç–—å…ˆéªŒçŸ¥è¯†çš„æœ‰é™èåˆï¼›(2)é€šç”¨å’Œè‚¿ç˜¤ç‰¹å®šç‰¹å¾ä¹‹é—´çš„ä¸å¹³è¡¡ï¼›(3)ä¸´åºŠé€‚åº”çš„é«˜è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MAST-Proï¼ˆåŸºäºçŸ¥è¯†é©±åŠ¨æç¤ºçš„é€‚åº”æ€§å¼ºæ³›åŒ–è‚¿ç˜¤åˆ†å‰²æ··åˆä¸“å®¶æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆäº†åŠ¨æ€æ··åˆä¸“å®¶ï¼ˆD-MoEï¼‰å’ŒçŸ¥è¯†é©±åŠ¨æç¤ºçš„æ³›è‚¿ç˜¤åˆ†å‰²æ–°æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæ–‡æœ¬å’Œè§£å‰–æç¤ºæä¾›äº†ç‰¹å®šé¢†åŸŸçš„å…ˆéªŒçŸ¥è¯†ï¼ŒæŒ‡å¯¼è‚¿ç˜¤è¡¨ç¤ºå­¦ä¹ ï¼Œè€ŒD-MoEåˆ™åŠ¨æ€é€‰æ‹©ä¸“å®¶æ¥å¹³è¡¡é€šç”¨å’Œè‚¿ç˜¤ç‰¹å®šç‰¹å¾çš„å­¦ä¹ ï¼Œä»è€Œæé«˜å„ç§è‚¿ç˜¤ç±»å‹çš„åˆ†å‰²ç²¾åº¦ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯ï¼Œä»¥æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€æ¥ä¼˜åŒ–MAST-Proã€‚åœ¨å¤šè§£å‰–è‚¿ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAST-Proè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹³å‡DSCå€¼æé«˜äº†é«˜è¾¾5.20%ï¼ŒåŒæ—¶å‡å°‘äº†91.04%çš„å¯è®­ç»ƒå‚æ•°ï¼Œä¸”ä¸å½±å“å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14355v1">PDF</a> 10 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†MAST-Proæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†åŠ¨æ€æ··åˆä¸“å®¶ï¼ˆD-MoEï¼‰å’ŒçŸ¥è¯†é©±åŠ¨æç¤ºï¼Œç”¨äºæ³›è‚¿ç˜¤åˆ†å‰²ã€‚é€šè¿‡ç»“åˆåŒ»å­¦å…ˆéªŒã€å¹³è¡¡é€šç”¨å’Œè‚¿ç˜¤ç‰¹å¼‚æ€§ç‰¹å¾å­¦ä¹ ï¼Œä»¥åŠé‡‡ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æé«˜è®¡ç®—æ•ˆç‡ï¼ŒMAST-Proåœ¨å¤šç§è‚¿ç˜¤ç±»å‹åˆ†å‰²ä¸­å®ç°äº†é«˜ç²¾åº¦å’Œé«˜æ•ˆèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚¿ç˜¤ç²¾å‡†åˆ†å‰²å¯¹ç™Œç—‡è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨é¢å¯¹æ³›è‚¿ç˜¤åˆ†å‰²æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹åŒ»å­¦å…ˆéªŒçš„èå…¥ã€é€šç”¨ä¸è‚¿ç˜¤ç‰¹å¼‚æ€§ç‰¹å¾ä¹‹é—´çš„ä¸å¹³è¡¡ä»¥åŠé«˜è®¡ç®—æˆæœ¬ã€‚</li>
<li>MAST-Proæ¡†æ¶é›†æˆäº†åŠ¨æ€æ··åˆä¸“å®¶ï¼ˆD-MoEï¼‰å’ŒçŸ¥è¯†é©±åŠ¨æç¤ºï¼Œä»¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>æ–‡æœ¬å’Œè§£å‰–æç¤ºä¸ºé¢†åŸŸç‰¹å®šå…ˆéªŒæä¾›äº†æŒ‡å¯¼ï¼Œå¼•å¯¼è‚¿ç˜¤è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>D-MoEèƒ½å¤ŸåŠ¨æ€é€‰æ‹©ä¸“å®¶ï¼Œä»¥å¹³è¡¡é€šç”¨å’Œè‚¿ç˜¤ç‰¹å¼‚æ€§ç‰¹å¾å­¦ä¹ ï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯ç”¨äºä¼˜åŒ–MAST-Proï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d58f6d928d236422a5961f364dafaa2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07aaaa3666fa91593163195a172b4cfb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multi-Prototype-Embedding-Refinement-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#Multi-Prototype-Embedding-Refinement-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Multi-Prototype Embedding Refinement for Semi-Supervised Medical Image   Segmentation"></a>Multi-Prototype Embedding Refinement for Semi-Supervised Medical Image   Segmentation</h2><p><strong>Authors:Yali Bi, Enyu Che, Yinan Chen, Yuanpeng He, Jingwei Qu</strong></p>
<p>Medical image segmentation aims to identify anatomical structures at the voxel-level. Segmentation accuracy relies on distinguishing voxel differences. Compared to advancements achieved in studies of the inter-class variance, the intra-class variance receives less attention. Moreover, traditional linear classifiers, limited by a single learnable weight per class, struggle to capture this finer distinction. To address the above challenges, we propose a Multi-Prototype-based Embedding Refinement method for semi-supervised medical image segmentation. Specifically, we design a multi-prototype-based classification strategy, rethinking the segmentation from the perspective of structural relationships between voxel embeddings. The intra-class variations are explored by clustering voxels along the distribution of multiple prototypes in each class. Next, we introduce a consistency constraint to alleviate the limitation of linear classifiers. This constraint integrates different classification granularities from a linear classifier and the proposed prototype-based classifier. In the thorough evaluation on two popular benchmarks, our method achieves superior performance compared with state-of-the-art methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Briley-byl123/MPER">https://github.com/Briley-byl123/MPER</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ—¨åœ¨ä½“ç´ çº§åˆ«ä¸Šè¯†åˆ«è§£å‰–ç»“æ„ã€‚åˆ†å‰²ç²¾åº¦ä¾èµ–äºåŒºåˆ†ä½“ç´ å·®å¼‚çš„èƒ½åŠ›ã€‚ç›¸è¾ƒäºç±»é—´æ–¹å·®çš„ç ”ç©¶è¿›å±•ï¼Œç±»å†…æ–¹å·®å—åˆ°çš„å…³æ³¨è¾ƒå°‘ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿçº¿æ€§åˆ†ç±»å™¨æ¯ç±»åªæœ‰ä¸€ä¸ªå¯å­¦ä¹ çš„æƒé‡ï¼Œéš¾ä»¥æ•æ‰è¿™ç§æ›´ç²¾ç»†çš„åŒºåˆ†ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šåŸå‹åµŒå…¥ç²¾åŒ–çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºå¤šåŸå‹çš„åˆ†ç±»ç­–ç•¥ï¼Œä»ä½“ç´ åµŒå…¥çš„ç»“æ„å…³ç³»è§’åº¦é‡æ–°æ€è€ƒåˆ†å‰²é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡èšç±»æ¯ç±»ä¸­çš„å¤šä¸ªåŸå‹åˆ†å¸ƒä¸Šçš„ä½“ç´ æ¥æ¢ç´¢ç±»å†…å˜åŒ–ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€è‡´æ€§çº¦æŸæ¥ç¼“è§£çº¿æ€§åˆ†ç±»å™¨çš„å±€é™æ€§ã€‚è¯¥çº¦æŸèåˆäº†çº¿æ€§åˆ†ç±»å™¨å’Œæ‰€æå‡ºçš„åŸºäºåŸå‹çš„åˆ†ç±»å™¨ä¹‹é—´çš„ä¸åŒåˆ†ç±»ç²’åº¦ã€‚åœ¨ä¸¤ä¸ªæµè¡ŒåŸºå‡†çš„ä¸¥æ ¼è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/Briley-byl123/MPER%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/Briley-byl123/MPERè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14343v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¤šåŸå‹åµŒå…¥ç²¾ä¿®çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ã€‚è¯¥æ–¹æ³•è®¾è®¡äº†ä¸€ç§å¤šåŸå‹åˆ†ç±»ç­–ç•¥ï¼Œä»åƒç´ åµŒå…¥çš„ç»“æ„å…³ç³»è§’åº¦é‡æ–°æ€è€ƒåˆ†å‰²é—®é¢˜ã€‚é€šè¿‡èšç±»æ¯ä¸ªç±»ä¸­çš„å¤šä¸ªåŸå‹åˆ†å¸ƒæ¥æ¢ç´¢ç±»å†…å˜åŒ–ï¼Œå¹¶å¼•å…¥ä¸€è‡´æ€§çº¦æŸæ¥ç¼“è§£çº¿æ€§åˆ†ç±»å™¨çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²æ—¨åœ¨ä»¥ä½“ç´ çº§åˆ«è¯†åˆ«è§£å‰–ç»“æ„ã€‚</li>
<li>åˆ†å‰²å‡†ç¡®æ€§ä¾èµ–äºåŒºåˆ†ä½“ç´ å·®å¼‚çš„èƒ½åŠ›ã€‚</li>
<li>ä¼ ç»Ÿçº¿æ€§åˆ†ç±»å™¨åœ¨æ•æ‰ç±»å†…ç»†å¾®å·®å¼‚æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¤šåŸå‹åµŒå…¥ç²¾ä¿®çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡èšç±»æ¯ä¸ªç±»åˆ«ä¸­çš„å¤šä¸ªåŸå‹åˆ†å¸ƒæ¥æ¢ç´¢ç±»å†…å˜åŒ–ã€‚</li>
<li>å¼•å…¥ä¸€è‡´æ€§çº¦æŸï¼Œç»“åˆçº¿æ€§åˆ†ç±»å™¨ä¸åŸå‹åˆ†ç±»å™¨è¿›è¡Œä¸åŒç²’åº¦åˆ†ç±»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14343">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76015879a8e2b2ac4c9f5cd2867bae8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c476b987e754b7dca10ceb90109c8863.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4eb2cc3f46b518770f3d1916b6486a18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d429afa5baff76ffc093c418acecbe29.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Modular-Edge-Device-Network-for-Surgery-Digitalization"><a href="#A-Modular-Edge-Device-Network-for-Surgery-Digitalization" class="headerlink" title="A Modular Edge Device Network for Surgery Digitalization"></a>A Modular Edge Device Network for Surgery Digitalization</h2><p><strong>Authors:Vincent Schorp, FrÃ©dÃ©ric Giraud, Gianluca PargÃ¤tzi, Michael WÃ¤spe, Lorenzo von Ritter-Zahony, Marcel Wegmann, John Garcia Henao, Dominique Cachin, Sebastiano Caprara, Philipp FÃ¼rnstahl, Fabio Carrillo</strong></p>
<p>Future surgical care demands real-time, integrated data to drive informed decision-making and improve patient outcomes. The pressing need for seamless and efficient data capture in the OR motivates our development of a modular solution that bridges the gap between emerging machine learning techniques and interventional medicine. We introduce a network of edge devices, called Data Hubs (DHs), that interconnect diverse medical sensors, imaging systems, and robotic tools via optical fiber and a centralized network switch. Built on the NVIDIA Jetson Orin NX, each DH supports multiple interfaces (HDMI, USB-C, Ethernet) and encapsulates device-specific drivers within Docker containers using the Isaac ROS framework and ROS2. A centralized user interface enables straightforward configuration and real-time monitoring, while an Nvidia DGX computer provides state-of-the-art data processing and storage. We validate our approach through an ultrasound-based 3D anatomical reconstruction experiment that combines medical imaging, pose tracking, and RGB-D data acquisition. </p>
<blockquote>
<p>æœªæ¥çš„å¤–ç§‘æ‰‹æœ¯æŠ¤ç†éœ€æ±‚è¦æ±‚å®æ—¶é›†æˆæ•°æ®ä»¥æ”¯æŒåšå‡ºæ˜æ™ºçš„å†³ç­–å¹¶æ”¹å–„æ‚£è€…ç»“æœã€‚æ‰‹æœ¯å®¤å†…æ— ç¼é«˜æ•ˆçš„æ•°æ®é‡‡é›†éœ€æ±‚ä¿ƒä½¿æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ¨¡å—åŒ–è§£å†³æ–¹æ¡ˆï¼Œä»¥å¼¥æ–°å…´æœºå™¨å­¦ä¹ æŠ€æœ¯ä¸ä»‹å…¥åŒ»å­¦ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¾¹ç¼˜è®¾å¤‡ç½‘ç»œï¼Œç§°ä¸ºæ•°æ®ä¸­å¿ƒï¼ˆDHsï¼‰ï¼Œå®ƒé€šè¿‡å…‰çº¤å’Œé›†ä¸­å¼ç½‘ç»œäº¤æ¢æœºäº’è”å„ç§åŒ»ç–—ä¼ æ„Ÿå™¨ã€æˆåƒç³»ç»Ÿå’Œæœºå™¨äººå·¥å…·ã€‚æ¯ä¸ªæ•°æ®ä¸­å¿ƒéƒ½åŸºäºNVIDIA Jetson Orin NXæ„å»ºï¼Œæ”¯æŒå¤šä¸ªæ¥å£ï¼ˆHDMIã€USB-Cã€ä»¥å¤ªç½‘ï¼‰ï¼Œå¹¶ä½¿ç”¨Isaac ROSæ¡†æ¶å’ŒROS2åœ¨Dockerå®¹å™¨ä¸­å°è£…è®¾å¤‡ç‰¹å®šé©±åŠ¨ç¨‹åºã€‚é›†ä¸­å¼ç”¨æˆ·ç•Œé¢å¯å®ç°ç®€æ˜“é…ç½®å’Œå®æ—¶ç›‘æ§ï¼Œè€ŒNvidia DGXè®¡ç®—æœºåˆ™æä¾›å…ˆè¿›çš„æ•°æ®å¤„ç†å’Œå­˜å‚¨åŠŸèƒ½ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºè¶…å£°çš„3Dè§£å‰–é‡å»ºå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯¥å®éªŒç»“åˆäº†åŒ»å­¦æˆåƒã€å§¿æ€è·Ÿè¸ªå’ŒRGB-Dæ•°æ®é‡‡é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14049v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰‹æœ¯æœªæ¥éœ€æ±‚å®æ—¶ã€æ•´åˆæ•°æ®ä»¥æ¨åŠ¨å†³ç­–åˆ¶å®šå¹¶æ”¹å–„æ‚£è€…ç»“æœã€‚ä¸ºè§£å†³æ‰‹æœ¯å®¤æ•°æ®æ— ç¼é«˜æ•ˆé‡‡é›†çš„è¿«åˆ‡éœ€æ±‚ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ¨¡å—åŒ–è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯æ¡¥æ¥æ–°å…´æœºå™¨å­¦ä¹ æŠ€æœ¯ä¸ä»‹å…¥åŒ»å­¦ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬å¼•å…¥äº†è¾¹ç¼˜è®¾å¤‡ç½‘ç»œï¼Œç§°ä¸ºæ•°æ®ä¸­å¿ƒï¼ˆDHsï¼‰ï¼Œé€šè¿‡å…‰çº¤å’Œé›†ä¸­å¼ç½‘ç»œäº¤æ¢æœºäº’è”å„ç§åŒ»ç–—ä¼ æ„Ÿå™¨ã€æˆåƒç³»ç»Ÿå’Œæœºå™¨äººå·¥å…·ã€‚æ¯ä¸ªæ•°æ®ä¸­å¿ƒéƒ½åŸºäºNVIDIA Jetson Orin NXæ„å»ºï¼Œæ”¯æŒå¤šä¸ªæ¥å£ï¼ˆHDMIã€USB-Cã€ä»¥å¤ªç½‘ï¼‰ï¼Œå¹¶ä½¿ç”¨Isaac ROSæ¡†æ¶å’ŒROS2å°è£…è®¾å¤‡ç‰¹å®šé©±åŠ¨ç¨‹åºåœ¨Dockerå®¹å™¨ä¸­ã€‚ä¸€ä¸ªé›†ä¸­çš„ç”¨æˆ·ç•Œé¢å¯å®ç°ç®€æ˜“é…ç½®å’Œå®æ—¶ç›‘æ§ï¼Œè€ŒNvidia DGXè®¡ç®—æœºåˆ™æä¾›å…ˆè¿›çš„æ•°æ®å¤„ç†å’Œå­˜å‚¨åŠŸèƒ½ã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆåŒ»å­¦æˆåƒã€å§¿æ€è¿½è¸ªå’ŒRGB-Dæ•°æ®é‡‡é›†çš„è¶…å£°ä¸‰ç»´é‡å»ºå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœªæ¥æ‰‹æœ¯éœ€æ±‚å®æ—¶æ•´åˆæ•°æ®ä»¥æé«˜å†³ç­–æ•ˆç‡å’Œæ‚£è€…æ²»ç–—æ•ˆæœã€‚</li>
<li>ä»‹ç»äº†è¾¹ç¼˜è®¾å¤‡ç½‘ç»œï¼ˆæ•°æ®ä¸­æ¢ï¼‰ä½œä¸ºæ‰‹æœ¯å®¤æ•°æ®æ”¶é›†çš„æ¨¡å—åŒ–è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ•°æ®ä¸­æ¢å¯è¿æ¥å¤šæ ·åŒ–çš„åŒ»ç–—ä¼ æ„Ÿå™¨ã€æˆåƒç³»ç»Ÿå’Œæœºå™¨äººå·¥å…·ã€‚</li>
<li>æ•°æ®ä¸­æ¢åŸºäºNVIDIA Jetson Orin NXæ„å»ºï¼Œå…·æœ‰å¤šç§æ¥å£å¹¶æ”¯æŒå¤šç§è®¾å¤‡é©±åŠ¨ç¨‹åºå°è£…ã€‚</li>
<li>é‡‡ç”¨Isaac ROSæ¡†æ¶å’ŒROS2å®ç°è®¾å¤‡é©±åŠ¨çš„å°è£…åŠé«˜æ•ˆæ•°æ®å¤„ç†ã€‚</li>
<li>é›†ä¸­åŒ–çš„ç”¨æˆ·ç•Œé¢æä¾›ç®€æ˜“é…ç½®å’Œå®æ—¶ç›‘æ§åŠŸèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f1d0e906d3145e252fba34c8500b936f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48ab2e11bcd41328db193b2f7c31b0aa.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Boosting-Semi-Supervised-Medical-Image-Segmentation-via-Masked-Image-Consistency-and-Discrepancy-Learning"><a href="#Boosting-Semi-Supervised-Medical-Image-Segmentation-via-Masked-Image-Consistency-and-Discrepancy-Learning" class="headerlink" title="Boosting Semi-Supervised Medical Image Segmentation via Masked Image   Consistency and Discrepancy Learning"></a>Boosting Semi-Supervised Medical Image Segmentation via Masked Image   Consistency and Discrepancy Learning</h2><p><strong>Authors:Pengcheng Zhou, Lantian Zhang, Wei Li</strong></p>
<p>Semi-supervised learning is of great significance in medical image segmentation by exploiting unlabeled data. Among its strategies, the co-training framework is prominent. However, previous co-training studies predominantly concentrate on network initialization variances and pseudo-label generation, while overlooking the equilibrium between information interchange and model diversity preservation. In this paper, we propose the Masked Image Consistency and Discrepancy Learning (MICD) framework with three key modules. The Masked Cross Pseudo Consistency (MCPC) module enriches context perception and small sample learning via pseudo-labeling across masked-input branches. The Cross Feature Consistency (CFC) module fortifies information exchange and model robustness by ensuring decoder feature consistency. The Cross Model Discrepancy (CMD) module utilizes EMA teacher networks to oversee outputs and preserve branch diversity. Together, these modules address existing limitations by focusing on fine-grained local information and maintaining diversity in a heterogeneous framework. Experiments on two public medical image datasets, AMOS and Synapse, demonstrate that our approach outperforms state-of-the-art methods. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®è¿›è¡ŒåŠç›‘ç£å­¦ä¹ å…·æœ‰é‡è¦æ„ä¹‰ã€‚åœ¨å…¶ç­–ç•¥ä¸­ï¼ŒååŒè®­ç»ƒæ¡†æ¶å°¤ä¸ºçªå‡ºã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ååŒè®­ç»ƒç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç½‘ç»œåˆå§‹åŒ–å·®å¼‚å’Œä¼ªæ ‡ç­¾ç”Ÿæˆä¸Šï¼Œå¿½è§†äº†ä¿¡æ¯äº¤æ¢ä¸æ¨¡å‹å¤šæ ·æ€§ä¿æŒä¹‹é—´çš„å¹³è¡¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¸¦æœ‰ä¸‰ä¸ªå…³é”®æ¨¡å—çš„Masked Image Consistency and Discrepancy Learning (MICD)æ¡†æ¶ã€‚å…¶ä¸­ï¼ŒMasked Cross Pseudo Consistency (MCPC)æ¨¡å—é€šè¿‡è·¨æ©ç è¾“å…¥åˆ†æ”¯çš„ä¼ªæ ‡ç­¾ä¸°å¯Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œå°æ ·æœ¬å­¦ä¹ ã€‚Cross Feature Consistency (CFC)æ¨¡å—é€šè¿‡ç¡®ä¿è§£ç å™¨ç‰¹å¾ä¸€è‡´æ€§ï¼Œå¼ºåŒ–ä¿¡æ¯äº¤æ¢å’Œæ¨¡å‹ç¨³å¥æ€§ã€‚Cross Model Discrepancy (CMD)æ¨¡å—åˆ©ç”¨EMAæ•™å¸ˆç½‘ç»œæ¥ç›‘ç£è¾“å‡ºå¹¶ä¿æŒåˆ†æ”¯å¤šæ ·æ€§ã€‚è¿™ä¸‰ä¸ªæ¨¡å—å…±åŒè§£å†³äº†ç°æœ‰é—®é¢˜ï¼Œé€šè¿‡å…³æ³¨ç²¾ç»†çš„å±€éƒ¨ä¿¡æ¯å¹¶åœ¨å¼‚æ„æ¡†æ¶ä¸­ä¿æŒå¤šæ ·æ€§ã€‚åœ¨å…¬å…±åŒ»å­¦å›¾åƒæ•°æ®é›†AMOSå’ŒSynapseä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14013v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­åŠç›‘ç£å­¦ä¹ åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå…¶ä¸­ååŒè®­ç»ƒæ¡†æ¶å°¤ä¸ºçªå‡ºã€‚ç„¶è€Œï¼Œä»¥å¾€çš„ååŒè®­ç»ƒç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç½‘ç»œåˆå§‹åŒ–å·®å¼‚å’Œä¼ªæ ‡ç­¾ç”Ÿæˆä¸Šï¼Œå¿½è§†äº†ä¿¡æ¯äº¤æ¢ä¸æ¨¡å‹å¤šæ ·æ€§ä¿æŒä¹‹é—´çš„å¹³è¡¡ã€‚æœ¬æ–‡æå‡ºå¸¦æœ‰ä¸‰ä¸ªå…³é”®æ¨¡å—çš„Masked Image Consistency and Discrepancy Learningï¼ˆMICDï¼‰æ¡†æ¶ã€‚å…¶ä¸­ï¼ŒMasked Cross Pseudo Consistencyï¼ˆMCPCï¼‰æ¨¡å—é€šè¿‡è·¨æ©è†œè¾“å…¥åˆ†æ”¯çš„ä¼ªæ ‡ç­¾ä¸°å¯Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œå°æ ·æœ¬å­¦ä¹ ï¼›Cross Feature Consistencyï¼ˆCFCï¼‰æ¨¡å—é€šè¿‡ç¡®ä¿è§£ç å™¨ç‰¹å¾ä¸€è‡´æ€§ï¼Œå¼ºåŒ–ä¿¡æ¯äº¤æ¢å’Œæ¨¡å‹ç¨³å¥æ€§ï¼›Cross Model Discrepancyï¼ˆCMDï¼‰æ¨¡å—åˆ©ç”¨EMAæ•™å¸ˆç½‘ç»œç›‘ç£è¾“å‡ºå¹¶ä¿æŒåˆ†æ”¯å¤šæ ·æ€§ã€‚è¿™ä¸‰ä¸ªæ¨¡å—å…±åŒè§£å†³äº†ç°æœ‰é™åˆ¶ï¼Œä¸“æ³¨äºç²¾ç»†çš„å±€éƒ¨ä¿¡æ¯ï¼Œå¹¶åœ¨å¼‚è´¨æ¡†æ¶ä¸­ä¿æŒå¤šæ ·æ€§ã€‚åœ¨å…¬å…±åŒ»å­¦å›¾åƒæ•°æ®é›†AMOSå’ŒSynapseä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>ä»¥å¾€çš„ååŒè®­ç»ƒç ”ç©¶ä¸»è¦å…³æ³¨ç½‘ç»œåˆå§‹åŒ–å’Œä¼ªæ ‡ç­¾ç”Ÿæˆï¼Œå¿½ç•¥äº†ä¿¡æ¯äº¤æ¢ä¸æ¨¡å‹å¤šæ ·æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>æå‡ºçš„MICDæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜å¹¶æ”¹è¿›åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ•ˆæœã€‚</li>
<li>MCPCæ¨¡å—é€šè¿‡ä¼ªæ ‡ç­¾ä¸°å¯Œä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œå°æ ·æœ¬å­¦ä¹ ã€‚</li>
<li>CFCæ¨¡å—å¼ºåŒ–ä¿¡æ¯äº¤æ¢å’Œæ¨¡å‹ç¨³å¥æ€§ï¼Œç¡®ä¿è§£ç å™¨ç‰¹å¾ä¸€è‡´æ€§ã€‚</li>
<li>CMDæ¨¡å—åˆ©ç”¨EMAæ•™å¸ˆç½‘ç»œæ¥ç›‘ç£è¾“å‡ºå¹¶ä¿æŒæ¨¡å‹åˆ†æ”¯çš„å¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3a6abfd4b54ffe4ae96252e65256cbb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26f2be627ebcda6c58934b959bcc0d11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3bab950967a7c2807d2d3ba274e4cbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-860dc841b1becabb8f26a022dd6d6956.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3c0d927f998020e4d7c65976b84b0e3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Striving-for-Simplicity-Simple-Yet-Effective-Prior-Aware-Pseudo-Labeling-for-Semi-Supervised-Ultrasound-Image-Segmentation"><a href="#Striving-for-Simplicity-Simple-Yet-Effective-Prior-Aware-Pseudo-Labeling-for-Semi-Supervised-Ultrasound-Image-Segmentation" class="headerlink" title="Striving for Simplicity: Simple Yet Effective Prior-Aware   Pseudo-Labeling for Semi-Supervised Ultrasound Image Segmentation"></a>Striving for Simplicity: Simple Yet Effective Prior-Aware   Pseudo-Labeling for Semi-Supervised Ultrasound Image Segmentation</h2><p><strong>Authors:Yaxiong Chen, Yujie Wang, Zixuan Zheng, Jingliang Hu, Yilei Shi, Shengwu Xiong, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>Medical ultrasound imaging is ubiquitous, but manual analysis struggles to keep pace. Automated segmentation can help but requires large labeled datasets, which are scarce. Semi-supervised learning leveraging both unlabeled and limited labeled data is a promising approach. State-of-the-art methods use consistency regularization or pseudo-labeling but grow increasingly complex. Without sufficient labels, these models often latch onto artifacts or allow anatomically implausible segmentations. In this paper, we present a simple yet effective pseudo-labeling method with an adversarially learned shape prior to regularize segmentations. Specifically, we devise an encoder-twin-decoder network where the shape prior acts as an implicit shape model, penalizing anatomically implausible but not ground-truth-deviating predictions. Without bells and whistles, our simple approach achieves state-of-the-art performance on two benchmarks under different partition protocols. We provide a strong baseline for future semi-supervised medical image segmentation. Code is available at <a target="_blank" rel="noopener" href="https://github.com/WUTCM-Lab/Shape-Prior-Semi-Seg">https://github.com/WUTCM-Lab/Shape-Prior-Semi-Seg</a>. </p>
<blockquote>
<p>åŒ»å­¦è¶…å£°æˆåƒæŠ€æœ¯éå¸¸æ™®éï¼Œä½†æ‰‹åŠ¨åˆ†æå¾ˆéš¾è·Ÿä¸Šå…¶å‘å±•çš„æ­¥ä¼ã€‚è™½ç„¶è‡ªåŠ¨åŒ–åˆ†å‰²æŠ€æœ¯å¯ä»¥è¾…åŠ©åˆ†æï¼Œä½†éœ€è¦å¤§é‡çš„æ ‡è®°æ•°æ®é›†ï¼Œè€Œè¿™äº›æ•°æ®å´å¾ˆç¨€ç¼ºã€‚åˆ©ç”¨æ— æ ‡ç­¾å’Œæœ‰é™æ ‡ç­¾æ•°æ®çš„åŠç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§å¾ˆæœ‰å‰æ™¯çš„æ–¹æ³•ã€‚æœ€å…ˆè¿›çš„æ–¹æ³•ä½¿ç”¨ä¸€è‡´æ€§æ­£åˆ™åŒ–æˆ–ä¼ªæ ‡ç­¾æŠ€æœ¯ï¼Œä½†æ¨¡å‹æ—¥ç›Šå¤æ‚ã€‚è‹¥æ²¡æœ‰è¶³å¤Ÿçš„æ ‡ç­¾ï¼Œè¿™äº›æ¨¡å‹å¾€å¾€ä¼šä¾é™„äºä¼ªåƒæˆ–å…è®¸è§£å‰–ä¸Šä¸å¯èƒ½çš„åˆ†å‰²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œåˆ©ç”¨å¯¹æŠ—å­¦ä¹ çš„å½¢çŠ¶å…ˆéªŒæ¥è§„èŒƒåˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç¼–ç å™¨-å­ªç”Ÿè§£ç å™¨ç½‘ç»œï¼Œå…¶ä¸­å½¢çŠ¶å…ˆéªŒå……å½“éšå¼å½¢çŠ¶æ¨¡å‹ï¼Œå¯¹è§£å‰–ä¸Šä¸å¯èƒ½ä½†å¹¶éåç¦»çœŸå®æƒ…å†µçš„é¢„æµ‹è¿›è¡Œæƒ©ç½šã€‚æˆ‘ä»¬çš„ç®€å•æ–¹æ³•åœ¨ä¸åŒçš„åˆ†åŒºåè®®ä¸‹å®ç°äº†ä¸¤ä¸ªåŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œä¸ºæœªæ¥åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†å¼ºæœ‰åŠ›çš„åŸºå‡†ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WUTCM-Lab/Shape-Prior-Semi-Seg%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/WUTCM-Lab/Shape-Prior-Semi-Segæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13987v1">PDF</a> MICCAI 2024</p>
<p><strong>Summary</strong><br>     åŒ»å­¦è¶…å£°æˆåƒå¹¿æ³›åº”ç”¨ï¼Œä½†æ‰‹åŠ¨åˆ†æéš¾ä»¥è·Ÿä¸Šé€Ÿåº¦ã€‚è‡ªåŠ¨åŒ–åˆ†å‰²å¯å¸®åŠ©ä½†éœ€æ±‚å¤§é‡æ ‡è®°æ•°æ®é›†ï¼Œè¿™äº›å¾ˆç¨€ç¼ºã€‚åŠç›‘ç£å­¦ä¹ åˆ©ç”¨æ— æ ‡ç­¾å’Œæœ‰é™æ ‡ç­¾æ•°æ®æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹æ³•ã€‚å½“å‰æ–¹æ³•ä½¿ç”¨ä¸€è‡´æ€§æ­£åˆ™åŒ–æˆ–ä¼ªæ ‡ç­¾ä½†è¶Šæ¥è¶Šå¤æ‚ã€‚æ²¡æœ‰è¶³å¤Ÿæ ‡ç­¾ï¼Œè¿™äº›æ¨¡å‹å¸¸å› é™„ç€äºä¼ªå½±æˆ–å…è®¸è§£å‰–ä¸Šä¸åˆç†åˆ†å‰²ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„ä¼ªæ ‡ç­¾æ–¹æ³•ä¸å¯¹æŠ—è®­ç»ƒå½¢çŠ¶å…ˆéªŒåˆ†å‰²æ–¹æ³•ç›¸ç»“åˆã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç¼–ç å™¨å’ŒåŒèƒèƒè§£ç å™¨ç½‘ç»œï¼Œå…¶ä¸­å½¢çŠ¶å…ˆéªŒå……å½“éšæ€§å½¢çŠ¶æ¨¡å‹ï¼Œä»¥è§£å‰–ä¸å¯èƒ½ä½œä¸ºé¢„æµ‹ç½šæ¬¾åŸºå‡†ä½†ä¸åº”åç¦»çœŸå®å€¼ã€‚æˆ‘ä»¬çš„ç®€å•æ–¹æ³•åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸‹å®ç°äº†ä¸šç•Œé¢†å…ˆæ€§èƒ½ï¼Œä¸ºæœªæ¥çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†å¼ºæœ‰åŠ›çš„åŸºçº¿ã€‚ä»£ç å…¬å¼€äºï¼š<a target="_blank" rel="noopener" href="https://github.com/WUTCM-Lab/Shape-Prior-Semi-Seg">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦è¶…å£°æˆåƒå¹¿æ³›åº”ç”¨ï¼Œä½†æ‰‹åŠ¨åˆ†æè€—æ—¶è€—åŠ›ï¼Œå¯»æ±‚è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆæˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚</li>
<li>è‡ªåŠ¨åŒ–åˆ†å‰²æŠ€æœ¯éœ€è¦å¤§é‡æ ‡è®°æ•°æ®é›†ï¼Œä½†å®é™…ä¸­è¿™äº›èµ„æºç¨€ç¼ºã€‚</li>
<li>åŠç›‘ç£å­¦ä¹ æ–¹æ³•èƒ½æœ‰æ•ˆåˆ©ç”¨æœ‰é™çš„æ ‡ç­¾æ•°æ®å’Œæ— æ ‡ç­¾æ•°æ®ï¼Œå±•ç°å‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚</li>
<li>å½“å‰å¤æ‚æ¨¡å‹æ–¹æ³•å®¹æ˜“å‡ºç°è§£å‰–ä¸Šä¸åˆç†çš„åˆ†å‰²ç»“æœã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå¯¹æŠ—æ€§è®­ç»ƒå½¢çŠ¶å…ˆéªŒçš„ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œå®ç°äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•è®¾è®¡äº†ä¸€ç§ç®€å•çš„ç¼–ç å™¨å’ŒåŒèƒèƒè§£ç å™¨ç½‘ç»œç»“æ„ï¼Œå…¶ä¸­å½¢çŠ¶å…ˆéªŒèµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13987">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8ea46453c189b0c5f3d229421b2f74c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c0c67caf9100a8560bbecd9a8f47a75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-008178297ce3e7904b9af5e71c2146c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f1ebebc63f50f7721e673df5b470dc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17aee324247c80d0a4d9d483e5596966.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models"><a href="#Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models" class="headerlink" title="Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models"></a>Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models</h2><p><strong>Authors:Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang</strong></p>
<p>Vision-language models (VLMs) have advanced reasoning in natural scenes, but their role in medical imaging remains underexplored. Medical reasoning tasks demand robust image analysis and well-justified answers, posing challenges due to the complexity of medical images. Transparency and trustworthiness are essential for clinical adoption and regulatory compliance. We introduce Med-R1, a framework exploring reinforcement learning (RL) to enhance VLMsâ€™ generalizability and trustworthiness in medical reasoning. Leveraging the DeepSeek strategy, we employ Group Relative Policy Optimization (GRPO) to guide reasoning paths via reward signals. Unlike supervised fine-tuning (SFT), which often overfits and lacks generalization, RL fosters robust and diverse reasoning. Med-R1 is evaluated across eight medical imaging modalities: CT, MRI, Ultrasound, Dermoscopy, Fundus Photography, Optical Coherence Tomography (OCT), Microscopy, and X-ray Imaging. Compared to its base model, Qwen2-VL-2B, Med-R1 achieves a 29.94% accuracy improvement and outperforms Qwen2-VL-72B, which has 36 times more parameters. Testing across five question types-modality recognition, anatomy identification, disease diagnosis, lesion grading, and biological attribute analysis Med-R1 demonstrates superior generalization, exceeding Qwen2-VL-2B by 32.06% and surpassing Qwen2-VL-72B in question-type generalization. These findings show that RL improves medical reasoning and enables parameter-efficient models to outperform significantly larger ones. With interpretable reasoning outputs, Med-R1 represents a promising step toward generalizable, trustworthy, and clinically viable medical VLMs. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªç„¶åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›å·²ç»å¾—åˆ°æå‡ï¼Œä½†å®ƒä»¬åœ¨åŒ»å­¦æˆåƒé¢†åŸŸçš„åº”ç”¨ä»ç„¶ç¼ºä¹è¶³å¤Ÿçš„æ¢ç´¢ã€‚åŒ»å­¦æ¨ç†ä»»åŠ¡éœ€è¦å¯é çš„å›¾åƒåˆ†æå’Œåˆç†çš„ç­”æ¡ˆï¼Œç”±äºåŒ»å­¦å›¾åƒçš„å¤æ‚æ€§ï¼Œè¿™æ„æˆäº†æŒ‘æˆ˜ã€‚é€æ˜åº¦å’Œå¯ä¿¡åº¦å¯¹äºä¸´åºŠé‡‡ç”¨å’Œæ³•è§„åˆè§„è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†Med-R1æ¡†æ¶ï¼Œæ¢ç´¢å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥å¢å¼ºVLMsåœ¨åŒ»å­¦æ¨ç†ä¸­çš„é€šç”¨æ€§å’Œå¯ä¿¡åº¦ã€‚åˆ©ç”¨DeepSeekç­–ç•¥ï¼Œæˆ‘ä»¬é‡‡ç”¨Group Relative Policy Optimization (GRPO)é€šè¿‡å¥–åŠ±ä¿¡å·å¼•å¯¼æ¨ç†è·¯å¾„ã€‚ä¸é€šå¸¸å¯¼è‡´è¿‡åº¦æ‹Ÿåˆå’Œç¼ºä¹æ³›åŒ–èƒ½åŠ›çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸åŒï¼Œå¼ºåŒ–å­¦ä¹ ä¿ƒè¿›ç¨³å¥å’Œå¤šæ ·åŒ–çš„æ¨ç†ã€‚Med-R1åœ¨å…«ç§åŒ»å­¦æˆåƒæ¨¡å¼ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šCTã€MRIã€è¶…å£°ã€çš®è‚¤é•œæ£€æŸ¥ã€çœ¼åº•æ‘„å½±ã€å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰ã€æ˜¾å¾®é•œå’ŒXå°„çº¿æˆåƒã€‚ä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼ŒMed-R1å®ç°äº†29.94%çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶è¶…è¶Šäº†å‚æ•°å¤šå‡º36å€çš„Qwen2-VL-72Bã€‚åœ¨äº”ç§é—®é¢˜ç±»å‹ï¼ˆæ¨¡æ€è¯†åˆ«ã€è§£å‰–ç»“æ„è¯†åˆ«ã€ç–¾ç—…è¯Šæ–­ã€ç—…å˜åˆ†çº§å’Œç”Ÿç‰©å±æ€§åˆ†æï¼‰çš„æµ‹è¯•ä¸Šï¼ŒMed-R1å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œç›¸è¾ƒäºQwen2-VL-2Bæé«˜32.06%ï¼Œå¹¶åœ¨é—®é¢˜ç±»å‹æ³›åŒ–ä¸Šè¶…è¶Šäº†Qwen2-VL-72Bã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½æå‡åŒ»å­¦æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä½¿å¾—å‚æ•°æ•ˆç‡æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šè§„æ¨¡æ›´å¤§çš„æ¨¡å‹ã€‚Med-R1çš„å¯è§£é‡Šæ¨ç†è¾“å‡ºä»£è¡¨äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ­¥éª¤ï¼Œæœç€é€šç”¨åŒ–ã€å¯ä¿¡èµ–å’Œä¸´åºŠä¸Šå¯è¡Œçš„åŒ»å­¦VLMsè¿ˆè¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13939v1">PDF</a> </p>
<p><strong>Summary</strong><br>    Med-R1æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯æé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—æ¨ç†ä¸­çš„é€šç”¨æ€§å’Œä¿¡ä»»åº¦ã€‚é€šè¿‡DeepSeekç­–ç•¥å¹¶é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰å¼•å¯¼æ¨ç†è·¯å¾„ï¼Œè¯¥æ¡†æ¶è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚ä¸åŸºå‡†æ¨¡å‹å’Œå¤§å‹æ¨¡å‹ç›¸æ¯”ï¼ŒMed-R1åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šçš„å‡†ç¡®æ€§æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†è§£é‡Šæ€§å¼ºã€é€šç”¨æ€§å¼ºå’Œå€¼å¾—ä¿¡èµ–çš„åŒ»å­¦VLMsçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-R1æ¡†æ¶é¦–æ¬¡æ¢ç´¢äº†å¼ºåŒ–å­¦ä¹ åœ¨åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œä»¥æé«˜å…¶é€šç”¨æ€§å’Œä¿¡ä»»åº¦ã€‚</li>
<li>é€šè¿‡DeepSeekç­–ç•¥å’ŒGRPOä¼˜åŒ–ï¼ŒMed-R1èƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼æ¨¡å‹æ¨ç†è·¯å¾„ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼Œå¼ºåŒ–å­¦ä¹ æœ‰åŠ©äºå®ç°æ›´ç¨³å¥å’Œå¤šæ ·åŒ–çš„æ¨ç†ã€‚</li>
<li>Med-R1åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶å¹¿æ³›é€‚ç”¨æ€§ã€‚</li>
<li>Med-R1åœ¨å‡†ç¡®æ€§ä¸Šè¾ƒåŸºå‡†æ¨¡å‹æœ‰æ‰€æé«˜ï¼Œä¸”ä¼˜äºå¤§å‹æ¨¡å‹ã€‚</li>
<li>Med-R1åœ¨å¤šç§é—®é¢˜ç±»å‹ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d41cf1f555ee818bedb1d3e9600d32f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-630d53122dda7aca60f3a3ccd8c64a19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-128122acf4102aa38fc1f1dd1300ff49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-815f2579c33a67045719bb040b3c9dab.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multi-Harmonic-Gridded-3D-Deconvolution-MH3D-for-Robust-and-Accurate-Image-Reconstruction-in-MPI-for-Single-Axis-Drive-Field-Scanners"><a href="#Multi-Harmonic-Gridded-3D-Deconvolution-MH3D-for-Robust-and-Accurate-Image-Reconstruction-in-MPI-for-Single-Axis-Drive-Field-Scanners" class="headerlink" title="Multi-Harmonic Gridded 3D Deconvolution (MH3D) for Robust and Accurate   Image Reconstruction in MPI for Single Axis Drive Field Scanners"></a>Multi-Harmonic Gridded 3D Deconvolution (MH3D) for Robust and Accurate   Image Reconstruction in MPI for Single Axis Drive Field Scanners</h2><p><strong>Authors:Toby Sanders, Justin J. Konkle, Erica E. Mason, Patrick W. Goodwill</strong></p>
<p>This article presents a new robust model for image reconstruction in magnetic particle imaging (MPI) for single-axis drive field scans, which is based on the deconvolution of gridded harmonic data. Gridded harmonic data, used commonly in MPI, does not map to underlying iron density but rather to the iron density convolved with the harmonic point-spread functions. We refer to the gridded harmonic data as harmonic portraits, since they only represent a portrait-like representation of the iron density, and a deconvolution method is implemented to reconstruct the true underlying density. The advantage of this new method is primarily in the intermediate data analysis that comes in the harmonic portrait domain, where we are able to perform artifact correction, parameter selection, and general data assessment and calibrations efficiently. Furthermore, we show with several examples that our new method closely compares qualitatively with current state-of-the-art image reconstruction models in MPI. While the general concept of gridding harmonic data in MPI is not new, the complete modeling and characterization in order to use the data for image reconstruction has remained an ongoing area of research. We provide detailed analysis, theoretical insights, and many nuanced techniques that make our new methodology and algorithm accurate and robust. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºç½‘æ ¼åŒ–è°æ³¢æ•°æ®è§£å·ç§¯çš„å•è½´é©±åŠ¨åœºæ‰«æç£ç²’å­æˆåƒï¼ˆMPIï¼‰å›¾åƒé‡å»ºæ–°æ¨¡å‹ã€‚ç½‘æ ¼åŒ–è°æ³¢æ•°æ®åœ¨MPIä¸­å¹¿æ³›ä½¿ç”¨ï¼Œå¹¶ä¸æ˜ å°„åˆ°åº•å±‚çš„é“å¯†åº¦ï¼Œè€Œæ˜¯æ˜ å°„åˆ°é“å¯†åº¦ä¸è°æ³¢ç‚¹æ‰©æ•£å‡½æ•°çš„å·ç§¯ã€‚æˆ‘ä»¬å°†ç½‘æ ¼åŒ–è°æ³¢æ•°æ®ç§°ä¸ºè°æ³¢è‚–åƒï¼Œå› ä¸ºå®ƒä»¬åªä»£è¡¨é“å¯†åº¦çš„è‚–åƒå¼è¡¨ç¤ºï¼Œå¹¶å®ç°äº†å·ç§¯æ–¹æ³•æ¥é‡å»ºçœŸæ­£çš„åº•å±‚å¯†åº¦ã€‚è¿™ç§æ–°æ–¹æ³•çš„ä¸»è¦ä¼˜ç‚¹åœ¨äºè°æ³¢è‚–åƒåŸŸä¸­çš„ä¸­é—´æ•°æ®åˆ†æï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨æ­¤æœ‰æ•ˆåœ°æ‰§è¡Œä¼ªå½±æ ¡æ­£ã€å‚æ•°é€‰æ‹©ä»¥åŠä¸€èˆ¬çš„æ•°æ®è¯„ä¼°å’Œæ ¡å‡†ã€‚æ­¤å¤–ï¼Œé€šè¿‡å‡ ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–°æ–¹æ³•åœ¨MPIå›¾åƒé‡å»ºæ¨¡å‹æ–¹é¢ä¸å½“å‰æœ€å…ˆè¿›æŠ€æœ¯è¿›è¡Œäº†å®šæ€§æ¯”è¾ƒã€‚è™½ç„¶MPIä¸­ç½‘æ ¼åŒ–è°æ³¢æ•°æ®çš„ä¸€èˆ¬æ¦‚å¿µå¹¶ä¸æ–°é¢–ï¼Œä½†ä¸ºäº†æ›´å¥½åœ°ä½¿ç”¨æ•°æ®è¿›è¡Œå›¾åƒé‡å»ºï¼Œå…¶å®Œæ•´å»ºæ¨¡å’Œè¡¨å¾ä¸€ç›´æ˜¯ç ”ç©¶é¢†åŸŸçš„ä¸€ä¸ªæŒç»­å…³æ³¨çš„ä¸»é¢˜ã€‚æˆ‘ä»¬æä¾›äº†è¯¦ç»†çš„åˆ†æã€ç†è®ºè§è§£å’Œè®¸å¤šå¾®å¦™çš„æŠ€æœ¯ï¼Œä½¿æˆ‘ä»¬çš„æ–°æ–¹æ³•å’Œç®—æ³•æ›´åŠ å‡†ç¡®å’Œç¨³å¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13802v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºç½‘æ ¼åŒ–è°æ³¢æ•°æ®è§£å·ç§¯çš„ç£å…±æŒ¯ç²’å­æˆåƒï¼ˆMPIï¼‰å•è½´é©±åŠ¨åœºæ‰«æå›¾åƒé‡å»ºæ–°æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨è°æ³¢è‚–åƒåŸŸè¿›è¡Œä¸­é—´æ•°æ®åˆ†æï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œä¼ªå½±æ ¡æ­£ã€å‚æ•°é€‰æ‹©ä»¥åŠæ•°æ®è¯„ä¼°å’Œæ ¡å‡†ã€‚é€šè¿‡å¤šä¸ªå®ä¾‹å¯¹æ¯”ï¼Œè¯¥æ–°æ–¹æ³•ä¸å½“å‰å…ˆè¿›çš„MPIå›¾åƒé‡å»ºæ¨¡å‹åœ¨å®šæ€§ä¸Šéå¸¸æ¥è¿‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„MPIå›¾åƒé‡å»ºæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºç½‘æ ¼åŒ–è°æ³¢æ•°æ®çš„è§£å·ç§¯ã€‚</li>
<li>ç½‘æ ¼åŒ–è°æ³¢æ•°æ®ï¼ˆç§°ä¸ºè°æ³¢è‚–åƒï¼‰åœ¨MPIä¸­ç”¨äºè¡¨ç¤ºé“å¯†åº¦çš„è‚–åƒå¼è¡¨ç¤ºï¼Œéœ€è¦é€šè¿‡è§£å·ç§¯æ–¹æ³•é‡å»ºçœŸå®çš„åŸºç¡€å¯†åº¦ã€‚</li>
<li>æ–°æ–¹æ³•çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºè°æ³¢è‚–åƒåŸŸä¸­çš„ä¸­é—´æ•°æ®åˆ†æï¼Œå¯é«˜æ•ˆè¿›è¡Œä¼ªå½±æ ¡æ­£ã€å‚æ•°é€‰æ‹©ä»¥åŠæ•°æ®è¯„ä¼°å’Œæ ¡å‡†ã€‚</li>
<li>é€šè¿‡å¤šä¸ªå®ä¾‹éªŒè¯ï¼Œæ–°æ–¹æ³•ä¸ç°æœ‰å…ˆè¿›MPIå›¾åƒé‡å»ºæ¨¡å‹åœ¨å®šæ€§ä¸Šéå¸¸æ¥è¿‘ã€‚</li>
<li>å°½ç®¡ç½‘æ ¼åŒ–è°æ³¢æ•°æ®åœ¨MPIä¸­çš„æ¦‚å¿µå¹¶éå…¨æ–°ï¼Œä½†å°†å…¶ç”¨äºå›¾åƒé‡å»ºçš„å®Œæ•´å»ºæ¨¡å’Œè¡¨å¾ä»æ˜¯ä¸€ä¸ªç ”ç©¶é¢†åŸŸã€‚</li>
<li>æ–‡ç« æä¾›äº†è¯¦ç»†çš„åˆ†æã€ç†è®ºè§è§£å’Œå¾®å¦™æŠ€æœ¯ï¼Œä½¿æ–°æ–¹æ³•å’Œç®—æ³•æ›´åŠ å‡†ç¡®å’Œç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5042f0b8e7f287347195b5d3b8f01b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f7092f9e70e755596a6bb3b43bb1722.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c039e6eac2498dafb5bf60b36a193a98.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SMILE-a-Scale-aware-Multiple-Instance-Learning-Method-for-Multicenter-STAS-Lung-Cancer-Histopathology-Diagnosis"><a href="#SMILE-a-Scale-aware-Multiple-Instance-Learning-Method-for-Multicenter-STAS-Lung-Cancer-Histopathology-Diagnosis" class="headerlink" title="SMILE: a Scale-aware Multiple Instance Learning Method for Multicenter   STAS Lung Cancer Histopathology Diagnosis"></a>SMILE: a Scale-aware Multiple Instance Learning Method for Multicenter   STAS Lung Cancer Histopathology Diagnosis</h2><p><strong>Authors:Liangrui Pan, Xiaoyu Li, Yutao Dou, Qiya Song, Jiadi Luo, Qingchun Liang, Shaoliang Peng</strong></p>
<p>Spread through air spaces (STAS) represents a newly identified aggressive pattern in lung cancer, which is known to be associated with adverse prognostic factors and complex pathological features. Pathologists currently rely on time consuming manual assessments, which are highly subjective and prone to variation. This highlights the urgent need for automated and precise diag nostic solutions. 2,970 lung cancer tissue slides are comprised from multiple centers, re-diagnosed them, and constructed and publicly released three lung cancer STAS datasets: STAS CSU (hospital), STAS TCGA, and STAS CPTAC. All STAS datasets provide corresponding pathological feature diagnoses and related clinical data. To address the bias, sparse and heterogeneous nature of STAS, we propose an scale-aware multiple instance learning(SMILE) method for STAS diagnosis of lung cancer. By introducing a scale-adaptive attention mechanism, the SMILE can adaptively adjust high attention instances, reducing over-reliance on local regions and promoting consistent detection of STAS lesions. Extensive experiments show that SMILE achieved competitive diagnostic results on STAS CSU, diagnosing 251 and 319 STAS samples in CPTAC andTCGA,respectively, surpassing clinical average AUC. The 11 open baseline results are the first to be established for STAS research, laying the foundation for the future expansion, interpretability, and clinical integration of computational pathology technologies. The datasets and code are available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/IJCAI25-1DA1">https://anonymous.4open.science/r/IJCAI25-1DA1</a>. </p>
<blockquote>
<p>é€šè¿‡ç©ºæ°”ä¼ æ’­çš„è‚ºç™Œæ–°ç—…ç†æ¨¡å¼ï¼ˆSTASï¼‰æ˜¯ä¸€ç§æ–°å‘ç°çš„ä¾µè¢­æ€§æ¨¡å¼ï¼Œä¸ä¸è‰¯é¢„åå› ç´ å’Œå¤æ‚çš„ç—…ç†ç‰¹å¾æœ‰å…³ã€‚ç—…ç†å­¦å®¶ç›®å‰ä¾èµ–äºè€—æ—¶çš„äººå·¥è¯„ä¼°ï¼Œè¿™äº›è¯„ä¼°å…·æœ‰å¾ˆå¤§çš„ä¸»è§‚æ€§å’Œå˜åŒ–æ€§ã€‚è¿™çªæ˜¾äº†å¯¹è‡ªåŠ¨åŒ–å’Œç²¾ç¡®è¯Šæ–­è§£å†³æ–¹æ¡ˆçš„è¿«åˆ‡éœ€æ±‚ã€‚æˆ‘ä»¬å¯¹æ¥è‡ªå¤šä¸ªä¸­å¿ƒçš„2970å¼ è‚ºç™Œç»„ç»‡åˆ‡ç‰‡è¿›è¡Œäº†é‡æ–°è¯Šæ–­ï¼Œå¹¶æ„å»ºäº†ä¸‰ä¸ªå…¬å¼€çš„è‚ºç™ŒSTASæ•°æ®é›†ï¼šSTAS CSUï¼ˆåŒ»é™¢ï¼‰ã€STAS TCGAå’ŒSTAS CPTACã€‚æ‰€æœ‰STASæ•°æ®é›†éƒ½æä¾›äº†ç›¸åº”çš„ç—…ç†ç‰¹å¾è¯Šæ–­å’Œç›¸å…³çš„ä¸´åºŠæ•°æ®ã€‚é’ˆå¯¹STASçš„åè§ã€ç¨€ç–æ€§å’Œå¼‚è´¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°ºåº¦æ„ŸçŸ¥çš„å¤šå®ä¾‹å­¦ä¹ ï¼ˆSMILEï¼‰æ–¹æ³•è¿›è¡Œè‚ºç™ŒSTASè¯Šæ–­ã€‚é€šè¿‡å¼•å…¥å°ºåº¦è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ï¼ŒSMILEå¯ä»¥è‡ªé€‚åº”åœ°è°ƒæ•´é«˜æ³¨æ„åŠ›å®ä¾‹ï¼Œå‡å°‘å¯¹å±€éƒ¨åŒºåŸŸçš„è¿‡åº¦ä¾èµ–ï¼Œä¿ƒè¿›STASç—…å˜çš„ä¸€è‡´æ£€æµ‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSMILEåœ¨STAS CSUä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¯Šæ–­ç»“æœï¼Œåœ¨CPTACå’ŒTCGAä¸­åˆ†åˆ«è¯Šæ–­äº†251ä¾‹å’Œ319ä¾‹STASæ ·æœ¬ï¼Œè¶…è¿‡äº†ä¸´åºŠå¹³å‡AUCå€¼ã€‚è¿™11ä¸ªå…¬å¼€åŸºå‡†ç»“æœå‡ä¸ºSTASç ”ç©¶çš„é¦–æ¬¡å»ºç«‹ï¼Œä¸ºæœªæ¥è®¡ç®—ç—…ç†å­¦æŠ€æœ¯çš„æ‰©å±•ã€å¯è§£é‡Šæ€§å’Œä¸´åºŠæ•´åˆå¥ å®šäº†åŸºç¡€ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/IJCAI25-1DA1%E8%8E%B7%E5%8F%96%E3%80%82">https://anonymous.4open.science/r/IJCAI25-1DA1è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13799v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€é¡¹å…³äºè‚ºç™Œæ–°å‘ç°çš„ç ”ç©¶ã€‚ç ”ç©¶ä¸­æå‡ºäº†ä¸€ç§åä¸ºSMILEçš„è‚ºç™ŒSTASè¯Šæ–­æ–¹æ³•ï¼Œé’ˆå¯¹ç›®å‰ç—…ç†å­¦ä¸­æ‰‹å·¥è¯„ä¼°æ—¶é—´é•¿ã€ä¸»è§‚æ€§é«˜ç­‰é—®é¢˜ï¼Œè¯¥æ³•å…·æœ‰è‡ªåŠ¨åŒ–å’Œç²¾ç¡®æ€§ä¼˜åŠ¿ã€‚ç ”ç©¶è€…å‘å¸ƒäº†ä¸‰ä¸ªè‚ºç™ŒSTASæ•°æ®é›†å¹¶è¿›è¡Œäº†æµ‹è¯•ï¼ŒSMILEè¡¨ç°å‡ºè‰¯å¥½çš„è¯Šæ–­æ•ˆæœï¼Œä¸ºè®¡ç®—ç—…ç†å­¦æŠ€æœ¯çš„æœªæ¥æ‰©å±•ã€è§£é‡Šå’Œä¸´åºŠæ•´åˆå¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä»‹ç»äº†æ–°å‘ç°çš„è‚ºç™Œæ‰©æ•£æ¨¡å¼â€”â€”é€šè¿‡ç©ºæ°”ç©ºé—´æ‰©æ•£ï¼ˆSTASï¼‰ï¼Œä¸ä¸è‰¯é¢„åå› ç´ å’Œå¤æ‚ç—…ç†ç‰¹å¾ç›¸å…³ã€‚</li>
<li>å½“å‰ç—…ç†å­¦è¯Šæ–­ä¸»è¦ä¾èµ–è€—æ—¶ä¸”ä¸»è§‚æ€§é«˜çš„æ‰‹åŠ¨è¯„ä¼°ï¼Œæ€¥éœ€è‡ªåŠ¨åŒ–å’Œç²¾ç¡®çš„è¯Šæ–­è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç ”ç©¶å‘å¸ƒäº†ä¸‰ä¸ªè‚ºç™ŒSTASæ•°æ®é›†ï¼šSTAS CSUã€STAS TCGAå’ŒSTAS CPTACï¼ŒåŒ…å«ç›¸åº”çš„ç—…ç†ç‰¹å¾è¯Šæ–­å’Œç›¸å…³çš„ä¸´åºŠæ•°æ®ã€‚</li>
<li>é’ˆå¯¹STASçš„åè§ã€ç¨€ç–å’Œå¼‚è´¨æ€§ç‰¹ç‚¹ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºSMILEçš„è¯Šæ–­æ–¹æ³•ã€‚</li>
<li>SMILEé€šè¿‡å¼•å…¥å°ºåº¦è‡ªé€‚åº”æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”è°ƒæ•´é«˜æ³¨æ„åŠ›å®ä¾‹ï¼Œå‡å°‘å±€éƒ¨åŒºåŸŸçš„è¿‡åº¦ä¾èµ–ï¼Œä¿ƒè¿›STASç—…å˜çš„ä¸€è‡´æ£€æµ‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSMILEåœ¨STAS CSUä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¯Šæ–­ç»“æœï¼Œå¹¶åœ¨CPTACå’ŒTCGAä¸­åˆ†åˆ«è¯Šæ–­å‡º251å’Œ319ä¸ªSTASæ ·æœ¬ï¼Œè¶…è¿‡äº†ä¸´åºŠå¹³å‡AUCã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13799">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-873b6f01c2842ff208c56bc1e4351d4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a3172b18e1061392617ce2f7837d296.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c66707b90b1288855486a5413b8027e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dcf7afd3e1dddcabc210293ae89c2d08.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="From-Pixels-to-Histopathology-A-Graph-Based-Framework-for-Interpretable-Whole-Slide-Image-Analysis"><a href="#From-Pixels-to-Histopathology-A-Graph-Based-Framework-for-Interpretable-Whole-Slide-Image-Analysis" class="headerlink" title="From Pixels to Histopathology: A Graph-Based Framework for Interpretable   Whole Slide Image Analysis"></a>From Pixels to Histopathology: A Graph-Based Framework for Interpretable   Whole Slide Image Analysis</h2><p><strong>Authors:Alexander Weers, Alexander H. Berger, Laurin Lux, Peter SchÃ¼ffler, Daniel Rueckert, Johannes C. Paetzold</strong></p>
<p>The histopathological classification of whole-slide images (WSIs) is a fundamental task in digital pathology; yet it requires extensive time and expertise from specialists. While deep learning methods show promising results, they typically process WSIs by dividing them into artificial patches, which inherently prevents a network from learning from the entire image context, disregards natural tissue structures and compromises interpretability. Our method overcomes this limitation through a novel graph-based framework that constructs WSI graph representations. The WSI-graph efficiently captures essential histopathological information in a compact form. We build tissue representations (nodes) that follow biological boundaries rather than arbitrary patches all while providing interpretable features for explainability. Through adaptive graph coarsening guided by learned embeddings, we progressively merge regions while maintaining discriminative local features and enabling efficient global information exchange. In our methodâ€™s final step, we solve the diagnostic task through a graph attention network. We empirically demonstrate strong performance on multiple challenging tasks such as cancer stage classification and survival prediction, while also identifying predictive factors using Integrated Gradients. Our implementation is publicly available at <a target="_blank" rel="noopener" href="https://github.com/HistoGraph31/pix2pathology">https://github.com/HistoGraph31/pix2pathology</a> </p>
<blockquote>
<p>å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„ç»„ç»‡ç—…ç†å­¦åˆ†ç±»æ˜¯æ•°å­—ç—…ç†å­¦ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œä½†è¿™éœ€è¦ä¸“å®¶çš„å¤§é‡æ—¶é—´å’Œä¸“ä¸šçŸ¥è¯†ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬é€šå¸¸é€šè¿‡å°†WSIåˆ†å‰²æˆäººå·¥è¡¥ä¸æ¥å¤„ç†ï¼Œè¿™å›ºæœ‰åœ°é˜»æ­¢ç½‘ç»œä»æ•´ä¸ªå›¾åƒä¸Šä¸‹æ–‡ä¸­å­¦ä¹ ï¼Œå¿½ç•¥äº†è‡ªç„¶ç»„ç»‡ç»“æ„å¹¶æŸå®³äº†å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åŸºäºå›¾çš„æ–°å‹æ¡†æ¶æ¥å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œè¯¥æ¡†æ¶æ„å»ºWSIå›¾è¡¨ç¤ºã€‚WSIå›¾ä»¥ç´§å‡‘çš„å½¢å¼æœ‰æ•ˆåœ°æ•è·äº†å…³é”®çš„ç»„ç»‡ç—…ç†å­¦ä¿¡æ¯ã€‚æˆ‘ä»¬æ„å»ºçš„ç»„ç»‡è¡¨ç¤ºï¼ˆèŠ‚ç‚¹ï¼‰éµå¾ªç”Ÿç‰©å­¦è¾¹ç•Œï¼Œè€Œä¸æ˜¯ä»»æ„è¡¥ä¸ï¼ŒåŒæ—¶æä¾›å¯è§£é‡Šç‰¹å¾ç”¨äºè§£é‡Šæ€§ã€‚é€šè¿‡ç”±å­¦ä¹ åµŒå…¥å¼•å¯¼çš„è‡ªé€‚åº”å›¾ç²—åŒ–ï¼Œæˆ‘ä»¬é€æ­¥åˆå¹¶åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒè¾¨åˆ«å±€éƒ¨ç‰¹å¾å¹¶å¯ç”¨é«˜æ•ˆçš„å…¨å±€ä¿¡æ¯äº¤æ¢ã€‚æˆ‘ä»¬æ–¹æ³•çš„æœ€åä¸€æ­¥æ˜¯é€šè¿‡å›¾æ³¨æ„åŠ›ç½‘ç»œæ¥è§£å†³è¯Šæ–­ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šå±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä¾‹å¦‚ç™Œç—‡åˆ†æœŸåˆ†ç±»å’Œç”Ÿå­˜é¢„æµ‹ï¼ŒåŒæ—¶ä½¿ç”¨é›†æˆæ¢¯åº¦è¯†åˆ«é¢„æµ‹å› ç´ ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/HistoGraph31/pix2pathology">https://github.com/HistoGraph31/pix2pathology</a> ä¸­å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11846v1">PDF</a> 11 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æ–°æ–¹æ³•ï¼Œç”¨äºè§£å†³æ•°å­—ç—…ç†å­¦ä¸­çš„å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰çš„ç—…ç†åˆ†ç±»é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºWSIå›¾è¡¨ç¤ºï¼Œæœ‰æ•ˆåœ°æ•è·äº†å…³é”®çš„ç»„ç»‡ç—…ç†å­¦ä¿¡æ¯ï¼Œæä¾›äº†å¯è§£é‡Šçš„ç‰¹å¾ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„è¯Šæ–­æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰æ—¶ï¼Œé€šå¸¸å°†å…¶åˆ’åˆ†ä¸ºäººå·¥è¡¥ä¸ï¼Œè¿™å¿½ç•¥äº†æ•´ä¸ªå›¾åƒçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€è‡ªç„¶ç»„ç»‡ç»“æ„ä»¥åŠè§£é‡Šæ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå›¾çš„æ–¹æ³•ï¼Œæ„å»ºäº†WSIå›¾è¡¨ç¤ºï¼Œä»¥å…‹æœè¿™ä¸€é™åˆ¶ã€‚</li>
<li>WSIå›¾è¡¨ç¤ºé€šè¿‡ç´§å‡‘çš„å½¢å¼æœ‰æ•ˆåœ°æ•è·äº†å…³é”®çš„ç»„ç»‡ç—…ç†å­¦ä¿¡æ¯ã€‚</li>
<li>è¯¥æ–¹æ³•æ„å»ºçš„ç»„ç»‡è¡¨ç¤ºï¼ˆèŠ‚ç‚¹ï¼‰éµå¾ªç”Ÿç‰©è¾¹ç•Œè€Œä¸æ˜¯ä»»æ„è¡¥ä¸ï¼Œæä¾›å¯è§£é‡Šçš„ç‰¹å¾ä»¥å¢å¼ºè§£é‡Šæ€§ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”å›¾ç²—åŒ–å¼•å¯¼å­¦ä¹ åµŒå…¥ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé€æ­¥åˆå¹¶åŒºåŸŸï¼ŒåŒæ—¶ä¿æŒé‰´åˆ«å±€éƒ¨ç‰¹å¾å¹¶å®ç°æœ‰æ•ˆçš„å…¨å±€ä¿¡æ¯äº¤æµã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å›¾æ³¨æ„åŠ›ç½‘ç»œè§£å†³è¯Šæ–­ä»»åŠ¡ï¼Œå¹¶åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¦‚ç™Œç—‡åˆ†æœŸåˆ†ç±»å’Œç”Ÿå­˜é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a61d40183cf173074cacdadcb608b75d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a98215e560e6bb90119ec6c32573b7e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96c836fac96e9345ed4481743a7f7586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b210192ee00a05fd6029555c46116e9f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Self-supervised-Contrastive-Learning-for-Multimodal-Text-Image-Analysis"><a href="#A-Survey-on-Self-supervised-Contrastive-Learning-for-Multimodal-Text-Image-Analysis" class="headerlink" title="A Survey on Self-supervised Contrastive Learning for Multimodal   Text-Image Analysis"></a>A Survey on Self-supervised Contrastive Learning for Multimodal   Text-Image Analysis</h2><p><strong>Authors:Asifullah Khan, Laiba Asmatullah, Anza Malik, Shahzaib Khan, Hamna Asif</strong></p>
<p>Self-supervised learning is a machine learning approach that generates implicit labels by learning underlined patterns and extracting discriminative features from unlabeled data without manual labelling. Contrastive learning introduces the concept of â€œpositiveâ€ and â€œnegativeâ€ samples, where positive pairs (e.g., variation of the same image&#x2F;object) are brought together in the embedding space, and negative pairs (e.g., views from different images&#x2F;objects) are pushed farther away. This methodology has shown significant improvements in image understanding and image text analysis without much reliance on labeled data. In this paper, we comprehensively discuss the terminologies, recent developments and applications of contrastive learning with respect to text-image models. Specifically, we provide an overview of the approaches of contrastive learning in text-image models in recent years. Secondly, we categorize the approaches based on different model structures. Thirdly, we further introduce and discuss the latest advances of the techniques used in the process such as pretext tasks for both images and text, architectural structures, and key trends. Lastly, we discuss the recent state-of-art applications of self-supervised contrastive learning Text-Image based models. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡å­¦ä¹ æ½œåœ¨çš„æ¨¡å¼å¹¶ä»æ— æ ‡ç­¾æ•°æ®ä¸­æå–åˆ¤åˆ«ç‰¹å¾ï¼Œä»è€Œç”Ÿæˆéšå¼æ ‡ç­¾ï¼Œè€Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚å¯¹æ¯”å­¦ä¹ å¼•å…¥äº†â€œæ­£æ ·æœ¬â€å’Œâ€œè´Ÿæ ·æœ¬â€çš„æ¦‚å¿µï¼Œå…¶ä¸­æ­£æ ·æœ¬å¯¹ï¼ˆä¾‹å¦‚ï¼ŒåŒä¸€å›¾åƒ&#x2F;å¯¹è±¡çš„å˜ä½“ï¼‰åœ¨åµŒå…¥ç©ºé—´ä¸­èšé›†åœ¨ä¸€èµ·ï¼Œè€Œè´Ÿæ ·æœ¬å¯¹ï¼ˆä¾‹å¦‚ï¼Œæ¥è‡ªä¸åŒå›¾åƒ&#x2F;å¯¹è±¡çš„è§†å›¾ï¼‰åˆ™è¢«æ¨å¼€ã€‚è¿™ç§æ–¹æ³•åœ¨å›¾åƒç†è§£å’Œå›¾åƒæ–‡æœ¬åˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè€Œä¸”ä¸éœ€è¦ä¾èµ–å¤§é‡çš„æ ‡æ³¨æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…¨é¢è®¨è®ºäº†ä¸æ–‡æœ¬-å›¾åƒæ¨¡å‹ç›¸å…³çš„å¯¹æ¯”å­¦ä¹ çš„æœ¯è¯­ã€æœ€æ–°å‘å±•ä»¥åŠåº”ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†è¿‘å¹´æ¥æ–‡æœ¬-å›¾åƒæ¨¡å‹ä¸­å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ ¹æ®ä¸åŒçš„æ¨¡å‹ç»“æ„å¯¹æ–¹æ³•è¿›è¡Œäº†åˆ†ç±»ã€‚å†æ¬¡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä»‹ç»å’Œè®¨è®ºäº†è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æœ€æ–°æŠ€æœ¯ï¼Œä¾‹å¦‚å›¾åƒå’Œæ–‡æœ¬çš„é¢„è®­ç»ƒä»»åŠ¡ã€æ¶æ„ç»“æ„å’Œå…³é”®è¶‹åŠ¿ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†åŸºäºæ–‡æœ¬-å›¾åƒæ¨¡å‹çš„è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ çš„æœ€æ–°åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11101v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è‡ªç›‘ç£å­¦ä¹ é€šè¿‡ä»éæ ‡è®°æ•°æ®ä¸­å­¦ä¹ æ½œåœ¨æ¨¡å¼å’Œæå–åˆ¤åˆ«ç‰¹å¾ï¼Œç”Ÿæˆéšå¼æ ‡ç­¾ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ï¼Œä¾¿èƒ½å®ç°æœºå™¨å­¦ä¹ ã€‚å¯¹æ¯”å­¦ä¹ å¼•å…¥äº†â€œæ­£æ ·æœ¬â€å’Œâ€œè´Ÿæ ·æœ¬â€çš„æ¦‚å¿µï¼Œé€šè¿‡æ‹‰è¿‘æ­£æ ·æœ¬å¯¹ï¼ˆå¦‚åŒä¸€å›¾åƒ&#x2F;å¯¹è±¡çš„å˜ä½“ï¼‰å¹¶åœ¨åµŒå…¥ç©ºé—´ä¸­æ¨å¼€è´Ÿæ ·æœ¬å¯¹ï¼ˆå¦‚æ¥è‡ªä¸åŒå›¾åƒ&#x2F;å¯¹è±¡çš„è§†å›¾ï¼‰ï¼Œä»è€Œè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚è¯¥æ–¹æ³•åœ¨å›¾åƒç†è§£å’Œæ–‡æœ¬å›¾åƒåˆ†ææ–¹é¢ï¼Œå¤§å¹…æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸”åœ¨ä¾èµ–æ ‡æ³¨æ•°æ®æ–¹é¢å¤§å¤§é™ä½ã€‚æœ¬æ–‡å…¨é¢æ¢è®¨äº†æ–‡æœ¬å›¾åƒæ¨¡å‹ä¸­å¯¹æ¯”å­¦ä¹ çš„æœ¯è¯­ã€æœ€æ–°å‘å±•åŠåº”ç”¨ï¼Œä»‹ç»äº†è¿‘å¹´å¯¹æ¯”å­¦ä¹ åœ¨æ–‡æœ¬å›¾åƒæ¨¡å‹ä¸­çš„æ–¹æ³•ï¼ŒæŒ‰æ¨¡å‹ç»“æ„åˆ†ç±»ï¼Œå¹¶æ·±å…¥ä»‹ç»äº†å›¾åƒå’Œæ–‡æœ¬çš„é¢„è®­ç»ƒä»»åŠ¡ã€æ¶æ„ç»“æ„å’Œå…³é”®è¶‹åŠ¿çš„æœ€æ–°è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ é€šè¿‡éæ ‡è®°æ•°æ®ä¸­çš„æ¨¡å¼å­¦ä¹ å’Œç‰¹å¾æå–ç”Ÿæˆéšå¼æ ‡ç­¾ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ é€šè¿‡åŒºåˆ†æ­£ã€è´Ÿæ ·æœ¬è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæé«˜å›¾åƒç†è§£å’Œæ–‡æœ¬åˆ†æçš„æ€§èƒ½ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ åœ¨æ–‡æœ¬å›¾åƒæ¨¡å‹ä¸­æœ‰ç€å¹¿æ³›çš„åº”ç”¨å’Œæœ€æ–°çš„ç ”ç©¶è¿›å±•ã€‚</li>
<li>æ¨¡å‹æ–¹æ³•å¯æŒ‰ç»“æ„è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>é¢„è®­ç»ƒä»»åŠ¡ã€æ¶æ„ç»“æ„å’Œå…³é”®è¶‹åŠ¿çš„æœ€æ–°è¿›å±•æ˜¯å¯¹æ¯”å­¦ä¹ çš„é‡ç‚¹ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ åœ¨ä¾èµ–æ ‡æ³¨æ•°æ®æ–¹é¢å¤§å¹…é™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a68b2ad541e976ae8a8a8a0676b1c8d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Memory-Efficient-3D-High-Resolution-Medical-Image-Synthesis-Using-CRF-Guided-GANs"><a href="#Memory-Efficient-3D-High-Resolution-Medical-Image-Synthesis-Using-CRF-Guided-GANs" class="headerlink" title="Memory-Efficient 3D High-Resolution Medical Image Synthesis Using   CRF-Guided GANs"></a>Memory-Efficient 3D High-Resolution Medical Image Synthesis Using   CRF-Guided GANs</h2><p><strong>Authors:Mahshid Shiri, Alessandro Bruno, Daniele Loiacono</strong></p>
<p>Generative Adversarial Networks (GANs) have many potential medical imaging applications. Due to the limited memory of Graphical Processing Units (GPUs), most current 3D GAN models are trained on low-resolution medical images, these models cannot scale to high-resolution or are susceptible to patchy artifacts. In this work, we propose an end-to-end novel GAN architecture that uses Conditional Random field (CRF) to model dependencies so that it can generate consistent 3D medical Images without exploiting memory. To achieve this purpose, the generator is divided into two parts during training, the first part produces an intermediate representation and CRF is applied to this intermediate representation to capture correlations. The second part of the generator produces a random sub-volume of image using a subset of the intermediate representation. This structure has two advantages: first, the correlations are modeled by using the features that the generator is trying to optimize. Second, the generator can generate full high-resolution images during inference. Experiments on Lung CTs and Brain MRIs show that our architecture outperforms state-of-the-art while it has lower memory usage and less complexity. </p>
<blockquote>
<p>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åœ¨åŒ»å­¦æˆåƒåº”ç”¨æ–¹é¢æ‹¥æœ‰è®¸å¤šæ½œåœ¨å¯èƒ½æ€§ã€‚ç”±äºå›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUï¼‰çš„å†…å­˜æœ‰é™ï¼Œå½“å‰å¤§å¤šæ•°3D GANæ¨¡å‹éƒ½æ˜¯åœ¨ä½åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒä¸Šè®­ç»ƒçš„ï¼Œè¿™äº›æ¨¡å‹æ— æ³•æ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡æˆ–å®¹æ˜“å‡ºç°æ–‘å—çŠ¶ä¼ªå½±ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ–°å‹GANæ¶æ„ï¼Œè¯¥æ¶æ„ä½¿ç”¨æ¡ä»¶éšæœºåœºï¼ˆCRFï¼‰æ¥å»ºæ¨¡ä¾èµ–æ€§ï¼Œä»¥ç”Ÿæˆä¸€è‡´çš„3DåŒ»å­¦å›¾åƒï¼Œè€Œæ— éœ€åˆ©ç”¨é¢å¤–å†…å­˜ã€‚ä¸ºäº†è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆå™¨è¢«åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†ç”Ÿæˆä¸­é—´è¡¨ç¤ºï¼Œå¹¶å¯¹è¯¥ä¸­é—´è¡¨ç¤ºåº”ç”¨CRFä»¥æ•è·ç›¸å…³æ€§ã€‚ç”Ÿæˆå™¨çš„ç¬¬äºŒéƒ¨åˆ†ä½¿ç”¨ä¸­é—´è¡¨ç¤ºçš„ä¸€ä¸ªå­é›†æ¥ç”Ÿæˆå›¾åƒçš„éšæœºå­ä½“ç§¯ã€‚è¿™ç§ç»“æ„æœ‰ä¸¤ä¸ªä¼˜ç‚¹ï¼šé¦–å…ˆï¼Œé€šè¿‡åˆ©ç”¨ç”Ÿæˆå™¨è¯•å›¾ä¼˜åŒ–çš„ç‰¹å¾æ¥å»ºæ¨¡ç›¸å…³æ€§ã€‚å…¶æ¬¡ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆå™¨å¯ä»¥ç”Ÿæˆå…¨é«˜åˆ†è¾¨ç‡å›¾åƒã€‚å¯¹è‚ºéƒ¨CTå’Œè„‘éƒ¨MRIçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¶æ„åœ¨å…·æœ‰è¾ƒä½å†…å­˜ä½¿ç”¨å’Œè¾ƒä½å¤æ‚æ€§çš„åŒæ—¶ï¼Œä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10899v1">PDF</a> Accepted to Artificial Intelligence for Healthcare Applications, 3rd   International Workshop ICPR 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç«¯åˆ°ç«¯GANæ¶æ„ï¼Œåˆ©ç”¨æ¡ä»¶éšæœºåœºï¼ˆCRFï¼‰è¿›è¡Œä¾èµ–å»ºæ¨¡ï¼Œæ—¨åœ¨ç”Ÿæˆä¸€è‡´çš„3DåŒ»å­¦å›¾åƒï¼Œæ— éœ€å ç”¨å¤§é‡å†…å­˜ã€‚è¯¥æ¶æ„é€šè¿‡è®­ç»ƒæ—¶åˆ†å‰²ç”Ÿæˆå™¨ï¼Œåˆ©ç”¨ä¸­é—´è¡¨ç¤ºå’ŒCRFæ•æ‰ç›¸å…³æ€§ï¼ŒåŒæ—¶ç”Ÿæˆéšæœºçš„å›¾åƒå­ä½“ç§¯ï¼Œä»è€Œå®ç°é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„ç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨è‚ºéƒ¨CTå’Œè„‘éƒ¨MRIä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼ŒåŒæ—¶å†…å­˜å ç”¨æ›´ä½ã€å¤æ‚åº¦æ›´å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANsåœ¨åŒ»å­¦æˆåƒä¸­æœ‰å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</li>
<li>å½“å‰3D GANæ¨¡å‹å—é™äºGPUå†…å­˜ï¼Œé€šå¸¸åªèƒ½å¤„ç†ä½åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯GANæ¶æ„ï¼Œåˆ©ç”¨CRFè¿›è¡Œä¾èµ–å»ºæ¨¡ã€‚</li>
<li>è¯¥æ¶æ„é€šè¿‡è®­ç»ƒæ—¶åˆ†å‰²ç”Ÿæˆå™¨ï¼Œåˆ©ç”¨ä¸­é—´è¡¨ç¤ºå’ŒCRFæ•æ‰ç›¸å…³æ€§ã€‚</li>
<li>ç”Ÿæˆå™¨å¯ä»¥ç”Ÿæˆå®Œæ•´çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¶æ„åœ¨è‚ºéƒ¨CTå’Œè„‘éƒ¨MRIä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10899">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-622d78be80120c8978ef70bc230c3b20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5588015cfb7e1a46cf55117657520cbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ae6b825e9c77e6ad551848a28518c07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9c2f482f431f9d6c6db27ec786c895e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LesionDiffusion-Towards-Text-controlled-General-Lesion-Synthesis"><a href="#LesionDiffusion-Towards-Text-controlled-General-Lesion-Synthesis" class="headerlink" title="LesionDiffusion: Towards Text-controlled General Lesion Synthesis"></a>LesionDiffusion: Towards Text-controlled General Lesion Synthesis</h2><p><strong>Authors:Henrui Tian, Wenhui Lei, Linrui Dai, Hanyu Chen, Xiaofan Zhang</strong></p>
<p>Fully-supervised lesion recognition methods in medical imaging face challenges due to the reliance on large annotated datasets, which are expensive and difficult to collect. To address this, synthetic lesion generation has become a promising approach. However, existing models struggle with scalability, fine-grained control over lesion attributes, and the generation of complex structures. We propose LesionDiffusion, a text-controllable lesion synthesis framework for 3D CT imaging that generates both lesions and corresponding masks. By utilizing a structured lesion report template, our model provides greater control over lesion attributes and supports a wider variety of lesion types. We introduce a dataset of 1,505 annotated CT scans with paired lesion masks and structured reports, covering 14 lesion types across 8 organs. LesionDiffusion consists of two components: a lesion mask synthesis network (LMNet) and a lesion inpainting network (LINet), both guided by lesion attributes and image features. Extensive experiments demonstrate that LesionDiffusion significantly improves segmentation performance, with strong generalization to unseen lesion types and organs, outperforming current state-of-the-art models. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/HengruiTianSJTU/LesionDiffusion">https://github.com/HengruiTianSJTU/LesionDiffusion</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œå…¨ç›‘ç£ç—…ç¶è¯†åˆ«æ–¹æ³•é¢ä¸´ç€ä¾èµ–äºå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†çš„æŒ‘æˆ˜ï¼Œè€Œè¿™äº›æ•°æ®é›†çš„æ”¶é›†æ—¢æ˜‚è´µåˆå›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œåˆæˆç—…ç¶ç”Ÿæˆå·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨å¯æ‰©å±•æ€§ã€ç—…ç¶å±æ€§çš„ç²¾ç»†æ§åˆ¶ä»¥åŠå¤æ‚ç»“æ„çš„ç”Ÿæˆæ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†LesionDiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº3D CTæˆåƒçš„æ–‡æœ¬å¯æ§ç—…ç¶åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆç—…ç¶åŠå…¶ç›¸åº”çš„æ©è†œã€‚é€šè¿‡åˆ©ç”¨ç»“æ„åŒ–ç—…ç¶æŠ¥å‘Šæ¨¡æ¿ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æä¾›äº†å¯¹ç—…ç¶å±æ€§æ›´å¤§çš„æ§åˆ¶åŠ›ï¼Œå¹¶æ”¯æŒæ›´å¤šç§ç±»çš„ç—…ç¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«1505ä¸ªæ ‡æ³¨CTæ‰«æçš„æ•°æ®é›†ï¼Œæ¯ä¸ªæ‰«æéƒ½æœ‰é…å¯¹çš„ç—…ç¶æ©è†œå’Œç»“æ„åŒ–æŠ¥å‘Šï¼Œè¦†ç›–8ä¸ªå™¨å®˜ä¸­çš„14ç§ç—…ç¶ç±»å‹ã€‚LesionDiffusionç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šç—…ç¶æ©è†œåˆæˆç½‘ç»œï¼ˆLMNetï¼‰å’Œç—…ç¶å¡«å……ç½‘ç»œï¼ˆLINetï¼‰ï¼Œä¸¤è€…å‡ç”±ç—…ç¶å±æ€§å’Œå›¾åƒç‰¹å¾å¼•å¯¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLesionDiffusionæ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œå¯¹æœªè§è¿‡çš„ç—…ç¶ç±»å‹å’Œå™¨å®˜å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/HengruiTianSJTU/LesionDiffusion%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/HengruiTianSJTU/LesionDiffusionä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00741v3">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒä¸­ç—…ç¶è¯†åˆ«æ‰€é¢ä¸´çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLesionDiffusionçš„æ–‡æœ¬å¯æ§ç—…ç¶åˆæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆç—…ç¶åŠå…¶å¯¹åº”çš„æ©è†œï¼Œåˆ©ç”¨ç»“æ„åŒ–ç—…ç¶æŠ¥å‘Šæ¨¡æ¿ï¼Œå®ç°å¯¹ç—…ç¶å±æ€§çš„æ›´ç²¾ç»†æ§åˆ¶ï¼Œå¹¶æ”¯æŒå¤šç§ç—…ç¶ç±»å‹ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ä¸ªåŒ…å«1,505ä¸ªå¸¦ç—…ç¶æ©è†œå’Œç»“æ„åŒ–æŠ¥å‘Šçš„æ ‡æ³¨CTæ‰«ææ•°æ®é›†ã€‚LesionDiffusionç”±ä¸¤ä¸ªç»„ä»¶æ„æˆï¼šç—…ç¶æ©è†œåˆæˆç½‘ç»œï¼ˆLMNetï¼‰å’Œç—…ç¶ä¿®å¤ç½‘ç»œï¼ˆLINetï¼‰ï¼Œä¸¤è€…å‡å—ç—…ç¶å±æ€§å’Œå›¾åƒç‰¹å¾çš„å¼•å¯¼ã€‚å®éªŒè¡¨æ˜ï¼ŒLesionDiffusionåœ¨åˆ†å‰²æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¯¹æœªè§è¿‡çš„ç—…ç¶ç±»å‹å’Œå™¨å®˜å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LesionDiffusionæ˜¯ä¸€ä¸ªç”¨äº3D CTæˆåƒçš„æ–‡æœ¬å¯æ§ç—…ç¶åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆç—…ç¶åŠå…¶å¯¹åº”çš„æ©è†œã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨ç»“æ„åŒ–ç—…ç¶æŠ¥å‘Šæ¨¡æ¿ï¼Œå®ç°å¯¹ç—…ç¶å±æ€§çš„æ›´ç²¾ç»†æ§åˆ¶ï¼Œå¹¶æ”¯æŒå¤šç§ç—…ç¶ç±»å‹ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåŒ…å«1,505ä¸ªæ ‡æ³¨CTæ‰«ææ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬é…å¯¹ç—…ç¶æ©è†œå’Œç»“æ„åŒ–æŠ¥å‘Šï¼Œè¦†ç›–8ä¸ªå™¨å®˜çš„14ç§ç—…ç¶ç±»å‹ã€‚</li>
<li>LesionDiffusionç”±LMNetå’ŒLINetä¸¤ä¸ªç»„ä»¶æ„æˆï¼Œå‡å—ç—…ç¶å±æ€§å’Œå›¾åƒç‰¹å¾çš„å¼•å¯¼ã€‚</li>
<li>å®éªŒè¡¨æ˜LesionDiffusionåœ¨ç—…ç¶åˆ†å‰²æ€§èƒ½ä¸Šæ˜¾è‘—æå‡ï¼Œä¸”å¯¹æœªè§è¿‡çš„ç—…ä¾‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå…·æœ‰æ½œåœ¨çš„ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00741">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-32d2ffb606db7183aadbd14c0d543700.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b601c0c98bc3f6e70d620d8dad67123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-402ce4f3de7dc83322b8332fbfca0a03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f5af67a85513b1f808fbde0ecc1e73a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Learning-of-Patch-Based-Smooth-Plus-Sparse-Models-for-Image-Reconstruction"><a href="#Learning-of-Patch-Based-Smooth-Plus-Sparse-Models-for-Image-Reconstruction" class="headerlink" title="Learning of Patch-Based Smooth-Plus-Sparse Models for Image   Reconstruction"></a>Learning of Patch-Based Smooth-Plus-Sparse Models for Image   Reconstruction</h2><p><strong>Authors:Stanislas Ducotterd, Sebastian Neumayer, Michael Unser</strong></p>
<p>We aim at the solution of inverse problems in imaging, by combining a penalized sparse representation of image patches with an unconstrained smooth one. This allows for a straightforward interpretation of the reconstruction. We formulate the optimization as a bilevel problem. The inner problem deploys classical algorithms while the outer problem optimizes the dictionary and the regularizer parameters through supervised learning. The process is carried out via implicit differentiation and gradient-based optimization. We evaluate our method for denoising, super-resolution, and compressed-sensing magnetic-resonance imaging. We compare it to other classical models as well as deep-learning-based methods and show that it always outperforms the former and also the latter in some instances. </p>
<blockquote>
<p>æˆ‘ä»¬æ—¨åœ¨é€šè¿‡ç»“åˆå›¾åƒå—çš„æƒ©ç½šç¨€ç–è¡¨ç¤ºå’Œæ— çº¦æŸå¹³æ»‘è¡¨ç¤ºæ¥è§£å†³æˆåƒä¸­çš„åé—®é¢˜ã€‚è¿™ä½¿å¾—é‡å»ºè¿‡ç¨‹æ›´å®¹æ˜“è§£é‡Šã€‚æˆ‘ä»¬å°†ä¼˜åŒ–é—®é¢˜åˆ¶å®šä¸ºåŒå±‚é—®é¢˜ã€‚å†…å±‚é—®é¢˜é‡‡ç”¨ç»å…¸ç®—æ³•ï¼Œå¤–å±‚é—®é¢˜é€šè¿‡ç›‘ç£å­¦ä¹ ä¼˜åŒ–å­—å…¸å’Œæ­£åˆ™åŒ–å‚æ•°ã€‚è¯¥è¿‡ç¨‹é€šè¿‡éšå¼å¾®åˆ†å’ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ¥å®ç°ã€‚æˆ‘ä»¬è¯„ä¼°äº†è¯¥æ–¹æ³•åœ¨å»å™ªã€è¶…åˆ†è¾¨ç‡å’Œå‹ç¼©æ„ŸçŸ¥ç£å…±æŒ¯æˆåƒæ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•ä¸å…¶ä»–ç»å…¸æ¨¡å‹ä»¥åŠåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶è¯æ˜åœ¨å¤šæ•°æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•éƒ½ä¼˜äºå‰è€…ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ä¹Ÿä¼˜äºåè€…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13070v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç»“åˆæƒ©ç½šç¨€ç–å›¾åƒè¡¥ä¸è¡¨ç¤ºä¸æ— çº¦æŸå¹³æ»‘è¡¨ç¤ºï¼Œè§£å†³æˆåƒä¸­çš„åé—®é¢˜ã€‚æå‡ºä¸€ç§åŒå±‚ä¼˜åŒ–é—®é¢˜ï¼Œå†…éƒ¨é—®é¢˜é‡‡ç”¨ç»å…¸ç®—æ³•ï¼Œå¤–éƒ¨é—®é¢˜é€šè¿‡ç›‘ç£å­¦ä¹ ä¼˜åŒ–å­—å…¸å’Œæ­£åˆ™åŒ–å‚æ•°ã€‚é€šè¿‡éšå¼å¾®åˆ†å’ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–å®ç°è¿™ä¸€è¿‡ç¨‹ã€‚ç»å»å™ªã€è¶…åˆ†è¾¨ç‡åŠç£å…±æŒ¯æˆåƒå‹ç¼©æ„ŸçŸ¥è¯„ä¼°ï¼Œè¯¥æ–¹æ³•åœ¨æŸäº›æƒ…å†µä¸‹ä¼˜äºç»å…¸æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•´åˆæƒ©ç½šç¨€ç–å›¾åƒè¡¥ä¸å’Œæ— çº¦æŸå¹³æ»‘è¡¨ç¤ºä»¥è§£å†³æˆåƒä¸­çš„åé—®é¢˜ã€‚</li>
<li>ä¼˜åŒ–é—®é¢˜è¢«è¡¨è¿°ä¸ºåŒå±‚ç»“æ„ï¼Œå…¶ä¸­å†…éƒ¨é—®é¢˜è¿ç”¨ç»å…¸ç®—æ³•ï¼Œå¤–éƒ¨é—®é¢˜ä¾§é‡äºä¼˜åŒ–å­—å…¸å’Œæ­£åˆ™åŒ–å‚æ•°ã€‚</li>
<li>é€šè¿‡éšå¼å¾®åˆ†å’ŒåŸºäºæ¢¯åº¦çš„ä¼˜åŒ–å®ç°æµç¨‹ã€‚</li>
<li>æ–¹æ³•åœ¨å»å™ªã€è¶…åˆ†è¾¨ç‡åŠç£å…±æŒ¯æˆåƒå‹ç¼©æ„ŸçŸ¥æ–¹é¢è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>ä¸ç»å…¸æ¨¡å‹ç›¸æ¯”ï¼Œæ­¤æ–¹æ³•è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ç”šè‡³è¶…è¶Šäº†æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c9c96aac8c79a6fcd3abe6c86dd90c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bb39a49ba3bb7b577e6740d69039031.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd0d4aeb6ae44ddd5fd8bd0e4044017d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ade3b6a63ec5b353dded0ff51c18c40.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Semantic-Consistency-Based-Uncertainty-Quantification-for-Factuality-in-Radiology-Report-Generation"><a href="#Semantic-Consistency-Based-Uncertainty-Quantification-for-Factuality-in-Radiology-Report-Generation" class="headerlink" title="Semantic Consistency-Based Uncertainty Quantification for Factuality in   Radiology Report Generation"></a>Semantic Consistency-Based Uncertainty Quantification for Factuality in   Radiology Report Generation</h2><p><strong>Authors:Chenyu Wang, Weichao Zhou, Shantanu Ghosh, Kayhan Batmanghelich, Wenchao Li</strong></p>
<p>Radiology report generation (RRG) has shown great potential in assisting radiologists by automating the labor-intensive task of report writing. While recent advancements have improved the quality and coherence of generated reports, ensuring their factual correctness remains a critical challenge. Although generative medical Vision Large Language Models (VLLMs) have been proposed to address this issue, these models are prone to hallucinations and can produce inaccurate diagnostic information. To address these concerns, we introduce a novel Semantic Consistency-Based Uncertainty Quantification framework that provides both report-level and sentence-level uncertainties. Unlike existing approaches, our method does not require modifications to the underlying model or access to its inner state, such as output token logits, thus serving as a plug-and-play module that can be seamlessly integrated with state-of-the-art models. Extensive experiments demonstrate the efficacy of our method in detecting hallucinations and enhancing the factual accuracy of automatically generated radiology reports. By abstaining from high-uncertainty reports, our approach improves factuality scores by $10$%, achieved by rejecting $20$% of reports using the \texttt{Radialog} model on the MIMIC-CXR dataset. Furthermore, sentence-level uncertainty flags the lowest-precision sentence in each report with an $82.9$% success rate. Our implementation is open-source and available at <a target="_blank" rel="noopener" href="https://github.com/BU-DEPEND-Lab/SCUQ-RRG">https://github.com/BU-DEPEND-Lab/SCUQ-RRG</a>. </p>
<blockquote>
<p>æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰åœ¨é€šè¿‡è‡ªåŠ¨åŒ–æŠ¥å‘Šå†™ä½œè¿™ä¸€åŠ³åŠ¨å¯†é›†å‹ä»»åŠ¡æ¥è¾…åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚è™½ç„¶æœ€è¿‘çš„è¿›å±•æé«˜äº†ç”ŸæˆæŠ¥å‘Šçš„è´¨é‡å’Œè¿è´¯æ€§ï¼Œä½†ç¡®ä¿äº‹å®æ­£ç¡®æ€§ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚è™½ç„¶å·²æå‡ºç”Ÿæˆå¼åŒ»å­¦è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†è¿™äº›æ¨¡å‹å®¹æ˜“äº§ç”Ÿå¹»è§‰å¹¶å¯èƒ½äº§ç”Ÿä¸å‡†ç¡®çš„è¯Šæ–­ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æ‹…å¿§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºè¯­ä¹‰ä¸€è‡´æ€§çš„ä¸ç¡®å®šæ€§é‡åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æä¾›æŠ¥å‘Šçº§å’Œå¥å­çº§çš„ä¸ç¡®å®šæ€§ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦å¯¹åº•å±‚æ¨¡å‹è¿›è¡Œä¿®æ”¹ï¼Œä¹Ÿä¸éœ€è¦è®¿é—®å…¶å†…éƒ¨çŠ¶æ€ï¼Œå¦‚è¾“å‡ºä»¤ç‰Œå¯¹æ•°å‡ ç‡ï¼Œå› æ­¤å¯ä»¥ä½œä¸ºå³æ’å³ç”¨æ¨¡å—æ— ç¼é›†æˆåˆ°æœ€æ–°æ¨¡å‹ä¸­ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨æ£€æµ‹å¹»è§‰å’Œæé«˜è‡ªåŠ¨ç”Ÿæˆçš„æ”¾å°„å­¦æŠ¥å‘Šçš„äº‹å®å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡é¿å…é«˜ä¸ç¡®å®šæ€§çš„æŠ¥å‘Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šä½¿ç”¨â€œRadialogâ€æ¨¡å‹æ‹’ç»20%çš„æŠ¥å‘Šï¼Œæé«˜äº†äº‹å®å¾—åˆ†10%ã€‚æ­¤å¤–ï¼Œå¥å­çº§ä¸ç¡®å®šæ€§ä»¥82.9%çš„æˆåŠŸç‡æ ‡è®°äº†æ¯ä¸ªæŠ¥å‘Šä¸­çš„æœ€ä½ç²¾åº¦å¥å­ã€‚æˆ‘ä»¬çš„å®ç°æ˜¯å¼€æºçš„ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BU-DEPEND-Lab/SCUQ-RRG%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/BU-DEPEND-Lab/SCUQ-RRGè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04606v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è¯­ä¹‰ä¸€è‡´æ€§åŸºäºä¸ç¡®å®šæ€§çš„é‡åŒ–æ¡†æ¶ï¼Œç”¨äºæé«˜æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰çš„å‡†ç¡®æ€§å’Œäº‹å®çœŸå®æ€§ã€‚è¯¥æ¡†æ¶èƒ½åœ¨æ— éœ€ä¿®æ”¹åº•å±‚æ¨¡å‹æˆ–è·å–å…¶å†…éƒ¨çŠ¶æ€çš„å‰æä¸‹ï¼Œæä¾›æŠ¥å‘Šçº§åˆ«å’Œå¥å­çº§åˆ«çš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œæ£€æµ‹å‡ºå¯èƒ½å‡ºç°çš„å¹»è§‰ï¼Œæé«˜è‡ªåŠ¨ç”Ÿæˆçš„æ”¾å°„å­¦æŠ¥å‘Šçš„äº‹å®å‡†ç¡®æ€§ã€‚é€šè¿‡æ‹’ç»é«˜ä¸ç¡®å®šæ€§çš„æŠ¥å‘Šï¼Œè¯¥æ–¹æ³•çš„å®é™…æ•ˆæœæ˜¯åœ¨MIMIC-CXRæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨Radialogæ¨¡å‹å®ç°çš„æŠ¥å‘Šäº‹å®æ€§å¾—åˆ†æé«˜äº†10%ï¼ŒåŒæ—¶æ‹’ç»äº†å¤§çº¦20%çš„æŠ¥å‘Šã€‚æ­¤å¤–ï¼Œå¥å­çº§åˆ«çš„ä¸ç¡®å®šæ€§æˆåŠŸæ ‡æ³¨äº†æ¯ä¸ªæŠ¥å‘Šä¸­çš„æœ€ä½ç²¾åº¦å¥å­ï¼ŒæˆåŠŸç‡ä¸º82.9%ã€‚æˆ‘ä»¬çš„å®ç°æ˜¯å¼€æºçš„ï¼Œå¯åœ¨æˆ‘ä»¬çš„GitHubä»“åº“æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/BU-DEPEND-Lab/SCUQ-RRG">https://github.com/BU-DEPEND-Lab/SCUQ-RRG</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ¡†æ¶é’ˆå¯¹ç”Ÿæˆå¼åŒ»ç–—è§†è§‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰å­˜åœ¨çš„å¹»è§‰é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ï¼Œç¡®ä¿æŠ¥å‘Šçš„å‡†ç¡®æ€§ã€‚</li>
<li>æ— éœ€ä¿®æ”¹åº•å±‚æ¨¡å‹æˆ–è®¿é—®å…¶å†…éƒ¨çŠ¶æ€ï¼Œä½œä¸ºå³æ’å³ç”¨æ¨¡å—æ— ç¼é›†æˆåˆ°æœ€æ–°æ¨¡å‹ä¸­ã€‚</li>
<li>é€šè¿‡æŠ¥å‘Šçº§åˆ«çš„ä¸ç¡®å®šæ€§æ£€æµ‹ï¼Œæé«˜äº†æŠ¥å‘Šçš„å‡†ç¡®æ€§ã€‚æ‹’ç»é«˜ä¸ç¡®å®šæ€§æŠ¥å‘Šåï¼ŒæŠ¥å‘Šçš„å‡†ç¡®æ€§å¾—åˆ†æé«˜äº†10%ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰å¼€æºå®ç°ï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
<li>å¥å­çº§åˆ«çš„ä¸ç¡®å®šæ€§æˆåŠŸæ ‡æ³¨äº†æŠ¥å‘Šçš„æœ€ä½ç²¾åº¦å¥å­ï¼ŒæˆåŠŸç‡ä¸º82.9%ã€‚</li>
<li>è¯¥æ¡†æ¶é€‚ç”¨äºMIMIC-CXRæ•°æ®é›†ä¸Šçš„Radialogæ¨¡å‹ï¼Œæœªæ¥å¯åº”ç”¨äºå…¶ä»–æ•°æ®é›†å’Œæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-965b6db4e973adb987d929b1a8f802a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0746275093e8e015f38947726913713e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b361efbdc7c7f38a64c64a05d9a2e7e1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Biologically-inspired-Semi-supervised-Semantic-Segmentation-for-Biomedical-Imaging"><a href="#Biologically-inspired-Semi-supervised-Semantic-Segmentation-for-Biomedical-Imaging" class="headerlink" title="Biologically-inspired Semi-supervised Semantic Segmentation for   Biomedical Imaging"></a>Biologically-inspired Semi-supervised Semantic Segmentation for   Biomedical Imaging</h2><p><strong>Authors:Luca Ciampi, Gabriele Lagani, Giuseppe Amato, Fabrizio Falchi</strong></p>
<p>We propose a novel bio-inspired semi-supervised learning approach for training downsampling-upsampling semantic segmentation architectures. The first stage does not use backpropagation. Rather, it exploits the Hebbian principle &#96;&#96;fire together, wire togetherâ€™â€™ as a local learning rule for updating the weights of both convolutional and transpose-convolutional layers, allowing unsupervised discovery of data features. In the second stage, the model is fine-tuned with standard backpropagation on a small subset of labeled data. We evaluate our methodology through experiments conducted on several widely used biomedical datasets, deeming that this domain is paramount in computer vision and is notably impacted by data scarcity. Results show that our proposed method outperforms SOTA approaches across different levels of label availability. Furthermore, we show that using our unsupervised stage to initialize the SOTA approaches leads to performance improvements. The code to replicate our experiments can be found at <a target="_blank" rel="noopener" href="https://github.com/ciampluca/hebbian-bootstraping-semi-supervised-medical-imaging">https://github.com/ciampluca/hebbian-bootstraping-semi-supervised-medical-imaging</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç”Ÿç‰©å¯å‘åŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒä¸‹é‡‡æ ·-ä¸Šé‡‡æ ·è¯­ä¹‰åˆ†å‰²æ¶æ„ã€‚ç¬¬ä¸€é˜¶æ®µä¸ä½¿ç”¨åå‘ä¼ æ’­ã€‚ç›¸åï¼Œå®ƒåˆ©ç”¨èµ«å¸ƒåŸåˆ™â€œä¸€èµ·æ”¾ç”µï¼Œä¸€èµ·è¿æ¥â€ä½œä¸ºå±€éƒ¨å­¦ä¹ è§„åˆ™ï¼Œä»¥æ›´æ–°å·ç§¯å±‚å’Œè½¬ç½®å·ç§¯å±‚çš„æƒé‡ï¼Œä»è€Œå®ç°æ•°æ®ç‰¹å¾çš„æ— ç›‘ç£å‘ç°ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ¨¡å‹ä½¿ç”¨æ ‡å‡†åå‘ä¼ æ’­å¯¹ä¸€å°éƒ¨åˆ†æ ‡è®°æ•°æ®è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬é€šè¿‡å‡ é¡¹å¹¿æ³›ä½¿ç”¨çš„ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†çš„å®éªŒè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè®¤ä¸ºè¿™ä¸€é¢†åŸŸåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸè‡³å…³é‡è¦ï¼Œå¹¶ä¸”å—åˆ°æ•°æ®ç¨€ç¼ºçš„æ˜¾è‘—å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡ç­¾å¯ç”¨æ€§çš„ä¸åŒçº§åˆ«ä¸Šéƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ— ç›‘ç£é˜¶æ®µæ¥åˆå§‹åŒ–æœ€æ–°æŠ€æœ¯æ–¹æ³•å¯ä»¥æé«˜æ€§èƒ½ã€‚å¤åˆ¶æˆ‘ä»¬å®éªŒçš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/ciampluca/hebbian-bootstraping-semi-supervised-medical-imaging%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ciampluca/hebbian-bootstraping-semi-supervised-medical-imagingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03192v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§åŸºäºç”Ÿç‰©å¯å‘çš„æ–°å‹åŠç›‘ç£å­¦ä¹ æ–¹æ³•åº”ç”¨äºè®­ç»ƒä¸‹é‡‡æ ·-ä¸Šé‡‡æ ·è¯­ä¹‰åˆ†å‰²æ¶æ„ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µä¸ä½¿ç”¨åå‘ä¼ æ’­ï¼Œè€Œæ˜¯åˆ©ç”¨èµ«å¸ƒåŸç†â€œä¸€èµ·æ”¾ç”µï¼Œä¸€èµ·è¿æ¥â€ä½œä¸ºå±€éƒ¨å­¦ä¹ è§„åˆ™æ¥æ›´æ–°å·ç§¯å±‚å’Œè½¬ç½®å·ç§¯å±‚çš„æƒé‡ï¼Œå®ç°æ•°æ®ç‰¹å¾çš„æ— ç›‘ç£å‘ç°ã€‚ç¬¬äºŒé˜¶æ®µä½¿ç”¨å°‘é‡æ ‡è®°æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨æ ‡å‡†åå‘ä¼ æ’­æ–¹æ³•ã€‚åœ¨å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡ç­¾å¯ç”¨æ€§çš„ä¸åŒå±‚æ¬¡ä¸Šéƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ˜¾ç¤ºï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ— ç›‘ç£é˜¶æ®µåˆå§‹åŒ–ç°æœ‰æŠ€æœ¯çš„æ–¹æ³•èƒ½å¤Ÿæé«˜æ€§èƒ½ã€‚ç›¸å…³å®éªŒä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/ciampluca/hebbian-bootstraping-semi-supervised-medical-imaging">https://github.com/ciampluca/hebbian-bootstraping-semi-supervised-medical-imaging</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºç”Ÿç‰©å¯å‘çš„æ–°å‹åŠç›‘ç£å­¦ä¹ æ–¹æ³•ç”¨äºè®­ç»ƒè¯­ä¹‰åˆ†å‰²æ¶æ„ã€‚</li>
<li>æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šæ— ç›‘ç£ç‰¹å¾å‘ç°å’ŒåŸºäºæ ‡è®°æ•°æ®çš„æ¨¡å‹å¾®è°ƒã€‚</li>
<li>åˆ©ç”¨èµ«å¸ƒåŸç†è¿›è¡Œæ— ç›‘ç£å­¦ä¹ ï¼Œæ›´æ–°å·ç§¯å±‚ä¸è½¬ç½®å·ç§¯å±‚çš„æƒé‡ã€‚</li>
<li>åœ¨ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œå¹¶æ˜¾ç¤ºäº†æ–¹æ³•åœ¨å„ç§æ ‡ç­¾å¯ç”¨æ€§å±‚æ¬¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜æ€§èƒ½ã€‚</li>
<li>æä¾›å®éªŒä»£ç è®¿é—®é“¾æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33892d7ae6a885c9c8a3d5b83905fb3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f10b4932e9fd6d658fd7949a9958b97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0961590f89164df6b22d0ab8cf5b68c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6220ccdad717656c44be42e7167a1eb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f15608aad095c3f68039a3af86063684.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Appearance-Matching-Adapter-for-Exemplar-based-Semantic-Image-Synthesis-in-the-Wild"><a href="#Appearance-Matching-Adapter-for-Exemplar-based-Semantic-Image-Synthesis-in-the-Wild" class="headerlink" title="Appearance Matching Adapter for Exemplar-based Semantic Image Synthesis   in-the-Wild"></a>Appearance Matching Adapter for Exemplar-based Semantic Image Synthesis   in-the-Wild</h2><p><strong>Authors:Siyoon Jin, Jisu Nam, Jiyoung Kim, Dahyun Chung, Yeong-Seok Kim, Joonhyung Park, Heonjeong Chu, Seungryong Kim</strong></p>
<p>Exemplar-based semantic image synthesis generates images aligned with semantic content while preserving the appearance of an exemplar. Conventional structure-guidance models like ControlNet, are limited as they rely solely on text prompts to control appearance and cannot utilize exemplar images as input. Recent tuning-free approaches address this by transferring local appearance via implicit cross-image matching in the augmented self-attention mechanism of pre-trained diffusion models. However, prior works are often restricted to single-object cases or foreground object appearance transfer, struggling with complex scenes involving multiple objects. To overcome this, we propose AM-Adapter (Appearance Matching Adapter) to address exemplar-based semantic image synthesis in-the-wild, enabling multi-object appearance transfer from a single scene-level image. AM-Adapter automatically transfers local appearances from the scene-level input. AM-Adapter alternatively provides controllability to map user-defined object details to specific locations in the synthesized images. Our learnable framework enhances cross-image matching within augmented self-attention by integrating semantic information from segmentation maps. To disentangle generation and matching, we adopt stage-wise training. We first train the structure-guidance and generation networks, followed by training the matching adapter while keeping the others frozen. During inference, we introduce an automated exemplar retrieval method for selecting exemplar image-segmentation pairs efficiently. Despite utilizing minimal learnable parameters, AM-Adapter achieves state-of-the-art performance, excelling in both semantic alignment and local appearance fidelity. Extensive ablations validate our design choices. Code and weights will be released.: <a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/AM-Adapter/">https://cvlab-kaist.github.io/AM-Adapter/</a> </p>
<blockquote>
<p>åŸºäºèŒƒä¾‹çš„è¯­ä¹‰å›¾åƒåˆæˆèƒ½å¤Ÿç”Ÿæˆä¸è¯­ä¹‰å†…å®¹å¯¹é½çš„å›¾åƒï¼ŒåŒæ—¶ä¿ç•™èŒƒä¾‹çš„å¤–è§‚ã€‚ä¼ ç»Ÿçš„ç»“æ„å¼•å¯¼æ¨¡å‹ï¼Œå¦‚ControlNetï¼Œå—é™äºä»…ä¾èµ–æ–‡æœ¬æç¤ºæ¥æ§åˆ¶å¤–è§‚ï¼Œæ— æ³•åˆ©ç”¨èŒƒä¾‹å›¾åƒä½œä¸ºè¾“å…¥ã€‚æœ€è¿‘çš„æ— éœ€å¾®è°ƒçš„æ–¹æ³•é€šè¿‡é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¢å¼ºè‡ªæ³¨æ„æœºåˆ¶ä¸­çš„éšå¼è·¨å›¾åƒåŒ¹é…æ¥è½¬ç§»å±€éƒ¨å¤–è§‚ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œé€šå¸¸å±€é™äºå•å¯¹è±¡æƒ…å†µæˆ–å‰æ™¯å¯¹è±¡å¤–è§‚è½¬ç§»ï¼Œå¯¹äºæ¶‰åŠå¤šä¸ªå¯¹è±¡çš„å¤æ‚åœºæ™¯æ„Ÿåˆ°å›°æ‰°ã€‚</p>
</blockquote>
<p>ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†AM-Adapterï¼ˆå¤–è§‚åŒ¹é…é€‚é…å™¨ï¼‰ï¼Œè§£å†³åŸºäºèŒƒä¾‹çš„è¯­ä¹‰å›¾åƒåˆæˆçš„é‡å¤–é—®é¢˜ï¼Œå®ç°ä»å•ä¸ªåœºæ™¯çº§åˆ«å›¾åƒè¿›è¡Œå¤šå¯¹è±¡å¤–è§‚è½¬ç§»ã€‚AM-Adapterè‡ªåŠ¨ä»åœºæ™¯çº§åˆ«çš„è¾“å…¥è½¬ç§»å±€éƒ¨å¤–è§‚ã€‚AM-Adapterè¿˜æä¾›å¯æ§æ€§ï¼Œå°†ç”¨æˆ·å®šä¹‰çš„å¯¹è±¡ç»†èŠ‚æ˜ å°„åˆ°åˆæˆå›¾åƒä¸­çš„ç‰¹å®šä½ç½®ã€‚æˆ‘ä»¬é€šè¿‡å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆåˆ†å‰²åœ°å›¾çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¢å¼ºå¢å¼ºè‡ªæ³¨æ„æœºåˆ¶å†…çš„è·¨å›¾åƒåŒ¹é…ã€‚ä¸ºäº†è§£å¼€ç”Ÿæˆå’ŒåŒ¹é…ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒã€‚æˆ‘ä»¬é¦–å…ˆè®­ç»ƒç»“æ„å¼•å¯¼å’Œç”Ÿæˆç½‘ç»œï¼Œç„¶åè®­ç»ƒåŒ¹é…é€‚é…å™¨ï¼ŒåŒæ—¶ä¿æŒå…¶ä»–ç½‘ç»œå†»ç»“ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªåŠ¨èŒƒä¾‹æ£€ç´¢æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåœ°é€‰æ‹©èŒƒä¾‹å›¾åƒ-åˆ†å‰²å¯¹ã€‚å°½ç®¡ä½¿ç”¨äº†æå°‘çš„å­¦ä¹ å‚æ•°ï¼Œä½†AM-Adapterè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è¯­ä¹‰å¯¹é½å’Œå±€éƒ¨å¤–è§‚ä¿çœŸåº¦æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚å¤§é‡çš„æ¶ˆèå®éªŒéªŒè¯äº†æˆ‘ä»¬çš„è®¾è®¡é€‰æ‹©ã€‚ç›¸å…³ä»£ç å’Œæƒé‡å·²å‘å¸ƒäºï¼š[<a target="_blank" rel="noopener" href="https://cvlab-kaist.github.io/AM-Adapter/]">https://cvlab-kaist.github.io/AM-Adapter/]</a></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03150v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºAM-Adapterçš„æ–°æ–¹æ³•ï¼Œç”¨äºåŸºäºèŒƒä¾‹çš„è¯­ä¹‰å›¾åƒåˆæˆã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå…‹æœç°æœ‰æŠ€æœ¯ä¸­å•å¯¹è±¡æ¡ˆä¾‹æˆ–å‰æ™¯å¯¹è±¡å¤–è§‚ä¼ é€’çš„é™åˆ¶ï¼Œå®ç°åœ¨å¤æ‚åœºæ™¯ä¸­å¤šå¯¹è±¡å¤–è§‚ä»å•ä¸€åœºæ™¯çº§åˆ«å›¾åƒçš„è½¬ç§»ã€‚é€šè¿‡é›†æˆåˆ†å‰²åœ°å›¾ä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼ŒAM-Adapteræé«˜äº†å¢å¼ºè‡ªæˆ‘æ³¨æ„åŠ›ä¸­çš„è·¨å›¾åƒåŒ¹é…èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨äº†åˆ†é˜¶æ®µè®­ç»ƒæ–¹æ³•ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå¼•å…¥è‡ªåŠ¨èŒƒä¾‹æ£€ç´¢æ–¹æ³•é«˜æ•ˆé€‰æ‹©èŒƒä¾‹å›¾åƒ-åˆ†å‰²å¯¹ã€‚å°½ç®¡ä½¿ç”¨è¾ƒå°‘çš„å¯å­¦ä¹ å‚æ•°ï¼ŒAM-Adapteråœ¨è¯­ä¹‰å¯¹é½å’Œå±€éƒ¨å¤–è§‚ä¿çœŸåº¦æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AM-Adapterå…‹æœäº†ä¾èµ–æ–‡æœ¬æç¤ºçš„ç°æœ‰ç»“æ„å¼•å¯¼æ¨¡å‹çš„å±€é™æ€§ï¼Œå¯ä»¥åˆ©ç”¨èŒƒä¾‹å›¾åƒä½œä¸ºè¾“å…¥è¿›è¡Œå›¾åƒåˆæˆã€‚</li>
<li>AM-Adapteré€šè¿‡å¢å¼ºè‡ªæˆ‘æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„è·¨å›¾åƒåŒ¹é…å®ç°äº†å±€éƒ¨å¤–è§‚çš„è½¬ç§»ï¼Œé€‚ç”¨äºå¤šå¯¹è±¡å¤æ‚åœºæ™¯ã€‚</li>
<li>è¯¥æ–¹æ³•é›†æˆäº†è¯­ä¹‰ä¿¡æ¯æ¥æé«˜è·¨å›¾åƒåŒ¹é…çš„æ•ˆèƒ½ï¼Œå¹¶é€šè¿‡åˆ†é˜¶æ®µè®­ç»ƒæ¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>AM-Adapteråœ¨è‡ªåŠ¨èŒƒä¾‹æ£€ç´¢æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½é«˜æ•ˆé€‰æ‹©èŒƒä¾‹å›¾åƒå’Œåˆ†å‰²é…å¯¹ã€‚</li>
<li>å°½ç®¡å‚æ•°è¾ƒå°‘ï¼ŒAM-Adapteråœ¨è¯­ä¹‰å›¾åƒåˆæˆçš„æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢è¾¾åˆ°æœ€ä½³çŠ¶æ€ã€‚</li>
<li>è¯¥æ–¹æ³•å°†åœ¨è¯­ä¹‰å¯¹é½å’Œå±€éƒ¨å¤–è§‚ä¿çœŸåº¦æ–¹é¢å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ºå›¾åƒåˆæˆé¢†åŸŸæä¾›æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe5868eab06f2569eb348c75c33c0aa8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7777c9010b59e354269fbb35ab67ace9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42ec79f73c6e5928e0e7200f386170df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f8023ae680108831a9cd97b2341827ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c9103d8f6c5d4ea6094ee778dd91086.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Posterior-Sampling-Ability-of-Plug-Play-Diffusion-Methods-in-Sparse-View-CT"><a href="#Evaluating-the-Posterior-Sampling-Ability-of-Plug-Play-Diffusion-Methods-in-Sparse-View-CT" class="headerlink" title="Evaluating the Posterior Sampling Ability of Plug&amp;Play Diffusion Methods   in Sparse-View CT"></a>Evaluating the Posterior Sampling Ability of Plug&amp;Play Diffusion Methods   in Sparse-View CT</h2><p><strong>Authors:Liam Moroy, Guillaume Bourmaud, FrÃ©dÃ©ric Champagnat, Jean-FranÃ§ois Giovannelli</strong></p>
<p>Plug&amp;Play (PnP) diffusion models are state-of-the-art methods in computed tomography (CT) reconstruction. Such methods usually consider applications where the sinogram contains a sufficient amount of information for the posterior distribution to be concentrated around a single mode, and consequently are evaluated using image-to-image metrics such as PSNR&#x2F;SSIM. Instead, we are interested in reconstructing compressible flow images from sinograms having a small number of projections, which results in a posterior distribution no longer concentrated or even multimodal. Thus, in this paper, we aim at evaluating the approximate posterior of PnP diffusion models and introduce two posterior evaluation properties. We quantitatively evaluate three PnP diffusion methods on three different datasets for several numbers of projections. We surprisingly find that, for each method, the approximate posterior deviates from the true posterior when the number of projections decreases. </p>
<blockquote>
<p>Plug&amp;Playï¼ˆPnPï¼‰æ‰©æ•£æ¨¡å‹æ˜¯ç›®å‰è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰é‡å»ºé¢†åŸŸçš„æœ€å‰æ²¿æ–¹æ³•ã€‚è¿™ç±»æ–¹æ³•é€šå¸¸åº”ç”¨äºè¾›æ ¼æ‹‰å§†å›¾ä¸­åŒ…å«è¶³å¤Ÿä¿¡æ¯ï¼Œä½¿å¾—åéªŒåˆ†å¸ƒé›†ä¸­åœ¨å•ä¸€æ¨¡å¼çš„æƒ…å†µï¼Œå› æ­¤å®ƒä»¬æ˜¯é€šè¿‡å›¾åƒåˆ°å›¾åƒçš„æŒ‡æ ‡ï¼ˆå¦‚å³°å€¼ä¿¡å™ªæ¯”&#x2F;ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼‰æ¥è¯„ä»·çš„ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯¹ä»è¾›æ ¼æ‹‰å§†å›¾é‡å»ºå¯å‹ç¼©æµåŠ¨å›¾åƒæ„Ÿå…´è¶£ï¼Œè¯¥è¾›æ ¼æ‹‰å§†å›¾å…·æœ‰å°‘é‡çš„æŠ•å½±ï¼Œå¯¼è‡´åéªŒåˆ†å¸ƒä¸å†é›†ä¸­ï¼Œç”šè‡³æ˜¯å¤šæ¨¡æ€çš„ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨è¯„ä¼°PnPæ‰©æ•£æ¨¡å‹çš„è¿‘ä¼¼åéªŒï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªåéªŒè¯„ä¼°å±æ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šå®šé‡è¯„ä¼°äº†ä¸‰ç§PnPæ‰©æ•£æ–¹æ³•ï¼Œè¿›è¡Œäº†å¤šæ¬¡æŠ•å½±ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ï¼Œå¯¹äºæ¯ç§æ–¹æ³•ï¼Œå½“æŠ•å½±æ•°é‡å‡å°‘æ—¶ï¼Œè¿‘ä¼¼åéªŒä¸çœŸå®åéªŒä¹‹é—´å­˜åœ¨åå·®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21301v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>PnPæ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æé‡å»ºä¸­æ˜¯å…ˆè¿›æŠ€æœ¯ï¼Œä½†åœ¨æŠ•å½±æ•°é‡è¾ƒå°‘çš„æƒ…å†µä¸‹é‡æ„å¯å‹ç¼©æµå›¾åƒæ—¶ï¼Œå…¶è¿‘ä¼¼åéªŒåˆ†å¸ƒä¼šåç¦»çœŸå®åéªŒåˆ†å¸ƒã€‚æœ¬æ–‡æ—¨åœ¨è¯„ä¼°PnPæ‰©æ•£æ¨¡å‹çš„è¿‘ä¼¼åéªŒï¼Œå¹¶å¼•å…¥ä¸¤ç§åéªŒè¯„ä¼°å±æ€§ï¼Œå¯¹ä¸‰ç§PnPæ‰©æ•£æ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†å’Œä¸åŒæŠ•å½±æ•°é‡ä¸Šè¿›è¡Œå®šé‡è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PnPæ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æé‡å»ºä¸­æ˜¯ä¸»æµæŠ€æœ¯ã€‚</li>
<li>å½“æŠ•å½±æ•°é‡è¾ƒå°‘æ—¶ï¼Œé‡æ„å¯å‹ç¼©æµå›¾åƒçš„è¿‘ä¼¼åéªŒåˆ†å¸ƒä¼šåç¦»çœŸå®åéªŒåˆ†å¸ƒã€‚</li>
<li>æœ¬æ–‡çš„ç›®æ ‡æ˜¯è¯„ä¼°PnPæ‰©æ•£æ¨¡å‹çš„è¿‘ä¼¼åéªŒã€‚</li>
<li>å¼•å…¥ä¸¤ç§åéªŒè¯„ä¼°å±æ€§æ¥è¯„ä»·PnPæ‰©æ•£æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸åŒæ•°æ®é›†å’Œä¸åŒæŠ•å½±æ•°é‡ä¸‹ï¼Œå¯¹ä¸‰ç§PnPæ‰©æ•£æ–¹æ³•è¿›è¡Œå®šé‡è¯„ä¼°ã€‚</li>
<li>å‘ç°æ¯ç§æ–¹æ³•çš„è¿‘ä¼¼åéªŒåœ¨æŠ•å½±æ•°é‡å‡å°‘æ—¶ä¼šåç¦»çœŸå®åéªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fc9cee43fc4a04579430e9026bf5c7f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6babed71ed2bfee288ad9237327b50a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ea902f14e5b979f6ab39ced661e5941.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00cc0a19dbf08f8be713d992c56c0ec9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Multi-modal-Vision-Pre-training-for-Medical-Image-Analysis"><a href="#Multi-modal-Vision-Pre-training-for-Medical-Image-Analysis" class="headerlink" title="Multi-modal Vision Pre-training for Medical Image Analysis"></a>Multi-modal Vision Pre-training for Medical Image Analysis</h2><p><strong>Authors:Shaohao Rui, Lingzhi Chen, Zhenyu Tang, Lilong Wang, Mianxin Liu, Shaoting Zhang, Xiaosong Wang</strong></p>
<p>Self-supervised learning has greatly facilitated medical image analysis by suppressing the training data requirement for real-world applications. Current paradigms predominantly rely on self-supervision within uni-modal image data, thereby neglecting the inter-modal correlations essential for effective learning of cross-modal image representations. This limitation is particularly significant for naturally grouped multi-modal data, e.g., multi-parametric MRI scans for a patient undergoing various functional imaging protocols in the same study. To bridge this gap, we conduct a novel multi-modal image pre-training with three proxy tasks to facilitate the learning of cross-modality representations and correlations using multi-modal brain MRI scans (over 2.4 million images in 16,022 scans of 3,755 patients), i.e., cross-modal image reconstruction, modality-aware contrastive learning, and modality template distillation. To demonstrate the generalizability of our pre-trained model, we conduct extensive experiments on various benchmarks with ten downstream tasks. The superior performance of our method is reported in comparison to state-of-the-art pre-training methods, with Dice Score improvement of 0.28%-14.47% across six segmentation benchmarks and a consistent accuracy boost of 0.65%-18.07% in four individual image classification tasks. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ é€šè¿‡å‡å°‘å¯¹ç°å®ä¸–ç•Œåº”ç”¨æ‰€éœ€è®­ç»ƒæ•°æ®çš„è¦æ±‚ï¼Œæå¤§åœ°ä¿ƒè¿›äº†åŒ»å­¦å›¾åƒåˆ†æã€‚å½“å‰çš„æ¨¡å¼ä¸»è¦ä¾èµ–äºå•æ¨¡æ€å›¾åƒæ•°æ®å†…çš„è‡ªç›‘ç£ï¼Œä»è€Œå¿½ç•¥äº†è·¨æ¨¡æ€å›¾åƒè¡¨ç¤ºæœ‰æ•ˆå­¦ä¹ æ‰€å¿…éœ€çš„è·¨æ¨¡æ€å…³è”ã€‚è¿™ä¸€å±€é™æ€§å¯¹äºè‡ªç„¶åˆ†ç»„çš„å¤šæ¨¡æ€æ•°æ®å°¤ä¸ºé‡è¦ï¼Œä¾‹å¦‚ï¼ŒåŒä¸€ç ”ç©¶ä¸­æ‚£è€…æ¥å—å¤šç§åŠŸèƒ½æˆåƒåè®®çš„å¤šå‚æ•°MRIæ‰«æã€‚ä¸ºäº†å¼¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å›¾åƒé¢„è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ä¸‰ä¸ªä»£ç†ä»»åŠ¡ä¿ƒè¿›ä½¿ç”¨å¤šæ¨¡æ€è„‘MRIæ‰«æï¼ˆè¶…è¿‡3755åæ‚£è€…çš„16,022æ¬¡æ‰«æä¸­çš„è¶…è¿‡240ä¸‡å¼ å›¾åƒï¼‰çš„è·¨æ¨¡æ€è¡¨ç¤ºå’Œå…³è”çš„å­¦ä¹ ï¼Œå³è·¨æ¨¡æ€å›¾åƒé‡å»ºã€æ¨¡æ€æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ å’Œæ¨¡æ€æ¨¡æ¿è’¸é¦ã€‚ä¸ºäº†å±•ç¤ºæˆ‘ä»¬é¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œæ¶‰åŠåé¡¹ä¸‹æ¸¸ä»»åŠ¡ã€‚ä¸æœ€æ–°çš„é¢„è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…­ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­æŠ¥å‘Šäº†è¾ƒé«˜çš„æ€§èƒ½æå‡ï¼ŒDiceå¾—åˆ†æé«˜äº†0.28%~14.47%ï¼Œåœ¨å››é¡¹å•ç‹¬çš„å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å‡†ç¡®ç‡ä¹ŸæŒç»­æå‡äº†0.65%~18.07%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10604v2">PDF</a> </p>
<p><strong>Summary</strong><br>    è‡ªç›‘ç£å­¦ä¹ é™ä½äº†åŒ»å­¦å›¾åƒåˆ†æå¯¹è®­ç»ƒæ•°æ®çš„è¦æ±‚ï¼Œä¿ƒè¿›äº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å‘å±•ã€‚å½“å‰ä¸»æµæ–¹æ³•ä¸»è¦ä¾èµ–å•æ¨¡æ€å›¾åƒæ•°æ®çš„è‡ªç›‘ç£ï¼Œå¿½ç•¥äº†è·¨æ¨¡æ€å›¾åƒè¡¨ç¤ºå­¦ä¹ ä¸­è‡³å…³é‡è¦çš„è·¨æ¨¡æ€å…³è”ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å¤šæ¨¡æ€å›¾åƒé¢„è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨ä¸‰ç§ä»£ç†ä»»åŠ¡å­¦ä¹ è·¨æ¨¡æ€è¡¨ç¤ºå’Œå…³è”ï¼Œå¹¶ä½¿ç”¨å¤šæ¨¡æ€è„‘MRIæ‰«ææ•°æ®ï¼ˆè¶…è¿‡2.4ç™¾ä¸‡å¼ å›¾åƒï¼‰è¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æœ€æ–°é¢„è®­ç»ƒæ–¹æ³•ï¼ŒDice Scoreåœ¨å…­ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­æé«˜äº†0.28%~14.47%ï¼Œåœ¨å››ä¸ªç‹¬ç«‹å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡æé«˜äº†0.65%~18.07%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ é™ä½äº†åŒ»å­¦å›¾åƒåˆ†æå¯¹è®­ç»ƒæ•°æ®çš„è¦æ±‚ã€‚</li>
<li>å½“å‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ä¸»è¦ä¾èµ–å•æ¨¡æ€å›¾åƒæ•°æ®çš„è‡ªç›‘ç£ï¼Œå¿½ç•¥äº†è·¨æ¨¡æ€å…³è”çš„é‡è¦æ€§ã€‚</li>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å›¾åƒé¢„è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨ä¸‰ç§ä»£ç†ä»»åŠ¡å­¦ä¹ è·¨æ¨¡æ€è¡¨ç¤ºå’Œå…³è”ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨å¤šæ¨¡æ€è„‘MRIæ‰«ææ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«è¶…è¿‡2.4ç™¾ä¸‡å¼ å›¾åƒã€‚</li>
<li>å®éªŒè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æœ€æ–°é¢„è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>åœ¨å…­ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•çš„Dice Scoreæé«˜äº†0.28%~14.47%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f739b1487666183dabf8a6981ce3884.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52c32e57e3b45bad3c019fc557b3816e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2703f044604cc4ec3b94f91a52e06f51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e484748b35a3f7f0081a8e2e3fdaa26.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Brain-Tumor-Classification-on-MRI-in-Light-of-Molecular-Markers"><a href="#Brain-Tumor-Classification-on-MRI-in-Light-of-Molecular-Markers" class="headerlink" title="Brain Tumor Classification on MRI in Light of Molecular Markers"></a>Brain Tumor Classification on MRI in Light of Molecular Markers</h2><p><strong>Authors:Jun Liu, Geng Yuan, Weihao Zeng, Hao Tang, Wenbin Zhang, Xue Lin, XiaoLin Xu, Dong Huang, Yanzhi Wang</strong></p>
<p>In research findings, co-deletion of the 1p&#x2F;19q gene is associated with clinical outcomes in low-grade gliomas. The ability to predict 1p19q status is critical for treatment planning and patient follow-up. This study aims to utilize a specially MRI-based convolutional neural network for brain cancer detection. Although public networks such as RestNet and AlexNet can effectively diagnose brain cancers using transfer learning, the model includes quite a few weights that have nothing to do with medical images. As a result, the diagnostic results are unreliable by the transfer learning model. To deal with the problem of trustworthiness, we create the model from the ground up, rather than depending on a pre-trained model. To enable flexibility, we combined convolution stacking with a dropout and full connect operation, it improved performance by reducing overfitting. During model training, we also supplement the given dataset and inject Gaussian noise. We use threeâ€“fold cross-validation to train the best selection model. Comparing InceptionV3, VGG16, and MobileNetV2 fine-tuned with pre-trained models, our model produces better results. On an validation set of 125 codeletion vs. 31 not codeletion images, the proposed network achieves 96.37% percent F1-score, 97.46% percent precision, and 96.34% percent recall when classifying 1p&#x2F;19q codeletion and not codeletion images. </p>
<blockquote>
<p>åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­å‘ç°ï¼Œ1p&#x2F;19qåŸºå› çš„è”åˆç¼ºå¤±ä¸ä½çº§åˆ«èƒ¶è´¨ç˜¤çš„ä¸´åºŠç»“æœæœ‰å…³ã€‚é¢„æµ‹1p19qçŠ¶æ€å¯¹äºæ²»ç–—è®¡åˆ’å’Œæ‚£è€…éšè®¿è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨åŸºäºMRIçš„å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œè„‘ç™Œæ£€æµ‹ã€‚è™½ç„¶RestNetå’ŒAlexNetç­‰å…¬å…±ç½‘ç»œå¯ä»¥é€šè¿‡è¿ç§»å­¦ä¹ æœ‰æ•ˆåœ°è¯Šæ–­è„‘ç™Œï¼Œä½†æ¨¡å‹ä¸­åŒ…æ‹¬è®¸å¤šä¸åŒ»å­¦å›¾åƒæ— å…³çš„æƒé‡ã€‚å› æ­¤ï¼Œè¿ç§»å­¦ä¹ æ¨¡å‹çš„è¯Šæ–­ç»“æœå¹¶ä¸å¯é ã€‚ä¸ºäº†è§£å†³å¯é æ€§çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä»é›¶å¼€å§‹åˆ›å»ºæ¨¡å‹ï¼Œè€Œä¸æ˜¯ä¾èµ–äºé¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸ºäº†æé«˜çµæ´»æ€§ï¼Œæˆ‘ä»¬å°†å·ç§¯å †å ä¸ä¸¢å¼ƒå’Œå…¨è¿æ¥æ“ä½œç›¸ç»“åˆï¼Œé€šè¿‡å‡å°‘è¿‡æ‹Ÿåˆæé«˜äº†æ€§èƒ½ã€‚åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¿˜è¡¥å……äº†ç»™å®šçš„æ•°æ®é›†å¹¶æ³¨å…¥äº†é«˜æ–¯å™ªå£°ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰æŠ˜äº¤å‰éªŒè¯æ¥è®­ç»ƒæœ€ä½³é€‰å‹æ¨¡å‹ã€‚ä¸é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒè¿‡çš„InceptionV3ã€VGG16å’ŒMobileNetV2ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å–å¾—äº†æ›´å¥½çš„ç»“æœã€‚åœ¨125å¼ codeletionä¸31å¼ écodeletionå›¾åƒçš„éªŒè¯é›†ä¸Šï¼Œæ‰€æå‡ºç½‘ç»œåœ¨åˆ†ç±»1p&#x2F;19q codeletionå’Œécodeletionå›¾åƒæ—¶ï¼Œè¾¾åˆ°äº†96.37%çš„F1åˆ†æ•°ã€97.46%çš„ç²¾ç¡®åº¦å’Œ96.34%çš„å¬å›ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19583v3">PDF</a> ICAIâ€™22 - The 24th International Conference on Artificial   Intelligence, The 2022 World Congress in Computer Science, Computer   Engineering, &amp; Applied Computing (CSCEâ€™22), Las Vegas, USA. The paper   acceptance rate 17% for regular papers. The publication of the CSCE 2022   conference proceedings has been delayed due to the pandemic</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡MRIæˆåƒçš„å·ç§¯ç¥ç»ç½‘ç»œåœ¨ä½çº§åˆ«èƒ¶è´¨ç˜¤ä¸­çš„è¯Šæ–­åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å…³äºæ¶‰åŠåŸºå› 1p&#x2F;19qè”åˆç¼ºå¤±çš„é¢„æµ‹ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹ï¼Œå…¶è‡ªä¸»æ„å»ºè€Œéä¾èµ–é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶èåˆå¤šç§æŠ€æœ¯ä¼˜åŒ–å…¶æ€§èƒ½å’Œå¯ä¿¡åº¦ï¼Œè¾¾åˆ°é«˜å‡†ç¡®åº¦çš„è¯Šæ–­æ•ˆæœã€‚å®éªŒæ˜¾ç¤ºå…¶åœ¨ç‰¹å®šçš„å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½æŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å‘ç°åŸºå› 1p&#x2F;19qè”åˆç¼ºå¤±ä¸ä½çº§åˆ«èƒ¶è´¨ç˜¤çš„ä¸´åºŠç»“æœæœ‰å…³ã€‚é¢„æµ‹æ­¤åŸºå› çŠ¶æ€å¯¹æ²»ç–—è®¡åˆ’å’Œæ‚£è€…éšè®¿è‡³å…³é‡è¦ã€‚</li>
<li>è¯¥ç ”ç©¶ä½¿ç”¨åŸºäºMRIçš„å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œè„‘ç™Œæ£€æµ‹ã€‚å°½ç®¡å…¬å…±ç½‘ç»œå¦‚RestNetå’ŒAlexNetå¯é€šè¿‡è¿ç§»å­¦ä¹ æœ‰æ•ˆè¯Šæ–­è„‘ç™Œï¼Œä½†å®ƒä»¬åœ¨æŸäº›æƒ…å†µä¸‹å­˜åœ¨è¯Šæ–­ç»“æœä¸å¯é çš„é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³å¯é æ€§é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿè‡ªä¸»æ„å»ºæ¨¡å‹ï¼Œæ‘’å¼ƒä¾èµ–é¢„è®­ç»ƒæ¨¡å‹çš„æ–¹å¼ã€‚</li>
<li>ç»“åˆå·ç§¯å †å ã€ä¸¢å¼ƒæ“ä½œå’Œå®Œå…¨è¿æ¥æ“ä½œï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å¹¶é™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚</li>
<li>åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯¹ç»™å®šæ•°æ®é›†è¿›è¡Œäº†è¡¥å……å¹¶æ·»åŠ äº†é«˜æ–¯å™ªå£°ã€‚</li>
<li>ä½¿ç”¨ä¸‰æŠ˜äº¤å‰éªŒè¯è®­ç»ƒæœ€ä½³æ¨¡å‹é€‰æ‹©ã€‚å¯¹æ¯”InceptionV3ã€VGG16å’ŒMobileNetV2ç­‰é¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒç»“æœï¼Œæ–°æ¨¡å‹è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c7a8c844581ac900a37abf6e5b2b579.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ff84d93aef756890e2e355e419c0227.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b66ed32d7df1eac87cb1528592ebee8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2465ace51409c291120c1b39011ea43e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4247b1dbc3cb7bc0c6315c37e9155e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f8830741816d921c0895e0b9be5429c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-668ed72c78842c90db837505bd073cb3.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  MoonCast High-Quality Zero-Shot Podcast Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-269f22fed228fdef8433d3ba3be92c1d.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  Periodontal Bone Loss Analysis via Keypoint Detection With Heuristic   Post-Processing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
