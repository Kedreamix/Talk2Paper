<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  MusicInfuser Making Video Diffusion Listen and Dance">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5d6a422a9a5048b5eea40aafa607f945.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-20-æ›´æ–°"><a href="#2025-03-20-æ›´æ–°" class="headerlink" title="2025-03-20 æ›´æ–°"></a>2025-03-20 æ›´æ–°</h1><h2 id="MusicInfuser-Making-Video-Diffusion-Listen-and-Dance"><a href="#MusicInfuser-Making-Video-Diffusion-Listen-and-Dance" class="headerlink" title="MusicInfuser: Making Video Diffusion Listen and Dance"></a>MusicInfuser: Making Video Diffusion Listen and Dance</h2><p><strong>Authors:Susung Hong, Ira Kemelmacher-Shlizerman, Brian Curless, Steven M. Seitz</strong></p>
<p>We introduce MusicInfuser, an approach for generating high-quality dance videos that are synchronized to a specified music track. Rather than attempting to design and train a new multimodal audio-video model, we show how existing video diffusion models can be adapted to align with musical inputs by introducing lightweight music-video cross-attention and a low-rank adapter. Unlike prior work requiring motion capture data, our approach fine-tunes only on dance videos. MusicInfuser achieves high-quality music-driven video generation while preserving the flexibility and generative capabilities of the underlying models. We introduce an evaluation framework using Video-LLMs to assess multiple dimensions of dance generation quality. The project page and code are available at <a target="_blank" rel="noopener" href="https://susunghong.github.io/MusicInfuser">https://susunghong.github.io/MusicInfuser</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†MusicInfuserï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆé«˜è´¨é‡èˆè¹ˆè§†é¢‘çš„æ–¹æ³•ï¼Œè¯¥è§†é¢‘ä¸æŒ‡å®šçš„éŸ³ä¹æ›²ç›®åŒæ­¥ã€‚æˆ‘ä»¬å¹¶æ²¡æœ‰å°è¯•è®¾è®¡å’Œè®­ç»ƒæ–°çš„å¤šæ¨¡æ€éŸ³é¢‘è§†é¢‘æ¨¡å‹ï¼Œè€Œæ˜¯å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å¯¹ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡å¼•å…¥è½»é‡çº§çš„éŸ³ä¹è§†é¢‘äº¤å‰æ³¨æ„åŠ›å’Œä½é˜¶é€‚é…å™¨æ¥é€‚åº”éŸ³ä¹è¾“å…¥ã€‚ä¸å…ˆå‰éœ€è¦åŠ¨ä½œæ•æ‰æ•°æ®çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…åœ¨èˆè¹ˆè§†é¢‘ä¸Šè¿›è¡Œå¾®è°ƒã€‚MusicInfuserå®ç°äº†é«˜è´¨é‡çš„éŸ³ä¹é©±åŠ¨è§†é¢‘ç”Ÿæˆï¼ŒåŒæ—¶ä¿ç•™äº†åŸºç¡€æ¨¡å‹çš„çµæ´»æ€§å’Œç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä½¿ç”¨Video-LLMsçš„è¯„ä¼°æ¡†æ¶ï¼Œä»å¤šä¸ªç»´åº¦è¯„ä¼°èˆè¹ˆç”Ÿæˆçš„è´¨é‡ã€‚é¡¹ç›®é¡µé¢å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://susunghong.github.io/MusicInfuser%E8%AE%BF%E9%97%AE%E3%80%82">https://susunghong.github.io/MusicInfuserè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14505v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://susunghong.github.io/MusicInfuser">https://susunghong.github.io/MusicInfuser</a></p>
<p><strong>Summary</strong></p>
<p>éŸ³ä¹Infuseræ–¹æ³•ç”Ÿæˆé«˜è´¨é‡èˆè¹ˆè§†é¢‘ï¼Œè¯¥è§†é¢‘åŒæ­¥ç‰¹å®šéŸ³ä¹æ›²ç›®ã€‚æ­¤æ–¹æ³•é€šè¿‡å¼•å…¥è½»é‡çº§éŸ³ä¹è§†é¢‘äº¤å‰æ³¨æ„åŠ›ä¸ä½é˜¶é€‚é…å™¨ï¼Œå±•ç¤ºäº†ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹å¦‚ä½•é€‚åº”éŸ³ä¹è¾“å…¥ã€‚ä¸éœ€è¦åŠ¨ä½œæ•æ‰æ•°æ®çš„æ—©æœŸå·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…å¯¹èˆè¹ˆè§†é¢‘è¿›è¡Œå¾®è°ƒã€‚MusicInfuserå®ç°äº†é«˜è´¨é‡çš„éŸ³ä¹é©±åŠ¨è§†é¢‘ç”Ÿæˆï¼ŒåŒæ—¶ä¿ç•™äº†åº•å±‚æ¨¡å‹çš„çµæ´»æ€§å’Œç”Ÿæˆèƒ½åŠ›ã€‚åŒæ—¶åˆ©ç”¨Video-LLMså»ºç«‹è¯„ä»·ä½“ç³»å¯¹èˆè¹ˆç”Ÿæˆè´¨é‡è¿›è¡Œäº†å¤šç»´è¯„ä»·ã€‚é¡¹ç›®å’Œä»£ç åœ¨ç½‘ç«™å¯ä¸‹è½½å’ŒæŸ¥é˜…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MusicInfuseræ˜¯ä¸€ç§ç”Ÿæˆé«˜è´¨é‡èˆè¹ˆè§†é¢‘çš„æ–¹æ³•ï¼Œèƒ½å¤ŸåŒæ­¥ç‰¹å®šéŸ³ä¹æ›²ç›®ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥è½»é‡çº§éŸ³ä¹è§†é¢‘äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å¾—ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿé€‚åº”éŸ³ä¹è¾“å…¥ã€‚</li>
<li>ä½é˜¶é€‚é…å™¨çš„å¼•å…¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä¸æ—©æœŸéœ€è¦åŠ¨ä½œæ•æ‰æ•°æ®çš„æ–¹æ³•ä¸åŒï¼ŒMusicInfuserä»…å¯¹èˆè¹ˆè§†é¢‘è¿›è¡Œå¾®è°ƒã€‚</li>
<li>MusicInfuserå®ç°äº†é«˜è´¨é‡çš„éŸ³ä¹é©±åŠ¨è§†é¢‘ç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•ä¿ç•™äº†åº•å±‚æ¨¡å‹çš„çµæ´»æ€§å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d5cb6655b3b12cb2c213a494f0f2e52c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11f8bd2bc6e0e6f8e13fade76ba18e28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2460722a94cce0a40f890e8a80a0862a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2137c940f8a6713e315ce582528cc135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-638208498e819940ab833bee66feedd2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Temporal-Consistency-for-LLM-Reasoning-Process-Error-Identification"><a href="#Temporal-Consistency-for-LLM-Reasoning-Process-Error-Identification" class="headerlink" title="Temporal Consistency for LLM Reasoning Process Error Identification"></a>Temporal Consistency for LLM Reasoning Process Error Identification</h2><p><strong>Authors:Jiacheng Guo, Yue Wu, Jiahao Qiu, Kaixuan Huang, Xinzhe Juan, Ling Yang, Mengdi Wang</strong></p>
<p>Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B&#x2F;8B distilled models to outperform all 70B&#x2F;72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/jcguo123/Temporal-Consistency">https://github.com/jcguo123/Temporal-Consistency</a> </p>
<blockquote>
<p>éªŒè¯å¯¹äºæœ‰æ•ˆçš„æ•°å­¦æ¨ç†è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´ä¸€è‡´æ€§æ–¹æ³•ï¼ŒéªŒè¯è€…å¯ä»¥æ ¹æ®ä¹‹å‰çš„è¯„ä¼°ç»“æœè¿­ä»£åœ°è°ƒæ•´è‡ªå·±çš„åˆ¤æ–­ã€‚ä¸åŒäºä¸€è½®éªŒè¯æˆ–å¤šæ¨¡å‹è¾©è®ºæ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä¸€ç³»åˆ—è‡ªæˆ‘åæ€è¡Œä¸ºçš„è¿è´¯æ€§æ¥æé«˜éªŒè¯çš„å‡†ç¡®æ€§ã€‚åœ¨å¤šæ ·åŒ–çš„æ•°å­¦è¿‡ç¨‹è¯¯å·®è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆMathcheckã€ProcessBenchå’ŒPRM800Kï¼‰ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºå‡†æ–¹æ³•ä¸Šå®ç°äº†æ€§èƒ½ä¸Šçš„æŒç»­æ”¹è¿›ã€‚å½“åº”ç”¨äºæœ€æ–°çš„DeepSeek R1è’¸é¦æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½¿7B&#x2F;8Bè’¸é¦æ¨¡å‹åœ¨ProcessBenchä¸Šä¼˜äºæ‰€æœ‰70B&#x2F;72Bæ¨¡å‹å’ŒGPT-4oã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨æˆ‘ä»¬æ–¹æ³•çš„14Bè’¸é¦æ¨¡å‹å®ç°äº†ä¸Deepseek-R1ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jcguo123/Temporal-Consistency">https://github.com/jcguo123/Temporal-Consistency</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14495v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šä¸€ç§åŸºäºæ—¶é—´ä¸€è‡´æ€§çš„æ–°éªŒè¯æ–¹æ³•èƒ½æå‡æ•°å­¦æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚å®ƒé€šè¿‡è¿­ä»£ç²¾ç»†éªŒè¯è¿‡ç¨‹æé«˜éªŒè¯çš„å‡†ç¡®æ€§ï¼Œä¸”ç›¸æ¯”ä¸€æ¬¡æ€§çš„éªŒè¯æˆ–å¤šæ¨¡å‹è¾©è®ºæ–¹æ³•æ›´å…·ä¼˜åŠ¿ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§æ•°å­¦è¿‡ç¨‹è¯¯å·®è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œå¹¶ä¸”å¯¹äºæœ€æ–°DeepSeek R1è’¸é¦æ¨¡å‹ä¹Ÿæœ‰è‰¯å¥½çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/jcguo123/Temporal-Consistency%E4%BE%9B%E5%85%AC%E4%BC%97%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/jcguo123/Temporal-Consistencyä¾›å…¬ä¼—è®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>éªŒè¯åœ¨æ•°å­¦æ¨ç†ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºäºæ—¶é—´ä¸€è‡´æ€§çš„éªŒè¯æ–¹æ³•ã€‚</li>
<li>æ­¤æ–¹æ³•é€šè¿‡è¿­ä»£ç²¾ç»†éªŒè¯è¿‡ç¨‹æé«˜éªŒè¯çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä¸å…¶ä»–éªŒè¯æ–¹æ³•ç›¸æ¯”ï¼Œå¦‚ä¸€æ¬¡æ€§éªŒè¯æˆ–å¤šæ¨¡å‹è¾©è®ºï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>åœ¨å¤šç§æ•°å­¦è¿‡ç¨‹è¯¯å·®è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸æœ€æ–°DeepSeek R1è’¸é¦æ¨¡å‹ç»“åˆè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œè’¸é¦çš„14Bæ¨¡å‹æ€§èƒ½å¯ä¸Deepseek-R1ç›¸åª²ç¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-864435f65ba915c9c421c72f02dcaca9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9f347cea6136be5e6d5647dfe7bd089.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6edba0c74726ea6a8f69b51e0be5c298.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02ad2c1bead1643cdbe57dd22a2df711.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49904da6b786b168eef1f756be2bbffd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fa6a0677d295b614a48c6f59f27860f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03907bb3d4660fd29148acbe611707e7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DAPO-An-Open-Source-LLM-Reinforcement-Learning-System-at-Scale"><a href="#DAPO-An-Open-Source-LLM-Reinforcement-Learning-System-at-Scale" class="headerlink" title="DAPO: An Open-Source LLM Reinforcement Learning System at Scale"></a>DAPO: An Open-Source LLM Reinforcement Learning System at Scale</h2><p><strong>Authors:Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, Mingxuan Wang</strong></p>
<p>Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the $\textbf{D}$ecoupled Clip and $\textbf{D}$ynamic s$\textbf{A}$mpling $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{DAPO}$) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL. </p>
<blockquote>
<p>æ¨ç†æ‰©å±•èµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹å‰æ‰€æœªæœ‰çš„æ¨ç†èƒ½åŠ›ï¼Œå¼ºåŒ–å­¦ä¹ æ˜¯æ¿€å‘å¤æ‚æ¨ç†çš„æ ¸å¿ƒæŠ€æœ¯ã€‚ç„¶è€Œï¼Œå‰æ²¿æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®æŠ€æœ¯ç»†èŠ‚è¢«éšè—ï¼ˆä¾‹å¦‚åœ¨OpenAI o1åšå®¢å’ŒDeepSeek R1æŠ€æœ¯æŠ¥å‘Šä¸­ï¼‰ï¼Œå› æ­¤ç¤¾åŒºéš¾ä»¥é‡ç°å…¶å¼ºåŒ–å­¦ä¹ è®­ç»ƒç»“æœã€‚æˆ‘ä»¬æå‡ºäº†è§£è€¦å‰ªè¾‘å’ŒåŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ç®—æ³•ï¼Œå¹¶å®Œå…¨å¼€æºäº†ä¸€ä¸ªå…ˆè¿›çš„å¤§å‹å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨Qwen2.5-30BåŸºç¡€æ¨¡å‹åœ¨AIME 2024ä¸Šå–å¾—äº†50åˆ†çš„æˆç»©ã€‚ä¸åŒäºä¹‹å‰éšç’è®­ç»ƒç»†èŠ‚çš„ç ”ç©¶ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç®—æ³•å››ä¸ªå…³é”®æŠ€æœ¯ï¼Œä½¿å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ å–å¾—æˆåŠŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨verlæ¡†æ¶ä¸Šå¼€æºäº†è®­ç»ƒä»£ç ï¼Œä»¥åŠç²¾å¿ƒç­–åˆ’å’Œå¤„ç†çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å¼€æºç³»ç»Ÿçš„è¿™äº›ç»„ä»¶æé«˜äº†å¯é‡å¤æ€§ï¼Œæ”¯æŒæœªæ¥åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14476v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://dapo-sia.github.io/">https://dapo-sia.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æ¨ç†å¤§æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡èƒ½åŠ›ï¼Œä½†æ ¸å¿ƒç»†èŠ‚è¢«éšè—ã€‚æˆ‘ä»¬æå‡ºDAPOç®—æ³•å¹¶å¼€æºä¸€ä¸ªå…ˆè¿›çš„å¤§è§„æ¨¡RLç³»ç»Ÿï¼Œä½¿ç”¨Qwen2.5-32BåŸºç¡€æ¨¡å‹åœ¨AIME 2024ä¸Šè·å¾—50åˆ†ã€‚æˆ‘ä»¬çš„ç®—æ³•åŒ…å«å››ä¸ªå…³é”®æŠ€æœ¯ï¼Œä¿ƒè¿›å¤§è§„æ¨¡LLM RLçš„æˆåŠŸã€‚åŒæ—¶å¼€æºè®­ç»ƒä»£ç å’Œæ•°æ®å¤„ç†é›†ï¼Œå¢å¼ºå¯å¤åˆ¶æ€§å’Œæ”¯æŒæœªæ¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¨ç†å¤§æ¨¡å‹åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æå‡èƒ½åŠ›ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„å¤§æ¨¡å‹ç»†èŠ‚è¢«éšè—ï¼Œç¤¾åŒºéš¾ä»¥å¤åˆ¶å…¶è®­ç»ƒç»“æœã€‚</li>
<li>æå‡ºDAPOç®—æ³•å¹¶æˆåŠŸåº”ç”¨äºå¤§è§„æ¨¡LLM RLã€‚</li>
<li>DAPOç®—æ³•åŒ…å«å››ä¸ªå…³é”®æŠ€æœ¯ï¼Œæœ‰åŠ©äºå¤§è§„æ¨¡LLM RLçš„æˆåŠŸã€‚</li>
<li>å¼€æºè®­ç»ƒä»£ç å’Œæ•°æ®å¤„ç†é›†ï¼Œå¢å¼ºå¯å¤åˆ¶æ€§å’Œæ”¯æŒæœªæ¥ç ”ç©¶ã€‚</li>
<li>ä½¿ç”¨Qwen2.5-32BåŸºç¡€æ¨¡å‹åœ¨AIME 2024ä¸Šè·å¾—50åˆ†çš„é«˜è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2a5f5362e301bf903157b5c79231bd4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db16af696ffdcd2c5766dcd2187d7ed1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c055e4050baf073fb00ea05bd12951f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-764ab49e819faaf000443b4d359989c8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Benchmarking-community-drug-response-prediction-models-datasets-models-tools-and-metrics-for-cross-dataset-generalization-analysis"><a href="#Benchmarking-community-drug-response-prediction-models-datasets-models-tools-and-metrics-for-cross-dataset-generalization-analysis" class="headerlink" title="Benchmarking community drug response prediction models: datasets,   models, tools, and metrics for cross-dataset generalization analysis"></a>Benchmarking community drug response prediction models: datasets,   models, tools, and metrics for cross-dataset generalization analysis</h2><p><strong>Authors:Alexander Partin, Priyanka Vasanthakumari, Oleksandr Narykov, Andreas Wilke, Natasha Koussa, Sara E. Jones, Yitan Zhu, Jamie C. Overbeek, Rajeev Jain, Gayara Demini Fernando, Cesar Sanchez-Villalobos, Cristina Garcia-Cardona, Jamaludin Mohd-Yusof, Nicholas Chia, Justin M. Wozniak, Souparno Ghosh, Ranadip Pal, Thomas S. Brettin, M. Ryan Weil, Rick L. Stevens</strong></p>
<p>Deep learning (DL) and machine learning (ML) models have shown promise in drug response prediction (DRP), yet their ability to generalize across datasets remains an open question, raising concerns about their real-world applicability. Due to the lack of standardized benchmarking approaches, model evaluations and comparisons often rely on inconsistent datasets and evaluation criteria, making it difficult to assess true predictive capabilities. In this work, we introduce a benchmarking framework for evaluating cross-dataset prediction generalization in DRP models. Our framework incorporates five publicly available drug screening datasets, six standardized DRP models, and a scalable workflow for systematic evaluation. To assess model generalization, we introduce a set of evaluation metrics that quantify both absolute performance (e.g., predictive accuracy across datasets) and relative performance (e.g., performance drop compared to within-dataset results), enabling a more comprehensive assessment of model transferability. Our results reveal substantial performance drops when models are tested on unseen datasets, underscoring the importance of rigorous generalization assessments. While several models demonstrate relatively strong cross-dataset generalization, no single model consistently outperforms across all datasets. Furthermore, we identify CTRPv2 as the most effective source dataset for training, yielding higher generalization scores across target datasets. By sharing this standardized evaluation framework with the community, our study aims to establish a rigorous foundation for model comparison, and accelerate the development of robust DRP models for real-world applications. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰å’Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹åœ¨è¯ç‰©ååº”é¢„æµ‹ï¼ˆDRPï¼‰ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨æ•°æ®é›†ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„é€‚ç”¨æ€§çš„æ‹…å¿§ã€‚ç”±äºç¼ºä¹æ ‡å‡†åŒ–çš„åŸºå‡†è¯„ä¼°æ–¹æ³•ï¼Œæ¨¡å‹è¯„ä¼°ä¸æ¯”è¾ƒé€šå¸¸ä¾èµ–äºä¸ä¸€è‡´çš„æ•°æ®é›†å’Œè¯„ä¼°æ ‡å‡†ï¼Œä½¿å¾—éš¾ä»¥è¯„ä¼°çœŸå®çš„é¢„æµ‹èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªç”¨äºè¯„ä¼°DRPæ¨¡å‹åœ¨è·¨æ•°æ®é›†é¢„æµ‹ä¸­æ³›åŒ–èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«äº†äº”ä¸ªå…¬å¼€çš„è¯ç‰©ç­›æŸ¥æ•°æ®é›†ã€å…­ä¸ªæ ‡å‡†åŒ–çš„DRPæ¨¡å‹ï¼Œä»¥åŠä¸€ä¸ªå¯æ‰©å±•çš„ç³»ç»Ÿè¯„ä¼°å·¥ä½œæµç¨‹ã€‚ä¸ºäº†è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€å¥—è¯„ä¼°æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡æ—¢è¡¡é‡ç»å¯¹æ€§èƒ½ï¼ˆä¾‹å¦‚è·¨æ•°æ®é›†çš„é¢„æµ‹ç²¾åº¦ï¼‰ï¼Œåˆè¡¡é‡ç›¸å¯¹æ€§èƒ½ï¼ˆä¾‹å¦‚ä¸å†…éƒ¨æ•°æ®é›†ç»“æœç›¸æ¯”çš„æ€§èƒ½ä¸‹é™ï¼‰ï¼Œä»è€Œå®ç°å¯¹æ¨¡å‹å¯è¿ç§»æ€§çš„æ›´å…¨é¢çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•æ—¶ï¼Œæ€§èƒ½å‡ºç°äº†å¤§å¹…ä¸‹é™ï¼Œè¿™çªæ˜¾äº†ä¸¥æ ¼è¯„ä¼°æ³›åŒ–èƒ½åŠ›çš„é‡è¦æ€§ã€‚è™½ç„¶æœ‰å‡ ä¸ªæ¨¡å‹è¡¨ç°å‡ºç›¸å¯¹è¾ƒå¼ºçš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œä½†æ²¡æœ‰å•ä¸€æ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå§‹ç»ˆè¡¨ç°æœ€ä½³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç¡®å®šCTRPv2ä¸ºæœ€æœ‰æ•ˆçš„è®­ç»ƒæºæ•°æ®é›†ï¼Œå…¶åœ¨ç›®æ ‡æ•°æ®é›†çš„æ³›åŒ–å¾—åˆ†è¾ƒé«˜ã€‚é€šè¿‡ä¸ç¤¾åŒºå…±äº«è¿™ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨å»ºç«‹ä¸¥æ ¼çš„æ¨¡å‹æ¯”è¾ƒåŸºç¡€ï¼Œå¹¶åŠ é€Ÿå¼€å‘ç”¨äºå®é™…åº”ç”¨çš„ç¨³å¥DRPæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14356v1">PDF</a> 18 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç”¨äºè¯„ä¼°è¯ç‰©ååº”é¢„æµ‹æ¨¡å‹è·¨æ•°æ®é›†é¢„æµ‹æ³›åŒ–èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«äº”ä¸ªå…¬å¼€è¯ç‰©ç­›æŸ¥æ•°æ®é›†ã€å…­ä¸ªæ ‡å‡†åŒ–çš„è¯ç‰©ååº”é¢„æµ‹æ¨¡å‹ä»¥åŠä¸€ä¸ªå¯ç³»ç»Ÿè¯„ä¼°çš„å·¥ä½œæµç¨‹ã€‚é€šè¿‡å¼•å…¥ä¸€ç³»åˆ—è¯„ä¼°æŒ‡æ ‡ï¼Œå¯¹æ¨¡å‹çš„ç»å¯¹æ€§èƒ½å’Œç›¸å¯¹æ€§èƒ½è¿›è¡Œäº†é‡åŒ–è¯„ä¼°ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„è¿ç§»èƒ½åŠ›çš„é‡è¦æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹åœ¨æœªè§æ•°æ®é›†ä¸Šçš„æ€§èƒ½æœ‰æ‰€ä¸‹é™ï¼Œè€ŒCTRPv2æ•°æ®é›†åœ¨è®­ç»ƒä¸­æœ€å…·æ³›åŒ–æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰å’Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹åœ¨è¯ç‰©ååº”é¢„æµ‹ï¼ˆDRPï¼‰ä¸­è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†å…¶åœ¨ä¸åŒæ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ä»å­˜åœ¨ç–‘é—®ã€‚</li>
<li>ç¼ºä¹æ ‡å‡†åŒ–çš„åŸºå‡†æµ‹è¯•æ–¹æ³•ï¼Œä½¿å¾—æ¨¡å‹è¯„ä¼°ä¸æ¯”è¾ƒä¾èµ–äºä¸ä¸€è‡´çš„æ•°æ®é›†å’Œè¯„ä¼°æ ‡å‡†ï¼Œéš¾ä»¥è¯„ä¼°å…¶çœŸæ­£çš„é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥çš„åŸºå‡†æµ‹è¯•æ¡†æ¶åŒ…å«äº”ä¸ªå…¬å¼€è¯ç‰©ç­›æŸ¥æ•°æ®é›†å’Œå…­ä¸ªæ ‡å‡†åŒ–çš„DRPæ¨¡å‹ï¼Œä»¥è¯„ä¼°è·¨æ•°æ®é›†çš„é¢„æµ‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ä¸€ç³»åˆ—è¯„ä¼°æŒ‡æ ‡ï¼Œå¯¹æ¨¡å‹çš„ç»å¯¹æ€§èƒ½å’Œç›¸å¯¹æ€§èƒ½è¿›è¡Œäº†é‡åŒ–ï¼Œä»¥æ›´å…¨é¢åœ°è¯„ä¼°æ¨¡å‹çš„è¿ç§»èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œåœ¨æœªè§æ•°æ®é›†ä¸Šæµ‹è¯•çš„æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œçªæ˜¾äº†ä¸¥æ ¼æ³›åŒ–è¯„ä¼°çš„é‡è¦æ€§ã€‚</li>
<li>è™½ç„¶æœ‰å¤šä¸ªæ¨¡å‹è¡¨ç°å‡ºç›¸å¯¹è¾ƒå¼ºçš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œä½†æ²¡æœ‰å•ä¸€æ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå§‹ç»ˆè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b1a1d63ea6804896a5aca8fed61d9d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0e203eae926765cb825f789e0aaa757.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a28394ce2017ea0c17d45b52e3fbcd2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5629d4f92f597bad73b9883ed4568115.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-054730760ce1bff49df7d0cb30eb34ff.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VEGGIE-Instructional-Editing-and-Reasoning-Video-Concepts-with-Grounded-Generation"><a href="#VEGGIE-Instructional-Editing-and-Reasoning-Video-Concepts-with-Grounded-Generation" class="headerlink" title="VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded   Generation"></a>VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded   Generation</h2><p><strong>Authors:Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan, Joyce Chai, Mohit Bansal</strong></p>
<p>Recent video diffusion models have enhanced video editing, but it remains challenging to handle instructional editing and diverse tasks (e.g., adding, removing, changing) within a unified framework. In this paper, we introduce VEGGIE, a Video Editor with Grounded Generation from Instructions, a simple end-to-end framework that unifies video concept editing, grounding, and reasoning based on diverse user instructions. Specifically, given a video and text query, VEGGIE first utilizes an MLLM to interpret user intentions in instructions and ground them to the video contexts, generating frame-specific grounded task queries for pixel-space responses. A diffusion model then renders these plans and generates edited videos that align with user intent. To support diverse tasks and complex instructions, we employ a curriculum learning strategy: first aligning the MLLM and video diffusion model with large-scale instructional image editing data, followed by end-to-end fine-tuning on high-quality multitask video data. Additionally, we introduce a novel data synthesis pipeline to generate paired instructional video editing data for model training. It transforms static image data into diverse, high-quality video editing samples by leveraging Image-to-Video models to inject dynamics. VEGGIE shows strong performance in instructional video editing with different editing skills, outperforming the best instructional baseline as a versatile model, while other models struggle with multi-tasking. VEGGIE also excels in video object grounding and reasoning segmentation, where other baselines fail. We further reveal how the multiple tasks help each other and highlight promising applications like zero-shot multimodal instructional and in-context video editing. </p>
<blockquote>
<p>æœ€è¿‘çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å·²ç»æé«˜äº†è§†é¢‘ç¼–è¾‘çš„èƒ½åŠ›ï¼Œä½†åœ¨ç»Ÿä¸€æ¡†æ¶å†…å¤„ç†æŒ‡ä»¤ç¼–è¾‘å’Œå¤šæ ·åŒ–ä»»åŠ¡ï¼ˆä¾‹å¦‚æ·»åŠ ã€åˆ é™¤ã€æ›´æ”¹ï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VEGGIEï¼Œä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„æ¥åœ°ç”Ÿæˆè§†é¢‘ç¼–è¾‘å™¨ï¼Œå®ƒæ˜¯ä¸€ä¸ªç®€å•ç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œç»Ÿä¸€äº†è§†é¢‘æ¦‚å¿µç¼–è¾‘ã€æ¥åœ°å’ŒåŸºäºå¤šæ ·ç”¨æˆ·æŒ‡ä»¤çš„æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªè§†é¢‘å’Œæ–‡æœ¬æŸ¥è¯¢ï¼ŒVEGGIEé¦–å…ˆåˆ©ç”¨MLLMæ¥è§£é‡Šç”¨æˆ·æ„å›¾çš„æŒ‡ä»¤å¹¶å°†å…¶æ¥åœ°åˆ°è§†é¢‘ä¸Šä¸‹æ–‡ï¼Œä¸ºåƒç´ ç©ºé—´å“åº”ç”Ÿæˆç‰¹å®šå¸§çš„æ¥åœ°ä»»åŠ¡æŸ¥è¯¢ã€‚ç„¶åï¼Œæ‰©æ•£æ¨¡å‹æ ¹æ®è¿™äº›è®¡åˆ’æ¸²æŸ“å¹¶ç”Ÿæˆç¬¦åˆç”¨æˆ·æ„å›¾çš„è§†é¢‘ã€‚ä¸ºäº†æ”¯æŒå¤šæ ·åŒ–çš„ä»»åŠ¡å’Œå¤æ‚çš„æŒ‡ä»¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼šé¦–å…ˆä½¿ç”¨å¤§è§„æ¨¡çš„æŒ‡ä»¤å›¾åƒç¼–è¾‘æ•°æ®å¯¹é½MLLMå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç„¶ååœ¨é«˜è´¨é‡çš„å¤šä»»åŠ¡è§†é¢‘æ•°æ®ä¸Šè¿›è¡Œç«¯åˆ°ç«¯çš„å¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°æ®åˆæˆç®¡é“ï¼Œä»¥ç”Ÿæˆç”¨äºæ¨¡å‹è®­ç»ƒçš„ä¸€å¯¹ä¸€æŒ‡ä»¤è§†é¢‘ç¼–è¾‘æ•°æ®ã€‚å®ƒé€šè¿‡åˆ©ç”¨å›¾åƒåˆ°è§†é¢‘çš„æ¨¡å‹æ³¨å…¥åŠ¨æ€æ€§ï¼Œå°†é™æ€å›¾åƒæ•°æ®è½¬æ¢ä¸ºå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è§†é¢‘ç¼–è¾‘æ ·æœ¬ã€‚VEGGIEåœ¨å…·æœ‰ä¸åŒç¼–è¾‘æŠ€èƒ½çš„æŒ‡ä»¤è§†é¢‘ç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½œä¸ºä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œå®ƒè¶…è¶Šäº†æœ€ä½³æŒ‡ä»¤åŸºçº¿ï¼Œè€Œå…¶ä»–æ¨¡å‹åœ¨å¤šä»»åŠ¡å¤„ç†æ–¹é¢åˆ™è¡¨ç°æŒ£æ‰ã€‚VEGGIEåœ¨è§†é¢‘å¯¹è±¡æ¥åœ°å’Œæ¨ç†åˆ†å‰²æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè€Œå…¶ä»–åŸºçº¿åˆ™æœªèƒ½è¾¾åˆ°è¿™ä¸€æ°´å¹³ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ­ç¤ºäº†å¤šä¸ªä»»åŠ¡æ˜¯å¦‚ä½•ç›¸äº’å¸®åŠ©çš„ï¼Œå¹¶å¼ºè°ƒäº†æœ‰å‰æ™¯çš„åº”ç”¨ï¼Œå¦‚é›¶æ ·æœ¬å¤šæ¨¡å¼æŒ‡ä»¤å’Œåœ¨ä¸Šä¸‹æ–‡ä¸­çš„è§†é¢‘ç¼–è¾‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14350v1">PDF</a> First three authors contributed equally. Project page:   <a target="_blank" rel="noopener" href="https://veggie-gen.github.io/">https://veggie-gen.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VEGGIEï¼Œä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„ç»Ÿåˆè§†é¢‘ç¼–è¾‘æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç†è§£ç”¨æˆ·æŒ‡ä»¤å¹¶å°†å…¶ä¸è§†é¢‘å†…å®¹å¯¹åº”èµ·æ¥ï¼Œå†é€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆç¬¦åˆç”¨æˆ·æ„å›¾çš„ç¼–è¾‘è§†é¢‘ã€‚ä¸ºæ”¯æŒå¤šæ ·åŒ–çš„ä»»åŠ¡å’Œå¤æ‚çš„æŒ‡ä»¤ï¼Œæœ¬æ–‡é‡‡ç”¨äº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°æ®åˆæˆç®¡é“æ¥ç”Ÿæˆè®­ç»ƒæ¨¡å‹æ‰€éœ€çš„è§†é¢‘ç¼–è¾‘æ•°æ®ã€‚VEGGIEåœ¨æŒ‡ä»¤è§†é¢‘ç¼–è¾‘ã€è§†é¢‘å¯¹è±¡å®šä½å’Œæ¨ç†åˆ†å‰²ç­‰å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VEGGIEæ˜¯ä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„è§†é¢‘ç¼–è¾‘æ¡†æ¶ï¼Œå¯ç»Ÿä¸€å¤„ç†è§†é¢‘æ¦‚å¿µç¼–è¾‘ã€å®šä½ä¸æ¨ç†ã€‚</li>
<li>åˆ©ç”¨MLLMè§£è¯»ç”¨æˆ·æŒ‡ä»¤å¹¶å°†å…¶ä¸è§†é¢‘å†…å®¹å¯¹åº”èµ·æ¥ï¼Œç”Ÿæˆç‰¹å®šå¸§çš„ä»»åŠ¡æŸ¥è¯¢ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ ¹æ®è¿™äº›è®¡åˆ’ç”Ÿæˆç¬¦åˆç”¨æˆ·æ„å›¾çš„ç¼–è¾‘è§†é¢‘ã€‚</li>
<li>é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥æ”¯æŒå¤šæ ·ä»»åŠ¡å’Œå¤æ‚æŒ‡ä»¤çš„å­¦ä¹ ã€‚</li>
<li>å¼•å…¥æ–°çš„æ•°æ®åˆæˆç®¡é“ï¼Œå°†é™æ€å›¾åƒæ•°æ®è½¬åŒ–ä¸ºé«˜è´¨é‡çš„è§†é¢‘ç¼–è¾‘æ ·æœ¬ã€‚</li>
<li>VEGGIEåœ¨æŒ‡ä»¤è§†é¢‘ç¼–è¾‘ã€è§†é¢‘å¯¹è±¡å®šä½å’Œæ¨ç†åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14350">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a63235a8dedb827a42d8917edfe5313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b1e9cf510817cc414098bcbf8e2b667.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b107c4552a56b5cb3d171cd65e6210a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e900a0068210883ca8d7217aaba69d7b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="JuDGE-Benchmarking-Judgment-Document-Generation-for-Chinese-Legal-System"><a href="#JuDGE-Benchmarking-Judgment-Document-Generation-for-Chinese-Legal-System" class="headerlink" title="JuDGE: Benchmarking Judgment Document Generation for Chinese Legal   System"></a>JuDGE: Benchmarking Judgment Document Generation for Chinese Legal   System</h2><p><strong>Authors:Weihang Su, Baoqing Yue, Qingyao Ai, Yiran Hu, Jiaqi Li, Changyue Wang, Kaiyuan Zhang, Yueyue Wu, Yiqun Liu</strong></p>
<p>This paper introduces JuDGE (Judgment Document Generation Evaluation), a novel benchmark for evaluating the performance of judgment document generation in the Chinese legal system. We define the task as generating a complete legal judgment document from the given factual description of the case. To facilitate this benchmark, we construct a comprehensive dataset consisting of factual descriptions from real legal cases, paired with their corresponding full judgment documents, which serve as the ground truth for evaluating the quality of generated documents. This dataset is further augmented by two external legal corpora that provide additional legal knowledge for the task: one comprising statutes and regulations, and the other consisting of a large collection of past judgment documents. In collaboration with legal professionals, we establish a comprehensive automated evaluation framework to assess the quality of generated judgment documents across various dimensions. We evaluate various baseline approaches, including few-shot in-context learning, fine-tuning, and a multi-source retrieval-augmented generation (RAG) approach, using both general and legal-domain LLMs. The experimental results demonstrate that, while RAG approaches can effectively improve performance in this task, there is still substantial room for further improvement. All the codes and datasets are available at: <a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE">https://github.com/oneal2000/JuDGE</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†JuDGEï¼ˆåˆ¤å†³æ–‡ä¹¦ç”Ÿæˆè¯„ä¼°ï¼‰è¿™ä¸€æ–°å‹çš„ä¸­æ–‡æ³•å¾‹ä½“ç³»ä¸­åˆ¤å†³æ–‡ä¹¦ç”Ÿæˆæ€§èƒ½è¯„ä¼°åŸºå‡†ã€‚æˆ‘ä»¬å°†ä»»åŠ¡å®šä¹‰ä¸ºæ ¹æ®ç»™å®šçš„æ¡ˆä»¶äº‹å®æè¿°ç”Ÿæˆå®Œæ•´çš„æ³•å¾‹åˆ¤å†³ä¹¦ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€åŸºå‡†çš„æ„å»ºï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬æ¥è‡ªçœŸå®æ³•å¾‹æ¡ˆä»¶çš„æ¡ˆæƒ…æè¿°åŠå…¶å¯¹åº”çš„å®Œæ•´åˆ¤å†³ä¹¦ï¼Œè¿™äº›åˆ¤å†³ä¹¦ä½œä¸ºè¯„ä¼°ç”Ÿæˆæ–‡æ¡£è´¨é‡çš„çœŸå®ä¾æ®ã€‚è¯¥æ•°æ®é›†é€šè¿‡ä¸¤ä¸ªå¤–éƒ¨æ³•å¾‹è¯­æ–™åº“è¿›è¡Œäº†æ‰©å……ï¼Œä¸ºä»»åŠ¡æä¾›äº†é¢å¤–çš„æ³•å¾‹çŸ¥è¯†ï¼šä¸€ä¸ªåŒ…å«æ³•è§„å’Œæ¡ä¾‹ï¼Œå¦ä¸€ä¸ªåˆ™åŒ…å«å¤§é‡ä»¥å¾€çš„åˆ¤å†³ä¹¦ã€‚æˆ‘ä»¬ä¸æ³•å¾‹ä¸“ä¸šäººå£«åˆä½œï¼Œå»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä»å¤šä¸ªç»´åº¦è¯„ä¼°ç”Ÿæˆçš„åˆ¤å†³ä¹¦çš„è´¨é‡ã€‚æˆ‘ä»¬è¯„ä¼°äº†å„ç§åŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬å°æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ã€å¾®è°ƒä»¥åŠå¤šæºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å‡ä½¿ç”¨é€šç”¨å’Œæ³•å¾‹é¢†åŸŸçš„LLMã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶RAGæ–¹æ³•å¯ä»¥æœ‰æ•ˆæé«˜æ­¤ä»»åŠ¡æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®é›†å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/oneal2000/JuDGEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14258v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†JuDGEï¼ˆåˆ¤å†³æ–‡ä¹¦ç”Ÿæˆè¯„ä¼°ï¼‰åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°åœ¨ä¸­å›½æ³•å¾‹ä½“ç³»ä¸‹åˆ¤å†³æ–‡ä¹¦ç”Ÿæˆçš„æ€§èƒ½ã€‚ä»»åŠ¡çš„å®šä¹‰æ˜¯æ ¹æ®ç»™å®šæ¡ˆä¾‹çš„äº‹å®æè¿°ç”Ÿæˆå®Œæ•´çš„æ³•å¾‹åˆ¤å†³ä¹¦ã€‚ä¸ºæ¨è¿›æ­¤åŸºå‡†æµ‹è¯•ï¼Œæ„å»ºäº†åŒ…å«çœŸå®æ¡ˆä¾‹äº‹å®æè¿°ä¸å…¶å¯¹åº”çš„å®Œæ•´åˆ¤å†³ä¹¦çš„ç»¼åˆæ•°æ®é›†ï¼Œä½œä¸ºè¯„ä¼°ç”Ÿæˆæ–‡ä¹¦è´¨é‡çš„ä¾æ®ã€‚æ­¤å¤–ï¼Œè¿˜åˆ©ç”¨ä¸¤ä¸ªå¤–éƒ¨æ³•å¾‹è¯­æ–™åº“æä¾›é¢å¤–çš„æ³•å¾‹çŸ¥è¯†ã€‚ä¸æ³•å¾‹ä¸“ä¸šäººå£«åˆä½œï¼Œå»ºç«‹äº†å…¨é¢çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä»å¤šä¸ªç»´åº¦è¯„ä¼°ç”Ÿæˆåˆ¤å†³ä¹¦çš„è´¨é‡ã€‚å¯¹åŒ…æ‹¬å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ã€å¾®è°ƒä»¥åŠå¤šæºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ç­‰åŸºçº¿æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨äº†ä¸€èˆ¬å’Œæ³•å¾‹é¢†åŸŸçš„LLMsã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAGæ–¹æ³•è™½ç„¶èƒ½æœ‰æ•ˆæå‡ä»»åŠ¡æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE%E3%80%82">https://github.com/oneal2000/JuDGEã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†JuDGEåŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºè¯„ä¼°ä¸­å›½æ³•å¾‹ç³»ç»Ÿä¸‹çš„åˆ¤å†³æ–‡ä¹¦ç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>å®šä¹‰äº†ä»æ¡ˆä¾‹äº‹å®æè¿°ç”Ÿæˆå®Œæ•´åˆ¤å†³ä¹¦çš„ä»»åŠ¡ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼ŒåŒ…å«çœŸå®æ¡ˆä¾‹çš„äº‹å®æè¿°å’Œå¯¹åº”çš„åˆ¤å†³ä¹¦ï¼Œä»¥åŠä¸¤ä¸ªå¤–éƒ¨æ³•å¾‹è¯­æ–™åº“ã€‚</li>
<li>ä¸æ³•å¾‹ä¸“ä¸šäººå£«åˆä½œï¼Œå»ºç«‹äº†å…¨é¢çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>è¯„ä¼°äº†å‡ ç§åŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ã€å¾®è°ƒä»¥åŠå¤šæºæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜RAGæ–¹æ³•èƒ½æœ‰æ•ˆæå‡ä»»åŠ¡æ€§èƒ½ï¼Œä½†ä»å­˜åœ¨æ”¹è¿›ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f1b4b233032a53f2f3024e0cb6d1357c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c12c7e7f91bd6666772101cdba937f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4a0d37c3ddd60aee0fb04a037e968a1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CTSAC-Curriculum-Based-Transformer-Soft-Actor-Critic-for-Goal-Oriented-Robot-Exploration"><a href="#CTSAC-Curriculum-Based-Transformer-Soft-Actor-Critic-for-Goal-Oriented-Robot-Exploration" class="headerlink" title="CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented   Robot Exploration"></a>CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented   Robot Exploration</h2><p><strong>Authors:Chunyu Yang, Shengben Bi, Yihui Xu, Xin Zhang</strong></p>
<p>With the increasing demand for efficient and flexible robotic exploration solutions, Reinforcement Learning (RL) is becoming a promising approach in the field of autonomous robotic exploration. However, current RL-based exploration algorithms often face limited environmental reasoning capabilities, slow convergence rates, and substantial challenges in Sim-To-Real (S2R) transfer. To address these issues, we propose a Curriculum Learning-based Transformer Reinforcement Learning Algorithm (CTSAC) aimed at improving both exploration efficiency and transfer performance. To enhance the robotâ€™s reasoning ability, a Transformer is integrated into the perception network of the Soft Actor-Critic (SAC) framework, leveraging historical information to improve the farsightedness of the strategy. A periodic review-based curriculum learning is proposed, which enhances training efficiency while mitigating catastrophic forgetting during curriculum transitions. Training is conducted on the ROS-Gazebo continuous robotic simulation platform, with LiDAR clustering optimization to further reduce the S2R gap. Experimental results demonstrate the CTSAC algorithm outperforms the state-of-the-art non-learning and learning-based algorithms in terms of success rate and success rate-weighted exploration time. Moreover, real-world experiments validate the strong S2R transfer capabilities of CTSAC. </p>
<blockquote>
<p>éšç€å¯¹é«˜æ•ˆã€çµæ´»æœºå™¨äººæ¢ç´¢è§£å†³æ–¹æ¡ˆçš„éœ€æ±‚ä¸æ–­å¢åŠ ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è‡ªä¸»æœºå™¨äººæ¢ç´¢é¢†åŸŸæˆä¸ºäº†ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼ŒåŸºäºå½“å‰çš„å¼ºåŒ–å­¦ä¹ æ¢ç´¢ç®—æ³•å¸¸å¸¸é¢ä¸´ç¯å¢ƒæ¨ç†èƒ½åŠ›æœ‰é™ã€æ”¶æ•›é€Ÿåº¦æ…¢ä»¥åŠåœ¨æ¨¡æ‹Ÿåˆ°çœŸå®ï¼ˆS2Rï¼‰è½¬ç§»ä¸­çš„å·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„Transformerå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆCTSACï¼‰ï¼Œæ—¨åœ¨æé«˜æ¢ç´¢æ•ˆç‡å’Œè½¬ç§»æ€§èƒ½ã€‚ä¸ºäº†å¢å¼ºæœºå™¨äººçš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†Transformeré›†æˆåˆ°Soft Actor-Criticï¼ˆSACï¼‰æ¡†æ¶çš„æ„ŸçŸ¥ç½‘ç»œä¸­ï¼Œåˆ©ç”¨å†å²ä¿¡æ¯æ¥æé«˜ç­–ç•¥çš„é•¿è¿œæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå®šæœŸè¯„å®¡çš„è¯¾ç¨‹å­¦ä¹ ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡ï¼ŒåŒæ—¶å‡è½»è¯¾ç¨‹è¿‡æ¸¡è¿‡ç¨‹ä¸­çš„ç¾éš¾æ€§é—å¿˜ã€‚è®­ç»ƒæ˜¯åœ¨ROS-Gazeboè¿ç»­æœºå™¨äººä»¿çœŸå¹³å°ä¸Šè¿›è¡Œçš„ï¼Œåˆ©ç”¨æ¿€å…‰é›·è¾¾èšç±»ä¼˜åŒ–æ¥è¿›ä¸€æ­¥ç¼©å°S2Rå·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCTSACç®—æ³•åœ¨æˆåŠŸç‡å’ŒåŠ æƒæ¢ç´¢æ—¶é—´æˆåŠŸç‡æ–¹é¢ä¼˜äºæœ€æ–°çš„éå­¦ä¹ å‹å’ŒåŸºäºå­¦ä¹ çš„ç®—æ³•ã€‚æ­¤å¤–ï¼ŒçœŸå®ä¸–ç•Œå®éªŒéªŒè¯äº†CTSACå¼ºå¤§çš„S2Rè½¬ç§»èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14254v1">PDF</a> 7pages,7 figures,Thesis received by 2025 ICRA</p>
<p><strong>Summary</strong>ï¼šéšç€å¯¹é«˜æ•ˆçµæ´»æœºå™¨äººæ¢ç´¢è§£å†³æ–¹æ¡ˆçš„éœ€æ±‚ä¸æ–­å¢åŠ ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è‡ªä¸»æœºå™¨äººæ¢ç´¢é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰åŸºäºRLçš„æ¢ç´¢ç®—æ³•é¢ä¸´ç€ç¯å¢ƒæ¨ç†èƒ½åŠ›æœ‰é™ã€æ”¶æ•›é€Ÿåº¦æ…¢ä»¥åŠæ¨¡æ‹Ÿåˆ°ç°å®ï¼ˆS2Rï¼‰è½¬ç§»æŒ‘æˆ˜ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¯¾ç¨‹å­¦ä¹ çš„Transformerå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆCTSACï¼‰ï¼Œæ—¨åœ¨æé«˜æ¢ç´¢æ•ˆç‡å’Œè½¬ç§»æ€§èƒ½ã€‚è¯¥ç®—æ³•å°†Transformeré›†æˆåˆ°Soft Actor-Criticï¼ˆSACï¼‰æ¡†æ¶çš„æ„ŸçŸ¥ç½‘ç»œä¸­ï¼Œåˆ©ç”¨å†å²ä¿¡æ¯æé«˜ç­–ç•¥çš„é•¿è¿œæ€§ã€‚åŒæ—¶ï¼Œæå‡ºäº†åŸºäºå‘¨æœŸæ€§è¯„å®¡çš„è¯¾ç¨‹å­¦ä¹ ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å¹¶å‡è½»è¯¾ç¨‹è¿‡æ¸¡æ—¶çš„ç¾éš¾æ€§é—å¿˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCTSACç®—æ³•åœ¨æˆåŠŸç‡å’ŒåŠ æƒæ¢ç´¢æ—¶é—´æ–¹é¢çš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„éå­¦ä¹ ç®—æ³•å’Œå­¦ä¹ ç®—æ³•ã€‚æ­¤å¤–ï¼ŒçœŸå®ä¸–ç•Œå®éªŒéªŒè¯äº†CTSACå¼ºå¤§çš„S2Rè½¬ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨è‡ªä¸»æœºå™¨äººæ¢ç´¢ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†é¢ä¸´ç¯å¢ƒæ¨ç†èƒ½åŠ›æœ‰é™ç­‰æŒ‘æˆ˜ã€‚</li>
<li>CTSACç®—æ³•é€šè¿‡é›†æˆTransformeråˆ°SACæ¡†æ¶çš„æ„ŸçŸ¥ç½‘ç»œï¼Œæé«˜äº†æœºå™¨äººç­–ç•¥çš„é•¿è¿œæ€§ã€‚</li>
<li>åŸºäºå‘¨æœŸæ€§è¯„å®¡çš„è¯¾ç¨‹å­¦ä¹ å¢å¼ºäº†è®­ç»ƒæ•ˆç‡å¹¶å‡è½»äº†ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>CTSACç®—æ³•åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œé€šè¿‡ROS-Gazeboè¿ç»­æœºå™¨äººä»¿çœŸå¹³å°è¿›è¡Œäº†è®­ç»ƒã€‚</li>
<li>LiDARèšç±»ä¼˜åŒ–è¿›ä¸€æ­¥å‡å°‘äº†S2Rå·®è·ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCTSACç®—æ³•åœ¨æˆåŠŸç‡å’ŒåŠ æƒæ¢ç´¢æ—¶é—´æ–¹é¢ä¼˜äºå…¶ä»–ç®—æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-beb371e430fbdb08e9636214cbe5fb08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9669a4c8d4e3a9c80af20ce3104f6db0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d29954792d15164831e236fd9b553be7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be7b224abdcd075735db0b5384816412.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e147d9a5755dd2a078e05da2d558059.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be39c37b60100a22d460f780b880e75e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd4879a3ad8e2341ea3261c857f077d6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models"><a href="#Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models" class="headerlink" title="Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models"></a>Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models</h2><p><strong>Authors:Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang</strong></p>
<p>Vision-language models (VLMs) have advanced reasoning in natural scenes, but their role in medical imaging remains underexplored. Medical reasoning tasks demand robust image analysis and well-justified answers, posing challenges due to the complexity of medical images. Transparency and trustworthiness are essential for clinical adoption and regulatory compliance. We introduce Med-R1, a framework exploring reinforcement learning (RL) to enhance VLMsâ€™ generalizability and trustworthiness in medical reasoning. Leveraging the DeepSeek strategy, we employ Group Relative Policy Optimization (GRPO) to guide reasoning paths via reward signals. Unlike supervised fine-tuning (SFT), which often overfits and lacks generalization, RL fosters robust and diverse reasoning. Med-R1 is evaluated across eight medical imaging modalities: CT, MRI, Ultrasound, Dermoscopy, Fundus Photography, Optical Coherence Tomography (OCT), Microscopy, and X-ray Imaging. Compared to its base model, Qwen2-VL-2B, Med-R1 achieves a 29.94% accuracy improvement and outperforms Qwen2-VL-72B, which has 36 times more parameters. Testing across five question types-modality recognition, anatomy identification, disease diagnosis, lesion grading, and biological attribute analysis Med-R1 demonstrates superior generalization, exceeding Qwen2-VL-2B by 32.06% and surpassing Qwen2-VL-72B in question-type generalization. These findings show that RL improves medical reasoning and enables parameter-efficient models to outperform significantly larger ones. With interpretable reasoning outputs, Med-R1 represents a promising step toward generalizable, trustworthy, and clinically viable medical VLMs. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªç„¶åœºæ™¯æ¨ç†æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨åŒ»å­¦æˆåƒä¸­çš„ä½œç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åŒ»å­¦æ¨ç†ä»»åŠ¡éœ€è¦å¯é çš„å›¾åƒåˆ†æå’Œåˆç†çš„ç­”æ¡ˆï¼Œç”±äºåŒ»å­¦å›¾åƒçš„å¤æ‚æ€§ï¼Œè¿™æ„æˆäº†æŒ‘æˆ˜ã€‚é€æ˜åº¦å’Œå¯ä¿¡åº¦å¯¹äºä¸´åºŠé‡‡ç”¨å’Œæ³•è§„åˆè§„è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†Med-R1æ¡†æ¶ï¼Œæ¢ç´¢å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æé«˜VLMsåœ¨åŒ»å­¦æ¨ç†ä¸­çš„é€šç”¨æ€§å’Œå¯ä¿¡åº¦ã€‚åˆ©ç”¨DeepSeekç­–ç•¥ï¼Œæˆ‘ä»¬é‡‡ç”¨é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰é€šè¿‡å¥–åŠ±ä¿¡å·æ¥æŒ‡å¯¼æ¨ç†è·¯å¾„ã€‚ä¸ç»å¸¸è¿‡åº¦æ‹Ÿåˆä¸”ç¼ºä¹æ³›åŒ–èƒ½åŠ›çš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸åŒï¼ŒRLä¿ƒè¿›ç¨³å¥å’Œå¤šæ ·åŒ–çš„æ¨ç†ã€‚Med-R1åœ¨å…«ç§åŒ»å­¦æˆåƒæ¨¡å¼ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šCTã€MRIã€è¶…å£°ã€çš®è‚¤é•œæ£€æŸ¥ã€çœ¼åº•æ‘„å½±ã€å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æï¼ˆOCTï¼‰ã€æ˜¾å¾®é•œå’ŒXå°„çº¿æˆåƒã€‚ä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼ŒMed-R1åœ¨å‡†ç¡®æ€§ä¸Šæé«˜äº†29.94%ï¼Œå¹¶ä¼˜äºå‚æ•°æ›´å¤šçš„Qwen2-VL-72Bã€‚åœ¨äº”ç§é—®é¢˜ç±»å‹ï¼ˆæ¨¡æ€è¯†åˆ«ã€è§£å‰–ç»“æ„è¯†åˆ«ã€ç–¾ç—…è¯Šæ–­ã€ç—…å˜åˆ†çº§å’Œç”Ÿç‰©å±æ€§åˆ†æï¼‰çš„æµ‹è¯•ä¸Šï¼ŒMed-R1è¡¨ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¿‡Qwen2-VL-2B 32.06%ï¼Œå¹¶åœ¨é—®é¢˜ç±»å‹æ³›åŒ–æ–¹é¢è¶…è¶ŠQwen2-VL-72Bã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæ”¹å–„åŒ»å­¦æ¨ç†ï¼Œå¹¶ä½¿å¾—å‚æ•°æ•ˆç‡æ¨¡å‹èƒ½å¤Ÿæ˜¾è‘—ä¼˜äºæ›´å¤§çš„æ¨¡å‹ã€‚Med-R1çš„å¯è§£é‡Šæ¨ç†è¾“å‡ºæ˜¯æœç€é€šç”¨ã€å¯ä¿¡å’Œä¸´åºŠä¸Šå¯è¡Œçš„åŒ»å­¦VLMsçš„æœ‰å¸Œæœ›çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13939v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†Med-R1æ¡†æ¶ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—æ¨ç†ä¸­çš„é€šç”¨æ€§å’Œå¯ä¿¡åº¦ã€‚é€šè¿‡DeepSeekç­–ç•¥åŠGroup Relative Policy Optimizationï¼ˆGRPOï¼‰å¼•å¯¼æ¨ç†è·¯å¾„ï¼Œç›¸è¾ƒäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼ŒRLä¿ƒè¿›ç¨³å¥å’Œå¤šæ ·åŒ–çš„æ¨ç†ã€‚Med-R1åœ¨å…«ç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå®ç°ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹29.94%çš„å‡†ç¡®åº¦æå‡ï¼Œä¸”åœ¨é—®é¢˜ç±»å‹æ³›åŒ–æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒRLæœ‰åŠ©äºæ”¹è¿›åŒ»ç–—æ¨ç†ï¼Œä½¿å‚æ•°æ•ˆç‡æ¨¡å‹åœ¨æ€§èƒ½æ–¹é¢è¶…è¶Šæ›´å¤§è§„æ¨¡æ¨¡å‹ï¼Œä¸ºé€šç”¨ã€å¯ä¿¡ä¸”ä¸´åºŠå¯è¡Œçš„åŒ»ç–—VLMsè¿ˆå‡ºé‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-R1æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—æ¨ç†ä¸­çš„é€šç”¨æ€§å’Œå¯ä¿¡åº¦ã€‚</li>
<li>DeepSeekç­–ç•¥å’ŒGroup Relative Policy Optimizationï¼ˆGRPOï¼‰ç”¨äºæŒ‡å¯¼æ¨ç†è·¯å¾„ã€‚</li>
<li>ç›¸è¾ƒäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¿ƒè¿›æ›´ç¨³å¥å’Œå¤šæ ·åŒ–çš„æ¨ç†ã€‚</li>
<li>Med-R1åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>Med-R1åœ¨é—®é¢˜ç±»å‹æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä¼˜äºåŸºå‡†æ¨¡å‹å’Œæ›´å¤§è§„æ¨¡æ¨¡å‹ã€‚</li>
<li>RLæœ‰åŠ©äºæ”¹è¿›åŒ»ç–—æ¨ç†ï¼Œæå‡æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d41cf1f555ee818bedb1d3e9600d32f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-630d53122dda7aca60f3a3ccd8c64a19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-128122acf4102aa38fc1f1dd1300ff49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-815f2579c33a67045719bb040b3c9dab.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MMR-A-Large-scale-Benchmark-Dataset-for-Multi-target-and-Multi-granularity-Reasoning-Segmentation"><a href="#MMR-A-Large-scale-Benchmark-Dataset-for-Multi-target-and-Multi-granularity-Reasoning-Segmentation" class="headerlink" title="MMR: A Large-scale Benchmark Dataset for Multi-target and   Multi-granularity Reasoning Segmentation"></a>MMR: A Large-scale Benchmark Dataset for Multi-target and   Multi-granularity Reasoning Segmentation</h2><p><strong>Authors:Donggon Jang, Yucheol Cho, Suin Lee, Taehyeon Kim, Dae-Shik Kim</strong></p>
<p>The fusion of Large Language Models with vision models is pioneering new possibilities in user-interactive vision-language tasks. A notable application is reasoning segmentation, where models generate pixel-level segmentation masks by comprehending implicit meanings in human instructions. However, seamless human-AI interaction demands more than just object-level recognition; it requires understanding both objects and the functions of their detailed parts, particularly in multi-target scenarios. For example, when instructing a robot to \textit{turn on the TVâ€}, there could be various ways to accomplish this command. Recognizing multiple objects capable of turning on the TV, such as the TV itself or a remote control (multi-target), provides more flexible options and aids in finding the optimized scenario. Furthermore, understanding specific parts of these objects, like the TVâ€™s button or the remoteâ€™s button (part-level), is important for completing the action. Unfortunately, current reasoning segmentation datasets predominantly focus on a single target object-level reasoning, which limits the detailed recognition of an objectâ€™s parts in multi-target contexts. To address this gap, we construct a large-scale dataset called Multi-target and Multi-granularity Reasoning (MMR). MMR comprises 194K complex and implicit instructions that consider multi-target, object-level, and part-level aspects, based on pre-existing image-mask sets. This dataset supports diverse and context-aware interactions by hierarchically providing object and part information. Moreover, we propose a straightforward yet effective framework for multi-target, object-level, and part-level reasoning segmentation. Experimental results on MMR show that the proposed method can reason effectively in multi-target and multi-granularity scenarios, while the existing reasoning segmentation model still has room for improvement. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å‹çš„èåˆåœ¨äº¤äº’å¼è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­å¼€åˆ›äº†æ–°çš„å¯èƒ½æ€§ã€‚ä¸€ä¸ªå…¸å‹çš„åº”ç”¨æ˜¯æ¨ç†åˆ†å‰²ï¼Œæ¨¡å‹é€šè¿‡ç†è§£äººç±»æŒ‡ä»¤ä¸­çš„éšå«æ„ä¹‰æ¥ç”Ÿæˆåƒç´ çº§çš„åˆ†å‰²æ©è†œã€‚ç„¶è€Œï¼Œæ— ç¼çš„äººæœºäº¤äº’ä¸ä»…ä»…æ˜¯åŸºäºç›®æ ‡çº§åˆ«çš„è¯†åˆ«ï¼›å®ƒè¦æ±‚äº†è§£å¯¹è±¡å’Œå®ƒä»¬è¯¦ç»†éƒ¨åˆ†çš„åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šç›®æ ‡åœºæ™¯ä¸­ã€‚ä¾‹å¦‚ï¼Œå½“æŒ‡ç¤ºæœºå™¨äººâ€œæ‰“å¼€ç”µè§†â€æ—¶ï¼Œå¯èƒ½æœ‰å¤šç§æ–¹å¼æ¥å®Œæˆè¿™ä¸ªå‘½ä»¤ã€‚è¯†åˆ«èƒ½å¤Ÿæ‰“å¼€ç”µè§†çš„å¤šç›®æ ‡å¯¹è±¡ï¼Œå¦‚ç”µè§†æœ¬èº«æˆ–é¥æ§å™¨ï¼ˆå¤šç›®æ ‡ï¼‰ï¼Œæä¾›äº†æ›´çµæ´»çš„é€‰æ‹©å¹¶æœ‰åŠ©äºæ‰¾åˆ°æœ€ä½³åœºæ™¯ã€‚æ­¤å¤–ï¼Œäº†è§£è¿™äº›å¯¹è±¡çš„ç‰¹å®šéƒ¨åˆ†ï¼Œå¦‚ç”µè§†çš„æŒ‰é’®æˆ–é¥æ§å™¨çš„æŒ‰é’®ï¼ˆéƒ¨åˆ†çº§åˆ«ï¼‰ï¼Œå¯¹äºå®ŒæˆåŠ¨ä½œä¹Ÿå¾ˆé‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ¨ç†åˆ†å‰²æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨å•ä¸€ç›®æ ‡å¯¹è±¡çº§åˆ«çš„æ¨ç†ä¸Šï¼Œè¿™é™åˆ¶äº†å¤šç›®æ ‡ä¸Šä¸‹æ–‡ä¸­å¯¹è±¡éƒ¨åˆ†çš„è¯¦ç»†è¯†åˆ«ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œç§°ä¸ºå¤šç›®æ ‡å¤šç²’åº¦æ¨ç†ï¼ˆMMRï¼‰ã€‚MMRç”±åŸºäºç°æœ‰å›¾åƒæ©æ¨¡é›†çš„19.4ä¸‡æ¡å¤æ‚ä¸”éšå«çš„æŒ‡ä»¤ç»„æˆï¼Œè¿™äº›æŒ‡ä»¤è€ƒè™‘äº†å¤šç›®æ ‡ã€å¯¹è±¡çº§åˆ«å’Œéƒ¨åˆ†çº§åˆ«çš„æ–¹é¢ã€‚è¯¥æ•°æ®é›†é€šè¿‡åˆ†å±‚æä¾›å¯¹è±¡å’Œéƒ¨åˆ†ä¿¡æ¯ï¼Œæ”¯æŒå¤šæ ·åŒ–å’Œæƒ…å¢ƒæ„ŸçŸ¥çš„äº¤äº’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å¤šç›®æ ‡ã€å¯¹è±¡çº§åˆ«å’Œéƒ¨åˆ†çº§åˆ«çš„æ¨ç†åˆ†å‰²æ¡†æ¶ã€‚åœ¨MMRä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šç›®æ ‡å’Œå¤šç²’åº¦åœºæ™¯ä¸­èƒ½å¤Ÿè¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ï¼Œè€Œç°æœ‰çš„æ¨ç†åˆ†å‰²æ¨¡å‹ä»æœ‰æ”¹è¿›çš„ç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13881v1">PDF</a> ICLR 2025, Code and dataset are available at   \url{<a target="_blank" rel="noopener" href="https://github.com/jdg900/MMR%7D">https://github.com/jdg900/MMR}</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å‹çš„èåˆåœ¨äº¤äº’å¼è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„åˆ›æ–°åº”ç”¨ã€‚ç‰¹åˆ«æ˜¯åœ¨æ¨ç†åˆ†å‰²é¢†åŸŸï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆåƒç´ çº§çš„åˆ†å‰²æ©è†œï¼Œé€šè¿‡ç†è§£äººç±»æŒ‡ä»¤ä¸­çš„éšå«æ„ä¹‰ã€‚ç„¶è€Œï¼Œæ— ç¼çš„äººæœºäº¤äº’ä¸ä»…éœ€è¦è¯†åˆ«å¯¹è±¡çº§åˆ«ï¼Œè¿˜éœ€è¦ç†è§£å¯¹è±¡åŠå…¶è¯¦ç»†éƒ¨åˆ†çš„åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šç›®æ ‡åœºæ™¯ä¸­ã€‚æ–‡ç« æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†Multi-target and Multi-granularity Reasoning (MMR)ï¼Œä»¥æ”¯æŒå¤šæ ·åŒ–å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„äº¤äº’ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªé’ˆå¯¹å¤šç›®æ ‡ã€å¯¹è±¡çº§åˆ«å’Œéƒ¨åˆ†çº§åˆ«çš„æ¨ç†åˆ†å‰²çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤šç›®æ ‡å’Œå¤šç²’åº¦åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å‹çš„èåˆä¸ºäº¤äº’å¼è§†è§‰è¯­è¨€ä»»åŠ¡å¸¦æ¥åˆ›æ–°å¯èƒ½æ€§ã€‚</li>
<li>æ¨ç†åˆ†å‰²æ˜¯å…¶ä¸­ä¸€é¡¹é‡è¦åº”ç”¨ï¼Œæ¨¡å‹é€šè¿‡ç†è§£äººç±»æŒ‡ä»¤ä¸­çš„éšå«æ„ä¹‰ç”Ÿæˆåƒç´ çº§åˆ†å‰²æ©è†œã€‚</li>
<li>äººæœºäº¤äº’éœ€è¦ç†è§£å¯¹è±¡åŠå…¶è¯¦ç»†éƒ¨åˆ†çš„åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šç›®æ ‡åœºæ™¯ä¸­ã€‚</li>
<li>å½“å‰æ¨ç†åˆ†å‰²æ•°æ®é›†ä¸»è¦å…³æ³¨å•ä¸€ç›®æ ‡å¯¹è±¡çº§åˆ«çš„æ¨ç†ï¼Œéœ€è¦æ›´å¤šå…³æ³¨å¤šç›®æ ‡åœºæ™¯ä¸‹çš„è¯¦ç»†å¯¹è±¡éƒ¨åˆ†è¯†åˆ«ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†MMRï¼Œæ”¯æŒå¤šæ ·åŒ–å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„äº¤äº’ï¼Œæ¶µç›–å¤šç›®æ ‡ã€å¯¹è±¡çº§åˆ«å’Œéƒ¨åˆ†çº§åˆ«çš„ä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šç›®æ ‡ã€å¯¹è±¡çº§åˆ«å’Œéƒ¨åˆ†çº§åˆ«çš„æ¨ç†åˆ†å‰²çš„æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-79e70b34c2282e26cf19dad0261b56ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a565568c870bfda13f311e5dc1d8bb50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0641c4afe699f04a85e0134b748460e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9905ecab08869593fe33df78713f5275.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40affda86edd633ab99f02c512de4ada.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Bridging-Social-Psychology-and-LLM-Reasoning-Conflict-Aware-Meta-Review-Generation-via-Cognitive-Alignment"><a href="#Bridging-Social-Psychology-and-LLM-Reasoning-Conflict-Aware-Meta-Review-Generation-via-Cognitive-Alignment" class="headerlink" title="Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review   Generation via Cognitive Alignment"></a>Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review   Generation via Cognitive Alignment</h2><p><strong>Authors:Wei Chen, Han Ding, Meng Yuan, Zhao Zhang, Deqing Wang, Fuzhen Zhuang</strong></p>
<p>The rapid growth of scholarly submissions has overwhelmed traditional peer review systems, driving the need for intelligent automation to preserve scientific rigor. While large language models (LLMs) show promise in automating manuscript critiques, their ability to synthesize high-stakes meta-reviews, which require conflict-aware reasoning and consensus derivation, remains underdeveloped. Existing methods fail to effectively handle conflicting viewpoints within differing opinions, and often introduce additional cognitive biases, such as anchoring effects and conformity bias.To overcome these limitations, we propose the Cognitive Alignment Framework (CAF), a dual-process architecture that transforms LLMs into adaptive scientific arbitrators. By operationalizing Kahnemanâ€™s dual-process theory, CAF introduces a three-step cognitive pipeline: review initialization, incremental integration, and cognitive alignment.Empirical validation shows that CAF outperforms existing LLM-based methods, with sentiment consistency gains reaching up to 19.47% and content consistency improving by as much as 12.95%. </p>
<blockquote>
<p>å­¦æœ¯ç¨¿ä»¶çš„è¿…é€Ÿå¢é•¿å·²ç»ä½¿ä¼ ç»Ÿçš„åŒè¡Œè¯„å®¡ç³»ç»Ÿä¸å ªé‡è´Ÿï¼Œè¿™ä¿ƒä½¿éœ€è¦æ™ºèƒ½è‡ªåŠ¨åŒ–æ¥ä¿æŒç§‘å­¦çš„ä¸¥è°¨æ€§ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–æ‰‹ç¨¿è¯„å®¡æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨ç»¼åˆé«˜é£é™©å…ƒè¯„å®¡æ–¹é¢çš„èƒ½åŠ›ï¼Œè¿™éœ€è¦æ„è¯†åˆ°å†²çªæ¨ç†å’Œå…±è¯†æ¨å¯¼ï¼Œä»ç„¶å¤„äºæœªå¼€å‘çŠ¶æ€ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†ä¸åŒè§‚ç‚¹ä¸­çš„å†²çªè§‚ç‚¹ï¼Œå¹¶ç»å¸¸å¼•å…¥é¢å¤–çš„è®¤çŸ¥åè§ï¼Œä¾‹å¦‚é”šå®šæ•ˆåº”å’Œæœä»åè§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è®¤çŸ¥å¯¹é½æ¡†æ¶ï¼ˆCAFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŒè¿‡ç¨‹æ¶æ„ï¼Œå¯å°†LLMè½¬æ¢ä¸ºè‡ªé€‚åº”çš„ç§‘å­¦ä»²è£è€…ã€‚é€šè¿‡å®æ–½Kahnemançš„åŒè¿‡ç¨‹ç†è®ºï¼ŒCAFå¼•å…¥äº†ä¸€ä¸ªä¸‰æ­¥éª¤çš„è®¤çŸ¥ç®¡é“ï¼šè¯„å®¡åˆå§‹åŒ–ã€å¢é‡é›†æˆå’Œè®¤çŸ¥å¯¹é½ã€‚å®è¯ç ”ç©¶è¯æ˜ï¼ŒCAFåœ¨æƒ…æ„Ÿä¸€è‡´æ€§æ–¹é¢çš„æå‡è¾¾åˆ°äº†é«˜è¾¾19.47%ï¼Œå†…å®¹ä¸€è‡´æ€§ä¹Ÿæé«˜äº†æœ€å¤šè¾¾12.95%ï¼Œè¡¨ç°ä¼˜äºç°æœ‰çš„LLMæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13879v1">PDF</a> 23 pages</p>
<p><strong>Summary</strong></p>
<p>æ–‡ç« æ¢è®¨å­¦æœ¯æäº¤çš„é«˜é€Ÿå¢é•¿å¯¹ä¼ ç»Ÿçš„åŒè¡Œè¯„å®¡ç³»ç»Ÿå¸¦æ¥çš„å‹åŠ›ï¼Œå¹¶æŒ‡å‡ºæ™ºèƒ½è‡ªåŠ¨åŒ–æ˜¯ç¼“è§£è¿™ä¸€å‹åŠ›çš„å…³é”®ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–æ‰‹ç¨¿è¯„å®¡æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨åˆæˆéœ€è¦å†²çªæ„ŸçŸ¥æ¨ç†å’Œå…±è¯†æ¨å¯¼çš„é«˜é£é™©å…ƒè¯„å®¡æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆå¤„ç†ä¸åŒè§‚ç‚¹ä¸­çš„å†²çªï¼Œå¹¶å¯èƒ½å¼•å…¥é¢å¤–çš„è®¤çŸ¥åè§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†è®¤çŸ¥å¯¹é½æ¡†æ¶ï¼ˆCAFï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†LLMè½¬åŒ–ä¸ºè‡ªé€‚åº”ç§‘å­¦ä»²è£è€…çš„åŒè¿‡ç¨‹æ¶æ„ã€‚CAFæ ¹æ®å¡å†…æ›¼çš„åŒé‡è¿‡ç¨‹ç†è®ºï¼Œè®¾è®¡äº†ä¸€ä¸ªåŒ…æ‹¬å®¡æŸ¥åˆå§‹åŒ–ã€å¢é‡é›†æˆå’Œè®¤çŸ¥å¯¹é½çš„ä¸‰æ­¥è®¤çŸ¥ç®¡é“ã€‚å®è¯éªŒè¯æ˜¾ç¤ºï¼ŒCAFåœ¨æƒ…æ„Ÿä¸€è‡´æ€§å’Œå†…å®¹ä¸€è‡´æ€§æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„LLMæ–¹æ³•ï¼Œå…¶å¢ç›Šåˆ†åˆ«é«˜è¾¾19.47%å’Œ12.95%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦æœ¯æäº¤é‡çš„è¿…é€Ÿå¢é•¿ä¿ƒä½¿å¯¹æ™ºèƒ½è‡ªåŠ¨åŒ–ä»¥ç»´æŒç§‘å­¦ä¸¥è°¨æ€§çš„éœ€æ±‚ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–æ‰‹ç¨¿è¯„å®¡ä¸­æœ‰æ½œåŠ›ï¼Œä½†åœ¨åˆæˆé«˜é£é™©çš„å…ƒè¯„å®¡æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†ä¸åŒè§‚ç‚¹ä¸­çš„å†²çªï¼Œå¹¶å¯èƒ½å¼•å…¥é¢å¤–çš„è®¤çŸ¥åè§ã€‚</li>
<li>æå‡ºäº†è®¤çŸ¥å¯¹é½æ¡†æ¶ï¼ˆCAFï¼‰æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒè¿‡ç¨‹æ¶æ„ã€‚</li>
<li>CAFåŸºäºå¡å†…æ›¼çš„åŒé‡è¿‡ç¨‹ç†è®ºï¼ŒåŒ…æ‹¬å®¡æŸ¥åˆå§‹åŒ–ã€å¢é‡é›†æˆå’Œè®¤çŸ¥å¯¹é½ä¸‰ä¸ªæ­¥éª¤ã€‚</li>
<li>CAFåœ¨æƒ…æ„Ÿä¸€è‡´æ€§å’Œå†…å®¹ä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰LLMæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13879">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5d6a422a9a5048b5eea40aafa607f945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-098e6604b2263ffdbd23662b57f30ce1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db11cffc531af2d0f77b0c02e9e4eea1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-025270a4faf6f0d14c1c96eb644832e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fffe07298001cba7f3659c48706614b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93b6893efd056041246d700dc4967965.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Counterfactual-experience-augmented-off-policy-reinforcement-learning"><a href="#Counterfactual-experience-augmented-off-policy-reinforcement-learning" class="headerlink" title="Counterfactual experience augmented off-policy reinforcement learning"></a>Counterfactual experience augmented off-policy reinforcement learning</h2><p><strong>Authors:Sunbowen Lee, Yicheng Gong, Chao Deng</strong></p>
<p>Reinforcement learning control algorithms face significant challenges due to out-of-distribution and inefficient exploration problems. While model-based reinforcement learning enhances the agentâ€™s reasoning and planning capabilities by constructing virtual environments, training such virtual environments can be very complex. In order to build an efficient inference model and enhance the representativeness of learning data, we propose the Counterfactual Experience Augmentation (CEA) algorithm. CEA leverages variational autoencoders to model the dynamic patterns of state transitions and introduces randomness to model non-stationarity. This approach focuses on expanding the learning data in the experience pool through counterfactual inference and performs exceptionally well in environments that follow the bisimulation assumption. Environments with bisimulation properties are usually represented by discrete observation and action spaces, we propose a sampling method based on maximum kernel density estimation entropy to extend CEA to various environments. By providing reward signals for counterfactual state transitions based on real information, CEA constructs a complete counterfactual experience to alleviate the out-of-distribution problem of the learning data, and outperforms general SOTA algorithms in environments with difference properties. Finally, we discuss the similarities, differences and properties of generated counterfactual experiences and real experiences. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Aegis1863/CEA">https://github.com/Aegis1863/CEA</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ æ§åˆ¶ç®—æ³•é¢ä¸´ç€ç”±äºåˆ†å¸ƒå¤–å’Œæ— æ•ˆæ¢ç´¢é—®é¢˜è€Œå¸¦æ¥çš„é‡å¤§æŒ‘æˆ˜ã€‚åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ é€šè¿‡æ„å»ºè™šæ‹Ÿç¯å¢ƒæ¥æå‡æ™ºèƒ½ä½“çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œä½†è®­ç»ƒè¿™æ ·çš„è™šæ‹Ÿç¯å¢ƒå¯èƒ½éå¸¸å¤æ‚ã€‚ä¸ºäº†æ„å»ºé«˜æ•ˆçš„æ¨ç†æ¨¡å‹å¹¶å¢å¼ºå­¦ä¹ æ•°æ®çš„ä»£è¡¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Counterfactual Experience Augmentationï¼ˆCEAï¼‰ç®—æ³•ã€‚CEAåˆ©ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨æ¥å»ºæ¨¡çŠ¶æ€è½¬æ¢çš„åŠ¨æ€æ¨¡å¼ï¼Œå¹¶å¼•å…¥éšæœºæ€§æ¥å»ºæ¨¡éå¹³ç¨³æ€§ã€‚è¯¥æ–¹æ³•ä¾§é‡äºé€šè¿‡åäº‹å®æ¨ç†æ‰©å±•ç»éªŒæ± ä¸­çš„å­¦ä¹ æ•°æ®ï¼Œåœ¨éµå¾ªåŒæ¨¡æ‹Ÿå‡è®¾çš„ç¯å¢ƒä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚å…·æœ‰åŒæ¨¡æ‹Ÿå±æ€§çš„ç¯å¢ƒé€šå¸¸è¡¨ç°ä¸ºç¦»æ•£è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæœ€å¤§æ ¸å¯†åº¦ä¼°è®¡ç†µçš„é‡‡æ ·æ–¹æ³•ï¼Œå°†CEAæ‰©å±•åˆ°å„ç§ç¯å¢ƒã€‚CEAåŸºäºçœŸå®ä¿¡æ¯ä¸ºåäº‹å®çŠ¶æ€è½¬æ¢æä¾›å¥–åŠ±ä¿¡å·ï¼Œæ„å»ºå®Œæ•´çš„åäº‹å®ç»éªŒï¼Œä»¥ç¼“è§£å­¦ä¹ æ•°æ®çš„åˆ†å¸ƒå¤–é—®é¢˜ï¼Œå¹¶åœ¨å…·æœ‰ä¸åŒå±æ€§çš„ç¯å¢ƒä¸­ä¼˜äºä¸€èˆ¬çš„æœ€å…ˆè¿›ç®—æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†ç”Ÿæˆçš„åäº‹å®ç»éªŒä¸çœŸå®ç»éªŒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€å·®å¼‚å’Œå±æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Aegis1863/CEA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Aegis1863/CEAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13842v1">PDF</a> Accepted by Neurocomputing,   <a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.neucom.2025.130017">https://doi.org/10.1016/j.neucom.2025.130017</a></p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ æ§åˆ¶ç®—æ³•é¢ä¸´åˆ†å¸ƒå¤–å’Œæ— æ•ˆæ¢ç´¢çš„é—®é¢˜ã€‚æ¨¡å‹åŒ–å¼ºåŒ–å­¦ä¹ é€šè¿‡æ„å»ºè™šæ‹Ÿç¯å¢ƒæå‡æ™ºèƒ½ä½“çš„æ¨ç†å’Œè§„åˆ’èƒ½åŠ›ï¼Œä½†è®­ç»ƒè¿™äº›è™šæ‹Ÿç¯å¢ƒå¯èƒ½éå¸¸å¤æ‚ã€‚ä¸ºäº†å»ºç«‹é«˜æ•ˆçš„æ¨ç†æ¨¡å‹å’Œå¢å¼ºå­¦ä¹ æ•°æ®çš„ä»£è¡¨æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå› æœç»éªŒçš„æ‰©å¢ç®—æ³•ï¼ˆCEAï¼‰ã€‚CEAåˆ©ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨æ¨¡æ‹ŸçŠ¶æ€è½¬æ¢çš„åŠ¨æ€æ¨¡å¼ï¼Œå¹¶å¼•å…¥éšæœºæ€§æ¨¡æ‹Ÿéç¨³å®šæ€§ã€‚è¯¥æ–¹æ³•ä¾§é‡äºé€šè¿‡å› æœæ¨æ–­æ‰©å……ç»éªŒæ± ä¸­çš„å­¦ä¹ æ•°æ®ï¼Œå¹¶åœ¨éµå¾ªåŒæ¨¡æ‹Ÿå‡è®¾çš„ç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ã€‚å¯¹äºç¦»æ•£è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´çš„ç¯å¢ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæœ€å¤§æ ¸å¯†åº¦ä¼°è®¡ç†µçš„é‡‡æ ·æ–¹æ³•ï¼Œä»¥æ‰©å±•CEAçš„åº”ç”¨èŒƒå›´ã€‚CEAé€šè¿‡ä¸ºå› æœçŠ¶æ€è½¬æ¢æä¾›åŸºäºçœŸå®ä¿¡æ¯çš„å¥–åŠ±ä¿¡å·ï¼Œæ„å»ºäº†å®Œæ•´çš„å› æœç»éªŒï¼Œç¼“è§£äº†å­¦ä¹ æ•°æ®åˆ†å¸ƒå¤–çš„é—®é¢˜ï¼Œå¹¶åœ¨å…·æœ‰ä¸åŒå±æ€§çš„ç¯å¢ƒä¸­ä¼˜äºä¸€èˆ¬çš„æœ€å…ˆè¿›ç®—æ³•ã€‚æ­¤å¤–æˆ‘ä»¬è¿˜æ¢è®¨äº†ç”Ÿæˆå‹å› æœç»éªŒå’ŒçœŸå®ç»éªŒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€å·®å¼‚æ€§å’Œå±æ€§ã€‚ç›¸å…³ä»£ç å¯è®¿é—®ç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Aegis1863/CEA%E3%80%82">https://github.com/Aegis1863/CEAã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ æ§åˆ¶ç®—æ³•é¢ä¸´åˆ†å¸ƒå¤–å’Œæ— æ•ˆæ¢ç´¢çš„æŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹åŒ–å¼ºåŒ–å­¦ä¹ é€šè¿‡æ„å»ºè™šæ‹Ÿç¯å¢ƒæå‡æ™ºèƒ½ä½“çš„èƒ½åŠ›ï¼Œä½†è®­ç»ƒå¤æ‚æ€§è¾ƒé«˜ã€‚</li>
<li>æå‡ºäº†åŸºäºå› æœç»éªŒçš„æ‰©å¢ç®—æ³•ï¼ˆCEAï¼‰æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>CEAåˆ©ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨æ¨¡æ‹ŸçŠ¶æ€è½¬æ¢ï¼Œå¹¶å¼•å…¥éšæœºæ€§ä»¥é€‚åº”éç¨³å®šæ€§ã€‚</li>
<li>CEAé€šè¿‡æ‰©å……ç»éªŒæ± ä¸­çš„å­¦ä¹ æ•°æ®ï¼Œåœ¨éµå¾ªåŒæ¨¡æ‹Ÿå‡è®¾çš„ç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æå‡ºäº†åŸºäºæœ€å¤§æ ¸å¯†åº¦ä¼°è®¡ç†µçš„é‡‡æ ·æ–¹æ³•ï¼Œä»¥æ‰©å±•CEAè‡³ä¸åŒç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13842">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9961785680c30403866a1b451e86f17a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-317c8ab319a5457ddbc7a88b60f47d14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecfde2ee097dfc12db5c7fbabc85840d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a48b3f69829ca7fb45bd047cbbc67295.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Pensez-Less-Data-Better-Reasoning-â€“-Rethinking-French-LLM"><a href="#Pensez-Less-Data-Better-Reasoning-â€“-Rethinking-French-LLM" class="headerlink" title="Pensez: Less Data, Better Reasoning â€“ Rethinking French LLM"></a>Pensez: Less Data, Better Reasoning â€“ Rethinking French LLM</h2><p><strong>Authors:Huy Hoang Ha</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, achieving strong performance in specialized domains like mathematical reasoning and non-English languages often requires extensive training on massive datasets. This paper investigates a contrasting approach: strategic fine-tuning on a small, high-quality, bilingual (English-French) dataset to enhance both the reasoning capabilities and French language proficiency of a large language model. Rather than relying on scale, we explore the hypothesis that targeted data curation and optimized training can achieve competitive, or even superior, performance. We demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000 carefully selected samples, significant improvements in mathematical reasoning. Specifically, Pensez 7B exhibits an increase in accuracy of the base model up to 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark. These results challenge the prevailing assumption that massive datasets are aprerequisite for strong reasoning performance in LLMs, highlighting the potential of strategic data curation and optimized fine-tuning for enhancing both specialized skills and multilingual capabilities. Our findings have implications for the efficient development of high-performing, multilingual LLMs, especially in resource-constrained scenarios. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨æ•°å­¦æ¨ç†å’Œéè‹±è¯­é¢†åŸŸç­‰ä¸“ä¸šåŒ–é¢†åŸŸå®ç°å“è¶Šæ€§èƒ½ï¼Œé€šå¸¸éœ€è¦åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œå¤§é‡è®­ç»ƒã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§ç›¸åçš„æ–¹æ³•ï¼šåœ¨å°å‹ã€é«˜è´¨é‡ã€åŒè¯­ï¼ˆè‹±è¯­-æ³•è¯­ï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ³•è¯­ç†Ÿç»ƒåº¦ã€‚æˆ‘ä»¬å¹¶ä¸ä¾èµ–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè€Œæ˜¯æ¢ç´¢æœ‰é’ˆå¯¹æ€§çš„æ•°æ®æ”¶é›†å’Œä¼˜åŒ–è®­ç»ƒèƒ½å¦è¾¾åˆ°ç«äº‰æ°´å¹³ç”šè‡³æ›´é«˜æ°´å¹³çš„æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡ä»…åœ¨ç²¾å¿ƒæŒ‘é€‰çš„2000ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œåœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å…·ä½“æ¥è¯´ï¼ŒPensez 7Båœ¨AIME25ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾20%ï¼Œåœ¨æ³•å›½MATH 5çº§åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†12%ã€‚è¿™äº›ç»“æœæŒ‘æˆ˜äº†å¤§è§„æ¨¡æ•°æ®é›†æ˜¯LLMå®ç°å¼ºå¤§æ¨ç†èƒ½åŠ›çš„å…ˆå†³æ¡ä»¶çš„æ™®éå‡è®¾ï¼Œçªæ˜¾äº†æœ‰é’ˆå¯¹æ€§çš„æ•°æ®æ”¶é›†å’Œä¼˜åŒ–å¾®è°ƒåœ¨å¢å¼ºä¸“é¡¹æŠ€èƒ½å’Œå¤šç§è¯­è¨€èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„å‘ç°å¯¹é«˜æ•ˆå¼€å‘é«˜æ€§èƒ½ã€å¤šè¯­è¨€çš„å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰å¯ç¤ºæ„ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13661v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨æ•°å­¦æ¨ç†å’Œéè‹±è¯­é¢†åŸŸçš„ä¸“ä¸šé¢†åŸŸï¼Œé€šå¸¸éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œå¹¿æ³›è®­ç»ƒã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§å¯¹æ¯”æ–¹æ³•ï¼šåœ¨å°å‹ã€é«˜è´¨é‡çš„åŒè¯­ï¼ˆè‹±è¯­-æ³•è¯­ï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒï¼Œä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ³•è¯­ç†Ÿç»ƒåº¦ã€‚ç ”ç©¶å‡è®¾æœ‰é’ˆå¯¹æ€§çš„æ•°æ®æ”¶é›†å’Œä¼˜åŒ–è®­ç»ƒå¯ä»¥è¾¾åˆ°ç«äº‰æ€§çš„æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½æ›´ä¼˜ç§€ã€‚é€šè¿‡ä»…ä½¿ç”¨ç²¾å¿ƒæŒ‘é€‰çš„2000ä¸ªæ ·æœ¬è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œåœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹å–„ã€‚å…·ä½“æ¥è¯´ï¼ŒPensez 7Båœ¨AIME25ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†20%ï¼Œåœ¨æ³•å›½MATH 5çº§åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†12%ã€‚è¿™äº›ç»“æœæŒ‘æˆ˜äº†å¤§è§„æ¨¡æ•°æ®é›†å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ•°å­¦æ¨ç†èƒ½åŠ›çš„å¿…è¦æ€§å‡è®¾ï¼Œçªæ˜¾äº†æœ‰é’ˆå¯¹æ€§çš„æ•°æ®æ”¶é›†å’Œä¼˜åŒ–å¾®è°ƒåœ¨å¢å¼ºç‰¹æ®ŠæŠ€èƒ½å’Œå¤šç§è¯­è¨€èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚è¿™å¯¹é«˜æ•ˆå¼€å‘é«˜æ€§èƒ½çš„ã€å¤šè¯­è¨€çš„LLMæ¨¡å‹æœ‰é‡è¦çš„å½±å“ã€‚è¯¥ç ”ç©¶ä¸ºèµ„æºæœ‰é™æƒ…å¢ƒä¸‹å¦‚ä½•æ¨è¿›ç›¸å…³ç ”å‘æä¾›äº†ä¸€ç§å¯è¡Œçš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMåœ¨ç‰¹å®šé¢†åŸŸå¦‚æ•°å­¦æ¨ç†å’Œéè‹±è¯­è¯­è¨€æ–¹é¢éœ€ä¸“é—¨è®­ç»ƒä»¥æå‡æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å°è§„æ¨¡é«˜è´¨é‡åŒè¯­æ•°æ®é›†çš„æˆ˜ç•¥å¾®è°ƒå¯å¢å¼ºæ¨¡å‹çš„æ¨ç†å’Œè¯­è¨€èƒ½åŠ›ã€‚</li>
<li>ä»…é€šè¿‡å°‘é‡é’ˆå¯¹æ€§æ ·æœ¬ç›‘ç£å¾®è°ƒå°±èƒ½æ˜¾è‘—æ”¹å–„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Pensez 7Bæ¨¡å‹åœ¨AIME25ä¸Šçš„å‡†ç¡®ç‡æå‡æ˜¾è‘—ï¼Œæ˜¾ç¤ºå‡ºä¼˜åŒ–è®­ç»ƒæ•°æ®ç­–ç•¥çš„æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶æŒ‘æˆ˜äº†å¤§è§„æ¨¡æ•°æ®é›†å¯¹äºLLMåœ¨æ•°å­¦æ¨ç†é¢†åŸŸçš„å¿…è¦æ€§å‡è®¾ã€‚</li>
<li>æœ‰é’ˆå¯¹æ€§çš„æ•°æ®æ”¶é›†å’Œä¼˜åŒ–å¾®è°ƒå¯¹äºå¢å¼ºç‰¹æ®ŠæŠ€èƒ½å’Œå¤šç§è¯­è¨€èƒ½åŠ›æœ‰é‡è¦ä½œç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8f481e7d0ccbcab27f9e89107495d3a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-376346e4aae64474b3c1c1ab1b7f97b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f48603ecbc02e0d9d9a8ae3ca367b7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8615721dc78079f793183456a731302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b601391796e98eff163d487d21a8b433.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VeriContaminated-Assessing-LLM-Driven-Verilog-Coding-for-Data-Contamination"><a href="#VeriContaminated-Assessing-LLM-Driven-Verilog-Coding-for-Data-Contamination" class="headerlink" title="VeriContaminated: Assessing LLM-Driven Verilog Coding for Data   Contamination"></a>VeriContaminated: Assessing LLM-Driven Verilog Coding for Data   Contamination</h2><p><strong>Authors:Zeng Wang, Minghao Shao, Jitendra Bhandari, Likhitha Mankali, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel</strong></p>
<p>Large Language Models (LLMs) have revolutionized code generation, achieving exceptional results on various established benchmarking frameworks. However, concerns about data contamination - where benchmark data inadvertently leaks into pre-training or fine-tuning datasets - raise questions about the validity of these evaluations. While this issue is known, limiting the industrial adoption of LLM-driven software engineering, hardware coding has received little to no attention regarding these risks. For the first time, we analyze state-of-the-art (SOTA) evaluation frameworks for Verilog code generation (VerilogEval and RTLLM), using established methods for contamination detection (CCD and Min-K% Prob). We cover SOTA commercial and open-source LLMs (CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1}, GPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned models (RTLCoder and Verigen). Our study confirms that data contamination is a critical concern. We explore mitigations and the resulting trade-offs for code quality vs fairness (i.e., reducing contamination toward unbiased benchmarking). </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å½»åº•æ”¹å˜äº†ä»£ç ç”Ÿæˆçš„æ–¹å¼ï¼Œåœ¨å„ç§æˆç†Ÿçš„åŸºå‡†æµ‹è¯•æ¡†æ¶ä¸Šå–å¾—äº†éå‡¡çš„æˆæœã€‚ç„¶è€Œï¼Œå¯¹æ•°æ®æ±¡æŸ“çš„æ‹…å¿§â€”â€”å³åŸºå‡†æµ‹è¯•æ•°æ®æ— æ„ä¸­æ³„éœ²åˆ°é¢„è®­ç»ƒæˆ–å¾®è°ƒæ•°æ®é›†â€”â€”å¯¹è¿™äº›è¯„ä¼°çš„æœ‰æ•ˆæ€§æå‡ºäº†è´¨ç–‘ã€‚è™½ç„¶è¿™ä¸ªé—®é¢˜å·²ç»ä¸ºäººæ‰€çŸ¥ï¼Œå¹¶é™åˆ¶äº†LLMé©±åŠ¨çš„è½¯ä»¶å·¥ç¨‹åœ¨å·¥ä¸šä¸­çš„åº”ç”¨ï¼Œä½†ç¡¬ä»¶ç¼–ç å‡ ä¹æ²¡æœ‰å…³æ³¨è¿™äº›é£é™©ã€‚æˆ‘ä»¬é¦–æ¬¡ä½¿ç”¨æ±¡æŸ“æ£€æµ‹ï¼ˆCCDå’ŒMin-K% Probï¼‰çš„æ—¢å®šæ–¹æ³•ï¼Œåˆ†æäº†æœ€å…ˆè¿›çš„Verilogä»£ç ç”Ÿæˆè¯„ä¼°æ¡†æ¶ï¼ˆVerilogEvalå’ŒRTLLMï¼‰ã€‚æˆ‘ä»¬æ¶µç›–äº†æœ€å…ˆè¿›çš„å•†ä¸šå’Œå¼€æºLLMï¼ˆCodeGen2.5ã€Minitron 4bã€Mistral 7bã€phi-4 miniã€LLaMA-{1,2,3.1}ã€GPT-{2,3.5,4o}ã€Deepseek-Coderå’ŒCodeQwen 1.5ï¼‰ï¼Œä»¥åŠåŸºå‡†æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ï¼ˆRTLCoderå’ŒVerigenï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¯å®äº†æ•°æ®æ±¡æŸ“æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚æˆ‘ä»¬æ¢è®¨äº†ç¼“è§£æ–¹æ³•å’Œä»£ç è´¨é‡ä¸å…¬å¹³æ€§ä¹‹é—´çš„æƒè¡¡ï¼ˆå³å‡å°‘æ±¡æŸ“ä»¥å®ç°å…¬å¹³çš„åŸºå‡†æµ‹è¯•ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13572v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†é©å‘½æ€§çš„æˆæœï¼Œä½†åœ¨é¢„è®­ç»ƒæˆ–å¾®è°ƒæ•°æ®é›†æ„å¤–æ³„éœ²åŸºå‡†æµ‹è¯•æ•°æ®çš„é—®é¢˜ä¸Šå¼•å‘äº†æœ‰æ•ˆæ€§è´¨ç–‘ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹æœ€å…ˆè¿›çš„Verilogä»£ç ç”Ÿæˆè¯„ä¼°æ¡†æ¶ï¼ˆVerilogEvalå’ŒRTLLMï¼‰è¿›è¡Œäº†åˆ†æï¼Œå¹¶ä½¿ç”¨äº†æ±¡æŸ“æ£€æµ‹æ–¹æ³•æ¥éªŒè¯æ•°æ®æ±¡æŸ“é—®é¢˜ã€‚ç ”ç©¶ç¡®è®¤æ•°æ®æ±¡æŸ“æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œå¹¶æ¢è®¨äº†ç¼“è§£æªæ–½åŠå…¶ä¸ä»£ç è´¨é‡å’Œå…¬å¹³æ€§ä¹‹é—´çš„æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>æ•°æ®æ±¡æŸ“é—®é¢˜å¯¹LLMçš„æœ‰æ•ˆæ€§è¯„ä»·æå‡ºäº†è´¨ç–‘ã€‚</li>
<li>é¦–æ¬¡å¯¹Verilogä»£ç ç”Ÿæˆçš„æœ€æ–°è¯„ä¼°æ¡†æ¶è¿›è¡Œäº†æ±¡æŸ“åˆ†æã€‚</li>
<li>ä½¿ç”¨å·²å»ºç«‹çš„æ±¡æŸ“æ£€æµ‹æ–¹æ³•è¯å®äº†æ•°æ®æ±¡æŸ“é—®é¢˜çš„å­˜åœ¨ã€‚</li>
<li>æ•°æ®æ±¡æŸ“æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œéœ€è¦è§£å†³ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†ç¼“è§£æ•°æ®æ±¡æŸ“çš„æªæ–½åŠå…¶ä¸ä»£ç è´¨é‡çš„æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff03c5c5972b3c9b0617a9aa0ef2ca67.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4709f17215072c2f1b7fbcfb5265a2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9cbc0b5bf0382965889bea139ea0c37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecd8384c56b1f966b0f8adc5a02e51b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e71ad7d5da003b540e398666068510b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-138220fc4df4e1398b017e4095b43b14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38da2c008a157bae10609cf38641b4c4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Hierarchical-Multi-Step-Reward-Models-for-Enhanced-Reasoning-in-Large-Language-Models"><a href="#Towards-Hierarchical-Multi-Step-Reward-Models-for-Enhanced-Reasoning-in-Large-Language-Models" class="headerlink" title="Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in   Large Language Models"></a>Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in   Large Language Models</h2><p><strong>Authors:Teng Wang, Zhangyi Jiang, Zhenqi He, Wenhan Yang, Yanan Zheng, Zeyu Li, Zifan He, Shenyang Tong, Hailei Gong</strong></p>
<p>Recent studies show that Large Language Models (LLMs) achieve strong reasoning capabilities through supervised fine-tuning or reinforcement learning. However, a key approach, the Process Reward Model (PRM), suffers from reward hacking, making it unreliable in identifying the best intermediate steps. In this paper, we propose a novel reward model approach, Hierarchical Reward Model (HRM), which evaluates both individual and consecutive reasoning steps from fine-grained and coarse-grained level. HRM performs better in assessing reasoning coherence and self-reflection, particularly when the previous reasoning step is incorrect. Furthermore, to address the inefficiency of autonomous generating PRM training data via Monte Carlo Tree Search (MCTS), we introduce a lightweight and effective data augmentation strategy called Hierarchical Node Compression (HNC) based on node merging (combining two consecutive reasoning steps into one step) in the tree structure. This approach diversifies MCTS results for HRM with negligible computational overhead, enhancing label robustness by introducing noise. Empirical results on the PRM800K dataset demonstrate that HRM, in conjunction with HNC, achieves superior stability and reliability in evaluation compared to PRM. Furthermore, cross-domain evaluations on MATH500 and GSM8K confirm HRMâ€™s superior generalization and robustness across diverse reasoning tasks. The code for all experiments will be released at https: &#x2F;&#x2F;github.com&#x2F;tengwang0318&#x2F;hierarchial_reward_model. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ å®ç°äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€ç§å…³é”®æ–¹æ³•â€”â€”è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å­˜åœ¨å¥–åŠ±ä½œå¼Šçš„é—®é¢˜ï¼Œä½¿å…¶åœ¨è¯†åˆ«æœ€ä½³ä¸­é—´æ­¥éª¤æ—¶å˜å¾—ä¸å¯é ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¨¡å‹æ–¹æ³•ï¼Œå³åˆ†å±‚å¥–åŠ±æ¨¡å‹ï¼ˆHRMï¼‰ï¼Œå®ƒå¯ä»¥ä»ç²¾ç»†ç²’åº¦å’Œç²—ç•¥ç²’åº¦ä¸¤ä¸ªå±‚é¢è¯„ä¼°å•ä¸ªå’Œè¿ç»­çš„æ¨ç†æ­¥éª¤ã€‚HRMåœ¨è¯„ä¼°æ¨ç†è¿è´¯æ€§å’Œè‡ªæˆ‘åæ€æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨å‰ä¸€ä¸ªæ¨ç†æ­¥éª¤é”™è¯¯çš„æƒ…å†µä¸‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³é€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è‡ªä¸»ç”ŸæˆPRMè®­ç»ƒæ•°æ®æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºèŠ‚ç‚¹åˆå¹¶ï¼ˆå°†ä¸¤ä¸ªè¿ç»­çš„æ¨ç†æ­¥éª¤åˆå¹¶ä¸ºä¸€ä¸ªæ­¥éª¤ï¼‰çš„æ ‘ç»“æ„ä¸­çš„åˆ†å±‚èŠ‚ç‚¹å‹ç¼©ï¼ˆHNCï¼‰çš„è½»é‡çº§æœ‰æ•ˆæ•°æ®å¢å¼ºç­–ç•¥ã€‚è¿™ç§æ–¹æ³•åœ¨HRMä¸­å®ç°äº†MCTSç»“æœçš„å¤šæ ·åŒ–ï¼ŒåŒæ—¶å‡ ä¹ä¸å¢åŠ è®¡ç®—å¼€é”€ï¼Œå¹¶é€šè¿‡å¼•å…¥å™ªå£°å¢å¼ºäº†æ ‡ç­¾çš„é²æ£’æ€§ã€‚åœ¨PRM800Kæ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼Œä¸PRMç›¸æ¯”ï¼ŒHRMç»“åˆHNCåœ¨è¯„ä¼°ä¸­å®ç°äº†æ›´é«˜çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚æ­¤å¤–ï¼ŒMATH500å’ŒGSM8Kçš„è·¨åŸŸè¯„ä¼°è¯å®äº†HRMåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ³›åŒ–å’Œé²æ£’æ€§ã€‚æ‰€æœ‰å®éªŒçš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/tengwang0318/hierarchial_reward_model%E4%B8%8A%E5%8F%91%E6%9C%8D%E3%80%82">https://github.com/tengwang0318/hierarchial_reward_modelä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13551v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å­˜åœ¨å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œæ— æ³•å¯é åœ°è¯†åˆ«æœ€ä½³ä¸­é—´æ­¥éª¤ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¨¡å‹æ–¹æ³•â€”â€”åˆ†å±‚å¥–åŠ±æ¨¡å‹ï¼ˆHRMï¼‰ï¼Œå¯ä»¥ä»ç²¾ç»†ç²’åº¦å’Œç²—ç•¥ç²’åº¦çº§åˆ«è¯„ä¼°å•ä¸ªå’Œè¿ç»­çš„æ¨ç†æ­¥éª¤ã€‚HRMåœ¨è¯„ä¼°æ¨ç†è¿è´¯æ€§å’Œè‡ªæˆ‘åæ€æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œå°¤å…¶åœ¨å…ˆå‰æ¨ç†æ­¥éª¤é”™è¯¯æ—¶ã€‚ä¸ºè§£å†³è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è‡ªä¸»ç”ŸæˆPRMè®­ç»ƒæ•°æ®çš„ä¸æ•ˆç‡é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºèŠ‚ç‚¹åˆå¹¶ï¼ˆå°†ä¸¤ä¸ªè¿ç»­çš„æ¨ç†æ­¥éª¤åˆå¹¶ä¸ºä¸€ä¸ªæ­¥éª¤ï¼‰çš„åˆ†å±‚èŠ‚ç‚¹å‹ç¼©ï¼ˆHNCï¼‰è½»é‡çº§æœ‰æ•ˆæ•°æ®å¢å¼ºç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨PRM800Kæ•°æ®é›†ä¸Šå®ç°äº†ä¸PRMç›¸æ¯”æ›´ç¨³å®šå¯é çš„è¯„ä¼°ç»“æœã€‚æ­¤å¤–ï¼ŒMATH500å’ŒGSM8Kçš„è·¨åŸŸè¯„ä¼°è¯æ˜äº†HRMåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­çš„ä¼˜å¼‚é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚æ‰€æœ‰å®éªŒçš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/tengwang0318/hierarchial_reward_model%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/tengwang0318/hierarchial_reward_modelå‘å¸ƒã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒæˆ–å¼ºåŒ–å­¦ä¹ å®ç°ã€‚</li>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å­˜åœ¨å¥–åŠ±é»‘å®¢é—®é¢˜ï¼Œå¯é æ€§å—æŸã€‚</li>
<li>å¼•å…¥åˆ†å±‚å¥–åŠ±æ¨¡å‹ï¼ˆHRMï¼‰ï¼Œèƒ½è¯„ä¼°å•ä¸ªå’Œè¿ç»­æ¨ç†æ­¥éª¤ï¼Œæå‡æ¨ç†è¿è´¯æ€§å’Œè‡ªæˆ‘åæ€èƒ½åŠ›ã€‚</li>
<li>HRMåœ¨å…ˆå‰æ¨ç†æ­¥éª¤é”™è¯¯æ—¶è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>æå‡ºåˆ†å±‚èŠ‚ç‚¹å‹ç¼©ï¼ˆHNCï¼‰æ•°æ®å¢å¼ºç­–ç•¥ï¼ŒåŸºäºèŠ‚ç‚¹åˆå¹¶ï¼Œæé«˜è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ç»“æœå¤šæ ·æ€§ï¼Œå¢å¼ºæ ‡ç­¾ç¨³å¥æ€§ã€‚</li>
<li>HRMç»“åˆHNCåœ¨PRM800Kæ•°æ®é›†ä¸Šå®ç°ç¨³å®šå¯é çš„è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19d314ff82722671b755ab6a6e9b2499.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7568558e1517c5182f2377d98298b5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d034c9d376834234002a6135fdcb7722.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c56a6508f1582ac255f3a414b9fbd22f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bbffa6422b78dc8703d9f456b946584.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c96591df0d55499f82fcce06c217a9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf8ccf9335031bf8e7c031227e134403.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Showdown-of-ChatGPT-vs-DeepSeek-in-Solving-Programming-Tasks"><a href="#A-Showdown-of-ChatGPT-vs-DeepSeek-in-Solving-Programming-Tasks" class="headerlink" title="A Showdown of ChatGPT vs DeepSeek in Solving Programming Tasks"></a>A Showdown of ChatGPT vs DeepSeek in Solving Programming Tasks</h2><p><strong>Authors:Ronas Shakya, Farhad Vadiee, Mohammad Khalil</strong></p>
<p>The advancement of large language models (LLMs) has created a competitive landscape for AI-assisted programming tools. This study evaluates two leading models: ChatGPT 03-mini and DeepSeek-R1 on their ability to solve competitive programming tasks from Codeforces. Using 29 programming tasks of three levels of easy, medium, and hard difficulty, we assessed the outcome of both models by their accepted solutions, memory efficiency, and runtime performance. Our results indicate that while both models perform similarly on easy tasks, ChatGPT outperforms DeepSeek-R1 on medium-difficulty tasks, achieving a 54.5% success rate compared to DeepSeek 18.1%. Both models struggled with hard tasks, thus highlighting some ongoing challenges LLMs face in handling highly complex programming problems. These findings highlight key differences in both model capabilities and their computational power, offering valuable insights for developers and researchers working to advance AI-driven programming tools. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºAIè¾…åŠ©ç¼–ç¨‹å·¥å…·åˆ›é€ äº†ä¸€ä¸ªç«äº‰ç¯å¢ƒã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸¤æ¬¾é¢†å…ˆæ¨¡å‹ï¼šChatGPT 03-miniå’ŒDeepSeek-R1ï¼Œå®ƒä»¬åœ¨è§£å†³Codeforcesç«èµ›ç¼–ç¨‹ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨29ä¸ªç¼–ç¨‹ä»»åŠ¡ï¼Œåˆ†ä¸ºå®¹æ˜“ã€ä¸­ç­‰å’Œå›°éš¾ä¸‰ä¸ªçº§åˆ«ï¼Œé€šè¿‡æ¥å—çš„è§£å†³æ–¹æ¡ˆã€å†…å­˜æ•ˆç‡å’Œè¿è¡Œæ—¶æ€§èƒ½æ¥è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä¸¤ä¸ªæ¨¡å‹åœ¨å®¹æ˜“çš„ä»»åŠ¡ä¸Šè¡¨ç°ç›¸ä¼¼ï¼Œä½†ChatGPTåœ¨ä¸­ç­‰éš¾åº¦çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºDeepSeek-R1ï¼ŒæˆåŠŸç‡ä¸º54.5%ï¼Œè€ŒDeepSeekçš„æˆåŠŸç‡ä»…ä¸º18.1%ã€‚ä¸¤ä¸ªæ¨¡å‹åœ¨å›°éš¾çš„ä»»åŠ¡ä¸Šéƒ½é‡åˆ°äº†å›°éš¾ï¼Œè¿™çªæ˜¾äº†LLMåœ¨å¤„ç†é«˜åº¦å¤æ‚çš„ç¼–ç¨‹é—®é¢˜æ—¶é¢ä¸´çš„ä¸€äº›æŒç»­æŒ‘æˆ˜ã€‚è¿™äº›å‘ç°çªå‡ºäº†ä¸¤ä¸ªæ¨¡å‹èƒ½åŠ›å’Œè®¡ç®—èƒ½åŠ›çš„å…³é”®å·®å¼‚ï¼Œä¸ºå¼€å‘å’Œç ”ç©¶AIé©±åŠ¨çš„ç¼–ç¨‹å·¥å…·çš„å¼€å‘äººå‘˜å’Œç ”ç©¶äººå‘˜æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13549v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶å¯¹ChatGPT 03-miniå’ŒDeepSeek-R1ä¸¤ç§é¢†å…ˆçš„AIè¾…åŠ©ç¼–ç¨‹å·¥å…·è¿›è¡Œäº†è¯„ä¼°ï¼Œä¸»è¦è€ƒå¯Ÿå®ƒä»¬åœ¨è§£å†³Codeforcesç«èµ›ç¼–ç¨‹ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸­ç­‰éš¾åº¦çš„ç¼–ç¨‹ä»»åŠ¡ä¸Šï¼ŒChatGPTè¡¨ç°ä¼˜äºDeepSeek-R1ï¼ŒæˆåŠŸç‡ä¸º54.5%ï¼Œè€ŒDeepSeekçš„æˆåŠŸç‡ä¸ºä»…18.1%ã€‚ä½†åœ¨é«˜éš¾åº¦çš„ç¼–ç¨‹ä»»åŠ¡ä¸Šï¼Œä¸¤è€…éƒ½é¢ä¸´æŒ‘æˆ˜ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå¼€å‘è€…å’Œç ”ç©¶äººå‘˜æä¾›äº†å…³äºAIé©±åŠ¨ç¼–ç¨‹å·¥å…·çš„å…³é”®å·®å¼‚å’Œèƒ½åŠ›çš„é‡è¦è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä¸ºAIè¾…åŠ©ç¼–ç¨‹å·¥å…·åˆ›é€ äº†ç«äº‰ç¯å¢ƒã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ChatGPT 03-miniå’ŒDeepSeek-R1ä¸¤ç§é¢†å…ˆæ¨¡å‹åœ¨è§£å†³ä¸åŒéš¾åº¦çº§åˆ«ç¼–ç¨‹ä»»åŠ¡æ—¶çš„è¡¨ç°ã€‚</li>
<li>åœ¨ä¸­ç­‰éš¾åº¦çš„ç¼–ç¨‹ä»»åŠ¡ä¸Šï¼ŒChatGPTè¡¨ç°ä¼˜äºDeepSeek-R1ã€‚</li>
<li>åœ¨é«˜éš¾åº¦çš„ç¼–ç¨‹ä»»åŠ¡ä¸Šï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹éƒ½é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ChatGPTå’ŒDeepSeekåœ¨è§£å†³ç¼–ç¨‹ä»»åŠ¡æ—¶ï¼Œè¿˜å­˜åœ¨å†…å­˜æ•ˆç‡å’Œè¿è¡Œæ—¶é—´æ€§èƒ½çš„å·®å¼‚ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¼€å‘è€…æä¾›äº†å…³äºä¸åŒAIç¼–ç¨‹å·¥å…·çš„èƒ½åŠ›å’Œå±€é™æ€§çš„é‡è¦ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5e00af8b47715f6b17e7d6f2a45b9725.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3f7c04485f138b0c7201da49b0b5857.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96f22e362929099cfc1528b65e5c71a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99d5d4a87da2ca6af76ab4009105ef7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57815d4fc535f25b3aec9b49a91d36a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad6cf107827165df15b5d65af5039c2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8915bc890d917ed159c438fd57bceb03.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DeepPerception-Advancing-R1-like-Cognitive-Visual-Perception-in-MLLMs-for-Knowledge-Intensive-Visual-Grounding"><a href="#DeepPerception-Advancing-R1-like-Cognitive-Visual-Perception-in-MLLMs-for-Knowledge-Intensive-Visual-Grounding" class="headerlink" title="DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs   for Knowledge-Intensive Visual Grounding"></a>DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs   for Knowledge-Intensive Visual Grounding</h2><p><strong>Authors:Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F. Wong, Xiaoyi Feng, Maosong Sun</strong></p>
<p>Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08% accuracy improvements on KVG-Bench and exhibiting +4.60% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at <a target="_blank" rel="noopener" href="https://github.com/thunlp/DeepPerception">https://github.com/thunlp/DeepPerception</a>. </p>
<blockquote>
<p>äººç±»ä¸“å®¶æ“…é•¿é€šè¿‡åˆ©ç”¨é¢†åŸŸçŸ¥è¯†æ¥ç»†åŒ–æ„ŸçŸ¥ç‰¹å¾ï¼Œè¿›è¡Œç²¾ç»†åŒ–çš„è§†è§‰è¾¨åˆ«ï¼Œè¿™æ˜¯å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å°šæœªå……åˆ†å‘å±•çš„ä¸€é¡¹èƒ½åŠ›ã€‚å°½ç®¡æ‹¥æœ‰å¤§é‡çš„ä¸“å®¶çº§çŸ¥è¯†ï¼ŒMLLMsåœ¨å°†æ¨ç†èå…¥è§†è§‰æ„ŸçŸ¥æ–¹é¢å´å­˜åœ¨å›°éš¾ï¼Œé€šå¸¸ä¼šäº§ç”Ÿç›´æ¥çš„å›åº”ï¼Œè€Œæ— éœ€è¿›è¡Œæ›´æ·±å…¥çš„åˆ†æã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†çŸ¥è¯†å¯†é›†å‹è§†è§‰å®šä½ï¼ˆKVGï¼‰ï¼Œè¿™æ˜¯ä¸€é¡¹éœ€è¦ç²¾ç»†æ„ŸçŸ¥å’Œç‰¹å®šé¢†åŸŸçŸ¥è¯†æ•´åˆçš„æ–°å‹è§†è§‰å®šä½ä»»åŠ¡ã€‚ä¸ºäº†åº”å¯¹KVGçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeepPerceptionï¼Œè¿™æ˜¯ä¸€ä¸ªå…·å¤‡è®¤çŸ¥è§†è§‰æ„ŸçŸ¥èƒ½åŠ›çš„MLLMå¢å¼ºç‰ˆæœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ï¼ˆ1ï¼‰è‡ªåŠ¨åŒ–æ•°æ®åˆæˆç®¡é“ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡ã€ä¸çŸ¥è¯†å¯¹é½çš„è®­ç»ƒæ ·æœ¬ï¼›ï¼ˆ2ï¼‰ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆç›‘ç£å¾®è°ƒç”¨äºè®¤çŸ¥æ¨ç†è„šæ‰‹æ¶å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥ä¼˜åŒ–æ„ŸçŸ¥ä¸è®¤çŸ¥ååŒä½œç”¨ã€‚ä¸ºäº†è¯„ä¼°æ€§èƒ½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†KVG-Benchæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–10ä¸ªé¢†åŸŸã€åŒ…å«1300ä¸ªæ‰‹åŠ¨æ•´ç†æµ‹è¯•æ¡ˆä¾‹çš„ç»¼åˆæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepPerceptionæ˜¾è‘—ä¼˜äºç›´æ¥å¾®è°ƒï¼Œåœ¨KVG-Benchä¸Šçš„å‡†ç¡®ç‡æé«˜äº†8.08%ï¼Œå¹¶ä¸”åœ¨è·¨åŸŸæ³›åŒ–æ–¹é¢è¾ƒåŸºçº¿æ–¹æ³•æé«˜äº†4.60%ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å°†è®¤çŸ¥è¿‡ç¨‹èå…¥MLLMsä»¥å®ç°äººç±»æ ·è§†è§‰æ„ŸçŸ¥çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€æ¨ç†ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚æ•°æ®ã€ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/thunlp/DeepPerception%E3%80%82">https://github.com/thunlp/DeepPerceptionã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12797v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢çš„ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ç²¾ç»†ç²’åº¦è§†è§‰è¾¨åˆ«å’Œé¢†åŸŸçŸ¥è¯†æ•´åˆæ–¹é¢çš„èƒ½åŠ›å°šå¾…å‘å±•ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºçŸ¥è¯†å¯†é›†å‹è§†è§‰æ¥åœ°ï¼ˆKVGï¼‰çš„æ–°è§†è§‰æ¥åœ°ä»»åŠ¡ï¼Œè¦æ±‚ç²¾ç»†çš„æ„ŸçŸ¥èƒ½åŠ›å’Œç‰¹å®šé¢†åŸŸçš„æ•´åˆçŸ¥è¯†ã€‚åŒæ—¶æå‡ºäº†ä¸€ç§è§£å†³è¿™ä¸€ä»»åŠ¡çš„æ–¹æ³•DeepPerceptionï¼Œå…¶å¢å¼ºäº†è®¤çŸ¥è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚å®ƒé€šè¿‡è‡ªåŠ¨åŒ–çš„æ•°æ®åˆæˆç®¡é“ç”Ÿæˆé«˜è´¨é‡çš„çŸ¥è¯†å¯¹é½è®­ç»ƒæ ·æœ¬ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ç»“åˆç›‘ç£å¾®è°ƒè¿›è¡Œè®¤çŸ¥æ¨ç†æ¶æ„å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ„ŸçŸ¥è®¤çŸ¥ååŒä½œç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepPerceptionåœ¨KVGåŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†8.08%ï¼Œå¹¶ä¸”åœ¨è·¨åŸŸæ³›åŒ–æ–¹é¢ä¹Ÿä¼˜äºåŸºçº¿æ–¹æ³•ã€‚è¿™ä¸ºå¤šæ¨¡æ€æ¨ç†ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç»†ç²’åº¦è§†è§‰è¾¨åˆ«å’Œé¢†åŸŸçŸ¥è¯†æ•´åˆæ–¹é¢ã€‚</li>
<li>çŸ¥è¯†å¯†é›†å‹è§†è§‰æ¥åœ°ï¼ˆKVGï¼‰ä»»åŠ¡è¦æ±‚æ¨¡å‹åŒæ—¶å…·å¤‡ç²¾ç»†çš„æ„ŸçŸ¥èƒ½åŠ›å’Œç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†æ•´åˆèƒ½åŠ›ã€‚</li>
<li>DeepPerceptionæ˜¯ä¸€ç§è§£å†³KVGä»»åŠ¡çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–çš„æ•°æ®åˆæˆç®¡é“ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>DeepPerceptioné‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ„ŸçŸ¥è®¤çŸ¥ååŒä½œç”¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepPerceptionåœ¨KVGåŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>DeepPerceptionåœ¨è·¨åŸŸæ³›åŒ–æ–¹é¢ä¹Ÿè¡¨ç°å‡ºä¼˜äºåŸºçº¿æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-472fb9eedf5332ecb2fdbb833cf4c88f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0abf3d2f81927ca777a7ff2ed0fd6f0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f87d4dad88887b8349de9e7b24324378.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6095da6c179530575e09bed5e0864e11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44eb91d579e94cde3e49fc376b57702a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MPBench-A-Comprehensive-Multimodal-Reasoning-Benchmark-for-Process-Errors-Identification"><a href="#MPBench-A-Comprehensive-Multimodal-Reasoning-Benchmark-for-Process-Errors-Identification" class="headerlink" title="MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process   Errors Identification"></a>MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process   Errors Identification</h2><p><strong>Authors:Zhaopan Xu, Pengfei Zhou, Jiaxin Ai, Wangbo Zhao, Kai Wang, Xiaojiang Peng, Wenqi Shao, Hongxun Yao, Kaipeng Zhang</strong></p>
<p>Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs. </p>
<blockquote>
<p>æ¨ç†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³å¤æ‚ä»»åŠ¡çš„åŸºæœ¬èƒ½åŠ›ï¼Œè€Œè¯†åˆ«è¿‡ç¨‹ä¸­çš„é”™è¯¯å¯¹äºæé«˜è¿™ç§èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œæå‡ºäº†è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ï¼Œæä¾›åˆ†æ­¥å¥–åŠ±ï¼Œä¾¿äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¼ºåŒ–å­¦ä¹ å’Œæ•°æ®ç”Ÿæˆï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å¯¼LLMèµ°å‘æ­£ç¡®çš„æ­¥éª¤ï¼Œä»è€Œæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PRMåŸºå‡†æµ‹è¯•éƒ½æ˜¯åŸºäºæ–‡æœ¬çš„ï¼Œä¾§é‡äºé”™è¯¯æ£€æµ‹ï¼Œå¿½ç•¥äº†å…¶ä»–åœºæ™¯ï¼Œå¦‚æ¨ç†æœç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†MPBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢ã€å¤šä»»åŠ¡ã€å¤šæ¨¡å¼çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°PRMåœ¨ä¸åŒåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚MPBenché‡‡ç”¨ä¸‰ç§è¯„ä¼°èŒƒå¼ï¼Œæ¯ä¸ªèŒƒå¼éƒ½é’ˆå¯¹PRMåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ç‰¹å®šè§’è‰²ï¼š1ï¼‰æ­¥éª¤æ­£ç¡®æ€§ï¼Œè¯„ä¼°æ¯ä¸ªä¸­é—´æ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ï¼›2ï¼‰ç­”æ¡ˆèšåˆï¼Œèšåˆå¤šä¸ªè§£å†³æ–¹æ¡ˆå¹¶é€‰æ‹©æœ€ä½³è§£å†³æ–¹æ¡ˆï¼›3ï¼‰æ¨ç†è¿‡ç¨‹æœç´¢ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å¯¼å¯¹æœ€ä½³æ¨ç†æ­¥éª¤çš„æœç´¢ã€‚é€šè¿‡è¿™äº›èŒƒå¼ï¼ŒMPBenchè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œå¹¶ä¸ºå¤šæ¨¡å¼PRMçš„å‘å±•æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12505v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿›è¡Œå¤æ‚ä»»åŠ¡æ—¶ï¼Œæ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰é€šè¿‡æä¾›åˆ†æ­¥å¥–åŠ±ï¼Œä¿ƒè¿›å¼ºåŒ–å­¦ä¹ å’Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®ç”Ÿæˆï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å¯¼LLMèµ°å‘æ­£ç¡®çš„æ­¥éª¤ï¼Œä»è€Œæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PRMåŸºå‡†æµ‹è¯•ä¸»è¦æ˜¯æ–‡æœ¬åŸºç¡€çš„ï¼Œä¾§é‡äºé”™è¯¯æ£€æµ‹ï¼Œå¿½ç•¥äº†å¦‚æ¨ç†æœç´¢ç­‰å…¶ä»–åœºæ™¯ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºMPBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡ã€å¤šæ¨¡æ€çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°PRMåœ¨å¤šç§åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚MPBenché‡‡ç”¨ä¸‰ç§è¯„ä¼°æ¨¡å¼ï¼Œåˆ†åˆ«é’ˆå¯¹PRMåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„ç‰¹å®šè§’è‰²ï¼š1ï¼‰æ­¥éª¤æ­£ç¡®æ€§ï¼Œè¯„ä¼°æ¯ä¸ªä¸­é—´æ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ï¼›2ï¼‰ç­”æ¡ˆèšåˆï¼Œèšåˆå¤šä¸ªè§£å†³æ–¹æ¡ˆå¹¶é€‰æ‹©æœ€ä½³ç­”æ¡ˆï¼›3ï¼‰æ¨ç†è¿‡ç¨‹æœç´¢ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¼•å¯¼æœ€ä¼˜æ¨ç†æ­¥éª¤çš„æœç´¢ã€‚é€šè¿‡è¿™äº›æ¨¡å¼ï¼ŒMPBenchè¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå¹¶ä¸ºå¤šæ¨¡æ€PRMçš„å‘å±•æä¾›è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿›è¡Œå¤æ‚ä»»åŠ¡æ—¶éœ€è¦å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¿‡ç¨‹çº§å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰å¯ä»¥æé«˜LLMçš„æ¨ç†å‡†ç¡®æ€§ã€‚</li>
<li>ç°æœ‰PRMåŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºæ–‡æœ¬åŸºç¡€çš„é”™è¯¯æ£€æµ‹ï¼Œå¿½ç•¥äº†å…¶ä»–åœºæ™¯ï¼Œå¦‚æ¨ç†æœç´¢ã€‚</li>
<li>MPBenchæ˜¯ä¸€ä¸ªå¤šä»»åŠ¡ã€å¤šæ¨¡æ€çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°PRMåœ¨å¤šç§åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>MPBenché‡‡ç”¨ä¸‰ç§è¯„ä¼°æ¨¡å¼ï¼ŒåŒ…æ‹¬æ­¥éª¤æ­£ç¡®æ€§ã€ç­”æ¡ˆèšåˆå’Œæ¨ç†è¿‡ç¨‹æœç´¢ã€‚</li>
<li>MPBenchä¸ºå…¨é¢è¯„ä¼°PRMæä¾›å¹³å°ï¼Œå¹¶æœ‰åŠ©äºäº†è§£å…¶åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c89ddabd11c041b38a0298e95b8e04ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ca84fa3a8151656e4ceb50a8c75a77a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09f249437d95434b9ee7a7c7c3fe8981.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81ff828745ceb45f948b35c216eb3175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7016cae4b239a86d9723dd1de050e86.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SPIN-Bench-How-Well-Do-LLMs-Plan-Strategically-and-Reason-Socially"><a href="#SPIN-Bench-How-Well-Do-LLMs-Plan-Strategically-and-Reason-Socially" class="headerlink" title="SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?"></a>SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?</h2><p><strong>Authors:Jianzhu Yao, Kevin Wang, Ryan Hsieh, Haisu Zhou, Tianqing Zou, Zerui Cheng, Zhangyang Wang, Pramod Viswanath</strong></p>
<p>Reasoning and strategic behavior in social interactions is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multi-hop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and humanâ€“AI teaming. Project Website: <a target="_blank" rel="noopener" href="https://spinbench.github.io/">https://spinbench.github.io/</a> </p>
<blockquote>
<p>åœ¨ç¤¾ä¼šäº’åŠ¨ä¸­çš„æ¨ç†å’Œæˆ˜ç•¥è¡Œä¸ºæ˜¯æ™ºèƒ½çš„æ ‡å¿—ã€‚è¿™ç§æ¨ç†å½¢å¼è¿œæ¯”é™æ€ç¯å¢ƒä¸­çš„å­¤ç«‹è§„åˆ’æˆ–æ¨ç†ä»»åŠ¡ï¼ˆä¾‹å¦‚æ•°å­¦é—®é¢˜è§£å†³ï¼‰æ›´ä¸ºå¤æ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæˆ˜ç•¥è§„åˆ’ã€äº’åŠ¨ä¸è°ˆåˆ¤ï¼ˆSPIN-Benchï¼‰â€ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¤šé¢†åŸŸè¯„ä¼°ï¼Œæ—¨åœ¨è¡¡é‡æˆ˜ç•¥è§„åˆ’å’Œç¤¾ä¼šæ¨ç†çš„æ™ºèƒ½æ°´å¹³ã€‚è™½ç„¶è®¸å¤šç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ç‹­éš˜çš„è§„åˆ’æˆ–å•ä»£ç†æ¨ç†ä¸Šï¼Œä½†SPIN-Benchç»“åˆäº†ç»å…¸PDDLä»»åŠ¡ã€ç«æŠ€æ£‹ç±»æ¸¸æˆã€åˆä½œå¡ç‰Œæ¸¸æˆå’Œå¤šä»£ç†è°ˆåˆ¤åœºæ™¯åœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚è¯¥æ¡†æ¶æ—¢åŒ…æ‹¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä¹ŸåŒ…æ‹¬ä¸€ä¸ªæ¨¡æ‹Ÿå’Œè¯„ä¼°å„ç§ç¤¾äº¤è®¾ç½®çš„åœºæ‰€ï¼Œä»¥æµ‹è¯•AIä»£ç†çš„æ¨ç†å’Œæˆ˜ç•¥è¡Œä¸ºã€‚æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿåœ°æ”¹å˜åŠ¨ä½œç©ºé—´ã€çŠ¶æ€å¤æ‚æ€§å’Œäº¤äº’ä»£ç†çš„æ•°é‡æ¥åˆ¶å®šSPIN-BenchåŸºå‡†æµ‹è¯•ï¼Œä»¥æ¨¡æ‹Ÿå„ç§ç¤¾äº¤ç¯å¢ƒï¼Œåœ¨è¿™äº›ç¯å¢ƒä¸­ï¼ŒæˆåŠŸä¸ä»…å–å†³äºæ–¹æ³•å’Œé€æ­¥çš„å†³ç­–åˆ¶å®šï¼Œè¿˜å–å†³äºå¯¹å…¶ä»–ï¼ˆå¯¹æŠ—æ€§æˆ–åˆä½œæ€§ï¼‰å‚ä¸è€…çš„æ¦‚å¿µæ¨æ–­ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè™½ç„¶å½“ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†åŸºæœ¬äº‹å®æ£€ç´¢å’ŒçŸ­æœŸè§„åˆ’æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦æ·±åº¦å¤šè·³æ¨ç†å’Œå¤§çŠ¶æ€ç©ºé—´çš„ç¤¾ä¼šé€‚åº”æ€§åè°ƒçš„ä»»åŠ¡ä¸­ï¼Œå®ƒä»¬ä¼šé‡åˆ°æ˜¾è‘—çš„æ€§èƒ½ç“¶é¢ˆã€‚æˆ‘ä»¬æœŸæœ›SPIN-Benchèƒ½æˆä¸ºæœªæ¥å…³äºç¨³å¥çš„å¤šä»£ç†è§„åˆ’ã€ç¤¾ä¼šæ¨ç†å’Œäººæœºåä½œçš„ç ”ç©¶å‚¬åŒ–å‰‚ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://spinbench.github.io/">https://spinbench.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12349v2">PDF</a> 51 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSPIN-Benchçš„æ–°å‹å¤šé¢†åŸŸè¯„ä¼°æ–¹æ³•ï¼Œæ—¨åœ¨æµ‹é‡æ™ºèƒ½çš„æˆ˜ç•¥è§„åˆ’å’Œç¤¾äº¤æ¨ç†èƒ½åŠ›ã€‚ä¸ç°æœ‰çš„ä¸»è¦èšç„¦äºç‹­çª„è§„åˆ’æˆ–å•ä¸€æ™ºèƒ½ä½“æ¨ç†çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒSPIN-Benchç»“åˆäº†PDDLä»»åŠ¡ã€ç«æŠ€æ£‹ç±»æ¸¸æˆã€åˆä½œå¡ç‰Œæ¸¸æˆå’Œå¤šæ™ºèƒ½ä½“è°ˆåˆ¤åœºæ™¯ï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­è¿›è¡Œè¯„ä¼°ã€‚æ¡†æ¶æ—¢åŒ…æ‹¬åŸºå‡†æµ‹è¯•ï¼Œä¹ŸåŒ…æ‹¬æ¨¡æ‹Ÿå’Œè¯„ä¼°å¤šç§ç¤¾äº¤è®¾ç½®çš„åœºæ‰€ï¼Œä»¥æµ‹è¯•äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“çš„æ¨ç†å’Œæˆ˜ç•¥è¡Œä¸ºã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡å½“ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸºæœ¬äº‹å®æ£€ç´¢å’ŒçŸ­æœŸè§„åˆ’æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦æ·±åº¦å¤šè·³æ¨ç†å’Œç¤¾äº¤åè°ƒçš„ä»»åŠ¡ä¸­ä»å­˜åœ¨æ˜¾è‘—æ€§èƒ½ç“¶é¢ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPIN-Benchæ˜¯ä¸€ç§æ–°å‹çš„å¤šé¢†åŸŸè¯„ä¼°æ–¹æ³•ï¼Œæ—¨åœ¨æµ‹é‡æ™ºèƒ½çš„æˆ˜ç•¥è§„åˆ’å’Œç¤¾äº¤æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦èšç„¦äºç‹­çª„è§„åˆ’æˆ–å•ä¸€æ™ºèƒ½ä½“æ¨ç†ï¼Œè€ŒSPIN-Benchç»“åˆäº†å¤šç§ä»»åŠ¡åœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>SPIN-BenchåŒ…æ‹¬åŸºå‡†æµ‹è¯•å’Œæ¨¡æ‹Ÿç¤¾äº¤è®¾ç½®çš„åœºæ‰€ï¼Œæ¨¡æ‹Ÿå¤šç§ç¤¾äº¤è®¾ç½®ä»¥æµ‹è¯•AIæ™ºèƒ½ä½“çš„æ¨ç†å’Œæˆ˜ç•¥è¡Œä¸ºã€‚</li>
<li>å½“ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éœ€è¦æ·±åº¦å¤šè·³æ¨ç†å’Œç¤¾äº¤åè°ƒçš„ä»»åŠ¡ä¸­ä»å­˜åœ¨æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>SPIN-Benché€šè¿‡ç³»ç»Ÿåœ°æ”¹å˜åŠ¨ä½œç©ºé—´ã€çŠ¶æ€å¤æ‚æ€§å’Œäº¤äº’æ™ºèƒ½ä½“çš„æ•°é‡æ¥æ¨¡æ‹Ÿå„ç§ç¤¾äº¤è®¾ç½®ã€‚</li>
<li>æˆåŠŸä¸ä»…å–å†³äºæ–¹æ³•å’Œé€æ­¥å†³ç­–åˆ¶å®šï¼Œè¿˜å–å†³äºå¯¹å…¶ä»–ï¼ˆå¯¹æŠ—æˆ–åˆä½œï¼‰å‚ä¸è€…çš„æ¦‚å¿µæ¨æ–­ã€‚</li>
<li>SPIN-Benchä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å‚¬åŒ–å‰‚ï¼ŒåŒ…æ‹¬ç¨³å¥çš„å¤šæ™ºèƒ½ä½“è§„åˆ’ã€ç¤¾äº¤æ¨ç†å’Œäººæœºåä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12349">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47f9b7a1078db759d212bcaa6243aa83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a25042afb63f292282ad1b051554613a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9a0f740f016d61f9ea223552e34db39.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PLM-Efficient-Peripheral-Language-Models-Hardware-Co-Designed-for-Ubiquitous-Computing"><a href="#PLM-Efficient-Peripheral-Language-Models-Hardware-Co-Designed-for-Ubiquitous-Computing" class="headerlink" title="PLM: Efficient Peripheral Language Models Hardware-Co-Designed for   Ubiquitous Computing"></a>PLM: Efficient Peripheral Language Models Hardware-Co-Designed for   Ubiquitous Computing</h2><p><strong>Authors:Cheng Deng, Luoyang Sun, Jiwen Jiang, Yongcheng Zeng, Xinjian Wu, Wenxin Zhao, Qingfa Xiao, Jiachuan Wang, Lei Chen, Lionel M. Ni, Haifeng Zhang, Jun Wang</strong></p>
<p>While scaling laws have been continuously validated in large language models (LLMs) with increasing model parameters, the inherent tension between the inference demands of LLMs and the limited resources of edge devices poses a critical challenge to the development of edge intelligence. Recently, numerous small language models have emerged, aiming to distill the capabilities of LLMs into smaller footprints. However, these models often retain the fundamental architectural principles of their larger counterparts, still imposing considerable strain on the storage and bandwidth capacities of edge devices. In this paper, we introduce the PLM, a Peripheral Language Model, developed through a co-design process that jointly optimizes model architecture and edge system constraints. The PLM utilizes a Multi-head Latent Attention mechanism and employs the squared ReLU activation function to encourage sparsity, thereby reducing peak memory footprint during inference. During training, we collect and reorganize open-source datasets, implement a multi-phase training strategy, and empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning rate scheduler. Additionally, we incorporate Reinforcement Learning from Human Feedback (RLHF) by adopting the ARIES preference learning approach. Following a two-phase SFT process, this method yields performance gains of 2% in general tasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel architecture, evaluation results demonstrate that PLM outperforms existing small language models trained on publicly available data while maintaining the lowest number of activated parameters. Furthermore, deployment across various edge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis, validates PLMâ€™s suitability for peripheral applications. The PLM series models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/plm-team/PLM">https://github.com/plm-team/PLM</a>. </p>
<blockquote>
<p>éšç€æ¨¡å‹å‚æ•°çš„å¢åŠ ï¼Œæ¯”ä¾‹å®šå¾‹åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¾—åˆ°äº†æŒç»­éªŒè¯ã€‚ç„¶è€Œï¼Œå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†éœ€æ±‚ä¸è¾¹ç¼˜è®¾å¤‡çš„æœ‰é™èµ„æºä¹‹é—´å­˜åœ¨çš„å›ºæœ‰çŸ›ç›¾ï¼Œç»™è¾¹ç¼˜æ™ºèƒ½çš„å‘å±•å¸¦æ¥äº†å…³é”®æŒ‘æˆ˜ã€‚è¿‘æœŸï¼Œå‡ºç°äº†è®¸å¤šå°è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å°†å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›è½¬åŒ–ä¸ºæ›´å°çš„å ç”¨ç©ºé—´ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¾€å¾€ä¿ç•™å…¶å¤§å‹å¯¹åº”æ¨¡å‹çš„åŸºæœ¬æ¶æ„åŸåˆ™ï¼Œä»ç„¶ç»™è¾¹ç¼˜è®¾å¤‡çš„å­˜å‚¨å’Œå¸¦å®½å®¹é‡å¸¦æ¥ç›¸å½“å¤§çš„å‹åŠ›ã€‚</p>
</blockquote>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§å‘¨è¾¹è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡è”åˆä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œè¾¹ç¼˜ç³»ç»Ÿçº¦æŸçš„ååŒè®¾è®¡è¿‡ç¨‹è€Œå¼€å‘ã€‚PLMåˆ©ç”¨å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶é‡‡ç”¨å¹³æ–¹ReLUæ¿€æ´»å‡½æ•°æ¥ä¿ƒè¿›ç¨€ç–æ€§ï¼Œä»è€Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å³°å€¼å†…å­˜å ç”¨ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†å’Œé‡ç»„äº†å¼€æºæ•°æ®é›†ï¼Œå®æ–½äº†å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå¹¶å¯¹é¢„çƒ­ç¨³å®šè¡°å‡æ’å®šï¼ˆWSDCï¼‰å­¦ä¹ ç‡è°ƒåº¦å™¨è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡é‡‡ç”¨ARIESåå¥½å­¦ä¹ æ–¹æ³•ï¼Œèå…¥äº†äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚ç»è¿‡ä¸¤é˜¶æ®µçš„SFTè¿‡ç¨‹åï¼Œè¯¥æ–¹æ³•åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šè·å¾—äº†2%çš„æ€§èƒ½æå‡ï¼Œåœ¨GSM8Kä»»åŠ¡ä¸Šè·å¾—äº†9%çš„æå‡ï¼Œåœ¨ç¼–ç ä»»åŠ¡ä¸Šè·å¾—äº†11%çš„æå‡ã€‚</p>
<p>é™¤äº†å…¶æ–°é¢–çš„æ¶æ„å¤–ï¼Œè¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒPLMåœ¨å…¬å¼€æ•°æ®ä¸Šè®­ç»ƒçš„ç°æœ‰å°è¯­è¨€æ¨¡å‹ç›¸æ¯”è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†æœ€ä½çš„æ´»åŠ¨å‚æ•°æ•°é‡ã€‚æ­¤å¤–ï¼Œåœ¨å„ç§è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ï¼ŒåŒ…æ‹¬æ¶ˆè´¹çº§GPUã€æ‰‹æœºå’ŒRaspberry Pisï¼ŒéªŒè¯äº†PLMé€‚ç”¨äºå‘¨è¾¹åº”ç”¨ã€‚PLMç³»åˆ—æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/plm-team/PLM">https://github.com/plm-team/PLM</a>å…¬å¼€è·å–ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12167v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ¨ç†éœ€æ±‚ä¸æœ‰é™èµ„æºä¹‹é—´å­˜åœ¨å›ºæœ‰çŸ›ç›¾ï¼Œå°å‹è¯­è¨€æ¨¡å‹åº”è¿è€Œç”Ÿï¼Œè¯•å›¾å°†LLMçš„èƒ½åŠ›æµ“ç¼©åˆ°æ›´å°çš„è§„æ¨¡ä¸­ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶ä¿ç•™äº†å¯¹è¾¹ç¼˜è®¾å¤‡çš„å­˜å‚¨å’Œå¸¦å®½èƒ½åŠ›çš„éœ€æ±‚è¾ƒå¤§çš„åŸºæœ¬æ¶æ„åŸåˆ™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œä»‹ç»äº†ä¸€ç§å‘¨è¾¹è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ï¼Œé€šè¿‡è”åˆä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œè¾¹ç¼˜ç³»ç»Ÿçº¦æŸçš„ååŒè®¾è®¡è¿‡ç¨‹å¼€å‘è€Œæˆã€‚PLMé‡‡ç”¨å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶å’Œå¹³æ–¹ReLUæ¿€æ´»å‡½æ•°æ¥é¼“åŠ±ç¨€ç–æ€§ï¼Œé™ä½æ¨ç†è¿‡ç¨‹ä¸­çš„å³°å€¼å†…å­˜å ç”¨ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ”¶é›†å’Œç»„ç»‡å¼€æºæ•°æ®é›†ã€å®æ–½å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ä»¥åŠè°ƒæŸ¥WSDCå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå¹¶ç»“åˆé‡‡ç”¨ARIESåå¥½å­¦ä¹ æ–¹æ³•çš„å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ï¼ŒPLMåœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šå®ç°äº†2%çš„æ€§èƒ½æå‡ï¼Œåœ¨GSM8Kä»»åŠ¡ä¸Šå®ç°äº†9%çš„æå‡ï¼Œåœ¨ç¼–ç ä»»åŠ¡ä¸Šå®ç°äº†11%çš„æå‡ã€‚åŒæ—¶ï¼ŒPLMç³»åˆ—æ¨¡å‹å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå­˜åœ¨æ¨ç†éœ€æ±‚ä¸æœ‰é™èµ„æºçš„çŸ›ç›¾ã€‚</li>
<li>å‘¨è¾¹è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰é€šè¿‡è”åˆä¼˜åŒ–æ¨¡å‹æ¶æ„å’Œè¾¹ç¼˜ç³»ç»Ÿçº¦æŸè¿›è¡Œå¼€å‘ã€‚</li>
<li>PLMé‡‡ç”¨å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶å’Œå¹³æ–¹ReLUæ¿€æ´»å‡½æ•°ä»¥é™ä½å†…å­˜å ç”¨ã€‚</li>
<li>PLMé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨æå‡æ€§èƒ½ã€‚</li>
<li>PLMç»“åˆå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æé«˜ä»»åŠ¡è¡¨ç°ã€‚</li>
<li>PLMåœ¨å¤šç§è¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œäº†éƒ¨ç½²ï¼ŒåŒ…æ‹¬æ¶ˆè´¹è€…çº§GPUã€æ‰‹æœºå’ŒRaspberry Piç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-142940cff76047f8cac441ddd8d084d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-957e332689b845f2b04dae14e004b300.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-297c05f6376ec4ad4815db42b33ccff5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0ca0bab6691aa042149c85b6c9d8c75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13c6d12b861dd39abed42fa8ae9baf0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4a5e9a73631efc6ab474d2cf9a3a6c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3f4ebb7fcdb92336312466784ada57e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0401b5fbcdd6d35fd9814e92318f1a5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LAG-MMLU-Benchmarking-Frontier-LLM-Understanding-in-Latvian-and-Giriama"><a href="#LAG-MMLU-Benchmarking-Frontier-LLM-Understanding-in-Latvian-and-Giriama" class="headerlink" title="LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama"></a>LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama</h2><p><strong>Authors:Naome A. Etori, Kevin Lu, Randu Karisa, Arturs Kanepajs</strong></p>
<p>As large language models (LLMs) rapidly advance, evaluating their performance is critical. LLMs are trained on multilingual data, but their reasoning abilities are mainly evaluated using English datasets. Hence, robust evaluation frameworks are needed using high-quality non-English datasets, especially low-resource languages (LRLs). This study evaluates eight state-of-the-art (SOTA) LLMs on Latvian and Giriama using a Massive Multitask Language Understanding (MMLU) subset curated with native speakers for linguistic and cultural relevance. Giriama is benchmarked for the first time. Our evaluation shows that OpenAIâ€™s o1 model outperforms others across all languages, scoring 92.8% in English, 88.8% in Latvian, and 70.8% in Giriama on 0-shot tasks. Mistral-large (35.6%) and Llama-70B IT (41%) have weak performance, on both Latvian and Giriama. Our results underscore the need for localized benchmarks and human evaluations in advancing cultural AI contextualization. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹å…¶æ€§èƒ½è¿›è¡Œè¯„ä¼°è‡³å…³é‡è¦ã€‚å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯åœ¨å¤šè¯­è¨€æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œä½†å®ƒä»¬çš„æ¨ç†èƒ½åŠ›ä¸»è¦ä½¿ç”¨è‹±è¯­æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚å› æ­¤ï¼Œéœ€è¦ä½¿ç”¨é«˜è´¨é‡çš„éè‹±è¯­æ•°æ®é›†æ¥æ„å»ºç¨³å¥çš„è¯„ä¼°æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä½èµ„æºè¯­è¨€ï¼ˆLRLsï¼‰ã€‚æœ¬ç ”ç©¶ä½¿ç”¨ç”±æ¯è¯­è€…ç­›é€‰çš„å·¨é‡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰å­é›†ï¼Œé’ˆå¯¹æ‹‰è„±ç»´äºšè¯­å’Œå‰é‡Œé©¬è¯­ï¼Œå¯¹å…«ç§æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯¥å­é›†è€ƒè™‘äº†è¯­è¨€å’Œæ–‡åŒ–çš„ç›¸å…³æ€§ã€‚å‰é‡Œé©¬è¯­æ˜¯é¦–æ¬¡è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒOpenAIçš„o1æ¨¡å‹åœ¨æ‰€æœ‰è¯­è¨€ä¸­çš„è¡¨ç°å‡è¶…è¿‡å…¶ä»–æ¨¡å‹ï¼Œåœ¨é›¶å°„å‡»ä»»åŠ¡ä¸­ï¼Œè‹±è¯­å¾—åˆ†ä¸º92.8%ï¼Œæ‹‰è„±ç»´äºšè¯­å¾—åˆ†ä¸º88.8%ï¼Œå‰é‡Œé©¬è¯­å¾—åˆ†ä¸º70.8%ã€‚Mistral-largeï¼ˆ35.6%ï¼‰å’ŒLlama-70B ITï¼ˆ41%ï¼‰åœ¨æ‹‰è„±ç»´äºšè¯­å’Œå‰é‡Œé©¬è¯­ä¸Šçš„è¡¨ç°å‡è¾ƒå¼±ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨è¿›è¡Œæ–‡åŒ–äººå·¥æ™ºèƒ½è¯­å¢ƒåŒ–æ¨è¿›æ—¶ï¼Œéœ€è¦æœ¬åœ°åŒ–çš„åŸºå‡†æµ‹è¯•å’Œäººå·¥è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11911v2">PDF</a> Accepted at NoDaLiDa&#x2F;Baltic-HLT 2025.   <a target="_blank" rel="noopener" href="https://hdl.handle.net/10062/107190">https://hdl.handle.net/10062/107190</a></p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹å…¶æ€§èƒ½è¿›è¡Œè¯„ä¼°è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶ä½¿ç”¨æœ¬æ—è¯­è€…æ•´ç†çš„å¤§å‹å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰å­é›†ï¼Œå¯¹æ‹‰è„±ç»´äºšè¯­å’ŒåŸºé‡Œäºšç›è¯­è¿™ä¸¤ç§è¯­è¨€çš„å…«ç§æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒOpenAIçš„o1æ¨¡å‹åœ¨æ‰€æœ‰è¯­è¨€ä¸­çš„è¡¨ç°æœ€ä½³ï¼Œé›¶æ ·æœ¬ä»»åŠ¡çš„å¾—åˆ†ç‡åˆ†åˆ«ä¸ºè‹±è¯­92.8%ã€æ‹‰è„±ç»´äºšè¯­88.8%ã€åŸºé‡Œäºšç›è¯­70.8%ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†é’ˆå¯¹ç‰¹å®šåŒºåŸŸçš„æ ‡å‡†åˆ¶å®šå’Œäººç±»è¯„ä¼°åœ¨æ¨åŠ¨æ–‡åŒ–äººå·¥æ™ºèƒ½è¯­å¢ƒåŒ–ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹å¤šè¯­è¨€æ•°æ®çš„æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†åŒ…æ‹¬æ‹‰è„±ç»´äºšè¯­å’ŒåŸºé‡Œäºšç›è¯­åœ¨å†…çš„éè‹±è¯­æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>OpenAIçš„o1æ¨¡å‹åœ¨å¤šç§è¯­è¨€ä¸­è¡¨ç°æœ€ä½³ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸Šã€‚</li>
<li>Mistral-largeå’ŒLlama-70B ITåœ¨æ‹‰è„±ç»´äºšè¯­å’ŒåŸºé‡Œäºšç›è¯­ä¸Šçš„è¡¨ç°è¾ƒå¼±ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æœ¬åœ°åŒ–åŸºå‡†æµ‹è¯•å’Œäººç±»è¯„ä¼°åœ¨æ¨åŠ¨æ–‡åŒ–äººå·¥æ™ºèƒ½è¯­å¢ƒåŒ–ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>LLMsçš„æ¨ç†èƒ½åŠ›ä¸»è¦é€šè¿‡è‹±è¯­æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œä½†å…¶åœ¨å…¶ä»–è¯­è¨€ä¸­çš„è¡¨ç°å¯èƒ½æœ‰æ‰€ä¸åŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b05aa50cd9f2014e576c3f871c95243e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9720102a987116c69b330ee740bfce2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e24ec3032530e7e84a43483f4661b31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c3c76c0fe74d75f789970e7de786b0d2.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-96f22e362929099cfc1528b65e5c71a8.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-20  Aligning Multimodal LLM with Human Preference A Survey
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b1b8c04a90717be36da770e69bba9395.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-18  One-Step Residual Shifting Diffusion for Image Super-Resolution via   Distillation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
