<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-03-20  MusicInfuser Making Video Diffusion Listen and Dance">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5d6a422a9a5048b5eea40aafa607f945.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-20-更新"><a href="#2025-03-20-更新" class="headerlink" title="2025-03-20 更新"></a>2025-03-20 更新</h1><h2 id="MusicInfuser-Making-Video-Diffusion-Listen-and-Dance"><a href="#MusicInfuser-Making-Video-Diffusion-Listen-and-Dance" class="headerlink" title="MusicInfuser: Making Video Diffusion Listen and Dance"></a>MusicInfuser: Making Video Diffusion Listen and Dance</h2><p><strong>Authors:Susung Hong, Ira Kemelmacher-Shlizerman, Brian Curless, Steven M. Seitz</strong></p>
<p>We introduce MusicInfuser, an approach for generating high-quality dance videos that are synchronized to a specified music track. Rather than attempting to design and train a new multimodal audio-video model, we show how existing video diffusion models can be adapted to align with musical inputs by introducing lightweight music-video cross-attention and a low-rank adapter. Unlike prior work requiring motion capture data, our approach fine-tunes only on dance videos. MusicInfuser achieves high-quality music-driven video generation while preserving the flexibility and generative capabilities of the underlying models. We introduce an evaluation framework using Video-LLMs to assess multiple dimensions of dance generation quality. The project page and code are available at <a target="_blank" rel="noopener" href="https://susunghong.github.io/MusicInfuser">https://susunghong.github.io/MusicInfuser</a>. </p>
<blockquote>
<p>我们介绍了MusicInfuser，这是一种生成高质量舞蹈视频的方法，该视频与指定的音乐曲目同步。我们并没有尝试设计和训练新的多模态音频视频模型，而是展示了如何通过对现有视频扩散模型进行微调，通过引入轻量级的音乐视频交叉注意力和低阶适配器来适应音乐输入。与先前需要动作捕捉数据的工作不同，我们的方法仅在舞蹈视频上进行微调。MusicInfuser实现了高质量的音乐驱动视频生成，同时保留了基础模型的灵活性和生成能力。我们引入了一个使用Video-LLMs的评估框架，从多个维度评估舞蹈生成的质量。项目页面和代码可在<a target="_blank" rel="noopener" href="https://susunghong.github.io/MusicInfuser%E8%AE%BF%E9%97%AE%E3%80%82">https://susunghong.github.io/MusicInfuser访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14505v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://susunghong.github.io/MusicInfuser">https://susunghong.github.io/MusicInfuser</a></p>
<p><strong>Summary</strong></p>
<p>音乐Infuser方法生成高质量舞蹈视频，该视频同步特定音乐曲目。此方法通过引入轻量级音乐视频交叉注意力与低阶适配器，展示了现有视频扩散模型如何适应音乐输入。与需要动作捕捉数据的早期工作不同，我们的方法仅对舞蹈视频进行微调。MusicInfuser实现了高质量的音乐驱动视频生成，同时保留了底层模型的灵活性和生成能力。同时利用Video-LLMs建立评价体系对舞蹈生成质量进行了多维评价。项目和代码在网站可下载和查阅。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MusicInfuser是一种生成高质量舞蹈视频的方法，能够同步特定音乐曲目。</li>
<li>该方法通过引入轻量级音乐视频交叉注意力机制，使得现有视频扩散模型能够适应音乐输入。</li>
<li>低阶适配器的引入提高了模型的性能。</li>
<li>与早期需要动作捕捉数据的方法不同，MusicInfuser仅对舞蹈视频进行微调。</li>
<li>MusicInfuser实现了高质量的音乐驱动视频生成。</li>
<li>该方法保留了底层模型的灵活性和生成能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14505">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d5cb6655b3b12cb2c213a494f0f2e52c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11f8bd2bc6e0e6f8e13fade76ba18e28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2460722a94cce0a40f890e8a80a0862a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2137c940f8a6713e315ce582528cc135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-638208498e819940ab833bee66feedd2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Temporal-Consistency-for-LLM-Reasoning-Process-Error-Identification"><a href="#Temporal-Consistency-for-LLM-Reasoning-Process-Error-Identification" class="headerlink" title="Temporal Consistency for LLM Reasoning Process Error Identification"></a>Temporal Consistency for LLM Reasoning Process Error Identification</h2><p><strong>Authors:Jiacheng Guo, Yue Wu, Jiahao Qiu, Kaixuan Huang, Xinzhe Juan, Ling Yang, Mengdi Wang</strong></p>
<p>Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B&#x2F;8B distilled models to outperform all 70B&#x2F;72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/jcguo123/Temporal-Consistency">https://github.com/jcguo123/Temporal-Consistency</a> </p>
<blockquote>
<p>验证对于有效的数学推理至关重要。我们提出了一种新的时间一致性方法，验证者可以根据之前的评估结果迭代地调整自己的判断。不同于一轮验证或多模型辩论方法，我们的方法利用一系列自我反思行为的连贯性来提高验证的准确性。在多样化的数学过程误差识别基准测试（Mathcheck、ProcessBench和PRM800K）上的经验评估表明，我们的方法在基准方法上实现了性能上的持续改进。当应用于最新的DeepSeek R1蒸馏模型时，我们的方法表现出强大的性能，使7B&#x2F;8B蒸馏模型在ProcessBench上优于所有70B&#x2F;72B模型和GPT-4o。值得注意的是，使用我们方法的14B蒸馏模型实现了与Deepseek-R1相当的性能。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/jcguo123/Temporal-Consistency">https://github.com/jcguo123/Temporal-Consistency</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14495v1">PDF</a> </p>
<p><strong>Summary</strong>：一种基于时间一致性的新验证方法能提升数学推理的有效性。它通过迭代精细验证过程提高验证的准确性，且相比一次性的验证或多模型辩论方法更具优势。该方法在多种数学过程误差识别基准测试上的表现均优于基准方法，并且对于最新DeepSeek R1蒸馏模型也有良好的表现。我们的代码已发布在<a target="_blank" rel="noopener" href="https://github.com/jcguo123/Temporal-Consistency%E4%BE%9B%E5%85%AC%E4%BC%97%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/jcguo123/Temporal-Consistency供公众访问。</a></p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>验证在数学推理中的重要性，并介绍了一种新的基于时间一致性的验证方法。</li>
<li>此方法通过迭代精细验证过程提高验证的准确性。</li>
<li>与其他验证方法相比，如一次性验证或多模型辩论，该方法具有优势。</li>
<li>在多种数学过程误差识别基准测试上，该方法表现优于其他方法。</li>
<li>该方法与最新DeepSeek R1蒸馏模型结合表现出良好的性能。</li>
<li>通过使用此方法，蒸馏的14B模型性能可与Deepseek-R1相媲美。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14495">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-864435f65ba915c9c421c72f02dcaca9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9f347cea6136be5e6d5647dfe7bd089.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6edba0c74726ea6a8f69b51e0be5c298.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02ad2c1bead1643cdbe57dd22a2df711.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49904da6b786b168eef1f756be2bbffd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fa6a0677d295b614a48c6f59f27860f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03907bb3d4660fd29148acbe611707e7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DAPO-An-Open-Source-LLM-Reinforcement-Learning-System-at-Scale"><a href="#DAPO-An-Open-Source-LLM-Reinforcement-Learning-System-at-Scale" class="headerlink" title="DAPO: An Open-Source LLM Reinforcement Learning System at Scale"></a>DAPO: An Open-Source LLM Reinforcement Learning System at Scale</h2><p><strong>Authors:Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, Mingxuan Wang</strong></p>
<p>Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the $\textbf{D}$ecoupled Clip and $\textbf{D}$ynamic s$\textbf{A}$mpling $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{DAPO}$) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL. </p>
<blockquote>
<p>推理扩展赋予大型语言模型前所未有的推理能力，强化学习是激发复杂推理的核心技术。然而，前沿推理大型语言模型的关键技术细节被隐藏（例如在OpenAI o1博客和DeepSeek R1技术报告中），因此社区难以重现其强化学习训练结果。我们提出了解耦剪辑和动态采样策略优化（DAPO）算法，并完全开源了一个先进的大型强化学习系统，该系统使用Qwen2.5-30B基础模型在AIME 2024上取得了50分的成绩。不同于之前隐瞒训练细节的研究，我们介绍了算法四个关键技术，使大规模语言模型强化学习取得成功。此外，我们在verl框架上开源了训练代码，以及精心策划和处理的数据集。我们开源系统的这些组件提高了可重复性，支持未来在大型语言模型强化学习领域的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14476v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://dapo-sia.github.io/">https://dapo-sia.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>推理大模型通过强化学习提升能力，但核心细节被隐藏。我们提出DAPO算法并开源一个先进的大规模RL系统，使用Qwen2.5-32B基础模型在AIME 2024上获得50分。我们的算法包含四个关键技术，促进大规模LLM RL的成功。同时开源训练代码和数据处理集，增强可复制性和支持未来研究。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>推理大模型利用强化学习提升能力。</li>
<li>当前先进的大模型细节被隐藏，社区难以复制其训练结果。</li>
<li>提出DAPO算法并成功应用于大规模LLM RL。</li>
<li>DAPO算法包含四个关键技术，有助于大规模LLM RL的成功。</li>
<li>开源训练代码和数据处理集，增强可复制性和支持未来研究。</li>
<li>使用Qwen2.5-32B基础模型在AIME 2024上获得50分的高表现。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14476">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2a5f5362e301bf903157b5c79231bd4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db16af696ffdcd2c5766dcd2187d7ed1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c055e4050baf073fb00ea05bd12951f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-764ab49e819faaf000443b4d359989c8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Benchmarking-community-drug-response-prediction-models-datasets-models-tools-and-metrics-for-cross-dataset-generalization-analysis"><a href="#Benchmarking-community-drug-response-prediction-models-datasets-models-tools-and-metrics-for-cross-dataset-generalization-analysis" class="headerlink" title="Benchmarking community drug response prediction models: datasets,   models, tools, and metrics for cross-dataset generalization analysis"></a>Benchmarking community drug response prediction models: datasets,   models, tools, and metrics for cross-dataset generalization analysis</h2><p><strong>Authors:Alexander Partin, Priyanka Vasanthakumari, Oleksandr Narykov, Andreas Wilke, Natasha Koussa, Sara E. Jones, Yitan Zhu, Jamie C. Overbeek, Rajeev Jain, Gayara Demini Fernando, Cesar Sanchez-Villalobos, Cristina Garcia-Cardona, Jamaludin Mohd-Yusof, Nicholas Chia, Justin M. Wozniak, Souparno Ghosh, Ranadip Pal, Thomas S. Brettin, M. Ryan Weil, Rick L. Stevens</strong></p>
<p>Deep learning (DL) and machine learning (ML) models have shown promise in drug response prediction (DRP), yet their ability to generalize across datasets remains an open question, raising concerns about their real-world applicability. Due to the lack of standardized benchmarking approaches, model evaluations and comparisons often rely on inconsistent datasets and evaluation criteria, making it difficult to assess true predictive capabilities. In this work, we introduce a benchmarking framework for evaluating cross-dataset prediction generalization in DRP models. Our framework incorporates five publicly available drug screening datasets, six standardized DRP models, and a scalable workflow for systematic evaluation. To assess model generalization, we introduce a set of evaluation metrics that quantify both absolute performance (e.g., predictive accuracy across datasets) and relative performance (e.g., performance drop compared to within-dataset results), enabling a more comprehensive assessment of model transferability. Our results reveal substantial performance drops when models are tested on unseen datasets, underscoring the importance of rigorous generalization assessments. While several models demonstrate relatively strong cross-dataset generalization, no single model consistently outperforms across all datasets. Furthermore, we identify CTRPv2 as the most effective source dataset for training, yielding higher generalization scores across target datasets. By sharing this standardized evaluation framework with the community, our study aims to establish a rigorous foundation for model comparison, and accelerate the development of robust DRP models for real-world applications. </p>
<blockquote>
<p>深度学习（DL）和机器学习（ML）模型在药物反应预测（DRP）中显示出潜力，但它们在数据集之间的泛化能力仍然是一个悬而未决的问题，这引发了人们对它们在现实世界中的适用性的担忧。由于缺乏标准化的基准评估方法，模型评估与比较通常依赖于不一致的数据集和评估标准，使得难以评估真实的预测能力。在这项工作中，我们介绍了一个用于评估DRP模型在跨数据集预测中泛化能力的基准测试框架。我们的框架包含了五个公开的药物筛查数据集、六个标准化的DRP模型，以及一个可扩展的系统评估工作流程。为了评估模型的泛化能力，我们引入了一套评估指标，这些指标既衡量绝对性能（例如跨数据集的预测精度），又衡量相对性能（例如与内部数据集结果相比的性能下降），从而实现对模型可迁移性的更全面的评估。我们的研究结果表明，当模型在未见过的数据集上进行测试时，性能出现了大幅下降，这突显了严格评估泛化能力的重要性。虽然有几个模型表现出相对较强的跨数据集泛化能力，但没有单一模型在所有数据集上始终表现最佳。此外，我们确定CTRPv2为最有效的训练源数据集，其在目标数据集的泛化得分较高。通过与社区共享这个标准化的评估框架，我们的研究旨在建立严格的模型比较基础，并加速开发用于实际应用的稳健DRP模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14356v1">PDF</a> 18 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个用于评估药物反应预测模型跨数据集预测泛化能力的基准测试框架。该框架包含五个公开药物筛查数据集、六个标准化的药物反应预测模型以及一个可系统评估的工作流程。通过引入一系列评估指标，对模型的绝对性能和相对性能进行了量化评估，揭示了模型在实际应用中的迁移能力的重要性。研究结果表明，模型在未见数据集上的性能有所下降，而CTRPv2数据集在训练中最具泛化效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习（DL）和机器学习（ML）模型在药物反应预测（DRP）中表现出潜力，但其在不同数据集的泛化能力仍存在疑问。</li>
<li>缺乏标准化的基准测试方法，使得模型评估与比较依赖于不一致的数据集和评估标准，难以评估其真正的预测能力。</li>
<li>引入的基准测试框架包含五个公开药物筛查数据集和六个标准化的DRP模型，以评估跨数据集的预测泛化能力。</li>
<li>通过一系列评估指标，对模型的绝对性能和相对性能进行了量化，以更全面地评估模型的迁移能力。</li>
<li>研究发现，在未见数据集上测试的模型性能显著下降，突显了严格泛化评估的重要性。</li>
<li>虽然有多个模型表现出相对较强的跨数据集泛化能力，但没有单一模型在所有数据集上始终表现最佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14356">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3b1a1d63ea6804896a5aca8fed61d9d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0e203eae926765cb825f789e0aaa757.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a28394ce2017ea0c17d45b52e3fbcd2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5629d4f92f597bad73b9883ed4568115.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-054730760ce1bff49df7d0cb30eb34ff.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VEGGIE-Instructional-Editing-and-Reasoning-Video-Concepts-with-Grounded-Generation"><a href="#VEGGIE-Instructional-Editing-and-Reasoning-Video-Concepts-with-Grounded-Generation" class="headerlink" title="VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded   Generation"></a>VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded   Generation</h2><p><strong>Authors:Shoubin Yu, Difan Liu, Ziqiao Ma, Yicong Hong, Yang Zhou, Hao Tan, Joyce Chai, Mohit Bansal</strong></p>
<p>Recent video diffusion models have enhanced video editing, but it remains challenging to handle instructional editing and diverse tasks (e.g., adding, removing, changing) within a unified framework. In this paper, we introduce VEGGIE, a Video Editor with Grounded Generation from Instructions, a simple end-to-end framework that unifies video concept editing, grounding, and reasoning based on diverse user instructions. Specifically, given a video and text query, VEGGIE first utilizes an MLLM to interpret user intentions in instructions and ground them to the video contexts, generating frame-specific grounded task queries for pixel-space responses. A diffusion model then renders these plans and generates edited videos that align with user intent. To support diverse tasks and complex instructions, we employ a curriculum learning strategy: first aligning the MLLM and video diffusion model with large-scale instructional image editing data, followed by end-to-end fine-tuning on high-quality multitask video data. Additionally, we introduce a novel data synthesis pipeline to generate paired instructional video editing data for model training. It transforms static image data into diverse, high-quality video editing samples by leveraging Image-to-Video models to inject dynamics. VEGGIE shows strong performance in instructional video editing with different editing skills, outperforming the best instructional baseline as a versatile model, while other models struggle with multi-tasking. VEGGIE also excels in video object grounding and reasoning segmentation, where other baselines fail. We further reveal how the multiple tasks help each other and highlight promising applications like zero-shot multimodal instructional and in-context video editing. </p>
<blockquote>
<p>最近的视频扩散模型已经提高了视频编辑的能力，但在统一框架内处理指令编辑和多样化任务（例如添加、删除、更改）仍然具有挑战性。在本文中，我们介绍了VEGGIE，一个基于指令的接地生成视频编辑器，它是一个简单端到端的框架，统一了视频概念编辑、接地和基于多样用户指令的推理。具体来说，给定一个视频和文本查询，VEGGIE首先利用MLLM来解释用户意图的指令并将其接地到视频上下文，为像素空间响应生成特定帧的接地任务查询。然后，扩散模型根据这些计划渲染并生成符合用户意图的视频。为了支持多样化的任务和复杂的指令，我们采用了课程学习策略：首先使用大规模的指令图像编辑数据对齐MLLM和视频扩散模型，然后在高质量的多任务视频数据上进行端到端的微调。此外，我们还引入了一种新的数据合成管道，以生成用于模型训练的一对一指令视频编辑数据。它通过利用图像到视频的模型注入动态性，将静态图像数据转换为多样化、高质量的视频编辑样本。VEGGIE在具有不同编辑技能的指令视频编辑方面表现出强大的性能，作为一个通用模型，它超越了最佳指令基线，而其他模型在多任务处理方面则表现挣扎。VEGGIE在视频对象接地和推理分割方面也表现出色，而其他基线则未能达到这一水平。我们还进一步揭示了多个任务是如何相互帮助的，并强调了有前景的应用，如零样本多模式指令和在上下文中的视频编辑。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14350v1">PDF</a> First three authors contributed equally. Project page:   <a target="_blank" rel="noopener" href="https://veggie-gen.github.io/">https://veggie-gen.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了VEGGIE，一个基于指令的统合视频编辑框架。该框架利用多模态语言模型（MLLM）理解用户指令并将其与视频内容对应起来，再通过扩散模型生成符合用户意图的编辑视频。为支持多样化的任务和复杂的指令，本文采用了一种课程学习策略，并引入了一种新的数据合成管道来生成训练模型所需的视频编辑数据。VEGGIE在指令视频编辑、视频对象定位和推理分割等多个任务上表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VEGGIE是一个基于指令的视频编辑框架，可统一处理视频概念编辑、定位与推理。</li>
<li>利用MLLM解读用户指令并将其与视频内容对应起来，生成特定帧的任务查询。</li>
<li>扩散模型根据这些计划生成符合用户意图的编辑视频。</li>
<li>采用课程学习策略支持多样任务和复杂指令的学习。</li>
<li>引入新的数据合成管道，将静态图像数据转化为高质量的视频编辑样本。</li>
<li>VEGGIE在指令视频编辑、视频对象定位和推理分割等任务上表现优秀。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14350">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0a63235a8dedb827a42d8917edfe5313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b1e9cf510817cc414098bcbf8e2b667.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b107c4552a56b5cb3d171cd65e6210a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e900a0068210883ca8d7217aaba69d7b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="JuDGE-Benchmarking-Judgment-Document-Generation-for-Chinese-Legal-System"><a href="#JuDGE-Benchmarking-Judgment-Document-Generation-for-Chinese-Legal-System" class="headerlink" title="JuDGE: Benchmarking Judgment Document Generation for Chinese Legal   System"></a>JuDGE: Benchmarking Judgment Document Generation for Chinese Legal   System</h2><p><strong>Authors:Weihang Su, Baoqing Yue, Qingyao Ai, Yiran Hu, Jiaqi Li, Changyue Wang, Kaiyuan Zhang, Yueyue Wu, Yiqun Liu</strong></p>
<p>This paper introduces JuDGE (Judgment Document Generation Evaluation), a novel benchmark for evaluating the performance of judgment document generation in the Chinese legal system. We define the task as generating a complete legal judgment document from the given factual description of the case. To facilitate this benchmark, we construct a comprehensive dataset consisting of factual descriptions from real legal cases, paired with their corresponding full judgment documents, which serve as the ground truth for evaluating the quality of generated documents. This dataset is further augmented by two external legal corpora that provide additional legal knowledge for the task: one comprising statutes and regulations, and the other consisting of a large collection of past judgment documents. In collaboration with legal professionals, we establish a comprehensive automated evaluation framework to assess the quality of generated judgment documents across various dimensions. We evaluate various baseline approaches, including few-shot in-context learning, fine-tuning, and a multi-source retrieval-augmented generation (RAG) approach, using both general and legal-domain LLMs. The experimental results demonstrate that, while RAG approaches can effectively improve performance in this task, there is still substantial room for further improvement. All the codes and datasets are available at: <a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE">https://github.com/oneal2000/JuDGE</a>. </p>
<blockquote>
<p>本文介绍了JuDGE（判决文书生成评估）这一新型的中文法律体系中判决文书生成性能评估基准。我们将任务定义为根据给定的案件事实描述生成完整的法律判决书。为了促进这一基准的构建，我们构建了一个综合数据集，其中包括来自真实法律案件的案情描述及其对应的完整判决书，这些判决书作为评估生成文档质量的真实依据。该数据集通过两个外部法律语料库进行了扩充，为任务提供了额外的法律知识：一个包含法规和条例，另一个则包含大量以往的判决书。我们与法律专业人士合作，建立了一个全面的自动化评估框架，从多个维度评估生成的判决书的质量。我们评估了各种基线方法，包括小样本上下文学习、微调以及多源检索增强生成（RAG）方法，这些方法均使用通用和法律领域的LLM。实验结果表明，虽然RAG方法可以有效提高此任务性能，但仍存在很大的改进空间。所有代码和数据集可在：<a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/oneal2000/JuDGE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14258v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了JuDGE（判决文书生成评估）基准测试，该测试旨在评估在中国法律体系下判决文书生成的性能。任务的定义是根据给定案例的事实描述生成完整的法律判决书。为推进此基准测试，构建了包含真实案例事实描述与其对应的完整判决书的综合数据集，作为评估生成文书质量的依据。此外，还利用两个外部法律语料库提供额外的法律知识。与法律专业人士合作，建立了全面的自动化评估框架，从多个维度评估生成判决书的质量。对包括少量上下文学习、微调以及多源检索增强生成（RAG）方法等基线方法进行了评估，使用了一般和法律领域的LLMs。实验结果表明，RAG方法虽然能有效提升任务性能，但仍存在很大的改进空间。相关代码和数据集可访问于：<a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE%E3%80%82">https://github.com/oneal2000/JuDGE。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了JuDGE基准测试，专注于评估中国法律系统下的判决文书生成性能。</li>
<li>定义了从案例事实描述生成完整判决书的任务。</li>
<li>构建了一个综合数据集，包含真实案例的事实描述和对应的判决书，以及两个外部法律语料库。</li>
<li>与法律专业人士合作，建立了全面的自动化评估框架。</li>
<li>评估了几种基线方法，包括少量上下文学习、微调以及多源检索增强生成方法。</li>
<li>实验结果表明RAG方法能有效提升任务性能，但仍存在改进空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f1b4b233032a53f2f3024e0cb6d1357c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c12c7e7f91bd6666772101cdba937f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4a0d37c3ddd60aee0fb04a037e968a1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CTSAC-Curriculum-Based-Transformer-Soft-Actor-Critic-for-Goal-Oriented-Robot-Exploration"><a href="#CTSAC-Curriculum-Based-Transformer-Soft-Actor-Critic-for-Goal-Oriented-Robot-Exploration" class="headerlink" title="CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented   Robot Exploration"></a>CTSAC: Curriculum-Based Transformer Soft Actor-Critic for Goal-Oriented   Robot Exploration</h2><p><strong>Authors:Chunyu Yang, Shengben Bi, Yihui Xu, Xin Zhang</strong></p>
<p>With the increasing demand for efficient and flexible robotic exploration solutions, Reinforcement Learning (RL) is becoming a promising approach in the field of autonomous robotic exploration. However, current RL-based exploration algorithms often face limited environmental reasoning capabilities, slow convergence rates, and substantial challenges in Sim-To-Real (S2R) transfer. To address these issues, we propose a Curriculum Learning-based Transformer Reinforcement Learning Algorithm (CTSAC) aimed at improving both exploration efficiency and transfer performance. To enhance the robot’s reasoning ability, a Transformer is integrated into the perception network of the Soft Actor-Critic (SAC) framework, leveraging historical information to improve the farsightedness of the strategy. A periodic review-based curriculum learning is proposed, which enhances training efficiency while mitigating catastrophic forgetting during curriculum transitions. Training is conducted on the ROS-Gazebo continuous robotic simulation platform, with LiDAR clustering optimization to further reduce the S2R gap. Experimental results demonstrate the CTSAC algorithm outperforms the state-of-the-art non-learning and learning-based algorithms in terms of success rate and success rate-weighted exploration time. Moreover, real-world experiments validate the strong S2R transfer capabilities of CTSAC. </p>
<blockquote>
<p>随着对高效、灵活机器人探索解决方案的需求不断增加，强化学习（RL）在自主机器人探索领域成为了一种有前途的方法。然而，基于当前的强化学习探索算法常常面临环境推理能力有限、收敛速度慢以及在模拟到真实（S2R）转移中的巨大挑战。为了解决这些问题，我们提出了一种基于课程学习的Transformer强化学习算法（CTSAC），旨在提高探索效率和转移性能。为了增强机器人的推理能力，我们将Transformer集成到Soft Actor-Critic（SAC）框架的感知网络中，利用历史信息来提高策略的长远性。我们提出了一种基于定期评审的课程学习，以提高训练效率，同时减轻课程过渡过程中的灾难性遗忘。训练是在ROS-Gazebo连续机器人仿真平台上进行的，利用激光雷达聚类优化来进一步缩小S2R差距。实验结果表明，CTSAC算法在成功率和加权探索时间成功率方面优于最新的非学习型和基于学习的算法。此外，真实世界实验验证了CTSAC强大的S2R转移能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14254v1">PDF</a> 7pages,7 figures,Thesis received by 2025 ICRA</p>
<p><strong>Summary</strong>：随着对高效灵活机器人探索解决方案的需求不断增加，强化学习（RL）在自主机器人探索领域展现出巨大潜力。然而，当前基于RL的探索算法面临着环境推理能力有限、收敛速度慢以及模拟到现实（S2R）转移挑战等问题。为解决这些问题，提出了一种基于课程学习的Transformer强化学习算法（CTSAC），旨在提高探索效率和转移性能。该算法将Transformer集成到Soft Actor-Critic（SAC）框架的感知网络中，利用历史信息提高策略的长远性。同时，提出了基于周期性评审的课程学习，以提高训练效率并减轻课程过渡时的灾难性遗忘。实验结果表明，CTSAC算法在成功率和加权探索时间方面的表现优于最先进的非学习算法和学习算法。此外，真实世界实验验证了CTSAC强大的S2R转移能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>强化学习在自主机器人探索中具有巨大潜力，但面临环境推理能力有限等挑战。</li>
<li>CTSAC算法通过集成Transformer到SAC框架的感知网络，提高了机器人策略的长远性。</li>
<li>基于周期性评审的课程学习增强了训练效率并减轻了灾难性遗忘。</li>
<li>CTSAC算法在模拟环境中表现出优异的性能，通过ROS-Gazebo连续机器人仿真平台进行了训练。</li>
<li>LiDAR聚类优化进一步减少了S2R差距。</li>
<li>实验结果表明，CTSAC算法在成功率和加权探索时间方面优于其他算法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14254">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-beb371e430fbdb08e9636214cbe5fb08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9669a4c8d4e3a9c80af20ce3104f6db0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d29954792d15164831e236fd9b553be7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be7b224abdcd075735db0b5384816412.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e147d9a5755dd2a078e05da2d558059.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be39c37b60100a22d460f780b880e75e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd4879a3ad8e2341ea3261c857f077d6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models"><a href="#Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models" class="headerlink" title="Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models"></a>Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models</h2><p><strong>Authors:Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang</strong></p>
<p>Vision-language models (VLMs) have advanced reasoning in natural scenes, but their role in medical imaging remains underexplored. Medical reasoning tasks demand robust image analysis and well-justified answers, posing challenges due to the complexity of medical images. Transparency and trustworthiness are essential for clinical adoption and regulatory compliance. We introduce Med-R1, a framework exploring reinforcement learning (RL) to enhance VLMs’ generalizability and trustworthiness in medical reasoning. Leveraging the DeepSeek strategy, we employ Group Relative Policy Optimization (GRPO) to guide reasoning paths via reward signals. Unlike supervised fine-tuning (SFT), which often overfits and lacks generalization, RL fosters robust and diverse reasoning. Med-R1 is evaluated across eight medical imaging modalities: CT, MRI, Ultrasound, Dermoscopy, Fundus Photography, Optical Coherence Tomography (OCT), Microscopy, and X-ray Imaging. Compared to its base model, Qwen2-VL-2B, Med-R1 achieves a 29.94% accuracy improvement and outperforms Qwen2-VL-72B, which has 36 times more parameters. Testing across five question types-modality recognition, anatomy identification, disease diagnosis, lesion grading, and biological attribute analysis Med-R1 demonstrates superior generalization, exceeding Qwen2-VL-2B by 32.06% and surpassing Qwen2-VL-72B in question-type generalization. These findings show that RL improves medical reasoning and enables parameter-efficient models to outperform significantly larger ones. With interpretable reasoning outputs, Med-R1 represents a promising step toward generalizable, trustworthy, and clinically viable medical VLMs. </p>
<blockquote>
<p>视觉语言模型（VLMs）在自然场景推理方面取得了进展，但它们在医学成像中的作用仍然未被充分探索。医学推理任务需要可靠的图像分析和合理的答案，由于医学图像的复杂性，这构成了挑战。透明度和可信度对于临床采用和法规合规至关重要。我们引入了Med-R1框架，探索强化学习（RL）来提高VLMs在医学推理中的通用性和可信度。利用DeepSeek策略，我们采用集团相对政策优化（GRPO）通过奖励信号来指导推理路径。与经常过度拟合且缺乏泛化能力的有监督微调（SFT）不同，RL促进稳健和多样化的推理。Med-R1在八种医学成像模式上进行了评估：CT、MRI、超声、皮肤镜检查、眼底摄影、光学相干断层扫描（OCT）、显微镜和X射线成像。与基准模型相比，Med-R1在准确性上提高了29.94%，并优于参数更多的Qwen2-VL-72B。在五种问题类型（模态识别、解剖结构识别、疾病诊断、病变分级和生物属性分析）的测试上，Med-R1表现出优越的泛化能力，超过Qwen2-VL-2B 32.06%，并在问题类型泛化方面超越Qwen2-VL-72B。这些发现表明，强化学习能够改善医学推理，并使得参数效率模型能够显著优于更大的模型。Med-R1的可解释推理输出是朝着通用、可信和临床上可行的医学VLMs的有希望的一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13939v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了Med-R1框架，利用强化学习（RL）提升视觉语言模型（VLMs）在医疗推理中的通用性和可信度。通过DeepSeek策略及Group Relative Policy Optimization（GRPO）引导推理路径，相较于监督微调（SFT），RL促进稳健和多样化的推理。Med-R1在八种医学成像模态上表现出卓越性能，实现相较于基准模型29.94%的准确度提升，且在问题类型泛化方面表现优异。研究结果表明，RL有助于改进医疗推理，使参数效率模型在性能方面超越更大规模模型，为通用、可信且临床可行的医疗VLMs迈出重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-R1框架利用强化学习（RL）增强视觉语言模型（VLMs）在医疗推理中的通用性和可信度。</li>
<li>DeepSeek策略和Group Relative Policy Optimization（GRPO）用于指导推理路径。</li>
<li>相较于监督微调（SFT），强化学习（RL）促进更稳健和多样化的推理。</li>
<li>Med-R1在多种医学成像模态上表现出卓越性能，实现显著的性能提升。</li>
<li>Med-R1在问题类型泛化方面表现出优势，优于基准模型和更大规模模型。</li>
<li>RL有助于改进医疗推理，提升模型的可解释性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13939">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d41cf1f555ee818bedb1d3e9600d32f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-630d53122dda7aca60f3a3ccd8c64a19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-128122acf4102aa38fc1f1dd1300ff49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-815f2579c33a67045719bb040b3c9dab.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MMR-A-Large-scale-Benchmark-Dataset-for-Multi-target-and-Multi-granularity-Reasoning-Segmentation"><a href="#MMR-A-Large-scale-Benchmark-Dataset-for-Multi-target-and-Multi-granularity-Reasoning-Segmentation" class="headerlink" title="MMR: A Large-scale Benchmark Dataset for Multi-target and   Multi-granularity Reasoning Segmentation"></a>MMR: A Large-scale Benchmark Dataset for Multi-target and   Multi-granularity Reasoning Segmentation</h2><p><strong>Authors:Donggon Jang, Yucheol Cho, Suin Lee, Taehyeon Kim, Dae-Shik Kim</strong></p>
<p>The fusion of Large Language Models with vision models is pioneering new possibilities in user-interactive vision-language tasks. A notable application is reasoning segmentation, where models generate pixel-level segmentation masks by comprehending implicit meanings in human instructions. However, seamless human-AI interaction demands more than just object-level recognition; it requires understanding both objects and the functions of their detailed parts, particularly in multi-target scenarios. For example, when instructing a robot to \textit{turn on the TV”}, there could be various ways to accomplish this command. Recognizing multiple objects capable of turning on the TV, such as the TV itself or a remote control (multi-target), provides more flexible options and aids in finding the optimized scenario. Furthermore, understanding specific parts of these objects, like the TV’s button or the remote’s button (part-level), is important for completing the action. Unfortunately, current reasoning segmentation datasets predominantly focus on a single target object-level reasoning, which limits the detailed recognition of an object’s parts in multi-target contexts. To address this gap, we construct a large-scale dataset called Multi-target and Multi-granularity Reasoning (MMR). MMR comprises 194K complex and implicit instructions that consider multi-target, object-level, and part-level aspects, based on pre-existing image-mask sets. This dataset supports diverse and context-aware interactions by hierarchically providing object and part information. Moreover, we propose a straightforward yet effective framework for multi-target, object-level, and part-level reasoning segmentation. Experimental results on MMR show that the proposed method can reason effectively in multi-target and multi-granularity scenarios, while the existing reasoning segmentation model still has room for improvement. </p>
<blockquote>
<p>将大型语言模型与视觉模型的融合在交互式视觉语言任务中开创了新的可能性。一个典型的应用是推理分割，模型通过理解人类指令中的隐含意义来生成像素级的分割掩膜。然而，无缝的人机交互不仅仅是基于目标级别的识别；它要求了解对象和它们详细部分的功能，特别是在多目标场景中。例如，当指示机器人“打开电视”时，可能有多种方式来完成这个命令。识别能够打开电视的多目标对象，如电视本身或遥控器（多目标），提供了更灵活的选择并有助于找到最佳场景。此外，了解这些对象的特定部分，如电视的按钮或遥控器的按钮（部分级别），对于完成动作也很重要。然而，当前的推理分割数据集主要集中在单一目标对象级别的推理上，这限制了多目标上下文中对象部分的详细识别。为了弥补这一空白，我们构建了一个大规模数据集，称为多目标多粒度推理（MMR）。MMR由基于现有图像掩模集的19.4万条复杂且隐含的指令组成，这些指令考虑了多目标、对象级别和部分级别的方面。该数据集通过分层提供对象和部分信息，支持多样化和情境感知的交互。此外，我们提出了一个简单有效的多目标、对象级别和部分级别的推理分割框架。在MMR上的实验结果表明，所提出的方法在多目标和多粒度场景中能够进行有效的推理，而现有的推理分割模型仍有改进的空间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13881v1">PDF</a> ICLR 2025, Code and dataset are available at   \url{<a target="_blank" rel="noopener" href="https://github.com/jdg900/MMR%7D">https://github.com/jdg900/MMR}</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了大型语言模型与视觉模型的融合在交互式视觉语言任务中的创新应用。特别是在推理分割领域，模型能够生成像素级的分割掩膜，通过理解人类指令中的隐含意义。然而，无缝的人机交互不仅需要识别对象级别，还需要理解对象及其详细部分的功能，特别是在多目标场景中。文章构建了一个大规模数据集Multi-target and Multi-granularity Reasoning (MMR)，以支持多样化和上下文感知的交互，并提供了一个针对多目标、对象级别和部分级别的推理分割的框架。实验结果证明了该方法在多目标和多粒度场景中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型与视觉模型的融合为交互式视觉语言任务带来创新可能性。</li>
<li>推理分割是其中一项重要应用，模型通过理解人类指令中的隐含意义生成像素级分割掩膜。</li>
<li>人机交互需要理解对象及其详细部分的功能，特别是在多目标场景中。</li>
<li>当前推理分割数据集主要关注单一目标对象级别的推理，需要更多关注多目标场景下的详细对象部分识别。</li>
<li>构建了一个大规模数据集MMR，支持多样化和上下文感知的交互，涵盖多目标、对象级别和部分级别的信息。</li>
<li>提出了一种针对多目标、对象级别和部分级别的推理分割的框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13881">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-79e70b34c2282e26cf19dad0261b56ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a565568c870bfda13f311e5dc1d8bb50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0641c4afe699f04a85e0134b748460e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9905ecab08869593fe33df78713f5275.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40affda86edd633ab99f02c512de4ada.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Bridging-Social-Psychology-and-LLM-Reasoning-Conflict-Aware-Meta-Review-Generation-via-Cognitive-Alignment"><a href="#Bridging-Social-Psychology-and-LLM-Reasoning-Conflict-Aware-Meta-Review-Generation-via-Cognitive-Alignment" class="headerlink" title="Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review   Generation via Cognitive Alignment"></a>Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review   Generation via Cognitive Alignment</h2><p><strong>Authors:Wei Chen, Han Ding, Meng Yuan, Zhao Zhang, Deqing Wang, Fuzhen Zhuang</strong></p>
<p>The rapid growth of scholarly submissions has overwhelmed traditional peer review systems, driving the need for intelligent automation to preserve scientific rigor. While large language models (LLMs) show promise in automating manuscript critiques, their ability to synthesize high-stakes meta-reviews, which require conflict-aware reasoning and consensus derivation, remains underdeveloped. Existing methods fail to effectively handle conflicting viewpoints within differing opinions, and often introduce additional cognitive biases, such as anchoring effects and conformity bias.To overcome these limitations, we propose the Cognitive Alignment Framework (CAF), a dual-process architecture that transforms LLMs into adaptive scientific arbitrators. By operationalizing Kahneman’s dual-process theory, CAF introduces a three-step cognitive pipeline: review initialization, incremental integration, and cognitive alignment.Empirical validation shows that CAF outperforms existing LLM-based methods, with sentiment consistency gains reaching up to 19.47% and content consistency improving by as much as 12.95%. </p>
<blockquote>
<p>学术稿件的迅速增长已经使传统的同行评审系统不堪重负，这促使需要智能自动化来保持科学的严谨性。虽然大型语言模型（LLM）在自动化手稿评审方面显示出潜力，但它们在综合高风险元评审方面的能力，这需要意识到冲突推理和共识推导，仍然处于未开发状态。现有方法无法有效处理不同观点中的冲突观点，并经常引入额外的认知偏见，例如锚定效应和服从偏见。为了克服这些局限性，我们提出了认知对齐框架（CAF），这是一种双过程架构，可将LLM转换为自适应的科学仲裁者。通过实施Kahneman的双过程理论，CAF引入了一个三步骤的认知管道：评审初始化、增量集成和认知对齐。实证研究证明，CAF在情感一致性方面的提升达到了高达19.47%，内容一致性也提高了最多达12.95%，表现优于现有的LLM方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13879v1">PDF</a> 23 pages</p>
<p><strong>Summary</strong></p>
<p>文章探讨学术提交的高速增长对传统的同行评审系统带来的压力，并指出智能自动化是缓解这一压力的关键。尽管大型语言模型（LLM）在自动化手稿评审方面显示出潜力，但在合成需要冲突感知推理和共识推导的高风险元评审方面仍存在不足。现有方法无法有效处理不同观点中的冲突，并可能引入额外的认知偏见。为解决这些问题，文章提出了认知对齐框架（CAF），这是一个将LLM转化为自适应科学仲裁者的双过程架构。CAF根据卡内曼的双重过程理论，设计了一个包括审查初始化、增量集成和认知对齐的三步认知管道。实证验证显示，CAF在情感一致性和内容一致性方面都优于现有的LLM方法，其增益分别高达19.47%和12.95%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>学术提交量的迅速增长促使对智能自动化以维持科学严谨性的需求。</li>
<li>大型语言模型（LLMs）在自动化手稿评审中有潜力，但在合成高风险的元评审方面存在局限。</li>
<li>现有方法难以处理不同观点中的冲突，并可能引入额外的认知偏见。</li>
<li>提出了认知对齐框架（CAF）来解决上述问题，这是一个双过程架构。</li>
<li>CAF基于卡内曼的双重过程理论，包括审查初始化、增量集成和认知对齐三个步骤。</li>
<li>CAF在情感一致性和内容一致性方面的表现优于现有LLM方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13879">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5d6a422a9a5048b5eea40aafa607f945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-098e6604b2263ffdbd23662b57f30ce1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db11cffc531af2d0f77b0c02e9e4eea1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-025270a4faf6f0d14c1c96eb644832e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fffe07298001cba7f3659c48706614b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93b6893efd056041246d700dc4967965.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Counterfactual-experience-augmented-off-policy-reinforcement-learning"><a href="#Counterfactual-experience-augmented-off-policy-reinforcement-learning" class="headerlink" title="Counterfactual experience augmented off-policy reinforcement learning"></a>Counterfactual experience augmented off-policy reinforcement learning</h2><p><strong>Authors:Sunbowen Lee, Yicheng Gong, Chao Deng</strong></p>
<p>Reinforcement learning control algorithms face significant challenges due to out-of-distribution and inefficient exploration problems. While model-based reinforcement learning enhances the agent’s reasoning and planning capabilities by constructing virtual environments, training such virtual environments can be very complex. In order to build an efficient inference model and enhance the representativeness of learning data, we propose the Counterfactual Experience Augmentation (CEA) algorithm. CEA leverages variational autoencoders to model the dynamic patterns of state transitions and introduces randomness to model non-stationarity. This approach focuses on expanding the learning data in the experience pool through counterfactual inference and performs exceptionally well in environments that follow the bisimulation assumption. Environments with bisimulation properties are usually represented by discrete observation and action spaces, we propose a sampling method based on maximum kernel density estimation entropy to extend CEA to various environments. By providing reward signals for counterfactual state transitions based on real information, CEA constructs a complete counterfactual experience to alleviate the out-of-distribution problem of the learning data, and outperforms general SOTA algorithms in environments with difference properties. Finally, we discuss the similarities, differences and properties of generated counterfactual experiences and real experiences. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Aegis1863/CEA">https://github.com/Aegis1863/CEA</a>. </p>
<blockquote>
<p>强化学习控制算法面临着由于分布外和无效探索问题而带来的重大挑战。基于模型的强化学习通过构建虚拟环境来提升智能体的推理和规划能力，但训练这样的虚拟环境可能非常复杂。为了构建高效的推理模型并增强学习数据的代表性，我们提出了Counterfactual Experience Augmentation（CEA）算法。CEA利用变分自动编码器来建模状态转换的动态模式，并引入随机性来建模非平稳性。该方法侧重于通过反事实推理扩展经验池中的学习数据，在遵循双模拟假设的环境中表现尤为出色。具有双模拟属性的环境通常表现为离散观测和动作空间，我们提出了一种基于最大核密度估计熵的采样方法，将CEA扩展到各种环境。CEA基于真实信息为反事实状态转换提供奖励信号，构建完整的反事实经验，以缓解学习数据的分布外问题，并在具有不同属性的环境中优于一般的最先进算法。最后，我们讨论了生成的反事实经验与真实经验之间的相似性、差异和属性。代码可在<a target="_blank" rel="noopener" href="https://github.com/Aegis1863/CEA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Aegis1863/CEA获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13842v1">PDF</a> Accepted by Neurocomputing,   <a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.neucom.2025.130017">https://doi.org/10.1016/j.neucom.2025.130017</a></p>
<p><strong>Summary</strong><br>强化学习控制算法面临分布外和无效探索的问题。模型化强化学习通过构建虚拟环境提升智能体的推理和规划能力，但训练这些虚拟环境可能非常复杂。为了建立高效的推理模型和增强学习数据的代表性，我们提出了基于因果经验的扩增算法（CEA）。CEA利用变分自动编码器模拟状态转换的动态模式，并引入随机性模拟非稳定性。该方法侧重于通过因果推断扩充经验池中的学习数据，并在遵循双模拟假设的环境中表现优异。对于离散观测和动作空间的环境，我们提出了一种基于最大核密度估计熵的采样方法，以扩展CEA的应用范围。CEA通过为因果状态转换提供基于真实信息的奖励信号，构建了完整的因果经验，缓解了学习数据分布外的问题，并在具有不同属性的环境中优于一般的最先进算法。此外我们还探讨了生成型因果经验和真实经验之间的相似性、差异性和属性。相关代码可访问网址：<a target="_blank" rel="noopener" href="https://github.com/Aegis1863/CEA%E3%80%82">https://github.com/Aegis1863/CEA。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习控制算法面临分布外和无效探索的挑战。</li>
<li>模型化强化学习通过构建虚拟环境提升智能体的能力，但训练复杂性较高。</li>
<li>提出了基于因果经验的扩增算法（CEA）来解决这些问题。</li>
<li>CEA利用变分自动编码器模拟状态转换，并引入随机性以适应非稳定性。</li>
<li>CEA通过扩充经验池中的学习数据，在遵循双模拟假设的环境中表现优异。</li>
<li>提出了基于最大核密度估计熵的采样方法，以扩展CEA至不同环境。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13842">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9961785680c30403866a1b451e86f17a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-317c8ab319a5457ddbc7a88b60f47d14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecfde2ee097dfc12db5c7fbabc85840d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a48b3f69829ca7fb45bd047cbbc67295.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Pensez-Less-Data-Better-Reasoning-–-Rethinking-French-LLM"><a href="#Pensez-Less-Data-Better-Reasoning-–-Rethinking-French-LLM" class="headerlink" title="Pensez: Less Data, Better Reasoning – Rethinking French LLM"></a>Pensez: Less Data, Better Reasoning – Rethinking French LLM</h2><p><strong>Authors:Huy Hoang Ha</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, achieving strong performance in specialized domains like mathematical reasoning and non-English languages often requires extensive training on massive datasets. This paper investigates a contrasting approach: strategic fine-tuning on a small, high-quality, bilingual (English-French) dataset to enhance both the reasoning capabilities and French language proficiency of a large language model. Rather than relying on scale, we explore the hypothesis that targeted data curation and optimized training can achieve competitive, or even superior, performance. We demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000 carefully selected samples, significant improvements in mathematical reasoning. Specifically, Pensez 7B exhibits an increase in accuracy of the base model up to 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark. These results challenge the prevailing assumption that massive datasets are aprerequisite for strong reasoning performance in LLMs, highlighting the potential of strategic data curation and optimized fine-tuning for enhancing both specialized skills and multilingual capabilities. Our findings have implications for the efficient development of high-performing, multilingual LLMs, especially in resource-constrained scenarios. </p>
<blockquote>
<p>大型语言模型（LLM）在各种自然语言处理任务中表现出了显著的能力。然而，在数学推理和非英语领域等专业化领域实现卓越性能，通常需要在大规模数据集上进行大量训练。本文研究了一种相反的方法：在小型、高质量、双语（英语-法语）数据集上进行有针对性的微调，以提高大型语言模型的推理能力和法语熟练度。我们并不依赖大规模数据集，而是探索有针对性的数据收集和优化训练能否达到竞争水平甚至更高水平的性能。我们通过仅在精心挑选的2000个样本上进行有针对性的监督微调（SFT），在数学推理方面取得了显著改进。具体来说，Pensez 7B在AIME25上的准确率提高了高达20%，在法国MATH 5级基准测试上的准确率提高了12%。这些结果挑战了大规模数据集是LLM实现强大推理能力的先决条件的普遍假设，突显了有针对性的数据收集和优化微调在增强专项技能和多种语言能力方面的潜力。我们的发现对高效开发高性能、多语言的大型语言模型具有启示意义，特别是在资源受限的场景下。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13661v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型在多种自然语言处理任务中展现出显著的能力。然而，在数学推理和非英语领域的专业领域，通常需要大规模数据集进行广泛训练。本文探索了一种对比方法：在小型、高质量的双语（英语-法语）数据集上进行有针对性的微调，以增强大型语言模型的推理能力和法语熟练度。研究假设有针对性的数据收集和优化训练可以达到竞争性的性能，甚至可能更优秀。通过仅使用精心挑选的2000个样本进行有针对性的监督微调（SFT），在数学推理方面取得了显著改善。具体来说，Pensez 7B在AIME25上的准确率提高了20%，在法国MATH 5级基准测试上的准确率提高了12%。这些结果挑战了大规模数据集对于大型语言模型进行数学推理能力的必要性假设，突显了有针对性的数据收集和优化微调在增强特殊技能和多种语言能力方面的潜力。这对高效开发高性能的、多语言的LLM模型有重要的影响。该研究为资源有限情境下如何推进相关研发提供了一种可行的视角。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM在特定领域如数学推理和非英语语言方面需专门训练以提升性能。</li>
<li>通过小规模高质量双语数据集的战略微调可增强模型的推理和语言能力。</li>
<li>仅通过少量针对性样本监督微调就能显著改善数学推理能力。</li>
<li>Pensez 7B模型在AIME25上的准确率提升显著，显示出优化训练数据策略的潜力。</li>
<li>研究挑战了大规模数据集对于LLM在数学推理领域的必要性假设。</li>
<li>有针对性的数据收集和优化微调对于增强特殊技能和多种语言能力有重要作用。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13661">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8f481e7d0ccbcab27f9e89107495d3a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-376346e4aae64474b3c1c1ab1b7f97b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f48603ecbc02e0d9d9a8ae3ca367b7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8615721dc78079f793183456a731302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b601391796e98eff163d487d21a8b433.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VeriContaminated-Assessing-LLM-Driven-Verilog-Coding-for-Data-Contamination"><a href="#VeriContaminated-Assessing-LLM-Driven-Verilog-Coding-for-Data-Contamination" class="headerlink" title="VeriContaminated: Assessing LLM-Driven Verilog Coding for Data   Contamination"></a>VeriContaminated: Assessing LLM-Driven Verilog Coding for Data   Contamination</h2><p><strong>Authors:Zeng Wang, Minghao Shao, Jitendra Bhandari, Likhitha Mankali, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel</strong></p>
<p>Large Language Models (LLMs) have revolutionized code generation, achieving exceptional results on various established benchmarking frameworks. However, concerns about data contamination - where benchmark data inadvertently leaks into pre-training or fine-tuning datasets - raise questions about the validity of these evaluations. While this issue is known, limiting the industrial adoption of LLM-driven software engineering, hardware coding has received little to no attention regarding these risks. For the first time, we analyze state-of-the-art (SOTA) evaluation frameworks for Verilog code generation (VerilogEval and RTLLM), using established methods for contamination detection (CCD and Min-K% Prob). We cover SOTA commercial and open-source LLMs (CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1}, GPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned models (RTLCoder and Verigen). Our study confirms that data contamination is a critical concern. We explore mitigations and the resulting trade-offs for code quality vs fairness (i.e., reducing contamination toward unbiased benchmarking). </p>
<blockquote>
<p>大型语言模型（LLM）已经彻底改变了代码生成的方式，在各种成熟的基准测试框架上取得了非凡的成果。然而，对数据污染的担忧——即基准测试数据无意中泄露到预训练或微调数据集——对这些评估的有效性提出了质疑。虽然这个问题已经为人所知，并限制了LLM驱动的软件工程在工业中的应用，但硬件编码几乎没有关注这些风险。我们首次使用污染检测（CCD和Min-K% Prob）的既定方法，分析了最先进的Verilog代码生成评估框架（VerilogEval和RTLLM）。我们涵盖了最先进的商业和开源LLM（CodeGen2.5、Minitron 4b、Mistral 7b、phi-4 mini、LLaMA-{1,2,3.1}、GPT-{2,3.5,4o}、Deepseek-Coder和CodeQwen 1.5），以及基准模型和微调模型（RTLCoder和Verigen）。我们的研究证实了数据污染是一个关键问题。我们探讨了缓解方法和代码质量与公平性之间的权衡（即减少污染以实现公平的基准测试）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13572v1">PDF</a> </p>
<p><strong>Summary</strong>：大型语言模型在代码生成方面取得了革命性的成果，但在预训练或微调数据集意外泄露基准测试数据的问题上引发了有效性质疑。本研究首次对最先进的Verilog代码生成评估框架（VerilogEval和RTLLM）进行了分析，并使用了污染检测方法来验证数据污染问题。研究确认数据污染是一个关键问题，并探讨了缓解措施及其与代码质量和公平性之间的权衡。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型在代码生成方面表现出卓越性能。</li>
<li>数据污染问题对LLM的有效性评价提出了质疑。</li>
<li>首次对Verilog代码生成的最新评估框架进行了污染分析。</li>
<li>使用已建立的污染检测方法证实了数据污染问题的存在。</li>
<li>数据污染是一个关键问题，需要解决。</li>
<li>研究探讨了缓解数据污染的措施及其与代码质量的权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13572">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ff03c5c5972b3c9b0617a9aa0ef2ca67.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b4709f17215072c2f1b7fbcfb5265a2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9cbc0b5bf0382965889bea139ea0c37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecd8384c56b1f966b0f8adc5a02e51b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e71ad7d5da003b540e398666068510b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-138220fc4df4e1398b017e4095b43b14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38da2c008a157bae10609cf38641b4c4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Hierarchical-Multi-Step-Reward-Models-for-Enhanced-Reasoning-in-Large-Language-Models"><a href="#Towards-Hierarchical-Multi-Step-Reward-Models-for-Enhanced-Reasoning-in-Large-Language-Models" class="headerlink" title="Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in   Large Language Models"></a>Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in   Large Language Models</h2><p><strong>Authors:Teng Wang, Zhangyi Jiang, Zhenqi He, Wenhan Yang, Yanan Zheng, Zeyu Li, Zifan He, Shenyang Tong, Hailei Gong</strong></p>
<p>Recent studies show that Large Language Models (LLMs) achieve strong reasoning capabilities through supervised fine-tuning or reinforcement learning. However, a key approach, the Process Reward Model (PRM), suffers from reward hacking, making it unreliable in identifying the best intermediate steps. In this paper, we propose a novel reward model approach, Hierarchical Reward Model (HRM), which evaluates both individual and consecutive reasoning steps from fine-grained and coarse-grained level. HRM performs better in assessing reasoning coherence and self-reflection, particularly when the previous reasoning step is incorrect. Furthermore, to address the inefficiency of autonomous generating PRM training data via Monte Carlo Tree Search (MCTS), we introduce a lightweight and effective data augmentation strategy called Hierarchical Node Compression (HNC) based on node merging (combining two consecutive reasoning steps into one step) in the tree structure. This approach diversifies MCTS results for HRM with negligible computational overhead, enhancing label robustness by introducing noise. Empirical results on the PRM800K dataset demonstrate that HRM, in conjunction with HNC, achieves superior stability and reliability in evaluation compared to PRM. Furthermore, cross-domain evaluations on MATH500 and GSM8K confirm HRM’s superior generalization and robustness across diverse reasoning tasks. The code for all experiments will be released at https: &#x2F;&#x2F;github.com&#x2F;tengwang0318&#x2F;hierarchial_reward_model. </p>
<blockquote>
<p>最近的研究表明，大型语言模型（LLM）通过监督微调或强化学习实现了强大的推理能力。然而，一种关键方法——过程奖励模型（PRM）存在奖励作弊的问题，使其在识别最佳中间步骤时变得不可靠。在本文中，我们提出了一种新的奖励模型方法，即分层奖励模型（HRM），它可以从精细粒度和粗略粒度两个层面评估单个和连续的推理步骤。HRM在评估推理连贯性和自我反思方面表现更好，特别是在前一个推理步骤错误的情况下。此外，为了解决通过蒙特卡洛树搜索（MCTS）自主生成PRM训练数据效率低下的问题，我们引入了一种基于节点合并（将两个连续的推理步骤合并为一个步骤）的树结构中的分层节点压缩（HNC）的轻量级有效数据增强策略。这种方法在HRM中实现了MCTS结果的多样化，同时几乎不增加计算开销，并通过引入噪声增强了标签的鲁棒性。在PRM800K数据集上的实证结果表明，与PRM相比，HRM结合HNC在评估中实现了更高的稳定性和可靠性。此外，MATH500和GSM8K的跨域评估证实了HRM在多种推理任务中的优越泛化和鲁棒性。所有实验的代码将在<a target="_blank" rel="noopener" href="https://github.com/tengwang0318/hierarchial_reward_model%E4%B8%8A%E5%8F%91%E6%9C%8D%E3%80%82">https://github.com/tengwang0318/hierarchial_reward_model上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13551v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在监督微调或强化学习中展现出强大的推理能力。然而，过程奖励模型（PRM）存在奖励黑客问题，无法可靠地识别最佳中间步骤。本文提出了一种新的奖励模型方法——分层奖励模型（HRM），可以从精细粒度和粗略粒度级别评估单个和连续的推理步骤。HRM在评估推理连贯性和自我反思方面表现更佳，尤其在先前推理步骤错误时。为解决蒙特卡洛树搜索（MCTS）自主生成PRM训练数据的不效率问题，我们引入了一种基于节点合并（将两个连续的推理步骤合并为一个步骤）的分层节点压缩（HNC）轻量级有效数据增强策略。该方法在PRM800K数据集上实现了与PRM相比更稳定可靠的评估结果。此外，MATH500和GSM8K的跨域评估证明了HRM在多种推理任务中的优异通用性和稳健性。所有实验的代码将在<a target="_blank" rel="noopener" href="https://github.com/tengwang0318/hierarchial_reward_model%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/tengwang0318/hierarchial_reward_model发布。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）具备强大的推理能力，通过监督微调或强化学习实现。</li>
<li>过程奖励模型（PRM）存在奖励黑客问题，可靠性受损。</li>
<li>引入分层奖励模型（HRM），能评估单个和连续推理步骤，提升推理连贯性和自我反思能力。</li>
<li>HRM在先前推理步骤错误时表现更优。</li>
<li>提出分层节点压缩（HNC）数据增强策略，基于节点合并，提高蒙特卡洛树搜索（MCTS）结果多样性，增强标签稳健性。</li>
<li>HRM结合HNC在PRM800K数据集上实现稳定可靠的评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13551">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-19d314ff82722671b755ab6a6e9b2499.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7568558e1517c5182f2377d98298b5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d034c9d376834234002a6135fdcb7722.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c56a6508f1582ac255f3a414b9fbd22f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bbffa6422b78dc8703d9f456b946584.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c96591df0d55499f82fcce06c217a9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf8ccf9335031bf8e7c031227e134403.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Showdown-of-ChatGPT-vs-DeepSeek-in-Solving-Programming-Tasks"><a href="#A-Showdown-of-ChatGPT-vs-DeepSeek-in-Solving-Programming-Tasks" class="headerlink" title="A Showdown of ChatGPT vs DeepSeek in Solving Programming Tasks"></a>A Showdown of ChatGPT vs DeepSeek in Solving Programming Tasks</h2><p><strong>Authors:Ronas Shakya, Farhad Vadiee, Mohammad Khalil</strong></p>
<p>The advancement of large language models (LLMs) has created a competitive landscape for AI-assisted programming tools. This study evaluates two leading models: ChatGPT 03-mini and DeepSeek-R1 on their ability to solve competitive programming tasks from Codeforces. Using 29 programming tasks of three levels of easy, medium, and hard difficulty, we assessed the outcome of both models by their accepted solutions, memory efficiency, and runtime performance. Our results indicate that while both models perform similarly on easy tasks, ChatGPT outperforms DeepSeek-R1 on medium-difficulty tasks, achieving a 54.5% success rate compared to DeepSeek 18.1%. Both models struggled with hard tasks, thus highlighting some ongoing challenges LLMs face in handling highly complex programming problems. These findings highlight key differences in both model capabilities and their computational power, offering valuable insights for developers and researchers working to advance AI-driven programming tools. </p>
<blockquote>
<p>大型语言模型（LLM）的进步为AI辅助编程工具创造了一个竞争环境。本研究评估了两款领先模型：ChatGPT 03-mini和DeepSeek-R1，它们在解决Codeforces竞赛编程任务方面的能力。我们使用29个编程任务，分为容易、中等和困难三个级别，通过接受的解决方案、内存效率和运行时性能来评估两个模型的结果。我们的结果表明，虽然两个模型在容易的任务上表现相似，但ChatGPT在中等难度的任务上表现优于DeepSeek-R1，成功率为54.5%，而DeepSeek的成功率仅为18.1%。两个模型在困难的任务上都遇到了困难，这突显了LLM在处理高度复杂的编程问题时面临的一些持续挑战。这些发现突出了两个模型能力和计算能力的关键差异，为开发和研究AI驱动的编程工具的开发人员和研究人员提供了有价值的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13549v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该研究对ChatGPT 03-mini和DeepSeek-R1两种领先的AI辅助编程工具进行了评估，主要考察它们在解决Codeforces竞赛编程任务方面的能力。实验结果显示，在中等难度的编程任务上，ChatGPT表现优于DeepSeek-R1，成功率为54.5%，而DeepSeek的成功率为仅18.1%。但在高难度的编程任务上，两者都面临挑战。这一研究为开发者和研究人员提供了关于AI驱动编程工具的关键差异和能力的重要见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）的进步为AI辅助编程工具创造了竞争环境。</li>
<li>研究评估了ChatGPT 03-mini和DeepSeek-R1两种领先模型在解决不同难度级别编程任务时的表现。</li>
<li>在中等难度的编程任务上，ChatGPT表现优于DeepSeek-R1。</li>
<li>在高难度的编程任务上，这两个模型都面临挑战。</li>
<li>ChatGPT和DeepSeek在解决编程任务时，还存在内存效率和运行时间性能的差异。</li>
<li>该研究为开发者提供了关于不同AI编程工具的能力和局限性的重要信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13549">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5e00af8b47715f6b17e7d6f2a45b9725.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3f7c04485f138b0c7201da49b0b5857.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96f22e362929099cfc1528b65e5c71a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99d5d4a87da2ca6af76ab4009105ef7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57815d4fc535f25b3aec9b49a91d36a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad6cf107827165df15b5d65af5039c2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8915bc890d917ed159c438fd57bceb03.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DeepPerception-Advancing-R1-like-Cognitive-Visual-Perception-in-MLLMs-for-Knowledge-Intensive-Visual-Grounding"><a href="#DeepPerception-Advancing-R1-like-Cognitive-Visual-Perception-in-MLLMs-for-Knowledge-Intensive-Visual-Grounding" class="headerlink" title="DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs   for Knowledge-Intensive Visual Grounding"></a>DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs   for Knowledge-Intensive Visual Grounding</h2><p><strong>Authors:Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek F. Wong, Xiaoyi Feng, Maosong Sun</strong></p>
<p>Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08% accuracy improvements on KVG-Bench and exhibiting +4.60% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at <a target="_blank" rel="noopener" href="https://github.com/thunlp/DeepPerception">https://github.com/thunlp/DeepPerception</a>. </p>
<blockquote>
<p>人类专家擅长通过利用领域知识来细化感知特征，进行精细化的视觉辨别，这是当前的多模态大型语言模型（MLLMs）尚未充分发展的一项能力。尽管拥有大量的专家级知识，MLLMs在将推理融入视觉感知方面却存在困难，通常会产生直接的回应，而无需进行更深入的分析。为了弥补这一差距，我们引入了知识密集型视觉定位（KVG），这是一项需要精细感知和特定领域知识整合的新型视觉定位任务。为了应对KVG的挑战，我们提出了DeepPerception，这是一个具备认知视觉感知能力的MLLM增强版本。我们的方法包括（1）自动化数据合成管道，用于生成高质量、与知识对齐的训练样本；（2）两阶段训练框架，结合监督微调用于认知推理脚手架和强化学习，以优化感知与认知协同作用。为了评估性能，我们推出了KVG-Bench数据集，这是一个涵盖10个领域、包含1300个手动整理测试案例的综合数据集。实验结果表明，DeepPerception显著优于直接微调，在KVG-Bench上的准确率提高了8.08%，并且在跨域泛化方面较基线方法提高了4.60%。我们的研究强调了将认知过程融入MLLMs以实现人类样视觉感知的重要性，并为多模态推理研究开辟了新的方向。数据、代码和模型已发布在<a target="_blank" rel="noopener" href="https://github.com/thunlp/DeepPerception%E3%80%82">https://github.com/thunlp/DeepPerception。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12797v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了当前多模态大型语言模型（MLLMs）在视觉感知方面的不足，尤其是在精细粒度视觉辨别和领域知识整合方面的能力尚待发展。为解决此问题，文章提出了一种名为知识密集型视觉接地（KVG）的新视觉接地任务，要求精细的感知能力和特定领域的整合知识。同时提出了一种解决这一任务的方法DeepPerception，其增强了认知视觉感知能力。它通过自动化的数据合成管道生成高质量的知识对齐训练样本，并采用两阶段训练框架结合监督微调进行认知推理架构和强化学习优化感知认知协同作用。实验结果表明，DeepPerception在KVG基准测试上的准确率提高了8.08%，并且在跨域泛化方面也优于基线方法。这为多模态推理研究提供了新的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前多模态大型语言模型（MLLMs）在视觉感知方面存在不足，特别是在精细粒度视觉辨别和领域知识整合方面。</li>
<li>知识密集型视觉接地（KVG）任务要求模型同时具备精细的感知能力和特定领域的知识整合能力。</li>
<li>DeepPerception是一种解决KVG任务的方法，通过自动化的数据合成管道生成高质量的训练样本。</li>
<li>DeepPerception采用两阶段训练框架，结合监督微调和强化学习优化感知认知协同作用。</li>
<li>实验结果表明，DeepPerception在KVG基准测试上的准确率显著提高。</li>
<li>DeepPerception在跨域泛化方面也表现出优于基线方法的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12797">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-472fb9eedf5332ecb2fdbb833cf4c88f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0abf3d2f81927ca777a7ff2ed0fd6f0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f87d4dad88887b8349de9e7b24324378.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6095da6c179530575e09bed5e0864e11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44eb91d579e94cde3e49fc376b57702a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MPBench-A-Comprehensive-Multimodal-Reasoning-Benchmark-for-Process-Errors-Identification"><a href="#MPBench-A-Comprehensive-Multimodal-Reasoning-Benchmark-for-Process-Errors-Identification" class="headerlink" title="MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process   Errors Identification"></a>MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process   Errors Identification</h2><p><strong>Authors:Zhaopan Xu, Pengfei Zhou, Jiaxin Ai, Wangbo Zhao, Kai Wang, Xiaojiang Peng, Wenqi Shao, Hongxun Yao, Kaipeng Zhang</strong></p>
<p>Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs. </p>
<blockquote>
<p>推理是大型语言模型（LLM）解决复杂任务的基本能力，而识别过程中的错误对于提高这种能力至关重要。最近，提出了过程级奖励模型（PRM），提供分步奖励，便于训练过程中的强化学习和数据生成，并在推理过程中引导LLM走向正确的步骤，从而提高推理准确性。然而，现有的PRM基准测试都是基于文本的，侧重于错误检测，忽略了其他场景，如推理搜索。为了弥补这一空白，我们引入了MPBench，这是一个全面、多任务、多模式的基准测试，旨在系统地评估PRM在不同场景中的有效性。MPBench采用三种评估范式，每个范式都针对PRM在推理过程中的特定角色：1）步骤正确性，评估每个中间推理步骤的正确性；2）答案聚合，聚合多个解决方案并选择最佳解决方案；3）推理过程搜索，在推理过程中引导对最佳推理步骤的搜索。通过这些范式，MPBench进行了全面的评估，并为多模式PRM的发展提供了见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12505v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在进行复杂任务时，推理能力至关重要。过程级奖励模型（PRM）通过提供分步奖励，促进强化学习和训练过程中的数据生成，并在推理过程中引导LLM走向正确的步骤，从而提高推理准确性。然而，现有的PRM基准测试主要是文本基础的，侧重于错误检测，忽略了如推理搜索等其他场景。为解决这一空白，我们推出MPBench，这是一个多任务、多模态的基准测试，旨在系统评估PRM在多种场景中的有效性。MPBench采用三种评估模式，分别针对PRM在推理过程中的特定角色：1）步骤正确性，评估每个中间推理步骤的正确性；2）答案聚合，聚合多个解决方案并选择最佳答案；3）推理过程搜索，在推理过程中引导最优推理步骤的搜索。通过这些模式，MPBench进行全面评估，并为多模态PRM的发展提供见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在进行复杂任务时需要具备强大的推理能力。</li>
<li>过程级奖励模型（PRM）可以提高LLM的推理准确性。</li>
<li>现有PRM基准测试主要侧重于文本基础的错误检测，忽略了其他场景，如推理搜索。</li>
<li>MPBench是一个多任务、多模态的基准测试，旨在系统评估PRM在多种场景中的有效性。</li>
<li>MPBench采用三种评估模式，包括步骤正确性、答案聚合和推理过程搜索。</li>
<li>MPBench为全面评估PRM提供平台，并有助于了解其在不同场景下的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12505">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c89ddabd11c041b38a0298e95b8e04ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ca84fa3a8151656e4ceb50a8c75a77a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09f249437d95434b9ee7a7c7c3fe8981.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81ff828745ceb45f948b35c216eb3175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7016cae4b239a86d9723dd1de050e86.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SPIN-Bench-How-Well-Do-LLMs-Plan-Strategically-and-Reason-Socially"><a href="#SPIN-Bench-How-Well-Do-LLMs-Plan-Strategically-and-Reason-Socially" class="headerlink" title="SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?"></a>SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?</h2><p><strong>Authors:Jianzhu Yao, Kevin Wang, Ryan Hsieh, Haisu Zhou, Tianqing Zou, Zerui Cheng, Zhangyang Wang, Pramod Viswanath</strong></p>
<p>Reasoning and strategic behavior in social interactions is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multi-hop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and human–AI teaming. Project Website: <a target="_blank" rel="noopener" href="https://spinbench.github.io/">https://spinbench.github.io/</a> </p>
<blockquote>
<p>在社会互动中的推理和战略行为是智能的标志。这种推理形式远比静态环境中的孤立规划或推理任务（例如数学问题解决）更为复杂。在本文中，我们提出了“战略规划、互动与谈判（SPIN-Bench）”，这是一种新的多领域评估，旨在衡量战略规划和社会推理的智能水平。虽然许多现有的基准测试主要集中在狭隘的规划或单代理推理上，但SPIN-Bench结合了经典PDDL任务、竞技棋类游戏、合作卡牌游戏和多代理谈判场景在一个统一框架中。该框架既包括一个基准测试，也包括一个模拟和评估各种社交设置的场所，以测试AI代理的推理和战略行为。我们通过系统地改变动作空间、状态复杂性和交互代理的数量来制定SPIN-Bench基准测试，以模拟各种社交环境，在这些环境中，成功不仅取决于方法和逐步的决策制定，还取决于对其他（对抗性或合作性）参与者的概念推断。我们的实验表明，虽然当代大型语言模型在处理基本事实检索和短期规划方面表现良好，但在需要深度多跳推理和大状态空间的社会适应性协调的任务中，它们会遇到显著的性能瓶颈。我们期望SPIN-Bench能成为未来关于稳健的多代理规划、社会推理和人机协作的研究催化剂。项目网站：<a target="_blank" rel="noopener" href="https://spinbench.github.io/">https://spinbench.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12349v2">PDF</a> 51 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>该文介绍了一种名为SPIN-Bench的新型多领域评估方法，旨在测量智能的战略规划和社交推理能力。与现有的主要聚焦于狭窄规划或单一智能体推理的基准测试不同，SPIN-Bench结合了PDDL任务、竞技棋类游戏、合作卡牌游戏和多智能体谈判场景，在一个统一框架中进行评估。框架既包括基准测试，也包括模拟和评估多种社交设置的场所，以测试人工智能智能体的推理和战略行为。实验表明，尽管当代大型语言模型在基本事实检索和短期规划方面表现良好，但在需要深度多跳推理和社交协调的任务中仍存在显著性能瓶颈。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPIN-Bench是一种新型的多领域评估方法，旨在测量智能的战略规划和社交推理能力。</li>
<li>现有基准测试主要聚焦于狭窄规划或单一智能体推理，而SPIN-Bench结合了多种任务在一个统一框架中进行评估。</li>
<li>SPIN-Bench包括基准测试和模拟社交设置的场所，模拟多种社交设置以测试AI智能体的推理和战略行为。</li>
<li>当代大型语言模型在需要深度多跳推理和社交协调的任务中仍存在性能瓶颈。</li>
<li>SPIN-Bench通过系统地改变动作空间、状态复杂性和交互智能体的数量来模拟各种社交设置。</li>
<li>成功不仅取决于方法和逐步决策制定，还取决于对其他（对抗或合作）参与者的概念推断。</li>
<li>SPIN-Bench为未来研究提供了催化剂，包括稳健的多智能体规划、社交推理和人机协作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12349">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-47f9b7a1078db759d212bcaa6243aa83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a25042afb63f292282ad1b051554613a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9a0f740f016d61f9ea223552e34db39.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PLM-Efficient-Peripheral-Language-Models-Hardware-Co-Designed-for-Ubiquitous-Computing"><a href="#PLM-Efficient-Peripheral-Language-Models-Hardware-Co-Designed-for-Ubiquitous-Computing" class="headerlink" title="PLM: Efficient Peripheral Language Models Hardware-Co-Designed for   Ubiquitous Computing"></a>PLM: Efficient Peripheral Language Models Hardware-Co-Designed for   Ubiquitous Computing</h2><p><strong>Authors:Cheng Deng, Luoyang Sun, Jiwen Jiang, Yongcheng Zeng, Xinjian Wu, Wenxin Zhao, Qingfa Xiao, Jiachuan Wang, Lei Chen, Lionel M. Ni, Haifeng Zhang, Jun Wang</strong></p>
<p>While scaling laws have been continuously validated in large language models (LLMs) with increasing model parameters, the inherent tension between the inference demands of LLMs and the limited resources of edge devices poses a critical challenge to the development of edge intelligence. Recently, numerous small language models have emerged, aiming to distill the capabilities of LLMs into smaller footprints. However, these models often retain the fundamental architectural principles of their larger counterparts, still imposing considerable strain on the storage and bandwidth capacities of edge devices. In this paper, we introduce the PLM, a Peripheral Language Model, developed through a co-design process that jointly optimizes model architecture and edge system constraints. The PLM utilizes a Multi-head Latent Attention mechanism and employs the squared ReLU activation function to encourage sparsity, thereby reducing peak memory footprint during inference. During training, we collect and reorganize open-source datasets, implement a multi-phase training strategy, and empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning rate scheduler. Additionally, we incorporate Reinforcement Learning from Human Feedback (RLHF) by adopting the ARIES preference learning approach. Following a two-phase SFT process, this method yields performance gains of 2% in general tasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel architecture, evaluation results demonstrate that PLM outperforms existing small language models trained on publicly available data while maintaining the lowest number of activated parameters. Furthermore, deployment across various edge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis, validates PLM’s suitability for peripheral applications. The PLM series models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/plm-team/PLM">https://github.com/plm-team/PLM</a>. </p>
<blockquote>
<p>随着模型参数的增加，比例定律在大语言模型（LLM）中得到了持续验证。然而，大语言模型的推理需求与边缘设备的有限资源之间存在的固有矛盾，给边缘智能的发展带来了关键挑战。近期，出现了许多小语言模型，旨在将大语言模型的能力转化为更小的占用空间。然而，这些模型往往保留其大型对应模型的基本架构原则，仍然给边缘设备的存储和带宽容量带来相当大的压力。</p>
</blockquote>
<p>在本文中，我们介绍了一种周边语言模型（PLM），该模型通过联合优化模型架构和边缘系统约束的协同设计过程而开发。PLM利用多头潜在注意力机制，并采用平方ReLU激活函数来促进稀疏性，从而减少推理过程中的峰值内存占用。在训练过程中，我们收集和重组了开源数据集，实施了多阶段训练策略，并对预热稳定衰减恒定（WSDC）学习率调度器进行了实证研究。此外，我们通过采用ARIES偏好学习方法，融入了人类反馈强化学习（RLHF）。经过两阶段的SFT过程后，该方法在一般任务上获得了2%的性能提升，在GSM8K任务上获得了9%的提升，在编码任务上获得了11%的提升。</p>
<p>除了其新颖的架构外，评估结果表明，PLM在公开数据上训练的现有小语言模型相比表现出色，同时保持了最低的活动参数数量。此外，在各种边缘设备上的部署，包括消费级GPU、手机和Raspberry Pis，验证了PLM适用于周边应用。PLM系列模型可在<a target="_blank" rel="noopener" href="https://github.com/plm-team/PLM">https://github.com/plm-team/PLM</a>公开获取。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.12167v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在边缘设备上的推理需求与有限资源之间存在固有矛盾，小型语言模型应运而生，试图将LLM的能力浓缩到更小的规模中。然而，这些模型仍然保留了对边缘设备的存储和带宽能力的需求较大的基本架构原则。在本文中，介绍了一种周边语言模型（PLM），通过联合优化模型架构和边缘系统约束的协同设计过程开发而成。PLM采用多头潜在注意力机制和平方ReLU激活函数来鼓励稀疏性，降低推理过程中的峰值内存占用。此外，通过收集和组织开源数据集、实施多阶段训练策略以及调查WSDC学习率调度器，并结合采用ARIES偏好学习方法的强化学习从人类反馈（RLHF），PLM在一般任务上实现了2%的性能提升，在GSM8K任务上实现了9%的提升，在编码任务上实现了11%的提升。同时，PLM系列模型已公开发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在边缘设备上存在推理需求与有限资源的矛盾。</li>
<li>周边语言模型（PLM）通过联合优化模型架构和边缘系统约束进行开发。</li>
<li>PLM采用多头潜在注意力机制和平方ReLU激活函数以降低内存占用。</li>
<li>PLM通过多阶段训练策略和学习率调度器提升性能。</li>
<li>PLM结合强化学习从人类反馈（RLHF）提高任务表现。</li>
<li>PLM在多种边缘设备上进行了部署，包括消费者级GPU、手机和Raspberry Pi等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.12167">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-142940cff76047f8cac441ddd8d084d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-957e332689b845f2b04dae14e004b300.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-297c05f6376ec4ad4815db42b33ccff5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0ca0bab6691aa042149c85b6c9d8c75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13c6d12b861dd39abed42fa8ae9baf0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4a5e9a73631efc6ab474d2cf9a3a6c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3f4ebb7fcdb92336312466784ada57e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0401b5fbcdd6d35fd9814e92318f1a5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LAG-MMLU-Benchmarking-Frontier-LLM-Understanding-in-Latvian-and-Giriama"><a href="#LAG-MMLU-Benchmarking-Frontier-LLM-Understanding-in-Latvian-and-Giriama" class="headerlink" title="LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama"></a>LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama</h2><p><strong>Authors:Naome A. Etori, Kevin Lu, Randu Karisa, Arturs Kanepajs</strong></p>
<p>As large language models (LLMs) rapidly advance, evaluating their performance is critical. LLMs are trained on multilingual data, but their reasoning abilities are mainly evaluated using English datasets. Hence, robust evaluation frameworks are needed using high-quality non-English datasets, especially low-resource languages (LRLs). This study evaluates eight state-of-the-art (SOTA) LLMs on Latvian and Giriama using a Massive Multitask Language Understanding (MMLU) subset curated with native speakers for linguistic and cultural relevance. Giriama is benchmarked for the first time. Our evaluation shows that OpenAI’s o1 model outperforms others across all languages, scoring 92.8% in English, 88.8% in Latvian, and 70.8% in Giriama on 0-shot tasks. Mistral-large (35.6%) and Llama-70B IT (41%) have weak performance, on both Latvian and Giriama. Our results underscore the need for localized benchmarks and human evaluations in advancing cultural AI contextualization. </p>
<blockquote>
<p>随着大型语言模型（LLM）的快速发展，对其性能进行评估至关重要。大型语言模型是在多语言数据上进行训练的，但它们的推理能力主要使用英语数据集进行评估。因此，需要使用高质量的非英语数据集来构建稳健的评估框架，特别是针对低资源语言（LRLs）。本研究使用由母语者筛选的巨量多任务语言理解（MMLU）子集，针对拉脱维亚语和吉里马语，对八种最先进的大型语言模型进行了评估，该子集考虑了语言和文化的相关性。吉里马语是首次进行基准测试。我们的评估结果显示，OpenAI的o1模型在所有语言中的表现均超过其他模型，在零射击任务中，英语得分为92.8%，拉脱维亚语得分为88.8%，吉里马语得分为70.8%。Mistral-large（35.6%）和Llama-70B IT（41%）在拉脱维亚语和吉里马语上的表现均较弱。我们的研究结果强调了在进行文化人工智能语境化推进时，需要本地化的基准测试和人工评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11911v2">PDF</a> Accepted at NoDaLiDa&#x2F;Baltic-HLT 2025.   <a target="_blank" rel="noopener" href="https://hdl.handle.net/10062/107190">https://hdl.handle.net/10062/107190</a></p>
<p><strong>Summary</strong></p>
<p>随着大型语言模型（LLM）的快速发展，对其性能进行评估至关重要。本研究使用本族语者整理的大型多任务语言理解（MMLU）子集，对拉脱维亚语和基里亚玛语这两种语言的八种最新大型语言模型进行了评估。研究结果显示，OpenAI的o1模型在所有语言中的表现最佳，零样本任务的得分率分别为英语92.8%、拉脱维亚语88.8%、基里亚玛语70.8%。本研究强调了针对特定区域的标准制定和人类评估在推动文化人工智能语境化中的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的评估至关重要，尤其是针对多语言数据的模型。</li>
<li>研究使用了包括拉脱维亚语和基里亚玛语在内的非英语数据集进行评估。</li>
<li>OpenAI的o1模型在多种语言中表现最佳，尤其在零样本任务上。</li>
<li>Mistral-large和Llama-70B IT在拉脱维亚语和基里亚玛语上的表现较弱。</li>
<li>研究强调了本地化基准测试和人类评估在推动文化人工智能语境化中的重要性。</li>
<li>LLMs的推理能力主要通过英语数据集进行评估，但其在其他语言中的表现可能有所不同。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b05aa50cd9f2014e576c3f871c95243e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9720102a987116c69b330ee740bfce2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e24ec3032530e7e84a43483f4661b31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c3c76c0fe74d75f789970e7de786b0d2.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-96f22e362929099cfc1528b65e5c71a8.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-03-20  Aligning Multimodal LLM with Human Preference A Survey
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b1b8c04a90717be36da770e69bba9395.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-18  One-Step Residual Shifting Diffusion for Image Super-Resolution via   Distillation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
