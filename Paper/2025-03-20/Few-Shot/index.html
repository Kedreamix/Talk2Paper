<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-03-20  JuDGE Benchmarking Judgment Document Generation for Chinese Legal   System">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-fdac063fef1430d06261f18209088ae7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-20-更新"><a href="#2025-03-20-更新" class="headerlink" title="2025-03-20 更新"></a>2025-03-20 更新</h1><h2 id="JuDGE-Benchmarking-Judgment-Document-Generation-for-Chinese-Legal-System"><a href="#JuDGE-Benchmarking-Judgment-Document-Generation-for-Chinese-Legal-System" class="headerlink" title="JuDGE: Benchmarking Judgment Document Generation for Chinese Legal   System"></a>JuDGE: Benchmarking Judgment Document Generation for Chinese Legal   System</h2><p><strong>Authors:Weihang Su, Baoqing Yue, Qingyao Ai, Yiran Hu, Jiaqi Li, Changyue Wang, Kaiyuan Zhang, Yueyue Wu, Yiqun Liu</strong></p>
<p>This paper introduces JuDGE (Judgment Document Generation Evaluation), a novel benchmark for evaluating the performance of judgment document generation in the Chinese legal system. We define the task as generating a complete legal judgment document from the given factual description of the case. To facilitate this benchmark, we construct a comprehensive dataset consisting of factual descriptions from real legal cases, paired with their corresponding full judgment documents, which serve as the ground truth for evaluating the quality of generated documents. This dataset is further augmented by two external legal corpora that provide additional legal knowledge for the task: one comprising statutes and regulations, and the other consisting of a large collection of past judgment documents. In collaboration with legal professionals, we establish a comprehensive automated evaluation framework to assess the quality of generated judgment documents across various dimensions. We evaluate various baseline approaches, including few-shot in-context learning, fine-tuning, and a multi-source retrieval-augmented generation (RAG) approach, using both general and legal-domain LLMs. The experimental results demonstrate that, while RAG approaches can effectively improve performance in this task, there is still substantial room for further improvement. All the codes and datasets are available at: <a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE">https://github.com/oneal2000/JuDGE</a>. </p>
<blockquote>
<p>本文介绍了JuDGE（判决文书生成评估）这一新的基准测试，该测试旨在评估中文法律体系中判决文书生成的性能。我们将任务定义为根据给定的案件事实描述生成完整的法律判决书。为了促进这一基准测试，我们构建了一个综合数据集，其中包括来自真实法律案件的案件事实描述及其相应的完整判决书，这些判决书作为评估生成文档质量的真实标准。该数据集通过两个外部法律语料库进行扩充，为任务提供了额外的法律知识：一个包含法规和条例，另一个则包含大量过去的判决书。我们与法律专业人士合作，建立了一个全面的自动化评估框架，从多个维度评估生成的判决书的质量。我们评估了各种基线方法，包括小样本上下文学习、微调以及多源检索增强生成（RAG）方法，使用通用和法律领域的LLMs。实验结果表明，虽然RAG方法可以有效提高此任务性能，但仍存在很大的改进空间。所有代码和数据集均可在：<a target="_blank" rel="noopener" href="https://github.com/oneal2000/JuDGE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/oneal2000/JuDGE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14258v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了JuDGE（判决文书生成评估）基准测试，该测试旨在评估中文法律系统中判决文书生成系统的性能。该任务被定义为根据给定的案件事实描述生成完整的法律判决书。为支持此基准测试，构建了包含真实法律案例的事实描述以及与相应完整判决书配对的数据集，作为评估生成文档质量的基准。此外，还通过两个外部法律语料库提供额外的法律知识，包括法规和条例以及大量过去的判决书。与法务专业人士合作，建立了一个全面的自动化评估框架，以评估生成判决书的质量。评估了几种基线方法，包括少样本上下文学习、微调以及多源检索增强生成（RAG）方法，使用通用和法律领域的LLMs。实验结果表明，虽然RAG方法可以有效提高此任务性能，但仍存在很大的改进空间。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>JuDGE是一个针对中文法律系统判决文书生成的基准测试。</li>
<li>任务是基于案件事实描述生成完整的法律判决书。</li>
<li>构建了包含真实案例的事实描述和相应判决书的综合数据集。</li>
<li>利用外部法律语料库提供额外法律知识。</li>
<li>建立了一个全面的自动化评估框架来评估生成的判决书质量。</li>
<li>评估了几种基线方法，包括少样本学习和多源检索增强生成方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f1b4b233032a53f2f3024e0cb6d1357c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c12c7e7f91bd6666772101cdba937f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4a0d37c3ddd60aee0fb04a037e968a1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Efficient-Transfer-Learning-for-Video-language-Foundation-Models"><a href="#Efficient-Transfer-Learning-for-Video-language-Foundation-Models" class="headerlink" title="Efficient Transfer Learning for Video-language Foundation Models"></a>Efficient Transfer Learning for Video-language Foundation Models</h2><p><strong>Authors:Haoxing Chen, Zizheng Huang, Yan Hong, Yanshuo Wang, Zhongcai Lyu, Zhuoer Xu, Jun Lan, Zhangxuan Gu</strong></p>
<p>Pre-trained vision-language models provide a robust foundation for efficient transfer learning across various downstream tasks. In the field of video action recognition, mainstream approaches often introduce additional modules to capture temporal information. Although the additional modules increase the capacity of model, enabling it to better capture video-specific inductive biases, existing methods typically introduce a substantial number of new parameters and are prone to catastrophic forgetting of previously acquired generalizable knowledge. In this paper, we propose a parameter-efficient Multi-modal Spatio-Temporal Adapter (MSTA) to enhance the alignment between textual and visual representations, achieving a balance between generalizable knowledge and task-specific adaptation. Furthermore, to mitigate over-fitting and enhance generalizability, we introduce a spatio-temporal description-guided consistency constraint.This constraint involves providing template inputs (e.g., “a video of {\textbf{cls}}“) to the trainable language branch and LLM-generated spatio-temporal descriptions to the pre-trained language branch, enforcing output consistency between the branches. This approach reduces overfitting to downstream tasks and enhances the distinguishability of the trainable branch within the spatio-temporal semantic space. We evaluate the effectiveness of our approach across four tasks: zero-shot transfer, few-shot learning, base-to-novel generalization, and fully-supervised learning. Compared to many state-of-the-art methods, our MSTA achieves outstanding performance across all evaluations, while using only 2-7% of the trainable parameters in the original model. </p>
<blockquote>
<p>预训练视觉语言模型为各种下游任务的高效迁移学习提供了坚实的基础。在视频动作识别领域，主流方法通常引入额外的模块来捕获时间信息。虽然额外的模块增加了模型的容量，使其能够更好地捕捉视频特定的归纳偏见，但现有方法通常引入大量新参数，并容易遗忘先前获得的可推广知识。在本文中，我们提出了一种参数有效的多模态时空适配器（MSTA），以提高文本和视觉表示之间的对齐，在可推广知识和任务特定适应之间取得平衡。此外，为了缓解过拟合并增强泛化能力，我们引入了时空描述引导的一致性约束。该约束涉及向可训练的语言分支提供模板输入（例如，“一个包含{\textbf{cls}}的视频”），并向预训练的语言分支提供LLM生成的时空描述，强制两个分支之间的输出一致性。这种方法减少了下游任务的过度拟合，增强了可训练分支在时空语义空间中的可区分性。我们在四项任务上评估了我们的方法的有效性：零样本迁移、小样例学习、基础到新颖的泛化以及完全监督学习。与许多最新方法相比，我们的MSTA在所有评估中都取得了卓越的性能，同时只使用了原始模型中2-7%的可训练参数。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11223v4">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>预训练视觉语言模型为高效跨各种下游任务的迁移学习提供了坚实的基础。针对视频动作识别领域，主流方法通常引入额外的模块来捕获时间信息。本文提出了一种参数有效的多模态时空适配器（MSTA），增强了文本和视觉表示之间的对齐，实现了通用知识和任务特定适应之间的平衡。此外，为了减轻过拟合并增强泛化能力，引入了时空描述引导的一致性约束。该约束涉及向可训练的语言分支提供模板输入（例如，“一个包含{\textbf{cls}}的视频”），并向预训练的语言分支提供LLM生成的时空描述，强制两个分支之间的输出一致性。该方法减少了过度拟合下游任务的情况，增强了可训练分支在时空语义空间内的区分度。在零样本迁移、小样学习、基础到新颖的泛化和全监督学习四项任务中验证了MSTA的有效性。与许多最先进的方法相比，我们的MSTA在所有评估中都取得了出色的表现，同时使用的可训练参数仅为原始模型的2-7%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练视觉语言模型为迁移学习提供了坚实的基础。</li>
<li>主流视频动作识别方法通过引入额外模块来捕获时间信息，但可能引入大量新参数并容易遗忘先前学到的通用知识。</li>
<li>提出了一种参数有效的多模态时空适配器（MSTA），以增强文本和视觉表示之间的对齐。</li>
<li>引入了时空描述引导的一致性约束，以提高模型的泛化能力并减少过度拟合。</li>
<li>该方法通过强制两个分支之间的输出一致性来优化模型性能。</li>
<li>MSTA在四项任务中均表现出卓越性能，使用的可训练参数相对较少。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11223">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6eebaa7194f189fe73544e02371d1d3c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e485121c9b3ff3d2ea35d1a9eea4a9fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb0d5dabe1d6ae7188ea2b661c8622fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f94d432dfb510248ca0974808cb4b02e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Action-Recognition-in-Surveillance-Videos"><a href="#Zero-Shot-Action-Recognition-in-Surveillance-Videos" class="headerlink" title="Zero-Shot Action Recognition in Surveillance Videos"></a>Zero-Shot Action Recognition in Surveillance Videos</h2><p><strong>Authors:Joao Pereira, Vasco Lopes, David Semedo, Joao Neves</strong></p>
<p>The growing demand for surveillance in public spaces presents significant challenges due to the shortage of human resources. Current AI-based video surveillance systems heavily rely on core computer vision models that require extensive finetuning, which is particularly difficult in surveillance settings due to limited datasets and difficult setting (viewpoint, low quality, etc.). In this work, we propose leveraging Large Vision-Language Models (LVLMs), known for their strong zero and few-shot generalization, to tackle video understanding tasks in surveillance. Specifically, we explore VideoLLaMA2, a state-of-the-art LVLM, and an improved token-level sampling method, Self-Reflective Sampling (Self-ReS). Our experiments on the UCF-Crime dataset show that VideoLLaMA2 represents a significant leap in zero-shot performance, with 20% boost over the baseline. Self-ReS additionally increases zero-shot action recognition performance to 44.6%. These results highlight the potential of LVLMs, paired with improved sampling techniques, for advancing surveillance video analysis in diverse scenarios. </p>
<blockquote>
<p>随着公共空间中监控需求的不断增长，由于人力资源短缺，这带来了重大挑战。当前基于人工智能的视频监控系统严重依赖于计算机视觉模型的核心技术，这需要大量的微调。然而，在监控环境中，由于数据集有限和设置困难（如观点、质量等），这尤其困难。在这项工作中，我们提议利用大型视觉语言模型（LVLMs）来解决监控中的视频理解任务，它们以强大的零样本和少样本泛化能力而闻名。具体来说，我们探索了最前沿的LVLM——VideoLLaMA2和改进的令牌级别采样方法——自我反射采样（Self-ReS）。我们在UCF-Crime数据集上的实验表明，VideoLLaMA2在零样本性能上实现了重大突破，比基线提高了20%。此外，Self-ReS将零样本动作识别性能提高了至44.6%。这些结果凸显了LVLMs与改进后的采样技术相结合在多种场景中推进监控视频分析的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21113v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AI在视频监控领域的应用面临人力资源短缺的挑战。本研究提出利用强大的零样本和少样本泛化能力的大型视觉语言模型（LVLMs）来解决视频监控中的理解任务。通过探索先进的LVLM——VideoLLaMA2和改进的token级别采样方法Self-Reflective Sampling（Self-ReS），在UCF-Crime数据集上的实验表明，VideoLLaMA2的零样本性能实现了显著的提升，相比基线有20%的提升。同时，Self-ReS将零样本动作识别性能提升至44.6%。这突显了LVLMs与改进采样技术在多种场景下的视频监控分析潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>公共空间的监控需求增长，但人力资源短缺，使得AI在视频监控领域的应用面临挑战。</li>
<li>当前AI视频监控系统主要依赖计算机视觉模型，需要大量微调，但在监控环境中由于数据集有限和设置复杂，这一任务尤为困难。</li>
<li>研究提出利用大型视觉语言模型（LVLMs）来解决视频理解任务，特别是VideoLLaMA2模型表现出强大的零样本和少样本泛化能力。</li>
<li>VideoLLaMA2模型在UCF-Crime数据集上的零样本性能显著提升，相比基线有20%的提升。</li>
<li>研究的另一个亮点是提出的改进token级别采样方法——Self-Reflective Sampling（Self-ReS），它能进一步提高零样本动作识别性能至44.6%。</li>
<li>LVLMs与改进采样技术的结合，为多种场景下的视频监控分析提供了巨大潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21113">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1798557e3a407dcf6d55bcd5d56fed7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c88425d02f7655be271b9a6afdb60a6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GeoMask3D-Geometrically-Informed-Mask-Selection-for-Self-Supervised-Point-Cloud-Learning-in-3D"><a href="#GeoMask3D-Geometrically-Informed-Mask-Selection-for-Self-Supervised-Point-Cloud-Learning-in-3D" class="headerlink" title="GeoMask3D: Geometrically Informed Mask Selection for Self-Supervised   Point Cloud Learning in 3D"></a>GeoMask3D: Geometrically Informed Mask Selection for Self-Supervised   Point Cloud Learning in 3D</h2><p><strong>Authors:Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, Milad Cheraghalikhani, Gustavo Adolfo Vargas Hakim, David Osowiechi, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers</strong></p>
<p>We introduce a pioneering approach to self-supervised learning for point clouds, employing a geometrically informed mask selection strategy called GeoMask3D (GM3D) to boost the efficiency of Masked Auto Encoders (MAE). Unlike the conventional method of random masking, our technique utilizes a teacher-student model to focus on intricate areas within the data, guiding the model’s focus toward regions with higher geometric complexity. This strategy is grounded in the hypothesis that concentrating on harder patches yields a more robust feature representation, as evidenced by the improved performance on downstream tasks. Our method also presents a complete-to-partial feature-level knowledge distillation technique designed to guide the prediction of geometric complexity utilizing a comprehensive context from feature-level information. Extensive experiments confirm our method’s superiority over State-Of-The-Art (SOTA) baselines, demonstrating marked improvements in classification, and few-shot tasks. </p>
<blockquote>
<p>我们介绍了一种用于点云的自我监督学习的开创性方法，采用了一种称为GeoMask3D（GM3D）的几何信息掩码选择策略，以提高掩码自动编码器（MAE）的效率。与传统的随机掩码方法不同，我们的技术采用师徒模型，专注于数据中的复杂区域，引导模型关注几何复杂性更高的区域。该策略基于假设，即关注更困难的补丁会产生更稳健的特征表示，如下游任务性能提高所证明的那样。我们的方法还提出了一种从全面到局部的特征级知识蒸馏技术，旨在利用特征级信息的全面上下文来指导几何复杂性的预测。大量实验证明我们的方法在最新技术基准上表现卓越，在分类和少镜头任务上表现出显着改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.12419v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种用于点云数据的自监督学习方法，采用名为GeoMask3D（GM3D）的几何信息掩码选择策略，提高Masked Auto Encoders（MAE）的效率。该方法利用教师-学生模型聚焦于数据中的复杂区域，假设关注较难的部分会得到更稳健的特征表示，并在下游任务中表现出更好的性能。此外，还提出了一种从全面特征信息中引导几何复杂度预测的全到部分特征级知识蒸馏技术。实验证明，该方法优于现有技术基线，在分类和少镜头任务中有显著改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种创新的自监督学习方法，用于点云数据的处理。</li>
<li>采用了名为GeoMask3D的几何信息掩码选择策略，提高Masked Auto Encoders的效率。</li>
<li>教师-学生模型聚焦于数据的复杂区域，以提高特征表示的稳健性。</li>
<li>假设关注较难的数据部分可以更好地在下游任务中表现。</li>
<li>提出了一种全到部分特征级别的知识蒸馏技术，用于从全面的特征信息中引导几何复杂度的预测。</li>
<li>实验证明该方法优于现有技术基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.12419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-43214466e7c247dcfe19cee30eacb84f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-92eb6eef39d9c20111377bbbbb13fa86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdac063fef1430d06261f18209088ae7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a0d42a216dc02a5a87ae4bde18a3c21.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Flatness-Improves-Backbone-Generalisation-in-Few-shot-Classification"><a href="#Flatness-Improves-Backbone-Generalisation-in-Few-shot-Classification" class="headerlink" title="Flatness Improves Backbone Generalisation in Few-shot Classification"></a>Flatness Improves Backbone Generalisation in Few-shot Classification</h2><p><strong>Authors:Rui Li, Martin Trapp, Marcus Klasson, Arno Solin</strong></p>
<p>Deployment of deep neural networks in real-world settings typically requires adaptation to new tasks with few examples. Few-shot classification (FSC) provides a solution to this problem by leveraging pre-trained backbones for fast adaptation to new classes. However, approaches for multi-domain FSC typically result in complex pipelines aimed at information fusion and task-specific adaptation without consideration of the importance of backbone training. In this work, we introduce an effective strategy for backbone training and selection in multi-domain FSC by utilizing flatness-aware training and fine-tuning. Our work is theoretically grounded and empirically performs on par or better than state-of-the-art methods despite being simpler. Further, our results indicate that backbone training is crucial for good generalisation in FSC across different adaptation methods. </p>
<blockquote>
<p>深度神经网络在实际环境中的部署通常需要适应具有少量示例的新任务。小样本分类（FSC）通过利用预训练的主干网络进行快速适应新类别来解决这个问题。然而，多域FSC的方法通常导致复杂的管道，旨在进行信息融合和任务特定适应，而没有考虑到主干训练的重要性。在这项工作中，我们介绍了一种利用平坦感知训练和微调的有效策略来进行多域FSC中的主干训练和选择。我们的工作是建立在理论基础上的，并且在实践中与最新方法的表现相当或更好，尽管它更简单。此外，我们的结果表明，主干训练对于不同适应方法在FSC中的良好泛化至关重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.07696v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了在多域小样本分类（FSC）中利用平坦感知训练和微调的有效策略进行骨干训练与选择的方法。该方法注重骨干训练的重要性，并在不同适应方法中实现良好的泛化性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>少数样本分类（FSC）利用预训练骨干网快速适应新类别。</li>
<li>多域FSC的方法通常导致复杂的管道，侧重于信息融合和任务特定适应。</li>
<li>本文提出一种有效策略进行骨干训练与选择，利用平坦感知训练和微调。</li>
<li>该策略在理论上有依据，并在实证上表现良好，与现有先进技术相当或更好，同时更为简洁。</li>
<li>强调骨干训练在小样本分类中的重要性。</li>
<li>本文方法在不同适应方法中实现良好的泛化性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.07696">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-531b3748fcdc664d44f29b0bfe8bcdc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4de11388cee4992dceb88112c9f2d687.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c15350a4df85f7e82f09137515c3dc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff46e04daf45e4d02b133de938de69d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd361c238e6673d2ba277bc7dbf8c17c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0294e2406ff8c77d28128d9e89d4183b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CHAIN-Enhancing-Generalization-in-Data-Efficient-GANs-via-lipsCHitz-continuity-constrAIned-Normalization"><a href="#CHAIN-Enhancing-Generalization-in-Data-Efficient-GANs-via-lipsCHitz-continuity-constrAIned-Normalization" class="headerlink" title="CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz   continuity constrAIned Normalization"></a>CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz   continuity constrAIned Normalization</h2><p><strong>Authors:Yao Ni, Piotr Koniusz</strong></p>
<p>Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our theoretical analyses firmly establishes CHAIN’s effectiveness in reducing gradients in latent features and weights, improving stability and generalization in GAN training. Empirical evidence supports our theory. CHAIN achieves state-of-the-art results in data-limited scenarios on CIFAR-10&#x2F;100, ImageNet, five low-shot and seven high-resolution few-shot image datasets. Code: <a target="_blank" rel="noopener" href="https://github.com/MaxwellYaoNi/CHAIN">https://github.com/MaxwellYaoNi/CHAIN</a> </p>
<blockquote>
<p>生成对抗网络（GANs）在图像生成方面取得了重大进展，但其性能严重依赖于大量的训练数据。在数据有限的情况下，GANs经常面临判别器过拟合和训练不稳定的问题。尽管批标准化（BN）已知可以提高泛化和训练稳定性，但在数据高效GAN的判别器中很少使用。我们的工作通过识别BN中的关键缺陷来解决这一问题：在中心化和缩放步骤中梯度爆炸的倾向。为了解决这一问题，我们提出了CHAIN（受lipsCHitz连续性约束的归一化），它用零均值正则化替代了传统的中心化步骤，并在缩放步骤中集成了Lipschitz连续性约束。CHAIN通过自适应地插值归一化和未归一化的特征，有效地避免了判别器过拟合，进一步增强了GAN的训练。我们的理论分析有力地证明了CHAIN在降低潜在特征和权重梯度方面的有效性，提高了GAN训练的稳定性和泛化能力。经验证据支持我们的理论。在CIFAR-10&#x2F;100、ImageNet、五个低镜头和七个高分辨率的小样本图像数据集上，CHAIN在数据有限的情况下实现了最新结果。代码：<a target="_blank" rel="noopener" href="https://github.com/MaxwellYaoNi/CHAIN">https://github.com/MaxwellYaoNi/CHAIN</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.00521v6">PDF</a> Accepted by CVPR 2024. 26 pages. Code:   <a target="_blank" rel="noopener" href="https://github.com/MaxwellYaoNi/CHAIN">https://github.com/MaxwellYaoNi/CHAIN</a></p>
<p><strong>Summary</strong><br>     工作解决了Batch Normalization在GAN训练中的数据效率问题，提出了一种新的归一化方法CHAIN，通过改进Batch Normalization中的梯度爆炸问题，有效避免判别器过拟合，提高GAN在有限数据场景下的性能和稳定性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GANs在图像生成方面取得了显著进展，但在有限数据场景下表现不佳，面临判别器过拟合和训练不稳定的问题。</li>
<li>Batch Normalization虽然能提高泛化和训练稳定性，但在数据效率高的GANs的判别器中很少使用。</li>
<li>工作发现Batch Normalization中存在梯度爆炸的问题。</li>
<li>CHAIN方法提出用零均值正则化替代传统中心化步骤，并在缩放步骤中引入Lipschitz连续性约束。</li>
<li>CHAIN通过自适应插值归一化和未归一化的特征，有效避免判别器过拟合。</li>
<li>CHAIN在CIFAR-10&#x2F;100、ImageNet、五个低分辨率和七个高分辨率的少量数据集上实现了最佳结果。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.00521">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6c66c4572087c28deb543810a7b86185.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-018c547fd327a6fd7361642ef6da157a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5239dabacd2c403a32430698362e802.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86ea643aa653ed7c1061de99abe8a132.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Learning-for-Mental-Disorder-Detection-A-Continuous-Multi-Prompt-Engineering-Approach-with-Medical-Knowledge-Injection"><a href="#Few-Shot-Learning-for-Mental-Disorder-Detection-A-Continuous-Multi-Prompt-Engineering-Approach-with-Medical-Knowledge-Injection" class="headerlink" title="Few-Shot Learning for Mental Disorder Detection: A Continuous   Multi-Prompt Engineering Approach with Medical Knowledge Injection"></a>Few-Shot Learning for Mental Disorder Detection: A Continuous   Multi-Prompt Engineering Approach with Medical Knowledge Injection</h2><p><strong>Authors:Haoxin Liu, Wenli Zhang, Jiaheng Xie, Buomsoo Kim, Zhu Zhang, Yidong Chai, Sudha Ram</strong></p>
<p>This study harnesses state-of-the-art AI technology for detecting mental disorders through user-generated textual content. Existing studies typically rely on fully supervised machine learning, which presents challenges such as the labor-intensive manual process of annotating extensive training data for each research problem and the need to design specialized deep learning architectures for each task. We propose a novel method to address these challenges by leveraging large language models and continuous multi-prompt engineering, which offers two key advantages: (1) developing personalized prompts that capture each user’s unique characteristics and (2) integrating structured medical knowledge into prompts to provide context for disease detection and facilitate predictive modeling. We evaluate our method using three widely prevalent mental disorders as research cases. Our method significantly outperforms existing methods, including feature engineering, architecture engineering, and discrete prompt engineering. Meanwhile, our approach demonstrates success in few-shot learning, i.e., requiring only a minimal number of training examples. Moreover, our method can be generalized to other rare mental disorder detection tasks with few positive labels. In addition to its technical contributions, our method has the potential to enhance the well-being of individuals with mental disorders and offer a cost-effective, accessible alternative for stakeholders beyond traditional mental disorder screening methods. </p>
<blockquote>
<p>本研究运用最先进的AI技术，通过用户生成的文本内容检测精神疾病。现有研究通常依赖于完全监督的机器学习方法，这带来了为每个研究问题对大量训练数据进行繁琐的手动标注以及为每个任务设计专门的深度学习架构的挑战。我们提出了一种新方法来解决这些挑战，该方法利用大型语言模型和连续的多提示工程，提供了两个主要优势：（1）开发个性化的提示，以捕捉每个用户的独特特征；（2）将结构化医学知识融入提示中，为疾病检测提供背景，促进预测建模。我们使用三种普遍存在的精神疾病作为研究案例来评估我们的方法。我们的方法显著优于现有方法，包括特征工程、架构工程和离散提示工程。同时，我们的方法在小样本学习方面取得了成功，即只需极少量的训练样本。此外，我们的方法可以推广到其他具有少量正面标签的罕见精神疾病检测任务。除了技术贡献外，我们的方法还有可能提高精神疾病患者的福祉，并为传统精神疾病筛查方法以外的利益相关者提供成本效益高、易于获得的替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.12988v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文利用先进的AI技术，通过用户生成的文本内容检测精神疾病。针对现有研究依赖全监督机器学习所面临的挑战，如为每个研究问题手动标注大量训练数据的劳动密集型过程和需要为每个任务设计专门的深度学习架构，我们提出了一种新方法。该方法利用大型语言模型和连续多提示工程，具有两个关键优势：一是开发能够捕捉每个用户独特特性的个性化提示，二是将结构化医学知识融入提示中，为疾病检测提供背景信息，促进预测建模。我们采用三种常见的精神疾病作为研究案例来评估我们的方法，结果显示该方法显著优于现有方法，包括特征工程、架构工程和离散提示工程。此外，该方法在少样本学习上取得了成功，即只需要极少量的训练样本。同时，我们的方法可以推广到其他具有少量阳性标签的罕见精神疾病检测任务。除了技术贡献外，该方法还有助于提高精神疾病的个体的福祉水平，并为传统精神疾病筛查方法之外的利益相关者提供成本效益高、易于访问的替代方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究使用先进的AI技术通过用户生成的文本内容检测精神疾病。</li>
<li>现有研究面临的挑战包括手动标注大量数据和设计专门架构的需求。</li>
<li>提出一种结合大型语言模型和连续多提示工程的新方法来解决这些挑战。</li>
<li>个性化提示能够捕捉用户的独特特性，同时融入医学知识以提高检测准确性。</li>
<li>该方法在三种常见精神疾病的检测上显著优于现有方法。</li>
<li>该方法在少样本学习上表现出优势，并有望推广到罕见精神疾病的检测任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.12988">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a6e8ba6783455233875fbe09e36cfe69.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MAC-SQL-A-Multi-Agent-Collaborative-Framework-for-Text-to-SQL"><a href="#MAC-SQL-A-Multi-Agent-Collaborative-Framework-for-Text-to-SQL" class="headerlink" title="MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL"></a>MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL</h2><p><strong>Authors:Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, LinZheng Chai, Zhao Yan, Qian-Wen Zhang, Di Yin, Xing Sun, Zhoujun Li</strong></p>
<p>Recent LLM-based Text-to-SQL methods usually suffer from significant performance degradation on “huge” databases and complex user questions that require multi-step reasoning. Moreover, most existing methods neglect the crucial significance of LLMs utilizing external tools and model collaboration. To address these challenges, we introduce MAC-SQL, a novel LLM-based multi-agent collaborative framework. Our framework comprises a core decomposer agent for Text-to-SQL generation with few-shot chain-of-thought reasoning, accompanied by two auxiliary agents that utilize external tools or models to acquire smaller sub-databases and refine erroneous SQL queries. The decomposer agent collaborates with auxiliary agents, which are activated as needed and can be expanded to accommodate new features or tools for effective Text-to-SQL parsing. In our framework, We initially leverage GPT-4 as the strong backbone LLM for all agent tasks to determine the upper bound of our framework. We then fine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging Code Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that SQL-Llama achieves a comparable execution accuracy of 43.94, compared to the baseline accuracy of 46.35 for vanilla GPT-4. At the time of writing, MAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the BIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test set (<a target="_blank" rel="noopener" href="https://github.com/wbbeyourself/MAC-SQL">https://github.com/wbbeyourself/MAC-SQL</a>). </p>
<blockquote>
<p>当前基于大型语言模型（LLM）的文本到SQL方法在处理“庞大”数据库和需要多步骤推理的复杂用户问题时，通常会出现显著的性能下降。此外，大多数现有方法忽略了利用外部工具和模型协作在LLM中的关键作用。为了应对这些挑战，我们引入了MAC-SQL，这是一种基于LLM的新型多智能体协作框架。我们的框架包括一个用于文本到SQL生成的核心分解器智能体，它具有少镜头思维链推理能力，并配备两个辅助智能体，用于利用外部工具或模型获取较小的子数据库并修正错误的SQL查询。分解器智能体与辅助智能体进行协作，辅助智能体根据需要被激活，并且可以扩展以容纳新特性或工具，从而实现有效的文本到SQL解析。在我们的框架中，我们首先利用GPT-4作为所有智能体任务的主要LLM后盾，以确定我们框架的上限。然后，我们通过利用Code Llama 7B对开源的遵循指令模型SQL-Llama进行微调，以完成GPT-4完成的任务。实验表明，SQL-Llama的执行精度达到了与基准精度相当的43.94%，而GPT-4的基准精度为46.35%。在撰写本文时，MAC-SQL+GPT-4在BIRD基准测试集上的执行精度达到了59.59%，在其保留的测试集上创下了新的最先进的性能记录（<a target="_blank" rel="noopener" href="https://github.com/wbbeyourself/MAC-SQL%EF%BC%89%E3%80%82">https://github.com/wbbeyourself/MAC-SQL）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.11242v6">PDF</a> Accepted by COLING 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>基于LLM的文本到SQL生成方法在大数据库和复杂用户问题上的性能下降显著，且忽视利用外部工具和模型协作的重要性。为解决这些问题，我们提出了MAC-SQL框架，该框架包含核心分解器、GPT-4等技术与辅助代理结合使用，以解决各种文本到SQL生成的问题。采用GPT-4作为基础LLM，精细化对开源的SQL指令进行编程来测试MAC-SQL的效力，初步证明了其效能。该框架已在BIRD基准测试集上实现了卓越的性能表现。有关最新成果详参GitHub地址。对于大量数据与高难度的推理问题处理更趋高效稳定，显示了卓越的未来潜力与应用前景。相关研究开辟了一条处理大规模复杂数据查询的便捷高效的新路径。利用智能与合作的结合，为自然语言处理领域带来革命性变革。更多信息参见GitHub链接。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM-based Text-to-SQL方法在大数据库和复杂用户问题上性能受限，需解决多步骤推理挑战。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.11242">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-af4231f613b84be422eb764c165139f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f211aaf9b8682fcdf10782dd7e355ec4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30019c8ff4071c5413a82923fa3b0e21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15ad9d94115d6285a44de5cae107ba08.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-620a6239123a8c73bdd87fa9c3a38a6d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-20/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-26e35e5ae27340594607504207c8cfcd.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-03-20  Improving LLM Video Understanding with 16 Frames Per Second
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1bfcf16d2f1dcb31142c9a8c39dd0b14.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-03-20  DARS Dynamic Action Re-Sampling to Enhance Coding Agent Performance by   Adaptive Tree Traversal
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">13632.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
