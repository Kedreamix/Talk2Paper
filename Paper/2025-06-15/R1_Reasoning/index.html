<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-15  ChartReasoner Code-Driven Modality Bridging for Long-Chain Reasoning in   Chart Question Answering">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-9eb026e43b0e88695451e9c2a8a04736.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    92 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-15-æ›´æ–°"><a href="#2025-06-15-æ›´æ–°" class="headerlink" title="2025-06-15 æ›´æ–°"></a>2025-06-15 æ›´æ–°</h1><h2 id="ChartReasoner-Code-Driven-Modality-Bridging-for-Long-Chain-Reasoning-in-Chart-Question-Answering"><a href="#ChartReasoner-Code-Driven-Modality-Bridging-for-Long-Chain-Reasoning-in-Chart-Question-Answering" class="headerlink" title="ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in   Chart Question Answering"></a>ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in   Chart Question Answering</h2><p><strong>Authors:Caijun Jia, Nan Xu, Jingxuan Wei, Qingli Wang, Lei Wang, Bihui Yu, Junnan Zhu</strong></p>
<p>Recently, large language models have shown remarkable reasoning capabilities through long-chain reasoning before responding. However, how to extend this capability to visual reasoning tasks remains an open challenge. Existing multimodal reasoning approaches transfer such visual reasoning task into textual reasoning task via several image-to-text conversions, which often lose critical structural and semantic information embedded in visualizations, especially for tasks like chart question answering that require a large amount of visual details. To bridge this gap, we propose ChartReasoner, a code-driven novel two-stage framework designed to enable precise, interpretable reasoning over charts. We first train a high-fidelity model to convert diverse chart images into structured ECharts codes, preserving both layout and data semantics as lossless as possible. Then, we design a general chart reasoning data synthesis pipeline, which leverages this pretrained transport model to automatically and scalably generate chart reasoning trajectories and utilizes a code validator to filter out low-quality samples. Finally, we train the final multimodal model using a combination of supervised fine-tuning and reinforcement learning on our synthesized chart reasoning dataset and experimental results on four public benchmarks clearly demonstrate the effectiveness of our proposed ChartReasoner. It can preserve the original details of the charts as much as possible and perform comparably with state-of-the-art open-source models while using fewer parameters, approaching the performance of proprietary systems like GPT-4o in out-of-domain settings. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡é•¿é“¾æ¨ç†åº”ç­”å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œç„¶è€Œï¼Œå¦‚ä½•å°†è¿™ä¸€èƒ½åŠ›æ‹“å±•åˆ°è§†è§‰æ¨ç†ä»»åŠ¡ä»æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ç°æœ‰çš„å¤šæ¨¡æ€æ¨ç†æ–¹æ³•é€šè¿‡å¤šæ¬¡å›¾åƒåˆ°æ–‡æœ¬çš„è½¬æ¢ï¼Œå°†è§†è§‰æ¨ç†ä»»åŠ¡è½¬æ¢ä¸ºæ–‡æœ¬æ¨ç†ä»»åŠ¡ï¼Œè¿™å¾€å¾€ä¼šä¸¢å¤±å¯è§†åŒ–ä¸­åµŒå…¥çš„å…³é”®ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤§é‡è§†è§‰ç»†èŠ‚çš„ä»»åŠ¡ï¼ˆå¦‚å›¾è¡¨é—®ç­”ï¼‰ä¸­ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ChartReasonerï¼Œè¿™æ˜¯ä¸€ç§ä»£ç é©±åŠ¨çš„æ–°å‹ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¯¹å›¾è¡¨çš„ç²¾ç¡®ã€å¯è§£é‡Šæ¨ç†ã€‚æˆ‘ä»¬é¦–å…ˆè®­ç»ƒä¸€ä¸ªé«˜ä¿çœŸæ¨¡å‹ï¼Œå°†å„ç§å›¾è¡¨å›¾åƒè½¬æ¢ä¸ºç»“æ„åŒ–çš„EChartsä»£ç ï¼Œå°½å¯èƒ½æ— æŸåœ°ä¿ç•™å¸ƒå±€å’Œæ•°æ®è¯­ä¹‰ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé€šç”¨çš„å›¾è¡¨æ¨ç†æ•°æ®åˆæˆç®¡é“ï¼Œå®ƒåˆ©ç”¨è¿™ä¸ªé¢„è®­ç»ƒçš„ä¼ è¾“æ¨¡å‹æ¥è‡ªåŠ¨å’Œå¯æ‰©å±•åœ°ç”Ÿæˆå›¾è¡¨æ¨ç†è½¨è¿¹ï¼Œå¹¶åˆ©ç”¨ä»£ç éªŒè¯å™¨æ¥è¿‡æ»¤æ‰ä½è´¨é‡æ ·æœ¬ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨åˆæˆçš„å›¾è¡¨æ¨ç†æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„æ–¹æ³•è®­ç»ƒæœ€ç»ˆçš„å¤šæ¨¡æ€æ¨¡å‹ã€‚åœ¨å››ä¸ªå…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœæ¸…æ¥šåœ°è¯æ˜äº†æˆ‘ä»¬çš„ChartReasonerçš„æœ‰æ•ˆæ€§ã€‚å®ƒèƒ½å¤Ÿå°½å¯èƒ½ä¿ç•™å›¾è¡¨çš„åŸå§‹ç»†èŠ‚ï¼Œä¸ä½¿ç”¨è¾ƒå°‘å‚æ•°çš„å…ˆè¿›å¼€æºæ¨¡å‹ç›¸æ¯”è¡¨ç°ç›¸å½“ï¼Œå¹¶åœ¨åŸŸå¤–ç¯å¢ƒä¸­æ¥è¿‘GPT-4oç­‰ä¸“æœ‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10116v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡å›¾åƒåˆ°æ–‡æœ¬çš„è½¬æ¢æ¥å®Œæˆè§†è§‰æ¨ç†ä»»åŠ¡ï¼Œä½†è¿™ç§è½¬æ¢å¸¸å¯¼è‡´é‡è¦ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯çš„ä¸¢å¤±ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ChartReasonerï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„ã€ä»¥ä»£ç é©±åŠ¨çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºå®ç°ç²¾ç¡®çš„å›¾è¡¨æ¨ç†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªé«˜ä¿çœŸæ¨¡å‹ï¼Œå°†å„ç§å›¾è¡¨å›¾åƒè½¬æ¢ä¸ºç»“æ„åŒ–çš„EChartsä»£ç ï¼Œå°½å¯èƒ½ä¿æŒå¸ƒå±€å’Œæ•°æ®è¯­ä¹‰çš„æ— æŸæ€§ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé€šç”¨çš„å›¾è¡¨æ¨ç†æ•°æ®åˆæˆç®¡é“ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„ä¼ è¾“æ¨¡å‹è‡ªåŠ¨ã€å¤§è§„æ¨¡åœ°ç”Ÿæˆå›¾è¡¨æ¨ç†è½¨è¿¹ï¼Œå¹¶ä½¿ç”¨ä»£ç éªŒè¯å™¨è¿‡æ»¤æ‰ä½è´¨é‡æ ·æœ¬ã€‚æœ€åï¼Œåœ¨åˆæˆçš„å›¾è¡¨æ¨ç†æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„ChartReasoneræ¨¡å‹åœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿå°½å¯èƒ½ä¿ç•™åŸå§‹å›¾è¡¨ç»†èŠ‚ï¼Œä¸ä½¿ç”¨è¾ƒå°‘å‚æ•°çš„å…ˆè¿›å¼€æºæ¨¡å‹ç›¸æ¯”è¡¨ç°ç›¸å½“ï¼Œå¹¶åœ¨åŸŸå¤–è®¾ç½®ä¸­æ¥è¿‘GPT-4oç­‰ä¸“æœ‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡å›¾åƒåˆ°æ–‡æœ¬çš„è½¬æ¢å®Œæˆè§†è§‰æ¨ç†ä»»åŠ¡ï¼Œä½†è¿™ç§è½¬æ¢ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚</li>
<li>ChartReasoneræ˜¯ä¸€ä¸ªæ–°é¢–çš„ã€ä»¥ä»£ç é©±åŠ¨çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºå®ç°ç²¾ç¡®çš„å›¾è¡¨æ¨ç†ã€‚</li>
<li>ChartReasoneråŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šé«˜ä¿çœŸæ¨¡å‹è®­ç»ƒã€å›¾è¡¨æ¨ç†æ•°æ®åˆæˆå’Œæ¨¡å‹è®­ç»ƒä¸æµ‹è¯•ã€‚</li>
<li>é«˜ä¿çœŸæ¨¡å‹å°†å›¾è¡¨å›¾åƒè½¬æ¢ä¸ºç»“æ„åŒ–çš„EChartsä»£ç ï¼Œä¿æŒå¸ƒå±€å’Œæ•°æ®è¯­ä¹‰çš„æ— æŸæ€§ã€‚</li>
<li>ChartReasoneråœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿä¿ç•™åŸå§‹å›¾è¡¨ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10116">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3934fa0a5276e67c19bf293deb88ee76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81c7f20b99fc0dbbf76aa83e21544b51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04618deefd659203c3884c421d8c6ae2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b02f920c4de63624fd2089b5e8f55c3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Omni-DPO-A-Dual-Perspective-Paradigm-for-Dynamic-Preference-Learning-of-LLMs"><a href="#Omni-DPO-A-Dual-Perspective-Paradigm-for-Dynamic-Preference-Learning-of-LLMs" class="headerlink" title="Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of   LLMs"></a>Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of   LLMs</h2><p><strong>Authors:Shangpin Peng, Weinong Wang, Zhuotao Tian, Senqiao Yang, Xing Wu, Haotian Xu, Chengquan Zhang, Takashi Isobe, Baotian Hu, Min Zhang</strong></p>
<p>Direct Preference Optimization (DPO) has become a cornerstone of reinforcement learning from human feedback (RLHF) due to its simplicity and efficiency. However, existing DPO-based approaches typically treat all preference pairs uniformly, ignoring critical variations in their inherent quality and learning utility, leading to suboptimal data utilization and performance. To address this challenge, we propose Omni-DPO, a dual-perspective optimization framework that jointly accounts for (1) the inherent quality of each preference pair and (2) the modelâ€™s evolving performance on those pairs. By adaptively weighting samples according to both data quality and the modelâ€™s learning dynamics during training, Omni-DPO enables more effective training data utilization and achieves better performance. Experimental results on various models and benchmarks demonstrate the superiority and generalization capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning tasks, Omni-DPO consistently outperforms the baseline methods across all benchmarks, providing strong empirical evidence for the effectiveness and robustness of our approach. Code and models will be available at <a target="_blank" rel="noopener" href="https://github.com/pspdada/Omni-DPO">https://github.com/pspdada/Omni-DPO</a>. </p>
<blockquote>
<p>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å› å…¶ç®€å•é«˜æ•ˆè€Œæˆä¸ºå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­çš„åŸºçŸ³ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºDPOçš„æ–¹æ³•é€šå¸¸å¯¹æ‰€æœ‰åå¥½å¯¹ä¸€è§†åŒä»ï¼Œå¿½ç•¥äº†å®ƒä»¬å†…åœ¨è´¨é‡å’Œå­¦ä¹ æ•ˆç”¨çš„é‡è¦å·®å¼‚ï¼Œå¯¼è‡´æ•°æ®åˆ©ç”¨ä¸è¶³å’Œæ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Omni-DPOï¼Œè¿™æ˜¯ä¸€ä¸ªåŒè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼ŒåŒæ—¶è€ƒè™‘äº†ï¼ˆ1ï¼‰æ¯ä¸ªåå¥½å¯¹çš„å†…åœ¨è´¨é‡å’Œï¼ˆ2ï¼‰æ¨¡å‹åœ¨è¿™äº›å¯¹ä¸Šçš„ä¸æ–­å˜åŒ–çš„æ€§èƒ½ã€‚Omni-DPOé€šè¿‡æ ¹æ®æ•°æ®è´¨é‡å’Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ åŠ¨æ€æ¥é€‚åº”æ€§åœ°åŠ æƒæ ·æœ¬ï¼Œä»è€Œå®ç°äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®åˆ©ç”¨å’Œæ›´å¥½çš„æ€§èƒ½ã€‚åœ¨å„ç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜äº†Omni-DPOçš„ä¼˜è¶Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨Omni-DPOå¾®è°ƒè¿‡çš„Gemma-2-9b-itåœ¨Arena-HardåŸºå‡†æµ‹è¯•ä¸Šå¤§å¹…è¶…è¶Šäº†é¢†å…ˆçš„LLM Claude 3 Opusï¼Œå¾—åˆ†é«˜å‡º6.7åˆ†ã€‚åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ŒOmni-DPOåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šå‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œä¸ºæˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§æä¾›äº†å¼ºæœ‰åŠ›çš„å®è¯è¯æ®ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/pspdada/Omni-DPO%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/pspdada/Omni-DPOä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10054v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸­çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­çš„æ ¸å¿ƒæ–¹æ³•ï¼Œå› å…¶ç®€å•é«˜æ•ˆè€Œå—åˆ°é‡è§†ã€‚ç„¶è€Œï¼Œç°æœ‰DPOæ–¹æ³•å¿½ç•¥åå¥½å¯¹å†…åœ¨è´¨é‡å’Œå­¦ä¹ æ•ˆç”¨çš„å·®å¼‚ï¼Œå¯¼è‡´æ•°æ®åˆ©ç”¨å’Œæ€§èƒ½ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºOmni-DPOï¼Œä¸€ä¸ªåŒè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼ŒåŒæ—¶è€ƒè™‘æ¯ä¸ªåå¥½å¯¹çš„å†…åœ¨è´¨é‡å’Œæ¨¡å‹çš„åŠ¨æ€æ€§èƒ½ã€‚é€šè¿‡æ ¹æ®æ•°æ®è´¨é‡å’Œæ¨¡å‹å­¦ä¹ åŠ¨æ€è‡ªé€‚åº”åœ°åŠ æƒæ ·æœ¬ï¼ŒOmni-DPOå®ç°äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®åˆ©ç”¨å’Œæ›´å¥½çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜Omni-DPOçš„ä¼˜è¶Šæ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­çš„æ ¸å¿ƒæ–¹æ³•ã€‚</li>
<li>ç°æœ‰DPOæ–¹æ³•å¿½ç•¥åå¥½å¯¹å†…åœ¨è´¨é‡å’Œå­¦ä¹ æ•ˆç”¨çš„å·®å¼‚ï¼Œå¯¼è‡´æ•°æ®åˆ©ç”¨å’Œæ€§èƒ½ä¸ä½³ã€‚</li>
<li>Omni-DPOæ˜¯ä¸€ä¸ªåŒè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼Œè€ƒè™‘æ¯ä¸ªåå¥½å¯¹çš„å†…åœ¨è´¨é‡å’Œæ¨¡å‹çš„åŠ¨æ€æ€§èƒ½ã€‚</li>
<li>Omni-DPOé€šè¿‡è‡ªé€‚åº”åŠ æƒæ ·æœ¬ï¼Œå®ç°äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®åˆ©ç”¨ã€‚</li>
<li>Omni-DPOåœ¨å¤šç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>åœ¨æ–‡æœ¬ç†è§£ä»»åŠ¡ä¸Šï¼Œä½¿ç”¨Omni-DPOçš„Gemma-2-9b-itæ¨¡å‹åœ¨Arena-HardåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—è¶…è¶Šäº†é¢†å…ˆçš„LLMæ¨¡å‹Claude 3 Opusã€‚</li>
<li>åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ŒOmni-DPOåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šå‡è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†å¼ºæœ‰åŠ›çš„å®è¯è¯æ®æ”¯æŒå…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f192e1d33209918e9a755104108eb40b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be5212410d8dbca68f18da9b522bfc2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7461ef40973494bcdf867abd8d70009e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc07bcd8b7020bab08b649acab6ce25c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eeba7dce5597e9015424fb2dd804c2e6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CoRT-Code-integrated-Reasoning-within-Thinking"><a href="#CoRT-Code-integrated-Reasoning-within-Thinking" class="headerlink" title="CoRT: Code-integrated Reasoning within Thinking"></a>CoRT: Code-integrated Reasoning within Thinking</h2><p><strong>Authors:Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, Dayiheng Liu</strong></p>
<p>Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable progress in natural language reasoning with long chain-of-thought (CoT), yet they remain inefficient or inaccurate when handling complex mathematical operations. Addressing these limitations through computational tools (e.g., computation libraries and symbolic solvers) is promising, but it introduces a technical challenge: Code Interpreter (CI) brings external knowledge beyond the modelâ€™s internal text representations, thus the direct combination is not efficient. This paper introduces CoRT, a post-training framework for teaching LRMs to leverage CI effectively and efficiently. As a first step, we address the data scarcity issue by synthesizing code-integrated reasoning data through Hint-Engineering, which strategically inserts different hints at appropriate positions to optimize LRM-CI interaction. We manually create 30 high-quality samples, upon which we post-train models ranging from 1.5B to 32B parameters, with supervised fine-tuning, rejection fine-tuning and reinforcement learning. Our experimental results demonstrate that Hint-Engineering models achieve 4% and 8% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging mathematical reasoning datasets. Furthermore, Hint-Engineering models use about 30% fewer tokens for the 32B model and 50% fewer tokens for the 1.5B model compared with the natural language models. The models and code are available at <a target="_blank" rel="noopener" href="https://github.com/ChengpengLi1003/CoRT">https://github.com/ChengpengLi1003/CoRT</a>. </p>
<blockquote>
<p>åƒO1å’ŒDeepSeek-R1ç­‰å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œå°¤å…¶æ˜¯åœ¨é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ–¹é¢ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å¤æ‚çš„æ•°å­¦è¿ç®—æ—¶ä»ç„¶æ•ˆç‡ä½ä¸‹æˆ–ä¸å¤Ÿå‡†ç¡®ã€‚é€šè¿‡è®¡ç®—å·¥å…·ï¼ˆä¾‹å¦‚è®¡ç®—åº“å’Œç¬¦å·æ±‚è§£å™¨ï¼‰æ¥è§£å†³è¿™äº›é™åˆ¶å¾ˆæœ‰å¸Œæœ›ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æŠ€æœ¯æŒ‘æˆ˜ï¼šä»£ç è§£é‡Šå™¨ï¼ˆCIï¼‰å¼•å…¥äº†æ¨¡å‹å†…éƒ¨æ–‡æœ¬è¡¨ç¤ºä¹‹å¤–çš„å¤–éƒ¨çŸ¥è¯†ï¼Œå› æ­¤ç›´æ¥ç»„åˆå¹¶ä¸é«˜æ•ˆã€‚æœ¬æ–‡ä»‹ç»äº†CoRTï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ•™æˆLRMæœ‰æ•ˆä¸”é«˜æ•ˆåœ°ä½¿ç”¨CIçš„åæœŸè®­ç»ƒæ¡†æ¶ã€‚ä½œä¸ºç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬é€šè¿‡Hint-Engineeringåˆæˆä»£ç é›†æˆæ¨ç†æ•°æ®æ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œè¿™æ˜¯ä¸€ç§æˆ˜ç•¥æ€§åœ°åœ¨ä¸åŒä½ç½®æ’å…¥æç¤ºä»¥ä¼˜åŒ–LRM-CIäº¤äº’çš„æ–¹æ³•ã€‚æˆ‘ä»¬æ‰‹åŠ¨åˆ›å»ºäº†30ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œåœ¨è¿™äº›æ ·æœ¬ä¸Šå¯¹èŒƒå›´ä»1.5Båˆ°32Bå‚æ•°çš„æ¨¡å‹è¿›è¡Œå†è®­ç»ƒï¼Œé‡‡ç”¨æœ‰ç›‘ç£å¾®è°ƒã€æ‹’ç»å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šï¼ŒHint-Engineeringæ¨¡å‹åœ¨DeepSeek-R1-Distill-Qwen-32Bå’ŒDeepSeek-R1-Distill-Qwen-1.5Bä¸Šåˆ†åˆ«å®ç°äº†4%å’Œ8%çš„ç»å¯¹æ”¹è¿›ã€‚æ­¤å¤–ï¼Œä¸è‡ªç„¶è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒHint-Engineeringæ¨¡å‹ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡å‡å°‘äº†çº¦30%ï¼ˆé’ˆå¯¹32Bæ¨¡å‹ï¼‰å’Œ50%ï¼ˆé’ˆå¯¹1.5Bæ¨¡å‹ï¼‰ã€‚æ¨¡å‹å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ChengpengLi1003/CoRT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ChengpengLi1003/CoRTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09820v2">PDF</a> work in progress</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚o1å’ŒDeepSeek-R1åœ¨è‡ªç„¶è¯­è¨€æ¨ç†æ–¹é¢å±•ç°å‡ºæ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤æ‚æ•°å­¦è¿ç®—æ—¶ä»å­˜æ•ˆç‡æˆ–å‡†ç¡®åº¦é—®é¢˜ã€‚æœ¬æ–‡é€šè¿‡è®¡ç®—å·¥å…·ï¼ˆå¦‚è®¡ç®—åº“å’Œç¬¦å·æ±‚è§£å™¨ï¼‰æ¥è§£å†³è¿™äº›é™åˆ¶å…·æœ‰æ½œåŠ›ï¼Œä½†è¿™ä¹Ÿå¸¦æ¥äº†æŠ€æœ¯æŒ‘æˆ˜ã€‚æ¨¡å‹ä»¥å¤–çš„çŸ¥è¯†æ— æ³•é€šè¿‡æ¨¡å‹çš„å†…éƒ¨æ–‡æœ¬è¡¨ç¤ºæ¥å¼•å…¥ï¼Œå› æ­¤ç›´æ¥ç»„åˆå¹¶ä¸é«˜æ•ˆã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCoRTçš„åæœŸè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯æœ‰æ•ˆåœ°æ•™æˆLRMså¦‚ä½•åˆ©ç”¨è®¡ç®—è§£é‡Šå™¨ï¼ˆCIï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹ï¼ˆHint-Engineeringï¼‰åˆæˆä»£ç é›†æˆæ¨ç†æ•°æ®ï¼Œåœ¨é€‚å½“çš„ä½ç½®æ’å…¥ä¸åŒçš„æç¤ºä»¥ä¼˜åŒ–LRM-CIäº¤äº’ã€‚æˆ‘ä»¬åœ¨30ä¸ªé«˜è´¨é‡æ ·æœ¬ä¸Šè¿›è¡Œäº†åæœŸè®­ç»ƒï¼Œè¿™äº›æ¨¡å‹å‚æ•°èŒƒå›´ä»1.5Båˆ°32Bï¼Œé‡‡ç”¨ç›‘ç£å¾®è°ƒã€æ‹’ç»å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæç¤ºå·¥ç¨‹æ¨¡å‹åœ¨DeepSeek-R1-Distill-Qwen-32Bå’ŒDeepSeek-R1-Distill-Qwen-1.5Bä¸Šåˆ†åˆ«å®ç°äº†4%å’Œ8%çš„ç»å¯¹æ”¹è¿›ï¼Œè·¨è¶Šäº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæç¤ºå·¥ç¨‹æ¨¡å‹ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡æ¯”è‡ªç„¶è¯­è¨€æ¨¡å‹å°‘çº¦30%ï¼ˆå¯¹äº32Bæ¨¡å‹ï¼‰å’Œ50%ï¼ˆå¯¹äº1.5Bæ¨¡å‹ï¼‰ã€‚æ¨¡å‹å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ChengpengLi1003/CoRT">https://github.com/ChengpengLi1003/CoRT</a>è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ•°å­¦è¿ç®—æ—¶å­˜åœ¨æ•ˆç‡å’Œå‡†ç¡®åº¦é—®é¢˜ã€‚</li>
<li>è®¡ç®—å·¥å…·ï¼ˆå¦‚è®¡ç®—åº“å’Œç¬¦å·æ±‚è§£å™¨ï¼‰ä¸ºè§£å†³è¿™äº›é—®é¢˜æä¾›äº†æ½œåŠ›ï¼Œä½†éœ€è¦è§£å†³æŠ€æœ¯æŒ‘æˆ˜ã€‚</li>
<li>CoRTæ¡†æ¶æ˜¯ä¸€ç§ç”¨äºæ•™æˆå¤§å‹æ¨ç†æ¨¡å‹æœ‰æ•ˆåˆ©ç”¨è®¡ç®—è§£é‡Šå™¨çš„æ–¹æ³•ã€‚</li>
<li>æç¤ºå·¥ç¨‹æ˜¯ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œç”¨äºåˆæˆä»£ç é›†æˆæ¨ç†æ•°æ®ï¼Œä¼˜åŒ–LRMä¸CIçš„äº¤äº’ã€‚</li>
<li>é€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œæç¤ºå·¥ç¨‹æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>æç¤ºå·¥ç¨‹æ¨¡å‹ä½¿ç”¨çš„ä»¤ç‰Œæ•°é‡ç›¸è¾ƒäºè‡ªç„¶è¯­è¨€æ¨¡å‹æœ‰æ‰€å‡å°‘ï¼Œè¡¨æ˜å…¶æ•ˆç‡æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09820">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-13b57656464c975d55cb3ff85ada8251.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6190eaa22e4fbe05d7c76da35c849f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e7d4970150bc91f66e3c5ea0f0ec73e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e65320635251df07fdde30a7cad51442.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b3a42da7f9dde91730323417395427d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-and-Effective-Alignment-of-Large-Language-Models"><a href="#Towards-Efficient-and-Effective-Alignment-of-Large-Language-Models" class="headerlink" title="Towards Efficient and Effective Alignment of Large Language Models"></a>Towards Efficient and Effective Alignment of Large Language Models</h2><p><strong>Authors:Yuxin Jiang</strong></p>
<p>Large language models (LLMs) exhibit remarkable capabilities across diverse tasks, yet aligning them efficiently and effectively with human expectations remains a critical challenge. This thesis advances LLM alignment by introducing novel methodologies in data collection, training, and evaluation. We first address alignment data collection. Existing approaches rely heavily on manually curated datasets or proprietary models. To overcome these limitations, we propose Lion, an adversarial distillation framework that iteratively refines training data by identifying and generating challenging instructions, enabling state-of-the-art zero-shot reasoning. Additionally, we introduce Web Reconstruction (WebR), a fully automated framework that synthesizes instruction-tuning data directly from raw web documents, significantly improving data diversity and scalability over existing synthetic data methods. Next, we enhance alignment training through novel optimization techniques. We develop Learning to Edit (LTE), a framework that enables LLMs to efficiently integrate new knowledge while preserving existing information. LTE leverages meta-learning to improve both real-time and batch knowledge updates. Furthermore, we introduce Bridging and Modeling Correlations (BMC), a refinement of Direct Preference Optimization (DPO) that explicitly captures token-level correlations in preference data, leading to superior alignment across QA and mathematical reasoning tasks. Finally, we tackle the challenge of evaluating alignment. Existing benchmarks emphasize response quality but overlook adherence to specific constraints. To bridge this gap, we introduce FollowBench, a multi-level, fine-grained benchmark assessing LLMsâ€™ ability to follow complex constraints across diverse instruction types. Our results expose key weaknesses in current modelsâ€™ constraint adherence, offering insights for future improvements. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆä¸”é«˜æ•ˆåœ°å°†å…¶ä¸äººç±»æœŸæœ›è¿›è¡Œå¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æœ¬è®ºæ–‡é€šè¿‡å¼•å…¥æ•°æ®æ”¶é›†ã€è®­ç»ƒå’Œè¯„ä¼°æ–¹é¢çš„æ–°å‹æ–¹æ³•è®ºæ¥ä¿ƒè¿›LLMçš„å¯¹é½ã€‚æˆ‘ä»¬é¦–å…ˆè§£å†³å¯¹é½æ•°æ®æ”¶é›†çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºäººå·¥ç¼–åˆ¶çš„æ•°æ®é›†æˆ–ä¸“æœ‰æ¨¡å‹ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Lionï¼Œä¸€ç§å¯¹æŠ—è’¸é¦æ¡†æ¶ï¼Œå®ƒé€šè¿‡è¯†åˆ«ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„æŒ‡ä»¤æ¥è¿­ä»£ä¼˜åŒ–è®­ç»ƒæ•°æ®ï¼Œä»è€Œå®ç°æœ€å…ˆè¿›çš„é›¶å°„å‡»ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†WebRï¼ˆç½‘ç»œé‡å»ºï¼‰ï¼Œä¸€ä¸ªå…¨è‡ªåŠ¨çš„æ¡†æ¶ï¼Œç›´æ¥ä»åŸå§‹ç½‘é¡µæ–‡æ¡£ä¸­åˆæˆæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œå¤§å¤§æé«˜äº†æ•°æ®å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ï¼Œè¶…è¿‡äº†ç°æœ‰çš„åˆæˆæ•°æ®æ–¹æ³•ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡æ–°å‹ä¼˜åŒ–æŠ€æœ¯å¢å¼ºå¯¹é½è®­ç»ƒã€‚æˆ‘ä»¬å¼€å‘äº†Learning to Editï¼ˆLTEï¼‰ï¼Œä¸€ä¸ªæ¡†æ¶ï¼Œä½¿LLMèƒ½å¤Ÿé«˜æ•ˆé›†æˆæ–°çŸ¥è¯†çš„åŒæ—¶ä¿ç•™ç°æœ‰ä¿¡æ¯ã€‚LTEåˆ©ç”¨å…ƒå­¦ä¹ æ¥æé«˜å®æ—¶å’Œæ‰¹é‡çŸ¥è¯†æ›´æ–°çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œäº†æ”¹è¿›ï¼Œæ¨å‡ºäº†Bridging and Modeling Correlationsï¼ˆBMCï¼‰ï¼Œå®ƒèƒ½å¤Ÿæ˜ç¡®æ•æ‰åå¥½æ•°æ®ä¸­çš„ä»¤ç‰Œçº§å…³è”ï¼Œä»è€Œåœ¨é—®ç­”å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å®ç°å¯¹é½çš„å“è¶Šè¡¨ç°ã€‚æœ€åï¼Œæˆ‘ä»¬è§£å†³äº†è¯„ä¼°å¯¹é½çš„æŒ‘æˆ˜ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¾§é‡äºå“åº”è´¨é‡ï¼Œä½†å¿½è§†äº†ç‰¹å®šçš„çº¦æŸæ¡ä»¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FollowBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šå±‚æ¬¡ã€ç²¾ç»†çš„åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°LLMéµå¾ªå„ç§æŒ‡ä»¤ç±»å‹ä¸­å¤æ‚çº¦æŸçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨çº¦æŸéµå¾ªæ–¹é¢çš„å…³é”®å¼±ç‚¹ï¼Œä¸ºæœªæ¥çš„æ”¹è¿›æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09329v1">PDF</a> PhD thesis</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­å±•ç°å‡ºæƒŠäººçš„èƒ½åŠ›ï¼Œä½†å¦‚ä½•æœ‰æ•ˆåœ°ä¸äººç±»æœŸæœ›å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥æ•°æ®æ”¶é›†ã€è®­ç»ƒå’Œè¯„ä¼°æ–¹é¢çš„æ–°æ–¹æ³•ï¼Œæ¨åŠ¨äº†LLMçš„å¯¹é½æ€§ã€‚é€šè¿‡å¯¹æŠ—è’¸é¦æ¡†æ¶Lionå’ŒWeb Reconstructionï¼ˆWebRï¼‰å…¨è‡ªåŠ¨æ¡†æ¶æ¥è§£å†³æ•°æ®æ”¶é›†é—®é¢˜ï¼Œæ˜¾è‘—æé«˜æ•°æ®çš„å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œè¿˜é€šè¿‡ä¼˜åŒ–æŠ€æœ¯æé«˜å¯¹é½è®­ç»ƒæ•ˆæœï¼Œå¼€å‘äº†Learning to Editï¼ˆLTEï¼‰æ¡†æ¶å’ŒBridging and Modeling Correlationsï¼ˆBMCï¼‰æ–¹æ³•ã€‚æœ€åï¼Œæœ¬ç ”ç©¶è§£å†³äº†è¯„ä¼°å¯¹é½æ€§çš„æŒ‘æˆ˜ï¼Œå¼•å…¥äº†FollowBenchå¤šå±‚æ¬¡ç²¾ç»†åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°LLMséµå¾ªå¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ã€‚ç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨éµå¾ªçº¦æŸæ–¹é¢çš„å¼±ç‚¹ï¼Œä¸ºæœªæ¥çš„æ”¹è¿›æä¾›äº†é‡è¦çº¿ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ ·ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å¯¹é½äºäººç±»æœŸæœ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥å¯¹æŠ—è’¸é¦æ¡†æ¶Lionå’ŒWeb Reconstructionï¼ˆWebRï¼‰è§£å†³æ•°æ®æ”¶é›†é—®é¢˜ï¼Œæé«˜æ•°æ®å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>é‡‡ç”¨ä¼˜åŒ–æŠ€æœ¯å¢å¼ºå¯¹é½è®­ç»ƒæ•ˆæœï¼ŒåŒ…æ‹¬Learning to Editï¼ˆLTEï¼‰å’ŒBridging and Modeling Correlationsï¼ˆBMCï¼‰ã€‚</li>
<li>ç°æœ‰è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å“åº”è´¨é‡ï¼Œå¿½è§†äº†ç‰¹å®šçº¦æŸçš„éµå¾ªæƒ…å†µã€‚</li>
<li>å¼•å…¥FollowBenchå¤šå±‚æ¬¡ç²¾ç»†åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°LLMséµå¾ªå¤æ‚æŒ‡ä»¤çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å‘ç°å½“å‰æ¨¡å‹åœ¨éµå¾ªçº¦æŸæ–¹é¢å­˜åœ¨å…³é”®å¼±ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b65663b1692ba3809ee620897d5329c7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Intra-Trajectory-Consistency-for-Reward-Modeling"><a href="#Intra-Trajectory-Consistency-for-Reward-Modeling" class="headerlink" title="Intra-Trajectory Consistency for Reward Modeling"></a>Intra-Trajectory Consistency for Reward Modeling</h2><p><strong>Authors:Chaoyang Zhou, Shunyu Liu, Zengmao Wang, Di Wang, Rong-Cheng Tu, Bo Du, Dacheng Tao</strong></p>
<p>Reward models are critical for improving large language models (LLMs), particularly in reinforcement learning from human feedback (RLHF) or inference-time verification. Current reward modeling typically relies on scores of overall responses to learn the outcome rewards for the responses. However, since the response-level scores are coarse-grained supervision signals, the reward model struggles to identify the specific components within a response trajectory that truly correlate with the scores, leading to poor generalization on unseen responses. In this paper, we propose to leverage generation probabilities to establish reward consistency between processes in the response trajectory, which allows the response-level supervisory signal to propagate across processes, thereby providing additional fine-grained signals for reward learning. Building on analysis under the Bayesian framework, we develop an intra-trajectory consistency regularization to enforce that adjacent processes with higher next-token generation probability maintain more consistent rewards. We apply the proposed regularization to the advanced outcome reward model, improving its performance on RewardBench. Besides, we show that the reward model trained with the proposed regularization induces better DPO-aligned policies and achieves better best-of-N (BON) inference-time verification results. Our code is provided in <a target="_blank" rel="noopener" href="https://github.com/chaoyang101/ICRM">https://github.com/chaoyang101/ICRM</a>. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹å¯¹äºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æˆ–æ¨ç†æ—¶é—´éªŒè¯ä¸­ã€‚å½“å‰çš„å¥–åŠ±å»ºæ¨¡é€šå¸¸ä¾èµ–äºæ•´ä½“å“åº”çš„åˆ†æ•°æ¥å­¦ä¹ å“åº”çš„ç»“æœå¥–åŠ±ã€‚ç„¶è€Œï¼Œç”±äºå“åº”çº§åˆ«çš„åˆ†æ•°æ˜¯ç²—ç²’åº¦çš„ç›‘ç£ä¿¡å·ï¼Œå¥–åŠ±æ¨¡å‹éš¾ä»¥è¯†åˆ«å“åº”è½¨è¿¹å†…çœŸæ­£ä¸åˆ†æ•°ç›¸å…³çš„ç‰¹å®šç»„ä»¶ï¼Œå¯¼è‡´åœ¨æœªè§è¿‡çš„å“åº”ä¸Šæ³›åŒ–æ€§èƒ½å·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨ç”Ÿæˆæ¦‚ç‡æ¥å»ºç«‹å“åº”è½¨è¿¹ä¸­è¿‡ç¨‹ä¹‹é—´çš„å¥–åŠ±ä¸€è‡´æ€§ï¼Œè¿™å…è®¸å“åº”çº§åˆ«çš„ç›‘ç£ä¿¡å·åœ¨è¿‡ç¨‹ä¹‹é—´ä¼ æ’­ï¼Œä»è€Œä¸ºå¥–åŠ±å­¦ä¹ æä¾›é¢å¤–çš„ç»†ç²’åº¦ä¿¡å·ã€‚æˆ‘ä»¬åœ¨è´å¶æ–¯æ¡†æ¶çš„åˆ†æåŸºç¡€ä¸Šï¼Œå¼€å‘äº†ä¸€ç§è½¨è¿¹å†…ä¸€è‡´æ€§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»¥å¼ºåˆ¶å…·æœ‰æ›´é«˜ä¸‹ä¸€ä¸ªä»¤ç‰Œç”Ÿæˆæ¦‚ç‡çš„ç›¸é‚»è¿‡ç¨‹ä¿æŒæ›´ä¸€è‡´çš„å¥–åŠ±ã€‚æˆ‘ä»¬å°†æ‰€æå‡ºçš„æ­£åˆ™åŒ–åº”ç”¨äºé«˜çº§ç»“æœå¥–åŠ±æ¨¡å‹ï¼Œåœ¨RewardBenchä¸Šæé«˜äº†å…¶æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä½¿ç”¨æ‰€æå‡ºæ­£åˆ™åŒ–è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹èƒ½è¯±å‘æ›´å¥½çš„DPOå¯¹é½ç­–ç•¥ï¼Œå¹¶åœ¨æœ€ä½³Nï¼ˆBONï¼‰æ¨ç†æ—¶é—´éªŒè¯ä¸­å®ç°æ›´å¥½çš„ç»“æœã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/chaoyang101/ICRM%E3%80%82">https://github.com/chaoyang101/ICRMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09096v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºåˆ©ç”¨ç”Ÿæˆæ¦‚ç‡å»ºç«‹å“åº”è½¨è¿¹ä¸­å¥–åŠ±ä¸€è‡´æ€§ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¥–åŠ±æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥è½¨è¿¹å†…ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œä½¿ç›¸é‚»çš„é«˜ç”Ÿæˆæ¦‚ç‡çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œä¿æŒæ›´ä¸€è‡´çš„å¥–åŠ±ï¼Œä»è€Œæé«˜å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨RewardBenchä¸Šè¡¨ç°å‡ºè¾ƒå¥½çš„æ€§èƒ½ï¼Œå¹¶å®ç°äº†æ›´å¥½çš„ç­–ç•¥å¯¹é½å’Œæœ€ä½³NéªŒè¯ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹å¯¹æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ å’Œæ¨ç†æ—¶é—´éªŒè¯æ–¹é¢ã€‚</li>
<li>å½“å‰å¥–åŠ±å»ºæ¨¡é€šå¸¸ä¾èµ–äºæ•´ä½“å“åº”çš„åˆ†æ•°æ¥å­¦ä¹ ç»“æœå¥–åŠ±ï¼Œä½†è¿™ç§æ–¹å¼éš¾ä»¥è¯†åˆ«ä¸åˆ†æ•°çœŸæ­£ç›¸å…³çš„å“åº”è½¨è¿¹ä¸­çš„ç‰¹å®šç»„ä»¶ã€‚</li>
<li>å¼•å…¥ç”Ÿæˆæ¦‚ç‡æ¥å»ºç«‹å“åº”è½¨è¿¹ä¸­çš„å¥–åŠ±ä¸€è‡´æ€§ï¼Œä½¿å“åº”çº§åˆ«çš„ç›‘ç£ä¿¡å·å¯ä»¥åœ¨è½¨è¿¹è¿‡ç¨‹ä¸­ä¼ æ’­ï¼Œä¸ºå¥–åŠ±å­¦ä¹ æä¾›é¢å¤–çš„ç²¾ç»†ç²’åº¦ä¿¡å·ã€‚</li>
<li>å¼€å‘å‡ºè½¨è¿¹å†…ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œå¼ºåˆ¶ç›¸é‚»çš„é«˜ç”Ÿæˆæ¦‚ç‡çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œä¿æŒæ›´ä¸€è‡´çš„å¥–åŠ±ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨RewardBenchä¸Šçš„æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œè¯´æ˜è¯¥æ¨¡å‹èƒ½æœ‰æ•ˆæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹å¯ä»¥äº§ç”Ÿæ›´å¥½çš„ç­–ç•¥å¯¹é½ï¼Œè¿™æ„å‘³ç€æ¨¡å‹èƒ½æ›´å¥½åœ°åæ˜ äººç±»çš„æ„å›¾å’ŒæœŸæœ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d89ec5f9811055befe02f475ee813dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df0ef3f5818cd7c76d078ffb5107a134.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-149b92e7dae17f505b8c5a2d5b406af7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DeepForm-Reasoning-Large-Language-Model-for-Communication-System-Formulation"><a href="#DeepForm-Reasoning-Large-Language-Model-for-Communication-System-Formulation" class="headerlink" title="DeepForm: Reasoning Large Language Model for Communication System   Formulation"></a>DeepForm: Reasoning Large Language Model for Communication System   Formulation</h2><p><strong>Authors:Panlong Wu, Ting Wang, Yifei Zhong, Haoqi Zhang, Zitong Wang, Fangxin Wang</strong></p>
<p>Communication system formulation is critical for advancing 6G and future wireless technologies, yet it remains a complex, expertise-intensive task. While Large Language Models (LLMs) offer potential, existing general-purpose models often lack the specialized domain knowledge, nuanced reasoning capabilities, and access to high-quality, domain-specific training data required for adapting a general LLM into an LLM specially for communication system formulation. To bridge this gap, we introduce DeepForm, the first reasoning LLM specially for automated communication system formulation. We propose the world-first large-scale, open-source dataset meticulously curated for this domain called Communication System Formulation Reasoning Corpus (CSFRC). Our framework employs a two-stage training strategy: first, Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge; second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated reasoning patterns like self-correction and verification. Extensive experiments demonstrate that our model achieves state-of-the-art performance, significantly outperforming larger proprietary LLMs on diverse senerios. We will release related resources to foster further research in this area after the paper is accepted. </p>
<blockquote>
<p>é€šä¿¡ç³»ç»Ÿå»ºæ¨¡æ˜¯æ¨åŠ¨6Gå’Œæœªæ¥æ— çº¿æŠ€æœ¯å‘å±•çš„å…³é”®ï¼Œä½†è¿™æ˜¯ä¸€é¡¹å¤æ‚ä¸”éœ€è¦ä¸“ä¸šæŠ€èƒ½çš„ä»»åŠ¡ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰æ½œåŠ›ï¼Œä½†ç°æœ‰çš„é€šç”¨æ¨¡å‹é€šå¸¸ç¼ºä¹ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ã€å¾®å¦™çš„æ¨ç†èƒ½åŠ›å’Œè®¿é—®é«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸçš„è®­ç»ƒæ•°æ®ï¼Œè¿™äº›å‡æ˜¯é€‚åº”é€šç”¨LLMä»¥ç”¨äºé€šä¿¡ç³»ç»Ÿå»ºæ¨¡æ‰€å¿…éœ€çš„ã€‚ä¸ºäº†å¼¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†DeepFormï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè‡ªåŠ¨åŒ–é€šä¿¡ç³»ç»Ÿå»ºæ¨¡çš„æ¨ç†LLMã€‚æˆ‘ä»¬æå‡ºäº†ä¸–ç•Œä¸Šç¬¬ä¸€ä¸ªé’ˆå¯¹æ­¤é¢†åŸŸç²¾å¿ƒç­–åˆ’çš„å¤§è§„æ¨¡å¼€æºæ•°æ®é›†ï¼Œåä¸ºé€šä¿¡ç³»ç»Ÿå»ºæ¨¡æ¨ç†è¯­æ–™åº“ï¼ˆCSFRCï¼‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆï¼Œä½¿ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä»¥æç‚¼é¢†åŸŸçŸ¥è¯†ï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨åŸºäºReMaxçš„æ–°å‹åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•C-ReMaxï¼Œä»¥åŸ¹å…»å…ˆè¿›çš„å»ºæ¨¡èƒ½åŠ›å¹¶æ¿€å‘è‡ªæˆ‘ä¿®æ­£å’ŒéªŒè¯ç­‰å¤æ‚æ¨ç†æ¨¡å¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å¤šç§åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºæ›´å¤§çš„ä¸“æœ‰LLMã€‚è®ºæ–‡è¢«æ¥å—åï¼Œæˆ‘ä»¬å°†å‘å¸ƒç›¸å…³èµ„æºä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08551v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†é€šä¿¡ç³»ç»Ÿçš„æ„å»ºå¯¹äºæ¨åŠ¨6Gå’Œæœªæ¥æ— çº¿æŠ€æœ¯çš„å‘å±•è‡³å…³é‡è¦ã€‚æ–‡ç« æŒ‡å‡ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šä¿¡ç³»ç»Ÿçš„æ„å»ºä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†ç°æœ‰çš„ä¸€èˆ¬æ¨¡å‹ç¼ºä¹ä¸“ä¸šé¢†åŸŸçŸ¥è¯†ã€æ¨ç†èƒ½åŠ›å’Œé«˜è´¨é‡é¢†åŸŸç‰¹å®šè®­ç»ƒæ•°æ®çš„é€‚åº”æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†DeepFormï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè‡ªåŠ¨åŒ–é€šä¿¡ç³»ç»Ÿæ„å»ºè®¾è®¡çš„æ¨ç†LLMã€‚åŒæ—¶ä»‹ç»äº†ä¸“ä¸ºè¯¥é¢†åŸŸç²¾å¿ƒç­–åˆ’çš„å¤§å‹å¼€æºæ•°æ®é›†Communication System Formulation Reasoning Corpusï¼ˆCSFRCï¼‰ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆä½¿ç”¨å¸¦æœ‰æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥æç‚¼é¢†åŸŸçŸ¥è¯†ï¼›å…¶æ¬¡æ˜¯åŸºäºReMaxçš„C-ReMaxæ–°å‹è§„åˆ™å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥åŸ¹å…»å…ˆè¿›çš„å»ºæ¨¡èƒ½åŠ›å’Œæ¿€å‘å¤æ‚çš„æ¨ç†æ¨¡å¼ï¼Œå¦‚è‡ªæˆ‘ä¿®æ­£å’ŒéªŒè¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§åœºæ™¯ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºæ›´å¤§çš„ä¸“æœ‰LLMã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šä¿¡ç³»ç»Ÿçš„æ„å»ºå¯¹æ¨åŠ¨æœªæ¥æ— çº¿æŠ€æœ¯çš„å‘å±•è‡³å…³é‡è¦ï¼Œéœ€è¦ä¸“ä¸šçš„çŸ¥è¯†å’ŒæŠ€æœ¯ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šä¿¡ç³»ç»Ÿæ„å»ºä¸­æœ‰åº”ç”¨æ½œåŠ›ï¼Œä½†ç°æœ‰æ¨¡å‹å­˜åœ¨é¢†åŸŸé€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>DeepFormæ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè‡ªåŠ¨åŒ–é€šä¿¡ç³»ç»Ÿæ„å»ºè®¾è®¡çš„æ¨ç†LLMï¼Œå¯ä»¥è§£å†³ç°æœ‰æ¨¡å‹çš„ä¸è¶³ã€‚</li>
<li>å¼•å…¥äº†åä¸ºCSFRCçš„å¤§å‹å¼€æºæ•°æ®é›†ï¼Œä¸“ä¸ºé€šä¿¡ç³»ç»Ÿæ„å»ºé¢†åŸŸç²¾å¿ƒç­–åˆ’ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŸºäºReMaxçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šç§åœºæ™¯ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96f024661a3eeb2473891f4550aa2746.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54a7a598f79451e0d2527daebd18c931.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc07cd428912766da80b3235fa737a6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-209fe704ab1a6bd5cd81ea3c45ab3ba3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61a04ebf85bcef9131d586d42838e055.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MasHost-Builds-It-All-Autonomous-Multi-Agent-System-Directed-by-Reinforcement-Learning"><a href="#MasHost-Builds-It-All-Autonomous-Multi-Agent-System-Directed-by-Reinforcement-Learning" class="headerlink" title="MasHost Builds It All: Autonomous Multi-Agent System Directed by   Reinforcement Learning"></a>MasHost Builds It All: Autonomous Multi-Agent System Directed by   Reinforcement Learning</h2><p><strong>Authors:Kuo Yang, Xingjie Yang, Linhui Yu, Qing Xu, Yan Fang, Xu Wang, Zhengyang Zhou, Yang Wang</strong></p>
<p>Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently emerged as a powerful paradigm for tackling complex real-world tasks. However, existing Mas construction methods typically rely on manually crafted interaction mechanisms or heuristic rules, introducing human biases and constraining the autonomous ability. Even with recent advances in adaptive Mas construction, existing systems largely remain within the paradigm of semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement Learning (RL)-based framework for autonomous and query-adaptive Mas design. By formulating Mas construction as a graph search problem, our proposed MasHost jointly samples agent roles and their interactions through a unified probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives pursued in prior works, we introduce component rationality as an additional and novel design principle in Mas. To achieve this multi-objective optimization, we propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy that collaboratively integrates group-relative advantages and action-wise rewards. To our knowledge, our proposed MasHost is the first RL-driven framework for autonomous Mas graph construction. Extensive experiments on six benchmarks demonstrate that MasHost consistently outperforms most competitive baselines, validating its effectiveness, efficiency, and structure rationality. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä»¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMasï¼‰å·²ä½œä¸ºä¸€ç§å¼ºå¤§çš„èŒƒå¼ï¼Œç”¨äºå¤„ç†å¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„Masæ„å»ºæ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨æ„å»ºäº¤äº’æœºåˆ¶æˆ–å¯å‘å¼è§„åˆ™ï¼Œè¿™å¼•å…¥äº†äººç±»åè§å¹¶é™åˆ¶äº†è‡ªä¸»æ€§èƒ½åŠ›ã€‚å°½ç®¡è‡ªé€‚åº”Masæ„å»ºæ–¹é¢æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œä½†ç°æœ‰ç³»ç»Ÿå¤§å¤šä»åœç•™åœ¨åŠè‡ªä¸»æ¨¡å¼çš„èŒƒå¼å†…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MasHostï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è‡ªä¸»å’ŒæŸ¥è¯¢è‡ªé€‚åº”Masè®¾è®¡æ¡†æ¶ã€‚é€šè¿‡å°†Masæ„å»ºåˆ¶å®šä¸ºå›¾æœç´¢é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºçš„MasHosté€šè¿‡ç»Ÿä¸€çš„æ¦‚ç‡é‡‡æ ·æœºåˆ¶è”åˆé‡‡æ ·æ™ºèƒ½ä½“è§’è‰²åŠå…¶äº¤äº’ã€‚é™¤äº†å…ˆå‰å·¥ä½œä¸­è¿½æ±‚å‡†ç¡®æ€§å’Œæ•ˆç‡ç›®æ ‡å¤–ï¼Œæˆ‘ä»¬è¿˜å°†ç»„ä»¶åˆç†æ€§ä½œä¸ºMasä¸­çš„é™„åŠ å’Œæ–°é¢–è®¾è®¡åŸåˆ™ã€‚ä¸ºäº†å®ç°è¿™ä¸€å¤šç›®æ ‡ä¼˜åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆHRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹RLç­–ç•¥ï¼Œèƒ½å¤ŸååŒæ•´åˆç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿å’Œè¡ŒåŠ¨å±‚é¢å¥–åŠ±ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æå‡ºçš„MasHostæ˜¯ç¬¬ä¸€ä¸ªç”¨äºè‡ªä¸»Maså›¾æ„å»ºçš„RLé©±åŠ¨æ¡†æ¶ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMasHostå§‹ç»ˆä¼˜äºå¤§å¤šæ•°ç«äº‰åŸºçº¿ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œç»“æ„åˆç†æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08507v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMasï¼‰å·²æˆä¸ºè§£å†³å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡çš„æœ‰åŠ›èŒƒå¼ã€‚ç„¶è€Œï¼Œç°æœ‰æ„å»ºæ–¹æ³•é€šå¸¸ä¾èµ–äººå·¥è®¾è®¡çš„äº¤äº’æœºåˆ¶å’Œå¯å‘å¼è§„åˆ™ï¼Œå¯¼è‡´äººç±»åè§å¹¶é™åˆ¶äº†è‡ªä¸»æ€§èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è‡ªä¸»ã€æŸ¥è¯¢è‡ªé€‚åº”çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡æ–¹æ¡ˆï¼Œå‘½åä¸ºMasHostã€‚MasHosté€šè¿‡åˆ¶å®šå›¾å½¢æœç´¢é—®é¢˜æ¥å®ç°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ„å»ºï¼Œè”åˆé‡‡æ ·æ™ºèƒ½ä½“çš„è§’è‰²åŠå…¶äº¤äº’ä½œç”¨ï¼Œå¹¶ä¸”é¦–æ¬¡å¼•å…¥äº†ç»„ä»¶ç†æ€§ä½œä¸ºæ–°çš„è®¾è®¡åŸåˆ™ã€‚ä¸ºäº†è¿›è¡Œå¤šç›®æ ‡ä¼˜åŒ–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥â€”â€”åˆ†å±‚ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆHRPOï¼‰ï¼Œå®ƒç»“åˆäº†ç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿å’Œè¡ŒåŠ¨å¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼ŒMasHoståœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šéƒ½è¡¨ç°ä¼˜äºå¤§å¤šæ•°ç«äº‰æ–¹æ¡ˆï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œç»“æ„åˆç†æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå·²æˆä¸ºè§£å†³å¤æ‚ä»»åŠ¡çš„é‡è¦èŒƒå¼ã€‚</li>
<li>å½“å‰æ„å»ºæ–¹æ³•ä¾èµ–äººå·¥è®¾è®¡æœºåˆ¶ï¼Œé™åˆ¶äº†ç³»ç»Ÿçš„è‡ªä¸»æ€§å¹¶å¼•å…¥äººç±»åè§ã€‚</li>
<li>MasHostæ˜¯é¦–ä¸ªä½¿ç”¨å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå›¾å½¢æ„å»ºæ¡†æ¶ï¼Œèƒ½è‡ªä¸»è”åˆé‡‡æ ·æ™ºèƒ½ä½“è§’è‰²åŠå…¶äº¤äº’ã€‚</li>
<li>MasHostå¼•å…¥äº†ç»„ä»¶ç†æ€§ä½œä¸ºæ–°çš„è®¾è®¡åŸåˆ™ï¼Œå¹¶ç»“åˆç¾¤ä½“ç›¸å¯¹ä¼˜åŠ¿å’Œè¡ŒåŠ¨å¥–åŠ±è¿›è¡Œå¤šç›®æ ‡ä¼˜åŒ–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91b162fb0a877651d4ab47dc8c08745f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6970965be6c1c999a66f43b64ceeb17e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TGRPO-Fine-tuning-Vision-Language-Action-Model-via-Trajectory-wise-Group-Relative-Policy-Optimization"><a href="#TGRPO-Fine-tuning-Vision-Language-Action-Model-via-Trajectory-wise-Group-Relative-Policy-Optimization" class="headerlink" title="TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise   Group Relative Policy Optimization"></a>TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise   Group Relative Policy Optimization</h2><p><strong>Authors:Zengjue Chen, Runliang Niu, He Kong, Qi Wang</strong></p>
<p>Recent advances in Vision-Language-Action (VLA) model have demonstrated strong generalization capabilities across diverse scenes, tasks, and robotic platforms when pretrained at large-scale datasets. However, these models still require task-specific fine-tuning in novel environments, a process that relies almost exclusively on supervised fine-tuning (SFT) using static trajectory datasets. Such approaches neither allow robot to interact with environment nor do they leverage feedback from live execution. Also, their success is critically dependent on the size and quality of the collected trajectories. Reinforcement learning (RL) offers a promising alternative by enabling closed-loop interaction and aligning learned policies directly with task objectives. In this work, we draw inspiration from the ideas of GRPO and propose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method. By fusing step-level and trajectory-level advantage signals, this method improves GRPOâ€™s group-level advantage estimation, thereby making the algorithm more suitable for online reinforcement learning training of VLA. Experimental results on ten manipulation tasks from the libero-object benchmark demonstrate that TGRPO consistently outperforms various baseline methods, capable of generating more robust and efficient policies across multiple tested scenarios. Our source codes are available at: <a target="_blank" rel="noopener" href="https://github.com/hahans/TGRPO">https://github.com/hahans/TGRPO</a> </p>
<blockquote>
<p>è¿‘æœŸVision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹çš„è¿›å±•è¡¨æ˜ï¼Œåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒåï¼Œè¿™äº›æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ã€ä»»åŠ¡å’Œæœºå™¨äººå¹³å°ä¹‹é—´å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æ–°ç¯å¢ƒä¸­ä»éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œè¿™ä¸€è¿‡ç¨‹å‡ ä¹å®Œå…¨ä¾èµ–äºä½¿ç”¨é™æ€è½¨è¿¹æ•°æ®é›†çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚è¿™äº›æ–¹æ³•æ—¢ä¸å…è®¸æœºå™¨äººä¸ç¯å¢ƒäº’åŠ¨ï¼Œä¹Ÿæ²¡æœ‰åˆ©ç”¨å®æ—¶æ‰§è¡Œåé¦ˆã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„æˆåŠŸä¸¥é‡ä¾èµ–äºæ”¶é›†çš„è½¨è¿¹çš„å¤§å°å’Œè´¨é‡ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒé€šè¿‡å®ç°é—­ç¯äº¤äº’å’Œå¯¹é½å­¦ä¹ æ”¿ç­–ä¸ä»»åŠ¡ç›®æ ‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»GRPOçš„æ€æƒ³ä¸­æ±²å–çµæ„Ÿï¼Œæå‡ºäº†è½¨è¿¹çº§ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTGRPOï¼‰æ–¹æ³•ã€‚é€šè¿‡èåˆæ­¥éª¤çº§å’Œè½¨è¿¹çº§ä¼˜åŠ¿ä¿¡å·ï¼Œè¯¥æ–¹æ³•æ”¹è¿›äº†GRPOçš„ç¾¤ä½“çº§ä¼˜åŠ¿ä¼°è®¡ï¼Œä»è€Œä½¿è¯¥ç®—æ³•æ›´é€‚åˆåœ¨çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒVLAã€‚åœ¨libero-objectåŸºå‡†æµ‹è¯•ä¸­çš„10ä¸ªæ“ä½œä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTGRPOæŒç»­ä¼˜äºå„ç§åŸºçº¿æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæµ‹è¯•åœºæ™¯ä¸­ç”Ÿæˆæ›´ç¨³å¥å’Œé«˜æ•ˆçš„ç­–ç•¥ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/hahans/TGRPO%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hahans/TGRPOè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08440v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹åœ¨å¤šæ ·åœºæ™¯ã€ä»»åŠ¡å’Œæœºå™¨äººå¹³å°ä¸Šçš„æ³›åŒ–èƒ½åŠ›æœ‰æ‰€çªç ´ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æ–°ç¯å¢ƒä¸­ä»éœ€è¦ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼Œè¿™ä¸€è¿‡ç¨‹å‡ ä¹å®Œå…¨ä¾èµ–äºä½¿ç”¨é™æ€è½¨è¿¹æ•°æ®é›†è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚è¿™ç§æ–¹æ³•æ—¢ä¸å…è®¸æœºå™¨äººä¸ç¯å¢ƒäº’åŠ¨ï¼Œä¹Ÿæ²¡æœ‰åˆ©ç”¨å®æ—¶æ‰§è¡Œçš„åé¦ˆã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡é—­ç¯äº’åŠ¨å’Œå¯¹ä»»åŠ¡ç›®æ ‡çš„ç›´æ¥ç­–ç•¥å¯¹é½ã€‚æœ¬ç ”ç©¶å—GRPOçš„å¯å‘ï¼Œæå‡ºäº†è½¨è¿¹çº§ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTGRPOï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•èåˆæ­¥éª¤çº§å’Œè½¨è¿¹çº§ä¼˜åŠ¿ä¿¡å·ï¼Œæ”¹è¿›äº†GRPOçš„ç¾¤ç»„çº§ä¼˜åŠ¿ä¼°è®¡ï¼Œä½¿ç®—æ³•æ›´é€‚åˆåœ¨çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒVLAã€‚åœ¨libero-objectåŸºå‡†çš„åä¸ªæ“ä½œä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTGRPOæŒç»­ä¼˜äºå„ç§åŸºçº¿æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæµ‹è¯•åœºæ™¯ä¸­ç”Ÿæˆæ›´ç¨³å¥å’Œé«˜æ•ˆçš„ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLAæ¨¡å‹è™½å…·å¤‡æ³›åŒ–èƒ½åŠ›ï¼Œä½†ä»éœ€åœ¨æ–°ç¯å¢ƒä¸­è¿›è¡Œç‰¹å®šä»»åŠ¡å¾®è°ƒã€‚</li>
<li>å½“å‰å¾®è°ƒæ–¹æ³•ä¸»è¦ä¾èµ–é™æ€è½¨è¿¹æ•°æ®é›†çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œé™åˆ¶æœºå™¨äººä¸ç¯å¢ƒäº’åŠ¨åŠå®æ—¶åé¦ˆåˆ©ç”¨ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸ºè§£å†³é—®é¢˜æä¾›æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡é—­ç¯äº’åŠ¨ç›´æ¥å¯¹é½ç­–ç•¥ä¸ä»»åŠ¡ç›®æ ‡ã€‚</li>
<li>TGRPOæ–¹æ³•ç»“åˆæ­¥éª¤çº§å’Œè½¨è¿¹çº§ä¼˜åŠ¿ä¿¡å·ï¼Œæ”¹è¿›GRPOçš„ç¾¤ç»„çº§ä¼˜åŠ¿ä¼°è®¡ã€‚</li>
<li>TGRPOé€‚åˆåœ¨çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒVLAï¼Œå¹¶åœ¨å¤šä¸ªæ“ä½œä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>TGRPOåœ¨libero-objectåŸºå‡†çš„åä¸ªä»»åŠ¡ä¸ŠæŒç»­ä¼˜äºå„ç§åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc0c44caa6ec5a5827ce2913d06cb7c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee49b90ee07ddd1a6f9444df7276d13c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Play-to-Generalize-Learning-to-Reason-Through-Game-Play"><a href="#Play-to-Generalize-Learning-to-Reason-Through-Game-Play" class="headerlink" title="Play to Generalize: Learning to Reason Through Game Play"></a>Play to Generalize: Learning to Reason Through Game Play</h2><p><strong>Authors:Yunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, Chen Wei</strong></p>
<p>Developing generalizable reasoning capabilities in multimodal large language models (MLLMs) remains challenging. Motivated by cognitive science literature suggesting that gameplay promotes transferable cognitive skills, we propose a novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs develop out-of-domain generalization of multimodal reasoning through playing arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM via reinforcement learning (RL) on simple arcade-like games, e.g. Snake, significantly enhances its downstream performance on multimodal math benchmarks like MathVista, and on multi-discipline questions like MMMU, without seeing any worked solutions, equations, or diagrams during RL, suggesting the capture of transferable reasoning skills. Remarkably, our model outperforms specialist models tuned on multimodal reasoning data in multimodal reasoning benchmarks, while preserving the base modelâ€™s performance on general visual benchmarks, a challenge where specialist models often fall short. Our findings suggest a new post-training paradigm: synthetic, rule-based games can serve as controllable and scalable pre-text tasks that unlock generalizable multimodal reasoning abilities in MLLMs. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å‘å±•å¯æ¨å¹¿çš„æ¨ç†èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å—è®¤çŸ¥ç§‘å­¦æ–‡çŒ®çš„å¯å‘ï¼Œè¯¥æ–‡çŒ®è¡¨æ˜æ¸¸æˆç©æ³•å¯ä»¥ä¿ƒè¿›å¯è¿ç§»çš„è®¤çŸ¥æŠ€èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åè®­ç»ƒèŒƒå¼ï¼Œå³è§†è§‰æ¸¸æˆå­¦ä¹ ï¼ˆViGaLï¼‰ã€‚åœ¨æ­¤èŒƒå¼ä¸­ï¼ŒMLLMsé€šè¿‡ç©ç±»ä¼¼è¡—æœºæ¸¸æˆæ¥å‘å±•è·¨åŸŸçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹å…·æœ‰7Bå‚æ•°çš„MLLMè¿›è¡Œåè®­ç»ƒï¼Œä½¿å…¶åœ¨ç®€å•çš„ç±»ä¼¼è¡—æœºæ¸¸æˆï¼ˆå¦‚Snakeï¼‰ä¸Šçš„è¡¨ç°æ˜¾è‘—å¢å¼ºã€‚å…¶åœ¨å¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MathVistaï¼‰å’Œå¤šå­¦ç§‘é—®é¢˜ï¼ˆå¦‚MMMUï¼‰ä¸Šçš„ä¸‹æ¸¸æ€§èƒ½å¾—åˆ°äº†æå‡ï¼Œä¸”åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­æœªè§ä»»ä½•è§£å†³æ–¹æ¡ˆã€æ–¹ç¨‹å¼æˆ–å›¾è¡¨ï¼Œè¿™è¡¨æ˜å…¶æŒæ¡äº†å¯è¿ç§»çš„æ¨ç†æŠ€èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šç§æ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç»è¿‡è¯¥ä»»åŠ¡ä¸“é—¨è®­ç»ƒçš„æ¨¡å‹ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬è§†è§‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¹Ÿä¸åŸºç¡€æ¨¡å‹ç›¸å½“ï¼Œè€Œè¿™æ˜¯ä¸€ä¸ªè®¸å¤šä¸“ä¸šæ¨¡å‹è¡¨ç°ä¸è¶³çš„é¢†åŸŸã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†ä¸€ç§æ–°å‹åè®­ç»ƒèŒƒå¼ï¼šåˆæˆã€åŸºäºè§„åˆ™çš„æ¸¸æˆå¯ä»¥ä½œä¸ºå¯æ§å’Œå¯æ‰©å±•çš„é¢„æ–‡æœ¬ä»»åŠ¡ï¼Œè§£é”MLLMä¸­çš„å¯æ³›åŒ–å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08011v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://yunfeixie233.github.io/ViGaL/">https://yunfeixie233.github.io/ViGaL/</a></p>
<p><strong>Summary</strong></p>
<p>æ¸¸æˆå­¦ä¹ å¯æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡è®¤çŸ¥ç§‘å­¦æ–‡çŒ®çš„å¯å‘ï¼Œæå‡ºä¸€ç§æ–°å‹çš„åè®­ç»ƒèŒƒå¼â€”â€”è§†è§‰æ¸¸æˆå­¦ä¹ ï¼ˆViGaLï¼‰ï¼Œä½¿MLLMsé€šè¿‡ç©ç±»ä¼¼è¡—æœºæ¸¸æˆå‘å±•å‡ºè·¨åŸŸçš„å¤šæ¨¡æ€æ¨ç†æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œå¯¹å«æœ‰7Bå‚æ•°çš„MLLMé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ–¹å¼è¿›è¡Œè¡—æœºæ¸¸æˆçš„åè®­ç»ƒï¼Œèƒ½æ˜¾è‘—æå‡å…¶åœ¨å¤šæ¨¡æ€æ•°å­¦åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MathVistaï¼‰å’Œå¤šå­¦ç§‘é—®é¢˜ï¼ˆå¦‚MMMUï¼‰ä¸Šçš„è¡¨ç°ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­æ— éœ€æ¥è§¦è§£é¢˜æ–¹æ¡ˆã€æ–¹ç¨‹å¼æˆ–å›¾è¡¨ï¼Œè¡¨æ˜æ¨¡å‹æŒæ¡äº†å¯è¿ç§»çš„æ¨ç†æŠ€èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¸“é—¨è®­ç»ƒçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œå¹¶ä¿æŒäº†åŸºç¡€æ¨¡å‹åœ¨é€šç”¨è§†è§‰åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜åˆæˆè§„åˆ™æ¸¸æˆå¯ä½œä¸ºå¯æ§ä¸”å¯æ‰©å±•çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œè§£é”MLLMsä¸­çš„æ³›åŒ–å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¸¸æˆå­¦ä¹ ä½œä¸ºä¸€ç§æ–°çš„åè®­ç»ƒèŒƒå¼ï¼Œæœ‰åŠ©äºæå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³›åŒ–æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç©ç±»ä¼¼è¡—æœºæ¸¸æˆï¼ŒMLLMså¯ä»¥å‘å±•å‡ºè·¨åŸŸçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨MLLMsçš„åè®­ç»ƒè¿‡ç¨‹ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œèƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°å­¦å’Œå¤šå­¦ç§‘é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚</li>
<li>æ¨¡å‹åœ¨æŒæ¡æ¨ç†æŠ€èƒ½çš„è¿‡ç¨‹ä¸­ï¼Œæ— éœ€æ¥è§¦è§£é¢˜æ–¹æ¡ˆã€æ–¹ç¨‹å¼æˆ–å›¾è¡¨ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ‰€æå‡ºçš„æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¸“é—¨è®­ç»ƒçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åœ¨é€šç”¨è§†è§‰åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½å¾—ä»¥ä¿æŒï¼Œè¡¨æ˜æ¸¸æˆå­¦ä¹ ä»»åŠ¡ä¸æŸå®³åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e95f6ea8e9f21d604ab818e6e53a21d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dac153d76d43fe736753dec1f0695097.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0c32a2e629cb32b21e18efcf432294d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GTR-CoT-Graph-Traversal-as-Visual-Chain-of-Thought-for-Molecular-Structure-Recognition"><a href="#GTR-CoT-Graph-Traversal-as-Visual-Chain-of-Thought-for-Molecular-Structure-Recognition" class="headerlink" title="GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular   Structure Recognition"></a>GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular   Structure Recognition</h2><p><strong>Authors:Jingchao Wang, Haote Yang, Jiang Wu, Yifan He, Xingjian Wei, Yinfan Wang, Chengjin Liu, Lingli Ge, Lijun Wu, Bin Wang, Dahua Lin, Conghui He</strong></p>
<p>Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the Graph Traversal as Visual Chain of Thought mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of Faithfully Recognize What Youâ€™ve Seen, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at <a target="_blank" rel="noopener" href="https://github.com/opendatalab/GTR-CoT">https://github.com/opendatalab/GTR-CoT</a>. </p>
<blockquote>
<p>å…‰å­¦åŒ–å­¦ç»“æ„è¯†åˆ«ï¼ˆOCSRï¼‰æ˜¯å°†åŒ–å­¦çŸ¥è¯†æ•°å­—åŒ–çš„å…³é”®è¿‡ç¨‹ï¼Œé€šè¿‡è¿™ä¸€æŠ€æœ¯å°†åˆ†å­å›¾åƒè½¬æ¢ä¸ºæœºå™¨å¯è¯»æ ¼å¼ã€‚è™½ç„¶è¿‘æœŸçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è¯¥ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶å›¾åƒæè¿°æ–¹æ³•å¾€å¾€éš¾ä»¥åº”å¯¹å¤æ‚çš„åˆ†å­ç»“æ„å’Œæ ‡æ³¨ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GTR-Mol-VLMè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒæœ‰ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰é€šè¿‡é€æ­¥è§£æåˆ†å­å›¾è¿›è¡ŒåŸå­é”®é¢„æµ‹æ¥æ¨¡æ‹Ÿäººç±»æ¨ç†è¿‡ç¨‹çš„â€œå›¾éå†è§†è§‰é“¾â€æœºåˆ¶ï¼›ï¼ˆ2ï¼‰ä»¥è§£å†³å›¾åƒä¸­çš„ç®€åŒ–ç»“æ„ä¸æ‰©å±•æ ‡æ³¨ä¹‹é—´ä¸åŒ¹é…é—®é¢˜çš„æ•°æ®ä¸­å¿ƒçš„å¿ å®è¯†åˆ«æ‰€çœ‹åˆ°å†…å®¹çš„åŸç†ã€‚ä¸ºäº†æ”¯æŒæ¨¡å‹å¼€å‘ï¼Œæˆ‘ä»¬æ„å»ºäº†å¤§è§„æ¨¡çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†GTR-CoT-1.3Mï¼Œå…¶ä¸­åŒ…å«ç»è¿‡ä»”ç»†æ ¡æ­£çš„æ ‡æ³¨ï¼Œå¹¶æ¨å‡ºäº†é’ˆå¯¹OCSRå›¾å½¢è§£æç²¾åº¦çš„ç²¾ç»†è¯„ä¼°è€Œè®¾è®¡çš„é¦–ä¸ªåŸºå‡†æµ‹è¯•MolRec-Benchã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒGTR-Mol-VLMä¸ä¸“ä¸šæ¨¡å‹ã€åŒ–å­¦é¢†åŸŸVLMå’Œå•†ä¸šé€šç”¨VLMç›¸æ¯”å–å¾—äº†ä¼˜è¶Šçš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å¤„ç†å¸¦æœ‰å®˜èƒ½å›¢ç¼©å†™çš„åˆ†å­å›¾åƒçš„åœºæ™¯ä¸­ï¼ŒGTR-Mol-VLMåœ¨SMILESå’Œå›¾å½¢æŒ‡æ ‡åŸºå‡†æµ‹è¯•ä¸­å‡æ¯”ç¬¬äºŒååŸºå‡†é«˜å‡ºçº¦14ä¸ªç™¾åˆ†ç‚¹ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿæ¨åŠ¨OCSRæŠ€æœ¯æ›´æœ‰æ•ˆåœ°æ»¡è¶³ç°å®ä¸–ç•Œçš„éœ€è¦ï¼Œä»è€Œä¿ƒè¿›åŒ–å­¦ä¿¡æ¯å­¦å’Œç§‘å­¦äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•ã€‚æˆ‘ä»¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/opendatalab/GTR-CoT">https://github.com/opendatalab/GTR-CoT</a>ä¸Šå‘å¸ƒGTR-CoTã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07553v2">PDF</a> </p>
<p><strong>Summary</strong><br>å…‰å­¦åŒ–å­¦ç»“æ„è¯†åˆ«ï¼ˆOCSRï¼‰æ˜¯æ•°å­—åŒ–åŒ–å­¦çŸ¥è¯†çš„é‡è¦æŠ€æœ¯ï¼Œé€šè¿‡å°†åˆ†å­å›¾åƒè½¬æ¢ä¸ºæœºå™¨å¯è¯»æ ¼å¼å®ç°çŸ¥è¯†æ•°å­—åŒ–ã€‚ä¸ºè§£å†³ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚åˆ†å­ç»“æ„å’Œä¸ä¸€è‡´æ³¨é‡Šæ–¹é¢çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶GTR-Mol-VLMã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯é€šè¿‡æ¨¡æ‹Ÿäººç±»æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡åºåˆ—åŸå­é”®é¢„æµ‹é€æ­¥è§£æåˆ†å­å›¾çš„è§†è§‰æ€è€ƒæœºåˆ¶ï¼›äºŒæ˜¯å¿ å®åæ˜ å›¾åƒå’Œæ‰©å±•æ³¨é‡Šé—´ä¸ä¸€è‡´çš„â€œå¿ å®è®¤è¯†æ‰€è§â€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä¸“ä¸šåŒ–æ¨¡å‹ã€åŒ–å­¦é¢†åŸŸçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œé€šç”¨å•†ä¸šè§†è§‰è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒGTR-Mol-VLMå®ç°äº†æ˜¾è‘—æ•ˆæœã€‚ç‰¹åˆ«æ˜¯å½“æ¶‰åŠå¸¦æœ‰å®˜èƒ½å›¢ç¼©å†™çš„åˆ†å­å›¾åƒæ—¶ï¼ŒGTR-Mol-VLMåœ¨SMILESå’Œå›¾å½¢æŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºç¬¬äºŒååŸºå‡†æµ‹è¯•çº¦14ä¸ªç™¾åˆ†ç‚¹ã€‚æœ¬æ–‡æœŸæœ›æ¨åŠ¨OCSRæŠ€æœ¯æ›´å¥½åœ°æ»¡è¶³ç°å®éœ€æ±‚ï¼Œæ¨åŠ¨åŒ–å­¦ä¿¡æ¯å­¦å’Œäººå·¥æ™ºèƒ½ç§‘å­¦é¢†åŸŸçš„å‘å±•ã€‚æˆ‘ä»¬å°†å…¬å¼€GTR-CoTæ•°æ®é›†çš„è®¿é—®åœ°å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…‰å­¦åŒ–å­¦ç»“æ„è¯†åˆ«ï¼ˆOCSRï¼‰æ˜¯å°†åŒ–å­¦çŸ¥è¯†æ•°å­—åŒ–çš„é‡è¦æŠ€æœ¯ã€‚</li>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚åˆ†å­ç»“æ„å’Œä¸ä¸€è‡´æ³¨é‡Šæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>GTR-Mol-VLMæ¡†æ¶å¼•å…¥äº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šæ¨¡æ‹Ÿäººç±»æ¨ç†çš„è§†è§‰æ€è€ƒæœºåˆ¶å’Œå¿ å®åæ˜ æ‰€è§çš„æ•°æ®ä¸­å¿ƒåŸåˆ™ã€‚</li>
<li>GTR-Mol-VLMå®ç°äº†åœ¨åŒ–å­¦ç»“æ„è¯†åˆ«ä¸Šçš„æ˜¾è‘—æ•ˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¸¦æœ‰å®˜èƒ½å›¢ç¼©å†™çš„åˆ†å­å›¾åƒæ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>GTR-Mol-VLMåœ¨SMILESå’Œå›¾å½¢æŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹çº¦14ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
<li>GTR-Mol-VLMæœ‰æœ›æ¨åŠ¨OCSRæŠ€æœ¯æ›´å¥½åœ°æ»¡è¶³ç°å®éœ€æ±‚ï¼Œä¿ƒè¿›åŒ–å­¦ä¿¡æ¯å­¦å’Œäººå·¥æ™ºèƒ½ç§‘å­¦é¢†åŸŸçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6254963c6362b9410cecc7d470ae6b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e66ff9a7989f7206f8faf4a9bcbc1b83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2356decec6238f37994b975776cad5e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-04f3054bcb2af3a39565c202059e7abb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c47d29fef216a4706ab4f75e44828a4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="EVINET-Towards-Open-World-Graph-Learning-via-Evidential-Reasoning-Network"><a href="#EVINET-Towards-Open-World-Graph-Learning-via-Evidential-Reasoning-Network" class="headerlink" title="EVINET: Towards Open-World Graph Learning via Evidential Reasoning   Network"></a>EVINET: Towards Open-World Graph Learning via Evidential Reasoning   Network</h2><p><strong>Authors:Weijie Guan, Haohui Wang, Jian Kang, Lihui Liu, Dawei Zhou</strong></p>
<p>Graph learning has been crucial to many real-world tasks, but they are often studied with a closed-world assumption, with all possible labels of data known a priori. To enable effective graph learning in an open and noisy environment, it is critical to inform the model users when the model makes a wrong prediction to in-distribution data of a known class, i.e., misclassification detection or when the model encounters out-of-distribution from novel classes, i.e., out-of-distribution detection. This paper introduces Evidential Reasoning Network (EVINET), a framework that addresses these two challenges by integrating Beta embedding within a subjective logic framework. EVINET includes two key modules: Dissonance Reasoning for misclassification detection and Vacuity Reasoning for out-of-distribution detection. Extensive experiments demonstrate that EVINET outperforms state-of-the-art methods across multiple metrics in the tasks of in-distribution classification, misclassification detection, and out-of-distribution detection. EVINET demonstrates the necessity of uncertainty estimation and logical reasoning for misclassification detection and out-of-distribution detection and paves the way for open-world graph learning. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/SSSKJ/EviNET">https://github.com/SSSKJ/EviNET</a>. </p>
<blockquote>
<p>å›¾å­¦ä¹ åœ¨è®¸å¤šçœŸå®ä»»åŠ¡ä¸­éƒ½è‡³å…³é‡è¦ï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯åœ¨å°é—­ä¸–ç•Œå‡è®¾ä¸‹è¿›è¡Œç ”ç©¶ï¼Œå³æ‰€æœ‰å¯èƒ½çš„æ•°æ®æ ‡ç­¾éƒ½æ˜¯äº‹å…ˆå·²çŸ¥çš„ã€‚ä¸ºäº†åœ¨å¼€æ”¾å’Œå˜ˆæ‚çš„ç¯å¢ƒä¸­å®ç°æœ‰æ•ˆçš„å›¾å­¦ä¹ ï¼Œå½“æ¨¡å‹å¯¹å·²çŸ¥ç±»åˆ«çš„å†…éƒ¨åˆ†å¸ƒæ•°æ®åšå‡ºé”™è¯¯é¢„æµ‹æ—¶ï¼Œå‘æ¨¡å‹ç”¨æˆ·æŠ¥å‘Šé”™è¯¯ï¼Œå³è¯¯åˆ†ç±»æ£€æµ‹ï¼Œæˆ–å½“æ¨¡å‹é‡åˆ°æ¥è‡ªæ–°ç±»åˆ«çš„å¤–éƒ¨åˆ†å¸ƒæ—¶ï¼Œå³å¤–éƒ¨åˆ†å¸ƒæ£€æµ‹ï¼Œè¿™æ˜¯è‡³å…³é‡è¦çš„ã€‚æœ¬æ–‡ä»‹ç»äº†è¯æ®æ¨ç†ç½‘ç»œï¼ˆEVINETï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ä¸»è§‚é€»è¾‘æ¡†æ¶é›†æˆBetaåµŒå…¥æ¥è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜çš„æ¡†æ¶ã€‚EVINETåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šç”¨äºè¯¯åˆ†ç±»æ£€æµ‹çš„ä¸ä¸€è‡´æ¨ç†å’Œç”¨äºå¤–éƒ¨åˆ†å¸ƒæ£€æµ‹çš„ç©ºæ´æ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEVINETåœ¨å†…éƒ¨åˆ†å¸ƒåˆ†ç±»ã€è¯¯åˆ†ç±»æ£€æµ‹å’Œå¤–éƒ¨åˆ†å¸ƒæ£€æµ‹çš„ä»»åŠ¡ä¸­ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚EVINETè¯æ˜äº†è¯¯åˆ†ç±»æ£€æµ‹å’Œå¤–éƒ¨åˆ†å¸ƒæ£€æµ‹ä¸­ä¸ç¡®å®šæ€§ä¼°è®¡å’Œé€»è¾‘æ¨ç†çš„å¿…è¦æ€§ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œå›¾å­¦ä¹ é“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SSSKJ/EviNET%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SSSKJ/EviNETæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07288v2">PDF</a> KDD 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºEVINETçš„æ¡†æ¶ï¼Œå®ƒè§£å†³äº†åœ¨å¼€æ”¾å’Œå™ªå£°ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆå›¾å­¦ä¹ çš„æŒ‘æˆ˜ã€‚é€šè¿‡æ•´åˆBetaåµŒå…¥å’Œä¸»è§‚é€»è¾‘æ¡†æ¶ï¼ŒEVINETåŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šç”¨äºè¯¯åˆ†ç±»æ£€æµ‹çš„å¼‚è®®æ¨ç†å’Œç”¨äºå¼‚å¸¸æ£€æµ‹çš„ç©ºç¼ºæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒEVINETåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•ï¼Œåœ¨åˆ†å¸ƒå†…åˆ†ç±»ã€è¯¯åˆ†ç±»æ£€æµ‹å’Œå¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¿™ä¸ºå¼€æ”¾ä¸–ç•Œå›¾å­¦ä¹ ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡å’Œé€»è¾‘æ¨ç†çš„å¿…è¦æ€§é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EVINETæ¡†æ¶è§£å†³äº†åœ¨å¼€æ”¾å’Œå™ªå£°ç¯å¢ƒä¸­å›¾å­¦ä¹ çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡æ•´åˆBetaåµŒå…¥å’Œä¸»è§‚é€»è¾‘æ¡†æ¶ï¼ŒEVINETåŒ…å«å¼‚è®®æ¨ç†å’Œç©ºç¼ºæ¨ç†ä¸¤ä¸ªå…³é”®æ¨¡å—ã€‚</li>
<li>EVINETå¯ç”¨äºå¤„ç†å·²çŸ¥ç±»åˆ«çš„è¯¯åˆ†ç±»æ£€æµ‹å’ŒæœªçŸ¥ç±»åˆ«å¼‚å¸¸æ£€æµ‹çš„é—®é¢˜ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒEVINETåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•ã€‚</li>
<li>EVINETå¼ºè°ƒä¸ç¡®å®šæ€§ä¼°è®¡å’Œé€»è¾‘æ¨ç†åœ¨è¯¯åˆ†ç±»æ£€æµ‹å’Œå¼‚å¸¸æ£€æµ‹ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>EVINETä¸ºå¼€æ”¾ä¸–ç•Œå›¾å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d5867dcf120db4374649af648ced85b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9461f2eb58f89f7010702eb3d6e4893.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b9cd8d0d29ce3dba3358a6746355669.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Q-Ponder-A-Unified-Training-Pipeline-for-Reasoning-based-Visual-Quality-Assessment"><a href="#Q-Ponder-A-Unified-Training-Pipeline-for-Reasoning-based-Visual-Quality-Assessment" class="headerlink" title="Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality   Assessment"></a>Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality   Assessment</h2><p><strong>Authors:Zhuoxuan Cai, Jian Zhang, Xinbin Yuan, Peng-Tao Jiang, Wenxiang Chen, Bowen Tang, Lujian Yao, Qiyuan Wang, Jinwen Chen, Bo Li</strong></p>
<p>Recent studies demonstrate that multimodal large language models (MLLMs) can proficiently evaluate visual quality through interpretable assessments. However, existing approaches typically treat quality scoring and reasoning descriptions as separate tasks with disjoint optimization objectives, leading to a trade-off: models adept at quality reasoning descriptions struggle with precise score regression, while score-focused models lack interpretability. This limitation hinders the full potential of MLLMs in visual quality assessment, where accuracy and interpretability should be mutually reinforcing. To address this, we propose a unified two-stage training framework comprising a cold-start stage and a reinforcement learning-based fine-tuning stage. Specifically, in the first stage, we distill high-quality data from a teacher model through expert-designed prompts, initializing reasoning capabilities via cross-entropy loss supervision. In the second stage, we introduce a novel reward with Group Relative Policy Optimization (GRPO) to jointly optimize scoring accuracy and reasoning consistency. We designate the models derived from these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show that Q-Ponder achieves state-of-the-art (SOTA) performance on quality score regression benchmarks, delivering up to 6.5% higher SRCC on cross-domain datasets. Furthermore, Q-Ponder significantly outperforms description-based SOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in description accuracy and reasonableness, demonstrating the generalization potential over diverse tasks. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿé€šè¿‡å¯è§£é‡Šè¯„ä¼°ç†Ÿç»ƒåœ°è¯„ä¼°è§†è§‰è´¨é‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å°†è´¨é‡è¯„åˆ†å’Œæ¨ç†æè¿°è§†ä¸ºå…·æœ‰ä¸åŒä¼˜åŒ–ç›®æ ‡çš„å•ç‹¬ä»»åŠ¡ï¼Œè¿™å¯¼è‡´äº†ä¸€ä¸ªæƒè¡¡ï¼šæ“…é•¿è´¨é‡æ¨ç†æè¿°çš„æ¨¡å‹åœ¨ç²¾ç¡®åˆ†æ•°å›å½’æ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œè€Œä¸“æ³¨äºåˆ†æ•°çš„æ¨¡å‹åˆ™ç¼ºä¹å¯è§£é‡Šæ€§ã€‚è¿™ä¸€å±€é™æ€§é˜»ç¢äº†MLLMsåœ¨è§†è§‰è´¨é‡è¯„ä¼°ä¸­çš„å…¨éƒ¨æ½œåŠ›ï¼Œå…¶ä¸­å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§åº”è¯¥ç›¸äº’å¢å¼ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒ…å«å†·å¯åŠ¨é˜¶æ®µå’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒé˜¶æ®µçš„ç»Ÿä¸€ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡ä¸“å®¶è®¾è®¡çš„æç¤ºä»æ•™å¸ˆæ¨¡å‹ä¸­æç‚¼é«˜è´¨é‡æ•°æ®ï¼Œé€šè¿‡äº¤å‰ç†µæŸå¤±ç›‘ç£æ¥åˆå§‹åŒ–æ¨ç†èƒ½åŠ›ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹å¥–åŠ±ä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥è”åˆä¼˜åŒ–è¯„åˆ†å‡†ç¡®æ€§å’Œæ¨ç†ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å°†è¿™ä¸¤ä¸ªé˜¶æ®µè¡ç”Ÿå‡ºçš„æ¨¡å‹æŒ‡å®šä¸ºQ-Ponder-CIå’ŒQ-Ponderã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒQ-Ponderåœ¨è´¨é‡åˆ†æ•°å›å½’åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ˆSOTAï¼‰ï¼Œåœ¨è·¨åŸŸæ•°æ®é›†ä¸Šçš„SRCCæé«˜äº†é«˜è¾¾6.5%ã€‚æ­¤å¤–ï¼ŒQ-Ponderåœ¨æè¿°å‡†ç¡®æ€§åŠåˆç†æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºäºæè¿°çš„æœ€æ–°æŠ€æœ¯æ°´å¹³æ¨¡å‹ï¼ŒåŒ…æ‹¬å…¶æ•™å¸ˆæ¨¡å‹Qwen-2.5-VL-72Bï¼Œè¿™æ˜¾ç¤ºäº†å…¶åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ³›åŒ–æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05384v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è¿‘æœŸç ”ç©¶å¦‚ä½•é€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œè§†è§‰è´¨é‡è¯„ä¼°ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†è´¨é‡è¯„åˆ†å’Œæ¨ç†æè¿°è§†ä¸ºå•ç‹¬çš„ä»»åŠ¡ï¼Œå¯¼è‡´ä¼˜åŒ–ç›®æ ‡åˆ†ç¦»ï¼Œæ¨¡å‹åœ¨ç²¾ç¡®è¯„åˆ†å’Œè§£é‡Šæ€§æ–¹é¢å­˜åœ¨æƒè¡¡é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µç»Ÿä¸€è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬å†·å¯åŠ¨é˜¶æ®µå’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒé˜¶æ®µã€‚é€šè¿‡ä¸“å®¶è®¾è®¡çš„æç¤ºä»æ•™å¸ˆæ¨¡å‹ä¸­æç‚¼é«˜è´¨é‡æ•°æ®ï¼Œé€šè¿‡äº¤å‰ç†µæŸå¤±ç›‘ç£åˆæ­¥æ¨ç†èƒ½åŠ›ã€‚åœ¨ç¬¬äºŒé˜¶æ®µä¸­ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹å¥–åŠ±ä¸é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥è”åˆä¼˜åŒ–è¯„åˆ†å‡†ç¡®æ€§å’Œæ¨ç†ä¸€è‡´æ€§ã€‚æ¨¡å‹åœ¨è´¨é‡è¯„åˆ†å›å½’åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œåœ¨è·¨åŸŸæ•°æ®é›†ä¸Šçš„SRCCæé«˜è¾¾6.5%ã€‚ç‰¹åˆ«æ˜¯åœ¨æè¿°å‡†ç¡®æ€§å’Œåˆç†æ€§æ–¹é¢ï¼ŒQ-Ponderæ˜¾è‘—ä¼˜äºåŸºäºæè¿°çš„æœ€æ–°æ¨¡å‹ï¼ŒåŒ…æ‹¬å…¶æ•™å¸ˆæ¨¡å‹Qwen-2.5-VL-72Bã€‚è¿™è¡¨æ˜å…¶åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ³›åŒ–æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿç†Ÿç»ƒåœ°è¿›è¡Œè§†è§‰è´¨é‡è¯„ä¼°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è´¨é‡è¯„åˆ†å’Œæ¨ç†æè¿°æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå°†å…¶è§†ä¸ºç‹¬ç«‹ä»»åŠ¡å¯¼è‡´ä¼˜åŒ–ç›®æ ‡åˆ†ç¦»çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µç»Ÿä¸€è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬å†·å¯åŠ¨å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒé˜¶æ®µï¼Œä»¥è§£å†³ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>åœ¨ç¬¬ä¸€é˜¶æ®µä¸­ï¼Œé€šè¿‡ä¸“å®¶è®¾è®¡çš„æç¤ºä»æ•™å¸ˆæ¨¡å‹ä¸­æç‚¼é«˜è´¨é‡æ•°æ®ï¼Œå¹¶é€šè¿‡äº¤å‰ç†µæŸå¤±ç›‘ç£åˆæ­¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨ç¬¬äºŒé˜¶æ®µä¸­ï¼Œå¼•å…¥æ–°å‹å¥–åŠ±ä¸é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œè”åˆä¼˜åŒ–è¯„åˆ†å‡†ç¡®æ€§å’Œæ¨ç†ä¸€è‡´æ€§ã€‚</li>
<li>Q-Ponderæ¨¡å‹åœ¨è´¨é‡è¯„åˆ†å›å½’åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¸”åœ¨è·¨åŸŸæ•°æ®é›†ä¸Šçš„SRCCæœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05384">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd7e9a8ab3469d64d65cd78604283fc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec65390d9262291373a27e0a4eb3e2a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eb026e43b0e88695451e9c2a8a04736.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5881c2b119700d18b3a1fca934d22ff4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CHANCERY-Evaluating-Corporate-Governance-Reasoning-Capabilities-in-Language-Models"><a href="#CHANCERY-Evaluating-Corporate-Governance-Reasoning-Capabilities-in-Language-Models" class="headerlink" title="CHANCERY: Evaluating Corporate Governance Reasoning Capabilities in   Language Models"></a>CHANCERY: Evaluating Corporate Governance Reasoning Capabilities in   Language Models</h2><p><strong>Authors:Lucas Irwin, Arda Kaz, Peiyao Sheng, Sewoong Oh, Pramod Viswanath</strong></p>
<p>Law has long been a domain that has been popular in natural language processing (NLP) applications. Reasoning (ratiocination and the ability to make connections to precedent) is a core part of the practice of the law in the real world. Nevertheless, while multiple legal datasets exist, none have thus far focused specifically on reasoning tasks. We focus on a specific aspect of the legal landscape by introducing a corporate governance reasoning benchmark (CHANCERY) to test a modelâ€™s ability to reason about whether executive&#x2F;board&#x2F;shareholderâ€™s proposed actions are consistent with corporate governance charters. This benchmark introduces a first-of-its-kind corporate governance reasoning test for language models - modeled after real world corporate governance law. The benchmark consists of a corporate charter (a set of governing covenants) and a proposal for executive action. The modelâ€™s task is one of binary classification: reason about whether the action is consistent with the rules contained within the charter. We create the benchmark following established principles of corporate governance - 24 concrete corporate governance principles established in and 79 real life corporate charters selected to represent diverse industries from a total dataset of 10k real life corporate charters. Evaluations on state-of-the-art (SOTA) reasoning models confirm the difficulty of the benchmark, with models such as Claude 3.7 Sonnet and GPT-4o achieving 64.5% and 75.2% accuracy respectively. Reasoning agents exhibit superior performance, with agents based on the ReAct and CodeAct frameworks scoring 76.1% and 78.1% respectively, further confirming the advanced legal reasoning capabilities required to score highly on the benchmark. We also conduct an analysis of the types of questions which current reasoning models struggle on, revealing insights into the legal reasoning capabilities of SOTA models. </p>
<blockquote>
<p>æ³•å¾‹é¢†åŸŸä¸€ç›´æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº”ç”¨ä¸­å¤‡å—å…³æ³¨çš„é¢†åŸŸã€‚æ¨ç†ï¼ˆé€»è¾‘æ€ç»´å’Œä¸å…ˆä¾‹çš„è”ç³»èƒ½åŠ›ï¼‰æ˜¯ç°å®ä¸–ç•Œä¸­æ³•å¾‹å®è·µçš„æ ¸å¿ƒéƒ¨åˆ†ã€‚å°½ç®¡å­˜åœ¨å¤šä¸ªæ³•å¾‹æ•°æ®é›†ï¼Œä½†ç›®å‰è¿˜æ²¡æœ‰ä¸“é—¨é’ˆå¯¹æ¨ç†ä»»åŠ¡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥å…¬å¸æ²»ç†æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆCHANCERYï¼‰æ¥å…³æ³¨æ³•å¾‹é¢†åŸŸçš„ä¸€ä¸ªç‰¹å®šæ–¹é¢ï¼Œä»¥æµ‹è¯•æ¨¡å‹å¯¹æ‰§è¡Œè‘£äº‹&#x2F;è‘£äº‹ä¼š&#x2F;è‚¡ä¸œæå‡ºçš„è¡ŒåŠ¨æ˜¯å¦ç¬¦åˆå…¬å¸æ²»ç†ç« ç¨‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•æ˜¯è¯­è¨€æ¨¡å‹ä¸­é¦–åˆ›çš„å…¬å¸æ²»ç†æ¨ç†æµ‹è¯•ï¼Œä»¥ç°å®ä¸–ç•Œä¸­çš„å…¬å¸æ²»ç†æ³•ä¸ºåŸå‹æ„å»ºã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬å…¬å¸å®ªç« ï¼ˆä¸€ç»„ç®¡ç†å¥‘çº¦ï¼‰å’Œæ‰§è¡Œè¡ŒåŠ¨çš„æè®®ã€‚æ¨¡å‹çš„ä»»åŠ¡æ˜¯äºŒåˆ†ç±»é—®é¢˜ä¹‹ä¸€ï¼šæ ¹æ®å®ªç« ä¸­çš„è§„åˆ™æ¥åˆ¤æ–­è¡ŒåŠ¨æ˜¯å¦ä¸€è‡´ã€‚æˆ‘ä»¬éµå¾ªå…¬å¸æ²»ç†çš„æ—¢å®šåŸåˆ™åˆ›å»ºäº†è¯¥åŸºå‡†æµ‹è¯•ï¼Œä»åŒ…å«1ä¸‡ä¸ªçœŸå®å…¬å¸å®ªç« çš„æ•°æ®é›†ä¸­é€‰å–äº†ä»£è¡¨ä¸åŒè¡Œä¸šçš„79ä¸ªçœŸå®å…¬å¸å®ªç« ï¼Œå¹¶ç¡®å®šäº†å…¶ä¸­ç¡®ç«‹çš„24ä¸ªå…·ä½“å…¬å¸æ²»ç†åŸåˆ™ã€‚å¯¹æœ€æ–°æ¨ç†æ¨¡å‹çš„è¯„ä¼°è¯å®äº†è¯¥åŸºå‡†æµ‹è¯•çš„éš¾åº¦ï¼Œå¦‚Claude 3.7 Sonnetå’ŒGPT-4oçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º64.5%å’Œ75.2%ã€‚æ¨ç†ä»£ç†è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŸºäºReActå’ŒCodeActæ¡†æ¶çš„ä»£ç†åˆ†åˆ«å¾—åˆ†76.1%å’Œ78.1%ï¼Œè¿™è¿›ä¸€æ­¥è¯å®äº†è¦åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸­å–å¾—é«˜åˆ†æ‰€éœ€çš„å…ˆè¿›æ³•å¾‹æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¯¹å½“å‰æ¨ç†æ¨¡å‹åœ¨é—®é¢˜ç±»å‹ä¸Šé‡åˆ°çš„å›°éš¾è¿›è¡Œäº†åˆ†æï¼Œæ­ç¤ºäº†æœ€æ–°æ¨¡å‹åœ¨æ³•å¾‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04636v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ³•å¾‹é¢†åŸŸåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº”ç”¨ä¸­çš„æµè¡Œç¨‹åº¦ï¼Œä»¥åŠæ¨ç†ï¼ˆåŒ…æ‹¬æ€è€ƒå’Œè”ç³»å…ˆä¾‹çš„èƒ½åŠ›ï¼‰åœ¨ç°å®æ³•å¾‹å®è·µä¸­çš„æ ¸å¿ƒåœ°ä½ã€‚å°½ç®¡å­˜åœ¨å¤šä¸ªæ³•å¾‹æ•°æ®é›†ï¼Œä½†è¿„ä»Šä¸ºæ­¢å°šæ— ä¸“é—¨é’ˆå¯¹æ¨ç†ä»»åŠ¡çš„æ•°æ®é›†ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»äº†ä¸€ä¸ªç‰¹å®šçš„æ³•å¾‹é¢†åŸŸâ€”â€”å…¬å¸æ²»ç†æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆCHANCERYï¼‰ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹å¯¹é«˜ç®¡ã€è‘£äº‹ä¼šæˆ–è‚¡ä¸œæè®®çš„è¡ŒåŠ¨æ˜¯å¦ç¬¦åˆå…¬å¸æ²»ç†ç« ç¨‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•å¼•å…¥äº†ä¸€ç§é¦–åˆ›çš„å…¬å¸æ²»ç†æ¨ç†æµ‹è¯•ï¼Œæ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„å…¬å¸æ²»ç†æ³•å¾‹ã€‚å®ƒç”±å…¬å¸å®ªç« ï¼ˆä¸€ç³»åˆ—ç®¡ç†å¥‘çº¦ï¼‰å’Œä¸€é¡¹é«˜ç®¡è¡ŒåŠ¨ææ¡ˆç»„æˆã€‚æ¨¡å‹çš„ä»»åŠ¡æ˜¯è¿›è¡ŒäºŒå…ƒåˆ†ç±»ï¼šåˆ¤æ–­è¡ŒåŠ¨æ˜¯å¦ç¬¦åˆå®ªç« ä¸­çš„è§„åˆ™ã€‚æœ¬æ–‡éµå¾ªå…¬å¸æ²»ç†çš„æ—¢å®šåŸåˆ™ï¼Œåˆ›å»ºäº†è¿™ä¸€åŸºå‡†æµ‹è¯•ï¼Œå¹¶åŸºäºç°å®ä¸–ç•Œçš„ä¼ä¸šå®ªç« æ•°æ®é›†ï¼Œä»ä¸­é€‰å–äº†ä»£è¡¨ä¸åŒè¡Œä¸šçš„79ä»½å®ªç« ã€‚å¯¹ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼ˆSOTAï¼‰æ¨ç†æ¨¡å‹çš„è¯„ä¼°è¯æ˜äº†è¯¥åŸºå‡†æµ‹è¯•çš„éš¾åº¦ï¼Œå¦‚Claude 3.7 Sonnetå’ŒGPT-4oåˆ†åˆ«å®ç°äº†64.5%å’Œ75.2%çš„å‡†ç¡®æ€§ã€‚è€Œå…·å¤‡Reactå’ŒCodeActæ¡†æ¶çš„æ¨ç†ä»£ç†è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œåˆ†åˆ«è¾¾åˆ°äº†76.1%å’Œ78.1%çš„å‡†ç¡®ç‡ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†åœ¨åŸºå‡†æµ‹è¯•ä¸Šå–å¾—é«˜åˆ†æ‰€éœ€çš„é«˜çº§æ³•å¾‹æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæœ¬æ–‡å¯¹å½“å‰æ¨ç†æ¨¡å‹åœ¨å“ªäº›ç±»å‹çš„é—®é¢˜ä¸Šå­˜åœ¨å›°éš¾è¿›è¡Œäº†åˆ†æï¼Œæ­ç¤ºäº†å…ˆè¿›æŠ€æœ¯æ¨¡å‹åœ¨æ³•å¾‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ³•å¾‹é¢†åŸŸåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­é¢‡å—æ¬¢è¿ï¼Œä¸”æ¨ç†åœ¨ç°å®æ³•å¾‹å®è·µä¸­å…·æœ‰æ ¸å¿ƒåœ°ä½ã€‚</li>
<li>è‡³ä»Šå°šæ— ä¸“é—¨é’ˆå¯¹æ³•å¾‹æ¨ç†ä»»åŠ¡çš„æ•°æ®é›†ã€‚</li>
<li>å¼•å…¥äº†é¦–ä¸ªå…¬å¸æ²»ç†æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆCHANCERYï¼‰ï¼Œç”¨ä»¥è¯„ä¼°æ¨¡å‹å¯¹å…¬å¸æ²»ç†åœºæ™¯çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…å«å…¬å¸å®ªç« å’Œè¡ŒåŠ¨ææ¡ˆï¼Œæ¨¡å‹éœ€åˆ¤æ–­è¡ŒåŠ¨æ˜¯å¦ç¬¦åˆå®ªç« è§„åˆ™ã€‚</li>
<li>éµå¾ªå…¬å¸æ²»ç†çš„æ—¢å®šåŸåˆ™ï¼ŒåŸºäºç°å®çš„ä¼ä¸šå®ªç« æ•°æ®é›†åˆ›å»ºæ­¤åŸºå‡†æµ‹è¯•ã€‚</li>
<li>ç°æœ‰å…ˆè¿›æŠ€æœ¯æ¨ç†æ¨¡å‹åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°è¯æ˜äº†å…¶éš¾åº¦ï¼Œè€Œå…·å¤‡ç‰¹å®šæ¡†æ¶çš„æ¨ç†ä»£ç†è¡¨ç°è¾ƒå¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-008a7423df53ba5f2c223d2fb41f6796.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ceeb1e39bfff816768326c1692c37f13.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-368924729e8aae91811cf976e1c382ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff5390e160a0d2860af39ec4a08aa638.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TableEval-A-Real-World-Benchmark-for-Complex-Multilingual-and-Multi-Structured-Table-Question-Answering"><a href="#TableEval-A-Real-World-Benchmark-for-Complex-Multilingual-and-Multi-Structured-Table-Question-Answering" class="headerlink" title="TableEval: A Real-World Benchmark for Complex, Multilingual, and   Multi-Structured Table Question Answering"></a>TableEval: A Real-World Benchmark for Complex, Multilingual, and   Multi-Structured Table Question Answering</h2><p><strong>Authors:Junnan Zhu, Jingyi Wang, Bohan Yu, Xiaoyu Wu, Junbo Li, Lei Wang, Nan Xu</strong></p>
<p>LLMs have shown impressive progress in natural language processing. However, they still face significant challenges in TableQA, where real-world complexities such as diverse table structures, multilingual data, and domain-specific reasoning are crucial. Existing TableQA benchmarks are often limited by their focus on simple flat tables and suffer from data leakage. Furthermore, most benchmarks are monolingual and fail to capture the cross-lingual and cross-domain variability in practical applications. To address these limitations, we introduce TableEval, a new benchmark designed to evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes tables with various structures (such as concise, hierarchical, and nested tables) collected from four domains (including government, finance, academia, and industry reports). Besides, TableEval features cross-lingual scenarios with tables in Simplified Chinese, Traditional Chinese, and English. To minimize the risk of data leakage, we collect all data from recent real-world documents. Considering that existing TableQA metrics fail to capture semantic accuracy, we further propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level. Experimental results have shown that SEAT achieves high agreement with human judgment. Extensive experiments on TableEval reveal critical gaps in the ability of state-of-the-art LLMs to handle these complex, real-world TableQA tasks, offering insights for future improvements. We make our dataset available here: <a target="_blank" rel="noopener" href="https://github.com/wenge-research/TableEval">https://github.com/wenge-research/TableEval</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨è¡¨æ ¼é—®ç­”ï¼ˆTableQAï¼‰é¢†åŸŸï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¡¨æ ¼é—®ç­”ä¸­ï¼Œç°å®ä¸–ç•Œä¸­çš„å¤æ‚æ€§è‡³å…³é‡è¦ï¼Œä¾‹å¦‚å„ç§è¡¨æ ¼ç»“æ„ã€å¤šè¯­è¨€æ•°æ®å’Œç‰¹å®šé¢†åŸŸçš„æ¨ç†ã€‚ç°æœ‰çš„TableQAåŸºå‡†æµ‹è¯•é€šå¸¸å±€é™äºç®€å•çš„å¹³é¢è¡¨æ ¼ï¼Œå¹¶å­˜åœ¨æ•°æ®æ³„éœ²çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°åŸºå‡†æµ‹è¯•éƒ½æ˜¯å•è¯­è¨€çš„ï¼Œæ— æ³•æ•è·å®é™…åº”ç”¨ä¸­çš„è·¨è¯­è¨€å’Œè·¨é¢†åŸŸå˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TableEvalï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®çš„è¡¨æ ¼é—®ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å…·ä½“æ¥è¯´ï¼ŒTableEvalåŒ…å«ä»å››ä¸ªé¢†åŸŸï¼ˆåŒ…æ‹¬æ”¿åºœã€é‡‘èã€å­¦æœ¯å’Œå·¥ä¸šæŠ¥å‘Šï¼‰æ”¶é›†çš„è¡¨æ ¼ï¼Œè¿™äº›è¡¨æ ¼å…·æœ‰å„ç§ç»“æ„ï¼ˆå¦‚ç®€æ´ã€åˆ†å±‚å’ŒåµŒå¥—è¡¨æ ¼ï¼‰ã€‚æ­¤å¤–ï¼ŒTableEvalå…·æœ‰ç®€ä½“ä¸­æ–‡ã€ç¹ä½“ä¸­æ–‡å’Œè‹±æ–‡çš„è·¨è¯­è¨€åœºæ™¯ã€‚ä¸ºäº†æœ€å°åŒ–æ•°æ®æ³„éœ²çš„é£é™©ï¼Œæˆ‘ä»¬ä»æœ€æ–°çš„ç°å®æ–‡æ¡£ä¸­æ”¶é›†æ‰€æœ‰æ•°æ®ã€‚è€ƒè™‘åˆ°ç°æœ‰çš„TableQAæŒ‡æ ‡æ— æ³•æ•æ‰è¯­ä¹‰å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†SEATï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå®ƒå¯ä»¥åœ¨å­é—®é¢˜çº§åˆ«è¯„ä¼°æ¨¡å‹å“åº”å’Œå‚è€ƒç­”æ¡ˆä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEATä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ã€‚åœ¨TableEvalä¸Šçš„å¹¿æ³›å®éªŒæ­ç¤ºäº†æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿™äº›å¤æ‚ã€ç°å®çš„è¡¨æ ¼é—®ç­”ä»»åŠ¡æ—¶çš„å…³é”®å·®è·ï¼Œä¸ºæœªæ¥çš„æ”¹è¿›æä¾›äº†è§è§£ã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹é“¾æ¥æä¾›äº†æˆ‘ä»¬çš„æ•°æ®é›†ï¼š<a target="_blank" rel="noopener" href="https://github.com/wenge-research/TableEval">https://github.com/wenge-research/TableEval</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03949v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨TableQAé¢†åŸŸä»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰TableQAåŸºå‡†æµ‹è¯•åœ¨è¡¨æ ¼ç»“æ„å¤šæ ·æ€§ã€å¤šè¯­ç§æ•°æ®å’Œé¢†åŸŸç‰¹å®šæ¨ç†ç­‰æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº†TableEvalæ–°åŸºå‡†æµ‹è¯•ã€‚TableEvalåŒ…å«æ¥è‡ªå››ä¸ªé¢†åŸŸçš„å„ç§ç»“æ„è¡¨æ ¼ï¼Œå¹¶å…·å¤‡ç®€åŒ–çš„ä¸­æ–‡ã€ä¼ ç»Ÿçš„ä¸­æ–‡å’Œè‹±æ–‡è·¨è¯­è¨€åœºæ™¯ã€‚æ­¤å¤–ï¼Œä¸ºè¯„ä¼°æ¨¡å‹å“åº”ä¸å‚è€ƒç­”æ¡ˆçš„è¯­ä¹‰å‡†ç¡®æ€§ï¼Œæå‡ºäº†SEATè¯„ä¼°æ¡†æ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºSEATä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ï¼ŒTableEvalä¸Šçš„å®éªŒæ­ç¤ºäº†é¡¶å°–LLMsåœ¨å¤„ç†å¤æ‚ã€å®é™…TableQAä»»åŠ¡æ—¶çš„å…³é”®å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨TableQAé¢†åŸŸä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰TableQAåŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç®€å•è¡¨æ ¼ï¼Œå­˜åœ¨æ•°æ®æ³„éœ²é—®é¢˜ã€‚</li>
<li>TableEvalæ–°åŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°LLMsåœ¨çœŸå®TableQAä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒ…å«å„ç§ç»“æ„çš„è¡¨æ ¼å’Œè·¨è¯­è¨€åœºæ™¯ã€‚</li>
<li>TableEvalè¡¨æ ¼æ•°æ®æ¥è‡ªæœ€æ–°çœŸå®æ–‡æ¡£ï¼Œå‡å°‘æ•°æ®æ³„éœ²é£é™©ã€‚</li>
<li>SEATè¯„ä¼°æ¡†æ¶ç”¨äºè¯„ä¼°æ¨¡å‹å“åº”ä¸å‚è€ƒç­”æ¡ˆçš„è¯­ä¹‰å‡†ç¡®æ€§ã€‚</li>
<li>SEATä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ï¼Œå®éªŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-94e0c918f6b3e630f743d77e84dbcd54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36b9a94e876595dd7efe3df07e865994.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9382373db947e8050f13553613aeea6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db475d1d0d0523a7db40612c780bebb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7685ae984299ec8c4fe41c0d93e33737.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28c9b58fa512a037535ad9579a005826.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Incentivizing-Reasoning-for-Advanced-Instruction-Following-of-Large-Language-Models"><a href="#Incentivizing-Reasoning-for-Advanced-Instruction-Following-of-Large-Language-Models" class="headerlink" title="Incentivizing Reasoning for Advanced Instruction-Following of Large   Language Models"></a>Incentivizing Reasoning for Advanced Instruction-Following of Large   Language Models</h2><p><strong>Authors:Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, Xing Sun</strong></p>
<p>Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at <a target="_blank" rel="noopener" href="https://github.com/yuleiqin/RAIF">https://github.com/yuleiqin/RAIF</a>. </p>
<blockquote>
<p>ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éµå¾ªå¤æ‚æŒ‡ä»¤æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨å¤šä¸ªçº¦æŸå¹¶ä»¥å¹¶è¡Œã€é“¾å’Œåˆ†æ”¯ç»“æ„ç»„ç»‡æ—¶ã€‚ä¸€ç§ç›´è§‚çš„è§£å†³æ–¹æ¡ˆï¼Œå³æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œæœ‰æœ›æ™®éæé«˜LLMçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°åŸå§‹çš„CoTç”±äºç®€å•åœ°å¤è¿°æŒ‡ä»¤è€Œå¯¹å…¶æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚å®ƒæœªèƒ½å‰¥ç¦»çº¦æŸçš„ç»„åˆï¼Œä»¥è¯†åˆ«å…¶è·¨ç±»å‹å’Œç»´åº¦å±‚æ¬¡çš„å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡æ¿€åŠ±æµ‹è¯•æ—¶é—´è®¡ç®—ç¼©æ”¾æ¥æé«˜LLMå¤„ç†å¤æ‚æŒ‡ä»¤çš„ç³»ç»Ÿæ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ ¹æ®ç°æœ‰åˆ†ç±»æ³•å¯¹å¤æ‚æŒ‡ä»¤è¿›è¡Œåˆ†è§£ï¼Œå¹¶æå‡ºä¸€ç§å¯å¤åˆ¶çš„æ•°æ®é‡‡é›†æ–¹æ³•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨å¯éªŒè¯çš„è§„åˆ™ä¸­å¿ƒå¥–åŠ±ä¿¡å·æ¥åŸ¹å…»ç‰¹å®šçš„æŒ‡ä»¤éµå¾ªæ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡æ ·æœ¬å¯¹æ¯”æ¥è§£å†³å¤æ‚æŒ‡ä»¤ä¸‹æ¨ç†çš„è‚¤æµ…å’Œéæœ¬è´¨æ€§è´¨ï¼Œä»¥æ›´å¥½åœ°å®æ–½CoTã€‚æˆ‘ä»¬è¿˜æ¨¡ä»¿ä¸“å®¶çš„è¡Œä¸ºå…‹éš†ï¼Œä¿ƒè¿›ä»å¿«é€Ÿæ€è€ƒçš„è¯­è¨€æ¨¡å‹åˆ°ç†Ÿç»ƒæ¨ç†è€…çš„ç¨³å®šåˆ†å¸ƒè½¬ç§»ã€‚åœ¨ä¸ƒä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯å®äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå…¶ä¸­1.5B LLMå®ç°äº†11.74%çš„å¢ç›Šï¼Œæ€§èƒ½å¯ä¸8B LLMç›¸åª²ç¾ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yuleiqin/RAIF%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/yuleiqin/RAIFè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01413v2">PDF</a> 13 pages of main body, 3 tables, 5 figures, 45 pages of appendix</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨å¤šç§å¹¶è¡Œã€é“¾å¼å’Œåˆ†æ”¯ç»“æ„çš„çº¦æŸæ—¶ã€‚è™½ç„¶é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä½œä¸ºä¸€ç§ç›´è§‚çš„è§£å†³æ–¹æ¡ˆæœ‰æœ›æ™®éæé«˜LLMçš„èƒ½åŠ›ï¼Œä½†æˆ‘ä»¬å‘ç°åŸå§‹çš„CoTç”±äºç®€å•çš„æŒ‡ä»¤å¤è¿°è€Œäº§ç”Ÿäº†è´Ÿé¢çš„æ€§èƒ½å½±å“ã€‚å®ƒæœªèƒ½æ·±å…¥å‰–æçº¦æŸçš„ç»„åˆï¼Œä»¥è¯†åˆ«ä¸åŒç±»å‹å’Œç»´åº¦ä¹‹é—´çš„å±‚æ¬¡å…³ç³»ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡æ¿€åŠ±æµ‹è¯•æ—¶çš„è®¡ç®—æ‰©å±•æ¥æé«˜LLMå¤„ç†å¤æ‚æŒ‡ä»¤èƒ½åŠ›çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ ¹æ®ç°æœ‰çš„åˆ†ç±»ä½“ç³»å¯¹å¤æ‚æŒ‡ä»¤è¿›è¡Œåˆ†è§£ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¯å¤åˆ¶çš„æ•°æ®é‡‡é›†æ–¹æ³•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¯éªŒè¯çš„è§„åˆ™ä¸­å¿ƒå¥–åŠ±ä¿¡å·æ¥åŸ¹å…»ä¸“é—¨çš„æŒ‡ä»¤è·Ÿéšæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è§£å†³äº†å¤æ‚æŒ‡ä»¤ä¸‹æµ…å±‚ã€éæœ¬è´¨æ¨ç†çš„ç‰¹ç‚¹ï¼Œé€šè¿‡æ ·æœ¬å¯¹æ¯”æ¥å¼ºåŒ–å…ˆè¿›çš„CoTå®æ–½ã€‚æˆ‘ä»¬è¿˜æ¨¡ä»¿ä¸“å®¶è¡Œä¸ºå…‹éš†ï¼Œä»¥åœ¨å¿«é€Ÿæ€è€ƒLLMå’ŒæŠ€èƒ½ç†Ÿç»ƒæ¨ç†è€…ä¹‹é—´å®ç°ç¨³å®šçš„åˆ†å¸ƒè½¬ç§»ã€‚åœ¨ä¸ƒä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯å®äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå…¶ä¸­1.5B LLMå–å¾—äº†11.74%çš„å¢ç›Šï¼Œæ€§èƒ½å¯ä¸8B LLMç›¸åª²ç¾ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yuleiqin/RAIF">é“¾æ¥</a>ä¸­è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†åŒ…å«å¤šé‡å¹¶è¡Œã€é“¾å¼å’Œåˆ†æ”¯ç»“æ„çš„å¤æ‚æŒ‡ä»¤æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ–¹æ³•å› ç®€å•æŒ‡ä»¤å¤è¿°è€Œå…·æœ‰è´Ÿé¢æ€§èƒ½å½±å“ï¼Œæœªèƒ½æ·±å…¥å‰–æçº¦æŸå…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šè¿‡æ¿€åŠ±æ¨ç†æ¥æé«˜LLMå¤„ç†å¤æ‚æŒ‡ä»¤èƒ½åŠ›çš„æ–¹æ³•ï¼Œæ¶‰åŠå¤æ‚æŒ‡ä»¤çš„åˆ†è§£ã€æ•°æ®é‡‡é›†ã€å¼ºåŒ–å­¦ä¹ å’Œè¡Œä¸ºå…‹éš†ã€‚</li>
<li>è¯¥æ–¹æ³•å¼ºåŒ–äº†è§„åˆ™ä¸­å¿ƒçš„å¥–åŠ±ä¿¡å·ï¼Œé€šè¿‡æ ·æœ¬å¯¹æ¯”å¼ºåŒ–å…ˆè¿›çš„CoTå®æ–½ã€‚</li>
<li>æ–¹æ³•çš„æœ‰æ•ˆæ€§åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šå¾—åˆ°å¹¿æ³›éªŒè¯ï¼Œå…¶ä¸­å°æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ï¼Œæ¥è¿‘å¤§å‹æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç›¸å…³ä»£ç å’Œæ•°æ®å·²åœ¨æŒ‡å®šé“¾æ¥ä¸­å…¬å¼€ï¼Œä¾¿äºç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8918ada4ce0cc3c66fafb69838000be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b629d4985e53f8af42fee0c70a98dd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60ac95dce296aa2b085924a323d5002e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Reinforcing-Video-Reasoning-with-Focused-Thinking"><a href="#Reinforcing-Video-Reasoning-with-Focused-Thinking" class="headerlink" title="Reinforcing Video Reasoning with Focused Thinking"></a>Reinforcing Video Reasoning with Focused Thinking</h2><p><strong>Authors:Jisheng Dang, Jingze Wu, Teng Wang, Xuanhui Lin, Nannan Zhu, Hongbo Chen, Wei-Shi Zheng, Meng Wang, Tat-Seng Chua</strong></p>
<p>Recent advancements in reinforcement learning, particularly through Group Relative Policy Optimization (GRPO), have significantly improved multimodal large language models for complex reasoning tasks. However, two critical limitations persist: 1) they often produce unfocused, verbose reasoning chains that obscure salient spatiotemporal cues and 2) binary rewarding fails to account for partially correct answers, resulting in high reward variance and inefficient learning. In this paper, we propose TW-GRPO, a novel framework that enhances visual reasoning with focused thinking and dense reward granularity. Specifically, we employs a token weighting mechanism that prioritizes tokens with high informational density (estimated by intra-group information entropy), suppressing redundant tokens like generic reasoning prefixes. Furthermore, we reformulate RL training by shifting from single-choice to multi-choice QA tasks, where soft rewards enable finer-grained gradient estimation by distinguishing partial correctness. Additionally, we propose question-answer inversion, a data augmentation strategy to generate diverse multi-choice samples from existing benchmarks. Experiments demonstrate state-of-the-art performance on several video reasoning and general understanding benchmarks. Notably, TW-GRPO achieves 50.4% accuracy on CLEVRER (18.8% improvement over Video-R1) and 65.8% on MMVU. Our codes are available at \href{<a target="_blank" rel="noopener" href="https://github.com/longmalongma/TW-GRPO%7D">https://github.com/longmalongma/TW-GRPO}</a>. </p>
<blockquote>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå·²ç»æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™ï¼š1ï¼‰å®ƒä»¬é€šå¸¸äº§ç”Ÿä¸èšç„¦ã€å†—é•¿çš„æ¨ç†é“¾ï¼Œæ©ç›–äº†é‡è¦çš„æ—¶ç©ºçº¿ç´¢ï¼›2ï¼‰äºŒå…ƒå¥–åŠ±æ— æ³•è€ƒè™‘éƒ¨åˆ†æ­£ç¡®çš„ç­”æ¡ˆï¼Œå¯¼è‡´å¥–åŠ±æ–¹å·®é«˜å’Œå­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TW-GRPOï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡èšç„¦æ€è€ƒå’Œå¯†é›†çš„å¥–åŠ±ç²’åº¦å¢å¼ºè§†è§‰æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ ‡è®°æƒé‡æœºåˆ¶ï¼Œä¼˜å…ˆå¤„ç†å…·æœ‰é«˜ä¿¡æ¯å¯†åº¦çš„æ ‡è®°ï¼ˆé€šè¿‡ç»„å†…ä¿¡æ¯ç†µä¼°è®¡ï¼‰ï¼ŒåŒæ—¶æŠ‘åˆ¶å†—ä½™æ ‡è®°ï¼Œå¦‚é€šç”¨æ¨ç†å‰ç¼€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡æ–°è®¾è®¡RLè®­ç»ƒï¼Œä»å•é€‰é—®é¢˜è½¬å‘å¤šé€‰é—®ç­”ä»»åŠ¡ï¼Œå…¶ä¸­è½¯å¥–åŠ±èƒ½å¤Ÿé€šè¿‡åŒºåˆ†éƒ¨åˆ†æ­£ç¡®æ€§æ¥å®ç°æ›´ç²¾ç»†çš„æ¢¯åº¦ä¼°è®¡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†é—®é¢˜ç­”æ¡ˆåè½¬ï¼Œè¿™æ˜¯ä¸€ç§æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¯ä»¥ä»ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­ç”Ÿæˆå¤šæ ·åŒ–çš„å¤šé€‰æ‹©æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªè§†é¢‘æ¨ç†å’Œé€šç”¨ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTW-GRPOåœ¨CLEVRERä¸Šå®ç°äº†50.4%çš„å‡†ç¡®ç‡ï¼ˆç›¸è¾ƒäºVideo-R1æœ‰18.8%çš„æå‡ï¼‰ï¼Œåœ¨MMVUä¸Šå®ç°äº†65.8%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/longmalongma/TW-GRPO%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/longmalongma/TW-GRPOè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24718v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ åœ¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰æ–¹é¢çš„è¿›å±•æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä¸€æ˜¯äº§ç”Ÿçš„æ¨ç†é“¾å¾€å¾€ä¸èšç„¦ä¸”å†—é•¿ï¼Œéš¾ä»¥æ•æ‰æ—¶ç©ºçº¿ç´¢ï¼›äºŒæ˜¯äºŒå…ƒå¥–åŠ±æœºåˆ¶æ— æ³•åº”å¯¹éƒ¨åˆ†æ­£ç¡®ç­”æ¡ˆï¼Œå¯¼è‡´å¥–åŠ±æ–¹å·®å¤§ä¸”å­¦ä¹ æ•ˆç‡ä½ã€‚æœ¬æ–‡æå‡ºTW-GRPOæ¡†æ¶ï¼Œé€šè¿‡æ ‡è®°æƒé‡æœºåˆ¶å¼ºåŒ–è§†è§‰æ¨ç†çš„èšç„¦æ€è€ƒï¼Œå¹¶å¼•å…¥ç²¾ç»†å¥–åŠ±ç²’åº¦ã€‚æ­¤å¤–ï¼Œä»å•é€‰é¢˜è½¬å‘å¤šé€‰é¢˜é—®ç­”ä»»åŠ¡ï¼Œè½¯å¥–åŠ±èƒ½å¤Ÿæ›´ç²¾ç»†åœ°ä¼°è®¡æ¢¯åº¦å¹¶åŒºåˆ†éƒ¨åˆ†æ­£ç¡®æ€§ã€‚å®éªŒè¯æ˜ï¼ŒTW-GRPOåœ¨å¤šä¸ªè§†é¢‘æ¨ç†å’Œé€šç”¨ç†è§£åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šï¼Œå¦‚åœ¨CLEVRERä¸Šè¾¾åˆ°50.4%çš„å‡†ç¡®ç‡ï¼ˆè¾ƒVideo-R1æé«˜18.8%ï¼‰ï¼Œåœ¨MMVUä¸Šè¾¾åˆ°65.8%ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€äºæŒ‡å®šé“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Group Relative Policy Optimizationï¼ˆGRPOï¼‰å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹å¤æ‚æ¨ç†ä»»åŠ¡æ€§èƒ½ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šæ¨ç†é“¾ä¸èšç„¦å’Œå¥–åŠ±æœºåˆ¶æ— æ³•åº”å¯¹éƒ¨åˆ†æ­£ç¡®ç­”æ¡ˆã€‚</li>
<li>TW-GRPOæ¡†æ¶é€šè¿‡æ ‡è®°æƒé‡æœºåˆ¶å¼ºåŒ–è§†è§‰æ¨ç†çš„èšç„¦æ€è€ƒï¼Œå¹¶å¼•å…¥ç²¾ç»†å¥–åŠ±ç²’åº¦æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ä»å•é€‰é¢˜è½¬å‘å¤šé€‰é¢˜é—®ç­”ä»»åŠ¡ï¼Œè½¯å¥–åŠ±èƒ½å¤Ÿæ›´ç²¾ç»†åœ°åŒºåˆ†ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚</li>
<li>TW-GRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šï¼Œå¦‚åœ¨CLEVRERå’ŒMMVUä¸Šçš„é«˜å‡†ç¡®ç‡è¡¨ç°ã€‚</li>
<li>ä»£ç å·²å…¬å¼€å¹¶æä¾›é“¾æ¥ä¾›å…¬ä¼—è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-577b84b79879803295df5ad465d46e10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3a23e9723f93aa2127b7aea300fbe70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c37a4e160fb362e4f3d64d191092857.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Enigmata-Scaling-Logical-Reasoning-in-Large-Language-Models-with-Synthetic-Verifiable-Puzzles"><a href="#Enigmata-Scaling-Logical-Reasoning-in-Large-Language-Models-with-Synthetic-Verifiable-Puzzles" class="headerlink" title="Enigmata: Scaling Logical Reasoning in Large Language Models with   Synthetic Verifiable Puzzles"></a>Enigmata: Scaling Logical Reasoning in Large Language Models with   Synthetic Verifiable Puzzles</h2><p><strong>Authors:Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, Hao Zhou, Mingxuan Wang</strong></p>
<p>Large Language Models (LLMs), such as OpenAIâ€™s o1 and DeepSeekâ€™s R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across seven categories, each with 1) a generator that produces unlimited examples with controllable difficulty and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from Enigmata further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of Enigmata. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Resources of this work can be found at <a target="_blank" rel="noopener" href="https://seed-enigmata.github.io/">https://seed-enigmata.github.io</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚OpenAIçš„o1å’ŒDeepSeekçš„R1ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æ•°å­¦å’Œç¼–ç ç­‰é«˜çº§æ¨ç†ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§£å†³äººç±»æ— éœ€é¢†åŸŸçŸ¥è¯†å³å¯è§£å†³çš„è°œé¢˜æ–¹é¢ä»é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬å¼•å…¥äº†Enigmataï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹æé«˜LLMçš„è°œé¢˜æ¨ç†æŠ€èƒ½è€Œé‡èº«å®šåˆ¶çš„ç»¼åˆå¥—ä»¶ã€‚å®ƒåŒ…æ‹¬7ä¸ªç±»åˆ«çš„36é¡¹ä»»åŠ¡ï¼Œæ¯ä¸ªç±»åˆ«éƒ½å…·å¤‡1ï¼‰èƒ½å¤Ÿäº§ç”Ÿæ— é™å¯æ§éš¾åº¦èŒƒä¾‹çš„ç”Ÿæˆå™¨ï¼Œä»¥åŠ2ï¼‰ç”¨äºè‡ªåŠ¨è¯„ä¼°çš„è§„åˆ™éªŒè¯å™¨ã€‚è¿™ç§ç”Ÿæˆå™¨-éªŒè¯å™¨çš„è®¾è®¡æ”¯æŒå¯æ‰©å±•çš„å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€ç²¾ç»†åˆ†æä»¥åŠæ— ç¼RLVRé›†æˆã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•Enigmata-Evalï¼Œå¹¶å¼€å‘äº†ä¼˜åŒ–çš„å¤šä»»åŠ¡RLVRç­–ç•¥ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ¨¡å‹Qwen2.5-32B-Enigmataåœ¨è°œé¢˜æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚Enigmata-Evalã€ARC-AGI 32.8%å’ŒARC-AGI 2 0.6%ï¼‰ä¸Šå§‹ç»ˆè¶…è¿‡o3-mini-highå’Œo1ã€‚å®ƒè¿˜èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°åŸŸå¤–è°œé¢˜åŸºå‡†æµ‹è¯•å’Œæ•°å­¦æ¨ç†ï¼Œå¤šä»»åŠ¡æƒè¡¡å¾ˆå°ã€‚å½“åœ¨æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚Seed1.5-Thinkingï¼ˆ20Bæ¿€æ´»å‚æ•°å’Œ200Bæ€»å‚æ•°ï¼‰ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼ŒEnigmataçš„è°œé¢˜æ•°æ®è¿›ä¸€æ­¥æé«˜äº†åœ¨é«˜çº§æ•°å­¦å’ŒSTEMæ¨ç†ä»»åŠ¡ï¼ˆå¦‚AIMEï¼ˆ2024-2025ï¼‰ã€BeyondAIMEå’ŒGPQAï¼ˆDiamondï¼‰ï¼‰ä¸Šçš„æœ€æ–°æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºEnigmataè‰¯å¥½çš„æ³›åŒ–æ•ˆç›Šã€‚è¿™é¡¹å·¥ä½œä¸ºæ¨è¿›LLMçš„é€»è¾‘æ¨ç†æä¾›äº†ä¸€ä¸ªç»Ÿä¸€ã€å¯æ§çš„æ¡†æ¶ã€‚è¯¥å·¥ä½œçš„èµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://seed-enigmata.github.ioæ‰¾åˆ°./">https://seed-enigmata.github.ioæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19914v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„æ•°å­¦å’Œç¼–ç ç­‰é«˜çº§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§£å†³äººç±»æ— éœ€é¢†åŸŸçŸ¥è¯†å³å¯è§£å†³çš„è°œé¢˜æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ¨å‡ºäº†Enigmataå¥—ä»¶ï¼Œæ—¨åœ¨æå‡LLMsçš„è°œé¢˜æ¨ç†èƒ½åŠ›ã€‚å®ƒåŒ…æ‹¬ä¸ƒå¤§ç±»åˆ«å…±36é¡¹ä»»åŠ¡ï¼Œæ¯ä¸ªç±»åˆ«éƒ½é…å¤‡å¯ç”Ÿæˆæ— é™ä¾‹é¢˜å¹¶æŒ‰éš¾åº¦æ§åˆ¶çš„ç”Ÿæˆå™¨å’ŒåŸºäºè§„åˆ™çš„è‡ªåŠ¨è¯„ä¼°éªŒè¯å™¨ã€‚Enigmata-Evalçš„ä¸¥æ ¼åŸºå‡†æµ‹è¯•å’Œä¼˜åŒ–çš„å¤šä»»åŠ¡RLVRç­–ç•¥è¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚ç»è¿‡Enigmataè®­ç»ƒçš„Qwen2.5-32Bæ¨¡å‹åœ¨è°œé¢˜æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†o3-mini-highå’Œo1ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚å½“åœ¨æ›´å¤§çš„æ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œå¦‚Seed1.5-Thinkingï¼ˆå·²æ¿€æ´»å‚æ•°ä¸ºäºŒåäº¿ï¼Œæ€»å‚æ•°ä¸ºäºŒç™¾äº¿ï¼‰ï¼Œæ¥è‡ªEnigmataçš„è°œé¢˜æ•°æ®è¿›ä¸€æ­¥æå‡äº†å…¶åœ¨é«˜çº§æ•°å­¦å’ŒSTEMæ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ­¤å·¥ä½œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€ã€å¯æ§çš„æ¡†æ¶ï¼Œä»¥æ¨åŠ¨LLMsçš„é€»è¾‘æ¨ç†èƒ½åŠ›å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é«˜çº§æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä½†ä»é¢ä¸´è§£å†³äººç±»æ— éœ€ç‰¹å®šé¢†åŸŸçŸ¥è¯†å³å¯è§£å†³çš„è°œé¢˜æŒ‘æˆ˜ã€‚</li>
<li>Enigmataå¥—ä»¶æ—¨åœ¨æå‡LLMsçš„è°œé¢˜æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä¸ƒå¤§ç±»åˆ«å…±36é¡¹ä»»åŠ¡ï¼Œé…å¤‡ç”Ÿæˆå™¨å’ŒéªŒè¯å™¨æ”¯æŒå¤šä»»åŠ¡è®­ç»ƒã€ç²¾ç»†åˆ†æä»¥åŠä¸RLVRæ— ç¼é›†æˆã€‚</li>
<li>Enigmataæä¾›ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ï¼ˆEnigmata-Evalï¼‰å’Œä¼˜åŒ–çš„å¤šä»»åŠ¡RLVRç­–ç•¥ã€‚</li>
<li>Qwen2.5-32Bæ¨¡å‹ç»è¿‡Enigmataè®­ç»ƒåï¼Œåœ¨è°œé¢˜æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Enigmataå¥—ä»¶æœ‰åŠ©äºæå‡LLMsåœ¨æ•°å­¦å’ŒSTEMé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Enigmataæä¾›äº†ä¸€ä¸ªç»Ÿä¸€ã€å¯æ§çš„æ¡†æ¶ï¼Œæœ‰åŠ©äºæ¨åŠ¨LLMsçš„é€»è¾‘æ¨ç†èƒ½åŠ›å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de9dfe1f98a2b74a32a2fdb39e2df875.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d1f206baf012458973de3c909a84b01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bff5af8d287bbac16ca8f1029cb5fcb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e1883f8ee733dc50ea93f5816f41fc7.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SynLogic-Synthesizing-Verifiable-Reasoning-Data-at-Scale-for-Learning-Logical-Reasoning-and-Beyond"><a href="#SynLogic-Synthesizing-Verifiable-Reasoning-Data-at-Scale-for-Learning-Logical-Reasoning-and-Beyond" class="headerlink" title="SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning   Logical Reasoning and Beyond"></a>SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning   Logical Reasoning and Beyond</h2><p><strong>Authors:Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He</strong></p>
<p>Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at <a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI/SynLogic">https://github.com/MiniMax-AI/SynLogic</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¦‚OpenAI-o1å’ŒDeepSeek R1ç­‰è¿›å±•è¡¨æ˜äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚è™½ç„¶å¼€æºå¤åˆ¶å·¥ä½œä¸»è¦å…³æ³¨æ•°å­¦å’Œç¼–ç é¢†åŸŸï¼Œä½†å¼€å‘é€šç”¨æ¨ç†èƒ½åŠ›çš„æ–¹æ³•å’Œèµ„æºä»ç„¶æ¢ç´¢ä¸è¶³ã€‚è¿™ä¸€å·®è·éƒ¨åˆ†æ˜¯ç”±äºæ”¶é›†é€‚åˆå¼ºåŒ–å­¦ä¹ çš„å¤šæ ·åŒ–å’Œå¯éªŒè¯çš„æ¨ç†æ•°æ®å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å‡è®¾é€»è¾‘æ¨ç†å¯¹äºå¼€å‘é€šç”¨æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå› ä¸ºé€»è¾‘æ˜¯æ¨ç†çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SynLogicï¼Œè¿™æ˜¯ä¸€ä¸ªæ•°æ®åˆæˆæ¡†æ¶å’Œæ•°æ®é›†ï¼Œå¯ä»¥å¤§è§„æ¨¡ç”Ÿæˆå¤šæ ·çš„é€»è¾‘æ¨ç†æ•°æ®ï¼Œæ¶µç›–35ç§ä¸åŒçš„é€»è¾‘æ¨ç†ä»»åŠ¡ã€‚SynLogicæ–¹æ³•èƒ½å¤Ÿæ§åˆ¶æ•°æ®å’Œè°ƒæ•´éš¾åº¦å’Œæ•°é‡ã€‚é‡è¦çš„æ˜¯ï¼Œæ‰€æœ‰ä¾‹å­éƒ½å¯ä»¥é€šè¿‡ç®€å•è§„åˆ™è¿›è¡ŒéªŒè¯ï¼Œä½¿å®ƒä»¬éå¸¸é€‚åˆå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬éªŒè¯äº†å¼ºåŒ–å­¦ä¹ åœ¨SynLogicæ•°æ®é›†ä¸Šè®­ç»ƒçš„æœ‰æ•ˆæ€§ï¼ŒåŸºäº7Bå’Œ32Bæ¨¡å‹ã€‚SynLogicåœ¨å¼€æºæ•°æ®é›†ä¸­å¤„äºæœ€å…ˆè¿›çš„é€»è¾‘æ¨ç†æ€§èƒ½åœ°ä½ï¼Œåœ¨BBEHä¸Šè¶…è¶Šäº†DeepSeek-R1-Distill-Qwen-32B 6ä¸ªç‚¹ã€‚æ­¤å¤–ï¼Œå°†SynLogicæ•°æ®ä¸æ•°å­¦å’Œç¼–ç ä»»åŠ¡æ··åˆï¼Œæé«˜äº†è¿™äº›é¢†åŸŸçš„è®­ç»ƒæ•ˆç‡ï¼Œå¹¶æ˜¾è‘—å¢å¼ºäº†æ¨ç†æ³›åŒ–èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ··åˆè®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†DeepSeek-R1-Zero-Qwen-32Bã€‚è¿™äº›å‘ç°ä½¿SynLogicæˆä¸ºæ¨åŠ¨LLMæ›´å¹¿æ³›æ¨ç†èƒ½åŠ›çš„é‡è¦èµ„æºã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/MiniMax-AI/SynLogic">https://github.com/MiniMax-AI/SynLogic</a>å¤„å¼€æºäº†æ•°æ®åˆæˆç®¡é“å’ŒSynLogicæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19641v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SynLogicæ•°æ®åˆæˆæ¡†æ¶å’Œæ•°æ®é›†ï¼Œç”¨äºç”Ÿæˆæ¶µç›–35ç§ä¸åŒé€»è¾‘æ¨ç†ä»»åŠ¡çš„å¤šæ ·åŒ–æ•°æ®ã€‚è¯¥æ•°æ®é›†å¯é€šè¿‡æ§åˆ¶åˆæˆæ•°æ®çš„éš¾åº¦å’Œæ•°é‡æ¥è°ƒæ•´ï¼Œæ‰€æœ‰å®ä¾‹éƒ½å¯ä»¥é€šè¿‡ç®€å•è§„åˆ™è¿›è¡ŒéªŒè¯ï¼Œéå¸¸é€‚åˆä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºSynLogicæ•°æ®é›†è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒå¯æœ‰æ•ˆæé«˜é€»è¾‘æ¨ç†æ€§èƒ½ï¼Œå¹¶ä¼˜äºå…¶ä»–å¼€æºæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œå°†SynLogicæ•°æ®ä¸æ•°å­¦å’Œç¼–ç ä»»åŠ¡æ··åˆè®­ç»ƒï¼Œå¯æé«˜è¿™äº›é¢†åŸŸçš„è®­ç»ƒæ•ˆç‡ï¼Œå¹¶æ˜¾è‘—å¢å¼ºæ¨ç†æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynLogicæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆå¤§è§„æ¨¡é€»è¾‘æ¨ç†æ•°æ®çš„åˆæˆæ¡†æ¶å’Œæ•°æ®é›†ã€‚</li>
<li>è¯¥æ•°æ®é›†æ¶µç›–35ç§ä¸åŒçš„é€»è¾‘æ¨ç†ä»»åŠ¡ï¼Œå¹¶å¯é€šè¿‡è°ƒæ•´éš¾åº¦å’Œæ•°é‡æ¥æ§åˆ¶æ•°æ®çš„åˆæˆã€‚</li>
<li>SynLogicæ•°æ®æ‰€æœ‰å®ä¾‹å‡å¯é€šè¿‡ç®€å•è§„åˆ™è¿›è¡ŒéªŒè¯ï¼Œé€‚åˆç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨SynLogicæ•°æ®é›†ä¸Šçš„è®­ç»ƒå¯æœ‰æ•ˆæé«˜é€»è¾‘æ¨ç†æ€§èƒ½ã€‚</li>
<li>SynLogicæ•°æ®åœ¨å¼€æ”¾æºä»£ç æ•°æ®é›†ä¸Šè¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œå°¤å…¶æ˜¯åœ¨BBEHä¸Šçš„å¾—åˆ†è¶…è¿‡äº†DeepSeek-R1-Distill-Qwen-32B 6ä¸ªç‚¹ã€‚</li>
<li>å°†SynLogicæ•°æ®ä¸æ•°å­¦å’Œç¼–ç ä»»åŠ¡æ··åˆè®­ç»ƒï¼Œå¯ä»¥æé«˜è¿™äº›é¢†åŸŸçš„è®­ç»ƒæ•ˆç‡å¹¶å¢å¼ºæ¨ç†æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87efdd0326ae62d95c933720dbd24fd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae46e603d036b51c6aee01664b142f07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ee478aea4d7b9a46e198f0148d35776.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d2d1aeb165ce18fca2291b8fbcf6cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce1ec7471c9c52fc4eb01fcfddb11526.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Delving-into-RL-for-Image-Generation-with-CoT-A-Study-on-DPO-vs-GRPO"><a href="#Delving-into-RL-for-Image-Generation-with-CoT-A-Study-on-DPO-vs-GRPO" class="headerlink" title="Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO"></a>Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO</h2><p><strong>Authors:Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, Pheng-Ann Heng</strong></p>
<p>Recent advancements underscore the significant role of Reinforcement Learning (RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). Two prominent RL algorithms, Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central to these developments, showcasing different pros and cons. Autoregressive image generation, also interpretable as a sequential CoT reasoning process, presents unique challenges distinct from LLM-based CoT reasoning. These encompass ensuring text-image consistency, improving image aesthetic quality, and designing sophisticated reward models, rather than relying on simpler rule-based rewards. While recent efforts have extended RL to this domain, these explorations typically lack an in-depth analysis of the domain-specific challenges and the characteristics of different RL strategies. To bridge this gap, we provide the first comprehensive investigation of the GRPO and DPO algorithms in autoregressive image generation, evaluating their in-domain performance and out-of-domain generalization, while scrutinizing the impact of different reward models on their respective capabilities. Our findings reveal that GRPO and DPO exhibit distinct advantages, and crucially, that reward models possessing stronger intrinsic generalization capabilities potentially enhance the generalization potential of the applied RL algorithms. Furthermore, we systematically explore three prevalent scaling strategies to enhance both their in-domain and out-of-domain proficiency, deriving unique insights into efficiently scaling performance for each paradigm. We hope our study paves a new path for inspiring future work on developing more effective RL algorithms to achieve robust CoT reasoning in the realm of autoregressive image generation. Code is released at <a target="_blank" rel="noopener" href="https://github.com/ZiyuGuo99/Image-Generation-CoT">https://github.com/ZiyuGuo99/Image-Generation-CoT</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å‘å±•çªæ˜¾äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½åŠ›ä¸­çš„é‡è¦ä½œç”¨ã€‚ä¸¤ç§çªå‡ºçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰â€”â€”æ˜¯è¿™äº›å‘å±•çš„æ ¸å¿ƒï¼Œå±•ç¤ºäº†å„è‡ªçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚è‡ªåŠ¨å›å½’å›¾åƒç”Ÿæˆï¼Œä¹Ÿå¯è§£é‡Šä¸ºä¸€ç§è¿ç»­çš„CoTæ¨ç†è¿‡ç¨‹ï¼Œå‘ˆç°å‡ºä¸åŸºäºLLMçš„CoTæ¨ç†ä¸åŒçš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜åŒ…æ‹¬ç¡®ä¿æ–‡æœ¬ä¸å›¾åƒçš„ä¸€è‡´æ€§ã€æé«˜å›¾åƒçš„å®¡ç¾è´¨é‡ï¼Œä»¥åŠè®¾è®¡å¤æ‚å¥–åŠ±æ¨¡å‹ï¼Œè€Œä¸æ˜¯ä¾èµ–æ›´ç®€å•çš„åŸºäºè§„åˆ™çš„å¥–åŠ±ã€‚å°½ç®¡æœ€è¿‘çš„åŠªåŠ›å·²ç»å°†å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°äº†è¿™ä¸ªé¢†åŸŸï¼Œä½†è¿™äº›æ¢ç´¢é€šå¸¸ç¼ºä¹å¯¹ç‰¹å®šé¢†åŸŸæŒ‘æˆ˜å’Œä¸åŒçš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ç‰¹ç‚¹çš„æ·±å…¥åˆ†æã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹è‡ªåŠ¨å›å½’å›¾åƒç”Ÿæˆä¸­çš„GRPOå’ŒDPOç®—æ³•è¿›è¡Œäº†é¦–æ¬¡å…¨é¢çš„è°ƒæŸ¥ï¼Œè¯„ä¼°äº†å®ƒä»¬çš„é¢†åŸŸå†…æ€§èƒ½å’Œè·¨é¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä»”ç»†ç ”ç©¶äº†ä¸åŒå¥–åŠ±æ¨¡å‹å¯¹å…¶å„è‡ªèƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒGRPOå’ŒDPOå…·æœ‰å„è‡ªç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œè€Œä¸”é‡è¦çš„æ˜¯ï¼Œå…·æœ‰æ›´å¼ºå†…åœ¨æ³›åŒ–èƒ½åŠ›çš„å¥–åŠ±æ¨¡å‹å¯èƒ½æé«˜äº†æ‰€åº”ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ³›åŒ–æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¢ç´¢äº†ä¸‰ç§æµè¡Œçš„æ‰©å±•ç­–ç•¥ï¼Œä»¥æé«˜å®ƒä»¬åœ¨ä¸åŒé¢†åŸŸçš„ä¸“ä¸šèƒ½åŠ›ï¼Œä¸ºæ¯ç§èŒƒå¼æœ‰æ•ˆåœ°æ‰©å±•æ€§èƒ½æä¾›äº†ç‹¬ç‰¹çš„è§è§£ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½ä¸ºæœªæ¥å¼€å‘æ›´æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å·¥ä½œé“ºå¹³é“è·¯ï¼Œä»¥å®ç°è‡ªåŠ¨å›å½’å›¾åƒç”Ÿæˆé¢†åŸŸçš„ç¨³å¥CoTæ¨ç†ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ZiyuGuo99/Image-Generation-CoT%E3%80%82">https://github.com/ZiyuGuo99/Image-Generation-CoTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17017v2">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/ZiyuGuo99/Image-Generation-CoT">https://github.com/ZiyuGuo99/Image-Generation-CoT</a></p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„é“¾å¼æ€ç»´æ¨ç†èƒ½åŠ›ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æœ¬æ–‡ä¸»è¦æ¢è®¨äº†Group Relative Policy Optimization (GRPO)å’ŒDirect Preference Optimization (DPO)ä¸¤ç§å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨è‡ªåŠ¨å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å‘ç°åœ¨è‡ªåŠ¨å›¾åƒç”Ÿæˆä¸­ï¼Œè¿™ä¸¤ç§ç®—æ³•å„æœ‰ä¼˜åŠ¿ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¼ºå†…åœ¨æ³›åŒ–èƒ½åŠ›çš„å¥–åŠ±æ¨¡å‹èƒ½å¤Ÿæå‡ç®—æ³•çš„æ³›åŒ–æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†ä¸‰ç§æµè¡Œçš„æ‰©å±•ç­–ç•¥æ¥æå‡ç®—æ³•åœ¨é¢†åŸŸå†…çš„è¡¨ç°ï¼Œå¹¶å¯¹äºæ¯ä¸€ç§ç­–ç•¥è¿›è¡Œäº†æ·±å…¥çš„åˆ†æã€‚æœ¬ç ”ç©¶çš„å‘ç°å°†æœ‰åŠ©äºä¸ºæœªæ¥ç ”å‘æ›´é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ç°è‡ªåŠ¨å›¾åƒç”Ÿæˆä¸­çš„ç¨³å¥æ€ç»´æ¨ç†å¼€è¾Ÿæ–°çš„è·¯å¾„ã€‚ä»£ç å·²å¼€æºäºGitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„é“¾å¼æ€ç»´æ¨ç†èƒ½åŠ›æ–¹é¢æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>Group Relative Policy Optimization (GRPO)å’ŒDirect Preference Optimization (DPO)åœ¨è‡ªåŠ¨å›¾åƒç”Ÿæˆä¸­å…·æœ‰ä¸åŒçš„ä¼˜åŠ¿ã€‚</li>
<li>å¥–åŠ±æ¨¡å‹æ‹¥æœ‰æ›´å¼ºçš„å†…åœ¨æ³›åŒ–èƒ½åŠ›æœ‰åŠ©äºå¢å¼ºå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ³›åŒ–æ½œåŠ›ã€‚</li>
<li>ä¸‰ç§æ‰©å±•ç­–ç•¥åœ¨æå‡ç®—æ³•åœ¨é¢†åŸŸå†…è¡¨ç°ä¸Šè¢«æ·±å…¥æ¢è®¨å¹¶äº§ç”Ÿç‹¬ç‰¹è§è§£ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-42a38db5685ffaa78221f4163e8e60ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2078dc50658837cf001220cb50c6cad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b2510c4e7adf79e2ddf6cf6ce5accac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9e4128c368775092031356c9fe7b433.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de52380d3e6be37cc816cc81b1cd5991.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Learning-to-Reason-via-Mixture-of-Thought-for-Logical-Reasoning"><a href="#Learning-to-Reason-via-Mixture-of-Thought-for-Logical-Reasoning" class="headerlink" title="Learning to Reason via Mixture-of-Thought for Logical Reasoning"></a>Learning to Reason via Mixture-of-Thought for Logical Reasoning</h2><p><strong>Authors:Tong Zheng, Lichang Chen, Simeng Han, R. Thomas McCoy, Heng Huang</strong></p>
<p>Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typically natural language. Although some methods explored modality selection or augmentation at inference time, the training process remains modality-blind, limiting synergy among modalities. To fill in this gap, we propose Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three complementary modalities: natural language, code, and a newly introduced symbolic modality, truth-table, which systematically enumerates logical cases and partially mitigates key failure modes in natural language reasoning. MoT adopts a two-phase design: (1) self-evolving MoT training, which jointly learns from filtered, self-generated rationales across modalities; and (2) MoT inference, which fully leverages the synergy of three modalities to produce better predictions. Experiments on logical reasoning benchmarks including FOLIO and ProofWriter demonstrate that our MoT framework consistently and significantly outperforms strong LLM baselines with single-modality chain-of-thought approaches, achieving up to +11.7pp average accuracy gain. Further analyses show that our MoT framework benefits both training and inference stages; that it is particularly effective on harder logical reasoning problems; and that different modalities contribute complementary strengths, with truth-table reasoning helping to overcome key bottlenecks in natural language inference. </p>
<blockquote>
<p>äººç±»è‡ªç„¶åˆ©ç”¨å¤šç§æ¨ç†æ¨¡å¼æ¥å­¦ä¹ å’Œè§£å†³é€»è¾‘é—®é¢˜ï¼Œä¾‹å¦‚è‡ªç„¶è¯­è¨€ã€ä»£ç å’Œç¬¦å·é€»è¾‘ç­‰ä¸åŒçš„è¡¨ç¤ºæ ¼å¼ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å¤šæ•°ç°æœ‰çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åªä½¿ç”¨å•ä¸€çš„æ¨ç†æ¨¡å¼ï¼Œé€šå¸¸æ˜¯è‡ªç„¶è¯­è¨€ã€‚å°½ç®¡ä¸€äº›æ–¹æ³•åœ¨æ¨ç†æ—¶é—´æ¢ç´¢äº†æ¨¡å¼é€‰æ‹©æˆ–å¢å¼ºï¼Œä½†è®­ç»ƒè¿‡ç¨‹ä»ç„¶å¯¹æ¨¡å¼æ˜¯ç›²æ€çš„ï¼Œé™åˆ¶äº†æ¨¡å¼ä¹‹é—´çš„ååŒä½œç”¨ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†Thought Mixtureï¼ˆMoTï¼‰æ¡†æ¶ï¼Œå®ƒä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸‰ç§äº’è¡¥æ¨¡å¼ä¹‹é—´è¿›è¡Œæ¨ç†ï¼šè‡ªç„¶è¯­è¨€ã€ä»£ç å’Œæ–°å¼•å…¥çš„ç¬¦å·æ¨¡å¼â€”â€”çœŸå€¼è¡¨ã€‚çœŸå€¼è¡¨ç³»ç»Ÿåœ°æšä¸¾é€»è¾‘æƒ…å†µï¼Œå¹¶éƒ¨åˆ†å¼¥è¡¥äº†è‡ªç„¶è¯­è¨€æ¨ç†ä¸­çš„å…³é”®å¤±è´¥æ¨¡å¼ã€‚MoTé‡‡ç”¨ä¸¤é˜¶æ®µè®¾è®¡ï¼šï¼ˆ1ï¼‰è‡ªæˆ‘è¿›åŒ–çš„MoTè®­ç»ƒï¼Œä»è¿‡æ»¤åçš„ã€è‡ªæˆ‘ç”Ÿæˆçš„è·¨æ¨¡å¼ç†æ€§è”åˆå­¦ä¹ ï¼›ï¼ˆ2ï¼‰MoTæ¨ç†ï¼Œå……åˆ†åˆ©ç”¨ä¸‰ç§æ¨¡å¼çš„ååŒä½œç”¨æ¥äº§ç”Ÿæ›´å¥½çš„é¢„æµ‹ã€‚åœ¨åŒ…æ‹¬FOLIOå’ŒProofWriteråœ¨å†…çš„é€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MoTæ¡†æ¶åœ¨å•æ¨¡æ€æ€ç»´é“¾æ–¹æ³•ä¸­å§‹ç»ˆæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾+11.7ppã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MoTæ¡†æ¶å¯¹è®­ç»ƒå’Œæ¨ç†é˜¶æ®µéƒ½æœ‰ç›Šï¼›å®ƒå¯¹æ›´å¤æ‚çš„é€»è¾‘æ¨ç†é—®é¢˜ç‰¹åˆ«æœ‰æ•ˆï¼›ä¸åŒçš„æ¨¡å¼è´¡çŒ®å‡ºäº’è¡¥çš„ä¼˜åŠ¿ï¼ŒçœŸå€¼è¡¨æ¨ç†æœ‰åŠ©äºå…‹æœè‡ªç„¶è¯­è¨€æ¨ç†ä¸­çš„å…³é”®ç“¶é¢ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15817v2">PDF</a> 38 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMixture-of-Thoughtï¼ˆMoTï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿LLMsèƒ½å¤Ÿåœ¨è‡ªç„¶è¯­è¨€ã€ä»£ç å’Œæ–°çš„ç¬¦å·æ¨¡æ€ï¼ˆçœŸç†è¡¨ï¼‰ä¹‹é—´è¿›è¡Œæ¨ç†ã€‚MoTæ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè®¾è®¡ï¼šè‡ªæˆ‘è¿›åŒ–çš„MoTè®­ç»ƒå’ŒMoTæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒMoTæ¡†æ¶åœ¨é€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸ŠæŒç»­ä¸”æ˜¾è‘—åœ°ä¼˜äºå•æ¨¡æ€çš„LLMåŸºçº¿æ–¹æ³•ï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾+11.7ppã€‚è¯¥æ¡†æ¶å¯¹è®­ç»ƒå’Œæ¨ç†é˜¶æ®µéƒ½æœ‰ç›Šï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³æ›´å¤æ‚çš„é€»è¾‘é—®é¢˜æ—¶æ•ˆæœæ˜¾è‘—ã€‚ä¸åŒæ¨¡æ€çš„è´¡çŒ®å…·æœ‰äº’è¡¥æ€§ï¼ŒçœŸç†è¡¨æ¨ç†æœ‰åŠ©äºå…‹æœè‡ªç„¶è¯­è¨€æ¨ç†ä¸­çš„å…³é”®ç“¶é¢ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»åˆ©ç”¨å¤šç§æ¨ç†æ¨¡å¼æ¥å­¦ä¹ å’Œè§£å†³é€»è¾‘é—®é¢˜ï¼Œè€Œç°æœ‰çš„LLMæ–¹æ³•å¤§å¤šåœ¨è®­ç»ƒæœŸé—´åªä½¿ç”¨å•ä¸€æ¨ç†æ¨¡å¼ã€‚</li>
<li>Mixture-of-Thoughtï¼ˆMoTï¼‰æ¡†æ¶èƒ½å¤Ÿå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œä½¿LLMsåœ¨è‡ªç„¶è¯­è¨€ã€ä»£ç å’Œç¬¦å·æ¨¡æ€ï¼ˆçœŸç†è¡¨ï¼‰ä¹‹é—´è¿›è¡Œæ¨ç†ã€‚</li>
<li>MoTæ¡†æ¶åŒ…å«ä¸¤é˜¶æ®µè®¾è®¡ï¼šè‡ªæˆ‘è¿›åŒ–çš„MoTè®­ç»ƒå’ŒMoTæ¨ç†ï¼Œèƒ½å¤Ÿå……åˆ†åˆ©ç”¨ä¸åŒæ¨¡æ€ä¹‹é—´çš„ååŒä½œç”¨ã€‚</li>
<li>å®éªŒè¡¨æ˜MoTæ¡†æ¶åœ¨é€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºå•æ¨¡æ€LLMæ–¹æ³•ã€‚</li>
<li>MoTæ¡†æ¶å¯¹è®­ç»ƒå’Œæ¨ç†é˜¶æ®µéƒ½æœ‰ç§¯æå½±å“ï¼Œå°¤å…¶æœ‰åŠ©äºè§£å†³å¤æ‚çš„é€»è¾‘é—®é¢˜ã€‚</li>
<li>çœŸç†è¡¨æ¨¡æ€åœ¨è‡ªç„¶è¯­è¨€æ¨ç†ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œæœ‰åŠ©äºå…‹æœå…³é”®ç“¶é¢ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc77d55798a0edf711ee8239c4394e0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a6e141f63c2d0334780af7ab1266413.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8f2d0056453fa4dc29d4d0d0a672206.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07836e01d775af572593ab167bcfdab7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-896ffb7787816f3a1f3f92e9dd468aa5.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-15/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-15/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-16/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5c48b8e8212f227dbb8a8da4815a0dfc.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-16  DISCO Balances the Scales Adaptive Domain- and Difficulty-Aware   Reinforcement Learning on Imbalanced Data
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-14/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0106ae2049aba135f37814fd5207b9e6.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-14  Motion-R1 Chain-of-Thought Reasoning and Reinforcement Learning for   Human Motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28791.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
