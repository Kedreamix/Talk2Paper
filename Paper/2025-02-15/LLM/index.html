<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-15  MME-CoT Benchmarking Chain-of-Thought in Large Multimodal Models for   Reasoning Quality, Robustness, and Efficiency">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f0371212a75e39cb72fce6ceda00ddd2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    70 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-15-æ›´æ–°"><a href="#2025-02-15-æ›´æ–°" class="headerlink" title="2025-02-15 æ›´æ–°"></a>2025-02-15 æ›´æ–°</h1><h2 id="MME-CoT-Benchmarking-Chain-of-Thought-in-Large-Multimodal-Models-for-Reasoning-Quality-Robustness-and-Efficiency"><a href="#MME-CoT-Benchmarking-Chain-of-Thought-in-Large-Multimodal-Models-for-Reasoning-Quality-Robustness-and-Efficiency" class="headerlink" title="MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for   Reasoning Quality, Robustness, and Efficiency"></a>MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for   Reasoning Quality, Robustness, and Efficiency</h2><p><strong>Authors:Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li</strong></p>
<p>Answering questions with Chain-of-Thought (CoT) has significantly enhanced the reasoning capabilities of Large Language Models (LLMs), yet its impact on Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth investigation. In this paper, we introduce MME-CoT, a specialized benchmark evaluating the CoT reasoning performance of LMMs, spanning six domains: math, science, OCR, logic, space-time, and general scenes. As the first comprehensive study in this area, we propose a thorough evaluation suite incorporating three novel metrics that assess the reasoning quality, robustness, and efficiency at a fine-grained level. Leveraging curated high-quality data and a unique evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs, uncovering several key insights: 1) Models with reflection mechanism demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and demonstrating the highest quality results; 2) CoT prompting often degrades LMM performance on perception-heavy tasks, suggesting a potentially harmful overthinking behavior; and 3) Although the CoT quality is high, LMMs with reflection exhibit significant inefficiency in both normal response and self-correction phases. We hope MME-CoT serves as a foundation for advancing multimodal reasoning in LMMs. Project Page: <a target="_blank" rel="noopener" href="https://mmecot.github.io/">https://mmecot.github.io/</a> </p>
<blockquote>
<p>é‡‡ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰å›ç­”é—®é¢˜çš„æ–¹å¼å·²ç»æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œç„¶è€Œå®ƒå¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„å½±å“ä»ç¼ºä¹ç³»ç»Ÿçš„è¯„ä¼°å’Œæ·±å…¥ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MME-CoTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LMMçš„CoTæ¨ç†æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å…­ä¸ªé¢†åŸŸï¼šæ•°å­¦ã€ç§‘å­¦ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ã€é€»è¾‘ã€æ—¶ç©ºå’Œä¸€èˆ¬åœºæ™¯ã€‚ä½œä¸ºè¯¥é¢†åŸŸçš„é¦–æ¬¡ç»¼åˆç ”ç©¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨é¢çš„è¯„ä¼°å¥—ä»¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸‰ä¸ªæ–°é¢–çš„æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡åœ¨ç»†å¾®å±‚é¢ä¸Šè¯„ä¼°æ¨ç†è´¨é‡ã€ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬åˆ©ç”¨ç²¾å¿ƒæŒ‘é€‰çš„é«˜è´¨é‡æ•°æ®å’Œç‹¬ç‰¹çš„è¯„ä¼°ç­–ç•¥ï¼Œå¯¹æœ€æ–°çš„LMMè¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ­ç¤ºäº†å‡ ä¸ªå…³é”®è§è§£ï¼š1ï¼‰å…·æœ‰åå°„æœºåˆ¶çš„æ¨¡å‹è¡¨ç°å‡ºä¼˜è¶Šçš„CoTè´¨é‡ï¼ŒKimi k1.5åœ¨CoTè´¨é‡æ–¹é¢è¶…è¶Šäº†GPT-4oå¹¶è·å¾—äº†æœ€é«˜è´¨é‡çš„ç»“æœï¼›2ï¼‰CoTæç¤ºå¾€å¾€ä¼šé™ä½LMMåœ¨æ„ŸçŸ¥å¯†é›†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜å¯èƒ½å­˜åœ¨æœ‰å®³çš„è¿‡åº¦æ€è€ƒè¡Œä¸ºï¼›3ï¼‰å°½ç®¡CoTè´¨é‡å¾ˆé«˜ï¼Œä½†å…·æœ‰åå°„çš„LMMåœ¨æ­£å¸¸å“åº”å’Œè‡ªæˆ‘çº æ­£é˜¶æ®µéƒ½è¡¨ç°å‡ºæ˜¾è‘—çš„ä½æ•ˆã€‚æˆ‘ä»¬å¸Œæœ›MME-CoTèƒ½ä¸ºæ¨è¿›LMMä¸­çš„å¤šæ¨¡æ€æ¨ç†å¥ å®šåŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://mmecot.github.io/">https://mmecot.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09621v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://mmecot.github.io/">https://mmecot.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºChain-of-Thoughtï¼ˆCoTï¼‰çš„å›ç­”é—®é¢˜æ–¹å¼æ˜¾è‘—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¯¹äºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„å½±å“ä»ç¼ºä¹ç³»ç»Ÿè¯„ä¼°å’Œæ·±å…¥ç ”ç©¶ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LMMçš„CoTæ¨ç†æ€§èƒ½çš„åŸºå‡†æµ‹è¯•MME-CoTï¼Œæ¶µç›–å…­ä¸ªé¢†åŸŸï¼šæ•°å­¦ã€ç§‘å­¦ã€OCRã€é€»è¾‘ã€æ—¶ç©ºå’Œä¸€èˆ¬åœºæ™¯ã€‚ä½œä¸ºè¯¥é¢†åŸŸçš„é¦–é¡¹ç»¼åˆç ”ç©¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»†è‡´çš„è¯„ä»·å¥—ä»¶ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæ–°çš„æŒ‡æ ‡ï¼Œç”¨äºç²¾ç»†åœ°è¯„ä¼°æ¨ç†è´¨é‡ã€ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚é€šè¿‡å¯¹æœ€æ–°LMMçš„æ·±å…¥åˆ†æï¼Œæœ¬æ–‡æ­ç¤ºäº†å‡ ä¸ªå…³é”®è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¸¦æœ‰åå°„æœºåˆ¶çš„æ¨¡å‹åœ¨CoTè´¨é‡ä¸Šè¡¨ç°å‡ºä¼˜åŠ¿ï¼ŒKimi k1.5åœ¨CoTè´¨é‡æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä¼˜äºGPT-4oã€‚</li>
<li>CoTæç¤ºåœ¨æ„ŸçŸ¥å¯†é›†å‹ä»»åŠ¡ä¸Šå¯èƒ½ä¼šé™ä½LMMçš„æ€§èƒ½ï¼Œè¡¨ç°å‡ºæ½œåœ¨çš„è¿‡åº¦æ€è€ƒè¡Œä¸ºã€‚</li>
<li>å°½ç®¡CoTè´¨é‡å¾ˆé«˜ï¼Œä½†å…·æœ‰åå°„åŠŸèƒ½çš„LMMåœ¨å¸¸è§„å“åº”å’Œè‡ªæˆ‘çº æ­£é˜¶æ®µå­˜åœ¨æ˜¾è‘—æ•ˆç‡é—®é¢˜ã€‚</li>
<li>MME-CoTä½œä¸ºåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä¸ºæ¨è¿›LMMä¸­çš„å¤šæ¨¡æ€æ¨ç†å¥ å®šåŸºç¡€ã€‚</li>
<li>ç ”ç©¶æ¶µç›–äº†æ•°å­¦ã€ç§‘å­¦ã€OCRã€é€»è¾‘ã€æ—¶ç©ºå’Œä¸€èˆ¬åœºæ™¯ç­‰å…­ä¸ªé¢†åŸŸçš„è¯„ä¼°ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªè¯„ä»·å¥—ä»¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ–°çš„æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°æ¨ç†è´¨é‡ã€ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a2ff3269361d9bc782839d0b334553e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2858cce6959d25ea1ffd17b6ca1a0863.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c3c4c68e5f0cd3e13e1deb1e9a6b9b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4fee05e62678d7167f70ddd9e0fb408.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbc4f03f33a31c91820a39126d9bac08.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Potential-of-Encoder-free-Architectures-in-3D-LMMs"><a href="#Exploring-the-Potential-of-Encoder-free-Architectures-in-3D-LMMs" class="headerlink" title="Exploring the Potential of Encoder-free Architectures in 3D LMMs"></a>Exploring the Potential of Encoder-free Architectures in 3D LMMs</h2><p><strong>Authors:Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao</strong></p>
<p>Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM early layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at <a target="_blank" rel="noopener" href="https://github.com/Ivan-Tang-3D/ENEL">https://github.com/Ivan-Tang-3D/ENEL</a> </p>
<blockquote>
<p>æ— ç¼–ç å™¨æ¶æ„å·²åœ¨2Dè§†è§‰é¢†åŸŸè¿›è¡Œäº†åˆæ­¥æ¢ç´¢ï¼Œç„¶è€Œï¼Œå®ƒä»¬æ˜¯å¦èƒ½æœ‰æ•ˆåº”ç”¨äº3Dç†è§£åœºæ™¯ä»æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢æ¢è®¨äº†æ— ç¼–ç å™¨æ¶æ„åœ¨å…‹æœåŸºäºç¼–ç å™¨çš„3Då¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„æŒ‘æˆ˜æ–¹é¢çš„æ½œåŠ›ã€‚è¿™äº›æŒ‘æˆ˜åŒ…æ‹¬éš¾ä»¥é€‚åº”ä¸åŒçš„ç‚¹äº‘åˆ†è¾¨ç‡ä»¥åŠç¼–ç å™¨æå–çš„ç‚¹ç‰¹å¾ä¸ç¬¦åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­ä¹‰éœ€æ±‚ã€‚æˆ‘ä»¬ç¡®å®šäº†3D LMMå»é™¤ç¼–ç å™¨å¹¶ä½¿LLMæ‰¿æ‹…3Dç¼–ç å™¨è§’è‰²çš„å…³é”®æ–¹é¢ï¼š1ï¼‰æˆ‘ä»¬åœ¨é¢„è®­ç»ƒé˜¶æ®µæå‡ºäº†LLMåµŒå…¥è¯­ä¹‰ç¼–ç ç­–ç•¥ï¼Œæ¢ç´¢äº†å„ç§ç‚¹äº‘è‡ªç›‘ç£æŸå¤±çš„å½±å“ã€‚å¹¶æå‡ºäº†æ··åˆè¯­ä¹‰æŸå¤±æ¥æå–é«˜çº§è¯­ä¹‰ã€‚2ï¼‰æˆ‘ä»¬åœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µå¼•å…¥äº†åˆ†å±‚å‡ ä½•èšåˆç­–ç•¥ã€‚è¿™å¯ä»¥å°†å½’çº³åç½®èå…¥LLMçš„æ—©æœŸå±‚ï¼Œä»¥å…³æ³¨ç‚¹äº‘çš„å±€éƒ¨ç»†èŠ‚ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªæ— ç¼–ç å™¨3D LMMï¼ŒENELã€‚æˆ‘ä»¬çš„7Bæ¨¡å‹ä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ShapeLLM-13Bç›¸ç«äº‰ï¼Œåœ¨åˆ†ç±»ã€æè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šåˆ†åˆ«å®ç°äº†55.0%ã€50.92%å’Œ42.7%çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ— ç¼–ç å™¨æ¶æ„åœ¨3Dç†è§£é¢†åŸŸæ›¿ä»£åŸºäºç¼–ç å™¨çš„æ¶æ„å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚[ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Ivan-Tang-3D/ENEL">https://github.com/Ivan-Tang-3D/ENEL</a>]</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09620v1">PDF</a> The code is released at <a target="_blank" rel="noopener" href="https://github.com/Ivan-Tang-3D/ENEL">https://github.com/Ivan-Tang-3D/ENEL</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡é¦–æ¬¡å…¨é¢æ¢è®¨äº†æ— ç¼–ç å™¨æ¶æ„åœ¨å…‹æœåŸºäºç¼–ç å™¨çš„ä¸‰ç»´å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æŒ‘æˆ˜æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶æå‡ºäº†LLMåµŒå…¥è¯­ä¹‰ç¼–ç ç­–ç•¥ä¸æ··åˆè¯­ä¹‰æŸå¤±æ–¹æ³•ï¼Œä»¥æå–é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå¼•å…¥å±‚æ¬¡åŒ–å‡ ä½•èšåˆç­–ç•¥ä»¥ä¸“æ³¨äºç‚¹äº‘çš„å±€éƒ¨ç»†èŠ‚ã€‚æœ€ç»ˆæ„å»ºå‡ºæ— ç¼–ç å™¨æ¶æ„çš„ä¸‰ç»´LMMâ€”â€”ENELæ¨¡å‹ã€‚åœ¨åˆ†ç±»ã€æè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šï¼ŒENELæ¨¡å‹ä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ShapeLLM-13Bç›¸æ¯”è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç°å‡ºæ— ç¼–ç å™¨æ¶æ„åœ¨ä¸‰ç»´ç†è§£é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒäºå¼€æºç½‘ç«™ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡ç« æ¢ç´¢äº†æ— ç¼–ç å™¨æ¶æ„åœ¨ä¸‰ç»´è§†è§‰é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ï¼Œé’ˆå¯¹ç°æœ‰åŸºäºç¼–ç å™¨çš„ä¸‰ç»´å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æŒ‘æˆ˜æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æå‡ºLLMåµŒå…¥è¯­ä¹‰ç¼–ç ç­–ç•¥ï¼Œåœ¨é¢„è®­ç»ƒé˜¶æ®µæ¢ç´¢å„ç§ç‚¹äº‘è‡ªç›‘ç£æŸå¤±çš„å½±å“ï¼Œå¹¶å¼•å…¥æ··åˆè¯­ä¹‰æŸå¤±ä»¥æå–é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥å±‚æ¬¡åŒ–å‡ ä½•èšåˆç­–ç•¥ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µå°†å½’çº³åè§èå…¥LLMæ—©æœŸå±‚ï¼Œä»¥å…³æ³¨ç‚¹äº‘çš„å±€éƒ¨ç»†èŠ‚ã€‚</li>
<li>æˆåŠŸæ„å»ºé¦–ä¸ªæ— ç¼–ç å™¨æ¶æ„çš„ä¸‰ç»´LMMâ€”â€”ENELæ¨¡å‹ï¼Œå¹¶åœ¨åˆ†ç±»ã€æè¿°å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå®ç°äº†ä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>ENELæ¨¡å‹æ˜¾ç¤ºäº†åœ¨å¤„ç†ä¸åŒç‚¹äº‘åˆ†è¾¨ç‡ä»¥åŠæ»¡è¶³å¤§å‹è¯­è¨€æ¨¡å‹è¯­ä¹‰éœ€æ±‚æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜æ— ç¼–ç å™¨æ¶æ„åœ¨ä¸‰ç»´ç†è§£é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯èƒ½ä¼šå–ä»£åŸºäºç¼–ç å™¨çš„æ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b27ff9324f7d878ecc3fdeb643dd91b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c01aa1281ce5577c7d9714bb9b54c07a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42a2ef0f117bd00f1bbfe04267ca733f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f77bd8e38aed258fde6e920fd9aefd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-533ef7105ecdabdbc0b93ba25df04bf2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0962701a83416d82e29054661185dc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3985fb578795947e31dff68211e343bb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Do-LLMs-Recognize-Your-Preferences-Evaluating-Personalized-Preference-Following-in-LLMs"><a href="#Do-LLMs-Recognize-Your-Preferences-Evaluating-Personalized-Preference-Following-in-LLMs" class="headerlink" title="Do LLMs Recognize Your Preferences? Evaluating Personalized Preference   Following in LLMs"></a>Do LLMs Recognize Your Preferences? Evaluating Personalized Preference   Following in LLMs</h2><p><strong>Authors:Siyan Zhao, Mingyi Hong, Yang Liu, Devamanyu Hazarika, Kaixiang Lin</strong></p>
<p>Large Language Models (LLMs) are increasingly used as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMsâ€™ ability to infer, memorize and adhere to user preferences in a long-context conversational setting. PrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we evaluated the aforementioned preference following capabilities of 10 open-source and proprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. Our benchmarking effort reveals that state-of-the-art LLMs face significant challenges in proactively following usersâ€™ preferences during conversations. In particular, in zero-shot settings, preference following accuracy falls below 10% at merely 10 turns (~3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. Furthermore, we show that fine-tuning on PrefEval significantly improves performance. We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMsâ€™ preference following abilities, paving the way for personalized conversational agents. Our code and dataset are available at <a target="_blank" rel="noopener" href="https://prefeval.github.io/">https://prefeval.github.io/</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨ä½œèŠå¤©æœºå™¨äººï¼Œç„¶è€Œå®ƒä»¬æ ¹æ®ç”¨æˆ·åå¥½è¿›è¡Œä¸ªæ€§åŒ–å“åº”çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬å¼•å…¥äº†PrefEvalï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMåœ¨é•¿æ—¶é—´ä¸Šä¸‹æ–‡å¯¹è¯ç¯å¢ƒä¸­æ¨æ–­ã€è®°å¿†å’Œéµå®ˆç”¨æˆ·åå¥½çš„èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚PrefEvalåŒ…å«3000ä¸ªæ‰‹åŠ¨æ•´ç†çš„ç”¨æˆ·åå¥½å’ŒæŸ¥è¯¢å¯¹ï¼Œæ¶µç›–20ä¸ªä¸»é¢˜ã€‚PrefEvalä»¥æ˜¾å¼å’Œéšå¼ä¸¤ç§å½¢å¼åŒ…å«ç”¨æˆ·ä¸ªæ€§åŒ–æˆ–åå¥½ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨ç”Ÿæˆä»»åŠ¡å’Œåˆ†ç±»ä»»åŠ¡æ¥è¯„ä¼°LLMçš„æ€§èƒ½ã€‚é€šè¿‡PrefEvalï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸Šè¿°10ä¸ªå¼€æºå’Œä¸“æœ‰LLMåœ¨å…·æœ‰ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆæœ€é•¿å¯è¾¾10ä¸‡ä»¤ç‰Œï¼‰çš„å¤šä¼šè¯å¯¹è¯ä¸­çš„åå¥½éµå¾ªèƒ½åŠ›ã€‚æˆ‘ä»¬ç”¨å„ç§æç¤ºã€è¿­ä»£åé¦ˆå’Œæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œå°–ç«¯LLMåœ¨å¯¹è¯ä¸­ä¸»åŠ¨éµå¾ªç”¨æˆ·åå¥½æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ï¼Œå¤§å¤šæ•°è¯„ä¼°æ¨¡å‹çš„åå¥½éµå¾ªå‡†ç¡®ç‡åœ¨ä»…10è½®ï¼ˆçº¦3000ä¸ªä»¤ç‰Œï¼‰å†…å°±ä¸‹é™åˆ°ä½äº10%ã€‚å³ä½¿åœ¨é•¿æ—¶é—´ä¸Šä¸‹æ–‡å¯¹è¯ä¸­é‡‡ç”¨å…ˆè¿›çš„æç¤ºå’Œæ£€ç´¢æ–¹æ³•ï¼Œåå¥½éµå¾ªèƒ½åŠ›ä»ç„¶ä¼šä¸‹é™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜åœ¨PrefEvalä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡PrefEvalå¯¹äºè¡¡é‡ã€ç†è§£å’Œæé«˜LLMçš„åå¥½éµå¾ªèƒ½åŠ›å…·æœ‰é‡è¦ä»·å€¼ï¼Œä¸ºä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººé“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://prefeval.github.io/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://prefeval.github.io/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09597v1">PDF</a> Accepted at ICLR 2025 as oral presentation. Code and data at:   <a target="_blank" rel="noopener" href="https://prefeval.github.io/">https://prefeval.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>LLMåœ¨èŠå¤©æœºå™¨äººé¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å…¶å¯¹ç”¨æˆ·åå¥½çš„ä¸ªæ€§åŒ–å“åº”èƒ½åŠ›ä»æœ‰å±€é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºPrefEvalåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨å…·æœ‰é•¿æœŸä¸Šä¸‹æ–‡å¯¹è¯ç¯å¢ƒä¸­å¯¹ç”¨æˆ·åå¥½çš„æ¨æ–­ã€è®°å¿†å’Œéµå¾ªèƒ½åŠ›ã€‚PrefEvalåŒ…å«æ¶µç›–20ä¸ªè¯é¢˜çš„3000ç»„äººå·¥ç­›é€‰çš„ç”¨æˆ·åå¥½å’ŒæŸ¥è¯¢é…å¯¹ï¼Œæ¶µç›–æ˜¾å¼ä¸éšå¼å½¢å¼çš„ç”¨æˆ·ä¸ªæ€§åŒ–æˆ–åå¥½ä¿¡æ¯ï¼Œå¹¶é€šè¿‡ç”Ÿæˆä¸åˆ†ç±»ä»»åŠ¡æ¥è¯„ä¼°LLMçš„è¡¨ç°ã€‚é€šè¿‡PrefEvalåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¯„ä¼°äº†10æ¬¾å¼€æºå’Œä¸“æœ‰LLMçš„åå¥½è·Ÿéšèƒ½åŠ›ï¼Œæ¶‰åŠå¤šä¼šè¯å¯¹è¯ï¼Œä¸Šä¸‹æ–‡é•¿åº¦å¯è¾¾10ä¸‡ä»¤ç‰Œã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯¹è¯ä¸­ä¸»åŠ¨éµå¾ªç”¨æˆ·åå¥½æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸­ï¼Œå¤§å¤šæ•°æ¨¡å‹çš„åå¥½éµå¾ªå‡†ç¡®æ€§åœ¨ä»…10è½®å¯¹è¯ï¼ˆçº¦3åƒä»¤ç‰Œï¼‰æ—¶å°±ä½äº10%ã€‚å³ä½¿ä½¿ç”¨é«˜çº§æç¤ºå’Œæ£€ç´¢æ–¹æ³•ï¼Œåœ¨é•¿æœŸä¸Šä¸‹æ–‡å¯¹è¯ä¸­ï¼Œåå¥½çš„éµå¾ªä»ç„¶ä¼šæ¶åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜åœ¨PrefEvalä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡PrefEvalå¯¹äºè¡¡é‡ã€ç†è§£å’Œæé«˜LLMçš„åå¥½è·Ÿéšèƒ½åŠ›å…·æœ‰å®è´µä»·å€¼ï¼Œä¸ºä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ä½œä¸ºèŠå¤©æœºå™¨äººåº”ç”¨æ—¶ï¼Œå¯¹ç”¨æˆ·åå¥½çš„ä¸ªæ€§åŒ–å“åº”èƒ½åŠ›å­˜åœ¨å±€é™ã€‚</li>
<li>PrefEvalåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°LLMåœ¨å…·æœ‰é•¿æœŸä¸Šä¸‹æ–‡å¯¹è¯ç¯å¢ƒä¸­å¯¹ç”¨æˆ·åå¥½çš„æ¨æ–­ã€è®°å¿†å’Œéµå¾ªèƒ½åŠ›ã€‚</li>
<li>PrefEvalåŒ…å«æ¶µç›–å¤šç§è¯é¢˜çš„ç”¨æˆ·åå¥½å’ŒæŸ¥è¯¢é…å¯¹ã€‚</li>
<li>LLMé¢ä¸´åœ¨å¯¹è¯ä¸­ä¸»åŠ¨éµå¾ªç”¨æˆ·åå¥½çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸‹ã€‚</li>
<li>å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿æœŸä¸Šä¸‹æ–‡å¯¹è¯ä¸­çš„åå¥½éµå¾ªèƒ½åŠ›ä»ç„¶ä¸è¶³ã€‚</li>
<li>åœ¨PrefEvalä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜LLMçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6432637e0c80e6c687b5fb58c6b788e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1f5804ef09fd718a68e44e19cf28165.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a70cfd667fcfd3367c28b22e832319c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e89e14a57b488927e4dbb6b8e5419750.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0f2f0a27ac5015ac4a6fcf8555a957f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Optimizing-GPT-for-Video-Understanding-Zero-Shot-Performance-and-Prompt-Engineering"><a href="#Optimizing-GPT-for-Video-Understanding-Zero-Shot-Performance-and-Prompt-Engineering" class="headerlink" title="Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt   Engineering"></a>Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt   Engineering</h2><p><strong>Authors:Mark Beliaev, Victor Yang, Madhura Raju, Jiachen Sun, Xinghai Hu</strong></p>
<p>In this study, we tackle industry challenges in video content classification by exploring and optimizing GPT-based models for zero-shot classification across seven critical categories of video quality. We contribute a novel approach to improving GPTâ€™s performance through prompt optimization and policy refinement, demonstrating that simplifying complex policies significantly reduces false negatives. Additionally, we introduce a new decomposition-aggregation-based prompt engineering technique, which outperforms traditional single-prompt methods. These experiments, conducted on real industry problems, show that thoughtful prompt design can substantially enhance GPTâ€™s performance without additional finetuning, offering an effective and scalable solution for improving video classification systems across various domains in industry. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹GPTæ¨¡å‹çš„æ¢ç´¢å’Œä¼˜åŒ–ï¼Œé’ˆå¯¹è§†é¢‘è´¨é‡çš„ä¸ƒå¤§å…³é”®ç±»åˆ«ï¼Œè¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»æ¥è§£å†³è§†é¢‘å†…å®¹åˆ†ç±»è¡Œä¸šçš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›GPTæ€§èƒ½çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æç¤ºä¼˜åŒ–å’Œæ”¿ç­–å®Œå–„ï¼Œè¯æ˜äº†ç®€åŒ–å¤æ‚æ”¿ç­–å¯ä»¥æ˜¾è‘—é™ä½å‡é˜´æ€§ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºåˆ†è§£èšåˆçš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œå®ƒä¼˜äºä¼ ç»Ÿçš„å•ä¸€æç¤ºæ–¹æ³•ã€‚è¿™äº›é’ˆå¯¹çœŸå®è¡Œä¸šé—®é¢˜è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæœ‰ç­–ç•¥æ€§çš„æç¤ºè®¾è®¡å¯ä»¥åœ¨ä¸å¢åŠ å¾®è°ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜GPTçš„æ€§èƒ½ï¼Œä¸ºå·¥ä¸šé¢†åŸŸå„ç§åŸŸçš„è§†é¢‘åˆ†ç±»ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09573v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶é€šè¿‡æ¢ç´¢å’Œä¼˜åŒ–GPTæ¨¡å‹è§£å†³è§†é¢‘å†…å®¹åˆ†ç±»è¡Œä¸šä¸­çš„æŒ‘æˆ˜ï¼Œå®ç°é›¶æ ·æœ¬åˆ†ç±»è§†é¢‘è´¨é‡çš„ä¸ƒä¸ªå…³é”®ç±»åˆ«ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§é€šè¿‡ä¼˜åŒ–æç¤ºå’Œç²¾ç‚¼ç­–ç•¥æ¥æå‡GPTæ€§èƒ½çš„æ–°æ–¹æ³•ï¼Œè¯æ˜ç®€åŒ–å¤æ‚ç­–ç•¥èƒ½æ˜¾è‘—é™ä½è¯¯æŠ¥ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºåˆ†è§£èšåˆçš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä¼˜äºä¼ ç»Ÿå•ä¸€æç¤ºæ–¹æ³•ã€‚é’ˆå¯¹å®é™…è¡Œä¸šé—®é¢˜çš„å®éªŒè¡¨æ˜ï¼Œç²¾å¿ƒè®¾è®¡çš„æç¤ºèƒ½æ˜¾è‘—æé«˜GPTæ€§èƒ½ï¼Œæ— éœ€é¢å¤–å¾®è°ƒï¼Œä¸ºæ”¹è¿›å„è¡Œä¸šè§†é¢‘åˆ†ç±»ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶è§£å†³äº†è§†é¢‘å†…å®¹åˆ†ç±»è¡Œä¸šçš„æŒ‘æˆ˜ï¼Œé€šè¿‡GPTæ¨¡å‹å®ç°é›¶æ ·æœ¬åˆ†ç±»ä¸ƒä¸ªå…³é”®è§†é¢‘è´¨é‡ç±»åˆ«ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¼˜åŒ–GPTæ€§èƒ½çš„æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼˜åŒ–æç¤ºå’Œç²¾ç‚¼ç­–ç•¥ã€‚</li>
<li>ç ”ç©¶å‘ç°ç®€åŒ–å¤æ‚ç­–ç•¥èƒ½æ˜¾è‘—é™ä½è¯¯æŠ¥ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºåˆ†è§£èšåˆçš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿå•ä¸€æç¤ºæ–¹æ³•ã€‚</li>
<li>å®éªŒè¡¨æ˜ç²¾å¿ƒè®¾è®¡çš„æç¤ºèƒ½æ˜¾è‘—æé«˜GPTæ€§èƒ½ï¼Œæ— éœ€é¢å¤–å¾®è°ƒã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ”¹è¿›å„è¡Œä¸šè§†é¢‘åˆ†ç±»ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0029957fbed760d7788a06dd6e9754f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4826bc936452cbc042c18d4c4f2c465a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e7296e0b0e44ebf97c3378344f08b98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25a7712fcd5f41607811c0477511e57c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MDCrow-Automating-Molecular-Dynamics-Workflows-with-Large-Language-Models"><a href="#MDCrow-Automating-Molecular-Dynamics-Workflows-with-Large-Language-Models" class="headerlink" title="MDCrow: Automating Molecular Dynamics Workflows with Large Language   Models"></a>MDCrow: Automating Molecular Dynamics Workflows with Large Language   Models</h2><p><strong>Authors:Quintina Campbell, Sam Cox, Jorge Medina, Brittany Watterson, Andrew D. White</strong></p>
<p>Molecular dynamics (MD) simulations are essential for understanding biomolecular systems but remain challenging to automate. Recent advances in large language models (LLM) have demonstrated success in automating complex scientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an agentic LLM assistant capable of automating MD workflows. MDCrow uses chain-of-thought over 40 expert-designed tools for handling and processing files, setting up simulations, analyzing the simulation outputs, and retrieving relevant information from literature and databases. We assess MDCrowâ€™s performance across 25 tasks of varying required subtasks and difficulty, and we evaluate the agentâ€™s robustness to both difficulty and prompt style. \texttt{gpt-4o} is able to complete complex tasks with low variance, followed closely by \texttt{llama3-405b}, a compelling open-source model. While prompt style does not influence the best modelsâ€™ performance, it has significant effects on smaller models. </p>
<blockquote>
<p>åˆ†å­åŠ¨åŠ›å­¦ï¼ˆMDï¼‰æ¨¡æ‹Ÿå¯¹äºç†è§£ç”Ÿç‰©åˆ†å­ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œä½†è‡ªåŠ¨åŒ–ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥è¡¨æ˜ï¼Œä½¿ç”¨åŸºäºLLMçš„ä»£ç†å¯ä»¥è‡ªåŠ¨æ‰§è¡Œå¤æ‚çš„ç§‘å­¦ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MDCrowï¼Œè¿™æ˜¯ä¸€æ¬¾èƒ½å¤Ÿè‡ªåŠ¨åŒ–MDå·¥ä½œæµç¨‹çš„LLMåŠ©ç†ã€‚MDCrowä½¿ç”¨æ€æƒ³é“¾ï¼Œç»“åˆäº†40ç§ä¸“å®¶è®¾è®¡çš„å·¥å…·ï¼Œç”¨äºå¤„ç†å’ŒåŠ å·¥æ–‡ä»¶ã€è®¾ç½®æ¨¡æ‹Ÿã€åˆ†ææ¨¡æ‹Ÿè¾“å‡ºä»¥åŠä»æ–‡çŒ®å’Œæ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚æˆ‘ä»¬è¯„ä¼°äº†MDCrowåœ¨25ä¸ªä¸åŒå­ä»»åŠ¡å’Œéš¾åº¦çš„ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶è¯„ä¼°äº†è¯¥ä»£ç†å¯¹éš¾åº¦å’Œæç¤ºé£æ ¼çš„ç¨³å¥æ€§ã€‚<code>gpt-4o</code>èƒ½å¤Ÿä»¥ä½æ–¹å·®å®Œæˆå¤æ‚ä»»åŠ¡ï¼Œç´§éšå…¶åçš„æ˜¯<code>llama3-405b</code>ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼•äººæ³¨ç›®çš„å¼€æºæ¨¡å‹ã€‚è™½ç„¶æç¤ºé£æ ¼ä¸ä¼šå½±å“æœ€ä½³æ¨¡å‹çš„è¡¨ç°ï¼Œä½†å®ƒå¯¹è¾ƒå°æ¨¡å‹çš„å½±å“æ˜¯æ˜¾è‘—çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09565v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MDæ¨¡æ‹Ÿè‡ªåŠ¨åŒ–æŒ‘æˆ˜é‡é‡ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºå…¶å¸¦æ¥çªç ´ã€‚æœ¬æ–‡ä»‹ç»MDCrowï¼Œä¸€æ¬¾åŸºäºLLMçš„è‡ªåŠ¨åŒ–MDå·¥ä½œæµç¨‹åŠ©ç†ã€‚MDCrowåˆ©ç”¨æ€ç»´é“¾ï¼Œé€šè¿‡å››åå¤šç§ä¸“ä¸šå·¥å…·å¤„ç†æ–‡ä»¶ã€è®¾ç½®æ¨¡æ‹Ÿã€åˆ†ææ¨¡æ‹Ÿè¾“å‡ºå¹¶ä»æ–‡çŒ®å’Œæ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚å¯¹MDCrowåœ¨25é¡¹ä¸åŒéš¾åº¦ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¿›è¡Œè¯„ä¼°ï¼Œè¯„ä¼°æ¨¡å‹å¯¹éš¾åº¦å’Œæç¤ºé£æ ¼çš„ç¨³å¥æ€§ã€‚å…¶ä¸­ï¼ŒGPT-4oè¡¨ç°å‡ºè‰²ï¼Œå®Œæˆå¤æ‚ä»»åŠ¡æ–¹å·®è¾ƒä½ï¼Œå…¶æ¬¡æ˜¯å¼€æºæ¨¡å‹llama3-405bã€‚æç¤ºé£æ ¼å¯¹æœ€ä½³æ¨¡å‹æ€§èƒ½å½±å“ä¸å¤§ï¼Œä½†å¯¹å°å‹æ¨¡å‹å½±å“æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MDCrowæ˜¯åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–MDå·¥ä½œæµç¨‹åŠ©ç†ï¼Œç»“åˆäº†ä¼—å¤šä¸“ä¸šå·¥å…·ä»¥å®Œæˆæ¨¡æ‹Ÿè¿‡ç¨‹ã€‚</li>
<li>MDCrowå¯ä»¥åœ¨æ–‡ä»¶å¤„ç†ã€æ¨¡æ‹Ÿè®¾ç½®ã€ç»“æœåˆ†æå’Œæ–‡çŒ®æ£€ç´¢ç­‰å¤šä¸ªç¯èŠ‚å‘æŒ¥ä½œç”¨ã€‚</li>
<li>GPT-4oåœ¨è‡ªåŠ¨åŒ–MDä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºå¯¹å¤æ‚ä»»åŠ¡çš„ç¨³å¥æ€§ã€‚</li>
<li>å¼€æºæ¨¡å‹llama3-405båŒæ ·å…·å¤‡å¼ºå¤§çš„è‡ªåŠ¨åŒ–èƒ½åŠ›ã€‚</li>
<li>MDCrowçš„è¯„ä¼°è·¨è¶Šäº†å¤šä¸ªä»»åŠ¡çº§åˆ«å’Œéš¾åº¦çº§åˆ«ï¼Œä»¥å…¨é¢äº†è§£å…¶æ€§èƒ½èŒƒå›´ã€‚</li>
<li>æç¤ºé£æ ¼å¯¹å°å‹æ¨¡å‹çš„æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œä½†å¯¹æœ€ä½³æ¨¡å‹çš„æ€§èƒ½å½±å“ä¸å¤§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a6645c51424934d8db384396887c60b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1bf1f673a9474fa7aa5be924483b63cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e48dc1019282ababc3f2d29478b7fbbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acf0731d33c2698d7232ffee7afe2289.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EmbodiedBench-Comprehensive-Benchmarking-Multi-modal-Large-Language-Models-for-Vision-Driven-Embodied-Agents"><a href="#EmbodiedBench-Comprehensive-Benchmarking-Multi-modal-Large-Language-Models-for-Vision-Driven-Embodied-Agents" class="headerlink" title="EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language   Models for Vision-Driven Embodied Agents"></a>EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language   Models for Vision-Driven Embodied Agents</h2><p><strong>Authors:Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang</strong></p>
<p>Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at <a target="_blank" rel="noopener" href="https://embodiedbench.github.io/">https://embodiedbench.github.io</a>. </p>
<blockquote>
<p>åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åˆ›å»ºå®ä½“ä»£ç†ä¸ºè§£å†³ç°å®ä¸–ç•Œä»»åŠ¡æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„é€”å¾„ã€‚è™½ç„¶ä»¥è¯­è¨€ä¸ºä¸­å¿ƒçš„å®ä½“ä»£ç†å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä½†ç”±äºç¼ºä¹å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŸºäºMLLMçš„å®ä½“ä»£ç†ä»ç„¶è¢«æ¢ç´¢å¾—å¾ˆå°‘ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†EmbodiedBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°è§†è§‰é©±åŠ¨çš„å®ä½“ä»£ç†çš„å¹¿æ³›åŸºå‡†æµ‹è¯•ã€‚EmbodiedBenchçš„ç‰¹ç‚¹åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åœ¨å››ä¸ªç¯å¢ƒä¸­åŒ…å«1128ä¸ªæµ‹è¯•ä»»åŠ¡é›†ï¼ŒèŒƒå›´ä»é«˜çº§è¯­ä¹‰ä»»åŠ¡ï¼ˆä¾‹å¦‚å®¶åŠ¡ï¼‰åˆ°æ¶‰åŠåŸå­åŠ¨ä½œçš„ä½çº§ä»»åŠ¡ï¼ˆä¾‹å¦‚å¯¼èˆªå’Œæ“ä½œï¼‰ï¼›ï¼ˆ2ï¼‰å…­ä¸ªç²¾å¿ƒç­–åˆ’çš„å­é›†è¯„ä¼°äº†å…³é”®ä»£ç†èƒ½åŠ›ï¼Œå¦‚å¸¸è¯†æ¨ç†ã€å¤æ‚æŒ‡ä»¤ç†è§£ã€ç©ºé—´æ„ŸçŸ¥ã€è§†è§‰æ„ŸçŸ¥å’Œé•¿æœŸè§„åˆ’ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬åœ¨EmbodiedBenchä¸­å¯¹13ä¸ªé¢†å…ˆçš„ä¸“ä¸šå’Œå¼€æºMLLMè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°ï¼šMLLMåœ¨é«˜çº§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä½çº§æ“ä½œä»»åŠ¡ä¸Šé‡åˆ°å›°éš¾ï¼Œæœ€ä½³æ¨¡å‹GPT-4oçš„å¹³å‡å¾—åˆ†ä»…ä¸º28.9%ã€‚EmbodiedBenchæä¾›äº†ä¸€ä¸ªå¤šæ–¹é¢çš„æ ‡å‡†åŒ–è¯„ä¼°å¹³å°ï¼Œå®ƒä¸ä»…çªå‡ºäº†ç°æœ‰æŒ‘æˆ˜ï¼Œè€Œä¸”ä¸ºæ¨è¿›åŸºäºMLLMçš„å®ä½“ä»£ç†æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://embodiedbench.github.ioè·å–./">https://embodiedbench.github.ioè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09560v1">PDF</a> 51 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å®ä½“ä»£ç†åœ¨è§£å†³ç°å®ä¸–ç•Œä»»åŠ¡æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼ŒMLLMå®ä½“ä»£ç†çš„ç ”ç©¶ä»å¤„äºèµ·æ­¥é˜¶æ®µã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EmbodiedBenchè¯„ä¼°å¹³å°ï¼Œç”¨äºè¯„ä¼°è§†è§‰é©±åŠ¨çš„å®ä½“ä»£ç†ã€‚EmbodiedBenchçš„ç‰¹ç‚¹æ˜¯ï¼šï¼ˆ1ï¼‰åœ¨å››ä¸ªç¯å¢ƒä¸­è®¾æœ‰æ¶µç›–èŒƒå›´å¹¿æ³›çš„1,128é¡¹æµ‹è¯•ä»»åŠ¡ï¼ŒåŒ…æ‹¬é«˜çº§è¯­ä¹‰ä»»åŠ¡ï¼ˆå¦‚å®¶åŠ¡ï¼‰å’Œä½çº§åŸå­åŠ¨ä½œä»»åŠ¡ï¼ˆå¦‚å¯¼èˆªå’Œæ“ä½œï¼‰ï¼›ï¼ˆ2ï¼‰ç²¾å¿ƒç­–åˆ’çš„å…­ä¸ªå­é›†è¯„ä¼°äº†ä»£ç†çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå¦‚å¸¸è¯†æ¨ç†ã€å¤æ‚æŒ‡ä»¤ç†è§£ã€ç©ºé—´æ„ŸçŸ¥ã€è§†è§‰æ„ŸçŸ¥å’Œé•¿æœŸè§„åˆ’ç­‰ã€‚é€šè¿‡å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å¯¹EmbodiedBenchä¸­çš„13æ¬¾ä¸»æµå’Œå¼€æºçš„MLLMè¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼šMLLMæ“…é•¿é«˜çº§ä»»åŠ¡ä½†éš¾ä»¥å¤„ç†ä½çº§æ“ä½œä»»åŠ¡ï¼Œå…¶ä¸­è¡¨ç°æœ€å¥½çš„æ¨¡å‹GPT-4oåœ¨æ“ä½œä»»åŠ¡ä¸Šçš„å¹³å‡å¾—åˆ†ä»…ä¸º28.9%ã€‚EmbodiedBenchæä¾›äº†ä¸€ä¸ªå¤šæ–¹é¢çš„æ ‡å‡†åŒ–è¯„ä¼°å¹³å°ï¼Œä¸ä»…çªå‡ºäº†ç°æœ‰æŒ‘æˆ˜ï¼Œä¹Ÿä¸ºæ¨åŠ¨MLLMå®ä½“ä»£ç†çš„å‘å±•æä¾›äº†å®è´µè§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://embodiedbench.github.ioä¸Šæ‰¾åˆ°./">https://embodiedbench.github.ioä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ„å»ºå®ä½“ä»£ç†ä¸ºåº”å¯¹ç°å®ä»»åŠ¡æä¾›æœ‰åŠ›é€”å¾„ã€‚<br>äºŒã€ç›®å‰MLLMå®ä½“ä»£ç†ç ”ç©¶å—é™äºç¼ºä¹å…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚<br>ä¸‰ã€EmbodiedBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰é©±åŠ¨å®ä½“ä»£ç†çš„è¯„ä¼°å¹³å°ã€‚<br>å››ã€EmbodiedBenchåŒ…å«å¤šæ ·åŒ–çš„æµ‹è¯•ä»»åŠ¡ï¼Œæ¶µç›–é«˜çº§è¯­ä¹‰ä»»åŠ¡å’Œä½çº§åŸå­åŠ¨ä½œä»»åŠ¡ã€‚<br>äº”ã€å…­ä¸ªç²¾å¿ƒç­–åˆ’çš„å­é›†è¯„ä¼°å®ä½“ä»£ç†çš„æ ¸å¿ƒèƒ½åŠ›ã€‚<br>å…­ã€ç ”ç©¶ç»“æœæ˜¾ç¤ºMLLMåœ¨å¤„ç†ä½çº§æ“ä½œä»»åŠ¡ä¸Šå­˜åœ¨å›°éš¾ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d57090bef305b412735fdafcd71fd49e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-913f35ebb24e35fab76c648fda33a040.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-30cff5b4a901e5f17ee5c4d9476b7e24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-322563172c0525fb5e619fd85698ac24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb248431a1e104fb82cf7ead3db3145e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dcb7c671eec9c41cfdabe24c193f983.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Explainable-AI-assisted-Optimization-for-Feynman-Integral-Reduction"><a href="#Explainable-AI-assisted-Optimization-for-Feynman-Integral-Reduction" class="headerlink" title="Explainable AI-assisted Optimization for Feynman Integral Reduction"></a>Explainable AI-assisted Optimization for Feynman Integral Reduction</h2><p><strong>Authors:Zhuo-Yang Song, Tong-Zhi Yang, Qing-Hong Cao, Ming-xing Luo, Hua Xing Zhu</strong></p>
<p>We present a novel approach to optimizing the reduction of Feynman integrals using integration-by-parts identities. By developing a priority function through the FunSearch algorithm, which combines large language models and genetic algorithms, we achieve significant improvements in memory usage and computational efficiency compared to traditional methods. Our approach demonstrates substantial reductions in the required seeding integrals, making previously intractable integrals more manageable. Tested on a variety of Feynman integrals, including one-loop and multi-loop cases with planar and non-planar configurations, our method demonstrates remarkable scalability and adaptability. For reductions of certain Feynman integrals with many dots and numerators, we observed an improvement by a factor of 3058 compared to traditional methods. This work provides a powerful and interpretable framework for optimizing IBP reductions, paving the way for more efficient and practical calculations in high-energy physics. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨åˆ†éƒ¨ç§¯åˆ†æ’ç­‰å¼ä¼˜åŒ–è´¹æ›¼ç§¯åˆ†çš„æ–°æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨FunSearchç®—æ³•å¼€å‘ä¼˜å…ˆçº§å‡½æ•°ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œé—ä¼ ç®—æ³•ï¼Œåœ¨å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¤§å¹…åº¦å‡å°‘äº†æ‰€éœ€çš„ç§å­ç§¯åˆ†ï¼Œä½¿ä»¥å‰æ— æ³•å¤„ç†çš„ç§¯åˆ†æ›´å®¹æ˜“ç®¡ç†ã€‚æˆ‘ä»¬åœ¨å„ç§ç±»å‹çš„è´¹æ›¼ç§¯åˆ†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼ŒåŒ…æ‹¬å…·æœ‰å¹³é¢å’Œéå¹³é¢é…ç½®çš„å•å¾ªç¯å’Œå¤šå¾ªç¯æƒ…å†µï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºæƒŠäººçš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚å¯¹äºå…·æœ‰è®¸å¤šç‚¹å’Œåˆ†å­çš„æŸäº›è´¹æ›¼ç§¯åˆ†çš„åŒ–ç®€ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”æé«˜äº†3058å€ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªå¼ºå¤§ä¸”å¯è§£é‡Šçš„ä¼˜åŒ–IBPåŒ–ç®€æ¡†æ¶ï¼Œä¸ºé«˜èƒ½ç‰©ç†çš„æ›´é«˜æ•ˆå’Œå®ç”¨çš„è®¡ç®—é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09544v1">PDF</a> 14 pages, 4 figures</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºFunSearchç®—æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°é¢–æ–¹æ³•ï¼Œç”¨äºä¼˜åŒ–è´¹æ›¼ç§¯åˆ†çš„ç¼©å‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼€å‘ä¼˜å…ˆçº§å‡½æ•°ï¼Œåœ¨å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å®ç°äº†å¯¹ä¼ ç»Ÿæ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ã€‚æ–°æ–¹æ³•åœ¨å¹³é¢å’Œéå¹³é¢é…ç½®çš„å•å›è·¯å’Œå¤šå›è·¯æƒ…å†µä¸‹æµ‹è¯•çš„è´¹æ›¼ç§¯åˆ†ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚å¯¹äºå…·æœ‰è®¸å¤šç‚¹å’Œåˆ†å­çš„æŸäº›è´¹æ›¼ç§¯åˆ†ï¼Œä¸ä¼ ç»Ÿçš„ç¼©å‡æ–¹æ³•ç›¸æ¯”ï¼Œè§‚å¯Ÿåˆ°æ•ˆç‡æé«˜äº†é«˜è¾¾ä¸‰åƒå¤šå€ã€‚è¯¥ç ”ç©¶ä¸ºä¼˜åŒ–IBPç¼©å‡æä¾›äº†å¼ºå¤§è€Œå¯è§£é‡Šæ€§çš„æ¡†æ¶ï¼Œä¸ºé«˜èƒ½ç‰©ç†ä¸­çš„æ›´å®ç”¨å’Œé«˜æ•ˆçš„è®¡ç®—é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†æ–°çš„æ–¹æ³•æ¥ä¼˜åŒ–è´¹æ›¼ç§¯åˆ†çš„å‡å°‘ï¼Œä½¿ç”¨FunSearchç®—æ³•ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¼€å‘ä¼˜å…ˆçº§å‡½æ•°ï¼Œæ˜¾è‘—æ”¹è¿›äº†å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§è´¹æ›¼ç§¯åˆ†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ï¼ŒåŒ…æ‹¬å•å›è·¯å’Œå¤šå›è·¯æƒ…å†µï¼Œä»¥åŠå¹³é¢å’Œéå¹³é¢é…ç½®ã€‚</li>
<li>å¯¹äºå…·æœ‰å¤æ‚ç»“æ„å’Œä¼—å¤šåˆ†å­å’Œç‚¹çš„è´¹æ›¼ç§¯åˆ†ï¼Œæ–°æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•çš„æ•ˆç‡æå‡æ˜¾è‘—ï¼Œè¾¾åˆ°ä¸‰åƒå¤šå€ã€‚</li>
<li>æ­¤æ–¹æ³•æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶æ¥ä¼˜åŒ–IBPç¼©å‡ã€‚</li>
<li>æ­¤æ¡†æ¶å¯èƒ½æ¨åŠ¨é«˜èƒ½ç‰©ç†ä¸­æ›´é«˜æ•ˆå’Œå®é™…çš„è®¡ç®—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d0f08a491ab40a254d3d06446fd1630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e4a0943219d96d7195194c48d7cfeaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1495b10353d9927b650fca883cf0032f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b2cf76ae0943049490a9f91c6541638.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Improve-LLM-based-Automatic-Essay-Scoring-with-Linguistic-Features"><a href="#Improve-LLM-based-Automatic-Essay-Scoring-with-Linguistic-Features" class="headerlink" title="Improve LLM-based Automatic Essay Scoring with Linguistic Features"></a>Improve LLM-based Automatic Essay Scoring with Linguistic Features</h2><p><strong>Authors:Zhaoyi Joey Hou, Alejandro Ciuba, Xiang Lorraine Li</strong></p>
<p>Automatic Essay Scoring (AES) assigns scores to student essays, reducing the grading workload for instructors. Developing a scoring system capable of handling essays across diverse prompts is challenging due to the flexibility and diverse nature of the writing task. Existing methods typically fall into two categories: supervised feature-based approaches and large language model (LLM)-based methods. Supervised feature-based approaches often achieve higher performance but require resource-intensive training. In contrast, LLM-based methods are computationally efficient during inference but tend to suffer from lower performance. This paper combines these approaches by incorporating linguistic features into LLM-based scoring. Experimental results show that this hybrid method outperforms baseline models for both in-domain and out-of-domain writing prompts. </p>
<blockquote>
<p>è‡ªåŠ¨ä½œæ–‡è¯„åˆ†ï¼ˆAESï¼‰ä¸ºå­¦ç”Ÿä½œæ–‡æ‰“åˆ†ï¼Œå‡è½»æ•™å¸ˆè¯„åˆ†çš„å·¥ä½œé‡ã€‚å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†ä¸åŒæç¤ºçš„ä½œæ–‡çš„è¯„åˆ†ç³»ç»Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå†™ä½œä»»åŠ¡å…·æœ‰çµæ´»æ€§å’Œå¤šæ ·æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºç›‘ç£ç‰¹å¾çš„æ–¹æ³•å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ã€‚åŸºäºç›‘ç£ç‰¹å¾çš„æ–¹æ³•é€šå¸¸æ€§èƒ½è¾ƒé«˜ï¼Œä½†éœ€è¦å¯†é›†çš„èµ„æºè¿›è¡Œè®­ç»ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºLLMçš„æ–¹æ³•åœ¨æ¨ç†è¿‡ç¨‹ä¸­è®¡ç®—æ•ˆç‡é«˜ï¼Œä½†æ€§èƒ½å¾€å¾€è¾ƒä½ã€‚æœ¬æ–‡ç»“åˆäº†è¿™ä¸¤ç§æ–¹æ³•ï¼Œé€šè¿‡å°†è¯­è¨€ç‰¹å¾çº³å…¥åŸºäºLLMçš„è¯„åˆ†ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ··åˆæ–¹æ³•å¯¹äºé¢†åŸŸå†…å¤–ï¼ˆin-domain and out-of-domainï¼‰çš„å†™ä½œæç¤ºéƒ½ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09497v1">PDF</a> To be published in the workshop Innovation and Responsibility in   AI-Supported Education (iRaise) at the 2025 Conference on Artificial   Intelligence (AAAI)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨ä½œæ–‡è¯„åˆ†ç³»ç»Ÿï¼ˆAESï¼‰ä¸ºå­¦ç”Ÿä½œæ–‡æ‰“åˆ†ï¼Œå‡è½»æ•™å¸ˆè¯„åˆ†å·¥ä½œé‡çš„åŠŸèƒ½ã€‚æ–‡ç« æŒ‡å‡ºå¼€å‘ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†ä¸åŒä¸»é¢˜çš„è‡ªåŠ¨ä½œæ–‡è¯„åˆ†ç³»ç»Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºç‰¹å¾çš„ç›‘ç£æ–¹æ³•å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ã€‚åŸºäºç‰¹å¾çš„ç›‘ç£æ–¹æ³•æ€§èƒ½è¾ƒé«˜ä½†éœ€è¦å¯†é›†çš„èµ„æºè®­ç»ƒï¼Œè€ŒLLMæ–¹æ³•è™½ç„¶è®¡ç®—æ•ˆç‡é«˜ä½†æ€§èƒ½è¾ƒä½ã€‚æœ¬æ–‡ç»“åˆäº†è¿™ä¸¤ç§æ–¹æ³•ï¼Œå°†è¯­è¨€ç‰¹å¾çº³å…¥LLMè¯„åˆ†ç³»ç»Ÿä¸­ï¼Œå®éªŒç»“æœè¡¨æ˜è¿™ç§æ··åˆæ–¹æ³•åœ¨ä¸»é¢˜å†…å¤–éƒ½ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨ä½œæ–‡è¯„åˆ†ç³»ç»Ÿï¼ˆAESï¼‰å¯ä»¥ä¸ºå­¦ç”Ÿä½œæ–‡æ‰“åˆ†ï¼Œå‡è½»æ•™å¸ˆå·¥ä½œé‡ã€‚</li>
<li>å¼€å‘è‡ªåŠ¨ä½œæ–‡è¯„åˆ†ç³»ç»Ÿé¢ä¸´å¤„ç†ä¸åŒä¸»é¢˜çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦åŒ…æ‹¬åŸºäºç‰¹å¾çš„ç›‘ç£æ–¹æ³•å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ã€‚</li>
<li>åŸºäºç‰¹å¾çš„ç›‘ç£æ–¹æ³•æ€§èƒ½è¾ƒé«˜ä½†éœ€è¦å¯†é›†çš„èµ„æºè®­ç»ƒã€‚</li>
<li>LLMæ–¹æ³•è®¡ç®—æ•ˆç‡é«˜ä½†æ€§èƒ½è¾ƒä½ã€‚</li>
<li>ç»“åˆè¯­è¨€ç‰¹å¾çš„LLMè¯„åˆ†ç³»ç»Ÿï¼ˆæ··åˆæ–¹æ³•ï¼‰åœ¨ä¸»é¢˜å†…å¤–éƒ½è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d393db6d1f5cf3893e395e6ac4552017.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d96ae2b4e4cf0503a8b739d142a7194b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SQuARE-Sequential-Question-Answering-Reasoning-Engine-for-Enhanced-Chain-of-Thought-in-Large-Language-Models"><a href="#SQuARE-Sequential-Question-Answering-Reasoning-Engine-for-Enhanced-Chain-of-Thought-in-Large-Language-Models" class="headerlink" title="SQuARE: Sequential Question Answering Reasoning Engine for Enhanced   Chain-of-Thought in Large Language Models"></a>SQuARE: Sequential Question Answering Reasoning Engine for Enhanced   Chain-of-Thought in Large Language Models</h2><p><strong>Authors:Daniel Fleischer, Moshe Berchansky, Gad Markovits, Moshe Wasserblat</strong></p>
<p>In the rapidly evolving field of Natural Language Processing, Large Language Models (LLMs) are tasked with increasingly complex reasoning challenges. Traditional methods like chain-of-thought prompting have shown promise but often fall short in fully leveraging a modelâ€™s reasoning capabilities. This paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to improve reasoning through a self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts models to generate and resolve multiple auxiliary questions before tackling the main query, promoting a more thorough exploration of various aspects of a topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models across multiple question-answering datasets, demonstrate that SQuARE significantly surpasses traditional CoT prompts and existing rephrase-and-respond methods. By systematically decomposing queries, SQuARE advances LLM capabilities in reasoning tasks. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/IntelLabs/RAG-FiT/tree/square">https://github.com/IntelLabs/RAG-FiT/tree/square</a>. </p>
<blockquote>
<p>åœ¨å¿«é€Ÿå‘å±•çš„è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´ç€æ—¥ç›Šå¤æ‚çš„æ¨ç†æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„è¯¸å¦‚æ€ç»´é“¾æç¤ºæ³•ç­‰æ–¹æ³•è™½ç„¶æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œä½†å¾€å¾€æœªèƒ½å……åˆ†åˆ©ç”¨æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†SQuAREï¼ˆé¡ºåºé—®ç­”æ¨ç†å¼•æ“ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æç¤ºæŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡è‡ªæˆ‘è´¨é—®èŒƒå¼æé«˜æ¨ç†èƒ½åŠ›ã€‚SQuAREå»ºç«‹åœ¨CoTæ¡†æ¶ä¹‹ä¸Šï¼Œæç¤ºæ¨¡å‹åœ¨è§£å†³ä¸»è¦æŸ¥è¯¢ä¹‹å‰ç”Ÿæˆå¹¶è§£å†³å¤šä¸ªè¾…åŠ©é—®é¢˜ï¼Œä»è€Œä¿ƒè¿›å¯¹ä¸»é¢˜å„æ–¹é¢çš„æ›´å…¨é¢çš„æ¢ç´¢ã€‚æˆ‘ä»¬åœ¨Llama 3å’ŒGPT-4oæ¨¡å‹ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å¤šé—®ç­”æ•°æ®é›†è¯„ä¼°ï¼Œè¯æ˜äº†SQuAREæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿçš„CoTæç¤ºå’Œç°æœ‰çš„é‡è¿°å’Œå›åº”æ–¹æ³•ã€‚é€šè¿‡ç³»ç»Ÿåœ°åˆ†è§£æŸ¥è¯¢ï¼ŒSQuAREæé«˜äº†LLMåœ¨æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/IntelLabs/RAG-FiT/tree/square">https://github.com/IntelLabs/RAG-FiT/tree/square</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09390v1">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´æ—¥ç›Šå¤æ‚çš„æ¨ç†æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚é“¾å¼æ€ç»´æç¤ºæ³•è™½æœ‰æ½œåŠ›ï¼Œä½†å¸¸å¸¸ä¸èƒ½å……åˆ†åˆ©ç”¨æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æç¤ºæŠ€æœ¯â€”â€”SQuAREï¼ˆåºè´¯é—®ç­”æ¨ç†å¼•æ“ï¼‰ï¼Œå®ƒé‡‡ç”¨è‡ªé—®è‡ªç­”æ¨¡å¼æ”¹å–„æ¨ç†èƒ½åŠ›ã€‚å»ºç«‹åœ¨CoTæ¡†æ¶ä¹‹ä¸Šï¼ŒSQuAREé€šè¿‡ç”Ÿæˆå¹¶è§£å†³å¤šä¸ªè¾…åŠ©é—®é¢˜æ¥åº”å¯¹ä¸»è¦æŸ¥è¯¢ï¼Œä¿ƒè¿›å¯¹ä¸»é¢˜å„æ–¹é¢çš„å…¨é¢æ¢ç´¢ã€‚åœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¯¹Llama 3å’ŒGPT-4oæ¨¡å‹çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSQuAREæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»ŸCoTæç¤ºå’Œç°æœ‰çš„é‡è¿°ä¸å›åº”æ–¹æ³•ã€‚é€šè¿‡ç³»ç»Ÿåœ°åˆ†è§£æŸ¥è¯¢ï¼ŒSQuAREæ¨åŠ¨äº†LLMåœ¨æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸé¢ä¸´å¤æ‚çš„æ¨ç†æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¦‚é“¾å¼æ€ç»´æç¤ºæ³•è™½ç„¶æœ‰ä¸€å®šæ½œåŠ›ï¼Œä½†æ— æ³•å……åˆ†åˆ©ç”¨æ¨¡å‹çš„å…¨éƒ¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SQuAREæ˜¯ä¸€ç§æ–°å‹çš„æç¤ºæŠ€æœ¯ï¼Œé‡‡ç”¨è‡ªé—®è‡ªç­”æ¨¡å¼æ¥æé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SQuAREå»ºç«‹åœ¨CoTæ¡†æ¶ä¹‹ä¸Šï¼Œé€šè¿‡ç”Ÿæˆå¹¶è§£å†³å¤šä¸ªè¾…åŠ©é—®é¢˜æ¥åº”å¯¹ä¸»è¦æŸ¥è¯¢ã€‚</li>
<li>SQuAREèƒ½ç³»ç»Ÿåœ°åˆ†è§£æŸ¥è¯¢ï¼Œä¿ƒè¿›å¯¹ä¸»é¢˜å„æ–¹é¢çš„å…¨é¢æ¢ç´¢ã€‚</li>
<li>åœ¨å¤šä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒSQuAREæ˜¾è‘—ä¼˜äºä¼ ç»ŸCoTæç¤ºå’Œé‡è¿°ä¸å›åº”æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-91ec3e4ab6ee6645e7b22033afcd34e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cedc3a879896472d46736744bfb9a02e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4296715efd2e1768efc5d21baf9316d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de66d3b8859785158dd6eae157ab4907.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37df58b1796d6c136dfa68f1652b4291.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Truth-Knows-No-Language-Evaluating-Truthfulness-Beyond-English"><a href="#Truth-Knows-No-Language-Evaluating-Truthfulness-Beyond-English" class="headerlink" title="Truth Knows No Language: Evaluating Truthfulness Beyond English"></a>Truth Knows No Language: Evaluating Truthfulness Beyond English</h2><p><strong>Authors:Blanca Calvo Figueras, Eneko Sagarzazu, Julen Etxaniz, Jeremy Barnes, Pablo Gamallo, Iria De Dios Flores, Rodrigo Agerri</strong></p>
<p>We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been conducted in English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Dataset and code are publicly available under open licenses. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªä¸“ä¸šç¿»è¯‘çš„TruthfulQAåŸºå‡†æµ‹è¯•æ‰©å±•ç‰ˆï¼Œè¯¥ç‰ˆæœ¬æ—¨åœ¨è¯„ä¼°å·´æ–¯å…‹è¯­ã€åŠ æ³°ç½—å°¼äºšè¯­ã€åŠ åˆ©è¥¿äºšè¯­å’Œè¥¿ç­ç‰™è¯­çš„çœŸå®æ€§ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çœŸå®æ€§è¯„ä»·ä¸»è¦éƒ½æ˜¯åœ¨è‹±è¯­ç¯å¢ƒä¸­è¿›è¡Œçš„ã€‚ç„¶è€Œï¼ŒLLMåœ¨ä¸åŒè¯­è¨€ä¸­ä¿æŒçœŸå®æ€§çš„èƒ½åŠ›ä»å¾…æ¢ç´¢ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¯„ä¼°äº†12æ¬¾æœ€å…ˆè¿›çš„å¼€æºLLMï¼Œé€šè¿‡äººå·¥è¯„ä»·ã€å¤šé¡¹é€‰æ‹©æŒ‡æ ‡å’ŒLLM-as-a-Judgeè¯„åˆ†å¯¹æ¯”åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤ä¼˜åŒ–æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶LLMåœ¨è‹±è¯­ä¸­çš„è¡¨ç°æœ€å¥½ï¼Œåœ¨å·´æ–¯å…‹è¯­ï¼ˆèµ„æºæœ€å°‘çš„è¯­è¨€ï¼‰ä¸­çš„è¡¨ç°æœ€å·®ï¼Œä½†æ€»ä½“æ¥è¯´ï¼Œä¸åŒè¯­è¨€é—´çš„çœŸå®æ€§å·®å¼‚æ¯”é¢„æœŸçš„è¦å°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°LLM-as-a-Judgeä¸äººç±»åˆ¤æ–­çš„è”ç³»æ¯”å¤šé¡¹é€‰æ‹©æŒ‡æ ‡æ›´ä¸ºç´§å¯†ï¼Œä¿¡æ¯åœ¨çœŸå®æ€§è¯„ä¼°ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜è¡¨æ˜ï¼Œæœºå™¨ç¿»è¯‘æ˜¯æ‰©å±•çœŸå®æ€§åŸºå‡†æµ‹è¯•åˆ°æ›´å¤šè¯­è¨€çš„ä¸€ç§å¯è¡Œæ–¹æ³•ï¼Œä¸ºä¸“ä¸šç¿»è¯‘æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ€åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œé€šç”¨çŸ¥è¯†é—®é¢˜çš„è·¨è¯­è¨€å¤„ç†èƒ½åŠ›ä¼˜äºè¯­å¢ƒå’Œæ—¶é—´ä¾èµ–é—®é¢˜ï¼Œè¿™å¼ºè°ƒäº†åœ¨è¿›è¡ŒçœŸå®æ€§è¯„ä¼°æ—¶éœ€è¦è€ƒè™‘åˆ°æ–‡åŒ–å’Œæ—¶é—´å˜é‡çš„å¿…è¦æ€§ã€‚æ•°æ®é›†å’Œä»£ç å‡å…¬å¼€æä¾›å¼€æ”¾è®¸å¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09387v1">PDF</a> 13 pages, 5 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å·´æ–¯å…‹è¯­ã€åŠ æ³°ç½—å°¼äºšè¯­ã€åŠ åˆ©è¥¿äºšè¯­å’Œè¥¿ç­ç‰™è¯­çš„çœŸå®æ€§è¯„ä»·åŸºå‡†æµ‹è¯•çš„ä¸“ä¸šç¿»è¯‘æ‰©å±•ç‰ˆæœ¬ã€‚è¯¥ç ”ç©¶è¯„ä¼°äº†12ä¸ªæœ€å…ˆè¿›çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¯¹æ¯”äº†åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼Œä½¿ç”¨äººç±»è¯„ä¼°ã€å¤šé¡¹é€‰æ‹©æŒ‡æ ‡å’ŒLLM-as-a-Judgeè¯„åˆ†æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMåœ¨è‹±è¯­ä¸­è¡¨ç°æœ€å¥½ï¼Œåœ¨å·´æ–¯å…‹è¯­ï¼ˆèµ„æºæœ€å°‘ï¼‰ä¸­è¡¨ç°æœ€å·®ï¼Œä½†æ€»ä½“æ¥è¯´ï¼Œä¸åŒè¯­è¨€ä¹‹é—´çš„çœŸå®æ€§å·®å¼‚å°äºé¢„æœŸã€‚æ­¤å¤–ï¼ŒLLM-as-a-Judgeä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§æ›´é«˜ï¼Œä¿¡æ¯æ€§åœ¨çœŸå®æ€§è¯„ä¼°ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚æœºå™¨ç¿»è¯‘å¯ä½œä¸ºå°†çœŸå®æ€§åŸºå‡†æµ‹è¯•æ‰©å±•åˆ°å…¶ä»–è¯­è¨€çš„å¯è¡Œæ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£ä¸“ä¸šç¿»è¯‘çš„é€”å¾„ã€‚æœ€åï¼Œæ™®éçŸ¥è¯†çš„ç›¸å…³é—®é¢˜åœ¨ä¸åŒè¯­è¨€ä¸­çš„å¤„ç†æƒ…å†µæ¯”ä¸Šä¸‹æ–‡å’Œæ—¶é—´ç›¸å…³çš„é—®é¢˜æ›´å¥½ï¼Œè¿™å¼ºè°ƒäº†åœ¨è¿›è¡ŒçœŸå®æ€§è¯„ä¼°æ—¶éœ€è¦è€ƒè™‘åˆ°æ–‡åŒ–å’Œæ—¶é—´çš„å˜åŒ–æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä»‹ç»äº†é’ˆå¯¹å¤šç§è¯­è¨€ï¼ˆå·´æ–¯å…‹è¯­ã€åŠ æ³°ç½—å°¼äºšè¯­ã€åŠ åˆ©è¥¿äºšè¯­å’Œè¥¿ç­ç‰™è¯­ï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çœŸå®æ€§è¯„ä¼°ã€‚</li>
<li>å¯¹æ¯”äº†12ä¸ªå…ˆè¿›çš„LLMæ¨¡å‹åœ¨å¤šç§è¯­è¨€ä¸­çš„è¡¨ç°ã€‚</li>
<li>å‘ç°LLMåœ¨è‹±è¯­ä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨å·´æ–¯å…‹è¯­ä¸­è¡¨ç°æœ€å·®ï¼Œä½†ä¸åŒè¯­è¨€é—´çš„çœŸå®æ€§å·®å¼‚æ€»ä½“è¾ƒå°ã€‚</li>
<li>LLM-as-a-Judgeè¯„åˆ†æ–¹æ³•ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§æ›´é«˜ã€‚</li>
<li>ä¿¡æ¯æ€§å¯¹çœŸå®æ€§è¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>æœºå™¨ç¿»è¯‘å¯ä½œä¸ºå°†çœŸå®æ€§åŸºå‡†æµ‹è¯•æ‰©å±•åˆ°å…¶ä»–è¯­è¨€çš„å¯è¡Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0a8102d83365dba738d8f816955b8ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bec9ba764e893145bb3dd4bb97f53d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-574680dc2dfd3110fcb9e37d31bd0379.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18eb79a0c81aec18e254210b34553d01.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c33ebb75cd1286bd8eff5deba1e6161.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3db408ab1a6f2e3cd710b28c7f9a8f55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74ef642b38f3873366a496a898fdfbc7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c78597dbdf9c65e073e3ddb66cd58ba.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TransMLA-Multi-Head-Latent-Attention-Is-All-You-Need"><a href="#TransMLA-Multi-Head-Latent-Attention-Is-All-You-Need" class="headerlink" title="TransMLA: Multi-Head Latent Attention Is All You Need"></a>TransMLA: Multi-Head Latent Attention Is All You Need</h2><p><strong>Authors:Fanxu Meng, Zengwei Yao, Muhan Zhang</strong></p>
<p>Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2&#x2F;V3&#x2F;R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce TransMLA, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1. </p>
<blockquote>
<p>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¸¸åœ¨ç°æœ‰ç¡¬ä»¶ä¸Šé‡åˆ°é€šä¿¡ç“¶é¢ˆï¼Œè€Œä¸æ˜¯çº¯ç²¹çš„è®¡ç®—çº¦æŸã€‚å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰é€šè¿‡é”®å€¼ï¼ˆKVï¼‰å±‚ä½¿ç”¨ä½é˜¶çŸ©é˜µæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä»è€Œå…è®¸å‹ç¼©çš„æ½œåœ¨KVçŠ¶æ€è¢«ç¼“å­˜ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—å‡å°‘äº†ä¸ä¼ ç»Ÿçš„å¤šå¤´æ³¨æ„åŠ›ç›¸æ¯”çš„KVç¼“å­˜å¤§å°ï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦æ›´å¿«ã€‚æ­¤å¤–ï¼ŒMLAé‡‡ç”¨ä¸ŠæŠ•å½±çŸ©é˜µæ¥å¢åŠ è¡¨è¾¾åŠ›ï¼Œä»¥é¢å¤–çš„è®¡ç®—æ¢å–å‡å°‘çš„é€šä¿¡å¼€é”€ã€‚å°½ç®¡MLAå·²åœ¨Deepseek V2&#x2F;V3&#x2F;R1ä¸­å±•ç°äº†å…¶é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ï¼Œä½†è®¸å¤šä¸»è¦æ¨¡å‹æä¾›å•†ä»ç„¶ä¾èµ–ç¾¤ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰å¹¶ä¸”æ²¡æœ‰å®£å¸ƒé‡‡ç”¨MLAçš„è®¡åˆ’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºGQAæ€»æ˜¯å¯ä»¥ç”±MLAè¡¨ç¤ºï¼ŒåŒæ—¶ä¿æŒç›¸åŒçš„KVç¼“å­˜å¼€é”€ï¼Œä½†åä¹‹åˆ™ä¸æˆç«‹ã€‚ä¸ºäº†é¼“åŠ±æ›´å¹¿æ³›åœ°ä½¿ç”¨MLAï¼Œæˆ‘ä»¬å¼•å…¥äº†TransMLAï¼Œè¿™æ˜¯ä¸€ç§åè®­ç»ƒæ–¹æ³•ï¼Œå¯å°†å¹¿æ³›ä½¿ç”¨çš„åŸºäºGQAçš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¾‹å¦‚LLaMAã€Qwenã€Mixtralï¼‰è½¬æ¢ä¸ºåŸºäºMLAçš„æ¨¡å‹ã€‚è½¬æ¢åï¼Œæ¨¡å‹å¯ä»¥è¿›è¡Œé¢å¤–çš„è®­ç»ƒä»¥æå‡è¡¨è¾¾åŠ›è€Œä¸ä¼šå¢åŠ KVç¼“å­˜å¤§å°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¡åˆ’å¼€å‘é’ˆå¯¹MLAçš„æ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œä»¥ä¿æŒè½¬æ¢æ¨¡å‹ä¸­çš„ä½å»¶è¿Ÿï¼Œä»è€Œå®ç°Deepseek R1çš„æ›´æœ‰æ•ˆè’¸é¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07864v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/fxmeng/TransMLA">https://github.com/fxmeng/TransMLA</a></p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°ä»£ç¡¬ä»¶ä¸Šé‡åˆ°çš„ç“¶é¢ˆæ›´å¤šæ˜¯é€šä¿¡ç“¶é¢ˆè€Œéè®¡ç®—çº¦æŸã€‚Multi-head Latent Attentionï¼ˆMLAï¼‰é€šè¿‡å…³é”®å€¼å±‚ä¸­çš„ä½ç§©çŸ©é˜µè§£å†³æ­¤é—®é¢˜ï¼Œå…è®¸å‹ç¼©æ½œåœ¨å…³é”®å€¼çŠ¶æ€è¢«ç¼“å­˜ï¼Œæ˜¾è‘—å‡å°‘å…³é”®å€¼ç¼“å­˜å¤§å°ï¼ŒåŠ å¿«æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼ŒMLAä½¿ç”¨ä¸ŠæŠ•å½±çŸ©é˜µæé«˜è¡¨è¾¾èƒ½åŠ›ï¼Œä»¥é¢å¤–çš„è®¡ç®—æ¢å–å‡å°‘é€šä¿¡å¼€é”€ã€‚è™½ç„¶MLAå·²åœ¨Deepseek V2&#x2F;V3&#x2F;R1ä¸­è¡¨ç°å‡ºé«˜æ•ˆå’Œæœ‰æ•ˆæ€§ï¼Œä½†è®¸å¤šä¸»è¦æ¨¡å‹æä¾›å•†ä»ä¾èµ–Group Query Attentionï¼ˆGQAï¼‰å¹¶æœªå®£å¸ƒé‡‡ç”¨MLAçš„è®¡åˆ’ã€‚æœ¬æ–‡å±•ç¤ºäº†GQAå¯ç”±MLAè¡¨ç¤ºå¹¶ä¿æŒç›¸åŒçš„å…³é”®å€¼ç¼“å­˜å¼€é”€ï¼Œä½†åä¹‹ä¸æˆç«‹ã€‚ä¸ºé¼“åŠ±æ›´å¹¿æ³›ä½¿ç”¨MLAï¼Œæˆ‘ä»¬å¼•å…¥äº†TransMLAï¼Œè¿™æ˜¯ä¸€ç§å°†å¹¿æ³›ä½¿ç”¨çš„åŸºäºGQAçš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚LLaMAã€Qwenã€Mixtralï¼‰è½¬æ¢ä¸ºåŸºäºMLAçš„æ¨¡å‹çš„åè®­ç»ƒæ–¹æ³•ã€‚è½¬æ¢åçš„æ¨¡å‹å¯è¿›ä¸€æ­¥è¿›è¡Œè®­ç»ƒä»¥æé«˜è¡¨è¾¾èƒ½åŠ›è€Œä¸å¢åŠ å…³é”®å€¼ç¼“å­˜å¤§å°ã€‚æˆ‘ä»¬è¿˜è®¡åˆ’å¼€å‘MLAç‰¹å®šçš„æ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œä»¥ä¿æŒè½¬æ¢æ¨¡å‹çš„ä½å»¶è¿Ÿï¼Œä»è€Œå®ç°Deepseek R1æ›´æœ‰æ•ˆçš„è’¸é¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°ä»£ç¡¬ä»¶ä¸Šé‡åˆ°çš„æŒ‘æˆ˜ä¸»è¦æ˜¯é€šä¿¡ç“¶é¢ˆè€Œéè®¡ç®—çº¦æŸã€‚</li>
<li>Multi-head Latent Attention (MLA) é€šè¿‡ä½¿ç”¨ä½ç§©çŸ©é˜µå‡å°‘å…³é”®å€¼ç¼“å­˜å¤§å°å¹¶åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚</li>
<li>MLAé‡‡ç”¨ä¸ŠæŠ•å½±çŸ©é˜µä»¥æé«˜è¡¨è¾¾èƒ½åŠ›å¹¶å‡å°‘é€šä¿¡å¼€é”€ã€‚</li>
<li>å°½ç®¡MLAåœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä½†è®¸å¤šä¸»è¦æ¨¡å‹æä¾›å•†ä»ä¾èµ–Group Query Attention (GQA)ã€‚</li>
<li>GQAå¯ç”±MLAè¡¨ç¤ºå¹¶ä¿æŒç›¸åŒçš„å…³é”®å€¼ç¼“å­˜å¼€é”€ï¼Œä½†åä¹‹ä¸æˆç«‹ã€‚è¿™æ„å‘³ç€è½¬æ¢ä¸ºMLAåå¯èƒ½æœ‰æ›´å¤§çš„ä¼˜åŒ–æ½œåŠ›ã€‚</li>
<li>TransMLAæ–¹æ³•èƒ½å°†ç°æœ‰çš„åŸºäºGQAçš„é¢„è®­ç»ƒæ¨¡å‹è½¬æ¢ä¸ºåŸºäºMLAçš„æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æé«˜æ•ˆç‡å’Œè¡¨è¾¾èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-363c7f7bf68e9c6197303d92dc70b854.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d96a66bed76c7d1392436922074d5828.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Sa2VA-Marrying-SAM2-with-LLaVA-for-Dense-Grounded-Understanding-of-Images-and-Videos"><a href="#Sa2VA-Marrying-SAM2-with-LLaVA-for-Dense-Grounded-Understanding-of-Images-and-Videos" class="headerlink" title="Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of   Images and Videos"></a>Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of   Images and Videos</h2><p><strong>Authors:Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, Ming-Hsuan Yang</strong></p>
<p>This work presents Sa2VA, the first unified model for dense grounded understanding of both images and videos. Unlike existing multi-modal large language models, which are often limited to specific modalities and tasks, Sa2VA supports a wide range of image and video tasks, including referring segmentation and conversation, with minimal one-shot instruction tuning. Sa2VA combines SAM-2, a foundation video segmentation model, with LLaVA, an advanced vision-language model, and unifies text, image, and video into a shared LLM token space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2 in producing precise masks, enabling a grounded, multi-modal understanding of both static and dynamic visual content. Additionally, we introduce Ref-SAV, an auto-labeled dataset containing over 72k object expressions in complex video scenes, designed to boost model performance. We also manually validate 2k video objects in the Ref-SAV datasets to benchmark referring video object segmentation in complex environments. Experiments show that Sa2VA achieves state-of-the-art across multiple tasks, particularly in referring video object segmentation, highlighting its potential for complex real-world applications. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Sa2VAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¯†é›†åœ°å¯¹å›¾åƒå’Œè§†é¢‘è¿›è¡Œæ¥åœ°ç†è§£ã€‚ä¸ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸åŒï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä»…é™äºç‰¹å®šçš„æ¨¡æ€å’Œä»»åŠ¡ï¼Œè€ŒSa2VAæ”¯æŒå¹¿æ³›çš„å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¼•ç”¨åˆ†å‰²å’Œå¯¹è¯ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ä¸€æ¬¡ç®€å•çš„æŒ‡ä»¤è°ƒæ•´æ¥å®ç°ã€‚Sa2VAç»“åˆäº†SAM-2åŸºç¡€è§†é¢‘åˆ†å‰²æ¨¡å‹å’ŒLLaVAé«˜çº§è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶å°†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç»Ÿä¸€åˆ°å…±äº«çš„å¤§å‹è¯­è¨€æ¨¡å‹æ ‡è®°ç©ºé—´ä¸­ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒSa2VAç”ŸæˆæŒ‡ä»¤æ ‡è®°ï¼ŒæŒ‡å¯¼SAM-2ç”Ÿæˆç²¾ç¡®è’™ç‰ˆï¼Œå®ç°å¯¹é™æ€å’ŒåŠ¨æ€è§†è§‰å†…å®¹çš„æ¥åœ°å¤šæ¨¡æ€ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†Ref-SAVï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨æ ‡è®°çš„æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡7ä¸‡å¤šä¸ªå¤æ‚è§†é¢‘åœºæ™¯ä¸­çš„å¯¹è±¡è¡¨è¾¾å¼ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æ‰‹åŠ¨éªŒè¯äº†Ref-SAVæ•°æ®é›†ä¸­çš„2åƒä¸ªè§†é¢‘å¯¹è±¡ï¼Œä»¥è¯„ä¼°å¤æ‚ç¯å¢ƒä¸­å¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²çš„åŸºå‡†ã€‚å®éªŒè¡¨æ˜ï¼ŒSa2VAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼•ç”¨è§†é¢‘å¯¹è±¡åˆ†å‰²æ–¹é¢ï¼Œçªæ˜¾å…¶åœ¨å¤æ‚ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04001v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://lxtgh.github.io/project/sa2va">https://lxtgh.github.io/project/sa2va</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Sa2VAæ¨¡å‹ï¼Œå®ƒæ˜¯é¦–ä¸ªæ”¯æŒå›¾åƒå’Œè§†é¢‘å¯†é›†æ¥åœ°ç†è§£çš„ä¸€ä½“åŒ–æ¨¡å‹ã€‚è¯¥æ¨¡å‹èåˆäº†SAM-2è§†é¢‘åˆ†å‰²æ¨¡å‹å’ŒLLaVAé«˜çº§è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶å°†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç»Ÿä¸€åˆ°å…±äº«çš„LLMä»¤ç‰Œç©ºé—´ä¸­ã€‚é€šè¿‡LLMç”ŸæˆæŒ‡ä»¤ä»¤ç‰Œï¼ŒæŒ‡å¯¼SAM-2äº§ç”Ÿç²¾ç¡®è’™ç‰ˆï¼Œå®ç°é™æ€å’ŒåŠ¨æ€è§†è§‰å†…å®¹çš„æ¥åœ°ã€å¤šæ¨¡æ€ç†è§£ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†ç”¨äºå¤æ‚è§†é¢‘åœºæ™¯çš„Ref-SAVè‡ªæ ‡æ³¨æ•°æ®é›†ï¼Œç”¨äºæå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒSa2VAåœ¨å¤šä»»åŠ¡ä¸­å®ç°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œå°¤å…¶åœ¨æŒ‡ä»£è§†é¢‘å¯¹è±¡åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sa2VAæ˜¯é¦–ä¸ªç»Ÿä¸€æ¨¡å‹ï¼Œæ”¯æŒå›¾åƒå’Œè§†é¢‘çš„å¯†é›†æ¥åœ°ç†è§£ã€‚</li>
<li>Sa2VAèƒ½å¤„ç†å¤šç§å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬æŒ‡ä»£åˆ†å‰²å’Œå¯¹è¯ã€‚</li>
<li>Sa2VAèåˆäº†SAM-2è§†é¢‘åˆ†å‰²æ¨¡å‹å’ŒLLaVAè§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>Sa2VAå°†æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç»Ÿä¸€åˆ°å…±äº«çš„LLMä»¤ç‰Œç©ºé—´ä¸­ã€‚</li>
<li>LLMç”Ÿæˆçš„æŒ‡ä»¤ä»¤ç‰ŒæŒ‡å¯¼SAM-2äº§ç”Ÿç²¾ç¡®è’™ç‰ˆï¼Œå®ç°å¤šæ¨¡æ€ç†è§£ã€‚</li>
<li>å¼•å…¥äº†Ref-SAVè‡ªæ ‡æ³¨æ•°æ®é›†ï¼Œç”¨äºæå‡æ¨¡å‹åœ¨å¤æ‚è§†é¢‘åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e674ac33d9fe6414948b90d1316e6404.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e8e2a54bf4514ebb2a031b381b0f026.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a204e60c549d249a5bb5495b06bea101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-615069321016dc30c055734f61fabc06.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2963b2cb82eb1a7044f6b2fc7eef650.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="WarriorCoder-Learning-from-Expert-Battles-to-Augment-Code-Large-Language-Models"><a href="#WarriorCoder-Learning-from-Expert-Battles-to-Augment-Code-Large-Language-Models" class="headerlink" title="WarriorCoder: Learning from Expert Battles to Augment Code Large   Language Models"></a>WarriorCoder: Learning from Expert Battles to Augment Code Large   Language Models</h2><p><strong>Authors:Huawen Feng, Pu Zhao, Qingfeng Sun, Can Xu, Fangkai Yang, Lu Wang, Qianli Ma, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang</strong></p>
<p>Despite recent progress achieved by code large language models (LLMs), their remarkable abilities are largely dependent on fine-tuning on the high-quality data, posing challenges for data collection and annotation. To address this, current methods often design various data flywheels to collect complex code instructions, enabling models to handle more intricate tasks. However, these approaches typically rely on off-the-shelf datasets and data augmentation from a limited set of proprietary LLMs (e.g., Claude, GPT4, and so on), which restricts the diversity of the constructed data and makes it prone to systemic biases. In this paper, we propose WarriorCoder, a novel paradigm learns from expert battles to address these limitations. Specifically, we create an arena where leading expert code LLMs challenge each other, with evaluations conducted by impartial judges. This competitive framework generates novel training data from scratch, leveraging the strengths of all participants. Experimental results show that WarriorCoder achieves state-of-the-art performance compared to previous models of the same size, even without relying on proprietary LLMs. </p>
<blockquote>
<p>å°½ç®¡ä»£ç å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬çš„å“è¶Šèƒ½åŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé«˜è´¨é‡æ•°æ®çš„å¾®è°ƒï¼Œè¿™ç»™æ•°æ®é‡‡é›†å’Œæ ‡æ³¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå½“å‰çš„æ–¹æ³•é€šå¸¸è®¾è®¡å„ç§æ•°æ®é£è½®æ¥æ”¶é›†å¤æ‚çš„ä»£ç æŒ‡ä»¤ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºç°æˆçš„æ•°æ®é›†å’Œæœ‰é™ä¸“æœ‰LLMï¼ˆå¦‚Claudeã€GPT4ç­‰ï¼‰çš„æ•°æ®å¢å¼ºï¼Œè¿™é™åˆ¶äº†æ„å»ºæ•°æ®çš„å¤šæ ·æ€§ï¼Œå¹¶ä½¿å…¶å®¹æ˜“å—ç³»ç»Ÿåè§çš„å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WarriorCoderï¼Œä¸€ç§æ–°å‹èŒƒå¼ï¼Œä»ä¸“å®¶å¯¹å†³ä¸­å­¦ä¹ ï¼Œä»¥è§£å†³è¿™äº›é™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç«æŠ€åœºï¼Œè®©é¢†å…ˆçš„ä¸“å®¶ä»£ç LLMç›¸äº’æŒ‘æˆ˜ï¼Œç”±å…¬æ­£è£åˆ¤è¿›è¡Œè¯„ä¼°ã€‚è¿™ä¸ªç«äº‰æ¡†æ¶ä»é›¶å¼€å§‹ç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®ï¼Œåˆ©ç”¨æ‰€æœ‰å‚ä¸è€…çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWarriorCoderåœ¨ç›¸åŒå¤§å°çš„æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œç”šè‡³ä¸éœ€è¦ä¾èµ–ä¸“æœ‰LLMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17395v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†WarriorCoderï¼Œä¸€ä¸ªä»ä¸“å®¶å¯¹æŠ—ä¸­å­¦ä¹ çš„æ–°å‹èŒƒå¼ï¼Œä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°æ®å¤„ç†å’Œæ³¨é‡Šæ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åˆ›å»ºä¸€ä¸ªé¢†å…ˆçš„ä¸“ä¸šä»£ç LLMsç«æŠ€åœºï¼Œé€šè¿‡å…¬æ­£çš„è¯„å§”è¿›è¡Œè¯„ä¼°ï¼Œç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWarriorCoderåœ¨ä¸ä¾èµ–ä¸“æœ‰LLMsçš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ä¸ç›¸åŒè§„æ¨¡çš„å‰æ¨¡å‹çš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶å·²ç»å–å¾—è¿›å±•ï¼Œä½†ä»éœ€ç²¾ç»†åŒ–è°ƒæ•´ä»¥å¤„ç†é«˜è´¨é‡æ•°æ®ã€‚</li>
<li>ç›®å‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–ç°æˆçš„æ•°æ®é›†å’Œæœ‰é™ä¸“æœ‰LLMsçš„æ•°æ®å¢å¼ºæ¥æ”¶é›†å¤æ‚çš„ä»£ç æŒ‡ä»¤ã€‚</li>
<li>ä¸“æœ‰LLMsçš„é™åˆ¶åŒ…æ‹¬æ•°æ®å¤šæ ·æ€§ä¸è¶³å’Œç³»ç»Ÿåè§ã€‚</li>
<li>WarriorCoderæ¡†æ¶é€šè¿‡åˆ›å»ºé¢†å…ˆçš„ä¸“ä¸šä»£ç LLMsç«æŠ€åœºæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>WarriorCoderåˆ©ç”¨æ‰€æœ‰å‚ä¸è€…çš„ä¼˜åŠ¿ç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒWarriorCoderåœ¨ä¸ä¾èµ–ä¸“æœ‰LLMsçš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17395">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-42ffc099feae5d5674537d912989bfd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3877351cc79efe5ff732c1d262cca83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d436755734b6351195f5a644938121c3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Diffusion-Transformer-Policy-Scaling-Diffusion-Transformer-for-Generalist-Vision-Language-Action-Learning"><a href="#Diffusion-Transformer-Policy-Scaling-Diffusion-Transformer-for-Generalist-Vision-Language-Action-Learning" class="headerlink" title="Diffusion Transformer Policy: Scaling Diffusion Transformer for   Generalist Vision-Language-Action Learning"></a>Diffusion Transformer Policy: Scaling Diffusion Transformer for   Generalist Vision-Language-Action Learning</h2><p><strong>Authors:Zhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu, Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai, Yuntao Chen</strong></p>
<p>Recent large vision-language action models pretrained on diverse robot datasets have demonstrated the potential for generalizing to new environments with a few in-domain data. However, those approaches usually predict individual discretized or continuous action by a small action head, which limits the ability in handling diverse action spaces. In contrast, we model the continuous action sequence with a large multi-modal diffusion transformer, dubbed as Diffusion Transformer Policy, in which we directly denoise action chunks by a large transformer model rather than a small action head for action embedding. By leveraging the scaling capability of transformers, the proposed approach can effectively model continuous end-effector actions across large diverse robot datasets, and achieve better generalization performance. Extensive experiments demonstrate the effectiveness and generalization of Diffusion Transformer Policy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world Franka arm, achieving consistent better performance on Real-to-Sim benchmark SimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo. Specifically, without bells and whistles, the proposed approach achieves state-of-the-art performance with only a single third-view camera stream in the Calvin task ABC-&gt;D, improving the average number of tasks completed in a row of 5 to 3.6, and the pretraining stage significantly facilitates the success sequence length on the Calvin by over 1.2. Project Page: <a target="_blank" rel="noopener" href="https://zhihou7.github.io/dit_policy_vla/">https://zhihou7.github.io/dit_policy_vla/</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹é€šè¿‡åœ¨å„ç§æœºå™¨äººæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå·²ç»æ˜¾ç¤ºå‡ºåœ¨æ–°ç¯å¢ƒä¸­ä½¿ç”¨å°‘é‡é¢†åŸŸæ•°æ®è¿›è¡Œæ³›åŒ–çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸é€šè¿‡å°å‹åŠ¨ä½œå¤´é¢„æµ‹ç¦»æ•£æˆ–è¿ç»­çš„åŠ¨ä½œï¼Œè¿™é™åˆ¶äº†å¤„ç†å¤šæ ·åŒ–åŠ¨ä½œç©ºé—´çš„èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨æ¥æ¨¡æ‹Ÿè¿ç»­åŠ¨ä½œåºåˆ—ï¼Œç§°ä¸ºæ‰©æ•£å˜å‹å™¨ç­–ç•¥ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå¤§å‹çš„å˜å‹å™¨æ¨¡å‹ç›´æ¥å»é™¤åŠ¨ä½œå—ä¸­çš„å™ªå£°ï¼Œè€Œä¸æ˜¯é€šè¿‡ä¸€ä¸ªå°å‹çš„åŠ¨ä½œå¤´è¿›è¡ŒåŠ¨ä½œåµŒå…¥ã€‚é€šè¿‡åˆ©ç”¨å˜å‹å™¨çš„å¯æ‰©å±•æ€§ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æ¨¡æ‹Ÿè·¨å¤§å‹å¤šæ ·åŒ–æœºå™¨äººæ•°æ®é›†çš„è¿ç»­æœ«ç«¯æ‰§è¡Œå™¨åŠ¨ä½œï¼Œå¹¶å®ç°æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDiffusion Transformer Policyåœ¨Maniskill2ã€Liberoã€Calvinå’ŒSimplerEnvä»¥åŠçœŸå®ä¸–ç•Œçš„Frankaæœºæ¢°è‡‚ä¸Šçš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œä¸OpenVLAå’ŒOctoç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨Real-to-SimåŸºå‡†æµ‹è¯•SimplerEnvã€çœŸå®ä¸–ç•Œçš„Frankaæœºæ¢°è‡‚å’ŒLiberoä¸Šè¡¨ç°ä¸€è‡´ä¸”æ›´å¥½ã€‚ç‰¹åˆ«çš„æ˜¯ï¼Œåœ¨ä¸ä½¿ç”¨ä»»ä½•èŠ±å“¨çš„æŠ€æœ¯çš„æƒ…å†µä¸‹ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä»…åœ¨Calvinä»»åŠ¡çš„ABC-&gt;Dä¸­ä½¿ç”¨å•ä¸ªç¬¬ä¸‰äººç§°è§†è§’çš„ç›¸æœºæµå°±å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°†è¿ç»­å®Œæˆä»»åŠ¡çš„å¹³å‡æ•°é‡ä»5æé«˜åˆ°3.6ï¼Œè€Œä¸”é¢„è®­ç»ƒé˜¶æ®µæ˜¾è‘—å¢åŠ äº†Calvinä»»åŠ¡ä¸Šçš„æˆåŠŸåºåˆ—é•¿åº¦è¶…è¿‡1. - é½é¸¿åšåšå£«ä¸ªäººç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://zhihou7.github.io/dit_policy_vla/">https://zhihou7.github.io/dit_policy_vla/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15959v3">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨çš„æ–°å‹æœºå™¨äººåŠ¨ä½œç­–ç•¥ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œå®ƒé€šè¿‡å¤§å‹å˜å‹å™¨æ¨¡å‹ç›´æ¥å¯¹åŠ¨ä½œç‰‡æ®µè¿›è¡Œå»å™ªï¼Œè€Œéé€šè¿‡å°å‹åŠ¨ä½œå¤´è¿›è¡ŒåŠ¨ä½œåµŒå…¥ã€‚è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå»ºæ¨¡å¤§å‹å¤šæ ·æœºå™¨äººæ•°æ®é›†ä¸­çš„è¿ç»­æœ«ç«¯æ‰§è¡Œå™¨åŠ¨ä½œï¼Œå¹¶å®ç°æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨æ¨¡æ‹Ÿåˆ°ç°å®ç¯å¢ƒçš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£å˜å‹å™¨çš„å¤§å‹è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ ·æœºå™¨äººæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ³›åŒ–åˆ°æ–°ç¯å¢ƒå¹¶å¤„ç†å¤šæ ·åŒ–çš„åŠ¨ä½œç©ºé—´ã€‚</li>
<li>é€šè¿‡å¤§å‹å˜å‹å™¨æ¨¡å‹ç›´æ¥å¯¹åŠ¨ä½œç‰‡æ®µè¿›è¡Œå»å™ªï¼Œæé«˜äº†åŠ¨ä½œåµŒå…¥çš„æ•ˆæœã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬Maniskill2ã€Liberoã€Calvinå’ŒSimplerEnvç­‰ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿåˆ°ç°å®ç¯å¢ƒçš„ä»»åŠ¡ä¸­ï¼Œè¯¥ç­–ç•¥å®ç°äº†æ¯”OpenVLAå’ŒOctoæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç­–ç•¥ä»…ä½¿ç”¨å•ä¸ªç¬¬ä¸‰äººç§°è§†è§’ç›¸æœºæµï¼Œåœ¨Calvinä»»åŠ¡ABC-&gt;Dä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4479e07b39a8de7b68d596e21726cfb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00a652f829a46d1224ec081fdfa93a59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a48abcee3297f1afd349d62ba9241b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-338b40674e9e90bfe793622a8441e73b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83ca834836b706621f4326e23adfbacb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1368f54c1ebf7e7419c4451fc07e173.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="An-Evolved-Universal-Transformer-Memory"><a href="#An-Evolved-Universal-Transformer-Memory" class="headerlink" title="An Evolved Universal Transformer Memory"></a>An Evolved Universal Transformer Memory</h2><p><strong>Authors:Edoardo Cetin, Qi Sun, Tianyu Zhao, Yujin Tang</strong></p>
<p>Prior methods propose to offset the escalating costs of modern foundation models by dropping specific parts of their contexts with hand-designed rules, while attempting to preserve their original performance. We overcome this trade-off with Neural Attention Memory Models (NAMMs), introducing a learned network for memory management that improves both the performance and efficiency of transformers. We evolve NAMMs atop pre-trained transformers to provide different latent contexts focusing on the most relevant information for individual layers and attention heads. NAMMs are universally applicable to any model using self-attention as they condition exclusively on the values in the produced attention matrices. Learning NAMMs on a small set of problems, we achieve substantial performance improvements across multiple long-context benchmarks while cutting the modelâ€™s input contexts up to a fraction of the original sizes. We show the generality of our conditioning enables zero-shot transfer of NAMMs trained only on language to entirely new transformer architectures even across input modalities, with their benefits carrying over to vision and reinforcement learning. </p>
<blockquote>
<p>å…ˆå‰çš„æ–¹æ³•æè®®é€šè¿‡åˆ©ç”¨æ‰‹å·¥è®¾è®¡çš„è§„åˆ™æ¥åˆ é™¤ç°ä»£åŸºç¡€æ¨¡å‹ç‰¹å®šéƒ¨åˆ†çš„ä¸Šä¸‹æ–‡æ¥æŠµæ¶ˆå…¶ä¸æ–­ä¸Šå‡çš„æˆæœ¬ï¼ŒåŒæ—¶è¯•å›¾ä¿æŒå…¶åŸå§‹æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡ç¥ç»æ³¨æ„åŠ›è®°å¿†æ¨¡å‹ï¼ˆNAMMsï¼‰å…‹æœäº†è¿™ä¸€æƒè¡¡ï¼Œå¼•å…¥äº†ç”¨äºè®°å¿†ç®¡ç†çš„å­¦ä¹ ç½‘ç»œï¼Œæé«˜äº†è½¬æ¢å™¨çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚æˆ‘ä»¬åœ¨é¢„è®­ç»ƒçš„è½¬æ¢å™¨ä¹‹ä¸Šå‘å±•NAMMsï¼Œä»¥æä¾›ä¸åŒçš„æ½œåœ¨ä¸Šä¸‹æ–‡ï¼Œä¸“æ³¨äºå„å±‚å’Œæ³¨æ„åŠ›å¤´æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚ç”±äºå®ƒä»¬ä»…ä¾èµ–äºç”Ÿæˆçš„æ³¨æ„åŠ›çŸ©é˜µä¸­çš„å€¼ï¼Œå› æ­¤NAMMså¯æ™®éé€‚ç”¨äºä»»ä½•ä½¿ç”¨è‡ªæ³¨æ„åŠ›çš„æ¨¡å‹ã€‚é€šè¿‡åœ¨ä¸€ç³»åˆ—é—®é¢˜ä¸Šå­¦ä¹ NAMMsï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªé•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶è¿˜å°†æ¨¡å‹çš„è¾“å…¥ä¸Šä¸‹æ–‡ç¼©å‡åˆ°åŸå§‹å¤§å°çš„ä¸€å°éƒ¨åˆ†ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¡ä»¶é€šç”¨æ€§ä½¿ä»…é€šè¿‡è¯­è¨€è®­ç»ƒçš„NAMMsèƒ½å¤Ÿé›¶æ ·æœ¬è¿ç§»åˆ°å…¨æ–°çš„è½¬æ¢å™¨æ¶æ„ï¼Œç”šè‡³è·¨è¶Šè¾“å…¥æ¨¡å¼ï¼Œå®ƒä»¬çš„ä¼˜åŠ¿ä¹Ÿå»¶ä¼¸åˆ°è®¡ç®—æœºè§†è§‰å’Œå¼ºåŒ–å­¦ä¹ é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13166v4">PDF</a> Published at ICLR 2025. Source code available at   <a target="_blank" rel="noopener" href="https://github.com/SakanaAI/evo-memory">https://github.com/SakanaAI/evo-memory</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºç¥ç»æ³¨æ„åŠ›è®°å¿†æ¨¡å‹ï¼ˆNAMMsï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›å’Œä¼˜åŒ–å˜å‹å™¨çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å­¦ä¹ åˆ°çš„å†…å­˜ç®¡ç†æœºåˆ¶æ¥å…‹æœä»¥å¾€æ–¹æ³•ä¸­æˆæœ¬å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡é¢„è®­ç»ƒçš„å˜å‹å™¨ï¼ŒNAMMsèƒ½å¤Ÿç”Ÿæˆé’ˆå¯¹ä¸åŒå±‚çº§å’Œæ³¨æ„åŠ›å¤´çš„ä¸åŒæ½œåœ¨ä¸Šä¸‹æ–‡ï¼Œä¸“æ³¨äºæœ€ç›¸å…³çš„ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºä»»ä½•ä½¿ç”¨è‡ªæ³¨æ„åŠ›çš„æ¨¡å‹ï¼Œå› ä¸ºå®ƒå®Œå…¨ä¾èµ–äºäº§ç”Ÿçš„æ³¨æ„åŠ›çŸ©é˜µçš„å€¼ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼ŒNAMMsåœ¨å¤šä¸ªé•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å°†æ¨¡å‹çš„è¾“å…¥ä¸Šä¸‹æ–‡å¤§å°ç¼©å‡è‡³åŸå§‹å¤§å°çš„æå°éƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†å…¶åœ¨è¯­è¨€è®­ç»ƒçš„NAMMså¯ä»¥é›¶è¿ç§»è‡³å…¨æ–°å˜å‹å™¨æ¶æ„ä¹ƒè‡³ä¸åŒè¾“å…¥æ¨¡å¼çš„ä¸€èˆ¬æ€§èƒ½åŠ›ï¼Œå¹¶æˆåŠŸå°†å…¶åº”ç”¨äºè§†è§‰å’Œå¼ºåŒ–å­¦ä¹ é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NAMMsé€šè¿‡å¼•å…¥å­¦ä¹ åˆ°çš„å†…å­˜ç®¡ç†æœºåˆ¶æ¥æ”¹è¿›å’Œä¼˜åŒ–å˜å‹å™¨çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚</li>
<li>NAMMså…‹æœäº†ä»¥å¾€æ–¹æ³•ä¸­æˆæœ¬å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>NAMMsèƒ½å¤Ÿç”Ÿæˆé’ˆå¯¹ä¸åŒå±‚çº§å’Œæ³¨æ„åŠ›å¤´çš„ä¸åŒæ½œåœ¨ä¸Šä¸‹æ–‡ï¼Œä¸“æ³¨äºæœ€ç›¸å…³çš„ä¿¡æ¯ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œé€‚ç”¨äºä»»ä½•ä½¿ç”¨è‡ªæ³¨æ„åŠ›çš„æ¨¡å‹ã€‚</li>
<li>NAMMsé€šè¿‡ç”Ÿæˆæ³¨æ„åŠ›çŸ©é˜µçš„å€¼è¿›è¡Œå·¥ä½œï¼Œå®ç°äº†æ›´é«˜æ•ˆå’Œçµæ´»çš„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªé•¿ä¸Šä¸‹æ–‡åŸºå‡†æµ‹è¯•ä¸­ï¼ŒNAMMså®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å°†æ¨¡å‹çš„è¾“å…¥ä¸Šä¸‹æ–‡å¤§å°å¤§å¹…ç¼©å‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-29999435f29cfaceb0422b417bbf6326.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4967aa645956f9a01cf0240fe0d98479.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ce924a6bbffd8ef1402232b139c13c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9eafcfc0b34f365af4909a064355f8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3c993621b935d696c61dc23af0a8b6e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Hello-Again-LLM-powered-Personalized-Agent-for-Long-term-Dialogue"><a href="#Hello-Again-LLM-powered-Personalized-Agent-for-Long-term-Dialogue" class="headerlink" title="Hello Again! LLM-powered Personalized Agent for Long-term Dialogue"></a>Hello Again! LLM-powered Personalized Agent for Long-term Dialogue</h2><p><strong>Authors:Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, Tat-Seng Chua</strong></p>
<p>Open-domain dialogue systems have seen remarkable advancements with the development of large language models (LLMs). Nonetheless, most existing dialogue systems predominantly focus on brief single-session interactions, neglecting the real-world demands for long-term companionship and personalized interactions with chatbots. Crucial to addressing this real-world need are event summary and persona management, which enable reasoning for appropriate long-term dialogue responses. Recent progress in the human-like cognitive and reasoning capabilities of LLMs suggests that LLM-based agents could significantly enhance automated perception, decision-making, and problem-solving. In response to this potential, we introduce a model-agnostic framework, the Long-term Dialogue Agent (LD-Agent), which incorporates three independently tunable modules dedicated to event perception, persona extraction, and response generation. For the event memory module, long and short-term memory banks are employed to separately focus on historical and ongoing sessions, while a topic-based retrieval mechanism is introduced to enhance the accuracy of memory retrieval. Furthermore, the persona module conducts dynamic persona modeling for both users and agents. The integration of retrieved memories and extracted personas is subsequently fed into the generator to induce appropriate responses. The effectiveness, generality, and cross-domain capabilities of LD-Agent are empirically demonstrated across various illustrative benchmarks, models, and tasks. The code is released at <a target="_blank" rel="noopener" href="https://github.com/leolee99/LD-Agent">https://github.com/leolee99/LD-Agent</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼Œå¼€æ”¾åŸŸå¯¹è¯ç³»ç»Ÿå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„å¯¹è¯ç³»ç»Ÿä¸»è¦å…³æ³¨çŸ­æš‚çš„å•ä¸€ä¼šè¯äº’åŠ¨ï¼Œå¿½è§†äº†ç°å®ä¸–ç•Œå¯¹é•¿æœŸä¼´ä¾£å…³ç³»å’Œä¸èŠå¤©æœºå™¨äººçš„ä¸ªæ€§åŒ–äº’åŠ¨çš„éœ€æ±‚ã€‚è¦è§£å†³è¿™ä¸€ç°å®éœ€æ±‚çš„å…³é”®æ˜¯äº‹ä»¶æ€»ç»“å’Œäººæ ¼ç®¡ç†ï¼Œå®ƒä»¬ä¸ºé€‚å½“çš„é•¿æœŸå¯¹è¯å›åº”æä¾›äº†æ¨ç†èƒ½åŠ›ã€‚æœ€è¿‘ï¼ŒLLMçš„äººç±»è®¤çŸ¥å’Œæ¨ç†èƒ½åŠ›çš„è¿›æ­¥è¡¨æ˜ï¼ŒåŸºäºLLMçš„ä»£ç†å¯ä»¥æ˜¾ç€å¢å¼ºè‡ªåŠ¨åŒ–æ„ŸçŸ¥ã€å†³ç­–å’Œé—®é¢˜è§£å†³ã€‚åŸºäºæ­¤æ½œåŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¨¡å‹æ— å…³æ¡†æ¶ï¼Œå³é•¿æœŸå¯¹è¯ä»£ç†ï¼ˆLD-Agentï¼‰ï¼Œå®ƒåŒ…æ‹¬ä¸‰ä¸ªç‹¬ç«‹å¯è°ƒæ¨¡å—ï¼Œä¸“é—¨ç”¨äºäº‹ä»¶æ„ŸçŸ¥ã€äººæ ¼æå–å’Œå“åº”ç”Ÿæˆã€‚å¯¹äºäº‹ä»¶å†…å­˜æ¨¡å—ï¼Œé•¿çŸ­æ—¶è®°å¿†åº“è¢«ç”¨æ¥åˆ†åˆ«å…³æ³¨å†å²ä¼šè¯å’Œæ­£åœ¨è¿›è¡Œä¸­çš„ä¼šè¯ï¼ŒåŒæ—¶å¼•å…¥åŸºäºä¸»é¢˜çš„æ£€ç´¢æœºåˆ¶æ¥æé«˜å†…å­˜æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œäººæ ¼æ¨¡å—ä¸ºç”¨æˆ·å’Œä»£ç†è¿›è¡ŒåŠ¨æ€äººæ ¼å»ºæ¨¡ã€‚æ£€ç´¢åˆ°çš„è®°å¿†å’Œæå–çš„äººæ ¼çš„æ•´åˆéšåè¢«è¾“å…¥åˆ°ç”Ÿæˆå™¨ä¸­ï¼Œä»¥äº§ç”Ÿé€‚å½“çš„å“åº”ã€‚LD-Agentçš„æœ‰æ•ˆæ€§ã€é€šç”¨æ€§å’Œè·¨åŸŸèƒ½åŠ›åœ¨å„ç§åŸºå‡†æµ‹è¯•ã€æ¨¡å‹å’Œä»»åŠ¡ä¸­å¾—åˆ°äº†å®è¯è¯æ˜ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/leolee99/LD-Agent%E3%80%82">https://github.com/leolee99/LD-Agentã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.05925v2">PDF</a> Accepted to NAACL 2025</p>
<p><strong>Summary</strong>ï¼š<br>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼Œå¼€æ”¾åŸŸå¯¹è¯ç³»ç»Ÿå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰å¯¹è¯ç³»ç»Ÿä¸»è¦å…³æ³¨çŸ­æš‚çš„å•ä¸€ä¼šè¯äº’åŠ¨ï¼Œå¿½è§†äº†ç°å®ä¸–ç•Œå¯¹é•¿æœŸä¼´ä¾£å…³ç³»å’Œä¸èŠå¤©æœºå™¨äººçš„ä¸ªæ€§åŒ–äº’åŠ¨çš„éœ€æ±‚ã€‚ä¸ºè§£å†³è¿™ä¸€éœ€æ±‚ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ¨¡å‹æ— å…³æ¡†æ¶â€”â€”é•¿æœŸå¯¹è¯ä»£ç†ï¼ˆLD-Agentï¼‰ï¼ŒåŒ…å«ä¸‰ä¸ªç‹¬ç«‹å¯è°ƒæ¨¡å—ï¼šäº‹ä»¶æ„ŸçŸ¥ã€äººæ ¼æå–å’Œå“åº”ç”Ÿæˆã€‚è¯¥æ¡†æ¶é‡‡ç”¨é•¿çŸ­æ—¶è®°å¿†åº“åˆ†åˆ«å…³æ³¨å†å²å’Œå½“å‰ä¼šè¯ï¼Œå¹¶å¼•å…¥ä¸»é¢˜åŸºäºçš„æ£€ç´¢æœºåˆ¶æ¥æé«˜è®°å¿†æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œäººæ ¼æ¨¡å—ä¸ºç”¨æˆ·å’Œä»£ç†è¿›è¡ŒåŠ¨æ€äººæ ¼å»ºæ¨¡ã€‚æ£€ç´¢åˆ°çš„è®°å¿†å’Œæå–çš„äººæ ¼é›†æˆåï¼Œä¼šç”Ÿæˆé€‚å½“çš„å“åº”ã€‚LD-Agentçš„æœ‰æ•ˆæ€§ã€é€šç”¨æ€§å’Œè·¨åŸŸèƒ½åŠ›åœ¨å„ç§åŸºå‡†æµ‹è¯•ã€æ¨¡å‹å’Œä»»åŠ¡ä¸­å¾—åˆ°äº†å®è¯è¯æ˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼€æ”¾åŸŸå¯¹è¯ç³»ç»Ÿå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å–å¾—æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>ç°æœ‰å¯¹è¯ç³»ç»Ÿä¸»è¦å…³æ³¨çŸ­æš‚å•ä¸€ä¼šè¯äº’åŠ¨ï¼Œç¼ºä¹é•¿æœŸä¼´ä¾£å’Œä¸ªæ€§åŒ–äº’åŠ¨çš„èƒ½åŠ›ã€‚</li>
<li>ä¸ºæ»¡è¶³ç°å®éœ€æ±‚ï¼Œå¼•å…¥é•¿æœŸå¯¹è¯ä»£ç†ï¼ˆLD-Agentï¼‰æ¡†æ¶ã€‚</li>
<li>LD-AgentåŒ…å«ä¸‰ä¸ªç‹¬ç«‹å¯è°ƒæ¨¡å—ï¼šäº‹ä»¶æ„ŸçŸ¥ã€äººæ ¼æå–å’Œå“åº”ç”Ÿæˆã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨é•¿çŸ­æ—¶è®°å¿†åº“ä»¥å…³æ³¨å†å²å’Œå½“å‰ä¼šè¯ï¼Œæé«˜è®°å¿†æ£€ç´¢å‡†ç¡®æ€§ã€‚</li>
<li>åŠ¨æ€äººæ ¼å»ºæ¨¡ä¸ºç”¨æˆ·å’Œä»£ç†æä¾›ä¸ªæ€§åŒ–äº’åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.05925">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87adabffa177add39a140454adf2b5ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2090b4ea15d5237678da5c9b71b48b75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0371212a75e39cb72fce6ceda00ddd2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Transformers-Learn-Low-Sensitivity-Functions-Investigations-and-Implications"><a href="#Transformers-Learn-Low-Sensitivity-Functions-Investigations-and-Implications" class="headerlink" title="Transformers Learn Low Sensitivity Functions: Investigations and   Implications"></a>Transformers Learn Low Sensitivity Functions: Investigations and   Implications</h2><p><strong>Authors:Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, Vatsal Sharan</strong></p>
<p>Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of their inductive biases and how those biases differ from other neural network architectures remains elusive. In this work, we identify the sensitivity of the model to token-wise random perturbations in the input as a unified metric which explains the inductive bias of transformers across different data modalities and distinguishes them from other architectures. We show that transformers have lower sensitivity than MLPs, CNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show that this low-sensitivity bias has important implications: i) lower sensitivity correlates with improved robustness; it can also be used as an efficient intervention to further improve the robustness of transformers; ii) it corresponds to flatter minima in the loss landscape; and iii) it can serve as a progress measure for grokking. We support these findings with theoretical results showing (weak) spectral bias of transformers in the NTK regime, and improved robustness due to the lower sensitivity. The code is available at <a target="_blank" rel="noopener" href="https://github.com/estija/sensitivity">https://github.com/estija/sensitivity</a>. </p>
<blockquote>
<p>Transformeræ¨¡å‹åœ¨è®¸å¤šä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œä½†å¯¹å…¶å½’çº³åç½®ä»¥åŠå¦‚ä½•ä¸å…¶ä»–ç¥ç»ç½‘ç»œæ¶æ„çš„åç½®ç›¸åŒºåˆ«çš„ç†è§£ä»ç„¶ä¸æ˜ç¡®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å¯¹è¾“å…¥ä¸­ä»¤ç‰Œçº§éšæœºæ‰°åŠ¨çš„æ•æ„Ÿæ€§ä½œä¸ºç»Ÿä¸€åº¦é‡æ ‡å‡†ï¼Œä»¥è§£é‡Šä¸åŒæ•°æ®æ¨¡å¼ä¸‹Transformerçš„å½’çº³åç½®ï¼Œå¹¶å°†å…¶ä¸å…¶ä»–æ¶æ„åŒºåˆ†å¼€ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä¸å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰ã€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰ã€ConvMixerså’Œé•¿çŸ­æ—¶è®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰ç›¸æ¯”ï¼ŒTransformeråœ¨è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸Šçš„æ•æ„Ÿæ€§è¾ƒä½ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œè¿™ç§ä½æ•æ„Ÿæ€§åç½®å…·æœ‰é‡è¦å½±å“ï¼šiï¼‰ä½æ•æ„Ÿæ€§ä¸æé«˜çš„é²æ£’æ€§ç›¸å…³ï¼›å®ƒè¿˜å¯ä»¥ä½œä¸ºè¿›ä¸€æ­¥æé«˜Transformeré²æ£’æ€§çš„æœ‰æ•ˆå¹²é¢„æªæ–½ï¼›iiï¼‰å®ƒä¸æŸå¤±æ™¯è§‚ä¸­çš„å¹³å¦æœ€å°å€¼ç›¸å¯¹åº”ï¼›iiiï¼‰å®ƒå¯ä»¥ä½œä¸ºå­¦ä¹ è¿›æ­¥çš„ä¸€ç§åº¦é‡ã€‚æˆ‘ä»¬ä»¥ç†è®ºç»“æœæ”¯æŒè¿™äº›å‘ç°ï¼Œå±•ç¤ºäº†åœ¨NTKèŒƒå›´å†…Transformerçš„ï¼ˆå¼±ï¼‰è°±åç½®ï¼Œä»¥åŠç”±äºä½æ•æ„Ÿæ€§è€Œæé«˜äº†é²æ£’æ€§ã€‚ä»£ç ä½äº <a target="_blank" rel="noopener" href="https://github.com/estija/sensitivity%E3%80%82">https://github.com/estija/sensitivityã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06925v2">PDF</a> ICLR 2025. 24 pages, 19 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Transformeræ¨¡å‹çš„å½’çº³åè§ï¼Œé€šè¿‡ç»Ÿä¸€åº¦é‡è¾“å…¥ä¸­çš„tokençº§éšæœºæ‰°åŠ¨æ•æ„Ÿæ€§æ¥è§£é‡Šå…¶åœ¨ä¸åŒæ•°æ®æ¨¡æ€ä¸‹çš„å½’çº³åè§ï¼Œå¹¶å°†å…¶ä¸å…¶ä»–æ¶æ„åŒºåˆ†å¼€æ¥ã€‚ç ”ç©¶å‘ç°ï¼ŒTransformeræ¨¡å‹çš„æ•æ„Ÿæ€§ä½äºMLPsã€CNNsã€ConvMixerså’ŒLSTMæ¨¡å‹ï¼Œåœ¨è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè¾ƒä½çš„æ•æ„Ÿæ€§åè§ã€‚æ­¤å¤–ï¼Œä½æ•æ„Ÿæ€§åè§å¯¹äºæé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œåœ¨æŸå¤±æ™¯è§‚ä¸­çš„å¹³å¦æœ€å°ç‚¹å¯»æ‰¾æœ‰é‡è¦ä½œç”¨ï¼ŒåŒæ—¶å¯ä»¥ä½œä¸ºæ”¹è¿›Transformeré²æ£’æ€§çš„æœ‰æ•ˆå¹²é¢„æ‰‹æ®µã€‚æœ¬æ–‡è¿˜æä¾›äº†ç†è®ºæ”¯æŒï¼Œå±•ç¤ºäº†Transformeråœ¨NTKçŠ¶æ€ä¸‹çš„å¼±è°±åå‘ä»¥åŠå› ä½æ•æ„Ÿæ€§è€Œæé«˜çš„é²æ£’æ€§ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹å…·æœ‰ç»Ÿä¸€çš„å½’çº³åè§åº¦é‡æ ‡å‡†ï¼Œå³è¾“å…¥ä¸­çš„tokençº§éšæœºæ‰°åŠ¨æ•æ„Ÿæ€§ã€‚</li>
<li>Transformeræ¨¡å‹åœ¨è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè¾ƒä½çš„æ•æ„Ÿæ€§åè§ï¼Œä¸å…¶ä»–ç½‘ç»œæ¶æ„ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>ä½æ•æ„Ÿæ€§åè§ä¸æé«˜æ¨¡å‹çš„é²æ£’æ€§ç›¸å…³ï¼Œå¯ä½œä¸ºè¿›ä¸€æ­¥æé«˜Transformeré²æ£’æ€§çš„å¹²é¢„æ‰‹æ®µã€‚</li>
<li>ä½æ•æ„Ÿæ€§åè§ä¸æŸå¤±æ™¯è§‚ä¸­çš„å¹³å¦æœ€å°ç‚¹æœ‰å…³ï¼Œæœ‰åŠ©äºåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æ‰¾åˆ°æ›´å¥½çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>Transformerçš„å¼±è°±åå‘åŠå…¶åœ¨NTKçŠ¶æ€ä¸‹çš„è¡¨ç°æ˜¯ç†è®ºæ”¯æŒçš„ä¸€éƒ¨åˆ†ï¼Œæœ‰åŠ©äºè§£é‡Šå…¶å½’çº³åè§å’Œé²æ£’æ€§çš„æé«˜ã€‚</li>
<li>ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶å’Œå®éªŒä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.06925">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-72e62faf11c8f3d6143404603fc74b45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62c91f2784bc2f3dcbac1bac5ca83371.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-186b3dc979afa84526ce1ea39a0ac56e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching"><a href="#Agent-OM-Leveraging-LLM-Agents-for-Ontology-Matching" class="headerlink" title="Agent-OM: Leveraging LLM Agents for Ontology Matching"></a>Agent-OM: Leveraging LLM Agents for Ontology Matching</h2><p><strong>Authors:Zhangcheng Qiang, Weiqing Wang, Kerry Taylor</strong></p>
<p>Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks. </p>
<blockquote>
<p>æœ¬ä½“åŒ¹é…ï¼ˆOMï¼‰èƒ½å¤Ÿé€šè¿‡å¯¹é½ç›¸å…³å®ä½“ï¼Œå®ç°åœ¨ä¸åŒæœ¬ä½“ä¹‹é—´çš„è¯­ä¹‰äº’æ“ä½œï¼Œå¹¶è§£å†³å…¶æ¦‚å¿µä¸Šçš„å¼‚è´¨æ€§ã€‚ç›®å‰ï¼ŒOMç³»ç»Ÿä¸»è¦æœ‰ä¸¤ç§æµè¡Œçš„è®¾è®¡èŒƒå¼ï¼šä¼ ç»Ÿçš„åŸºäºçŸ¥è¯†çš„ä¸“å®¶ç³»ç»Ÿå’Œè¾ƒæ–°çš„åŸºäºæœºå™¨å­¦ä¹ çš„é¢„æµ‹ç³»ç»Ÿã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒLLMä»£ç†å·²ç»å½»åº•æ”¹å˜äº†æ•°æ®å·¥ç¨‹ï¼Œå¹¶ä¸”åœ¨è®¸å¤šé¢†åŸŸå¾—åˆ°äº†åˆ›é€ æ€§çš„åº”ç”¨ï¼Œä½†å®ƒä»¬åœ¨OMé¢†åŸŸçš„æ½œåŠ›ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°å‹åŸºäºLLMçš„ä»£ç†é©±åŠ¨è®¾è®¡èŒƒå¼çš„OMç³»ç»Ÿã€‚è€ƒè™‘åˆ°åœ¨åˆ©ç”¨LLMä»£ç†è¿›è¡ŒOMæ—¶é¢ä¸´çš„è‹¥å¹²ç‰¹å®šæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå³Agent-OMï¼ˆç”¨äºæœ¬ä½“åŒ¹é…çš„ä»£ç†ï¼‰ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªç”¨äºæ£€ç´¢å’ŒåŒ¹é…çš„Siameseä»£ç†å’Œä¸€ç»„OMå·¥å…·ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸€ä¸ªæ¦‚å¿µéªŒè¯ç³»ç»Ÿä¸­å®ç°ã€‚å¯¹ä¸‰ä¸ªæœ¬ä½“å¯¹é½è¯„ä¼°å€¡è®®ï¼ˆOAEIï¼‰èµ›é“ä¸æœ€æ–°OMç³»ç»Ÿçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ç®€å•OMä»»åŠ¡ä¸Šçš„ç»“æœéå¸¸æ¥è¿‘é•¿æœŸä»¥æ¥çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å¤æ‚å’Œå°‘æ ·æœ¬OMä»»åŠ¡ä¸Šå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00326v9">PDF</a> 19 pages, 12 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ¬ç ”ç©¶çš„ä»‹ç»ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒLLMä»£ç†äººåœ¨æœ¬ä½“åŒ¹é…ï¼ˆOMï¼‰ç³»ç»Ÿä¸­çš„åº”ç”¨ç ”ç©¶ä»å¤„äºåˆçº§é˜¶æ®µã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºLLMä»£ç†çš„æœ¬ä½“åŒ¹é…è®¾è®¡èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³å½“å‰OMç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜ã€‚æ‰€æå‡ºçš„Agent-OMæ¡†æ¶ç”±ä¸¤ä¸ªç”¨äºæ£€ç´¢å’ŒåŒ¹é…çš„Siameseä»£ç†ç»„æˆï¼Œç»“åˆä¸€å¥—OMå·¥å…·ã€‚ç»è¿‡åˆæ­¥çš„ç³»ç»ŸéªŒè¯ï¼Œå…¶æ€§èƒ½å·²æ¥è¿‘ç°æœ‰çš„å…ˆè¿›OMç³»ç»Ÿåœ¨ç®€å•ä»»åŠ¡ä¸Šçš„æœ€ä½³è¡¨ç°ï¼Œå¹¶åœ¨å¤æ‚å’Œå°‘æ•°æ ·æœ¬çš„OMä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°å‹æœ¬ä½“åŒ¹é…ï¼ˆOMï¼‰ç³»ç»Ÿè®¾è®¡èŒƒå¼ã€‚</li>
<li>è¯¥è®¾è®¡èŒƒå¼åˆ©ç”¨LLMä»£ç†æ¥è§£å†³ä¸åŒæœ¬ä½“ä¹‹é—´çš„è¯­ä¹‰äº’æ“ä½œæ€§å’Œæ¦‚å¿µå¼‚è´¨æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šç”¨æ¡†æ¶Agent-OMï¼ŒåŒ…å«ä¸¤ä¸ªç”¨äºæ£€ç´¢å’ŒåŒ¹é…çš„Siameseä»£ç†ä»¥åŠä¸€å¥—OMå·¥å…·ã€‚</li>
<li>ç³»ç»Ÿåœ¨ç®€å•çš„æœ¬ä½“åŒ¹é…ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ¥è¿‘å½“å‰æœ€ä½³æ°´å¹³ã€‚</li>
<li>åœ¨å¤æ‚çš„å’Œå°‘é‡æ ·æœ¬çš„æœ¬ä½“åŒ¹é…ä»»åŠ¡ä¸Šï¼Œç³»ç»Ÿæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li>
<li>æ­¤æ¡†æ¶å…·æœ‰ä¸€å®šçš„åˆ›æ–°æ€§å’Œæ½œåŠ›ï¼Œä½†ä¹Ÿéœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œæ”¹è¿›ä»¥ä¼˜åŒ–å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ecdd80c6916c6d20bdc896b6378b0698.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f8ce04bbe6794ef29e1c14efd92931b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cde61049b12372f9a36b0b53054ed8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a28263bc37c69f1762c6ed4d90716bb5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-15/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-15/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-15/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-89583288a4f08b6a8395375248739cc3.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-15  EmbodiedBench Comprehensive Benchmarking Multi-modal Large Language   Models for Vision-Driven Embodied Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fb6803249d476e3ed1a138e741d792b8.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-14  A General Pipeline for Glomerulus Whole-Slide Image Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16663.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
