<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-15  Diffusing DeBias a Recipe for Turning a Bug into a Feature">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2dcc09f8fe93c0d9bedcec151c2dae43.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-15-æ›´æ–°"><a href="#2025-02-15-æ›´æ–°" class="headerlink" title="2025-02-15 æ›´æ–°"></a>2025-02-15 æ›´æ–°</h1><h2 id="Diffusing-DeBias-a-Recipe-for-Turning-a-Bug-into-a-Feature"><a href="#Diffusing-DeBias-a-Recipe-for-Turning-a-Bug-into-a-Feature" class="headerlink" title="Diffusing DeBias: a Recipe for Turning a Bug into a Feature"></a>Diffusing DeBias: a Recipe for Turning a Bug into a Feature</h2><p><strong>Authors:Massimiliano Ciranni, Vito Paolo Pastore, Roberto Di Via, Enzo Tartaglione, Francesca Odone, Vittorio Murino</strong></p>
<p>Deep learning model effectiveness in classification tasks is often challenged by the quality and quantity of training data which, whenever containing strong spurious correlations between specific attributes and target labels, can result in unrecoverable biases in model predictions. Tackling these biases is crucial in improving model generalization and trust, especially in real-world scenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting as a plug-in for common methods in model debiasing while exploiting the inherent bias-learning tendency of diffusion models. Our approach leverages conditional diffusion models to generate synthetic bias-aligned images, used to train a bias amplifier model, to be further employed as an auxiliary method in different unsupervised debiasing approaches. Our proposed method, which also tackles the common issue of training set memorization typical of this type of tech- niques, beats current state-of-the-art in multiple benchmark datasets by significant margins, demonstrating its potential as a versatile and effective tool for tackling dataset bias in deep learning applications. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å¸¸å¸¸å—åˆ°è®­ç»ƒæ•°æ®è´¨é‡å’Œæ•°é‡çš„æŒ‘æˆ˜ã€‚è®­ç»ƒæ•°æ®ä¸­ç‰¹å®šå±æ€§ä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„è™šå‡å…³è”æ—¶ï¼Œä¼šå¯¼è‡´æ¨¡å‹é¢„æµ‹ä¸­å‡ºç°ä¸å¯æŒ½å›çš„åè§ã€‚è§£å†³è¿™äº›åè§å¯¹äºæé«˜æ¨¡å‹çš„é€šç”¨æ€§å’Œä¿¡ä»»åº¦è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨ç°å®ä¸–ç•Œçš„åœºæ™¯ä¸­ã€‚æœ¬æ–‡æå‡ºäº†Diffusing DeBiasï¼ˆDDBï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå¯ä½œä¸ºé€šç”¨æ¨¡å‹å»åæ–¹æ³•çš„æ’ä»¶ï¼ŒåŒæ—¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹å›ºæœ‰çš„åå­¦ä¹ å€¾å‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆåç½®å¯¹é½å›¾åƒï¼Œç”¨äºè®­ç»ƒåç½®æ”¾å¤§å™¨æ¨¡å‹ï¼Œè¿›ä¸€æ­¥ç”¨äºä¸åŒçš„æ— ç›‘ç£å»åç½®æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•è¿˜è§£å†³äº†æ­¤ç±»æŠ€æœ¯å…¸å‹çš„è®­ç»ƒé›†è®°å¿†åŒ–é—®é¢˜ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå¤§å¹…è¶…è¶Šäº†å½“å‰æœ€æ–°æŠ€æœ¯ï¼Œè¯æ˜å…¶ä½œä¸ºè§£å†³æ·±åº¦å­¦ä¹ åº”ç”¨ä¸­æ•°æ®é›†åè§çš„é€šç”¨å’Œæœ‰æ•ˆå·¥å…·çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09564v1">PDF</a> 29 Pages, 12 Figures</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ç»å¸¸å—åˆ°è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œæ•°é‡çš„æŒ‘æˆ˜ã€‚å½“è®­ç»ƒæ•°æ®åŒ…å«ç‰¹å®šå±æ€§ä¸ç›®æ ‡æ ‡ç­¾ä¹‹é—´çš„å¼ºçƒˆè™šå‡å…³è”æ—¶ï¼Œä¼šå¯¼è‡´æ¨¡å‹é¢„æµ‹ä¸­å‡ºç°ä¸å¯æŒ½å›çš„åè§ã€‚æœ¬è®ºæ–‡æå‡ºä¸€ç§åä¸ºDiffusing DeBiasï¼ˆDDBï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä½œä¸ºé€šç”¨æ¨¡å‹å»åæ–¹æ³•çš„æ’ä»¶ï¼ŒåŒæ—¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å›ºæœ‰åå­¦ä¹ å€¾å‘ã€‚DDBæ–¹æ³•åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆåç½®å¯¹é½å›¾åƒï¼Œç”¨äºè®­ç»ƒåç½®æ”¾å¤§å™¨æ¨¡å‹ï¼Œå¯ä½œä¸ºä¸åŒæ— ç›‘ç£å»åæ–¹æ³•çš„è¾…åŠ©æ–¹æ³•ã€‚è¯¥æ–¹æ³•è§£å†³äº†å½“å‰æŠ€æœ¯ä¸­æ™®éå­˜åœ¨çš„è®­ç»ƒé›†è®°å¿†é—®é¢˜ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†å½“å‰å…ˆè¿›æŠ€æœ¯ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ·±åº¦å­¦ä¹ åº”ç”¨ä¸­è§£å†³æ•°æ®é›†åè§çš„æ½œåŠ›å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸­é¢ä¸´æ•°æ®è´¨é‡å’Œæ•°é‡çš„æŒ‘æˆ˜ã€‚</li>
<li>è®­ç»ƒæ•°æ®ä¸­çš„è™šå‡å…³è”ä¼šå¯¼è‡´æ¨¡å‹é¢„æµ‹å‡ºç°åè§ã€‚</li>
<li>Diffusing DeBias (DDB)æ˜¯ä¸€ç§æ–°çš„å»åæ–¹æ³•ï¼Œå¯ä½œä¸ºé€šç”¨æ’ä»¶ç”¨äºæ¨¡å‹å»åã€‚</li>
<li>DDBåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å›ºæœ‰åå­¦ä¹ å€¾å‘ã€‚</li>
<li>DDBé€šè¿‡ç”Ÿæˆåˆæˆåç½®å¯¹é½å›¾åƒæ¥è®­ç»ƒåç½®æ”¾å¤§å™¨æ¨¡å‹ã€‚</li>
<li>DDBå¯ä½œä¸ºä¸åŒæ— ç›‘ç£å»åæ–¹æ³•çš„è¾…åŠ©æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09564">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a8b107ea07bae58d09bb5ab041751e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7de77b375a1010c091b5bf595eaaed59.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Redistribute-Ensemble-Training-for-Mitigating-Memorization-in-Diffusion-Models"><a href="#Redistribute-Ensemble-Training-for-Mitigating-Memorization-in-Diffusion-Models" class="headerlink" title="Redistribute Ensemble Training for Mitigating Memorization in Diffusion   Models"></a>Redistribute Ensemble Training for Mitigating Memorization in Diffusion   Models</h2><p><strong>Authors:Xiaoliu Guan, Yu Wu, Huayang Huang, Xiao Liu, Jiaxu Miao, Yi Yang</strong></p>
<p>Diffusion models, known for their tremendous ability to generate high-quality samples, have recently raised concerns due to their data memorization behavior, which poses privacy risks. Recent methods for memory mitigation have primarily addressed the issue within the context of the text modality in cross-modal generation tasks, restricting their applicability to specific conditions. In this paper, we propose a novel method for diffusion models from the perspective of visual modality, which is more generic and fundamental for mitigating memorization. Directly exposing visual data to the model increases memorization risk, so we design a framework where models learn through proxy model parameters instead. Specially, the training dataset is divided into multiple shards, with each shard training a proxy model, then aggregated to form the final model. Additionally, practical analysis of training losses illustrates that the losses for easily memorable images tend to be obviously lower. Thus, we skip the samples with abnormally low loss values from the current mini-batch to avoid memorizing. However, balancing the need to skip memorization-prone samples while maintaining sufficient training data for high-quality image generation presents a key challenge. Thus, we propose IET-AGC+, which redistributes highly memorizable samples between shards, to mitigate these samples from over-skipping. Furthermore, we dynamically augment samples based on their loss values to further reduce memorization. Extensive experiments and analysis on four datasets show that our method successfully reduces memory capacity while maintaining performance. Moreover, we fine-tune the pre-trained diffusion models, e.g., Stable Diffusion, and decrease the memorization score by 46.7%, demonstrating the effectiveness of our method. Code is available in: <a target="_blank" rel="noopener" href="https://github.com/liuxiao-guan/IET_AGC">https://github.com/liuxiao-guan/IET_AGC</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å› å…¶ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬çš„å‡ºè‰²èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ï¼Œä½†æœ€è¿‘å…¶æ•°æ®è®°å¿†è¡Œä¸ºå¼•å‘äº†å…³äºéšç§é£é™©çš„æ‹…å¿§ã€‚æœ€è¿‘çš„è®°å¿†ç¼“è§£æ–¹æ³•ä¸»è¦è§£å†³äº†è·¨æ¨¡æ€ç”Ÿæˆä»»åŠ¡ä¸­æ–‡æœ¬æ¨¡æ€çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨ç‰¹å®šæ¡ä»¶ä¸‹çš„é€‚ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»è§†è§‰æ¨¡æ€çš„è§’åº¦æå‡ºäº†ä¸€ç§æ–°å‹æ‰©æ•£æ¨¡å‹ç¼“è§£è®°å¿†çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ç¼“è§£è®°å¿†æ–¹é¢æ›´ä¸ºé€šç”¨å’ŒåŸºç¡€ã€‚ç›´æ¥å°†è§†è§‰æ•°æ®æš´éœ²ç»™æ¨¡å‹ä¼šå¢åŠ è®°å¿†é£é™©ï¼Œå› æ­¤æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå…¶ä¸­æ¨¡å‹é€šè¿‡ä»£ç†æ¨¡å‹å‚æ•°è¿›è¡Œå­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œè®­ç»ƒæ•°æ®é›†è¢«åˆ†æˆå¤šä¸ªåˆ†ç‰‡ï¼Œæ¯ä¸ªåˆ†ç‰‡è®­ç»ƒä¸€ä¸ªä»£ç†æ¨¡å‹ï¼Œç„¶åèšåˆå½¢æˆæœ€ç»ˆæ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¯¹è®­ç»ƒæŸå¤±çš„å®é™…åˆ†æè¡¨æ˜ï¼Œå®¹æ˜“è®°å¿†çš„å›¾åƒçš„æŸå¤±å¾€å¾€æ˜æ˜¾è¾ƒä½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»å½“å‰å°å‹æ‰¹æ¬¡ä¸­è·³è¿‡æŸå¤±å€¼å¼‚å¸¸çš„æ ·æœ¬ä»¥é¿å…è®°å¿†ã€‚ç„¶è€Œï¼Œå¹³è¡¡è·³è¿‡æ˜“è®°å¿†æ ·æœ¬çš„éœ€æ±‚ä¸ç»´æŒé«˜è´¨é‡å›¾åƒç”Ÿæˆçš„å……è¶³è®­ç»ƒæ•°æ®ä¹‹é—´å‘ˆç°å…³é”®æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†IET-AGC+ã€‚å®ƒå°†é«˜åº¦å¯è®°å¿†çš„æ ·æœ¬é‡æ–°åˆ†é…ç»™ä¸åŒçš„åˆ†ç‰‡ï¼Œä»¥ç¼“è§£è¿™äº›æ ·æœ¬è¢«è¿‡åº¦è·³è¿‡çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ ¹æ®æŸå¤±å€¼åŠ¨æ€åœ°æ‰©å……æ ·æœ¬ä»¥è¿›ä¸€æ­¥å‡å°‘è®°å¿†ã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒå’Œåˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡å°‘å†…å­˜å®¹é‡çš„åŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆä¾‹å¦‚Stable Diffusionï¼‰è¿›è¡Œäº†å¾®è°ƒï¼Œå°†è®°å¿†åˆ†æ•°é™ä½äº†46.7%ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/liuxiao-guan/IET_AGC%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/liuxiao-guan/IET_AGCä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09434v1">PDF</a> 12 pages,9 figures. arXiv admin note: substantial text overlap with   arXiv:2407.15328</p>
<p><strong>æ‘˜è¦</strong><br>æ‰©æ•£æ¨¡å‹å…·æœ‰ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬çš„å‡ºè‰²èƒ½åŠ›ï¼Œä½†ä¹Ÿå­˜åœ¨æ•°æ®è®°å¿†è¡Œä¸ºï¼Œå¸¦æ¥éšç§é£é™©ã€‚æœ¬æ–‡ä»è§†è§‰æ¨¡å¼çš„è§’åº¦æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ï¼Œç”¨äºå‡è½»è®°å¿†è´Ÿæ‹…ï¼Œå…·æœ‰æ›´é€šç”¨å’ŒåŸºæœ¬çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒä»£ç†æ¨¡å‹å‚æ•°çš„æ–¹å¼ï¼Œä½¿æ¨¡å‹ä¸ç›´æ¥æ¥è§¦è§†è§‰æ•°æ®ï¼Œé™ä½è®°å¿†é£é™©ã€‚æˆ‘ä»¬å°†è®­ç»ƒæ•°æ®é›†åˆ†æˆå¤šä¸ªåˆ†ç‰‡ï¼Œæ¯ä¸ªåˆ†ç‰‡è®­ç»ƒä¸€ä¸ªä»£ç†æ¨¡å‹ï¼Œç„¶åèšåˆå½¢æˆæœ€ç»ˆæ¨¡å‹ã€‚åŒæ—¶ï¼Œé€šè¿‡å¯¹è®­ç»ƒæŸå¤±çš„åˆ†æï¼Œæˆ‘ä»¬è·³è¿‡å½“å‰æ‰¹æ¬¡ä¸­æŸå¤±å€¼å¼‚å¸¸ä½çš„æ ·æœ¬ï¼Œé¿å…è®°å¿†ã€‚ç„¶è€Œï¼Œåœ¨è·³è¿‡æ˜“è®°å¿†æ ·æœ¬çš„åŒæ—¶ï¼Œè¿˜éœ€ä¿æŒè¶³å¤Ÿçš„é«˜è´¨é‡å›¾åƒç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œè¿™æ˜¯å…³é”®æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†IET-AGC+æ–¹æ³•ï¼Œé€šè¿‡é‡æ–°åˆ†é…é«˜åº¦å¯è®°å¿†çš„æ ·æœ¬åˆ†ç‰‡æ¥ç¼“è§£è¿™äº›æ ·æœ¬çš„è¿‡åº¦è·³è¿‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ ¹æ®æ ·æœ¬çš„æŸå¤±å€¼è¿›è¡ŒåŠ¨æ€æ‰©å……ï¼Œè¿›ä¸€æ­¥å‡å°‘è®°å¿†ã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒå’Œåˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é™ä½å†…å­˜å®¹é‡çš„åŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶å°†è®°å¿†åˆ†æ•°é™ä½äº†46.7%ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/liuxiao-guan/IET_AGC%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/liuxiao-guan/IET_AGCä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å…·æœ‰ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬çš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨æ•°æ®è®°å¿†è¡Œä¸ºï¼Œå¼•å‘éšç§é£é™©ã€‚</li>
<li>æœ¬æ–‡ä»è§†è§‰æ¨¡å¼çš„è§’åº¦æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ï¼Œä»¥æ›´é€šç”¨å’ŒåŸºæœ¬çš„é€‚ç”¨æ€§å‡è½»è®°å¿†è´Ÿæ‹…ã€‚</li>
<li>é€šè¿‡è®­ç»ƒä»£ç†æ¨¡å‹å‚æ•°ï¼Œé™ä½æ¨¡å‹å¯¹è§†è§‰æ•°æ®çš„ç›´æ¥æ¥è§¦ï¼Œä»è€Œé™ä½è®°å¿†é£é™©ã€‚</li>
<li>æå‡ºå°†è®­ç»ƒæ•°æ®é›†åˆ†æˆå¤šä¸ªåˆ†ç‰‡ï¼Œå¹¶é€šè¿‡èšåˆå½¢æˆæœ€ç»ˆæ¨¡å‹ã€‚</li>
<li>é€šè¿‡åˆ†æè®­ç»ƒæŸå¤±æ¥è·³è¿‡æ˜“è®°å¿†çš„æ ·æœ¬ï¼ŒåŒæ—¶ä¿æŒè¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ä»¥ç»´æŒé«˜è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>æå‡ºäº†IET-AGC+æ–¹æ³•ï¼Œé€šè¿‡é‡æ–°åˆ†é…é«˜åº¦å¯è®°å¿†çš„æ ·æœ¬åˆ†ç‰‡æ¥ç¼“è§£è¿‡åº¦è·³è¿‡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-857671a86528503dfcff7f8d391325f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0237108b51f6731a190622093c038c76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39db0fdfe8320ce7b3fe3b7c7f8070c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7fc4efde3ca239002212907bf5eef19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbe2408e2576bc9c27b519758c41ce35.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ImageRAG-Dynamic-Image-Retrieval-for-Reference-Guided-Image-Generation"><a href="#ImageRAG-Dynamic-Image-Retrieval-for-Reference-Guided-Image-Generation" class="headerlink" title="ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation"></a>ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation</h2><p><strong>Authors:Rotem Shalev-Arkushin, Rinon Gal, Amit H. Bermano, Ohad Fried</strong></p>
<p>Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts. To address this challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with image generation models. We propose ImageRAG, a method that dynamically retrieves relevant images based on a given text prompt, and uses them as context to guide the generation process. Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation. In contrast, ImageRAG leverages the capabilities of existing image conditioning models, and does not require RAG-specific training. Our approach is highly adaptable and can be applied across different model types, showing significant improvement in generating rare and fine-grained concepts using different base models.   Our project page is available at: <a target="_blank" rel="noopener" href="https://rotem-shalev.github.io/ImageRAG">https://rotem-shalev.github.io/ImageRAG</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿå®ç°é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„è§†è§‰å†…å®¹åˆæˆã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç”Ÿæˆç¨€æœ‰æˆ–æœªè§æ¦‚å¿µæ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸å›¾åƒç”Ÿæˆæ¨¡å‹ç»“åˆçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ä½¿ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ImageRAGæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ ¹æ®ç»™å®šçš„æ–‡æœ¬æç¤ºåŠ¨æ€æ£€ç´¢ç›¸å…³å›¾åƒï¼Œå¹¶å°†å…¶ç”¨ä½œä¸Šä¸‹æ–‡æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚å…ˆå‰ä½¿ç”¨æ£€ç´¢å›¾åƒæ¥æ”¹å–„ç”Ÿæˆçš„æ–¹æ³•ï¼Œéƒ½æ˜¯é’ˆå¯¹åŸºäºæ£€ç´¢çš„ç”Ÿæˆè¿›è¡Œä¸“é—¨è®­ç»ƒçš„æ¨¡å‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒImageRAGåˆ©ç”¨ç°æœ‰çš„å›¾åƒæ¡ä»¶æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¹¶ä¸”ä¸éœ€è¦é’ˆå¯¹RAGè¿›è¡Œä¸“é—¨è®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•é«˜åº¦çµæ´»ï¼Œå¯åº”ç”¨äºä¸åŒç±»å‹çš„æ¨¡å‹ï¼Œåœ¨ä½¿ç”¨ä¸åŒåŸºç¡€æ¨¡å‹ç”Ÿæˆç¨€æœ‰å’Œç²¾ç»†æ¦‚å¿µæ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ä½äºï¼š<a target="_blank" rel="noopener" href="https://rotem-shalev.github.io/ImageRAG">https://rotem-shalev.github.io/ImageRAG</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09411v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿå®ç°é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹åˆæˆï¼Œä½†åœ¨ç”Ÿæˆç¨€æœ‰æˆ–æœªè§æ¦‚å¿µæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸å›¾åƒç”Ÿæˆæ¨¡å‹ç»“åˆçš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ImageRAGæ–¹æ³•ï¼Œå®ƒå¯ä»¥æ ¹æ®ç»™å®šçš„æ–‡æœ¬æç¤ºåŠ¨æ€æ£€ç´¢ç›¸å…³å›¾åƒï¼Œå¹¶å°†å…¶ç”¨ä½œä¸Šä¸‹æ–‡æ¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ä¹‹å‰ä½¿ç”¨æ£€ç´¢å›¾åƒæ”¹è¿›ç”Ÿæˆçš„æ–¹æ³•ç›¸æ¯”ï¼ŒImageRAGåˆ©ç”¨ç°æœ‰å›¾åƒæ¡ä»¶æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ— éœ€è¿›è¡ŒRAGç‰¹å®šè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰é«˜åº¦é€‚åº”æ€§ï¼Œå¯åº”ç”¨äºä¸åŒç±»å‹çš„æ¨¡å‹ï¼Œåœ¨ä½¿ç”¨ä¸åŒåŸºç¡€æ¨¡å‹ç”Ÿæˆç¨€æœ‰å’Œç²¾ç»†æ¦‚å¿µæ—¶è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåˆæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„è§†è§‰å†…å®¹ã€‚</li>
<li>åœ¨ç”Ÿæˆç¨€æœ‰æˆ–æœªè§æ¦‚å¿µæ—¶ï¼Œæ‰©æ•£æ¨¡å‹é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†ImageRAGæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºåŠ¨æ€æ£€ç´¢ç›¸å…³å›¾åƒã€‚</li>
<li>ImageRAGåˆ©ç”¨ç°æœ‰å›¾åƒæ¡ä»¶æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ— éœ€è¿›è¡Œç‰¹å®šè®­ç»ƒã€‚</li>
<li>ImageRAGå…·æœ‰é«˜åº¦é€‚åº”æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äºä¸åŒç±»å‹çš„æ¨¡å‹ã€‚</li>
<li>ImageRAGåœ¨ç”Ÿæˆç¨€æœ‰å’Œç²¾ç»†æ¦‚å¿µæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e6b1e3f58ab5ece43ddb7427dddac88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64ec7b76440d5189851a854df17e3115.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-130817f6b395b65dc7ccb9eb3138effe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53a05596c74a88173aaaeadad863aa1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d69b70abd0aacf61d2f11c2887cc454.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ConsistentDreamer-View-Consistent-Meshes-Through-Balanced-Multi-View-Gaussian-Optimization"><a href="#ConsistentDreamer-View-Consistent-Meshes-Through-Balanced-Multi-View-Gaussian-Optimization" class="headerlink" title="ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View   Gaussian Optimization"></a>ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View   Gaussian Optimization</h2><p><strong>Authors:Onat Åahin, Mohammad Altillawi, George Eskandar, Carlos Carbone, Ziyuan Liu</strong></p>
<p>Recent advances in diffusion models have significantly improved 3D generation, enabling the use of assets generated from an image for embodied AI simulations. However, the one-to-many nature of the image-to-3D problem limits their use due to inconsistent content and quality across views. Previous models optimize a 3D model by sampling views from a view-conditioned diffusion prior, but diffusion models cannot guarantee view consistency. Instead, we present ConsistentDreamer, where we first generate a set of fixed multi-view prior images and sample random views between them with another diffusion model through a score distillation sampling (SDS) loss. Thereby, we limit the discrepancies between the views guided by the SDS loss and ensure a consistent rough shape. In each iteration, we also use our generated multi-view prior images for fine-detail reconstruction. To balance between the rough shape and the fine-detail optimizations, we introduce dynamic task-dependent weights based on homoscedastic uncertainty, updated automatically in each iteration. Additionally, we employ opacity, depth distortion, and normal alignment losses to refine the surface for mesh extraction. Our method ensures better view consistency and visual quality compared to the state-of-the-art. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•æå¤§åœ°æ”¹è¿›äº†3Dç”Ÿæˆï¼Œä½¿å¾—å¯ä»¥ä½¿ç”¨ä»å›¾åƒç”Ÿæˆçš„èµ„äº§è¿›è¡Œå®ä½“AIæ¨¡æ‹Ÿã€‚ç„¶è€Œï¼Œå›¾åƒåˆ°3Dçš„ä¸€å¯¹å¤šç‰¹æ€§ç”±äºä¸åŒè§†è§’çš„å†…å®¹å’Œè´¨é‡ä¸ä¸€è‡´ï¼Œé™åˆ¶äº†å…¶ä½¿ç”¨ã€‚ä¹‹å‰çš„æ¨¡å‹é€šè¿‡ä»è§†å›¾æ¡ä»¶æ‰©æ•£å…ˆéªŒä¸­é‡‡æ ·è§†å›¾æ¥ä¼˜åŒ–3Dæ¨¡å‹ï¼Œä½†æ‰©æ•£æ¨¡å‹æ— æ³•ä¿è¯è§†å›¾çš„ä¸€è‡´æ€§ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ConsistentDreamerï¼Œæˆ‘ä»¬é¦–å…ˆç”Ÿæˆä¸€ç»„å›ºå®šçš„å¤šè§†è§’å…ˆéªŒå›¾åƒï¼Œå¹¶åœ¨å®ƒä»¬ä¹‹é—´ä½¿ç”¨å¦ä¸€ä¸ªæ‰©æ•£æ¨¡å‹é€šè¿‡å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æŸå¤±æ¥é‡‡æ ·éšæœºè§†å›¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é™åˆ¶äº†ç”±SDSæŸå¤±å¼•å¯¼çš„è§†è§’ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶ç¡®ä¿äº†ä¸€è‡´çš„ç²—ç•¥å½¢çŠ¶ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ç”Ÿæˆçš„å¤šè§†è§’å…ˆéªŒå›¾åƒè¿›è¡Œç»†èŠ‚é‡å»ºã€‚ä¸ºäº†å¹³è¡¡ç²—ç•¥å½¢çŠ¶å’Œç»†èŠ‚ä¼˜åŒ–çš„å¹³è¡¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºåŒæ–¹å·®ä¸ç¡®å®šæ€§çš„åŠ¨æ€ä»»åŠ¡ä¾èµ–æƒé‡ï¼Œå¹¶åœ¨æ¯æ¬¡è¿­ä»£ä¸­è‡ªåŠ¨æ›´æ–°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨é€æ˜åº¦ã€æ·±åº¦å¤±çœŸå’Œæ³•çº¿å¯¹é½æŸå¤±æ¥ä¼˜åŒ–ç½‘æ ¼æå–çš„è¡¨é¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”æ›´å¥½çš„è§†å›¾ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09278v1">PDF</a> Manuscript accepted by Pattern Recognition Letters</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²æ˜¾è‘—æé«˜3Dç”Ÿæˆèƒ½åŠ›ï¼Œå¯å®ç°ä»å›¾åƒç”Ÿæˆèµ„äº§ç”¨äºå®ä½“AIæ¨¡æ‹Ÿã€‚ç„¶è€Œï¼Œå›¾åƒåˆ°3Dçš„ä¸€å¯¹å¤šé—®é¢˜ç”±äºå…¶å†…å®¹åœ¨ä¸åŒè§†è§’çš„ä¸ä¸€è‡´æ€§é™åˆ¶äº†å…¶ä½¿ç”¨ã€‚æˆ‘ä»¬æå‡ºConsistentDreamerï¼Œé€šè¿‡ç”Ÿæˆå›ºå®šå¤šè§†è§’å…ˆéªŒå›¾åƒï¼Œå¹¶åœ¨å®ƒä»¬ä¹‹é—´é€šè¿‡è¯„åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æŸå¤±ä½¿ç”¨å¦ä¸€ä¸ªæ‰©æ•£æ¨¡å‹è¿›è¡Œéšæœºè§†å›¾é‡‡æ ·ï¼Œä¿è¯è§†å›¾ä¸€è‡´æ€§ã€‚è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨ç”Ÿæˆçš„å¤šè§†è§’å…ˆéªŒå›¾åƒè¿›è¡Œç»†èŠ‚é‡å»ºï¼Œå¹¶å¼•å…¥åŸºäºä»»åŠ¡ä¾èµ–æ€§çš„åŠ¨æ€æƒé‡ä»¥å¹³è¡¡å¤§ä½“å½¢çŠ¶ä¸ç»†èŠ‚ä¼˜åŒ–çš„å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨é€æ˜åº¦ã€æ·±åº¦å¤±çœŸå’Œæ³•çº¿å¯¹é½æŸå¤±æ¥ä¼˜åŒ–è¡¨é¢ç½‘æ ¼æå–ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯ï¼Œç¡®ä¿äº†æ›´å¥½çš„è§†å›¾ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•æ¨åŠ¨äº†3Dç”Ÿæˆçš„æ˜¾è‘—æ”¹è¿›ï¼Œæ”¯æŒä»å›¾åƒç”Ÿæˆèµ„äº§ç”¨äºAIæ¨¡æ‹Ÿã€‚</li>
<li>å›¾åƒåˆ°3Dçš„ä¸€å¯¹å¤šé—®é¢˜å¯¼è‡´å†…å®¹åœ¨ä¸åŒè§†è§’ä¸‹çš„ä¸ä¸€è‡´æ€§ï¼Œé™åˆ¶äº†ä½¿ç”¨ã€‚</li>
<li>ConsistentDreameré€šè¿‡ç”Ÿæˆå›ºå®šå¤šè§†è§’å…ˆéªŒå›¾åƒå’Œè¯„åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰æŸå¤±ä¿è¯è§†å›¾ä¸€è‡´æ€§ã€‚</li>
<li>è¿­ä»£è¿‡ç¨‹ä¸­åˆ©ç”¨å¤šè§†è§’å…ˆéªŒå›¾åƒè¿›è¡Œç»†èŠ‚é‡å»ºï¼Œå¹¶å¼•å…¥åŠ¨æ€ä»»åŠ¡ä¾èµ–æ€§æƒé‡ä»¥å¹³è¡¡å½¢çŠ¶ä¸ç»†èŠ‚ã€‚</li>
<li>é‡‡ç”¨é€æ˜åº¦ã€æ·±åº¦å¤±çœŸå’Œæ³•çº¿å¯¹é½æŸå¤±ä¼˜åŒ–è¡¨é¢ç½‘æ ¼æå–ã€‚</li>
<li>æ–¹æ³•ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯ï¼Œæé«˜äº†è§†å›¾ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3f5b484191f425b45fae0a8610399edd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b06bfe841c6014fd329f738cc71e0a8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46af84bd9f238566fc646dc28ed905d6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cffc384ba3089eb2ae2c5787b33377a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dcc09f8fe93c0d9bedcec151c2dae43.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e22a5bf33737577166968e6b00c4c45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0a89a03413a178b8048f46c3648e21d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="From-large-language-models-to-multimodal-AI-A-scoping-review-on-the-potential-of-generative-AI-in-medicine"><a href="#From-large-language-models-to-multimodal-AI-A-scoping-review-on-the-potential-of-generative-AI-in-medicine" class="headerlink" title="From large language models to multimodal AI: A scoping review on the   potential of generative AI in medicine"></a>From large language models to multimodal AI: A scoping review on the   potential of generative AI in medicine</h2><p><strong>Authors:Lukas Buess, Matthias Keicher, Nassir Navab, Andreas Maier, Soroosh Tayebi Arasteh</strong></p>
<p>Generative artificial intelligence (AI) models, such as diffusion models and OpenAIâ€™s ChatGPT, are transforming medicine by enhancing diagnostic accuracy and automating clinical workflows. The field has advanced rapidly, evolving from text-only large language models for tasks such as clinical documentation and decision support to multimodal AI systems capable of integrating diverse data modalities, including imaging, text, and structured data, within a single model. The diverse landscape of these technologies, along with rising interest, highlights the need for a comprehensive review of their applications and potential. This scoping review explores the evolution of multimodal AI, highlighting its methods, applications, datasets, and evaluation in clinical settings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed, IEEE Xplore, and Web of Science, prioritizing recent studies published up to the end of 2024. After rigorous screening, 144 papers were included, revealing key trends and challenges in this dynamic field. Our findings underscore a shift from unimodal to multimodal approaches, driving innovations in diagnostic support, medical report generation, drug discovery, and conversational AI. However, critical challenges remain, including the integration of heterogeneous data types, improving model interpretability, addressing ethical concerns, and validating AI systems in real-world clinical settings. This review summarizes the current state of the art, identifies critical gaps, and provides insights to guide the development of scalable, trustworthy, and clinically impactful multimodal AI solutions in healthcare. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¨¡å‹ï¼Œå¦‚æ‰©æ•£æ¨¡å‹å’ŒOpenAIçš„ChatGPTï¼Œæ­£åœ¨é€šè¿‡æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œè‡ªåŠ¨åŒ–ä¸´åºŠå·¥ä½œæµç¨‹æ¥æ”¹å˜åŒ»å­¦é¢†åŸŸã€‚è¯¥é¢†åŸŸå‘å±•è¿…é€Ÿï¼Œä»ä»…ç”¨äºä¸´åºŠæ–‡æ¡£å’Œå†³ç­–æ”¯æŒä»»åŠ¡çš„æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿›åŒ–åˆ°èƒ½å¤Ÿåœ¨å•ä¸€æ¨¡å‹ä¸­æ•´åˆåŒ…æ‹¬æˆåƒã€æ–‡æœ¬å’Œç»“æ„æ•°æ®åœ¨å†…çš„å¤šç§æ•°æ®æ¨¡å¼çš„å¤šæ¨¡æ€AIç³»ç»Ÿã€‚è¿™äº›æŠ€æœ¯çš„å¤šæ ·åŒ–æ™¯è§‚ä»¥åŠæ—¥ç›Šå¢é•¿çš„å…´è¶£ï¼Œçªæ˜¾å‡ºéœ€è¦å…¨é¢å›é¡¾å…¶åº”ç”¨å’Œæ½œåŠ›ã€‚æœ¬èŒƒå›´å®¡æŸ¥æ¢è®¨äº†å¤šæ¨¡æ€AIçš„æ¼”å˜ï¼Œçªå‡ºå…¶æ–¹æ³•ã€åº”ç”¨ã€æ•°æ®é›†å’Œåœ¨ä¸´åºŠç¯å¢ƒä¸­çš„è¯„ä¼°ã€‚æˆ‘ä»¬éµå¾ªPRISMA-ScRæŒ‡å—ï¼Œç³»ç»Ÿåœ°æŸ¥è¯¢äº†PubMedã€IEEE Xploreå’ŒWeb of Scienceï¼Œä¼˜å…ˆè€ƒè™‘æˆªè‡³2024å¹´åº•å‘è¡¨çš„æœ€æ–°ç ”ç©¶ã€‚ç»è¿‡ä¸¥æ ¼çš„ç­›é€‰ï¼Œå…±çº³å…¥144ç¯‡è®ºæ–‡ï¼Œæ­ç¤ºäº†è¿™ä¸€åŠ¨æ€é¢†åŸŸçš„å…³é”®è¶‹åŠ¿å’ŒæŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†ä¸€ä¸ªä»å•æ¨¡æ€åˆ°å¤šæ¨¡æ€æ–¹æ³•çš„è½¬å˜ï¼Œæ¨åŠ¨äº†åœ¨è¯Šæ–­æ”¯æŒã€åŒ»ç–—æŠ¥å‘Šç”Ÿæˆã€è¯ç‰©å‘ç°å’Œä¼šè¯AIæ–¹é¢çš„åˆ›æ–°ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•´åˆå¼‚è´¨æ•°æ®ç±»å‹ã€æé«˜æ¨¡å‹å¯è§£é‡Šæ€§ã€è§£å†³ä¼¦ç†é—®é¢˜å’Œåœ¨ç°å®ä¸´åºŠç¯å¢ƒä¸­éªŒè¯AIç³»ç»Ÿã€‚æœ¬ç»¼è¿°æ€»ç»“äº†å½“å‰çš„ç ”ç©¶ç°çŠ¶ï¼Œè¯†åˆ«äº†å…³é”®å·®è·ï¼Œå¹¶æä¾›äº†è§è§£ï¼Œä»¥æŒ‡å¯¼å¼€å‘å¯æ‰©å±•ã€å¯ä¿¡å’Œå¯¹ä¸´åºŠæœ‰å½±å“çš„åŒ»ç–—å¤šæ¨¡æ€AIè§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09242v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹å’ŒOpenAIçš„ChatGPTï¼‰æ­£åœ¨é€šè¿‡æé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œè‡ªåŠ¨åŒ–ä¸´åºŠå·¥ä½œæµç¨‹æ¥æ¨åŠ¨åŒ»å­¦é¢†åŸŸçš„å˜é©ã€‚ä»æœ€åˆçš„ä»…æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘å±•åˆ°ç°åœ¨çš„å¤šæ¨¡æ€AIç³»ç»Ÿï¼Œèƒ½å¤Ÿæ•´åˆå„ç§æ•°æ®ç±»å‹ï¼ˆå¦‚å½±åƒã€æ–‡æœ¬å’Œç»“æ„æ•°æ®ï¼‰ï¼Œå±•ç°äº†AIæŠ€æœ¯çš„å¤šæ ·åŒ–æ ¼å±€åŠå…¶ä¸æ–­å¢é•¿çš„å…´è¶£ã€‚è¿™ç¯‡ç»¼è¿°æ–‡ç« æ¢ç´¢äº†å¤šæ¨¡æ€AIçš„æ¼”å˜ï¼Œå¹¶éµå¾ªPRISMA-ScRæŒ‡å—ç³»ç»Ÿåœ°æŸ¥è¯¢äº†ç›¸å…³æ–‡çŒ®ï¼Œæ­ç¤ºäº†å…³é”®è¶‹åŠ¿å’ŒæŒ‘æˆ˜ã€‚å½“å‰ï¼Œå¤šæ¨¡æ€AIæ­£åœ¨æ¨åŠ¨è¯Šæ–­æ”¯æŒã€åŒ»ç–—æŠ¥å‘Šç”Ÿæˆã€è¯ç‰©å‘ç°å’Œå¯¹è¯å¼AIç­‰é¢†åŸŸçš„åˆ›æ–°ï¼Œä½†ä»é¢ä¸´æ•°æ®æ•´åˆã€æ¨¡å‹è§£é‡Šæ€§ã€ä¼¦ç†é—®é¢˜å’Œç°å®ä¸´åºŠç¯å¢ƒä¸­çš„ç³»ç»ŸéªŒè¯ç­‰æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ­£åœ¨åŒ»å­¦é¢†åŸŸå¸¦æ¥å˜é©ï¼Œæé«˜è¯Šæ–­å‡†ç¡®æ€§å’Œè‡ªåŠ¨åŒ–ä¸´åºŠå·¥ä½œæµç¨‹ã€‚</li>
<li>å¤šæ¨¡æ€AIç³»ç»Ÿèƒ½å¤Ÿæ•´åˆå¤šç§æ•°æ®ç±»å‹ï¼ŒåŒ…æ‹¬å½±åƒã€æ–‡æœ¬å’Œç»“æ„æ•°æ®ã€‚</li>
<li>ç»¼åˆå®¡æŸ¥å¤šæ¨¡æ€AIçš„æ–¹æ³•ã€åº”ç”¨ã€æ•°æ®é›†å’Œä¸´åºŠç¯å¢ƒè¯„ä¼°æ˜¯å¿…è¦çš„ã€‚</li>
<li>ä»å•ä¸€çš„æ¨¡æ€è½¬å‘å¤šæ¨¡æ€æ–¹æ³•æ˜¯ä¸€ä¸ªå…³é”®è¶‹åŠ¿ã€‚</li>
<li>å¤šæ¨¡æ€AIåœ¨è¯Šæ–­æ”¯æŒã€åŒ»ç–—æŠ¥å‘Šç”Ÿæˆã€è¯ç‰©å‘ç°å’Œå¯¹è¯å¼AIç­‰é¢†åŸŸæœ‰åˆ›æ–°åº”ç”¨ã€‚</li>
<li>å¤šæ¨¡æ€AIé¢ä¸´æ•°æ®æ•´åˆã€æ¨¡å‹è§£é‡Šæ€§ã€ä¼¦ç†é—®é¢˜å’Œç°å®ä¸´åºŠç¯å¢ƒä¸­çš„éªŒè¯ç­‰æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09242">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2281d78d1d6570142a9ace2a2dd0a031.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43fdcd378ef18abaca6e2df77729851d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c15b422205db2aff5661fb5ee3dd209.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="E-MD3C-Taming-Masked-Diffusion-Transformers-for-Efficient-Zero-Shot-Object-Customization"><a href="#E-MD3C-Taming-Masked-Diffusion-Transformers-for-Efficient-Zero-Shot-Object-Customization" class="headerlink" title="E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot   Object Customization"></a>E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot   Object Customization</h2><p><strong>Authors:Trung X. Pham, Zhang Kang, Ji Woo Hong, Xuran Zheng, Chang D. Yoo</strong></p>
<p>We propose E-MD3C ($\underline{E}$fficient $\underline{M}$asked $\underline{D}$iffusion Transformer with Disentangled $\underline{C}$onditions and $\underline{C}$ompact $\underline{C}$ollector), a highly efficient framework for zero-shot object image customization. Unlike prior works reliant on resource-intensive Unet architectures, our approach employs lightweight masked diffusion transformers operating on latent patches, offering significantly improved computational efficiency. The framework integrates three core components: (1) an efficient masked diffusion transformer for processing autoencoder latents, (2) a disentangled condition design that ensures compactness while preserving background alignment and fine details, and (3) a learnable Conditions Collector that consolidates multiple inputs into a compact representation for efficient denoising and learning. E-MD3C outperforms the existing approach on the VITON-HD dataset across metrics such as PSNR, FID, SSIM, and LPIPS, demonstrating clear advantages in parameters, memory efficiency, and inference speed. With only $\frac{1}{4}$ of the parameters, our Transformer-based 468M model delivers $2.5\times$ faster inference and uses $\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent diffusion model. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†E-MD3Cï¼ˆå¸¦æœ‰åˆ†ç¦»æ¡ä»¶å’Œç´§å‡‘æ”¶é›†å™¨çš„æœ‰æ•ˆæ©ç æ‰©æ•£è½¬æ¢å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„é›¶æ ·æœ¬ç›®æ ‡å›¾åƒå®šåˆ¶æ¡†æ¶ã€‚ä¸åŒäºä»¥å¾€ä¾èµ–èµ„æºå¯†é›†å‹çš„Unetæ¶æ„çš„ç ”ç©¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è½»é‡çº§çš„æ©ç æ‰©æ•£è½¬æ¢å™¨ï¼Œåœ¨æ½œåœ¨è¡¥ä¸ä¸Šè¿è¡Œï¼Œå¤§å¤§æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚è¯¥æ¡†æ¶é›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šï¼ˆ1ï¼‰ç”¨äºå¤„ç†è‡ªåŠ¨ç¼–ç å™¨æ½œåœ¨æ€§çš„é«˜æ•ˆæ©ç æ‰©æ•£è½¬æ¢å™¨ï¼›ï¼ˆ2ï¼‰åˆ†ç¦»æ¡ä»¶è®¾è®¡ï¼Œç¡®ä¿ç´§å‡‘æ€§ï¼ŒåŒæ—¶ä¿ç•™èƒŒæ™¯å¯¹é½å’Œç»†èŠ‚ï¼›ï¼ˆ3ï¼‰å¯å­¦ä¹ çš„æ¡ä»¶æ”¶é›†å™¨ï¼Œå°†å¤šä¸ªè¾“å…¥æ•´åˆæˆç´§å‡‘è¡¨ç¤ºï¼Œä¾¿äºé«˜æ•ˆå»å™ªå’Œå­¦ä¹ ã€‚åœ¨VITON-HDæ•°æ®é›†ä¸Šï¼ŒE-MD3Cåœ¨PSNRã€FIDã€SSIMå’ŒLPIPSç­‰æŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œåœ¨å‚æ•°ã€å†…å­˜æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦æ–¹é¢æ˜¾ç¤ºå‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚ä»…ä½¿ç”¨å››åˆ†ä¹‹ä¸€çš„å‚æ•°ï¼Œæˆ‘ä»¬åŸºäºTransformerçš„4.68äº¿å‚æ•°æ¨¡å‹å®ç°äº†2.5å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œå¹¶ä½¿ç”¨äº†ä¸‰åˆ†ä¹‹äºŒçš„GPUå†…å­˜ï¼Œç›¸è¾ƒäºåŸºäº17.2äº¿å‚æ•°Unetçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09164v1">PDF</a> 16 pages, 14 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºE-MD3Cæ¡†æ¶ï¼Œä¸€ä¸ªé«˜æ•ˆçš„å¯¹è±¡å›¾åƒå®šåˆ¶æ–¹æ³•ã€‚å®ƒé‡‡ç”¨è½»é‡çº§çš„æœ‰æ©ç æ‰©æ•£å˜å‹å™¨å¤„ç†æ½œåœ¨è¡¥ä¸ï¼Œä¸åŒäºä¾èµ–èµ„æºå¯†é›†å‹Unetæ¶æ„çš„å…ˆå‰æ–¹æ³•ã€‚æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé«˜æ•ˆæ©ç æ‰©æ•£å˜å‹å™¨å¤„ç†è‡ªåŠ¨ç¼–ç å™¨æ½œåœ¨æ•°æ®ã€è§£çº ç¼ æ¡ä»¶è®¾è®¡ç¡®ä¿ç´§å‡‘æ€§å¹¶ä¿ç•™èƒŒæ™¯å¯¹é½å’Œç»†èŠ‚ä»¥åŠå¯å­¦ä¹ çš„æ¡ä»¶æ”¶é›†å™¨åˆå¹¶å¤šä¸ªè¾“å…¥è¿›è¡Œç´§å‡‘è¡¨ç¤ºä»¥å®ç°é«˜æ•ˆå»å™ªå’Œå­¦ä¹ ã€‚E-MD3Cåœ¨VITON-HDæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨PSNRã€FIDã€SSIMå’ŒLPIPSç­‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºä¼˜åŠ¿ï¼ŒåŒæ—¶åœ¨å‚æ•°ã€å†…å­˜æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦æ–¹é¢ä¹Ÿæœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>E-MD3Cæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„é›¶å°„å‡»å¯¹è±¡å›¾åƒå®šåˆ¶æ¡†æ¶ã€‚</li>
<li>ä¸åŒäºä¾èµ–èµ„æºå¯†é›†å‹Unetæ¶æ„çš„æ–¹æ³•ï¼ŒE-MD3Cé‡‡ç”¨è½»é‡çº§æ©ç æ‰©æ•£å˜å‹å™¨å¤„ç†æ½œåœ¨è¡¥ä¸ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¤„ç†è‡ªåŠ¨ç¼–ç å™¨æ½œåœ¨æ•°æ®çš„æ•ˆç‡æ©ç æ‰©æ•£å˜å‹å™¨ã€ç¡®ä¿ç´§å‡‘æ€§çš„è§£çº ç¼ æ¡ä»¶è®¾è®¡ä»¥åŠç”¨äºé«˜æ•ˆå»å™ªå’Œå­¦ä¹ çš„æ¡ä»¶æ”¶é›†å™¨ã€‚</li>
<li>E-MD3Cåœ¨VITON-HDæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè¡¨ç°åœ¨PSNRã€FIDã€SSIMå’ŒLPIPSç­‰æŒ‡æ ‡ä¸Šã€‚</li>
<li>E-MD3Cåœ¨å‚æ•°ä½¿ç”¨ã€å†…å­˜æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>E-MD3Cèƒ½å¤Ÿå®ç°å¿«é€Ÿæ¨ç†ï¼Œä½¿ç”¨å‚æ•°çš„å››åˆ†ä¹‹ä¸€å°±èƒ½è¾¾åˆ°2.5å€çš„é€Ÿåº¦æå‡ï¼Œå¹¶ä¸”ç›¸æ¯”åŸºäºUnetçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ä½¿ç”¨ä¸‰åˆ†ä¹‹äºŒçš„GPUå†…å­˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09164">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69718601c9543e8459f4e5dc15b8e392.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5914e53760d130b43c5bfd5a83c0b79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cde7c32019ecaafc2082dc51afb48e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-590d136f0396b86f5587432dc2b0f8b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0795608458ee906e40cd75456ed1ecf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42656dc0cdbdee659a8debb84bc8103d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-475ccc96bdad7af0a11481b489053edc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="StyleBlend-Enhancing-Style-Specific-Content-Creation-in-Text-to-Image-Diffusion-Models"><a href="#StyleBlend-Enhancing-Style-Specific-Content-Creation-in-Text-to-Image-Diffusion-Models" class="headerlink" title="StyleBlend: Enhancing Style-Specific Content Creation in Text-to-Image   Diffusion Models"></a>StyleBlend: Enhancing Style-Specific Content Creation in Text-to-Image   Diffusion Models</h2><p><strong>Authors:Zichong Chen, Shijin Wang, Yang Zhou</strong></p>
<p>Synthesizing visually impressive images that seamlessly align both text prompts and specific artistic styles remains a significant challenge in Text-to-Image (T2I) diffusion models. This paper introduces StyleBlend, a method designed to learn and apply style representations from a limited set of reference images, enabling content synthesis of both text-aligned and stylistically coherent. Our approach uniquely decomposes style into two components, composition and texture, each learned through different strategies. We then leverage two synthesis branches, each focusing on a corresponding style component, to facilitate effective style blending through shared features without affecting content generation. StyleBlend addresses the common issues of text misalignment and weak style representation that previous methods have struggled with. Extensive qualitative and quantitative comparisons demonstrate the superiority of our approach. </p>
<blockquote>
<p>åœ¨æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œåˆæˆè§†è§‰ä¸Šä»¤äººå°è±¡æ·±åˆ»çš„å›¾åƒï¼Œè¿™äº›å›¾åƒèƒ½å¤Ÿæ— ç¼å¯¹é½æ–‡æœ¬æç¤ºå’Œç‰¹å®šçš„è‰ºæœ¯é£æ ¼ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†StyleBlendæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨ä»æœ‰é™çš„å‚è€ƒå›¾åƒé›†ä¸­å­¦ä¹ å’Œåº”ç”¨é£æ ¼è¡¨ç¤ºï¼Œä»è€Œå®ç°æ–‡æœ¬å¯¹é½å’Œé£æ ¼è¿è´¯çš„å†…å®¹åˆæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ç‹¬ç‰¹åœ°å°†é£æ ¼åˆ†è§£æˆä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šæ„å›¾å’Œçº¹ç†ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½é€šè¿‡ä¸åŒçš„ç­–ç•¥å­¦ä¹ ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸¤ä¸ªåˆæˆåˆ†æ”¯ï¼Œæ¯ä¸ªåˆ†æ”¯ä¸“æ³¨äºç›¸åº”çš„é£æ ¼ç»„ä»¶ï¼Œé€šè¿‡å…±äº«ç‰¹å¾æ¥å®ç°æœ‰æ•ˆçš„é£æ ¼æ··åˆï¼Œè€Œä¸ä¼šå½±å“å†…å®¹ç”Ÿæˆã€‚StyleBlendè§£å†³äº†ä¹‹å‰æ–¹æ³•ä¸­æ–‡æœ¬å¯¹ä¸å‡†å’Œé£æ ¼è¡¨ç°ä¸è¶³ç­‰å¸¸è§é—®é¢˜ã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡å¯¹æ¯”è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09064v1">PDF</a> Accepted to Eurographics 2025. Project page:   <a target="_blank" rel="noopener" href="https://zichongc.github.io/StyleBlend/">https://zichongc.github.io/StyleBlend/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†StyleBlendæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ—¨åœ¨ä»æœ‰é™å‚è€ƒå›¾åƒä¸­å­¦ä¹ å¹¶åº”ç”¨é£æ ¼è¡¨ç¤ºï¼Œå®ç°æ–‡æœ¬å¯¹é½å’Œé£æ ¼ä¸€è‡´çš„å†…å®¹åˆæˆã€‚è¯¥æ–¹æ³•å°†é£æ ¼åˆ†è§£ä¸ºä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼šæ„å›¾å’Œçº¹ç†ï¼Œå¹¶é€šè¿‡ä¸¤ä¸ªåˆæˆåˆ†æ”¯åˆ†åˆ«å¤„ç†æ¯ä¸ªé£æ ¼ç»„ä»¶ï¼Œä»¥é€šè¿‡å…±äº«ç‰¹å¾è¿›è¡Œæœ‰æ•ˆé£æ ¼èåˆï¼Œè€Œä¸ä¼šå½±å“å†…å®¹ç”Ÿæˆã€‚StyleBlendè§£å†³äº†æ–‡æœ¬ä¸å¯¹é½å’Œé£æ ¼è¡¨ç¤ºä¸è¶³ç­‰å¸¸è§é—®é¢˜ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„å®šæ€§å’Œå®šé‡æ¯”è¾ƒè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StyleBlendæ˜¯ä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä»æœ‰é™å‚è€ƒå›¾åƒä¸­å­¦ä¹ å¹¶åº”ç”¨é£æ ¼è¡¨ç¤ºã€‚</li>
<li>è¯¥æ–¹æ³•å°†é£æ ¼åˆ†è§£ä¸ºæ„å›¾å’Œçº¹ç†ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ã€‚</li>
<li>StyleBlendé€šè¿‡ä¸¤ä¸ªåˆæˆåˆ†æ”¯å¤„ç†æ¯ä¸ªé£æ ¼ç»„ä»¶ï¼Œä»¥å®ç°æœ‰æ•ˆé£æ ¼èåˆã€‚</li>
<li>StyleBlendè§£å†³äº†æ–‡æœ¬ä¸å¯¹é½å’Œå¼±é£æ ¼è¡¨ç¤ºç­‰å¸¸è§é—®é¢˜ã€‚</li>
<li>StyleBlendæ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡æ¯”è¾ƒä¸­å‡è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å…±äº«ç‰¹å¾è¿›è¡Œé£æ ¼èåˆï¼ŒåŒæ—¶ä¸å½±å“å†…å®¹ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0abc62a0ed6cf1db69d76f894c2106f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad3d89a2ad1a96570e3dad5f6b1003c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a504c85a0c03481668bfb9be52c2cbd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2a52f3087a1df0aa084f3458a9de3ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-84ab1dda060b11471599730cdcf8fc71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92a492c6b130de193380b34cee4d961a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-Through-a-Global-Lens-Are-They-Culturally-Inclusive"><a href="#Diffusion-Models-Through-a-Global-Lens-Are-They-Culturally-Inclusive" class="headerlink" title="Diffusion Models Through a Global Lens: Are They Culturally Inclusive?"></a>Diffusion Models Through a Global Lens: Are They Culturally Inclusive?</h2><p><strong>Authors:Zahra Bayramli, Ayhan Suleymanzade, Na Min An, Huzama Ahmad, Eunsu Kim, Junyeong Park, James Thorne, Alice Oh</strong></p>
<p>Text-to-image diffusion models have recently enabled the creation of visually compelling, detailed images from textual prompts. However, their ability to accurately represent various cultural nuances remains an open question. In our work, we introduce CultDiff benchmark, evaluating state-of-the-art diffusion models whether they can generate culturally specific images spanning ten countries. We show that these models often fail to generate cultural artifacts in architecture, clothing, and food, especially for underrepresented country regions, by conducting a fine-grained analysis of different similarity aspects, revealing significant disparities in cultural relevance, description fidelity, and realism compared to real-world reference images. With the collected human evaluations, we develop a neural-based image-image similarity metric, namely, CultDiff-S, to predict human judgment on real and generated images with cultural artifacts. Our work highlights the need for more inclusive generative AI systems and equitable dataset representation over a wide range of cultures. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹æœ€è¿‘å·²ç»èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆè§†è§‰å¸å¼•åŠ›å¼ºã€ç»†èŠ‚ä¸°å¯Œçš„å›¾åƒã€‚ç„¶è€Œï¼Œå®ƒä»¬å‡†ç¡®è¡¨ç°å„ç§æ–‡åŒ–ç»†å¾®å·®åˆ«çš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†CultDiffåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹æ˜¯å¦èƒ½ç”Ÿæˆæ¶µç›–åä¸ªå›½å®¶çš„æ–‡åŒ–ç‰¹å®šå›¾åƒã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä¸åŒç›¸ä¼¼åº¦æ–¹é¢è¿›è¡Œç²¾ç»†åˆ†æï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå»ºç­‘ã€æœè£…å’Œé£Ÿå“ç­‰æ–‡åŒ–æ–‡ç‰©æ—¶ç»å¸¸å¤±è´¥ï¼Œå°¤å…¶æ˜¯å¯¹ä»£è¡¨æ€§ä¸è¶³çš„å›½å®¶åœ°åŒºã€‚ä¸çœŸå®ä¸–ç•Œå‚è€ƒå›¾åƒç›¸æ¯”ï¼Œåœ¨æ–‡åŒ–ç›¸å…³æ€§ã€æè¿°ä¿çœŸåº¦å’Œç°å®æ„Ÿæ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ç»“åˆæ”¶é›†çš„äººç±»è¯„ä¼°æ•°æ®ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºç¥ç»çš„å›¾åƒå›¾åƒç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†â€”â€”CultDiff-Sï¼Œç”¨äºé¢„æµ‹äººç±»å¯¹å¸¦æœ‰æ–‡åŒ–æ–‡ç‰©çš„çœŸå®å’Œç”Ÿæˆå›¾åƒçš„åˆ¤æ–­ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†æ›´éœ€è¦åŒ…å®¹æ€§çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç³»ç»Ÿå’Œå„ç§æ–‡åŒ–çš„æ•°æ®é›†ä»£è¡¨å‡è¡¡æ€§çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08914v1">PDF</a> 17 pages, 17 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿä»æ–‡æœ¬æç¤ºç”Ÿæˆè§†è§‰å¸å¼•åŠ›å¼ºã€ç»†èŠ‚ä¸°å¯Œçš„å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å‡†ç¡®è¡¨ç°å„ç§æ–‡åŒ–ç»†å¾®å·®åˆ«æ–¹é¢ä»å­˜åœ¨ç–‘é—®ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†CultDiffåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹æ˜¯å¦èƒ½ç”Ÿæˆæ¶µç›–åä¸ªå›½å®¶çš„æ–‡åŒ–ç‰¹å®šå›¾åƒã€‚æˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå»ºç­‘ã€æœè£…å’Œé£Ÿå“ç­‰æ–‡åŒ–äº§ç‰©æ—¶ç»å¸¸å¤±è´¥ï¼Œç‰¹åˆ«æ˜¯å¯¹æ¬ ä»£è¡¨çš„å›½å®¶åœ°åŒºã€‚é€šè¿‡ä¸åŒç›¸ä¼¼æ€§çš„ç²¾ç»†åˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸æ–‡åŒ–ç›¸å…³æ€§ã€æè¿°ä¿çœŸåº¦å’Œç°å®æ„Ÿä¸çœŸå®ä¸–ç•Œå‚è€ƒå›¾åƒç›¸æ¯”å­˜åœ¨çš„æ˜¾è‘—å·®è·ã€‚æˆ‘ä»¬æ”¶é›†äº†äººç±»è¯„ä¼°ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºç¥ç»çš„å›¾åƒå›¾åƒç›¸ä¼¼åº¦åº¦é‡æ ‡å‡†â€”â€”CultDiff-Sï¼Œç”¨äºé¢„æµ‹äººç±»å¯¹å¸¦æœ‰æ–‡åŒ–äº§ç‰©çš„çœŸå®å’Œç”Ÿæˆå›¾åƒçš„åˆ¤æ–­ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†æ›´å…·åŒ…å®¹æ€§çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ç³»ç»Ÿå’Œå¹¿æ³›æ–‡åŒ–å‡è¡¡æ•°æ®é›†è¡¨ç¤ºçš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆè§†è§‰å¸å¼•åŠ›å¼ºã€ç»†èŠ‚ä¸°å¯Œçš„å›¾åƒã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨å‡†ç¡®è¡¨ç°å„ç§æ–‡åŒ–ç»†å¾®å·®åˆ«æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>CultDiffåŸºå‡†æµ‹è¯•è¢«å¼•å…¥ï¼Œç”¨äºè¯„ä¼°æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ¶µç›–å¤šä¸ªå›½å®¶çš„æ–‡åŒ–ç‰¹å®šå›¾åƒçš„èƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä¸æ–‡åŒ–ç›¸å…³çš„å›¾åƒï¼ˆå¦‚å»ºç­‘ã€æœè£…ã€é£Ÿå“ï¼‰æ—¶å¸¸å¸¸å‡ºç°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¯¹ä»£è¡¨æ€§ä¸è¶³çš„å›½å®¶åœ°åŒºã€‚</li>
<li>å¯¹æ¨¡å‹ç”Ÿæˆçš„å›¾åƒä¸çœŸå®å›¾åƒè¿›è¡Œäº†å¤šæ–¹é¢çš„ç›¸ä¼¼æ€§åˆ†æï¼Œæ­ç¤ºäº†æ–‡åŒ–ç›¸å…³æ€§ã€æè¿°ä¿çœŸåº¦å’Œç°å®æ„Ÿçš„å·®è·ã€‚</li>
<li>é€šè¿‡äººç±»è¯„ä¼°ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºç¥ç»çš„å›¾åƒå›¾åƒç›¸ä¼¼åº¦åº¦é‡æ ‡å‡†â€”â€”CultDiff-Sã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c4fea4e5ab37e9898052b345d20b942.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82de683abe8a0df9cf3ac8f3fab5a6ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c17506259f4b038a50c45c7362f7aec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09d09083d03a086737c7f8abe7aa0b80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18361995f65d369bb2b5d1599cf33b03.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DejAIvu-Identifying-and-Explaining-AI-Art-on-the-Web-in-Real-Time-with-Saliency-Maps"><a href="#DejAIvu-Identifying-and-Explaining-AI-Art-on-the-Web-in-Real-Time-with-Saliency-Maps" class="headerlink" title="DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with   Saliency Maps"></a>DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with   Saliency Maps</h2><p><strong>Authors:Jocelyn Dzuong</strong></p>
<p>The recent surge in advanced generative models, such as diffusion models and generative adversarial networks (GANs), has led to an alarming rise in AI-generated images across various domains on the web. While such technologies offer benefits such as democratizing artistic creation, they also pose challenges in misinformation, digital forgery, and authenticity verification. Additionally, the uncredited use of AI-generated images in media and marketing has sparked significant backlash from online communities. In response to this, we introduce DejAIvu, a Chrome Web extension that combines real-time AI-generated image detection with saliency-based explainability while users browse the web. Using an ONNX-optimized deep learning model, DejAIvu automatically analyzes images on websites such as Google Images, identifies AI-generated content using model inference, and overlays a saliency heatmap to highlight AI-related artifacts. Our approach integrates efficient in-browser inference, gradient-based saliency analysis, and a seamless user experience, ensuring that AI detection is both transparent and interpretable. We also evaluate DejAIvu across multiple pretrained architectures and benchmark datasets, demonstrating high accuracy and low latency, making it a practical and deployable tool for enhancing AI image accountability. The code for this system can be found at <a target="_blank" rel="noopener" href="https://github.com/Noodulz/dejAIvu">https://github.com/Noodulz/dejAIvu</a>. </p>
<blockquote>
<p>è¿‘æœŸå…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ¶Œç°ï¼Œå¯¼è‡´ç½‘ä¸Šå„é¢†åŸŸAIç”Ÿæˆçš„å›¾åƒæ€¥å‰§å¢åŠ ã€‚è™½ç„¶è¿™äº›æŠ€æœ¯æä¾›äº†æ°‘ä¸»åŒ–è‰ºæœ¯åˆ›ä½œç­‰å¥½å¤„ï¼Œä½†å®ƒä»¬ä¹Ÿå¸¦æ¥äº†å…³äºè™šå‡ä¿¡æ¯ã€æ•°å­—ä¼ªé€ å’Œèº«ä»½éªŒè¯çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œåª’ä½“å’Œè¥é”€ä¸­æœªç»æˆæƒä½¿ç”¨çš„AIç”Ÿæˆçš„å›¾åƒå¼•å‘äº†åœ¨çº¿ç¤¾åŒºçš„å¼ºçƒˆåå¼¹ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DejAIvuï¼Œè¿™æ˜¯ä¸€æ¬¾Chromeç½‘é¡µæ‰©å±•ç¨‹åºï¼Œå®ƒç»“åˆäº†å®æ—¶AIç”Ÿæˆçš„å›¾åƒæ£€æµ‹å’Œç”¨æˆ·æµè§ˆç½‘é¡µæ—¶çš„åŸºäºæ˜¾è‘—æ€§çš„è§£é‡Šæ€§ã€‚DejAIvuä½¿ç”¨ä¼˜åŒ–çš„ONNXæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè‡ªåŠ¨åˆ†æç½‘ç«™ä¸Šçš„å›¾åƒï¼ˆå¦‚Google Imagesï¼‰ï¼Œé€šè¿‡æ¨¡å‹æ¨ç†è¯†åˆ«AIç”Ÿæˆçš„å†…å®¹ï¼Œå¹¶é€šè¿‡è¦†ç›–æ˜¾è‘—æ€§çƒ­å›¾æ¥çªå‡ºAIç›¸å…³çš„ä¼ªå½±ã€‚æˆ‘ä»¬çš„æ–¹æ³•èåˆäº†é«˜æ•ˆçš„æµè§ˆå™¨å†…æ¨ç†ã€åŸºäºæ¢¯åº¦çš„æ˜¾è‘—æ€§åˆ†æå’Œæ— ç¼ç”¨æˆ·ä½“éªŒï¼Œç¡®ä¿AIæ£€æµ‹æ—¢é€æ˜åˆæ˜“äºè§£é‡Šã€‚æˆ‘ä»¬è¿˜å¯¹DejAIvuè¿›è¡Œäº†å¤šä¸ªé¢„è®­ç»ƒæ¶æ„å’ŒåŸºå‡†æ•°æ®é›†çš„è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºå…¶é«˜å‡†ç¡®æ€§å’Œä½å»¶è¿Ÿæ€§ï¼Œä½¿å…¶æˆä¸ºå¢å¼ºAIå›¾åƒè´£ä»»æ€§çš„å®ç”¨ä¸”å¯éƒ¨ç½²çš„å·¥å…·ã€‚è¯¥ç³»ç»Ÿçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Noodulz/dejAIvu%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Noodulz/dejAIvuæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08821v1">PDF</a> 5 pages, 3 figures, submitted to IJCAI 2025 demo track</p>
<p><strong>Summary</strong><br>     å…ˆè¿›ç”Ÿæˆæ¨¡å‹å¦‚æ‰©æ•£æ¨¡å‹å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„å…´èµ·ï¼Œå¯¼è‡´ç½‘ä¸Šå„é¢†åŸŸAIç”Ÿæˆçš„å›¾ç‰‡æ¿€å¢ï¼Œå¸¦æ¥è‰ºæœ¯åˆ›ä½œæ°‘ä¸»åŒ–çš„åŒæ—¶ï¼Œä¹Ÿå¼•å‘è™šå‡ä¿¡æ¯ã€æ•°å­—ä¼ªé€ å’Œèº«ä»½è®¤è¯ç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ¨å‡ºDejAIvuæµè§ˆå™¨æ’ä»¶ï¼Œç»“åˆå®æ—¶AIç”Ÿæˆå›¾åƒæ£€æµ‹ä¸åŸºäºæ˜¾è‘—æ€§çš„è§£é‡Šæ€§ï¼Œç”¨æˆ·æµè§ˆç½‘é¡µæ—¶å³å¯ä½¿ç”¨ã€‚å®ƒé€šè¿‡ä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è‡ªåŠ¨åˆ†æç½‘ç«™å›¾åƒï¼Œåˆ©ç”¨æ¨¡å‹æ¨ç†è¯†åˆ«AIç”Ÿæˆå†…å®¹ï¼Œå¹¶å åŠ æ˜¾è‘—æ€§çƒ­å›¾ä»¥çªå‡ºAIç›¸å…³ç—•è¿¹ã€‚è¯¥æ–¹æ¡ˆèåˆé«˜æ•ˆæµè§ˆå™¨æ¨ç†ã€åŸºäºæ¢¯åº¦çš„æ˜¾è‘—æ€§åˆ†æå’Œæ— ç¼ç”¨æˆ·ä½“éªŒï¼Œç¡®ä¿AIæ£€æµ‹æ—¢é€æ˜åˆå…·å¯è§£é‡Šæ€§ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒDejAIvuåœ¨å¤šæ¶æ„å’ŒåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®ç‡å’Œä½å»¶è¿Ÿï¼Œæ˜¯å¢å¼ºAIå›¾åƒè´£ä»»åˆ¶çš„å®ç”¨å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆè¿›ç”Ÿæˆæ¨¡å‹å¦‚æ‰©æ•£æ¨¡å‹å’ŒGANsåœ¨ç½‘ä¸Šç”Ÿæˆå¤§é‡AIå›¾åƒï¼Œå¸¦æ¥è‰ºæœ¯åˆ›ä½œæ°‘ä¸»åŒ–çš„åŒæ—¶ä¹Ÿå¸¦æ¥æŒ‘æˆ˜ï¼Œå¦‚è™šå‡ä¿¡æ¯ã€æ•°å­—ä¼ªé€ å’Œèº«ä»½è®¤è¯é—®é¢˜ã€‚</li>
<li>DejAIvuæ˜¯ä¸€ä¸ªChromeæµè§ˆå™¨æ’ä»¶ï¼Œå¯ä»¥å®æ—¶æ£€æµ‹AIç”Ÿæˆçš„å›¾åƒï¼Œå¹¶ç»“åˆæ˜¾è‘—æ€§åˆ†ææä¾›è§£é‡Šã€‚</li>
<li>DejAIvuä½¿ç”¨ä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è‡ªåŠ¨åˆ†æç½‘ç«™å›¾åƒï¼Œé€šè¿‡æ¨¡å‹æ¨ç†è¯†åˆ«AIç”Ÿæˆå†…å®¹ã€‚</li>
<li>è¯¥æ’ä»¶é€šè¿‡å åŠ æ˜¾è‘—æ€§çƒ­å›¾çªå‡ºAIç›¸å…³ç—•è¿¹ï¼Œå¸®åŠ©ç”¨æˆ·è¯†åˆ«AIç”Ÿæˆçš„å›¾åƒã€‚</li>
<li>DejAIvuç»“åˆå®æ—¶AIæ£€æµ‹ã€æ¢¯åº¦æ˜¾è‘—æ€§åˆ†æå’Œæ— ç¼ç”¨æˆ·ä½“éªŒï¼Œç¡®ä¿AIæ£€æµ‹çš„é€æ˜æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>DejAIvuåœ¨å¤šæ¶æ„å’ŒåŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºé«˜å‡†ç¡®ç‡å’Œä½å»¶è¿Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fae0855267fb50e3bc6e5b29deadd6f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-016b7f2175e761325c065ff02ebecf95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-977467f71d770b9062d079a36fd06fb2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cdddce1b9a31973ca461a799838706b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddb6544746b73eea53e65d1936e47a5d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HistoSmith-Single-Stage-Histology-Image-Label-Generation-via-Conditional-Latent-Diffusion-for-Enhanced-Cell-Segmentation-and-Classification"><a href="#HistoSmith-Single-Stage-Histology-Image-Label-Generation-via-Conditional-Latent-Diffusion-for-Enhanced-Cell-Segmentation-and-Classification" class="headerlink" title="HistoSmith: Single-Stage Histology Image-Label Generation via   Conditional Latent Diffusion for Enhanced Cell Segmentation and   Classification"></a>HistoSmith: Single-Stage Histology Image-Label Generation via   Conditional Latent Diffusion for Enhanced Cell Segmentation and   Classification</h2><p><strong>Authors:Valentina Vadori, Jean-Marie GraÃ¯c, Antonella Peruffo, Livio Finos, Ujwala Kiran Chaudhari, Enrico Grisan</strong></p>
<p>Precise segmentation and classification of cell instances are vital for analyzing the tissue microenvironment in histology images, supporting medical diagnosis, prognosis, treatment planning, and studies of brain cytoarchitecture. However, the creation of high-quality annotated datasets for training remains a major challenge. This study introduces a novel single-stage approach (HistoSmith) for generating image-label pairs to augment histology datasets. Unlike state-of-the-art methods that utilize diffusion models with separate components for label and image generation, our approach employs a latent diffusion model to learn the joint distribution of cellular layouts, classification masks, and histology images. This model enables tailored data generation by conditioning on user-defined parameters such as cell types, quantities, and tissue types. Trained on the Conic H&amp;E histopathology dataset and the Nissl-stained CytoDArk0 dataset, the model generates realistic and diverse labeled samples. Experimental results demonstrate improvements in cell instance segmentation and classification, particularly for underrepresented cell types like neutrophils in the Conic dataset. These findings underscore the potential of our approach to address data scarcity challenges. </p>
<blockquote>
<p>ç²¾ç¡®åˆ†å‰²å’Œåˆ†ç±»ç»†èƒå®ä¾‹å¯¹äºåˆ†æç»„ç»‡å¾®ç¯å¢ƒåœ¨æ˜¾å¾®å›¾åƒä¸­è‡³å…³é‡è¦ï¼Œæ”¯æŒåŒ»å­¦è¯Šæ–­ã€é¢„åã€æ²»ç–—è®¡åˆ’å’Œå¤§è„‘ç»†èƒç»“æ„ç ”ç©¶ã€‚ç„¶è€Œï¼Œåˆ›å»ºé«˜è´¨é‡çš„è®­ç»ƒæ ‡æ³¨æ•°æ®é›†ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å•é˜¶æ®µæ–¹æ³•ï¼ˆHistoSmithï¼‰ï¼Œç”¨äºç”Ÿæˆå›¾åƒæ ‡ç­¾å¯¹ä»¥æ‰©å……æ˜¾å¾®æ•°æ®é›†ã€‚ä¸åŒäºä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ ‡ç­¾å’Œå›¾åƒç”Ÿæˆçš„é«˜çº§æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ ç»†èƒå¸ƒå±€ã€åˆ†ç±»æ©ç å’Œæ˜¾å¾®å›¾åƒçš„è”åˆåˆ†å¸ƒã€‚æ­¤æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ç”¨æˆ·å®šä¹‰çš„å‚æ•°è¿›è¡Œå®šåˆ¶æ•°æ®ç”Ÿæˆï¼Œå¦‚ç»†èƒç±»å‹ã€æ•°é‡å’Œç»„ç»‡ç±»å‹ã€‚è¯¥æ¨¡å‹åœ¨Conic H&amp;Eç—…ç†å­¦æ•°æ®é›†å’ŒNisslæŸ“è‰²çš„CytoDArk0æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯ç”ŸæˆçœŸå®å¤šæ ·çš„æ ‡æ³¨æ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç»†èƒå®ä¾‹åˆ†å‰²å’Œåˆ†ç±»æ–¹é¢æœ‰æ‰€æ”¹å–„ï¼Œå°¤å…¶æ˜¯åœ¨Conicæ•°æ®é›†ä¸­å¯¹ä¸­æ€§ç²’ç»†èƒç­‰ä»£è¡¨æ€§ä¸è¶³çš„ç»†èƒç±»å‹æ›´æ˜¯å¦‚æ­¤ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•è§£å†³æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08754v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„å•é˜¶æ®µæ–¹æ³•ï¼ˆHistoSmithï¼‰ï¼Œç”¨äºç”Ÿæˆå›¾åƒ-æ ‡ç­¾å¯¹ä»¥æ‰©å……ç»„ç»‡å­¦æ•°æ®é›†ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å­¦ä¹ ç»†èƒå¸ƒå±€ã€åˆ†ç±»æ©è†œå’Œç»„ç»‡å­¦å›¾åƒçš„è”åˆåˆ†å¸ƒï¼Œä¸åŒäºç›®å‰æœ€å…ˆç«¯çš„æ–¹æ³•ã€‚é€šè¿‡æ ¹æ®ç”¨æˆ·å®šä¹‰çš„å‚æ•°ï¼ˆå¦‚ç»†èƒç±»å‹ã€æ•°é‡å’Œç»„ç»‡ç±»å‹ï¼‰è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œè¯¥æ¨¡å‹å¯å®ç°æœ‰é’ˆå¯¹æ€§çš„æ•°æ®ç”Ÿæˆã€‚åœ¨Conic H&amp;Eç—…ç†æ•°æ®é›†å’ŒNisslæŸ“è‰²çš„CytoDArk0æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥ç”ŸæˆçœŸå®ä¸”å¤šæ ·çš„å¸¦æ ‡ç­¾æ ·æœ¬ã€‚å¯¹äºä»£è¡¨æ€§ä¸è¶³çš„ç»†èƒç±»å‹ï¼Œå¦‚ä¸­æ€§ç²’ç»†èƒç­‰ï¼Œå…¶åœ¨ç»†èƒå®ä¾‹åˆ†å‰²å’Œåˆ†ç±»æ–¹é¢æœ‰æ˜æ˜¾æ”¹è¿›ã€‚æ­¤ç ”ç©¶å±•ç°å‡ºæ–°æ–¹æ³•è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†æ–°å‹å•é˜¶æ®µæ–¹æ³•ï¼ˆHistoSmithï¼‰ç”Ÿæˆå›¾åƒ-æ ‡ç­¾å¯¹ï¼Œä»¥æ‰©å……ç»„ç»‡å­¦æ•°æ®é›†ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹å­¦ä¹ ç»†èƒå¸ƒå±€ã€åˆ†ç±»æ©è†œå’Œç»„ç»‡å­¦å›¾åƒçš„è”åˆåˆ†å¸ƒã€‚</li>
<li>ç”¨æˆ·å¯æ ¹æ®ç‰¹å®šå‚æ•°ï¼ˆå¦‚ç»†èƒç±»å‹ã€æ•°é‡ã€ç»„ç»‡ç±»å‹ï¼‰è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå®ç°å®šåˆ¶æ•°æ®ç”Ÿæˆã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œèƒ½ç”ŸæˆçœŸå®ä¸”å¤šæ ·çš„å¸¦æ ‡ç­¾æ ·æœ¬ã€‚</li>
<li>æ­¤æ–¹æ³•åœ¨ç»†èƒå®ä¾‹åˆ†å‰²å’Œåˆ†ç±»æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯å¯¹ä»£è¡¨æ€§ä¸è¶³çš„ç»†èƒç±»å‹ã€‚</li>
<li>æ–¹æ³•å…·æœ‰è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08754">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ef6d15804850edfd43bdcfee258548a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c93db541c5267141979cf45374f82185.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d6059529895ba0dc31efeca5c3decc8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9a2344b09d85ddca684202fedf67c9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07c5c80afcdcec4cb95af252d495b713.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-585c2caf59a4e6f2f5e3718de53f32a5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Skrr-Skip-and-Re-use-Text-Encoder-Layers-for-Memory-Efficient-Text-to-Image-Generation"><a href="#Skrr-Skip-and-Re-use-Text-Encoder-Layers-for-Memory-Efficient-Text-to-Image-Generation" class="headerlink" title="Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient   Text-to-Image Generation"></a>Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient   Text-to-Image Generation</h2><p><strong>Authors:Hoigi Seo, Wongi Jeong, Jae-sun Seo, Se Young Chun</strong></p>
<p>Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­çš„å¤§è§„æ¨¡æ–‡æœ¬ç¼–ç å™¨å·²ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ä¸åŒäºä¾èµ–å¤šæ¬¡è¿­ä»£æ­¥éª¤çš„å»å™ªæ¨¡å—ï¼Œæ–‡æœ¬ç¼–ç å™¨ä»…éœ€ä¸€æ¬¡å‰å‘ä¼ é€’å³å¯äº§ç”Ÿæ–‡æœ¬åµŒå…¥ã€‚ç„¶è€Œï¼Œå°½ç®¡æ–‡æœ¬ç¼–ç å™¨å¯¹æ€»æ¨ç†æ—¶é—´å’Œæµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰çš„è´¡çŒ®å¾ˆå°ï¼Œä½†å®ƒä»¬çš„å†…å­˜ä½¿ç”¨éœ€æ±‚å¾ˆé«˜ï¼Œé«˜è¾¾å»å™ªæ¨¡å—çš„å…«å€ã€‚ä¸ºäº†è§£å†³è¿™ç§ä½æ•ˆé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Skip and Re-use layersï¼ˆSkrrï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸ºT2Iæ‰©æ•£æ¨¡å‹ä¸­çš„æ–‡æœ¬ç¼–ç å™¨ä¸“é—¨è®¾è®¡çš„ç®€å•æœ‰æ•ˆçš„å‰ªæç­–ç•¥ã€‚Skrråˆ©ç”¨transformerå—ä¸­çš„å›ºæœ‰å†—ä½™ï¼Œä»¥é’ˆå¯¹T2Iä»»åŠ¡çš„æ–¹å¼é€‰æ‹©æ€§åœ°è·³è¿‡æˆ–é‡ç”¨æŸäº›å±‚ï¼Œä»è€Œåœ¨ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹å‡å°‘å†…å­˜æ¶ˆè€—ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨è¾ƒé«˜çš„ç¨€ç–åº¦ä¸‹ï¼ŒSkrrä¹Ÿèƒ½ä¿æŒä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„å›¾ç‰‡è´¨é‡ï¼Œå¹¶ä¸”ä¼˜äºç°æœ‰çš„å—çŠ¶å‰ªææ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSkrråœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å†…å­˜æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ï¼ŒåŒ…æ‹¬FIDã€CLIPã€DreamSimå’ŒGenEvalåˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08690v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­çš„å¤§è§„æ¨¡æ–‡æœ¬ç¼–ç å™¨åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è™½ç„¶æ–‡æœ¬ç¼–ç å™¨åªéœ€ä¸€æ¬¡å‰å‘ä¼ é€’å³å¯äº§ç”Ÿæ–‡æœ¬åµŒå…¥ï¼Œä¸åŒäºä¾èµ–å¤šæ¬¡è¿­ä»£æ­¥éª¤çš„å»å™ªæ¨¡å—ï¼Œä½†æ–‡æœ¬ç¼–ç å™¨å¯¹å†…å­˜çš„éœ€æ±‚è¾ƒé«˜ï¼Œç”šè‡³é«˜è¾¾å»å™ªæ¨¡å—çš„å…«å€ã€‚ä¸ºè§£å†³è¿™ä¸€æ•ˆç‡é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºSkip and Re-use layersï¼ˆSkrrï¼‰çš„ç®€æ´æœ‰æ•ˆä¿®å‰ªç­–ç•¥ï¼Œä¸“é—¨é’ˆå¯¹T2Iæ‰©æ•£æ¨¡å‹ä¸­çš„æ–‡æœ¬ç¼–ç å™¨è®¾è®¡ã€‚Skrré€šè¿‡é€‰æ‹©æ€§åœ°è·³è¿‡æˆ–é‡ç”¨æŸäº›å±‚ï¼Œåˆ©ç”¨transformerå—ä¸­çš„å›ºæœ‰å†—ä½™ï¼Œä»¥é€‚åˆT2Iä»»åŠ¡çš„æ–¹å¼å‡å°‘å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¸æŸå®³æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒSkrråœ¨é«˜ç¨€ç–åº¦ä¸‹ä¿æŒä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„å›¾ç‰‡è´¨é‡ï¼Œä¼˜äºç°æœ‰çš„å—çŠ¶ä¿®å‰ªæ–¹æ³•ã€‚æ­¤å¤–ï¼ŒSkrråœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„å†…å­˜æ•ˆç‡ï¼Œåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬FIDã€CLIPã€DreamSimå’ŒGenEvalåˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ–‡æœ¬ç¼–ç å™¨åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>æ–‡æœ¬ç¼–ç å™¨å¯¹å†…å­˜éœ€æ±‚è¾ƒé«˜ï¼Œç”šè‡³æ¯”å»å™ªæ¨¡å—é«˜å‡ºå…«å€ã€‚</li>
<li>Skrræ˜¯ä¸€ç§é’ˆå¯¹æ–‡æœ¬ç¼–ç å™¨çš„ä¸“é—¨ä¿®å‰ªç­–ç•¥ï¼Œæ—¨åœ¨æé«˜T2Iæ‰©æ•£æ¨¡å‹çš„å†…å­˜æ•ˆç‡ã€‚</li>
<li>Skrré€šè¿‡é€‰æ‹©æ€§åœ°è·³è¿‡æˆ–é‡ç”¨æŸäº›å±‚æ¥åˆ©ç”¨transformerå—ä¸­çš„å†—ä½™ä¿¡æ¯ã€‚</li>
<li>Skrrèƒ½å¤Ÿåœ¨é«˜ç¨€ç–åº¦ä¸‹ä¿æŒä¸åŸå§‹æ¨¡å‹ç›¸å½“çš„å›¾ç‰‡è´¨é‡ã€‚</li>
<li>Skrråœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬FIDã€CLIPã€DreamSimå’ŒGenEvalåˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f06ea2b91c62088a7a18c69b43d3fb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f99ed331e4f495655dd35f1bd6430ea3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a87b8777b874dacf8a7960f5c0b4afa6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f12a50f5a69efffd7936c577f59ce5d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e18925cef0e5411cf89900acd4d43a8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Training-Free-Safe-Denoisers-for-Safe-Use-of-Diffusion-Models"><a href="#Training-Free-Safe-Denoisers-for-Safe-Use-of-Diffusion-Models" class="headerlink" title="Training-Free Safe Denoisers for Safe Use of Diffusion Models"></a>Training-Free Safe Denoisers for Safe Use of Diffusion Models</h2><p><strong>Authors:Mingyu Kim, Dongjun Kim, Amman Yusuf, Stefano Ermon, Mi Jung Park</strong></p>
<p>There is growing concern over the safety of powerful diffusion models (DMs), as they are often misused to produce inappropriate, not-safe-for-work (NSFW) content or generate copyrighted material or data of individuals who wish to be forgotten. Many existing methods tackle these issues by heavily relying on text-based negative prompts or extensively retraining DMs to eliminate certain features or samples. In this paper, we take a radically different approach, directly modifying the sampling trajectory by leveraging a negation set (e.g., unsafe images, copyrighted data, or datapoints needed to be excluded) to avoid specific regions of data distribution, without needing to retrain or fine-tune DMs. We formally derive the relationship between the expected denoised samples that are safe and those that are not safe, leading to our $\textit{safe}$ denoiser which ensures its final samples are away from the area to be negated. Inspired by the derivation, we develop a practical algorithm that successfully produces high-quality samples while avoiding negation areas of the data distribution in text-conditional, class-conditional, and unconditional image generation scenarios. These results hint at the great potential of our training-free safe denoiser for using DMs more safely. </p>
<blockquote>
<p>å…³äºå¼ºå¤§çš„æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„å®‰å…¨é—®é¢˜æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬ç»å¸¸è¢«è¯¯ç”¨äºäº§ç”Ÿä¸é€‚å½“ã€ä¸é€‚åˆå·¥ä½œåœºåˆï¼ˆNSFWï¼‰çš„å†…å®¹ï¼Œæˆ–ç”Ÿæˆç‰ˆæƒææ–™ï¼Œæˆ–ç”Ÿæˆé‚£äº›å¸Œæœ›è¢«é—å¿˜çš„ä¸ªäººæ•°æ®ã€‚è®¸å¤šç°æœ‰æ–¹æ³•é€šè¿‡ä¾èµ–åŸºäºæ–‡æœ¬çš„åå‘æç¤ºæˆ–å¤§é‡é‡æ–°è®­ç»ƒDMsæ¥æ¶ˆé™¤æŸäº›ç‰¹å¾æˆ–æ ·æœ¬ï¼Œæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡‡å–äº†æˆªç„¶ä¸åŒçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¦å®šé›†ï¼ˆä¾‹å¦‚ä¸å®‰å…¨çš„å›¾åƒã€ç‰ˆæƒæ•°æ®æˆ–éœ€è¦æ’é™¤çš„æ•°æ®ç‚¹ï¼‰ï¼Œç›´æ¥ä¿®æ”¹é‡‡æ ·è½¨è¿¹ï¼Œé¿å…äº†æ•°æ®åˆ†å¸ƒçš„ç‰¹å®šåŒºåŸŸï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒDMsã€‚æˆ‘ä»¬æ­£å¼æ¨å¯¼äº†æœŸæœ›çš„é™å™ªæ ·æœ¬ä¹‹é—´å®‰å…¨å’Œä¸å®‰å…¨æ ·æœ¬çš„å…³ç³»ï¼Œä»è€Œå½¢æˆäº†æˆ‘ä»¬çš„å®‰å…¨é™å™ªå™¨ï¼Œç¡®ä¿æœ€ç»ˆçš„æ ·æœ¬è¿œç¦»éœ€è¦å¦å®šçš„åŒºåŸŸã€‚å—æ¨å¯¼çš„å¯å‘ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å®ç”¨ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨æ–‡æœ¬æ¡ä»¶ã€ç±»åˆ«æ¡ä»¶å’Œæ— æ¡ä»¶å›¾åƒç”Ÿæˆåœºæ™¯ä¸­æˆåŠŸäº§ç”Ÿäº†é«˜è´¨é‡æ ·æœ¬ï¼ŒåŒæ—¶é¿å…äº†æ•°æ®åˆ†å¸ƒçš„å¦å®šåŒºåŸŸã€‚è¿™äº›ç»“æœæš—ç¤ºäº†æˆ‘ä»¬çš„æ— è®­ç»ƒå®‰å…¨é™å™ªå™¨åœ¨ä½¿ç”¨DMæ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08011v2">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹æ­£é¢ä¸´è¶Šæ¥è¶Šå¤šçš„å®‰å…¨æ€§é—®é¢˜ï¼Œå…¶è¯¯ç”¨å¯èƒ½ä¼šç”Ÿæˆä¸é€‚å½“çš„å†…å®¹æˆ–ä¾µçŠ¯ç‰ˆæƒå’Œéšç§æ•°æ®ã€‚ç°æœ‰çš„æ–¹æ³•å¤§å¤šä¾èµ–æ–‡æœ¬è´Ÿé¢æç¤ºæˆ–é‡æ–°è®­ç»ƒæ¨¡å‹æ¥æ¶ˆé™¤é—®é¢˜ç‰¹å¾æˆ–æ ·æœ¬ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¦å®šé›†ç›´æ¥ä¿®æ”¹é‡‡æ ·è½¨è¿¹æ¥é¿å…æ•°æ®åˆ†å¸ƒä¸­çš„ç‰¹å®šåŒºåŸŸï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ã€‚æœ¬æ–‡æ¨å¯¼äº†å®‰å…¨å’Œä¸å®‰å…¨å»å™ªæ ·æœ¬ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å¼€å‘äº†å®ç”¨ç®—æ³•ï¼ŒæˆåŠŸç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼ŒåŒæ—¶é¿å…äº†å¦å®šåŒºåŸŸã€‚è¿™ä¸ºæ‰©æ•£æ¨¡å‹çš„å®‰å…¨ä½¿ç”¨æä¾›äº†å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„å®‰å…¨æ€§æˆä¸ºå…³æ³¨çš„é—®é¢˜ï¼Œå…¶è¯¯ç”¨å¯èƒ½å¯¼è‡´ç”Ÿæˆä¸é€‚å½“å†…å®¹æˆ–ä¾µçŠ¯ç‰ˆæƒå’Œéšç§æ•°æ®ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬è´Ÿé¢æç¤ºæˆ–é‡æ–°è®­ç»ƒæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¦å®šé›†ç›´æ¥ä¿®æ”¹é‡‡æ ·è½¨è¿¹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æ¨å¯¼äº†å®‰å…¨å’Œä¸å®‰å…¨å»å™ªæ ·æœ¬ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å¼€å‘äº†å®ç”¨ç®—æ³•ã€‚</li>
<li>è¯¥ç®—æ³•æˆåŠŸç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼ŒåŒæ—¶é¿å…å¦å®šåŒºåŸŸçš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>æœ¬æ–‡çš„æ–¹æ³•ä¸ºæ‰©æ•£æ¨¡å‹çš„å®‰å…¨ä½¿ç”¨æä¾›äº†å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-343d4acea6f841baf9d7d03261e21452.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e53f662dd1e469348b5b9aabbab3ff25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4aca1988939b95a9558d4560ce0252e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9385144f4fdcbdf028b7ca4b6be6338.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-303b62d6f7a01ddf70a982bf0b7bf411.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3f3b3154fabd339535264a798f82277.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MRS-A-Fast-Sampler-for-Mean-Reverting-Diffusion-based-on-ODE-and-SDE-Solvers"><a href="#MRS-A-Fast-Sampler-for-Mean-Reverting-Diffusion-based-on-ODE-and-SDE-Solvers" class="headerlink" title="MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE   Solvers"></a>MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE   Solvers</h2><p><strong>Authors:Ao Li, Wei Fang, Hongbo Zhao, Le Lu, Ge Yang, Minfeng Xu</strong></p>
<p>In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation. </p>
<blockquote>
<p>åœ¨æ‰©æ•£æ¨¡å‹çš„åº”ç”¨ä¸­ï¼Œå¯æ§ç”Ÿæˆå…·æœ‰å®é™…æ„ä¹‰ï¼Œä½†ä¹Ÿå…·æœ‰æŒ‘æˆ˜æ€§ã€‚å½“å‰çš„å¯æ§ç”Ÿæˆæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä¿®æ”¹æ‰©æ•£æ¨¡å‹çš„è¯„åˆ†å‡½æ•°ï¼Œè€Œå‡å€¼å›å½’ï¼ˆMRï¼‰æ‰©æ•£åˆ™ç›´æ¥ä¿®æ”¹éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰çš„ç»“æ„ï¼Œä½¿å¾—èå…¥å›¾åƒæ¡ä»¶æ›´åŠ ç®€å•è‡ªç„¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ— è®­ç»ƒå¿«é€Ÿé‡‡æ ·å™¨å¹¶ä¸ç›´æ¥é€‚ç”¨äºMRæ‰©æ•£ã€‚å› æ­¤ï¼ŒMRæ‰©æ•£éœ€è¦æ•°ç™¾ä¸ªåŠŸèƒ½è¯„ä¼°ï¼ˆNFEï¼‰æ¥è·å¾—é«˜è´¨é‡æ ·æœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºMRSï¼ˆMRé‡‡æ ·å™¨ï¼‰çš„æ–°ç®—æ³•ï¼Œä»¥å‡å°‘MRæ‰©æ•£çš„é‡‡æ ·NFEã€‚æˆ‘ä»¬è§£å†³äº†åå‘æ—¶é—´SDEå’Œä¸MRæ‰©æ•£ç›¸å…³çš„æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆPF-ODEï¼‰ï¼Œå¹¶å¾—å‡ºåŠè§£æè§£ã€‚è¿™äº›è§£å†³æ–¹æ¡ˆç”±ä¸€ä¸ªåˆ†æå‡½æ•°å’Œä¸€ä¸ªç”±ç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„ç§¯åˆ†ç»„æˆã€‚åŸºäºè¿™ä¸ªè§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯ä»¥ä»¥æ›´å°‘çš„æ­¥éª¤ç”Ÿæˆé«˜è´¨é‡çš„æ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦è®­ç»ƒï¼Œæ”¯æŒåŒ…æ‹¬å™ªå£°é¢„æµ‹ã€æ•°æ®é¢„æµ‹å’Œé€Ÿåº¦é¢„æµ‹åœ¨å†…çš„æ‰€æœ‰ä¸»æµå‚æ•°åŒ–æ–¹æ³•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMRé‡‡æ ·å™¨åœ¨10ä¸ªä¸åŒçš„å›¾åƒæ¢å¤ä»»åŠ¡ä¸­ä¿æŒäº†é«˜é‡‡æ ·è´¨é‡ï¼Œå¹¶å®ç°äº†10åˆ°20å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬çš„ç®—æ³•åŠ é€Ÿäº†MRæ‰©æ•£çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä½¿å…¶åœ¨å¯æ§ç”Ÿæˆä¸­æ›´åŠ å®ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07856v2">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å¯æ§ç”Ÿæˆæ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•MRSï¼Œç”¨äºè§£å†³MRæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·é—®é¢˜ã€‚è¯¥ç®—æ³•åŸºäºåŠè§£æè§£ï¼Œé€šè¿‡è§£å†³åå‘æ—¶é—´SDEå’Œæ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆPF-ODEï¼‰æ¥ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼Œæ— éœ€è®­ç»ƒä¸”æ”¯æŒä¸»æµå‚æ•°åŒ–æ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼ŒMR Sampleråœ¨åä¸ªä¸åŒçš„å›¾åƒæ¢å¤ä»»åŠ¡ä¸Šæé«˜äº†é‡‡æ ·é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„é‡‡æ ·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¯æ§ç”Ÿæˆæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¿®æ”¹æ‰©æ•£æ¨¡å‹çš„å¾—åˆ†å‡½æ•°ã€‚</li>
<li>MRæ‰©æ•£æ–¹æ³•ç›´æ¥ä¿®æ”¹éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰çš„ç»“æ„ï¼Œä½¿å›¾åƒæ¡ä»¶çš„èå…¥æ›´ç®€å•è‡ªç„¶ã€‚</li>
<li>å½“å‰çš„æ— è®­ç»ƒå¿«é€Ÿé‡‡æ ·å™¨ä¸èƒ½ç›´æ¥åº”ç”¨äºMRæ‰©æ•£ã€‚</li>
<li>æ–°ç®—æ³•MRSè§£å†³MRæ‰©æ•£çš„é‡‡æ ·é—®é¢˜ï¼ŒåŸºäºåŠè§£æè§£ã€‚</li>
<li>MRSè§£å†³åå‘æ—¶é—´SDEå’Œæ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆPF-ODEï¼‰æ¥ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚</li>
<li>MR Samplerç®—æ³•æ— éœ€è®­ç»ƒï¼Œæ”¯æŒå¤šç§ä¸»æµå‚æ•°åŒ–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db639627caff48a89c722ad6aa7e0df3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-715f6f4c7a6a31bc86cffcf0024918fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10991a13c40b06e24c616fa3a30f9b1d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VIIS-Visible-and-Infrared-Information-Synthesis-for-Severe-Low-light-Image-Enhancement"><a href="#VIIS-Visible-and-Infrared-Information-Synthesis-for-Severe-Low-light-Image-Enhancement" class="headerlink" title="VIIS: Visible and Infrared Information Synthesis for Severe Low-light   Image Enhancement"></a>VIIS: Visible and Infrared Information Synthesis for Severe Low-light   Image Enhancement</h2><p><strong>Authors:Chen Zhao, Mengyuan Yu, Fan Yang, Peiguang Jing</strong></p>
<p>Images captured in severe low-light circumstances often suffer from significant information absence. Existing singular modality image enhancement methods struggle to restore image regions lacking valid information. By leveraging light-impervious infrared images, visible and infrared image fusion methods have the potential to reveal information hidden in darkness. However, they primarily emphasize inter-modal complementation but neglect intra-modal enhancement, limiting the perceptual quality of output images. To address these limitations, we propose a novel task, dubbed visible and infrared information synthesis (VIIS), which aims to achieve both information enhancement and fusion of the two modalities. Given the difficulty in obtaining ground truth in the VIIS task, we design an information synthesis pretext task (ISPT) based on image augmentation. We employ a diffusion model as the framework and design a sparse attention-based dual-modalities residual (SADMR) conditioning mechanism to enhance information interaction between the two modalities. This mechanism enables features with prior knowledge from both modalities to adaptively and iteratively attend to each modalityâ€™s information during the denoising process. Our extensive experiments demonstrate that our model qualitatively and quantitatively outperforms not only the state-of-the-art methods in relevant fields but also the newly designed baselines capable of both information enhancement and fusion. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Chenz418/VIIS">https://github.com/Chenz418/VIIS</a>. </p>
<blockquote>
<p>åœ¨ä¸¥é‡ä½å…‰ç¯å¢ƒä¸‹æ•æ‰çš„å›¾åƒé€šå¸¸ç¼ºä¹é‡è¦ä¿¡æ¯ã€‚ç°æœ‰çš„å•ä¸€æ¨¡æ€å›¾åƒå¢å¼ºæ–¹æ³•åœ¨æ¢å¤ç¼ºä¹æœ‰æ•ˆä¿¡æ¯å›¾åƒåŒºåŸŸæ–¹é¢è¡¨ç°å›°éš¾ã€‚é€šè¿‡åˆ©ç”¨ä¸å—å…‰çº¿å½±å“çš„çº¢å¤–å›¾åƒï¼Œå¯è§å…‰å’Œçº¢å¤–å›¾åƒèåˆæ–¹æ³•å…·æœ‰æ­ç¤ºéšè—åœ¨é»‘æš—ä¸­çš„ä¿¡æ¯çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸»è¦å¼ºè°ƒè·¨æ¨¡æ€äº’è¡¥ï¼Œå´å¿½ç•¥äº†æ¨¡æ€å†…å¢å¼ºï¼Œä»è€Œé™åˆ¶äº†è¾“å‡ºå›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼Œç§°ä¸ºå¯è§å…‰å’Œçº¢å¤–ä¿¡æ¯åˆæˆï¼ˆVIISï¼‰ï¼Œæ—¨åœ¨å®ç°ä¸¤ä¸ªæ¨¡æ€çš„ä¿¡æ¯å¢å¼ºå’Œèåˆã€‚è€ƒè™‘åˆ°VIISä»»åŠ¡ä¸­è·å–çœŸå®æ ·æœ¬çš„éš¾åº¦ï¼Œæˆ‘ä»¬åŸºäºå›¾åƒå¢å¼ºè®¾è®¡äº†ä¸€ä¸ªä¿¡æ¯åˆæˆé¢„è®­ç»ƒä»»åŠ¡ï¼ˆISPTï¼‰ã€‚æˆ‘ä»¬ä»¥æ‰©æ•£æ¨¡å‹ä¸ºæ¡†æ¶ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºç¨€ç–æ³¨æ„åŠ›çš„åŒæ¨¡æ€æ®‹å·®ï¼ˆSADMRï¼‰æ¡ä»¶æœºåˆ¶ï¼Œä»¥å¢å¼ºä¸¤ä¸ªæ¨¡æ€ä¹‹é—´çš„ä¿¡æ¯äº¤äº’ã€‚è¯¥æœºåˆ¶ä½¿å¾—å…·æœ‰æ¥è‡ªä¸¤ä¸ªæ¨¡æ€çš„å…ˆéªŒçŸ¥è¯†çš„ç‰¹å¾èƒ½å¤Ÿåœ¨å»å™ªè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°è¿­ä»£å…³æ³¨æ¯ä¸ªæ¨¡æ€çš„ä¿¡æ¯ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä»…åœ¨ç›¸å…³é¢†åŸŸå®šæ€§å’Œå®šé‡ä¸Šè¶…è¶Šäº†æœ€æ–°æ–¹æ³•ï¼Œè€Œä¸”åœ¨æ–°è®¾è®¡çš„åŸºçº¿æ¨¡å‹ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œè¿™äº›åŸºçº¿æ¨¡å‹èƒ½å¤ŸåŒæ—¶å®ç°ä¿¡æ¯å¢å¼ºå’Œèåˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Chenz418/VIIS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Chenz418/VIISæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13655v2">PDF</a> Accepted to WACV 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ä½å…‰ç¯å¢ƒä¸‹å›¾åƒä¿¡æ¯ç¼ºå¤±çš„é—®é¢˜ï¼Œç°æœ‰å›¾åƒå¢å¼ºæ–¹æ³•éš¾ä»¥æ¢å¤ç¼ºå¤±ä¿¡æ¯ã€‚é€šè¿‡ç»“åˆçº¢å¤–å›¾åƒï¼Œå¯è§å…‰å’Œçº¢å¤–å›¾åƒèåˆæ–¹æ³•èƒ½å¤Ÿæ­ç¤ºéšè—ä¿¡æ¯ã€‚ä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è·¨æ¨¡æ€äº’è¡¥ï¼Œå¿½è§†å•æ¨¡æ€å†…å¢å¼ºï¼Œå½±å“è¾“å‡ºå›¾åƒæ„ŸçŸ¥è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæå‡ºå¯è§å…‰å’Œçº¢å¤–ä¿¡æ¯åˆæˆï¼ˆVIISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å®ç°ä¿¡æ¯å¢å¼ºä¸èåˆã€‚ä¸ºè§£å†³VIISä»»åŠ¡ç¼ºä¹çœŸå®æ ‡æ³¨çš„é—®é¢˜ï¼Œè®¾è®¡åŸºäºå›¾åƒå¢å¼ºçš„ä¿¡æ¯åˆæˆé¢„è®­ç»ƒä»»åŠ¡ï¼ˆISPTï¼‰ã€‚é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œè®¾è®¡ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶çš„åŒæ¨¡æ€æ®‹å·®ï¼ˆSADMRï¼‰æ¡ä»¶æœºåˆ¶ï¼Œå¢å¼ºä¸¤æ¨¡æ€é—´çš„ä¿¡æ¯äº¤äº’ã€‚å®éªŒè¯æ˜ï¼Œæ¨¡å‹åœ¨ç›¸å…³é¢†åŸŸçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•å’Œæ–°è®¾è®¡çš„åŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½å…‰ç¯å¢ƒä¸‹æ•è·çš„å›¾åƒç»å¸¸ç¼ºä¹ä¿¡æ¯ï¼Œç°æœ‰å›¾åƒå¢å¼ºæ–¹æ³•éš¾ä»¥å®Œå…¨æ¢å¤è¿™äº›ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡ç»“åˆçº¢å¤–å›¾åƒï¼Œå¯è§å…‰å’Œçº¢å¤–å›¾åƒèåˆæ–¹æ³•å¯ä»¥æ­ç¤ºéšè—åœ¨é»‘æš—ä¸­çš„ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è·¨æ¨¡æ€çš„äº’è¡¥ä¿¡æ¯ï¼Œä½†å¿½è§†äº†å•æ¨¡æ€å†…çš„å¢å¼ºï¼Œå½±å“äº†è¾“å‡ºå›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>æå‡ºäº†å¯è§å…‰å’Œçº¢å¤–ä¿¡æ¯åˆæˆï¼ˆVIISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨åŒæ—¶å®ç°ä¿¡æ¯å¢å¼ºå’Œèåˆã€‚</li>
<li>ä¸ºäº†è§£å†³VIISä»»åŠ¡ä¸­ç¼ºä¹çœŸå®æ ‡æ³¨çš„é—®é¢˜ï¼Œè®¾è®¡äº†ä¸€ä¸ªåŸºäºå›¾åƒå¢å¼ºçš„ä¿¡æ¯åˆæˆé¢„è®­ç»ƒä»»åŠ¡ï¼ˆISPTï¼‰ã€‚</li>
<li>é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œå¹¶è®¾è®¡äº†SADMRæ¡ä»¶æœºåˆ¶æ¥å¢å¼ºå¯è§å…‰å’Œçº¢å¤–å›¾åƒä¹‹é—´çš„ä¿¡æ¯äº¤äº’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7a90ca7c80bdd41bd780a150480ad564.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9e1c6deda7aa914b89f17d5bc207f03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e5f7e8ca525fa0277d0790ae1dc9fc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d667ae4a456350597b52a1556071a8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b1a32ad7254776f74a9080364fe4183.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42094ded1273105cc0528506dc8ddb15.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Noise-Matters-Diffusion-Model-based-Urban-Mobility-Generation-with-Collaborative-Noise-Priors"><a href="#Noise-Matters-Diffusion-Model-based-Urban-Mobility-Generation-with-Collaborative-Noise-Priors" class="headerlink" title="Noise Matters: Diffusion Model-based Urban Mobility Generation with   Collaborative Noise Priors"></a>Noise Matters: Diffusion Model-based Urban Mobility Generation with   Collaborative Noise Priors</h2><p><strong>Authors:Yuheng Zhang, Yuan Yuan, Jingtao Ding, Jian Yuan, Yong Li</strong></p>
<p>With global urbanization, the focus on sustainable cities has largely grown, driving research into equity, resilience, and urban planning, which often relies on mobility data. The rise of web-based apps and mobile devices has provided valuable user data for mobility-related research. However, real-world mobility data is costly and raises privacy concerns. To protect privacy while retaining key features of real-world movement, the demand for synthetic data has steadily increased. Recent advances in diffusion models have shown great potential for mobility trajectory generation due to their ability to model randomness and uncertainty. However, existing approaches often directly apply identically distributed (i.i.d.) noise sampling from image generation techniques, which fail to account for the spatiotemporal correlations and social interactions that shape urban mobility patterns. In this paper, we propose CoDiffMob, a diffusion model for urban mobility generation with collaborative noise priors, we emphasize the critical role of noise in diffusion models for generating mobility data. By leveraging both individual movement characteristics and population-wide dynamics, we construct novel collaborative noise priors that provide richer and more informative guidance throughout the generation process. Extensive experiments demonstrate the superiority of our method, with generated data accurately capturing both individual preferences and collective patterns, achieving an improvement of over 32%. Furthermore, it can effectively replace web-derived mobility data to better support downstream applications, while safeguarding user privacy and fostering a more secure and ethical web. This highlights its tremendous potential for applications in sustainable city-related research. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/CoDiffMob">https://github.com/tsinghua-fib-lab/CoDiffMob</a>. </p>
<blockquote>
<p>éšç€å…¨çƒåŸå¸‚åŒ–è¿›ç¨‹çš„æ¨è¿›ï¼Œå¯¹å¯æŒç»­åŸå¸‚å‘å±•çš„å…³æ³¨æ—¥ç›Šå¢åŠ ï¼Œæ¨åŠ¨äº†å…³äºå…¬å¹³ã€éŸ§æ€§å’ŒåŸå¸‚è§„åˆ’çš„ç ”ç©¶ï¼Œè¿™äº›ç ”ç©¶é€šå¸¸ä¾èµ–äºå‡ºè¡Œæ•°æ®ã€‚ç½‘ç»œåº”ç”¨ç¨‹åºå’Œç§»åŠ¨è®¾å¤‡çš„æ™®åŠä¸ºä¸å‡ºè¡Œç›¸å…³çš„ç ”ç©¶æä¾›äº†å®è´µçš„ç”¨æˆ·æ•°æ®ã€‚ç„¶è€Œï¼ŒçœŸå®ä¸–ç•Œçš„å‡ºè¡Œæ•°æ®æˆæœ¬é«˜æ˜‚ï¼Œå¹¶å¼•å‘éšç§æ‹…å¿§ã€‚ä¸ºäº†åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶ä¿ç•™ç°å®ä¸–ç•Œçš„ç§»åŠ¨å…³é”®ç‰¹å¾ï¼Œå¯¹åˆæˆæ•°æ®çš„éœ€æ±‚ç¨³æ­¥å¢åŠ ã€‚æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•åœ¨ç”Ÿæˆç§»åŠ¨è½¨è¿¹æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿæ¨¡æ‹Ÿéšæœºæ€§å’Œä¸ç¡®å®šæ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ç›´æ¥ä»å›¾åƒç”ŸæˆæŠ€æœ¯ä¸­é‡‡ç”¨ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰å™ªå£°é‡‡æ ·ï¼Œå¿½ç•¥äº†æ—¶ç©ºå…³è”å’Œç¤¾ä¼šäº’åŠ¨å¯¹åŸå¸‚å‡ºè¡Œæ¨¡å¼çš„å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CoDiffMobï¼Œè¿™æ˜¯ä¸€ç§å¸¦æœ‰åä½œå™ªå£°å…ˆéªŒçš„åŸå¸‚ç§»åŠ¨æ€§ç”Ÿæˆæ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬å¼ºè°ƒäº†å™ªå£°åœ¨æ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆç§»åŠ¨æ€§æ•°æ®çš„å…³é”®ä½œç”¨ã€‚é€šè¿‡åˆ©ç”¨ä¸ªä½“ç§»åŠ¨ç‰¹å¾å’Œæ•´ä½“äººå£åŠ¨æ€ï¼Œæˆ‘ä»¬æ„å»ºäº†æ–°å‹åä½œå™ªå£°å…ˆéªŒï¼Œä¸ºç”Ÿæˆè¿‡ç¨‹æä¾›æ›´ä¸°å¯Œã€æ›´å…·ä¿¡æ¯é‡çš„æŒ‡å¯¼ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œç”Ÿæˆçš„æ•°æ®å‡†ç¡®æ•æ‰äº†ä¸ªä½“åå¥½å’Œé›†ä½“æ¨¡å¼ï¼Œå®ç°äº†è¶…è¿‡32%çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥æœ‰æ•ˆåœ°æ›¿ä»£ç½‘ç»œæ´¾ç”Ÿçš„ç§»åŠ¨æ•°æ®ï¼Œä»¥æ›´å¥½åœ°æ”¯æŒä¸‹æ¸¸åº”ç”¨ç¨‹åºï¼ŒåŒæ—¶ä¿æŠ¤ç”¨æˆ·éšç§å¹¶ä¿ƒè¿›æ›´å®‰å…¨ã€æ›´é“å¾·çš„äº’è”ç½‘å‘å±•ã€‚è¿™å‡¸æ˜¾äº†å…¶åœ¨å¯æŒç»­åŸå¸‚ç›¸å…³ç ”ç©¶ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/CoDiffMob%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/tsinghua-fib-lab/CoDiffMobè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05000v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å…¨çƒåŸå¸‚åŒ–çš„æ¨è¿›ï¼Œå¯¹å¯æŒç»­åŸå¸‚çš„ç ”ç©¶æ—¥ç›Šé‡è§†ï¼Œæ¶‰åŠå…¬å¹³ã€éŸ§æ€§å’ŒåŸå¸‚è§„åˆ’ç­‰é¢†åŸŸã€‚ç”±äºçœŸå®ä¸–ç•Œç§»åŠ¨æ•°æ®æˆæœ¬é«˜æ˜‚ä¸”å­˜åœ¨éšç§æ‹…å¿§ï¼Œåˆæˆæ•°æ®çš„éœ€æ±‚é€æ¸å¢é•¿ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCoDiffMobçš„æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç”ŸæˆåŸå¸‚ç§»åŠ¨æ€§æ•°æ®ï¼Œé‡‡ç”¨åä½œå™ªå£°å…ˆéªŒæŠ€æœ¯ï¼Œå¼ºè°ƒå™ªå£°åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„é‡è¦æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆä¸ªä½“ç§»åŠ¨ç‰¹æ€§å’Œç¾¤ä½“åŠ¨æ€ï¼Œæ„å»ºä¸°å¯Œçš„åä½œå™ªå£°å…ˆéªŒï¼ŒæŒ‡å¯¼æ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®æ•æ‰ä¸ªä½“åå¥½å’Œé›†ä½“æ¨¡å¼ï¼Œæœ‰æ•ˆæ›¿ä»£ç½‘ç»œæ´¾ç”Ÿç§»åŠ¨æ•°æ®ï¼Œä¿æŠ¤ç”¨æˆ·éšç§å¹¶ä¿ƒè¿›æ›´å®‰å…¨ã€æ›´é“å¾·çš„ç½‘ç»œå»ºè®¾ã€‚è¿™ä¸ºå¯æŒç»­åŸå¸‚ç›¸å…³ç ”ç©¶æä¾›äº†å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨çƒåŸå¸‚åŒ–æ¨åŠ¨äº†å¯æŒç»­åŸå¸‚ç ”ç©¶çš„å¢é•¿ï¼Œæ¶‰åŠå…¬å¹³ã€éŸ§æ€§å’ŒåŸå¸‚è§„åˆ’ç­‰é¢†åŸŸã€‚</li>
<li>çœŸå®ä¸–ç•Œç§»åŠ¨æ•°æ®æˆæœ¬é«˜æ˜‚ä¸”å­˜åœ¨éšç§æ‹…å¿§ï¼Œåˆæˆæ•°æ®éœ€æ±‚å¢åŠ ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹æ–¹æ³•åœ¨å¤„ç†åŸå¸‚ç§»åŠ¨è½¨è¿¹ç”Ÿæˆæ—¶ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘æ—¶ç©ºå…³è”å’Œç¤¾ä¼šäº’åŠ¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„CoDiffMobæ¨¡å‹åˆ©ç”¨åä½œå™ªå£°å…ˆéªŒæŠ€æœ¯ï¼Œå¼ºè°ƒå™ªå£°åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>CoDiffMobæ¨¡å‹ç»“åˆä¸ªä½“ç§»åŠ¨ç‰¹æ€§å’Œç¾¤ä½“åŠ¨æ€ï¼Œæ„å»ºä¸°å¯Œçš„åä½œå™ªå£°å…ˆéªŒã€‚</li>
<li>å®éªŒè¡¨æ˜CoDiffMobæ¨¡å‹èƒ½å¤Ÿå‡†ç¡®æ•æ‰ä¸ªä½“åå¥½å’Œé›†ä½“æ¨¡å¼ï¼Œæœ‰æ•ˆæ›¿ä»£ç½‘ç»œæ´¾ç”Ÿç§»åŠ¨æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd7b9a0a071f9c7ef031749142d6ae3b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-557af3866edb5ffa917c88c62dc65bc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71adda8c50e45b2e51971b228ee43e26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a7afe05a4fffcf18103abfbec90a6e4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="SSP-IR-Semantic-and-Structure-Priors-for-Diffusion-based-Realistic-Image-Restoration"><a href="#SSP-IR-Semantic-and-Structure-Priors-for-Diffusion-based-Realistic-Image-Restoration" class="headerlink" title="SSP-IR: Semantic and Structure Priors for Diffusion-based Realistic   Image Restoration"></a>SSP-IR: Semantic and Structure Priors for Diffusion-based Realistic   Image Restoration</h2><p><strong>Authors:Yuhong Zhang, Hengsheng Zhang, Zhengxue Cheng, Rong Xie, Li Song, Wenjun Zhang</strong></p>
<p>Realistic image restoration is a crucial task in computer vision, and diffusion-based models for image restoration have garnered significant attention due to their ability to produce realistic results. Restoration can be seen as a controllable generation conditioning on priors. However, due to the severity of image degradation, existing diffusion-based restoration methods cannot fully exploit priors from low-quality images and still have many challenges in perceptual quality, semantic fidelity, and structure accuracy. Based on the challenges, we introduce a novel image restoration method, SSP-IR. Our approach aims to fully exploit semantic and structure priors from low-quality images to guide the diffusion model in generating semantically faithful and structurally accurate natural restoration results. Specifically, we integrate the visual comprehension capabilities of Multimodal Large Language Models (explicit) and the visual representations of the original image (implicit) to acquire accurate semantic prior. To extract degradation-independent structure prior, we introduce a Processor with RGB and FFT constraints to extract structure prior from the low-quality images, guiding the diffusion model and preventing the generation of unreasonable artifacts. Lastly, we employ a multi-level attention mechanism to integrate the acquired semantic and structure priors. The qualitative and quantitative results demonstrate that our method outperforms other state-of-the-art methods overall on both synthetic and real-world datasets. Our project page is <a target="_blank" rel="noopener" href="https://zyhrainbow.github.io/projects/SSP-IR">https://zyhrainbow.github.io/projects/SSP-IR</a>. </p>
<blockquote>
<p>å›¾åƒä¿®å¤æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒä¿®å¤ç”±äºå…¶èƒ½ç”Ÿæˆé€¼çœŸçš„ç»“æœè€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ä¿®å¤å¯ä»¥è¢«è§†ä¸ºåŸºäºå…ˆéªŒçš„å—æ§ç”Ÿæˆã€‚ç„¶è€Œï¼Œç”±äºå›¾åƒé€€åŒ–çš„ä¸¥é‡æ€§ï¼Œç°æœ‰çš„åŸºäºæ‰©æ•£çš„ä¿®å¤æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨æ¥è‡ªä½è´¨é‡å›¾åƒçš„å…ˆéªŒä¿¡æ¯ï¼Œå¹¶ä¸”åœ¨æ„ŸçŸ¥è´¨é‡ã€è¯­ä¹‰ä¿çœŸåº¦å’Œç»“æ„å‡†ç¡®æ€§æ–¹é¢ä»ç„¶é¢ä¸´è®¸å¤šæŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒä¿®å¤æ–¹æ³•SSP-IRã€‚æˆ‘ä»¬çš„æ–¹æ³•æ—¨åœ¨å……åˆ†åˆ©ç”¨æ¥è‡ªä½è´¨é‡å›¾åƒçš„è¯­ä¹‰å’Œç»“æ„å…ˆéªŒï¼Œä»¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¯­ä¹‰ä¸Šå¿ å®ã€ç»“æ„ä¸Šå‡†ç¡®çš„è‡ªç„¶ä¿®å¤ç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç»“åˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›ï¼ˆæ˜¾å¼ï¼‰å’ŒåŸå§‹å›¾åƒçš„è§†è§‰è¡¨ç¤ºï¼ˆéšå¼ï¼‰æ¥è·å–å‡†ç¡®çš„è¯­ä¹‰å…ˆéªŒã€‚ä¸ºäº†æå–ä¸é€€åŒ–æ— å…³çš„ç»“æ„å…ˆéªŒï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¸¦æœ‰RGBå’ŒFFTçº¦æŸçš„å¤„ç†å™¨ï¼Œä»ä½è´¨é‡å›¾åƒä¸­æå–ç»“æ„å…ˆéªŒï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ï¼Œé˜²æ­¢ç”Ÿæˆä¸åˆç†çš„ä¼ªå½±ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šçº§æ³¨æ„åŠ›æœºåˆ¶æ¥æ•´åˆè·å¾—çš„è¯­ä¹‰å’Œç»“æ„å…ˆéªŒã€‚å®šæ€§å’Œå®šé‡ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æ€»ä½“è¡¨ç°å‡ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯ <a target="_blank" rel="noopener" href="https://zyhrainbow.github.io/projects/SSP-IR%E3%80%82">https://zyhrainbow.github.io/projects/SSP-IRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.03635v2">PDF</a> To be published in IEEE TCSVT</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒæ¢å¤æ–¹æ³•SSP-IRã€‚è¯¥æ–¹æ³•æ—¨åœ¨ä»ä½è´¨é‡å›¾åƒä¸­å……åˆ†æå–è¯­ä¹‰å’Œç»“æ„å…ˆéªŒï¼Œä»¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¯­ä¹‰ä¸Šå¿ å®ã€ç»“æ„ä¸Šå‡†ç¡®çš„è‡ªç„¶æ¢å¤ç»“æœã€‚é€šè¿‡æ•´åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›å’ŒåŸå§‹å›¾åƒçš„è§†è§‰è¡¨ç¤ºï¼Œè·å–å‡†ç¡®çš„è¯­ä¹‰å…ˆéªŒã€‚å¼•å…¥å¤„ç†å™¨æå–ç»“æ„å…ˆéªŒï¼Œé‡‡ç”¨RGBå’ŒFFTçº¦æŸï¼Œé˜²æ­¢ç”Ÿæˆä¸åˆç†çš„ä¼ªå½±ã€‚æœ€åï¼Œé‡‡ç”¨å¤šçº§æ³¨æ„åŠ›æœºåˆ¶æ•´åˆè·å–çš„è¯­ä¹‰å’Œç»“æ„å…ˆéªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šå‡ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒæ¢å¤ä¸­å› å…¶èƒ½äº§ç”ŸçœŸå®ç»“æœè€Œå¤‡å—å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒæ¢å¤ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ„ŸçŸ¥è´¨é‡ã€è¯­ä¹‰ä¿çœŸåº¦å’Œç»“æ„å‡†ç¡®æ€§ã€‚</li>
<li>SSP-IRæ–¹æ³•æ—¨åœ¨ä»ä½è´¨é‡å›¾åƒä¸­å……åˆ†æå–è¯­ä¹‰å’Œç»“æ„å…ˆéªŒã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›å’ŒåŸå§‹å›¾åƒçš„è§†è§‰è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥å¤„ç†å™¨æå–ç»“æ„å…ˆéªŒï¼Œé‡‡ç”¨RGBå’ŒFFTçº¦æŸé˜²æ­¢ç”Ÿæˆä¸åˆç†ä¼ªå½±ã€‚</li>
<li>é‡‡ç”¨å¤šçº§åˆ«æ³¨æ„åŠ›æœºåˆ¶æ•´åˆè¯­ä¹‰å’Œç»“æ„å…ˆéªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.03635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fe507442ef4ce370e6e1f076ab9015f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d429575795091006c3574e2f3ebc1166.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68f0b01ac7585329726cc47a0ba06b19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92cb4e757389bd6699479376737869e6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-15/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-15/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-4ccec59a98e20bb864e5b8a69ef88467.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-17  Utilizing 3D Fast Spin Echo Anatomical Imaging to Reduce the Number of   Contrast Preparations in $T_{1Ï}$ Quantification of Knee Cartilage Using   Learning-Based Methods
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-15/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-de9db48bec8c7b0f5d028c8d47fb5204.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-15  DenseSplat Densifying Gaussian Splatting SLAM with Neural Radiance   Prior
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">13921.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
