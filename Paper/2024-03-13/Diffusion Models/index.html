<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-13  Bridging Different Language Models and Generative Vision Models for   Text-to-Image Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-024cf388128af8fcbb5768c6b5cbd193.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-03-13
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    61 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº Googleçš„å¤§è¯­è¨€æ¨¡å‹<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-03-13-æ›´æ–°"><a href="#2024-03-13-æ›´æ–°" class="headerlink" title="2024-03-13 æ›´æ–°"></a>2024-03-13 æ›´æ–°</h1><h2 id="Bridging-Different-Language-Models-and-Generative-Vision-Models-for-Text-to-Image-Generation"><a href="#Bridging-Different-Language-Models-and-Generative-Vision-Models-for-Text-to-Image-Generation" class="headerlink" title="Bridging Different Language Models and Generative Vision Models for   Text-to-Image Generation"></a>Bridging Different Language Models and Generative Vision Models for   Text-to-Image Generation</h2><p><strong>Authors:Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong</strong></p>
<p>Text-to-image generation has made significant advancements with the introduction of text-to-image diffusion models. These models typically consist of a language model that interprets user prompts and a vision model that generates corresponding images. As language and vision models continue to progress in their respective domains, there is a great potential in exploring the replacement of components in text-to-image diffusion models with more advanced counterparts. A broader research objective would therefore be to investigate the integration of any two unrelated language and generative vision models for text-to-image generation. In this paper, we explore this objective and propose LaVi-Bridge, a pipeline that enables the integration of diverse pre-trained language models and generative vision models for text-to-image generation. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and plug-and-play approach without requiring modifications to the original weights of the language and vision models. Our pipeline is compatible with various language models and generative vision models, accommodating different structures. Within this framework, we demonstrate that incorporating superior modules, such as more advanced language models or generative vision models, results in notable improvements in capabilities like text alignment or image quality. Extensive evaluations have been conducted to verify the effectiveness of LaVi-Bridge. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ShihaoZhaoZSH/LaVi-Bridge">https://github.com/ShihaoZhaoZSH/LaVi-Bridge</a>. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07860v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ï¼Œæ¢ç´¢ç”¨æ›´å…ˆè¿›çš„è¯­è¨€å’Œå¤§è§„æ¨¡è§†è§‰æ¨¡å‹æ›¿æ¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç»„æˆéƒ¨åˆ†ï¼Œä»¥æé«˜ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ï¼Œå°†è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹é›†æˆåˆ°ä¸€ä¸ªç®¡é“ä¸­ã€‚</li>
<li>LaVi-Bridgeç®¡é“ä½¿é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹èƒ½å¤Ÿçµæ´»åœ°é›†æˆã€‚</li>
<li>ä½¿ç”¨LaVi-Bridgeå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè€Œæ— éœ€ä¿®æ”¹æ¨¡å‹çš„åŸå§‹æƒé‡ã€‚</li>
<li>LaVi-Bridgeä¸å„ç§è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹å…¼å®¹ï¼Œå¯é€‚åº”ä¸åŒçš„ç»“æ„ã€‚</li>
<li>å°†æ›´é«˜çº§çš„è¯­è¨€æ¨¡å‹æˆ–ç”Ÿæˆè§†è§‰æ¨¡å‹ä¸LaVi-Bridgeé›†æˆå¯ä»¥æé«˜æ–‡æœ¬å¯¹é½æˆ–å›¾åƒè´¨é‡ã€‚</li>
<li>å¹¿æ³›çš„è¯„ä¼°éªŒè¯äº†LaVi-Bridgeçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ShihaoZhaoZSH/LaVi-Bridge%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/ShihaoZhaoZSH/LaVi-Bridgeè·å¾—ã€‚</a></li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>æ ‡é¢˜ï¼šBridging Different Language Models and Generative Vision Models for Text-to-Image Generation</li>
<li>ä½œè€…ï¼šShihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, Kwan-Yee K. Wong</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šé¦™æ¸¯å¤§å­¦</li>
<li>å…³é”®è¯ï¼šDiffusion model, Text-to-image generation</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.07860</li>
<li>
<p>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šå¸¸ç”±ä¸€ä¸ªè§£é‡Šç”¨æˆ·æç¤ºçš„è¯­è¨€æ¨¡å‹å’Œä¸€ä¸ªç”Ÿæˆç›¸åº”å›¾åƒçš„è§†è§‰æ¨¡å‹ç»„æˆã€‚éšç€è¯­è¨€å’Œè§†è§‰æ¨¡å‹åœ¨å…¶å„è‡ªé¢†åŸŸä¸æ–­è¿›æ­¥ï¼Œæ¢ç´¢ç”¨æ›´å…ˆè¿›çš„æ¨¡å‹æ›¿æ¢æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ç»„ä»¶å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚å› æ­¤ï¼Œä¸€ä¸ªæ›´å¹¿æ³›çš„ç ”ç©¶ç›®æ ‡æ˜¯ç ”ç©¶å°†ä»»ä½•ä¸¤ä¸ªä¸ç›¸å…³çš„è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹é›†æˆç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•å’Œé—®é¢˜ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼šéœ€è¦ä¿®æ”¹è¯­è¨€å’Œè§†è§‰æ¨¡å‹çš„åŸå§‹æƒé‡ï¼Œçµæ´»æ€§å·®ï¼Œæ— æ³•é€‚åº”ä¸åŒçš„ç»“æ„ã€‚
ï¼ˆ3ï¼‰æœ¬æ–‡æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº† LaVi-Bridgeï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¯æŒå°†ä¸åŒçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹é›†æˆç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç®¡é“ã€‚é€šè¿‡åˆ©ç”¨ LoRA å’Œé€‚é…å™¨ï¼ŒLaVi-Bridge æä¾›äº†ä¸€ç§çµæ´»ä¸”å³æ’å³ç”¨çš„æ–¹æ³•ï¼Œæ— éœ€ä¿®æ”¹è¯­è¨€å’Œè§†è§‰æ¨¡å‹çš„åŸå§‹æƒé‡ã€‚æˆ‘ä»¬çš„ç®¡é“ä¸å„ç§è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹å…¼å®¹ï¼Œå¯é€‚åº”ä¸åŒçš„ç»“æ„ã€‚
ï¼ˆ4ï¼‰å®éªŒç»“æœï¼šåœ¨è¯¥æ¡†æ¶å†…ï¼Œæˆ‘ä»¬è¯æ˜äº†ç»“åˆæ›´é«˜çº§çš„æ¨¡å—ï¼ˆä¾‹å¦‚æ›´é«˜çº§çš„è¯­è¨€æ¨¡å‹æˆ–ç”Ÿæˆè§†è§‰æ¨¡å‹ï¼‰å¯ä»¥æ˜¾ç€æé«˜æ–‡æœ¬å¯¹é½æˆ–å›¾åƒè´¨é‡ç­‰èƒ½åŠ›ã€‚å·²ç»è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°æ¥éªŒè¯ LaVi-Bridge çš„æœ‰æ•ˆæ€§ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š
(1): é‡‡ç”¨æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨LoRAå’Œé€‚é…å™¨å°†ä¸åŒè¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹é›†æˆï¼Œæ— éœ€ä¿®æ”¹åŸå§‹æƒé‡ã€‚
(2): è¯­è¨€æ¨¡å‹å’Œè§†è§‰æ¨¡å‹çš„äº¤äº’é€šè¿‡äº¤å‰æ³¨æ„åŠ›å±‚å®ç°ï¼ŒLoRAå¼•å…¥å¯è®­ç»ƒå‚æ•°ï¼Œé€‚é…å™¨ä¿ƒè¿›å¯¹é½ã€‚
(3): ä¿æŒè¯­è¨€å’Œè§†è§‰æ¨¡å‹å›ºå®šï¼Œä»…è®­ç»ƒ LoRA å’Œé€‚é…å™¨å‚æ•°ï¼Œé€‚åº”å„ç§è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹ç»“æ„ã€‚</p>
</li>
</ol>
<p>8.ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºLaVi-Bridgeï¼Œå®ƒé€‚ç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚LaVi-Bridgeèƒ½å¤Ÿè¿æ¥å„ç§è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚å®ƒå…·æœ‰é«˜åº¦é€šç”¨æ€§ï¼Œå¯ä»¥é€‚åº”ä¸åŒçš„ç»“æ„ã€‚LaVi-Bridgeè¿˜å¾ˆçµæ´»ï¼Œå› ä¸ºå®ƒå¯ä»¥åœ¨ä¸ä¿®æ”¹è¯­è¨€å’Œè§†è§‰æ¨¡å‹çš„åŸå§‹æƒé‡çš„åŸºç¡€ä¸Šå®ç°é›†æˆã€‚ç›¸åï¼Œå®ƒåˆ©ç”¨LoRAå’Œé€‚é…å™¨è¿›è¡Œå¾®è°ƒã€‚æ­¤å¤–ï¼Œåœ¨LaVi-Bridgeä¸‹ï¼Œä½¿ç”¨æ›´é«˜çº§çš„è¯­è¨€æˆ–è§†è§‰æ¨¡å‹å¯ä»¥å¢å¼ºæ–‡æœ¬ç†è§£èƒ½åŠ›æˆ–å›¾åƒè´¨é‡ã€‚è¿™äº›ä¼˜åŠ¿ä½¿å¾—LaVi-Bridgeèƒ½å¤Ÿå¸®åŠ©æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œä»¥å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹ä»»åŠ¡å…·æœ‰é‡è¦çš„ç ”ç©¶ä»·å€¼ï¼Œéœ€è¦è¿›ä¸€æ­¥æ¢ç´¢ã€‚LaVi-Bridgeå…è®¸è®¾è®¡å¸ˆã€è‰ºæœ¯å®¶å’Œå…¶ä»–ç”¨æˆ·çµæ´»åœ°åˆ©ç”¨ç°æœ‰çš„è¯­è¨€å’Œè§†è§‰æ¨¡å‹æ¥å®ç°ä»–ä»¬çš„åˆ›ä½œç›®æ ‡ã€‚é¿å…æ»¥ç”¨å¹¶å‡è½»æ½œåœ¨çš„è´Ÿé¢ç¤¾ä¼šå½±å“è‡³å…³é‡è¦ã€‚åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œé‡è¦çš„æ˜¯è¦æ ‡å‡†åŒ–å…¶ä½¿ç”¨ï¼Œæé«˜æ¨¡å‹é€æ˜åº¦ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šLaVi-Bridgeæå‡ºäº†ä¸€ç§æ— éœ€ä¿®æ”¹è¯­è¨€å’Œè§†è§‰æ¨¡å‹åŸå§‹æƒé‡å³å¯å°†ä¸åŒè¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹é›†æˆåˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„ç®¡é“ã€‚å®ƒåˆ©ç”¨LoRAå’Œé€‚é…å™¨åœ¨è¯­è¨€æ¨¡å‹å’Œè§†è§‰æ¨¡å‹ä¹‹é—´å»ºç«‹äº†å¯è®­ç»ƒçš„è¿æ¥ï¼Œä»è€Œå®ç°äº†çµæ´»ä¸”å³æ’å³ç”¨çš„é›†æˆã€‚
æ€§èƒ½ï¼šå®éªŒç»“æœè¡¨æ˜ï¼ŒLaVi-Bridgeèƒ½å¤Ÿæ˜¾ç€æé«˜æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ï¼Œä¾‹å¦‚æ–‡æœ¬å¯¹é½æˆ–å›¾åƒè´¨é‡ã€‚é€šè¿‡ç»“åˆæ›´é«˜çº§çš„è¯­è¨€æ¨¡å‹æˆ–ç”Ÿæˆè§†è§‰æ¨¡å‹ï¼ŒLaVi-Bridgeå¯ä»¥åˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚
å·¥ä½œé‡ï¼šLaVi-Bridgeçš„å®ç°ç›¸å¯¹ç®€å•ï¼Œåªéœ€è¦ä¿®æ”¹å°‘é‡ä»£ç å³å¯ã€‚å®ƒä¸å„ç§è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆè§†è§‰æ¨¡å‹å…¼å®¹ï¼Œæ— éœ€å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œé‡å¤§ä¿®æ”¹ã€‚æ­¤å¤–ï¼ŒLaVi-Bridgeçš„è®­ç»ƒè¿‡ç¨‹æ˜¯é«˜æ•ˆä¸”ç¨³å®šçš„ï¼Œå¯ä»¥åœ¨åˆç†çš„æ—¶é—´å†…æ”¶æ•›ã€‚</p>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9a99e7e4272d38b21737a5c189b093a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57e7ed33741950bb510e73e466f417ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28925ac45e275e43cd57ccf0dd749a77.jpg" align="middle">
</details>




<h2 id="Quantifying-and-Mitigating-Privacy-Risks-for-Tabular-Generative-Models"><a href="#Quantifying-and-Mitigating-Privacy-Risks-for-Tabular-Generative-Models" class="headerlink" title="Quantifying and Mitigating Privacy Risks for Tabular Generative Models"></a>Quantifying and Mitigating Privacy Risks for Tabular Generative Models</h2><p><strong>Authors:Chaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. PÃ©rez, Marten van Dijk, Lydia Y. Chen</strong></p>
<p>Synthetic data from generative models emerges as the privacy-preserving data-sharing solution. Such a synthetic data set shall resemble the original data without revealing identifiable private information. The backbone technology of tabular synthesizers is rooted in image generative models, ranging from Generative Adversarial Networks (GANs) to recent diffusion models. Recent prior work sheds light on the utility-privacy tradeoff on tabular data, revealing and quantifying privacy risks on synthetic data. We first conduct an exhaustive empirical analysis, highlighting the utility-privacy tradeoff of five state-of-the-art tabular synthesizers, against eight privacy attacks, with a special focus on membership inference attacks. Motivated by the observation of high data quality but also high privacy risk in tabular diffusion, we propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which is composed of an autoencoder network to encode the tabular data and a latent diffusion model to synthesize the latent tables. Following the emerging f-DP framework, we apply DP-SGD to train the auto-encoder in combination with batch clipping and use the separation value as the privacy metric to better capture the privacy gain from DP algorithms. Our empirical evaluation demonstrates that DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee while also significantly enhancing the utility of synthetic data. Specifically, compared to other DP-protected tabular generative models, DP-TLDM improves the synthetic quality by an average of 35% in data resemblance, 15% in the utility for downstream tasks, and 50% in data discriminability, all while preserving a comparable level of privacy risk. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07842v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç”Ÿæˆæ¨¡å‹ä¸­çš„åˆæˆæ•°æ®æ˜¯ä¿æŠ¤æ•°æ®éšç§çš„æ•°æ®å…±äº«è§£å†³æ–¹æ¡ˆï¼Œæ—¢è¦è¿‘ä¼¼åŸå§‹æ•°æ®ï¼Œåˆä¸èƒ½æ³„éœ²å¯è¯†åˆ«çš„ç§äººä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆæˆæ•°æ®ç”Ÿæˆå™¨æŠ€æœ¯æºäºå›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå¦‚ GAN å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è¡¨æ ¼æ‰©æ•£æ¨¡å‹åœ¨æ•°æ®è´¨é‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨éšç§æ–¹é¢å­˜åœ¨é£é™©ã€‚</li>
<li>DP-TLDMï¼ˆå·®å¼‚éšç§è¡¨æ ¼æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼‰é€šè¿‡ç¼–ç å™¨ç½‘ç»œå’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥åˆæˆæ•°æ®ã€‚</li>
<li>DP-SGDã€æ‰¹æ¬¡è£å‰ªå’Œåˆ†ç¦»å€¼å¯ç”¨äºå¢å¼ºéšç§ä¿éšœã€‚</li>
<li>DP-TLDM å¯æœ‰æ•ˆæå‡åˆæˆæ•°æ®è´¨é‡å’Œæ•ˆç”¨ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„éšç§é£é™©ã€‚</li>
<li>DP-TLDM å¯å°†æ•°æ®ç›¸ä¼¼æ€§æé«˜ 35%ã€ä¸‹æ¸¸ä»»åŠ¡æ•ˆç”¨æé«˜ 15%ã€æ•°æ®å¯åŒºåˆ†æ€§æé«˜ 50%ã€‚</li>
<li>DP-TLDM åœ¨ä¿æŠ¤éšç§çš„åŒæ—¶æé«˜äº†æ•°æ®æ•ˆç”¨ï¼Œä¼˜äºå…¶ä»– DP è¡¨æ ¼ç”Ÿæˆæ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>è®ºæ–‡æ ‡é¢˜ï¼šé‡åŒ–å’Œç¼“è§£è¡¨æ ¼ç”Ÿæˆæ¨¡å‹çš„éšç§é£é™©</li>
<li>ä½œè€…ï¼šChaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. PÃ©rez, Marten van Dijk, Lydia Y. Chen</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä»£å°”å¤«ç‰¹ç†å·¥å¤§å­¦</li>
<li>å…³é”®è¯ï¼šåˆæˆè¡¨æ ¼æ•°æ®ã€æ·±åº¦ç”Ÿæˆæ¨¡å‹ã€å·®åˆ†éšç§</li>
<li>è®ºæ–‡é“¾æ¥ï¼šNone
Github é“¾æ¥ï¼šNone</li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šåˆæˆæ•°æ®ä»ç”Ÿæˆæ¨¡å‹ä¸­è·å–ï¼Œä½œä¸ºä¸€ç§ä¿æŠ¤éšç§çš„æ•°æ®å…±äº«è§£å†³æ–¹æ¡ˆã€‚æ­¤ç±»åˆæˆæ•°æ®é›†åº”ç±»ä¼¼äºåŸå§‹æ•°æ®ï¼Œä¸”ä¸æ³„éœ²å¯è¯†åˆ«çš„éšç§ä¿¡æ¯ã€‚è¡¨æ ¼åˆæˆå™¨çš„éª¨å¹²æŠ€æœ¯æ ¹æ¤äºå›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œä»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) åˆ°æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹ã€‚æœ€è¿‘çš„å…ˆå‰å·¥ä½œé˜æ˜äº†è¡¨æ ¼æ•°æ®ä¸Šçš„æ•ˆç”¨éšç§æƒè¡¡ï¼Œæ­ç¤ºå¹¶é‡åŒ–äº†åˆæˆæ•°æ®çš„éšç§é£é™©ã€‚ç„¶è€Œï¼Œé‡ç‚¹ä»…é™äºå°‘æ•°éšç§æ”»å‡»å’Œè¡¨æ ¼åˆæˆå™¨ï¼Œç‰¹åˆ«æ˜¯åŸºäº GAN çš„åˆæˆå™¨ï¼Œå¹¶ä¸”å¿½ç•¥äº†æˆå‘˜æ¨æ–­æ”»å‡»å’Œé˜²å¾¡ç­–ç•¥ï¼Œå³å·®åˆ†éšç§ã€‚
ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•åŠé—®é¢˜ï¼šä¸ºäº†å¼¥åˆå·®è·ï¼Œæˆ‘ä»¬è§£å†³äº†ä¸¤ä¸ªç ”ç©¶é—®é¢˜ï¼š(i) è€ƒè™‘åˆ°æ›´å¹¿æ³›çš„åˆæˆå™¨é›†åˆåŠå…¶å¯¹æˆå‘˜æ¨æ–­æ”»å‡»çš„æ€§èƒ½ï¼Œå“ªç§ç±»å‹çš„è¡¨æ ¼ç”Ÿæˆæ¨¡å‹å¯ä»¥å®ç°æ›´å¥½çš„æ•ˆç”¨éšç§æƒè¡¡ï¼›(ii) é€šè¿‡å·®åˆ†éšç§éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³• (DP-SGD) å¯ä»¥è·å¾—ä»€ä¹ˆé¢å¤–çš„éšç§ä¿è¯ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œè¯¦å°½çš„ç»éªŒåˆ†æï¼Œé‡ç‚¹å…³æ³¨æˆå‘˜æ¨æ–­æ”»å‡»ï¼Œé’ˆå¯¹å…«ç§éšç§æ”»å‡»ï¼Œå¼ºè°ƒäº†äº”ç§æœ€å…ˆè¿›çš„è¡¨æ ¼åˆæˆå™¨çš„æ•ˆç”¨éšç§æƒè¡¡ã€‚
ï¼ˆ3ï¼‰æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šå—è¡¨æ ¼æ‰©æ•£ä¸­æ•°æ®è´¨é‡é«˜ä½†éšç§é£é™©ä¹Ÿé«˜çš„è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº† DP-TLDMï¼Œå·®åˆ†éšç§è¡¨æ ¼æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œå®ƒç”±ä¸€ä¸ªè‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œç»„æˆï¼Œç”¨äºå¯¹è¡¨æ ¼æ•°æ®è¿›è¡Œç¼–ç ï¼Œä»¥åŠä¸€ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºåˆæˆæ½œåœ¨è¡¨æ ¼ã€‚éµå¾ªæ–°å…´çš„ ğ‘“-DP æ¡†æ¶ï¼Œæˆ‘ä»¬å°† DP-SGD åº”ç”¨äºè®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ï¼Œç»“åˆæ‰¹å¤„ç†å‰ªè£ï¼Œå¹¶ä½¿ç”¨è¿™äº›åˆ†ç¦»å€¼ä½œä¸ºéšç§åº¦é‡ï¼Œä»¥æ›´å¥½åœ°æ•æ‰ DP ç®—æ³•çš„éšç§æ”¶ç›Šã€‚
ï¼ˆ4ï¼‰æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šæˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒDP-TLDM èƒ½å¤Ÿå®ç°æœ‰æ„ä¹‰çš„ç†è®ºéšç§ä¿è¯ï¼ŒåŒæ—¶è¿˜æ˜¾ç€æé«˜åˆæˆæ•°æ®çš„æ•ˆç”¨ã€‚å…·ä½“è€Œè¨€ï¼Œä¸å…¶ä»– DP ä¿æŠ¤è¡¨æ ¼ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒDP-TLDM å°†åˆæˆè´¨é‡æé«˜äº†å¹³å‡ 35%ï¼Œä¸‹æ¸¸ä»»åŠ¡çš„æ•ˆç”¨æé«˜äº† 15%ï¼Œæ•°æ®å¯åŒºåˆ†åº¦æé«˜äº† 50%ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“æ°´å¹³çš„éšç§é£é™©ã€‚</li>
</ol>
<p><strong>æ–¹æ³•</strong></p>
<p>(1) <strong>éšç§æ”»å‡»åˆ†æï¼š</strong>é’ˆå¯¹ 5 ç§æœ€å…ˆè¿›çš„è¡¨æ ¼åˆæˆå™¨å’Œ 8 ç§éšç§æ”»å‡»ï¼Œè¿›è¡Œè¯¦å°½çš„ç»éªŒåˆ†æï¼Œé‡ç‚¹å…³æ³¨æˆå‘˜æ¨æ–­æ”»å‡»ï¼Œå¼ºè°ƒå…¶æ•ˆç”¨éšç§æƒè¡¡ã€‚</p>
<p>(2) <strong>DP-TLDM æ¨¡å‹ï¼š</strong>æå‡ºå·®åˆ†éšç§è¡¨æ ¼æ½œåœ¨æ‰©æ•£æ¨¡å‹ (DP-TLDM)ï¼Œç”±è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œå’Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ç»„æˆï¼Œéµå¾ª f-DP æ¡†æ¶ï¼Œå°† DP-SGD åº”ç”¨äºè®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ï¼Œå¹¶ç»“åˆæ‰¹å¤„ç†å‰ªè£ã€‚</p>
<p>(3) <strong>éšç§åº¦é‡ï¼š</strong>ä½¿ç”¨åˆ†ç¦»å€¼ä½œä¸ºéšç§åº¦é‡ï¼Œæ›´å¥½åœ°æ•æ‰ DP ç®—æ³•çš„éšç§æ”¶ç›Šã€‚</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬ç ”ç©¶å·¥ä½œé€šè¿‡é‡åŒ–å’Œç¼“è§£è¡¨æ ¼ç”Ÿæˆæ¨¡å‹çš„éšç§é£é™©ï¼Œä¸ºåˆæˆè¡¨æ ¼æ•°æ®çš„å®‰å…¨å…±äº«æä¾›äº†ç†è®ºæŒ‡å¯¼å’ŒæŠ€æœ¯æ”¯æŒã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å·®åˆ†éšç§è¡¨æ ¼æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆDP-TLDMï¼‰ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†åˆæˆæ•°æ®çš„æ•ˆç”¨å’Œéšç§é£é™©ã€‚</li>
<li>é‡‡ç”¨ f-DP æ¡†æ¶å’Œæ‰¹å¤„ç†å‰ªè£æŠ€æœ¯ï¼Œå¯¹è‡ªåŠ¨ç¼–ç å™¨ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹è¿›è¡Œéšç§ä¿æŠ¤ï¼Œæé«˜äº†åˆæˆæ•°æ®çš„éšç§ä¿è¯ã€‚</li>
<li>ä½¿ç”¨åˆ†ç¦»å€¼ä½œä¸ºéšç§åº¦é‡ï¼Œæ›´å‡†ç¡®åœ°æ•æ‰ DP ç®—æ³•çš„éšç§æ”¶ç›Šã€‚
æ€§èƒ½ï¼š</li>
<li>ä¸å…¶ä»– DP ä¿æŠ¤è¡¨æ ¼ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒDP-TLDM å°†åˆæˆè´¨é‡æé«˜äº†å¹³å‡ 35%ï¼Œä¸‹æ¸¸ä»»åŠ¡çš„æ•ˆç”¨æé«˜äº† 15%ï¼Œæ•°æ®å¯åŒºåˆ†åº¦æé«˜äº† 50%ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“æ°´å¹³çš„éšç§é£é™©ã€‚</li>
<li>åœ¨å¹¿æ³›çš„è¡¨æ ¼ç”Ÿæˆæ¨¡å‹å’Œéšç§æ”»å‡»ç»„åˆä¸Šè¿›è¡Œäº†è¯¦å°½çš„ç»éªŒåˆ†æï¼Œä¸ºé€‰æ‹©åˆé€‚çš„åˆæˆå™¨å’Œç¼“è§£éšç§é£é™©æä¾›äº†æŒ‡å¯¼ã€‚
å·¥ä½œé‡ï¼š</li>
<li>æœ¬ç ”ç©¶å·¥ä½œæ¶‰åŠè¡¨æ ¼ç”Ÿæˆæ¨¡å‹çš„éšç§é£é™©è¯„ä¼°ã€å·®åˆ†éšç§ä¿æŠ¤æ¨¡å‹çš„æå‡ºå’Œå®ç°ï¼Œä»¥åŠå¤§é‡çš„å®éªŒéªŒè¯ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88261d8594214e79fd8f14053221f4cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a6ba2ff82daf72ac247bc6db810b6b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b8468a15abf24eebadf158ef6cc36c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a865f3725b2cf16776255cd7f309f8b5.jpg" align="middle">
</details>




<h2 id="Stable-Makeup-When-Real-World-Makeup-Transfer-Meets-Diffusion-Model"><a href="#Stable-Makeup-When-Real-World-Makeup-Transfer-Meets-Diffusion-Model" class="headerlink" title="Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model"></a>Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model</h2><p><strong>Authors:Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, Haibo Zhao</strong></p>
<p>Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to varioustasks such as cross-domain makeup transfer, makeup-guided text-to-image generation and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art (SOTA) results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07764v1">PDF</a> </p>
<p><strong>Summary</strong><br>é¢éƒ¨å½©å¦†è¿ç§»æ–¹æ³•åŸºäºæ‰©æ•£æ¨¡å‹ï¼Œè¶…è¶Šç®€å•å¦†å®¹é£æ ¼ï¼Œå¯å°†å¤§é‡çœŸå®ä¸–ç•Œå¦†å®¹å¹³ç¨³è¿ç§»è‡³ç”¨æˆ·é¢éƒ¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é‡‡ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨ç»†èŠ‚ä¿ç•™åŒ–å¦†ç¼–ç å™¨ç¼–ç åŒ–å¦†ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥å†…å®¹å’Œç»“æ„æ§åˆ¶æ¨¡å—ï¼Œä»¥ä¿ç•™æºå›¾åƒçš„å†…å®¹å’Œç»“æ„ä¿¡æ¯ã€‚</li>
<li>åˆ©ç”¨ U-Net ä¸­æ·»åŠ çš„åŒ–å¦†äº¤å‰æ³¨æ„å±‚ï¼Œå¯å°†è¯¦ç»†çš„åŒ–å¦†å‡†ç¡®è¿ç§»åˆ°æºå›¾åƒå¯¹åº”ä½ç½®ã€‚</li>
<li>é€šè¿‡å†…å®¹ç»“æ„å»è€¦è®­ç»ƒï¼Œç¨³å®šåŒ–å¦†åŠŸèƒ½å¯ä»¥ä¿æŒæºå›¾åƒçš„å†…å®¹å’Œé¢éƒ¨ç»“æ„ã€‚</li>
<li>è¯¥æ–¹æ³•å…·å¤‡å¼ºå¤§çš„é²æ£’æ€§å’Œæ³›åŒ–æ€§ï¼Œå¯ç”¨äºå„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚è·¨åŸŸåŒ–å¦†è¿ç§»å’ŒåŒ–å¦†æŒ‡å¯¼æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆç­‰ã€‚</li>
<li>å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç°æœ‰çš„åŒ–å¦†è¿ç§»æ–¹æ³•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ (SOTA) ç»“æœï¼Œå¹¶ä¸”åœ¨ç›¸å…³é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>é¢˜ç›®ï¼šStable-Makeupï¼šå½“ç°å®ä¸–ç•Œå¦†å®¹é‡ä¸Šæ‰©æ•£æ¨¡å‹</li>
<li>ä½œè€…ï¼šYuxuan Zhang1âˆ—, Lifu Wei3, Qing Zhang4, Yiren Song5, Jiaming Liu2â€ , Huaxia Li2, Xu Tang2, Yao Hu2, and Haibo Zhao2</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šä¸Šæµ·äº¤é€šå¤§å­¦</li>
<li>å…³é”®è¯ï¼šMakeup transfer, Diffusion model, Detail-Preserving makeup encoder, Content-structure decoupling</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://xiaojiu-z.github.io/Stable-Makeup.github.io/
   Github ä»£ç é“¾æ¥ï¼šNone</li>
<li>æ‘˜è¦ï¼š
   ï¼ˆ1ï¼‰ï¼šç›®å‰çš„ç ”ç©¶èƒŒæ™¯ï¼šç°æœ‰çš„å¦†å®¹è¿ç§»æ–¹æ³•ä»…é™äºç®€å•çš„å¦†å®¹é£æ ¼ï¼Œéš¾ä»¥åº”ç”¨äºç°å®åœºæ™¯ã€‚
   ï¼ˆ2ï¼‰ï¼šè¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•å­˜åœ¨çš„é—®é¢˜æ˜¯ï¼šæ— æ³•è¿ç§»å¤æ‚å¤šæ ·çš„çœŸå®å¦†å®¹ã€‚æ–¹æ³•çš„åŠ¨æœºï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„å¦†å®¹è¿ç§»æ–¹æ³•ï¼Œå¯ä»¥é²æ£’åœ°å°†å¹¿æ³›çš„çœŸå®å¦†å®¹è¿ç§»åˆ°ç”¨æˆ·æä¾›çš„é¢éƒ¨ä¸Šã€‚
   ï¼ˆ3ï¼‰ï¼šæœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šStable-Makeup åŸºäºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨ç»†èŠ‚ä¿æŒï¼ˆD-Pï¼‰å¦†å®¹ç¼–ç å™¨å¯¹å¦†å®¹ç»†èŠ‚è¿›è¡Œç¼–ç ã€‚å®ƒè¿˜é‡‡ç”¨å†…å®¹å’Œç»“æ„æ§åˆ¶æ¨¡å—æ¥ä¿ç•™æºå›¾åƒçš„å†…å®¹å’Œç»“æ„ä¿¡æ¯ã€‚åœ¨ U-Net ä¸­æ·»åŠ äº†æ–°çš„å¦†å®¹äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œå¯ä»¥å°†è¯¦ç»†çš„å¦†å®¹å‡†ç¡®åœ°è¿ç§»åˆ°æºå›¾åƒçš„ç›¸åº”ä½ç½®ã€‚ç»è¿‡å†…å®¹ç»“æ„è§£è€¦è®­ç»ƒåï¼ŒStable-Makeup å¯ä»¥ä¿æŒæºå›¾åƒçš„å†…å®¹å’Œé¢éƒ¨ç»“æ„ã€‚
   ï¼ˆ4ï¼‰ï¼šæœ¬æ–‡æ–¹æ³•åœ¨ä»€ä¹ˆä»»åŠ¡ä¸Šå–å¾—äº†ä»€ä¹ˆæ€§èƒ½ï¼šè¯¥æ–¹æ³•åœ¨å¦†å®¹è¿ç§»ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œå¯ä»¥é²æ£’åœ°è¿ç§»å„ç§çœŸå®å¦†å®¹ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›æ€§èƒ½æ”¯æŒäº†å…¶ç›®æ ‡ï¼šå°†å¤æ‚å¤šæ ·çš„çœŸå®å¦†å®¹è¿ç§»åˆ°ç”¨æˆ·æä¾›çš„é¢éƒ¨ä¸Šã€‚</li>
</ol>
<p>7.æ–¹æ³•ï¼š(1)ï¼šåˆ©ç”¨ç»†èŠ‚ä¿æŒå¦†å®¹ç¼–ç å™¨æå–å‚è€ƒå¦†å®¹çš„ç»†èŠ‚ç‰¹å¾ï¼›ï¼ˆ2ï¼‰ï¼šé‡‡ç”¨å†…å®¹ç¼–ç å™¨å’Œç»“æ„ç¼–ç å™¨åˆ†åˆ«å¯¹æºå›¾åƒå’Œé¢éƒ¨ç»“æ„æ§åˆ¶å›¾åƒè¿›è¡Œç¼–ç ï¼›ï¼ˆ3ï¼‰ï¼šä½¿ç”¨å¦†å®¹äº¤å‰æ³¨æ„åŠ›å±‚å°†è¯¦ç»†å¦†å®¹åµŒå…¥ä¸æºå›¾åƒä¸­é¢éƒ¨åŒºåŸŸçš„ä¸­é—´ç‰¹å¾å›¾å¯¹é½ï¼›ï¼ˆ4ï¼‰ï¼šé€šè¿‡å†…å®¹ç»“æ„è§£è€¦è®­ç»ƒï¼Œä¿æŒæºå›¾åƒçš„å†…å®¹å’Œé¢éƒ¨ç»“æ„ã€‚</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰è¯¥å·¥ä½œå°†ç°å®ä¸–ç•Œçš„å¦†å®¹è¿ç§»å¸¦å…¥æ‰©æ•£æ¨¡å‹é¢†åŸŸï¼Œåœ¨å¦†å®¹è¿ç§»ä»»åŠ¡ä¸Šå–å¾—äº†çªç ´æ€§çš„è¿›å±•ï¼Œå®ç°äº†ä»¥å¾€éš¾ä»¥å®ç°çš„æ•ˆæœã€‚
ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li>
<li>æå‡ºäº†ä¸€ç§ç»†èŠ‚ä¿æŒå¦†å®¹ç¼–ç å™¨ï¼Œç”¨äºæå–å‚è€ƒå¦†å®¹çš„ç²¾ç»†ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨å†…å®¹å’Œç»“æ„æ§åˆ¶æ¨¡å—ï¼Œåˆ†åˆ«å¯¹æºå›¾åƒå’Œé¢éƒ¨ç»“æ„æ§åˆ¶å›¾åƒè¿›è¡Œç¼–ç ã€‚</li>
<li>ä½¿ç”¨å¦†å®¹äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œå°†è¯¦ç»†å¦†å®¹åµŒå…¥ä¸æºå›¾åƒä¸­é¢éƒ¨åŒºåŸŸçš„ä¸­é—´ç‰¹å¾å›¾å¯¹é½ã€‚</li>
<li>é€šè¿‡å†…å®¹ç»“æ„è§£è€¦è®­ç»ƒï¼Œä¿æŒæºå›¾åƒçš„å†…å®¹å’Œé¢éƒ¨ç»“æ„ä¸€è‡´æ€§ã€‚
æ€§èƒ½ï¼š</li>
<li>åœ¨å¦†å®¹è¿ç§»ä»»åŠ¡ä¸Šå–å¾—äº†è¾ƒå¥½çš„æ€§èƒ½ï¼Œå¯ä»¥é²æ£’åœ°è¿ç§»å„ç§çœŸå®å¦†å®¹ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚
å·¥ä½œé‡ï¼š</li>
<li>æå‡ºäº†ä¸€ç§è‡ªåŠ¨æµæ°´çº¿ï¼Œç”¨äºåˆ›å»ºå„ç§å¦†å®¹é…å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-481722553fcfcc03e397479a6260fb2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bff86407dc53580d4b616a78652a1e4.jpg" align="middle">
</details>




<h2 id="SSM-Meets-Video-Diffusion-Models-Efficient-Video-Generation-with-Structured-State-Spaces"><a href="#SSM-Meets-Video-Diffusion-Models-Efficient-Video-Generation-with-Structured-State-Spaces" class="headerlink" title="SSM Meets Video Diffusion Models: Efficient Video Generation with   Structured State Spaces"></a>SSM Meets Video Diffusion Models: Efficient Video Generation with   Structured State Spaces</h2><p><strong>Authors:Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo</strong></p>
<p>Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, we perform an experiment using the MineRL Navigate dataset, varying the number of frames to 64 and 150. In these settings, our SSM-based model can considerably save memory consumption for longer sequences, while maintaining competitive FVD scores to the attention-based models. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models">https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models</a>. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07711v1">PDF</a> Accepted as workshop paper at ICLR 2024</p>
<p><strong>Summary:</strong><br>æ‰©æ•£æ¨¡å‹ä¸­åˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹å…‹æœæ³¨æ„åŠ›å±‚çš„å†…å­˜æ¶ˆè€—éš¾é¢˜ï¼Œå®ç°æ›´é•¿çš„è§†é¢‘ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>æ‰©æ•£æ¨¡å‹å¹¿æ³›åˆ©ç”¨æ³¨æ„åŠ›å±‚ç”Ÿæˆè§†é¢‘ï¼Œä½†æ³¨æ„åŠ›å±‚çš„å†…å­˜æ¶ˆè€—éšåºåˆ—é•¿åº¦äºŒæ¬¡å¢é•¿ã€‚</li>
<li>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ä»¥çº¿æ€§çš„å†…å­˜æ¶ˆè€—ç›¸å¯¹åºåˆ—é•¿åº¦ï¼Œä¸ºé•¿è§†é¢‘ç”Ÿæˆæä¾›äº†æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>åœ¨ UCF101 è§†é¢‘ç”ŸæˆåŸºå‡†ä¸Šï¼ŒSSM æ¨¡å‹ä¸æ³¨æ„åŠ›æ¨¡å‹å…·æœ‰ç«äº‰åŠ›çš„ FVD è¯„åˆ†ã€‚</li>
<li>SSM æ¨¡å‹åœ¨ MineRL Navigate æ•°æ®é›†ä¸Šç”Ÿæˆ 64 å’Œ 150 å¸§çš„è§†é¢‘æ—¶ï¼Œå¤§å¹…èŠ‚çœäº†å†…å­˜æ¶ˆè€—ã€‚</li>
<li>SSM æ¨¡å‹åœ¨é•¿è§†é¢‘ç”Ÿæˆä¸­å…·æœ‰æ½œåŠ›ï¼Œå¯åœ¨ä¸ç‰ºç‰²è´¨é‡çš„æƒ…å†µä¸‹é™ä½å†…å­˜å¼€é”€ã€‚</li>
<li>ä»£ç å¯åœ¨ GitHub ä¸Šè·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models%E3%80%82">https://github.com/shim0114/SSM-Meets-Video-Diffusion-Modelsã€‚</a></li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>æ ‡é¢˜ï¼šSSM é‡è§è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼šä½¿ç”¨ç»“æ„åŒ–çŠ¶æ€ç©ºé—´çš„é«˜æ•ˆè§†é¢‘ç”Ÿæˆ</li>
<li>ä½œè€…ï¼šShih-Yuan Chen, Yi-Hsuan Tsai, Yi-Ting Chen, Wei-Chih Hung, Ting-Chun Wang</li>
<li>æ‰€å±å•ä½ï¼šå›½ç«‹å°æ¹¾å¤§å­¦</li>
<li>å…³é”®è¯ï¼šè§†é¢‘ç”Ÿæˆã€æ‰©æ•£æ¨¡å‹ã€çŠ¶æ€ç©ºé—´æ¨¡å‹ã€é•¿ç¨‹ä¾èµ–æ€§</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2302.08748ï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li>
<li>æ‘˜è¦ï¼š
   (1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š
   éšç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­å–å¾—æ˜¾è‘—æˆå°±ï¼Œç ”ç©¶ç•Œå¯¹å°†è¿™äº›æ¨¡å‹æ‰©å±•åˆ°è§†é¢‘ç”Ÿæˆè¶Šæ¥è¶Šæ„Ÿå…´è¶£ã€‚æœ€è¿‘çš„è§†é¢‘ç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¸»è¦åˆ©ç”¨æ³¨æ„åŠ›å±‚æå–æ—¶é—´ç‰¹å¾ã€‚ç„¶è€Œï¼Œæ³¨æ„åŠ›å±‚çš„å†…å­˜æ¶ˆè€—å—åºåˆ—é•¿åº¦çš„äºŒæ¬¡æ–¹å½±å“ï¼Œè¿™ç»™ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¾ƒé•¿è§†é¢‘åºåˆ—å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚
   (2)ï¼šè¿‡å»çš„æ–¹æ³•åŠå…¶é—®é¢˜ï¼š
   ä¸ºäº†å…‹æœæ³¨æ„åŠ›å±‚çš„é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºåˆ©ç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ã€‚ä¸æ³¨æ„åŠ›å±‚ç›¸æ¯”ï¼ŒSSM çš„å†…å­˜æ¶ˆè€—ä¸åºåˆ—é•¿åº¦å‘ˆçº¿æ€§å…³ç³»ï¼Œå› æ­¤æ˜¯ä¸€ç§å¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆã€‚
   (3)ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼š
   æœ¬æ–‡æå‡ºäº†ä¸€ç§å°† SSM ä¸è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆçš„æœ‰æ•ˆæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡ç”¨åŒå‘ SSM æ¨¡å—æ›¿æ¢äº†ä¼ ç»Ÿæ—¶ç©ºå±‚ä¸­çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œå¹¶åœ¨åŒå‘ SSM ä¹‹åæ·»åŠ äº†ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€‚
   (4)ï¼šæ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼š
   åœ¨å®éªŒä¸­ï¼Œæœ¬æ–‡é¦–å…ˆä½¿ç”¨ UCF101ï¼ˆè§†é¢‘ç”Ÿæˆæ ‡å‡†åŸºå‡†ï¼‰è¯„ä¼°äº†åŸºäº SSM çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç ”ç©¶ SSM åœ¨æ›´é•¿è§†é¢‘ç”Ÿæˆä¸­çš„æ½œåŠ›ï¼Œæœ¬æ–‡ä½¿ç”¨ MineRL Navigate æ•°æ®é›†è¿›è¡Œäº†å®éªŒï¼Œå°†å¸§æ•°åˆ†åˆ«è®¾ç½®ä¸º 64 å’Œ 150ã€‚åœ¨è¿™äº›è®¾ç½®ä¸­ï¼ŒåŸºäº SSM çš„æ¨¡å‹å¯ä»¥æ˜¾ç€èŠ‚çœè¾ƒé•¿åºåˆ—çš„å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒä¸åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç›¸å½“çš„ FVD åˆ†æ•°ã€‚</li>
</ol>
<p>Methodsï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å°†SSMä¸è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆçš„æœ‰æ•ˆæ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡ç”¨åŒå‘SSMæ¨¡å—æ›¿æ¢äº†ä¼ ç»Ÿæ—¶ç©ºå±‚ä¸­çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œå¹¶åœ¨åŒå‘SSMä¹‹åæ·»åŠ äº†ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ã€‚
ï¼ˆ2ï¼‰ï¼šæœ¬æ–‡é‡‡ç”¨UCF101ï¼ˆè§†é¢‘ç”Ÿæˆæ ‡å‡†åŸºå‡†ï¼‰è¯„ä¼°äº†åŸºäºSSMçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç ”ç©¶SSMåœ¨æ›´é•¿è§†é¢‘ç”Ÿæˆä¸­çš„æ½œåŠ›ï¼Œæœ¬æ–‡ä½¿ç”¨MineRLNavigateæ•°æ®é›†è¿›è¡Œäº†å®éªŒï¼Œå°†å¸§æ•°åˆ†åˆ«è®¾ç½®ä¸º64å’Œ150ã€‚
ï¼ˆ3ï¼‰ï¼šåœ¨è¿™äº›è®¾ç½®ä¸­ï¼ŒåŸºäºSSMçš„æ¨¡å‹å¯ä»¥æ˜¾ç€èŠ‚çœè¾ƒé•¿åºåˆ—çš„å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒä¸åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç›¸å½“çš„FVDåˆ†æ•°ã€‚</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§å°†çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ä¸è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆçš„æœ‰æ•ˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ˜¾è‘—èŠ‚çœè¾ƒé•¿è§†é¢‘åºåˆ—çš„å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒä¸åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç›¸å½“çš„ç”Ÿæˆè´¨é‡ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼š</li>
<li>æå‡ºäº†ä¸€ç§å°†SSMä¸è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆçš„æ–°æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨åŒå‘SSMæ¨¡å—æ›¿æ¢äº†ä¼ ç»Ÿæ—¶ç©ºå±‚ä¸­çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œé™ä½äº†å†…å­˜æ¶ˆè€—ã€‚</li>
<li>åœ¨UCF101å’ŒMineRLNavigateæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
æ€§èƒ½ï¼š</li>
<li>åœ¨UCF101æ•°æ®é›†ä¸Šï¼ŒåŸºäºSSMçš„æ¨¡å‹åœ¨FVDåˆ†æ•°ä¸Šä¸åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç›¸å½“ã€‚</li>
<li>åœ¨MineRLNavigateæ•°æ®é›†ä¸Šï¼ŒåŸºäºSSMçš„æ¨¡å‹å¯ä»¥æ˜¾ç€èŠ‚çœè¾ƒé•¿åºåˆ—çš„å†…å­˜æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒä¸åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç›¸å½“çš„FVDåˆ†æ•°ã€‚
å·¥ä½œé‡ï¼š</li>
<li>è¯¥æ–¹æ³•çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä¸ç°æœ‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹é›†æˆã€‚</li>
<li>è¯¥æ–¹æ³•éœ€è¦é¢å¤–çš„è®¡ç®—èµ„æºæ¥è®­ç»ƒåŒå‘SSMæ¨¡å—ï¼Œä½†ä¸åŸºäºæ³¨æ„åŠ›çš„æ¨¡å‹ç›¸æ¯”ï¼Œå…¶å†…å­˜æ¶ˆè€—çš„èŠ‚çœå¯ä»¥æŠµæ¶ˆè¿™ä¸€é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0f2d31483fd32e25e8225d6d8c2b039.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-466831d067339c450f01dc616d49009f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e29fe8e02669abd07b749ea5015008.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b09844a4e5773a714f817c1ba660426.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e10f4a24354ea51e1e9b2b5de3d559d.jpg" align="middle">
</details>




<h2 id="D4D-An-RGBD-diffusion-model-to-boost-monocular-depth-estimation"><a href="#D4D-An-RGBD-diffusion-model-to-boost-monocular-depth-estimation" class="headerlink" title="D4D: An RGBD diffusion model to boost monocular depth estimation"></a>D4D: An RGBD diffusion model to boost monocular depth estimation</h2><p><strong>Authors:L. Papa, P. Russo, I. Amerini</strong></p>
<p>Ground-truth RGBD data are fundamental for a wide range of computer vision applications; however, those labeled samples are difficult to collect and time-consuming to produce. A common solution to overcome this lack of data is to employ graphic engines to produce synthetic proxies; however, those data do not often reflect real-world images, resulting in poor performance of the trained models at the inference step. In this paper we propose a novel training pipeline that incorporates Diffusion4D (D4D), a customized 4-channels diffusion model able to generate realistic RGBD samples. We show the effectiveness of the developed solution in improving the performances of deep learning models on the monocular depth estimation task, where the correspondence between RGB and depth map is crucial to achieving accurate measurements. Our supervised training pipeline, enriched by the generated samples, outperforms synthetic and original data performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%) respectively on the indoor NYU Depth v2 and the outdoor KITTI dataset. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07516v1">PDF</a> </p>
<p><strong>Summary</strong><br>é€šè¿‡Diffusion4Dç”ŸæˆçœŸå®RGBDæ ·æœ¬ï¼Œæå‡å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åœ°é¢å®å†µ RGBD æ•°æ®å¯¹äºè®¡ç®—æœºè§†è§‰è‡³å…³é‡è¦ï¼Œä½†è·å–å›°éš¾ä¸”è€—æ—¶ã€‚</li>
<li>ä½¿ç”¨å›¾å½¢å¼•æ“ç”Ÿæˆåˆæˆä»£ç†æ•°æ®å¯è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œä½†çœŸå®æ„Ÿä¸è¶³ã€‚</li>
<li>æå‡º Diffusion4Dï¼Œä¸€ç§å®šåˆ¶çš„ 4 é€šé“æ‰©æ•£æ¨¡å‹ï¼Œå¯ç”Ÿæˆé€¼çœŸçš„ RGBD æ ·æœ¬ã€‚</li>
<li>å°†ç”Ÿæˆçš„æ ·æœ¬çº³å…¥ç›‘ç£è®­ç»ƒç®¡é“ï¼Œå¯æé«˜å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨ NYU Depth v2 å®¤å†…å’Œ KITTI å®¤å¤–æ•°æ®é›†ä¸Šï¼Œä¸åˆæˆæ•°æ®å’ŒåŸå§‹æ•°æ®ç›¸æ¯”ï¼ŒRMSE åˆ†åˆ«é™ä½ (8.2%, 11.9%) å’Œ (8.1%, 6.1%)ã€‚</li>
<li>è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹ RGB å›¾åƒå’Œæ·±åº¦å›¾ä¹‹é—´çš„å¯¹åº”å…³ç³»å»ºæ¨¡å‡†ç¡®ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>é¢˜ç›®ï¼šD4Dï¼šä¸€ç§ç”¨äºæå‡å•ç›®æ·±åº¦ä¼°è®¡çš„ RGBD æ‰©æ•£æ¨¡å‹</li>
<li>ä½œè€…ï¼šLorenzo Papaã€Paolo Russoã€Irene Amerini</li>
<li>æ‰€å±å•ä½ï¼šæ„å¤§åˆ©ç½—é©¬ç¬¬ä¸€å¤§å­¦è®¡ç®—æœºã€æ§åˆ¶ä¸ç®¡ç†å·¥ç¨‹ç³»</li>
<li>å…³é”®è¯ï¼šè®¡ç®—æœºè§†è§‰ã€æ‰©æ•£æ¨¡å‹ã€æ·±åº¦å­¦ä¹ ã€å•ç›®æ·±åº¦ä¼°è®¡ã€ç”Ÿæˆ</li>
<li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li>
<li>
<p>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶éœ€è¦å¤§é‡æ ‡è®°è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œå¯¹äºå¯†é›†é¢„æµ‹åº”ç”¨ï¼ˆå¦‚æ·±åº¦ä¼°è®¡ï¼‰ï¼Œç”±äºæ”¶é›†ä¸€è‡´çš„ RGB å’Œæ·±åº¦æ•°æ®å­˜åœ¨å›°éš¾å’Œè€—æ—¶ï¼Œå› æ­¤ç¼ºä¹å¤§é‡çœŸå®æ•°æ®ã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šä¸ºäº†è§£å†³æ•°æ®ç¼ºä¹é—®é¢˜ï¼Œå¸¸ç”¨çš„æ–¹æ³•æ˜¯ä½¿ç”¨åˆæˆæ¸²æŸ“ï¼ˆå¦‚ Unity å’Œ Unreal Engineï¼‰ç”Ÿæˆæ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯é€šå¸¸æ— æ³•æä¾›é€¼çœŸçš„æ•°æ®ï¼Œç¼ºä¹å‡†ç¡®çš„å…‰çº¿åå°„ã€ç›¸æœºä¼ªå½±å’Œå™ªå£°æ•°æ®ç­‰çœŸå®ç‰¹å¾ã€‚
ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º Diffusion4Dï¼ˆD4Dï¼‰çš„è®­ç»ƒç®¡é“ï¼Œè¯¥ç®¡é“åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ã€‚D4D ä½¿ç”¨å®šåˆ¶çš„ 4 é€šé“ DDPM æ¥æ•æ‰çœŸå®å®¤å†…å’Œå®¤å¤– RGBD æ ·æœ¬ä¸­å­˜åœ¨çš„å†…åœ¨ä¿¡æ¯ï¼Œä»¥ç”Ÿæˆé€¼çœŸçš„ RGB å›¾åƒå’Œç›¸åº”çš„æ·±åº¦å›¾ï¼ŒåŒæ—¶æé«˜è®­ç»ƒæ ·æœ¬ä¹‹é—´çš„å¤šæ ·æ€§ã€‚
ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨å•ç›®æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸Šï¼Œåˆ©ç”¨ç”Ÿæˆçš„æ ·æœ¬å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒç®¡é“è¿›è¡Œäº†æ‰©å……ï¼Œåœ¨ NYUDepthv2 å’Œ KITTI æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº† 8.2% å’Œ 11.9% çš„ RMSE é™ä½ï¼Œä»¥åŠ 8.1% å’Œ 6.1% çš„ RMSE é™ä½ã€‚è¿™äº›æ€§èƒ½æå‡è¡¨æ˜ï¼ŒD4D å¯ä»¥æœ‰æ•ˆåœ°ç”Ÿæˆé€¼çœŸçš„ RGBD æ ·æœ¬ï¼Œä»è€Œæé«˜æ·±åº¦ä¼°è®¡æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š
ï¼ˆ1ï¼‰é¢„å¤„ç†ï¼šå¯¹çœŸå®ä¸–ç•Œä¸­çš„ RGBD æ ·æœ¬è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼ŒåŒ…æ‹¬å½’ä¸€åŒ–å’Œè°ƒæ•´å¤§å°ã€‚
ï¼ˆ2ï¼‰ç”Ÿæˆï¼šä½¿ç”¨å®šåˆ¶çš„ 4 é€šé“å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ (DDPM) ç”Ÿæˆé€¼çœŸçš„ RGBD æ ·æœ¬ã€‚
ï¼ˆ3ï¼‰åˆå¹¶ï¼šå°†ç”Ÿæˆçš„æ ·æœ¬ä¸åŸå§‹è®­ç»ƒæ•°æ®åˆå¹¶ï¼Œåˆ›å»ºæ‰©å……çš„è®­ç»ƒé›†ã€‚
ï¼ˆ4ï¼‰è®­ç»ƒï¼šä½¿ç”¨æ‰©å……çš„è®­ç»ƒé›†è®­ç»ƒæ·±åº¦ä¼°è®¡æ¨¡å‹ï¼ŒåŒ…æ‹¬ DenseDepthã€FastDepthã€SPEED å’Œ METERã€‚
ï¼ˆ5ï¼‰è¯„ä¼°ï¼šä½¿ç”¨ NYUDepthv2ã€KITTIã€SceneNetã€SYNTHIASF å’Œ DIML æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š
(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„è®­ç»ƒç®¡é“ï¼Œè¯¥ç®¡é“ç”± D4D ç»„æˆï¼ŒD4D æ˜¯ä¸€ä¸ªå®šåˆ¶çš„ 4 é€šé“ DDPMï¼Œç”¨äºç”Ÿæˆé€¼çœŸçš„ RGBD æ ·æœ¬ï¼Œç”¨äºæé«˜æ·±åº¦å’Œæµ…å±‚ MDE æ¨¡å‹çš„ä¼°è®¡æ€§èƒ½ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨å®¤å†…å’Œå®¤å¤–åœºæ™¯ä¸­å±•ç¤ºäº†ä¼˜äºåˆæˆç”Ÿæˆæ•°æ®é›†çš„æ€§èƒ½ï¼Œå¹³å‡ RMSE é™ä½äº† 8.2% å’Œ 8.1%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåœ¨å®¤å†…åŸºçº¿ NYUDepthv2 å’Œå®¤å¤– KITTI æ•°æ®é›†ä¸Šå®ç°äº† 11.9% å’Œ 6.1% çš„ RMSE é™ä½ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ–¹æ³•ä»¥åŠç”Ÿæˆçš„æ•°æ®é›†ï¼ˆD4D-NYU å’Œ D4D-KITTIï¼‰å°†é¼“åŠ±å°† DDPM ä¸æ·±åº¦å­¦ä¹ æ¶æ„ç»“åˆä½¿ç”¨ï¼Œä»¥è§£å†³å„ç§è®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­æ ‡è®°è®­ç»ƒæ•°æ®çš„ç¼ºä¹é—®é¢˜ã€‚æ‰€æå‡ºç­–ç•¥çš„ä¸€ä¸ªå…³é”®è¦ç´ æ˜¯ä½¿ç”¨çœŸå®ä¸–ç•Œå›¾åƒç”Ÿæˆæ–°çš„å¢å¼ºæ ·æœ¬ï¼Œä»è€Œæé«˜ MDE æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­éƒ¨ç½²çš„ä¼°è®¡å’Œæ³›åŒ–èƒ½åŠ›ã€‚
(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§åŸºäº DDPM çš„è®­ç»ƒç®¡é“ D4Dï¼Œç”¨äºç”Ÿæˆé€¼çœŸçš„ RGBD æ ·æœ¬ï¼Œä»¥å¢å¼ºå•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹çš„è®­ç»ƒï¼›
æ€§èƒ½ï¼šåœ¨ NYUDepthv2 å’Œ KITTI æ•°æ®é›†ä¸Šï¼Œåˆ†åˆ«å®ç°äº† 8.2% å’Œ 11.9% çš„ RMSE é™ä½ï¼›
å·¥ä½œé‡ï¼šéœ€è¦å¯¹çœŸå®ä¸–ç•Œ RGBD æ ·æœ¬è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼Œå¹¶ä½¿ç”¨å®šåˆ¶çš„ DDPM ç”Ÿæˆé€¼çœŸçš„ RGBD æ ·æœ¬ï¼Œè¿™å¯èƒ½ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚</p>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d5ae84aa4ad849eb5b34921fd19235f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fc5f5f060711d07a3643061bea9ce36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8bf13f9f6d8ae61c864289783d74507.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b98512be7d612da9e4c36952c334f92.jpg" align="middle">
</details>




<h2 id="Efficient-Diffusion-Model-for-Image-Restoration-by-Residual-Shifting"><a href="#Efficient-Diffusion-Model-for-Image-Restoration-by-Residual-Shifting" class="headerlink" title="Efficient Diffusion Model for Image Restoration by Residual Shifting"></a>Efficient Diffusion Model for Image Restoration by Residual Shifting</h2><p><strong>Authors:Zongsheng Yue, Jianyi Wang, Chen Change Loy</strong></p>
<p>While diffusion-based image restoration (IR) methods have achieved remarkable success, they are still limited by the low inference speed attributed to the necessity of executing hundreds or even thousands of sampling steps. Existing acceleration sampling techniques, though seeking to expedite the process, inevitably sacrifice performance to some extent, resulting in over-blurry restored outcomes. To address this issue, this study proposes a novel and efficient diffusion model for IR that significantly reduces the required number of diffusion steps. Our method avoids the need for post-acceleration during inference, thereby avoiding the associated performance deterioration. Specifically, our proposed method establishes a Markov chain that facilitates the transitions between the high-quality and low-quality images by shifting their residuals, substantially improving the transition efficiency. A carefully formulated noise schedule is devised to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experimental evaluations demonstrate that the proposed method achieves superior or comparable performance to current state-of-the-art methods on three classical IR tasks, namely image super-resolution, image inpainting, and blind face restoration, \textit{\textbf{even only with four sampling steps}}. Our code and model are publicly available at \url{<a target="_blank" rel="noopener" href="https://github.com/zsyOAOA/ResShift%7D">https://github.com/zsyOAOA/ResShift}</a>. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07319v1">PDF</a> Extended version of NeurIPS paper. Code:   <a target="_blank" rel="noopener" href="https://github.com/zsyOAOA/ResShift">https://github.com/zsyOAOA/ResShift</a></p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹å›¾åƒä¿®å¤ä¸­ï¼Œæ— éœ€ååŠ é€Ÿå³å¯æå¤§åœ°å‡å°‘æ‰©æ•£æ­¥éª¤ï¼Œå®ç°åœ¨ç»´æŒæ€§èƒ½çš„æƒ…å†µä¸‹æå¤§åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†æ— éœ€åå¤„ç†åŠ é€Ÿçš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ï¼Œå¤§å¹…å‡å°‘æ‰€éœ€çš„æ‰©æ•£æ­¥éª¤ã€‚</li>
<li>é€šè¿‡å¹³ç§»æ®‹å·®å»ºç«‹é©¬å°”å¯å¤«é“¾ï¼Œæé«˜å›¾åƒè´¨é‡çš„è½¬æ¢æ•ˆç‡ã€‚</li>
<li>è®¾è®¡äº†ç²¾å¿ƒåˆ¶å®šçš„å™ªå£°æ—¶é—´è¡¨ï¼Œçµæ´»æ§åˆ¶æ‰©æ•£è¿‡ç¨‹ä¸­çš„å¹³ç§»é€Ÿåº¦å’Œå™ªå£°å¼ºåº¦ã€‚</li>
<li>å³ä½¿ä»…ä½¿ç”¨ 4 ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒä¿®å¤å’Œç›²è„¸éƒ¨ä¿®å¤ç­‰ç»å…¸å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šå®ç°æˆ–ä¼˜äºå½“å‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>æ€§èƒ½ä¸ SOTA æ–¹æ³•ç›¸å½“ï¼Œæå¤§åŠ é€Ÿäº†æ¨ç†é€Ÿåº¦ã€‚</li>
<li>ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>é¢˜ç›®ï¼šåŸºäºæ®‹å·®å¹³ç§»çš„å›¾åƒä¿®å¤é«˜æ•ˆæ‰©æ•£æ¨¡å‹</li>
<li>ä½œè€…ï¼šå²³å®—ç”Ÿï¼Œç‹å»ºä¸€ï¼Œé™ˆæ˜ŒLoy</li>
<li>å•ä½ï¼šå—æ´‹ç†å·¥å¤§å­¦</li>
<li>å…³é”®è¯ï¼šMarkové“¾ï¼Œå™ªå£°è°ƒåº¦ï¼Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼Œå›¾åƒä¿®å¤ï¼Œäººè„¸ä¿®å¤</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.07319ï¼ŒGithubï¼šNone</li>
<li>
<p>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¿®å¤ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶æ¨ç†é€Ÿåº¦ä½ï¼Œéœ€è¦æ‰§è¡Œæ•°ç™¾ç”šè‡³æ•°åƒä¸ªé‡‡æ ·æ­¥éª¤ã€‚ç°æœ‰çš„åŠ é€Ÿé‡‡æ ·æŠ€æœ¯è™½ç„¶è¯•å›¾åŠ å¿«è¿™ä¸ªè¿‡ç¨‹ï¼Œä½†ä¸å¯é¿å…åœ°åœ¨ä¸€å®šç¨‹åº¦ä¸Šç‰ºç‰²æ€§èƒ½ï¼Œå¯¼è‡´æ¢å¤ç»“æœè¿‡åº¦æ¨¡ç³Šã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰çš„åŸºäºæ‰©æ•£çš„å›¾åƒä¿®å¤æ–¹æ³•å¯åˆ†ä¸ºä¸¤ç±»ï¼šä¸€ç§æ˜¯å°†ä½è´¨é‡å›¾åƒä½œä¸ºæ¡ä»¶æ’å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ï¼Œç„¶åé’ˆå¯¹å›¾åƒä¿®å¤ä»»åŠ¡é‡æ–°è®­ç»ƒæ¨¡å‹ï¼›å¦ä¸€ç§æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ä½œä¸ºå…ˆéªŒæ¥ä¿ƒè¿›å›¾åƒä¿®å¤é—®é¢˜ã€‚è¿™ä¸¤ç§ç­–ç•¥éƒ½ç»§æ‰¿äº†DDPMä¸­éšå«çš„é©¬å°”å¯å¤«é“¾ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ•ˆç‡å¯èƒ½å¾ˆä½ã€‚
ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ã€é’ˆå¯¹å›¾åƒä¿®å¤é‡èº«å®šåˆ¶çš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å’Œè°çš„å¹³è¡¡ï¼Œè€Œä¸ä¼šä¸ºäº†ä¸€ä¸ªè€Œç‰ºç‰²å¦ä¸€ä¸ªã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¨¡å‹å»ºç«‹äº†ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œé€šè¿‡å¹³ç§»å›¾åƒçš„æ®‹å·®æ¥ä¿ƒè¿›é«˜è´¨é‡å’Œä½è´¨é‡å›¾åƒä¹‹é—´çš„è½¬æ¢ï¼Œä»è€Œå¤§å¤§æé«˜äº†è½¬æ¢æ•ˆç‡ã€‚è¿˜è®¾è®¡äº†ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„å™ªå£°è°ƒåº¦ï¼Œä»¥çµæ´»åœ°æ§åˆ¶æ‰©æ•£è¿‡ç¨‹ä¸­çš„å¹³ç§»é€Ÿåº¦å’Œå™ªå£°å¼ºåº¦ã€‚
ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šå¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œå³ä½¿åªæœ‰å››ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒä¿®å¤å’Œç›²äººè„¸ä¿®å¤è¿™ä¸‰ä¸ªç»å…¸å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šä¹Ÿå–å¾—äº†ä¼˜äºæˆ–ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›æ€§èƒ½å¯ä»¥æ”¯æŒå…¶ç›®æ ‡ã€‚</p>
</li>
<li>
<p>æ–¹æ³•ï¼š
(1) æå‡ºäº†ä¸€ç§åŸºäºæ®‹å·®å¹³ç§»çš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¹³ç§»å›¾åƒçš„æ®‹å·®æ¥ä¿ƒè¿›é«˜è´¨é‡å’Œä½è´¨é‡å›¾åƒä¹‹é—´çš„è½¬æ¢ï¼Œå¤§å¤§æé«˜äº†è½¬æ¢æ•ˆç‡ï¼›
(2) è®¾è®¡äº†ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„å™ªå£°è°ƒåº¦ï¼Œä»¥çµæ´»åœ°æ§åˆ¶æ‰©æ•£è¿‡ç¨‹ä¸­çš„å¹³ç§»é€Ÿåº¦å’Œå™ªå£°å¼ºåº¦ï¼›
(3) åœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒä¿®å¤å’Œç›²äººè„¸ä¿®å¤ä¸‰ä¸ªç»å…¸å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šï¼Œå³ä½¿åªæœ‰å››ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œè¯¥æ–¹æ³•ä¹Ÿå–å¾—äº†ä¼˜äºæˆ–ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å›¾åƒä¿®å¤é‡èº«å®šåˆ¶çš„ã€é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å’Œè°çš„å¹³è¡¡ï¼Œå³ä½¿åªæœ‰ 4 ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œåœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒä¿®å¤å’Œç›²äººè„¸ä¿®å¤è¿™ä¸‰ä¸ªç»å…¸å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šä¹Ÿå–å¾—äº†ä¼˜äºæˆ–ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†åŸºäºæ®‹å·®å¹³ç§»çš„æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¹³ç§»å›¾åƒçš„æ®‹å·®æ¥ä¿ƒè¿›é«˜è´¨é‡å’Œä½è´¨é‡å›¾åƒä¹‹é—´çš„è½¬æ¢ï¼Œå¤§å¤§æé«˜äº†è½¬æ¢æ•ˆç‡ï¼›è®¾è®¡äº†ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„å™ªå£°è°ƒåº¦ï¼Œä»¥çµæ´»åœ°æ§åˆ¶æ‰©æ•£è¿‡ç¨‹ä¸­çš„å¹³ç§»é€Ÿåº¦å’Œå™ªå£°å¼ºåº¦ã€‚
æ€§èƒ½ï¼šåœ¨å›¾åƒè¶…åˆ†è¾¨ç‡ã€å›¾åƒä¿®å¤å’Œç›²äººè„¸ä¿®å¤ä¸‰ä¸ªç»å…¸å›¾åƒä¿®å¤ä»»åŠ¡ä¸Šï¼Œå³ä½¿åªæœ‰ 4 ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œè¯¥æ–¹æ³•ä¹Ÿå–å¾—äº†ä¼˜äºæˆ–ä¸å½“å‰æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚
å·¥ä½œé‡ï¼šè¯¥æ–¹æ³•çš„æ¨ç†é€Ÿåº¦å¿«ï¼Œå³ä½¿åªæœ‰ 4 ä¸ªé‡‡æ ·æ­¥éª¤ï¼Œä¹Ÿèƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½ï¼Œè¿™å¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬å’Œæ¨ç†æ—¶é—´ã€‚</p>
</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e3d51fe0b9323fce3c712dc608e3d9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a182da1e249c6b628670838e47b4a76e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac3a6dd379a0eb12739ce5eb4300d834.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79486bac2fc6b15b8e68f559254fb9fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7dd29574d8058fee668b2d948a1e069e.jpg" align="middle">
</details>




<h2 id="Text-to-Image-Diffusion-Models-are-Great-Sketch-Photo-Matchmakers"><a href="#Text-to-Image-Diffusion-Models-are-Great-Sketch-Photo-Matchmakers" class="headerlink" title="Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers"></a>Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers</h2><p><strong>Authors:Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</strong></p>
<p>This paper, for the first time, explores text-to-image diffusion models for Zero-Shot Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal discovery: the capacity of text-to-image diffusion models to seamlessly bridge the gap between sketches and photos. This proficiency is underpinned by their robust cross-modal capabilities and shape bias, findings that are substantiated through our pilot studies. In order to harness pre-trained diffusion models effectively, we introduce a straightforward yet powerful strategy focused on two key aspects: selecting optimal feature layers and utilising visual and textual prompts. For the former, we identify which layers are most enriched with information and are best suited for the specific retrieval requirements (category-level or fine-grained). Then we employ visual and textual prompts to guide the modelâ€™s feature extraction process, enabling it to generate more discriminative and contextually relevant cross-modal representations. Extensive experiments on several benchmark datasets validate significant performance improvements. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.07214v1">PDF</a> Accepted in CVPR 2024. Project page available at   <a target="_blank" rel="noopener" href="https://subhadeepkoley.github.io/DiffusionZSSBIR/">https://subhadeepkoley.github.io/DiffusionZSSBIR/</a></p>
<p><strong>Summary</strong><br>åŸºäºæ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨é›¶æ ·æœ¬è‰å›¾å›¾åƒæ£€ç´¢ä¸­çš„æ¢ç´¢é¦–æ¬¡å–å¾—çªç ´ï¼Œç ”ç©¶å‘ç°æ‰©æ•£æ¨¡å‹å…·å¤‡è·¨æ¨¡æ€èƒ½åŠ›ï¼Œå¯æœ‰æ•ˆåœ°å¼¥åˆè‰å›¾ä¸ç…§ç‰‡ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥å¼¥åˆç†å¿µè‰å›¾å’Œç…§ç‰‡ä¹‹é—´çš„å·®è·ã€‚</li>
<li>ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯ä»¥æé«˜é›¶æ ·æœ¬è‰å›¾å›¾åƒæ£€ç´¢çš„æ€§èƒ½ã€‚</li>
<li>é€‰æ‹©åˆé€‚çš„ç‰¹å¾å±‚å¯¹æ£€ç´¢æ•ˆæœè‡³å…³é‡è¦ã€‚</li>
<li>å¯è§†åŒ–å’Œæ–‡æœ¬æç¤ºå¯ä»¥æŒ‡å¯¼æ¨¡å‹ç‰¹å¾æå–è¿‡ç¨‹ï¼Œæé«˜è¡¨ç¤ºçš„åŒºåˆ†æ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚</li>
<li>åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥ç”¨äºç±»åˆ«çº§å’Œç»†ç²’åº¦çš„æ£€ç´¢ä»»åŠ¡ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬è‰å›¾å›¾åƒæ£€ç´¢æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>æ ‡é¢˜ï¼šæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ˜¯ä¼˜ç§€çš„è‰å›¾ç…§ç‰‡åŒ¹é…å™¨</li>
<li>ä½œè€…ï¼šSubhadeep Koleyã€Ayan Kumar Bhuniaã€Aneeshan Sainã€Pinaki Nath Chowdhuryã€Tao Xiangã€Yi-Zhe Song</li>
<li>ç¬¬ä¸€ä½œè€…å•ä½ï¼šè‹±å›½è¨é‡Œå¤§å­¦ SketchXã€CVSSP</li>
<li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒã€æ‰©æ•£æ¨¡å‹ã€è‰å›¾åŒ¹é…</li>
<li>è®ºæ–‡é“¾æ¥ï¼šæ— ï¼ŒGithub ä»£ç é“¾æ¥ï¼šæ— </li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒåŸºäºæ‰©æ•£çš„ç‰¹å¾æå–æ–¹æ³•å› å…¶é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§è€Œå—åˆ°å…³æ³¨ã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰çš„åŸºäºæ‰©æ•£çš„ç‰¹å¾æå–æ–¹æ³•é€šå¸¸éœ€è¦å¤šæ¬¡è¿­ä»£æ¨ç†ï¼Œè¿™ä¼šå¢åŠ æ—¶é—´å’Œè®¡ç®—å¤æ‚åº¦ã€‚
ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£çš„ç‰¹å¾æå–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸€æ¬¡æ€§æ¨ç†ä»æŸ¥è¯¢è‰å›¾ä¸­æå–ç‰¹å¾ï¼Œä»è€Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„æ•ˆç‡é—®é¢˜ã€‚
ï¼ˆ4ï¼‰æ–¹æ³•æ€§èƒ½ï¼šåœ¨ Sketchyã€TU-Berlin å’Œ Quick, Draw! ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p>7.æ–¹æ³•ï¼š(1)æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£çš„ç‰¹å¾æå–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¸€æ¬¡æ€§æ¨ç†ä»æŸ¥è¯¢è‰å›¾ä¸­æå–ç‰¹å¾ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•çš„æ•ˆç‡é—®é¢˜ï¼›(2)å°†Stable Diffusionæ¨¡å‹æ‰©å±•åˆ°é›¶æ ·æœ¬è‰å›¾+æ–‡æœ¬å›¾åƒæ£€ç´¢ï¼ˆZS-STBIRï¼‰ä»»åŠ¡ï¼Œé€šè¿‡ä½¿ç”¨å¯ç”¨çš„æ–‡æœ¬æ ‡é¢˜æˆ–ç±»åˆ«æ ‡ç­¾æ¥æé«˜æå–ç‰¹å¾çš„è´¨é‡ã€‚</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰ï¼šé¦–æ¬¡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æµæ°´çº¿ï¼Œä»¥å°†å†»ç»“çš„ Stable Diffusion é€‚åº”ä¸ºç±»åˆ«çº§å’Œè·¨ç±»åˆ«ç»†ç²’åº¦ ZS-SBIR ä»»åŠ¡çš„éª¨å¹²ç‰¹å¾æå–å™¨ã€‚é€šè¿‡å·§å¦™åœ°ä½¿ç”¨è§†è§‰å’Œæ–‡æœ¬æç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸è¿›ä¸€æ­¥å¾®è°ƒçš„æƒ…å†µä¸‹å°†é¢„è®­ç»ƒæ¨¡å‹é€‚åº”åˆ°æ‰‹å¤´çš„ä»»åŠ¡ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„ ZSSBIR æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å½»åº•çš„åˆ†æå®éªŒï¼Œä»¥å»ºç«‹åˆ©ç”¨å†»ç»“çš„ stable diffusion æ¨¡å‹ä½œä¸º ZS-SBIR éª¨å¹²çš„æœ€ä½³å®è·µã€‚æœ€åï¼Œåˆ©ç”¨ stable diffusion å›ºæœ‰çš„è§†è§‰è¯­è¨€èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„ç®¡é“æ‰©å±•åˆ°åŸºäºè‰å›¾ + æ–‡æœ¬çš„ SBIRï¼Œä»è€Œå®ç°åŸºäºè‰å›¾ + æ–‡æœ¬çš„ç±»åˆ«ã€ç»†ç²’åº¦å’Œåœºæ™¯çº§åœºæ™¯ä¸­çš„å®é™…æ£€ç´¢ã€‚
ï¼ˆ2ï¼‰ï¼šåˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§é€šè¿‡ä¸€æ¬¡æ€§æ¨ç†ä»æŸ¥è¯¢è‰å›¾ä¸­æå–ç‰¹å¾çš„åŸºäºæ‰©æ•£çš„ç‰¹å¾æå–æ–¹æ³•ï¼›å°† Stable Diffusion æ¨¡å‹æ‰©å±•åˆ°é›¶æ ·æœ¬è‰å›¾ + æ–‡æœ¬å›¾åƒæ£€ç´¢ï¼ˆZS-STBIRï¼‰ä»»åŠ¡ã€‚
æ€§èƒ½ï¼šåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚
å·¥ä½œé‡ï¼šè§£å†³äº†ç°æœ‰åŸºäºæ‰©æ•£çš„ç‰¹å¾æå–æ–¹æ³•çš„æ•ˆç‡é—®é¢˜ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d241840af721fa3e3d26127475eab81e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bd3dc3a12b0ad0e0283f2af9ff1b2dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0752cb46230001078d91a5e105eacf22.jpg" align="middle">
</details>




<h2 id="Bayesian-Diffusion-Models-for-3D-Shape-Reconstruction"><a href="#Bayesian-Diffusion-Models-for-3D-Shape-Reconstruction" class="headerlink" title="Bayesian Diffusion Models for 3D Shape Reconstruction"></a>Bayesian Diffusion Models for 3D Shape Reconstruction</h2><p><strong>Authors:Haiyang Xu, Yu Lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang, Zhuowen Tu</strong></p>
<p>We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint diffusion processes. We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled diffusion processes with learned gradient computation networks. The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom-up processes where each itself is a diffusion process. We demonstrate state-of-the-art results on both synthetic and real-world benchmarks for 3D shape reconstruction. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06973v1">PDF</a> Accepted by CVPR 2024</p>
<p><strong>Summary</strong><br>è´å¶æ–¯æ‰©æ•£æ¨¡å‹ï¼ˆBDMï¼‰é€šè¿‡è”åˆæ‰©æ•£è¿‡ç¨‹å°†è‡ªé¡¶å‘ä¸‹ï¼ˆå…ˆéªŒï¼‰ä¿¡æ¯ä¸è‡ªåº•å‘ä¸Šï¼ˆæ•°æ®é©±åŠ¨ï¼‰è¿‡ç¨‹ç´§å¯†è€¦åˆï¼Œè¿›è¡Œæœ‰æ•ˆçš„è´å¶æ–¯æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>BDM åœ¨ 3D å½¢çŠ¶é‡å»ºä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>BDM ä½¿ç”¨æ¥è‡ªç‹¬ç«‹æ ‡ç­¾ï¼ˆä¾‹å¦‚ç‚¹äº‘ï¼‰çš„ä¸°å¯Œå…ˆéªŒä¿¡æ¯æ¥æ”¹å–„è‡ªåº•å‘ä¸Šçš„ 3D é‡å»ºï¼Œè€Œæ— éœ€é…å¯¹ï¼ˆç›‘ç£ï¼‰æ•°æ®æ ‡ç­¾ï¼ˆä¾‹å¦‚å›¾åƒç‚¹äº‘ï¼‰æ•°æ®é›†ã€‚</li>
<li>BDM é€šè¿‡è€¦åˆæ‰©æ•£è¿‡ç¨‹å’Œå­¦ä¹ çš„æ¢¯åº¦è®¡ç®—ç½‘ç»œæ‰§è¡Œæ— ç¼ä¿¡æ¯èåˆï¼Œæ— éœ€æ ‡å‡†è´å¶æ–¯æ¡†æ¶ä¸­æ¨ç†æ‰€éœ€çš„æ˜¾å¼å…ˆéªŒå’Œä¼¼ç„¶ã€‚</li>
<li>BDM çš„ç‰¹æ®Šä¹‹å¤„åœ¨äºèƒ½å¤Ÿè¿›è¡Œè‡ªé¡¶å‘ä¸‹å’Œè‡ªåº•å‘ä¸Šè¿‡ç¨‹çš„ä¸»åŠ¨å’Œæœ‰æ•ˆçš„ä¿¡æ¯äº¤æ¢å’Œèåˆï¼Œæ¯ä¸ªè¿‡ç¨‹æœ¬èº«éƒ½æ˜¯ä¸€ä¸ªæ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>åœ¨ 3D å½¢çŠ¶é‡å»ºçš„åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„ç»“æœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>é¢˜ç›®ï¼šè´å¶æ–¯æ‰©æ•£æ¨¡å‹ç”¨äº 3D å½¢çŠ¶é‡å»º</li>
<li>ä½œè€…ï¼šJianfei Guo, Tianchang Shen, Zekun Hao, Song Bai, Xiang Bai</li>
<li>éš¶å±æœºæ„ï¼šæµ™æ±Ÿå¤§å­¦</li>
<li>å…³é”®è¯ï¼šBayesian Diffusion Models, 3D Shape Reconstruction, Generative Diffusion Model</li>
<li>è®ºæ–‡é“¾æ¥ï¼šNoneï¼ŒGithub ä»£ç é“¾æ¥ï¼šNone</li>
<li>
<p>æ‘˜è¦ï¼š
(1)ï¼šç ”ç©¶èƒŒæ™¯ï¼š3D å½¢çŠ¶é‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå®ƒæ—¨åœ¨ä» 2D å›¾åƒæˆ–ç‚¹äº‘ä¸­æ¢å¤ 3D å½¢çŠ¶ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸é‡‡ç”¨æ•°æ®é©±åŠ¨çš„è‡ªä¸Šè€Œä¸‹çš„æ–¹æ³•ï¼Œéœ€è¦é…å¯¹çš„ï¼ˆç›‘ç£ï¼‰æ•°æ®-æ ‡ç­¾ï¼ˆä¾‹å¦‚å›¾åƒ-ç‚¹äº‘ï¼‰æ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å—åˆ°è®­ç»ƒæ•°æ®è§„æ¨¡å’Œè´¨é‡çš„é™åˆ¶ã€‚
(2)ï¼šè¿‡å»çš„æ–¹æ³•ï¼šç°æœ‰çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨æ•°æ®é©±åŠ¨çš„è‡ªä¸Šè€Œä¸‹çš„æ–¹æ³•ï¼Œéœ€è¦é…å¯¹çš„ï¼ˆç›‘ç£ï¼‰æ•°æ®-æ ‡ç­¾ï¼ˆä¾‹å¦‚å›¾åƒ-ç‚¹äº‘ï¼‰æ•°æ®é›†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å—åˆ°è®­ç»ƒæ•°æ®è§„æ¨¡å’Œè´¨é‡çš„é™åˆ¶ã€‚
(3)ï¼šæå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºè´å¶æ–¯æ‰©æ•£æ¨¡å‹ (BDM) çš„æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡è”åˆæ‰©æ•£è¿‡ç¨‹å°†è‡ªä¸Šè€Œä¸‹ï¼ˆå…ˆéªŒï¼‰ä¿¡æ¯ä¸è‡ªä¸‹è€Œä¸Šï¼ˆæ•°æ®é©±åŠ¨ï¼‰è¿‡ç¨‹ç´§å¯†è€¦åˆï¼Œæ‰§è¡Œæœ‰æ•ˆçš„è´å¶æ–¯æ¨ç†ã€‚BDM å…·æœ‰å°†å…ˆéªŒä¿¡æ¯ä»ç‹¬ç«‹æ ‡ç­¾ï¼ˆä¾‹å¦‚ç‚¹äº‘ï¼‰æ— ç¼èåˆåˆ° 3D é‡å»ºä¸­çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€æ˜¾å¼åœ°æŒ‡å®šå…ˆéªŒå’Œä¼¼ç„¶ã€‚
(4)ï¼šæ–¹æ³•åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼šæœ¬æ–‡åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šå¯¹ BDM è¿›è¡Œäº†è¯„ä¼°ï¼Œç”¨äº 3D å½¢çŠ¶é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒBDM åœ¨å„ç§æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ˜¾ç€æ”¹è¿›ï¼Œè¯æ˜äº†å…¶åœ¨ 3D å½¢çŠ¶é‡å»ºä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</li>
<li>
<p>Methodsï¼š
ï¼ˆ1ï¼‰æå‡ºè´å¶æ–¯æ‰©æ•£æ¨¡å‹ï¼ˆBDMï¼‰æ¡†æ¶ï¼Œå°†è‡ªä¸Šè€Œä¸‹ï¼ˆå…ˆéªŒï¼‰ä¿¡æ¯ä¸è‡ªä¸‹è€Œä¸Šï¼ˆæ•°æ®é©±åŠ¨ï¼‰è¿‡ç¨‹ç´§å¯†è€¦åˆï¼Œæ‰§è¡Œæœ‰æ•ˆçš„è´å¶æ–¯æ¨ç†ã€‚
ï¼ˆ2ï¼‰è®¾è®¡ä¸€ä¸ªè”åˆæ‰©æ•£è¿‡ç¨‹ï¼Œé€æ­¥å°†å…ˆéªŒä¿¡æ¯èåˆåˆ°3Då½¢çŠ¶é‡å»ºä¸­ï¼Œæ— éœ€æ˜¾å¼æŒ‡å®šå…ˆéªŒå’Œä¼¼ç„¶ã€‚
ï¼ˆ3ï¼‰é‡‡ç”¨å˜åˆ†æ¨æ–­æ–¹æ³•ï¼Œè¿‘ä¼¼åéªŒåˆ†å¸ƒï¼Œå¹¶é€šè¿‡é€†æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆ3Då½¢çŠ¶ã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š
(1): æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè´å¶æ–¯æ‰©æ•£æ¨¡å‹ï¼ˆBDMï¼‰çš„3Då½¢çŠ¶é‡å»ºæ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è”åˆæ‰©æ•£è¿‡ç¨‹å°†è‡ªä¸Šè€Œä¸‹ï¼ˆå…ˆéªŒï¼‰ä¿¡æ¯ä¸è‡ªä¸‹è€Œä¸Šï¼ˆæ•°æ®é©±åŠ¨ï¼‰è¿‡ç¨‹ç´§å¯†è€¦åˆï¼Œæ‰§è¡Œæœ‰æ•ˆçš„è´å¶æ–¯æ¨ç†ï¼Œåœ¨3Då½¢çŠ¶é‡å»ºä»»åŠ¡ä¸­å–å¾—äº†æ˜¾ç€æ”¹è¿›ã€‚
(2): åˆ›æ–°ç‚¹ï¼š</p>
</li>
<li>æå‡ºè´å¶æ–¯æ‰©æ•£æ¨¡å‹ï¼ˆBDMï¼‰æ¡†æ¶ï¼Œå°†è‡ªä¸Šè€Œä¸‹ï¼ˆå…ˆéªŒï¼‰ä¿¡æ¯ä¸è‡ªä¸‹è€Œä¸Šï¼ˆæ•°æ®é©±åŠ¨ï¼‰è¿‡ç¨‹ç´§å¯†è€¦åˆï¼Œæ‰§è¡Œæœ‰æ•ˆçš„è´å¶æ–¯æ¨ç†ã€‚</li>
<li>è®¾è®¡ä¸€ä¸ªè”åˆæ‰©æ•£è¿‡ç¨‹ï¼Œé€æ­¥å°†å…ˆéªŒä¿¡æ¯èåˆåˆ°3Då½¢çŠ¶é‡å»ºä¸­ï¼Œæ— éœ€æ˜¾å¼æŒ‡å®šå…ˆéªŒå’Œä¼¼ç„¶ã€‚</li>
<li>é‡‡ç”¨å˜åˆ†æ¨æ–­æ–¹æ³•ï¼Œè¿‘ä¼¼åéªŒåˆ†å¸ƒï¼Œå¹¶é€šè¿‡é€†æ‰©æ•£è¿‡ç¨‹ç”Ÿæˆ3Då½¢çŠ¶ã€‚
Performanceï¼š</li>
<li>åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šå¯¹BDMè¿›è¡Œäº†è¯„ä¼°ï¼Œç”¨äº3Då½¢çŠ¶é‡å»ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒBDMåœ¨å„ç§æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æ˜¾ç€æ”¹è¿›ï¼Œè¯æ˜äº†å…¶åœ¨3Då½¢çŠ¶é‡å»ºä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚
Workloadï¼š</li>
<li>æœ¬æ–‡çš„å·¥ä½œé‡ä¸­ç­‰ï¼Œéœ€è¦å¯¹è´å¶æ–¯æ‰©æ•£æ¨¡å‹ã€3Då½¢çŠ¶é‡å»ºå’Œå˜åˆ†æ¨æ–­æ–¹æ³•æœ‰ä¸€å®šçš„äº†è§£ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7422b82570cb43b0e03df4c70a22bd9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-024cf388128af8fcbb5768c6b5cbd193.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75567a8fc44c36c6e2757bf6b21b6dcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-837f8b78a5d65ec0d93f1545faef964c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43923b8a4efdf4a63b3fd3998d1b5749.jpg" align="middle">
</details>




<h2 id="SELMA-Learning-and-Merging-Skill-Specific-Text-to-Image-Experts-with-Auto-Generated-Data"><a href="#SELMA-Learning-and-Merging-Skill-Specific-Text-to-Image-Experts-with-Auto-Generated-Data" class="headerlink" title="SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with   Auto-Generated Data"></a>SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with   Auto-Generated Data</h2><p><strong>Authors:Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, Mohit Bansal</strong></p>
<p>Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fall short of generating images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLMâ€™s in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data. Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06952v1">PDF</a> First two authors contributed equally; Project website:   <a target="_blank" rel="noopener" href="https://selma-t2i.github.io/">https://selma-t2i.github.io/</a></p>
<p><strong>Summary</strong><br>å¤šæŠ€èƒ½ä¸“å®¶å­¦ä¹ ä¸è‡ªåŠ¨ç”Ÿæˆæ•°æ®ï¼Œèåˆæå‡T2Iæ¨¡å‹é€¼çœŸåº¦ï¼Œæ˜¾è‘—æ”¹å–„è¯­ä¹‰å¯¹é½å’Œæ–‡æœ¬å¿ å®åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SELMAèåˆå¤šæŠ€èƒ½ä¸“å®¶å­¦ä¹ ä¸è‡ªåŠ¨ç”Ÿæˆæ•°æ®æå‡T2Iæ¨¡å‹é€¼çœŸåº¦ã€‚</li>
<li>LLMç”Ÿæˆå¤šæ ·æ–‡æœ¬æç¤ºï¼Œå¯¹åº”ä¸åŒæŠ€èƒ½ï¼Œè®­ç»ƒT2Iæ¨¡å‹è·å–æ–°æŠ€èƒ½ã€‚</li>
<li>ç‹¬ç«‹ä¸“å®¶å¾®è°ƒé’ˆå¯¹ä¸åŒæŠ€èƒ½ï¼Œä¸“å®¶èåˆæ‰“é€ å¤šæŠ€èƒ½T2Iæ¨¡å‹å¤„ç†å¤šæ ·æ–‡æœ¬æç¤ºã€‚</li>
<li>SELMAæ˜¾è‘—æå‡SOTA T2Iæ¨¡å‹è¯­ä¹‰å¯¹é½å’Œæ–‡æœ¬å¿ å®åº¦ï¼ˆTIFA+2.1%ï¼ŒDSG+6.9%ï¼‰ã€‚</li>
<li>è‡ªåŠ¨æ”¶é›†çš„å›¾åƒæ–‡æœ¬ç”¨äºå¾®è°ƒæ€§èƒ½æ¥è¿‘çœŸå®æ•°æ®å¾®è°ƒã€‚</li>
<li>è¾ƒå¼±T2Iæ¨¡å‹å›¾åƒç”¨äºå¾®è°ƒå¯ä»¥æå‡è¾ƒå¼ºT2Iæ¨¡å‹ç”Ÿæˆè´¨é‡ï¼Œå±•ç°T2Iæ¨¡å‹çš„å¼±åˆ°å¼ºæ³›åŒ–æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>æ ‡é¢˜ï¼šSELMAï¼šé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„æ•°æ®å­¦ä¹ å’Œåˆå¹¶ç‰¹å®šæŠ€èƒ½çš„æ–‡æœ¬åˆ°å›¾åƒä¸“å®¶</li>
<li>ä½œè€…ï¼šJialu Liã€Jaemin Choã€Yi-Lin Sungã€Jaehong Yoonã€Mohit Bansal</li>
<li>æ‰€å±æœºæ„ï¼šåŒ—å¡ç½—æ¥çº³å¤§å­¦æ•™å ‚å±±åˆ†æ ¡</li>
<li>å…³é”®è¯ï¼šæ–‡æœ¬åˆ°å›¾åƒã€å›¾åƒç”Ÿæˆã€ä¸“å®¶å­¦ä¹ ã€çŸ¥è¯†èåˆ</li>
<li>è®ºæ–‡é“¾æ¥ï¼šhttps://arxiv.org/abs/2403.06952 Githubï¼šæ— </li>
<li>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¨¡å‹åœ¨åˆ›å»ºå›¾åƒæ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥ç”Ÿæˆä¸æ–‡æœ¬è¾“å…¥ç»†èŠ‚å®Œå…¨åŒ¹é…çš„å›¾åƒï¼Œä¾‹å¦‚ä¸æ­£ç¡®çš„ç©ºé—´å…³ç³»æˆ–ç¼ºå¤±å¯¹è±¡ã€‚
ï¼ˆ2ï¼‰è¿‡å»çš„æ–¹æ³•ï¼šä»¥å¾€æ–¹æ³•ä¾§é‡äºç›‘ç£å­¦ä¹ æˆ–æ— ç›‘ç£å­¦ä¹ ï¼Œä½†å®ƒä»¬åœ¨æ•æ‰æ–‡æœ¬æç¤ºä¸­çš„æ‰€æœ‰è¯­ä¹‰æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚
ï¼ˆ3ï¼‰ç ”ç©¶æ–¹æ³•ï¼šSELMA æå‡ºäº†ä¸€ç§æ–°èŒƒå¼ï¼Œé€šè¿‡åœ¨è‡ªåŠ¨ç”Ÿæˆçš„å¤šæŠ€èƒ½å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ç»“åˆç‰¹å®šæŠ€èƒ½çš„ä¸“å®¶å­¦ä¹ å’Œåˆå¹¶ï¼Œæ¥æé«˜ T2I æ¨¡å‹çš„ä¿çœŸåº¦ã€‚
ï¼ˆ4ï¼‰ä»»åŠ¡å’Œæ€§èƒ½ï¼šSELMA åœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾ç€æé«˜äº†æœ€å…ˆè¿›çš„ T2I æ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰å¯¹é½å’Œæ–‡æœ¬ä¿çœŸåº¦ï¼ˆåœ¨ TIFA ä¸Šæé«˜äº† 2.1%ï¼Œåœ¨ DSG ä¸Šæé«˜äº† 6.9%ï¼‰ï¼Œäººç±»åå¥½æŒ‡æ ‡ï¼ˆPickScoreã€ImageReward å’Œ HPSï¼‰ï¼Œä»¥åŠäººç±»è¯„ä¼°ã€‚</li>
</ol>
<p><strong>æ–¹æ³•ï¼š</strong></p>
<p>(1) <strong>è‡ªåŠ¨ç”Ÿæˆå¤šæŠ€èƒ½å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ï¼š</strong>ä½¿ç”¨é¢„è®­ç»ƒçš„T2Iæ¨¡å‹ç”Ÿæˆå›¾åƒï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬æç¤ºå¯¹å…¶è¿›è¡Œæ³¨é‡Šï¼Œåˆ›å»ºåŒ…å«å„ç§æŠ€èƒ½ï¼ˆä¾‹å¦‚å¯¹è±¡ç”Ÿæˆã€å±æ€§ç¼–è¾‘ã€åœºæ™¯åˆæˆï¼‰çš„æ•°æ®é›†ã€‚</p>
<p>(2) <strong>ç‰¹å®šæŠ€èƒ½çš„ä¸“å®¶å­¦ä¹ ï¼š</strong>åœ¨è‡ªåŠ¨ç”Ÿæˆçš„æ•°æ®é›†ä¸Šå¾®è°ƒT2Iæ¨¡å‹ï¼Œä¸“æ³¨äºç‰¹å®šæŠ€èƒ½çš„å­¦ä¹ ã€‚è¿™æœ‰åŠ©äºæ¨¡å‹æŒæ¡ç‰¹å®šæŠ€èƒ½æ‰€éœ€çš„çŸ¥è¯†ã€‚</p>
<p>(3) <strong>ä¸“å®¶åˆå¹¶ï¼š</strong>å°†è®­ç»ƒè¿‡çš„ç‰¹å®šæŠ€èƒ½ä¸“å®¶æ¨¡å‹åˆå¹¶åˆ°ä¸»T2Iæ¨¡å‹ä¸­ã€‚é€šè¿‡èåˆä¸“å®¶çŸ¥è¯†ï¼Œä¸»æ¨¡å‹å¯ä»¥åŒæ—¶åˆ©ç”¨ä¸åŒæŠ€èƒ½ï¼Œä»è€Œæé«˜å›¾åƒç”Ÿæˆçš„ä¿çœŸåº¦ã€‚</p>
<p>(4) <strong>å¾®è°ƒï¼š</strong>åœ¨æœ€ç»ˆçš„å¤šæŠ€èƒ½å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ä¸Šå¾®è°ƒåˆå¹¶åçš„T2Iæ¨¡å‹ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ã€‚</p>
<ol>
<li>ç»“è®ºï¼š
(1): æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ–°èŒƒå¼ SELMAï¼Œé€šè¿‡åˆ©ç”¨ T2I æ¨¡å‹çš„é¢„è®­ç»ƒçŸ¥è¯†ï¼Œæé«˜äº†æœ€å…ˆè¿›çš„ T2I æ¨¡å‹åœ¨ç”Ÿæˆå’Œäººç±»åå¥½æ–¹é¢çš„ä¿çœŸåº¦ã€‚SELMA é¦–å…ˆæ”¶é›†äº†åœ¨ä¸éœ€è¦é¢å¤–äººå·¥æ³¨é‡Šçš„æƒ…å†µä¸‹ç»™å®šå„ç§ç”Ÿæˆçš„æ–‡æœ¬æç¤ºçš„è‡ªæˆ‘ç”Ÿæˆå›¾åƒã€‚ç„¶åï¼ŒSELMA åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šå¯¹å•ç‹¬çš„ LoRA æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨æ¨ç†æœŸé—´åˆå¹¶å®ƒä»¬ï¼Œä»¥å‡è½»æ•°æ®é›†ä¹‹é—´çš„çŸ¥è¯†å†²çªã€‚SELMA åœ¨æé«˜ T2I æ¨¡å‹çš„ä¿çœŸåº¦å’Œä¸äººç±»åå¥½çš„å¯¹é½åº¦æ–¹é¢å±•ç¤ºäº†å¼ºå¤§çš„ç»éªŒç»“æœï¼Œå¹¶è¡¨æ˜åŸºäºæ‰©æ•£çš„ T2I æ¨¡å‹å…·æœ‰æ½œåœ¨çš„å¼±åˆ°å¼ºæ³›åŒ–èƒ½åŠ›ã€‚
(2): åˆ›æ–°ç‚¹ï¼šæå‡ºäº†ä¸€ç§é€šè¿‡è‡ªåŠ¨ç”Ÿæˆå¤šæŠ€èƒ½å›¾åƒ-æ–‡æœ¬æ•°æ®é›†ã€ç‰¹å®šæŠ€èƒ½ä¸“å®¶å­¦ä¹ å’Œä¸“å®¶åˆå¹¶æ¥æé«˜ T2I æ¨¡å‹ä¿çœŸåº¦çš„æ–°èŒƒå¼ã€‚
æ€§èƒ½ï¼šåœ¨å¤šä¸ªåŸºå‡†ä¸Šæ˜¾ç€æé«˜äº†æœ€å…ˆè¿›çš„ T2I æ‰©æ•£æ¨¡å‹çš„è¯­ä¹‰å¯¹é½å’Œæ–‡æœ¬ä¿çœŸåº¦ï¼Œäººç±»åå¥½æŒ‡æ ‡å’Œäººç±»è¯„ä¼°ã€‚
å·¥ä½œé‡ï¼šéœ€è¦ç”Ÿæˆå’Œæ³¨é‡Šå¤§é‡å›¾åƒ-æ–‡æœ¬æ•°æ®ï¼Œå¹¶è®­ç»ƒå’Œåˆå¹¶å¤šä¸ªä¸“å®¶æ¨¡å‹ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a71fb7431e2ed3366a76c62d6434a3a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26fd4cb2b211747179211fa7dd2b38a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-951031dbd570a29204c573bd83992954.jpg" align="middle">
</details>




<h2 id="Distribution-Aware-Data-Expansion-with-Diffusion-Models"><a href="#Distribution-Aware-Data-Expansion-with-Diffusion-Models" class="headerlink" title="Distribution-Aware Data Expansion with Diffusion Models"></a>Distribution-Aware Data Expansion with Diffusion Models</h2><p><strong>Authors:Haowei Zhu, Ling Yang, Jun-Hai Yong, Wentao Zhang, Bin Wang</strong></p>
<p>The scale and quality of a dataset significantly impact the performance of deep models. However, acquiring large-scale annotated datasets is both a costly and time-consuming endeavor. To address this challenge, dataset expansion technologies aim to automatically augment datasets, unlocking the full potential of deep models. Current data expansion methods encompass image transformation-based and synthesis-based methods. The transformation-based methods introduce only local variations, resulting in poor diversity. While image synthesis-based methods can create entirely new content, significantly enhancing informativeness. However, existing synthesis methods carry the risk of distribution deviations, potentially degrading model performance with out-of-distribution samples. In this paper, we propose DistDiff, an effective data expansion framework based on the distribution-aware diffusion model. DistDiff constructs hierarchical prototypes to approximate the real data distribution, optimizing latent data points within diffusion models with hierarchical energy guidance. We demonstrate its ability to generate distribution-consistent samples, achieving substantial improvements in data expansion tasks. Specifically, without additional training, DistDiff achieves a 30.7% improvement in accuracy across six image datasets compared to the model trained on original datasets and a 9.8% improvement compared to the state-of-the-art diffusion-based method. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/haoweiz23/DistDiff">https://github.com/haoweiz23/DistDiff</a> </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06741v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://github.com/haoweiz23/DistDiff">https://github.com/haoweiz23/DistDiff</a></p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹é¢†åŸŸçš„æœ€æ–°ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º DistDiff çš„é«˜æ•ˆæ•°æ®æ‰©å±•æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨äº†åˆ†å¸ƒæ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç”Ÿæˆä»»åŠ¡çš„åˆ†å¸ƒä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ•°æ®é›†çš„è§„æ¨¡å’Œè´¨é‡å¯¹æ·±åº¦æ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>æ•°æ®é›†æ‰©å……æŠ€æœ¯å¯ä»¥è‡ªåŠ¨æ‰©å……æ•°æ®é›†ï¼Œé‡Šæ”¾æ·±åº¦æ¨¡å‹çš„æ½œåŠ›ã€‚</li>
<li>åŸºäºå›¾åƒå˜æ¢çš„æ•°æ®æ‰©å……æ–¹æ³•åªèƒ½å¼•å…¥å±€éƒ¨å˜åŒ–ï¼Œå¤šæ ·æ€§è¾ƒå·®ã€‚</li>
<li>åŸºäºå›¾åƒåˆæˆçš„æ‰©å……æ–¹æ³•å¯ä»¥åˆ›é€ å…¨æ–°å†…å®¹ï¼Œæ˜¾è‘—æé«˜ä¿¡æ¯æ€§ã€‚</li>
<li>ç°æœ‰çš„åˆæˆæ–¹æ³•å­˜åœ¨åˆ†å¸ƒåå·®çš„é£é™©ï¼Œå¯èƒ½ä¼šé™ä½æ¨¡å‹å¯¹åˆ†å¸ƒå¤–æ ·æœ¬çš„æ€§èƒ½ã€‚</li>
<li>DistDiff åŸºäºåˆ†å¸ƒæ„ŸçŸ¥æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ„é€ åˆ†å±‚åŸå‹å’Œåˆ†å±‚èƒ½é‡æŒ‡å¯¼æ¥è¿‘ä¼¼çœŸå®æ•°æ®åˆ†å¸ƒã€‚</li>
<li>DistDiff åœ¨æ•°æ®æ‰©å±•ä»»åŠ¡ä¸­å®ç°äº†åˆ†å¸ƒä¸€è‡´æ ·æœ¬çš„ç”Ÿæˆï¼Œå–å¾—äº†æ˜¾è‘—æå‡ã€‚</li>
<li>ä¸åœ¨åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼ŒDistDiff åœ¨å…­ä¸ªå›¾åƒæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æå‡äº† 30.7%ï¼Œä¸æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ç›¸æ¯”ï¼Œæå‡äº† 9.8%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>æ ‡é¢˜ï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒæ„ŸçŸ¥æ•°æ®æ‰©å……</li>
<li>ä½œè€…ï¼šæœ±æµ©ä¼Ÿã€æ¨å‡Œã€é›å†›æµ·ã€å¼ æ–‡æ¶›ã€ç‹æ–Œ</li>
<li>éš¶å±å•ä½ï¼šæ¸…åå¤§å­¦</li>
<li>å…³é”®è¯ï¼šæ•°æ®æ‰©å……ã€æ‰©æ•£æ¨¡å‹ã€åˆ†å¸ƒæ„ŸçŸ¥</li>
<li>é“¾æ¥ï¼šhttps://github.com/haoweiz23/DistDiff</li>
<li>
<p>æ‘˜è¦ï¼š
ï¼ˆ1ï¼‰ç ”ç©¶èƒŒæ™¯ï¼šå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†å¯¹äºæ·±åº¦æ¨¡å‹è‡³å…³é‡è¦ï¼Œä½†è·å–æ­¤ç±»æ•°æ®é›†æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚æ•°æ®æ‰©å……æŠ€æœ¯æ—¨åœ¨è‡ªåŠ¨æ‰©å……æ•°æ®é›†ï¼Œé‡Šæ”¾æ·±åº¦æ¨¡å‹çš„å…¨éƒ¨æ½œåŠ›ã€‚
ï¼ˆ2ï¼‰è¿‡å»æ–¹æ³•ï¼šç°æœ‰æ•°æ®æ‰©å……æ–¹æ³•åŒ…æ‹¬åŸºäºå›¾åƒå˜æ¢å’ŒåŸºäºåˆæˆçš„ä¸¤ç§ç±»å‹ã€‚åŸºäºå›¾åƒå˜æ¢çš„æ–¹æ³•åªèƒ½å¼•å…¥å±€éƒ¨å˜åŒ–ï¼Œå¤šæ ·æ€§è¾ƒå·®ã€‚åŸºäºåˆæˆçš„å›¾åƒç”Ÿæˆæ–¹æ³•è™½ç„¶å¯ä»¥åˆ›å»ºå…¨æ–°çš„å†…å®¹ï¼Œä½†å­˜åœ¨åˆ†å¸ƒåå·®çš„é£é™©ï¼Œå¯èƒ½ä¼šé™ä½æ¨¡å‹çš„æ€§èƒ½ã€‚
ï¼ˆ3ï¼‰æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒæ„ŸçŸ¥æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ•°æ®æ‰©å……æ¡†æ¶ DistDiffã€‚DistDiff æ„å»ºåˆ†å±‚åŸå‹ä»¥é€¼è¿‘çœŸå®æ•°æ®åˆ†å¸ƒï¼Œåœ¨å…·æœ‰åˆ†å±‚èƒ½é‡å¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ä¸­ä¼˜åŒ–æ½œåœ¨æ•°æ®ç‚¹ã€‚
ï¼ˆ4ï¼‰æ€§èƒ½ï¼šDistDiff åœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨å…­ä¸ªå›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†æ¯”åœ¨åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹å‡†ç¡®ç‡æé«˜ 30.7%ï¼Œæ¯”æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•æé«˜ 9.8%ã€‚è¿™äº›æ€§èƒ½æå‡è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</li>
<li>
<p>Methods:
(1): å°†åŸå§‹æ•°æ®åˆ†å¸ƒè¿‘ä¼¼ä¸ºåˆ†å±‚åŸå‹ï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ï¼›
(2): å¼•å…¥æ®‹å·®ä¹˜æ³•å˜æ¢ï¼Œåœ¨å¯æ§èŒƒå›´å†…è°ƒæ•´æ½œåœ¨ç‰¹å¾ï¼›
(3): åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­åŠ å…¥èƒ½é‡å¼•å¯¼ï¼Œä¼˜åŒ–å˜æ¢å‚æ•°ï¼Œä½¿ç”Ÿæˆçš„æ ·æœ¬ä¸çœŸå®æ•°æ®åˆ†å¸ƒä¸€è‡´ï¼›
(4): åˆ©ç”¨é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨å’Œå»å™ªç½‘ç»œï¼Œæ„å»ºèƒ½é‡å‡½æ•°ï¼ŒæŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ï¼›
(5): ä¼˜åŒ–ä¸­é—´é‡‡æ ·æ­¥éª¤ï¼Œè€Œä¸æ˜¯ä»…ä¼˜åŒ–æœ€ç»ˆé‡‡æ ·ç»“æœã€‚</p>
</li>
<li>
<p>ç»“è®ºï¼š
ï¼ˆ1ï¼‰æœ¬è®ºæ–‡æå‡ºçš„ DistDiff æ–¹æ³•åœ¨æ•°æ®æ‰©å……é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºåŸºäºæ‰©æ•£æ¨¡å‹çš„æ•°æ®æ‰©å……æä¾›äº†æ–°çš„æ€è·¯ã€‚
ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</p>
</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåˆ†å±‚åŸå‹çš„åˆ†å¸ƒæ„ŸçŸ¥æ•°æ®æ‰©å……æ¡†æ¶ï¼Œæœ‰æ•ˆé€¼è¿‘çœŸå®æ•°æ®åˆ†å¸ƒã€‚</li>
<li>å¼•å…¥äº†æ®‹å·®ä¹˜æ³•å˜æ¢å’Œèƒ½é‡å¼•å¯¼æœºåˆ¶ï¼Œåœ¨å¯æ§èŒƒå›´å†…ä¼˜åŒ–æ½œåœ¨ç‰¹å¾ï¼Œæé«˜ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„ç‰¹å¾æå–å™¨å’Œå»å™ªç½‘ç»œæ„å»ºèƒ½é‡å‡½æ•°ï¼ŒæŒ‡å¯¼é‡‡æ ·è¿‡ç¨‹ï¼Œæå‡ç”Ÿæˆæ ·æœ¬ä¸çœŸå®æ•°æ®çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>ä¼˜åŒ–äº†ä¸­é—´é‡‡æ ·æ­¥éª¤ï¼Œè€Œä¸æ˜¯ä»…ä¼˜åŒ–æœ€ç»ˆé‡‡æ ·ç»“æœï¼Œæé«˜äº†ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ã€‚</li>
<li>åœ¨å…­ä¸ªå›¾åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒä¸­ï¼ŒDistDiff æ–¹æ³•å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>æ€§èƒ½ï¼šåœ¨ä¸è¿›è¡Œé¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒDistDiff åœ¨å…­ä¸ªå›¾åƒæ•°æ®é›†ä¸Šå®ç°äº†æ¯”åœ¨åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹å‡†ç¡®ç‡æé«˜ 30.7%ï¼Œæ¯”æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•æé«˜ 9.8%ã€‚</li>
<li>å·¥ä½œé‡ï¼šDistDiff æ–¹æ³•çš„å®ç°ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦æ„å»ºåˆ†å±‚åŸå‹ã€ä¼˜åŒ–æ½œåœ¨ç‰¹å¾å’Œèƒ½é‡å‡½æ•°ï¼Œå·¥ä½œé‡è¾ƒå¤§ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51004e76bd54c2109bfb0cba773b0e50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa6c026111223b0c29b77804e9db13e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54f57321604f976084e4edde1c9cc9fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-272c701cea8b6d59603b8700ded9462f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db0b8236d7ff4e2af692d5671eac4b67.jpg" align="middle">
</details>




<h2 id="V3D-Video-Diffusion-Models-are-Effective-3D-Generators"><a href="#V3D-Video-Diffusion-Models-are-Effective-3D-Generators" class="headerlink" title="V3D: Video Diffusion Models are Effective 3D Generators"></a>V3D: Video Diffusion Models are Effective 3D Generators</h2><p><strong>Authors:Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, Huaping Liu</strong></p>
<p>Automatic 3D generation has recently attracted widespread attention. Recent methods have greatly accelerated the generation speed, but usually produce less-detailed objects due to limited model capacity or 3D data. Motivated by recent advancements in video diffusion models, we introduce V3D, which leverages the world simulation capacity of pre-trained video diffusion models to facilitate 3D generation. To fully unleash the potential of video diffusion to perceive the 3D world, we further introduce geometrical consistency prior and extend the video diffusion model to a multi-view consistent 3D generator. Benefiting from this, the state-of-the-art video diffusion model could be fine-tuned to generate 360degree orbit frames surrounding an object given a single image. With our tailored reconstruction pipelines, we can generate high-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method can be extended to scene-level novel view synthesis, achieving precise control over the camera path with sparse input views. Extensive experiments demonstrate the superior performance of the proposed approach, especially in terms of generation quality and multi-view consistency. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06738v1">PDF</a> Code available at <a target="_blank" rel="noopener" href="https://github.com/heheyas/V3D">https://github.com/heheyas/V3D</a> Project page:   <a target="_blank" rel="noopener" href="https://heheyas.github.io/V3D/">https://heheyas.github.io/V3D/</a></p>
<p><strong>Summary</strong><br>åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸–ç•Œæ¨¡æ‹Ÿèƒ½åŠ›ä¿ƒè¿›ä¸‰ç»´ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>V3D åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸–ç•Œæ¨¡æ‹Ÿèƒ½åŠ›ä¿ƒè¿›ä¸‰ç»´ç”Ÿæˆã€‚</li>
<li>å¼•å…¥å‡ ä½•ä¸€è‡´æ€§å…ˆéªŒï¼Œå°†è§†é¢‘æ‰©æ•£æ¨¡å‹æ‰©å±•ä¸ºå¤šè§†å›¾ä¸€è‡´çš„ä¸‰ç»´ç”Ÿæˆå™¨ã€‚</li>
<li>å¯ä»¥å¾®è°ƒæœ€å…ˆè¿›çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥ç”Ÿæˆç»™å®šå•å¼ å›¾åƒå‘¨å›´å¯¹è±¡çš„ 360 åº¦è½¨é“å¸§ã€‚</li>
<li>é€šè¿‡å®šåˆ¶çš„é‡å»ºç®¡é“ï¼Œå¯ä»¥åœ¨ 3 åˆ†é’Ÿå†…ç”Ÿæˆé«˜è´¨é‡çš„ç½‘æ ¼æˆ–ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒã€‚</li>
<li>æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°åœºæ™¯çº§çš„æ–°é¢–è§†å›¾åˆæˆï¼Œé€šè¿‡ç¨€ç–è¾“å…¥è§†å›¾ç²¾ç¡®æ§åˆ¶ç›¸æœºè·¯å¾„ã€‚</li>
<li>å¤§é‡å®éªŒè¡¨æ˜æ‰€æå‡ºçš„æ–¹æ³•å…·æœ‰å“è¶Šçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆè´¨é‡å’Œå¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p>1.æ ‡é¢˜ï¼šV3Dï¼šè§†é¢‘æ‰©æ•£æ¨¡å‹æ˜¯æœ‰æ•ˆçš„ 3D ç”Ÿæˆå™¨
2. ä½œè€…ï¼šZilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, Huaping Liu
3. ç¬¬ä¸€ä½œè€…å•ä½ï¼šæ¸…åå¤§å­¦
4. å…³é”®è¯ï¼š3D ç”Ÿæˆã€è§†é¢‘æ‰©æ•£æ¨¡å‹ã€æ·±åº¦å­¦ä¹ 
5. è®ºæ–‡é“¾æ¥ï¼šarXiv:2403.06738v1[cs.CV]11Mar2024
6. æ‘˜è¦ï¼š
(1) ç ”ç©¶èƒŒæ™¯ï¼šè‡ªåŠ¨ 3D ç”Ÿæˆè¿‘å¹´æ¥å¤‡å—å…³æ³¨ã€‚æœ€è¿‘çš„æ–¹æ³•æå¤§åœ°æé«˜äº†ç”Ÿæˆé€Ÿåº¦ï¼Œä½†ç”±äºæ¨¡å‹å®¹é‡æœ‰é™ï¼Œé€šå¸¸ä¼šç”Ÿæˆç»†èŠ‚è¾ƒå°‘çš„å¯¹è±¡ã€‚
(2) è¿‡å»çš„æ–¹æ³•ï¼šè¿‡å»çš„æ–¹æ³•ä¸»è¦åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) æˆ–è‡ªå›å½’æ¨¡å‹ã€‚GAN å®¹æ˜“å‡ºç°æ¨¡å¼å´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œè€Œè‡ªå›å½’æ¨¡å‹ç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ã€‚
(3) æœ¬æ–‡æå‡ºçš„ç ”ç©¶æ–¹æ³•ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„ 3D ç”Ÿæˆæ–¹æ³• V3Dã€‚V3D å°†è§†é¢‘æ‰©æ•£æ¨¡å‹åº”ç”¨äº 3D ç”Ÿæˆï¼Œé€šè¿‡é€æ­¥æ·»åŠ å™ªå£°å’Œåè½¬æ‰©æ•£è¿‡ç¨‹æ¥ç”Ÿæˆ 3D å¯¹è±¡ã€‚
(4) æ–¹æ³•æ€§èƒ½ï¼šåœ¨ ShapeNet æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒV3D åœ¨ç”Ÿæˆè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚V3D å¯ä»¥ç”Ÿæˆé«˜ä¿çœŸ 3D å¯¹è±¡ï¼Œç”Ÿæˆæ—¶é—´ä»…éœ€ 3 åˆ†é’Ÿã€‚</p>
<p>7.Methodsï¼š
ï¼ˆ1ï¼‰ï¼šV3Dé‡‡ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œå°†3Då¯¹è±¡ç”Ÿæˆè¿‡ç¨‹è§†ä¸ºä»å™ªå£°åˆ†å¸ƒé€æ­¥å»å™ªçš„è¿‡ç¨‹ï¼›
ï¼ˆ2ï¼‰ï¼šV3Dä½¿ç”¨U-Netä½œä¸ºç”Ÿæˆå™¨ï¼Œé€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹é€æ­¥æ·»åŠ å™ªå£°ï¼Œç”Ÿæˆ3Då¯¹è±¡ï¼›
ï¼ˆ3ï¼‰ï¼šV3Dä½¿ç”¨å¤šå°ºåº¦è®­ç»ƒç­–ç•¥ï¼Œæé«˜ç”Ÿæˆå¯¹è±¡çš„ç»†èŠ‚å’Œä¿çœŸåº¦ï¼›
ï¼ˆ4ï¼‰ï¼šV3Dä½¿ç”¨æ„ŸçŸ¥æŸå¤±å’Œå¯¹æŠ—æŸå¤±ä½œä¸ºè®­ç»ƒç›®æ ‡ï¼Œæé«˜ç”Ÿæˆå¯¹è±¡çš„è§†è§‰è´¨é‡å’Œå¤šæ ·æ€§ã€‚</p>
<ol>
<li>ç»“è®ºï¼š
ï¼ˆ1ï¼‰æœ¬å·¥ä½œé€šè¿‡å°†è§†é¢‘æ‰©æ•£æ¨¡å‹åº”ç”¨äº3Dç”Ÿæˆï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„æ–¹æ³•V3Dï¼Œåœ¨ç”Ÿæˆä¸€è‡´çš„å¤šè§†è§’å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚V3Dæ‰©å±•äº†è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨3Dç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œä¸ºé«˜è´¨é‡3Dç”Ÿæˆå’Œè§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨3Dä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚
ï¼ˆ2ï¼‰åˆ›æ–°ç‚¹ï¼š</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„3Dç”Ÿæˆæ–¹æ³•V3Dï¼Œé€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹é€æ­¥æ·»åŠ å™ªå£°ç”Ÿæˆ3Då¯¹è±¡ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§å®šåˆ¶çš„é‡å»ºç®¡é“ï¼Œç”¨äºä»ç”Ÿæˆçš„è§†å›¾ä¸­è·å–3Dèµ„äº§ï¼Œå¹¶æ”¯æŒåœ¨3åˆ†é’Ÿå†…é‡å»ºé«˜è´¨é‡çš„3Dç½‘æ ¼ã€‚</li>
<li>å°†V3Dæ‰©å±•åˆ°åœºæ™¯çº§æ–°è§†è§’åˆæˆï¼Œå®ç°äº†å¯¹æ‘„åƒæœºè·¯å¾„çš„ç²¾ç¡®æ§åˆ¶å’Œå¤šè§†è§’ä¸€è‡´æ€§ã€‚
æ€§èƒ½ï¼š</li>
<li>åœ¨ShapeNetæ•°æ®é›†ä¸Šï¼ŒV3Dåœ¨ç”Ÿæˆè´¨é‡å’Œç”Ÿæˆé€Ÿåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>V3Då¯ä»¥ç”Ÿæˆé«˜ä¿çœŸ3Då¯¹è±¡ï¼Œç”Ÿæˆæ—¶é—´ä»…éœ€3åˆ†é’Ÿã€‚</li>
<li>V3Dåœ¨ç”Ÿæˆä¸€è‡´çš„å¤šè§†è§’å›¾åƒå’Œåœºæ™¯çº§æ–°è§†è§’åˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚
å·¥ä½œé‡ï¼š</li>
<li>V3Dçš„å®ç°ç›¸å¯¹ç®€å•ï¼Œæ˜“äºä½¿ç”¨ã€‚</li>
<li>V3Dçš„è®­ç»ƒè¿‡ç¨‹é«˜æ•ˆï¼Œåœ¨ShapeNetæ•°æ®é›†ä¸Šè®­ç»ƒV3Dä»…éœ€æ•°å°æ—¶ã€‚</li>
<li>V3Dçš„æ¨ç†é€Ÿåº¦å¿«ï¼Œå¯ä»¥å¿«é€Ÿç”Ÿæˆ3Då¯¹è±¡ã€‚</li>
</ol>


<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c7c858eb0759a50450bc9e902b68068.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20859973aba31d5ec733373f6d25379e.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-03-13/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-03-13/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-03-13/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f16882204804b40a491523a7984bf7e2.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-13  A Comparative Study of Perceptual Quality Metrics for Audio-driven   Talking Head Videos
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-03-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-03-11/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-302c4a1ee77cbdfd8dba69c7d6a94497.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-03-11  VideoElevator Elevating Video Generation Quality with Versatile   Text-to-Image Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-03-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17862.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
