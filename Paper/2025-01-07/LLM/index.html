<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-01-07  VITA-1.5 Towards GPT-4o Level Real-Time Vision and Speech Interaction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-d038ca7ce01a33796accd474d3a68b23.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-07-更新"><a href="#2025-01-07-更新" class="headerlink" title="2025-01-07 更新"></a>2025-01-07 更新</h1><h2 id="VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction"><a href="#VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction" class="headerlink" title="VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"></a>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</h2><p><strong>Authors:Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. </p>
<blockquote>
<p>最近的多模态大型语言模型（MLLM）通常专注于整合视觉和文本模态，较少强调语音在增强交互中的作用。然而，语音在多模态对话系统中扮演着至关重要的角色，由于在根本的模态差异，实现在视觉和语音任务中的高性能仍然是一个巨大的挑战。在本文中，我们提出了一种精心设计的多阶段训练方法论，逐步训练LLM理解视觉和语音信息，最终实现流畅的视觉和语音交互。我们的方法不仅保留了强大的视觉语言功能，还实现了高效的语音对话能力，无需单独的ASR和TTS模块，从而显著加快了多模态端到端的响应速度。通过与图像、视频和语音任务的最新先进模型进行比较，我们证明了我们的模型具有强大的视觉和语音功能，可实现近乎实时的视觉和语音交互。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01957v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a></p>
<p><strong>Summary</strong>：</p>
<p>近期多模态大型语言模型（MLLMs）在集成视觉和文本模态方面投入了大量关注，但忽视了语音在增强交互中的作用。本文提出了一种精心设计的多阶段训练方法，逐步训练LLM理解视觉和语音信息，最终实现流畅的视听觉交互。该方法不仅保留了强大的视觉语言功能，还能实现高效的语音对话能力，无需单独的ASR和TTS模块，显著提高了多模态端到端的响应速度。通过与图像、视频和语音任务的最新技术相比，证明该模型具有强大的视觉和语音功能，可实现近乎实时的视听觉交互。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>多模态大型语言模型（MLLMs）在集成视觉和文本模态时忽视了语音的重要性。</li>
<li>语音在多模态对话系统中扮演重要角色。</li>
<li>实现视听觉任务的高性能是一个重大挑战，因为不同模态之间存在根本性差异。</li>
<li>提出了一种多阶段训练方法，使LLM能够处理视觉和语音信息，促进流畅的视听觉交互。</li>
<li>该方法不仅保持强大的视觉语言功能，还具备高效的语音对话能力。</li>
<li>该模型无需额外的ASR和TTS模块，提高了多模态端到端的响应速度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01957">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f6c7f87eb451dea02bc9161918cf5e03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-458784e265fee8832f68789bf9d105f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e262945a140a0fbcdb7ddae1a7741766.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-396c9589614e6d25affa510578046020.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-200583adacfa33383773d2e9f3835530.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Cold-Start-Recommendation-towards-the-Era-of-Large-Language-Models-LLMs-A-Comprehensive-Survey-and-Roadmap"><a href="#Cold-Start-Recommendation-towards-the-Era-of-Large-Language-Models-LLMs-A-Comprehensive-Survey-and-Roadmap" class="headerlink" title="Cold-Start Recommendation towards the Era of Large Language Models   (LLMs): A Comprehensive Survey and Roadmap"></a>Cold-Start Recommendation towards the Era of Large Language Models   (LLMs): A Comprehensive Survey and Roadmap</h2><p><strong>Authors:Weizhi Zhang, Yuanchen Bei, Liangwei Yang, Henry Peng Zou, Peilin Zhou, Aiwei Liu, Yinghui Li, Hao Chen, Jianling Wang, Yu Wang, Feiran Huang, Sheng Zhou, Jiajun Bu, Allen Lin, James Caverlee, Fakhri Karray, Irwin King, Philip S. Yu</strong></p>
<p>Cold-start problem is one of the long-standing challenges in recommender systems, focusing on accurately modeling new or interaction-limited users or items to provide better recommendations. Due to the diversification of internet platforms and the exponential growth of users and items, the importance of cold-start recommendation (CSR) is becoming increasingly evident. At the same time, large language models (LLMs) have achieved tremendous success and possess strong capabilities in modeling user and item information, providing new potential for cold-start recommendations. However, the research community on CSR still lacks a comprehensive review and reflection in this field. Based on this, in this paper, we stand in the context of the era of large language models and provide a comprehensive review and discussion on the roadmap, related literature, and future directions of CSR. Specifically, we have conducted an exploration of the development path of how existing CSR utilizes information, from content features, graph relations, and domain information, to the world knowledge possessed by large language models, aiming to provide new insights for both the research and industrial communities on CSR. Related resources of cold-start recommendations are collected and continuously updated for the community in <a target="_blank" rel="noopener" href="https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation">https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation</a>. </p>
<blockquote>
<p>冷启动问题是推荐系统长期以来的挑战之一，主要关注如何对新用户或交互受限的用户或项目进行准确建模，以提供更好的推荐。由于互联网平台的多样化和用户项目的指数级增长，冷启动推荐（CSR）的重要性变得越来越明显。同时，大型语言模型（LLM）已经取得了巨大的成功，在建模用户和项目信息方面拥有强大的能力，为冷启动推荐提供了新的潜力。然而，CSR研究领域仍然缺乏对该领域的全面回顾和反思。基于此，本文站在大型语言模型的时代背景下，全面回顾和讨论CSR的发展道路、相关文献和未来方向。具体来说，我们探索了现有CSR如何利用信息的发展路径，从内容特征、图关系到领域信息，再到大型语言模型所拥有的世界知识，旨在为CSR的研究和工业界提供新的见解。冷启动推荐的相关资源已收集并在<a target="_blank" rel="noopener" href="https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation">https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation</a>上持续更新，以供相关社区使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01945v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型时代下的冷启动推荐挑战综述。本文全面回顾和讨论了冷启动推荐（CSR）的研究进展、相关文献和未来方向。文章探讨了现有CSR如何利用信息的发展路径，从内容特征、图关系到领域信息，再到大型语言模型所拥有的世界知识，为CSR的研究和工业界提供了新的见解。相关资源可通过<a target="_blank" rel="noopener" href="https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation获取。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>冷启动问题是推荐系统长期存在的挑战，主要针对新用户或交互受限的用户或项目进行准确建模，以提供更好的推荐。</li>
<li>随着互联网平台的多样化和用户项目的指数级增长，冷启动推荐（CSR）的重要性日益凸显。</li>
<li>大型语言模型（LLMs）在建模用户和项目信息方面取得了巨大成功，为冷启动推荐提供了新的潜力。</li>
<li>现有CSR的研究在如何利用信息方面进行了探索，包括内容特征、图关系和领域信息。</li>
<li>CSR的研究进展和未来方向进行了全面回顾和讨论，为研究和工业界提供了新的见解。</li>
<li>可以通过<a target="_blank" rel="noopener" href="https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation%E8%8E%B7%E5%8F%96%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation获取相关资源。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01945">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cd7a17d320f150a8bd1304999f8a879e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46fa0449b345679b4c360be57bcaba92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af95844e49b3801c44e7a651931067ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79022001edd677ed58b3e3ea31ed6c4f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Virgo-A-Preliminary-Exploration-on-Reproducing-o1-like-MLLM"><a href="#Virgo-A-Preliminary-Exploration-on-Reproducing-o1-like-MLLM" class="headerlink" title="Virgo: A Preliminary Exploration on Reproducing o1-like MLLM"></a>Virgo: A Preliminary Exploration on Reproducing o1-like MLLM</h2><p><strong>Authors:Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen</strong></p>
<p>Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.   To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Virgo">https://github.com/RUCAIBox/Virgo</a>. </p>
<blockquote>
<p>最近，基于大型语言模型（LLM）的慢思考推理系统已经引起了广泛关注，它们通过扩展推理时间来扩大规模。将这一能力适应于多模态大型语言模型（MLLM）的兴趣也在日益增长。考虑到MLLM处理不同模态下更复杂的数据语义，实现多模态慢思考系统更具挑战性。为了解决这个问题，本文探索了一种简单的方法，即通过少量文本长思考数据对功能强大的MLLM进行微调，从而构建了一个多模态慢思考系统Virgo（视觉推理与长思考）。我们发现，用自然语言表达的长思考过程可以有效地转移到MLLM上。而且，似乎这种文本推理数据在激发MLLM的慢思考能力方面甚至比视觉推理数据更有效。虽然这项工作只是初步的，但它证明了慢思考能力从根本上与语言模型组件相关联，可以跨模态或领域进行转移。这一发现可用于指导开发更强大的慢思考推理系统。我们在<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Virgo">https://github.com/RUCAIBox/Virgo</a>上发布了我们的资源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01904v1">PDF</a> Technical Report on Slow Thinking with LLMs: Visual Reasoning</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的慢思考推理系统通过扩展推理时间获得了广泛关注。为适应多模态大型语言模型（MLLM）的这一能力，研究者们正不断探索。本文探索了一种简单的方法，即通过微调具备能力的MLLM与少量文本长形式思维数据，实现多模态慢思考系统——维京（Visual reasoning with long thought）。研究结果表明，自然语言表达的长期推理过程可以有效地转移到MLLM上，并且这种文本推理数据似乎比视觉推理数据更能激发MLLM的慢思考能力。这为开发更强大的慢思考推理系统提供了重要启示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>慢思考推理系统通过扩展大型语言模型的推理时间而受到广泛关注。</li>
<li>多模态大型语言模型的慢思考能力适应是一个新兴的研究方向。</li>
<li>通过微调具备能力的多模态大型语言模型与少量文本长形式思维数据，可以实现多模态慢思考系统——维京（Virgo）。</li>
<li>自然语言的长期推理过程可以有效地转移到多模态大型语言模型上。</li>
<li>文本推理数据比视觉推理数据更能激发多模态大型语言模型的慢思考能力。</li>
<li>维京系统的发现表明慢思考能力根本上与语言模型组件相关，可跨模态或领域进行转移。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01904">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a5fbe97bd1212b71db61d370b3397be4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df6ee89efbb8872580676dc28e357613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8397b0ed0c4639773591021f86d47def.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40b58f4c4bb67d1dc3dbf31df5a966ee.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AgentRefine-Enhancing-Agent-Generalization-through-Refinement-Tuning"><a href="#AgentRefine-Enhancing-Agent-Generalization-through-Refinement-Tuning" class="headerlink" title="AgentRefine: Enhancing Agent Generalization through Refinement Tuning"></a>AgentRefine: Enhancing Agent Generalization through Refinement Tuning</h2><p><strong>Authors:Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu</strong></p>
<p>Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research. </p>
<blockquote>
<p>基于大型语言模型（LLM）的代理已经证明了它们执行复杂任务的能力，类似于人类。然而，开源LLM和商业模型（如GPT系列）之间仍存在很大差距。在本文中，我们专注于通过指令微调提高LLM的代理泛化能力。我们首先观察到，现有的代理训练语料库在保留的评价集上取得了令人满意的结果，但未能推广到保留外的数据集。这些代理调整工作面临着严重的格式错误，并且经常长时间陷入同样的错误。我们分析认为，泛化能力差的根源在于对几种手动代理环境的过度适应以及对新情况的适应不足。他们难以执行错误的行动步骤，无法从经验中学习，而只是记住现有的观察-行动关系。受此启发，我们提出了一种新型的AgentRefine框架用于代理调整。核心思想是使模型能够通过轨迹中的观察来纠正自己的错误。具体来说，我们提出了一个代理合成框架，以涵盖各种环境和任务，并提示强大的LLM根据环境反馈细化其错误行动。AgentRefine在多种代理任务上的泛化能力方面显著优于最新的代理调整工作。它还具有更好的抗扰动性，并能在推理中产生多样化的想法。我们的发现建立了代理泛化和自我完善之间的关联，并为未来的研究提供了新的范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01702v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大语言模型（LLM）已展现出类似人类处理复杂任务的能力。但开源LLM与商业模型如GPT系列间仍存在差距。本文聚焦于通过指令微调提升LLM的代理泛化能力。研究发现，现有代理训练语料库在内部评估集上表现良好，但在外部评估集上泛化失败。本文分析了其源于过度适应手动代理环境以及难以适应新情境的问题。为此，本文提出了新颖的AgentRefine框架进行代理调整，其核心在于让模型通过轨迹观察学习纠正错误。AgentRefine显著提升了现有代理调整工作在多样化代理任务上的泛化能力，并具有更好的抗扰动性和推理多样性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM已展现出处理复杂任务的能力，但与商业模型如GPT系列相比仍有差距。</li>
<li>现有LLM代理训练存在过度适应特定环境的问题，导致在新情境下泛化能力弱。</li>
<li>AgentRefine框架旨在提升LLM的代理泛化能力，通过让模型学习纠正错误来提高性能。</li>
<li>AgentRefine框架包含多样化的环境任务，并提示强LLM根据环境反馈修正错误行动。</li>
<li>AgentRefine在多样化代理任务上的泛化能力显著优于现有工作。</li>
<li>AgentRefine具有更好的抗扰动性，能在推理过程中产生多样化思考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01702">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d038ca7ce01a33796accd474d3a68b23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88161e3dc0e407c08178fa9ee8f00577.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ded54c4fc8f22277871fda225df4e9ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4af506b5091c5002bfebb929d32ae1b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dbd55274cb9289be6953d7d5368945a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7b2ca290aff6d45948646de103b3bbf.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Qwen2-5-Technical-Report"><a href="#Qwen2-5-Technical-Report" class="headerlink" title="Qwen2.5 Technical Report"></a>Qwen2.5 Technical Report</h2><p><strong>Authors: Qwen,  :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu</strong></p>
<p>In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models. </p>
<blockquote>
<p>在这份报告中，我们介绍了Qwen2.5，这是一系列为了满足不同需求而设计的大型语言模型（LLM）。相较于之前的版本，Qwen 2.5在预训练和后训练阶段都得到了显著提升。在预训练方面，我们将高质量的预训练数据集从之前的7万亿标记扩展到18万亿标记。这为常识、专业知识和推理能力提供了坚实的基础。在后训练方面，我们实施了复杂的监督微调，使用了超过1百万个样本，以及多阶段强化学习。后训练技术提高了人类偏好，并显著改善了长文本生成、结构化数据分析和指令遵循能力。为了有效地处理各种用例，我们推出了丰富的Qwen2.5 LLM系列。开放权重产品包括基础版和指令调优模型，也有量化版本可供选择。此外，对于托管解决方案，专有模型目前包括两个混合专家（MoE）变体：Qwen2.5-Turbo和Qwen2.5-Plus，两者都可在阿里云模型工作室中使用。Qwen2.5已在广泛的语言理解、推理、数学、编码、人类偏好对齐等基准测试中表现出卓越性能。特别是开放权重的旗舰产品Qwen2.5-72B-Instruct，在多个公开和专有模型中表现出色，与最先进的开放权重模型Llama-3-405B-Instruct相比具有竞争力，尽管后者规模大约是前者的5倍。Qwen2.5-Turbo和Qwen2.5-Plus提供了出色的性价比，与GPT-4o-mini和GPT-4o相比表现良好。此外，作为基石，Qwen2.5模型在训练专业模型如Qwen2.5-Math、Qwen2.5-Coder、QwQ和多模态模型中发挥了重要作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15115v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了Qwen2.5系列大型语言模型（LLMs），该系列模型旨在满足不同的需求。相较于之前的版本，Qwen 2.5在预训练和后训练阶段都有显著的提升。在预训练方面，高质量预训练数据集从之前的7万亿标记扩展到了18万亿标记，为常识、专业知识和推理能力提供了坚实的基础。后训练方面，采用复杂的监督微调技术，使用超过1百万样本进行训练，并采用多阶段强化学习。这些后训练技术提高了对人类偏好的符合度，并显著改善了长文本生成、结构化数据分析和指令遵循能力。为了有效应对各种用例，我们推出了丰富的Qwen2.5 LLM系列。开放权重产品包括基础版和指令调优模型，还有量化版本。此外，对于托管解决方案，还包括两个专有模型MoE（混合专家）变体：Qwen2.5-Turbo和Qwen2.5-Plus，两者均可在阿里云模型工作室通过阿里巴巴云获得。Qwen2.5系列模型在多种评估基准测试中表现优异，如语言理解、推理、数学、编码、人类偏好对齐等。特别是开放权重的旗舰产品Qwen2.5-72B-Instruct在多个公开和专有模型中表现出色，与当前先进的开放权重模型Llama-3-405B-Instruct相比也具有竞争力。而Qwen2.5-Turbo和Qwen2.5-Plus在成本效益方面表现优异。此外，Qwen2.5系列模型还为训练专业模型和多模态模型提供了基础。</p>
<p><strong>要点总结</strong></p>
<ol>
<li>Qwen2.5系列大型语言模型（LLMs）为满足不同需求而设计。</li>
<li>相较于之前版本，Qwen 2.5在预训练数据集、后训练技术和性能上有所提升。</li>
<li>Qwen 2.5系列包括多种规模和类型的模型，如基础版、指令调优版、量化版等。</li>
<li>Qwen2.5在后训练方面采用了复杂的监督微调技术和多阶段强化学习，提高了长文本生成、结构化数据分析和指令遵循能力。</li>
<li>Qwen2.5系列在多种基准测试中表现优异，包括语言理解、推理、数学、编码等。</li>
<li>旗舰产品Qwen2.5-72B-Instruct在公开和专有模型中表现出色，与当前最先进的模型具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15115">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f7ee645251758f7620cfd0d6ccd74960.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48058794ecf9bdf047a296b899c8ac64.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OmniFlatten-An-End-to-end-GPT-Model-for-Seamless-Voice-Conversation"><a href="#OmniFlatten-An-End-to-end-GPT-Model-for-Seamless-Voice-Conversation" class="headerlink" title="OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation"></a>OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation</h2><p><strong>Authors:Qinglin Zhang, Luyao Cheng, Chong Deng, Qian Chen, Wen Wang, Siqi Zheng, Jiaqing Liu, Hai Yu, Chaohong Tan, Zhihao Du, Shiliang Zhang</strong></p>
<p>Full-duplex spoken dialogue systems significantly surpass traditional turn-based dialogue systems, as they allow simultaneous bidirectional communication, closely mirroring human-human interactions. However, achieving low latency and natural interactions in full-duplex dialogue systems remains a significant challenge, especially considering human conversation dynamics such as interruptions, backchannels, and overlapping speech. In this paper, we introduce a novel End-to-End GPT-based model OmniFlatten for full-duplex conversation, capable of effectively modeling the complex behaviors inherent to natural conversations with low latency. To achieve full-duplex conversation capabilities, we propose a multi-stage post-training scheme that progressively adapts a text large language model (LLM) backbone into a speech-text dialogue LLM, capable of generating text and speech in real time, without modifying the architecture of the backbone LLM. The training process comprises three stages: modality alignment, half-duplex dialogue learning, and full-duplex dialogue learning. In all training stages, we standardize the data using a flattening operation, which enables unifying the training methods and the GPT backbone across different modalities and tasks. Our approach offers a simple modeling technique and a promising research direction for developing efficient and natural end-to-end full-duplex spoken dialogue systems. Audio samples of dialogues generated by OmniFlatten can be found at this web site (<a target="_blank" rel="noopener" href="https://omniflatten.github.io/">https://omniflatten.github.io/</a>). </p>
<blockquote>
<p>全双工对话系统显著超越了传统的基于轮转的对话系统，因为它们允许同时双向通信，紧密地模拟了人与人之间的互动。然而，在全双工对话系统中实现低延迟和自然交互仍然是一个重大挑战，尤其是在考虑人类对话动态（如中断、反馈通道和重叠语音）时。在本文中，我们引入了一种基于GPT的新型端到端全双工对话模型OmniFlatten，它能够有效地对自然对话中固有的复杂行为进行建模，实现低延迟。为了实现全双工对话功能，我们提出了一种多阶段后训练方案，该方案逐步将一个文本大型语言模型（LLM）主干改造为一个语音文本对话LLM，能够实时生成文本和语音，无需修改主干LLM的架构。训练过程包括三个阶段：模态对齐、半双工对话学习和全双工对话学习。在所有训练阶段中，我们使用展平操作来标准化数据，这使我们能够在不同的模态和任务中使用统一的训练方法和GPT主干。我们的方法提供了一种简单的建模技术，并为开发高效、自然的端到端全双工对话系统提供了一个有前景的研究方向。OmniFlatten生成对话的音频样本可以在这个网站找到（<a target="_blank" rel="noopener" href="https://omniflatten.github.io/%EF%BC%89%E3%80%82">https://omniflatten.github.io/）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17799v2">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>全双工对话系统允许同时双向通信，更贴近人与人之间的交流。本文提出了一种基于GPT的OmniFlatten模型，用于全双工对话，有效建模自然对话的复杂行为并实现低延迟。通过多阶段后训练方案，将文本大型语言模型（LLM）改编为语音文本对话LLM，实现实时生成文本和语音，无需修改架构。此方法为开发高效、自然的端到端全双工对话系统提供了简单建模技巧和具有前景的研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>全双工对话系统模拟人类交流，允许同时双向通信。</li>
<li>OmniFlatten模型用于全双工对话，有效建模自然对话的复杂行为。</li>
<li>提出多阶段后训练方案，使文本LLM具备语音文本对话能力。</li>
<li>训练过程中采用数据标准化操作，统一不同模态和任务下的训练方法。</li>
<li>OmniFlatten模型可实现低延迟对话。</li>
<li>模型在多种对话模态下具有通用性。</li>
<li>音频样本可在指定网站找到。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17799">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d80bae390aae70960cef2d63dc55564.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9489cf8ace90506ea752cbe69810cb3e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b6b2a919827e26b2ea80272d06b6f99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34cdc3674402202cf8981978a0590484.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Knowledge-Circuits-in-Pretrained-Transformers"><a href="#Knowledge-Circuits-in-Pretrained-Transformers" class="headerlink" title="Knowledge Circuits in Pretrained Transformers"></a>Knowledge Circuits in Pretrained Transformers</h2><p><strong>Authors:Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen</strong></p>
<p>The remarkable capabilities of modern large language models are rooted in their vast repositories of knowledge encoded within their parameters, enabling them to perceive the world and engage in reasoning. The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers. To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head. In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge. The experiments, conducted with GPT2 and TinyLLAMA, have allowed us to observe how certain information heads, relation heads, and Multilayer Perceptrons collaboratively encode knowledge within the model. Moreover, we evaluate the impact of current knowledge editing techniques on these knowledge circuits, providing deeper insights into the functioning and constraints of these editing methodologies. Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning. We believe the knowledge circuits hold potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing. Code and data are available in <a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowledgeCircuits">https://github.com/zjunlp/KnowledgeCircuits</a>. </p>
<blockquote>
<p>现代大型语言模型的卓越能力根植于其参数中编码的庞大知识库，使它们能够感知世界并参与推理。这些模型如何存储知识的内部工作机制长期以来一直是研究人员关注的热点和调查对象。迄今为止，大多数研究都集中在这些模型的孤立组件上，如多层感知器和注意力头。在本文中，我们深入语言模型的计算图，以揭示对于表达特定知识至关重要的知识回路。我们与GPT2和TinyLLAMA进行的实验使我们能够观察到某些信息头、关系头和多层感知器如何在模型中协同编码知识。此外，我们还评估了当前知识编辑技术对这些知识回路的影响，为这些编辑方法的运作和局限性提供了更深入的了解。最后，我们利用知识回路分析和解释语言模型的行为，如幻觉和上下文学习。我们相信知识回路在推动我们对Transformer的理解以及指导知识编辑的改进设计方面具有潜力。代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowledgeCircuits%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjunlp/KnowledgeCircuits找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.17969v4">PDF</a> NeurIPS 2024, 26 pages</p>
<p><strong>Summary</strong><br>现代大型语言模型的强大能力源于其参数中编码的丰富知识库，能够感知世界并进行推理。本文深入探究了语言模型的计算图，揭示了表达特定知识的关键知识回路。通过GPT2和TinyLLAMA的实验，我们观察到信息头、关系头和多层感知器等如何协同在模型中编码知识。此外，本文还评估了当前知识编辑技术对知识回路的影响，为这些编辑方法的运作和局限性提供了深刻见解。最后，我们利用知识回路分析和解释语言模型的行为，如幻觉和上下文学习。知识回路有助于我们更好地理解Transformer并引导知识编辑的改进设计。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现代大型语言模型的强大能力源于其参数中编码的丰富知识库。</li>
<li>语言模型的知识回路对于表达特定知识至关重要。</li>
<li>信息头、关系头和多层感知器等协同在模型中编码知识。</li>
<li>当前知识编辑技术对知识回路有影响。</li>
<li>知识回路可用于分析和解释语言模型的行为，如幻觉和上下文学习。</li>
<li>知识回路研究有助于更好地理解Transformer。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.17969">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ad47fb0b4724e2d15740b7bbd23fba0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d3cb131479da7b37c780048cc82c152.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8734983426fdd7a6d09d51c8e813eb10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4262ffa2ba0634752220b6817b9d40ab.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Agent-Planning-with-World-Knowledge-Model"><a href="#Agent-Planning-with-World-Knowledge-Model" class="headerlink" title="Agent Planning with World Knowledge Model"></a>Agent Planning with World Knowledge Model</h2><p><strong>Authors:Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</strong></p>
<p>Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the &#96;&#96;real’’ physical world. Imitating humans’ mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent’s understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/WKM">https://github.com/zjunlp/WKM</a>. </p>
<blockquote>
<p>近期，直接使用大型语言模型（LLM）作为代理模型来执行交互式规划任务的研究取得了值得赞扬的成果。尽管取得了成就，然而，它们在全局规划方面仍面临着盲目的试错问题，在局部规划方面由于缺乏对“真实”物理世界的理解，会产生幻觉行为。</p>
</blockquote>
<p>本文中，我们引入参数化世界知识模型（WKM），以辅助代理规划，模仿人类在执行任务前提供全局先验知识，并在任务过程中保持局部动态知识。具体来说，我们引导代理模型从专家轨迹和采样轨迹中自行合成知识。然后，我们开发WKM，提供任务先验知识以指导全局规划，并提供动态状态知识以辅助局部规划。</p>
<p>在三个复杂真实世界模拟数据集上，使用三种最新开源LLM（Mistral-7B、Gemma-7B和Llama-3-8B）进行的实验结果表明，我们的方法相比各种强大的基线方法可以实现优越的性能。此外，我们通过分析证明，我们的WKM可以有效地减轻盲目试错和幻觉行为问题，为代理对世界的理解提供强有力的支持。其他有趣的发现包括：1）我们的实例级任务知识可以更好地推广到未见过的任务；2）弱的WKM可以指导强代理模型规划；3）统一的WKM训练具有进一步开发的潜力。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14205v4">PDF</a> NeurIPS 2024</p>
<p><strong>摘要</strong></p>
<p>本文介绍了参数化世界知识模型（WKM）在促进基于大语言模型（LLM）的代理规划方面的应用。通过结合专家轨迹和采样轨迹，WKM为代理模型提供任务先验知识，以指导全局规划，并提供动态状态知识以辅助局部规划。实验结果表明，该方法在三个复杂现实世界的模拟数据集上，使用三种最新的开源LLM，相较于各种强大的基线方法，取得了卓越的性能。此外，WKM能有效缓解盲目试错和幻觉动作问题，为代理对世界的理解提供了强有力的支持。研究还发现，实例级任务知识可以更好泛化到未见过的任务，弱WKM可以指导强代理模型规划，统一WKM训练具有进一步开发的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>介绍大语言模型（LLM）在交互式规划任务中的直接应用已取得显著成果。</li>
<li>LLM仍面临全球规划中的无脑试错和局部规划中的幻觉动作问题。</li>
<li>提出参数化世界知识模型（WKM）以促进代理规划。</li>
<li>WKM能结合专家轨迹和采样轨迹，为代理模型提供先验任务知识和动态状态知识。</li>
<li>实验证明WKM在复杂现实世界的模拟数据集上表现优越。</li>
<li>WKM能有效缓解盲目试错和幻觉动作问题，增强代理对世界的理解。</li>
<li>研究发现实例级任务知识泛化性好，弱WKM可指导强代理模型规划，统一WKM训练具有发展潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14205">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-10e548841f8670214eada6a38e4e8c4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c988b7da694d55c09897345596a1403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1b82792178546b9e8265d0e4b645cf5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-07/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-07/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-07/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e1b82792178546b9e8265d0e4b645cf5.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-01-07  QuArch A Question-Answering Dataset for AI Agents in Computer   Architecture
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d5dc747136839484c54d22b2d79fc207.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-01-06  VideoAnydoor High-fidelity Video Object Insertion with Precise Motion   Control
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24417.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
