<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-07  VITA-1.5 Towards GPT-4o Level Real-Time Vision and Speech Interaction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-d038ca7ce01a33796accd474d3a68b23.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    35 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-07-æ›´æ–°"><a href="#2025-01-07-æ›´æ–°" class="headerlink" title="2025-01-07 æ›´æ–°"></a>2025-01-07 æ›´æ–°</h1><h2 id="VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction"><a href="#VITA-1-5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction" class="headerlink" title="VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"></a>VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</h2><p><strong>Authors:Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é€šå¸¸ä¸“æ³¨äºæ•´åˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€ï¼Œè¾ƒå°‘å¼ºè°ƒè¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œè¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œç”±äºåœ¨æ ¹æœ¬çš„æ¨¡æ€å·®å¼‚ï¼Œå®ç°åœ¨è§†è§‰å’Œè¯­éŸ³ä»»åŠ¡ä¸­çš„é«˜æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•è®ºï¼Œé€æ­¥è®­ç»ƒLLMç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œæœ€ç»ˆå®ç°æµç•…çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè¿˜å®ç°äº†é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ— éœ€å•ç‹¬çš„ASRå’ŒTTSæ¨¡å—ï¼Œä»è€Œæ˜¾è‘—åŠ å¿«äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚é€šè¿‡ä¸å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡çš„æœ€æ–°å…ˆè¿›æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³åŠŸèƒ½ï¼Œå¯å®ç°è¿‘ä¹å®æ—¶çš„è§†è§‰å’Œè¯­éŸ³äº¤äº’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01957v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/VITA-MLLM/VITA">https://github.com/VITA-MLLM/VITA</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é›†æˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€æ–¹é¢æŠ•å…¥äº†å¤§é‡å…³æ³¨ï¼Œä½†å¿½è§†äº†è¯­éŸ³åœ¨å¢å¼ºäº¤äº’ä¸­çš„ä½œç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç²¾å¿ƒè®¾è®¡çš„å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé€æ­¥è®­ç»ƒLLMç†è§£è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œæœ€ç»ˆå®ç°æµç•…çš„è§†å¬è§‰äº¤äº’ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¿ç•™äº†å¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè¿˜èƒ½å®ç°é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ï¼Œæ— éœ€å•ç‹¬çš„ASRå’ŒTTSæ¨¡å—ï¼Œæ˜¾è‘—æé«˜äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚é€šè¿‡ä¸å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³ä»»åŠ¡çš„æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œè¯æ˜è¯¥æ¨¡å‹å…·æœ‰å¼ºå¤§çš„è§†è§‰å’Œè¯­éŸ³åŠŸèƒ½ï¼Œå¯å®ç°è¿‘ä¹å®æ—¶çš„è§†å¬è§‰äº¤äº’ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é›†æˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€æ—¶å¿½è§†äº†è¯­éŸ³çš„é‡è¦æ€§ã€‚</li>
<li>è¯­éŸ³åœ¨å¤šæ¨¡æ€å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>å®ç°è§†å¬è§‰ä»»åŠ¡çš„é«˜æ€§èƒ½æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºä¸åŒæ¨¡æ€ä¹‹é—´å­˜åœ¨æ ¹æœ¬æ€§å·®å¼‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œä½¿LLMèƒ½å¤Ÿå¤„ç†è§†è§‰å’Œè¯­éŸ³ä¿¡æ¯ï¼Œä¿ƒè¿›æµç•…çš„è§†å¬è§‰äº¤äº’ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…ä¿æŒå¼ºå¤§çš„è§†è§‰è¯­è¨€åŠŸèƒ½ï¼Œè¿˜å…·å¤‡é«˜æ•ˆçš„è¯­éŸ³å¯¹è¯èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹æ— éœ€é¢å¤–çš„ASRå’ŒTTSæ¨¡å—ï¼Œæé«˜äº†å¤šæ¨¡æ€ç«¯åˆ°ç«¯çš„å“åº”é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6c7f87eb451dea02bc9161918cf5e03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-458784e265fee8832f68789bf9d105f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e262945a140a0fbcdb7ddae1a7741766.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-396c9589614e6d25affa510578046020.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-200583adacfa33383773d2e9f3835530.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Cold-Start-Recommendation-towards-the-Era-of-Large-Language-Models-LLMs-A-Comprehensive-Survey-and-Roadmap"><a href="#Cold-Start-Recommendation-towards-the-Era-of-Large-Language-Models-LLMs-A-Comprehensive-Survey-and-Roadmap" class="headerlink" title="Cold-Start Recommendation towards the Era of Large Language Models   (LLMs): A Comprehensive Survey and Roadmap"></a>Cold-Start Recommendation towards the Era of Large Language Models   (LLMs): A Comprehensive Survey and Roadmap</h2><p><strong>Authors:Weizhi Zhang, Yuanchen Bei, Liangwei Yang, Henry Peng Zou, Peilin Zhou, Aiwei Liu, Yinghui Li, Hao Chen, Jianling Wang, Yu Wang, Feiran Huang, Sheng Zhou, Jiajun Bu, Allen Lin, James Caverlee, Fakhri Karray, Irwin King, Philip S. Yu</strong></p>
<p>Cold-start problem is one of the long-standing challenges in recommender systems, focusing on accurately modeling new or interaction-limited users or items to provide better recommendations. Due to the diversification of internet platforms and the exponential growth of users and items, the importance of cold-start recommendation (CSR) is becoming increasingly evident. At the same time, large language models (LLMs) have achieved tremendous success and possess strong capabilities in modeling user and item information, providing new potential for cold-start recommendations. However, the research community on CSR still lacks a comprehensive review and reflection in this field. Based on this, in this paper, we stand in the context of the era of large language models and provide a comprehensive review and discussion on the roadmap, related literature, and future directions of CSR. Specifically, we have conducted an exploration of the development path of how existing CSR utilizes information, from content features, graph relations, and domain information, to the world knowledge possessed by large language models, aiming to provide new insights for both the research and industrial communities on CSR. Related resources of cold-start recommendations are collected and continuously updated for the community in <a target="_blank" rel="noopener" href="https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation">https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation</a>. </p>
<blockquote>
<p>å†·å¯åŠ¨é—®é¢˜æ˜¯æ¨èç³»ç»Ÿé•¿æœŸä»¥æ¥çš„æŒ‘æˆ˜ä¹‹ä¸€ï¼Œä¸»è¦å…³æ³¨å¦‚ä½•å¯¹æ–°ç”¨æˆ·æˆ–äº¤äº’å—é™çš„ç”¨æˆ·æˆ–é¡¹ç›®è¿›è¡Œå‡†ç¡®å»ºæ¨¡ï¼Œä»¥æä¾›æ›´å¥½çš„æ¨èã€‚ç”±äºäº’è”ç½‘å¹³å°çš„å¤šæ ·åŒ–å’Œç”¨æˆ·é¡¹ç›®çš„æŒ‡æ•°çº§å¢é•¿ï¼Œå†·å¯åŠ¨æ¨èï¼ˆCSRï¼‰çš„é‡è¦æ€§å˜å¾—è¶Šæ¥è¶Šæ˜æ˜¾ã€‚åŒæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œåœ¨å»ºæ¨¡ç”¨æˆ·å’Œé¡¹ç›®ä¿¡æ¯æ–¹é¢æ‹¥æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä¸ºå†·å¯åŠ¨æ¨èæä¾›äº†æ–°çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒCSRç ”ç©¶é¢†åŸŸä»ç„¶ç¼ºä¹å¯¹è¯¥é¢†åŸŸçš„å…¨é¢å›é¡¾å’Œåæ€ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡ç«™åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ—¶ä»£èƒŒæ™¯ä¸‹ï¼Œå…¨é¢å›é¡¾å’Œè®¨è®ºCSRçš„å‘å±•é“è·¯ã€ç›¸å…³æ–‡çŒ®å’Œæœªæ¥æ–¹å‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç°æœ‰CSRå¦‚ä½•åˆ©ç”¨ä¿¡æ¯çš„å‘å±•è·¯å¾„ï¼Œä»å†…å®¹ç‰¹å¾ã€å›¾å…³ç³»åˆ°é¢†åŸŸä¿¡æ¯ï¼Œå†åˆ°å¤§å‹è¯­è¨€æ¨¡å‹æ‰€æ‹¥æœ‰çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œæ—¨åœ¨ä¸ºCSRçš„ç ”ç©¶å’Œå·¥ä¸šç•Œæä¾›æ–°çš„è§è§£ã€‚å†·å¯åŠ¨æ¨èçš„ç›¸å…³èµ„æºå·²æ”¶é›†å¹¶åœ¨<a target="_blank" rel="noopener" href="https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation">https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation</a>ä¸ŠæŒç»­æ›´æ–°ï¼Œä»¥ä¾›ç›¸å…³ç¤¾åŒºä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01945v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä»£ä¸‹çš„å†·å¯åŠ¨æ¨èæŒ‘æˆ˜ç»¼è¿°ã€‚æœ¬æ–‡å…¨é¢å›é¡¾å’Œè®¨è®ºäº†å†·å¯åŠ¨æ¨èï¼ˆCSRï¼‰çš„ç ”ç©¶è¿›å±•ã€ç›¸å…³æ–‡çŒ®å’Œæœªæ¥æ–¹å‘ã€‚æ–‡ç« æ¢è®¨äº†ç°æœ‰CSRå¦‚ä½•åˆ©ç”¨ä¿¡æ¯çš„å‘å±•è·¯å¾„ï¼Œä»å†…å®¹ç‰¹å¾ã€å›¾å…³ç³»åˆ°é¢†åŸŸä¿¡æ¯ï¼Œå†åˆ°å¤§å‹è¯­è¨€æ¨¡å‹æ‰€æ‹¥æœ‰çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œä¸ºCSRçš„ç ”ç©¶å’Œå·¥ä¸šç•Œæä¾›äº†æ–°çš„è§è§£ã€‚ç›¸å…³èµ„æºå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendationè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†·å¯åŠ¨é—®é¢˜æ˜¯æ¨èç³»ç»Ÿé•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ï¼Œä¸»è¦é’ˆå¯¹æ–°ç”¨æˆ·æˆ–äº¤äº’å—é™çš„ç”¨æˆ·æˆ–é¡¹ç›®è¿›è¡Œå‡†ç¡®å»ºæ¨¡ï¼Œä»¥æä¾›æ›´å¥½çš„æ¨èã€‚</li>
<li>éšç€äº’è”ç½‘å¹³å°çš„å¤šæ ·åŒ–å’Œç”¨æˆ·é¡¹ç›®çš„æŒ‡æ•°çº§å¢é•¿ï¼Œå†·å¯åŠ¨æ¨èï¼ˆCSRï¼‰çš„é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å»ºæ¨¡ç”¨æˆ·å’Œé¡¹ç›®ä¿¡æ¯æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä¸ºå†·å¯åŠ¨æ¨èæä¾›äº†æ–°çš„æ½œåŠ›ã€‚</li>
<li>ç°æœ‰CSRçš„ç ”ç©¶åœ¨å¦‚ä½•åˆ©ç”¨ä¿¡æ¯æ–¹é¢è¿›è¡Œäº†æ¢ç´¢ï¼ŒåŒ…æ‹¬å†…å®¹ç‰¹å¾ã€å›¾å…³ç³»å’Œé¢†åŸŸä¿¡æ¯ã€‚</li>
<li>CSRçš„ç ”ç©¶è¿›å±•å’Œæœªæ¥æ–¹å‘è¿›è¡Œäº†å…¨é¢å›é¡¾å’Œè®¨è®ºï¼Œä¸ºç ”ç©¶å’Œå·¥ä¸šç•Œæä¾›äº†æ–°çš„è§è§£ã€‚</li>
<li>å¯ä»¥é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation%E8%8E%B7%E5%8F%96%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendationè·å–ç›¸å…³èµ„æºã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd7a17d320f150a8bd1304999f8a879e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46fa0449b345679b4c360be57bcaba92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af95844e49b3801c44e7a651931067ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79022001edd677ed58b3e3ea31ed6c4f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Virgo-A-Preliminary-Exploration-on-Reproducing-o1-like-MLLM"><a href="#Virgo-A-Preliminary-Exploration-on-Reproducing-o1-like-MLLM" class="headerlink" title="Virgo: A Preliminary Exploration on Reproducing o1-like MLLM"></a>Virgo: A Preliminary Exploration on Reproducing o1-like MLLM</h2><p><strong>Authors:Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen</strong></p>
<p>Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.   To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Virgo">https://github.com/RUCAIBox/Virgo</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ…¢æ€è€ƒæ¨ç†ç³»ç»Ÿå·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå®ƒä»¬é€šè¿‡æ‰©å±•æ¨ç†æ—¶é—´æ¥æ‰©å¤§è§„æ¨¡ã€‚å°†è¿™ä¸€èƒ½åŠ›é€‚åº”äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å…´è¶£ä¹Ÿåœ¨æ—¥ç›Šå¢é•¿ã€‚è€ƒè™‘åˆ°MLLMå¤„ç†ä¸åŒæ¨¡æ€ä¸‹æ›´å¤æ‚çš„æ•°æ®è¯­ä¹‰ï¼Œå®ç°å¤šæ¨¡æ€æ…¢æ€è€ƒç³»ç»Ÿæ›´å…·æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æ¢ç´¢äº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œå³é€šè¿‡å°‘é‡æ–‡æœ¬é•¿æ€è€ƒæ•°æ®å¯¹åŠŸèƒ½å¼ºå¤§çš„MLLMè¿›è¡Œå¾®è°ƒï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€æ…¢æ€è€ƒç³»ç»ŸVirgoï¼ˆè§†è§‰æ¨ç†ä¸é•¿æ€è€ƒï¼‰ã€‚æˆ‘ä»¬å‘ç°ï¼Œç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„é•¿æ€è€ƒè¿‡ç¨‹å¯ä»¥æœ‰æ•ˆåœ°è½¬ç§»åˆ°MLLMä¸Šã€‚è€Œä¸”ï¼Œä¼¼ä¹è¿™ç§æ–‡æœ¬æ¨ç†æ•°æ®åœ¨æ¿€å‘MLLMçš„æ…¢æ€è€ƒèƒ½åŠ›æ–¹é¢ç”šè‡³æ¯”è§†è§‰æ¨ç†æ•°æ®æ›´æœ‰æ•ˆã€‚è™½ç„¶è¿™é¡¹å·¥ä½œåªæ˜¯åˆæ­¥çš„ï¼Œä½†å®ƒè¯æ˜äº†æ…¢æ€è€ƒèƒ½åŠ›ä»æ ¹æœ¬ä¸Šä¸è¯­è¨€æ¨¡å‹ç»„ä»¶ç›¸å…³è”ï¼Œå¯ä»¥è·¨æ¨¡æ€æˆ–é¢†åŸŸè¿›è¡Œè½¬ç§»ã€‚è¿™ä¸€å‘ç°å¯ç”¨äºæŒ‡å¯¼å¼€å‘æ›´å¼ºå¤§çš„æ…¢æ€è€ƒæ¨ç†ç³»ç»Ÿã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Virgo">https://github.com/RUCAIBox/Virgo</a>ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01904v1">PDF</a> Technical Report on Slow Thinking with LLMs: Visual Reasoning</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ…¢æ€è€ƒæ¨ç†ç³»ç»Ÿé€šè¿‡æ‰©å±•æ¨ç†æ—¶é—´è·å¾—äº†å¹¿æ³›å…³æ³¨ã€‚ä¸ºé€‚åº”å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¿™ä¸€èƒ½åŠ›ï¼Œç ”ç©¶è€…ä»¬æ­£ä¸æ–­æ¢ç´¢ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œå³é€šè¿‡å¾®è°ƒå…·å¤‡èƒ½åŠ›çš„MLLMä¸å°‘é‡æ–‡æœ¬é•¿å½¢å¼æ€ç»´æ•°æ®ï¼Œå®ç°å¤šæ¨¡æ€æ…¢æ€è€ƒç³»ç»Ÿâ€”â€”ç»´äº¬ï¼ˆVisual reasoning with long thoughtï¼‰ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè‡ªç„¶è¯­è¨€è¡¨è¾¾çš„é•¿æœŸæ¨ç†è¿‡ç¨‹å¯ä»¥æœ‰æ•ˆåœ°è½¬ç§»åˆ°MLLMä¸Šï¼Œå¹¶ä¸”è¿™ç§æ–‡æœ¬æ¨ç†æ•°æ®ä¼¼ä¹æ¯”è§†è§‰æ¨ç†æ•°æ®æ›´èƒ½æ¿€å‘MLLMçš„æ…¢æ€è€ƒèƒ½åŠ›ã€‚è¿™ä¸ºå¼€å‘æ›´å¼ºå¤§çš„æ…¢æ€è€ƒæ¨ç†ç³»ç»Ÿæä¾›äº†é‡è¦å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ…¢æ€è€ƒæ¨ç†ç³»ç»Ÿé€šè¿‡æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ—¶é—´è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›é€‚åº”æ˜¯ä¸€ä¸ªæ–°å…´çš„ç ”ç©¶æ–¹å‘ã€‚</li>
<li>é€šè¿‡å¾®è°ƒå…·å¤‡èƒ½åŠ›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸å°‘é‡æ–‡æœ¬é•¿å½¢å¼æ€ç»´æ•°æ®ï¼Œå¯ä»¥å®ç°å¤šæ¨¡æ€æ…¢æ€è€ƒç³»ç»Ÿâ€”â€”ç»´äº¬ï¼ˆVirgoï¼‰ã€‚</li>
<li>è‡ªç„¶è¯­è¨€çš„é•¿æœŸæ¨ç†è¿‡ç¨‹å¯ä»¥æœ‰æ•ˆåœ°è½¬ç§»åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šã€‚</li>
<li>æ–‡æœ¬æ¨ç†æ•°æ®æ¯”è§†è§‰æ¨ç†æ•°æ®æ›´èƒ½æ¿€å‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ…¢æ€è€ƒèƒ½åŠ›ã€‚</li>
<li>ç»´äº¬ç³»ç»Ÿçš„å‘ç°è¡¨æ˜æ…¢æ€è€ƒèƒ½åŠ›æ ¹æœ¬ä¸Šä¸è¯­è¨€æ¨¡å‹ç»„ä»¶ç›¸å…³ï¼Œå¯è·¨æ¨¡æ€æˆ–é¢†åŸŸè¿›è¡Œè½¬ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01904">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a5fbe97bd1212b71db61d370b3397be4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df6ee89efbb8872580676dc28e357613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8397b0ed0c4639773591021f86d47def.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40b58f4c4bb67d1dc3dbf31df5a966ee.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AgentRefine-Enhancing-Agent-Generalization-through-Refinement-Tuning"><a href="#AgentRefine-Enhancing-Agent-Generalization-through-Refinement-Tuning" class="headerlink" title="AgentRefine: Enhancing Agent Generalization through Refinement Tuning"></a>AgentRefine: Enhancing Agent Generalization through Refinement Tuning</h2><p><strong>Authors:Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu</strong></p>
<p>Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†å·²ç»è¯æ˜äº†å®ƒä»¬æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œç±»ä¼¼äºäººç±»ã€‚ç„¶è€Œï¼Œå¼€æºLLMå’Œå•†ä¸šæ¨¡å‹ï¼ˆå¦‚GPTç³»åˆ—ï¼‰ä¹‹é—´ä»å­˜åœ¨å¾ˆå¤§å·®è·ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºé€šè¿‡æŒ‡ä»¤å¾®è°ƒæé«˜LLMçš„ä»£ç†æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆè§‚å¯Ÿåˆ°ï¼Œç°æœ‰çš„ä»£ç†è®­ç»ƒè¯­æ–™åº“åœ¨ä¿ç•™çš„è¯„ä»·é›†ä¸Šå–å¾—äº†ä»¤äººæ»¡æ„çš„ç»“æœï¼Œä½†æœªèƒ½æ¨å¹¿åˆ°ä¿ç•™å¤–çš„æ•°æ®é›†ã€‚è¿™äº›ä»£ç†è°ƒæ•´å·¥ä½œé¢ä¸´ç€ä¸¥é‡çš„æ ¼å¼é”™è¯¯ï¼Œå¹¶ä¸”ç»å¸¸é•¿æ—¶é—´é™·å…¥åŒæ ·çš„é”™è¯¯ã€‚æˆ‘ä»¬åˆ†æè®¤ä¸ºï¼Œæ³›åŒ–èƒ½åŠ›å·®çš„æ ¹æºåœ¨äºå¯¹å‡ ç§æ‰‹åŠ¨ä»£ç†ç¯å¢ƒçš„è¿‡åº¦é€‚åº”ä»¥åŠå¯¹æ–°æƒ…å†µçš„é€‚åº”ä¸è¶³ã€‚ä»–ä»¬éš¾ä»¥æ‰§è¡Œé”™è¯¯çš„è¡ŒåŠ¨æ­¥éª¤ï¼Œæ— æ³•ä»ç»éªŒä¸­å­¦ä¹ ï¼Œè€Œåªæ˜¯è®°ä½ç°æœ‰çš„è§‚å¯Ÿ-è¡ŒåŠ¨å…³ç³»ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„AgentRefineæ¡†æ¶ç”¨äºä»£ç†è°ƒæ•´ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡è½¨è¿¹ä¸­çš„è§‚å¯Ÿæ¥çº æ­£è‡ªå·±çš„é”™è¯¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»£ç†åˆæˆæ¡†æ¶ï¼Œä»¥æ¶µç›–å„ç§ç¯å¢ƒå’Œä»»åŠ¡ï¼Œå¹¶æç¤ºå¼ºå¤§çš„LLMæ ¹æ®ç¯å¢ƒåé¦ˆç»†åŒ–å…¶é”™è¯¯è¡ŒåŠ¨ã€‚AgentRefineåœ¨å¤šç§ä»£ç†ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°çš„ä»£ç†è°ƒæ•´å·¥ä½œã€‚å®ƒè¿˜å…·æœ‰æ›´å¥½çš„æŠ—æ‰°åŠ¨æ€§ï¼Œå¹¶èƒ½åœ¨æ¨ç†ä¸­äº§ç”Ÿå¤šæ ·åŒ–çš„æƒ³æ³•ã€‚æˆ‘ä»¬çš„å‘ç°å»ºç«‹äº†ä»£ç†æ³›åŒ–å’Œè‡ªæˆ‘å®Œå–„ä¹‹é—´çš„å…³è”ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01702v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å±•ç°å‡ºç±»ä¼¼äººç±»å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚ä½†å¼€æºLLMä¸å•†ä¸šæ¨¡å‹å¦‚GPTç³»åˆ—é—´ä»å­˜åœ¨å·®è·ã€‚æœ¬æ–‡èšç„¦äºé€šè¿‡æŒ‡ä»¤å¾®è°ƒæå‡LLMçš„ä»£ç†æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰ä»£ç†è®­ç»ƒè¯­æ–™åº“åœ¨å†…éƒ¨è¯„ä¼°é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤–éƒ¨è¯„ä¼°é›†ä¸Šæ³›åŒ–å¤±è´¥ã€‚æœ¬æ–‡åˆ†æäº†å…¶æºäºè¿‡åº¦é€‚åº”æ‰‹åŠ¨ä»£ç†ç¯å¢ƒä»¥åŠéš¾ä»¥é€‚åº”æ–°æƒ…å¢ƒçš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†æ–°é¢–çš„AgentRefineæ¡†æ¶è¿›è¡Œä»£ç†è°ƒæ•´ï¼Œå…¶æ ¸å¿ƒåœ¨äºè®©æ¨¡å‹é€šè¿‡è½¨è¿¹è§‚å¯Ÿå­¦ä¹ çº æ­£é”™è¯¯ã€‚AgentRefineæ˜¾è‘—æå‡äº†ç°æœ‰ä»£ç†è°ƒæ•´å·¥ä½œåœ¨å¤šæ ·åŒ–ä»£ç†ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„æŠ—æ‰°åŠ¨æ€§å’Œæ¨ç†å¤šæ ·æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå·²å±•ç°å‡ºå¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä½†ä¸å•†ä¸šæ¨¡å‹å¦‚GPTç³»åˆ—ç›¸æ¯”ä»æœ‰å·®è·ã€‚</li>
<li>ç°æœ‰LLMä»£ç†è®­ç»ƒå­˜åœ¨è¿‡åº¦é€‚åº”ç‰¹å®šç¯å¢ƒçš„é—®é¢˜ï¼Œå¯¼è‡´åœ¨æ–°æƒ…å¢ƒä¸‹æ³›åŒ–èƒ½åŠ›å¼±ã€‚</li>
<li>AgentRefineæ¡†æ¶æ—¨åœ¨æå‡LLMçš„ä»£ç†æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡è®©æ¨¡å‹å­¦ä¹ çº æ­£é”™è¯¯æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>AgentRefineæ¡†æ¶åŒ…å«å¤šæ ·åŒ–çš„ç¯å¢ƒä»»åŠ¡ï¼Œå¹¶æç¤ºå¼ºLLMæ ¹æ®ç¯å¢ƒåé¦ˆä¿®æ­£é”™è¯¯è¡ŒåŠ¨ã€‚</li>
<li>AgentRefineåœ¨å¤šæ ·åŒ–ä»£ç†ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›æ˜¾è‘—ä¼˜äºç°æœ‰å·¥ä½œã€‚</li>
<li>AgentRefineå…·æœ‰æ›´å¥½çš„æŠ—æ‰°åŠ¨æ€§ï¼Œèƒ½åœ¨æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿå¤šæ ·åŒ–æ€è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d038ca7ce01a33796accd474d3a68b23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88161e3dc0e407c08178fa9ee8f00577.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ded54c4fc8f22277871fda225df4e9ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4af506b5091c5002bfebb929d32ae1b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dbd55274cb9289be6953d7d5368945a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7b2ca290aff6d45948646de103b3bbf.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Qwen2-5-Technical-Report"><a href="#Qwen2-5-Technical-Report" class="headerlink" title="Qwen2.5 Technical Report"></a>Qwen2.5 Technical Report</h2><p><strong>Authors: Qwen,  :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu</strong></p>
<p>In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models. </p>
<blockquote>
<p>åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Qwen2.5ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ä¸ºäº†æ»¡è¶³ä¸åŒéœ€æ±‚è€Œè®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç›¸è¾ƒäºä¹‹å‰çš„ç‰ˆæœ¬ï¼ŒQwen 2.5åœ¨é¢„è®­ç»ƒå’Œåè®­ç»ƒé˜¶æ®µéƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚åœ¨é¢„è®­ç»ƒæ–¹é¢ï¼Œæˆ‘ä»¬å°†é«˜è´¨é‡çš„é¢„è®­ç»ƒæ•°æ®é›†ä»ä¹‹å‰çš„7ä¸‡äº¿æ ‡è®°æ‰©å±•åˆ°18ä¸‡äº¿æ ‡è®°ã€‚è¿™ä¸ºå¸¸è¯†ã€ä¸“ä¸šçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›æä¾›äº†åšå®çš„åŸºç¡€ã€‚åœ¨åè®­ç»ƒæ–¹é¢ï¼Œæˆ‘ä»¬å®æ–½äº†å¤æ‚çš„ç›‘ç£å¾®è°ƒï¼Œä½¿ç”¨äº†è¶…è¿‡1ç™¾ä¸‡ä¸ªæ ·æœ¬ï¼Œä»¥åŠå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ã€‚åè®­ç»ƒæŠ€æœ¯æé«˜äº†äººç±»åå¥½ï¼Œå¹¶æ˜¾è‘—æ”¹å–„äº†é•¿æ–‡æœ¬ç”Ÿæˆã€ç»“æ„åŒ–æ•°æ®åˆ†æå’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚ä¸ºäº†æœ‰æ•ˆåœ°å¤„ç†å„ç§ç”¨ä¾‹ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸°å¯Œçš„Qwen2.5 LLMç³»åˆ—ã€‚å¼€æ”¾æƒé‡äº§å“åŒ…æ‹¬åŸºç¡€ç‰ˆå’ŒæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼Œä¹Ÿæœ‰é‡åŒ–ç‰ˆæœ¬å¯ä¾›é€‰æ‹©ã€‚æ­¤å¤–ï¼Œå¯¹äºæ‰˜ç®¡è§£å†³æ–¹æ¡ˆï¼Œä¸“æœ‰æ¨¡å‹ç›®å‰åŒ…æ‹¬ä¸¤ä¸ªæ··åˆä¸“å®¶ï¼ˆMoEï¼‰å˜ä½“ï¼šQwen2.5-Turboå’ŒQwen2.5-Plusï¼Œä¸¤è€…éƒ½å¯åœ¨é˜¿é‡Œäº‘æ¨¡å‹å·¥ä½œå®¤ä¸­ä½¿ç”¨ã€‚Qwen2.5å·²åœ¨å¹¿æ³›çš„è¯­è¨€ç†è§£ã€æ¨ç†ã€æ•°å­¦ã€ç¼–ç ã€äººç±»åå¥½å¯¹é½ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯å¼€æ”¾æƒé‡çš„æ——èˆ°äº§å“Qwen2.5-72B-Instructï¼Œåœ¨å¤šä¸ªå…¬å¼€å’Œä¸“æœ‰æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸æœ€å…ˆè¿›çš„å¼€æ”¾æƒé‡æ¨¡å‹Llama-3-405B-Instructç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå°½ç®¡åè€…è§„æ¨¡å¤§çº¦æ˜¯å‰è€…çš„5å€ã€‚Qwen2.5-Turboå’ŒQwen2.5-Plusæä¾›äº†å‡ºè‰²çš„æ€§ä»·æ¯”ï¼Œä¸GPT-4o-miniå’ŒGPT-4oç›¸æ¯”è¡¨ç°è‰¯å¥½ã€‚æ­¤å¤–ï¼Œä½œä¸ºåŸºçŸ³ï¼ŒQwen2.5æ¨¡å‹åœ¨è®­ç»ƒä¸“ä¸šæ¨¡å‹å¦‚Qwen2.5-Mathã€Qwen2.5-Coderã€QwQå’Œå¤šæ¨¡æ€æ¨¡å‹ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15115v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Qwen2.5ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¯¥ç³»åˆ—æ¨¡å‹æ—¨åœ¨æ»¡è¶³ä¸åŒçš„éœ€æ±‚ã€‚ç›¸è¾ƒäºä¹‹å‰çš„ç‰ˆæœ¬ï¼ŒQwen 2.5åœ¨é¢„è®­ç»ƒå’Œåè®­ç»ƒé˜¶æ®µéƒ½æœ‰æ˜¾è‘—çš„æå‡ã€‚åœ¨é¢„è®­ç»ƒæ–¹é¢ï¼Œé«˜è´¨é‡é¢„è®­ç»ƒæ•°æ®é›†ä»ä¹‹å‰çš„7ä¸‡äº¿æ ‡è®°æ‰©å±•åˆ°äº†18ä¸‡äº¿æ ‡è®°ï¼Œä¸ºå¸¸è¯†ã€ä¸“ä¸šçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›æä¾›äº†åšå®çš„åŸºç¡€ã€‚åè®­ç»ƒæ–¹é¢ï¼Œé‡‡ç”¨å¤æ‚çš„ç›‘ç£å¾®è°ƒæŠ€æœ¯ï¼Œä½¿ç”¨è¶…è¿‡1ç™¾ä¸‡æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ã€‚è¿™äº›åè®­ç»ƒæŠ€æœ¯æé«˜äº†å¯¹äººç±»åå¥½çš„ç¬¦åˆåº¦ï¼Œå¹¶æ˜¾è‘—æ”¹å–„äº†é•¿æ–‡æœ¬ç”Ÿæˆã€ç»“æ„åŒ–æ•°æ®åˆ†æå’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚ä¸ºäº†æœ‰æ•ˆåº”å¯¹å„ç§ç”¨ä¾‹ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸°å¯Œçš„Qwen2.5 LLMç³»åˆ—ã€‚å¼€æ”¾æƒé‡äº§å“åŒ…æ‹¬åŸºç¡€ç‰ˆå’ŒæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼Œè¿˜æœ‰é‡åŒ–ç‰ˆæœ¬ã€‚æ­¤å¤–ï¼Œå¯¹äºæ‰˜ç®¡è§£å†³æ–¹æ¡ˆï¼Œè¿˜åŒ…æ‹¬ä¸¤ä¸ªä¸“æœ‰æ¨¡å‹MoEï¼ˆæ··åˆä¸“å®¶ï¼‰å˜ä½“ï¼šQwen2.5-Turboå’ŒQwen2.5-Plusï¼Œä¸¤è€…å‡å¯åœ¨é˜¿é‡Œäº‘æ¨¡å‹å·¥ä½œå®¤é€šè¿‡é˜¿é‡Œå·´å·´äº‘è·å¾—ã€‚Qwen2.5ç³»åˆ—æ¨¡å‹åœ¨å¤šç§è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚è¯­è¨€ç†è§£ã€æ¨ç†ã€æ•°å­¦ã€ç¼–ç ã€äººç±»åå¥½å¯¹é½ç­‰ã€‚ç‰¹åˆ«æ˜¯å¼€æ”¾æƒé‡çš„æ——èˆ°äº§å“Qwen2.5-72B-Instructåœ¨å¤šä¸ªå…¬å¼€å’Œä¸“æœ‰æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸å½“å‰å…ˆè¿›çš„å¼€æ”¾æƒé‡æ¨¡å‹Llama-3-405B-Instructç›¸æ¯”ä¹Ÿå…·æœ‰ç«äº‰åŠ›ã€‚è€ŒQwen2.5-Turboå’ŒQwen2.5-Plusåœ¨æˆæœ¬æ•ˆç›Šæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼ŒQwen2.5ç³»åˆ—æ¨¡å‹è¿˜ä¸ºè®­ç»ƒä¸“ä¸šæ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>è¦ç‚¹æ€»ç»“</strong></p>
<ol>
<li>Qwen2.5ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºæ»¡è¶³ä¸åŒéœ€æ±‚è€Œè®¾è®¡ã€‚</li>
<li>ç›¸è¾ƒäºä¹‹å‰ç‰ˆæœ¬ï¼ŒQwen 2.5åœ¨é¢„è®­ç»ƒæ•°æ®é›†ã€åè®­ç»ƒæŠ€æœ¯å’Œæ€§èƒ½ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>Qwen 2.5ç³»åˆ—åŒ…æ‹¬å¤šç§è§„æ¨¡å’Œç±»å‹çš„æ¨¡å‹ï¼Œå¦‚åŸºç¡€ç‰ˆã€æŒ‡ä»¤è°ƒä¼˜ç‰ˆã€é‡åŒ–ç‰ˆç­‰ã€‚</li>
<li>Qwen2.5åœ¨åè®­ç»ƒæ–¹é¢é‡‡ç”¨äº†å¤æ‚çš„ç›‘ç£å¾®è°ƒæŠ€æœ¯å’Œå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼Œæé«˜äº†é•¿æ–‡æœ¬ç”Ÿæˆã€ç»“æ„åŒ–æ•°æ®åˆ†æå’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</li>
<li>Qwen2.5ç³»åˆ—åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬è¯­è¨€ç†è§£ã€æ¨ç†ã€æ•°å­¦ã€ç¼–ç ç­‰ã€‚</li>
<li>æ——èˆ°äº§å“Qwen2.5-72B-Instructåœ¨å…¬å¼€å’Œä¸“æœ‰æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f7ee645251758f7620cfd0d6ccd74960.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48058794ecf9bdf047a296b899c8ac64.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="OmniFlatten-An-End-to-end-GPT-Model-for-Seamless-Voice-Conversation"><a href="#OmniFlatten-An-End-to-end-GPT-Model-for-Seamless-Voice-Conversation" class="headerlink" title="OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation"></a>OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation</h2><p><strong>Authors:Qinglin Zhang, Luyao Cheng, Chong Deng, Qian Chen, Wen Wang, Siqi Zheng, Jiaqing Liu, Hai Yu, Chaohong Tan, Zhihao Du, Shiliang Zhang</strong></p>
<p>Full-duplex spoken dialogue systems significantly surpass traditional turn-based dialogue systems, as they allow simultaneous bidirectional communication, closely mirroring human-human interactions. However, achieving low latency and natural interactions in full-duplex dialogue systems remains a significant challenge, especially considering human conversation dynamics such as interruptions, backchannels, and overlapping speech. In this paper, we introduce a novel End-to-End GPT-based model OmniFlatten for full-duplex conversation, capable of effectively modeling the complex behaviors inherent to natural conversations with low latency. To achieve full-duplex conversation capabilities, we propose a multi-stage post-training scheme that progressively adapts a text large language model (LLM) backbone into a speech-text dialogue LLM, capable of generating text and speech in real time, without modifying the architecture of the backbone LLM. The training process comprises three stages: modality alignment, half-duplex dialogue learning, and full-duplex dialogue learning. In all training stages, we standardize the data using a flattening operation, which enables unifying the training methods and the GPT backbone across different modalities and tasks. Our approach offers a simple modeling technique and a promising research direction for developing efficient and natural end-to-end full-duplex spoken dialogue systems. Audio samples of dialogues generated by OmniFlatten can be found at this web site (<a target="_blank" rel="noopener" href="https://omniflatten.github.io/">https://omniflatten.github.io/</a>). </p>
<blockquote>
<p>å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿçš„åŸºäºè½®è½¬çš„å¯¹è¯ç³»ç»Ÿï¼Œå› ä¸ºå®ƒä»¬å…è®¸åŒæ—¶åŒå‘é€šä¿¡ï¼Œç´§å¯†åœ°æ¨¡æ‹Ÿäº†äººä¸äººä¹‹é—´çš„äº’åŠ¨ã€‚ç„¶è€Œï¼Œåœ¨å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿä¸­å®ç°ä½å»¶è¿Ÿå’Œè‡ªç„¶äº¤äº’ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è€ƒè™‘äººç±»å¯¹è¯åŠ¨æ€ï¼ˆå¦‚ä¸­æ–­ã€åé¦ˆé€šé“å’Œé‡å è¯­éŸ³ï¼‰æ—¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºGPTçš„æ–°å‹ç«¯åˆ°ç«¯å…¨åŒå·¥å¯¹è¯æ¨¡å‹OmniFlattenï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹è‡ªç„¶å¯¹è¯ä¸­å›ºæœ‰çš„å¤æ‚è¡Œä¸ºè¿›è¡Œå»ºæ¨¡ï¼Œå®ç°ä½å»¶è¿Ÿã€‚ä¸ºäº†å®ç°å…¨åŒå·¥å¯¹è¯åŠŸèƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µåè®­ç»ƒæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé€æ­¥å°†ä¸€ä¸ªæ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸»å¹²æ”¹é€ ä¸ºä¸€ä¸ªè¯­éŸ³æ–‡æœ¬å¯¹è¯LLMï¼Œèƒ½å¤Ÿå®æ—¶ç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³ï¼Œæ— éœ€ä¿®æ”¹ä¸»å¹²LLMçš„æ¶æ„ã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šæ¨¡æ€å¯¹é½ã€åŠåŒå·¥å¯¹è¯å­¦ä¹ å’Œå…¨åŒå·¥å¯¹è¯å­¦ä¹ ã€‚åœ¨æ‰€æœ‰è®­ç»ƒé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å±•å¹³æ“ä½œæ¥æ ‡å‡†åŒ–æ•°æ®ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸åŒçš„æ¨¡æ€å’Œä»»åŠ¡ä¸­ä½¿ç”¨ç»Ÿä¸€çš„è®­ç»ƒæ–¹æ³•å’ŒGPTä¸»å¹²ã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸€ç§ç®€å•çš„å»ºæ¨¡æŠ€æœ¯ï¼Œå¹¶ä¸ºå¼€å‘é«˜æ•ˆã€è‡ªç„¶çš„ç«¯åˆ°ç«¯å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚OmniFlattenç”Ÿæˆå¯¹è¯çš„éŸ³é¢‘æ ·æœ¬å¯ä»¥åœ¨è¿™ä¸ªç½‘ç«™æ‰¾åˆ°ï¼ˆ<a target="_blank" rel="noopener" href="https://omniflatten.github.io/%EF%BC%89%E3%80%82">https://omniflatten.github.io/ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17799v2">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿå…è®¸åŒæ—¶åŒå‘é€šä¿¡ï¼Œæ›´è´´è¿‘äººä¸äººä¹‹é—´çš„äº¤æµã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºGPTçš„OmniFlattenæ¨¡å‹ï¼Œç”¨äºå…¨åŒå·¥å¯¹è¯ï¼Œæœ‰æ•ˆå»ºæ¨¡è‡ªç„¶å¯¹è¯çš„å¤æ‚è¡Œä¸ºå¹¶å®ç°ä½å»¶è¿Ÿã€‚é€šè¿‡å¤šé˜¶æ®µåè®­ç»ƒæ–¹æ¡ˆï¼Œå°†æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¹ç¼–ä¸ºè¯­éŸ³æ–‡æœ¬å¯¹è¯LLMï¼Œå®ç°å®æ—¶ç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³ï¼Œæ— éœ€ä¿®æ”¹æ¶æ„ã€‚æ­¤æ–¹æ³•ä¸ºå¼€å‘é«˜æ•ˆã€è‡ªç„¶çš„ç«¯åˆ°ç«¯å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿæä¾›äº†ç®€å•å»ºæ¨¡æŠ€å·§å’Œå…·æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿæ¨¡æ‹Ÿäººç±»äº¤æµï¼Œå…è®¸åŒæ—¶åŒå‘é€šä¿¡ã€‚</li>
<li>OmniFlattenæ¨¡å‹ç”¨äºå…¨åŒå·¥å¯¹è¯ï¼Œæœ‰æ•ˆå»ºæ¨¡è‡ªç„¶å¯¹è¯çš„å¤æ‚è¡Œä¸ºã€‚</li>
<li>æå‡ºå¤šé˜¶æ®µåè®­ç»ƒæ–¹æ¡ˆï¼Œä½¿æ–‡æœ¬LLMå…·å¤‡è¯­éŸ³æ–‡æœ¬å¯¹è¯èƒ½åŠ›ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨æ•°æ®æ ‡å‡†åŒ–æ“ä½œï¼Œç»Ÿä¸€ä¸åŒæ¨¡æ€å’Œä»»åŠ¡ä¸‹çš„è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>OmniFlattenæ¨¡å‹å¯å®ç°ä½å»¶è¿Ÿå¯¹è¯ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šç§å¯¹è¯æ¨¡æ€ä¸‹å…·æœ‰é€šç”¨æ€§ã€‚</li>
<li>éŸ³é¢‘æ ·æœ¬å¯åœ¨æŒ‡å®šç½‘ç«™æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17799">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d80bae390aae70960cef2d63dc55564.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9489cf8ace90506ea752cbe69810cb3e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b6b2a919827e26b2ea80272d06b6f99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34cdc3674402202cf8981978a0590484.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Knowledge-Circuits-in-Pretrained-Transformers"><a href="#Knowledge-Circuits-in-Pretrained-Transformers" class="headerlink" title="Knowledge Circuits in Pretrained Transformers"></a>Knowledge Circuits in Pretrained Transformers</h2><p><strong>Authors:Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen</strong></p>
<p>The remarkable capabilities of modern large language models are rooted in their vast repositories of knowledge encoded within their parameters, enabling them to perceive the world and engage in reasoning. The inner workings of how these models store knowledge have long been a subject of intense interest and investigation among researchers. To date, most studies have concentrated on isolated components within these models, such as the Multilayer Perceptrons and attention head. In this paper, we delve into the computation graph of the language model to uncover the knowledge circuits that are instrumental in articulating specific knowledge. The experiments, conducted with GPT2 and TinyLLAMA, have allowed us to observe how certain information heads, relation heads, and Multilayer Perceptrons collaboratively encode knowledge within the model. Moreover, we evaluate the impact of current knowledge editing techniques on these knowledge circuits, providing deeper insights into the functioning and constraints of these editing methodologies. Finally, we utilize knowledge circuits to analyze and interpret language model behaviors such as hallucinations and in-context learning. We believe the knowledge circuits hold potential for advancing our understanding of Transformers and guiding the improved design of knowledge editing. Code and data are available in <a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowledgeCircuits">https://github.com/zjunlp/KnowledgeCircuits</a>. </p>
<blockquote>
<p>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„å“è¶Šèƒ½åŠ›æ ¹æ¤äºå…¶å‚æ•°ä¸­ç¼–ç çš„åºå¤§çŸ¥è¯†åº“ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿæ„ŸçŸ¥ä¸–ç•Œå¹¶å‚ä¸æ¨ç†ã€‚è¿™äº›æ¨¡å‹å¦‚ä½•å­˜å‚¨çŸ¥è¯†çš„å†…éƒ¨å·¥ä½œæœºåˆ¶é•¿æœŸä»¥æ¥ä¸€ç›´æ˜¯ç ”ç©¶äººå‘˜å…³æ³¨çš„çƒ­ç‚¹å’Œè°ƒæŸ¥å¯¹è±¡ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œå¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨è¿™äº›æ¨¡å‹çš„å­¤ç«‹ç»„ä»¶ä¸Šï¼Œå¦‚å¤šå±‚æ„ŸçŸ¥å™¨å’Œæ³¨æ„åŠ›å¤´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥è¯­è¨€æ¨¡å‹çš„è®¡ç®—å›¾ï¼Œä»¥æ­ç¤ºå¯¹äºè¡¨è¾¾ç‰¹å®šçŸ¥è¯†è‡³å…³é‡è¦çš„çŸ¥è¯†å›è·¯ã€‚æˆ‘ä»¬ä¸GPT2å’ŒTinyLLAMAè¿›è¡Œçš„å®éªŒä½¿æˆ‘ä»¬èƒ½å¤Ÿè§‚å¯Ÿåˆ°æŸäº›ä¿¡æ¯å¤´ã€å…³ç³»å¤´å’Œå¤šå±‚æ„ŸçŸ¥å™¨å¦‚ä½•åœ¨æ¨¡å‹ä¸­ååŒç¼–ç çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†å½“å‰çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯å¯¹è¿™äº›çŸ¥è¯†å›è·¯çš„å½±å“ï¼Œä¸ºè¿™äº›ç¼–è¾‘æ–¹æ³•çš„è¿ä½œå’Œå±€é™æ€§æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨çŸ¥è¯†å›è·¯åˆ†æå’Œè§£é‡Šè¯­è¨€æ¨¡å‹çš„è¡Œä¸ºï¼Œå¦‚å¹»è§‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚æˆ‘ä»¬ç›¸ä¿¡çŸ¥è¯†å›è·¯åœ¨æ¨åŠ¨æˆ‘ä»¬å¯¹Transformerçš„ç†è§£ä»¥åŠæŒ‡å¯¼çŸ¥è¯†ç¼–è¾‘çš„æ”¹è¿›è®¾è®¡æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zjunlp/KnowledgeCircuits%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zjunlp/KnowledgeCircuitsæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.17969v4">PDF</a> NeurIPS 2024, 26 pages</p>
<p><strong>Summary</strong><br>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›æºäºå…¶å‚æ•°ä¸­ç¼–ç çš„ä¸°å¯ŒçŸ¥è¯†åº“ï¼Œèƒ½å¤Ÿæ„ŸçŸ¥ä¸–ç•Œå¹¶è¿›è¡Œæ¨ç†ã€‚æœ¬æ–‡æ·±å…¥æ¢ç©¶äº†è¯­è¨€æ¨¡å‹çš„è®¡ç®—å›¾ï¼Œæ­ç¤ºäº†è¡¨è¾¾ç‰¹å®šçŸ¥è¯†çš„å…³é”®çŸ¥è¯†å›è·¯ã€‚é€šè¿‡GPT2å’ŒTinyLLAMAçš„å®éªŒï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¿¡æ¯å¤´ã€å…³ç³»å¤´å’Œå¤šå±‚æ„ŸçŸ¥å™¨ç­‰å¦‚ä½•ååŒåœ¨æ¨¡å‹ä¸­ç¼–ç çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¯„ä¼°äº†å½“å‰çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯å¯¹çŸ¥è¯†å›è·¯çš„å½±å“ï¼Œä¸ºè¿™äº›ç¼–è¾‘æ–¹æ³•çš„è¿ä½œå’Œå±€é™æ€§æä¾›äº†æ·±åˆ»è§è§£ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨çŸ¥è¯†å›è·¯åˆ†æå’Œè§£é‡Šè¯­è¨€æ¨¡å‹çš„è¡Œä¸ºï¼Œå¦‚å¹»è§‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚çŸ¥è¯†å›è·¯æœ‰åŠ©äºæˆ‘ä»¬æ›´å¥½åœ°ç†è§£Transformerå¹¶å¼•å¯¼çŸ¥è¯†ç¼–è¾‘çš„æ”¹è¿›è®¾è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›æºäºå…¶å‚æ•°ä¸­ç¼–ç çš„ä¸°å¯ŒçŸ¥è¯†åº“ã€‚</li>
<li>è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å›è·¯å¯¹äºè¡¨è¾¾ç‰¹å®šçŸ¥è¯†è‡³å…³é‡è¦ã€‚</li>
<li>ä¿¡æ¯å¤´ã€å…³ç³»å¤´å’Œå¤šå±‚æ„ŸçŸ¥å™¨ç­‰ååŒåœ¨æ¨¡å‹ä¸­ç¼–ç çŸ¥è¯†ã€‚</li>
<li>å½“å‰çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯å¯¹çŸ¥è¯†å›è·¯æœ‰å½±å“ã€‚</li>
<li>çŸ¥è¯†å›è·¯å¯ç”¨äºåˆ†æå’Œè§£é‡Šè¯­è¨€æ¨¡å‹çš„è¡Œä¸ºï¼Œå¦‚å¹»è§‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ã€‚</li>
<li>çŸ¥è¯†å›è·¯ç ”ç©¶æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£Transformerã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.17969">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad47fb0b4724e2d15740b7bbd23fba0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d3cb131479da7b37c780048cc82c152.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8734983426fdd7a6d09d51c8e813eb10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4262ffa2ba0634752220b6817b9d40ab.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Agent-Planning-with-World-Knowledge-Model"><a href="#Agent-Planning-with-World-Knowledge-Model" class="headerlink" title="Agent Planning with World Knowledge Model"></a>Agent Planning with World Knowledge Model</h2><p><strong>Authors:Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</strong></p>
<p>Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the &#96;&#96;realâ€™â€™ physical world. Imitating humansâ€™ mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agentâ€™s understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/WKM">https://github.com/zjunlp/WKM</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œç›´æ¥ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä»£ç†æ¨¡å‹æ¥æ‰§è¡Œäº¤äº’å¼è§„åˆ’ä»»åŠ¡çš„ç ”ç©¶å–å¾—äº†å€¼å¾—èµæ‰¬çš„æˆæœã€‚å°½ç®¡å–å¾—äº†æˆå°±ï¼Œç„¶è€Œï¼Œå®ƒä»¬åœ¨å…¨å±€è§„åˆ’æ–¹é¢ä»é¢ä¸´ç€ç›²ç›®çš„è¯•é”™é—®é¢˜ï¼Œåœ¨å±€éƒ¨è§„åˆ’æ–¹é¢ç”±äºç¼ºä¹å¯¹â€œçœŸå®â€ç‰©ç†ä¸–ç•Œçš„ç†è§£ï¼Œä¼šäº§ç”Ÿå¹»è§‰è¡Œä¸ºã€‚</p>
</blockquote>
<p>æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥å‚æ•°åŒ–ä¸–ç•ŒçŸ¥è¯†æ¨¡å‹ï¼ˆWKMï¼‰ï¼Œä»¥è¾…åŠ©ä»£ç†è§„åˆ’ï¼Œæ¨¡ä»¿äººç±»åœ¨æ‰§è¡Œä»»åŠ¡å‰æä¾›å…¨å±€å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶åœ¨ä»»åŠ¡è¿‡ç¨‹ä¸­ä¿æŒå±€éƒ¨åŠ¨æ€çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å¯¼ä»£ç†æ¨¡å‹ä»ä¸“å®¶è½¨è¿¹å’Œé‡‡æ ·è½¨è¿¹ä¸­è‡ªè¡ŒåˆæˆçŸ¥è¯†ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘WKMï¼Œæä¾›ä»»åŠ¡å…ˆéªŒçŸ¥è¯†ä»¥æŒ‡å¯¼å…¨å±€è§„åˆ’ï¼Œå¹¶æä¾›åŠ¨æ€çŠ¶æ€çŸ¥è¯†ä»¥è¾…åŠ©å±€éƒ¨è§„åˆ’ã€‚</p>
<p>åœ¨ä¸‰ä¸ªå¤æ‚çœŸå®ä¸–ç•Œæ¨¡æ‹Ÿæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ä¸‰ç§æœ€æ–°å¼€æºLLMï¼ˆMistral-7Bã€Gemma-7Bå’ŒLlama-3-8Bï¼‰è¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸æ¯”å„ç§å¼ºå¤§çš„åŸºçº¿æ–¹æ³•å¯ä»¥å®ç°ä¼˜è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æè¯æ˜ï¼Œæˆ‘ä»¬çš„WKMå¯ä»¥æœ‰æ•ˆåœ°å‡è½»ç›²ç›®è¯•é”™å’Œå¹»è§‰è¡Œä¸ºé—®é¢˜ï¼Œä¸ºä»£ç†å¯¹ä¸–ç•Œçš„ç†è§£æä¾›å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚å…¶ä»–æœ‰è¶£çš„å‘ç°åŒ…æ‹¬ï¼š1ï¼‰æˆ‘ä»¬çš„å®ä¾‹çº§ä»»åŠ¡çŸ¥è¯†å¯ä»¥æ›´å¥½åœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„ä»»åŠ¡ï¼›2ï¼‰å¼±çš„WKMå¯ä»¥æŒ‡å¯¼å¼ºä»£ç†æ¨¡å‹è§„åˆ’ï¼›3ï¼‰ç»Ÿä¸€çš„WKMè®­ç»ƒå…·æœ‰è¿›ä¸€æ­¥å¼€å‘çš„æ½œåŠ›ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14205v4">PDF</a> NeurIPS 2024</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å‚æ•°åŒ–ä¸–ç•ŒçŸ¥è¯†æ¨¡å‹ï¼ˆWKMï¼‰åœ¨ä¿ƒè¿›åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†è§„åˆ’æ–¹é¢çš„åº”ç”¨ã€‚é€šè¿‡ç»“åˆä¸“å®¶è½¨è¿¹å’Œé‡‡æ ·è½¨è¿¹ï¼ŒWKMä¸ºä»£ç†æ¨¡å‹æä¾›ä»»åŠ¡å…ˆéªŒçŸ¥è¯†ï¼Œä»¥æŒ‡å¯¼å…¨å±€è§„åˆ’ï¼Œå¹¶æä¾›åŠ¨æ€çŠ¶æ€çŸ¥è¯†ä»¥è¾…åŠ©å±€éƒ¨è§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå¤æ‚ç°å®ä¸–ç•Œçš„æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨ä¸‰ç§æœ€æ–°çš„å¼€æºLLMï¼Œç›¸è¾ƒäºå„ç§å¼ºå¤§çš„åŸºçº¿æ–¹æ³•ï¼Œå–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒWKMèƒ½æœ‰æ•ˆç¼“è§£ç›²ç›®è¯•é”™å’Œå¹»è§‰åŠ¨ä½œé—®é¢˜ï¼Œä¸ºä»£ç†å¯¹ä¸–ç•Œçš„ç†è§£æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œå®ä¾‹çº§ä»»åŠ¡çŸ¥è¯†å¯ä»¥æ›´å¥½æ³›åŒ–åˆ°æœªè§è¿‡çš„ä»»åŠ¡ï¼Œå¼±WKMå¯ä»¥æŒ‡å¯¼å¼ºä»£ç†æ¨¡å‹è§„åˆ’ï¼Œç»Ÿä¸€WKMè®­ç»ƒå…·æœ‰è¿›ä¸€æ­¥å¼€å‘çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äº¤äº’å¼è§„åˆ’ä»»åŠ¡ä¸­çš„ç›´æ¥åº”ç”¨å·²å–å¾—æ˜¾è‘—æˆæœã€‚</li>
<li>LLMä»é¢ä¸´å…¨çƒè§„åˆ’ä¸­çš„æ— è„‘è¯•é”™å’Œå±€éƒ¨è§„åˆ’ä¸­çš„å¹»è§‰åŠ¨ä½œé—®é¢˜ã€‚</li>
<li>æå‡ºå‚æ•°åŒ–ä¸–ç•ŒçŸ¥è¯†æ¨¡å‹ï¼ˆWKMï¼‰ä»¥ä¿ƒè¿›ä»£ç†è§„åˆ’ã€‚</li>
<li>WKMèƒ½ç»“åˆä¸“å®¶è½¨è¿¹å’Œé‡‡æ ·è½¨è¿¹ï¼Œä¸ºä»£ç†æ¨¡å‹æä¾›å…ˆéªŒä»»åŠ¡çŸ¥è¯†å’ŒåŠ¨æ€çŠ¶æ€çŸ¥è¯†ã€‚</li>
<li>å®éªŒè¯æ˜WKMåœ¨å¤æ‚ç°å®ä¸–ç•Œçš„æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>WKMèƒ½æœ‰æ•ˆç¼“è§£ç›²ç›®è¯•é”™å’Œå¹»è§‰åŠ¨ä½œé—®é¢˜ï¼Œå¢å¼ºä»£ç†å¯¹ä¸–ç•Œçš„ç†è§£ã€‚</li>
<li>ç ”ç©¶å‘ç°å®ä¾‹çº§ä»»åŠ¡çŸ¥è¯†æ³›åŒ–æ€§å¥½ï¼Œå¼±WKMå¯æŒ‡å¯¼å¼ºä»£ç†æ¨¡å‹è§„åˆ’ï¼Œç»Ÿä¸€WKMè®­ç»ƒå…·æœ‰å‘å±•æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-10e548841f8670214eada6a38e4e8c4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c988b7da694d55c09897345596a1403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1b82792178546b9e8265d0e4b645cf5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-07/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-07/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-07/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e1b82792178546b9e8265d0e4b645cf5.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-07  QuArch A Question-Answering Dataset for AI Agents in Computer   Architecture
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-06/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d5dc747136839484c54d22b2d79fc207.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-06  VideoAnydoor High-fidelity Video Object Insertion with Precise Motion   Control
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24417.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
