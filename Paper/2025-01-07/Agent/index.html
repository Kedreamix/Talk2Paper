<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-01-07  QuArch A Question-Answering Dataset for AI Agents in Computer   Architecture">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e1b82792178546b9e8265d0e4b645cf5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    36 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-07-更新"><a href="#2025-01-07-更新" class="headerlink" title="2025-01-07 更新"></a>2025-01-07 更新</h1><h2 id="QuArch-A-Question-Answering-Dataset-for-AI-Agents-in-Computer-Architecture"><a href="#QuArch-A-Question-Answering-Dataset-for-AI-Agents-in-Computer-Architecture" class="headerlink" title="QuArch: A Question-Answering Dataset for AI Agents in Computer   Architecture"></a>QuArch: A Question-Answering Dataset for AI Agents in Computer   Architecture</h2><p><strong>Authors:Shvetank Prakash, Andrew Cheng, Jason Yik, Arya Tschand, Radhika Ghosal, Ikechukwu Uchendu, Jessica Quaye, Jeffrey Ma, Shreyas Grampurohit, Sofia Giannuzzi, Arnav Balyan, Fin Amin, Aadya Pipersenia, Yash Choudhary, Ankita Nayak, Amir Yazdanbakhsh, Vijay Janapa Reddi</strong></p>
<p>We introduce QuArch, a dataset of 1500 human-validated question-answer pairs designed to evaluate and enhance language models’ understanding of computer architecture. The dataset covers areas including processor design, memory systems, and performance optimization. Our analysis highlights a significant performance gap: the best closed-source model achieves 84% accuracy, while the top small open-source model reaches 72%. We observe notable struggles in memory systems, interconnection networks, and benchmarking. Fine-tuning with QuArch improves small model accuracy by up to 8%, establishing a foundation for advancing AI-driven computer architecture research. The dataset and leaderboard are at <a target="_blank" rel="noopener" href="https://harvard-edge.github.io/QuArch/">https://harvard-edge.github.io/QuArch/</a>. </p>
<blockquote>
<p>我们介绍了QuArch数据集，它包含1500组经过人工验证的问题答案对，旨在评估和增强语言模型对计算机架构的理解。该数据集涵盖了处理器设计、内存系统和性能优化等领域。我们的分析突显了显著的性能差距：最佳封闭源模型的准确率为84%，而顶尖的小型开源模型的准确率仅为72%。我们在内存系统、互联网络和基准测试方面遇到了明显的困难。使用QuArch进行微调可以提高小型模型的准确率，最高可提高8%，为推进AI驱动的计算机架构研究奠定了基础。数据集和排行榜位于<a target="_blank" rel="noopener" href="https://harvard-edge.github.io/QuArch/%E3%80%82">https://harvard-edge.github.io/QuArch/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01892v1">PDF</a> </p>
<p><strong>摘要</strong><br>QuArch是一个包含人类验证的包含处理器设计、内存系统以及性能优化等领域的问题答案对的数据库。它旨在评估和增强语言模型对计算机架构的理解。分析显示，最好的闭源模型准确率可达百分之八十四，而表现最佳的小型开源模型准确率也达到百分之七十二，但在内存系统、互联网络和基准测试方面存在显著困难。使用QuArch微调可以提高小型模型的准确率至百分之八。该数据集和排行榜可访问<a target="_blank" rel="noopener" href="https://harvard-edge.github.io/QuArch/%E3%80%82">https://harvard-edge.github.io/QuArch/。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>QuArch数据集包含有人类验证的问题答案对，旨在评估和增强语言模型对计算机架构的理解。</li>
<li>数据集涵盖处理器设计、内存系统和性能优化等关键领域。</li>
<li>分析了语言模型在计算机架构方面的性能差距，最好的闭源模型与最佳小型开源模型的表现存在差异。</li>
<li>在内存系统、互联网络和基准测试方面存在挑战。</li>
<li>使用QuArch微调可以提高小型模型的准确率。</li>
<li>QuArch数据集有助于推动AI驱动的计算机架构研究的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01892">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5e343790bd5026b810d57a4991166f14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33461718585d3de7342566523573144a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22d17f0068696747f0aaeec8a6f58e13.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Conversational-Online-Learning-for-Adaptive-LLM-Response-Identification"><a href="#Multi-Agent-Conversational-Online-Learning-for-Adaptive-LLM-Response-Identification" class="headerlink" title="Multi-Agent Conversational Online Learning for Adaptive LLM Response   Identification"></a>Multi-Agent Conversational Online Learning for Adaptive LLM Response   Identification</h2><p><strong>Authors:Xiangxiang Dai, Yuejin Xie, Maoli Liu, Xuchuang Wang, Zhuohua Li, Huanyu Wang, John C. S. Lui</strong></p>
<p>The remarkable generative capability of large language models (LLMs) has sparked a growing interest in automatically generating responses for different applications. Given the dynamic nature of user preferences and the uncertainty of LLM response performance, it is crucial to design efficient online learning algorithms to identify optimal LLM responses (i.e., high-quality responses that also meet user preferences). Most existing online algorithms adopt a centralized approach and fail to leverage explicit user preferences for more efficient and personalized LLM response identification. In contrast, this paper introduces \textit{MACO} (\underline{M}ulti-\underline{A}gent \underline{C}onversational \underline{O}nline Learning for Adaptive LLM Response Identification): 1) The online LLM response identification process is accelerated by multiple local agents (such as smartphones), while enhancing data privacy; 2) A novel conversational mechanism is proposed to adaptively conduct conversations for soliciting user preferences (e.g., a preference for a humorous tone over a serious one in generated responses), so to minimize uncertainty in preference estimation. Our theoretical analysis demonstrates that \cadi\ is near-optimal regarding cumulative regret. Additionally, \cadi\ offers reduced communication costs and computational complexity by eliminating the traditional, computing-intensive &#96;&#96;G-optimal design” found in previous works. Extensive experiments with the open LLM \textit{Llama}, coupled with two different embedding models from Google and OpenAI for text vector representation, demonstrate that \cadi\ significantly outperforms the current state-of-the-art in online LLM response identification. </p>
<blockquote>
<p>大型语言模型（LLM）的出色生成能力引发了人们对不同应用自动生成响应的浓厚兴趣。考虑到用户偏好的动态性和LLM响应性能的不确定性，设计高效的在线学习算法来识别最佳的LLM响应（即高质量且符合用户偏好的响应）至关重要。现有的大多数在线算法采用集中式方法，未能充分利用明确的用户偏好来进行更高效、个性化的LLM响应识别。相比之下，本文介绍了\textit{MACO}（用于自适应LLM响应识别的多代理在线对话学习）：1）在线LLM响应识别过程通过多个本地代理（如智能手机）加速，同时增强数据隐私；2）提出了一种新的对话机制，以自适应的方式进行对话，以征求用户偏好（例如，在生成的响应中更喜欢幽默的语调而不是严肃的语调），从而最小化偏好估计中的不确定性。我们的理论分析表明，\cadi\在累积遗憾方面接近最优。此外，\cadi\通过消除先前工作中的计算密集型“G最优设计”，降低了通信成本和计算复杂性。使用开源LLM“Llama”进行的广泛实验，结合谷歌和OpenAI的两个不同嵌入模型进行文本向量表示，表明\cadi\在在线LLM响应识别方面显著优于当前最新技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01849v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的生成能力引起了广泛关注，并广泛应用于自动生成响应的不同应用。针对用户偏好动态性和LLM响应性能的不确定性，设计高效的在线学习算法至关重要。本文提出了一种基于多智能体的在线学习方法MACO，用于自适应LLM响应识别。MACO通过多个本地代理（如智能手机）加速在线LLM响应识别过程，提高数据隐私保护。它采用新颖的对话机制，通过对话了解用户偏好，降低偏好估计的不确定性。MACO具有近最优的累积遗憾值，并降低了通信成本和计算复杂性。实验表明，MACO在在线LLM响应识别方面显著优于当前最先进技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的生成能力在自动生成响应的不同应用中受到广泛关注。</li>
<li>设计和实施高效的在线学习算法对于识别符合用户偏好的高质量LLM响应至关重要。</li>
<li>MACO方法通过多个本地代理（如智能手机）加速在线LLM响应识别，增强数据隐私保护。</li>
<li>MACO采用新颖的对话机制了解用户偏好，降低偏好估计的不确定性。</li>
<li>MACO具有理论上的近最优性能，在累积遗憾方面表现良好。</li>
<li>MACO降低了通信成本和计算复杂性，优于传统的计算密集型方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01849">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ee702e8e1112983d3d51b9ef26eb2b73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5de89fdcb2739fec859b3c84ac9012e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a48bd037ecc6ef2fa4e190f861993182.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e10697dfc99937228589804c8a1b0189.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MoColl-Agent-Based-Specific-and-General-Model-Collaboration-for-Image-Captioning"><a href="#MoColl-Agent-Based-Specific-and-General-Model-Collaboration-for-Image-Captioning" class="headerlink" title="MoColl: Agent-Based Specific and General Model Collaboration for Image   Captioning"></a>MoColl: Agent-Based Specific and General Model Collaboration for Image   Captioning</h2><p><strong>Authors:Pu Yang, Bin Dong</strong></p>
<p>Image captioning is a critical task at the intersection of computer vision and natural language processing, with wide-ranging applications across various domains. For complex tasks such as diagnostic report generation, deep learning models require not only domain-specific image-caption datasets but also the incorporation of relevant general knowledge to provide contextual accuracy. Existing approaches exhibit inherent limitations: specialized models excel in capturing domain-specific details but lack generalization, while vision-language models (VLMs) built on large language models (LLMs) leverage general knowledge but struggle with domain-specific adaptation. To address these limitations, this paper proposes a novel agent-enhanced model collaboration framework, which we called \textbf{MoColl}, designed to effectively integrate domain-specific and general knowledge. Specifically, our approach is to decompose complex image captioning tasks into a series of interconnected question-answer subtasks. A trainable visual question answering (VQA) model is employed as a specialized tool to focus on domain-specific visual analysis, answering task-specific questions based on image content. Concurrently, an LLM-based agent with general knowledge formulates these questions and synthesizes the resulting question-answer pairs into coherent captions. Beyond its role in leveraging the VQA model, the agent further guides its training to enhance its domain-specific capabilities. Experimental results on radiology report generation validate the effectiveness of the proposed framework, demonstrating significant improvements in the quality of generated reports. </p>
<blockquote>
<p>图像标注是计算机视觉和自然语言处理领域的交叉任务，具有广泛的应用范围。对于生成诊断报告等复杂任务，深度学习模型不仅需要特定领域的图像标注数据集，还需要融入相关的通用知识来提供上下文准确性。现有方法存在固有的局限性：特定领域的模型擅长捕捉特定领域的细节，但缺乏泛化能力；而基于大型语言模型的视觉语言模型（VLMs）则利用通用知识，但在特定领域的适应性方面遇到困难。为了解决这些局限性，本文提出了一种新型代理增强模型协作框架，我们称之为MoColl，旨在有效地整合特定领域知识和通用知识。具体来说，我们的方法是将复杂的图像标注任务分解为一系列相互关联的问答子任务。我们采用可训练的视觉问答（VQA）模型作为专用工具，专注于特定领域的视觉分析，根据图像内容回答特定任务的问题。同时，一个基于大型语言模型的代理利用通用知识来制定这些问题，并将结果的问题答案对综合成连贯的标注。除了引导VQA模型的作用外，代理还能进一步引导其训练以增强其特定领域的能力。在放射学报告生成方面的实验验证了所提框架的有效性，显示出生成的报告质量显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01834v1">PDF</a> </p>
<p><strong>Summary</strong><br>     图像描述是计算机视觉和自然语言处理领域的核心任务之一，广泛应用于多个领域。针对诊断报告生成等复杂任务，深度学习模型不仅需要特定的图像描述数据集，还需要融入相关的通用知识以提高语境准确性。现有方法存在局限性：特定模型擅长捕捉特定领域的细节，但缺乏通用性；而基于大型语言模型的视觉语言模型（VLMs）虽然可以利用通用知识，但在特定领域的适应性上却遇到困难。为解决这些问题，本文提出了一种新型代理增强模型协作框架（MoColl），旨在有效整合特定领域和通用知识。该框架通过将复杂的图像描述任务分解为一系列相互关联的问答子任务来发挥作用。一个训练有素的视觉问答（VQA）模型作为专用工具，专注于特定领域的视觉分析，根据图像内容回答特定任务的问题。同时，具有通用知识的LLM代理负责提出问题和整合问答对形成连贯的描述。除了引导VQA模型的应用外，代理还能进一步提升其特定领域的技能。在放射学报告生成方面的实验验证了该框架的有效性，显著提高了生成报告的质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像描述是计算机视觉和自然语言处理领域的交叉任务，具有广泛的应用领域。</li>
<li>深度学习模型在复杂任务如诊断报告生成中需要特定的图像描述数据集和通用知识。</li>
<li>现有方法存在局限性：特定模型缺乏通用性，而基于大型语言模型的视觉语言模型在特定领域适应性上存在问题。</li>
<li>提出了一种新型的代理增强模型协作框架（MoColl），整合特定领域和通用知识。</li>
<li>通过将复杂的图像描述任务分解为一系列相互关联的问答子任务来发挥作用。</li>
<li>采用视觉问答（VQA）模型作为专用工具，回答特定任务的问题，而LLM代理则负责提出问题和整合问答对。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01834">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d8336c1f9eefbcc0b66b7521523fe98c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4917c0a589e3bb6f90baaa052cbc024a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b92626a788df88a2a174da8757a5d59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7850f40c125395b24e8cad281c507d49.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SDPO-Segment-Level-Direct-Preference-Optimization-for-Social-Agents"><a href="#SDPO-Segment-Level-Direct-Preference-Optimization-for-Social-Agents" class="headerlink" title="SDPO: Segment-Level Direct Preference Optimization for Social Agents"></a>SDPO: Segment-Level Direct Preference Optimization for Social Agents</h2><p><strong>Authors:Aobo Kong, Wentao Ma, Shiwan Zhao, Yongbin Li, Yuchuan Wu, Ke Wang, Xiaoqian Liu, Qicheng Li, Yong Qin, Fei Huang</strong></p>
<p>Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex goal-oriented social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across a variety of agent tasks. Existing DPO-based approaches for multi-turn interactions are divided into turn-level and session-level methods. The turn-level method is overly fine-grained, focusing exclusively on individual turns, while session-level methods are too coarse-grained, often introducing training noise. To address these limitations, we propose Segment-Level Direct Preference Optimization (SDPO), which focuses on specific key segments within interactions to optimize multi-turn agent behavior while minimizing training noise. Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring SDPO’s potential to advance the social intelligence of LLM-based agents. We release our code and data at <a target="_blank" rel="noopener" href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO">https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO</a>. </p>
<blockquote>
<p>由大型语言模型（LLM）驱动的社会代理能够模拟人类的社会行为，但在处理复杂的以目标为导向的社会对话方面表现不足。直接偏好优化（DPO）在各种代理任务中，已被证明在将LLM行为与人的偏好对齐方面非常有效。现有的基于DPO的多轮交互方法分为轮级和会话级方法。轮级方法过于精细，只关注个别轮次，而会话级方法则过于粗糙，经常引入训练噪声。为了解决这些局限性，我们提出了分段级直接偏好优化（SDPO），它专注于交互中的特定关键段，以优化多轮代理行为，同时最小化训练噪声。在SOTOPIA基准测试上的评估表明，SDPO调优的代理始终优于现有的DPO方法和专有LLM，如GPT-4o，这突显了SDPO在推进基于LLM的代理的社会智能方面的潜力。我们的代码和数据已在<a target="_blank" rel="noopener" href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01821v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型驱动的社会代理能够模拟人类社交行为，但在处理复杂的、以目标为导向的社会对话方面存在不足。为解决现有直接偏好优化（DPO）方法在处理多轮交互时的局限性，我们提出了分段级别直接偏好优化（SDPO），专注于交互中的特定关键段落，以优化多轮代理行为并减少训练噪声。评估表明，SDPO调优的代理在SOTOPIA基准测试中表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）驱动的社会代理可以模拟人类社交行为，但在处理目标导向的社会对话时存在不足。</li>
<li>直接偏好优化（DPO）对于对齐LLM行为和人类偏好在各种代理任务中已被证明是有效的。</li>
<li>现有DPO方法在处理多轮交互时存在局限性，分为轮级和会话级方法。轮级方法过于精细，专注于个别轮次；会话级方法则过于粗糙，常引入训练噪声。</li>
<li>为解决上述问题，提出了分段级别直接偏好优化（SDPO），专注于交互中的特定关键段落进行优化，以减少训练噪声。</li>
<li>SDPO在SOTOPIA基准测试中的表现优于现有的DPO方法和GPT-4o等专有LLM，显示出其在提高LLM基础代理的社会智能方面的潜力。</li>
<li>我们已在指定链接公开了相关代码和数据，以便进一步研究和应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01821">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fda69ddcde89e23c3428ba78f3a6c8fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c63a2cd5da3a4bee1b4ee50de67b9af8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10915b050ec92c6b68d267e0892d4014.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4d0dfa47668766807503aefdfcc2ff6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4080768d3af372118292bc76e487e64c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AgentRefine-Enhancing-Agent-Generalization-through-Refinement-Tuning"><a href="#AgentRefine-Enhancing-Agent-Generalization-through-Refinement-Tuning" class="headerlink" title="AgentRefine: Enhancing Agent Generalization through Refinement Tuning"></a>AgentRefine: Enhancing Agent Generalization through Refinement Tuning</h2><p><strong>Authors:Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu</strong></p>
<p>Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research. </p>
<blockquote>
<p>基于大型语言模型（LLM）的代理已经证明了它们执行复杂任务的能力，如同人类。然而，开源LLM和商业模型（如GPT系列）之间仍存在巨大差距。在本文中，我们专注于通过指令调整提高LLM的代理泛化能力。我们首先观察到，现有的代理训练语料库在内部评估集上表现良好，但却无法泛化到外部集。这些代理调整工作面临着严重的格式错误，并经常长时间陷入同样的错误。我们分析认为，泛化能力差的根源在于对多种手动代理环境的过度适应以及对新情况的适应能力不足。他们无法采取正确的行动步骤，无法从经验中学习，而只是记忆现有的观察-行动关系。在此基础上，我们提出了新颖的AgentRefine框架用于代理调整。核心思想是使模型能够通过轨迹中的观察来纠正其错误。具体来说，我们提出了一个代理合成框架，以涵盖各种环境和任务，并提示强大的LLM根据环境反馈来纠正其错误行动。AgentRefine在多种代理任务的泛化能力方面显著优于最新的代理调整工作。它还具有更好的抗扰动性，并在推理过程中能产生多样化的思想。我们的发现建立了代理泛化和自我完善之间的关联，为未来研究提供了新的范式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01702v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在复杂任务上的表现已接近人类水平，但开源LLM与商业模型如GPT系列之间仍存在差距。本文聚焦于通过指令微调提升LLM的代理泛化能力。研究发现，现有代理训练语料库在封闭测试集上表现良好，但在开放测试集上泛化能力较差。为解决这一问题，本文提出了新型的AgentRefine框架，该框架旨在让模型通过观察轨迹学习纠正错误。AgentRefine显著提升了现有代理调整工作在多样化代理任务上的泛化能力，并展现出更好的鲁棒性和推理多样性。本文的发现为代理泛化与自我修正之间的关联提供了新的视角和研究范式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在复杂任务上的表现已接近人类水平，但存在开源与商业模型间的差距。</li>
<li>现有LLM代理训练存在泛化能力不足的问题，无法适应新情境。</li>
<li>AgentRefine框架被提出用于提升LLM的代理泛化能力，通过自我修正学习来纠正错误。</li>
<li>AgentRefine框架包含多样化的环境和任务，并成功引导LLM根据环境反馈修正错误动作。</li>
<li>AgentRefine显著提升了在多样化代理任务上的泛化能力，展现更好的鲁棒性和推理多样性。</li>
<li>本文发现代理泛化与自我修正之间的关联为未来研究提供了新的视角。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01702">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d038ca7ce01a33796accd474d3a68b23.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88161e3dc0e407c08178fa9ee8f00577.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ded54c4fc8f22277871fda225df4e9ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4af506b5091c5002bfebb929d32ae1b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7dbd55274cb9289be6953d7d5368945a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7b2ca290aff6d45948646de103b3bbf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Based-Multi-Agent-System-Augmented-Complex-Event-Processing-Pipeline-for-Internet-of-Multimedia-Things"><a href="#Large-Language-Model-Based-Multi-Agent-System-Augmented-Complex-Event-Processing-Pipeline-for-Internet-of-Multimedia-Things" class="headerlink" title="Large Language Model Based Multi-Agent System Augmented Complex Event   Processing Pipeline for Internet of Multimedia Things"></a>Large Language Model Based Multi-Agent System Augmented Complex Event   Processing Pipeline for Internet of Multimedia Things</h2><p><strong>Authors:Talha Zeeshan, Abhishek Kumar, Susanna Pirttikangas, Sasu Tarkoma</strong></p>
<p>This paper presents the development and evaluation of a Large Language Model (LLM), also known as foundation models, based multi-agent system framework for complex event processing (CEP) with a focus on video query processing use cases. The primary goal is to create a proof-of-concept (POC) that integrates state-of-the-art LLM orchestration frameworks with publish&#x2F;subscribe (pub&#x2F;sub) tools to address the integration of LLMs with current CEP systems. Utilizing the Autogen framework in conjunction with Kafka message brokers, the system demonstrates an autonomous CEP pipeline capable of handling complex workflows. Extensive experiments evaluate the system’s performance across varying configurations, complexities, and video resolutions, revealing the trade-offs between functionality and latency. The results show that while higher agent count and video complexities increase latency, the system maintains high consistency in narrative coherence. This research builds upon and contributes to, existing novel approaches to distributed AI systems, offering detailed insights into integrating such systems into existing infrastructures. </p>
<blockquote>
<p>本文介绍了一个基于大型语言模型（LLM）的多智能体系统框架的开发与评估，用于复杂事件处理（CEP），重点关注视频查询处理用例。主要目标是创建一个概念验证（POC），将最新的LLM编排框架与发布&#x2F;订阅（pub&#x2F;sub）工具集成，以解决LLM与当前CEP系统的集成问题。该系统利用Autogen框架和Kafka消息代理，展示了一个能够处理复杂工作流程的自主CEP管道。通过广泛的实验，对系统在不同配置、复杂度和视频分辨率下的性能进行了评估，揭示了功能与延迟之间的权衡。结果表明，虽然较高的智能体数量和视频复杂度会增加延迟，但系统在叙事连贯性方面保持高度一致性。该研究建立在现有的分布式人工智能系统的新颖方法之上，并为其做出贡献，为将此类系统融入现有基础设施提供了详细的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00906v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本论文提出并评估了一种基于大型语言模型（LLM）的多智能体系统框架，主要应用于复杂事件处理（CEP）。其专注于视频查询处理用例，旨在创建一种将先进的LLM编排框架与发布&#x2F;订阅工具结合的原型概念，以解决LLM与当前CEP系统的集成问题。通过结合Autogen框架和Kafka消息代理，该系统展示了能够处理复杂工作流程的自主CEP管道。实验结果显示，尽管在智能体数量增加和视频复杂度提高的情况下，系统延迟有所增加，但在叙事连贯性方面保持高度一致性。本研究对分布式AI系统的集成进行了深入的探索和改进，提供了现有基础设施中集成的深入见解。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关键要点总结：</p>
<ol>
<li>大型语言模型在多智能体系统框架中被用来进行复杂事件处理，特别是视频查询处理。</li>
<li>该系统旨在整合先进的LLM编排框架与发布&#x2F;订阅工具，以解决LLM与当前CEP系统的集成问题。</li>
<li>利用Autogen框架和Kafka消息代理技术实现了自主CEP管道。</li>
<li>实验评估了系统在不同配置、复杂度和视频分辨率下的性能表现。</li>
<li>智能体数量和视频复杂度的增加会增加系统延迟。</li>
<li>系统在叙事连贯性方面表现良好，即使在较高的复杂度和延迟下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00906">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-09744d47e8eeb07b96b9b06e60277b63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-716bcb1c96f4c247b0d16cf4400a87f8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLM-Reasoning-with-Multi-Path-Collaborative-Reactive-and-Reflection-agents"><a href="#Enhancing-LLM-Reasoning-with-Multi-Path-Collaborative-Reactive-and-Reflection-agents" class="headerlink" title="Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and   Reflection agents"></a>Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and   Reflection agents</h2><p><strong>Authors:Chengbo He, Bochao Zou, Xin Li, Jiansheng Chen, Junliang Xing, Huimin Ma</strong></p>
<p>Agents have demonstrated their potential in scientific reasoning tasks through large language models. However, they often face challenges such as insufficient accuracy and degeneration of thought when handling complex reasoning tasks, which impede their performance. To overcome these issues, we propose the Reactive and Reflection agents with Multi-Path Reasoning (RR-MP) Framework, aimed at enhancing the reasoning capabilities of LLMs. Our approach improves scientific reasoning accuracy by employing a multi-path reasoning mechanism where each path consists of a reactive agent and a reflection agent that collaborate to prevent degeneration of thought inherent in single-agent reliance. Additionally, the RR-MP framework does not require additional training; it utilizes multiple dialogue instances for each reasoning path and a separate summarizer to consolidate insights from all paths. This design integrates diverse perspectives and strengthens reasoning across each path. We conducted zero-shot and few-shot evaluations on tasks involving moral scenarios, college-level physics, and mathematics. Experimental results demonstrate that our method outperforms baseline approaches, highlighting the effectiveness and advantages of the RR-MP framework in managing complex scientific reasoning tasks. </p>
<blockquote>
<p>人工智能代理通过大规模语言模型在科学推理任务中展示了其潜力。然而，在处理复杂的推理任务时，它们经常面临准确性不足和思想退化等挑战，这阻碍了它们的性能。为了克服这些问题，我们提出了具有多路径推理的响应与反思代理（RR-MP）框架，旨在增强大型语言模型的推理能力。我们的方法采用多路径推理机制来提高科学推理的准确性，每条路径包含一个响应代理和一个反思代理，它们协同工作以防止单一代理依赖所导致的思想退化。此外，RR-MP框架不需要额外的训练；它利用每个推理路径的多个对话实例和一个单独的摘要器来整合所有路径的见解。这种设计融合了不同的观点，加强了每条路径的推理能力。我们在涉及道德场景、大学物理和数学的任务上进行了零样本和少样本评估。实验结果表明，我们的方法优于基准方法，突显了RR-MP框架在处理复杂科学推理任务中的有效性和优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00430v2">PDF</a> </p>
<p><strong>总结</strong></p>
<p>基于多路径推理机制的反应与反思智能体（RR-MP）框架增强了语言大模型在科学推理任务中的能力。通过采用包含反应智能体和反思智能体的多路径推理机制，RR-MP框架能有效防止单一智能体推理过程中的思维退化问题。无需额外训练，该框架利用不同推理路径的对话实例和汇总器来整合观点，加强每条路径的推理能力。在道德情境、大学物理和数学任务上的零样本和少样本评估表明，该方法优于基准方法，突显了RR-MP框架在处理复杂科学推理任务时的优势和效果。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Agents在科研推理任务中具有巨大潜力，但仍面临准确性不足和应对复杂任务时思维退化等挑战。</li>
<li>提出的RR-MP框架旨在增强语言大模型（LLMs）的推理能力。</li>
<li>RR-MP框架采用多路径推理机制，包含反应智能体和反思智能体，以克服单一智能体的思维退化问题。</li>
<li>该框架利用不同推理路径的对话实例，通过汇总器整合观点，加强了每条路径的推理能力。</li>
<li>RR-MP框架设计灵活，不需要额外训练。</li>
<li>实验结果表明，RR-MP框架在复杂科学推理任务上的表现优于基准方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00430">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-93de0469ccfbb33bde4a4e537efbad84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e22c4ad6aefd38bdb3ab700d96a4f544.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67d605dcdfdcbb24ae7240c5325489f3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="KG4Diagnosis-A-Hierarchical-Multi-Agent-LLM-Framework-with-Knowledge-Graph-Enhancement-for-Medical-Diagnosis"><a href="#KG4Diagnosis-A-Hierarchical-Multi-Agent-LLM-Framework-with-Knowledge-Graph-Enhancement-for-Medical-Diagnosis" class="headerlink" title="KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge   Graph Enhancement for Medical Diagnosis"></a>KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge   Graph Enhancement for Medical Diagnosis</h2><p><strong>Authors:Kaiwen Zuo, Yirui Jiang, Fan Mo, Pietro Lio</strong></p>
<p>Integrating Large Language Models (LLMs) in healthcare diagnosis demands systematic frameworks that can handle complex medical scenarios while maintaining specialized expertise. We present KG4Diagnosis, a novel hierarchical multi-agent framework that combines LLMs with automated knowledge graph construction, encompassing 362 common diseases across medical specialties. Our framework mirrors real-world medical systems through a two-tier architecture: a general practitioner (GP) agent for initial assessment and triage, coordinating with specialized agents for in-depth diagnosis in specific domains. The core innovation lies in our end-to-end knowledge graph generation methodology, incorporating: (1) semantic-driven entity and relation extraction optimized for medical terminology, (2) multi-dimensional decision relationship reconstruction from unstructured medical texts, and (3) human-guided reasoning for knowledge expansion. KG4Diagnosis serves as an extensible foundation for specialized medical diagnosis systems, with capabilities to incorporate new diseases and medical knowledge. The framework’s modular design enables seamless integration of domain-specific enhancements, making it valuable for developing targeted medical diagnosis systems. We provide architectural guidelines and protocols to facilitate adoption across medical contexts. </p>
<blockquote>
<p>将大型语言模型（LLMs）整合到医疗诊断中，需要能够处理复杂医疗场景并保持专业知识的系统性框架。我们提出了KG4Diagnosis，这是一种新型分层多智能体框架，它将LLMs与自动化知识图谱构建相结合，涵盖362种常见疾病，涉及医学各个专业。我们的框架通过两层架构反映现实医疗系统：全科医师（GP）智能体进行初步评估和分级，并与针对特定领域的专业智能体协调进行深入诊断。核心创新在于我们端到端的知识图谱生成方法，包括：（1）针对医学术语优化的语义驱动实体和关系提取，（2）从非结构化医疗文本中重建多维决策关系，（3）用于知识扩展的人机协同推理。KG4Diagnosis是一个可扩展的基础，用于专业医疗诊断系统，有能力融入新的疾病和医学知识。该框架的模块化设计使其能够无缝集成特定领域的增强功能，对于开发有针对性的医疗诊断系统非常有价值。我们提供架构指南和协议，以推动其在医疗环境中的采用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16833v2">PDF</a> 10 pages,5 figures,published to AAAI-25 Bridge Program</p>
<p><strong>Summary</strong>：KG4Diagnosis框架结合大型语言模型与知识图谱构建技术，为医疗保健诊断提供了系统化解决方案。该框架采用分层多智能体架构，包括初级评估与协调的智能体及专业领域的深度诊断智能体，覆盖362种常见疾病。其核心创新在于端到端的知识图谱生成方法，包括针对医学术语优化的语义驱动实体和关系提取、从非结构化医学文本中重建的多维度决策关系以及人类引导的知识扩展推理。KG4Diagnosis框架为开发专业医疗诊断系统提供了可扩展的基础，并能轻松集成领域特定增强功能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>KG4Diagnosis是一个整合大型语言模型的系统化医疗诊断框架。</li>
<li>该框架采用分层多智能体架构，包括初级评估与协调的智能体及专业领域智能体。</li>
<li>KG4Diagnosis覆盖362种常见疾病，并具备扩展能力以纳入新的疾病和医学知识。</li>
<li>框架的核心创新在于其端到端的知识图谱生成方法，包括语义驱动的实体和关系提取、多维度决策关系重建和人类引导的知识扩展。</li>
<li>该框架能够从非结构化医学文本中提取信息，并据此进行诊断决策。</li>
<li>KG4Diagnosis的模块化设计使无缝集成特定领域的增强功能成为可能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16833">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-80bf68225d1bee6beba8e62461336758.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f50afebfc586d931d790904ce420fc4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-144a2f441c16fa0f57c3081d6b171dc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4cb7c4aecc4a233f64b49a8137451e34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-336e50ee4a5e3968ea86e2a2fea37f14.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Agent-Planning-with-World-Knowledge-Model"><a href="#Agent-Planning-with-World-Knowledge-Model" class="headerlink" title="Agent Planning with World Knowledge Model"></a>Agent Planning with World Knowledge Model</h2><p><strong>Authors:Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</strong></p>
<p>Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the &#96;&#96;real’’ physical world. Imitating humans’ mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent’s understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zjunlp/WKM">https://github.com/zjunlp/WKM</a>. </p>
<blockquote>
<p>近期，直接使用大型语言模型（LLM）作为代理模型执行交互式规划任务的研究取得了值得赞扬的成果。然而，尽管取得了成就，它们仍然在全球规划方面存在盲目试错的问题，并在局部规划生成幻觉行为，这是因为它们对“真实”物理世界的理解不足。</p>
</blockquote>
<p>本文中，我们引入参数化世界知识模型（WKM）来促进代理规划，模仿人类的精神世界知识模型，在任务前提供全局先验知识，并在任务期间保持局部动态知识。具体地，我们引导代理模型从专家轨迹和采样轨迹中自我合成知识。然后，我们开发WKM，提供任务先验知识来指导全局规划，以及动态状态知识来辅助局部规划。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14205v4">PDF</a> NeurIPS 2024</p>
<p><strong>摘要</strong><br>大型语言模型（LLM）作为代理模型直接执行交互式规划任务取得了可喜的成果，但仍存在全球规划中的盲目试错和本地规划中的幻想行为问题。为了解决这个问题，本文引入了参数化世界知识模型（WKM），使代理模型能够从专家轨迹和采样轨迹中自我合成知识。实验结果表明，该方法在三个复杂现实世界的模拟数据集上，使用三种最先进的开源LLM（Mistral-7B、Gemma-7B和Llama-3-8B）时，相比各种强大的基线方法具有优越的性能。此外，本文分析了WKM能有效减轻盲目试错和幻想行为问题，为代理模型理解世界提供有力支持。还发现实例级任务知识能更好地推广到未见过的任务，弱WKM能指导强代理模型规划，统一WKM训练有进一步开发的潜力。代码可在<a target="_blank" rel="noopener" href="https://github.com/zjunlp/WKM">https://github.com/zjunlp/WKM</a>获取。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLMs）在执行交互式规划任务时虽然取得显著成果，但在全球规划和本地规划中仍存在盲目试错和幻想行为问题。</li>
<li>引入参数化世界知识模型（WKM）以增强代理模型对现实世界理解的缺乏的问题。该模型能够结合专家轨迹和采样轨迹进行自我合成知识。</li>
<li>在三个复杂现实世界的模拟数据集上进行的实验表明，使用WKM的方法相较于基线方法具有优越性能。</li>
<li>WKM能有效减轻盲目试错和幻想行为问题，改善代理模型的世界理解力。此外，它能够推广实例级任务知识到未见过的任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14205">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-10e548841f8670214eada6a38e4e8c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c988b7da694d55c09897345596a1403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1b82792178546b9e8265d0e4b645cf5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-07/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-07/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-07/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e22c4ad6aefd38bdb3ab700d96a4f544.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-01-07  Adaptive Few-shot Prompting for Machine Translation with Pre-trained   Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-07/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d038ca7ce01a33796accd474d3a68b23.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-01-07  VITA-1.5 Towards GPT-4o Level Real-Time Vision and Speech Interaction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
