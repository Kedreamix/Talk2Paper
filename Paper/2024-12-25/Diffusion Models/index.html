<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  FaceLift Single Image to 3D Head with View Generation and GS-LRM">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-afa28c3d81953ef5271254f685ce67f1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-25-æ›´æ–°"><a href="#2024-12-25-æ›´æ–°" class="headerlink" title="2024-12-25 æ›´æ–°"></a>2024-12-25 æ›´æ–°</h1><h2 id="FaceLift-Single-Image-to-3D-Head-with-View-Generation-and-GS-LRM"><a href="#FaceLift-Single-Image-to-3D-Head-with-View-Generation-and-GS-LRM" class="headerlink" title="FaceLift: Single Image to 3D Head with View Generation and GS-LRM"></a>FaceLift: Single Image to 3D Head with View Generation and GS-LRM</h2><p><strong>Authors:Weijie Lyu, Yi Zhou, Ming-Hsuan Yang, Zhixin Shu</strong></p>
<p>We present FaceLift, a feed-forward approach for rapid, high-quality, 360-degree head reconstruction from a single image. Our pipeline begins by employing a multi-view latent diffusion model that generates consistent side and back views of the head from a single facial input. These generated views then serve as input to a GS-LRM reconstructor, which produces a comprehensive 3D representation using Gaussian splats. To train our system, we develop a dataset of multi-view renderings using synthetic 3D human head as-sets. The diffusion-based multi-view generator is trained exclusively on synthetic head images, while the GS-LRM reconstructor undergoes initial training on Objaverse followed by fine-tuning on synthetic head data. FaceLift excels at preserving identity and maintaining view consistency across views. Despite being trained solely on synthetic data, FaceLift demonstrates remarkable generalization to real-world images. Through extensive qualitative and quantitative evaluations, we show that FaceLift outperforms state-of-the-art methods in 3D head reconstruction, highlighting its practical applicability and robust performance on real-world images. In addition to single image reconstruction, FaceLift supports video inputs for 4D novel view synthesis and seamlessly integrates with 2D reanimation techniques to enable 3D facial animation. Project page: <a target="_blank" rel="noopener" href="https://weijielyu.github.io/FaceLift">https://weijielyu.github.io/FaceLift</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†FaceLiftï¼Œè¿™æ˜¯ä¸€ç§å¿«é€Ÿã€é«˜è´¨é‡çš„å•å›¾åƒ360åº¦å¤´éƒ¨é‡å»ºçš„å‰é¦ˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„ç®¡é“é¦–å…ˆé‡‡ç”¨å¤šè§†è§’æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä»å•ä¸ªé¢éƒ¨è¾“å…¥ç”Ÿæˆä¸€è‡´çš„å¤´éƒ¨ä¾§é¢å’ŒèƒŒé¢è§†å›¾ã€‚è¿™äº›ç”Ÿæˆçš„è§†å›¾ç„¶åä½œä¸ºGS-LRMé‡å»ºå™¨çš„è¾“å…¥ï¼Œä½¿ç”¨é«˜æ–¯æ–‘ç‚¹ç”Ÿæˆå…¨é¢çš„3Dè¡¨ç¤ºã€‚ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„ç³»ç»Ÿï¼Œæˆ‘ä»¬ä½¿ç”¨åˆæˆ3Däººå¤´æ•°æ®é›†å¼€å‘äº†ä¸€ä¸ªå¤šè§†è§’æ¸²æŸ“æ•°æ®é›†ã€‚åŸºäºæ‰©æ•£çš„å¤šè§†è§’ç”Ÿæˆå™¨ä»…æ¥å—åˆæˆå¤´éƒ¨å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œè€ŒGS-LRMé‡å»ºå™¨åˆ™é¦–å…ˆåœ¨Objaverseä¸Šè¿›è¡Œåˆæ­¥è®­ç»ƒï¼Œç„¶ååœ¨åˆæˆå¤´éƒ¨æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚FaceLiftæ“…é•¿åœ¨è§†å›¾ä¹‹é—´ä¿æŒèº«ä»½ä¸å˜å’Œä¿æŒè§†å›¾ä¸€è‡´æ€§ã€‚å°½ç®¡ä»…å¯¹åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†FaceLiftåœ¨çœŸå®å›¾åƒä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¹¿æ³›çš„è´¨é‡å’Œæ•°é‡è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†FaceLiftåœ¨3Då¤´éƒ¨é‡å»ºæ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œçªå‡ºäº†å…¶åœ¨çœŸå®å›¾åƒä¸Šçš„å®é™…é€‚ç”¨æ€§å’Œç¨³å¥æ€§èƒ½ã€‚é™¤äº†å•å›¾åƒé‡å»ºå¤–ï¼ŒFaceLiftè¿˜æ”¯æŒè§†é¢‘è¾“å…¥ä»¥å®ç°4Dæ–°é¢–è§†å›¾åˆæˆï¼Œå¹¶ä¸2DåŠ¨ç”»æŠ€æœ¯æ— ç¼é›†æˆä»¥å®ç°3Dé¢éƒ¨åŠ¨ç”»ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://weijielyu.github.io/FaceLift%E3%80%82">https://weijielyu.github.io/FaceLiftã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17812v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://weijielyu.github.io/FaceLift">https://weijielyu.github.io/FaceLift</a></p>
<p><strong>Summary</strong></p>
<p>FaceLiftæ˜¯ä¸€ä¸ªåŸºäºå•å›¾åƒçš„é«˜æ•ˆã€é«˜è´¨é‡çš„360åº¦å¤´éƒ¨é‡å»ºæ–¹æ³•ã€‚å®ƒé‡‡ç”¨å‰é¦ˆæ–¹æ³•ï¼Œåˆ©ç”¨å¤šè§†å›¾æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€è‡´çš„ä¾§é¢å’ŒèƒŒé¢è§†å›¾ï¼Œå†é€šè¿‡GS-LRMé‡å»ºå™¨ç”Ÿæˆå…¨é¢çš„3Dè¡¨ç¤ºã€‚FaceLiftåœ¨ä¿ç•™èº«ä»½å’Œä¿æŒè§†å›¾ä¸€è‡´æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°½ç®¡åªåœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨çœŸå®å›¾åƒä¸­å±•ç¤ºå‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FaceLiftæ˜¯ä¸€ä¸ªåŸºäºå•å›¾åƒçš„å¿«é€Ÿã€é«˜è´¨é‡çš„å¤´éƒ¨é‡å»ºæ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨å¤šè§†å›¾æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€è‡´çš„ä¾§é¢å’ŒèƒŒé¢è§†å›¾ã€‚</li>
<li>GS-LRMé‡å»ºå™¨ç”¨äºç”Ÿæˆå…¨é¢çš„3Dè¡¨ç¤ºã€‚</li>
<li>FaceLiftåœ¨ä¿ç•™èº«ä»½å’Œè§†å›¾ä¸€è‡´æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä»…é€šè¿‡åˆæˆæ•°æ®è®­ç»ƒï¼ŒFaceLiftåœ¨çœŸå®å›¾åƒä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>FaceLiftæ”¯æŒè§†é¢‘è¾“å…¥ï¼Œç”¨äº4Dæ–°è§†è§’åˆæˆã€‚</li>
<li>FaceLiftå¯æ— ç¼é›†æˆ2Då†åŠ¨ç”»æŠ€æœ¯ï¼Œå®ç°3Dé¢éƒ¨åŠ¨ç”»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-349eec9112cef735aaf6ae4647cdbd29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc8da03525039b4ae3e9044e917243e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd63245f29e72293d12e29ceb8c5e623.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1e271b62307e29e5a34919c49d9602d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e78fba8fb441278560f1de775d5d63b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62f6ee998869e53fbfbf42868e93a43d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-Superposition-of-Diffusion-Models-Using-the-Ito-Density-Estimator"><a href="#The-Superposition-of-Diffusion-Models-Using-the-Ito-Density-Estimator" class="headerlink" title="The Superposition of Diffusion Models Using the ItÃ´ Density Estimator"></a>The Superposition of Diffusion Models Using the ItÃ´ Density Estimator</h2><p><strong>Authors:Marta Skreta, Lazar Atanackovic, Avishek Joey Bose, Alexander Tong, Kirill Neklyudov</strong></p>
<p>The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. SuperDiff leverages a new scalable It^o density estimator for the log likelihood of the diffusion SDE which incurs no additional overhead compared to the well-known Hutchinsonâ€™s estimator needed for divergence calculations. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performed solely through composition during inference, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiff is efficient during inference time, and mimics traditional composition operators such as the logical OR and the logical AND. We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, and improved unconditional de novo structure design of proteins. <a target="_blank" rel="noopener" href="https://github.com/necludov/super-diffusion">https://github.com/necludov/super-diffusion</a> </p>
<blockquote>
<p>éšç€å®¹æ˜“è®¿é—®çš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¯’æ­¦çºªçˆ†å‘å¼å¢é•¿ï¼Œå¯¹äºåœ¨ä¸éœ€è¦é‡æ–°è®­ç»ƒå¤§å‹ç»„åˆæ¨¡å‹çš„å¤§é‡è®¡ç®—è´Ÿæ‹…çš„å‰æä¸‹ï¼Œå°†å¤šä¸ªä¸åŒçš„é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç»“åˆèµ·æ¥çš„æ–¹æ³•çš„éœ€æ±‚é€æ¸æ˜¾ç°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•æ¥è§£å†³åœ¨ç”Ÿæˆé˜¶æ®µç»“åˆå¤šä¸ªé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•åŸºäºåä¸ºå åŠ çš„æ–°é¢–æ¡†æ¶ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬ä»è‘—åçš„è¿ç»­æ–¹ç¨‹ä¸­æ¨å¯¼å‡ºçš„ä¸¥æ ¼åŸºæœ¬åŸç†å‡ºå‘ï¼Œå¾—å‡ºå åŠ çš„ç»“è®ºï¼Œå¹¶ä¸ºSuperDiffä¸­ç»“åˆæ‰©æ•£æ¨¡å‹è®¾è®¡äº†ä¸¤ç§é‡èº«å®šåˆ¶çš„æ–°å‹ç®—æ³•ã€‚SuperDiffåˆ©ç”¨ä¸€ç§æ–°çš„å¯æ‰©å±•çš„It^oå¯†åº¦ä¼°è®¡å™¨å¯¹æ‰©æ•£éšæœºå¾®åˆ†æ–¹ç¨‹çš„å¯¹æ•°ä¼¼ç„¶è¿›è¡Œä¼°è®¡ï¼Œä¸ç”¨äºè®¡ç®—å‘æ•£çš„è‘—åHutchinsonä¼°è®¡å™¨ç›¸æ¯”ï¼Œæ— éœ€é¢å¤–çš„å¼€é”€ã€‚æˆ‘ä»¬è¯æ˜äº†SuperDiffåœ¨å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸Šçš„å¯æ‰©å±•æ€§ï¼Œå› ä¸ºå åŠ ä»…åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡ç»„åˆå®ç°ï¼Œå¹¶ä¸”ç”±äºå®ƒé€šè¿‡è‡ªåŠ¨é‡æ–°åŠ æƒæ–¹æ¡ˆç»“åˆäº†ä¸åŒçš„é¢„è®­ç»ƒå‘é‡åœºï¼Œå› æ­¤å¯ä»¥è½»æ¾å®ç°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†SuperDiffåœ¨æ¨ç†æ—¶é—´ä¸Šçš„æ•ˆç‡ï¼Œå¹¶æ¨¡ä»¿äº†ä¼ ç»Ÿçš„ç»„åˆè¿ç®—ç¬¦ï¼Œå¦‚é€»è¾‘ORå’Œé€»è¾‘ANDã€‚æˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜äº†åœ¨CIFAR-10ä¸Šç”Ÿæˆæ›´å¤šä¸åŒå›¾åƒã€ä½¿ç”¨Stable Diffusionè¿›è¡Œæ›´ç²¾ç¡®çš„æç¤ºæ¡ä»¶å›¾åƒç¼–è¾‘ä»¥åŠæ”¹è¿›æ— æ¡ä»¶çš„è›‹ç™½è´¨å…¨æ–°ç»“æ„è®¾è®¡æ–¹é¢çš„å®ç”¨æ€§ã€‚è¯¦æƒ…è¯·è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/necludov/super-diffusion%E4%BA%86%E8%A7%A3%E3%80%82">https://github.com/necludov/super-diffusionäº†è§£ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17762v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºSuperDiffçš„æ–°æ¡†æ¶ï¼Œç”¨äºåœ¨ç”Ÿæˆé˜¶æ®µç»“åˆå¤šä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚SuperDiffåˆ©ç”¨æ–°çš„å¯æ‰©å±•It^oå¯†åº¦ä¼°è®¡å™¨è®¡ç®—æ‰©æ•£éšæœºå¾®åˆ†æ–¹ç¨‹çš„å¯¹æ•°ä¼¼ç„¶å€¼ï¼Œæ— éœ€é¢å¤–çš„å¼€é”€ã€‚é€šè¿‡ç»„åˆä¸åŒçš„é¢„è®­ç»ƒå‘é‡åœºï¼ŒSuperDiffèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆæ›´å¤šæ ·åŒ–çš„å›¾åƒï¼Œæ›´å¿ å®äºæç¤ºæ¡ä»¶çš„å›¾åƒç¼–è¾‘ï¼Œä»¥åŠæ”¹è¿›çš„æ— æ¡ä»¶è›‹ç™½è´¨è®¾è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SuperDiffæ¡†æ¶å…è®¸ç»“åˆå¤šä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€è¿›è¡Œå¤§å‹ç»„åˆæ¨¡å‹çš„é‡æ–°è®­ç»ƒï¼Œé™ä½äº†è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>SuperDiffåˆ©ç”¨It^oå¯†åº¦ä¼°è®¡å™¨è®¡ç®—æ‰©æ•£æ¨¡å‹çš„å¯¹æ•°ä¼¼ç„¶å€¼ï¼Œå…·æœ‰å¯æ‰©å±•æ€§ã€‚</li>
<li>SuperDiffé€šè¿‡ç»„åˆä¸åŒçš„é¢„è®­ç»ƒå‘é‡åœºå®ç°å›¾åƒç”Ÿæˆã€å›¾åƒç¼–è¾‘å’Œè›‹ç™½è´¨è®¾è®¡ç­‰åŠŸèƒ½ã€‚</li>
<li>SuperDiffåœ¨ç”Ÿæˆé˜¶æ®µè¿›è¡Œå åŠ ï¼Œå¯æ‰©å±•åˆ°å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>SuperDiffå…·æœ‰æ— ç—›å®ç°ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–é‡æ–°åŠ æƒæ–¹æ¡ˆç»„åˆä¸åŒçš„é¢„è®­ç»ƒå‘é‡åœºã€‚</li>
<li>SuperDiffåœ¨æ¨ç†æ—¶æ•ˆç‡é«˜ï¼Œå¯ä»¥æ¨¡æ‹Ÿä¼ ç»Ÿçš„ç»„åˆè¿ç®—ç¬¦å¦‚é€»è¾‘ORå’Œé€»è¾‘ANDã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒSuperDiffåœ¨CIFAR-10å›¾åƒç”Ÿæˆã€Stable Diffusionçš„æç¤ºæ¡ä»¶å›¾åƒç¼–è¾‘å’Œè›‹ç™½è´¨è®¾è®¡ç­‰æ–¹é¢å…·æœ‰å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f7d314b80fc765aa63524302b7efa18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c9ded40ee9e773c67a1de1e5519d261.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db84a3ca4170d22b4df50a1c29c9f04e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Bias-Free-Training-Paradigm-for-More-General-AI-generated-Image-Detection"><a href="#A-Bias-Free-Training-Paradigm-for-More-General-AI-generated-Image-Detection" class="headerlink" title="A Bias-Free Training Paradigm for More General AI-generated Image   Detection"></a>A Bias-Free Training Paradigm for More General AI-generated Image   Detection</h2><p><strong>Authors:Fabrizio Guillaro, Giada Zingarini, Ben Usman, Avneesh Sud, Davide Cozzolino, Luisa Verdoliva</strong></p>
<p>Successful forensic detectors can produce excellent results in supervised learning benchmarks but struggle to transfer to real-world applications. We believe this limitation is largely due to inadequate training data quality. While most research focuses on developing new algorithms, less attention is given to training data selection, despite evidence that performance can be strongly impacted by spurious correlations such as content, format, or resolution. A well-designed forensic detector should detect generator specific artifacts rather than reflect data biases. To this end, we propose B-Free, a bias-free training paradigm, where fake images are generated from real ones using the conditioning procedure of stable diffusion models. This ensures semantic alignment between real and fake images, allowing any differences to stem solely from the subtle artifacts introduced by AI generation. Through content-based augmentation, we show significant improvements in both generalization and robustness over state-of-the-art detectors and more calibrated results across 27 different generative models, including recent releases, like FLUX and Stable Diffusion 3.5. Our findings emphasize the importance of a careful dataset curation, highlighting the need for further research in dataset design. Code and data will be publicly available at <a target="_blank" rel="noopener" href="https://grip-unina.github.io/B-Free/">https://grip-unina.github.io/B-Free/</a> </p>
<blockquote>
<p>æˆåŠŸçš„å–è¯æ£€æµ‹å™¨èƒ½å¤Ÿåœ¨æœ‰ç›‘ç£å­¦ä¹ çš„åŸºå‡†æµ‹è¯•ä¸­äº§ç”Ÿä¼˜ç§€çš„ç»“æœï¼Œä½†åœ¨è½¬å‘å®é™…åº”ç”¨ç¨‹åºæ—¶å´é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ç§é™åˆ¶ä¸»è¦æ˜¯å› ä¸ºè®­ç»ƒæ•°æ®è´¨é‡ä¸è¶³ã€‚è™½ç„¶å¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨å¼€å‘æ–°ç®—æ³•ä¸Šï¼Œä½†å¯¹è®­ç»ƒæ•°æ®é€‰æ‹©æ–¹é¢çš„å…³æ³¨å´å¾ˆå°‘ï¼Œå°½ç®¡æœ‰è¯æ®è¡¨æ˜ï¼Œæ€§èƒ½å¯èƒ½ä¼šå—åˆ°å†…å®¹ã€æ ¼å¼æˆ–åˆ†è¾¨ç‡ç­‰è™šå‡å…³è”çš„å¼ºå¤§å½±å“ã€‚ä¸€ä¸ªè®¾è®¡è‰¯å¥½çš„å–è¯æ£€æµ‹å™¨åº”è¯¥æ£€æµ‹ç”Ÿæˆå™¨ç‰¹å®šçš„ä¼ªå½±ï¼Œè€Œä¸æ˜¯åæ˜ æ•°æ®åè§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†B-Freeï¼Œä¸€ç§æ— åè®­ç»ƒèŒƒå¼ï¼Œå…¶ä¸­è™šå‡å›¾åƒæ˜¯ä»çœŸå®çš„å›¾åƒä½¿ç”¨ç¨³å®šæ‰©æ•£æ¨¡å‹çš„è°ƒèŠ‚ç¨‹åºç”Ÿæˆçš„ã€‚è¿™ç¡®ä¿äº†çœŸå®å’Œè™šå‡å›¾åƒä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œä½¿å¾—ä»»ä½•å·®å¼‚éƒ½ä»…æºäºAIç”Ÿæˆæ‰€å¼•å…¥çš„ç»†å¾®ä¼ªå½±ã€‚é€šè¿‡åŸºäºå†…å®¹çš„å¢å¼ºï¼Œæˆ‘ä»¬åœ¨æœ€å…ˆè¿›çš„æ£€æµ‹å™¨ä¸Šå®ç°äº†æ˜¾è‘—çš„ä¸€èˆ¬åŒ–å’Œç¨³å¥æ€§æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨åŒ…æ‹¬æœ€æ–°å‘å¸ƒçš„FLUXå’ŒStable Diffusion 3.5ç­‰27ä¸ªä¸åŒçš„ç”Ÿæˆæ¨¡å‹ä¸Šè·å¾—äº†æ›´æ ¡å‡†çš„ç»“æœã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†æ•°æ®é›†ç²¾å¿ƒæ•´ç†çš„é‡è¦æ€§ï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®é›†è®¾è®¡éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶çš„éœ€æ±‚ã€‚ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://grip-unina.github.io/B-Free/%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://grip-unina.github.io/B-Free/å…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17671v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºB-Freeçš„æ— åè®­ç»ƒèŒƒå¼ï¼Œä»¥è§£å†³ç°æœ‰æ³•åŒ»æ£€æµ‹å™¨åœ¨çœŸå®åº”ç”¨åœºæ™¯ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚ä½œè€…è®¤ä¸ºè¿™ä¸€é—®é¢˜çš„æ ¹æºåœ¨äºè®­ç»ƒæ•°æ®è´¨é‡ä¸è¶³ï¼Œå› æ­¤ä»–ä»¬é€šè¿‡ç”ŸæˆçœŸå®å›¾åƒç”Ÿæˆè™šå‡å›¾åƒçš„æ–¹å¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¿™ç§æ–¹å¼ç¡®ä¿äº†çœŸå®å’Œè™šå‡å›¾åƒä¹‹é—´çš„è¯­ä¹‰å¯¹é½ï¼Œä»è€Œä½¿å¾—ä»»ä½•å·®å¼‚ä»…æºäºAIç”Ÿæˆæ‰€å¼•å…¥çš„ç»†å¾®ç‰¹å¾ã€‚é€šè¿‡å†…å®¹å¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ£€æµ‹å™¨åœ¨é€šç”¨æ€§å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶ä¸”å¯¹åŒ…æ‹¬æœ€æ–°å‘å¸ƒçš„æ¨¡å‹å¦‚FLUXå’ŒStable Diffusion 3.5åœ¨å†…çš„27ç§ç”Ÿæˆæ¨¡å‹æœ‰æ›´å‡†ç¡®çš„è¯„ä¼°ç»“æœã€‚ç ”ç©¶å¼ºè°ƒäº†æ•°æ®é›†ç­›é€‰çš„é‡è¦æ€§ï¼Œå¹¶çªå‡ºäº†æ•°æ®é›†è®¾è®¡éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ³•åŒ»æ£€æµ‹å™¨åœ¨çœŸå®åº”ç”¨åœºæ™¯ä¸­è¡¨ç°ä¸è¶³ï¼Œä¸»è¦é—®é¢˜åœ¨äºè®­ç»ƒæ•°æ®è´¨é‡ã€‚</li>
<li>å¤§éƒ¨åˆ†ç ”ç©¶å…³æ³¨äºå¼€å‘æ–°ç®—æ³•ï¼Œè€Œè®­ç»ƒæ•°æ®é€‰æ‹©å´è¢«å¿½è§†ã€‚</li>
<li>æ–‡ä¸­æå‡ºB-Freeæ— åè®­ç»ƒèŒƒå¼ï¼Œé€šè¿‡ç”ŸæˆçœŸå®å›¾åƒåˆ›å»ºè™šå‡å›¾åƒæ¥ç¡®ä¿è¯­ä¹‰å¯¹é½ã€‚</li>
<li>å†…å®¹å¢å¼ºæ–¹æ³•æ˜¾è‘—æé«˜äº†æ£€æµ‹å™¨çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>B-Freeæ–¹æ³•ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯æœ‰æ›´å‡†ç¡®çš„è¯„ä¼°ç»“æœï¼Œé€‚ç”¨äºå¤šç§ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æ•°æ®é›†ç­›é€‰çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-24652003027e190b94415dab88a8ccd9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e55559a4c4a45d650616d77b886a321.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d4fc102976343418cb396ba420a9ada.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2ee32e819ab33799d664c3045d2c3ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c5ecd99a73fe51d2798982b7b1bdd18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fedc2ba55a6289cf1f6a36cfe8a87d04.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DreamFit-Garment-Centric-Human-Generation-via-a-Lightweight-Anything-Dressing-Encoder"><a href="#DreamFit-Garment-Centric-Human-Generation-via-a-Lightweight-Anything-Dressing-Encoder" class="headerlink" title="DreamFit: Garment-Centric Human Generation via a Lightweight   Anything-Dressing Encoder"></a>DreamFit: Garment-Centric Human Generation via a Lightweight   Anything-Dressing Encoder</h2><p><strong>Authors:Ente Lin, Xujie Zhang, Fuwei Zhao, Yuxuan Luo, Xin Dong, Long Zeng, Xiaodan Liang</strong></p>
<p>Diffusion models for garment-centric human generation from text or image prompts have garnered emerging attention for their great application potential. However, existing methods often face a dilemma: lightweight approaches, such as adapters, are prone to generate inconsistent textures; while finetune-based methods involve high training costs and struggle to maintain the generalization capabilities of pretrained diffusion models, limiting their performance across diverse scenarios. To address these challenges, we propose DreamFit, which incorporates a lightweight Anything-Dressing Encoder specifically tailored for the garment-centric human generation. DreamFit has three key advantages: (1) \textbf{Lightweight training}: with the proposed adaptive attention and LoRA modules, DreamFit significantly minimizes the model complexity to 83.4M trainable parameters. (2)\textbf{Anything-Dressing}: Our model generalizes surprisingly well to a wide range of (non-)garments, creative styles, and prompt instructions, consistently delivering high-quality results across diverse scenarios. (3) \textbf{Plug-and-play}: DreamFit is engineered for smooth integration with any community control plugins for diffusion models, ensuring easy compatibility and minimizing adoption barriers. To further enhance generation quality, DreamFit leverages pretrained large multi-modal models (LMMs) to enrich the prompt with fine-grained garment descriptions, thereby reducing the prompt gap between training and inference. We conduct comprehensive experiments on both $768 \times 512$ high-resolution benchmarks and in-the-wild images. DreamFit surpasses all existing methods, highlighting its state-of-the-art capabilities of garment-centric human generation. </p>
<blockquote>
<p>é’ˆå¯¹æ–‡æœ¬æˆ–å›¾åƒæç¤ºçš„ä»¥æœè£…ä¸ºä¸­å¿ƒçš„äººä½“ç”Ÿæˆï¼Œæ‰©æ•£æ¨¡å‹å› å…¶å·¨å¤§çš„åº”ç”¨æ½œåŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¸¸å¸¸é¢ä¸´ä¸€ä¸ªå›°å¢ƒï¼šè½»é‡çº§æ–¹æ³•ï¼ˆå¦‚é€‚é…å™¨ï¼‰å®¹æ˜“äº§ç”Ÿä¸ä¸€è‡´çš„çº¹ç†ï¼›è€ŒåŸºäºå¾®è°ƒçš„æ–¹æ³•æ¶‰åŠé«˜æ˜‚çš„è®­ç»ƒæˆæœ¬ï¼Œä¸”éš¾ä»¥ç»´æŒé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ï¼Œè¿™åœ¨å¤šç§åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ä¸­æœ‰æ‰€é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DreamFitã€‚å®ƒèå…¥äº†ä¸€ä¸ªä¸“ä¸ºä»¥æœè£…ä¸ºä¸­å¿ƒçš„äººä½“ç”Ÿæˆå®šåˆ¶çš„Anything-Dressing Encoderã€‚DreamFitæœ‰ä¸‰ä¸ªä¸»è¦ä¼˜åŠ¿ï¼š</p>
</blockquote>
<p>ï¼ˆ1ï¼‰<strong>è½»é‡çº§è®­ç»ƒ</strong>ï¼šé€šè¿‡æå‡ºçš„è‡ªé€‚åº”æ³¨æ„åŠ›å’ŒLoRAæ¨¡å—ï¼ŒDreamFitæå¤§åœ°ç®€åŒ–äº†æ¨¡å‹å¤æ‚åº¦ï¼Œåªæœ‰83.4Må¯è®­ç»ƒå‚æ•°ã€‚<br>ï¼ˆ2ï¼‰<strong>é€‚åº”æ€§å¼º</strong>ï¼šæˆ‘ä»¬çš„æ¨¡å‹å¯¹å„ç§ï¼ˆéï¼‰æœè£…ã€åˆ›æ„é£æ ¼å’Œæç¤ºæŒ‡ä»¤çš„é€‚åº”æ€§ä»¤äººæƒŠè®¶åœ°å¥½ï¼Œåœ¨å„ç§åœºæ™¯ä¸‹éƒ½èƒ½æä¾›é«˜è´¨é‡çš„ç»“æœã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17644v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬æå‡ºäº†é’ˆå¯¹æœè£…ä¸­å¿ƒåŒ–äººç±»ç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•â€”â€”DreamFitã€‚å®ƒç»“åˆè½»é‡åŒ–è®­ç»ƒå’ŒAnything-Dressingç¼–ç å™¨ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ã€‚DreamFitå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šè½»é‡åŒ–è®­ç»ƒã€å¹¿æ³›çš„é€‚ç”¨æ€§ã€ä»¥åŠä¸æ‰©æ•£æ¨¡å‹çš„ç¤¾åŒºæ§åˆ¶æ’ä»¶æ— ç¼é›†æˆçš„èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ¥ä¸°å¯Œæç¤ºï¼Œè¿›ä¸€æ­¥æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DreamFité‡‡ç”¨è½»é‡çº§è®­ç»ƒï¼Œé€šè¿‡è‡ªé€‚åº”æ³¨æ„åŠ›å’ŒLoRAæ¨¡å—æ˜¾è‘—å‡å°‘æ¨¡å‹å¤æ‚æ€§ã€‚</li>
<li>DreamFitå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿé€‚ç”¨äºå¹¿æ³›çš„æœè£…å’Œéæœè£…é¢†åŸŸï¼Œå…·æœ‰åˆ›æ„é£æ ¼å’Œæç¤ºæŒ‡ä»¤ã€‚</li>
<li>DreamFitè®¾è®¡ç”¨äºä¸ä»»ä½•æ‰©æ•£æ¨¡å‹çš„ç¤¾åŒºæ§åˆ¶æ’ä»¶æ— ç¼é›†æˆï¼Œç¡®ä¿å…¼å®¹æ€§å¹¶é™ä½é‡‡ç”¨é—¨æ§›ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸°å¯Œæç¤ºï¼Œç¼©å°äº†è®­ç»ƒå’Œæ¨ç†ä¹‹é—´çš„æç¤ºå·®è·ã€‚</li>
<li>DreamFitåœ¨é«˜åˆ†è¾¨ç‡åŸºå‡†å’Œé‡ç”Ÿå›¾åƒä¸Šçš„å®éªŒè¡¨ç°è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºå…¶åœ¨æœè£…ä¸­å¿ƒåŒ–äººç±»ç”Ÿæˆé¢†åŸŸçš„æœ€å…ˆè¿›çš„èƒ½åŠ›ã€‚</li>
<li>DreamFitå¯¹äºè§£å†³æ‰©æ•£æ¨¡å‹åœ¨æœè£…ä¸­å¿ƒåŒ–äººç±»ç”Ÿæˆåº”ç”¨ä¸­çš„çº¹ç†ä¸ä¸€è‡´æ€§å’Œé«˜è®­ç»ƒæˆæœ¬é—®é¢˜å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-776ac6476c2399f3189241e484292296.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0db7d66b861315be2d9d8895556b278.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b85489edabeca79565cd0cd571a8b238.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-237d8d9fea20ce8fa5ebcf60b0aaa8d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e8fb5f46271415f0808b5394e80d533.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Retention-Score-Quantifying-Jailbreak-Risks-for-Vision-Language-Models"><a href="#Retention-Score-Quantifying-Jailbreak-Risks-for-Vision-Language-Models" class="headerlink" title="Retention Score: Quantifying Jailbreak Risks for Vision Language Models"></a>Retention Score: Quantifying Jailbreak Risks for Vision Language Models</h2><p><strong>Authors:Zaitang Li, Pin-Yu Chen, Tsung-Yi Ho</strong></p>
<p>The emergence of Vision-Language Models (VLMs) is a significant advancement in integrating computer vision with Large Language Models (LLMs) to enhance multi-modal machine learning capabilities. However, this progress has also made VLMs vulnerable to sophisticated adversarial attacks, raising concerns about their reliability. The objective of this paper is to assess the resilience of VLMs against jailbreak attacks that can compromise model safety compliance and result in harmful outputs. To evaluate a VLMâ€™s ability to maintain its robustness against adversarial input perturbations, we propose a novel metric called the \textbf{Retention Score}. Retention Score is a multi-modal evaluation metric that includes Retention-I and Retention-T scores for quantifying jailbreak risks in visual and textual components of VLMs. Our process involves generating synthetic image-text pairs using a conditional diffusion model. These pairs are then predicted for toxicity score by a VLM alongside a toxicity judgment classifier. By calculating the margin in toxicity scores, we can quantify the robustness of the VLM in an attack-agnostic manner. Our work has four main contributions. First, we prove that Retention Score can serve as a certified robustness metric. Second, we demonstrate that most VLMs with visual components are less robust against jailbreak attacks than the corresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find that the security settings in Google Gemini significantly affect the score and robustness. Moreover, the robustness of GPT4V is similar to the medium settings of Gemini. Finally, our approach offers a time-efficient alternative to existing adversarial attack methods and provides consistent model robustness rankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‡ºç°æ˜¯æ•´åˆè®¡ç®—æœºè§†è§‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥å¢å¼ºå¤šæ¨¡æ€æœºå™¨å­¦ä¹ èƒ½åŠ›çš„é‡è¦è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™ä¸€è¿›å±•ä¹Ÿä½¿å¾—VLMså®¹æ˜“å—åˆ°é«˜çº§å¯¹æŠ—æ€§æ”»å‡»çš„å¨èƒï¼Œå¼•å‘å¯¹å…¶å¯é æ€§çš„æ‹…å¿§ã€‚æœ¬æ–‡çš„ç›®æ ‡æ˜¯é’ˆå¯¹å¯èƒ½å±åŠæ¨¡å‹å®‰å…¨åˆè§„å¹¶äº§ç”Ÿæœ‰å®³è¾“å‡ºçš„è¶Šç‹±æ”»å‡»ï¼Œè¯„ä¼°VLMsçš„éŸ§æ€§ã€‚ä¸ºäº†è¯„ä¼°VLMåœ¨å¯¹æŠ—æ€§è¾“å…¥æ‰°åŠ¨ä¸‹çš„ç¨³å¥æ€§ç»´æŒèƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œç•™å­˜ç‡åˆ†æ•°â€çš„æ–°æŒ‡æ ‡ã€‚ç•™å­˜ç‡åˆ†æ•°æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ç”¨äºé‡åŒ–VLMè§†è§‰å’Œæ–‡æœ¬ç»„ä»¶ä¸­çš„è¶Šç‹±é£é™©çš„ç•™å­˜-Iå’Œç•™å­˜-Tåˆ†æ•°ã€‚æˆ‘ä»¬çš„æµç¨‹åŒ…æ‹¬ä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆå›¾åƒæ–‡æœ¬å¯¹ã€‚è¿™äº›å¯¹ç„¶åç”±ä¸€ä¸ªVLMå’Œä¸€ä¸ªæ¯’æ€§åˆ¤æ–­åˆ†ç±»å™¨è¿›è¡Œæ¯’æ€§åˆ†æ•°é¢„æµ‹ã€‚é€šè¿‡è®¡ç®—æ¯’æ€§åˆ†æ•°çš„å·®å¼‚ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥ä¸€ç§ä¸å—æ”»å‡»å½±å“çš„æ–¹å¼é‡åŒ–VLMçš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸»è¦æœ‰å››ä¸ªä¸»è¦è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜äº†ç•™å­˜ç‡åˆ†æ•°å¯ä»¥ä½œä¸ºè®¤è¯çš„ç¨³å¥æ€§æŒ‡æ ‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¯æ˜å…·æœ‰è§†è§‰ç»„ä»¶çš„å¤§å¤šæ•°VLMç›¸å¯¹äºç›¸åº”çš„çº¯VLMæ›´ä¸å®¹æ˜“æŠµå¾¡è¶Šç‹±æ”»å‡»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†é»‘ç›’VLM APIï¼Œå¹¶å‘ç°Google Geminiçš„å®‰å…¨è®¾ç½®ä¼šæ˜¾è‘—å½±å“åˆ†æ•°å’Œç¨³å¥æ€§ã€‚è€Œä¸”GPT4Vçš„ç¨³å¥æ€§ä¸Geminiçš„ä¸­ç­‰è®¾ç½®ç›¸ä¼¼ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†å¯¹ç°æœ‰å¯¹æŠ—æ”»å‡»æ–¹æ³•çš„æ—¶æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶åœ¨åŒ…æ‹¬MiniGPT-4ã€InstructBLIPå’ŒLLaVAçš„VLMsä¸Šè¯„ä¼°æ—¶æä¾›äº†ä¸€è‡´çš„æ¨¡å‹ç¨³å¥æ€§æ’åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17544v1">PDF</a> 14 pages, 8 figures, AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶äº†Vision-Language Modelsï¼ˆVLMsï¼‰å¯¹æŠ—ç›‘ç‹±ç ´åæ”»å‡»ï¼ˆjailbreak attacksï¼‰çš„é²æ£’æ€§é—®é¢˜ã€‚æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡Retention Scoreæ¥é‡åŒ–VLMåœ¨è§†è§‰å’Œæ–‡æœ¬ç»„ä»¶ä¸­çš„ç›‘ç‹±ç ´åé£é™©ã€‚é€šè¿‡åˆæˆå›¾åƒ-æ–‡æœ¬å¯¹å¹¶ä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹é¢„æµ‹æ¯’æ€§åˆ†æ•°ï¼Œå†è®¡ç®—æ¯’æ€§åˆ†æ•°çš„å·®å¼‚æ¥è¯„ä¼°VLMçš„é²æ£’æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå¤§å¤šæ•°å¸¦æœ‰è§†è§‰ç»„ä»¶çš„VLMsç›¸è¾ƒäºå¯¹åº”çš„çº¯VLMsæ›´æ˜“äºå—åˆ°ç›‘ç‹±ç ´åæ”»å‡»çš„å½±å“ã€‚Google Geminiçš„å®‰å…¨è®¾ç½®ä¼šå½±å“å…¶å¾—åˆ†å’Œé²æ£’æ€§ã€‚æœ¬æ–‡æä¾›äº†ä¸€ç§æ—¶é—´æ•ˆç‡é«˜çš„æ›¿ä»£ç°æœ‰å¯¹æŠ—æ”»å‡»æ–¹æ³•çš„æ–¹å¼ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªVLMä¸Šçš„è¯„ä¼°ç»“æœå…·æœ‰ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision-Language Models (VLMs) åœ¨å¤šæ¨¡æ€æœºå™¨å­¦ä¹ é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä¹Ÿé¢ä¸´ç€é«˜çº§å¯¹æŠ—æ”»å‡»çš„é£é™©ï¼Œå¼•å‘äº†å¯¹å…¶å¯é æ€§çš„å…³æ³¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Retention Scoreè¿™ä¸€æ–°å‹è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–VLMåœ¨è§†è§‰å’Œæ–‡æœ¬ç»„ä»¶ä¸­çš„ç›‘ç‹±ç ´åé£é™©ã€‚</li>
<li>é€šè¿‡åˆæˆå›¾åƒ-æ–‡æœ¬å¯¹å¹¶ä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹é¢„æµ‹æ¯’æ€§åˆ†æ•°ï¼Œè¿›è€Œè¯„ä¼°VLMå¯¹æŠ—ç›‘ç‹±ç ´åæ”»å‡»çš„é²æ£’æ€§ã€‚</li>
<li>å¤§å¤šæ•°å¸¦æœ‰è§†è§‰ç»„ä»¶çš„VLMsç›¸è¾ƒäºçº¯VLMsæ›´æ˜“å—åˆ°æ”»å‡»ã€‚</li>
<li>Google Geminiçš„å®‰å…¨è®¾ç½®å½±å“å…¶å¾—åˆ†å’Œé²æ£’æ€§ï¼ŒGPT4Vçš„é²æ£’æ€§ä¸Geminiçš„ä¸­ç­‰è®¾ç½®ç›¸ä¼¼ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„è¯„ä¼°æ–¹æ³•æä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ›¿ä»£ç°æœ‰å¯¹æŠ—æ”»å‡»æ–¹æ³•çš„æ–¹å¼ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªVLMä¸Šçš„è¯„ä¼°ç»“æœå…·æœ‰ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-acd4aa82f23502c10c0080969883856d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80547aacd41db9dafa8abfc97625f4fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6eea7e5e3d069cda73d54b0b39356590.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Free-viewpoint-Human-Animation-with-Pose-correlated-Reference-Selection"><a href="#Free-viewpoint-Human-Animation-with-Pose-correlated-Reference-Selection" class="headerlink" title="Free-viewpoint Human Animation with Pose-correlated Reference Selection"></a>Free-viewpoint Human Animation with Pose-correlated Reference Selection</h2><p><strong>Authors:Fa-Ting Hong, Zhan Xu, Haiyang Liu, Qinjie Lin, Luchuan Song, Zhixin Shu, Yang Zhou, Duygu Ceylan, Dan Xu</strong></p>
<p>Diffusion-based human animation aims to animate a human character based on a source human image as well as driving signals such as a sequence of poses. Leveraging the generative capacity of diffusion model, existing approaches are able to generate high-fidelity poses, but struggle with significant viewpoint changes, especially in zoom-in&#x2F;zoom-out scenarios where camera-character distance varies. This limits the applications such as cinematic shot type plan or camera control. We propose a pose-correlated reference selection diffusion network, supporting substantial viewpoint variations in human animation. Our key idea is to enable the network to utilize multiple reference images as input, since significant viewpoint changes often lead to missing appearance details on the human body. To eliminate the computational cost, we first introduce a novel pose correlation module to compute similarities between non-aligned target and source poses, and then propose an adaptive reference selection strategy, utilizing the attention map to identify key regions for animation generation. To train our model, we curated a large dataset from public TED talks featuring varied shots of the same character, helping the model learn synthesis for different perspectives. Our experimental results show that with the same number of reference images, our model performs favorably compared to the current SOTA methods under large viewpoint change. We further show that the adaptive reference selection is able to choose the most relevant reference regions to generate humans under free viewpoints. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„äººç±»åŠ¨ç”»æ—¨åœ¨æ ¹æ®æºäººç‰©å›¾åƒä»¥åŠé©±åŠ¨ä¿¡å·ï¼ˆå¦‚ä¸€ç³»åˆ—å§¿åŠ¿ï¼‰æ¥é©±åŠ¨ä¸€ä¸ªäººç‰©è§’è‰²ã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç°æœ‰æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸåº¦çš„å§¿åŠ¿ï¼Œä½†åœ¨è§†ç‚¹å˜åŒ–è¾ƒå¤§æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼©æ”¾åœºæ™¯ï¼ˆå¦‚æ‘„åƒæœºä¸è§’è‰²çš„è·ç¦»å˜åŒ–ï¼‰ä¸­å°¤ä¸ºå¦‚æ­¤ã€‚è¿™é™åˆ¶äº†å…¶åœ¨ç”µå½±æ‹æ‘„è®¡åˆ’æˆ–ç›¸æœºæ§åˆ¶ç­‰æ–¹é¢çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å§¿æ€ç›¸å…³å‚è€ƒé€‰æ‹©æ‰©æ•£ç½‘ç»œï¼Œæ”¯æŒäººç±»åŠ¨ç”»ä¸­çš„å¤§å¹…è§†ç‚¹å˜åŒ–ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯è®©ç½‘ç»œèƒ½å¤Ÿä½¿ç”¨å¤šä¸ªå‚è€ƒå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå› ä¸ºè§†ç‚¹çš„å¤§å¹…å˜åŒ–é€šå¸¸ä¼šå¯¼è‡´äººä½“å¤–è§‚ç»†èŠ‚ç¼ºå¤±ã€‚ä¸ºäº†é™ä½è®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å§¿æ€ç›¸å…³æ€§æ¨¡å—ï¼Œç”¨äºè®¡ç®—æœªå¯¹é½çš„ç›®æ ‡å’Œæºå§¿æ€ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œç„¶åæå‡ºäº†ä¸€ç§è‡ªé€‚åº”å‚è€ƒé€‰æ‹©ç­–ç•¥ï¼Œåˆ©ç”¨æ³¨æ„åŠ›å›¾æ¥è¯†åˆ«åŠ¨ç”»ç”Ÿæˆçš„å…³é”®åŒºåŸŸã€‚ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬ä»å…¬å…±çš„TEDæ¼”è®²ä¸­æ•´ç†äº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«åŒä¸€è§’è‰²çš„ä¸åŒè§†è§’çš„ç‰‡æ®µï¼Œå¸®åŠ©æ¨¡å‹å­¦ä¹ ä¸åŒè§†è§’çš„åˆæˆã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒæ•°é‡çš„å‚è€ƒå›¾åƒä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤§è§†ç‚¹å˜åŒ–çš„æƒ…å†µä¸‹ä¸å½“å‰æœ€ä½³æ–¹æ³•ç›¸æ¯”è¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œè‡ªé€‚åº”å‚è€ƒé€‰æ‹©èƒ½å¤Ÿé€‰æ‹©æœ€ç›¸å…³çš„å‚è€ƒåŒºåŸŸæ¥åœ¨è‡ªç”±è§†è§’ä¸‹ç”Ÿæˆäººç‰©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17290v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„äººä½“åŠ¨ç”»æŠ€æœ¯é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿæ ¹æ®æºäººä½“å›¾åƒå’Œä¸€ç³»åˆ—å§¿æ€é©±åŠ¨ä¿¡å·ç”Ÿæˆé«˜ä¿çœŸåº¦çš„å§¿æ€ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨åº”å¯¹è§†è§’å˜åŒ–è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œç‰¹åˆ«æ˜¯åœ¨é•œå¤´æ¨æ‹‰ï¼ˆzoom-in&#x2F;zoom-outï¼‰åœºæ™¯ä¸­ï¼Œç”±äºæ‘„åƒæœºä¸è§’è‰²çš„è·ç¦»å˜åŒ–ï¼Œä¼šå‡ºç°æ€§èƒ½æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å§¿æ€ç›¸å…³çš„å‚è€ƒé€‰æ‹©æ‰©æ•£ç½‘ç»œï¼ˆPose-correlated Reference Selection Diffusion Networkï¼‰ï¼Œä»¥æ”¯æŒäººä½“åŠ¨ç”»ä¸­çš„å¤§å¹…åº¦è§†è§’å˜åŒ–ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç½‘ç»œèƒ½å¤Ÿä½¿ç”¨å¤šä¸ªå‚è€ƒå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå› ä¸ºå¤§å¹…åº¦çš„è§†è§’å˜åŒ–é€šå¸¸ä¼šå¯¼è‡´äººä½“å¤–è§‚ç»†èŠ‚ç¼ºå¤±ã€‚é€šè¿‡å¼•å…¥æ–°çš„å§¿æ€ç›¸å…³æ€§æ¨¡å—å’Œè‡ªé€‚åº”å‚è€ƒé€‰æ‹©ç­–ç•¥ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°å¤„ç†ä¸åŒè§†è§’çš„äººä½“åŠ¨ç”»ç”Ÿæˆé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒæ•°é‡çš„å‚è€ƒå›¾åƒä¸‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤§è§†è§’å˜åŒ–ä¸‹çš„æ€§èƒ½ä¼˜äºå½“å‰æœ€ä½³æ–¹æ³•ã€‚è‡ªé€‚åº”å‚è€ƒé€‰æ‹©ç­–ç•¥èƒ½å¤Ÿé€‰æ‹©æœ€ç›¸å…³çš„å‚è€ƒåŒºåŸŸï¼Œä»¥åœ¨è‡ªç”±è§†è§’ä¸‹ç”Ÿæˆäººä½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºåŸºäºæºå›¾åƒå’Œé©±åŠ¨ä¿¡å·çš„äººä½“åŠ¨ç”»åˆ¶ä½œï¼Œèƒ½ç”Ÿæˆé«˜ä¿çœŸåº¦çš„å§¿æ€ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è§†è§’å˜åŒ–æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é•œå¤´æ¨æ‹‰åœºæ™¯ä¸­ã€‚</li>
<li>æå‡ºçš„å§¿æ€ç›¸å…³çš„å‚è€ƒé€‰æ‹©æ‰©æ•£ç½‘ç»œèƒ½å¤Ÿæ”¯æŒå¤§å¹…åº¦è§†è§’å˜åŒ–çš„äººä½“åŠ¨ç”»ã€‚</li>
<li>ç½‘ç»œä½¿ç”¨å¤šä¸ªå‚è€ƒå›¾åƒä½œä¸ºè¾“å…¥ï¼Œä»¥å¼¥è¡¥è§†è§’å˜åŒ–å¯¼è‡´çš„å¤–è§‚ç»†èŠ‚ç¼ºå¤±ã€‚</li>
<li>é€šè¿‡å¼•å…¥å§¿æ€ç›¸å…³æ€§æ¨¡å—å’Œè‡ªé€‚åº”å‚è€ƒé€‰æ‹©ç­–ç•¥ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆå¤„ç†ä¸åŒè§†è§’çš„äººä½“åŠ¨ç”»ç”Ÿæˆé—®é¢˜ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤§è§†è§’å˜åŒ–ä¸‹çš„æ€§èƒ½ä¼˜äºå½“å‰æœ€ä½³æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d25e7e843628651c091b4b95d57678e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad09c98731f10a935f16217a2bba3b18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91204f2d79b9ea7574b7a6392d369f11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6248515f46fc609214fda8d912a6e7fc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Conditional-Diffusion-Model-for-Electrical-Impedance-Tomography-Image-Reconstruction"><a href="#A-Conditional-Diffusion-Model-for-Electrical-Impedance-Tomography-Image-Reconstruction" class="headerlink" title="A Conditional Diffusion Model for Electrical Impedance Tomography Image   Reconstruction"></a>A Conditional Diffusion Model for Electrical Impedance Tomography Image   Reconstruction</h2><p><strong>Authors:Shuaikai Shi, Ruiyuan Kang, Panos Liatsis</strong></p>
<p>Electrical impedance tomography (EIT) is a non-invasive imaging technique, capable of reconstructing images of the electrical conductivity of tissues and materials. It is popular in diverse application areas, from medical imaging to industrial process monitoring and tactile sensing, due to its low cost, real-time capabilities and non-ionizing nature. EIT visualizes the conductivity distribution within a body by measuring the boundary voltages, given a current injection. However, EIT image reconstruction is ill-posed due to the mismatch between the under-sampled voltage data and the high-resolution conductivity image. A variety of approaches, both conventional and deep learning-based, have been proposed, capitalizing on the use of spatial regularizers, and the paradigm of image regression. In this research, a novel method based on the conditional diffusion model for EIT reconstruction is proposed, termed CDEIT. Specifically, CDEIT consists of the forward diffusion process, which first gradually adds Gaussian noise to the clean conductivity images, and a reverse denoising process, which learns to predict the original conductivity image from its noisy version, conditioned on the boundary voltages. Following model training, CDEIT applies the conditional reverse process on test voltage data to generate the desired conductivities. Moreover, we provide the details of a normalization procedure, which demonstrates how EIT image reconstruction models trained on simulated datasets can be applied on real datasets with varying sizes, excitation currents and background conductivities. Experiments conducted on a synthetic dataset and two real datasets demonstrate that the proposed model outperforms state-of-the-art methods. The CDEIT software is available as open-source (<a target="_blank" rel="noopener" href="https://github.com/shuaikaishi/CDEIT">https://github.com/shuaikaishi/CDEIT</a>) for reproducibility purposes. </p>
<blockquote>
<p>ç”µé˜»æŠ—æˆåƒï¼ˆEITï¼‰æ˜¯ä¸€ç§éä¾µå…¥æ€§çš„æˆåƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿé‡å»ºç»„ç»‡å’Œææ–™çš„ç”µå¯¼ç‡å›¾åƒã€‚ç”±äºå…¶æˆæœ¬ä½ã€å®æ—¶æ€§èƒ½å¼ºå’Œéç”µç¦»æ€§è´¨ï¼ŒEITåœ¨åŒ»å­¦æˆåƒã€å·¥ä¸šè¿‡ç¨‹ç›‘æ§å’Œè§¦è§‰ä¼ æ„Ÿç­‰å„ä¸ªé¢†åŸŸå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚EITé€šè¿‡æµ‹é‡è¾¹ç•Œç”µå‹æ¥å¯è§†åŒ–ä½“å†…çš„ç”µå¯¼ç‡åˆ†å¸ƒï¼Œç»™å®šç”µæµæ³¨å…¥ã€‚ç„¶è€Œï¼Œç”±äºæ¬ é‡‡æ ·ç”µå‹æ•°æ®ä¸é«˜åˆ†è¾¨ç‡ç”µå¯¼ç‡å›¾åƒä¹‹é—´çš„ä¸åŒ¹é…ï¼ŒEITå›¾åƒé‡å»ºæ˜¯ä¸€ä¸ªä¸é€‚å®šé—®é¢˜ã€‚å·²ç»æå‡ºäº†è®¸å¤šä¼ ç»Ÿå’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç©ºé—´æ­£åˆ™åŒ–å™¨å’Œå›¾åƒå›å½’èŒƒå¼ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ç”¨äºEITé‡å»ºï¼Œç§°ä¸ºCDEITã€‚å…·ä½“æ¥è¯´ï¼ŒCDEITåŒ…æ‹¬æ­£å‘æ‰©æ•£è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹é¦–å…ˆåœ¨æ¸…æ´ç”µå¯¼ç‡å›¾åƒä¸Šé€æ¸æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œä»¥åŠåå‘å»å™ªè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹å­¦ä¼šæ ¹æ®è¾¹ç•Œç”µå‹é¢„æµ‹åŸå§‹ç”µå¯¼ç‡å›¾åƒã€‚æ¨¡å‹è®­ç»ƒå®Œæˆåï¼ŒCDEITå¯¹æµ‹è¯•ç”µå‹æ•°æ®åº”ç”¨æ¡ä»¶åå‘è¿‡ç¨‹ä»¥ç”Ÿæˆæ‰€éœ€çš„ç”µå¯¼ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†å½’ä¸€åŒ–è¿‡ç¨‹çš„ç»†èŠ‚ï¼Œè¯¥è¿‡ç¨‹å±•ç¤ºäº†å¦‚ä½•åœ¨ä¸åŒå¤§å°ã€æ¿€åŠ±ç”µæµå’ŒèƒŒæ™¯ç”µå¯¼ç‡çš„çœŸå®æ•°æ®é›†ä¸Šåº”ç”¨ç»è¿‡æ¨¡æ‹Ÿæ•°æ®é›†è®­ç»ƒçš„EITå›¾åƒé‡å»ºæ¨¡å‹ã€‚åœ¨åˆæˆæ•°æ®é›†å’Œä¸¤ä¸ªçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚CDEITè½¯ä»¶ä½œä¸ºå¼€æºè½¯ä»¶å¯ä¾›ä½¿ç”¨ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/shuaikaishi/CDEIT%EF%BC%89%EF%BC%8C%E4%BB%A5%E4%BE%BF%E4%BA%8E%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E5%AE%9E%E9%AA%8C%E3%80%82">https://github.com/shuaikaishi/CDEITï¼‰ï¼Œä»¥ä¾¿äºå¯é‡å¤æ€§å®éªŒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16979v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç”µé˜»å±‚ææˆåƒæŠ€æœ¯ï¼ˆEITï¼‰æ˜¯ä¸€ç§éä¾µå…¥å¼çš„æˆåƒæŠ€æœ¯ï¼Œèƒ½é€šè¿‡æµ‹é‡è¾¹ç•Œç”µå‹é‡å»ºç»„ç»‡å’Œææ–™çš„ç”µå¯¼ç‡å›¾åƒã€‚è¯¥æŠ€æœ¯åœ¨åŒ»ç–—æˆåƒã€å·¥ä¸šè¿‡ç¨‹ç›‘æ§å’Œè§¦è§‰æ„Ÿåº”ç­‰å¤šä¸ªé¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„EITé‡å»ºæ–°æ–¹æ³•ï¼Œç§°ä¸ºCDEITã€‚è¯¥æ–¹æ³•é€šè¿‡æ­£å‘æ‰©æ•£è¿‡ç¨‹é€æ­¥å‘æ¸…æ´ç”µå¯¼ç‡å›¾åƒæ·»åŠ é«˜æ–¯å™ªå£°ï¼Œå¹¶é€šè¿‡åå‘å»å™ªè¿‡ç¨‹å­¦ä¹ ä»å«å™ªå£°å›¾åƒé¢„æµ‹åŸå§‹ç”µå¯¼ç‡å›¾åƒã€‚ç»è¿‡æ¨¡å‹è®­ç»ƒåï¼ŒCDEITå¯åº”ç”¨äºæµ‹è¯•ç”µå‹æ•°æ®ç”Ÿæˆæ‰€éœ€çš„ç”µå¯¼ç‡ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æä¾›äº†ä¸€ç§å½’ä¸€åŒ–ç¨‹åºï¼Œè¯¥ç¨‹åºå±•ç¤ºäº†å¦‚ä½•å°†åœ¨æ¨¡æ‹Ÿæ•°æ®é›†ä¸Šè®­ç»ƒçš„EITå›¾åƒé‡å»ºæ¨¡å‹åº”ç”¨äºå…·æœ‰ä¸åŒå¤§å°ã€æ¿€å‘ç”µæµå’ŒèƒŒæ™¯ç”µå¯¼ç‡çš„çœŸå®æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒCDEITè½¯ä»¶å·²ä½œä¸ºå¼€æºè½¯ä»¶ä¾›ä¸‹è½½ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EITæ˜¯ä¸€ç§éä¾µå…¥å¼çš„æˆåƒæŠ€æœ¯ï¼Œé€šè¿‡æµ‹é‡è¾¹ç•Œç”µå‹é‡å»ºç”µå¯¼ç‡å›¾åƒã€‚</li>
<li>EITåœ¨åŒ»ç–—æˆåƒã€å·¥ä¸šè¿‡ç¨‹ç›‘æ§å’Œè§¦è§‰æ„Ÿåº”ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„EITé‡å»ºæ–°æ–¹æ³•CDEITã€‚</li>
<li>CDEITåŒ…å«æ­£å‘æ‰©æ•£è¿‡ç¨‹å’Œåå‘å»å™ªè¿‡ç¨‹ï¼Œé€šè¿‡é¢„æµ‹åŸå§‹ç”µå¯¼ç‡å›¾åƒä»å«å™ªå£°å›¾åƒä¸­å­¦ä¹ ã€‚</li>
<li>CDEITæ¨¡å‹åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>CDEITè½¯ä»¶å·²ä½œä¸ºå¼€æºè½¯ä»¶ä¾›ä¸‹è½½ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11ceec1b251346bed91afa1a68ac78f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-428480e4c51a239db0299f8e9b11c00c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a321e7a916d483b8e39c0317027ccfdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-630caf5b4bae6c467f788ff7ce9775d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a58b6e689deed22ade977cfb4070cd2a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PromptDresser-Improving-the-Quality-and-Controllability-of-Virtual-Try-On-via-Generative-Textual-Prompt-and-Prompt-aware-Mask"><a href="#PromptDresser-Improving-the-Quality-and-Controllability-of-Virtual-Try-On-via-Generative-Textual-Prompt-and-Prompt-aware-Mask" class="headerlink" title="PromptDresser: Improving the Quality and Controllability of Virtual   Try-On via Generative Textual Prompt and Prompt-aware Mask"></a>PromptDresser: Improving the Quality and Controllability of Virtual   Try-On via Generative Textual Prompt and Prompt-aware Mask</h2><p><strong>Authors:Jeongho Kim, Hoiyeong Jin, Sunghyun Park, Jaegul Choo</strong></p>
<p>Recent virtual try-on approaches have advanced by fine-tuning the pre-trained text-to-image diffusion models to leverage their powerful generative ability. However, the use of text prompts in virtual try-on is still underexplored. This paper tackles a text-editable virtual try-on task that changes the clothing item based on the provided clothing image while editing the wearing style (e.g., tucking style, fit) according to the text descriptions. In the text-editable virtual try-on, three key aspects exist: (i) designing rich text descriptions for paired person-clothing data to train the model, (ii) addressing the conflicts where textual information of the existing personâ€™s clothing interferes the generation of the new clothing, and (iii) adaptively adjust the inpainting mask aligned with the text descriptions, ensuring proper editing areas while preserving the original personâ€™s appearance irrelevant to the new clothing. To address these aspects, we propose PromptDresser, a text-editable virtual try-on model that leverages large multimodal model (LMM) assistance to enable high-quality and versatile manipulation based on generative text prompts. Our approach utilizes LMMs via in-context learning to generate detailed text descriptions for person and clothing images independently, including pose details and editing attributes using minimal human cost. Moreover, to ensure the editing areas, we adjust the inpainting mask depending on the text prompts adaptively. We found that our approach, utilizing detailed text prompts, not only enhances text editability but also effectively conveys clothing details that are difficult to capture through images alone, thereby enhancing image quality. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/rlawjdghek/PromptDresser">https://github.com/rlawjdghek/PromptDresser</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè™šæ‹Ÿè¯•ç©¿æŠ€æœ¯é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥åˆ©ç”¨å…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œè™šæ‹Ÿè¯•ç©¿ä¸­ä½¿ç”¨æ–‡æœ¬æç¤ºä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚æœ¬æ–‡è§£å†³äº†ä¸€ä¸ªå¯æ–‡æœ¬ç¼–è¾‘çš„è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ ¹æ®æä¾›çš„æœè£…å›¾åƒæ”¹å˜æœè£…é¡¹ç›®ï¼ŒåŒæ—¶æ ¹æ®æ–‡æœ¬æè¿°ç¼–è¾‘ç©¿ç€é£æ ¼ï¼ˆä¾‹å¦‚ï¼Œé¢†å£é£æ ¼ã€åˆèº«åº¦ï¼‰ã€‚åœ¨å¯æ–‡æœ¬ç¼–è¾‘çš„è™šæ‹Ÿè¯•ç©¿ä¸­ï¼Œå­˜åœ¨ä¸‰ä¸ªå…³é”®æ–¹é¢ï¼šï¼ˆiï¼‰ä¸ºé…å¯¹çš„äºº-æœè£…æ•°æ®è®¾è®¡ä¸°å¯Œçš„æ–‡æœ¬æè¿°ä»¥è®­ç»ƒæ¨¡å‹ï¼Œï¼ˆiiï¼‰è§£å†³ç°æœ‰æœè£…æ–‡æœ¬ä¿¡æ¯çš„å†²çªï¼Œè¿™äº›å†²çªä¼šå¹²æ‰°æ–°æœè£…çš„ç”Ÿæˆï¼Œï¼ˆiiiï¼‰è‡ªé€‚åº”è°ƒæ•´ä¸æ–‡æœ¬æè¿°å¯¹é½çš„ä¿®å¤æ©ç ï¼Œç¡®ä¿é€‚å½“çš„ç¼–è¾‘åŒºåŸŸåŒæ—¶ä¿ç•™ä¸æ–°æœè£…æ— å…³çš„åŸäººç‰©å¤–è§‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†PromptDresserï¼Œä¸€ä¸ªå¯æ–‡æœ¬ç¼–è¾‘çš„è™šæ‹Ÿè¯•ç©¿æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„è¾…åŠ©åŠŸèƒ½ï¼Œå®ç°åŸºäºç”Ÿæˆæ–‡æœ¬æç¤ºçš„é«˜è´¨é‡ã€å¤šåŠŸèƒ½æ“ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œä¸ºäººç‰©å’Œæœè£…å›¾åƒç‹¬ç«‹ç”Ÿæˆè¯¦ç»†çš„æ–‡æœ¬æè¿°ï¼ŒåŒ…æ‹¬å§¿åŠ¿ç»†èŠ‚å’Œç¼–è¾‘å±æ€§ï¼Œè€Œæ— éœ€å¤§é‡çš„äººåŠ›æˆæœ¬ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿ç¼–è¾‘åŒºåŸŸï¼Œæˆ‘ä»¬æ ¹æ®æ–‡æœ¬æç¤ºè‡ªé€‚åº”åœ°è°ƒæ•´ä¿®å¤æ©ç ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨è¯¦ç»†çš„æ–‡æœ¬æç¤ºçš„æ–¹æ³•ä¸ä»…æé«˜äº†æ–‡æœ¬çš„å¯ç¼–è¾‘æ€§ï¼Œè€Œä¸”æœ‰æ•ˆåœ°ä¼ è¾¾äº†å•å‡­å›¾åƒéš¾ä»¥æ•æ‰çš„æœè£…ç»†èŠ‚ï¼Œä»è€Œæé«˜äº†å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/rlawjdghek/PromptDresser%E5%A4%96%E5%A4%84%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/rlawjdghek/PromptDresserå¤„è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16978v1">PDF</a> 20 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸè™šæ‹Ÿè¯•ç©¿æŠ€æœ¯çš„å‘å±•ï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¾®è°ƒï¼Œå……åˆ†å‘æŒ¥äº†å…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œè™šæ‹Ÿè¯•ç©¿ä¸­ä½¿ç”¨æ–‡æœ¬æç¤ºä»è¢«å¿½è§†ã€‚æœ¬æ–‡è§£å†³äº†ä¸€ä¸ªå¯æ–‡æœ¬ç¼–è¾‘çš„è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ï¼Œæ ¹æ®æä¾›çš„æœè£…å›¾åƒæ”¹å˜æœè£…é¡¹ç›®ï¼Œå¹¶æ ¹æ®æ–‡æœ¬æè¿°ç¼–è¾‘ç©¿ç€é£æ ¼ï¼ˆä¾‹å¦‚ï¼Œé¢†å£é£æ ¼ã€è´´åˆåº¦ç­‰ï¼‰ã€‚åœ¨å¯æ–‡æœ¬ç¼–è¾‘çš„è™šæ‹Ÿè¯•ç©¿ä¸­ï¼Œå­˜åœ¨ä¸‰ä¸ªå…³é”®æ–¹é¢ï¼šï¼ˆiï¼‰ä¸ºé…å¯¹çš„äºº-æœè£…æ•°æ®è®¾è®¡ä¸°å¯Œçš„æ–‡æœ¬æè¿°ä»¥è®­ç»ƒæ¨¡å‹ï¼Œï¼ˆiiï¼‰è§£å†³ç°æœ‰æœè£…æ–‡æœ¬ä¿¡æ¯çš„å†²çªï¼Œå¹²æ‰°æ–°æœè£…çš„ç”Ÿæˆï¼Œï¼ˆiiiï¼‰è‡ªé€‚åº”è°ƒæ•´ä¸æ–‡æœ¬æè¿°å¯¹é½çš„å¡«å……æ©ç ï¼Œç¡®ä¿é€‚å½“çš„ç¼–è¾‘åŒºåŸŸåŒæ—¶ä¿ç•™ä¸æ–°æœè£…æ— å…³çš„åŸå¤–è§‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PromptDresserâ€”â€”ä¸€ä¸ªå¯æ–‡æœ¬ç¼–è¾‘çš„è™šæ‹Ÿè¯•ç©¿æ¨¡å‹ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„è¾…åŠ©ï¼Œå®ç°åŸºäºç”Ÿæˆæ–‡æœ¬æç¤ºçš„é«˜è´¨é‡ã€å¤šåŠŸèƒ½æ“ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ åˆ©ç”¨LMMsç”Ÿæˆäººç‰©å’Œæœè£…å›¾åƒçš„è¯¦ç»†æ–‡æœ¬æè¿°ï¼ŒåŒ…æ‹¬å§¿åŠ¿ç»†èŠ‚å’Œç¼–è¾‘å±æ€§ï¼Œå‡ ä¹æ— éœ€äººå·¥å¹²é¢„ã€‚æ­¤å¤–ï¼Œä¸ºç¡®ä¿ç¼–è¾‘åŒºåŸŸï¼Œæˆ‘ä»¬æ ¹æ®æ–‡æœ¬æç¤ºè‡ªé€‚åº”è°ƒæ•´å¡«å……æ©ç ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨è¯¦ç»†çš„æ–‡æœ¬æç¤ºä¸ä»…æé«˜äº†æ–‡æœ¬çš„å¯ç¼–è¾‘æ€§ï¼Œè¿˜æœ‰æ•ˆåœ°ä¼ è¾¾äº†å›¾åƒéš¾ä»¥æ•æ‰çš„æœè£…ç»†èŠ‚ï¼Œä»è€Œæé«˜äº†å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/rlawjdghek/PromptDresser">https://github.com/rlawjdghek/PromptDresser</a>è·å–ã€‚</p>
<p><strong>è¦ç‚¹æ€»ç»“</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºPromptDresserçš„æ–‡æœ¬å¯ç¼–è¾‘è™šæ‹Ÿè¯•ç©¿æ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶ç»“åˆå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ç”Ÿæˆè¯¦ç»†çš„æ–‡æœ¬æè¿°ï¼ŒåŒ…æ‹¬å§¿åŠ¿ç»†èŠ‚å’Œç¼–è¾‘å±æ€§ï¼Œä¸”å‡ ä¹æ— éœ€äººå·¥å¹²é¢„ã€‚</li>
<li>è§£å†³åœ¨è™šæ‹Ÿè¯•ç©¿ä¸­ä½¿ç”¨æ–‡æœ¬æç¤ºçš„å…³é”®é—®é¢˜ï¼Œå¦‚è®¾è®¡ä¸°å¯Œçš„æ–‡æœ¬æè¿°ã€è§£å†³æ–‡æœ¬ä¿¡æ¯å†²çªå’Œè‡ªé€‚åº”è°ƒæ•´å¡«å……æ©ç ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨è¯¦ç»†çš„æ–‡æœ¬æç¤ºï¼Œä¸ä»…æé«˜äº†æ–‡æœ¬çš„å¯ç¼–è¾‘æ€§ï¼Œè¿˜æé«˜äº†å›¾åƒè´¨é‡ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æä¾›çš„æœè£…å›¾åƒæ”¹å˜æœè£…é¡¹ç›®å¹¶ç¼–è¾‘ç©¿ç€é£æ ¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9ea89de4beda3eb65ac863d395dafe2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3385bb6e5e498a09b7a378a3becee66b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e733e85a79de1c78a49e25279f1068c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c573fdbb8d86d840885797de81fa6798.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FADA-Fast-Diffusion-Avatar-Synthesis-with-Mixed-Supervised-Multi-CFG-Distillation"><a href="#FADA-Fast-Diffusion-Avatar-Synthesis-with-Mixed-Supervised-Multi-CFG-Distillation" class="headerlink" title="FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG   Distillation"></a>FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG   Distillation</h2><p><strong>Authors:Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi Yang, Zhou Zhao</strong></p>
<p>Diffusion-based audio-driven talking avatar methods have recently gained attention for their high-fidelity, vivid, and expressive results. However, their slow inference speed limits practical applications. Despite the development of various distillation techniques for diffusion models, we found that naive diffusion distillation methods do not yield satisfactory results. Distilled models exhibit reduced robustness with open-set input images and a decreased correlation between audio and video compared to teacher models, undermining the advantages of diffusion models. To address this, we propose FADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation). We first designed a mixed-supervised loss to leverage data of varying quality and enhance the overall model capability as well as robustness. Additionally, we propose a multi-CFG distillation with learnable tokens to utilize the correlation between audio and reference image conditions, reducing the threefold inference runs caused by multi-CFG with acceptable quality degradation. Extensive experiments across multiple datasets show that FADA generates vivid videos comparable to recent diffusion model-based methods while achieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage <a target="_blank" rel="noopener" href="http://fadavatar.github.io/">http://fadavatar.github.io</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¶æ–¹æ³•å› å…¶é«˜ä¿çœŸã€ç”ŸåŠ¨ã€è¡¨è¾¾ä¸°å¯Œçš„ç»“æœè€Œè¿‘æœŸå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œå…¶ç¼“æ…¢çš„æ¨ç†é€Ÿåº¦é™åˆ¶äº†å®é™…åº”ç”¨ã€‚å°½ç®¡ä¸ºæ‰©æ•£æ¨¡å‹å¼€å‘äº†å„ç§è’¸é¦æŠ€æœ¯ï¼Œä½†æˆ‘ä»¬å‘ç°å¤©çœŸçš„æ‰©æ•£è’¸é¦æ–¹æ³•å¹¶æ²¡æœ‰äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœã€‚ä¸æ•™å¸ˆæ¨¡å‹ç›¸æ¯”ï¼Œè’¸é¦æ¨¡å‹å¯¹å¼€æ”¾é›†è¾“å…¥å›¾åƒçš„ç¨³å¥æ€§é™ä½ï¼ŒéŸ³é¢‘å’Œè§†é¢‘çš„å…³è”æ€§å‡å¼±ï¼Œè¿™å‰Šå¼±äº†æ‰©æ•£æ¨¡å‹çš„ä¼˜åŠ¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FADAï¼ˆæ··åˆç›‘ç£å¤šCFGè’¸é¦çš„å¿«é€Ÿæ‰©æ•£äººå¶åˆæˆï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§æ··åˆç›‘ç£æŸå¤±ï¼Œä»¥åˆ©ç”¨ä¸åŒè´¨é‡çš„æ•°æ®ï¼Œå¢å¼ºæ¨¡å‹çš„æ€»ä½“èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šCFGè’¸é¦ä¸å¯å­¦ä¹ ä»¤ç‰Œçš„æ–¹æ³•ï¼Œåˆ©ç”¨éŸ³é¢‘å’Œå‚è€ƒå›¾åƒæ¡ä»¶ä¹‹é—´çš„å…³è”æ€§ï¼Œé€šè¿‡å‡å°‘å¤šCFGå¼•èµ·çš„ä¸‰å€æ¨ç†è¿è¡Œï¼Œå®ç°å¯æ¥å—çš„è´¨é‡ä¸‹é™ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFADAç”Ÿæˆçš„è§†é¢‘ç”ŸåŠ¨ï¼Œä¸æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶å®ç°äº†4.17-12.5å€çš„NFEåŠ é€Ÿã€‚æ¼”ç¤ºè¯·è®¿é—®æˆ‘ä»¬çš„ç½‘é¡µ<a target="_blank" rel="noopener" href="http://fadavatar.github.io./">http://fadavatar.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16915v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹é©±åŠ¨éŸ³é¢‘çš„èŠå¤©äººå¶æŠ€æœ¯å› ç”Ÿæˆé«˜è´¨é‡ã€ç”ŸåŠ¨å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„ç»“æœè€Œå—åˆ°å…³æ³¨ï¼Œä½†å…¶ç¼“æ…¢çš„æ¨ç†é€Ÿåº¦é™åˆ¶äº†å®é™…åº”ç”¨ã€‚ä¸ºè§£å†³ç°æœ‰è’¸é¦æŠ€æœ¯å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚é™ä½æ¨¡å‹çš„ç¨³å¥æ€§å’ŒéŸ³é¢‘ä¸è§†é¢‘çš„å…³è”æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FADAï¼ˆå¸¦æœ‰æ··åˆç›‘ç£å¤šCFGè’¸é¦çš„å¿«é€Ÿæ‰©æ•£äººå¶åˆæˆï¼‰ã€‚æˆ‘ä»¬é€šè¿‡è®¾è®¡æ··åˆç›‘ç£æŸå¤±æ¥æé«˜æ¨¡å‹å¯¹ä¸åŒè´¨é‡æ•°æ®çš„å¤„ç†èƒ½åŠ›ï¼ŒåŒæ—¶æé«˜å…¶æ•´ä½“æ€§èƒ½å’Œç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¸¦æœ‰å¯å­¦ä¹ æ ‡è®°çš„å¤šCFGè’¸é¦æ–¹æ³•ï¼Œåˆ©ç”¨éŸ³é¢‘å’Œå‚è€ƒå›¾åƒæ¡ä»¶ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå‡å°‘å› å¤šCFGå¯¼è‡´çš„ä¸‰å€æ¨ç†è¿è¡Œæ¬¡æ•°ï¼ŒåŒæ—¶ä¿è¯å¯æ¥å—çš„è´¨é‡æŸå¤±ã€‚å®éªŒè¯æ˜ï¼ŒFADAåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šç”Ÿæˆçš„è§†é¢‘ç”ŸåŠ¨ä¸”é€¼çœŸï¼Œä¸åŸºäºæ‰©æ•£æ¨¡å‹çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶å®ç°äº†4.17è‡³12.5å€çš„NFEåŠ é€Ÿã€‚ç›¸å…³æ¼”ç¤ºè§†é¢‘å·²ä¸Šä¼ è‡³æˆ‘ä»¬çš„ç½‘é¡µï¼š<a target="_blank" rel="noopener" href="http://fadavatar.github.io./">http://fadavatar.github.ioã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹é©±åŠ¨éŸ³é¢‘çš„èŠå¤©äººå¶æŠ€æœ¯å—åˆ°å…³æ³¨ï¼Œä½†æ¨ç†é€Ÿåº¦æ…¢é™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li>
<li>ç°æœ‰è’¸é¦æŠ€æœ¯åœ¨æ‰©æ•£æ¨¡å‹ä¸­å­˜åœ¨é—®é¢˜ï¼Œå½±å“æ¨¡å‹çš„ç¨³å¥æ€§å’ŒéŸ³é¢‘ä¸è§†é¢‘çš„å…³è”æ€§ã€‚</li>
<li>æå‡ºFADAæ–¹æ³•ï¼Œé€šè¿‡æ··åˆç›‘ç£æŸå¤±æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¢å¼ºç¨³å¥æ€§ã€‚</li>
<li>FADAåˆ©ç”¨éŸ³é¢‘å’Œå‚è€ƒå›¾åƒæ¡ä»¶ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå‡å°‘å¤šCFGå¯¼è‡´çš„æ¨ç†è¿è¡Œæ¬¡æ•°ã€‚</li>
<li>FADAåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ç”Ÿæˆè§†é¢‘è¡¨ç°ç”ŸåŠ¨ä¸”é€¼çœŸï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>FADAå®ç°äº†æ˜¾è‘—çš„æ¨ç†é€Ÿåº¦æå‡ï¼Œè¾¾åˆ°4.17è‡³12.5å€çš„NFEåŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac8e3d52641d51324f0bac0ed5ce2708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3769c34386028559346ecacead8ad08c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GANFusion-Feed-Forward-Text-to-3D-with-Diffusion-in-GAN-Space"><a href="#GANFusion-Feed-Forward-Text-to-3D-with-Diffusion-in-GAN-Space" class="headerlink" title="GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space"></a>GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space</h2><p><strong>Authors:Souhaib Attaiki, Paul Guerrero, Duygu Ceylan, Niloy J. Mitra, Maks Ovsjanikov</strong></p>
<p>We train a feed-forward text-to-3D diffusion generator for human characters using only single-view 2D data for supervision. Existing 3D generative models cannot yet match the fidelity of image or video generative models. State-of-the-art 3D generators are either trained with explicit 3D supervision and are thus limited by the volume and diversity of existing 3D data. Meanwhile, generators that can be trained with only 2D data as supervision typically produce coarser results, cannot be text-conditioned, or must revert to test-time optimization. We observe that GAN- and diffusion-based generators have complementary qualities: GANs can be trained efficiently with 2D supervision to produce high-quality 3D objects but are hard to condition on text. In contrast, denoising diffusion models can be conditioned efficiently but tend to be hard to train with only 2D supervision. We introduce GANFusion, which starts by generating unconditional triplane features for 3D data using a GAN architecture trained with only single-view 2D data. We then generate random samples from the GAN, caption them, and train a text-conditioned diffusion model that directly learns to sample from the space of good triplane features that can be decoded into 3D objects. </p>
<blockquote>
<p>æˆ‘ä»¬ä»…ä½¿ç”¨å•è§†å›¾2Dæ•°æ®è¿›è¡Œç›‘ç£ï¼Œè®­ç»ƒäº†ä¸€ä¸ªå‰é¦ˆæ–‡æœ¬åˆ°3Dæ‰©æ•£ç”Ÿæˆå™¨ï¼Œç”¨äºç”Ÿæˆäººç‰©è§’è‰²ã€‚ç°æœ‰çš„3Dç”Ÿæˆæ¨¡å‹è¿˜æ— æ³•ä¸å›¾åƒæˆ–è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ä¿çœŸåº¦ç›¸åŒ¹é…ã€‚æœ€å…ˆè¿›çš„3Dç”Ÿæˆå™¨è¦ä¹ˆæ¥å—æ˜ç¡®çš„3Dç›‘ç£è®­ç»ƒï¼Œå› æ­¤å—åˆ°ç°æœ‰3Dæ•°æ®çš„æ•°é‡å’Œå¤šæ ·æ€§çš„é™åˆ¶ã€‚åŒæ—¶ï¼Œé‚£äº›ä»…æ¥å—2Dæ•°æ®ä½œä¸ºç›‘ç£çš„ç”Ÿæˆå™¨é€šå¸¸ä¼šäº§ç”Ÿè¾ƒç²—ç³™çš„ç»“æœï¼Œæ— æ³•æ ¹æ®æ–‡æœ¬è¿›è¡Œè°ƒæ•´ï¼Œå¿…é¡»åœ¨æµ‹è¯•æ—¶è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬å‘ç°GANå’Œæ‰©æ•£ç”Ÿæˆå™¨å…·æœ‰äº’è¡¥çš„ç‰¹æ€§ï¼šGANå¯ä»¥ç”¨2Dç›‘ç£æœ‰æ•ˆåœ°è®­ç»ƒæ¥äº§ç”Ÿé«˜è´¨é‡çš„3Då¯¹è±¡ï¼Œä½†å¾ˆéš¾æ ¹æ®æ–‡æœ¬è¿›è¡Œè°ƒæ•´ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé™å™ªæ‰©æ•£æ¨¡å‹å¯ä»¥æ ¹æ®æ¡ä»¶è¿›è¡Œæœ‰æ•ˆé‡‡æ ·ï¼Œä½†å¾€å¾€éš¾ä»¥ä»…ä½¿ç”¨2Dç›‘ç£è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å¼•å…¥äº†GANFusionï¼Œå®ƒé¦–å…ˆä½¿ç”¨ä»…æ¥å—å•è§†å›¾2Dæ•°æ®è®­ç»ƒçš„GANæ¶æ„ï¼Œä¸º3Dæ•°æ®ç”Ÿæˆæ— æ¡ä»¶çš„ä¸‰å¹³é¢ç‰¹å¾ã€‚ç„¶åæˆ‘ä»¬ä»GANä¸­ç”Ÿæˆéšæœºæ ·æœ¬ï¼Œç»™å®ƒä»¬æ·»åŠ å­—å¹•ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªæ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç›´æ¥å­¦ä¹ ä»è‰¯å¥½çš„ä¸‰å¹³é¢ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œé‡‡æ ·ï¼Œè¿™äº›ç‰¹å¾å¯ä»¥è¢«è§£ç ä¸º3Då¯¹è±¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16717v1">PDF</a> <a target="_blank" rel="noopener" href="https://ganfusion.github.io/">https://ganfusion.github.io/</a></p>
<p><strong>Summary</strong><br>     ä½¿ç”¨å•ä¸€è§†è§’çš„äºŒç»´æ•°æ®ç›‘ç£ï¼Œè®­ç»ƒäº†ä¸€ä¸ªå‰é¦ˆæ–‡æœ¬åˆ°ä¸‰ç»´æ‰©æ•£ç”Ÿæˆå™¨ï¼Œç”¨äºç”Ÿæˆäººç±»è§’è‰²ã€‚ç°æœ‰ä¸‰ç»´ç”Ÿæˆæ¨¡å‹çš„ä¿çœŸåº¦å°šæ— æ³•ä¸å›¾åƒæˆ–è§†é¢‘ç”Ÿæˆæ¨¡å‹ç›¸åª²ç¾ã€‚æœ¬ç ”ç©¶ç»“åˆGANå’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œé€šè¿‡GANæ¶æ„ç”Ÿæˆæ— æ¡ä»¶çš„ä¸‰ç»´æ•°æ®triplaneç‰¹å¾ï¼Œç„¶åè®­ç»ƒæ–‡æœ¬è°ƒèŠ‚çš„æ‰©æ•£æ¨¡å‹ï¼Œç›´æ¥ä»è‰¯å¥½çš„triplaneç‰¹å¾ç©ºé—´ä¸­é‡‡æ ·å¹¶è§£ç ä¸ºä¸‰ç»´ç‰©ä½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨ä»…äºŒç»´æ•°æ®ç›‘ç£è®­ç»ƒäº†æ–‡æœ¬åˆ°ä¸‰ç»´æ‰©æ•£ç”Ÿæˆå™¨ã€‚</li>
<li>å½“å‰ä¸‰ç»´ç”Ÿæˆæ¨¡å‹çš„ä¿çœŸåº¦å°šæ— æ³•ä¸å›¾åƒæˆ–è§†é¢‘ç”Ÿæˆæ¨¡å‹åŒ¹æ•Œã€‚</li>
<li>GANå’Œæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä¸Šå…·æœ‰äº’è¡¥æ€§è´¨ã€‚</li>
<li>GANFusionæ–¹æ³•é¦–å…ˆä½¿ç”¨GANæ¶æ„ç”Ÿæˆæ— æ¡ä»¶çš„ä¸‰ç»´æ•°æ®triplaneç‰¹å¾ã€‚</li>
<li>é€šè¿‡å¯¹GANç”Ÿæˆçš„æ ·æœ¬è¿›è¡Œæè¿°ï¼Œè®­ç»ƒäº†æ–‡æœ¬è°ƒèŠ‚çš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å¯ä»¥ç›´æ¥ä»è‰¯å¥½çš„triplaneç‰¹å¾ç©ºé—´ä¸­é‡‡æ ·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7c70940dd4269c05089c24b53125fec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40e67532efd85372b59da69406e25bd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c673abefd0494d1b1696857121c6cbe3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TCAQ-DM-Timestep-Channel-Adaptive-Quantization-for-Diffusion-Models"><a href="#TCAQ-DM-Timestep-Channel-Adaptive-Quantization-for-Diffusion-Models" class="headerlink" title="TCAQ-DM: Timestep-Channel Adaptive Quantization for Diffusion Models"></a>TCAQ-DM: Timestep-Channel Adaptive Quantization for Diffusion Models</h2><p><strong>Authors:Haocheng Huang, Jiaxin Chen, Jinyang Guo, Ruiyi Zhan, Yunhong Wang</strong></p>
<p>Diffusion models have achieved remarkable success in the image and video generation tasks. Nevertheless, they often require a large amount of memory and time overhead during inference, due to the complex network architecture and considerable number of timesteps for iterative diffusion. Recently, the post-training quantization (PTQ) technique has proved a promising way to reduce the inference cost by quantizing the float-point operations to low-bit ones. However, most of them fail to tackle with the large variations in the distribution of activations across distinct channels and timesteps, as well as the inconsistent of input between quantization and inference on diffusion models, thus leaving much room for improvement. To address the above issues, we propose a novel method dubbed Timestep-Channel Adaptive Quantization for Diffusion Models (TCAQ-DM). Specifically, we develop a timestep-channel joint reparameterization (TCR) module to balance the activation range along both the timesteps and channels, facilitating the successive reconstruction procedure. Subsequently, we employ a dynamically adaptive quantization (DAQ) module that mitigate the quantization error by selecting an optimal quantizer for each post-Softmax layers according to their specific types of distributions. Moreover, we present a progressively aligned reconstruction (PAR) strategy to mitigate the bias caused by the input mismatch. Extensive experiments on various benchmarks and distinct diffusion models demonstrate that the proposed method substantially outperforms the state-of-the-art approaches in most cases, especially yielding comparable FID metrics to the full precision model on CIFAR-10 in the W6A6 setting, while enabling generating available images in the W4A4 settings. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç”±äºç½‘ç»œæ¶æ„å¤æ‚å’Œæ‰©æ•£è¿­ä»£çš„æ—¶åºæ­¥æ•°ä¼—å¤šï¼Œå®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šå¸¸éœ€è¦å¤§é‡çš„å†…å­˜å’Œæ—¶é—´å¼€é”€ã€‚æœ€è¿‘ï¼Œåè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æŠ€æœ¯è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œé€šè¿‡é‡åŒ–æµ®ç‚¹è¿ç®—åˆ°ä½ä½è¿ç®—æ¥é™ä½æ¨ç†æˆæœ¬ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•æ— æ³•å¤„ç†ä¸åŒé€šé“å’Œæ—¶åºæ­¥æ•°ä¹‹é—´æ¿€æ´»åˆ†å¸ƒçš„å·¨å¤§å·®å¼‚ï¼Œä»¥åŠæ‰©æ•£æ¨¡å‹é‡åŒ–ä¸è¾“å…¥ä¹‹é—´çš„ä¸ä¸€è‡´ï¼Œå› æ­¤ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ‰©æ•£æ¨¡å‹çš„æ—¶é—´æ­¥é•¿é€šé“è‡ªé€‚åº”é‡åŒ–ï¼ˆTCAQ-DMï¼‰çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ—¶é—´æ­¥é•¿é€šé“è”åˆå†å‚æ•°åŒ–ï¼ˆTCRï¼‰æ¨¡å—ï¼Œä»¥å¹³è¡¡æ—¶åºæ­¥é•¿å’Œé€šé“ä¸Šçš„æ¿€æ´»èŒƒå›´ï¼Œä¿ƒè¿›è¿ç»­çš„é‡å»ºè¿‡ç¨‹ã€‚éšåï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŠ¨æ€è‡ªé€‚åº”é‡åŒ–ï¼ˆDAQï¼‰æ¨¡å—ï¼Œé€šè¿‡æ ¹æ®åSoftmaxå±‚çš„ç‰¹å®šåˆ†å¸ƒç±»å‹é€‰æ‹©æœ€ä½³é‡åŒ–å™¨ï¼Œå‡è½»é‡åŒ–è¯¯å·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é€æ­¥å¯¹é½é‡å»ºï¼ˆPARï¼‰ç­–ç•¥ï¼Œä»¥å‡è½»è¾“å…¥ä¸åŒ¹é…é€ æˆçš„åå·®ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•å’Œä¸åŒçš„æ‰©æ•£æ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½å¤§å¤§ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨CIFAR-10çš„W6A *è®¾ç½®ä¸‹å®ç°äº†ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸è¿‘çš„FIDæŒ‡æ ‡ï¼ŒåŒæ—¶åœ¨W4A *è®¾ç½®ä¸‹å®ç°äº†å¯ç”¨çš„å›¾åƒç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16700v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å…¶å¤æ‚çš„ç½‘ç»œæ¶æ„å’Œè¿­ä»£çš„æ‰©æ•£è¿‡ç¨‹éœ€è¦å¤§é‡çš„å†…å­˜å’Œæ—¶é—´å¼€é”€ã€‚ä¸ºé™ä½æ¨ç†æˆæœ¬ï¼Œç ”ç©¶è€…é‡‡ç”¨äº†ä¸€ç§ç§°ä¸ºTCAQ-DMçš„é‡åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›ç°æœ‰çš„æŠ€æœ¯éš¾é¢˜ã€‚æ­¤æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¹³è¡¡æ¿€æ´»èŒƒå›´çš„TCRæ¨¡å—æ¥å¢å¼ºæ¨¡å‹çš„è¿ç»­é‡å»ºè¿‡ç¨‹ã€‚éšåä½¿ç”¨ä¸€ç§è‡ªé€‚åº”çš„é‡åŒ–æ–¹æ³•æ¥é€‰æ‹©é€‚åˆæ¯ç§åˆ†å¸ƒçš„æœ€ä½³é‡åŒ–å™¨ã€‚æ­¤å¤–ï¼Œé€šè¿‡é‡‡ç”¨é€æ­¥å¯¹é½é‡å»ºç­–ç•¥æ¥å‡å°‘è¾“å…¥ä¸åŒ¹é…å¼•èµ·çš„åå·®ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨CIFAR-10æ•°æ®é›†ä¸Šçš„FIDæŒ‡æ ‡ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆé¢†åŸŸå±•ç°å¼ºå¤§æ€§èƒ½ï¼Œä½†éœ€é¢å¯¹å¤§é‡çš„å†…å­˜å’Œæ—¶é—´æ¶ˆè€—é—®é¢˜ã€‚</li>
<li>ä¸ºäº†å‡å°‘æ¨ç†é˜¶æ®µçš„å¼€é”€ï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºTCAQ-DMçš„æ–°é‡åŒ–æ–¹æ³•ã€‚</li>
<li>TCAQ-DMä¸­çš„TCRæ¨¡å—å¹³è¡¡äº†æ¿€æ´»èŒƒå›´ï¼Œæœ‰åŠ©äºå¢å¼ºæ¨¡å‹çš„è¿ç»­é‡å»ºè¿‡ç¨‹ã€‚</li>
<li>DAQæ¨¡å—æ ¹æ®ç‰¹å®šåˆ†å¸ƒç±»å‹é€‰æ‹©æœ€ä½³é‡åŒ–å™¨ï¼Œä»¥å‡å°‘é‡åŒ–è¯¯å·®ã€‚</li>
<li>PARç­–ç•¥ç”¨äºå‡è½»è¾“å…¥ä¸åŒ¹é…å¼•èµ·çš„åå·®é—®é¢˜ã€‚</li>
<li>åœ¨å„ç§åŸºå‡†æµ‹è¯•å’Œä¸åŒæ‰©æ•£æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜TCAQ-DMçš„æœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨CIFAR-10æ•°æ®é›†ä¸Šå–å¾—äº†çªå‡ºçš„FIDæŒ‡æ ‡ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aba98acac7489471d2e287b539a0e7b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3020b15f0df29cab9e72188713ecb399.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bef492f3dfd754396a18d2a913663b78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91e1e51bccfd6b5ea094c5046e10b1a4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CustomTTT-Motion-and-Appearance-Customized-Video-Generation-via-Test-Time-Training"><a href="#CustomTTT-Motion-and-Appearance-Customized-Video-Generation-via-Test-Time-Training" class="headerlink" title="CustomTTT: Motion and Appearance Customized Video Generation via   Test-Time Training"></a>CustomTTT: Motion and Appearance Customized Video Generation via   Test-Time Training</h2><p><strong>Authors:Xiuli Bi, Jian Lu, Bo Liu, Xiaodong Cun, Yong Zhang, Weisheng Li, Bin Xiao</strong></p>
<p>Benefiting from large-scale pre-training of text-video pairs, current text-to-video (T2V) diffusion models can generate high-quality videos from the text description. Besides, given some reference images or videos, the parameter-efficient fine-tuning method, i.e. LoRA, can generate high-quality customized concepts, e.g., the specific subject or the motions from a reference video. However, combining the trained multiple concepts from different references into a single network shows obvious artifacts. To this end, we propose CustomTTT, where we can joint custom the appearance and the motion of the given video easily. In detail, we first analyze the prompt influence in the current video diffusion model and find the LoRAs are only needed for the specific layers for appearance and motion customization. Besides, since each LoRA is trained individually, we propose a novel test-time training technique to update parameters after combination utilizing the trained customized models. We conduct detailed experiments to verify the effectiveness of the proposed methods. Our method outperforms several state-of-the-art works in both qualitative and quantitative evaluations. </p>
<blockquote>
<p>å¾—ç›Šäºå¤§è§„æ¨¡æ–‡æœ¬-è§†é¢‘å¯¹æ•°æ®çš„é¢„è®­ç»ƒï¼Œå½“å‰çš„æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹å¯ä»¥æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ã€‚æ­¤å¤–ï¼Œç»™å®šä¸€äº›å‚è€ƒå›¾åƒæˆ–è§†é¢‘ï¼Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼ˆä¾‹å¦‚LoRAï¼‰å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„è‡ªå®šæ¦‚å¿µï¼Œä¾‹å¦‚ä»å‚è€ƒè§†é¢‘ä¸­çš„ç‰¹å®šä¸»é¢˜æˆ–åŠ¨ä½œã€‚ç„¶è€Œï¼Œå°†æ¥è‡ªä¸åŒå‚è€ƒçš„è®­ç»ƒè¿‡çš„å¤šä¸ªæ¦‚å¿µåˆå¹¶åˆ°å•ä¸ªç½‘ç»œä¸­ä¼šå‡ºç°æ˜æ˜¾çš„ä¼ªå½±ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CustomTTTï¼Œå¯ä»¥è½»æ¾åœ°ç»“åˆç»™å®šè§†é¢‘çš„å¤–è§‚å’ŒåŠ¨ä½œè¿›è¡Œè‡ªå®šä¹‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†æå½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„æç¤ºå½±å“ï¼Œå¹¶å‘ç°å¯¹äºå¤–è§‚å’ŒåŠ¨ä½œè‡ªå®šä¹‰è€Œè¨€ï¼ŒLoRAä»…éœ€è¦ç‰¹å®šå±‚ã€‚æ­¤å¤–ï¼Œç”±äºæ¯ä¸ªLoRAéƒ½æ˜¯å•ç‹¬è®­ç»ƒçš„ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æµ‹è¯•æ—¶è®­ç»ƒæŠ€æœ¯ï¼Œåˆ©ç”¨è®­ç»ƒå¥½çš„è‡ªå®šä¹‰æ¨¡å‹åœ¨ç»„åˆåè¿›è¡Œå‚æ•°æ›´æ–°ã€‚æˆ‘ä»¬è¿›è¡Œäº†è¯¦ç»†çš„å®éªŒæ¥éªŒè¯æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°æ–¹é¢éƒ½ä¼˜äºå‡ ç§æœ€æ–°æŠ€æœ¯çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15646v2">PDF</a> Accepted in AAAI 2025. Project Page: <a target="_blank" rel="noopener" href="https://customttt.github.io/">https://customttt.github.io/</a>   Code: <a target="_blank" rel="noopener" href="https://github.com/RongPiKing/CustomTTT">https://github.com/RongPiKing/CustomTTT</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†å½“å‰æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒçš„æ–‡æœ¬-è§†é¢‘å¯¹ç”Ÿæˆé«˜è´¨é‡è§†é¢‘çš„èƒ½åŠ›ã€‚é€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰ï¼Œå¯ä»¥æ ¹æ®å‚è€ƒå›¾åƒæˆ–è§†é¢‘ç”Ÿæˆé«˜è´¨é‡å®šåˆ¶åŒ–æ¦‚å¿µã€‚ç„¶è€Œï¼Œä»å¤šä¸ªå‚è€ƒä¸­è®­ç»ƒçš„æ¦‚å¿µç»“åˆåˆ°å•ä¸€ç½‘ç»œä¸­ä¼šäº§ç”Ÿæ˜æ˜¾ç‘•ç–µã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†CustomTTTï¼Œèƒ½å¤Ÿè½»æ¾ç»“åˆç»™å®šè§†é¢‘çš„å¤–è§‚å’Œè¿åŠ¨ã€‚ç ”ç©¶å‘ç°LoRAä»…å¯¹ç‰¹å®šå±‚è¿›è¡Œå¤–è§‚å’Œè¿åŠ¨å®šåˆ¶æ‰€éœ€ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨æµ‹è¯•æ—¶è®­ç»ƒæŠ€æœ¯æ›´æ–°ç»“åˆåçš„å‚æ•°ã€‚å®éªŒéªŒè¯æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸Šå‡ä¼˜äºç°æœ‰å‰æ²¿å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹èƒ½åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒç”Ÿæˆé«˜è´¨é‡è§†é¢‘ã€‚</li>
<li>å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼ˆå¦‚LoRAï¼‰å…è®¸æ ¹æ®å‚è€ƒå›¾åƒæˆ–è§†é¢‘ç”Ÿæˆå®šåˆ¶åŒ–çš„æ¦‚å¿µã€‚</li>
<li>ç»“åˆå¤šä¸ªå‚è€ƒæ¦‚å¿µåˆ°å•ä¸€ç½‘ç»œä¸­ä¼šäº§ç”Ÿç‘•ç–µã€‚</li>
<li>CustomTTTæ–¹æ³•èƒ½å¤Ÿè½»æ¾ç»“åˆç»™å®šè§†é¢‘çš„å¤–è§‚å’Œè¿åŠ¨ã€‚</li>
<li>LoRAä»…å¯¹ç‰¹å®šå±‚è¿›è¡Œå¤–è§‚å’Œè¿åŠ¨å®šåˆ¶çš„éœ€æ±‚ã€‚</li>
<li>åˆ©ç”¨æµ‹è¯•æ—¶è®­ç»ƒæŠ€æœ¯æ›´æ–°ç»“åˆåçš„å‚æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6df1cf6eb3330461bd02631aa1b84566.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-126d05fe6292ac9faa0c7d0948b68b7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05d39e1b0b64ddbb836a2c2dbef06f12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf8d56f0056223d1d4f5b911258ec96b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46407ef1002b5a9761c561f4242ebe0f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Zero-Shot-Low-Light-Image-Enhancement-with-Diffusion-Prior"><a href="#Zero-Shot-Low-Light-Image-Enhancement-with-Diffusion-Prior" class="headerlink" title="Zero-Shot Low Light Image Enhancement with Diffusion Prior"></a>Zero-Shot Low Light Image Enhancement with Diffusion Prior</h2><p><strong>Authors:Joshua Cho, Sara Aghajanzadeh, Zhen Zhu, D. A. Forsyth</strong></p>
<p>Balancing aesthetic quality with fidelity when enhancing images from challenging, degraded sources is a core objective in computational photography. In this paper, we address low light image enhancement (LLIE), a task in which dark images often contain limited visible information. Diffusion models, known for their powerful image enhancement capacities, are a natural choice for this problem. However, their deep generative priors can also lead to hallucinations, introducing non-existent elements or substantially altering the visual semantics of the original scene. In this work, we introduce a novel zero-shot method for controlling and refining the generative behavior of diffusion models for dark-to-light image conversion tasks. Our method demonstrates superior performance over existing state-of-the-art methods in the task of low-light image enhancement, as evidenced by both quantitative metrics and qualitative analysis. </p>
<blockquote>
<p>åœ¨è®¡ç®—æ‘„å½±ä¸­ï¼Œå¹³è¡¡å¢å¼ºæ¥è‡ªå…·æœ‰æŒ‘æˆ˜æ€§ã€é€€åŒ–æºçš„å›¾åƒæ—¶çš„ç¾å­¦è´¨é‡ä¸ä¿çœŸåº¦æ˜¯æ ¸å¿ƒç›®æ ‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³ä½å…‰å›¾åƒå¢å¼ºï¼ˆLLIEï¼‰çš„é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªæš—å›¾åƒé€šå¸¸åŒ…å«æœ‰é™å¯è§ä¿¡æ¯çš„ä»»åŠ¡ã€‚æ‰©æ•£æ¨¡å‹ä»¥å…¶å¼ºå¤§çš„å›¾åƒå¢å¼ºèƒ½åŠ›è€Œé—»åï¼Œæ˜¯æ­¤é—®é¢˜çš„è‡ªç„¶é€‰æ‹©ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ·±åº¦ç”Ÿæˆå…ˆéªŒä¹Ÿå¯èƒ½å¯¼è‡´å¹»è§‰ï¼Œå¼•å…¥ä¸å­˜åœ¨çš„å…ƒç´ æˆ–å¤§å¹…æ”¹å˜åŸå§‹åœºæ™¯çš„å¯è§†è¯­ä¹‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç”¨äºæ§åˆ¶å’Œä¼˜åŒ–æ‰©æ•£æ¨¡å‹åœ¨æš—åˆ°äº®å›¾åƒè½¬æ¢ä»»åŠ¡ä¸­çš„ç”Ÿæˆè¡Œä¸ºçš„æ–°å‹é›¶æ ·æœ¬æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä½å…‰å›¾åƒå¢å¼ºä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¿™ç”±å®šé‡æŒ‡æ ‡å’Œå®šæ€§åˆ†æå‡å¯ä»¥è¯æ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.13401v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡èšç„¦äºä½å…‰å›¾åƒå¢å¼ºä»»åŠ¡ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹æ¥è§£å†³åœ¨å…‰ç…§ä¸è¶³çš„æ¡ä»¶ä¸‹å›¾åƒä¸­ä¿¡æ¯æœ‰é™çš„é—®é¢˜ã€‚é€šè¿‡ä¸€ç§æ–°çš„é›¶æ ·æœ¬æ–¹æ³•ï¼Œå®ç°äº†å¯¹æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè¡Œä¸ºè¿›è¡Œæ§åˆ¶å’Œä¼˜åŒ–ï¼Œä½¿æ¨¡å‹åœ¨ä½å…‰å›¾åƒå¢å¼ºä»»åŠ¡ä¸­å…·æœ‰ä¼˜å¼‚æ€§èƒ½ã€‚è¿™ç§æ–°æ–¹æ³•é€šè¿‡å®šé‡æŒ‡æ ‡å’Œå®šæ€§åˆ†æè¯æ˜äº†å…¶åœ¨ç°æœ‰æŠ€æœ¯ä¸­çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å…³æ³¨è®¡ç®—æ‘„å½±ä¸­çš„æ ¸å¿ƒé—®é¢˜ï¼Œå³åœ¨æŒ‘æˆ˜æ€§å¼ºã€é€€åŒ–ä¸¥é‡çš„å›¾åƒæºä¸­å¹³è¡¡ç¾å­¦è´¨é‡ä¸ä¿çœŸåº¦ã€‚</li>
<li>ä½å…‰å›¾åƒå¢å¼ºï¼ˆLLIEï¼‰æ˜¯å…¶ä¸­ä¸€ä¸ªä»»åŠ¡ï¼Œæ¶‰åŠå¤„ç†é»‘æš—ç¯å¢ƒä¸­ä¿¡æ¯æœ‰é™çš„å›¾åƒã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å› å…¶å¼ºå¤§çš„å›¾åƒå¢å¼ºèƒ½åŠ›è€Œæˆä¸ºè§£å†³æ­¤é—®é¢˜çš„è‡ªç„¶é€‰æ‹©ã€‚</li>
<li>ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹çš„æ·±åº¦ç”Ÿæˆå…ˆéªŒä¹Ÿå¯èƒ½å¯¼è‡´å¹»è§‰ï¼Œå¼•å…¥ä¸å­˜åœ¨çš„å…ƒç´ æˆ–å¤§å¹…æ”¹å˜åŸå§‹åœºæ™¯çš„å¯è§†è¯­ä¹‰ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„é›¶æ ·æœ¬æ–¹æ³•æ¥æ§åˆ¶å’Œä¼˜åŒ–æ‰©æ•£æ¨¡å‹åœ¨æš—å…‰å›¾åƒè½¬æ¢ä»»åŠ¡ä¸­çš„ç”Ÿæˆè¡Œä¸ºã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§åˆ†æä¸­å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.13401">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Diffusion Models/2412.13401v2/page_0_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7861a08327269ab0a651559863b77403.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0845f041cb641b66730d1508115cfa2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afa28c3d81953ef5271254f685ce67f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e23b344ae40afc9fd7fc6021e4151eb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf3985e7ed752ad8ef609f2ae10ce15d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CC-Diff-Enhancing-Contextual-Coherence-in-Remote-Sensing-Image-Synthesis"><a href="#CC-Diff-Enhancing-Contextual-Coherence-in-Remote-Sensing-Image-Synthesis" class="headerlink" title="CC-Diff: Enhancing Contextual Coherence in Remote Sensing Image   Synthesis"></a>CC-Diff: Enhancing Contextual Coherence in Remote Sensing Image   Synthesis</h2><p><strong>Authors:Mu Zhang, Yunfan Liu, Yue Liu, Hongtian Yu, Qixiang Ye</strong></p>
<p>Accurately depicting real-world landscapes in remote sensing (RS) images requires precise alignment between objects and their environment. However, most existing synthesis methods for natural images prioritize foreground control, often reducing the background to plain textures. This neglects the interaction between foreground and background, which can lead to incoherence in RS scenarios. In this paper, we introduce CC-Diff, a Diffusion Model-based approach for RS image generation with enhanced Context Coherence. To capture spatial interdependence, we propose a sequential pipeline where background generation is conditioned on synthesized foreground instances. Distinct learnable queries are also employed to model both the complex background texture and its semantic relation to the foreground. Extensive experiments demonstrate that CC-Diff outperforms state-of-the-art methods in visual fidelity, semantic accuracy, and positional precision, excelling in both RS and natural image domains. CC-Diff also shows strong trainability, improving detection accuracy by 2.04 mAP on DOTA and 2.25 mAP on the COCO benchmark. </p>
<blockquote>
<p>åœ¨é¥æ„Ÿï¼ˆRSï¼‰å›¾åƒä¸­å‡†ç¡®æç»˜çœŸå®ä¸–ç•Œçš„æ™¯è§‚éœ€è¦ç‰©ä½“ä¸å…¶ç¯å¢ƒä¹‹é—´çš„ç²¾ç¡®å¯¹é½ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„è‡ªç„¶å›¾åƒåˆæˆæ–¹æ³•éƒ½ä¼˜å…ˆè¿›è¡Œå‰æ™¯æ§åˆ¶ï¼Œç»å¸¸å°†èƒŒæ™¯ç®€åŒ–ä¸ºçº¯çº¹ç†ã€‚è¿™å¿½ç•¥äº†å‰æ™¯å’ŒèƒŒæ™¯ä¹‹é—´çš„äº¤äº’ï¼Œå¯èƒ½å¯¼è‡´é¥æ„Ÿåœºæ™¯ä¸­çš„ä¸è¿è´¯æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CC-Diffï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„é¥æ„Ÿå›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå…·æœ‰å¢å¼ºçš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚ä¸ºäº†æ•æ‰ç©ºé—´ç›¸å…³æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé¡ºåºæµç¨‹ï¼Œå…¶ä¸­èƒŒæ™¯ç”Ÿæˆæ˜¯åœ¨åˆæˆçš„å‰æ™¯å®ä¾‹æ¡ä»¶ä¸‹è¿›è¡Œçš„ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨ä¸åŒçš„å¯å­¦ä¹ æŸ¥è¯¢æ¥å¯¹å¤æ‚çš„èƒŒæ™¯çº¹ç†åŠå…¶ä¸å‰æ™¯çš„è¯­ä¹‰å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCC-Diffåœ¨è§†è§‰ä¿çœŸåº¦ã€è¯­ä¹‰å‡†ç¡®æ€§å’Œå®šä½ç²¾åº¦æ–¹é¢å‡ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨é¥æ„Ÿå’Œè‡ªç„¶å›¾åƒé¢†åŸŸå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚CC-Diffè¿˜æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å¯è®­ç»ƒæ€§ï¼Œåœ¨DOTAä¸Šæé«˜äº†2.04 mAPçš„æ£€æµ‹ç²¾åº¦ï¼Œåœ¨COCOåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†2.25 mAPã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.08464v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹é¥æ„Ÿå›¾åƒä¸­çš„çœŸå®ä¸–ç•Œæ™¯è§‚æç»˜é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å¢å¼ºæ–¹æ³•CC-Diffã€‚è¯¥æ–¹æ³•é€šè¿‡è€ƒè™‘å‰æ™¯ä¸èƒŒæ™¯ä¹‹é—´çš„äº¤äº’ä½œç”¨ï¼Œæé«˜äº†é¥æ„Ÿå›¾åƒç”Ÿæˆçš„ç©ºé—´è¿è´¯æ€§ã€‚å®éªŒè¯æ˜ï¼ŒCC-Diffåœ¨è§†è§‰ä¿çœŸåº¦ã€è¯­ä¹‰å‡†ç¡®æ€§å’Œå®šä½ç²¾åº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒåŒæ—¶åœ¨é¥æ„Ÿå›¾åƒå’Œè‡ªç„¶å›¾åƒé¢†åŸŸå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒCC-Diffåœ¨DOTAå’ŒCOCOåŸºå‡†æµ‹è¯•ä¸Šçš„ç›®æ ‡æ£€æµ‹ç²¾åº¦åˆ†åˆ«æé«˜äº†2.04 mAPå’Œ2.25 mAPã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CC-Diffæ–¹æ³•å¼ºè°ƒäº†å‰æ™¯ä¸èƒŒæ™¯ä¹‹é—´çš„äº¤äº’ä½œç”¨ï¼Œæé«˜äº†é¥æ„Ÿå›¾åƒç”Ÿæˆä¸­çš„ç©ºé—´è¿è´¯æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„èƒŒæ™¯ç”Ÿæˆæ¡ä»¶æ–¹æ³•ï¼Œä¾èµ–äºåˆæˆçš„å‰æ™¯å®ä¾‹ã€‚</li>
<li>é‡‡ç”¨ä¸åŒçš„å­¦ä¹ æŸ¥è¯¢æ¨¡æ‹Ÿå¤æ‚èƒŒæ™¯çº¹ç†åŠå…¶ä¸å‰æ™¯çš„è¯­ä¹‰å…³ç³»ã€‚</li>
<li>å®éªŒè¯æ˜CC-Diffåœ¨è§†è§‰ä¿çœŸåº¦ã€è¯­ä¹‰å‡†ç¡®æ€§å’Œå®šä½ç²¾åº¦æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>CC-Diffåœ¨è‡ªç„¶å›¾åƒå’Œé¥æ„Ÿå›¾åƒé¢†åŸŸå‡å±•ç°å‡ºä¼˜ç§€æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒCC-Diffæé«˜äº†ç›®æ ‡æ£€æµ‹ç²¾åº¦ï¼Œå¯¹é¥æ„Ÿåº”ç”¨å…·æœ‰é‡è¦ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.08464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c08cb9958d10376dc4c68828a1ed72e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ed910a52932186b8173136b43049cc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0614f7a5ceff9a03b46ed443d78c6d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0df51cf4df07c894818863e62a13b9f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c037adc9b093b6f98a1c87b90e337a76.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance"><a href="#Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance" class="headerlink" title="Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance"></a>Enhanced Generative Data Augmentation for Semantic Segmentation via   Stronger Guidance</h2><p><strong>Authors:Quang-Huy Che, Duc-Tri Le, Bich-Nga Pham, Duc-Khai Lam, Vinh-Tiep Nguyen</strong></p>
<p>Data augmentation is crucial for pixel-wise annotation tasks like semantic segmentation, where labeling requires significant effort and intensive labor. Traditional methods, involving simple transformations such as rotations and flips, create new images but often lack diversity along key semantic dimensions and fail to alter high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable Generative models offer data augmentation methods for semantic segmentation tasks by using prompts and visual references from the original image. However, these models face challenges in generating synthetic images that accurately reflect the content and structure of the original image due to difficulties in creating effective prompts and visual references. In this work, we introduce an effective data augmentation pipeline for semantic segmentation using Controllable Diffusion model. Our proposed method includes efficient prompt generation using \textit{Class-Prompt Appending} and \textit{Visual Prior Blending} to enhance attention to labeled classes in real images, allowing the pipeline to generate a precise number of augmented images while preserving the structure of segmentation-labeled classes. In addition, we implement a \textit{class balancing algorithm} to ensure a balanced training dataset when merging the synthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates its effectiveness in generating high-quality synthetic images for semantic segmentation. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance%7D%7Bthis">https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance}{this</a> https URL}. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºå¯¹äºåƒç´ çº§æ ‡æ³¨ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ï¼‰è‡³å…³é‡è¦ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦å¤§é‡äººåŠ›è¿›è¡Œæ ‡æ³¨ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ï¼Œå¦‚æ—‹è½¬å’Œç¿»è½¬ç­‰ç®€å•å˜æ¢ï¼Œè™½ç„¶å¯ä»¥ç”Ÿæˆæ–°å›¾åƒï¼Œä½†å¾€å¾€åœ¨å…³é”®çš„è¯­ä¹‰ç»´åº¦ä¸Šç¼ºä¹å¤šæ ·æ€§ï¼Œå¹¶ä¸”æ— æ³•æ”¹å˜é«˜çº§è¯­ä¹‰å±æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç”Ÿæˆæ¨¡å‹ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ¥å¢å¼ºæ•°æ®ã€‚å¯æ§ç”Ÿæˆæ¨¡å‹é€šè¿‡ä½¿ç”¨åŸå§‹å›¾åƒçš„æç¤ºå’Œè§†è§‰å‚è€ƒï¼Œä¸ºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡æä¾›æ•°æ®å¢å¼ºæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®åæ˜ åŸå§‹å›¾åƒå†…å®¹å’Œç»“æ„çš„åˆæˆå›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºåˆ›å»ºæœ‰æ•ˆçš„æç¤ºå’Œè§†è§‰å‚è€ƒå…·æœ‰éš¾åº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä½¿ç”¨å¯æ§æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ•°æ®å¢å¼ºç®¡é“ï¼Œç”¨äºè¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨â€œç±»æç¤ºé™„åŠ â€å’Œâ€œè§†è§‰å…ˆéªŒæ··åˆâ€è¿›è¡Œæœ‰æ•ˆæç¤ºç”Ÿæˆï¼Œä»¥æé«˜å¯¹çœŸå®å›¾åƒä¸­æ ‡è®°ç±»çš„å…³æ³¨ï¼Œä½¿ç®¡é“èƒ½å¤Ÿåœ¨ä¿ç•™åˆ†å‰²æ ‡è®°ç±»ç»“æ„çš„åŒæ—¶ç”Ÿæˆç²¾ç¡®æ•°é‡çš„å¢å¼ºå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ç§â€œç±»åˆ«å¹³è¡¡ç®—æ³•â€ï¼Œä»¥ç¡®ä¿åœ¨åˆå¹¶åˆæˆå›¾åƒå’ŒåŸå§‹å›¾åƒæ—¶è·å¾—å¹³è¡¡çš„è®­ç»ƒæ•°æ®é›†ã€‚åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®¡é“åœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²æ–¹é¢éå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance">æ­¤URL</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.06002v3">PDF</a> Accepted to ICPRAM 2025</p>
<p><strong>æ‘˜è¦</strong><br>æ•°æ®å¢å¼ºåœ¨åƒç´ çº§æ ‡æ³¨ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ï¼‰ä¸­è‡³å…³é‡è¦ï¼Œå› ä¸ºæ ‡æ³¨éœ€è¦å¤§é‡åŠ³åŠ¨ã€‚ä¼ ç»Ÿæ–¹æ³•è™½ç„¶èƒ½ç”Ÿæˆæ–°å›¾åƒï¼Œä½†åœ¨å…³é”®è¯­ä¹‰ç»´åº¦ä¸Šç¼ºä¹å¤šæ ·æ€§ï¼Œæ— æ³•æ”¹å˜é«˜çº§è¯­ä¹‰å±æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç”Ÿæˆæ¨¡å‹å·²æˆä¸ºæ•°æ®å¢å¼ºçš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ¥æ‰©å¤§æ•°æ®é›†ã€‚å¯æ§ç”Ÿæˆæ¨¡å‹ä¸ºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡æä¾›äº†æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨åŸå§‹å›¾åƒçš„æç¤ºå’Œè§†è§‰å‚è€ƒæ¥ç”Ÿæˆå›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆå‡†ç¡®åæ˜ åŸå§‹å›¾åƒå†…å®¹å’Œç»“æ„çš„åˆæˆå›¾åƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼ŒåŸå› æ˜¯åˆ›å»ºæœ‰æ•ˆçš„æç¤ºå’Œè§†è§‰å‚è€ƒå­˜åœ¨å›°éš¾ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºå¯æ§æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ•°æ®å¢å¼ºç®¡é“ï¼Œç”¨äºè¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨Class-Prompt Appendingå’ŒVisual Prior Blendingè¿›è¡Œé«˜æ•ˆæç¤ºç”Ÿæˆï¼Œä»¥æé«˜å¯¹çœŸå®å›¾åƒä¸­æ ‡è®°ç±»çš„å…³æ³¨ï¼Œä½¿ç®¡é“èƒ½å¤Ÿåœ¨ä¿ç•™åˆ†å‰²æ ‡è®°ç±»ç»“æ„çš„åŒæ—¶ç”Ÿæˆç²¾ç¡®æ•°é‡çš„å¢å¼ºå›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†class balancing algorithmï¼Œä»¥ç¡®ä¿åœ¨åˆå¹¶åˆæˆå›¾åƒå’ŒåŸå§‹å›¾åƒæ—¶è®­ç»ƒæ•°æ®é›†å¹³è¡¡ã€‚åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®¡é“åœ¨ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²æ–¹é¢éå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance">æ­¤httpsé“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ•°æ®å¢å¼ºåœ¨åƒç´ çº§æ ‡æ³¨ä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>ä¼ ç»Ÿæ•°æ®å¢å¼ºæ–¹æ³•ç¼ºä¹åœ¨å…³é”®è¯­ä¹‰ç»´åº¦ä¸Šçš„å¤šæ ·æ€§ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¯æ§ç”Ÿæˆæ¨¡å‹ï¼Œæ˜¯è§£å†³æ•°æ®å¢å¼ºé—®é¢˜çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºç®¡é“ï¼Œç»“åˆClass-Prompt Appendingå’ŒVisual Prior BlendingæŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜åˆæˆå›¾åƒçš„è´¨é‡å¹¶ä¿ç•™åŸå§‹å›¾åƒçš„ç»“æ„ä¿¡æ¯ã€‚</li>
<li>å®æ–½äº†ä¸€ç§class balancing algorithmï¼Œä»¥ç¡®ä¿åˆæˆå’ŒåŸå§‹å›¾åƒçš„å¹³è¡¡èåˆã€‚</li>
<li>åœ¨PASCAL VOCæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†è¯¥ç®¡é“çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.06002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7fbdaf4c72fdcbe13e289d445d413692.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca6f23b95f20add47a4d0515e8947a89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91dd43e3d48806df3e2fa23090a86993.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b3c65fb7936a29984cc349b14779630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a097a22cba61b6668399667be306c49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c298870b43ce121bb270ff9dab12e16e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models"><a href="#Rate-Adaptive-Generative-Semantic-Communication-Using-Conditional-Diffusion-Models" class="headerlink" title="Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models"></a>Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models</h2><p><strong>Authors:Pujing Yang, Guangyi Zhang, Yunlong Cai</strong></p>
<p>Recent advances in deep learning-based joint source-channel coding (DJSCC) have shown promise for end-to-end semantic image transmission. However, most existing schemes primarily focus on optimizing pixel-wise metrics, which often fail to align with human perception, leading to lower perceptual quality. In this letter, we propose a novel generative DJSCC approach using conditional diffusion models to enhance the perceptual quality of transmitted images. Specifically, by utilizing entropy models, we effectively manage transmission bandwidth based on the estimated entropy of transmitted sym-bols. These symbols are then used at the receiver as conditional information to guide a conditional diffusion decoder in image reconstruction. Our model is built upon the emerging advanced mamba-like linear attention (MLLA) skeleton, which excels in image processing tasks while also offering fast inference speed. Besides, we introduce a multi-stage training strategy to ensure the stability and improve the overall performance of the model. Simulation results demonstrate that our proposed method significantly outperforms existing approaches in terms of perceptual quality. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„è”åˆæºä¿¡é“ç¼–ç ï¼ˆDJSCCï¼‰çš„æœ€æ–°è¿›å±•åœ¨ç«¯åˆ°ç«¯è¯­ä¹‰å›¾åƒä¼ è¾“æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ¡ˆä¸»è¦é›†ä¸­åœ¨ä¼˜åŒ–åƒç´ çº§çš„æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡å¾€å¾€ä¸äººç±»æ„ŸçŸ¥ä¸ä¸€è‡´ï¼Œå¯¼è‡´æ„ŸçŸ¥è´¨é‡ä¸‹é™ã€‚åœ¨æœ¬ä¿¡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ–°å‹ç”Ÿæˆå¼DJSCCæ–¹æ³•ï¼Œä»¥æé«˜ä¼ è¾“å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨ç†µæ¨¡å‹ï¼Œæ ¹æ®ä¼ è¾“ç¬¦å·çš„ä¼°è®¡ç†µæœ‰æ•ˆåœ°ç®¡ç†ä¼ è¾“å¸¦å®½ã€‚è¿™äº›ç¬¦å·ç„¶ååœ¨æ¥æ”¶å™¨ç«¯ä½œä¸ºæ¡ä»¶ä¿¡æ¯ï¼Œç”¨äºæŒ‡å¯¼å›¾åƒé‡å»ºä¸­çš„æ¡ä»¶æ‰©æ•£è§£ç å™¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹å»ºç«‹åœ¨æ–°å…´çš„ç±»å“ºä¹³åŠ¨ç‰©çº¿æ€§æ³¨æ„åŠ›ï¼ˆMLLAï¼‰éª¨æ¶ä¹‹ä¸Šï¼Œè¯¥éª¨æ¶åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æä¾›å¿«é€Ÿçš„æ¨ç†é€Ÿåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„ç¨³å®šæ€§å¹¶æé«˜å…¶æ•´ä½“æ€§èƒ½ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02597v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ‘˜è¦ç ”ç©¶äº†æ·±åº¦å­¦ä¹ åœ¨è”åˆæºä¿¡é“ç¼–ç æŠ€æœ¯ä¸Šçš„åº”ç”¨åŠå…¶å¯¹è¯­ä¹‰å›¾åƒä¼ è¾“çš„é©æ–°å½±å“ã€‚ç°æœ‰çš„å¤šæ•°ç­–ç•¥å…³æ³¨åƒç´ çº§çš„ä¼˜åŒ–ï¼Œå¹¶æœªå……åˆ†æ»¡è¶³äººç±»çš„æ„ŸçŸ¥éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå¼è”åˆæºä¿¡é“ç¼–ç æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ä¼ è¾“å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚é€šè¿‡åˆ©ç”¨ç†µæ¨¡å‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®ä¼ è¾“ç¬¦å·çš„ä¼°è®¡ç†µæœ‰æ•ˆåœ°ç®¡ç†ä¼ è¾“å¸¦å®½ã€‚æ¥æ”¶ç«¯åˆ©ç”¨è¿™äº›ç¬¦å·ä½œä¸ºæ¡ä»¶ä¿¡æ¯ï¼Œå¼•å¯¼å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­çš„æ¡ä»¶æ‰©æ•£è§£ç å™¨ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åŸºäºå…ˆè¿›çš„mamba-likeçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶æ„å»ºï¼Œå…·å¤‡å›¾åƒå¤„ç†ä¼˜åŠ¿ï¼Œä¸”æ¨ç†é€Ÿåº¦å¿«ã€‚ç ”ç©¶é‡‡ç”¨äº†å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ä»¥ç¡®ä¿æ¨¡å‹ç¨³å®šæ€§å¹¶æå‡æ€§èƒ½ã€‚ä»¿çœŸå®éªŒæ˜¾ç¤ºï¼Œæœ¬æ–‡æ–¹æ³•åœ¨æ„ŸçŸ¥è´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬çš„å…³é”®æ´å¯Ÿç‚¹æ¦‚è¿°ï¼š</p>
<ol>
<li>ç ”ç©¶ç„¦ç‚¹ï¼šæœ¬æ–‡ç ”ç©¶æ·±åº¦å­¦ä¹ åœ¨è”åˆæºä¿¡é“ç¼–ç æŠ€æœ¯ï¼ˆDJSCCï¼‰çš„æœ€æ–°è¿›å±•åŠå…¶å¯¹è¯­ä¹‰å›¾åƒä¼ è¾“çš„å½±å“ã€‚</li>
<li>ç°å­˜é—®é¢˜ï¼šå¤šæ•°ç°æœ‰æ–¹æ¡ˆä¸»è¦å…³æ³¨åƒç´ çº§çš„ä¼˜åŒ–æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡å¹¶ä¸æ€»æ˜¯ä¸äººç±»æ„ŸçŸ¥è´¨é‡ç›¸ç¬¦ã€‚</li>
<li>æ–°æ–¹æ³•æå‡ºï¼šå¼•å…¥åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå¼DJSCCæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ä¼ è¾“å›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>ç†µæ¨¡å‹çš„åº”ç”¨ï¼šåˆ©ç”¨ç†µæ¨¡å‹æ¥ç®¡ç†ä¼ è¾“å¸¦å®½ï¼Œæ ¹æ®ä¼ è¾“ç¬¦å·çš„ä¼°è®¡ç†µè¿›è¡Œæœ‰æ•ˆç‡è°ƒæ•´ã€‚</li>
<li>æ¥æ”¶ç«¯å¤„ç†ï¼šæ¥æ”¶ç«¯ä½¿ç”¨è¿™äº›ç¬¦å·ä½œä¸ºæ¡ä»¶ä¿¡æ¯ï¼ŒæŒ‡å¯¼å›¾åƒé‡å»ºè¿‡ç¨‹ä¸­çš„æ¡ä»¶æ‰©æ•£è§£ç å™¨ã€‚</li>
<li>æ¨¡å‹åŸºç¡€ï¼šæ¨¡å‹å»ºç«‹åœ¨mamba-likeçº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ä¹‹ä¸Šï¼Œè¯¥æœºåˆ¶åœ¨å›¾åƒå¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶æä¾›å¿«é€Ÿçš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>è®­ç»ƒç­–ç•¥ï¼šé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ä»¥ç¡®ä¿æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eff982c45e695cae805b96e6113523d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b91ed06a7a95f47ccfceb2e9725c4363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aabf79dae9b7fc082b1b85d8c6c06d9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c893e4c70b00b37fcfbb0f773226c9c7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GRPose-Learning-Graph-Relations-for-Human-Image-Generation-with-Pose-Priors"><a href="#GRPose-Learning-Graph-Relations-for-Human-Image-Generation-with-Pose-Priors" class="headerlink" title="GRPose: Learning Graph Relations for Human Image Generation with Pose   Priors"></a>GRPose: Learning Graph Relations for Human Image Generation with Pose   Priors</h2><p><strong>Authors:Xiangchen Yin, Donglin Di, Lei Fan, Hao Li, Wei Chen, Xiaofei Gou, Yang Song, Xiao Sun, Xun Yang</strong></p>
<p>Recent methods using diffusion models have made significant progress in human image generation with various control signals such as pose priors. However, existing efforts are still struggling to generate high-quality images with consistent pose alignment, resulting in unsatisfactory output. In this paper, we propose a framework that delves into the graph relations of pose priors to provide control information for human image generation. The main idea is to establish a graph topological structure between the pose priors and latent representation of diffusion models to capture the intrinsic associations between different pose parts. A Progressive Graph Integrator (PGI) is designed to learn the spatial relationships of the pose priors with the graph structure, adopting a hierarchical strategy within an Adapter to gradually propagate information across different pose parts. Besides, a pose perception loss is introduced based on a pretrained pose estimation network to minimize the pose differences. Extensive qualitative and quantitative experiments conducted on the Human-Art and LAION-Human datasets clearly demonstrate that our model can achieve significant performance improvement over the latest benchmark models. The code is available at \url{<a target="_blank" rel="noopener" href="https://xiangchenyin.github.io/GRPose/%7D">https://xiangchenyin.github.io/GRPose/}</a>. </p>
<blockquote>
<p>è¿‘æœŸä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•åœ¨äººåƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œé€šè¿‡å„ç§æ§åˆ¶ä¿¡å·å¦‚å§¿æ€å…ˆéªŒè¿›è¡Œæ§åˆ¶ã€‚ç„¶è€Œï¼Œç°æœ‰åŠªåŠ›ä»éš¾ä»¥ç”Ÿæˆå…·æœ‰ä¸€è‡´å§¿æ€å¯¹é½çš„é«˜è´¨é‡å›¾åƒï¼Œå¯¼è‡´è¾“å‡ºæ•ˆæœä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ·±å…¥ç ”ç©¶å§¿æ€å…ˆéªŒçš„å›¾å…³ç³»ï¼Œä¸ºäººåƒç”Ÿæˆæä¾›æ§åˆ¶ä¿¡æ¯ã€‚ä¸»è¦æ€æƒ³æ˜¯åœ¨å§¿æ€å…ˆéªŒå’Œæ‰©æ•£æ¨¡å‹çš„æ½œåœ¨è¡¨ç¤ºä¹‹é—´å»ºç«‹å›¾æ‹“æ‰‘ç»“æ„ï¼Œä»¥æ•è·ä¸åŒå§¿æ€éƒ¨åˆ†ä¹‹é—´çš„å†…åœ¨å…³è”ã€‚è®¾è®¡äº†ä¸€ç§æ¸è¿›å¼å›¾å½¢é›†æˆå™¨ï¼ˆPGIï¼‰ï¼Œé‡‡ç”¨é€‚é…å™¨å†…çš„åˆ†å±‚ç­–ç•¥æ¥å­¦ä¹ å§¿æ€å…ˆéªŒçš„ç©ºé—´å…³ç³»ä¸å›¾å½¢ç»“æ„ï¼Œå¹¶é€æ­¥ä¼ æ’­ä¸åŒå§¿æ€éƒ¨åˆ†ä¹‹é—´çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒåŸºäºé¢„è®­ç»ƒçš„å§¿æ€ä¼°è®¡ç½‘ç»œå¼•å…¥äº†ä¸€ç§å§¿æ€æ„ŸçŸ¥æŸå¤±ï¼Œä»¥æœ€å°åŒ–å§¿æ€å·®å¼‚ã€‚åœ¨Human-Artå’ŒLAION-Humanæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®šæ€§å’Œå®šé‡å®éªŒæ¸…æ¥šåœ°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åœ¨æœ€æ–°åŸºå‡†æ¨¡å‹ä¸Šå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://xiangchenyin.github.io/GRPose/">https://xiangchenyin.github.io/GRPose/</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16540v2">PDF</a> Accepted at AAAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå§¿æ€å…ˆéªŒå›¾å…³ç³»çš„æ¡†æ¶ï¼Œç”¨äºæ§åˆ¶äººç±»å›¾åƒç”Ÿæˆã€‚é€šè¿‡å»ºç«‹å§¿æ€å…ˆéªŒå’Œæ‰©æ•£æ¨¡å‹æ½œåœ¨è¡¨ç¤ºä¹‹é—´çš„å›¾æ‹“æ‰‘ç»“æ„ï¼Œæ•æ‰ä¸åŒå§¿æ€éƒ¨ä½ä¹‹é—´çš„å†…åœ¨å…³è”ã€‚è®¾è®¡äº†ä¸€ä¸ªæ¸è¿›å¼å›¾é›†æˆå™¨ï¼ˆPGIï¼‰æ¥å­¦ä¹ å§¿æ€å…ˆéªŒçš„ç©ºé—´å…³ç³»ï¼Œå¹¶é‡‡ç”¨é€‚é…å™¨å†…çš„åˆ†å±‚ç­–ç•¥é€æ­¥ä¼ æ’­ä¸åŒå§¿æ€éƒ¨ä½çš„ä¿¡æ¯ã€‚å¼•å…¥åŸºäºé¢„è®­ç»ƒå§¿æ€ä¼°è®¡ç½‘ç»œçš„å§¿æ€æ„ŸçŸ¥æŸå¤±ï¼Œä»¥æœ€å°åŒ–å§¿æ€å·®å¼‚ã€‚åœ¨Human-Artå’ŒLAION-Humanæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æœ€æ–°åŸºå‡†æ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¸¦æœ‰æ§åˆ¶ä¿¡å·ï¼ˆå¦‚å§¿æ€å…ˆéªŒï¼‰çš„äººç±»å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´ç”Ÿæˆé«˜è´¨é‡ä¸”å§¿æ€ä¸€è‡´çš„å›¾åƒçš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåŸºäºå§¿æ€å…ˆéªŒå›¾å…³ç³»çš„æ¡†æ¶ï¼Œé€šè¿‡æ•æ‰ä¸åŒå§¿æ€éƒ¨ä½ä¹‹é—´çš„å†…åœ¨å…³è”ï¼Œæä¾›å¯¹äººç±»å›¾åƒç”Ÿæˆçš„æ§åˆ¶ä¿¡æ¯ã€‚</li>
<li>è®¾è®¡äº†æ¸è¿›å¼å›¾é›†æˆå™¨ï¼ˆPGIï¼‰æ¥å­¦ä¹ å§¿æ€å…ˆéªŒçš„ç©ºé—´å…³ç³»ï¼Œå¹¶é‡‡ç”¨é€‚é…å™¨å†…çš„åˆ†å±‚ç­–ç•¥é€æ­¥ä¼ æ’­ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥åŸºäºé¢„è®­ç»ƒå§¿æ€ä¼°è®¡ç½‘ç»œçš„å§¿æ€æ„ŸçŸ¥æŸå¤±ï¼Œä»¥æœ€å°åŒ–ç”Ÿæˆçš„å›¾åƒä¸é¢„æœŸå§¿æ€ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†æœ€æ–°çš„åŸºå‡†æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºä»–äººä½¿ç”¨å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b698edad7bc99d8971fa9987d4a8932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfa64093e89f4175905f44ad246177e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-783c329fb593a2c90337d5342f8c2cb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73d430df2dabcf25e3909b0bd3cf80b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f5c34f3c92001b1e00da9a4686f3bcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6981f14b2e2299c3cbc87fb30616c187.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Imagen-3"><a href="#Imagen-3" class="headerlink" title="Imagen 3"></a>Imagen 3</h2><p><strong>Authors: Imagen-Team-Google,  :, Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Lluis Castrejon, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, Hongliang Fei, Nando de Freitas, Yilin Gao, Evgeny Gladchenko, Sergio GÃ³mez Colmenarejo, Mandy Guo, Alex Haig, Will Hawkins, Hexiang Hu, Huilian Huang, Tobenna Peter Igwe, Christos Kaplanis, Siavash Khodadadeh, Yelin Kim, Ksenia Konyushkova, Karol Langner, Eric Lau, Rory Lawton, Shixin Luo, SoÅˆa MokrÃ¡, Henna Nandwani, Yasumasa Onoe, AÃ¤ron van den Oord, Zarana Parekh, Jordi Pont-Tuset, Hang Qi, Rui Qian, Deepak Ramachandran, Poorva Rane, Abdullah Rashwan, Ali Razavi, Robert Riachi, Hansa Srinivasan, Srivatsan Srinivasan, Robin Strudel, Benigno Uria, Oliver Wang, Su Wang, Austin Waters, Chris Wolff, Auriel Wright, Zhisheng Xiao, Hao Xiong, Keyang Xu, Marc van Zee, Junlin Zhang, Katie Zhang, Wenlei Zhou, Konrad Zolna, Ola Aboubakar, Canfer Akbulut, Oscar Akerlund, Isabela Albuquerque, Nina Anderson, Marco Andreetto, Lora Aroyo, Ben Bariach, David Barker, Sherry Ben, Dana Berman, Courtney Biles, Irina Blok, Pankil Botadra, Jenny Brennan, Karla Brown, John Buckley, Rudy Bunel, Elie Bursztein, Christina Butterfield, Ben Caine, Viral Carpenter, Norman Casagrande, Ming-Wei Chang, Solomon Chang, Shamik Chaudhuri, Tony Chen, John Choi, Dmitry Churbanau, Nathan Clement, Matan Cohen, Forrester Cole, Mikhail Dektiarev, Vincent Du, Praneet Dutta, Tom Eccles, Ndidi Elue, Ashley Feden, Shlomi Fruchter, Frankie Garcia, Roopal Garg, Weina Ge, Ahmed Ghazy, Bryant Gipson, Andrew Goodman, Dawid GÃ³rny, Sven Gowal, Khyatti Gupta, Yoni Halpern, Yena Han, Susan Hao, Jamie Hayes, Jonathan Heek, Amir Hertz, Ed Hirst, Emiel Hoogeboom, Tingbo Hou, Heidi Howard, Mohamed Ibrahim, Dirichi Ike-Njoku, Joana Iljazi, Vlad Ionescu, William Isaac, Reena Jana, Gemma Jennings, Donovon Jenson, Xuhui Jia, Kerry Jones, Xiaoen Ju, Ivana Kajic, Christos Kaplanis, Burcu Karagol Ayan, Jacob Kelly, Suraj Kothawade, Christina Kouridi, Ira Ktena, Jolanda Kumakaw, Dana Kurniawan, Dmitry Lagun, Lily Lavitas, Jason Lee, Tao Li, Marco Liang, Maggie Li-Calis, Yuchi Liu, Javier Lopez Alberca, Matthieu Kim Lorrain, Peggy Lu, Kristian Lum, Yukun Ma, Chase Malik, John Mellor, Thomas Mensink, Inbar Mosseri, Tom Murray, Aida Nematzadeh, Paul Nicholas, Signe NÃ¸rly, JoÃ£o Gabriel Oliveira, Guillermo Ortiz-Jimenez, Michela Paganini, Tom Le Paine, Roni Paiss, Alicia Parrish, Anne Peckham, Vikas Peswani, Igor Petrovski, Tobias Pfaff, Alex Pirozhenko, Ryan Poplin, Utsav Prabhu, Yuan Qi, Matthew Rahtz, Cyrus Rashtchian, Charvi Rastogi, Amit Raul, Ali Razavi, Sylvestre-Alvise Rebuffi, Susanna Ricco, Felix Riedel, Dirk Robinson, Pankaj Rohatgi, Bill Rosgen, Sarah Rumbley, Moonkyung Ryu, Anthony Salgado, Tim Salimans, Sahil Singla, Florian Schroff, Candice Schumann, Tanmay Shah, Eleni Shaw, Gregory Shaw, Brendan Shillingford, Kaushik Shivakumar, Dennis Shtatnov, Zach Singer, Evgeny Sluzhaev, Valerii Sokolov, Thibault Sottiaux, Florian Stimberg, Brad Stone, David Stutz, Yu-Chuan Su, Eric Tabellion, Shuai Tang, David Tao, Kurt Thomas, Gregory Thornton, Andeep Toor, Cristian Udrescu, Aayush Upadhyay, Cristina Vasconcelos, Alex Vasiloff, Andrey Voynov, Amanda Walker, Luyu Wang, Miaosen Wang, Simon Wang, Stanley Wang, Qifei Wang, Yuxiao Wang, Ãgoston Weisz, Olivia Wiles, Chenxia Wu, Xingyu Federico Xu, Andrew Xue, Jianbo Yang, Luo Yu, Mete Yurtoglu, Ali Zand, Han Zhang, Jiageng Zhang, Catherine Zhao, Adilet Zhaxybay, Miao Zhou, Shengqi Zhu, Zhenkai Zhu, Dawn Bloxwich, Mahyar Bordbar, Luis C. Cobo, Eli Collins, Shengyang Dai, Tulsee Doshi, Anca Dragan, Douglas Eck, Demis Hassabis, Sissie Hsiao, Tom Hume, Koray Kavukcuoglu, Helen King, Jack Krawczyk, Yeqing Li, Kathy Meier-Hellstern, Andras Orban, Yury Pinsky, Amar Subramanya, Oriol Vinyals, Ting Yu, Yori Zwols</strong></p>
<p>We introduce Imagen 3, a latent diffusion model that generates high quality images from text prompts. We describe our quality and responsibility evaluations. Imagen 3 is preferred over other state-of-the-art (SOTA) models at the time of evaluation. In addition, we discuss issues around safety and representation, as well as methods we used to minimize the potential harm of our models. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Imagen 3ï¼Œè¿™æ˜¯ä¸€ä¸ªä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬æè¿°äº†æˆ‘ä»¬çš„è´¨é‡å’Œè´£ä»»è¯„ä¼°ã€‚åœ¨è¯„ä¼°æ—¶ï¼ŒImagen 3ä¼˜äºå…¶ä»–å½“æ—¶çš„æœ€å…ˆè¿›ï¼ˆSOTAï¼‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†å®‰å…¨å’Œè¡¨å¾æ–¹é¢çš„é—®é¢˜ï¼Œä»¥åŠæˆ‘ä»¬ç”¨æ¥æœ€å°åŒ–æ¨¡å‹æ½œåœ¨å±å®³çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.07009v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬ä»‹ç»äº†Imagen 3ï¼Œä¸€ç§ä»æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬æè¿°äº†å…¶è´¨é‡å’Œè´£ä»»è¯„ä¼°ï¼Œç›¸è¾ƒäºå½“æ—¶çš„å…¶ä»–æœ€å…ˆè¿›æ¨¡å‹ï¼ŒImagen 3æ›´èƒœä¸€ç­¹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿæ¢è®¨äº†å®‰å…¨æ€§å’Œè¡¨å¾ç›¸å…³é—®é¢˜ï¼Œä»¥åŠæˆ‘ä»¬ç”¨äºå‡å°‘æ¨¡å‹æ½œåœ¨å±å®³çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Imagen 3æ˜¯ä¸€ç§èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>Imagen 3åœ¨è´¨é‡è¯„ä¼°ä¸­è¢«è®¤ä¸ºæ˜¯ä¼˜äºå…¶ä»–æœ€å…ˆè¿›æ¨¡å‹çš„ã€‚</li>
<li>è¯¥ç ”ç©¶å¯¹å…¶æ¨¡å‹è¿›è¡Œäº†è´£ä»»è¯„ä¼°ã€‚</li>
<li>å®‰å…¨æ€§æ˜¯æ¨¡å‹åº”ç”¨ä¸­çš„é‡è¦è®®é¢˜ï¼Œå¹¶è®¨è®ºäº†ä¸è¡¨å¾ç›¸å…³çš„é—®é¢˜ã€‚</li>
<li>è¯¥ç ”ç©¶è®¨è®ºäº†å¦‚ä½•æœ€å°åŒ–æ¨¡å‹çš„æ½œåœ¨å±å®³ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿé€šè¿‡æ–‡æœ¬æç¤ºç”Ÿæˆå›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.07009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9fa412f4336c0ac0e859de25937bde43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b6b30c047a9619caea8f89dc0910444.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67a65e2e9f7862f81856abfe1206cf6a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ID-Sculpt-ID-aware-3D-Head-Generation-from-Single-In-the-wild-Portrait-Image"><a href="#ID-Sculpt-ID-aware-3D-Head-Generation-from-Single-In-the-wild-Portrait-Image" class="headerlink" title="ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait   Image"></a>ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait   Image</h2><p><strong>Authors:Jinkun Hao, Junshu Tang, Jiangning Zhang, Ran Yi, Yijia Hong, Moran Li, Weijian Cao, Yating Wang, Chengjie Wang, Lizhuang Ma</strong></p>
<p>While recent works have achieved great success on image-to-3D object generation, high quality and fidelity 3D head generation from a single image remains a great challenge. Previous text-based methods for generating 3D heads were limited by text descriptions and image-based methods struggled to produce high-quality head geometry. To handle this challenging problem, we propose a novel framework, ID-Sculpt, to generate high-quality 3D heads while preserving their identities. Our work incorporates the identity information of the portrait image into three parts: 1) geometry initialization, 2) geometry sculpting, and 3) texture generation stages. Given a reference portrait image, we first align the identity features with text features to realize ID-aware guidance enhancement, which contains the control signals representing the face information. We then use the canny map, ID features of the portrait image, and a pre-trained text-to-normal&#x2F;depth diffusion model to generate ID-aware geometry supervision, and 3D-GAN inversion is employed to generate ID-aware geometry initialization. Furthermore, with the ability to inject identity information into 3D head generation, we use ID-aware guidance to calculate ID-aware Score Distillation (ISD) for geometry sculpting. For texture generation, we adopt the ID Consistent Texture Inpainting and Refinement which progressively expands the view for texture inpainting to obtain an initialization UV texture map. We then use the ID-aware guidance to provide image-level supervision for noisy multi-view images to obtain a refined texture map. Extensive experiments demonstrate that we can generate high-quality 3D heads with accurate geometry and texture from a single in-the-wild portrait image. </p>
<blockquote>
<p>è¿‘æœŸçš„å·¥ä½œåœ¨å›¾åƒåˆ°3Dç‰©ä½“çš„ç”Ÿæˆä¸Šå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡å’Œé«˜ä¿çœŸåº¦çš„3Då¤´åƒä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¹‹å‰åŸºäºæ–‡æœ¬çš„æ–¹æ³•ç”Ÿæˆ3Då¤´åƒå—é™äºæ–‡æœ¬æè¿°ï¼Œè€ŒåŸºäºå›¾åƒçš„æ–¹æ³•å¾ˆéš¾äº§ç”Ÿé«˜è´¨é‡çš„å¤´åƒå‡ ä½•ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ID-Sculptï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„3Då¤´åƒï¼ŒåŒæ—¶ä¿ç•™å…¶èº«ä»½ç‰¹å¾ã€‚æˆ‘ä»¬çš„å·¥ä½œå°†è‚–åƒå›¾åƒçš„èº«ä»½ä¿¡æ¯èå…¥ä¸‰ä¸ªé˜¶æ®µï¼š1ï¼‰å‡ ä½•åˆå§‹åŒ–ï¼Œ2ï¼‰å‡ ä½•é›•å¡‘ï¼Œ3ï¼‰çº¹ç†ç”Ÿæˆã€‚ç»™å®šä¸€ä¸ªå‚è€ƒè‚–åƒå›¾åƒï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡æ–‡æœ¬ç‰¹å¾å¯¹é½èº«ä»½ç‰¹å¾ï¼Œä»¥å®ç°IDæ„ŸçŸ¥æŒ‡å¯¼å¢å¼ºï¼Œå…¶ä¸­åŒ…å«ä»£è¡¨é¢éƒ¨ä¿¡æ¯çš„æ§åˆ¶ä¿¡å·ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨Cannyåœ°å›¾ã€è‚–åƒå›¾åƒçš„èº«ä»½ç‰¹å¾ä»¥åŠé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°æ³•çº¿&#x2F;æ·±åº¦æ‰©æ•£æ¨¡å‹æ¥ç”ŸæˆIDæ„ŸçŸ¥çš„å‡ ä½•ç›‘ç£ï¼Œå¹¶åˆ©ç”¨3D-GANåè½¬æ¥ç”ŸæˆIDæ„ŸçŸ¥çš„å‡ ä½•åˆå§‹åŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡å‘3Då¤´åƒç”Ÿæˆä¸­æ³¨å…¥èº«ä»½ä¿¡æ¯çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä½¿ç”¨IDæ„ŸçŸ¥æŒ‡å¯¼æ¥è®¡ç®—ç”¨äºå‡ ä½•é›•å¡‘çš„IDæ„ŸçŸ¥å¾—åˆ†è’¸é¦ï¼ˆISDï¼‰ã€‚å¯¹äºçº¹ç†ç”Ÿæˆï¼Œæˆ‘ä»¬é‡‡ç”¨èº«ä»½ä¸€è‡´æ€§çº¹ç†ä¿®å¤ä¸ç»†åŒ–æ³•ï¼Œé€æ­¥æ‰©å±•è§†å›¾è¿›è¡Œçº¹ç†ä¿®å¤ï¼Œä»¥è·å¾—åˆå§‹UVçº¹ç†è´´å›¾ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨IDæ„ŸçŸ¥æŒ‡å¯¼ä¸ºå™ªå£°å¤šè§†è§’å›¾åƒæä¾›å›¾åƒçº§ç›‘ç£ï¼Œä»¥è·å¾—ç²¾ç»†çº¹ç†è´´å›¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸€å¼ é‡å¤–çš„è‚–åƒå›¾åƒç”Ÿæˆé«˜è´¨é‡ã€å‡ ä½•å’Œçº¹ç†å‡†ç¡®çš„3Då¤´åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16710v3">PDF</a> Accepted by AAAI 2025; Project page:   <a target="_blank" rel="noopener" href="https://jinkun-hao.github.io/ID-Sculpt/">https://jinkun-hao.github.io/ID-Sculpt/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ID-Sculptï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ä¸”ä¿æŒèº«ä»½ä¿¡æ¯çš„3Då¤´åƒã€‚è¯¥æ¡†æ¶å°†è‚–åƒå›¾åƒçš„èº«ä»½ä¿¡æ¯èå…¥ä¸‰ä¸ªé˜¶æ®µï¼šå‡ ä½•åˆå§‹åŒ–ã€å‡ ä½•é›•å¡‘å’Œçº¹ç†ç”Ÿæˆã€‚é€šè¿‡èº«ä»½æ„ŸçŸ¥æŒ‡å¯¼å¢å¼ºå’ŒIDæ„ŸçŸ¥å‡ ä½•ç›‘ç£ï¼Œå®ç°IDæ„ŸçŸ¥å‡ ä½•åˆå§‹åŒ–ã€‚åˆ©ç”¨3D-GANåæ¼”è¿›è¡Œå‡ ä½•é›•å¡‘çš„IDæ„ŸçŸ¥è¯„åˆ†è’¸é¦ï¼Œå¹¶é‡‡ç”¨IDä¸€è‡´çº¹ç†ä¿®å¤ä¸ç»†åŒ–ï¼Œé€æ­¥æ‰©å±•è§†å›¾è¿›è¡Œçº¹ç†å¡«å……ï¼Œè·å¾—åˆå§‹UVçº¹ç†è´´å›¾ï¼Œè¿›è€Œåœ¨å™ªå£°å¤šè§’åº¦å›¾åƒä¸Šæä¾›IDæ„ŸçŸ¥æŒ‡å¯¼ï¼Œå¾—åˆ°ç²¾ç»†çº¹ç†è´´å›¾ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»å•å¼ é‡å¤–è‚–åƒå›¾åƒç”Ÿæˆé«˜è´¨é‡ã€å‡†ç¡®çš„3Då¤´åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ID-Sculptï¼Œæ—¨åœ¨è§£å†³ä»å•å¼ å›¾åƒç”Ÿæˆé«˜è´¨é‡3Då¤´åƒçš„æŒ‘æˆ˜ã€‚</li>
<li>æ¡†æ¶å°†èº«ä»½ä¿¡æ¯å…±äº«åˆ°å‡ ä½•åˆå§‹åŒ–ã€å‡ ä½•é›•å¡‘å’Œçº¹ç†ç”Ÿæˆä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>é€šè¿‡èº«ä»½æ„ŸçŸ¥æŒ‡å¯¼å¢å¼ºå’ŒIDæ„ŸçŸ¥å‡ ä½•ç›‘ç£å®ç°IDæ„ŸçŸ¥å‡ ä½•åˆå§‹åŒ–ã€‚</li>
<li>åˆ©ç”¨3D-GANåæ¼”è¿›è¡Œå‡ ä½•é›•å¡‘çš„IDæ„ŸçŸ¥è¯„åˆ†è’¸é¦ã€‚</li>
<li>é‡‡ç”¨IDä¸€è‡´çº¹ç†ä¿®å¤ä¸ç»†åŒ–ï¼Œé€æ­¥æ‰©å±•è§†å›¾è¿›è¡Œçº¹ç†å¡«å……ï¼Œè·å¾—é«˜è´¨é‡çš„çº¹ç†è´´å›¾ã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿä»å•å¼ é‡å¤–è‚–åƒå›¾åƒç”Ÿæˆå‡†ç¡®ä¸”é€¼çœŸçš„3Då¤´åƒã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒéªŒè¯äº†æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e392fa954faf62ba18cfdb28b4bd3b59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c16ea30d0c3612840684653ecc9e653.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-304dd237598c4c95fac0e6a7a6bcabe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42b4bb5107fd5bf1b41a3707a2f0fa03.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Magic-Boost-Boost-3D-Generation-with-Mutli-View-Conditioned-Diffusion"><a href="#Magic-Boost-Boost-3D-Generation-with-Mutli-View-Conditioned-Diffusion" class="headerlink" title="Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion"></a>Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion</h2><p><strong>Authors:Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang, Huichao Zhang, Xiaofeng Yang, Xiu Li, Jiashi Feng, Guosheng Lin</strong></p>
<p>Benefiting from the rapid development of 2D diffusion models, 3D content generation has witnessed significant progress. One promising solution is to finetune the pre-trained 2D diffusion models to produce multi-view images and then reconstruct them into 3D assets via feed-forward sparse-view reconstruction models. However, limited by the 3D inconsistency in the generated multi-view images and the low reconstruction resolution of the feed-forward reconstruction models, the generated 3d assets are still limited to incorrect geometries and blurry textures. To address this problem, we present a multi-view based refine method, named Magic-Boost, to further refine the generation results. In detail, we first propose a novel multi-view conditioned diffusion model which extracts 3d prior from the synthesized multi-view images to synthesize high-fidelity novel view images and then introduce a novel iterative-update strategy to adopt it to provide precise guidance to refine the coarse generated results through a fast optimization process. Conditioned on the strong 3d priors extracted from the synthesized multi-view images, Magic-Boost is capable of providing precise optimization guidance that well aligns with the coarse generated 3D assets, enriching the local detail in both geometry and texture within a short time ($\sim15$min). Extensive experiments show Magic-Boost greatly enhances the coarse generated inputs, generates high-quality 3D assets with rich geometric and textural details. (Project Page: <a target="_blank" rel="noopener" href="https://magic-research.github.io/magic-boost/">https://magic-research.github.io/magic-boost/</a>) </p>
<blockquote>
<p>å¾—ç›Šäº2Dæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œ3Då†…å®¹ç”Ÿæˆå·²ç»å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¤šè§†è§’å›¾åƒï¼Œç„¶åé€šè¿‡å‰é¦ˆç¨€ç–è§†å›¾é‡å»ºæ¨¡å‹å°†å®ƒä»¬é‡å»ºä¸º3Dèµ„äº§ã€‚ç„¶è€Œï¼Œç”±äºç”Ÿæˆçš„å¤šè§†è§’å›¾åƒä¸­çš„3Dä¸ä¸€è‡´æ€§å’Œå‰é¦ˆé‡å»ºæ¨¡å‹çš„ä½é‡å»ºåˆ†è¾¨ç‡ï¼Œç”Ÿæˆçš„3Dèµ„äº§ä»ç„¶å—é™äºä¸æ­£ç¡®çš„å‡ ä½•å½¢çŠ¶å’Œæ¨¡ç³Šçš„çº¹ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šè§†è§’çš„ç»†åŒ–æ–¹æ³•ï¼Œåä¸ºMagic-Boostï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–ç”Ÿæˆç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šè§†è§’æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»åˆæˆçš„å¤šè§†è§’å›¾åƒä¸­æå–3Då…ˆéªŒçŸ¥è¯†ï¼Œåˆæˆé«˜ä¿çœŸåº¦çš„æ–°è§†è§’å›¾åƒï¼Œç„¶åå¼•å…¥äº†ä¸€ç§æ–°å‹è¿­ä»£æ›´æ–°ç­–ç•¥ï¼Œå°†å…¶åº”ç”¨äºä¸ºç²—ç•¥ç”Ÿæˆçš„ç»“æœæä¾›ç²¾ç¡®æŒ‡å¯¼ï¼Œé€šè¿‡å¿«é€Ÿä¼˜åŒ–è¿‡ç¨‹è¿›è¡Œç»†åŒ–ã€‚åŸºäºä»åˆæˆå¤šè§†è§’å›¾åƒä¸­æå–çš„å¼ºå¤§3Då…ˆéªŒçŸ¥è¯†ï¼ŒMagic-Boostèƒ½å¤Ÿæä¾›ä¸ç²—ç•¥ç”Ÿæˆçš„3Dèµ„äº§é«˜åº¦åŒ¹é…çš„ç²¾ç¡®ä¼˜åŒ–æŒ‡å¯¼ï¼Œåœ¨çŸ­æ—¶é—´å†…ï¼ˆ~15åˆ†é’Ÿï¼‰ä¸°å¯Œå‡ ä½•å’Œçº¹ç†çš„å±€éƒ¨ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMagic-Boostæå¤§åœ°æå‡äº†ç²—ç•¥ç”Ÿæˆçš„è¾“å…¥ï¼Œç”Ÿæˆäº†é«˜è´¨é‡ã€å…·æœ‰ä¸°å¯Œå‡ ä½•å’Œçº¹ç†ç»†èŠ‚çš„3Dèµ„äº§ã€‚ï¼ˆé¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://magic-research.github.io/magic-boost/%EF%BC%89">https://magic-research.github.io/magic-boost/ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.06429v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¾—ç›ŠäºäºŒç»´æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œä¸‰ç»´å†…å®¹ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¤šè§†å›¾å›¾åƒï¼Œç„¶åé€šè¿‡å‰é¦ˆç¨€ç–è§†å›¾é‡å»ºæ¨¡å‹å°†å…¶é‡å»ºä¸ºä¸‰ç»´èµ„äº§ã€‚ç„¶è€Œï¼Œç”±äºç”Ÿæˆçš„å¤šè§†å›¾å›¾åƒä¸­çš„ä¸‰ç»´ä¸ä¸€è‡´æ€§å’Œå‰é¦ˆé‡å»ºæ¨¡å‹çš„é‡æ„åˆ†è¾¨ç‡è¾ƒä½ï¼Œç”Ÿæˆçš„3Dèµ„äº§ä»ç„¶å­˜åœ¨å‡ ä½•ä¸æ­£ç¡®å’Œçº¹ç†æ¨¡ç³Šçš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šè§†å›¾çš„ç²¾ç»†æ–¹æ³•ï¼Œåä¸ºMagic-Boostï¼Œä»¥è¿›ä¸€æ­¥æ”¹è¿›ç”Ÿæˆç»“æœã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºä¸€ç§æ–°å‹çš„å¤šè§†å›¾æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä»åˆæˆçš„å¤šè§†å›¾å›¾åƒä¸­æå–ä¸‰ç»´å…ˆéªŒçŸ¥è¯†æ¥åˆæˆé«˜ä¿çœŸåº¦çš„æ–°è§†å›¾å›¾åƒï¼Œç„¶åå¼•å…¥ä¸€ç§æ–°å‹è¿­ä»£æ›´æ–°ç­–ç•¥æ¥é€‚åº”å®ƒï¼Œä»¥é€šè¿‡å¿«é€Ÿä¼˜åŒ–è¿‡ç¨‹ä¸ºç²—ç³™çš„ç”Ÿæˆç»“æœæä¾›ç²¾ç¡®çš„æŒ‡å¯¼ã€‚åŸºäºä»åˆæˆå¤šè§†å›¾å›¾åƒä¸­æå–çš„å¼ºå¤§ä¸‰ç»´å…ˆéªŒçŸ¥è¯†ï¼ŒMagic-Boostèƒ½å¤Ÿæä¾›ä¸ç²—ç³™ç”Ÿæˆçš„3Dèµ„äº§é«˜åº¦å¯¹é½çš„ç²¾ç¡®ä¼˜åŒ–æŒ‡å¯¼ï¼ŒçŸ­æ—¶é—´å†…ï¼ˆ~15åˆ†é’Ÿï¼‰ä¸°å¯Œäº†å‡ ä½•å’Œçº¹ç†çš„å±€éƒ¨ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMagic-Boostæå¤§åœ°æé«˜äº†ç²—ç•¥ç”Ÿæˆçš„è¾“å…¥ï¼Œç”Ÿæˆäº†é«˜è´¨é‡ã€å…·æœ‰ä¸°å¯Œå‡ ä½•å’Œçº¹ç†ç»†èŠ‚çš„ä¸‰ç»´èµ„äº§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>äºŒç»´æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†ä¸‰ç»´å†…å®¹ç”Ÿæˆé¢†åŸŸçš„è¿›æ­¥ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¤šè§†å›¾å›¾åƒæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚</li>
<li>ç”Ÿæˆçš„å¤šè§†å›¾å›¾åƒä¸­çš„ä¸‰ç»´ä¸ä¸€è‡´æ€§å’Œå‰é¦ˆé‡å»ºæ¨¡å‹çš„é‡æ„åˆ†è¾¨ç‡é™åˆ¶æ˜¯ç°æœ‰æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>Magic-Boostæ˜¯ä¸€ç§åŸºäºå¤šè§†å›¾çš„ç²¾ç»†æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›ç”Ÿæˆç»“æœã€‚</li>
<li>Magic-Booståˆ©ç”¨å¤šè§†å›¾æ¡ä»¶æ‰©æ•£æ¨¡å‹ä»åˆæˆçš„å¤šè§†å›¾å›¾åƒä¸­æå–ä¸‰ç»´å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>Magic-Boosté‡‡ç”¨è¿­ä»£æ›´æ–°ç­–ç•¥æ¥ç²¾ç¡®ä¼˜åŒ–ç²—ç³™ç”Ÿæˆçš„3Dèµ„äº§ã€‚</li>
<li>Magic-Boostèƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´èµ„äº§ï¼Œå…·æœ‰ä¸°å¯Œå‡ ä½•å’Œçº¹ç†ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.06429">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48c693086a6d06d6a3d17ec15be038f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c42c826143996d4d401b6904c54cbe66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8988b07e744eb3dc19c9fc90f6a974.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a374cf29144fd1c01be3d35c827197a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f0f33a094a9658ff5689cf67fb9df83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e9340ded73d0a52eed397d9a332c8ff.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a42eaf6bb487e03d164d45d2c35b4a5c.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  Assessment of Deep-Learning Methods for the Enhancement of Experimental   Low Dose Dental CBCT Volumes
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-707173a3a7fc763b3069420ce0526011.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  Editing Implicit and Explicit Representations of Radiance Fields A   Survey
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17196.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
