<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  Token Statistics Transformer Linear-Time Attention via Variational Rate   Reduction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-283b0d3aeaa210624fb80d95d900a582.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-25-æ›´æ–°"><a href="#2024-12-25-æ›´æ–°" class="headerlink" title="2024-12-25 æ›´æ–°"></a>2024-12-25 æ›´æ–°</h1><h2 id="Token-Statistics-Transformer-Linear-Time-Attention-via-Variational-Rate-Reduction"><a href="#Token-Statistics-Transformer-Linear-Time-Attention-via-Variational-Rate-Reduction" class="headerlink" title="Token Statistics Transformer: Linear-Time Attention via Variational Rate   Reduction"></a>Token Statistics Transformer: Linear-Time Attention via Variational Rate   Reduction</h2><p><strong>Authors:Ziyang Wu, Tianjiao Ding, Yifu Lu, Druv Pai, Jingyuan Zhang, Weida Wang, Yaodong Yu, Yi Ma, Benjamin D. Haeffele</strong></p>
<p>The attention operator is arguably the key distinguishing factor of transformer architectures, which have demonstrated state-of-the-art performance on a variety of tasks. However, transformer attention operators often impose a significant computational burden, with the computational complexity scaling quadratically with the number of tokens. In this work, we propose a novel transformer attention operator whose computational complexity scales linearly with the number of tokens. We derive our network architecture by extending prior work which has shown that a transformer style architecture naturally arises by â€œwhite-boxâ€ architecture design, where each layer of the network is designed to implement an incremental optimization step of a maximal coding rate reduction objective (MCR$^2$). Specifically, we derive a novel variational form of the MCR$^2$ objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention (TSSA). TSSA has linear computational and memory complexity and radically departs from the typical attention architecture that computes pairwise similarities between tokens. Experiments on vision, language, and long sequence tasks show that simply swapping TSSA for standard self-attention, which we refer to as the Token Statistics Transformer (ToST), achieves competitive performance with conventional transformers while being significantly more computationally efficient and interpretable. Our results also somewhat call into question the conventional wisdom that pairwise similarity style attention mechanisms are critical to the success of transformer architectures. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/RobinWu218/ToST">https://github.com/RobinWu218/ToST</a>. </p>
<blockquote>
<p>æ³¨æ„åŠ›æ“ä½œå¯ä»¥è¯´æ˜¯åŒºåˆ†å˜å‹å™¨æ¶æ„çš„å…³é”®è¦ç´ ï¼Œè¿™ç§æ¶æ„åœ¨å„ç§ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå˜å‹å™¨æ³¨æ„åŠ›æ“ä½œé€šå¸¸ä¼šå¸¦æ¥å¾ˆå¤§çš„è®¡ç®—è´Ÿæ‹…ï¼Œå…¶è®¡ç®—å¤æ‚åº¦éšä»¤ç‰Œæ•°é‡çš„å¢åŠ è€Œå‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹å˜å‹å™¨æ³¨æ„åŠ›æ“ä½œï¼Œå…¶è®¡ç®—å¤æ‚åº¦éšä»¤ç‰Œæ•°é‡çº¿æ€§å¢é•¿ã€‚æˆ‘ä»¬é€šè¿‡æ‰©å±•å…ˆå‰çš„å·¥ä½œæ¥æ¨å¯¼æˆ‘ä»¬çš„ç½‘ç»œæ¶æ„ï¼Œå…ˆå‰çš„å·¥ä½œæ˜¾ç¤ºï¼Œé€šè¿‡â€œç™½ç›’â€æ¶æ„è®¾è®¡è‡ªç„¶åœ°äº§ç”Ÿäº†å˜å‹å™¨é£æ ¼çš„æ¶æ„ï¼Œè¯¥æ¶æ„çš„æ¯ä¸€å±‚éƒ½è¢«è®¾è®¡æˆå®ç°æœ€å¤§ç¼–ç ç‡é™ä½ç›®æ ‡ï¼ˆMCR$^2$ï¼‰çš„å¢é‡ä¼˜åŒ–æ­¥éª¤ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¨å¯¼äº†MCR$^2$ç›®æ ‡çš„æ–°å˜åˆ†å½¢å¼ï¼Œå¹¶è¡¨æ˜ç”±æ­¤å˜åˆ†ç›®æ ‡çš„å±•å¼€æ¢¯åº¦ä¸‹é™æ‰€å¾—åˆ°çš„æ¶æ„ä¼šå¯¼è‡´ä¸€ç§æ–°çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œç§°ä¸ºToken Statistics Self-Attentionï¼ˆTSSAï¼‰ã€‚TSSAå…·æœ‰çº¿æ€§çš„è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦ï¼Œä¸è®¡ç®—ä»¤ç‰Œä¹‹é—´æˆå¯¹ç›¸ä¼¼æ€§çš„å…¸å‹æ³¨æ„åŠ›æ¶æ„æœ‰å¾ˆå¤§çš„ä¸åŒã€‚åœ¨è§†è§‰ã€è¯­è¨€å’Œé•¿åºåˆ—ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå°†TSSAæ›¿æ¢ä¸ºæ ‡å‡†è‡ªæ³¨æ„åŠ›ï¼ˆæˆ‘ä»¬å°†å…¶ç§°ä¸ºToken Statistics Transformerï¼ˆToSTï¼‰ï¼‰å³å¯å®ç°ä¸å¸¸è§„å˜å‹å™¨ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡æ›´é«˜ã€æ›´å…·å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç»“æœä¹Ÿå¯¹ä¸€ä¸ªæ™®éçš„è§‚å¿µæå‡ºäº†è´¨ç–‘ï¼Œå³æˆå¯¹ç›¸ä¼¼æ€§é£æ ¼çš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯å˜å‹å™¨æ¶æ„æˆåŠŸçš„å…³é”®ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/RobinWu218/ToST%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/RobinWu218/ToSTä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17810v1">PDF</a> 24 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹å˜å‹å™¨æ³¨æ„åŠ›æ“ä½œå™¨ï¼Œå…¶è®¡ç®—å¤æ‚åº¦éšä»¤ç‰Œæ•°é‡çš„å¢åŠ è€Œçº¿æ€§å¢é•¿ï¼Œè§£å†³äº†ä¼ ç»Ÿå˜å‹å™¨æ³¨æ„åŠ›æ“ä½œå™¨è®¡ç®—è´Ÿæ‹…è¾ƒé‡çš„é—®é¢˜ã€‚é€šè¿‡æ‰©å±•â€œç™½ç›’â€æ¶æ„è®¾è®¡å…ˆå‰çš„ä½œå“ï¼Œæˆ‘ä»¬æ´¾ç”Ÿäº†ä¸€ç§æ–°çš„ç½‘ç»œæ¶æ„ï¼Œè¯¥æ¶æ„æ—¨åœ¨å®ç°æœ€å¤§ç¼–ç ç‡é™ä½ç›®æ ‡ï¼ˆMCRÂ²ï¼‰çš„å¢é‡ä¼˜åŒ–æ­¥éª¤ã€‚ç”±æ­¤è¡ç”Ÿå‡ºåä¸ºToken Statistics Self-Attentionï¼ˆTSSAï¼‰çš„æ–°å‹æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ƒåœ¨è§†è§‰ã€è¯­è¨€å’Œé•¿åºåˆ—ä»»åŠ¡ä¸Šçš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶åœ¨è®¡ç®—å’Œè§£é‡Šæ€§æ–¹é¢æ›´åŠ é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹å˜å‹å™¨æ³¨æ„åŠ›æ“ä½œå™¨å…·æœ‰çº¿æ€§è®¡ç®—å¤æ‚åº¦ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ“ä½œå™¨ï¼Œèƒ½æ˜¾è‘—é™ä½è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>è¯¥ç ”ç©¶é€šè¿‡â€œç™½ç›’â€æ¶æ„è®¾è®¡æ‰©å±•äº†å…ˆå‰çš„ä½œå“ï¼Œå®ç°äº†ä¸€ç§æ–°çš„ç½‘ç»œæ¶æ„ã€‚</li>
<li>æ–°å‹ç½‘ç»œæ¶æ„æ—¨åœ¨å®ç°æœ€å¤§ç¼–ç ç‡é™ä½ç›®æ ‡ï¼ˆMCRÂ²ï¼‰çš„å¢é‡ä¼˜åŒ–æ­¥éª¤ã€‚</li>
<li>æ´¾ç”Ÿå‡ºçš„Token Statistics Self-Attentionï¼ˆTSSAï¼‰æ¨¡å—åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>TSSAä¸åŒäºä¼ ç»Ÿçš„è®¡ç®—ä»¤ç‰Œé—´ç›¸ä¼¼æ€§çš„æ³¨æ„åŠ›æ¶æ„ã€‚</li>
<li>ç”¨Token Statistics Transformerï¼ˆToSTï¼‰æ›¿æ¢æ ‡å‡†è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½åœ¨ä¿æŒç«äº‰åŠ›çš„åŒæ—¶ï¼Œå®ç°æ›´é«˜çš„è®¡ç®—æ•ˆç‡å’Œè§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f6e32040aba92b0c76da2047e7a2481.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-80f8ba3155ebc4639c35a30b5120e2dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-950ed595b7197fd821001f35c89cee10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0784605ba5fed2abe7b5769f86e96c3f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="YuLan-Mini-An-Open-Data-efficient-Language-Model"><a href="#YuLan-Mini-An-Open-Data-efficient-Language-Model" class="headerlink" title="YuLan-Mini: An Open Data-efficient Language Model"></a>YuLan-Mini: An Open Data-efficient Language Model</h2><p><strong>Authors:Yiwen Hu, Huatong Song, Jia Deng, Jiapeng Wang, Jie Chen, Kun Zhou, Yutao Zhu, Jinhao Jiang, Zican Dong, Wayne Xin Zhao, Ji-Rong Wen</strong></p>
<p>Effective pre-training of large language models (LLMs) has been challenging due to the immense resource demands and the complexity of the technical processes involved. This paper presents a detailed technical report on YuLan-Mini, a highly capable base model with 2.42B parameters that achieves top-tier performance among models of similar parameter scale. Our pre-training approach focuses on enhancing training efficacy through three key technical contributions: an elaborate data pipeline combines data cleaning with data schedule strategies, a robust optimization method to mitigate training instability, and an effective annealing approach that incorporates targeted data selection and long context training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves performance comparable to industry-leading models that require significantly more data. To facilitate reproduction, we release the full details of the data composition for each training phase. Project details can be accessed at the following link: <a target="_blank" rel="noopener" href="https://github.com/RUC-GSAI/YuLan-Mini">https://github.com/RUC-GSAI/YuLan-Mini</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆé¢„è®­ç»ƒä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå…¶å·¨å¤§çš„èµ„æºéœ€æ±‚å’ŒæŠ€æœ¯è¿‡ç¨‹çš„å¤æ‚æ€§ã€‚æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†YuLan-Miniçš„æŠ€æœ¯æŠ¥å‘Šï¼ŒYuLan-Miniæ˜¯ä¸€ä¸ªå…·æœ‰2.42äº¿å‚æ•°çš„åŸºç¡€æ¨¡å‹ï¼Œåœ¨åŒç±»å‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸­è¾¾åˆ°äº†ä¸€æµçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒæ–¹æ³•é€šè¿‡ä¸‰ä¸ªä¸»è¦æŠ€æœ¯è´¡çŒ®æ¥æé«˜è®­ç»ƒæ•ˆæœï¼šç²¾å¿ƒè®¾è®¡çš„æ•°æ®ç®¡é“ç»“åˆäº†æ•°æ®æ¸…ç†å’Œæ•°æ®è°ƒåº¦ç­–ç•¥ï¼Œä¸€ç§ç¨³å¥çš„ä¼˜åŒ–æ–¹æ³•æ¥å‡è½»è®­ç»ƒçš„ä¸ç¨³å®šæ€§ï¼Œä»¥åŠæœ‰æ•ˆçš„é€€ç«æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ç›®æ ‡æ•°æ®é€‰æ‹©å’Œé•¿ä¸Šä¸‹æ–‡è®­ç»ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒYuLan-Miniåœ¨1.08ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶æ€§èƒ½ä¸éœ€è¦å¤§é‡æ•°æ®çš„è¡Œä¸šé¢†å…ˆæ¨¡å‹ç›¸å½“ã€‚ä¸ºäº†æ–¹ä¾¿å¤åˆ¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æ¯ä¸ªè®­ç»ƒé˜¶æ®µæ•°æ®ç»„æˆçš„å®Œæ•´ç»†èŠ‚ã€‚é¡¹ç›®è¯¦æƒ…å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/RUC-GSAI/YuLan-Mini%E3%80%82">https://github.com/RUC-GSAI/YuLan-Miniã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17743v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>YuLan-Miniæ˜¯ä¸€ä¸ªæ‹¥æœ‰å¼ºå¤§æ€§èƒ½çš„åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰2.42äº¿å‚æ•°ï¼Œåœ¨ç±»ä¼¼å‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ã€‚å…¶é¢„è®­ç»ƒç­–ç•¥é€šè¿‡ä¸‰ä¸ªä¸»è¦æŠ€æœ¯è´¡çŒ®æé«˜è®­ç»ƒæ•ˆç‡ï¼šç²¾å¿ƒè®¾è®¡çš„ç»“åˆæ•°æ®æ¸…æ´—ä¸æ•°æ®è°ƒåº¦ç­–ç•¥çš„æ•°æ®ç®¡é“ã€ç¨³å¥çš„ä¼˜åŒ–æ–¹æ³•ä»¥è§£å†³è®­ç»ƒä¸ç¨³å®šé—®é¢˜ï¼Œä»¥åŠæœ‰æ•ˆé€€ç«ç­–ç•¥è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æ•°æ®é€‰æ‹©å’Œé•¿è¯­å¢ƒè®­ç»ƒã€‚YuLan-Miniä»…é€šè¿‡è®­ç»ƒåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šï¼ˆä»…ä½¿ç”¨çº¦1.08ä¸‡äº¿æ ‡è®°ï¼‰å°±èƒ½å®ç°ä¸è¡Œä¸šé¢†å…ˆæ¨¡å‹çš„æ€§èƒ½ç›¸å½“ã€‚æˆ‘ä»¬å…¬å¼€äº†æ¯ä¸ªè®­ç»ƒé˜¶æ®µçš„æ•°æ®ç»„æˆç»†èŠ‚ï¼Œä»¥ä¾¿èƒ½å¤Ÿé‡æ–°è¯•éªŒå¹¶ç¡®è®¤å…¶æ€§èƒ½è¡¨ç°ã€‚è¯¦ç»†æƒ…å†µè¯·å‚è§é¡¹ç›®ç½‘å€ï¼šXXXï¼ˆå°†é¡¹ç›®çš„GitHubåœ°å€æ’å…¥XXXå¤„ï¼‰ã€‚æœ‰å…³YuLan-Miniæ¨¡å‹æ›´è¯¦ç»†çš„æ€§èƒ½åˆ†æè¯·å‚è§æˆ‘ä»¬çš„GitHubé¡µé¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ol>
<li>YuLan-Miniæ¨¡å‹æ˜¯ä¸€æ¬¾å…·å¤‡é«˜èƒ½åŠ›çš„åŸºç¡€æ¨¡å‹ï¼Œåœ¨ç›¸åŒå‚æ•°è§„æ¨¡çš„æ¨¡å‹ä¸­è¡¨ç°å‡ºé¡¶çº§æ€§èƒ½ã€‚è¯¥æ¨¡å‹å®ç°äº†ä¼˜åŒ–çš„é¢„è®­ç»ƒè¿‡ç¨‹ã€‚å®ƒçš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºåªéœ€å°‘é‡çš„è®¡ç®—èµ„æºä¾¿èƒ½å±•ç°å‡ºå‡ºè‰²çš„æ€§èƒ½è¡¨ç°ã€‚è¯¦ç»†æƒ…å†µå¯é€šè¿‡ä»¥ä¸‹é“¾æ¥æŸ¥çœ‹ï¼šXXXï¼ˆå¡«å…¥GitHubåœ°å€ï¼‰ã€‚ </li>
<li>æ•°æ®ç®¡é“è®¾è®¡æ˜¯é¢„è®­ç»ƒæˆåŠŸçš„å…³é”®ä¹‹ä¸€ï¼Œç»“åˆäº†æ•°æ®æ¸…æ´—å’Œæ•°æ®è°ƒåº¦ç­–ç•¥ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚è¿™ä¸€ç­–ç•¥å¯¹æ•°æ®çš„å¤„ç†å’Œå®‰æ’æå‡äº†æ¨¡å‹åœ¨é¢ä¸´å„ç§å®é™…æƒ…å¢ƒæ—¶çš„çµæ´»æ€§å’Œé€‚åº”èƒ½åŠ›ã€‚å®ƒèƒ½å¤Ÿåœ¨å¤æ‚ä¸”ä¸æ–­å˜åŒ–çš„è¾“å…¥æ•°æ®é¢å‰ä¿æŒé«˜æ•ˆçš„è®­ç»ƒçŠ¶æ€ã€‚æ­¤éƒ¨åˆ†çš„ç»†èŠ‚è¢«è¯¦ç»†æè¿°å¹¶å…¬å¼€åˆ†äº«ä»¥ä¾›å­¦ä¹ å’Œå€Ÿé‰´ã€‚å› æ­¤æœ‰å……è¶³çš„å¯é‡å¤å®ç°çš„å¯èƒ½æ€§å’Œé«˜åº¦æ”¹è¿›çš„æ½œåŠ›ç©ºé—´ã€‚æˆ‘ä»¬åœ¨å…¬å¼€ç½‘ç«™ä¸Šå…¬å¸ƒäº†è®­ç»ƒæ•°æ®é›†çš„å…¨è²Œä¾›åç»­ç ”ç©¶å’Œåˆ›æ–°ç”¨é€”çš„æ•°æ®çš„åˆ†äº«å¹³å°ã€‚ </li>
<li>æ¨¡å‹é‡‡ç”¨äº†ç¨³å¥çš„ä¼˜åŒ–æ–¹æ³•æ¥è§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸ç¨³å®šé—®é¢˜ï¼Œè¿™æœ‰åŠ©äºç¡®ä¿æ¨¡å‹åœ¨å„ç§æƒ…å†µä¸‹éƒ½èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½è¡¨ç°ã€‚é€šè¿‡ä¼˜åŒ–ç®—æ³•çš„ç¨³å®šæ€§å’Œå¯é æ€§ï¼Œæ¨¡å‹åœ¨é¢ä¸´å¤æ‚ä»»åŠ¡æ—¶èƒ½å¤Ÿå±•ç°å‡ºæ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å…·ä½“æ–¹æ³•å·²è¢«è¯¦ç»†æè¿°å¹¶åœ¨å…¬å¼€ç½‘ç«™ä¸Šå…¬å¸ƒä»¥ä¾›å‚è€ƒå’Œæ”¹è¿›å­¦ä¹ ç›®çš„ç­‰ç»†èŠ‚å·²å®Œå…¨å…¬å¼€å¹¶å¯ä»¥åœ¨æŒ‡å®šç½‘ç«™ä¸Šè¿›è¡ŒæŸ¥é˜…ã€‚è¿™ä¸ºæœªæ¥çš„ç ”ç©¶è€…æä¾›äº†é‡è¦çš„å­¦ä¹ æœºä¼šå’ŒæŒ‘æˆ˜æ–°çš„ä¼˜åŒ–ç­–ç•¥çš„å¥‘æœºä»¥ä¾¿ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒæ‰¾åˆ°æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆåŒæ—¶ä¿æŒæ¨¡å‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚åŒæ—¶è¿™ä¸€ä¸¾æªä¹Ÿä¿ƒè¿›äº†è¡Œä¸šå†…çš„çŸ¥è¯†å…±äº«å’Œåˆä½œæ¨åŠ¨è¯­è¨€æ¨¡å‹æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œå‘å±•ã€‚æ­¤å¤–æˆ‘ä»¬å…¬å¼€äº†å…·ä½“çš„ä¼˜åŒ–æ–¹æ³•ç»†èŠ‚ä»¥ä¾¿å…¶ä»–ç ”ç©¶è€…èƒ½å¤Ÿåœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›å’Œåˆ›æ–°ä»¥æ¨åŠ¨è¯­è¨€æ¨¡å‹æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3365ad4e093f3420f7446da5df9d6fbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-648f7d936b60a691ee1c5522c25faae4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc60d420c15664de124cc051aafab70f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-283b0d3aeaa210624fb80d95d900a582.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6d09a28a0e16e485695d99e279bed65.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Knowledge-Editing-through-Chain-of-Thought"><a href="#Knowledge-Editing-through-Chain-of-Thought" class="headerlink" title="Knowledge Editing through Chain-of-Thought"></a>Knowledge Editing through Chain-of-Thought</h2><p><strong>Authors:Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated exceptional capabilities across a wide range of natural language processing (NLP) tasks. However, keeping these models up-to-date with evolving world knowledge remains a significant challenge due to the high costs of frequent retraining. To address this challenge, knowledge editing techniques have emerged to update LLMs with new information without rebuilding the model from scratch. Among these, the in-context editing paradigm stands out for its effectiveness in integrating new knowledge while preserving the modelâ€™s original capabilities. Despite its potential, existing in-context knowledge editing methods are often task-specific, focusing primarily on multi-hop QA tasks using structured knowledge triples. Moreover, their reliance on few-shot prompting for task decomposition makes them unstable and less effective in generalizing across diverse tasks.   In response to these limitations, we propose EditCoT, a novel knowledge editing framework that flexibly and efficiently updates LLMs across various tasks without retraining. EditCoT works by generating a chain-of-thought (CoT) for a given input and then iteratively refining this CoT process using a CoT editor based on updated knowledge. We evaluate EditCoT across a diverse range of benchmarks, covering multiple languages and tasks. The results demonstrate that our approach achieves state-of-the-art performance while offering superior generalization, effectiveness, and stability compared to existing methods, marking a significant advancement in the field of knowledge updating. Code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/bebr2/EditCoT">https://github.com/bebr2/EditCoT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºæ›´æ–°ä¸–ç•ŒçŸ¥è¯†çš„æˆæœ¬é«˜æ˜‚ï¼Œä¿æŒè¿™äº›æ¨¡å‹çš„æœ€æ–°çŠ¶æ€ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼ŒçŸ¥è¯†ç¼–è¾‘æŠ€æœ¯åº”è¿è€Œç”Ÿï¼Œå¯ä»¥åœ¨ä¸é‡å»ºæ¨¡å‹çš„æƒ…å†µä¸‹æ›´æ–°LLMçš„æ–°ä¿¡æ¯ã€‚å…¶ä¸­ï¼Œä¸Šä¸‹æ–‡ç¼–è¾‘èŒƒå¼å› å…¶èƒ½å¤Ÿåœ¨é›†æˆæ–°çŸ¥è¯†çš„åŒæ—¶ä¿ç•™æ¨¡å‹çš„åŸå§‹èƒ½åŠ›è€Œè„±é¢–è€Œå‡ºã€‚å°½ç®¡æ½œåŠ›å·¨å¤§ï¼Œä½†ç°æœ‰çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ç¼–è¾‘æ–¹æ³•å¾€å¾€æ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ï¼Œä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨ç»“æ„åŒ–çŸ¥è¯†ä¸‰å…ƒç»„çš„å¤šè·³é—®ç­”ä»»åŠ¡ä¸Šã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¯¹ä»»åŠ¡åˆ†è§£çš„å°‘é‡æç¤ºä¾èµ–ä½¿å…¶ä¸ç¨³å®šï¼Œå¹¶ä¸”åœ¨è·¨ä¸åŒä»»åŠ¡æ³›åŒ–æ–¹é¢æ•ˆæœè¾ƒå·®ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†EditCoTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çŸ¥è¯†ç¼–è¾‘æ¡†æ¶ï¼Œèƒ½å¤Ÿçµæ´»æœ‰æ•ˆåœ°åœ¨å„ç§ä»»åŠ¡ä¸Šæ›´æ–°LLMï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚EditCoTé€šè¿‡ä¸ºç»™å®šè¾“å…¥ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œç„¶åä½¿ç”¨åŸºäºæ›´æ–°çŸ¥è¯†çš„CoTç¼–è¾‘å™¨è¿­ä»£åœ°æ”¹è¿›è¿™ä¸€æ€ç»´é“¾è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨æ¶µç›–å¤šç§è¯­è¨€å’Œä»»åŠ¡çš„å¤šç§åŸºå‡†æµ‹è¯•ä¸Šå¯¹EditCoTè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ³›åŒ–æ€§ã€æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ ‡å¿—ç€çŸ¥è¯†æ›´æ–°é¢†åŸŸçš„ä¸€ä¸ªé‡å¤§è¿›å±•ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/bebr2/EditCoT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/bebr2/EditCoTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17727v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>LLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†éšç€ä¸–ç•ŒçŸ¥è¯†çš„ä¸æ–­æ¼”å˜ï¼Œå¦‚ä½•æŒç»­æ›´æ–°è¿™äº›æ¨¡å‹æˆä¸ºä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå‡ºç°äº†çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯ï¼Œæ— éœ€ä»å¤´é‡å»ºå³å¯æ›´æ–°LLMçš„æ–°çŸ¥è¯†ã€‚å…¶ä¸­ï¼Œä¸Šä¸‹æ–‡ç¼–è¾‘èŒƒå¼èƒ½æœ‰æ•ˆæ•´åˆæ–°çŸ¥è¯†å¹¶ä¿ç•™æ¨¡å‹çš„åŸå§‹èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰ä¸Šä¸‹æ–‡çŸ¥è¯†ç¼–è¾‘æ–¹æ³•å¾€å¾€é’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼Œå¦‚å¤šè·³é—®ç­”ä»»åŠ¡ï¼Œä¸”ä¾èµ–å°‘é‡æç¤ºè¿›è¡Œä»»åŠ¡åˆ†è§£ï¼Œå¯¼è‡´å…¶åœ¨è·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢è¡¨ç°ä¸ç¨³å®šã€æ•ˆæœè¾ƒå·®ã€‚é’ˆå¯¹è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºEditCoTçŸ¥è¯†ç¼–è¾‘æ¡†æ¶ï¼Œèƒ½çµæ´»æœ‰æ•ˆåœ°æ›´æ–°LLMåœ¨å„ç§ä»»åŠ¡ä¸­çš„çŸ¥è¯†è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚EditCoTé€šè¿‡ä¸ºç»™å®šè¾“å…¥ç”Ÿæˆæ€ç»´é“¾ï¼Œç„¶åä½¿ç”¨åŸºäºæ›´æ–°çŸ¥è¯†çš„æ€ç»´é“¾ç¼–è¾‘å™¨è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨å¤šç§è¯­è¨€å’Œä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸Šå¯¹EditCoTè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨æ³›åŒ–èƒ½åŠ›ã€æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œæ ‡å¿—ç€çŸ¥è¯†æ›´æ–°é¢†åŸŸçš„é‡å¤§è¿›å±•ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bebr2/EditCoT">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†æ¨¡å‹çŸ¥è¯†çš„æŒç»­æ›´æ–°æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯å·²å‡ºç°ï¼Œèƒ½åœ¨ä¸é‡å»ºæ¨¡å‹çš„æƒ…å†µä¸‹æ›´æ–°LLMçš„æ–°çŸ¥è¯†ã€‚</li>
<li>ä¸Šä¸‹æ–‡ç¼–è¾‘èŒƒå¼æ˜¯æ•´åˆæ–°çŸ¥è¯†å¹¶ä¿ç•™æ¨¡å‹åŸå§‹èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç°æœ‰ä¸Šä¸‹æ–‡çŸ¥è¯†ç¼–è¾‘æ–¹æ³•å­˜åœ¨ä»»åŠ¡ç‰¹å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„å±€é™æ€§ã€‚</li>
<li>EditCoTæ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›å±€é™ï¼Œèƒ½çµæ´»æœ‰æ•ˆåœ°æ›´æ–°LLMåœ¨å„ç§ä»»åŠ¡ä¸­çš„çŸ¥è¯†ã€‚</li>
<li>EditCoTé€šè¿‡ç”Ÿæˆå’Œè¿­ä»£ä¼˜åŒ–æ€ç»´é“¾æ¥å·¥ä½œï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒEditCoTåœ¨æ³›åŒ–èƒ½åŠ›ã€æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f472e31e6ac9de51829b7b63d25d818a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfb8cd6183b6f8fdb3ba4a079571b5e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7f525fb1d898a3b9e60bd906a249150.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56026f175b786530e630070f421a79a1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Safety-A-Holistic-Survey"><a href="#Large-Language-Model-Safety-A-Holistic-Survey" class="headerlink" title="Large Language Model Safety: A Holistic Survey"></a>Large Language Model Safety: A Holistic Survey</h2><p><strong>Authors:Dan Shi, Tianhao Shen, Yufei Huang, Zhigen Li, Yongqi Leng, Renren Jin, Chuang Liu, Xinwei Wu, Zishan Guo, Linhao Yu, Ling Shi, Bojian Jiang, Deyi Xiong</strong></p>
<p>The rapid development and deployment of large language models (LLMs) have introduced a new frontier in artificial intelligence, marked by unprecedented capabilities in natural language understanding and generation. However, the increasing integration of these models into critical applications raises substantial safety concerns, necessitating a thorough examination of their potential risks and associated mitigation strategies.   This survey provides a comprehensive overview of the current landscape of LLM safety, covering four major categories: value misalignment, robustness to adversarial attacks, misuse, and autonomous AI risks. In addition to the comprehensive review of the mitigation methodologies and evaluation resources on these four aspects, we further explore four topics related to LLM safety: the safety implications of LLM agents, the role of interpretability in enhancing LLM safety, the technology roadmaps proposed and abided by a list of AI companies and institutes for LLM safety, and AI governance aimed at LLM safety with discussions on international cooperation, policy proposals, and prospective regulatory directions.   Our findings underscore the necessity for a proactive, multifaceted approach to LLM safety, emphasizing the integration of technical solutions, ethical considerations, and robust governance frameworks. This survey is intended to serve as a foundational resource for academy researchers, industry practitioners, and policymakers, offering insights into the challenges and opportunities associated with the safe integration of LLMs into society. Ultimately, it seeks to contribute to the safe and beneficial development of LLMs, aligning with the overarching goal of harnessing AI for societal advancement and well-being. A curated list of related papers has been publicly available at <a target="_blank" rel="noopener" href="https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers">https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿…é€Ÿå‘å±•å’Œéƒ¨ç½²ä¸ºäººå·¥æ™ºèƒ½å¼€å¯äº†ä¸€ä¸ªæ–°çºªå…ƒï¼Œä½“ç°åœ¨å‰æ‰€æœªæœ‰çš„è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¸Šã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æ—¥ç›Šé›†æˆåˆ°å…³é”®åº”ç”¨ä¸­ï¼Œå¼•å‘äº†å¤§é‡çš„å®‰å…¨å…³åˆ‡ï¼Œéœ€è¦å¯¹å®ƒä»¬å¯èƒ½å­˜åœ¨çš„é£é™©å’Œç›¸å…³ç¼“è§£ç­–ç•¥è¿›è¡Œå…¨é¢å®¡æŸ¥ã€‚è¿™ç¯‡ç»¼è¿°å…¨é¢æ¦‚è¿°äº†å½“å‰LLMå®‰å…¨é¢†åŸŸçš„ç°çŠ¶ï¼Œæ¶µç›–äº†å››å¤§ç±»åˆ«ï¼šä»·å€¼ä¸å¯¹é½ã€å¯¹æŠ—æ€§æ”»å‡»çš„ç¨³å¥æ€§ã€è¯¯ç”¨å’Œè‡ªä¸»äººå·¥æ™ºèƒ½é£é™©ã€‚é™¤äº†å¯¹è¿™å››ä¸ªæ–¹é¢çš„ç¼“è§£æ–¹æ³•å’Œè¯„ä¼°èµ„æºçš„å…¨é¢å›é¡¾å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ¢è®¨äº†ä¸LLMå®‰å…¨ç›¸å…³çš„å››ä¸ªè¯é¢˜ï¼šLLMä»£ç†çš„å®‰å…¨å½±å“ã€å¯è§£é‡Šæ€§åœ¨å¢å¼ºLLMå®‰å…¨ä¸­çš„ä½œç”¨ã€ä¸€ç³»åˆ—äººå·¥æ™ºèƒ½å…¬å¸å’Œç ”ç©¶æ‰€é’ˆå¯¹LLMå®‰å…¨æå‡ºå¹¶éµå¾ªçš„æŠ€æœ¯è·¯çº¿å›¾ï¼Œä»¥åŠæ—¨åœ¨ä¿ƒè¿›LLMå®‰å…¨çš„AIæ²»ç†ï¼ŒåŒ…æ‹¬å›½é™…åˆä½œã€æ”¿ç­–å»ºè®®å’Œç›‘ç®¡æ–¹å‘å‰æ™¯çš„è®¨è®ºã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†å¯¹LLMå®‰å…¨é‡‡å–ç§¯æå¤šå…ƒæ–¹æ³•çš„é‡è¦æ€§ï¼Œé‡ç‚¹æ•´åˆæŠ€æœ¯è§£å†³æ–¹æ¡ˆã€é“å¾·è€ƒé‡ä»¥åŠç¨³å¥çš„æ²»ç†æ¡†æ¶ã€‚è¿™ç¯‡ç»¼è¿°æ—¨åœ¨ä¸ºå­¦é™¢ç ”ç©¶äººå‘˜ã€è¡Œä¸šä»ä¸šè€…ä»¥åŠæ”¿ç­–åˆ¶å®šè€…æä¾›åŸºç¡€èµ„æºï¼Œæ·±å…¥äº†è§£å°†LLMå®‰å…¨é›†æˆåˆ°ç¤¾ä¼šä¸­çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚æœ€ç»ˆï¼Œå®ƒæ—¨åœ¨ä¿ƒè¿›LLMçš„å®‰å…¨å’Œæœ‰ç›Šå‘å±•ï¼Œç¬¦åˆåˆ©ç”¨äººå·¥æ™ºèƒ½æ¨åŠ¨ç¤¾ä¼šè¿›æ­¥å’Œç¦ç¥‰çš„æ€»ç›®æ ‡ã€‚ç›¸å…³è®ºæ–‡çš„ç²¾é€‰åˆ—è¡¨å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papersä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17686v1">PDF</a> 158 pages, 18 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿…é€Ÿå‘å±•å’Œéƒ¨ç½²ä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸå¼€è¾Ÿäº†æ–°çš„å‰æ²¿ï¼Œå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å…³é”®é¢†åŸŸä¸­çš„é›†æˆå¼•å‘äº†é‡è¦çš„å®‰å…¨é—®é¢˜ï¼Œéœ€è¦è¿›è¡Œå…¨é¢çš„é£é™©è¯„ä¼°å’Œç­–ç•¥ç¼“è§£ã€‚æœ¬æ–‡ç»¼è¿°äº†LLMå®‰å…¨çš„ç°çŠ¶ï¼ŒåŒ…æ‹¬ä»·å€¼é”™ä½ã€å¯¹æŠ—æ”»å‡»çš„ç¨³å®šæ€§ã€è¯¯ç”¨å’Œè‡ªä¸»AIé£é™©ç­‰å››å¤§ç±»ã€‚é™¤äº†ç¼“è§£æ–¹æ³•å’Œè¯„ä¼°èµ„æºçš„å…¨é¢å›é¡¾ï¼Œè¿˜æ¢è®¨äº†ä¸LLMå®‰å…¨ç›¸å…³çš„å››ä¸ªè¯é¢˜ï¼šLLMä»£ç†çš„å®‰å…¨å½±å“ã€å¯è§£é‡Šæ€§åœ¨å¢å¼ºLLMå®‰å…¨ä¸­çš„ä½œç”¨ã€AIå…¬å¸å’Œç ”ç©¶æ‰€é’ˆå¯¹LLMå®‰å…¨çš„æŠ€æœ¯è·¯çº¿å›¾ä»¥åŠæ—¨åœ¨å®ç°LLMå®‰å…¨çš„AIæ²»ç†ï¼ŒåŒ…æ‹¬å›½é™…åˆä½œã€æ”¿ç­–å»ºè®®å’Œç›‘ç®¡æ–¹å‘ã€‚ç ”ç©¶å‘ç°ï¼Œéœ€è¦ç§¯æã€å¤šæ–¹é¢çš„æ–¹æ³•æ¥è§£å†³LLMå®‰å…¨é—®é¢˜ï¼Œå¼ºè°ƒæŠ€æœ¯è§£å†³æ–¹æ¡ˆã€é“å¾·è€ƒé‡ä»¥åŠå¥å…¨æ²»ç†æ¡†æ¶çš„æ•´åˆã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºå­¦é™¢ç ”ç©¶äººå‘˜ã€è¡Œä¸šä»ä¸šè€…ä»¥åŠå†³ç­–è€…æä¾›åŸºç¡€æ€§èµ„æºï¼Œæ·±å…¥äº†è§£å°†LLMå®‰å…¨é›†æˆåˆ°ç¤¾ä¼šä¸­çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚æœ€ç»ˆç›®æ ‡æ˜¯ä¿ƒè¿›LLMçš„å®‰å…¨å’Œæœ‰ç›Šå‘å±•ï¼Œå®ç°äººå·¥æ™ºèƒ½æœåŠ¡äºç¤¾ä¼šè¿›æ­¥å’Œç¦ç¥‰çš„ç›®æ ‡ã€‚ç›¸å…³è®ºæ–‡åˆ—è¡¨å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers">https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå¼•å‘æ–°çš„å‰æ²¿ï¼Œå…·å¤‡å¼ºå¤§çš„è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>LLMçš„é›†æˆå’Œåº”ç”¨å¸¦æ¥å®è´¨çš„å®‰å…¨é—®é¢˜ï¼ŒåŒ…æ‹¬ä»·å€¼é”™ä½ã€å¯¹æŠ—ç¨³å®šæ€§ç­‰å››å¤§ç±»é£é™©ã€‚</li>
<li>ç»¼è¿°æä¾›äº†å…¨é¢çš„LLMå®‰å…¨å®¡æŸ¥ï¼ŒåŒ…æ‹¬ç›¸å…³çš„ç¼“è§£æ–¹æ³•å’Œè¯„ä¼°èµ„æºã€‚</li>
<li>æ¢è®¨äº†LLMå®‰å…¨ç›¸å…³çš„å››ä¸ªè¯é¢˜ï¼ŒåŒ…æ‹¬å®‰å…¨å½±å“ã€å¯è§£é‡Šæ€§çš„ä½œç”¨ã€æŠ€æœ¯è·¯çº¿å›¾å’ŒAIæ²»ç†ã€‚</li>
<li>éœ€è¦ç§¯æã€å¤šæ–¹é¢çš„ç­–ç•¥æ¥è§£å†³LLMå®‰å…¨é—®é¢˜ï¼Œç»“åˆæŠ€æœ¯è§£å†³æ–¹æ¡ˆã€é“å¾·è€ƒé‡å’Œå¥å…¨æ²»ç†æ¡†æ¶ã€‚</li>
<li>æœ¬æ–‡æ—¨åœ¨ä¸ºä¸åŒé¢†åŸŸçš„ç ”ç©¶äººå‘˜æä¾›å…³äºLLMå®‰å…¨çš„åŸºç¡€èµ„æºï¼Œä»¥æ·±å…¥äº†è§£ç›¸å…³æŒ‘æˆ˜å’Œæœºé‡ã€‚</li>
<li>æ—¨åœ¨ä¿ƒè¿›LLMçš„å®‰å…¨å’Œæœ‰ç›Šå‘å±•ï¼Œä½¿å…¶æ›´å¥½åœ°æœåŠ¡äºç¤¾ä¼šè¿›æ­¥å’Œç¦ç¥‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a8e8c82c4385e54115a39b14518348c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SCBench-A-Sports-Commentary-Benchmark-for-Video-LLMs"><a href="#SCBench-A-Sports-Commentary-Benchmark-for-Video-LLMs" class="headerlink" title="SCBench: A Sports Commentary Benchmark for Video LLMs"></a>SCBench: A Sports Commentary Benchmark for Video LLMs</h2><p><strong>Authors:Kuangzhi Ge, Lingjun Chen, Kevin Zhang, Yulin Luo, Tianyu Shi, Liaoyuan Fan, Xiang Li, Guanqun Wang, Shanghang Zhang</strong></p>
<p>Recently, significant advances have been made in Video Large Language Models (Video LLMs) in both academia and industry. However, methods to evaluate and benchmark the performance of different Video LLMs, especially their fine-grained, temporal visual capabilities, remain very limited. On one hand, current benchmarks use relatively simple videos (e.g., subtitled movie clips) where the model can understand the entire video by processing just a few frames. On the other hand, their datasets lack diversity in task format, comprising only QA or multi-choice QA, which overlooks the modelsâ€™ capacity for generating in-depth and precise texts. Sports videos, which feature intricate visual information, sequential events, and emotionally charged commentary, present a critical challenge for Video LLMs, making sports commentary an ideal benchmarking task. Inspired by these challenges, we propose a novel task: sports video commentary generation, developed $\textbf{SCBench}$ for Video LLMs. To construct such a benchmark, we introduce (1) $\textbf{SCORES}$, a six-dimensional metric specifically designed for our task, upon which we propose a GPT-based evaluation method, and (2) $\textbf{CommentarySet}$, a dataset consisting of 5,775 annotated video clips and ground-truth labels tailored to our metric. Based on SCBench, we conduct comprehensive evaluations on multiple Video LLMs (e.g. VILA, Video-LLaVA, etc.) and chain-of-thought baseline methods. Our results found that InternVL-Chat-2 achieves the best performance with 5.44, surpassing the second-best by 1.04. Our work provides a fresh perspective for future research, aiming to enhance modelsâ€™ overall capabilities in complex visual understanding tasks. Our dataset will be released soon. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œåœ¨è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰æ–¹é¢éƒ½å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œè¯„ä¼°å’Œè¡¡é‡ä¸åŒè§†é¢‘å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯å…¶ç²¾ç»†çš„ã€æ—¶é—´æ€§çš„è§†è§‰èƒ½åŠ›ï¼Œä»ç„¶éå¸¸æœ‰é™ã€‚ä¸€æ–¹é¢ï¼Œå½“å‰çš„æ ‡å‡†ä¸»è¦ä½¿ç”¨ç›¸å¯¹ç®€å•çš„è§†é¢‘ï¼ˆä¾‹å¦‚å¸¦å­—å¹•çš„ç”µå½±ç‰‡æ®µï¼‰ï¼Œè¿™äº›è§†é¢‘çš„æ¨¡å‹å¯ä»¥é€šè¿‡å¤„ç†å°‘æ•°å‡ ä¸ªå¸§æ¥ç†è§£æ•´ä¸ªè§†é¢‘ã€‚å¦ä¸€æ–¹é¢ï¼Œä»–ä»¬çš„æ•°æ®é›†åœ¨ä»»åŠ¡æ ¼å¼ä¸Šç¼ºä¹å¤šæ ·æ€§ï¼Œä»…é™äºé—®ç­”æˆ–å¤šé€‰é—®ç­”ï¼Œå¿½è§†äº†æ¨¡å‹ç”Ÿæˆæ·±å…¥å’Œç²¾ç¡®æ–‡æœ¬çš„èƒ½åŠ›ã€‚ä½“è‚²è§†é¢‘å…·æœ‰å¤æ‚è§†è§‰ä¿¡æ¯ã€è¿ç»­äº‹ä»¶å’Œæƒ…ç»ªåŒ–çš„è§£è¯´è¯ï¼Œä¸ºè§†é¢‘å¤§è¯­è¨€æ¨¡å‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œä½¿å¾—ä½“è‚²è§£è¯´æˆä¸ºç†æƒ³çš„åŸºå‡†æµ‹è¯•ä»»åŠ¡ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°ä»»åŠ¡ï¼šä½“è‚²è§†é¢‘è§£è¯´ç”Ÿæˆï¼Œå¹¶å¼€å‘äº†è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•SCBenchã€‚ä¸ºäº†æ„å»ºè¿™æ ·ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ï¼ˆ1ï¼‰ä¸“ä¸ºæˆ‘ä»¬çš„ä»»åŠ¡è®¾è®¡çš„å…­ç»´åº¦é‡æ ‡å‡†SCORESï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†åŸºäºGPTçš„è¯„ä¼°æ–¹æ³•ï¼›ï¼ˆ2ï¼‰æ•°æ®é›†CommentarySetï¼Œè¯¥æ•°æ®é›†åŒ…å«5775ä¸ªå¸¦æ³¨é‡Šçš„è§†é¢‘å‰ªè¾‘å’Œé’ˆå¯¹æˆ‘ä»¬åº¦é‡çš„çœŸå®æ ‡ç­¾ã€‚åŸºäºSCBenchåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¯¹å¤šä¸ªè§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚VILAã€Video-LLaVAç­‰ï¼‰è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œä»¥åŠåŸºäºæ€ç»´é“¾çš„åŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒInternVL-Chat-2è¡¨ç°æœ€ä½³ï¼Œå¾—åˆ†5.44ï¼Œæ¯”ç¬¬äºŒåé«˜å‡º1.04ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨å¤æ‚è§†è§‰ç†è§£ä»»åŠ¡æ–¹é¢çš„æ•´ä½“èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17637v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Video LLMé¢†åŸŸçš„è¿›æ­¥æ˜¾è‘—ï¼Œä½†åœ¨è¯„ä¼°å…¶æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯ç²¾ç»†çš„æ—¶ç©ºè§†è§‰èƒ½åŠ›æ–¹é¢ä»å­˜åœ¨å±€é™ã€‚å½“å‰è¯„ä¼°å·¥å…·ä½¿ç”¨çš„è§†é¢‘è¾ƒä¸ºç®€å•ï¼Œæ— æ³•å…¨é¢è¯„ä¼°æ¨¡å‹çš„è§†è§‰å¤„ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ä¸ªæ–°çš„ä»»åŠ¡â€”â€”ä½“è‚²è§†é¢‘è¯„è®ºç”Ÿæˆï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°Video LLMçš„åŸºå‡†æµ‹è¯•SCBenchã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬ä¸€ä¸ªå…­ç»´è¯„ä¼°æŒ‡æ ‡SCORESå’Œä¸€ä¸ªä¸ä¹‹åŒ¹é…çš„æ•°æ®é›†CommentarySetã€‚ç»è¿‡å…¨é¢è¯„ä¼°ï¼ŒInternVL-Chat-2è¡¨ç°æœ€ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video LLMé¢†åŸŸè¿‘æœŸæœ‰é‡å¤§è¿›å±•ï¼Œä½†è¯„ä¼°å…¶æ€§èƒ½å°¤å…¶æ˜¯ç²¾ç»†æ—¶ç©ºè§†è§‰èƒ½åŠ›çš„å·¥å…·ä»ç„¶éå¸¸æœ‰é™ã€‚</li>
<li>å½“å‰ä½¿ç”¨çš„è§†é¢‘åŸºå‡†æµ‹è¯•è¿‡äºç®€å•ï¼Œæ— æ³•å……åˆ†è¯„ä¼°æ¨¡å‹çš„è§†è§‰å¤„ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°ä»»åŠ¡â€”â€”ä½“è‚²è§†é¢‘è¯„è®ºç”Ÿæˆï¼Œä½œä¸ºå¯¹Video LLMæ›´å…¨é¢çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåä¸ºSCBenchçš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå…­ç»´è¯„ä¼°æŒ‡æ ‡SCORESå’Œä¸€ä¸ªé…å¥—æ•°æ®é›†CommentarySetã€‚</li>
<li>é€šè¿‡SCBenchåŸºå‡†æµ‹è¯•å¯¹å¤šä¸ªVideo LLMè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</li>
<li>InternVL-Chat-2åœ¨æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17637">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-81dd490c5298467c77424cb6b5a86f26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-009c314da06d0df2042ee9de03881b38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e82154f6db86668519d156fa0d62ceac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5eab512cd3fcf7f3a30c8a4492402ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6605013ebdb49771567ad1cb6a08688f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="EasyTime-Time-Series-Forecasting-Made-Easy"><a href="#EasyTime-Time-Series-Forecasting-Made-Easy" class="headerlink" title="EasyTime: Time Series Forecasting Made Easy"></a>EasyTime: Time Series Forecasting Made Easy</h2><p><strong>Authors:Xiangfei Qiu, Xiuwen Li, Ruiyang Pang, Zhicheng Pan, Xingjian Wu, Liu Yang, Jilin Hu, Yang Shu, Xuesong Lu, Chengcheng Yang, Chenjuan Guo, Aoying Zhou, Christian S. Jensen, Bin Yang</strong></p>
<p>Time series forecasting has important applications across diverse domains. EasyTime, the system we demonstrate, facilitates easy use of time-series forecasting methods by researchers and practitioners alike. First, EasyTime enables one-click evaluation, enabling researchers to evaluate new forecasting methods using the suite of diverse time series datasets collected in the preexisting time series forecasting benchmark (TFB). This is achieved by leveraging TFBâ€™s flexible and consistent evaluation pipeline. Second, when practitioners must perform forecasting on a new dataset, a nontrivial first step is often to find an appropriate forecasting method. EasyTime provides an Automated Ensemble module that combines the promising forecasting methods to yield superior forecasting accuracy compared to individual methods. Third, EasyTime offers a natural language Q&amp;A module leveraging large language models. Given a question like â€œWhich method is best for long term forecasting on time series with strong seasonality?â€, EasyTime converts the question into SQL queries on the database of results obtained by TFB and then returns an answer in natural language and charts. By demonstrating EasyTime, we intend to show how it is possible to simplify the use of time series forecasting and to offer better support for the development of new generations of time series forecasting methods. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—é¢„æµ‹åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰é‡è¦çš„åº”ç”¨ã€‚æˆ‘ä»¬å±•ç¤ºçš„EasyTimeç³»ç»Ÿï¼Œä¾¿äºç ”ç©¶è€…å’Œå®è·µè€…ä½¿ç”¨æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•ã€‚é¦–å…ˆï¼ŒEasyTimeå®ç°äº†ä¸€é”®è¯„ä¼°åŠŸèƒ½ï¼Œç ”ç©¶è€…å¯ä»¥ä½¿ç”¨é¢„å­˜åœ¨çš„æ—¶é—´åºåˆ—é¢„æµ‹åŸºå‡†æµ‹è¯•ï¼ˆTFBï¼‰ä¸­æ”¶é›†çš„å„ç§æ—¶é—´åºåˆ—æ•°æ®é›†æ¥è¯„ä¼°æ–°çš„é¢„æµ‹æ–¹æ³•ã€‚è¿™æ˜¯é€šè¿‡åˆ©ç”¨TFBçµæ´»ä¸”ä¸€è‡´çš„è¯„ä»·ç®¡é“å®ç°çš„ã€‚å…¶æ¬¡ï¼Œå½“å®è·µè€…å¿…é¡»åœ¨æ–°æ•°æ®é›†ä¸Šè¿›è¡Œé¢„æµ‹æ—¶ï¼Œç»å¸¸ç¬¬ä¸€æ­¥å°±æ˜¯è¦æ‰¾åˆ°ä¸€ç§åˆé€‚çš„é¢„æµ‹æ–¹æ³•ã€‚EasyTimeæä¾›äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–é›†æˆæ¨¡å—ï¼Œè¯¥æ¨¡å—ç»“åˆäº†æœ‰å‰é€”çš„é¢„æµ‹æ–¹æ³•ï¼Œä¸å•ä¸ªæ–¹æ³•ç›¸æ¯”ï¼Œäº§ç”Ÿäº†æ›´é«˜çš„é¢„æµ‹ç²¾åº¦ã€‚ç¬¬ä¸‰ï¼ŒEasyTimeæä¾›äº†ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªç„¶è¯­è¨€é—®ç­”æ¨¡å—ã€‚å¯¹äºè¯¸å¦‚â€œå¯¹äºå…·æœ‰å¼ºçƒˆå­£èŠ‚æ€§çš„æ—¶é—´åºåˆ—ï¼Œå“ªç§æ–¹æ³•æœ€é€‚åˆé•¿æœŸé¢„æµ‹ï¼Ÿâ€è¿™æ ·çš„é—®é¢˜ï¼ŒEasyTimeå°†é—®é¢˜è½¬æ¢ä¸ºå¯¹TFBç»“æœæ•°æ®åº“ä¸­çš„SQLæŸ¥è¯¢ï¼Œç„¶åä»¥è‡ªç„¶è¯­è¨€è¿”å›ç­”æ¡ˆå’Œå›¾è¡¨ã€‚é€šè¿‡å±•ç¤ºEasyTimeï¼Œæˆ‘ä»¬çš„æ„å›¾æ˜¯è¡¨æ˜å¦‚ä½•ç®€åŒ–æ—¶é—´åºåˆ—é¢„æµ‹çš„ä½¿ç”¨ï¼Œå¹¶ä¸ºæ–°ä¸€ä»£æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•çš„å‘å±•æä¾›æ›´å¥½çš„æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17603v1">PDF</a> Accepted by ICDE2025</p>
<p><strong>Summary</strong></p>
<p>æ—¶é—´åºåˆ—é¢„æµ‹åœ¨å¤šä¸ªé¢†åŸŸéƒ½æœ‰é‡è¦åº”ç”¨ã€‚EasyTimeç³»ç»Ÿä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•çš„æ˜“ç”¨å·¥å…·ã€‚å®ƒæä¾›ä¸€é”®å¼è¯„ä¼°åŠŸèƒ½ï¼Œä½¿ç”¨å·²å­˜åœ¨çš„æ—¶é—´åºåˆ—é¢„æµ‹åŸºå‡†æµ‹è¯•å¥—ä»¶æ¥è¯„ä¼°æ–°çš„é¢„æµ‹æ–¹æ³•ï¼›å…·æœ‰è‡ªåŠ¨é›†æˆæ¨¡å—ï¼Œå°†ä¸åŒé¢„æµ‹æ–¹æ³•è¿›è¡Œç»„åˆï¼Œä»¥æé«˜é¢„æµ‹ç²¾åº¦ï¼›å¹¶æä¾›è‡ªç„¶è¯­è¨€é—®ç­”æ¨¡å—ï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹å›ç­”ç›¸å…³é—®é¢˜å¹¶å±•ç¤ºå›¾è¡¨ã€‚EasyTimeæ—¨åœ¨ç®€åŒ–æ—¶é—´åºåˆ—é¢„æµ‹çš„ä½¿ç”¨å¹¶æ¨åŠ¨æ–°ä¸€ä»£æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EasyTimeç³»ç»Ÿç®€åŒ–äº†æ—¶é—´åºåˆ—é¢„æµ‹çš„ä½¿ç”¨ï¼Œä½¿ç ”ç©¶äººå‘˜å’Œå®è·µè€…æ›´å®¹æ˜“é‡‡ç”¨æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•ã€‚</li>
<li>EasyTimeæä¾›äº†ä¸€é”®å¼è¯„ä¼°åŠŸèƒ½ï¼Œå…è®¸ç”¨æˆ·è½»æ¾è¯„ä¼°æ–°çš„æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•ã€‚å®ƒä½¿ç”¨é¢„å­˜åœ¨çš„æ—¶é—´åºåˆ—é¢„æµ‹åŸºå‡†æµ‹è¯•å¥—ä»¶æ¥æ¯”è¾ƒè¿™äº›æ–¹æ³•ã€‚</li>
<li>EasyTimeå…·å¤‡è‡ªåŠ¨é›†æˆæ¨¡å—ï¼Œå¯ä»¥å°†ä¸åŒçš„æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•è¿›è¡Œç»„åˆï¼Œä»¥è·å¾—æ›´é«˜çš„é¢„æµ‹ç²¾åº¦ã€‚</li>
<li>é€šè¿‡è‡ªç„¶è¯­è¨€é—®ç­”æ¨¡å—ï¼ŒEasyTimeèƒ½å¤Ÿå›ç­”å…³äºæ—¶é—´åºåˆ—é¢„æµ‹çš„ç›¸å…³é—®é¢˜ï¼Œå¹¶é€šè¿‡å›¾è¡¨å±•ç¤ºç­”æ¡ˆã€‚</li>
<li>EasyTimeåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å®ç°è‡ªç„¶è¯­è¨€å¤„ç†åŠŸèƒ½ï¼Œå¢å¼ºäº†ç³»ç»Ÿçš„äº¤äº’æ€§å’Œå®ç”¨æ€§ã€‚</li>
<li>EasyTimeæ—¨åœ¨æ”¯æŒæ–°ä¸€ä»£æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•çš„å‘å±•ï¼Œæ¨åŠ¨ç›¸å…³ç ”ç©¶çš„è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9877b48babcfb61fc3c7db4df2bcc3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-401fada5f7eac20a7154b9930728a181.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32d5eab5e6b51fcbf4d2f4e7d6881d7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cecb082ee525e0050a873b2ad38ee0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ed9f1fe67b3d51d030b8da7feb1963b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LiveIdeaBench-Evaluating-LLMsâ€™-Scientific-Creativity-and-Idea-Generation-with-Minimal-Context"><a href="#LiveIdeaBench-Evaluating-LLMsâ€™-Scientific-Creativity-and-Idea-Generation-with-Minimal-Context" class="headerlink" title="LiveIdeaBench: Evaluating LLMsâ€™ Scientific Creativity and Idea   Generation with Minimal Context"></a>LiveIdeaBench: Evaluating LLMsâ€™ Scientific Creativity and Idea   Generation with Minimal Context</h2><p><strong>Authors:Kai Ruan, Xuan Wang, Jixiang Hong, Hao Sun</strong></p>
<p>While Large Language Models (LLMs) have demonstrated remarkable capabilities in scientific tasks, existing evaluation frameworks primarily assess their performance using rich contextual inputs, overlooking their ability to generate novel ideas from minimal information. We introduce LiveIdeaBench, a comprehensive benchmark that evaluates LLMsâ€™ scientific creativity and divergent thinking capabilities using single-keyword prompts. Drawing from Guilfordâ€™s creativity theory, our framework employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across four key dimensions: originality, feasibility, fluency, and flexibility. Through extensive experimentation with 20 leading models across 1,180 keywords spanning 18 scientific domains, we reveal that scientific creative ability shows distinct patterns from general intelligence metrics. Notably, our results demonstrate that models like QwQ-32B-preview achieve comparable creative performance to top-tier models like o1-preview, despite significant gaps in their general intelligence scores. These findings highlight the importance of specialized evaluation frameworks for scientific creativity and suggest that the development of creative capabilities in LLMs may follow different trajectories than traditional problem-solving abilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç§‘ç ”ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸»è¦ä½¿ç”¨ä¸°å¯Œçš„ä¸Šä¸‹æ–‡è¾“å…¥æ¥è¯„ä¼°å…¶æ€§èƒ½ï¼Œå¿½è§†äº†å®ƒä»¬ä»å°‘é‡ä¿¡æ¯ä¸­ç”Ÿæˆæ–°æƒ³æ³•çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†LiveIdeaBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨å•å…³é”®è¯æç¤ºæ¥è¯„ä¼°LLMçš„ç§‘ç ”åˆ›é€ åŠ›å’Œå‘æ•£æ€ç»´èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŸºäºå‰å°”ç¦å¾·çš„åˆ›é€ åŠ›ç†è®ºï¼Œé‡‡ç”¨å…ˆè¿›çš„LLMåŠ¨æ€é¢æ¿ï¼Œå¯¹ç”Ÿæˆçš„åˆ›æ„ä»å››ä¸ªå…³é”®ç»´åº¦è¿›è¡Œè¯„ä¼°ï¼šåŸåˆ›æ€§ã€å¯è¡Œæ€§ã€æµç•…æ€§å’Œçµæ´»æ€§ã€‚é€šè¿‡å¯¹18ä¸ªç§‘å­¦é¢†åŸŸçš„1,180ä¸ªå…³é”®è¯è¿›è¡Œçš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°ç§‘å­¦åˆ›é€ åŠ›è¡¨ç°å‡ºä¸é€šç”¨æ™ºåŠ›æŒ‡æ ‡ä¸åŒçš„æ¨¡å¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°½ç®¡åœ¨ä¸€äº›æ™®é€šæ™ºåŠ›æµ‹è¯•ä¸Šæœ‰å·®è·ï¼ŒåƒQwQ-32B-previewè¿™æ ·çš„æ¨¡å‹ä¹Ÿèƒ½åœ¨åˆ›æ„ä¸Šè¾¾åˆ°é¡¶çº§æ¨¡å‹å¦‚o1-previewçš„æ°´å¹³ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†é’ˆå¯¹ç§‘å­¦åˆ›é€ åŠ›è¿›è¡Œä¸“é¡¹è¯„ä¼°çš„é‡è¦æ€§ï¼Œå¹¶æš—ç¤ºLLMä¸­åˆ›é€ åŠ›çš„å‘å±•å¯èƒ½ä¸ä¼ ç»Ÿçš„è§£å†³é—®é¢˜çš„èƒ½åŠ›ä¸åŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17596v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç§‘å­¦ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—èƒ½åŠ›ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸»è¦ä½¿ç”¨ä¸°å¯Œçš„ä¸Šä¸‹æ–‡è¾“å…¥æ¥è¯„ä¼°å…¶æ€§èƒ½ï¼Œå¿½è§†äº†å®ƒä»¬ä»æå°‘çš„ä¿¡æ¯ä¸­ç”Ÿæˆæ–°æƒ³æ³•çš„èƒ½åŠ›ã€‚æœ¬æ–‡å¼•å…¥LiveIdeaBenchï¼Œä¸€ä¸ªå…¨é¢è¯„ä¼°LLMç§‘å­¦åˆ›é€ åŠ›å’Œå‘æ•£æ€ç»´çš„æ¡†æ¶ï¼Œä½¿ç”¨å•å…³é”®è¯æç¤ºè¿›è¡Œè¯„ä¼°ã€‚è¯¥æ¡†æ¶å€Ÿé‰´å‰å°”ç¦å¾·çš„åˆ›é€ åŠ›ç†è®ºï¼Œé€šè¿‡åŠ¨æ€é¢æ¿è¯„ä¼°ç”Ÿæˆçš„åˆ›æ„åœ¨å››ä¸ªå…³é”®ç»´åº¦ä¸Šçš„è¡¨ç°ï¼šåŸåˆ›æ€§ã€å¯è¡Œæ€§ã€æµç•…æ€§å’Œçµæ´»æ€§ã€‚é€šè¿‡å¯¹18ä¸ªç§‘å­¦é¢†åŸŸçš„1,180ä¸ªå…³é”®è¯çš„20ä¸ªé¢†å…ˆæ¨¡å‹çš„å¤§é‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°ç§‘å­¦åˆ›é€ åŠ›è¡¨ç°å‡ºä¸åŒäºä¸€èˆ¬æ™ºåŠ›æŒ‡æ ‡çš„ç‹¬ç‰¹æ¨¡å¼ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåƒQwQ-32B-previewè¿™æ ·çš„æ¨¡å‹åœ¨åˆ›é€ æ€§è¡¨ç°æ–¹é¢ä¸é¡¶çº§æ¨¡å‹o1-previewç›¸å½“ï¼Œå°½ç®¡å®ƒä»¬åœ¨ä¸€èˆ¬æ™ºåŠ›å¾—åˆ†ä¸Šå­˜åœ¨æ˜¾è‘—å·®è·ã€‚è¿™è¡¨æ˜ä¸“é—¨çš„è¯„ä¼°æ¡†æ¶å¯¹äºç§‘å­¦åˆ›é€ åŠ›è‡³å…³é‡è¦ï¼Œå¹¶ä¸”LLMçš„å‘å±•åˆ›é€ åŠ›å¯èƒ½éµå¾ªä¸ä¼ ç»Ÿé—®é¢˜è§£å†³èƒ½åŠ›ä¸åŒçš„è½¨è¿¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç§‘å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç°æœ‰è¯„ä¼°æ¡†æ¶ä¸»è¦å…³æ³¨ä¸°å¯Œä¸Šä¸‹æ–‡è¾“å…¥ä¸‹çš„æ€§èƒ½è¯„ä¼°ã€‚</li>
<li>å¼•å…¥LiveIdeaBenchæ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°LLMçš„ç§‘å­¦åˆ›é€ åŠ›å’Œå‘æ•£æ€ç»´ã€‚</li>
<li>å€Ÿé‰´å‰å°”ç¦å¾·çš„åˆ›é€ åŠ›ç†è®ºï¼Œä»å››ä¸ªå…³é”®ç»´åº¦è¯„ä¼°ç”Ÿæˆçš„åˆ›æ„ï¼šåŸåˆ›æ€§ã€å¯è¡Œæ€§ã€æµç•…æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>é€šè¿‡å¤§é‡å®éªŒå‘ç°ç§‘å­¦åˆ›é€ åŠ›è¡¨ç°å…·æœ‰ç‹¬ç‰¹æ€§ï¼Œä¸åŒäºä¸€èˆ¬æ™ºåŠ›æŒ‡æ ‡ã€‚</li>
<li>QwQ-32B-previewç­‰æ¨¡å‹åœ¨åˆ›é€ æ€§è¡¨ç°ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸ä¸€èˆ¬æ™ºåŠ›å¾—åˆ†å­˜åœ¨å·®è·ã€‚</li>
<li>ä¸“é—¨çš„è¯„ä¼°æ¡†æ¶å¯¹ç§‘å­¦åˆ›é€ åŠ›çš„è¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-13bb0b74d2541332b908564da5116847.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c43ff4b06f1aaf2cecb09e88ef973ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7387aaf2db3d08e72cd68a5a0458d0f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-274de6d3efa82b5f6f28230cebbd6c0c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HumanVBench-Exploring-Human-Centric-Video-Understanding-Capabilities-of-MLLMs-with-Synthetic-Benchmark-Data"><a href="#HumanVBench-Exploring-Human-Centric-Video-Understanding-Capabilities-of-MLLMs-with-Synthetic-Benchmark-Data" class="headerlink" title="HumanVBench: Exploring Human-Centric Video Understanding Capabilities of   MLLMs with Synthetic Benchmark Data"></a>HumanVBench: Exploring Human-Centric Video Understanding Capabilities of   MLLMs with Synthetic Benchmark Data</h2><p><strong>Authors:Ting Zhou, Daoyuan Chen, Qirui Jiao, Bolin Ding, Yaliang Li, Ying Shen</strong></p>
<p>In the domain of Multimodal Large Language Models (MLLMs), achieving human-centric video understanding remains a formidable challenge. Existing benchmarks primarily emphasize object and action recognition, often neglecting the intricate nuances of human emotions, behaviors, and speech visual alignment within video content. We present HumanVBench, an innovative benchmark meticulously crafted to bridge these gaps in the evaluation of video MLLMs. HumanVBench comprises 17 carefully designed tasks that explore two primary dimensions: inner emotion and outer manifestations, spanning static and dynamic, basic and complex, as well as single-modal and cross-modal aspects. With two advanced automated pipelines for video annotation and distractor-included QA generation, HumanVBench utilizes diverse state-of-the-art (SOTA) techniques to streamline benchmark data synthesis and quality assessment, minimizing human annotation dependency tailored to human-centric multimodal attributes. A comprehensive evaluation across 16 SOTA video MLLMs reveals notable limitations in current performance, especially in cross-modal and temporal alignment, underscoring the necessity for further refinement toward achieving more human-like understanding. HumanVBench is open-sourced to facilitate future advancements and real-world applications in video MLLMs. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é¢†åŸŸï¼Œå®ç°ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£ä»ç„¶æ˜¯ä¸€é¡¹å·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å¼ºè°ƒå¯¹è±¡å’ŒåŠ¨ä½œè¯†åˆ«ï¼Œå¾€å¾€å¿½è§†äº†è§†é¢‘å†…å®¹ä¸­äººç±»æƒ…ç»ªã€è¡Œä¸ºå’Œè¯­éŸ³è§†è§‰å¯¹é½çš„ç»†å¾®å·®åˆ«ã€‚æˆ‘ä»¬æ¨å‡ºäº†HumanVBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„åˆ›æ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¼¥è¡¥è§†é¢‘MLLMè¯„ä¼°ä¸­çš„è¿™äº›å·®è·ã€‚HumanVBenchåŒ…å«17ä¸ªç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ï¼Œæ¢ç´¢ä¸¤ä¸ªä¸»è¦ç»´åº¦ï¼šå†…åœ¨æƒ…ç»ªå’Œå¤–åœ¨è¡¨ç°ï¼Œæ¶µç›–é™æ€å’ŒåŠ¨æ€ã€åŸºæœ¬å’Œå¤æ‚ï¼Œä»¥åŠå•æ¨¡æ€å’Œè·¨æ¨¡æ€æ–¹é¢ã€‚HumanVBenchä½¿ç”¨ä¸¤ä¸ªå…ˆè¿›çš„è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œè§†é¢‘æ³¨é‡Šå’ŒåŒ…å«å¹²æ‰°é¡¹çš„QAç”Ÿæˆï¼Œåˆ©ç”¨å¤šç§æœ€æ–°æŠ€æœ¯ç®€åŒ–åŸºå‡†æ•°æ®åˆæˆå’Œè´¨é‡è¯„ä¼°ï¼Œå‡å°‘å¯¹äººç±»æ³¨é‡Šçš„ä¾èµ–ï¼Œé’ˆå¯¹ä»¥äººç±»ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å±æ€§è¿›è¡Œå®šåˆ¶ã€‚å¯¹16ä¸ªæœ€æ–°è§†é¢‘MLLMçš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æ€§èƒ½å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå°¤å…¶åœ¨è·¨æ¨¡æ€å’Œæ—¶é—´å¯¹é½æ–¹é¢ï¼Œè¿™å¼ºè°ƒäº†å¯¹è¿›ä¸€æ­¥æ”¹è¿›ä»¥å®ç°æ›´äººæ€§åŒ–çš„ç†è§£çš„å¿…è¦æ€§ã€‚HumanVBenchå¼€æºï¼Œä»¥ä¿ƒè¿›è§†é¢‘MLLMçš„æœªæ¥å‘å±•å’Œå®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17574v1">PDF</a> 22 pages, 24 figures, 4 tables</p>
<p><strong>Summary</strong><br>å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚å½“å‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å¯¹è±¡å’ŒåŠ¨ä½œè¯†åˆ«ï¼Œå¿½ç•¥äº†äººç±»æƒ…æ„Ÿã€è¡Œä¸ºå’Œè¯­éŸ³è§†è§‰å¯¹é½ç­‰ç»†å¾®å·®åˆ«ã€‚æœ¬æ–‡æå‡ºHumanVBenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¶µç›–ç²¾å¿ƒè®¾è®¡çš„åä¸ƒé¡¹ä»»åŠ¡ï¼ŒåŒ…æ‹¬å†…åœ¨æƒ…æ„Ÿå’Œå¤–åœ¨è¡¨ç°ä¸¤ä¸ªæ–¹é¢ï¼Œå…¨é¢æ¢ç´¢è§†é¢‘ç†è§£çš„å¤šç§æ–¹é¢ã€‚HumanVBenchä½¿ç”¨é«˜çº§è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œè§†é¢‘æ³¨é‡Šå’ŒåŒ…å«å¹²æ‰°é¡¹çš„QAç”Ÿæˆï¼Œæ—¨åœ¨ç®€åŒ–åŸºå‡†æµ‹è¯•æ•°æ®åˆæˆå’Œè´¨é‡è¯„ä¼°æµç¨‹ï¼Œå‡å°‘å¯¹äººå·¥æ³¨é‡Šçš„ä¾èµ–ã€‚å¯¹ç›®å‰é¡¶å°–çš„è§†é¢‘MLLMsçš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºå…¶åœ¨è·¨æ¨¡æ€å’Œæ—¶é—´å¯¹é½æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ï¼Œå¼ºè°ƒéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ä»¥å®ç°æ›´äººæ€§åŒ–çš„ç†è§£ã€‚HumanVBenchå·²å¼€æºä»¥ä¿ƒè¿›æœªæ¥è§†é¢‘MLLMsçš„è¿›æ­¥å’Œå®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Multimedia Large Language Models (MLLMs)åœ¨è§†é¢‘ç†è§£æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å¯¹è±¡å’ŒåŠ¨ä½œè¯†åˆ«ï¼Œå¿½è§†äº†äººç±»æƒ…æ„Ÿçš„å¤æ‚æ€§ã€‚</li>
<li>HumanVBenchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ—¨åœ¨å¡«è¡¥è¿™äº›è¯„ä¼°ç©ºç™½ã€‚</li>
<li>HumanVBenchåŒ…å«ç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ï¼Œæ¶µç›–å†…åœ¨æƒ…æ„Ÿå’Œå¤–åœ¨è¡¨ç°çš„å¤šä¸ªæ–¹é¢ã€‚</li>
<li>HumanVBenchä½¿ç”¨è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œè§†é¢‘æ³¨é‡Šå’ŒQAç”Ÿæˆï¼Œä»¥å‡å°‘äººå·¥å¹²é¢„ã€‚</li>
<li>å¯¹é¡¶å°–MLLMsçš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºå…¶åœ¨è·¨æ¨¡æ€å’Œæ—¶é—´å¯¹é½æ–¹é¢çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53cc7f9a310986678d9cd3b641524e89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a0631a4f5c771b320709a53dfbac91c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9430fe6c748ae3cc3954141adc32df25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-baf8cb80579d87d8c190e8f8c910e76f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GQSA-Group-Quantization-and-Sparsity-for-Accelerating-Large-Language-Model-Inference"><a href="#GQSA-Group-Quantization-and-Sparsity-for-Accelerating-Large-Language-Model-Inference" class="headerlink" title="GQSA: Group Quantization and Sparsity for Accelerating Large Language   Model Inference"></a>GQSA: Group Quantization and Sparsity for Accelerating Large Language   Model Inference</h2><p><strong>Authors:Chao Zeng, Songwei Liu, Shu Yang, Fangmin Chen, Xing Mei, Lean Fu</strong></p>
<p>With the rapid growth in the scale and complexity of large language models (LLMs), the costs of training and inference have risen substantially. Model compression has emerged as a mainstream solution to reduce memory usage and computational overhead. This paper presents Group Quantization and Sparse Acceleration (\textbf{GQSA}), a novel compression technique tailored for LLMs. Traditional methods typically focus exclusively on either quantization or sparsification, but relying on a single strategy often results in significant performance loss at high compression rates. In contrast, GQSA integrates quantization and sparsification in a tightly coupled manner, leveraging GPU-friendly structured group sparsity and quantization for efficient acceleration. The proposed method consists of three key steps. First, GQSA applies group structured pruning to adhere to GPU-friendly sparse pattern constraints. Second, a two-stage sparsity-aware training process is employed to maximize performance retention after compression. Finally, the framework adopts the Block Sparse Row (BSR) format to enable practical deployment and efficient execution. Experimental results on the LLaMA model family show that GQSA achieves an excellent balance between model speed and accuracy. Furthermore, on the latest LLaMA-3 and LLaMA-3.1 models, GQSA outperforms existing LLM compression techniques significantly. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§„æ¨¡å’Œå¤æ‚æ€§çš„å¿«é€Ÿå¢é•¿ï¼Œè®­ç»ƒå’Œæ¨ç†çš„æˆæœ¬ä¹Ÿå¤§å¹…å¢åŠ ã€‚æ¨¡å‹å‹ç¼©å·²æˆä¸ºå‡å°‘å†…å­˜ä½¿ç”¨å’Œè®¡ç®—å¼€é”€çš„ä¸»æµè§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºäº†Group Quantization and Sparse Accelerationï¼ˆGQSAï¼‰è¿™ä¸€é’ˆå¯¹LLMçš„æ–°å‹å‹ç¼©æŠ€æœ¯ã€‚ä¼ ç»Ÿçš„æ–¹æ³•é€šå¸¸åªä¸“æ³¨äºé‡åŒ–æˆ–ç¨€ç–åŒ–ï¼Œä½†åœ¨é«˜å‹ç¼©ç‡ä¸‹ï¼Œä»…ä¾èµ–å•ä¸€ç­–ç•¥å¾€å¾€ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGQSAç´§å¯†åœ°å°†é‡åŒ–å’Œç¨€ç–åŒ–ç»“åˆåœ¨ä¸€èµ·ï¼Œåˆ©ç”¨GPUå‹å¥½çš„ç»“æ„åŒ–ç»„ç¨€ç–å’Œé‡åŒ–æ¥å®ç°é«˜æ•ˆåŠ é€Ÿã€‚è¯¥æ–¹æ³•ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ã€‚é¦–å…ˆï¼ŒGQSAåº”ç”¨ç»„ç»“æ„åŒ–ä¿®å‰ªä»¥ç¬¦åˆGPUå‹å¥½çš„ç¨€ç–æ¨¡å¼çº¦æŸã€‚å…¶æ¬¡ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µç¨€ç–æ„ŸçŸ¥è®­ç»ƒè¿‡ç¨‹ï¼Œä»¥åœ¨å‹ç¼©åæœ€å¤§åŒ–æ€§èƒ½ä¿ç•™ã€‚æœ€åï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å—ç¨€ç–è¡Œï¼ˆBSRï¼‰æ ¼å¼ï¼Œä»¥å®ç°å®é™…éƒ¨ç½²å’Œé«˜æ•ˆæ‰§è¡Œã€‚åœ¨LLaMAæ¨¡å‹å®¶æ—ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGQSAåœ¨æ¨¡å‹é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´è¾¾åˆ°äº†å“è¶Šå¹³è¡¡ã€‚æ­¤å¤–ï¼Œåœ¨æœ€æ–°çš„LLaMA-3å’ŒLLaMA-3.1æ¨¡å‹ä¸Šï¼ŒGQSAæ˜¾è‘—ä¼˜äºç°æœ‰çš„LLMå‹ç¼©æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17560v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§„æ¨¡å’Œå¤æ‚æ€§è¿…é€Ÿå¢é•¿ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†æˆæœ¬å¤§å¹…ä¸Šå‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ¨¡å‹å‹ç¼©æˆä¸ºä¸»æµè§£å†³æ–¹æ¡ˆï¼Œä»¥å‡å°‘å†…å­˜ä½¿ç”¨å’Œè®¡ç®—å¼€é”€ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹LLMçš„æ–°å‹å‹ç¼©æŠ€æœ¯â€”â€”é›†å›¢é‡åŒ–ä¸ç¨€ç–åŠ é€Ÿï¼ˆGQSAï¼‰ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸åªä¸“æ³¨äºé‡åŒ–æˆ–ç¨€ç–åŒ–ï¼Œä½†å•ä¸€ç­–ç•¥åœ¨é«˜å‹ç¼©ç‡ä¸‹å¾€å¾€ä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½æŸå¤±ã€‚ç›¸åï¼ŒGQSAç´§å¯†ç»“åˆé‡åŒ–å’Œç¨€ç–åŒ–ï¼Œåˆ©ç”¨GPUå‹å¥½çš„ç»“æ„åŒ–é›†å›¢ç¨€ç–æ€§å’Œé‡åŒ–è¿›è¡Œé«˜æ•ˆåŠ é€Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGQSAåœ¨æ¨¡å‹é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ï¼Œå¹¶ä¸”åœ¨æœ€æ–°çš„LLAMA-3å’ŒLLAMA-3.1æ¨¡å‹ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„LLMå‹ç¼©æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´è®­ç»ƒå’Œæ¨ç†æˆæœ¬ä¸Šå‡çš„é—®é¢˜ï¼Œæ¨¡å‹å‹ç¼©æ˜¯ä¸»æµè§£å†³æ–¹æ¡ˆã€‚</li>
<li>GQSAæ˜¯ä¸€ç§æ–°å‹çš„LLMå‹ç¼©æŠ€æœ¯ï¼Œç»“åˆäº†é‡åŒ–å’Œç¨€ç–åŒ–ï¼Œä»¥æé«˜æ€§èƒ½å¹¶å‡å°‘èµ„æºæ¶ˆè€—ã€‚</li>
<li>ä¼ ç»Ÿå‹ç¼©æ–¹æ³•å¾€å¾€åªä¸“æ³¨äºé‡åŒ–æˆ–ç¨€ç–åŒ–ï¼Œå•ä¸€ç­–ç•¥åœ¨é«˜å‹ç¼©ç‡ä¸‹å¯èƒ½å¯¼è‡´æ€§èƒ½æŸå¤±ã€‚</li>
<li>GQSAåˆ©ç”¨GPUå‹å¥½çš„ç»“æ„åŒ–é›†å›¢ç¨€ç–æ€§å’Œé‡åŒ–è¿›è¡ŒåŠ é€Ÿã€‚</li>
<li>GQSAåŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šé›†å›¢ç»“æ„åŒ–ä¿®å‰ªã€ä¸¤é˜¶æ®µç¨€ç–æ„ŸçŸ¥è®­ç»ƒé‡‡ç”¨å—ç¨€ç–è¡Œï¼ˆBSRï¼‰æ ¼å¼ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒGQSAåœ¨æ¨¡å‹é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—äº†è‰¯å¥½çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d1b65ad9f83ab898b90876af2fc75e28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-749595d9b077dad46ff5382b0e422a49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e921a64caaa58efea007fc95356e2aeb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f6d7e8d343dc6a78b4379c833455816.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e706a583406b3c745efb863a5011761.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb60e3496db4c3c09d5725fcda4de77b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Retention-Score-Quantifying-Jailbreak-Risks-for-Vision-Language-Models"><a href="#Retention-Score-Quantifying-Jailbreak-Risks-for-Vision-Language-Models" class="headerlink" title="Retention Score: Quantifying Jailbreak Risks for Vision Language Models"></a>Retention Score: Quantifying Jailbreak Risks for Vision Language Models</h2><p><strong>Authors:Zaitang Li, Pin-Yu Chen, Tsung-Yi Ho</strong></p>
<p>The emergence of Vision-Language Models (VLMs) is a significant advancement in integrating computer vision with Large Language Models (LLMs) to enhance multi-modal machine learning capabilities. However, this progress has also made VLMs vulnerable to sophisticated adversarial attacks, raising concerns about their reliability. The objective of this paper is to assess the resilience of VLMs against jailbreak attacks that can compromise model safety compliance and result in harmful outputs. To evaluate a VLMâ€™s ability to maintain its robustness against adversarial input perturbations, we propose a novel metric called the \textbf{Retention Score}. Retention Score is a multi-modal evaluation metric that includes Retention-I and Retention-T scores for quantifying jailbreak risks in visual and textual components of VLMs. Our process involves generating synthetic image-text pairs using a conditional diffusion model. These pairs are then predicted for toxicity score by a VLM alongside a toxicity judgment classifier. By calculating the margin in toxicity scores, we can quantify the robustness of the VLM in an attack-agnostic manner. Our work has four main contributions. First, we prove that Retention Score can serve as a certified robustness metric. Second, we demonstrate that most VLMs with visual components are less robust against jailbreak attacks than the corresponding plain VLMs. Additionally, we evaluate black-box VLM APIs and find that the security settings in Google Gemini significantly affect the score and robustness. Moreover, the robustness of GPT4V is similar to the medium settings of Gemini. Finally, our approach offers a time-efficient alternative to existing adversarial attack methods and provides consistent model robustness rankings when evaluated on VLMs including MiniGPT-4, InstructBLIP, and LLaVA. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‡ºç°æ˜¯è®¡ç®—æœºè§†è§‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é›†æˆçš„ä¸€å¤§è¿›æ­¥ï¼Œè¿™å¢å¼ºäº†å¤šæ¨¡æ€æœºå™¨å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ä¸€è¿›å±•ä¹Ÿä½¿å¾—VLMå®¹æ˜“å—åˆ°é«˜çº§å¯¹æŠ—æ€§æ”»å‡»çš„å¨èƒï¼Œäººä»¬å¯¹å®ƒä»¬çš„å¯é æ€§æå‡ºæ‹…å¿§ã€‚æœ¬æ–‡çš„ç›®æ ‡æ˜¯è¯„ä¼°VLMå¯¹æŠ—ç›‘ç‹±çªç ´æ”»å‡»ï¼ˆå¯èƒ½å±åŠæ¨¡å‹å®‰å…¨åˆè§„æ€§å¹¶äº§ç”Ÿæœ‰å®³è¾“å‡ºï¼‰çš„éŸ§æ€§ã€‚ä¸ºäº†è¯„ä¼°VLMåœ¨å¯¹æŠ—å¯¹æŠ—æ€§è¾“å…¥æ‰°åŠ¨æ—¶çš„ç¨³å¥æ€§ç»´æŒèƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†ï¼Œç§°ä¸ºâ€œä¿ç•™åˆ†æ•°â€ã€‚ä¿ç•™åˆ†æ•°æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ç”¨äºé‡åŒ–VLMè§†è§‰å’Œæ–‡æœ¬ç»„ä»¶ä¸­çš„ç›‘ç‹±çªç ´é£é™©çš„ä¿ç•™-Iå’Œä¿ç•™-Tåˆ†æ•°ã€‚æˆ‘ä»¬çš„æµç¨‹åŒ…æ‹¬ä½¿ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆå›¾åƒæ–‡æœ¬å¯¹ã€‚è¿™äº›å¯¹éšåè¢«VLMå’Œä¸€ä¸ªæ¯’æ€§åˆ¤æ–­åˆ†ç±»å™¨é¢„æµ‹æ¯’æ€§åˆ†æ•°ã€‚é€šè¿‡è®¡ç®—æ¯’æ€§åˆ†æ•°çš„å·®å¼‚ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥ä¸€ç§ç‹¬ç«‹äºæ”»å‡»çš„æ–¹å¼é‡åŒ–VLMçš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œæœ‰å››ä¸ªä¸»è¦è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜ä¿ç•™åˆ†æ•°å¯ä»¥ä½œä¸ºç»è¿‡è®¤è¯çš„ç¨³å¥æ€§æŒ‡æ ‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œå…·æœ‰è§†è§‰åˆ†é‡çš„VLMå¤§å¤šæ•°åœ¨é¢å¯¹ç›‘ç‹±çªç ´æ”»å‡»æ—¶ï¼Œæ¯”ç›¸åº”çš„çº¯VLMæ›´ä¸ç¨³å¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†é»‘ç›’VLM APIï¼Œå¹¶å‘ç°Google Geminiä¸­çš„å®‰å…¨è®¾ç½®ä¼šæ˜¾è‘—å½±å“åˆ†æ•°å’Œç¨³å¥æ€§ã€‚è€Œä¸”GPT4Vçš„ç¨³å¥æ€§ä¸Geminiçš„ä¸­ç­‰è®¾ç½®ç›¸ä¼¼ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†å¯¹ç°æœ‰å¯¹æŠ—æ”»å‡»æ–¹æ³•çš„æ—¶é—´æ•ˆç‡æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶åœ¨åŒ…æ‹¬MiniGPT-4ã€InstructBLIPå’ŒLLaVAçš„VLMä¸Šæä¾›äº†ä¸€è‡´çš„æ¨¡å‹ç¨³å¥æ€§æ’åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17544v1">PDF</a> 14 pages, 8 figures, AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é›†æˆè®¡ç®—æœºè§†è§‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ–¹é¢çš„æ˜¾è‘—è¿›å±•ï¼Œå¹¶æ¢è®¨äº†å…¶å¯¹æŠ—æ€§æ”»å‡»çš„è„†å¼±æ€§ã€‚æ–‡ç« æ—¨åœ¨è¯„ä¼°VLMså¯¹æŠ—èƒ½å¤Ÿå±å®³æ¨¡å‹å®‰å…¨æ€§çš„ç›‘ç‹±çªç ´æ”»å‡»çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”ç•™å­˜ç‡åˆ†æ•°ï¼Œè¯¥æŒ‡æ ‡æ˜¯ä¸€ç§å¤šæ¨¡å¼è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–VLMåœ¨è§†è§‰å’Œæ–‡æœ¬ç»„ä»¶ä¸­çš„ç›‘ç‹±çªç ´é£é™©ã€‚é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ–‡æœ¬å¯¹å¹¶é¢„æµ‹å…¶æ¯’æ€§åˆ†æ•°ï¼Œå¯ä»¥é‡åŒ–VLMåœ¨æ”»å‡»æ— å…³æ–¹å¼ä¸‹çš„ç¨³å¥æ€§ã€‚æ–‡ç« çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šè¯æ˜äº†ç•™å­˜ç‡åˆ†æ•°å¯ä»¥ä½œä¸ºç»è¿‡è®¤è¯çš„ç¨³å¥æ€§æŒ‡æ ‡ï¼›å‘ç°å¤§å¤šæ•°å¸¦æœ‰è§†è§‰ç»„ä»¶çš„VLMå¯¹ç›‘ç‹±çªç ´æ”»å‡»çš„ç¨³å¥æ€§è¾ƒå·®ï¼›è¯„ä¼°äº†è°·æ­ŒåŒå­æ˜Ÿç­‰é»‘ç›’å­VLM APIçš„å®‰å…¨æ€§è®¾ç½®å¯¹ç•™å­˜ç‡åˆ†æ•°å’Œç¨³å¥æ€§çš„å½±å“ï¼›GPT4Vçš„ç¨³å¥æ€§ä¸åŒå­æ˜Ÿçš„ä¸­ç­‰è®¾ç½®ç›¸ä¼¼ï¼›è¯¥æ–¹æ³•æä¾›äº†ç°æœ‰å¯¹æŠ—æ€§æ”»å‡»æ–¹æ³•çš„æ—¶æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶åœ¨åŒ…æ‹¬MiniGPT-4ã€InstructBLIPå’ŒLLaVAç­‰VLMsä¸Šçš„è¯„ä¼°ä¸­æä¾›äº†ä¸€è‡´çš„æ¨¡å‹ç¨³å¥æ€§æ’åã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ˜¯è®¡ç®—æœºè§†è§‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆçš„æ˜¾è‘—è¿›å±•ï¼Œä½†æ˜“å—åˆ°å¤æ‚çš„å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚</li>
<li>æ–‡ç« æ—¨åœ¨è¯„ä¼°VLMså¯¹æŠ—ç›‘ç‹±çªç ´æ”»å‡»çš„èƒ½åŠ›ï¼Œæå‡ºç•™å­˜ç‡åˆ†æ•°ä½œä¸ºæ–°çš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>ç•™å­˜ç‡åˆ†æ•°åŒ…æ‹¬ç•™å­˜-Iå’Œç•™å­˜-Tåˆ†æ•°ï¼Œç”¨äºé‡åŒ–VLMåœ¨è§†è§‰å’Œæ–‡æœ¬ç»„ä»¶ä¸­çš„ç›‘ç‹±çªç ´é£é™©ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ–‡æœ¬å¯¹å¹¶é¢„æµ‹å…¶æ¯’æ€§åˆ†æ•°ï¼Œå¯ä»¥è¯„ä¼°VLMçš„ç¨³å¥æ€§ã€‚</li>
<li>å¤§å¤šæ•°å¸¦æœ‰è§†è§‰ç»„ä»¶çš„VLMå¯¹ç›‘ç‹±çªç ´æ”»å‡»çš„ç¨³å¥æ€§è¾ƒå·®ã€‚</li>
<li>è°·æ­ŒåŒå­æ˜Ÿç­‰é»‘ç›’å­VLM APIçš„å®‰å…¨æ€§è®¾ç½®å½±å“ç•™å­˜ç‡åˆ†æ•°å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-acd4aa82f23502c10c0080969883856d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80547aacd41db9dafa8abfc97625f4fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6eea7e5e3d069cda73d54b0b39356590.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Boosting-LLM-via-Learning-from-Data-Iteratively-and-Selectively"><a href="#Boosting-LLM-via-Learning-from-Data-Iteratively-and-Selectively" class="headerlink" title="Boosting LLM via Learning from Data Iteratively and Selectively"></a>Boosting LLM via Learning from Data Iteratively and Selectively</h2><p><strong>Authors:Qi Jia, Siyu Ren, Ziheng Qin, Fuzhao Xue, Jinjie Ni, Yang You</strong></p>
<p>Datasets nowadays are generally constructed from multiple sources and using different synthetic techniques, making data de-noising and de-duplication crucial before being used for post-training. In this work, we propose to perform instruction tuning by iterative data selection (\ApproachName{}). We measure the quality of a sample from complexity and diversity simultaneously. Instead of calculating the complexity score once for all before fine-tuning, we highlight the importance of updating this model-specific score during fine-tuning to accurately accommodate the dynamic changes of the model. On the other hand, the diversity score is defined on top of the samplesâ€™ responses under the consideration of their informativeness. IterIT integrates the strengths of both worlds by iteratively updating the complexity score for the top-ranked samples and greedily selecting the ones with the highest complexity-diversity score. Experiments on multiple instruction-tuning data demonstrate consistent improvements of IterIT over strong baselines. Moreover, our approach also generalizes well to domain-specific scenarios and different backbone models. All resources will be available at <a target="_blank" rel="noopener" href="https://github.com/JiaQiSJTU/IterIT">https://github.com/JiaQiSJTU/IterIT</a>. </p>
<blockquote>
<p>å½“å‰ï¼Œæ•°æ®é›†é€šå¸¸é€šè¿‡å¤šç§æ¥æºå’Œä¸åŒåˆæˆæŠ€æœ¯æ„å»ºï¼Œå› æ­¤åœ¨ç”¨äºåè®­ç»ƒä¹‹å‰ï¼Œæ•°æ®å»å™ªå’Œå»é‡è‡³å…³é‡è¦ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡è¿­ä»£æ•°æ®é€‰æ‹©ï¼ˆIterITï¼‰æ‰§è¡ŒæŒ‡ä»¤è°ƒæ•´ã€‚æˆ‘ä»¬ä»å¤æ‚æ€§å’Œå¤šæ ·æ€§ä¸¤ä¸ªæ–¹é¢æ¥è¡¡é‡æ ·æœ¬çš„è´¨é‡ã€‚æˆ‘ä»¬å¼ºè°ƒåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ›´æ–°è¿™ç§æ¨¡å‹ç‰¹å®šè¯„åˆ†çš„é‡è¦æ€§ï¼Œä»¥ä¾¿å‡†ç¡®é€‚åº”æ¨¡å‹çš„åŠ¨æ€å˜åŒ–ï¼Œè€Œä¸æ˜¯åœ¨å¾®è°ƒä¹‹å‰å¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œä¸€æ¬¡å¤æ‚æ€§è¯„åˆ†ã€‚å¦ä¸€æ–¹é¢ï¼Œå¤šæ ·æ€§è¯„åˆ†æ˜¯åœ¨è€ƒè™‘æ ·æœ¬ä¿¡æ¯çš„åŸºç¡€ä¸Šå®šä¹‰çš„ã€‚IterITé€šè¿‡è¿­ä»£æ›´æ–°æœ€é«˜æ’åæ ·æœ¬çš„å¤æ‚æ€§è¯„åˆ†å¹¶è´ªå©ªåœ°é€‰æ‹©å…·æœ‰æœ€é«˜å¤æ‚æ€§-å¤šæ ·æ€§è¯„åˆ†çš„æ ·æœ¬ï¼Œèåˆäº†ä¸¤è€…çš„ä¼˜åŠ¿ã€‚åœ¨å¤šä¸ªæŒ‡ä»¤è°ƒæ•´æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒIterITåœ¨å¼ºåŸºçº¿ä¹‹ä¸Šå®ç°äº†ä¸€è‡´æ€§çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿå¾ˆå¥½åœ°é€‚ç”¨äºç‰¹å®šé¢†åŸŸçš„åœºæ™¯å’Œä¸åŒçš„ä¸»å¹²æ¨¡å‹ã€‚æ‰€æœ‰èµ„æºéƒ½å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JiaQiSJTU/IterIT%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/JiaQiSJTU/IterITä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17365v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é€šè¿‡è¿­ä»£æ•°æ®é€‰æ‹©è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼ŒåŒæ—¶è€ƒè™‘æ ·æœ¬çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œæå‡ºäº†IterITæ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åŠ¨æ€æ›´æ–°æ¨¡å‹ç‰¹å®šçš„å¤æ‚æ€§å¾—åˆ†ï¼Œå¹¶åŸºäºæ ·æœ¬å“åº”çš„ä¿¡æ¯æ€§å®šä¹‰å¤šæ ·æ€§å¾—åˆ†ã€‚é€šè¿‡è¿­ä»£æ›´æ–°é«˜æ’åæ ·æœ¬çš„å¤æ‚æ€§å¾—åˆ†å¹¶è´ªå©ªé€‰æ‹©å…·æœ‰æœ€é«˜å¤æ‚æ€§-å¤šæ ·æ€§å¾—åˆ†çš„æ ·æœ¬ï¼ŒIterITå®ç°äº†å¯¹å¤šä¸ªæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†çš„æŒç»­æ”¹è¿›ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„åŸŸç‰¹å®šåœºæ™¯å’Œæ¨¡å‹é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é›†çš„æ„å»ºé€šå¸¸æ¶‰åŠå¤šæºæ•°æ®å’Œä¸åŒçš„åˆæˆæŠ€æœ¯ï¼Œå› æ­¤æ•°æ®å»å™ªå’Œå»é‡éå¸¸é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šè¿‡è¿­ä»£æ•°æ®é€‰æ‹©è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´çš„æ–¹æ³•ï¼ˆIterITï¼‰ã€‚</li>
<li>IterITåŒæ—¶è€ƒè™‘æ ·æœ¬çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>è°ƒè¯•è¿‡ç¨‹ä¸­åŠ¨æ€æ›´æ–°æ¨¡å‹ç‰¹å®šçš„å¤æ‚æ€§å¾—åˆ†ã€‚</li>
<li>IterITé€šè¿‡è¿­ä»£æ›´æ–°é«˜æ’åæ ·æœ¬çš„å¤æ‚æ€§å¾—åˆ†å¹¶é€‰æ‹©å…·æœ‰æœ€é«˜å¤æ‚æ€§-å¤šæ ·æ€§å¾—åˆ†çš„æ ·æœ¬ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒIterITåœ¨å¤šä¸ªæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ä¸ŠæŒç»­æ”¹è¿›ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œé€‚ç”¨äºä¸åŒçš„é¢†åŸŸå’ŒèƒŒæ™¯æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d67dd8156f63ced26ce4ca812dc39ce6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f967a10d28392f16a5c6627a17ef47b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8ecb63d8efb1262b84cb150632c7d41d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2377490e35e56927542ff99a69f0496.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ACECode-A-Reinforcement-Learning-Framework-for-Aligning-Code-Efficiency-and-Correctness-in-Code-Language-Models"><a href="#ACECode-A-Reinforcement-Learning-Framework-for-Aligning-Code-Efficiency-and-Correctness-in-Code-Language-Models" class="headerlink" title="ACECode: A Reinforcement Learning Framework for Aligning Code Efficiency   and Correctness in Code Language Models"></a>ACECode: A Reinforcement Learning Framework for Aligning Code Efficiency   and Correctness in Code Language Models</h2><p><strong>Authors:Chengran Yang, Hong Jin Kang, Jieke Shi, David Lo</strong></p>
<p>CodeLLMs have demonstrated remarkable advancements in software engineering tasks. However, while these models can generate functionally correct code, they often produce code that is inefficient in terms of runtime. This inefficiency is particularly problematic in resource-constrained environments, impacting software performance and sustainability.   Existing approaches for optimizing code efficiency for CodeLLMs like SOAP and PIE exhibit certain limitations. SOAP requires a compatible execution environment and predefined test cases for iterative code modification, while PIE focuses on instruction tuning, improving efficiency but compromising correctness. These shortcomings highlight the need for a fine-tuning framework that optimizes both efficiency and correctness without relying on predefined test cases or specific execution environments.   To bridge this gap, we introduce ACECode, a reinforcement learning-based fine-tuning framework that aligns CodeLLMs with dual objectives of efficiency and correctness. ACECode combines three key steps: (1) generating code with an actor CodeLLM, (2) calculating a training-free reward signal derived from code execution feedback for each generated code, and (3) optimizing the CodeLLM via Proximal Policy Optimization (PPO) algorithm. This reward signal enables joint assessment of efficiency and correctness without manual labeling.   We evaluate ACECode by fine-tuning four SOTA (state-of-the-art) CodeLLMs and comparing their code with three baselines: original, instruction-tuned, and PIE-tuned CodeLLMs. Extensive experiment results suggest that \tool{} significantly improves the efficiency and correctness of generated code against all baselines for all CodeLLMs. Specifically, CodeLLMs fine-tuned with ACECode improve pass@1 by 1.84% to 14.51% and reduce runtime in 65% to 72% of cases compared to original CodeLLMs. </p>
<blockquote>
<p>CodeLLMsåœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè™½ç„¶è¿™äº›æ¨¡å‹èƒ½å¤Ÿç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„ä»£ç ï¼Œä½†å®ƒä»¬å¸¸å¸¸ç”Ÿæˆçš„ä»£ç åœ¨è¿è¡Œæ—¶æ•ˆç‡ä¸é«˜ã€‚è¿™ç§ä½æ•ˆåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å°¤å…¶æˆé—®é¢˜ï¼Œå½±å“äº†è½¯ä»¶æ€§èƒ½å’Œå¯æŒç»­æ€§ã€‚ç°æœ‰çš„é’ˆå¯¹CodeLLMä¼˜åŒ–ä»£ç æ•ˆç‡çš„æ–¹æ³•ï¼Œå¦‚SOAPå’ŒPIEï¼Œéƒ½è¡¨ç°å‡ºä¸€å®šçš„å±€é™æ€§ã€‚SOAPéœ€è¦å…¼å®¹çš„æ‰§è¡Œç¯å¢ƒå’Œé¢„è®¾æµ‹è¯•ç”¨ä¾‹æ¥è¿›è¡Œè¿­ä»£ä»£ç ä¿®æ”¹ï¼Œè€ŒPIEåˆ™ä¸“æ³¨äºæŒ‡ä»¤è°ƒæ•´ä»¥æé«˜æ•ˆç‡ï¼Œä½†å¯èƒ½ä¼šç‰ºç‰²æ­£ç¡®æ€§ã€‚è¿™äº›ç¼ºç‚¹å‡¸æ˜¾äº†éœ€è¦ä¸€ä¸ªå¾®è°ƒæ¡†æ¶æ¥åŒæ—¶ä¼˜åŒ–æ•ˆç‡å’Œæ­£ç¡®æ€§ï¼Œè€Œæ— éœ€ä¾èµ–é¢„è®¾æµ‹è¯•ç”¨ä¾‹æˆ–ç‰¹å®šçš„æ‰§è¡Œç¯å¢ƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ACECodeï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿CodeLLMç¬¦åˆæ•ˆç‡å’Œæ­£ç¡®æ€§çš„åŒé‡ç›®æ ‡ã€‚ACECodeç»“åˆäº†ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šï¼ˆ1ï¼‰ä½¿ç”¨actor CodeLLMç”Ÿæˆä»£ç ï¼›ï¼ˆ2ï¼‰è®¡ç®—åŸºäºä»£ç æ‰§è¡Œåé¦ˆçš„æ— éœ€è®­ç»ƒçš„å¥–åŠ±ä¿¡å·ï¼Œç”¨äºè¯„ä¼°æ¯ä¸ªç”Ÿæˆä»£ç ï¼›ï¼ˆ3ï¼‰é€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ç®—æ³•ä¼˜åŒ–CodeLLMã€‚è¯¥å¥–åŠ±ä¿¡å·èƒ½å¤Ÿåœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼ŒåŒæ—¶å¯¹æ•ˆç‡å’Œæ­£ç¡®æ€§è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬é€šè¿‡å¾®è°ƒå››ä¸ªæœ€å…ˆè¿›çš„CodeLLMå¹¶ä¸å…¶åŸå§‹ä»£ç ã€æŒ‡ä»¤è°ƒæ•´ä»£ç å’ŒPIEè°ƒæ•´ä»£ç è¿›è¡Œæ¯”è¾ƒï¼Œæ¥è¯„ä¼°ACECodeçš„æ•ˆæœã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ‰€æœ‰åŸºçº¿ç›¸æ¯”ï¼ŒACECodeæ˜¾è‘—æé«˜äº†ç”Ÿæˆä»£ç çš„æ•ˆç‡æ­£ç¡®æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨ACECodeå¾®è°ƒçš„CodeLLMåœ¨pass@1ä¸Šæé«˜äº†1.84%è‡³14.51%ï¼Œå¹¶åœ¨65%è‡³72%çš„æƒ…å†µä¸‹å‡å°‘äº†è¿è¡Œæ—¶é—´ï¼Œä¸åŸå§‹CodeLLMç›¸æ¯”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17264v1">PDF</a> </p>
<p><strong>Summary</strong><br>ä»£ç LLMåœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬ç”Ÿæˆçš„ä»£ç å¾€å¾€åœ¨è¿è¡Œæ•ˆç‡æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚ç°æœ‰ä¼˜åŒ–æ–¹æ³•å¦‚SOAPå’ŒPIEå­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦ä¸€ç§èƒ½å¤ŸåŒæ—¶ä¼˜åŒ–æ•ˆç‡å’Œæ­£ç¡®æ€§çš„ç²¾ç»†è°ƒæ•´æ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥ACECodeï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„ç²¾ç»†è°ƒæ•´æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®æ­¥éª¤ä¼˜åŒ–CodeLLMï¼Œä»¥å®ç°å¯¹æ•ˆç‡å’Œæ­£ç¡®æ€§çš„åŒé‡ç›®æ ‡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒACECodeèƒ½æ˜¾è‘—æé«˜ç”Ÿæˆä»£ç çš„æ•ˆç‡æ­£ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç LLMåœ¨ç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„ä»£ç æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†è¿è¡Œæ•ˆç‡æœ‰å¾…æé«˜ã€‚</li>
<li>ç°æœ‰ä¼˜åŒ–æ–¹æ³•å¦‚SOAPå’ŒPIEå­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ›´ç²¾ç»†çš„è°ƒæ•´æ¡†æ¶ã€‚</li>
<li>ACECodeæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„ç²¾ç»†è°ƒæ•´æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–CodeLLMçš„æ•ˆç‡ä¸æ­£ç¡®æ€§ã€‚</li>
<li>ACECodeé€šè¿‡ä¸‰ä¸ªå…³é”®æ­¥éª¤å®ç°ä¼˜åŒ–ï¼šç”Ÿæˆä»£ç ã€è®¡ç®—å¥–åŠ±ä¿¡å·ã€ä½¿ç”¨PPOç®—æ³•ä¼˜åŒ–CodeLLMã€‚</li>
<li>ACECodeèƒ½è‡ªåŠ¨è¯„ä¼°ä»£ç æ•ˆç‡å’Œæ­£ç¡®æ€§ï¼Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚</li>
<li>å¯¹æ¯”å®éªŒè¡¨æ˜ï¼ŒACECodeæ˜¾è‘—æé«˜ç”Ÿæˆä»£ç çš„æ•ˆç‡ä¸æ­£ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯ä¸æœ€æ–°CodeLLMç›¸æ¯”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2d43864e5001c4c3dcf5c3f60b2c6698.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-edcb7dabcf6a2031c1fb568ca7367f24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a887110a54705f393a2015c40b461563.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GCS-M3VLT-Guided-Context-Self-Attention-based-Multi-modal-Medical-Vision-Language-Transformer-for-Retinal-Image-Captioning"><a href="#GCS-M3VLT-Guided-Context-Self-Attention-based-Multi-modal-Medical-Vision-Language-Transformer-for-Retinal-Image-Captioning" class="headerlink" title="GCS-M3VLT: Guided Context Self-Attention based Multi-modal Medical   Vision Language Transformer for Retinal Image Captioning"></a>GCS-M3VLT: Guided Context Self-Attention based Multi-modal Medical   Vision Language Transformer for Retinal Image Captioning</h2><p><strong>Authors:Teja Krishna Cherukuri, Nagur Shareef Shaik, Jyostna Devi Bodapati, Dong Hye Ye</strong></p>
<p>Retinal image analysis is crucial for diagnosing and treating eye diseases, yet generating accurate medical reports from images remains challenging due to variability in image quality and pathology, especially with limited labeled data. Previous Transformer-based models struggled to integrate visual and textual information under limited supervision. In response, we propose a novel vision-language model for retinal image captioning that combines visual and textual features through a guided context self-attention mechanism. This approach captures both intricate details and the global clinical context, even in data-scarce scenarios. Extensive experiments on the DeepEyeNet dataset demonstrate a 0.023 BLEU@4 improvement, along with significant qualitative advancements, highlighting the effectiveness of our model in generating comprehensive medical captions. </p>
<blockquote>
<p>è§†ç½‘è†œå›¾åƒåˆ†æåœ¨çœ¼ç–¾çš„è¯Šæ–­å’Œæ²»ç–—ä¸­è‡³å…³é‡è¦ï¼Œç„¶è€Œï¼Œç”±äºå›¾åƒè´¨é‡å’Œç—…ç†å­¦å˜åŒ–å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡è®°æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œä»å›¾åƒç”Ÿæˆå‡†ç¡®çš„åŒ»ç–—æŠ¥å‘Šä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¹‹å‰çš„åŸºäºTransformerçš„æ¨¡å‹åœ¨æœ‰é™ç›‘ç£ä¸‹éš¾ä»¥æ•´åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè§†ç½‘è†œå›¾åƒæè¿°çš„æ–°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å¼•å¯¼ä¸Šä¸‹æ–‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ç»“åˆè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•å³ä½¿åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸‹ä¹Ÿèƒ½æ•æ‰åˆ°å¤æ‚çš„ç»†èŠ‚å’Œå…¨å±€ä¸´åºŠèƒŒæ™¯ã€‚åœ¨DeepEyeNetæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBLEU@4æé«˜äº†0.023ï¼ŒåŒæ—¶åœ¨è´¨é‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œçªå‡ºäº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨ç”Ÿæˆå…¨é¢çš„åŒ»ç–—æè¿°ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17251v1">PDF</a> This paper has been accepted for presentation at the IEEE   International Conference on Acoustics, Speech, and Signal Processing (ICASSP   2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„è·¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºè§†ç½‘è†œå›¾åƒæè¿°ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å¯¼ä¸Šä¸‹æ–‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ï¼Œå³ä½¿åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½æ•æ‰ç»†èŠ‚å’Œå…¨å±€ä¸´åºŠèƒŒæ™¯ã€‚åœ¨DeepEyeNetæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œç”Ÿæˆäº†å…¨é¢çš„åŒ»å­¦æè¿°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†ç½‘è†œå›¾åƒåˆ†æå¯¹è¯Šæ–­å’Œæ²»ç–—çœ¼ç—…è‡³å…³é‡è¦ã€‚</li>
<li>ç”Ÿæˆå‡†ç¡®çš„åŒ»ç–—æŠ¥å‘Šæ˜¯è§†ç½‘è†œå›¾åƒåˆ†æçš„ä¸€å¤§æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå›¾åƒè´¨é‡å’Œç—…ç†å­¦çš„å·®å¼‚ä»¥åŠæ ‡è®°æ•°æ®çš„æœ‰é™æ€§ã€‚</li>
<li>ä»¥å¾€çš„Transformeræ¨¡å‹åœ¨æœ‰é™çš„ç›‘ç£ä¸‹éš¾ä»¥æ•´åˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>æ–°æ¨¡å‹é€šè¿‡å¼•å¯¼ä¸Šä¸‹æ–‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ç»“åˆè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ•æ‰ç»†è‡´çš„ç»†èŠ‚å’Œå…¨å±€ä¸´åºŠèƒŒæ™¯ï¼Œå³ä½¿åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å‘æŒ¥ä½œç”¨ã€‚</li>
<li>åœ¨DeepEyeNetæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ–°æ¨¡å‹åœ¨ç”ŸæˆåŒ»å­¦æè¿°æ–¹é¢æœ‰æ˜¾è‘—æ”¹è¿›ï¼ŒBLEU@4å¾—åˆ†æé«˜äº†0.023ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b87b34cc47ed519d3b573633ff708690.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bf22d0f51614b107fd3ea53e329d4f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aea00f83b6d723148386cc4bfd443c54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-267d27ef4e53417bef10e9f0a2955792.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="On-Fusing-ChatGPT-and-Ensemble-Learning-in-Discon-tinuous-Named-Entity-Recognition-in-Health-Corpora"><a href="#On-Fusing-ChatGPT-and-Ensemble-Learning-in-Discon-tinuous-Named-Entity-Recognition-in-Health-Corpora" class="headerlink" title="On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity   Recognition in Health Corpora"></a>On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity   Recognition in Health Corpora</h2><p><strong>Authors:Tzu-Chieh Chen, Wen-Yang Lin</strong></p>
<p>Named Entity Recognition has traditionally been a key task in natural language processing, aiming to identify and extract important terms from unstructured text data. However, a notable challenge for contemporary deep-learning NER models has been identifying discontinuous entities, which are often fragmented within the text. To date, methods to address Discontinuous Named Entity Recognition have not been explored using ensemble learning to the best of our knowledge. Furthermore, the rise of large language models, such as ChatGPT in recent years, has shown significant effectiveness across many NLP tasks. Most existing approaches, however, have primarily utilized ChatGPT as a problem-solving tool rather than exploring its potential as an integrative element within ensemble learning algorithms. In this study, we investigated the integration of ChatGPT as an arbitrator within an ensemble method, aiming to enhance performance on DNER tasks. Our method combines five state-of-the-art NER models with ChatGPT using custom prompt engineering to assess the robustness and generalization capabilities of the ensemble algorithm. We conducted experiments on three benchmark medical datasets, comparing our method against the five SOTA models, individual applications of GPT-3.5 and GPT-4, and a voting ensemble method. The results indicate that our proposed fusion of ChatGPT with the ensemble learning algorithm outperforms the SOTA results in the CADEC, ShARe13, and ShARe14 datasets, showcasing its potential to enhance NLP applications in the healthcare domain. </p>
<blockquote>
<p>å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä¸€ç›´æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨ä»éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ä¸­è¯†åˆ«å’Œæå–é‡è¦æœ¯è¯­ã€‚ç„¶è€Œï¼Œå¯¹äºå½“ä»£æ·±åº¦å­¦ä¹ çš„NERæ¨¡å‹æ¥è¯´ï¼Œè¯†åˆ«ä¸è¿ç»­çš„å®ä½“æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„æŒ‘æˆ˜ï¼Œè¿™äº›å®ä½“é€šå¸¸åœ¨æ–‡æœ¬ä¸­æ˜¯åˆ†æ•£çš„ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿„ä»Šä¸ºæ­¢ï¼Œå°šæœªæœ‰æ–¹æ³•å°è¯•ä½¿ç”¨é›†æˆå­¦ä¹ æ¥è§£å†³ä¸è¿ç»­çš„å‘½åå®ä½“è¯†åˆ«é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿‘å¹´æ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦å°†ChatGPTç”¨ä½œé—®é¢˜è§£å†³å·¥å…·ï¼Œè€Œæ²¡æœ‰æ¢ç´¢å…¶åœ¨é›†æˆå­¦ä¹ ç®—æ³•ä¸­ä½œä¸ºæ•´åˆå…ƒç´ çš„å¯èƒ½æ€§ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†ChatGPTä½œä¸ºä»²è£è€…åœ¨é›†æˆæ–¹æ³•ä¸­çš„æ•´åˆï¼Œæ—¨åœ¨æé«˜å…¶åœ¨DNERä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†äº”ä¸ªæœ€å…ˆè¿›çš„NERæ¨¡å‹ä¸ChatGPTï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„æç¤ºå·¥ç¨‹æ¥è¯„ä¼°é›†æˆç®—æ³•çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†åŒ»ç–—æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸äº”ä¸ªæœ€æ–°æ¨¡å‹ã€GPT-3.5å’ŒGPT-4çš„å•ç‹¬åº”ç”¨ä»¥åŠæŠ•ç¥¨é›†æˆæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„å°†ChatGPTä¸é›†æˆå­¦ä¹ ç®—æ³•èåˆçš„æ–¹æ³•åœ¨CADECã€ShARe13å’ŒShARe14æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨å¢å¼ºåŒ»ç–—ä¿å¥é¢†åŸŸNLPåº”ç”¨çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16976v1">PDF</a> 13 pages</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æ¢ç´¢äº†å°†ChatGPTä½œä¸ºé›†æˆå­¦ä¹ ç®—æ³•ä¸­çš„ä»²è£è€…ï¼Œæ—¨åœ¨æé«˜å…¶åœ¨æ–­ç»­å‘½åå®ä½“è¯†åˆ«ï¼ˆDNERï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ç»“åˆäº†äº”ç§æœ€å…ˆè¿›çš„NERæ¨¡å‹ä¸ChatGPTï¼Œä½¿ç”¨è‡ªå®šä¹‰æç¤ºå·¥ç¨‹æ¥è¯„ä¼°é›†æˆç®—æ³•çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†åŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥èåˆæ–¹æ³•åœ¨CADECã€ShARe13å’ŒShARe14æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ChatGPTåœ¨é›†æˆå­¦ä¹ ä¸­çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œå°¤å…¶æ˜¯åœ¨æ–­ç»­å‘½åå®ä½“è¯†åˆ«ï¼ˆDNERï¼‰ä»»åŠ¡ä¸Šã€‚</li>
<li>å°†ChatGPTä¸é›†æˆå­¦ä¹ ç®—æ³•ç»“åˆï¼Œæ—¨åœ¨æé«˜DNERä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ç»“åˆäº†äº”ç§æœ€å…ˆè¿›çš„NERæ¨¡å‹ä¸ChatGPTï¼Œä½¿ç”¨è‡ªå®šä¹‰æç¤ºå·¥ç¨‹ã€‚</li>
<li>å®éªŒåœ¨ä¸‰ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šè¿›è¡Œï¼ŒåŒ…æ‹¬CADECã€ShARe13å’ŒShARe14ã€‚</li>
<li>èåˆæ–¹æ³•çš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>ChatGPTçš„èåˆæé«˜äº†é›†æˆç®—æ³•çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a00c4a4023ecc6cdcfbdf4c6a1e36efd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd7ca731864da6d9be0dcb058f910092.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ea8d1ea3233462fb37f2b02da6c8e8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56582858b34603c5b0ef2f765558b6a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cbd7be6a34377b2d3a8216ad7c41a3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0acc2fd1f320c184100e977828cb876f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fec0b7d7e50fb4bb5fe8c2ec11f4286b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-093cb56c6dfd249348ea38fc12dc9852.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="System-2-Mathematical-Reasoning-via-Enriched-Instruction-Tuning"><a href="#System-2-Mathematical-Reasoning-via-Enriched-Instruction-Tuning" class="headerlink" title="System-2 Mathematical Reasoning via Enriched Instruction Tuning"></a>System-2 Mathematical Reasoning via Enriched Instruction Tuning</h2><p><strong>Authors:Huanqia Cai, Yijun Yang, Zhifeng Li</strong></p>
<p>Solving complex mathematical problems via system-2 reasoning is a natural human skill, yet it remains a significant challenge for current large language models (LLMs). We identify the scarcity of deliberate multi-step reasoning data as a primary limiting factor. To this end, we introduce Enriched Instruction Tuning (EIT), a method that enriches existing human-annotated mathematical datasets by synergizing human and AI feedback to create fine-grained reasoning trajectories. These datasets are then used to fine-tune open-source LLMs, enhancing their mathematical reasoning abilities without reliance on any symbolic verification program. Concretely, EIT is composed of two critical steps: Enriching with Reasoning Plan (ERP) and Enriching with Reasoning Step (ERS). The former generates a high-level plan that breaks down complex instructions into a sequence of simpler objectives, while ERS fills in reasoning contexts often overlooked by human annotators, creating a smoother reasoning trajectory for LLM fine-tuning. Unlike existing CoT prompting methods that generate reasoning chains only depending on LLMâ€™s internal knowledge, our method leverages human-annotated initial answers as &#96;&#96;meta-knowledgeâ€™â€™ to help LLMs generate more detailed and precise reasoning processes, leading to a more trustworthy LLM expert for complex mathematical problems. In experiments, EIT achieves an accuracy of 84.1% on GSM8K and 32.5% on MATH, surpassing state-of-the-art fine-tuning and prompting methods, and even matching the performance of tool-augmented methods. </p>
<blockquote>
<p>è§£å†³å¤æ‚æ•°å­¦é—®é¢˜é€šè¿‡ç³»ç»Ÿ2æ¨ç†æ˜¯ä¸€ç§äººç±»è‡ªç„¶æŠ€èƒ½ï¼Œä½†å¯¹å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç¡®å®šäº†ç¼ºä¹æœ‰æ„è¯†çš„å¤šæ­¥éª¤æ¨ç†æ•°æ®æ˜¯ä¸»è¦é™åˆ¶å› ç´ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸°å¯ŒæŒ‡ä»¤è°ƒæ•´ï¼ˆEITï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡ååŒäººç±»å’ŒAIåé¦ˆæ¥ä¸°å¯Œç°æœ‰çš„äººç±»æ³¨é‡Šæ•°å­¦æ•°æ®é›†ï¼Œä»è€Œåˆ›å»ºç²¾ç»†çš„æ¨ç†è½¨è¿¹ã€‚è¿™äº›æ•°æ®é›†éšåç”¨äºå¾®è°ƒå¼€æºLLMï¼Œå¢å¼ºå®ƒä»¬çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ä¾èµ–ä»»ä½•ç¬¦å·éªŒè¯ç¨‹åºã€‚å…·ä½“æ¥è¯´ï¼ŒEITç”±ä¸¤ä¸ªå…³é”®æ­¥éª¤ç»„æˆï¼šä¸°å¯Œæ¨ç†è®¡åˆ’ï¼ˆERPï¼‰å’Œä¸°å¯Œæ¨ç†æ­¥éª¤ï¼ˆERSï¼‰ã€‚å‰è€…ç”Ÿæˆä¸€ä¸ªé«˜çº§è®¡åˆ’ï¼Œå°†å¤æ‚æŒ‡ä»¤åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´ç®€å•ç›®æ ‡ï¼Œè€ŒERSåˆ™å¡«è¡¥äº†äººç±»æ³¨é‡Šè€…ç»å¸¸å¿½ç•¥çš„æ¨ç†èƒŒæ™¯ï¼Œä¸ºLLMå¾®è°ƒåˆ›å»ºæ›´å¹³æ»‘çš„æ¨ç†è½¨è¿¹ã€‚ä¸ç°æœ‰çš„ä»…ä¾èµ–äºLLMå†…éƒ¨çŸ¥è¯†çš„CoTæç¤ºæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äººç±»æ³¨é‡Šçš„åˆå§‹ç­”æ¡ˆä½œä¸ºâ€œå…ƒçŸ¥è¯†â€æ¥å¸®åŠ©LLMç”Ÿæˆæ›´è¯¦ç»†ã€æ›´ç²¾ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œä½¿LLMæˆä¸ºè§£å†³å¤æ‚æ•°å­¦é—®é¢˜çš„æ›´å¯ä¿¡èµ–çš„ä¸“å®¶ã€‚åœ¨å®éªŒä¸­ï¼ŒEITåœ¨GSM8Kä¸Šè¾¾åˆ°äº†84.1%çš„å‡†ç¡®ç‡ï¼Œåœ¨MATHä¸Šè¾¾åˆ°äº†32.5%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†æœ€æ–°çš„å¾®è°ƒæç¤ºæ–¹æ³•ï¼Œç”šè‡³ä¸å·¥å…·å¢å¼ºæ–¹æ³•çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16964v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>é€šè¿‡ç³»ç»Ÿ2æ¨ç†è§£å†³å¤æ‚çš„æ•°å­¦é—®é¢˜æ˜¯äººç±»çš„ä¸€ç§è‡ªç„¶èƒ½åŠ›ï¼Œä½†ä»æ˜¯å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡å¤§æŒ‘æˆ˜ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºä¸€ç§åä¸ºä¸°å¯ŒæŒ‡ä»¤è°ƒä¼˜ï¼ˆEITï¼‰çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡ååŒäººç±»å’ŒAIåé¦ˆæ¥åˆ›å»ºç²¾ç»†çš„æ¨ç†è½¨è¿¹ï¼Œä»è€Œä¸°å¯Œç°æœ‰çš„äººç±»æ³¨é‡Šæ•°å­¦æ•°æ®é›†ã€‚EITç”±ä¸¤ä¸ªå…³é”®æ­¥éª¤ç»„æˆï¼šä»¥æ¨ç†è®¡åˆ’è¿›è¡Œä¸°å¯Œï¼ˆERPï¼‰å’Œä»¥æ¨ç†æ­¥éª¤è¿›è¡Œä¸°å¯Œï¼ˆERSï¼‰ã€‚ERPç”Ÿæˆé«˜çº§è®¡åˆ’ï¼Œå°†å¤æ‚æŒ‡ä»¤åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´ç®€å•ç›®æ ‡ï¼Œè€ŒERSå¡«è¡¥äº†å¸¸è¢«äººç±»æ³¨é‡Šå™¨å¿½è§†çš„æ¨ç†ä¸Šä¸‹æ–‡ï¼Œä¸ºLLMå¾®è°ƒåˆ›å»ºæ›´å¹³æ»‘çš„æ¨ç†è½¨è¿¹ã€‚EITåˆ©ç”¨äººç±»æ³¨é‡Šçš„åˆå§‹ç­”æ¡ˆä½œä¸ºâ€œå…ƒçŸ¥è¯†â€ï¼Œå¸®åŠ©LLMç”Ÿæˆæ›´è¯¦ç»†ã€æ›´ç²¾ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œåœ¨GSM8Kå’ŒMATHä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEITçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†84.1%å’Œ32.5%ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¾®è°ƒæ–¹æ³•å’Œæç¤ºæ–¹æ³•ï¼Œç”šè‡³ä¸å·¥å…·è¾…åŠ©æ–¹æ³•çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚çš„æ•°å­¦é—®é¢˜æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹æœ‰æ„è¯†çš„åˆ†æ­¥æ¨ç†æ•°æ®æ˜¯ä¸»è¦é™åˆ¶å› ç´ ä¹‹ä¸€ã€‚</li>
<li>Enriched Instruction Tuning (EIT) æ–¹æ³•é€šè¿‡ååŒäººç±»å’ŒAIåé¦ˆä¸°å¯Œç°æœ‰çš„äººç±»æ³¨é‡Šæ•°å­¦æ•°æ®é›†ã€‚</li>
<li>EITåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼šEnriching with Reasoning Plan (ERP) å’Œ Enriching with Reasoning Step (ERS)ã€‚</li>
<li>ERPç”Ÿæˆé«˜çº§è®¡åˆ’ï¼Œå°†å¤æ‚æŒ‡ä»¤åˆ†è§£ä¸ºä¸€ç³»åˆ—æ›´ç®€å•ç›®æ ‡ã€‚</li>
<li>ERSå¼¥è¡¥äº†å¸¸è¢«å¿½è§†çš„æ¨ç†ä¸Šä¸‹æ–‡ï¼Œæœ‰åŠ©äºä¸ºLLMåˆ›å»ºæ›´å¹³æ»‘çš„æ¨ç†è½¨è¿¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d0cc6bbda58e1fd11b08a945ddbf4d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a688e3a315843d5e31c4e58083f4cb4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e19dcdf0880026d32a267ba6662fba8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b39d70f8382b8d3ed44c3fc72c9b7d55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07848e27733cfcda25b6d11d5f04cc9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-802dd9034951db089614c63021a86751.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Two-in-One-Unified-Multi-Person-Interactive-Motion-Generation-by-Latent-Diffusion-Transformer"><a href="#Two-in-One-Unified-Multi-Person-Interactive-Motion-Generation-by-Latent-Diffusion-Transformer" class="headerlink" title="Two-in-One: Unified Multi-Person Interactive Motion Generation by Latent   Diffusion Transformer"></a>Two-in-One: Unified Multi-Person Interactive Motion Generation by Latent   Diffusion Transformer</h2><p><strong>Authors:Boyuan Li, Xihua Wang, Ruihua Song, Wenbing Huang</strong></p>
<p>Multi-person interactive motion generation, a critical yet under-explored domain in computer character animation, poses significant challenges such as intricate modeling of inter-human interactions beyond individual motions and generating two motions with huge differences from one text condition. Current research often employs separate module branches for individual motions, leading to a loss of interaction information and increased computational demands. To address these challenges, we propose a novel, unified approach that models multi-person motions and their interactions within a single latent space. Our approach streamlines the process by treating interactive motions as an integrated data point, utilizing a Variational AutoEncoder (VAE) for compression into a unified latent space, and performing a diffusion process within this space, guided by the natural language conditions. Experimental results demonstrate our methodâ€™s superiority over existing approaches in generation quality, performing text condition in particular when motions have significant asymmetry, and accelerating the generation efficiency while preserving high quality. </p>
<blockquote>
<p>å¤šäººäº¤äº’è¿åŠ¨ç”Ÿæˆæ˜¯è®¡ç®—æœºè§’è‰²åŠ¨ç”»ä¸­ä¸€ä¸ªè‡³å…³é‡è¦ä½†å°šæœªè¢«å……åˆ†ç ”ç©¶çš„é¢†åŸŸï¼Œå®ƒå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œä¾‹å¦‚è¶…è¶Šä¸ªä½“è¿åŠ¨çš„å¤æ‚äººé™…äº’åŠ¨å»ºæ¨¡ï¼Œä»¥åŠæ ¹æ®æ–‡æœ¬æ¡ä»¶ç”Ÿæˆä¸¤ç§å·®å¼‚å·¨å¤§çš„è¿åŠ¨ã€‚ç›®å‰çš„ç ”ç©¶ç»å¸¸ä¸ºä¸ªä½“è¿åŠ¨é‡‡ç”¨å•ç‹¬çš„æ¨¡å—åˆ†æ”¯ï¼Œè¿™å¯¼è‡´äº†äº’åŠ¨ä¿¡æ¯çš„ä¸¢å¤±å’Œè®¡ç®—éœ€æ±‚çš„å¢åŠ ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»Ÿä¸€æ–¹æ³•ï¼Œåœ¨ä¸€ä¸ªå•ä¸€çš„æ½œåœ¨ç©ºé—´å†…å¯¹å¤šäººè¿åŠ¨åŠå…¶äº’åŠ¨è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å°†å¯¹äº¤äº’è¿åŠ¨ä½œä¸ºä¸€ä¸ªæ•´åˆçš„æ•°æ®ç‚¹æ¥å¤„ç†ï¼Œåˆ©ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰å‹ç¼©åˆ°ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ï¼Œå¹¶åœ¨è¯¥ç©ºé—´å†…è¿›è¡Œæ‰©æ•£è¿‡ç¨‹ï¼Œç”±è‡ªç„¶è¯­è¨€æ¡ä»¶å¼•å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è¿åŠ¨å…·æœ‰æ˜¾è‘—ä¸å¯¹ç§°æ€§çš„æ–‡æœ¬æ¡ä»¶æ—¶ï¼Œèƒ½æé«˜ç”Ÿæˆæ•ˆç‡åŒæ—¶ä¿æŒé«˜è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16670v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„ç»Ÿä¸€æ–¹æ³•ï¼Œç”¨äºåœ¨å•ä¸ªæ½œåœ¨ç©ºé—´å†…å¯¹å¤šäººè¿åŠ¨åŠå…¶äº’åŠ¨è¿›è¡Œå»ºæ¨¡ã€‚è¯¥æ–¹æ³•å°†äº’åŠ¨è¿åŠ¨è§†ä¸ºä¸€ä¸ªæ•´ä½“æ•°æ®ç‚¹ï¼Œä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¿›è¡Œå‹ç¼©åˆ°ç»Ÿä¸€æ½œåœ¨ç©ºé—´ï¼Œå¹¶åœ¨è¯¥ç©ºé—´å†…è¿›è¡Œæ‰©æ•£è¿‡ç¨‹ï¼Œç”±è‡ªç„¶è¯­è¨€æ¡ä»¶å¼•å¯¼ã€‚è¯¥æ–¹æ³•è§£å†³äº†å¤šäººäº’åŠ¨è¿åŠ¨ç”Ÿæˆä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚è¶…è¶Šä¸ªäººè¿åŠ¨çš„å¤æ‚äººé™…äº’åŠ¨å»ºæ¨¡ï¼Œä»¥åŠä»ä¸€ä¸ªæ–‡æœ¬æ¡ä»¶ç”Ÿæˆä¸¤ä¸ªå·®å¼‚å·¨å¤§çš„è¿åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿åŠ¨æœ‰æ˜¾è‘—å·®å¼‚çš„æ–‡æœ¬æ¡ä»¶ä¸‹è¡¨ç°ä¼˜è¶Šï¼ŒåŒæ—¶æé«˜äº†ç”Ÿæˆæ•ˆç‡å¹¶ä¿æŒé«˜è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šäººäº’åŠ¨è¿åŠ¨ç”Ÿæˆæ˜¯è®¡ç®—æœºè§’è‰²åŠ¨ç”»ä¸­çš„å…³é”®ä½†å°šæœªå……åˆ†æ¢ç´¢çš„é¢†åŸŸï¼Œå­˜åœ¨å¤æ‚çš„äººé™…äº’åŠ¨å»ºæ¨¡å’Œä»å•ä¸€æ–‡æœ¬æ¡ä»¶ç”Ÿæˆå·®å¼‚å¤§çš„ä¸¤ä¸ªè¿åŠ¨ç­‰æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰ç ”ç©¶é€šå¸¸ä½¿ç”¨å•ç‹¬çš„æ¨¡å—åˆ†æ”¯æ¥å¤„ç†ä¸ªäººè¿åŠ¨ï¼Œå¯¼è‡´äº’åŠ¨ä¿¡æ¯çš„ä¸¢å¤±å’Œè®¡ç®—éœ€æ±‚çš„å¢åŠ ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»Ÿä¸€æ–¹æ³•ï¼Œåœ¨å•ä¸ªæ½œåœ¨ç©ºé—´å†…å¯¹å¤šäººè¿åŠ¨å’Œäº’åŠ¨è¿›è¡Œå»ºæ¨¡ï¼Œç®€åŒ–äº†æµç¨‹ã€‚</li>
<li>ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰è¿›è¡Œå‹ç¼©åˆ°ç»Ÿä¸€æ½œåœ¨ç©ºé—´ï¼Œå¹¶è¿›è¡Œæ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•å—åˆ°è‡ªç„¶è¯­è¨€æ¡ä»¶çš„å¼•å¯¼ï¼Œæ ¹æ®æ–‡æœ¬æ¡ä»¶ç”Ÿæˆè¿åŠ¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡ã€æ–‡æœ¬æ¡ä»¶ä¸‹çš„è¿åŠ¨ç”Ÿæˆä»¥åŠç”Ÿæˆæ•ˆç‡å’Œè´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd4126ef7efad92e873ec4b052ce47e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d538af093b5388c073a3c0fcb499805.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2480d53dbfaeb7a1f9ff5e8b52150960.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55446c23e03320493fc701f636812913.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Can-Be-a-Foundation-for-Hidden-Rationale-Based-Retrieval"><a href="#Large-Language-Model-Can-Be-a-Foundation-for-Hidden-Rationale-Based-Retrieval" class="headerlink" title="Large Language Model Can Be a Foundation for Hidden Rationale-Based   Retrieval"></a>Large Language Model Can Be a Foundation for Hidden Rationale-Based   Retrieval</h2><p><strong>Authors:Luo Ji, Feixiang Guo, Teng Chen, Qingqing Gu, Xiaoyu Wang, Ningyuan Xi, Yihong Wang, Peng Yu, Yue Zhao, Hongyang Lei, Zhonglin Jiang, Yong Chen</strong></p>
<p>Despite the recent advancement in Retrieval-Augmented Generation (RAG) systems, most retrieval methodologies are often developed for factual retrieval, which assumes query and positive documents are semantically similar. In this paper, we instead propose and study a more challenging type of retrieval task, called hidden rationale retrieval, in which query and document are not similar but can be inferred by reasoning chains, logic relationships, or empirical experiences. To address such problems, an instruction-tuned Large language model (LLM) with a cross-encoder architecture could be a reasonable choice. To further strengthen pioneering LLM-based retrievers, we design a special instruction that transforms the retrieval task into a generative task by prompting LLM to answer a binary-choice question. The model can be fine-tuned with direct preference optimization (DPO). The framework is also optimized for computational efficiency with no performance degradation. We name this retrieval framework by RaHoRe and verify its zero-shot and fine-tuned performance superiority on Emotional Support Conversation (ESC), compared with previous retrieval works. Our study suggests the potential to employ LLM as a foundation for a wider scope of retrieval tasks. Our codes, models, and datasets are available on <a target="_blank" rel="noopener" href="https://github.com/flyfree5/LaHoRe">https://github.com/flyfree5/LaHoRe</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿæœ‰æ‰€è¿›å±•ï¼Œä½†å¤§å¤šæ•°æ£€ç´¢æ–¹æ³•å¾€å¾€é’ˆå¯¹äº‹å®æ£€ç´¢è€Œå¼€å‘ï¼Œè¿™å‡è®¾æŸ¥è¯¢å’Œæ­£é¢æ–‡æ¡£åœ¨è¯­ä¹‰ä¸Šæ˜¯ç›¸ä¼¼çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºå¹¶ç ”ç©¶äº†ä¸€ç§æ›´å…·æŒ‘æˆ˜æ€§çš„æ£€ç´¢ä»»åŠ¡ï¼Œå³éšè—é€»è¾‘æ£€ç´¢ï¼Œå…¶ä¸­æŸ¥è¯¢å’Œæ–‡æ¡£å¹¶ä¸ç›¸ä¼¼ï¼Œä½†å¯ä»¥é€šè¿‡æ¨ç†é“¾ã€é€»è¾‘å…³ç³»æˆ–ç»éªŒæ¥æ¨æ–­ã€‚ä¸ºè§£å†³æ­¤ç±»é—®é¢˜ï¼Œé‡‡ç”¨å¸¦æœ‰äº¤å‰ç¼–ç å™¨æ¶æ„çš„æŒ‡ä»¤ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯èƒ½æ˜¯ä¸€ä¸ªåˆç†çš„é€‰æ‹©ã€‚ä¸ºäº†è¿›ä¸€æ­¥åŠ å¼ºåŸºäºLLMçš„æ£€ç´¢å™¨ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç‰¹æ®ŠæŒ‡ä»¤ï¼Œé€šè¿‡å°†æ£€ç´¢ä»»åŠ¡è½¬åŒ–ä¸ºç”Ÿæˆä»»åŠ¡ï¼Œæç¤ºLLMå›ç­”äºŒé€‰ä¸€çš„é—®é¢˜ã€‚è¯¥æ¨¡å‹å¯é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œå¾®è°ƒã€‚è¯¥æ¡†æ¶åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢ä¹Ÿè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä¸”ä¸ä¼šé™ä½æ€§èƒ½ã€‚æˆ‘ä»¬å°†è¿™ç§æ£€ç´¢æ¡†æ¶å‘½åä¸ºRaHoReï¼Œå¹¶åœ¨æƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ä¸ŠéªŒè¯äº†å…¶é›¶æ ·æœ¬å’Œå¾®è°ƒåçš„æ€§èƒ½ä¼˜è¶Šæ€§ï¼Œä¸ä¹‹å‰çš„ç ”ç©¶ç›¸æ¯”å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæœ‰æ½œåŠ›å°†LLMç”¨ä½œæ›´å¹¿æ³›æ£€ç´¢ä»»åŠ¡çš„åŸºç¡€ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/flyfree5/LaHoRe">https://github.com/flyfree5/LaHoRe</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16615v1">PDF</a> 11 pages, 3 figures, accepted by ECIR 2025</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºè§£å†³éšè—é€»è¾‘æ£€ç´¢ä»»åŠ¡ã€‚ä¸åŒäºä¼ ç»Ÿçš„ä»¥äº‹å®æ£€ç´¢ä¸ºä¸»çš„æ–¹æ³•ï¼Œè¯¥ä»»åŠ¡é€šè¿‡é€»è¾‘æ¨ç†é“¾ã€é€»è¾‘å…³ç³»æˆ–ç»éªŒæ¥æ¨ç†æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡ç‰¹æ®ŠæŒ‡ä»¤å°†æ£€ç´¢ä»»åŠ¡è½¬åŒ–ä¸ºç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨äºŒé€‰ä¸€é—®é¢˜æ¥å¼•å¯¼LLMè¿›è¡Œæ£€ç´¢ï¼Œé‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰è¿›è¡Œå¾®è°ƒï¼Œæ„å»ºçš„è®¡ç®—æ•ˆç‡é«˜çš„æ¨¡å‹æ¡†æ¶RaHoReåœ¨æƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ£€ç´¢ä»»åŠ¡ç±»å‹â€”â€”éšè—é€»è¾‘æ£€ç´¢ï¼Œè¯¥ä»»åŠ¡éœ€è¦æ¨ç†æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹é—´çš„å…³ç³»ï¼Œä¸åŒäºä¼ ç»Ÿçš„å‡è®¾æŸ¥è¯¢å’Œæ­£é¢æ–‡æ¡£è¯­ä¹‰ç›¸ä¼¼çš„æ£€ç´¢æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè§£å†³éšè—é€»è¾‘æ£€ç´¢ä»»åŠ¡ï¼Œåˆ©ç”¨å…¶è·¨ç¼–ç å™¨æ¶æ„åº”å¯¹å¤æ‚çš„è¯­ä¹‰å…³ç³»ã€‚</li>
<li>é€šè¿‡ç‰¹æ®ŠæŒ‡ä»¤å°†æ£€ç´¢ä»»åŠ¡è½¬åŒ–ä¸ºç”Ÿæˆä»»åŠ¡ï¼Œä¿ƒè¿›LLMå¯¹æŸ¥è¯¢çš„å›åº”ï¼Œé‡‡ç”¨äºŒé€‰ä¸€é—®é¢˜çš„å½¢å¼è¿›è¡Œæç¤ºã€‚</li>
<li>é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ‰€æ„å»ºçš„æ¨¡å‹æ¡†æ¶RaHoReåœ¨è®¡ç®—æ•ˆç‡ä¸Šè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä¸”æ€§èƒ½æœªå‡ºç°ä¸‹é™ã€‚</li>
<li>åœ¨æƒ…æ„Ÿæ”¯æŒå¯¹è¯ï¼ˆESCï¼‰ä»»åŠ¡ä¸Šï¼ŒRaHoReçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„æ£€ç´¢å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5497649b8ec5b86c6f92ffb6850e00ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3b2b89a427d4b380fa17714ef881254.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0920e60f79b54c5a7d6a92f9b419db6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd3ec3fb8d25fc0ee14964a58a983572.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b30b754e9e350bd549d0ebeccbac3716.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning"><a href="#QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning" class="headerlink" title="QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning"></a>QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning</h2><p><strong>Authors:Xinyang Tong, Pengxiang Ding, Donglin Wang, Wenjie Zhang, Can Cui, Mingyang Sun, Yiguo Fan, Han Zhao, Hongyin Zhang, Yonghao Dang, Siteng Huang, Shangke Lyu</strong></p>
<p>This paper addresses the inherent inference latency challenges associated with deploying multimodal large language models (MLLM) in quadruped vision-language-action (QUAR-VLA) tasks. Our investigation reveals that conventional parameter reduction techniques ultimately impair the performance of the language foundation model during the action instruction tuning phase, making them unsuitable for this purpose. We introduce a novel latency-free quadruped MLLM model, dubbed QUART-Online, designed to enhance inference efficiency without degrading the performance of the language foundation model. By incorporating Action Chunk Discretization (ACD), we compress the original action representation space, mapping continuous action values onto a smaller set of discrete representative vectors while preserving critical information. Subsequently, we fine-tune the MLLM to integrate vision, language, and compressed actions into a unified semantic space. Experimental results demonstrate that QUART-Online operates in tandem with the existing MLLM system, achieving real-time inference in sync with the underlying controller frequency, significantly boosting the success rate across various tasks by 65%. Our project page is <a target="_blank" rel="noopener" href="https://quart-online.github.io/">https://quart-online.github.io</a>. </p>
<blockquote>
<p>æœ¬æ–‡ç€çœ¼äºåœ¨å››è¶³è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆQUAR-VLAï¼‰ä»»åŠ¡ä¸­éƒ¨ç½²å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ‰€é¢ä¸´çš„å›ºæœ‰æ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°ï¼Œä¼ ç»Ÿçš„å‚æ•°å‡å°‘æŠ€æœ¯æœ€ç»ˆä¼šåœ¨åŠ¨ä½œæŒ‡ä»¤è°ƒæ•´é˜¶æ®µæŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å…¶ä¸é€‚åˆæ­¤ç›®çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ— å»¶è¿Ÿå››è¶³MLLMæ¨¡å‹ï¼Œåä¸ºQUART-Onlineï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¸é™ä½è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡é‡‡ç”¨åŠ¨ä½œå—ç¦»æ•£åŒ–ï¼ˆACDï¼‰ï¼Œæˆ‘ä»¬å‹ç¼©äº†åŸå§‹çš„åŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œå°†è¿ç»­çš„åŠ¨ä½œå€¼æ˜ å°„åˆ°ä¸€ç»„è¾ƒå°çš„ç¦»æ•£ä»£è¡¨å‘é‡ä¸Šï¼ŒåŒæ—¶ä¿ç•™äº†å…³é”®ä¿¡æ¯ã€‚éšåï¼Œæˆ‘ä»¬å¯¹MLLMè¿›è¡Œäº†å¾®è°ƒï¼Œä»¥å°†è§†è§‰ã€è¯­è¨€å’Œå‹ç¼©åŠ¨ä½œé›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQUART-Onlineä¸ç°æœ‰çš„MLLMç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°äº†ä¸åº•å±‚æ§åˆ¶å™¨é¢‘ç‡åŒæ­¥çš„å®æ—¶æ¨ç†ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­çš„æˆåŠŸç‡æé«˜äº†65%ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://quart-online.github.io./">https://quart-online.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15576v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†åœ¨å››è¶³è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆQUAR-VLAï¼‰ä»»åŠ¡ä¸­éƒ¨ç½²å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ—¶é¢ä¸´çš„å›ºæœ‰æ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ— å»¶è¿Ÿå››è¶³MLLMæ¨¡å‹â€”â€”QUART-Onlineï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¸æŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡é‡‡ç”¨åŠ¨ä½œç‰‡æ®µç¦»æ•£åŒ–ï¼ˆACDï¼‰æŠ€æœ¯ï¼Œå‹ç¼©åŸå§‹åŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œå°†è¿ç»­åŠ¨ä½œå€¼æ˜ å°„åˆ°ä¸€ç»„è¾ƒå°çš„ç¦»æ•£ä»£è¡¨å‘é‡ä¸Šï¼ŒåŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚éšåï¼Œå¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œä»¥å°†è§†è§‰ã€è¯­è¨€å’Œå‹ç¼©åŠ¨ä½œé›†æˆåˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQUART-Onlineä¸ç°æœ‰MLLMç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°äº†ä¸åº•å±‚æ§åˆ¶å™¨é¢‘ç‡åŒæ­¥çš„å®æ—¶æ¨ç†ï¼Œåœ¨å„é¡¹ä»»åŠ¡ä¸­çš„æˆåŠŸç‡æé«˜äº†65%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ä¸»è¦æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å››è¶³è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆQUAR-VLAï¼‰ä»»åŠ¡ä¸­çš„æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ— å»¶è¿Ÿå››è¶³MLLMæ¨¡å‹â€”â€”QUART-Onlineï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ä¸”ä¸æŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åŠ¨ä½œç‰‡æ®µç¦»æ•£åŒ–ï¼ˆACDï¼‰æŠ€æœ¯å‹ç¼©åŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œæ˜ å°„è¿ç»­åŠ¨ä½œå€¼åˆ°ç¦»æ•£ä»£è¡¨å‘é‡ã€‚</li>
<li>QUART-Onlineæ¨¡å‹é€šè¿‡å¾®è°ƒé›†æˆè§†è§‰ã€è¯­è¨€å’Œå‹ç¼©åŠ¨ä½œåˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒQUART-Onlineä¸ç°æœ‰MLLMç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°å®æ—¶æ¨ç†ã€‚</li>
<li>QUART-Onlineåœ¨ä¸åŒä»»åŠ¡ä¸­çš„æˆåŠŸç‡æé«˜äº†65%ã€‚</li>
<li>é¡¹ç›®é¡µé¢ä¸º<a target="_blank" rel="noopener" href="https://quart-online.github.io./">https://quart-online.github.ioã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a75b9cc2e84c6bc570e180f8b0e0a67d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5071df8abe069937262a9a6e06267494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e4eddd7fdd31deac970be73f82feadd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c05f5679f422ab01b536c333d520143e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0bf716a748a7d8e9e84f4b1f2274cd5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Advanced-Reasoning-and-Transformation-Engine-for-Multi-Step-Insight-Synthesis-in-Data-Analytics-with-Large-Language-Models"><a href="#Advanced-Reasoning-and-Transformation-Engine-for-Multi-Step-Insight-Synthesis-in-Data-Analytics-with-Large-Language-Models" class="headerlink" title="Advanced Reasoning and Transformation Engine for Multi-Step Insight   Synthesis in Data Analytics with Large Language Models"></a>Advanced Reasoning and Transformation Engine for Multi-Step Insight   Synthesis in Data Analytics with Large Language Models</h2><p><strong>Authors:Atin Sakkeer Hussain</strong></p>
<p>This paper presents the Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework designed to augment Large Language Models (LLMs) for solving complex, multi-step data analytics tasks. ARTEMIS-DA integrates three core components: the Planner, which dissects complex user queries into structured, sequential instructions encompassing data preprocessing, transformation, predictive modeling, and visualization; the Coder, which dynamically generates and executes Python code to implement these instructions; and the Grapher, which interprets generated visualizations to derive actionable insights. By orchestrating the collaboration between these components, ARTEMIS-DA effectively manages sophisticated analytical workflows involving advanced reasoning, multi-step transformations, and synthesis across diverse data modalities. The framework achieves state-of-the-art (SOTA) performance on benchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to tackle intricate analytical tasks with precision and adaptability. By combining the reasoning capabilities of LLMs with automated code generation and execution and visual analysis, ARTEMIS-DA offers a robust, scalable solution for multi-step insight synthesis, addressing a wide range of challenges in data analytics. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ç”¨äºæ•°æ®è§£æä¸­çš„å¤šæ­¥éª¤è§è§£åˆæˆçš„å…ˆè¿›æ¨ç†ä¸è½¬æ¢å¼•æ“ï¼ˆARTEMIS-DAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥è§£å†³å¤æ‚çš„å¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡çš„æ–°å‹æ¡†æ¶ã€‚ARTEMIS-DAé›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šPlannerï¼Œå®ƒå°†å¤æ‚çš„ç”¨æˆ·æŸ¥è¯¢è§£æä¸ºç»“æ„åŒ–çš„é¡ºåºæŒ‡ä»¤ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€è½¬æ¢ã€é¢„æµ‹å»ºæ¨¡å’Œå¯è§†åŒ–ï¼›Coderï¼Œå®ƒåŠ¨æ€ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç ä»¥æ‰§è¡Œè¿™äº›æŒ‡ä»¤ï¼›Grapherï¼Œå®ƒè§£é‡Šç”Ÿæˆçš„å¯è§†åŒ–ä»¥è·å–å¯æ“ä½œçš„è§è§£ã€‚é€šè¿‡åè°ƒè¿™äº›ç»„ä»¶ä¹‹é—´çš„åä½œï¼ŒARTEMIS-DAæœ‰æ•ˆåœ°ç®¡ç†æ¶‰åŠé«˜çº§æ¨ç†ã€å¤šæ­¥éª¤è½¬æ¢å’Œè·¨ä¸åŒæ•°æ®æ¨¡æ€çš„åˆæˆçš„é«˜çº§åˆ†æå·¥ä½œæµç¨‹ã€‚è¯¥æ¡†æ¶åœ¨WikiTableQuestionså’ŒTabFactç­‰åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¯æ˜äº†å…¶å¤„ç†å¤æ‚åˆ†æä»»åŠ¡çš„ç²¾ç¡®æ€§å’Œé€‚åº”æ€§ã€‚é€šè¿‡å°†LLMçš„æ¨ç†èƒ½åŠ›ä¸è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œæ‰§è¡Œä»¥åŠè§†è§‰åˆ†æç›¸ç»“åˆï¼ŒARTEMIS-DAä¸ºå¤šæ­¥éª¤è§è§£åˆæˆæä¾›äº†ç¨³å¥ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†æ•°æ®åˆ†æä¸­çš„å¹¿æ³›æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14146v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ARTEMIS-DAæ˜¯ä¸€ä¸ªå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³å¤æ‚å¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡çš„æ–°å‹æ¡†æ¶ã€‚å®ƒåŒ…å«è§„åˆ’å™¨ã€ç¼–ç å™¨å’Œç»˜å›¾ä»ªä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œèƒ½æœ‰æ•ˆç®¡ç†æ¶‰åŠé«˜çº§æ¨ç†ã€å¤šæ­¥éª¤è½¬æ¢å’Œè·¨ä¸åŒæ•°æ®æ¨¡æ€çš„ç»¼åˆåˆ†æçš„é«˜çº§åˆ†æå·¥ä½œæµç¨‹ï¼Œå®ç°WikiTableQuestionså’ŒTabFactç­‰åŸºå‡†æµ‹è¯•ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ARTEMIS-DAæ˜¯ä¸€ä¸ªç”¨äºå¤šæ­¥éª¤æ´å¯Ÿåˆæˆçš„å…ˆè¿›æ¨ç†å’Œè½¬æ¢å¼•æ“ã€‚</li>
<li>å®ƒæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥è§£å†³å¤æ‚çš„æ•°æ®åˆ†æä»»åŠ¡ã€‚</li>
<li>ARTEMIS-DAåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè§„åˆ’å¸ˆã€ç¼–ç å™¨å’Œç»˜å›¾ä»ªã€‚</li>
<li>è§„åˆ’å¸ˆè´Ÿè´£å°†å¤æ‚çš„ç”¨æˆ·æŸ¥è¯¢åˆ†è§£ä¸ºç»“æ„åŒ–ã€åºåˆ—åŒ–çš„æŒ‡ä»¤ã€‚</li>
<li>ç¼–ç å™¨èƒ½å¤ŸåŠ¨æ€ç”Ÿæˆå’Œæ‰§è¡ŒPythonä»£ç æ¥å®ç°è¿™äº›æŒ‡ä»¤ã€‚</li>
<li>ç»˜å›¾ä»ªå¯ä»¥è§£é‡Šç”Ÿæˆçš„å¯è§†åŒ–æ¥æ´¾ç”Ÿå‡ºå¯æ“ä½œçš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b51711ee5c88cba3551c3a56bb34420b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-152cb53ea94c8c505412d75c9c95d10d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddf9f19888890e2735f18e5a265d2623.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98388b1a862e9c32483d1e214e3693f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa422c61b4ec8a0eb6de9b04ffc26d61.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fced7d64cd21b5b085a70d33764b6aba.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  Multi-Modal Grounded Planning and Efficient Replanning For Learning   Embodied Agents with A Few Examples
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-24/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-65ac03b94fdcf69fc1e2a81513a2ef78.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-24  MORTAR Metamorphic Multi-turn Testing for LLM-based Dialogue Systems
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">9017.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
