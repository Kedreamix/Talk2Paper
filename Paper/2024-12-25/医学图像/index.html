<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  FaceLift Single Image to 3D Head with View Generation and GS-LRM">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-cc8da03525039b4ae3e9044e917243e7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-25-æ›´æ–°"><a href="#2024-12-25-æ›´æ–°" class="headerlink" title="2024-12-25 æ›´æ–°"></a>2024-12-25 æ›´æ–°</h1><h2 id="FaceLift-Single-Image-to-3D-Head-with-View-Generation-and-GS-LRM"><a href="#FaceLift-Single-Image-to-3D-Head-with-View-Generation-and-GS-LRM" class="headerlink" title="FaceLift: Single Image to 3D Head with View Generation and GS-LRM"></a>FaceLift: Single Image to 3D Head with View Generation and GS-LRM</h2><p><strong>Authors:Weijie Lyu, Yi Zhou, Ming-Hsuan Yang, Zhixin Shu</strong></p>
<p>We present FaceLift, a feed-forward approach for rapid, high-quality, 360-degree head reconstruction from a single image. Our pipeline begins by employing a multi-view latent diffusion model that generates consistent side and back views of the head from a single facial input. These generated views then serve as input to a GS-LRM reconstructor, which produces a comprehensive 3D representation using Gaussian splats. To train our system, we develop a dataset of multi-view renderings using synthetic 3D human head as-sets. The diffusion-based multi-view generator is trained exclusively on synthetic head images, while the GS-LRM reconstructor undergoes initial training on Objaverse followed by fine-tuning on synthetic head data. FaceLift excels at preserving identity and maintaining view consistency across views. Despite being trained solely on synthetic data, FaceLift demonstrates remarkable generalization to real-world images. Through extensive qualitative and quantitative evaluations, we show that FaceLift outperforms state-of-the-art methods in 3D head reconstruction, highlighting its practical applicability and robust performance on real-world images. In addition to single image reconstruction, FaceLift supports video inputs for 4D novel view synthesis and seamlessly integrates with 2D reanimation techniques to enable 3D facial animation. Project page: <a target="_blank" rel="noopener" href="https://weijielyu.github.io/FaceLift">https://weijielyu.github.io/FaceLift</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†FaceLiftï¼Œè¿™æ˜¯ä¸€ç§å‰é¦ˆæ–¹æ³•ï¼Œå¯ä»¥ä»å•å¼ å›¾åƒå¿«é€Ÿç”Ÿæˆé«˜è´¨é‡ã€360åº¦çš„å¤´éƒ¨é‡å»ºã€‚æˆ‘ä»¬çš„æµç¨‹é¦–å…ˆé‡‡ç”¨å¤šè§†è§’æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œæ ¹æ®å•å¼ é¢éƒ¨è¾“å…¥ç”Ÿæˆä¸€è‡´çš„ä¾§é¢å’ŒèƒŒé¢è§†è§’ã€‚è¿™äº›ç”Ÿæˆçš„è§†è§’ç„¶åä½œä¸ºGS-LRMé‡å»ºå™¨çš„è¾“å…¥ï¼Œä½¿ç”¨é«˜æ–¯splatç”Ÿæˆå…¨é¢çš„3Dè¡¨ç¤ºã€‚ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„ç³»ç»Ÿï¼Œæˆ‘ä»¬ä½¿ç”¨åˆæˆ3Däººå¤´æ•°æ®é›†å¼€å‘äº†ä¸€ä¸ªå¤šè§†è§’æ¸²æŸ“æ•°æ®é›†ã€‚åŸºäºæ‰©æ•£çš„å¤šè§†è§’ç”Ÿæˆå™¨åªæ¥å—åˆæˆå¤´éƒ¨å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œè€ŒGS-LRMé‡å»ºå™¨é¦–å…ˆåœ¨Objaverseä¸Šè¿›è¡Œåˆæ­¥è®­ç»ƒï¼Œç„¶ååœ¨åˆæˆå¤´éƒ¨æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚FaceLiftæ“…é•¿åœ¨ä¸åŒè§†è§’ä¹‹é—´ä¿æŒèº«ä»½ä¸€è‡´æ€§å’Œè§†è§’ä¸€è‡´æ€§ã€‚å°½ç®¡åªæ¥å—åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†FaceLiftåœ¨çœŸå®å›¾åƒä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¹¿æ³›çš„è´¨é‡å’Œæ•°é‡è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†FaceLiftåœ¨3Då¤´éƒ¨é‡å»ºæ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œçªå‡ºäº†å…¶åœ¨çœŸå®å›¾åƒä¸Šçš„å®ç”¨æ€§å’Œç¨³å¥æ€§èƒ½ã€‚é™¤äº†å•å›¾åƒé‡å»ºå¤–ï¼ŒFaceLiftè¿˜æ”¯æŒè§†é¢‘è¾“å…¥è¿›è¡Œ4Dæ–°é¢–è§†è§’åˆæˆï¼Œå¹¶ä¸2Då†åŠ¨ç”»æŠ€æœ¯æ— ç¼é›†æˆä»¥å®ç°3Dé¢éƒ¨åŠ¨ç”»ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://weijielyu.github.io/FaceLift%E3%80%82">https://weijielyu.github.io/FaceLiftã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17812v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://weijielyu.github.io/FaceLift">https://weijielyu.github.io/FaceLift</a></p>
<p><strong>Summary</strong></p>
<p>FaceLiftæ˜¯ä¸€ç§åŸºäºå‰é¦ˆæ–¹æ³•çš„å¿«é€Ÿé«˜è´¨é‡å•å›¾åƒ360åº¦å¤´éƒ¨é‡å»ºæŠ€æœ¯ã€‚å®ƒé€šè¿‡å¤šè§†è§’æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸€è‡´çš„ä¾§é¢å’ŒèƒŒé¢è§†å›¾ï¼Œç„¶ååˆ©ç”¨GS-LRMé‡å»ºå™¨ç”Ÿæˆå…¨é¢çš„3Dè¡¨ç¤ºã€‚å°½ç®¡ä»…åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒFaceLiftåœ¨çœŸå®å›¾åƒä¸Šè¡¨ç°å‡ºå“è¶Šçš„é€šç”¨åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒFaceLiftè¿˜æ”¯æŒè§†é¢‘è¾“å…¥ä»¥å®ç°4Dæ–°é¢–è§†å›¾åˆæˆï¼Œå¹¶ä¸2DåŠ¨ç”»æŠ€æœ¯æ— ç¼é›†æˆä»¥å®ç°3Dé¢éƒ¨åŠ¨ç”»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FaceLiftæ˜¯ä¸€ç§åŸºäºå‰é¦ˆæ–¹æ³•çš„å¿«é€Ÿé«˜è´¨é‡å¤´éƒ¨é‡å»ºæŠ€æœ¯ã€‚</li>
<li>é€šè¿‡å¤šè§†è§’æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¾§é¢å’ŒèƒŒé¢è§†å›¾ã€‚</li>
<li>GS-LRMé‡å»ºå™¨ç”Ÿæˆå…¨é¢çš„3Dè¡¨ç¤ºã€‚</li>
<li>ä»…ç”¨åˆæˆæ•°æ®è®­ç»ƒï¼Œä½†åœ¨çœŸå®å›¾åƒä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„é€šç”¨åŒ–èƒ½åŠ›ã€‚</li>
<li>æ”¯æŒè§†é¢‘è¾“å…¥ä»¥å®ç°4Dæ–°é¢–è§†å›¾åˆæˆã€‚</li>
<li>ä¸2DåŠ¨ç”»æŠ€æœ¯æ— ç¼é›†æˆä»¥å®ç°3Dé¢éƒ¨åŠ¨ç”»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-349eec9112cef735aaf6ae4647cdbd29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc8da03525039b4ae3e9044e917243e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd63245f29e72293d12e29ceb8c5e623.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1e271b62307e29e5a34919c49d9602d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e78fba8fb441278560f1de775d5d63b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62f6ee998869e53fbfbf42868e93a43d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MRANet-A-Modified-Residual-Attention-Networks-for-Lung-and-Colon-Cancer-Classification"><a href="#MRANet-A-Modified-Residual-Attention-Networks-for-Lung-and-Colon-Cancer-Classification" class="headerlink" title="MRANet: A Modified Residual Attention Networks for Lung and Colon Cancer   Classification"></a>MRANet: A Modified Residual Attention Networks for Lung and Colon Cancer   Classification</h2><p><strong>Authors:Diponkor Bala, S M Rakib Ul Karim, Rownak Ara Rasul</strong></p>
<p>Lung and colon cancers are predominant contributors to cancer mortality. Early and accurate diagnosis is crucial for effective treatment. By utilizing imaging technology in different image detection, learning models have shown promise in automating cancer classification from histopathological images. This includes the histopathological diagnosis, an important factor in cancer type identification. This research focuses on creating a high-efficiency deep-learning model for identifying lung and colon cancer from histopathological images. We proposed a novel approach based on a modified residual attention network architecture. The model was trained on a dataset of 25,000 high-resolution histopathological images across several classes. Our proposed model achieved an exceptional accuracy of 99.30%, 96.63%, and 97.56% for two, three, and five classes, respectively; those are outperforming other state-of-the-art architectures. This study presents a highly accurate deep learning model for lung and colon cancer classification. The superior performance of our proposed model addresses a critical need in medical AI applications. </p>
<blockquote>
<p>è‚ºç™Œå’Œç»“è‚ ç™Œæ˜¯å¯¼è‡´ç™Œç—‡æ­»äº¡ç‡è¾ƒé«˜çš„ä¸¤ç§ç–¾ç—…ï¼Œæ—©æœŸå‡†ç¡®è¯Šæ–­å¯¹æœ‰æ•ˆæ²»ç–—è‡³å…³é‡è¦ã€‚é€šè¿‡åˆ©ç”¨ä¸åŒå›¾åƒæ£€æµ‹ä¸­çš„æˆåƒæŠ€æœ¯ï¼Œå­¦ä¹ æ¨¡å‹åœ¨è‡ªåŠ¨ä»ç»„ç»‡ç—…ç†å­¦å›¾åƒè¿›è¡Œç™Œç—‡åˆ†ç±»æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œè¿™åŒ…æ‹¬ç»„ç»‡ç—…ç†å­¦è¯Šæ–­ï¼Œè¿™æ˜¯è¯†åˆ«ç™Œç—‡ç±»å‹çš„é‡è¦å› ç´ ã€‚æœ¬ç ”ç©¶è‡´åŠ›äºåˆ›å»ºä¸€ä¸ªé«˜æ•ˆçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºä»ç»„ç»‡ç—…ç†å­¦å›¾åƒä¸­è¯†åˆ«è‚ºç™Œå’Œç»“è‚ ç™Œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ”¹è¿›åçš„æ®‹å·®æ³¨æ„åŠ›ç½‘ç»œæ¶æ„çš„æ–°æ–¹æ³•ã€‚è¯¥æ¨¡å‹åœ¨åŒ…å«å¤šä¸ªç±»åˆ«çš„25,000å¼ é«˜åˆ†è¾¨ç‡ç»„ç»‡ç—…ç†å­¦å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨ä¸¤ä¸ªç±»åˆ«ã€ä¸‰ä¸ªç±»åˆ«å’Œäº”ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†æƒŠäººçš„99.30%ã€96.63%å’Œ97.56%ï¼Œè¶…è¿‡äº†å…¶ä»–å…ˆè¿›çš„æ¶æ„ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºè‚ºç™Œå’Œç»“è‚ ç™Œåˆ†ç±»çš„é«˜ç²¾åº¦æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºçš„æ¨¡å‹çš„å“è¶Šæ€§èƒ½æ»¡è¶³äº†åŒ»ç–—äººå·¥æ™ºèƒ½åº”ç”¨ä¸­çš„å…³é”®éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17700v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‚ºç™Œå’Œç»“è‚ ç™Œæ˜¯ç™Œç—‡æ­»äº¡çš„ä¸»è¦è´¡çŒ®è€…ï¼Œæ—©æœŸå‡†ç¡®è¯Šæ–­å¯¹æœ‰æ•ˆæ²»ç–—è‡³å…³é‡è¦ã€‚åˆ©ç”¨å›¾åƒæ£€æµ‹ä¸­çš„æˆåƒæŠ€æœ¯ï¼Œå­¦ä¹ æ¨¡å‹åœ¨è‡ªåŠ¨ä»ç—…ç†å›¾åƒè¿›è¡Œç™Œç—‡åˆ†ç±»æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»äº†ä¸€ç§åŸºäºæ”¹è¿›å‹æ®‹å·®æ³¨æ„åŠ›ç½‘ç»œæ¶æ„çš„é«˜æ•ˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºè¯†åˆ«è‚ºç™Œå’Œç»“è‚ ç™Œã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªç±»åˆ«çš„25000å¼ é«˜åˆ†è¾¨ç‡ç—…ç†å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯¹äºŒã€ä¸‰ã€äº”ç±»åˆ†ç±»çš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†99.30%ã€96.63%å’Œ97.56%ï¼Œè¶…è¿‡äº†å…¶ä»–æœ€å…ˆè¿›æ¶æ„çš„æ€§èƒ½ã€‚æœ¬ç ”ç©¶ä¸ºè§£å†³åŒ»å­¦äººå·¥æ™ºèƒ½åº”ç”¨ä¸­çš„è¿«åˆ‡éœ€æ±‚æä¾›äº†ä¸€ä¸ªé«˜åº¦å‡†ç¡®çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚ºç™Œå’Œç»“è‚ ç™Œæ˜¯ç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ï¼Œæ—©æœŸå‡†ç¡®è¯Šæ–­å¯¹æ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è‡ªåŠ¨ä»ç—…ç†å›¾åƒè¿›è¡Œç™Œç—‡åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ”¹è¿›å‹æ®‹å·®æ³¨æ„åŠ›ç½‘ç»œæ¶æ„çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºè¯†åˆ«è‚ºç™Œå’Œç»“è‚ ç™Œã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªç±»åˆ«çš„ç—…ç†å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå–å¾—äº†é«˜å‡†ç¡®ç‡ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½è¶…è¿‡äº†å…¶ä»–æœ€å…ˆè¿›æ¶æ„ï¼Œä¸ºè§£å†³åŒ»å­¦äººå·¥æ™ºèƒ½åº”ç”¨ä¸­çš„éœ€æ±‚æä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>è¯¥æ¨¡å‹çš„åº”ç”¨æœ‰åŠ©äºæå‡ç™Œç—‡è¯Šæ–­çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9425cf4d64a086bcabb4b0c9506a80e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e56e8e0815375906cd3c8be83fcafd61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ab7e046a963a8a0951a57d84267192e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8deb6ac4ed54cc2f012bd2ec2cd0306b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa9be0b6cc0e5e300aeec7df41907188.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35edd7b96fef864d9d339274450c25a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad463fd6fcbad4c2e07c6d4203e8d755.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26baec8399410b6b9e21a24bc9412e29.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Assessment-of-Deep-Learning-Methods-for-the-Enhancement-of-Experimental-Low-Dose-Dental-CBCT-Volumes"><a href="#Assessment-of-Deep-Learning-Methods-for-the-Enhancement-of-Experimental-Low-Dose-Dental-CBCT-Volumes" class="headerlink" title="Assessment of Deep-Learning Methods for the Enhancement of Experimental   Low Dose Dental CBCT Volumes"></a>Assessment of Deep-Learning Methods for the Enhancement of Experimental   Low Dose Dental CBCT Volumes</h2><p><strong>Authors:Louise Friotâ€“Giroux, FranÃ§oise Peyrin, VoichiÅ£a Maxim</strong></p>
<p>Cone-beam tomography enables rapid 3D acquisitions, making it a suitable imaging modality for dental imaging. However, as with all X-ray techniques, the main challenge is to reduce the dose while maintaining good image quality. Moreover, dental reconstructions face a series of issues stemming from truncated projections as well as metal and cone beam artifacts. The aim here is to investigate the ability of neural networks to improve the quality of 3D CBCT dental images at low doses. We test different configurations of convolutional neural networks, trained in a supervised way to reduce artifacts and noise present in analytically reconstructed volumes. In a study on 32 experimental cone beam volumes, we show their capacity to preserve and enhance details while still reducing the artifacts. The best results are obtained with a 3D U-Net which compares advantageously with a TV regularized iterative method and is considerably faster. </p>
<blockquote>
<p>é”¥å½¢æŸæ–­å±‚æ‰«ææŠ€æœ¯èƒ½å¤Ÿå®ç°å¿«é€Ÿçš„ä¸‰ç»´é‡‡é›†ï¼Œä½¿å…¶æˆä¸ºç‰™ç§‘æˆåƒçš„åˆé€‚æˆåƒæ–¹å¼ã€‚ç„¶è€Œï¼Œä¸æ‰€æœ‰Xå°„çº¿æŠ€æœ¯ä¸€æ ·ï¼Œä¸»è¦æŒ‘æˆ˜æ˜¯åœ¨ä¿æŒè‰¯å¥½å›¾åƒè´¨é‡çš„åŒæ—¶å‡å°‘å‰‚é‡ã€‚æ­¤å¤–ï¼Œç‰™ç§‘é‡å»ºè¿˜é¢ä¸´ä¸€ç³»åˆ—ç”±æˆªæ–­æŠ•å½±ä»¥åŠé‡‘å±å’Œé”¥å½¢æŸä¼ªå½±å¼•èµ·çš„é—®é¢˜ã€‚è¿™é‡Œçš„ç›®æ ‡æ˜¯ç ”ç©¶ç¥ç»ç½‘ç»œåœ¨æé«˜ä½å‰‚é‡ä¸‰ç»´é”¥å½¢æŸCTç‰™ç§‘å›¾åƒè´¨é‡æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æµ‹è¯•äº†ä¸åŒé…ç½®çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œä»¥ç›‘ç£å­¦ä¹ çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä»¥å‡å°‘åˆ†æé‡å»ºä½“ç§¯ä¸­å­˜åœ¨çš„ä¼ªå½±å’Œå™ªå£°ã€‚åœ¨32ä¸ªå®éªŒæ€§é”¥å½¢æŸä½“ç§¯çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å®ƒä»¬åœ¨ä¿ç•™å’Œå¢å¼ºç»†èŠ‚çš„åŒæ—¶å‡å°‘ä¼ªå½±çš„èƒ½åŠ›ã€‚ä½¿ç”¨ä¸‰ç»´U-Netè·å¾—æœ€ä½³ç»“æœï¼Œå®ƒè¾ƒTVæ­£åˆ™åŒ–è¿­ä»£æ–¹æ³•æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ä¸”é€Ÿåº¦æ›´å¿«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17423v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦é”¥æŸå±‚ææŠ€æœ¯å¯è¿…é€Ÿå®ç°ä¸‰ç»´æˆåƒï¼Œé€‚åˆç‰™ç§‘æˆåƒã€‚å…¶æŒ‘æˆ˜åœ¨äºå¦‚ä½•åœ¨ä¿æŒè‰¯å¥½å›¾åƒè´¨é‡çš„åŒæ—¶é™ä½å‰‚é‡ã€‚ç ”ç©¶æ—¨åœ¨æ¢ç´¢ç¥ç»ç½‘ç»œå¯¹ä½å‰‚é‡ä¸‰ç»´CBCTç‰™ç§‘å›¾åƒè´¨é‡æå‡çš„æ•ˆæœã€‚æµ‹è¯•äº†ä¸åŒé…ç½®çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ çš„æ–¹å¼è®­ç»ƒä»¥å‡å°‘è§£æé‡å»ºä½“ç§¯ä¸­çš„ä¼ªå½±å’Œå™ªå£°ã€‚åœ¨32ä¸ªå®éªŒé”¥æŸä½“ç§¯çš„ç ”ç©¶ä¸­ï¼Œè¯æ˜äº†è¿™äº›ç½‘ç»œåœ¨ä¿ç•™å’Œå¢å¼ºç»†èŠ‚çš„åŒæ—¶å‡å°‘ä¼ªå½±çš„èƒ½åŠ›ã€‚æœ€ä½³ç»“æœç”±ä¸‰ç»´U-Netè·å¾—ï¼Œä¸TVæ­£åˆ™åŒ–è¿­ä»£æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ï¼Œä¸”é€Ÿåº¦æ›´å¿«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é”¥æŸå±‚ææŠ€æœ¯å¯å®ç°å¿«é€Ÿä¸‰ç»´æˆåƒï¼Œé€‚ç”¨äºç‰™ç§‘æˆåƒã€‚</li>
<li>é™ä½å‰‚é‡åŒæ—¶ä¿æŒè‰¯å¥½å›¾åƒè´¨é‡æ˜¯å…¶ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶æ—¨åœ¨æ¢ç´¢ç¥ç»ç½‘ç»œåœ¨æå‡ä½å‰‚é‡ä¸‰ç»´CBCTç‰™ç§‘å›¾åƒè´¨é‡æ–¹é¢çš„ä½œç”¨ã€‚</li>
<li>é€šè¿‡ç›‘ç£å­¦ä¹ è®­ç»ƒä¸åŒé…ç½®çš„å·ç§¯ç¥ç»ç½‘ç»œä»¥å‡å°‘è§£æé‡å»ºä½“ç§¯ä¸­çš„ä¼ªå½±å’Œå™ªå£°ã€‚</li>
<li>åœ¨å®éªŒç ”ç©¶ä¸­ï¼Œç¥ç»ç½‘ç»œèƒ½å¤Ÿä¿ç•™å’Œå¢å¼ºç»†èŠ‚ï¼ŒåŒæ—¶å‡å°‘ä¼ªå½±ã€‚</li>
<li>ä¸‰ç»´U-Netè·å¾—æœ€ä½³ç»“æœï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17423">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-56562c7bbd1ee7608aca4384f3415d50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a42eaf6bb487e03d164d45d2c35b4a5c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba2b4704736ae19ed7dc907cb2986eda.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PointVoxelFormer-â€“-Reviving-point-cloud-networks-for-3D-medical-imaging"><a href="#PointVoxelFormer-â€“-Reviving-point-cloud-networks-for-3D-medical-imaging" class="headerlink" title="PointVoxelFormer â€“ Reviving point cloud networks for 3D medical imaging"></a>PointVoxelFormer â€“ Reviving point cloud networks for 3D medical imaging</h2><p><strong>Authors:Mattias Paul Heinrich</strong></p>
<p>Point clouds are a very efficient way to represent volumetric data in medical imaging. First, they do not occupy resources for empty spaces and therefore can avoid trade-offs between resolution and field-of-view for voxel-based 3D convolutional networks (CNNs) - leading to smaller and robust models. Second, they provide a modality agnostic representation of anatomical surfaces and shapes to avoid domain gaps for generic geometric models. Third, they remove identifiable patient-specific information and may increase privacy preservation when publicly sharing data. Despite their benefits, point clouds are still underexplored in medical imaging compared to volumetric 3D CNNs and vision transformers. To date both datasets and stringent studies on comparative strengths and weaknesses of methodological choices are missing. Interactions and information exchange of spatially close points - e.g. through k-nearest neighbour graphs in edge convolutions or point transformations - within points clouds are crucial for learning geometrically meaningful features but may incur computational bottlenecks. This work presents a hybrid approach that combines point-wise operations with intermediate differentiable rasterisation and dense localised CNNs. For deformable point cloud registration, we devise an early fusion scheme for coordinate features that joins both clouds within a common reference frame and is coupled with an inverse consistent, two-step alignment architecture. Our extensive experiments on three different datasets for segmentation and registration demonstrate that our method, PointVoxelFormer, enables very compact models that excel with threefold speed-ups, fivefold memory reduction and over 30% registration error reduction against edge convolutions and other state-of-the-art models in geometric deep learning. </p>
<blockquote>
<p>ç‚¹äº‘æ˜¯åŒ»å­¦æˆåƒä¸­è¡¨ç¤ºä½“ç§¯æ•°æ®çš„ä¸€ç§éå¸¸é«˜æ•ˆçš„æ–¹å¼ã€‚é¦–å…ˆï¼Œå®ƒä»¬ä¸å ç”¨ç©ºç©ºé—´çš„èµ„æºï¼Œå› æ­¤å¯ä»¥é¿å…åŸºäºä½“ç´ çš„3Då·ç§¯ç½‘ç»œï¼ˆCNNï¼‰åœ¨åˆ†è¾¨ç‡å’Œè§†é‡ä¹‹é—´çš„æƒè¡¡ï¼Œä»è€Œå¸¦æ¥æ›´å°ä¸”æ›´ç¨³å¥çš„æ¨¡å‹ã€‚å…¶æ¬¡ï¼Œå®ƒä»¬æä¾›äº†ä¸€ç§é€šç”¨çš„è§£å‰–è¡¨é¢å’Œå½¢çŠ¶çš„è¡¨ç¤ºæ–¹æ³•ï¼Œé¿å…äº†é€šç”¨å‡ ä½•æ¨¡å‹çš„é¢†åŸŸå·®å¼‚ã€‚ç¬¬ä¸‰ï¼Œå®ƒä»¬æ¶ˆé™¤äº†å¯è¯†åˆ«æ‚£è€…ç‰¹å®šçš„ä¿¡æ¯ï¼Œåœ¨å…¬å¼€å…±äº«æ•°æ®æ—¶å¯èƒ½å¢åŠ éšç§ä¿æŠ¤ã€‚å°½ç®¡ç‚¹äº‘å…·æœ‰è¯¸å¤šä¼˜ç‚¹ï¼Œä½†åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œä¸ä½“ç§¯3D CNNå’Œè§†è§‰å˜å‹å™¨ç›¸æ¯”ï¼Œå¯¹ç‚¹äº‘çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œç¼ºå°‘æ•°æ®é›†ä»¥åŠå¯¹æ–¹æ³•è®ºé€‰æ‹©æ¯”è¾ƒä¼˜åŠ¿å’ŒåŠ£åŠ¿çš„ä¸¥æ ¼ç ”ç©¶ã€‚ç‚¹äº‘å†…éƒ¨ç©ºé—´ä¸Šæ¥è¿‘çš„ç‚¹ä¹‹é—´çš„äº¤äº’å’Œä¿¡æ¯äº¤æ¢è‡³å…³é‡è¦ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡è¾¹ç¼˜å·ç§¯ä¸­çš„kæœ€è¿‘é‚»å›¾æˆ–ç‚¹å˜æ¢ï¼‰ï¼Œè¿™å¯¹äºå­¦ä¹ å‡ ä½•ä¸Šæœ‰æ„ä¹‰çš„ç‰¹å¾å¯èƒ½æ˜¯è®¡ç®—ç“¶é¢ˆã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§ç»“åˆç‚¹æ“ä½œå’Œä¸­é—´å¯å¾®åˆ†å…‰æ …åŒ–ä»¥åŠå¯†é›†å±€éƒ¨CNNçš„æ··åˆæ–¹æ³•ã€‚å¯¹äºå¯å˜å½¢ç‚¹äº‘æ³¨å†Œï¼Œæˆ‘ä»¬ä¸ºåæ ‡ç‰¹å¾è®¾è®¡äº†ä¸€ç§æ—©æœŸèåˆæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå°†ä¸¤ä¸ªäº‘å†…åµŒåˆ°ä¸€ä¸ªå…±åŒå‚è€ƒæ¡†æ¶ä¸­ï¼Œå¹¶ä¸é€†å‘ä¸€è‡´çš„ä¸¤æ­¥å¯¹é½æ¶æ„ç›¸ç»“åˆã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„PointVoxelFormeræ–¹æ³•ä½¿æ¨¡å‹éå¸¸ç´§å‡‘ï¼Œåœ¨åˆ†å‰²å’Œæ³¨å†Œæ–¹é¢å®ç°äº†ä¸‰å€çš„é€Ÿåº¦æå‡ã€äº”å€çš„å†…å­˜å‡å°‘å’Œè¶…è¿‡30%çš„æ³¨å†Œé”™è¯¯ç‡é™ä½ï¼Œç›¸è¾ƒäºè¾¹ç¼˜å·ç§¯å’Œå…¶ä»–å‡ ä½•æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€å…ˆè¿›æ¨¡å‹è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17390v1">PDF</a> 15 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>ç‚¹äº‘åœ¨åŒ»å­¦æˆåƒä¸­ä»£è¡¨ä½“ç§¯æ•°æ®æ˜¯ä¸€ç§éå¸¸é«˜æ•ˆçš„æ–¹å¼ã€‚å®ƒå¯é¿å…å¯¹ç©ºç©ºé—´çš„èµ„æºå ç”¨ï¼Œä¸ºåŸºäºä½“ç´ çš„3Då·ç§¯ç½‘ç»œï¼ˆCNNï¼‰å¸¦æ¥æ›´å°ã€æ›´ç¨³å¥çš„æ¨¡å‹ã€‚ç‚¹äº‘è¿˜æä¾›äº†é€šç”¨çš„å‡ ä½•æ¨¡å‹è¡¨ç¤ºï¼Œå¯é¿å…é¢†åŸŸå·®è·ï¼ŒåŒæ—¶æé«˜éšç§ä¿æŠ¤ã€‚å°½ç®¡ç‚¹äº‘åœ¨åŒ»å­¦æˆåƒä¸­çš„åº”ç”¨ä»ç›¸å¯¹è¾ƒå°‘ï¼Œä½†æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆç‚¹æ“ä½œå’Œä¸­é—´å¯å¾®åˆ†æ …æ ¼åŒ–çš„æ··åˆæ–¹æ³•ï¼Œä»¥åŠä¸å¯å˜å½¢ç‚¹äº‘æ³¨å†Œçš„æ—©æœŸèåˆæ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†éå¸¸ç´§å‡‘çš„æ¨¡å‹ï¼Œå…·æœ‰ä¸‰å€é€Ÿåº¦æå‡ã€äº”å€å†…å­˜å‡å°‘å’Œè¶…è¿‡30%çš„æ³¨å†Œè¯¯å·®å‡å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‚¹äº‘åœ¨åŒ»å­¦æˆåƒä¸­ä»£è¡¨ä½“ç§¯æ•°æ®å…·æœ‰é«˜æ•ˆç‡ã€‚</li>
<li>ç‚¹äº‘å¯é¿å…å¯¹ç©ºç©ºé—´çš„èµ„æºå ç”¨ï¼Œä¸ºåŸºäºä½“ç´ çš„3Då·ç§¯ç½‘ç»œå¸¦æ¥æ›´å°ã€æ›´ç¨³å¥çš„æ¨¡å‹ã€‚</li>
<li>ç‚¹äº‘æä¾›äº†é€šç”¨çš„å‡ ä½•æ¨¡å‹è¡¨ç¤ºï¼Œæœ‰åŠ©äºé¿å…é¢†åŸŸå·®è·å’Œæé«˜éšç§ä¿æŠ¤ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œç‚¹äº‘åœ¨åŒ»å­¦æˆåƒä¸­çš„åº”ç”¨ä»ç„¶ç›¸å¯¹è¾ƒå°‘ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆç‚¹æ“ä½œå’Œä¸­é—´å¯å¾®åˆ†æ …æ ¼åŒ–çš„æ··åˆæ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸å¯å˜å½¢ç‚¹äº‘æ³¨å†Œçš„æ—©æœŸèåˆæ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-847f08176c94e6be4a9b1620ca07de8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9165acaf8ae102903c92afd3c45c2e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31c35448f22bff487645f54b68be8672.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="QTSeg-A-Query-Token-Based-Architecture-for-Efficient-2D-Medical-Image-Segmentation"><a href="#QTSeg-A-Query-Token-Based-Architecture-for-Efficient-2D-Medical-Image-Segmentation" class="headerlink" title="QTSeg: A Query Token-Based Architecture for Efficient 2D Medical Image   Segmentation"></a>QTSeg: A Query Token-Based Architecture for Efficient 2D Medical Image   Segmentation</h2><p><strong>Authors:Phuong-Nam Tran, Nhat Truong Pham, Duc Ngoc Minh Dang, Eui-Nam Huh, Choong Seon Hong</strong></p>
<p>Medical image segmentation is crucial in assisting medical doctors in making diagnoses and enabling accurate automatic diagnosis. While advanced convolutional neural networks (CNNs) excel in segmenting regions of interest with pixel-level precision, they often struggle with long-range dependencies, which is crucial for enhancing model performance. Conversely, transformer architectures leverage attention mechanisms to excel in handling long-range dependencies. However, the computational complexity of transformers grows quadratically, posing resource-intensive challenges, especially with high-resolution medical images. Recent research aims to combine CNN and transformer architectures to mitigate their drawbacks and enhance performance while keeping resource demands low. Nevertheless, existing approaches have not fully leveraged the strengths of both architectures to achieve high accuracy with low computational requirements. To address this gap, we propose a novel architecture for 2D medical image segmentation (QTSeg) that leverages a feature pyramid network (FPN) as the image encoder, a multi-level feature fusion (MLFF) as the adaptive module between encoder and decoder and a multi-query mask decoder (MQM Decoder) as the mask decoder. In the first step, an FPN model extracts pyramid features from the input image. Next, MLFF is incorporated between the encoder and decoder to adapt features from different encoder stages to the decoder. Finally, an MQM Decoder is employed to improve mask generation by integrating query tokens with pyramid features at all stages of the mask decoder. Our experimental results show that QTSeg outperforms state-of-the-art methods across all metrics with lower computational demands than the baseline and the existing methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/tpnam0901/QTSeg">https://github.com/tpnam0901/QTSeg</a> (v0.1.0) </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºååŠ©åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­å’Œå®ç°å‡†ç¡®çš„è‡ªåŠ¨è¯Šæ–­è‡³å…³é‡è¦ã€‚è™½ç„¶å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰åœ¨åˆ†å‰²æ„Ÿå…´è¶£åŒºåŸŸæ–¹é¢å…·æœ‰åƒç´ çº§ç²¾åº¦ï¼Œä½†å®ƒä»¬å¾€å¾€åœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»æ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œè¿™å¯¹äºæé«˜æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚ç›¸åï¼Œtransformeræ¶æ„åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œéšç€è®¡ç®—å¤æ‚åº¦çš„å¢é•¿ï¼Œå…¶å¤æ‚åº¦å‘ˆç°äºŒæ¬¡æ–¹çš„å¢é•¿ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒæ¥è¯´èµ„æºå¯†é›†å‹æŒ‘æˆ˜æ›´åŠ æ˜æ˜¾ã€‚æœ€è¿‘çš„ç ”ç©¶æ—¨åœ¨ç»“åˆCNNå’Œtransformeræ¶æ„ä»¥ç¼“è§£å®ƒä»¬çš„ç¼ºç‚¹å¹¶å¢å¼ºæ€§èƒ½çš„åŒæ—¶ä¿æŒä½èµ„æºéœ€æ±‚ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å°šæœªå……åˆ†åˆ©ç”¨ä¸¤ç§æ¶æ„çš„ä¼˜åŠ¿æ¥å®ç°é«˜å‡†ç¡®æ€§å¹¶æ»¡è¶³ä½è®¡ç®—è¦æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºäºŒç»´åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆQTSegï¼‰çš„æ–°å‹æ¶æ„ï¼Œè¯¥æ¶æ„åˆ©ç”¨ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆFPNï¼‰ä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œå¤šçº§ç‰¹å¾èåˆï¼ˆMLFFï¼‰ä½œä¸ºç¼–ç å™¨ä¸è§£ç å™¨ä¹‹é—´çš„è‡ªé€‚åº”æ¨¡å—ï¼Œä»¥åŠå¤šæŸ¥è¯¢æ©è†œè§£ç å™¨ï¼ˆMQM Decoderï¼‰ä½œä¸ºæ©è†œè§£ç å™¨ã€‚é¦–å…ˆï¼ŒFPNæ¨¡å‹ä»è¾“å…¥å›¾åƒä¸­æå–é‡‘å­—å¡”ç‰¹å¾ã€‚ç„¶åï¼ŒMLFFè¢«æ•´åˆåˆ°ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´ï¼Œä»¥å°†ä¸åŒç¼–ç å™¨é˜¶æ®µçš„ç‰¹å¾è‡ªé€‚åº”åˆ°è§£ç å™¨ã€‚æœ€åï¼Œä½¿ç”¨MQM Decoderé€šè¿‡åœ¨ä¸æ©è†œè§£ç å™¨æ‰€æœ‰é˜¶æ®µçš„æŸ¥è¯¢ä»¤ç‰Œé›†æˆä¸­æ”¹è¿›æ©è†œç”Ÿæˆã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒQTSegåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œå¹¶ä¸”ç›¸å¯¹äºåŸºçº¿æ–¹æ³•å’Œç°æœ‰æ–¹æ³•çš„è®¡ç®—éœ€æ±‚æ›´ä½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tpnam0901/QTSeg%EF%BC%88v0.1.0%EF%BC%89%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tpnam0901/QTSegï¼ˆv0.1.0ï¼‰æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17241v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºåŒ»ç”Ÿè¯Šæ–­å’Œè‡ªåŠ¨è¯Šæ–­çš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨åƒç´ çº§ç²¾ç¡®åˆ†å‰²æ„Ÿå…´è¶£åŒºåŸŸæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ç›¸åï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å˜å‹å™¨æ¶æ„æ“…é•¿å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œä½†è®¡ç®—å¤æ‚åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿ï¼Œå¯¹äºé«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒå°¤å…¶å¦‚æ­¤ã€‚æœ€è¿‘çš„ç ”ç©¶æ—¨åœ¨ç»“åˆCNNå’Œå˜å‹å™¨æ¶æ„ä»¥å–é•¿è¡¥çŸ­ï¼ŒåŒæ—¶ä¿æŒèµ„æºéœ€æ±‚è¾ƒä½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¹¶æœªå……åˆ†åˆ©ç”¨ä¸¤è€…çš„ä¼˜åŠ¿ï¼Œå®ç°é«˜å‡†ç¡®ç‡ä¸”è®¡ç®—é‡ä½çš„ç›®æ ‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„äºŒç»´åŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„ï¼ˆQTSegï¼‰ï¼Œå®ƒåˆ©ç”¨ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆFPNï¼‰ä½œä¸ºå›¾åƒç¼–ç å™¨ï¼Œå¤šå±‚æ¬¡ç‰¹å¾èåˆï¼ˆMLFFï¼‰ä½œä¸ºç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„è‡ªé€‚åº”æ¨¡å—ï¼Œä»¥åŠå¤šæŸ¥è¯¢æ©è†œè§£ç å™¨ï¼ˆMQMè§£ç å™¨ï¼‰ä½œä¸ºæ©è†œè§£ç å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQTSegåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯æ–¹æ³•ï¼ŒåŒæ—¶ç›¸è¾ƒäºåŸºå‡†æ–¹æ³•å’Œç°æœ‰æ–¹æ³•çš„è®¡ç®—éœ€æ±‚æ›´ä½ã€‚ä»£ç å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/tpnam0901/QTSeg%EF%BC%88v0.1.0%EF%BC%89%E3%80%82">https://github.com/tpnam0901/QTSegï¼ˆv0.1.0ï¼‰ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹åŒ»ç”Ÿè¯Šæ–­å’Œè‡ªåŠ¨è¯Šæ–­çš„é‡è¦æ€§ã€‚</li>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨å¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²æ—¶çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚</li>
<li>å˜å‹å™¨æ¶æ„åœ¨å¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»æ–¹é¢çš„ä¼˜åŠ¿ï¼ŒåŠå…¶åœ¨é«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç»“åˆCNNå’Œå˜å‹å™¨çš„æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨ä¸¤è€…çš„ä¼˜åŠ¿ã€‚</li>
<li>æå‡ºçš„QTSegæ¶æ„åŒ…æ‹¬ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆFPNï¼‰å›¾åƒç¼–ç å™¨ã€å¤šå±‚æ¬¡ç‰¹å¾èåˆï¼ˆMLFFï¼‰è‡ªé€‚åº”æ¨¡å—å’Œå¤šæŸ¥è¯¢æ©è†œè§£ç å™¨ï¼ˆMQMè§£ç å™¨ï¼‰ã€‚</li>
<li>QTSegåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•ï¼ŒåŒæ—¶è®¡ç®—éœ€æ±‚è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17241">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2faa4d96ebea1466cd86f3a526605455.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d91b34dff1139d42b1c6a9194a3fc201.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4c22e54a29b2cc4fe82d345bed51c11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3646cf7a0a2783921a1225bf3dc69f7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de2a6a6219f2190f077fb52d077688fe.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rethinking-Cancer-Gene-Identification-through-Graph-Anomaly-Analysis"><a href="#Rethinking-Cancer-Gene-Identification-through-Graph-Anomaly-Analysis" class="headerlink" title="Rethinking Cancer Gene Identification through Graph Anomaly Analysis"></a>Rethinking Cancer Gene Identification through Graph Anomaly Analysis</h2><p><strong>Authors:Yilong Zang, Lingfei Ren, Yue Li, Zhikang Wang, David Antony Selby, Zheng Wang, Sebastian Josef Vollmer, Hongzhi Yin, Jiangning Song, Junhang Wu</strong></p>
<p>Graph neural networks (GNNs) have shown promise in integrating protein-protein interaction (PPI) networks for identifying cancer genes in recent studies. However, due to the insufficient modeling of the biological information in PPI networks, more faithfully depiction of complex protein interaction patterns for cancer genes within the graph structure remains largely unexplored. This study takes a pioneering step toward bridging biological anomalies in protein interactions caused by cancer genes to statistical graph anomaly. We find a unique graph anomaly exhibited by cancer genes, namely weight heterogeneity, which manifests as significantly higher variance in edge weights of cancer gene nodes within the graph. Additionally, from the spectral perspective, we demonstrate that the weight heterogeneity could lead to the â€œflattening outâ€ of spectral energy, with a concentration towards the extremes of the spectrum. Building on these insights, we propose the HIerarchical-Perspective Graph Neural Network (HIPGNN) that not only determines spectral energy distribution variations on the spectral perspective, but also perceives detailed protein interaction context on the spatial perspective. Extensive experiments are conducted on two reprocessed datasets STRINGdb and CPDB, and the experimental results demonstrate the superiority of HIPGNN. </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨æœ€è¿‘çš„ç ”ç©¶ä¸­æ˜¾ç¤ºå‡ºåœ¨æ•´åˆè›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰ç½‘ç»œä»¥è¯†åˆ«ç™Œç—‡åŸºå› çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºPPIç½‘ç»œä¸­ç”Ÿç‰©ä¿¡æ¯å»ºæ¨¡çš„ä¸è¶³ï¼Œå¯¹å›¾ç»“æ„å†…ç™Œç—‡åŸºå› çš„å¤æ‚è›‹ç™½è´¨ç›¸äº’ä½œç”¨æ¨¡å¼çš„æ›´çœŸå®æè¿°ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶ç‡å…ˆè¿ˆå‡ºä¸€æ­¥ï¼Œå°†è›‹ç™½è´¨ç›¸äº’ä½œç”¨ä¸­çš„ç”Ÿç‰©å¼‚å¸¸ä¸ç”±ç™Œç—‡åŸºå› å¼•èµ·çš„ç»Ÿè®¡å›¾å¼‚å¸¸è”ç³»èµ·æ¥ã€‚æˆ‘ä»¬å‘ç°äº†ä¸€ç§ç‰¹æ®Šçš„ç™Œç—‡åŸºå› æ‰€è¡¨ç°å‡ºçš„å›¾å¼‚å¸¸ç°è±¡ï¼Œå³æƒé‡å¼‚è´¨æ€§ï¼Œå®ƒè¡¨ç°ä¸ºå›¾ä¸­ç™Œç—‡åŸºå› èŠ‚ç‚¹è¾¹æƒé‡çš„æ–¹å·®æ˜¾è‘—å¢å¤§ã€‚æ­¤å¤–ï¼Œä»å…‰è°±çš„è§’åº¦ï¼Œæˆ‘ä»¬è¯æ˜äº†æƒé‡å¼‚è´¨æ€§å¯èƒ½å¯¼è‡´å…‰è°±èƒ½é‡çš„â€œå¹³å¦åŒ–â€ï¼Œå¹¶é›†ä¸­åœ¨å…‰è°±çš„æç«¯éƒ¨åˆ†ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚è§†è§’å›¾ç¥ç»ç½‘ç»œï¼ˆHIPGNNï¼‰ï¼Œå®ƒä¸ä»…åœ¨å…‰è°±è§†è§’ä¸Šç¡®å®šå…‰è°±èƒ½é‡åˆ†å¸ƒçš„å˜åŒ–ï¼Œè€Œä¸”åœ¨ç©ºé—´è§†è§’ä¸Šæ„ŸçŸ¥è¯¦ç»†çš„è›‹ç™½è´¨ç›¸äº’ä½œç”¨ä¸Šä¸‹æ–‡ã€‚åœ¨ä¸¤ä¸ªç»è¿‡å¤„ç†çš„STRINGdbå’ŒCPDBæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå®éªŒç»“æœè¡¨æ˜HIPGNNçš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17240v1">PDF</a> It has been accepted by the AAAI 2025 conference</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æ¢ç´¢äº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨æ•´åˆè›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰ç½‘ç»œä¸­çš„æ½œåŠ›ï¼Œä»¥è¯†åˆ«ç™Œç—‡åŸºå› ã€‚é’ˆå¯¹PPIç½‘ç»œä¸­ç”Ÿç‰©ä¿¡æ¯å»ºæ¨¡ä¸è¶³çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶é¦–æ¬¡å°†ç™Œç—‡åŸºå› å¼•èµ·çš„è›‹ç™½è´¨ç›¸äº’ä½œç”¨ä¸­çš„ç”Ÿç‰©å­¦å¼‚å¸¸ä¸å›¾ç»Ÿè®¡å¼‚å¸¸è”ç³»èµ·æ¥ã€‚ç ”ç©¶å‘ç°ç™Œç—‡åŸºå› å…·æœ‰ç‹¬ç‰¹çš„å›¾å¼‚å¸¸ç°è±¡â€”â€”æƒé‡å¼‚è´¨æ€§ï¼Œè¡¨ç°ä¸ºå›¾ä¸­ç™Œç—‡åŸºå› èŠ‚ç‚¹è¾¹æƒé‡çš„æ–¹å·®æ˜¾è‘—å¢å¤§ã€‚æ­¤å¤–ï¼Œä»å…‰è°±è§’åº¦ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†æƒé‡å¼‚è´¨æ€§å¯èƒ½å¯¼è‡´å…‰è°±èƒ½é‡çš„â€œæ‰å¹³åŒ–â€ï¼Œé›†ä¸­åœ¨å…‰è°±çš„æç«¯éƒ¨åˆ†ã€‚åŸºäºè¿™äº›è§è§£ï¼Œè¯¥ç ”ç©¶æå‡ºäº†åˆ†å±‚è§†è§’å›¾ç¥ç»ç½‘ç»œï¼ˆHIPGNNï¼‰ï¼Œå®ƒä¸ä»…åœ¨å…‰è°±è§†è§’ä¸Šç¡®å®šäº†å…‰è°±èƒ½é‡åˆ†å¸ƒçš„å˜åŒ–ï¼Œè¿˜åœ¨ç©ºé—´è§†è§’ä¸Šæ„ŸçŸ¥åˆ°äº†è¯¦ç»†çš„è›‹ç™½è´¨ç›¸äº’ä½œç”¨ä¸Šä¸‹æ–‡ã€‚ç»è¿‡åœ¨ä¸¤ä¸ªé‡æ–°å¤„ç†çš„æ•°æ®é›†STRINGdbå’ŒCPDBä¸Šçš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†HIPGNNçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨æ•´åˆè›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰ç½‘ç»œä»¥è¯†åˆ«ç™Œç—‡åŸºå› æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>PPIç½‘ç»œä¸­ç”Ÿç‰©ä¿¡æ¯å»ºæ¨¡ä¸è¶³ï¼Œéœ€è¦æ›´çœŸå®åœ°æç»˜ç™Œç—‡åŸºå› åœ¨å›¾å½¢ç»“æ„ä¸­çš„å¤æ‚ç›¸äº’ä½œç”¨æ¨¡å¼ã€‚</li>
<li>ç™Œç—‡åŸºå› å…·æœ‰ç‹¬ç‰¹çš„å›¾å¼‚å¸¸ç°è±¡â€”â€”æƒé‡å¼‚è´¨æ€§ï¼Œè¡¨ç°ä¸ºå›¾ä¸­ç™Œç—‡åŸºå› èŠ‚ç‚¹è¾¹æƒé‡çš„æ–¹å·®å¢å¤§ã€‚</li>
<li>æƒé‡å¼‚è´¨æ€§å¯èƒ½å¯¼è‡´å…‰è°±èƒ½é‡çš„â€œæ‰å¹³åŒ–â€ï¼Œé›†ä¸­åœ¨å…‰è°±çš„æç«¯éƒ¨åˆ†ã€‚</li>
<li>åˆ†å±‚è§†è§’å›¾ç¥ç»ç½‘ç»œï¼ˆHIPGNNï¼‰ç»“åˆäº†å…‰è°±ä¸ç©ºé—´è§†è§’ï¼Œèƒ½æ„ŸçŸ¥è¯¦ç»†çš„è›‹ç™½è´¨ç›¸äº’ä½œç”¨ä¸Šä¸‹æ–‡ã€‚</li>
<li>HIPGNNä¸ä»…ç¡®å®šäº†å…‰è°±èƒ½é‡åˆ†å¸ƒçš„å˜åŒ–ï¼Œè¿˜åœ¨å®éªŒä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cb246d92bfe8e1ecdfffad505ddf21f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2f4bad78e3a08ef1586d5a43e797a70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2445cf9287c6cfeb059ca66b886b509b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6866a278637c93ed706933b274be6837.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47cfcc2b4b581f73195e45d1a7a9af24.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MatchMiner-AI-An-Open-Source-Solution-for-Cancer-Clinical-Trial-Matching"><a href="#MatchMiner-AI-An-Open-Source-Solution-for-Cancer-Clinical-Trial-Matching" class="headerlink" title="MatchMiner-AI: An Open-Source Solution for Cancer Clinical Trial   Matching"></a>MatchMiner-AI: An Open-Source Solution for Cancer Clinical Trial   Matching</h2><p><strong>Authors:Ethan Cerami, Pavel Trukhanov, Morgan A. Paul, Michael J. Hassett, Irbaz B. Riaz, James Lindsay, Emily Mallaber, Harry Klein, Gufran Gungor, Matthew Galvin, Stephen C. Van Nostrand, Joyce Yu, Tali Mazor, Kenneth L. Kehl</strong></p>
<p>Clinical trials drive improvements in cancer treatments and outcomes. However, most adults with cancer do not participate in trials, and trials often fail to enroll enough patients to answer their scientific questions. Artificial intelligence could accelerate matching of patients to appropriate clinical trials. Here, we describe the development and evaluation of the MatchMiner-AI pipeline for clinical trial searching and ranking. MatchMiner-AI focuses on matching patients to potential trials based on core criteria describing clinical â€œspaces,â€ or disease contexts, targeted by a trial. It aims to accelerate the human work of identifying potential matches, not to fully automate trial screening. The pipeline includes modules for extraction of key information from a patientâ€™s longitudinal electronic health record; rapid ranking of candidate trial-patient matches based on embeddings in vector space; and classification of whether a candidate match represents a reasonable clinical consideration. Code and synthetic data are available at <a target="_blank" rel="noopener" href="https://huggingface.co/ksg-dfci/MatchMiner-AI">https://huggingface.co/ksg-dfci/MatchMiner-AI</a> . Model weights based on synthetic data are available at <a target="_blank" rel="noopener" href="https://huggingface.co/ksg-dfci/TrialSpace">https://huggingface.co/ksg-dfci/TrialSpace</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/ksg-dfci/TrialChecker">https://huggingface.co/ksg-dfci/TrialChecker</a> . A simple cancer clinical trial search engine to demonstrate pipeline components is available at <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/ksg-dfci/trial_search_alpha">https://huggingface.co/spaces/ksg-dfci/trial_search_alpha</a> . </p>
<blockquote>
<p>ä¸´åºŠè¯•éªŒæ¨åŠ¨äº†ç™Œç—‡æ²»ç–—å’Œç»“æœçš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç™Œç—‡æ‚£è€…å¹¶æ²¡æœ‰å‚ä¸åˆ°ä¸´åºŠè¯•éªŒä¸­ï¼Œè€Œä¸”è¯•éªŒå¾€å¾€æ— æ³•æ‹›å‹Ÿåˆ°è¶³å¤Ÿçš„æ‚£è€…æ¥å›ç­”å…¶ç§‘å­¦é—®é¢˜ã€‚äººå·¥æ™ºèƒ½å¯ä»¥åŠ é€Ÿæ‚£è€…ä¸åˆé€‚çš„ä¸´åºŠè¯•éªŒçš„åŒ¹é…ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æè¿°äº†MatchMiner-AIç®¡é“çš„ä¸´åºŠè¯•éªŒæœç´¢å’Œæ’åçš„å¼€å‘ä¸è¯„ä»·ã€‚MatchMiner-AIä¸“æ³¨äºæ ¹æ®æè¿°ä¸´åºŠè¯•éªŒæ‰€é’ˆå¯¹çš„ä¸´åºŠâ€œç©ºé—´â€æˆ–ç–¾ç—…èƒŒæ™¯çš„æ ¸å¿ƒæ ‡å‡†ï¼Œä¸ºæ‚£è€…åŒ¹é…æ½œåœ¨çš„è¯•éªŒã€‚å®ƒçš„ç›®æ ‡æ˜¯ä¸ºäº†åŠ é€Ÿè¯†åˆ«æ½œåœ¨åŒ¹é…çš„äººå·¥å·¥ä½œï¼Œè€Œä¸æ˜¯å®Œå…¨è‡ªåŠ¨åŒ–è¯•éªŒç­›é€‰ã€‚è¯¥ç®¡é“åŒ…æ‹¬ä»æ‚£è€…çš„çºµå‘ç”µå­å¥åº·è®°å½•ä¸­æå–å…³é”®ä¿¡æ¯çš„æ¨¡å—ï¼›åŸºäºå‘é‡ç©ºé—´ä¸­çš„åµŒå…¥å¿«é€Ÿæ’åˆ—å€™é€‰è¯•éªŒæ‚£è€…åŒ¹é…ï¼›ä»¥åŠåˆ†ç±»å€™é€‰åŒ¹é…æ˜¯å¦ä»£è¡¨åˆç†çš„ä¸´åºŠè€ƒè™‘ã€‚ä»£ç å’Œåˆæˆæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/ksg-dfci/MatchMiner-AI%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82%E5%9F%BA%E4%BA%8E%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E5%8F%AF%E5%9C%A8https://huggingface.co/ksg-dfci/TrialSpace%E5%92%8Chttps://huggingface.co/ksg-dfci/TrialChecker%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%99%8C%E7%97%87%E4%B8%B4%E5%BA%8A%E8%AF%95%E9%AA%8C%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%EF%BC%8C%E7%94%A8%E4%BA%8E%E5%B1%95%E7%A4%BA%E7%AE%A1%E9%81%93%E7%BB%84%E4%BB%B6%EF%BC%8C%E5%8F%AF%E5%9C%A8https://huggingface.co/spaces/ksg-dfci/trial_search_alpha%E4%B8%8A%E8%AE%BF%E9%97%AE%E3%80%82">https://huggingface.co/ksg-dfci/MatchMiner-AIä¸Šæ‰¾åˆ°ã€‚åŸºäºåˆæˆæ•°æ®çš„æ¨¡å‹æƒé‡å¯åœ¨https://huggingface.co/ksg-dfci/TrialSpaceå’Œhttps://huggingface.co/ksg-dfci/TrialCheckerä¸Šæ‰¾åˆ°ã€‚ä¸€ä¸ªç®€å•çš„ç™Œç—‡ä¸´åºŠè¯•éªŒæœç´¢å¼•æ“ï¼Œç”¨äºå±•ç¤ºç®¡é“ç»„ä»¶ï¼Œå¯åœ¨https://huggingface.co/spaces/ksg-dfci/trial_search_alphaä¸Šè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17228v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½åŒ¹é…æ‚£è€…ä¸åˆé€‚çš„ä¸´åºŠè¯•éªŒå¯åŠ é€Ÿç™Œç—‡æ²»ç–—ä¸æˆæœçš„æ”¹è¿›ã€‚MatchMiner-AIç®¡é“çš„å¼€å‘ä¸è¯„ä¼°ç”¨äºä¸´åºŠè¯•éªŒæœç´¢ä¸æ’åã€‚å…¶é‡ç‚¹æ˜¯æ ¹æ®æè¿°ä¸´åºŠè¯•éªŒæ ¸å¿ƒæ ‡å‡†çš„ä¸´åºŠâ€œç©ºé—´â€æˆ–ç–¾ç—…èƒŒæ™¯ï¼Œä¸ºæ‚£è€…åŒ¹é…æ½œåœ¨è¯•éªŒã€‚æ—¨åœ¨åŠ é€Ÿäººç±»è¯†åˆ«æ½œåœ¨åŒ¹é…çš„å·¥ä½œï¼Œè€Œéå®Œå…¨è‡ªåŠ¨åŒ–è¯•éªŒç­›é€‰ã€‚åŒ…æ‹¬ä»æ‚£è€…çºµå‘ç”µå­ç—…å†ä¸­æå–å…³é”®ä¿¡æ¯çš„æ¨¡å—ã€åŸºäºå‘é‡ç©ºé—´åµŒå…¥å¿«é€Ÿæ’åè¯•éªŒæ‚£è€…åŒ¹é…ä»¥åŠåˆ†ç±»å€™é€‰åŒ¹é…æ˜¯å¦ä»£è¡¨åˆç†çš„ä¸´åºŠè€ƒé‡ã€‚ç›¸å…³èµ„æºå’Œæ¨¡å‹æƒé‡å¯é€šè¿‡ç‰¹å®šé“¾æ¥è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠè¯•éªŒå¯¹ç™Œç—‡æ²»ç–—ä¸æˆæœçš„æ”¹è¿›è‡³å…³é‡è¦ï¼Œä½†æ‚£è€…å‚ä¸åº¦ä¸è¶³å’Œæ‚£è€…æ‹›å‹Ÿå›°éš¾æ˜¯å¸¸è§é—®é¢˜ã€‚</li>
<li>MatchMiner-AIæ—¨åœ¨é€šè¿‡äººå·¥æ™ºèƒ½åŠ é€Ÿæ‚£è€…ä¸åˆé€‚ä¸´åºŠè¯•éªŒçš„åŒ¹é…è¿‡ç¨‹ã€‚</li>
<li>MatchMiner-AIç®¡é“åŒ…æ‹¬ä»æ‚£è€…ç”µå­ç—…å†ä¸­æå–å…³é”®ä¿¡æ¯çš„æ¨¡å—ã€‚</li>
<li>è¯¥ç³»ç»ŸåŸºäºå‘é‡ç©ºé—´åµŒå…¥è¿›è¡Œè¯•éªŒæ‚£è€…åŒ¹é…çš„å¿«é€Ÿæ’åã€‚</li>
<li>MatchMiner-AIæ—¨åœ¨è¾…åŠ©äººç±»è¯†åˆ«æ½œåœ¨åŒ¹é…ï¼Œè€Œéå®Œå…¨è‡ªåŠ¨åŒ–è¯•éªŒç­›é€‰ã€‚</li>
<li>åˆç†åˆ©ç”¨äººå·¥æ™ºèƒ½çš„æŠ€æœ¯å’Œèµ„æºå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://huggingface.co/ksg-dfci/MatchMiner-AI%E7%AD%89%E9%93%BE%E6%8E%A5%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/ksg-dfci/MatchMiner-AIç­‰é“¾æ¥è·å–ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b13ca1dde7df9d15ff1fa6a3c3a5c98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c76b91b492142c7d2f7226507e40c10e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72ffae186c0533d9c139c4bb1f55de6f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Potential-of-Convolutional-Neural-Networks-for-Cancer-Detection"><a href="#The-Potential-of-Convolutional-Neural-Networks-for-Cancer-Detection" class="headerlink" title="The Potential of Convolutional Neural Networks for Cancer Detection"></a>The Potential of Convolutional Neural Networks for Cancer Detection</h2><p><strong>Authors:Hossein Molaeian, Kaveh Karamjani, Sina Teimouri, Saeed Roshani, Sobhan Roshani</strong></p>
<p>Early detection of cancer is critical in improving treatment outcomes and increasing survival rates, particularly for common cancers such as lung, breast, and prostate which collectively contribute to a significant global mortality burden. With advancements in imaging technologies and data processing, Convolutional Neural Networks (CNNs) have emerged as a powerful tool for analyzing and classifying medical images, enabling more precise cancer detection. This paper provides a comprehensive review of recent studies leveraging CNN models for detecting ten different types of cancer. Each study employs distinct CNN architectures to identify patterns associated with these cancers, utilizing diverse datasets. Key differences and strengths of these architectures are meticulously compared and analyzed, highlighting their efficacy in improving early detection. Beyond reviewing the performance and limitations of CNN-based cancer detection methods, this study explores the feasibility of integrating CNNs into clinical settings as an early detection tool, potentially complementing or replacing traditional methods. Despite significant progress, challenges remain, including data diversity, result interpretation, and ethical considerations. By identifying the best-performing CNN architectures and providing a comparative analysis, this study aims to contribute a comprehensive perspective on the application of CNNs in cancer detection and their role in advancing diagnostic capabilities in healthcare. </p>
<blockquote>
<p>ç™Œç—‡çš„æ—©æœŸå‘ç°å¯¹äºæé«˜æ²»ç–—æ•ˆæœå’Œå¢åŠ å­˜æ´»ç‡è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè‚ºç™Œã€ä¹³è…ºç™Œå’Œå‰åˆ—è…ºç™Œç­‰å¸¸è§ç™Œç—‡ï¼Œå®ƒä»¬å…±åŒé€ æˆäº†å…¨çƒå¤§é‡çš„æ­»äº¡è´Ÿæ‹…ã€‚éšç€æˆåƒæŠ€æœ¯å’Œæ•°æ®å¤„ç†çš„å‘å±•ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å·²ç»æˆä¸ºåˆ†æå’Œåˆ†ç±»åŒ»å­¦å›¾åƒçš„å¼ºå¤§å·¥å…·ï¼Œä½¿ç™Œç—‡æ£€æµ‹æ›´åŠ ç²¾ç¡®ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†æœ€è¿‘åˆ©ç”¨CNNæ¨¡å‹æ£€æµ‹åç§ä¸åŒç±»å‹ç™Œç—‡çš„ç ”ç©¶ã€‚æ¯é¡¹ç ”ç©¶é‡‡ç”¨ä¸åŒçš„CNNæ¶æ„æ¥è¯†åˆ«ä¸è¿™äº›ç™Œç—‡ç›¸å…³çš„æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨å„ç§æ•°æ®é›†ã€‚æœ¬æ–‡ä»”ç»†æ¯”è¾ƒå’Œåˆ†æè¿™äº›æ¶æ„çš„ä¸»è¦å·®å¼‚å’Œä¼˜ç‚¹ï¼Œçªå‡ºå…¶åœ¨æé«˜æ—©æœŸæ£€æµ‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é™¤äº†å›é¡¾åŸºäºCNNçš„ç™Œç—‡æ£€æµ‹æ–¹æ³•çš„æ€§èƒ½å’Œå±€é™æ€§å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æ¢è®¨äº†å°†CNNé›†æˆåˆ°ä¸´åºŠç¯å¢ƒä¸­ä½œä¸ºæ—©æœŸæ£€æµ‹å·¥å…·çš„å¯è¡Œæ€§ï¼Œå¯èƒ½è¡¥å……æˆ–æ›¿ä»£ä¼ ç»Ÿæ–¹æ³•ã€‚å°½ç®¡å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®å¤šæ ·æ€§ã€ç»“æœè§£é‡Šå’Œä¼¦ç†è€ƒè™‘ã€‚é€šè¿‡ç¡®å®šè¡¨ç°æœ€ä½³çš„CNNæ¶æ„å¹¶æä¾›æ¯”è¾ƒåˆ†æï¼Œæœ¬ç ”ç©¶æ—¨åœ¨ä¸ºCNNåœ¨ç™Œç—‡æ£€æµ‹ä¸­çš„åº”ç”¨åŠå…¶åœ¨æé«˜åŒ»ç–—è¯Šæ–­èƒ½åŠ›ä¸­çš„ä½œç”¨æä¾›å…¨é¢è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17155v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç»¼è¿°äº†åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹æ£€æµ‹åŒ…æ‹¬è‚ºç™Œã€ä¹³è…ºç™Œå’Œå‰åˆ—è…ºç™Œåœ¨å†…çš„åç§ä¸åŒç±»å‹ç™Œç—‡çš„æœ€æ–°ç ”ç©¶ã€‚ç ”ç©¶æ¯”è¾ƒåˆ†æäº†ä¸åŒCNNæ¶æ„çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å¼ºè°ƒäº†å®ƒä»¬åœ¨æé«˜æ—©æœŸæ£€æµ‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é™¤äº†å›é¡¾CNNåœ¨ç™Œç—‡æ£€æµ‹ä¸­çš„åº”ç”¨åŠå…¶å±€é™æ€§ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†å°†CNNé›†æˆåˆ°ä¸´åºŠç¯å¢ƒä¸­ä½œä¸ºæ—©æœŸæ£€æµ‹å·¥å…·çš„å¯è¡Œæ€§ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºCNNåœ¨ç™Œç—‡æ£€æµ‹ä¸­çš„åº”ç”¨æä¾›ä¸€ä¸ªå…¨é¢çš„è§†è§’ï¼Œæ¨åŠ¨åŒ»ç–—è¯Šæ–­èƒ½åŠ›çš„æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CNNæ¨¡å‹è¢«å¹¿æ³›åº”ç”¨äºåˆ†æå’Œåˆ†ç±»åŒ»å­¦å›¾åƒï¼Œä¸ºç™Œç—‡æ£€æµ‹æä¾›æ›´ç²¾ç¡®çš„æ–¹æ³•ã€‚</li>
<li>ä¸åŒç±»å‹çš„ç™Œç—‡æ£€æµ‹é‡‡ç”¨äº†å¤šç§CNNæ¶æ„ï¼Œä»¥è¯†åˆ«ä¸ç™Œç—‡ç›¸å…³çš„æ¨¡å¼ã€‚</li>
<li>CNNæ¶æ„ä¹‹é—´çš„å…³é”®å·®å¼‚å’Œä¼˜åŠ¿å¾—åˆ°äº†è¯¦ç»†æ¯”è¾ƒå’Œåˆ†æã€‚</li>
<li>CNNåœ¨ç™Œç—‡æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§å’Œå±€é™æ€§å¾—åˆ°äº†è¯„ä¼°ã€‚</li>
<li>å°†CNNé›†æˆåˆ°ä¸´åºŠç¯å¢ƒä½œä¸ºæ—©æœŸæ£€æµ‹å·¥å…·çš„å¯è¡Œæ€§å¾—åˆ°äº†æ¢è®¨ã€‚</li>
<li>å°½ç®¡æœ‰æ˜¾è‘—è¿›å±•ï¼Œä½†CNNåœ¨ç™Œç—‡æ£€æµ‹ä¸­ä»é¢ä¸´æ•°æ®å¤šæ ·æ€§ã€ç»“æœè§£è¯»å’Œä¼¦ç†è€ƒè™‘ç­‰æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-72ef3dd0ea24ed68e026d8cec60f8f2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15f075374e0886ab1ad86fe04924303b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92ff6db79bf1b300fd5e044a1ae684df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-474afd59ac5873fc4025f4a3d38ef51f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76fa7c0610006ec8410d9cfb03831c16.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="An-OpenMind-for-3D-medical-vision-self-supervised-learning"><a href="#An-OpenMind-for-3D-medical-vision-self-supervised-learning" class="headerlink" title="An OpenMind for 3D medical vision self-supervised learning"></a>An OpenMind for 3D medical vision self-supervised learning</h2><p><strong>Authors:Tassilo Wald, Constantin Ulrich, Jonathan Suprijadi, Michal Nohel, Robin Peretzke, Klaus H. Maier-Hein</strong></p>
<p>The field of 3D medical vision self-supervised learning lacks consistency and standardization. While many methods have been developed it is impossible to identify the current state-of-the-art, due to i) varying and small pre-training datasets, ii) varying architectures, and iii) being evaluated on differing downstream datasets. In this paper we bring clarity to this field and lay the foundation for further method advancements: We a) publish the largest publicly available pre-training dataset comprising 114k 3D brain MRI volumes and b) benchmark existing SSL methods under common architectures and c) provide the code of our framework publicly to facilitate rapid adoption and reproduction. This pre-print \textit{only describes} the dataset contribution (a); Data, benchmark, and codebase will be made available shortly. </p>
<blockquote>
<p>åŒ»å­¦ä¸‰ç»´è§†è§‰è‡ªç›‘ç£å­¦ä¹ é¢†åŸŸç¼ºä¹ä¸€è‡´æ€§å’Œæ ‡å‡†åŒ–ã€‚è™½ç„¶å·²ç»å¼€å‘äº†è®¸å¤šæ–¹æ³•ï¼Œä½†ç”±äºi)é¢„è®­ç»ƒæ•°æ®é›†å¤§å°ä¸ä¸€ä¸”å„å¼‚ï¼Œii)æ¶æ„å„å¼‚ï¼Œä»¥åŠiii)åœ¨ä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„è¯„ä¼°å­˜åœ¨å·®å¼‚ï¼Œå› æ­¤æ— æ³•ç¡®å®šå½“å‰çš„æœ€ä½³æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿è¯¥é¢†åŸŸæ›´åŠ æ¸…æ™°ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„æ–¹æ³•å‘å±•å¥ å®šåŸºç¡€ï¼šæˆ‘ä»¬a)å…¬å¸ƒäº†æœ€å¤§çš„å…¬å¼€é¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«11.4ä¸‡ä»½ä¸‰ç»´è„‘MRIæ•°æ®å·ï¼›b)å¯¹ç°æœ‰SSLæ–¹æ³•åœ¨é€šç”¨æ¶æ„ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼›c)å…¬å¼€æˆ‘ä»¬çš„æ¡†æ¶ä»£ç ï¼Œä»¥ä¿ƒè¿›å¿«é€Ÿé‡‡ç”¨å’Œå¤åˆ¶ã€‚è¿™ç¯‡é¢„æ‰“å°è®ºæ–‡ä»…æè¿°æ•°æ®é›†è´¡çŒ®éƒ¨åˆ†ï¼ˆaï¼‰ï¼›æ•°æ®ã€åŸºå‡†å’Œä»£ç åº“å°†åœ¨è¿‘æœŸå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17041v1">PDF</a> Pre-Print for Challenge proposal; Dataset, Benchmark and Codebase   will be made available shortly once Benchmarking concludes</p>
<p><strong>Summary</strong><br>     åŒ»å­¦ä¸‰ç»´è§†è§‰è‡ªç›‘ç£å­¦ä¹ é¢†åŸŸç¼ºä¹ä¸€è‡´æ€§å’Œæ ‡å‡†åŒ–ã€‚æœ¬æ–‡ä¸ºè§£å†³æ­¤é—®é¢˜åšå‡ºè´¡çŒ®ï¼Œå…¬å¼€äº†æœ€å¤§çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«11.4ä¸‡ä»½ä¸‰ç»´è„‘éƒ¨MRIä½“ç§¯æ•°æ®ï¼Œä¸ºç°æœ‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æä¾›äº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å…¬å¼€äº†ä»£ç æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›å¿«é€Ÿé‡‡ç”¨å’Œå¤ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦ä¸‰ç»´è§†è§‰è‡ªç›‘ç£å­¦ä¹ é¢†åŸŸç¼ºä¹ä¸€è‡´æ€§å’Œæ ‡å‡†åŒ–ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥è¯†åˆ«å½“å‰æœ€ä½³å®è·µï¼ŒåŸå› åœ¨äºé¢„è®­ç»ƒæ•°æ®é›†å¤§å°ä¸ä¸€ã€æ¶æ„å„å¼‚ã€ä¸‹æ¸¸æ•°æ®é›†è¯„ä¼°ä¸åŒã€‚</li>
<li>æœ¬æ–‡å…¬å¼€äº†æœ€å¤§çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«11.4ä¸‡ä»½ä¸‰ç»´è„‘éƒ¨MRIä½“ç§¯æ•°æ®ã€‚</li>
<li>ä¸ºç°æœ‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æä¾›äº†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å…¬å¼€äº†ä»£ç æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›å¿«é€Ÿé‡‡ç”¨å’Œå¤ç°ã€‚</li>
<li>æœ¬æ–‡ä»…æè¿°äº†æ•°æ®é›†è´¡çŒ®éƒ¨åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17041">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e315bb55aa4592c79fa0baa5cbbd459.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e452e11fe7ba3c37d26e0ed4bb939cb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5661a5ad9a8a6c1174110a51e84f6191.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2dd13189cf0818c205fa5e70fe4f4796.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2252659a14787fffd712f1a48b3e3820.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Conditional-Diffusion-Model-for-Electrical-Impedance-Tomography-Image-Reconstruction"><a href="#A-Conditional-Diffusion-Model-for-Electrical-Impedance-Tomography-Image-Reconstruction" class="headerlink" title="A Conditional Diffusion Model for Electrical Impedance Tomography Image   Reconstruction"></a>A Conditional Diffusion Model for Electrical Impedance Tomography Image   Reconstruction</h2><p><strong>Authors:Shuaikai Shi, Ruiyuan Kang, Panos Liatsis</strong></p>
<p>Electrical impedance tomography (EIT) is a non-invasive imaging technique, capable of reconstructing images of the electrical conductivity of tissues and materials. It is popular in diverse application areas, from medical imaging to industrial process monitoring and tactile sensing, due to its low cost, real-time capabilities and non-ionizing nature. EIT visualizes the conductivity distribution within a body by measuring the boundary voltages, given a current injection. However, EIT image reconstruction is ill-posed due to the mismatch between the under-sampled voltage data and the high-resolution conductivity image. A variety of approaches, both conventional and deep learning-based, have been proposed, capitalizing on the use of spatial regularizers, and the paradigm of image regression. In this research, a novel method based on the conditional diffusion model for EIT reconstruction is proposed, termed CDEIT. Specifically, CDEIT consists of the forward diffusion process, which first gradually adds Gaussian noise to the clean conductivity images, and a reverse denoising process, which learns to predict the original conductivity image from its noisy version, conditioned on the boundary voltages. Following model training, CDEIT applies the conditional reverse process on test voltage data to generate the desired conductivities. Moreover, we provide the details of a normalization procedure, which demonstrates how EIT image reconstruction models trained on simulated datasets can be applied on real datasets with varying sizes, excitation currents and background conductivities. Experiments conducted on a synthetic dataset and two real datasets demonstrate that the proposed model outperforms state-of-the-art methods. The CDEIT software is available as open-source (<a target="_blank" rel="noopener" href="https://github.com/shuaikaishi/CDEIT">https://github.com/shuaikaishi/CDEIT</a>) for reproducibility purposes. </p>
<blockquote>
<p>ç”µé˜»æŠ—æˆåƒï¼ˆEITï¼‰æ˜¯ä¸€ç§éä¾µå…¥æ€§çš„æˆåƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿé‡å»ºç»„ç»‡å’Œææ–™çš„ç”µå¯¼ç‡å›¾åƒã€‚ç”±äºå…¶æˆæœ¬ä½ã€å®æ—¶æ€§èƒ½å¼ºå’Œéç”µç¦»æ€§è´¨ï¼ŒEITåœ¨åŒ»å­¦æˆåƒã€å·¥ä¸šè¿‡ç¨‹ç›‘æ§å’Œè§¦è§‰æ„ŸçŸ¥ç­‰å¤šæ ·åŒ–åº”ç”¨é¢†åŸŸå¹¿å—æ¬¢è¿ã€‚EITé€šè¿‡æµ‹é‡ç”µæµæ³¨å…¥æ—¶çš„è¾¹ç•Œç”µå‹æ¥å¯è§†åŒ–ä½“å†…çš„ç”µå¯¼ç‡åˆ†å¸ƒã€‚ç„¶è€Œï¼Œç”±äºé‡‡æ ·ä¸è¶³çš„ç”µå‹æ•°æ®ä¸é«˜åˆ†è¾¨ç‡ç”µå¯¼ç‡å›¾åƒä¹‹é—´çš„ä¸åŒ¹é…ï¼ŒEITå›¾åƒé‡å»ºæ˜¯ä¸€ä¸ªé€‚å®šæ€§é—®é¢˜ã€‚å·²ç»æå‡ºäº†è®¸å¤šä¼ ç»Ÿå’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç©ºé—´æ­£åˆ™åŒ–å’Œå›¾åƒå›å½’èŒƒå¼ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„EITé‡å»ºæ–°æ–¹æ³•ï¼Œç§°ä¸ºCDEITã€‚å…·ä½“æ¥è¯´ï¼ŒCDEITåŒ…æ‹¬æ­£å‘æ‰©æ•£è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹é¦–å…ˆåœ¨å¹²å‡€çš„ç”µå¯¼ç‡å›¾åƒä¸Šé€æ¸æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œä»¥åŠåå‘å»å™ªè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹å­¦ä¼šæ ¹æ®è¾¹ç•Œç”µå‹é¢„æµ‹åŸå§‹ç”µå¯¼ç‡å›¾åƒã€‚æ¨¡å‹è®­ç»ƒå®Œæˆåï¼ŒCDEITå°†æ¡ä»¶åå‘è¿‡ç¨‹åº”ç”¨äºæµ‹è¯•ç”µå‹æ•°æ®ï¼Œç”Ÿæˆæ‰€éœ€çš„ç”µå¯¼ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å½’ä¸€åŒ–æµç¨‹çš„è¯¦ç»†ä¿¡æ¯ï¼Œå±•ç¤ºäº†å¦‚ä½•åœ¨ä¸åŒå¤§å°ã€æ¿€åŠ±ç”µæµå’ŒèƒŒæ™¯ç”µå¯¼ç‡çš„çœŸå®æ•°æ®é›†ä¸Šåº”ç”¨ç»è¿‡æ¨¡æ‹Ÿæ•°æ®é›†è®­ç»ƒçš„EITå›¾åƒé‡å»ºæ¨¡å‹ã€‚åœ¨åˆæˆæ•°æ®é›†å’Œä¸¤ä¸ªçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚CDEITè½¯ä»¶ä½œä¸ºå¼€æºè½¯ä»¶ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/shuaikaishi/CDEIT%EF%BC%89%E6%8F%90%E4%BE%9B%EF%BC%8C%E4%BB%A5%E4%BE%9B%E5%8F%AF%E9%87%8D%E5%A4%8D%E5%AE%9E%E9%AA%8C%E4%B9%8B%E7%94%A8%E3%80%82">https://github.com/shuaikaishi/CDEITï¼‰æä¾›ï¼Œä»¥ä¾›å¯é‡å¤å®éªŒä¹‹ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16979v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>EITï¼ˆç”µæ°”é˜»æŠ—æˆåƒæŠ€æœ¯ï¼‰æ˜¯ä¸€ç§éä¾µå…¥æ€§çš„æˆåƒæŠ€æœ¯ï¼Œèƒ½é‡å»ºç»„ç»‡å’Œææ–™çš„ç”µå¯¼ç‡å›¾åƒã€‚ç”±äºå…¶ä½æˆæœ¬ã€å®æ—¶æ€§å’Œéç”µç¦»æ€§è´¨ï¼ŒEITå¹¿æ³›åº”ç”¨äºåŒ»å­¦æˆåƒã€å·¥ä¸šè¿‡ç¨‹ç›‘æ§å’Œè§¦è§‰æ„ŸçŸ¥ç­‰é¢†åŸŸã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„EITé‡å»ºæ–°æ–¹æ³•ï¼Œç§°ä¸ºCDEITã€‚è¯¥æ–¹æ³•åŒ…æ‹¬æ­£å‘æ‰©æ•£è¿‡ç¨‹ï¼ˆå‘å¹²å‡€çš„ç”µå¯¼ç‡å›¾åƒé€æ¸æ·»åŠ é«˜æ–¯å™ªå£°ï¼‰å’Œåå‘å»å™ªè¿‡ç¨‹ï¼ˆå­¦ä¹ ä»å«å™ªç‰ˆæœ¬é¢„æµ‹åŸå§‹ç”µå¯¼ç‡å›¾åƒï¼‰ã€‚ç»è¿‡æ¨¡å‹è®­ç»ƒåï¼ŒCDEITåº”ç”¨äºæµ‹è¯•ç”µå‹æ•°æ®ç”Ÿæˆæ‰€éœ€çš„å¯¼ç”µç‡ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ç§å½’ä¸€åŒ–ç¨‹åºï¼Œè¯¥ç¨‹åºå±•ç¤ºäº†å¦‚ä½•å°†EITå›¾åƒé‡å»ºæ¨¡å‹ä»æ¨¡æ‹Ÿæ•°æ®é›†åº”ç”¨åˆ°å…·æœ‰ä¸åŒå¤§å°ã€æ¿€å‘ç”µæµå’ŒèƒŒæ™¯ç”µå¯¼ç‡çš„çœŸå®æ•°æ®é›†ä¸Šã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒCDEITè½¯ä»¶ä½œä¸ºå¼€æºè½¯ä»¶å¯ä¾›ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EITæ˜¯ä¸€ç§éä¾µå…¥æ€§çš„æˆåƒæŠ€æœ¯ï¼Œå¯é‡å»ºç»„ç»‡å’Œææ–™çš„ç”µå¯¼ç‡å›¾åƒã€‚</li>
<li>EITåœ¨åŒ»å­¦æˆåƒã€å·¥ä¸šè¿‡ç¨‹ç›‘æ§å’Œè§¦è§‰æ„ŸçŸ¥ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>CDEITæ˜¯ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„EITé‡å»ºæ–°æ–¹æ³•ã€‚</li>
<li>CDEITåŒ…æ‹¬æ­£å‘æ‰©æ•£è¿‡ç¨‹å’Œåå‘å»å™ªè¿‡ç¨‹ã€‚</li>
<li>CDEITæ¨¡å‹åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>CDEITè½¯ä»¶ä½œä¸ºå¼€æºè½¯ä»¶å¯ä¾›ä½¿ç”¨ï¼Œæœ‰åŠ©äºæé«˜ç ”ç©¶çš„å¯é‡å¤æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11ceec1b251346bed91afa1a68ac78f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-428480e4c51a239db0299f8e9b11c00c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a321e7a916d483b8e39c0317027ccfdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-630caf5b4bae6c467f788ff7ce9775d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a58b6e689deed22ade977cfb4070cd2a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PINN-EMFNet-PINN-based-and-Enhanced-Multi-Scale-Feature-Fusion-Network-for-Breast-Ultrasound-Images-Segmentation"><a href="#PINN-EMFNet-PINN-based-and-Enhanced-Multi-Scale-Feature-Fusion-Network-for-Breast-Ultrasound-Images-Segmentation" class="headerlink" title="PINN-EMFNet: PINN-based and Enhanced Multi-Scale Feature Fusion Network   for Breast Ultrasound Images Segmentation"></a>PINN-EMFNet: PINN-based and Enhanced Multi-Scale Feature Fusion Network   for Breast Ultrasound Images Segmentation</h2><p><strong>Authors:Jiajun Ding, Beiyao Zhu, Wenjie Wang, Shurong Zhang, Dian Zhua, Zhao Liua</strong></p>
<p>With the rapid development of deep learning and computer vision technologies, medical image segmentation plays a crucial role in the early diagnosis of breast cancer. However, due to the characteristics of breast ultrasound images, such as low contrast, speckle noise, and the highly diverse morphology of tumors, existing segmentation methods exhibit significant limitations in terms of accuracy and robustness. To address these challenges, this study proposes a PINN-based and Enhanced Multi-Scale Feature Fusion Network. The network introduces a Hierarchical Aggregation Encoder in the backbone, which efficiently integrates and globally models multi-scale features through several structural innovations and a novel PCAM module. In the decoder section, a Multi-Scale Feature Refinement Decoder is employed, which, combined with a Multi-Scale Supervision Mechanism and a correction module, significantly improves segmentation accuracy and adaptability. Additionally, the loss function incorporating the PINN mechanism introduces physical constraints during the segmentation process, enhancing the modelâ€™s ability to accurately delineate tumor boundaries. Comprehensive evaluations on two publicly available breast ultrasound datasets, BUSIS and BUSI, demonstrate that the proposed method outperforms previous segmentation approaches in terms of segmentation accuracy and robustness, particularly under conditions of complex noise and low contrast, effectively improving the accuracy and reliability of tumor segmentation. This method provides a more precise and robust solution for computer-aided diagnosis of breast ultrasound images. </p>
<blockquote>
<p>éšç€æ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒåŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¹³è…ºç™Œçš„æ—©æœŸè¯Šæ–­ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œç”±äºä¹³è…ºè¶…å£°å›¾åƒçš„ç‰¹æ€§ï¼Œå¦‚å¯¹æ¯”åº¦ä½ã€æ–‘ç‚¹å™ªå£°å’Œè‚¿ç˜¤å½¢æ€é«˜åº¦å¤šæ ·ï¼Œç°æœ‰çš„åˆ†å‰²æ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºPINNçš„å¢å¼ºå¤šå°ºåº¦ç‰¹å¾èåˆç½‘ç»œã€‚è¯¥ç½‘ç»œåœ¨ä¸»å¹²ä¸­å¼•å…¥åˆ†å±‚èšåˆç¼–ç å™¨ï¼Œé€šè¿‡è‹¥å¹²ç»“æ„åˆ›æ–°å’Œæ–°å‹PCAMæ¨¡å—ï¼Œæœ‰æ•ˆåœ°æ•´åˆå’Œå…¨å±€å»ºæ¨¡å¤šå°ºåº¦ç‰¹å¾ã€‚åœ¨è§£ç å™¨éƒ¨åˆ†ï¼Œé‡‡ç”¨å¤šå°ºåº¦ç‰¹å¾ç»†åŒ–è§£ç å™¨ï¼Œç»“åˆå¤šå°ºåº¦ç›‘ç£æœºåˆ¶å’Œæ ¡æ­£æ¨¡å—ï¼Œæ˜¾è‘—æé«˜åˆ†å‰²ç²¾åº¦å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°ç»“åˆPINNæœºåˆ¶ï¼Œåœ¨åˆ†å‰²è¿‡ç¨‹ä¸­å¼•å…¥ç‰©ç†çº¦æŸï¼Œå¢å¼ºäº†æ¨¡å‹å‡†ç¡®å‹¾å‹’è‚¿ç˜¤è¾¹ç•Œçš„èƒ½åŠ›ã€‚å¯¹ä¸¤ä¸ªå…¬å¼€å¯ç”¨çš„ä¹³è…ºè¶…å£°æ•°æ®é›†BUSISå’ŒBUSIçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨åˆ†å‰²å‡†ç¡®æ€§å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºå…ˆå‰çš„åˆ†å‰²æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å™ªå£°å’Œä½å¯¹æ¯”åº¦æ¡ä»¶ä¸‹ï¼Œæœ‰æ•ˆæé«˜è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚è¯¥æ–¹æ³•ä¸ºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­ä¹³è…ºè¶…å£°å›¾åƒæä¾›äº†æ›´ç²¾ç¡®å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16937v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ åŠè®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼ŒåŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¹³è…ºç™Œæ—©æœŸè¯Šæ–­ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚ä½†ç”±äºä¹³è…ºè¶…å£°å›¾åƒç‰¹ç‚¹ï¼Œå¦‚å¯¹æ¯”åº¦ä½ã€æ–‘ç‚¹å™ªå£°åŠè‚¿ç˜¤å½¢æ€å¤šæ ·æ€§ï¼Œç°æœ‰åˆ†å‰²æ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸ç¨³å¥æ€§ä¸Šå­˜åœ¨æ˜¾è‘—å±€é™ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºPINNå’Œå¢å¼ºå¤šå°ºåº¦ç‰¹å¾èåˆç½‘ç»œçš„è§£å†³æ–¹æ¡ˆã€‚ç½‘ç»œåœ¨ä¸»å¹²ä¸­å¼•å…¥åˆ†å±‚èšåˆç¼–ç å™¨ï¼Œé€šè¿‡è‹¥å¹²ç»“æ„åˆ›æ–°å’Œæ–°å‹PCAMæ¨¡å—ï¼Œæœ‰æ•ˆæ•´åˆå¹¶å…¨å±€å»ºæ¨¡å¤šå°ºåº¦ç‰¹å¾ã€‚è§£ç å™¨éƒ¨åˆ†é‡‡ç”¨å¤šå°ºåº¦ç‰¹å¾ç»†åŒ–è§£ç å™¨ï¼Œç»“åˆå¤šå°ºåº¦ç›‘ç£æœºåˆ¶å’Œæ ¡æ­£æ¨¡å—ï¼Œæ˜¾è‘—æé«˜åˆ†å‰²å‡†ç¡®æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼ŒæŸå¤±å‡½æ•°ç»“åˆPINNæœºåˆ¶ï¼Œåœ¨åˆ†å‰²è¿‡ç¨‹ä¸­å¼•å…¥ç‰©ç†çº¦æŸï¼Œå¢å¼ºæ¨¡å‹ç²¾ç¡®æç»˜è‚¿ç˜¤è¾¹ç•Œçš„èƒ½åŠ›ã€‚åœ¨å…¬å¼€å¯ç”¨çš„ä¸¤ä¸ªä¹³è…ºè¶…å£°æ•°æ®é›†BUSISå’ŒBUSIä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²å‡†ç¡®æ€§å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºå…ˆå‰çš„åˆ†å‰²æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å™ªå£°å’Œä½å¯¹æ¯”åº¦æ¡ä»¶ä¸‹ï¼Œæœ‰æ•ˆæé«˜è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ä¸ºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­ä¹³è…ºè¶…å£°å›¾åƒæä¾›æ›´ç²¾ç¡®å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¹³è…ºç™Œæ—©æœŸè¯Šæ–­ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰ä¹³è…ºè¶…å£°å›¾åƒåˆ†å‰²æ–¹æ³•é¢ä¸´å‡†ç¡®æ€§ä¸ç¨³å¥æ€§æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶æå‡ºä¸€ç§åŸºäºPINNå’Œå¢å¼ºå¤šå°ºåº¦ç‰¹å¾èåˆç½‘ç»œçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç½‘ç»œå¼•å…¥åˆ†å±‚èšåˆç¼–ç å™¨å’Œå¤šå°ºåº¦ç‰¹å¾ç»†åŒ–è§£ç å™¨ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>PINNæœºåˆ¶ç»“åˆç‰©ç†çº¦æŸæé«˜æ¨¡å‹ç²¾ç¡®æç»˜è‚¿ç˜¤è¾¹ç•Œçš„èƒ½åŠ›ã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²å‡†ç¡®æ€§å’Œç¨³å¥æ€§ä¸Šä¼˜äºå…ˆå‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9faa8e0518995a46e24db949039c86ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f4e2e85e5f13496a54a09661ee541c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2763a185660a9986e98a89699314c52.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Evaluation-of-radiomic-feature-harmonization-techniques-for-benign-and-malignant-pulmonary-nodules"><a href="#Evaluation-of-radiomic-feature-harmonization-techniques-for-benign-and-malignant-pulmonary-nodules" class="headerlink" title="Evaluation of radiomic feature harmonization techniques for benign and   malignant pulmonary nodules"></a>Evaluation of radiomic feature harmonization techniques for benign and   malignant pulmonary nodules</h2><p><strong>Authors:Claire Huchthausen, Menglin Shi, Gabriel L. A. de Sousa, Jonathan Colen, Emery Shelley, James Larner, Krishni Wijesooriya</strong></p>
<p>BACKGROUND: Radiomics provides quantitative features of pulmonary nodules (PNs) which could aid lung cancer diagnosis, but medical image acquisition variability is an obstacle to clinical application. Acquisition effects may differ between radiomic features from benign vs. malignant PNs. PURPOSE: We evaluated how to account for differences between benign and malignant PNs when correcting radiomic featuresâ€™ acquisition dependency. METHODS: We used 567 chest CT scans grouped as benign, malignant, or lung cancer screening (mixed benign, malignant). ComBat harmonization was applied to extracted features for variation in 4 acquisition parameters. We compared: harmonizing without distinction, harmonizing with a covariate to preserve distinctions between subgroups, and harmonizing subgroups separately. Significant ($p\le0.05$) Kruskal-Wallis tests showed whether harmonization removed acquisition dependency. A LASSO-SVM pipeline was trained on successfully harmonized features to predict malignancy. To evaluate predictive information in these features, the trained harmonization estimators and predictive model were applied to unseen test sets. Harmonization and predictive performance were assessed for 10 trials of 5-fold cross-validation. RESULTS: An average 2.1% of features (95% CI:1.9-2.4%) were acquisition-independent when harmonized without distinction, 27.3% (95% CI:25.7-28.9%) when harmonized with a covariate, and 90.9% (95% CI:90.4-91.5%) when harmonized separately. Data harmonized separately or with a covariate trained models with higher ROC-AUC for screening scans than data harmonized without distinction between benign and malignant PNs (Delong test, adjusted $p\le0.05$). CONCLUSIONS: Radiomic features of benign and malignant PNs need different corrective transformations to recover acquisition-independent distributions. This can be done by harmonizing separately or with a covariate. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šæ”¾å°„ç»„å­¦æä¾›äº†è‚ºç»“èŠ‚ï¼ˆPNsï¼‰çš„å®šé‡ç‰¹å¾ï¼Œè¿™æœ‰åŠ©äºè‚ºç™Œçš„è¯Šæ–­ï¼Œä½†åŒ»å­¦å›¾åƒé‡‡é›†çš„å˜å¼‚æ€§æ˜¯ä¸´åºŠåº”ç”¨ä¸­çš„éšœç¢ã€‚è‰¯æ€§å’Œæ¶æ€§è‚ºç»“èŠ‚ä¹‹é—´çš„æ”¾å°„å­¦ç‰¹å¾å¯èƒ½å­˜åœ¨é‡‡é›†æ•ˆæœä¸Šçš„å·®å¼‚ã€‚ç›®çš„ï¼šæˆ‘ä»¬è¯„ä¼°äº†å¦‚ä½•åœ¨æ ¡æ­£æ”¾å°„å­¦ç‰¹å¾çš„é‡‡é›†ä¾èµ–æ€§æ—¶è€ƒè™‘è‰¯æ€§å’Œæ¶æ€§è‚ºç»“èŠ‚ä¹‹é—´çš„å·®å¼‚ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬ä½¿ç”¨567å¼ èƒ¸éƒ¨CTæ‰«æå›¾åƒï¼ŒæŒ‰è‰¯æ€§ã€æ¶æ€§æˆ–è‚ºç™Œç­›æŸ¥ï¼ˆæ··åˆè‰¯æ€§å’Œæ¶æ€§ï¼‰è¿›è¡Œåˆ†ç»„ã€‚åº”ç”¨ComBatå’Œè°æ³•å¯¹æå–çš„ç‰¹å¾è¿›è¡Œå˜å¼‚æ ¡æ­£ï¼Œå˜å¼‚æ¶‰åŠ4ä¸ªé‡‡é›†å‚æ•°ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä»¥ä¸‹ä¸‰ç§æ–¹æ³•ï¼šæ— åŒºåˆ«çš„å’Œè°æ³•ã€ä½¿ç”¨åå˜é‡ä¿ç•™äºšç»„é—´å·®å¼‚çš„å’Œåæ³•ï¼Œä»¥åŠåˆ†åˆ«å’Œè°äºšç»„çš„æ–¹æ³•ã€‚Kruskal-Wallisæ£€éªŒï¼ˆpâ‰¤0.05ï¼‰æ˜¾ç¤ºå’Œè°åŒ–æ˜¯å¦æ¶ˆé™¤äº†é‡‡é›†ä¾èµ–æ€§ã€‚ä½¿ç”¨æˆåŠŸå’Œè°åŒ–çš„ç‰¹å¾è®­ç»ƒLASSO-SVMç®¡é“ä»¥é¢„æµ‹æ¶æ€§ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›ç‰¹å¾ä¸­çš„é¢„æµ‹ä¿¡æ¯ï¼Œå°†ç»è¿‡è®­ç»ƒçš„å’Œè°åŒ–ä¼°è®¡å™¨å’Œé¢„æµ‹æ¨¡å‹åº”ç”¨äºæœªè§è¿‡çš„æµ‹è¯•é›†ã€‚å’Œè°åŒ–å’Œé¢„æµ‹æ€§èƒ½å‡ç»è¿‡10æ¬¡5æŠ˜äº¤å‰éªŒè¯çš„è¯•éªŒè¿›è¡Œè¯„ä¼°ã€‚ç»“æœï¼šå½“æ— åŒºåˆ«å’Œè°åŒ–æ—¶ï¼Œå¹³å‡æœ‰2.1%ï¼ˆ95%ç½®ä¿¡åŒºé—´ï¼š1.9-2.4%ï¼‰çš„ç‰¹å¾ä¸é‡‡é›†æ— å…³ï¼›å½“ä½¿ç”¨åå˜é‡å’Œè°åŒ–æ—¶ï¼Œå¹³å‡æœ‰27.3%ï¼ˆ95%ç½®ä¿¡åŒºé—´ï¼š25.7-28.9%ï¼‰çš„ç‰¹å¾ä¸é‡‡é›†æ— å…³ï¼›å½“åˆ†åˆ«å’Œè°åŒ–æ—¶ï¼Œå¹³å‡æœ‰90.9%ï¼ˆ95%ç½®ä¿¡åŒºé—´ï¼š90.4-91.5%ï¼‰çš„ç‰¹å¾ä¸é‡‡é›†æ— å…³ã€‚å¯¹äºç­›æŸ¥æ‰«æï¼Œä¸åœ¨è‰¯æ€§å’Œæ¶æ€§è‚ºç»“èŠ‚ä¹‹é—´æ— åŒºåˆ«åœ°å’Œè°åŒ–æ•°æ®ç›¸æ¯”ï¼Œåˆ†åˆ«å’Œè°åŒ–æˆ–ä¸åå˜é‡ä¸€èµ·å’Œè°åŒ–çš„æ•°æ®è®­ç»ƒæ¨¡å‹çš„ROC-AUCæ›´é«˜ï¼ˆç»è°ƒæ•´çš„Delongæ£€éªŒï¼Œpâ‰¤0.05ï¼‰ã€‚ç»“è®ºï¼šè‰¯æ€§å’Œæ¶æ€§è‚ºç»“èŠ‚çš„æ”¾å°„å­¦ç‰¹å¾éœ€è¦ä¸åŒçš„æ ¡æ­£è½¬æ¢æ¥æ¢å¤ä¸é‡‡é›†æ— å…³çš„åˆ†å¸ƒã€‚è¿™å¯ä»¥é€šè¿‡åˆ†åˆ«å’Œè°åŒ–æˆ–ä½¿ç”¨åå˜é‡çš„æ–¹æ³•æ¥å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16758v1">PDF</a> 15 pages, 3 figures, plus supplemental material</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ç ”ç©¶äº†æ”¾å°„ç»„å­¦ç‰¹å¾åœ¨è‚ºç»“èŠ‚è¯Šæ–­ä¸­çš„åº”ç”¨ï¼Œæ¢è®¨äº†å¦‚ä½•æ ¡æ­£æ”¾å°„ç»„å­¦ç‰¹å¾é‡‡é›†è¿‡ç¨‹ä¸­çš„å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è‰¯æ¶æ€§è‚ºç»“èŠ‚ä¹‹é—´çš„å·®å¼‚ã€‚é€šè¿‡åº”ç”¨ComBatå’Œè°åŒ–æŠ€æœ¯ï¼Œç ”ç©¶å‘ç°åˆ†åˆ«å¯¹è‰¯æ¶æ€§è‚ºç»“èŠ‚è¿›è¡Œå’Œè°åŒ–å¤„ç†æˆ–ç”¨ä¸€ä¸ªåå˜é‡è¿›è¡Œå’Œè°åŒ–å¤„ç†å¯ä»¥æ›´å¥½åœ°å»é™¤é‡‡é›†å‚æ•°çš„å·®å¼‚ï¼Œæé«˜é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¾å°„ç»„å­¦ç‰¹å¾å¯¹äºè‚ºç»“èŠ‚çš„è‰¯æ¶æ€§è¯Šæ–­æœ‰é‡è¦ä½œç”¨ï¼Œä½†åŒ»å­¦å›¾åƒé‡‡é›†è¿‡ç¨‹ä¸­çš„å·®å¼‚æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨ComBatå’Œè°åŒ–æŠ€æœ¯å¯ä»¥æ¶ˆé™¤æ”¾å°„ç»„å­¦ç‰¹å¾é‡‡é›†è¿‡ç¨‹ä¸­çš„å·®å¼‚ã€‚</li>
<li>è‰¯æ¶æ€§è‚ºç»“èŠ‚çš„æ”¾å°„ç»„å­¦ç‰¹å¾éœ€è¦ä¸åŒçš„æ ¡æ­£è½¬æ¢ä»¥æ¢å¤ç‹¬ç«‹äºé‡‡é›†çš„åˆ†å¸ƒã€‚</li>
<li>åˆ†åˆ«å’Œè°åŒ–è‰¯æ¶æ€§è‚ºç»“èŠ‚æ•°æ®æˆ–ä½¿ç”¨åå˜é‡å¯ä»¥æé«˜é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ–‡ä¸­æåˆ°äº†ä¸‰ç§å’Œè°åŒ–æ–¹æ³•ï¼šæ— åŒºåˆ«çš„å’Œè°åŒ–ã€ä½¿ç”¨åå˜é‡çš„å’Œè°åŒ–ã€ä»¥åŠåˆ†åˆ«å’Œè°åŒ–ä¸åŒå­ç¾¤ä½“ã€‚</li>
<li>é€šè¿‡å¯¹æ¨¡å‹è¿›è¡Œ10æ¬¡5å€äº¤å‰éªŒè¯ï¼Œå‘ç°ä½¿ç”¨åˆ†åˆ«å’Œè°åŒ–æˆ–å¸¦æœ‰åå˜é‡çš„å’Œè°åŒ–æ–¹æ³•å¯ä»¥æ›´å¥½åœ°å»é™¤é‡‡é›†ä¾èµ–æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-832b38d01719c39e5a94ecdb3d4761b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11a290ffb49d344fd032b674b03f4fda.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Solving-Inverse-Problems-via-Diffusion-Optimal-Control"><a href="#Solving-Inverse-Problems-via-Diffusion-Optimal-Control" class="headerlink" title="Solving Inverse Problems via Diffusion Optimal Control"></a>Solving Inverse Problems via Diffusion Optimal Control</h2><p><strong>Authors:Henry Li, Marcus Pereira</strong></p>
<p>Existing approaches to diffusion-based inverse problem solvers frame the signal recovery task as a probabilistic sampling episode, where the solution is drawn from the desired posterior distribution. This framework suffers from several critical drawbacks, including the intractability of the conditional likelihood function, strict dependence on the score network approximation, and poor $\mathbf{x}_0$ prediction quality. We demonstrate that these limitations can be sidestepped by reframing the generative process as a discrete optimal control episode. We derive a diffusion-based optimal controller inspired by the iterative Linear Quadratic Regulator (iLQR) algorithm. This framework is fully general and able to handle any differentiable forward measurement operator, including super-resolution, inpainting, Gaussian deblurring, nonlinear deblurring, and even highly nonlinear neural classifiers. Furthermore, we show that the idealized posterior sampling equation can be recovered as a special case of our algorithm. We then evaluate our method against a selection of neural inverse problem solvers, and establish a new baseline in image reconstruction with inverse problems. </p>
<blockquote>
<p>ç°æœ‰çš„åŸºäºæ‰©æ•£çš„é€†é—®é¢˜æ±‚è§£æ–¹æ³•å°†ä¿¡å·æ¢å¤ä»»åŠ¡æ„å»ºä¸ºæ¦‚ç‡é‡‡æ ·äº‹ä»¶ï¼Œå…¶ä¸­è§£æ˜¯ä»æœŸæœ›çš„åéªŒåˆ†å¸ƒä¸­å¾—å‡ºçš„ã€‚è¿™ä¸€æ¡†æ¶å­˜åœ¨å‡ ä¸ªå…³é”®ç¼ºç‚¹ï¼ŒåŒ…æ‹¬æ¡ä»¶ä¼¼ç„¶å‡½æ•°çš„ä¸æ˜“å¤„ç†æ€§ã€å¯¹è¯„åˆ†ç½‘ç»œè¿‘ä¼¼çš„ä¸¥æ ¼ä¾èµ–æ€§ä»¥åŠç³Ÿç³•çš„$\mathbf{x}_0$é¢„æµ‹è´¨é‡ã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡é‡æ–°æ„å»ºç”Ÿæˆè¿‡ç¨‹ä½œä¸ºç¦»æ•£æœ€ä¼˜æ§åˆ¶äº‹ä»¶ï¼Œå¯ä»¥è§„é¿è¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬å—åˆ°è¿­ä»£çº¿æ€§äºŒæ¬¡è°ƒèŠ‚å™¨ï¼ˆiLQRï¼‰ç®—æ³•çš„å¯å‘ï¼Œæ¨å¯¼å‡ºäº†åŸºäºæ‰©æ•£çš„æœ€ä¼˜æ§åˆ¶å™¨ã€‚è¿™ä¸€æ¡†æ¶é€šç”¨æ€§åè¶³ï¼Œèƒ½å¤Ÿå¤„ç†ä»»ä½•å¯åŒºåˆ†çš„æ­£å‘æµ‹é‡ç®—å­ï¼ŒåŒ…æ‹¬è¶…åˆ†è¾¨ç‡ã€å›¾åƒè¡¥å…¨ã€é«˜æ–¯å»æ¨¡ç³Šã€éçº¿æ€§å»æ¨¡ç³Šï¼Œç”šè‡³æ˜¯é«˜åº¦éçº¿æ€§çš„ç¥ç»ç½‘ç»œåˆ†ç±»å™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜ç†æƒ³åŒ–çš„åéªŒé‡‡æ ·æ–¹ç¨‹å¯ä»¥ä½œä¸ºæˆ‘ä»¬ç®—æ³•çš„ä¸€ä¸ªç‰¹ä¾‹è¿›è¡Œæ¢å¤ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹ä¸€ç³»åˆ—ç¥ç»é€†é—®é¢˜æ±‚è§£å™¨è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åœ¨é€†é—®é¢˜å›¾åƒé‡å»ºä¸­å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16748v1">PDF</a> Presented at NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£çš„é€†é—®é¢˜æ±‚è§£å™¨çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ç”Ÿæˆè¿‡ç¨‹é‡æ–°æ„å»ºä¸ºç¦»æ•£æœ€ä¼˜æ§åˆ¶äº‹ä»¶ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚æ¡ä»¶æ¦‚ç‡å‡½æ•°çš„ä¸å¯è®¡ç®—æ€§ã€å¯¹è¯„åˆ†ç½‘ç»œè¿‘ä¼¼çš„ä¸¥æ ¼ä¾èµ–ä»¥åŠé¢„æµ‹è´¨é‡ä¸ä½³ç­‰é—®é¢˜ã€‚æ–°æ–¹æ³•é‡‡ç”¨åŸºäºè¿­ä»£çº¿æ€§äºŒæ¬¡è°ƒèŠ‚å™¨ï¼ˆiLQRï¼‰ç®—æ³•çš„æœ€ä¼˜æ§åˆ¶å™¨ï¼Œå¯å¤„ç†ä»»ä½•å¯å¾®çš„å‰å‘æµ‹é‡ç®—å­ï¼ŒåŒ…æ‹¬è¶…åˆ†è¾¨ç‡ã€å›¾åƒè¡¥å…¨ã€é«˜æ–¯å»æ¨¡ç³Šã€éçº¿æ€§å»æ¨¡ç³Šç”šè‡³é«˜åº¦éçº¿æ€§çš„ç¥ç»ç½‘ç»œåˆ†ç±»å™¨ã€‚åœ¨è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•å»ºç«‹äº†æ–°çš„å›¾åƒé‡å»ºé€†é—®é¢˜åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ‰©æ•£é€†é—®é¢˜æ±‚è§£æ–¹æ³•å°†ä¿¡å·æ¢å¤ä»»åŠ¡è§†ä¸ºæ¦‚ç‡é‡‡æ ·äº‹ä»¶ï¼Œä»æœŸæœ›çš„åéªŒåˆ†å¸ƒä¸­æŠ½å–è§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¿™ç§æ–¹æ³•å­˜åœ¨å…³é”®ç¼ºç‚¹ï¼Œå¦‚æ¡ä»¶æ¦‚ç‡å‡½æ•°ä¸å¯è®¡ç®—ã€ä¾èµ–è¯„åˆ†ç½‘ç»œè¿‘ä¼¼å’Œé¢„æµ‹è´¨é‡ä¸ä½³ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å°†ç”Ÿæˆè¿‡ç¨‹é‡æ–°æ„å»ºä¸ºç¦»æ•£æœ€ä¼˜æ§åˆ¶äº‹ä»¶æ¥è§£å†³è¿™äº›é™åˆ¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè¿­ä»£çº¿æ€§äºŒæ¬¡è°ƒèŠ‚å™¨ï¼ˆiLQRï¼‰ç®—æ³•çš„æœ€ä¼˜æ§åˆ¶å™¨ã€‚</li>
<li>è¯¥æ–¹æ³•å¯å¤„ç†å„ç§å¯å¾®çš„å‰å‘æµ‹é‡ç®—å­ï¼ŒåŒ…æ‹¬è¶…åˆ†è¾¨ç‡ã€å›¾åƒè¡¥å…¨ã€å»æ¨¡ç³Šå’Œç¥ç»ç½‘ç»œåˆ†ç±»å™¨ã€‚</li>
<li>ç†æƒ³åŒ–åéªŒé‡‡æ ·æ–¹ç¨‹å¯ä½œä¸ºè¯¥ç®—æ³•çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3dcd9c3e79877fc504f287ba67e6b550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08963dcea3a0451ce11909ac576f6f22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef1872bbf028d3fce8419edf2a2d7d3e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09733d35b01226f227fe8ceb7b8b3210.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65a97deec2a3689cb2c675392450487e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="PB-UAP-Hybrid-Universal-Adversarial-Attack-For-Image-Segmentation"><a href="#PB-UAP-Hybrid-Universal-Adversarial-Attack-For-Image-Segmentation" class="headerlink" title="PB-UAP: Hybrid Universal Adversarial Attack For Image Segmentation"></a>PB-UAP: Hybrid Universal Adversarial Attack For Image Segmentation</h2><p><strong>Authors:Yufei Song, Ziqi Zhou, Minghui Li, Xianlong Wang, Menghao Deng, Wei Wan, Shengshan Hu, Leo Yu Zhang</strong></p>
<p>With the rapid advancement of deep learning, the model robustness has become a significant research hotspot, \ie, adversarial attacks on deep neural networks. Existing works primarily focus on image classification tasks, aiming to alter the modelâ€™s predicted labels. Due to the output complexity and deeper network architectures, research on adversarial examples for segmentation models is still limited, particularly for universal adversarial perturbations. In this paper, we propose a novel universal adversarial attack method designed for segmentation models, which includes dual feature separation and low-frequency scattering modules. The two modules guide the training of adversarial examples in the pixel and frequency space, respectively. Experiments demonstrate that our method achieves high attack success rates surpassing the state-of-the-art methods, and exhibits strong transferability across different models. </p>
<blockquote>
<p>éšç€æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•ï¼Œæ¨¡å‹çš„ç¨³å¥æ€§å·²æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶çƒ­ç‚¹ï¼Œå³å¯¹æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œå¯¹æŠ—æ€§æ”»å‡»ã€‚ç°æœ‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼Œæ—¨åœ¨æ”¹å˜æ¨¡å‹çš„é¢„æµ‹æ ‡ç­¾ã€‚ç”±äºè¾“å‡ºå¤æ‚æ€§å’Œæ›´æ·±çš„ç½‘ç»œæ¶æ„ï¼Œå…³äºåˆ†å‰²æ¨¡å‹çš„å¯¹æŠ—æ€§ç¤ºä¾‹çš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯å¯¹é€šç”¨å¯¹æŠ—æ€§æ‰°åŠ¨çš„ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹åˆ†å‰²æ¨¡å‹çš„æ–°å‹é€šç”¨å¯¹æŠ—æ€§æ”»å‡»æ–¹æ³•ï¼ŒåŒ…æ‹¬åŒç‰¹å¾åˆ†ç¦»å’Œä½é¢‘æ•£å°„æ¨¡å—ã€‚è¿™ä¸¤ä¸ªæ¨¡å—åˆ†åˆ«åœ¨åƒç´ å’Œé¢‘ç‡ç©ºé—´æŒ‡å¯¼å¯¹æŠ—æ€§ç¤ºä¾‹çš„è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†è¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ä¸åŒæ¨¡å‹ä¹‹é—´è¡¨ç°å‡ºå¼ºå¤§çš„è¿ç§»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16651v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€æ·±åº¦å­¦ä¹ çš„é«˜é€Ÿå‘å±•ï¼Œæ¨¡å‹ç¨³å¥æ€§å·²æˆä¸ºç ”ç©¶çƒ­ç‚¹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹æŠ—æ€§æ”»å‡»æ–¹é¢çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚ç›®å‰ï¼Œå¤§éƒ¨åˆ†ç ”ç©¶å…³æ³¨å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œè‡´åŠ›äºæ”¹å˜æ¨¡å‹é¢„æµ‹æ ‡ç­¾ã€‚ç”±äºè¾“å‡ºå¤æ‚å’Œç½‘ç»œæ¶æ„è¾ƒæ·±ï¼Œé’ˆå¯¹åˆ†å‰²æ¨¡å‹çš„å¯¹æŠ—æ€§ä¾‹å­ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯é€šç”¨å¯¹æŠ—æ€§æ‰°åŠ¨ã€‚æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹åˆ†å‰²æ¨¡å‹çš„æ–°å‹é€šç”¨å¯¹æŠ—æ€§æ”»å‡»æ–¹æ³•ï¼ŒåŒ…æ‹¬åŒç‰¹å¾åˆ†ç¦»å’Œä½é¢‘æ•£å°„æ¨¡å—ã€‚è¿™ä¸¤ä¸ªæ¨¡å—åˆ†åˆ«åœ¨åƒç´ å’Œé¢‘ç‡ç©ºé—´æŒ‡å¯¼å¯¹æŠ—æ€§ç¤ºä¾‹çš„è®­ç»ƒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ”»å‡»æˆåŠŸç‡é«˜è¶…ç°æœ‰æ–¹æ³•ï¼Œä¸”åœ¨ä¸åŒæ¨¡å‹é—´å±•ç°å‡ºå¼ºå¤§çš„è¿ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹ç¨³å¥æ€§æˆä¸ºç ”ç©¶çƒ­ç‚¹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹æŠ—æ€§æ”»å‡»æ–¹é¢ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨å›¾åƒåˆ†ç±»ä»»åŠ¡çš„å¯¹æŠ—æ€§æ”»å‡»ï¼Œæ”¹å˜æ¨¡å‹é¢„æµ‹æ ‡ç­¾ã€‚</li>
<li>åˆ†å‰²æ¨¡å‹çš„å¯¹æŠ—æ€§ä¾‹å­ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œå°¤å…¶æ˜¯é€šç”¨å¯¹æŠ—æ€§æ‰°åŠ¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹é€šç”¨å¯¹æŠ—æ€§æ”»å‡»æ–¹æ³•ï¼Œé€‚ç”¨äºåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•åŒ…æ‹¬åŒç‰¹å¾åˆ†ç¦»å’Œä½é¢‘æ•£å°„ä¸¤ä¸ªæ¨¡å—ï¼Œåˆ†åˆ«åœ¨åƒç´ å’Œé¢‘ç‡ç©ºé—´æŒ‡å¯¼è®­ç»ƒã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ”»å‡»æˆåŠŸç‡é«˜è¶…å‡ºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16651">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7684edbe5def7a789ae7c9925dca8646.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-666a2e364bdc3d82cbe2ef77953ce70b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0fa161313acebbfb90cfd30e15094f0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Optimal-dosing-of-anti-cancer-treatment-under-drug-induced-plasticity"><a href="#Optimal-dosing-of-anti-cancer-treatment-under-drug-induced-plasticity" class="headerlink" title="Optimal dosing of anti-cancer treatment under drug-induced plasticity"></a>Optimal dosing of anti-cancer treatment under drug-induced plasticity</h2><p><strong>Authors:Einar Bjarki Gunnarsson, Benedikt Vilji MagnÃºsson, Jasmine Foo</strong></p>
<p>While cancer has traditionally been considered a genetic disease, mounting evidence indicates an important role for non-genetic (epigenetic) mechanisms. Common anti-cancer drugs have recently been observed to induce the adoption of reversible drug-tolerant cell states, thereby accelerating the evolution of drug resistance. Determining how to optimally balance the competing goals of killing the tumor bulk and delaying resistance evolution in this scenario is a nontrivial question of high clinical importance. In this work, we use a combined mathematical and computational approach to study optimal dosing of anti-cancer drug treatment under drug-induced cell plasticity. Our results show that the optimal treatment steers the tumor into a fixed equilibrium composition while balancing the trade-off between cell kill and tolerance induction in a precisely quantifiable way. Under linear induction of tolerance, a low-dose constant strategy is optimal in equilibrium, while under uniform induction of tolerance, alternating between a large dose and no dose is best. The directionality of drug induction, whether the drug elevates transitions from sensitivity to tolerance or inhibits transitions back, significantly affects optimal dosing. To demonstrate the applicability of our approach, we use it to identify an optimal low-dose strategy for colorectal cancer using publicly available in vitro data. </p>
<blockquote>
<p>è™½ç„¶ç™Œç—‡å†æ¥è¢«è§†ä¸ºä¸€ç§é—ä¼ æ€§ç–¾ç—…ï¼Œä½†è¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜éé—ä¼ ï¼ˆè¡¨è§‚é—ä¼ ï¼‰æœºåˆ¶ä¹Ÿå‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æœ€è¿‘è§‚å¯Ÿåˆ°å¸¸ç”¨çš„æŠ—ç™Œè¯ç‰©ä¼šè¯±å¯¼äº§ç”Ÿå¯é€†çš„è€è¯ç»†èƒçŠ¶æ€ï¼Œä»è€ŒåŠ é€Ÿè¯ç‰©æŠµæŠ—æ€§çš„è¿›åŒ–ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚ä½•åœ¨æ€æ­»è‚¿ç˜¤ä¸»ä½“å’Œå»¶ç¼“æŠµæŠ—æ€§è¿›åŒ–è¿™ä¸¤ä¸ªç›¸äº’ç«äº‰çš„ç›®æ ‡ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡ï¼Œæ˜¯ä¸€ä¸ªå…·æœ‰é‡è¦ä¸´åºŠæ„ä¹‰çš„éç®€å•é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨æ•°å­¦å’Œè®¡ç®—ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œç ”ç©¶è¯ç‰©è¯±å¯¼çš„ç»†èƒå¯å¡‘æ€§ä¸‹çš„æŠ—ç™Œè¯ç‰©æ²»ç–—çš„æœ€ä½³å‰‚é‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæœ€ä½³æ²»ç–—ç­–ç•¥å°†è‚¿ç˜¤å¼•å¯¼åˆ°ä¸€ä¸ªå›ºå®šçš„å¹³è¡¡çŠ¶æ€ï¼ŒåŒæ—¶ä»¥å¯é‡åŒ–çš„æ–¹å¼å¹³è¡¡ç»†èƒæ€ä¼¤å’Œè¯±å¯¼è€å—ä¹‹é—´çš„æƒè¡¡ã€‚åœ¨è€å—æ€§çš„çº¿æ€§è¯±å¯¼ä¸‹ï¼Œåœ¨å¹³è¡¡çŠ¶æ€ä¸‹ï¼Œä½å‰‚é‡æ’å®šç­–ç•¥æ˜¯æœ€ä¼˜çš„ï¼›è€Œåœ¨å‡åŒ€è¯±å¯¼è€å—æ€§ä¸‹ï¼Œå¤§å‰‚é‡ä¸æ— å‰‚é‡ä¹‹é—´çš„äº¤æ›¿äº¤æ›¿æ•ˆæœæœ€å¥½ã€‚è¯ç‰©è¯±å¯¼çš„æ–¹å‘æ€§ï¼Œå³è¯ç‰©æ˜¯ä¿ƒä½¿ä»æ•æ„ŸçŠ¶æ€å‘è€å—çŠ¶æ€çš„è½¬å˜è¿˜æ˜¯æŠ‘åˆ¶åå‘è½¬å˜ï¼Œéƒ½ä¼šæ˜¾è‘—å½±å“æœ€ä½³å‰‚é‡ã€‚ä¸ºäº†è¯æ˜æˆ‘ä»¬æ–¹æ³•çš„åº”ç”¨æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨å®ƒæ¥æ ¹æ®å…¬å¼€çš„ä½“å¤–æ•°æ®ä¸ºç»“è‚ ç™Œç¡®å®šæœ€ä½³ä½å‰‚é‡ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16391v1">PDF</a> 46 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†ä¸€ç§æ–°çš„æŠ—ç™Œè¯ç‰©æ²»ç–—æ–¹å¼çš„ç ”ç©¶ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„æŠ—ç™Œè¯ç‰©å¯èƒ½ä¼šè¯±å¯¼è¯ç‰©è€å—æ€§ç»†èƒçŠ¶æ€çš„å‡ºç°ï¼Œä»è€ŒåŠ é€Ÿè¯ç‰©æŠµæŠ—çš„æ¼”åŒ–ã€‚æœ¬ç ”ç©¶é‡‡ç”¨æ•°å­¦ä¸è®¡ç®—ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œç ”ç©¶äº†è¯ç‰©è¯±å¯¼çš„ç»†èƒå¯å¡‘æ€§ä¸‹çš„æœ€ä¼˜æŠ—ç™Œè¯ç‰©å‰‚é‡ã€‚ç»“æœè¡¨æ˜ï¼Œæœ€ä¼˜æ²»ç–—æ–¹æ¡ˆèƒ½å¤Ÿä½¿è‚¿ç˜¤è¾¾åˆ°å›ºå®šçš„å¹³è¡¡çŠ¶æ€ï¼Œåœ¨ç»†èƒæ€ä¼¤å’Œè€å—è¯±å¯¼ä¹‹é—´å–å¾—ç²¾ç¡®çš„å¯é‡åŒ–çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç™Œç—‡ä¸ä»…ä»…æ˜¯ä¸€ç§é—ä¼ æ€§ç–¾ç—…ï¼Œéé—ä¼ ï¼ˆè¡¨è§‚é—ä¼ ï¼‰æœºåˆ¶ä¹Ÿèµ·ç€é‡è¦ä½œç”¨ã€‚</li>
<li>å¸¸è§æŠ—ç™Œè¯ç‰©ä¼šè¯±å¯¼å¯é€†çš„è¯ç‰©è€å—æ€§ç»†èƒçŠ¶æ€ï¼Œä»è€ŒåŠ é€Ÿè¯ç‰©æŠµæŠ—çš„æ¼”åŒ–ã€‚</li>
<li>å¹³è¡¡æ€æ­»è‚¿ç˜¤ä¸»ä½“å’Œå»¶ç¼“æŠµæŠ—æ¼”åŒ–çš„ç›®æ ‡æ˜¯ä¸€ä¸ªé‡è¦çš„ä¸´åºŠé—®é¢˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ•°å­¦å’Œè®¡ç®—ç»“åˆçš„æ–¹æ³•ï¼Œç ”ç©¶äº†è¯ç‰©è¯±å¯¼çš„ç»†èƒå¯å¡‘æ€§ä¸‹çš„æœ€ä¼˜æŠ—ç™Œè¯ç‰©å‰‚é‡ã€‚</li>
<li>æœ€ä¼˜æ²»ç–—æ–¹æ¡ˆèƒ½ä½¿è‚¿ç˜¤è¾¾åˆ°å›ºå®šå¹³è¡¡çŠ¶æ€ï¼Œåœ¨ç»†èƒæ€ä¼¤å’Œè€å—è¯±å¯¼ä¹‹é—´å–å¾—ç²¾ç¡®å¹³è¡¡ã€‚</li>
<li>è¯ç‰©è¯±å¯¼çš„æ–¹å‘æ€§ï¼Œå³è¯ç‰©æ˜¯å¦æé«˜äº†ä»æ•æ„ŸçŠ¶æ€åˆ°è€å—çŠ¶æ€çš„è¿‡æ¸¡ï¼Œæˆ–æ˜¯å¦æŠ‘åˆ¶äº†ä»è€å—çŠ¶æ€å›åˆ°æ•æ„ŸçŠ¶æ€çš„è¿‡æ¸¡ï¼Œä¼šæ˜¾è‘—å½±å“æœ€ä¼˜è¯ç‰©å‰‚é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a0f2105c7970ebfcfa2547a2142812d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b91e7151e533a1204563fc38226eb5d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0671537ef43c92af7aaa1c5b8a92388.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="VerSe-Integrating-Multiple-Queries-as-Prompts-for-Versatile-Cardiac-MRI-Segmentation"><a href="#VerSe-Integrating-Multiple-Queries-as-Prompts-for-Versatile-Cardiac-MRI-Segmentation" class="headerlink" title="VerSe: Integrating Multiple Queries as Prompts for Versatile Cardiac MRI   Segmentation"></a>VerSe: Integrating Multiple Queries as Prompts for Versatile Cardiac MRI   Segmentation</h2><p><strong>Authors:Bangwei Guo, Meng Ye, Yunhe Gao, Bingyu Xin, Leon Axel, Dimitris Metaxas</strong></p>
<p>Despite the advances in learning-based image segmentation approach, the accurate segmentation of cardiac structures from magnetic resonance imaging (MRI) remains a critical challenge. While existing automatic segmentation methods have shown promise, they still require extensive manual corrections of the segmentation results by human experts, particularly in complex regions such as the basal and apical parts of the heart. Recent efforts have been made on developing interactive image segmentation methods that enable human-in-the-loop learning. However, they are semi-automatic and inefficient, due to their reliance on click-based prompts, especially for 3D cardiac MRI volumes. To address these limitations, we propose VerSe, a Versatile Segmentation framework to unify automatic and interactive segmentation through mutiple queries. Our key innovation lies in the joint learning of object and click queries as prompts for a shared segmentation backbone. VerSe supports both fully automatic segmentation, through object queries, and interactive mask refinement, by providing click queries when needed. With the proposed integrated prompting scheme, VerSe demonstrates significant improvement in performance and efficiency over existing methods, on both cardiac MRI and out-of-distribution medical imaging datasets. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bangwayne/Verse">https://github.com/bangwayne/Verse</a>. </p>
<blockquote>
<p>å°½ç®¡åŸºäºå­¦ä¹ çš„å›¾åƒåˆ†å‰²æ–¹æ³•å–å¾—äº†è¿›å±•ï¼Œä½†ä»ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å‡†ç¡®åˆ†å‰²å¿ƒè„ç»“æ„ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚è™½ç„¶ç°æœ‰çš„è‡ªåŠ¨åˆ†å‰²æ–¹æ³•æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„å‰æ™¯ï¼Œä½†å®ƒä»¬ä»ç„¶éœ€è¦ä¸“å®¶å¯¹åˆ†å‰²ç»“æœè¿›è¡Œå¤§é‡æ‰‹åŠ¨ä¿®æ­£ï¼Œç‰¹åˆ«æ˜¯åœ¨å¿ƒè„åº•éƒ¨å’Œé¡¶éƒ¨ç­‰å¤æ‚åŒºåŸŸã€‚è¿‘æœŸå·²ç»å¼€å‘å‡ºäº¤äº’å¼å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œå®ç°äº†äººç±»ç¯å†…å­¦ä¹ ã€‚ç„¶è€Œï¼Œå®ƒä»¬æ˜¯åŠè‡ªåŠ¨çš„ä¸”æ•ˆç‡ä¸é«˜ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºåŸºäºç‚¹å‡»çš„æç¤ºï¼Œç‰¹åˆ«æ˜¯å¯¹äº3Då¿ƒè„MRIä½“ç§¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†VerSeï¼Œä¸€ä¸ªé€šç”¨åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡å¤šä¸ªæŸ¥è¯¢ç»Ÿä¸€è‡ªåŠ¨å’Œäº¤äº’å¼åˆ†å‰²ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°åœ¨äºè”åˆå­¦ä¹ å¯¹è±¡å’Œç‚¹å‡»æŸ¥è¯¢ï¼Œä½œä¸ºå…±äº«åˆ†å‰²éª¨å¹²çš„æç¤ºã€‚VerSeæ—¢æ”¯æŒé€šè¿‡å¯¹è±¡æŸ¥è¯¢è¿›è¡Œå®Œå…¨è‡ªåŠ¨åˆ†å‰²ï¼Œä¹Ÿæ”¯æŒåœ¨éœ€è¦æ—¶é€šè¿‡ç‚¹å‡»æŸ¥è¯¢æä¾›äº¤äº’å¼è’™ç‰ˆç»†åŒ–ã€‚é€šè¿‡æå‡ºçš„é›†æˆæç¤ºæ–¹æ¡ˆï¼ŒVerSeåœ¨å¿ƒè„MRIå’Œç¦»ç¾¤åˆ†å¸ƒçš„åŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œæ€§èƒ½å’Œæ•ˆç‡å‡å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/bangwayne/Verse%E3%80%82">https://github.com/bangwayne/Verseã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16381v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¿ƒè„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„ç²¾å‡†åˆ†å‰²ä»æ˜¯å­¦ä¹ åŸºç¡€çš„å›¾åƒåˆ†å‰²æ–¹æ³•çš„ä¸€å¤§æŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰è‡ªåŠ¨åˆ†å‰²æ–¹æ³•å…·æœ‰æ½œåŠ›ï¼Œä½†ä»éœ€ä¸“å®¶å¯¹åˆ†å‰²ç»“æœè¿›è¡Œå¤§é‡æ‰‹åŠ¨ä¿®æ­£ï¼Œç‰¹åˆ«æ˜¯åœ¨å¿ƒè„åŸºéƒ¨å’Œé¡¶éƒ¨ç­‰å¤æ‚åŒºåŸŸã€‚ä¸ºæ”¹å–„æ­¤æƒ…å†µï¼Œç ”ç©¶è€…æå‡ºäº†VerSeï¼ˆVersatile Segmentationï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¤šæŸ¥è¯¢ç»Ÿä¸€è‡ªåŠ¨å’Œäº¤äº’å¼åˆ†å‰²ã€‚å…¶å…³é”®åˆ›æ–°åœ¨äºå¯¹è±¡æŸ¥è¯¢å’Œç‚¹å‡»æŸ¥è¯¢çš„è”åˆå­¦ä¹ ï¼Œä¸ºå…±äº«åˆ†å‰²éª¨å¹²æä¾›æç¤ºã€‚VerSeæ—¢æ”¯æŒå®Œå…¨è‡ªåŠ¨åˆ†å‰²ï¼ˆé€šè¿‡å¯¹è±¡æŸ¥è¯¢ï¼‰ï¼Œåˆæ”¯æŒåœ¨éœ€è¦æ—¶é€šè¿‡ç‚¹å‡»æŸ¥è¯¢è¿›è¡Œäº¤äº’å¼æ©è†œç»†åŒ–ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVerSeåœ¨å¿ƒè„MRIå’Œç¦»æ•£åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„æ€§èƒ½å’Œæ•ˆç‡å‡æœ‰æ˜¾è‘—æé«˜ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/bangwayne/Verse%E3%80%82">https://github.com/bangwayne/Verseã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¿ƒè„MRIçš„ç²¾å‡†åˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œéœ€è¦å¼€å‘æ›´é«˜æ•ˆå’Œå‡†ç¡®çš„æ–¹æ³•ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨åˆ†å‰²æ–¹æ³•åœ¨å¤æ‚åŒºåŸŸå¦‚å¿ƒè„åŸºéƒ¨å’Œé¡¶éƒ¨ä»éœ€è¦æ‰‹åŠ¨ä¿®æ­£ã€‚</li>
<li>VerSeæ¡†æ¶ç»“åˆäº†è‡ªåŠ¨å’Œäº¤äº’å¼åˆ†å‰²ï¼Œé€šè¿‡å¤šæŸ¥è¯¢è¿›è¡Œç»Ÿä¸€å¤„ç†ã€‚</li>
<li>VerSeçš„å…³é”®åˆ›æ–°åœ¨äºè”åˆå­¦ä¹ å¯¹è±¡æŸ¥è¯¢å’Œç‚¹å‡»æŸ¥è¯¢ï¼Œä¸ºåˆ†å‰²æä¾›æç¤ºã€‚</li>
<li>VerSeæ”¯æŒå…¨è‡ªåŠ¨å’Œäº¤äº’å¼æ©è†œç»†åŒ–ï¼Œé€‚åº”ä¸åŒéœ€æ±‚ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒVerSeåœ¨å¿ƒè„MRIå’Œå…¶ä»–åŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„æ€§èƒ½å’Œæ•ˆç‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-93e07843c3703ec6624cae298f735eca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c05f5be86ea87f066a56da8c0cdf8bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0b138bdf440290611ad35a7690689be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86a0ef29f1d6af803b9c897483359c71.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DAMPER-A-Dual-Stage-Medical-Report-Generation-Framework-with-Coarse-Grained-MeSH-Alignment-and-Fine-Grained-Hypergraph-Matching"><a href="#DAMPER-A-Dual-Stage-Medical-Report-Generation-Framework-with-Coarse-Grained-MeSH-Alignment-and-Fine-Grained-Hypergraph-Matching" class="headerlink" title="DAMPER: A Dual-Stage Medical Report Generation Framework with   Coarse-Grained MeSH Alignment and Fine-Grained Hypergraph Matching"></a>DAMPER: A Dual-Stage Medical Report Generation Framework with   Coarse-Grained MeSH Alignment and Fine-Grained Hypergraph Matching</h2><p><strong>Authors:Xiaofei Huang, Wenting Chen, Jie Liu, Qisheng Lu, Xiaoling Luo, Linlin Shen</strong></p>
<p>Medical report generation is crucial for clinical diagnosis and patient management, summarizing diagnoses and recommendations based on medical imaging. However, existing work often overlook the clinical pipeline involved in report writing, where physicians typically conduct an initial quick review followed by a detailed examination. Moreover, current alignment methods may lead to misaligned relationships. To address these issues, we propose DAMPER, a dual-stage framework for medical report generation that mimics the clinical pipeline of report writing in two stages. In the first stage, a MeSH-Guided Coarse-Grained Alignment (MCG) stage that aligns chest X-ray (CXR) image features with medical subject headings (MeSH) features to generate a rough keyphrase representation of the overall impression. In the second stage, a Hypergraph-Enhanced Fine-Grained Alignment (HFG) stage that constructs hypergraphs for image patches and report annotations, modeling high-order relationships within each modality and performing hypergraph matching to capture semantic correlations between image regions and textual phrases. Finally,the coarse-grained visual features, generated MeSH representations, and visual hypergraph features are fed into a report decoder to produce the final medical report. Extensive experiments on public datasets demonstrate the effectiveness of DAMPER in generating comprehensive and accurate medical reports, outperforming state-of-the-art methods across various evaluation metrics. </p>
<blockquote>
<p>åŒ»å­¦æŠ¥å‘Šç”Ÿæˆå¯¹ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—ç®¡ç†è‡³å…³é‡è¦ï¼Œå®ƒæ˜¯åŸºäºåŒ»å­¦æˆåƒå¯¹è¯Šæ–­å’Œå»ºè®®è¿›è¡Œæ±‡æ€»çš„å…³é”®ç¯èŠ‚ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œå¾€å¾€å¿½ç•¥äº†æŠ¥å‘Šå†™ä½œä¸­æ¶‰åŠçš„ä¸´åºŠæµç¨‹ï¼ŒåŒ»ç”Ÿé€šå¸¸å…ˆè¿›è¡Œåˆæ­¥å¿«é€Ÿå®¡æŸ¥ï¼Œç„¶åè¿›è¡Œè¯¦ç»†æ£€æŸ¥ã€‚æ­¤å¤–ï¼Œå½“å‰çš„å¯¹é½æ–¹æ³•å¯èƒ½å¯¼è‡´å…³ç³»é”™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DAMPERï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŒ»å­¦æŠ¥å‘Šç”Ÿæˆçš„åŒé˜¶æ®µæ¡†æ¶ï¼Œå®ƒæ¨¡ä»¿äº†ä¸¤é˜¶æ®µæŠ¥å‘Šå†™ä½œçš„ä¸´åºŠæµç¨‹ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé‡‡ç”¨MeSHæŒ‡å¯¼çš„ç²—ç²’åº¦å¯¹é½ï¼ˆMCGï¼‰é˜¶æ®µï¼Œå°†èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰å›¾åƒç‰¹å¾ä¸åŒ»å­¦ä¸»é¢˜è¯è¡¨ï¼ˆMeSHï¼‰ç‰¹å¾å¯¹é½ï¼Œç”Ÿæˆæ•´ä½“å°è±¡çš„ç²—ç•¥å…³é”®è¯è¡¨ç¤ºã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œé‡‡ç”¨è¶…å›¾å¢å¼ºç»†ç²’åº¦å¯¹é½ï¼ˆHFGï¼‰é˜¶æ®µï¼Œä¸ºå›¾åƒè¡¥ä¸å’ŒæŠ¥å‘Šæ³¨é‡Šæ„å»ºè¶…å›¾ï¼Œå¯¹æ¯ç§æ¨¡æ€å†…éƒ¨çš„é«˜é˜¶å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶æ‰§è¡Œè¶…å›¾åŒ¹é…ä»¥æ•è·å›¾åƒåŒºåŸŸå’Œæ–‡æœ¬çŸ­è¯­ä¹‹é—´çš„è¯­ä¹‰ç›¸å…³æ€§ã€‚æœ€åï¼Œå°†ç²—ç²’åº¦è§†è§‰ç‰¹å¾ã€ç”Ÿæˆçš„MeSHè¡¨ç¤ºå’Œè§†è§‰è¶…å›¾ç‰¹å¾è¾“å…¥æŠ¥å‘Šè§£ç å™¨ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„åŒ»å­¦æŠ¥å‘Šã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDAMPERåœ¨ç”Ÿæˆå…¨é¢å‡†ç¡®çš„åŒ»å­¦æŠ¥å‘Šæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œåœ¨å„é¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14535v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦æŠ¥å‘Šç”Ÿæˆçš„é‡è¦æ€§åŠå…¶åœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—æ‚£è€…ç®¡ç†ä¸­çš„ä½œç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¿½ç•¥ä¸´åºŠæŠ¥å‘Šå†™ä½œæµç¨‹å’Œå¯èƒ½å¯¼è‡´çš„å¯¹é½é—®é¢˜ï¼Œæå‡ºäº†DAMPERåŒé˜¶æ®µæ¡†æ¶ï¼Œæ¨¡ä»¿ä¸´åºŠæŠ¥å‘Šå†™ä½œæµç¨‹ï¼ŒåŒ…æ‹¬MeSHæŒ‡å¯¼çš„ç²—ç²’åº¦å¯¹é½å’Œè¶…å›¾å¢å¼ºçš„ç»†ç²’åº¦å¯¹é½ï¼Œå¹¶å®ç°äº†åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„ä¼˜è‰¯è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æŠ¥å‘Šç”Ÿæˆåœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—æ‚£è€…ç®¡ç†ä¸­èµ·åˆ°é‡è¦ä½œç”¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†ä¸´åºŠæŠ¥å‘Šå†™ä½œæµç¨‹ï¼Œé€šå¸¸åˆ†ä¸ºåˆæ­¥å®¡æŸ¥å’Œè¯¦ç»†æ£€æŸ¥ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>DAMPERæ¡†æ¶æ¨¡ä»¿ä¸´åºŠæŠ¥å‘Šå†™ä½œæµç¨‹ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šMeSHæŒ‡å¯¼çš„ç²—ç²’åº¦å¯¹é½å’ŒHypergraphå¢å¼ºçš„ç»†ç²’åº¦å¯¹é½ã€‚</li>
<li>MeSHæŒ‡å¯¼çš„ç²—ç²’åº¦å¯¹é½é˜¶æ®µå°†CXRå›¾åƒç‰¹å¾ä¸MeSHç‰¹å¾å¯¹é½ï¼Œç”Ÿæˆæ•´ä½“å°è±¡çš„å…³é”®çŸ­è¯­è¡¨ç¤ºã€‚</li>
<li>Hypergraphå¢å¼ºçš„ç»†ç²’åº¦å¯¹é½é˜¶æ®µæ„å»ºå›¾åƒè¡¥ä¸å’ŒæŠ¥å‘Šæ³¨é‡Šçš„è¶…å›¾ï¼Œå»ºç«‹æ¯ç§æ¨¡æ€å†…çš„é«˜é˜¶å…³ç³»ï¼Œå¹¶é€šè¿‡è¶…å›¾åŒ¹é…æ•æ‰å›¾åƒåŒºåŸŸå’Œæ–‡æœ¬çŸ­è¯­ä¹‹é—´çš„è¯­ä¹‰å…³è”ã€‚</li>
<li>DAMPERæ¡†æ¶å®ç°äº†åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„ä¼˜è‰¯è¡¨ç°ï¼Œç”Ÿæˆçš„åŒ»å­¦æŠ¥å‘Šå…¨é¢ä¸”å‡†ç¡®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdfdf6ba26511444d4a652b548de8959.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96930df957550099eead51cd8520d356.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-794b3cd3e8212de13cc04d29c046d076.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Ultra-High-Resolution-Segmentation-via-Boundary-Enhanced-Patch-Merging-Transformer"><a href="#Ultra-High-Resolution-Segmentation-via-Boundary-Enhanced-Patch-Merging-Transformer" class="headerlink" title="Ultra-High Resolution Segmentation via Boundary-Enhanced Patch-Merging   Transformer"></a>Ultra-High Resolution Segmentation via Boundary-Enhanced Patch-Merging   Transformer</h2><p><strong>Authors:Haopeng Sun, Yingwei Zhang, Lumin Xu, Sheng Jin, Yiqiang Chen</strong></p>
<p>Segmentation of ultra-high resolution (UHR) images is a critical task with numerous applications, yet it poses significant challenges due to high spatial resolution and rich fine details. Recent approaches adopt a dual-branch architecture, where a global branch learns long-range contextual information and a local branch captures fine details. However, they struggle to handle the conflict between global and local information while adding significant extra computational cost. Inspired by the human visual systemâ€™s ability to rapidly orient attention to important areas with fine details and filter out irrelevant information, we propose a novel UHR segmentation method called Boundary-enhanced Patch-merging Transformer (BPT). BPT consists of two key components: (1) Patch-Merging Transformer (PMT) for dynamically allocating tokens to informative regions to acquire global and local representations, and (2) Boundary-Enhanced Module (BEM) that leverages boundary information to enrich fine details. Extensive experiments on multiple UHR image segmentation benchmarks demonstrate that our BPT outperforms previous state-of-the-art methods without introducing extra computational overhead. Codes will be released to facilitate research. </p>
<blockquote>
<p>è¶…é«˜åˆ†è¾¨ç‡ï¼ˆUHRï¼‰å›¾åƒåˆ†å‰²æ˜¯ä¸€ä¸ªå…·æœ‰è®¸å¤šåº”ç”¨çš„å…³é”®ä»»åŠ¡ï¼Œä½†ç”±äºå…¶é«˜ç©ºé—´åˆ†è¾¨ç‡å’Œä¸°å¯Œçš„ç»†èŠ‚è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æœ€è¿‘çš„æ–¹æ³•é‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œå…¶ä¸­å…¨å±€åˆ†æ”¯å­¦ä¹ é•¿ç¨‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œå±€éƒ¨åˆ†æ”¯æ•è·ç»†èŠ‚ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯ä¹‹é—´çš„å†²çªæ—¶é‡åˆ°å›°éš¾ï¼ŒåŒæ—¶å¢åŠ äº†ç›¸å½“å¤§çš„é¢å¤–è®¡ç®—æˆæœ¬ã€‚å—äººç±»è§†è§‰ç³»ç»Ÿèƒ½å¤Ÿå¿«é€Ÿå°†æ³¨æ„åŠ›å®šå‘åˆ°å…·æœ‰ç»†èŠ‚çš„é‡è¦åŒºåŸŸå¹¶è¿‡æ»¤æ‰æ— å…³ä¿¡æ¯çš„èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„UHRåˆ†å‰²æ–¹æ³•ï¼Œç§°ä¸ºè¾¹ç•Œå¢å¼ºè¡¥ä¸åˆå¹¶è½¬æ¢å™¨ï¼ˆBPTï¼‰ã€‚BPTç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šï¼ˆ1ï¼‰è¡¥ä¸åˆå¹¶è½¬æ¢å™¨ï¼ˆPMTï¼‰ï¼Œç”¨äºåŠ¨æ€åˆ†é…ä»¤ç‰Œä»¥è·å–å…¨å±€å’Œå±€éƒ¨è¡¨ç¤ºä¿¡æ¯çš„æœ‰æ„ä¹‰åŒºåŸŸï¼›ï¼ˆ2ï¼‰è¾¹ç•Œå¢å¼ºæ¨¡å—ï¼ˆBEMï¼‰ï¼Œåˆ©ç”¨è¾¹ç•Œä¿¡æ¯æ¥ä¸°å¯Œç»†èŠ‚ã€‚åœ¨å¤šä¸ªUHRå›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„BPTæ–¹æ³•åœ¨æ²¡æœ‰ä»»ä½•é¢å¤–è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æˆ‘ä»¬å°†å‘å¸ƒä»£ç ä»¥ä¿ƒè¿›ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10181v2">PDF</a> This paper has been accepted by AAAI 2025, 10 pages, 4 figures</p>
<p><strong>Summary</strong><br>è¶…é«˜åˆ†è¾¨ç‡ï¼ˆUHRï¼‰å›¾åƒåˆ†å‰²æ˜¯ä¸€é¡¹å…·æœ‰è®¸å¤šåº”ç”¨çš„å…³é”®ä»»åŠ¡ï¼Œä½†ç”±äºé«˜ç©ºé—´åˆ†è¾¨ç‡å’Œä¸°å¯Œçš„ç»†èŠ‚è€Œé¢ä¸´æŒ‘æˆ˜ã€‚æœ€è¿‘çš„æ–¹æ³•é‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œå…¨å±€åˆ†æ”¯å­¦ä¹ é•¿èŒƒå›´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå±€éƒ¨åˆ†æ”¯æ•æ‰ç»†èŠ‚ã€‚ç„¶è€Œï¼Œå®ƒä»¬éš¾ä»¥å¤„ç†å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯ä¹‹é—´çš„å†²çªï¼Œå¹¶å¢åŠ äº†é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚å—äººç±»è§†è§‰ç³»ç»Ÿèƒ½å¤Ÿå¿«é€Ÿå°†æ³¨æ„åŠ›é›†ä¸­åœ¨å…·æœ‰ç»†èŠ‚çš„é‡è¦åŒºåŸŸå¹¶è¿‡æ»¤æ‰æ— å…³ä¿¡æ¯çš„èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„UHRåˆ†å‰²æ–¹æ³•ï¼Œç§°ä¸ºè¾¹ç•Œå¢å¼ºè¡¥ä¸åˆå¹¶è½¬æ¢å™¨ï¼ˆBPTï¼‰ã€‚BPTç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šï¼ˆ1ï¼‰è¡¥ä¸åˆå¹¶è½¬æ¢å™¨ï¼ˆPMTï¼‰ç”¨äºåŠ¨æ€åˆ†é…ä»¤ç‰Œä»¥è·å–å…¨å±€å’Œå±€éƒ¨è¡¨ç¤ºï¼›ï¼ˆ2ï¼‰è¾¹ç•Œå¢å¼ºæ¨¡å—ï¼ˆBEMï¼‰åˆ©ç”¨è¾¹ç•Œä¿¡æ¯æ¥ä¸°å¯Œç»†èŠ‚ã€‚åœ¨å¤šä¸ªUHRå›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„BPTåœ¨ä¸éœ€è¦å¼•å…¥é¢å¤–è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…é«˜åˆ†è¾¨ç‡ï¼ˆUHRï¼‰å›¾åƒåˆ†å‰²æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦å¤„ç†é«˜ç©ºé—´åˆ†è¾¨ç‡å’Œä¸°å¯Œç»†èŠ‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œä½†éš¾ä»¥å¹³è¡¡å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯ï¼Œä¸”è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>æå‡ºçš„Boundary-enhanced Patch-merging Transformerï¼ˆBPTï¼‰æ–¹æ³•å—åˆ°äººç±»è§†è§‰ç³»ç»Ÿçš„å¯å‘ï¼Œèƒ½å¤ŸåŠ¨æ€åˆ†é…æ³¨æ„åŠ›åˆ°é‡è¦åŒºåŸŸã€‚</li>
<li>BPTåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šPatch-Merging Transformerï¼ˆPMTï¼‰å’ŒBoundary-Enhanced Moduleï¼ˆBEMï¼‰ã€‚</li>
<li>PMTç”¨äºè·å–å…¨å±€å’Œå±€éƒ¨è¡¨ç¤ºï¼Œè€ŒBEMåˆ©ç”¨è¾¹ç•Œä¿¡æ¯æ¥å¢å¼ºç»†èŠ‚ã€‚</li>
<li>åœ¨å¤šä¸ªUHRå›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šï¼ŒBPTè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10181">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-710dd02beaf7ecedc1ab265e4b0a395b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-636f0de13d7fe4f187f05c2f933056b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d29dade9a8174750ee2b09c65e7ed797.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65301682517fdbb19ddef24184e81c70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f83e0d6b260814e301d14a9c3c13ac40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eae6281da9f4d442912bb15da0851ac5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39fda143ee464dc0f435b1b7dcb2259d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation"></a>FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation</h2><p><strong>Authors:Yuntian Bo, Yazhou Zhu, Lunbo Li, Haofeng Zhang</strong></p>
<p>Existing few-shot medical image segmentation (FSMIS) models fail to address a practical issue in medical imaging: the domain shift caused by different imaging techniques, which limits the applicability to current FSMIS tasks. To overcome this limitation, we focus on the cross-domain few-shot medical image segmentation (CD-FSMIS) task, aiming to develop a generalized model capable of adapting to a broader range of medical image segmentation scenarios with limited labeled data from the novel target domain. Inspired by the characteristics of frequency domain similarity across different domains, we propose a Frequency-aware Matching Network (FAMNet), which includes two key components: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion (MSF) module. The FAM module tackles two problems during the meta-learning phase: 1) intra-domain variance caused by the inherent support-query bias, due to the different appearances of organs and lesions, and 2) inter-domain variance caused by different medical imaging techniques. Additionally, we design an MSF module to integrate the different frequency features decoupled by the FAM module, and further mitigate the impact of inter-domain variance on the modelâ€™s segmentation performance. Combining these two modules, our FAMNet surpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation models on three cross-domain datasets, achieving state-of-the-art performance in the CD-FSMIS task. </p>
<blockquote>
<p>ç°æœ‰çš„ä¸€äº›åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„å°‘æ•°æ‹æ‘„åœºæ™¯æ— æ³•åº”å¯¹åŒ»å­¦æˆåƒä¸­çš„ä¸€ä¸ªå®é™…é—®é¢˜ï¼šç”±äºä¸åŒæˆåƒæŠ€æœ¯é€ æˆçš„é¢†åŸŸåç§»é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨å½“å‰åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸“æ³¨äºè·¨åŸŸåŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹çš„å°‘æ•°æ‹æ‘„åœºæ™¯ï¼ˆCD-FSMISä»»åŠ¡ï¼‰ï¼Œæ—¨åœ¨å¼€å‘ä¸€ç§é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„æ–°ç›®æ ‡åŸŸæ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹é€‚åº”æ›´å¹¿æ³›çš„åŒ»å­¦å›¾åƒåˆ†å‰²åœºæ™¯ã€‚å—ä¸åŒé¢†åŸŸä¹‹é—´é¢‘åŸŸç›¸ä¼¼æ€§ç‰¹å¾çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šé¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰æ¨¡å—å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰æ¨¡å—ã€‚FAMæ¨¡å—è§£å†³äº†å…ƒå­¦ä¹ é˜¶æ®µçš„ä¸¤ä¸ªé—®é¢˜ï¼š1ï¼‰ç”±äºå™¨å®˜å’Œç—…å˜çš„ä¸åŒå¤–è§‚å¯¼è‡´çš„å†…åœ¨æ”¯æ’‘æŸ¥è¯¢åå·®æ‰€é€ æˆçš„é¢†åŸŸå†…æ–¹å·®ï¼›ä»¥åŠç”±ä¸åŒåŒ»å­¦æˆåƒæŠ€æœ¯å¼•èµ·çš„é¢†åŸŸé—´æ–¹å·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªMSFæ¨¡å—æ¥æ•´åˆç”±FAMæ¨¡å—åˆ†ç¦»çš„ä¸åŒçš„é¢‘ç‡ç‰¹å¾ï¼Œå¹¶è¿›ä¸€æ­¥å‡è½»é¢†åŸŸé—´æ–¹å·®å¯¹æ¨¡å‹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚ç»“åˆè¿™ä¸¤ä¸ªæ¨¡å—ï¼Œæˆ‘ä»¬çš„FAMNetåœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ•°æ‹æ‘„è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œåœ¨CD-FSMISä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09319v3">PDF</a> Accepted by the 39th Annual AAAI Conference on Artificial   Intelligence (AAAI-25)</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç°æœ‰åŒ»ç–—å›¾åƒåˆ†å‰²æ¨¡å‹å› ä¸åŒæˆåƒæŠ€æœ¯å¯¼è‡´çš„é¢†åŸŸåç§»é—®é¢˜ï¼Œæå‡ºä¸€ç§é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼ŒåŒ…å«é¢‘ç‡æ„ŸçŸ¥åŒ¹é…å’Œå¤šå…‰è°±èåˆä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œä»¥æå‡æ¨¡å‹å¯¹å¤šç§åŒ»ç–—å›¾åƒåˆ†å‰²åœºæ™¯çš„é€‚åº”æ€§ã€‚FAMNetæˆåŠŸè¶…è¶Šäº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ•°è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œåœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šå®ç°äº†CD-FSMISä»»åŠ¡çš„æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢†åŸŸåç§»é—®é¢˜æ˜¯åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­çš„ä¸€ä¸ªå®é™…é—®é¢˜ï¼Œç”±äºä¸åŒæˆåƒæŠ€æœ¯å¯¼è‡´çš„ã€‚</li>
<li>é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰æ—¨åœ¨è§£å†³è·¨åŸŸå°‘æ•°åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡ã€‚</li>
<li>FAMNetåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šé¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰æ¨¡å—å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰æ¨¡å—ã€‚</li>
<li>FAMæ¨¡å—è§£å†³äº†å…ƒå­¦ä¹ é˜¶æ®µçš„åŸŸå†…å’ŒåŸŸé—´æ–¹å·®é—®é¢˜ã€‚</li>
<li>MSFæ¨¡å—ç”¨äºé›†æˆç”±FAMæ¨¡å—åˆ†ç¦»çš„ä¸åŒé¢‘ç‡ç‰¹å¾ï¼Œå¹¶å‡å°‘åŸŸé—´æ–¹å·®å¯¹æ¨¡å‹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚</li>
<li>FAMNetåœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ•°è¯­ä¹‰åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>FAMNetåœ¨CD-FSMISä»»åŠ¡ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb17c5e92f42f7d91e3164ec653e424b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0796f64fbec939deef49b781663f0ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-def190b9c5343be34695a04abbf24490.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Magnetic-Resonance-Imaging-Feature-Based-Subtyping-and-Model-Ensemble-for-Enhanced-Brain-Tumor-Segmentation"><a href="#Magnetic-Resonance-Imaging-Feature-Based-Subtyping-and-Model-Ensemble-for-Enhanced-Brain-Tumor-Segmentation" class="headerlink" title="Magnetic Resonance Imaging Feature-Based Subtyping and Model Ensemble   for Enhanced Brain Tumor Segmentation"></a>Magnetic Resonance Imaging Feature-Based Subtyping and Model Ensemble   for Enhanced Brain Tumor Segmentation</h2><p><strong>Authors:Zhifan Jiang, Daniel CapellÃ¡n-MartÃ­n, Abhijeet Parida, Austin Tapp, Xinyang Liu, MarÃ­a J. Ledesma-Carbayo, Syed Muhammad Anwar, Marius George Linguraru</strong></p>
<p>Accurate and automatic segmentation of brain tumors in multi-parametric magnetic resonance imaging (mpMRI) is essential for quantitative measurements, which play an increasingly important role in clinical diagnosis and prognosis. The International Brain Tumor Segmentation (BraTS) Challenge 2024 offers a unique benchmarking opportunity, including various types of brain tumors in both adult and pediatric populations, such as pediatric brain tumors (PED), meningiomas (MEN-RT) and brain metastases (MET), among others. Compared to previous editions, BraTS 2024 has implemented changes to substantially increase clinical relevance, such as refined tumor regions for evaluation. We propose a deep learning-based ensemble approach that integrates state-of-the-art segmentation models. Additionally, we introduce innovative, adaptive pre- and post-processing techniques that employ MRI-based radiomic analyses to differentiate tumor subtypes. Given the heterogeneous nature of the tumors present in the BraTS datasets, this approach enhances the precision and generalizability of segmentation models. On the final testing sets, our method achieved mean lesion-wise Dice similarity coefficients of 0.926, 0.801, and 0.688 for the whole tumor in PED, MEN-RT, and MET, respectively. These results demonstrate the effectiveness of our approach in improving segmentation performance and generalizability for various brain tumor types.   The source code of our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/Precision-Medical-Imaging-Group/HOPE-Segmenter-Kids">https://github.com/Precision-Medical-Imaging-Group/HOPE-Segmenter-Kids</a>. Additionally, an open-source web-application is accessible at <a target="_blank" rel="noopener" href="https://segmenter.hope4kids.io/">https://segmenter.hope4kids.io/</a> which uses the docker container aparida12&#x2F;brats-peds-2024:v20240913 . </p>
<blockquote>
<p>åœ¨å¤šå‚æ•°ç£å…±æŒ¯æˆåƒï¼ˆmpMRIï¼‰ä¸­ï¼Œå¯¹è„‘è‚¿ç˜¤è¿›è¡Œå‡†ç¡®ã€è‡ªåŠ¨åˆ†å‰²å¯¹äºå®šé‡æµ‹é‡éå¸¸é‡è¦ï¼Œè¿™åœ¨ä¸´åºŠè¯Šæ–­å’Œé¢„åä¸­å‘æŒ¥ç€è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ã€‚2024å¹´å›½é™…è„‘è‚¿ç˜¤åˆ†å‰²ï¼ˆBraTSï¼‰æŒ‘æˆ˜èµ›æä¾›äº†ä¸€ä¸ªç‹¬ç‰¹çš„åŸºå‡†æµ‹è¯•æœºä¼šï¼ŒåŒ…æ‹¬æˆäººå’Œå„¿ç«¥äººç¾¤ä¸­çš„å„ç§è„‘è‚¿ç˜¤ç±»å‹ï¼Œå¦‚å„¿ç«¥è„‘è‚¿ç˜¤ï¼ˆPEDï¼‰ã€è„‘è†œç˜¤ï¼ˆMEN-RTï¼‰å’Œè„‘è½¬ç§»ï¼ˆMETï¼‰ç­‰ã€‚ä¸ä¹‹å‰çš„ç‰ˆæœ¬ç›¸æ¯”ï¼ŒBraTS 2024å®æ–½äº†å˜åŒ–ï¼Œä»¥å¤§å¤§å¢åŠ ä¸´åºŠç›¸å…³æ€§ï¼Œä¾‹å¦‚å¯¹è¯„ä¼°è‚¿ç˜¤åŒºåŸŸçš„ç²¾ç»†åˆ’åˆ†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„é›†æˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é›†æˆäº†æœ€å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ›æ–°ã€è‡ªé€‚åº”çš„é¢„å¤„ç†å’Œåå¤„ç†æŠ€å·§ï¼Œé‡‡ç”¨åŸºäºMRIçš„æ”¾å°„ç»„å­¦åˆ†ææ¥åŒºåˆ†è‚¿ç˜¤äºšå‹ã€‚é‰´äºBraTSæ•°æ®é›†ä¸­å­˜åœ¨çš„è‚¿ç˜¤çš„å¼‚è´¨æ€§ï¼Œè¿™ç§æ–¹æ³•æé«˜äº†åˆ†å‰²æ¨¡å‹çš„ç²¾åº¦å’Œé€šç”¨æ€§ã€‚åœ¨æœ€ç»ˆæµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†PEDã€MEN-RTå’ŒMETå…¨è‚¿ç˜¤çš„ç—…å˜çº§å¹³å‡Diceç›¸ä¼¼ç³»æ•°åˆ†åˆ«ä¸º0.926ã€0.801å’Œ0.688ã€‚è¿™äº›ç»“æœè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨æ”¹è¿›å„ç§è„‘è‚¿ç˜¤ç±»å‹çš„åˆ†å‰²æ€§èƒ½å’Œé€šç”¨æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å®ç°çš„æºä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/Precision-Medical-Imaging-Group/HOPE-Segmenter-Kids%E8%8E%B7%E5%8F%96%E3%80%82%E6%AD%A4%E5%A4%96%EF%BC%8C%E4%B8%80%E4%B8%AA%E5%BC%80%E6%BA%90%E7%9A%84web%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E5%8F%AF%E5%9C%A8[https://segmenter.hope4kids.io/%E4%B8%8A%E4%BD%BF%E7%94%A8%EF%BC%8C%E8%AF%A5%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E4%BD%BF%E7%94%A8docker%E5%AE%B9%E5%99%A8aparida12/brats-peds-2024:v20240913%E3%80%82](https://segmenter.hope4kids.io/%E4%B8%8A%E4%BD%BF%E7%94%A8%EF%BC%8C%E8%AF%A5%E7%9A%84%E5%AE%BE%E7%BD%AE%E5%B9%B3%E5%AE%B9aparida12/brats-peds-2024:v20240913%E3%80%82)">https://github.com/Precision-Medical-Imaging-Group/HOPE-Segmenter-Kidsè·å–ã€‚æ­¤å¤–ï¼Œä¸€ä¸ªå¼€æºçš„webåº”ç”¨ç¨‹åºå¯åœ¨[https://segmenter.hope4kids.io/ä¸Šä½¿ç”¨ï¼Œè¯¥åº”ç”¨ç¨‹åºä½¿ç”¨dockerå®¹å™¨aparida12/brats-peds-2024:v20240913ã€‚](https://segmenter.hope4kids.io/%E4%B8%8A%E4%BD%BF%E7%94%A8%EF%BC%8C%E8%AF%A5%E7%9A%84%E5%AE%BE%E7%BD%AE%E5%B9%B3%E5%AE%B9aparida12/brats-peds-2024:v20240913%E3%80%82)</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04094v3">PDF</a> 11 pages, 4 figures, 3 tables. This paper was accepted at   MICCAI-BraTS 2024</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„é›†æˆæ–¹æ³•ï¼Œç”¨äºå¤šå‚æ•°ç£å…±æŒ¯æˆåƒï¼ˆmpMRIï¼‰ä¸­çš„è„‘è‚¿ç˜¤å‡†ç¡®è‡ªåŠ¨åˆ†å‰²ã€‚è¯¥æ–¹æ³•ç»“åˆæœ€å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹ï¼Œå¹¶å¼•å…¥åˆ›æ–°çš„é¢„å¤„ç†å’Œåå¤„ç†æŠ€æœ¯ï¼Œä»¥åŒºåˆ†è‚¿ç˜¤äºšå‹ã€‚åœ¨BraTS 2024æŒ‘æˆ˜æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•æé«˜äº†åˆ†å‰²æ€§èƒ½å’Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯¹ä¸åŒç±»å‹çš„è„‘è‚¿ç˜¤å¦‚PEDã€MEN-RTå’ŒMETçš„æ•´ç˜¤Diceç›¸ä¼¼ç³»æ•°åˆ†åˆ«è¾¾åˆ°0.926ã€0.801å’Œ0.688ã€‚æºä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šï¼Œå¹¶æœ‰ä¸€ä¸ªå¼€æ”¾æºç çš„webåº”ç”¨ç¨‹åºå¯ä¾›ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BraTS 2024æŒ‘æˆ˜æä¾›äº†ç‹¬ç‰¹çš„åŸºå‡†æµ‹è¯•æœºä¼šï¼ŒåŒ…å«æˆäººå’Œå„¿ç«¥çš„å¤šç§ç±»å‹çš„è„‘è‚¿ç˜¤ã€‚</li>
<li>ä¸ä¹‹å‰çš„ç‰ˆæœ¬ç›¸æ¯”ï¼ŒBraTS 2024å¢åŠ äº†ä¸´åºŠç›¸å…³æ€§æ›´é«˜çš„æ”¹è¿›ï¼Œå¦‚æ›´ç²¾ç»†çš„è‚¿ç˜¤åŒºåŸŸè¯„ä¼°ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„é›†æˆæ–¹æ³•ï¼Œæ•´åˆæœ€å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹è¿›è¡Œè„‘è‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>å¼•å…¥åˆ›æ–°çš„é¢„å¤„ç†å’Œåå¤„ç†æŠ€æœ¯ï¼Œåˆ©ç”¨MRIçš„æ”¾å°„ç»„å­¦åˆ†æåŒºåˆ†è‚¿ç˜¤äºšå‹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨BraTS 2024æ•°æ®é›†ä¸Šå®ç°äº†è¾ƒé«˜çš„åˆ†å‰²æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä¸åŒç±»å‹çš„è„‘è‚¿ç˜¤ã€‚</li>
<li>å…¬å¼€çš„æºä»£ç åŒ…æ‹¬ä¸€ä¸ªGitHubé“¾æ¥å’Œä¸€ä¸ªwebåº”ç”¨ç¨‹åºï¼Œæ–¹ä¾¿ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-154c21363aa46da48c13bbe6584f6efb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de005a1ffce73e883035345fab662be9.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-46660c58a4177a91d3aa3f203cdd12d5.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  VERSA A Versatile Evaluation Toolkit for Speech, Audio, and Music
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a42eaf6bb487e03d164d45d2c35b4a5c.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  Assessment of Deep-Learning Methods for the Enhancement of Experimental   Low Dose Dental CBCT Volumes
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17982k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
