<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2024-12-25  Editing Implicit and Explicit Representations of Radiance Fields A   Survey">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-707173a3a7fc763b3069420ce0526011.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    22 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-25-更新"><a href="#2024-12-25-更新" class="headerlink" title="2024-12-25 更新"></a>2024-12-25 更新</h1><h2 id="Editing-Implicit-and-Explicit-Representations-of-Radiance-Fields-A-Survey"><a href="#Editing-Implicit-and-Explicit-Representations-of-Radiance-Fields-A-Survey" class="headerlink" title="Editing Implicit and Explicit Representations of Radiance Fields: A   Survey"></a>Editing Implicit and Explicit Representations of Radiance Fields: A   Survey</h2><p><strong>Authors:Arthur Hubert, Gamal Elghazaly, Raphael Frank</strong></p>
<p>Neural Radiance Fields (NeRF) revolutionized novel view synthesis in recent years by offering a new volumetric representation, which is compact and provides high-quality image rendering. However, the methods to edit those radiance fields developed slower than the many improvements to other aspects of NeRF. With the recent development of alternative radiance field-based representations inspired by NeRF as well as the worldwide rise in popularity of text-to-image models, many new opportunities and strategies have emerged to provide radiance field editing. In this paper, we deliver a comprehensive survey of the different editing methods present in the literature for NeRF and other similar radiance field representations. We propose a new taxonomy for classifying existing works based on their editing methodologies, review pioneering models, reflect on current and potential new applications of radiance field editing, and compare state-of-the-art approaches in terms of editing options and performance. </p>
<blockquote>
<p>神经辐射场（NeRF）为体积表示法提供了一种新的形式，这种形式紧凑且能提供高质量图像渲染，从而彻底改变了近年来的新视角合成技术。然而，编辑这些辐射场的方法的发展速度比NeRF其他方面改进的速度要慢。随着最近受NeRF启发的其他辐射场表示方法的发展以及全球文本到图像模型的普及，出现了许多新的机会和策略来提供辐射场编辑功能。在本文中，我们对文献中针对NeRF和其他类似辐射场表示的不同编辑方法进行了全面的调查。我们基于编辑方法论提出了新的分类法来分类现有作品，回顾了开创性模型，思考了辐射场编辑的当前和潜在新应用，并在编辑选项和性能方面比较了最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17628v1">PDF</a> </p>
<p><strong>Summary</strong><br>NeRF的神经网络辐射场为新型视图合成提供了高质量图像渲染的紧凑体积表示，但其编辑方法发展较慢。随着受NeRF启发的其他辐射场表示方法的出现以及文本到图像模型的普及，新兴的策略和机会涌现，用于编辑辐射场。本文全面综述了文献中针对NeRF和其他类似辐射场表示的不同编辑方法，提出了基于编辑方法的新分类法，回顾了开创性模型，思考了辐射场编辑的当前和潜在应用，并比较了编辑选项和性能方面的最新先进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF通过其神经网络辐射场为新型视图合成提供了高质量图像渲染的紧凑体积表示。</li>
<li>NeRF的编辑方法发展相对较慢，但新的辐射场表示方法和文本到图像模型的普及为其带来了新的机遇。</li>
<li>本文全面综述了针对NeRF和其他辐射场表示的编辑方法，并提出了基于编辑方法的新分类法。</li>
<li>文章回顾了一些开创性的模型在辐射场编辑领域的应用。</li>
<li>当前和未来的辐射场编辑应用被探讨，包括可能的新的使用场景和趋势。</li>
<li>文章比较了不同编辑方法的性能以及它们在编辑选项方面的优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17628">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-10a2b0f87f41421105a00a51dad1238f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LUCES-MV-A-Multi-View-Dataset-for-Near-Field-Point-Light-Source-Photometric-Stereo"><a href="#LUCES-MV-A-Multi-View-Dataset-for-Near-Field-Point-Light-Source-Photometric-Stereo" class="headerlink" title="LUCES-MV: A Multi-View Dataset for Near-Field Point Light Source   Photometric Stereo"></a>LUCES-MV: A Multi-View Dataset for Near-Field Point Light Source   Photometric Stereo</h2><p><strong>Authors:Fotios Logothetis, Ignas Budvytis, Stephan Liwicki, Roberto Cipolla</strong></p>
<p>The biggest improvements in Photometric Stereo (PS) field has recently come from adoption of differentiable volumetric rendering techniques such as NeRF or Neural SDF achieving impressive reconstruction error of 0.2mm on DiLiGenT-MV benchmark. However, while there are sizeable datasets for environment lit objects such as Digital Twin Catalogue (DTS), there are only several small Photometric Stereo datasets which often lack challenging objects (simple, smooth, untextured) and practical, small form factor (near-field) light setup.   To address this, we propose LUCES-MV, the first real-world, multi-view dataset designed for near-field point light source photometric stereo. Our dataset includes 15 objects with diverse materials, each imaged under varying light conditions from an array of 15 LEDs positioned 30 to 40 centimeters from the camera center. To facilitate transparent end-to-end evaluation, our dataset provides not only ground truth normals and ground truth object meshes and poses but also light and camera calibration images.   We evaluate state-of-the-art near-field photometric stereo algorithms, highlighting their strengths and limitations across different material and shape complexities. LUCES-MV dataset offers an important benchmark for developing more robust, accurate and scalable real-world Photometric Stereo based 3D reconstruction methods. </p>
<blockquote>
<p>近期光度立体视觉（PS）领域最大的进展主要来自于采用可微分体积渲染技术，如NeRF或Neural SDF，在DiLiGenT-MV基准测试上实现了令人印象深刻的重建误差为0.2毫米。然而，虽然有大量针对环境照明物体的数据集，如数字孪生目录（DTS），但光度立体视觉数据集仅有少数几个，这些数据集通常缺乏具有挑战性的物体（简单、光滑、无纹理）以及实用、小尺寸的近场光照设置。为了解决这一问题，我们提出了LUCES-MV，这是专为近场点光源光度立体视觉设计的第一份现实世界多视角数据集。我们的数据集包含15个不同材质的对象，每个对象都在不同的光照条件下，由一组距离相机中心30至40厘米的15个LED拍摄。为了方便端到端的透明评估，我们的数据集不仅提供了地面真实法线、地面真实物体网格和姿态，还提供了光和相机的校准图像。我们评估了最先进的近场光度立体视觉算法，突出了它们在不同材质和形状复杂度上的优势和局限性。LUCES-MV数据集为开发更稳健、准确和可扩展的现实世界光度立体视觉基于3D重建方法提供了重要基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16737v1">PDF</a> </p>
<p><strong>Summary</strong><br>神经网络体积渲染技术（NeRF）和数字孪生目录（DTS）数据集的发展促进了光度立体视觉领域的进展。针对近场光度立体视觉缺乏挑战性和实用性问题，提出LUCES-MV数据集，包含多种材质物体和复杂的近场光源配置，促进更稳健、准确、可扩展的光度立体视觉方法的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRF等可微分体积渲染技术在重建误差方面取得了显著进展。</li>
<li>目前存在大型环境照明物体数据集如Digital Twin Catalogue（DTS），但针对近场光度立体视觉的数据集较少且缺乏挑战性。</li>
<li>LUCES-MV数据集是首个针对近场点光源光度立体的真实世界多视角数据集。</li>
<li>LUCES-MV包含多种材质物体，在变化的光照条件下进行成像。</li>
<li>数据集提供地面真实法线、物体网格、姿态以及光和相机校准图像。</li>
<li>LUCES-MV数据集为近场光度立体视觉算法提供了重要的基准测试。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16737">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-01f0a1f048821fabb9bd7b22c8ed9179.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f83cad92b61aa66cc839a3888fa6708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-716c4f8a374d3538fa05d8cd3e044545.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0506d6ef4c4d1651eaf183777189095.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfccd8334582642287ef21b6dfd1a9d5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GANFusion-Feed-Forward-Text-to-3D-with-Diffusion-in-GAN-Space"><a href="#GANFusion-Feed-Forward-Text-to-3D-with-Diffusion-in-GAN-Space" class="headerlink" title="GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space"></a>GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space</h2><p><strong>Authors:Souhaib Attaiki, Paul Guerrero, Duygu Ceylan, Niloy J. Mitra, Maks Ovsjanikov</strong></p>
<p>We train a feed-forward text-to-3D diffusion generator for human characters using only single-view 2D data for supervision. Existing 3D generative models cannot yet match the fidelity of image or video generative models. State-of-the-art 3D generators are either trained with explicit 3D supervision and are thus limited by the volume and diversity of existing 3D data. Meanwhile, generators that can be trained with only 2D data as supervision typically produce coarser results, cannot be text-conditioned, or must revert to test-time optimization. We observe that GAN- and diffusion-based generators have complementary qualities: GANs can be trained efficiently with 2D supervision to produce high-quality 3D objects but are hard to condition on text. In contrast, denoising diffusion models can be conditioned efficiently but tend to be hard to train with only 2D supervision. We introduce GANFusion, which starts by generating unconditional triplane features for 3D data using a GAN architecture trained with only single-view 2D data. We then generate random samples from the GAN, caption them, and train a text-conditioned diffusion model that directly learns to sample from the space of good triplane features that can be decoded into 3D objects. </p>
<blockquote>
<p>我们仅使用单视图2D数据进行监督，训练了一个前馈文本到3D扩散生成器，用于生成人物角色。现有的3D生成模型还无法与图像或视频生成模型的保真度相匹配。最先进的3D生成器要么接受明确的3D监督训练，因此受到现有3D数据量和多样性的限制。同时，只能接受2D数据作为监督训练的生成器通常会产生较粗糙的结果，无法根据文本进行调整，或在测试时需要进行优化。我们发现基于GAN和扩散的生成器具有互补的特性：GAN可以仅通过2D监督有效地训练，产生高质量的3D对象，但很难根据文本进行调整。相比之下，降噪扩散模型可以根据文本进行高效的条件生成，但仅用2D数据进行训练时往往很难训练。我们引入了GANFusion，它首先使用仅接受单视图2D数据训练的GAN架构，为3D数据生成无条件triplane特征。然后我们从GAN生成随机样本，进行描述，并训练一个文本条件扩散模型，该模型直接学习从可以解码为3D对象的良好triplane特征空间中进行采样。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16717v1">PDF</a> <a target="_blank" rel="noopener" href="https://ganfusion.github.io/">https://ganfusion.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文训练了一个前馈文本到三维扩散生成器，用于生成人物角色，仅使用单视图二维数据进行监督。现有的三维生成模型尚无法匹配图像或视频生成模型的保真度。本研究结合了GAN和扩散模型的优点，通过GAN架构生成无条件的三维数据triplane特征，然后使用文本条件扩散模型进行训练，直接学习从良好的triplane特征空间中进行采样，这些特征可以被解码为三维对象。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究训练了一个前馈文本到三维扩散生成器，利用单视图二维数据进行监督。</li>
<li>当前三维生成模型的保真度尚未达到图像或视频生成模型的水平。</li>
<li>GAN和扩散模型各有优点，研究尝试结合两者。</li>
<li>GAN架构用于生成无条件的三维数据triplane特征。</li>
<li>文本条件扩散模型直接学习从良好的triplane特征空间中进行采样。</li>
<li>这种结合方法能够利用二维数据监督生成高质量的三维对象。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16717">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a7c70940dd4269c05089c24b53125fec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40e67532efd85372b59da69406e25bd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c673abefd0494d1b1696857121c6cbe3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AdvIRL-Reinforcement-Learning-Based-Adversarial-Attacks-on-3D-NeRF-Models"><a href="#AdvIRL-Reinforcement-Learning-Based-Adversarial-Attacks-on-3D-NeRF-Models" class="headerlink" title="AdvIRL: Reinforcement Learning-Based Adversarial Attacks on 3D NeRF   Models"></a>AdvIRL: Reinforcement Learning-Based Adversarial Attacks on 3D NeRF   Models</h2><p><strong>Authors:Tommy Nguyen, Mehmet Ergezer, Christian Green</strong></p>
<p>The increasing deployment of AI models in critical applications has exposed them to significant risks from adversarial attacks. While adversarial vulnerabilities in 2D vision models have been extensively studied, the threat landscape for 3D generative models, such as Neural Radiance Fields (NeRF), remains underexplored. This work introduces \textit{AdvIRL}, a novel framework for crafting adversarial NeRF models using Instant Neural Graphics Primitives (Instant-NGP) and Reinforcement Learning. Unlike prior methods, \textit{AdvIRL} generates adversarial noise that remains robust under diverse 3D transformations, including rotations and scaling, enabling effective black-box attacks in real-world scenarios. Our approach is validated across a wide range of scenes, from small objects (e.g., bananas) to large environments (e.g., lighthouses). Notably, targeted attacks achieved high-confidence misclassifications, such as labeling a banana as a slug and a truck as a cannon, demonstrating the practical risks posed by adversarial NeRFs. Beyond attacking, \textit{AdvIRL}-generated adversarial models can serve as adversarial training data to enhance the robustness of vision systems. The implementation of \textit{AdvIRL} is publicly available at \url{<a target="_blank" rel="noopener" href="https://github.com/Tommy-Nguyen-cpu/AdvIRL/tree/MultiView-Clean%7D">https://github.com/Tommy-Nguyen-cpu/AdvIRL/tree/MultiView-Clean}</a>, ensuring reproducibility and facilitating future research. </p>
<blockquote>
<p>随着人工智能模型在关键应用中的部署越来越多，它们遭受敌对攻击的风险也显著上升。虽然二维视觉模型中的敌对脆弱性已经得到了广泛的研究，但针对三维生成模型（如神经辐射场（NeRF））的威胁态势仍被低估。本研究引入了名为AdvIRL的新框架，该框架使用即时神经图形原始（Instant-NGP）和强化学习来制作敌对NeRF模型。与先前的方法不同，AdvIRL生成的敌对噪声在多种三维变换（包括旋转和缩放）下保持稳健，从而在现实场景中实现了有效的黑盒攻击。我们的方法经过各种场景的验证，从小型物体（例如香蕉）到大型环境（例如灯塔）都有涵盖。值得注意的是，有针对性的攻击实现了高置信度的误分类，例如将香蕉标记为蛞蝓，将卡车标记为大炮，这显示了敌对NeRFs带来的实际风险。除了攻击之外，AdvIRL生成的敌对模型还可以作为对抗训练数据，以增强视觉系统的稳健性。AdvIRL的实现可在<a target="_blank" rel="noopener" href="https://github.com/Tommy-Nguyen-cpu/AdvIRL/tree/MultiView-Clean">https://github.com/Tommy-Nguyen-cpu/AdvIRL/tree/MultiView-Clean</a>公开访问，以确保可重复性并促进未来研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16213v1">PDF</a> Accepted to The AAAI-25 Workshop on Artificial Intelligence for Cyber   Security (AICS)</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了针对三维生成模型（如神经辐射场NeRF）对抗性攻击的威胁。文章提出了一种名为AdvIRL的新框架，用于创建对抗性NeRF模型，使用即时神经图形原语（Instant-NGP）和强化学习。AdvIRL生成能够在多种三维变换（包括旋转和缩放）下保持稳定的对抗噪声，可实现现实世界场景中的有效黑盒攻击。该方法在不同场景（从小型物体到大型环境）中得到了验证，并且有针对性的攻击取得了高置信度的误分类结果。除了攻击功能外，AdvIRL生成的对抗模型还可以用作对抗训练数据，以提高视觉系统的鲁棒性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对抗性攻击对三维生成模型（如NeRF）构成重大风险，而针对这些模型的威胁景观仍被低估。</li>
<li>AdvIRL框架是一种针对NeRF模型的新型对抗攻击方法，结合了Instant-NGP和强化学习。</li>
<li>AdvIRL能够在多种三维变换下生成稳健的对抗噪声，实现有效的黑盒攻击。</li>
<li>AdvIRL在多种场景中进行了验证，包括小型物体和大型环境，并展示了高置信度的误分类结果。</li>
<li>AdvIRL不仅可用于攻击，还可生成对抗模型作为训练数据，提高视觉系统的鲁棒性。</li>
<li>AdvIRL的实施公开可用，确保了可重复性和未来研究的便利。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16213">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2bc2a81951d1a7b425715d60f8782987.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84e1102cad5f8a94e7c9c52d5d108c69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adc2dbc6819059bef9cf038443cab289.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d86a2e481946ffd7812845f53960301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bef4682373ee8a7bb9a8e1554a5df6e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Sequence-Matters-Harnessing-Video-Models-in-3D-Super-Resolution"><a href="#Sequence-Matters-Harnessing-Video-Models-in-3D-Super-Resolution" class="headerlink" title="Sequence Matters: Harnessing Video Models in 3D Super-Resolution"></a>Sequence Matters: Harnessing Video Models in 3D Super-Resolution</h2><p><strong>Authors:Hyun-kyu Ko, Dongheok Park, Youngin Park, Byeonghyeon Lee, Juhee Han, Eunbyung Park</strong></p>
<p>3D super-resolution aims to reconstruct high-fidelity 3D models from low-resolution (LR) multi-view images. Early studies primarily focused on single-image super-resolution (SISR) models to upsample LR images into high-resolution images. However, these methods often lack view consistency because they operate independently on each image. Although various post-processing techniques have been extensively explored to mitigate these inconsistencies, they have yet to fully resolve the issues. In this paper, we perform a comprehensive study of 3D super-resolution by leveraging video super-resolution (VSR) models. By utilizing VSR models, we ensure a higher degree of spatial consistency and can reference surrounding spatial information, leading to more accurate and detailed reconstructions. Our findings reveal that VSR models can perform remarkably well even on sequences that lack precise spatial alignment. Given this observation, we propose a simple yet practical approach to align LR images without involving fine-tuning or generating ‘smooth’ trajectory from the trained 3D models over LR images. The experimental results show that the surprisingly simple algorithms can achieve the state-of-the-art results of 3D super-resolution tasks on standard benchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets. Project page: <a target="_blank" rel="noopener" href="https://ko-lani.github.io/Sequence-Matters">https://ko-lani.github.io/Sequence-Matters</a> </p>
<blockquote>
<p>三维超分辨率旨在从低分辨率（LR）多视角图像重建高保真三维模型。早期研究主要集中在单图像超分辨率（SISR）模型上，将LR图像上采样为高分辨率图像。然而，这些方法通常缺乏视角一致性，因为它们独立地处理每张图像。尽管已经广泛探索了各种后处理技术来缓解这些不一致性，但它们尚未完全解决这些问题。</p>
</blockquote>
<p>在本文中，我们通过对利用视频超分辨率（VSR）模型的3D超分辨率进行深入研究。通过利用VSR模型，我们确保了更高的空间一致性，并且可以引用周围的空间信息，从而导致更精确和详细的重建。我们的研究结果表明，即使在缺乏精确空间对齐的序列上，VSR模型也可以表现出非常出色的性能。鉴于此观察结果，我们提出了一种简单而实用的方法来对齐LR图像，而无需进行微调或从训练的3D模型中对LR图像生成“平滑”轨迹。实验结果表明，这些出人意料的简单算法可以在标准基准数据集（如NeRF-synthetic和MipNeRF-360数据集）上实现3D超分辨率任务的最新结果。项目页面：<a target="_blank" rel="noopener" href="https://ko-lani.github.io/Sequence-Matters">https://ko-lani.github.io/Sequence-Matters</a></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11525v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ko-lani.github.io/Sequence-Matters">https://ko-lani.github.io/Sequence-Matters</a></p>
<p><strong>摘要</strong><br>本文探讨了利用视频超分辨率（VSR）模型进行3D超分辨率重建的方法。通过利用VSR模型，研究确保了更高的空间一致性，并能引用周围的空间信息，从而实现了更准确和详细的重建。研究发现在缺乏精确空间对齐的序列上，VSR模型也能表现出色。提出了一种简单实用的方法，无需微调或生成基于训练好的三维模型的平滑轨迹，就能实现对低分辨率图像的对齐。实验结果表明，这种出人意料的简单算法在标准数据集上实现了三维超分辨率任务的最新结果。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究采用了视频超分辨率（VSR）模型进行3D超分辨率重建，保证了更高的空间一致性。</li>
<li>通过引用周围的空间信息，VSR模型能提供更准确和详细的重建结果。</li>
<li>VSR模型在缺乏精确空间对齐的序列上也能展现出卓越性能。</li>
<li>提出了一种简单实用的低分辨率图像对齐方法，无需复杂操作如微调或生成平滑轨迹。</li>
<li>该方法能够在标准数据集上实现出色的三维超分辨率重建结果。</li>
<li>研究采用了广泛的实验验证，证明了该方法在标准数据集上的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11525">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fe1ceb1e5fc3d8fbc7990d43635b2806.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-063098e15fa97620c3146ce5e00f76b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-707173a3a7fc763b3069420ce0526011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d5bd673ec44e72153b94659a75c7904.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3381673e405e5b6c7f8b507bcacd8eaf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="WavePlanes-Compact-Hex-Planes-for-Dynamic-Novel-View-Synthesis"><a href="#WavePlanes-Compact-Hex-Planes-for-Dynamic-Novel-View-Synthesis" class="headerlink" title="WavePlanes: Compact Hex Planes for Dynamic Novel View Synthesis"></a>WavePlanes: Compact Hex Planes for Dynamic Novel View Synthesis</h2><p><strong>Authors:Adrian Azzarelli, Nantheera Anantrasirichai, David R Bull</strong></p>
<p>Dynamic Novel View Synthesis (Dynamic NVS) enhances NVS technologies to model moving 3-D scenes. However, current methods are resource intensive and challenging to compress. To address this, we present WavePlanes, a fast and more compact hex plane representation, applicable to both Neural Radiance Fields and Gaussian Splatting methods. Rather than modeling many feature scales separately (as done previously), we use the inverse discrete wavelet transform to reconstruct features at varying scales. This leads to a more compact representation and allows us to explore wavelet-based compression schemes for further gains. The proposed compression scheme exploits the sparsity of wavelet coefficients, by applying hard thresholding to the wavelet planes and storing nonzero coefficients and their locations on each plane in a Hash Map. Compared to the state-of-the-art (SotA), WavePlanes is significantly smaller, less resource demanding and competitive in reconstruction quality. Compared to small SotA models, WavePlanes outperforms methods in both model size and quality of novel views. </p>
<blockquote>
<p>动态Novel View Synthesis（Dynamic NVS）技术增强了NVS技术，以模拟移动的3D场景。然而，当前的方法资源密集且压缩具有挑战性。为了解决这一问题，我们提出了WavePlanes，这是一种快速且更紧凑的六平面表示方法，适用于神经辐射场和高斯喷涂方法。我们并不单独建模许多特征尺度（如以前所做的那样），而是使用逆离散小波变换来重建不同尺度的特征。这导致了更紧凑的表示形式，并允许我们进一步探索基于小波压缩方案以获得更多收益。所提出的压缩方案通过应用硬阈值处理小波平面并利用哈希映射存储每个平面上的非零系数及其位置来利用小波系数的稀疏性。与最新技术相比，WavePlanes体积更小、资源消耗更低并且在重建质量方面具有很强的竞争力。与小型最新技术模型相比，WavePlanes在模型大小和新颖视角的质量方面都表现出卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02218v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>动态场景的新型视图合成技术（Dynamic NVS）对NVS技术进行了增强，可以建模移动的3D场景。然而，当前的方法资源消耗大且压缩困难。为解决这一问题，我们提出了WavePlanes，这是一种快速且更紧凑的六平面表示法，适用于神经网络辐射场和高斯溅射方法。我们利用逆离散小波变换来重建不同尺度的特征，而不是像过去那样分别建模许多特征尺度。这导致了更紧凑的表示形式，并允许我们进一步探索基于小波压缩方案以获得更多收益。所提出的压缩方案利用小波系数的稀疏性，通过对小波平面应用硬阈值处理并存储每个平面上的非零系数及其位置哈希映射来实现压缩。与最新技术相比，WavePlanes体积更小、资源消耗更低、重建质量更具竞争力。相较于小型最新技术模型，WavePlanes在模型大小和新视图的质量方面都表现出优越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动态场景的新型视图合成技术（Dynamic NVS）增强了NVS技术，使其能够建模移动的3D场景。</li>
<li>当前方法存在资源消耗大且压缩困难的问题。</li>
<li>WavePlanes是一种适用于神经网络辐射场和高斯溅射方法的快速且更紧凑的六平面表示法。</li>
<li>利用逆离散小波变换进行特征重建，实现了更紧凑的模型表示。</li>
<li>提出了基于小波压缩方案的压缩策略，利用小波系数的稀疏性进行硬阈值处理。</li>
<li>与现有技术相比，WavePlanes具有更小的体积、更低的资源消耗和更具竞争力的重建质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.02218">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7173d381f3f168972b0e17b820a77066.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-057c14a7b87f9b7beda558e34f75f6b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00e6963b44a714894d16b37492c972e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0796692f66ff58ea127b82f5711bf19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7676642074a4bb3e44d5abf80728104c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80a24dbac7b953ff7aacba1b0d035bd9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-afa28c3d81953ef5271254f685ce67f1.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-12-25  FaceLift Single Image to 3D Head with View Generation and GS-LRM
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c4bd39fe1382d5c5937f6b8f92557747.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2024-12-25  FaceLift Single Image to 3D Head with View Generation and GS-LRM
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">15437.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
