<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2024-12-25  VERSA A Versatile Evaluation Toolkit for Speech, Audio, and Music">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-46660c58a4177a91d3aa3f203cdd12d5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-25-更新"><a href="#2024-12-25-更新" class="headerlink" title="2024-12-25 更新"></a>2024-12-25 更新</h1><h2 id="VERSA-A-Versatile-Evaluation-Toolkit-for-Speech-Audio-and-Music"><a href="#VERSA-A-Versatile-Evaluation-Toolkit-for-Speech-Audio-and-Music" class="headerlink" title="VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music"></a>VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music</h2><p><strong>Authors:Jiatong Shi, Hye-jin Shim, Jinchuan Tian, Siddhant Arora, Haibin Wu, Darius Petermann, Jia Qi Yip, You Zhang, Yuxun Tang, Wangyou Zhang, Dareen Safar Alharthi, Yichen Huang, Koichi Saito, Jionghao Han, Yiwen Zhao, Chris Donahue, Shinji Watanabe</strong></p>
<p>In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals. The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient. With full installation, VERSA offers 63 metrics with 711 metric variations based on different configurations. These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions. As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios. To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation. The toolkit is available at <a target="_blank" rel="noopener" href="https://github.com/shinjiwlab/versa">https://github.com/shinjiwlab/versa</a>. </p>
<blockquote>
<p>在这项工作中，我们介绍了VERSA，这是一个统一标准化的评估工具包，用于各种语音、音频和音乐信号。该工具包具有Python风格的接口，具有灵活的配置和依赖控制，使其友好高效。在完全安装后，VERSA提供基于不同配置的63个指标，共计有711种指标变化。这些指标包括利用各种外部资源的评估，包括匹配和非匹配的参考音频、文本转录和文本描述。作为一个轻便而全面的工具包，VERSA能够支持对各种下游场景的评估。为了展示其能力，这项工作突出了VERSA的一些用例示例，包括音频编码、语音合成、语音增强、歌唱合成和音乐生成。该工具包可在<a target="_blank" rel="noopener" href="https://github.com/shinjiwlab/versa%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/shinjiwlab/versa找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17667v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了VERSA这一统一标准化的评估工具包，适用于各种语音、音频和音乐信号的评估。它具备Pythonic接口，配置灵活，依赖控制性强，使用方便且效率高。VERSA提供63种度量指标，基于不同配置有711种度量指标变化。它能利用包括匹配和非匹配参考音频、文本转录和文本字幕在内的外部资源进行评价。作为轻便而全面的工具包，VERSA支持各种下游场景的评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VERSA是一个统一标准化的评估工具包，适用于语音、音频和音乐信号的评估。</li>
<li>VERSA具备Pythonic接口，具有灵活的配置和依赖控制。</li>
<li>VERSA提供多种度量指标，可根据不同配置产生不同的度量指标变化。</li>
<li>VERSA能利用外部资源进行评价，包括匹配和非匹配参考音频、文本转录和文本字幕。</li>
<li>VERSA支持多种下游场景的评估，如音频编码、语音合成、语音增强、歌唱合成和音乐生成等。</li>
<li>VERSA工具包已公开发布，可供公众使用。</li>
<li>VERSA工具包网址为<a target="_blank" rel="noopener" href="https://github.com/shinjiwlab/versa%E3%80%82">https://github.com/shinjiwlab/versa。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ca5810467e21064aad7ef23ae592e12a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f43ca7b05cd368a4682ed95008e06032.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-105333784beee2cd2b863f3f3fd741b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00d391b42d6acbe6ed460f7bb19e84c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-081514bb790a0108b7e0dde5a0b7d40d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Incremental-Disentanglement-for-Environment-Aware-Zero-Shot-Text-to-Speech-Synthesis"><a href="#Incremental-Disentanglement-for-Environment-Aware-Zero-Shot-Text-to-Speech-Synthesis" class="headerlink" title="Incremental Disentanglement for Environment-Aware Zero-Shot   Text-to-Speech Synthesis"></a>Incremental Disentanglement for Environment-Aware Zero-Shot   Text-to-Speech Synthesis</h2><p><strong>Authors:Ye-Xin Lu, Hui-Peng Du, Zheng-Yan Sheng, Yang Ai, Zhen-Hua Ling</strong></p>
<p>This paper proposes an Incremental Disentanglement-based Environment-Aware zero-shot text-to-speech (TTS) method, dubbed IDEA-TTS, that can synthesize speech for unseen speakers while preserving the acoustic characteristics of a given environment reference speech. IDEA-TTS adopts VITS as the TTS backbone. To effectively disentangle the environment, speaker, and text factors, we propose an incremental disentanglement process, where an environment estimator is designed to first decompose the environmental spectrogram into an environment mask and an enhanced spectrogram. The environment mask is then processed by an environment encoder to extract environment embeddings, while the enhanced spectrogram facilitates the subsequent disentanglement of the speaker and text factors with the condition of the speaker embeddings, which are extracted from the environmental speech using a pretrained environment-robust speaker encoder. Finally, both the speaker and environment embeddings are conditioned into the decoder for environment-aware speech generation. Experimental results demonstrate that IDEA-TTS achieves superior performance in the environment-aware TTS task, excelling in speech quality, speaker similarity, and environmental similarity. Additionally, IDEA-TTS is also capable of the acoustic environment conversion task and achieves state-of-the-art performance. </p>
<blockquote>
<p>本文提出了一种基于增量解耦的环境感知零样本文本到语音（TTS）方法，称为IDEA-TTS。该方法可以在未见过说话人的情况下合成语音，同时保留给定环境参考语音的声学特征。IDEA-TTS采用VITS作为TTS的骨干。为了有效地解开环境、说话人和文本因素，我们提出了一个增量解耦过程，其中设计了一个环境估计器，首先将环境频谱图分解为一个环境掩码和一个增强频谱图。然后，环境掩码被环境编码器处理以提取环境嵌入，而增强频谱图有助于在说话人嵌入的条件下解开说话人和文本因素，这些说话人嵌入是从环境语音中使用预训练的环境鲁棒说话人编码器提取的。最后，说话人和环境嵌入都被输入到解码器中进行环境感知的语音生成。实验结果表明，IDEA-TTS在环境感知TTS任务中取得了优越的性能，在语音质量、说话人相似度和环境相似度方面表现出色。此外，IDEA-TTS还具备声音环境转换任务的能力，并达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16977v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>该论文提出了一种基于增量解耦的环境感知零样本文本转语音（TTS）方法，名为IDEA-TTS。该方法能够合成未见过的说话人的语音，同时保留给定环境参考语音的声学特征。IDEA-TTS采用VITS作为TTS骨架，并通过增量解耦过程有效地解开环境、说话人和文本因素。首先，环境估计器将环境频谱图分解为环境掩码和增强频谱图。环境掩码经环境编码器处理提取环境嵌入，同时增强频谱图在说话人嵌入的条件下促进说话人和文本因素的后续解耦。说话人嵌入是从环境语音中使用预训练的环境鲁棒说话人编码器提取的。最后，将说话人和环境嵌入作为条件输入到解码器中，以进行环境感知的语音生成。实验结果表明，IDEA-TTS在环境感知TTS任务上取得卓越性能，尤其在语音质量、说话人相似性和环境相似性方面。此外，IDEA-TTS还具备声音环境转换任务的能力，并达到最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IDEA-TTS是一种环境感知的零样本文本转语音（TTS）方法，能合成未见过的说话人的语音，同时保留环境参考语音的声学特征。</li>
<li>IDEA-TTS采用增量解耦过程，有效解开环境、说话人和文本因素。</li>
<li>环境估计器能分解环境频谱图，生成环境掩码和增强频谱图。</li>
<li>环境编码器和说话人编码器分别提取环境嵌入和说话人嵌入。</li>
<li>IDEA-TTS将说话人和环境嵌入作为条件输入到解码器，进行环境感知的语音生成。</li>
<li>实验表明，IDEA-TTS在环境感知TTS任务上表现卓越，尤其在语音质量、说话人相似性、环境相似性方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16977">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-281a5d9690211cc4f718b2a9083136cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9eb575ca2c3c0ececfdc76e7e109d20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8437713026e3aa8d147083e65ab6fb41.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Autoregressive-Speech-Synthesis-with-Next-Distribution-Prediction"><a href="#Autoregressive-Speech-Synthesis-with-Next-Distribution-Prediction" class="headerlink" title="Autoregressive Speech Synthesis with Next-Distribution Prediction"></a>Autoregressive Speech Synthesis with Next-Distribution Prediction</h2><p><strong>Authors:Xinfa Zhu, Wenjie Tian, Lei Xie</strong></p>
<p>We introduce KALL-E, a novel autoregressive (AR) language modeling approach with next-distribution prediction for text-to-speech (TTS) synthesis. Unlike existing methods, KALL-E directly models and predicts the continuous speech distribution conditioned on text without relying on VAE- or diffusion-based components. Specifically, we use WaveVAE to extract continuous speech distributions from waveforms instead of using discrete speech tokens. A single AR language model predicts these continuous speech distributions from text, with a Kullback-Leibler divergence loss as the constraint. Experimental results show that KALL-E outperforms open-source implementations of YourTTS, VALL-E, NaturalSpeech 2, and CosyVoice in terms of naturalness and speaker similarity in zero-shot TTS scenarios. Moreover, KALL-E demonstrates exceptional zero-shot capabilities in emotion and accent cloning. Importantly, KALL-E presents a more straightforward and effective paradigm for using continuous speech representations in TTS. Audio samples are available at: \url{<a target="_blank" rel="noopener" href="https://zxf-icpc.github.io/kalle/%7D">https://zxf-icpc.github.io/kalle/}</a>. </p>
<blockquote>
<p>我们介绍了KALL-E，这是一种新型的自回归（AR）语言建模方法，具有基于文本到语音（TTS）合成的下一个分布预测功能。与现有方法不同，KALL-E直接对文本条件下的连续语音分布进行建模和预测，无需依赖VAE或基于扩散的组件。具体来说，我们使用WaveVAE从波形中提取连续语音分布，而不是使用离散语音标记。一个单一的AR语言模型根据文本预测这些连续的语音分布，以Kullback-Leibler散度损失作为约束。实验结果表明，在零样本TTS场景中，KALL-E在自然度和说话人相似性方面优于YourTTS、VALL-E、NaturalSpeech 2和CosyVoice的开源实现。此外，KALL-E在情感和口音克隆方面表现出出色的零样本能力。重要的是，KALL-E为在TTS中使用连续语音表示提供了更简单有效的范式。音频样本可在：[<a target="_blank" rel="noopener" href="https://zxf-icpc.github.io/kalle/]%E8%8E%B7%E5%8F%96%E3%80%82">https://zxf-icpc.github.io/kalle/]获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16846v1">PDF</a> Technical report, work in progress</p>
<p><strong>Summary</strong><br>     本文介绍了KALL-E，一种新型的用于文本转语音（TTS）合成的自回归（AR）语言建模方法。该方法直接对文本条件下的连续语音分布进行建模和预测，无需依赖VAE或扩散模型。使用WaveVAE从波形中提取连续语音分布，并由单一AR语言模型预测这些分布，以Kullback-Leibler散度损失作为约束。实验结果显示，KALL-E在自然度和说话人相似性方面优于其他开源TTS实现，并展现出卓越的零样本情感和口音模仿能力。此外，KALL-E为TTS中使用连续语音表示提供了更简洁有效的范式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KALL-E是一种新型的文本转语音（TTS）合成方法，采用自回归（AR）语言建模技术。</li>
<li>与传统方法不同，KALL-E直接预测文本条件下的连续语音分布，无需依赖VAE或扩散模型。</li>
<li>KALL-E使用WaveVAE从波形中提取连续语音分布。</li>
<li>KALL-E在自然度和说话人相似性方面表现出优异性能，优于其他开源TTS实现。</li>
<li>KALL-E展现出零样本情感和口音模仿能力。</li>
<li>KALL-E为TTS中的连续语音表示提供了简洁有效的范式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16846">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe269f8abdb949147d2793d77a5f4ee1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d21003a0e48ef2fa88626ea3b3c0f63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdd91cd9027a0ffae81fcec129b8cdb3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25e0b9d9457f25b72ec15db514170b9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e679efd03e9fa4af064d827be73db8d8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Interleaved-Speech-Text-Language-Models-are-Simple-Streaming-Text-to-Speech-Synthesizers"><a href="#Interleaved-Speech-Text-Language-Models-are-Simple-Streaming-Text-to-Speech-Synthesizers" class="headerlink" title="Interleaved Speech-Text Language Models are Simple Streaming Text to   Speech Synthesizers"></a>Interleaved Speech-Text Language Models are Simple Streaming Text to   Speech Synthesizers</h2><p><strong>Authors:Yifan Yang, Ziyang Ma, Shujie Liu, Jinyu Li, Hui Wang, Lingwei Meng, Haiyang Sun, Yuzhe Liang, Ruiyang Xu, Yuxuan Hu, Yan Lu, Rui Zhao, Xie Chen</strong></p>
<p>This paper introduces Interleaved Speech-Text Language Model (IST-LM) for streaming zero-shot Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts in duration prediction and grapheme-to-phoneme alignment. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system without complicated engineering optimization, which has a limited gap with the non-streaming system. IST-LM is conceptually simple and empirically powerful, paving the way for streaming TTS with minimal overhead while largely maintaining performance, showcasing broad prospects coupled with real-time text stream from LLMs. </p>
<blockquote>
<p>本文介绍了用于流式零点击文本转语音（TTS）的交织语音文本语言模型（IST-LM）。不同于许多之前的方法，IST-LM直接在固定比例的交织文本和语音令牌序列上进行训练，无需在持续时间预测和字母到音素的对齐方面付出额外的努力。文本块大小与语音块大小的比例对IST-LM的性能至关重要。为了探究这一点，我们对训练数据进行了全面的统计分析，并与最终性能进行了相关性分析，发现了几个关键因素：1）语音令牌与其对应文本令牌之间的距离；2）每个语音令牌可访问的未来文本令牌的数量；3）语音令牌先于其对应该文本令牌出现的频率。实验结果展示了如何构建一个无需复杂工程优化的最佳流式TTS系统，其与非流式系统的差距有限。IST-LM概念简单，经验强大，为流式TTS铺平了道路，在保持性能的同时实现了最小的额外开销，展示了与来自大型语言模型的实时文本流相结合的广阔前景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16102v2">PDF</a> Submitted to ICME 2025</p>
<p><strong>Summary</strong></p>
<p>本论文介绍了用于流式零基础Text-to-Speech（TTS）的交织语音文本语言模型（IST-LM）。IST-LM直接训练交织序列的文本和语音标记，通过固定比例消除对持续时间预测和字母到音素对齐的额外需求。文本块大小与语音块大小的比例对IST-LM的性能至关重要。通过一系列统计分析和与最终性能的相关性分析，发现了影响性能的关键因素，包括语音标记与其对应文本标记之间的距离、每个语音标记可访问的未来文本标记的数量以及语音标记的频率先于它们的对应文本标记。实验结果证明了实现最佳流式TTS系统的可能性，无需复杂的工程优化，与非流式系统之间的差距有限。IST-LM概念简单，经验强大，为流式TTS提供了广阔的前景，具有实时文本流的能力，同时性能损失较小。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IST-LM模型可直接训练交织序列的文本和语音标记，通过固定比例进行训练，简化了流程。</li>
<li>文本块与语音块大小的比例对IST-LM模型性能至关重要。</li>
<li>通过统计分析发现影响IST-LM性能的关键因素包括语音和文本标记之间的距离、未来文本标记的可访问数量以及语音标记超前于文本标记的频率。</li>
<li>实验结果证明了实现流式TTS系统的可能性，无需复杂的工程优化。</li>
<li>IST-LM模型与非流式系统之间的性能差距有限。</li>
<li>IST-LM模型概念简单且经验强大，为流式TTS提供了广阔的应用前景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16102">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-528ded899ca23c84cd30cf5769c9df27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-835525750dfd2222025634d9b5a5b476.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37a239013f7ea19152afe4629b276fb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53c91f1d9dcb6c44fe8e3e361bf6e252.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multi-Source-Spatial-Knowledge-Understanding-for-Immersive-Visual-Text-to-Speech"><a href="#Multi-Source-Spatial-Knowledge-Understanding-for-Immersive-Visual-Text-to-Speech" class="headerlink" title="Multi-Source Spatial Knowledge Understanding for Immersive Visual   Text-to-Speech"></a>Multi-Source Spatial Knowledge Understanding for Immersive Visual   Text-to-Speech</h2><p><strong>Authors:Shuwei He, Rui Liu</strong></p>
<p>Visual Text-to-Speech (VTTS) aims to take the environmental image as the prompt to synthesize reverberant speech for the spoken content. Previous works focus on the RGB modality for global environmental modeling, overlooking the potential of multi-source spatial knowledge like depth, speaker position, and environmental semantics. To address these issues, we propose a novel multi-source spatial knowledge understanding scheme for immersive VTTS, termed MS2KU-VTTS. Specifically, we first prioritize RGB image as the dominant source and consider depth image, speaker position knowledge from object detection, and Gemini-generated semantic captions as supplementary sources. Afterwards, we propose a serial interaction mechanism to effectively integrate both dominant and supplementary sources. The resulting multi-source knowledge is dynamically integrated based on the respective contributions of each source.This enriched interaction and integration of multi-source spatial knowledge guides the speech generation model, enhancing the immersive speech experience. Experimental results demonstrate that the MS$^2$KU-VTTS surpasses existing baselines in generating immersive speech. Demos and code are available at: <a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/MS2KU-VTTS">https://github.com/AI-S2-Lab/MS2KU-VTTS</a>. </p>
<blockquote>
<p>视觉文本到语音（VTTS）旨在以环境图像为提示，合成回响的语音内容。以前的工作主要关注RGB模式进行全局环境建模，忽略了深度、说话者位置和环境语义等多源空间知识的潜力。为了解决这些问题，我们提出了一种用于沉浸式VTTS的多源空间知识理解方案，称为MS2KU-VTTS。具体来说，我们首先以RGB图像作为主要来源，并将深度图像、来自对象检测的说话者位置知识以及Gemini生成的语义字幕作为辅助来源。然后，我们提出了一种串行交互机制，以有效地整合主要和辅助来源。最终的多源知识是基于每个源的各自贡献动态地集成的。这种多源空间知识的丰富交互和整合指导语音生成模型，增强沉浸式的语音体验。实验结果表明，MS$^2$KU-VTTS在生成沉浸式语音方面超过了现有基线。演示和代码可在：<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/MS2KU-VTTS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AI-S2-Lab/MS2KU-VTTS获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14101v2">PDF</a> 5 pages, 1 figure, Accepted by ICASSP’2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了视觉文本语音转换（VTTS）的目标，并指出以往的研究主要关注RGB模态的全局环境建模，忽视了深度、说话人位置和环境语义等多源空间知识的潜力。为解决这些问题，本文提出了一种名为MS^2KU-VTTS的多源空间知识理解方案，该方案以RGB图像为主要来源，同时考虑深度图像、说话人位置知识和Gemini生成的语义字幕等辅助来源。通过串行交互机制有效地整合了主要和辅助来源，基于各来源的贡献动态整合多源知识。这种多源空间知识的丰富交互和整合，提高了语音生成模型的指导效果，增强了沉浸式语音体验。实验结果表明，MS^2KU-VTTS在生成沉浸式语音方面超越了现有基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VTTS旨在根据环境图像合成回声语音。</li>
<li>以往研究主要关注RGB模态的环境建模，忽视了多源空间知识的重要性。</li>
<li>MS^2KU-VTTS方案提出以RGB图像为主要来源，并结合深度图像、说话人位置知识和语义字幕等辅助来源。</li>
<li>通过串行交互机制整合主要和辅助来源，实现多源知识的动态整合。</li>
<li>多源空间知识的丰富交互和整合增强了语音生成模型的指导效果。</li>
<li>MS^2KU-VTTS在生成沉浸式语音方面超越了现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14101">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ee089c9ec8bc48e5c25843602bb9fc8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-540d0f9217aeacd256de2408315bcfc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e31971a3fc3fb4ba42d0e51ee726c39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f68267381a7a6bcaa435ccbd94eb08c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="NanoVoice-Efficient-Speaker-Adaptive-Text-to-Speech-for-Multiple-Speakers"><a href="#NanoVoice-Efficient-Speaker-Adaptive-Text-to-Speech-for-Multiple-Speakers" class="headerlink" title="NanoVoice: Efficient Speaker-Adaptive Text-to-Speech for Multiple   Speakers"></a>NanoVoice: Efficient Speaker-Adaptive Text-to-Speech for Multiple   Speakers</h2><p><strong>Authors:Nohil Park, Heeseung Kim, Che Hyun Lee, Jooyoung Choi, Jiheum Yeom, Sungroh Yoon</strong></p>
<p>We present NanoVoice, a personalized text-to-speech model that efficiently constructs voice adapters for multiple speakers simultaneously. NanoVoice introduces a batch-wise speaker adaptation technique capable of fine-tuning multiple references in parallel, significantly reducing training time. Beyond building separate adapters for each speaker, we also propose a parameter sharing technique that reduces the number of parameters used for speaker adaptation. By incorporating a novel trainable scale matrix, NanoVoice mitigates potential performance degradation during parameter sharing. NanoVoice achieves performance comparable to the baselines, while training 4 times faster and using 45 percent fewer parameters for speaker adaptation with 40 reference voices. Extensive ablation studies and analysis further validate the efficiency of our model. </p>
<blockquote>
<p>我们提出了NanoVoice，这是一种个性化的文本到语音模型，能够高效地同时为多个说话者构建语音适配器。NanoVoice引入了一种批处理说话者自适应技术，能够并行微调多个参考，从而显著减少训练时间。除了为每个说话者构建单独的适配器外，我们还提出了一种参数共享技术，以减少用于说话者自适应的参数数量。通过引入一个新型的可训练比例矩阵，NanoVoice缓解了参数共享期间可能出现的性能下降问题。NanoVoice的性能与基线相当，同时训练速度是基线的4倍，使用参数进行说话者自适应时减少了45%。广泛的消融研究和分析进一步验证了我们的模型效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15760v2">PDF</a> IEEE International Conference on Acoustics, Speech, and Signal   Processing (ICASSP), 2025, Demo Page: <a target="_blank" rel="noopener" href="https://nanovoice.github.io/">https://nanovoice.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>NanoVoice是一个个性化的文本到语音模型，可高效地同时为多个说话者构建语音适配器。它引入了一种批处理说话者适应技术，能够并行微调多个参考项，从而大大缩短训练时间。除了为每个说话者构建单独的适配器外，还提出了一种参数共享技术，减少了用于说话者适应的参数数量。通过引入新型的可训练比例矩阵，NanoVoice在参数共享时减轻了性能下降的潜在风险。NanoVoice的性能与基线相当，训练速度提高了4倍，使用参数进行说话者适应时减少了45%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NanoVoice是一个文本到语音模型，能同时为多个说话者构建语音适配器。</li>
<li>它采用批处理说话者适应技术，能并行微调多个参考项，提高训练效率。</li>
<li>NanoVoice提出参数共享技术，减少说话者适应所需的参数数量。</li>
<li>通过引入可训练比例矩阵，NanoVoice在参数共享时保持性能稳定。</li>
<li>NanoVoice的性能与基线相当，训练速度提升4倍，参数使用减少45%。</li>
<li>进行了广泛的消融研究和分析，进一步验证了模型的效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15760">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b9785a5a98711f10b8167a1c69c0266a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6d1f62781447f565b27d2b15c775630.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b58f850e7947ac5983ff7f0a0a990b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60eb50c57418bf7d0814c984ebe6bd4a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VoiceGuider-Enhancing-Out-of-Domain-Performance-in-Parameter-Efficient-Speaker-Adaptive-Text-to-Speech-via-Autoguidance"><a href="#VoiceGuider-Enhancing-Out-of-Domain-Performance-in-Parameter-Efficient-Speaker-Adaptive-Text-to-Speech-via-Autoguidance" class="headerlink" title="VoiceGuider: Enhancing Out-of-Domain Performance in Parameter-Efficient   Speaker-Adaptive Text-to-Speech via Autoguidance"></a>VoiceGuider: Enhancing Out-of-Domain Performance in Parameter-Efficient   Speaker-Adaptive Text-to-Speech via Autoguidance</h2><p><strong>Authors:Jiheum Yeom, Heeseung Kim, Jooyoung Choi, Che Hyun Lee, Nohil Park, Sungroh Yoon</strong></p>
<p>When applying parameter-efficient finetuning via LoRA onto speaker adaptive text-to-speech models, adaptation performance may decline compared to full-finetuned counterparts, especially for out-of-domain speakers. Here, we propose VoiceGuider, a parameter-efficient speaker adaptive text-to-speech system reinforced with autoguidance to enhance the speaker adaptation performance, reducing the gap against full-finetuned models. We carefully explore various ways of strengthening autoguidance, ultimately finding the optimal strategy. VoiceGuider as a result shows robust adaptation performance especially on extreme out-of-domain speech data. We provide audible samples in our demo page. </p>
<blockquote>
<p>当通过LoRA应用参数高效的微调至自适应说话人的文本到语音模型时，与全微调模型相比，自适应性能可能会下降，特别是对于非域内的说话人。针对这一问题，我们提出了VoiceGuider，这是一个通过自动指导增强的参数高效自适应文本到语音系统，以提高说话人自适应性能，缩小与全微调模型之间的差距。我们小心翼翼地探索了加强自动指导的各种方式，并找到了最佳策略。VoiceGuider的结果显示，其在极端非域语音数据上表现出稳健的自适应性能。我们在演示页面上提供了可听的样本。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15759v2">PDF</a> IEEE International Conference on Acoustics, Speech, and Signal   Processing (ICASSP), 2025, Demo Page: <a target="_blank" rel="noopener" href="https://voiceguider.github.io/">https://voiceguider.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>在采用LoRA进行参数有效微调以适配语音自适应文本到语音模型时，相比全微调的模型，适配性能可能会出现下降，特别是针对非域内的发言人。为此，我们提出了VoiceGuider，这是一个参数高效的语音自适应文本到语音系统，通过自动指导强化来提升语音适配性能，缩小与全微调模型的差距。我们深入探索了增强自动指导的各种方法，并找到了最佳策略。VoiceGuider在极端非域语音数据上展现出强大的适应性。我们在演示页面上提供了可听的样本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRA应用于语音自适应文本到语音模型的参数微调可能不如全微调模型效果好，特别是对于非域内的发言人。</li>
<li>VoiceGuider是一个参数高效的语音自适应文本到语音系统，旨在提高语音适配性能。</li>
<li>VoiceGuider通过强化自动指导来缩小与全微调模型的性能差距。</li>
<li>VoiceGuider在探索增强自动指导方法的过程中找到了最佳策略。</li>
<li>VoiceGuider在极端非域语音数据上展现出强大的适应性。</li>
<li>演示页面上提供了可听的样本，以便评估VoiceGuider的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15759">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9112fc85431af49e01e11faab161dbf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05889e4ac4ae9f23063e3d916d892b99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c7b2a9efa449e1ddaaad87e7454b04a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-016703487e861605fd3629e812d9d0fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5af3f8574589d06a6556533f9cd8bc9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Full-text-Error-Correction-for-Chinese-Speech-Recognition-with-Large-Language-Model"><a href="#Full-text-Error-Correction-for-Chinese-Speech-Recognition-with-Large-Language-Model" class="headerlink" title="Full-text Error Correction for Chinese Speech Recognition with Large   Language Model"></a>Full-text Error Correction for Chinese Speech Recognition with Large   Language Model</h2><p><strong>Authors:Zhiyuan Tang, Dong Wang, Shen Huang, Shidong Shang</strong></p>
<p>Large Language Models (LLMs) have demonstrated substantial potential for error correction in Automatic Speech Recognition (ASR). However, most research focuses on utterances from short-duration speech recordings, which are the predominant form of speech data for supervised ASR training. This paper investigates the effectiveness of LLMs for error correction in full-text generated by ASR systems from longer speech recordings, such as transcripts from podcasts, news broadcasts, and meetings. First, we develop a Chinese dataset for full-text error correction, named ChFT, utilizing a pipeline that involves text-to-speech synthesis, ASR, and error-correction pair extractor. This dataset enables us to correct errors across contexts, including both full-text and segment, and to address a broader range of error types, such as punctuation restoration and inverse text normalization, thus making the correction process comprehensive. Second, we fine-tune a pre-trained LLM on the constructed dataset using a diverse set of prompts and target formats, and evaluate its performance on full-text error correction. Specifically, we design prompts based on full-text and segment, considering various output formats, such as directly corrected text and JSON-based error-correction pairs. Through various test settings, including homogeneous, up-to-date, and hard test sets, we find that the fine-tuned LLMs perform well in the full-text setting with different prompts, each presenting its own strengths and weaknesses. This establishes a promising baseline for further research. The dataset is available on the website. </p>
<blockquote>
<p>大型语言模型（LLM）在自动语音识别（ASR）的错误纠正方面展现出了巨大的潜力。然而，大多数研究都集中在来自短期语音记录的片段上，这是有监督的ASR训练的主要形式。本文研究了LLM在由ASR系统从较长的语音记录生成的完整文本中的错误纠正效果，例如来自播客、新闻广播和会议的转录文本。首先，我们开发了一个用于全文错误纠正的中文数据集，名为ChFT，该数据集采用涉及文本到语音合成、ASR和错误校正对提取器的管道。该数据集使我们能够在不同语境中纠正错误，包括全文和片段，并处理更广泛的错误类型，如标点恢复和逆文本规范化，从而使校正过程更加全面。其次，我们在构建的数据集上对预训练的LLM进行了微调，使用了各种提示和目标格式，并评估了其在全文错误纠正方面的性能。具体来说，我们基于全文和片段设计提示，考虑各种输出格式，如直接校正的文本和基于JSON的错误校正对。通过包括同质、最新和困难测试集在内的各种测试环境，我们发现经过微调后的LLM在不同的提示下表现良好，各有其优势和劣势。这为未来的研究奠定了有前景的基准。数据集可在网站上获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.07790v2">PDF</a> ICASSP 2025</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）在自动语音识别（ASR）的错误校正方面显示出巨大潜力。然而，大多数研究都集中在来自短语音录音的语音片段上，这些是目前监督式ASR训练的主要形式。本文探讨了LLM在由ASR系统从较长语音录音（如播客、新闻广播和会议记录）生成的全文上的错误校正效果。首先，我们开发了一个用于全文错误校正的中文数据集ChFT，该数据集采用文本到语音合成、ASR和错误校正对提取器构成的管道实现。该数据集使我们能够在不同语境中纠正错误，包括全文和段落，并处理更广泛的错误类型，如标点恢复和反向文本归一化，从而使校正过程更加全面。其次，我们在构建的数据集上对预训练的LLM进行了微调，使用了各种提示和目标格式，并对全文错误校正的性能进行了评估。特别是，我们设计了基于全文和段落的提示，并考虑了各种输出格式，如直接纠正的文本和基于JSON的错误校正对。通过包括同质的、最新的和困难的测试集在内的各种测试设置，我们发现经过微调后的LLM在不同的提示下，在全文设置中表现良好，各有其优缺点。这为未来的研究提供了一个有希望的基准。数据集可在网站上获得。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型在自动语音识别中的错误校正方面表现出巨大潜力。</li>
<li>现有的研究主要关注短语音录音的语音片段，本文则专注于由ASR系统生成的全文错误校正。</li>
<li>开发了一个用于全文错误校正的中文数据集ChFT，该数据集能在不同语境中纠正错误并处理广泛的错误类型。</li>
<li>通过微调预训练的LLM和对不同提示及目标格式的使用，对LLM在全文错误校正中的性能进行了评估。</li>
<li>在多种测试设置下，发现经过微调后的LLM在全文设置中表现良好。</li>
<li>LLM的提示设计在全文和段落级别都有考虑，并考虑了多种输出格式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.07790">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2b65f4d824e1db9858fb68cb7a985861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26be500de3a31e2fa5336219bb82d568.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d53bac2df374ec0bdb946ce3369c3d6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6425d0d71f48f50bba28352e7a21696e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46660c58a4177a91d3aa3f203cdd12d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db5b0db0a3844add6c2b0a17f8e97838.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="vec2wav-2-0-Advancing-Voice-Conversion-via-Discrete-Token-Vocoders"><a href="#vec2wav-2-0-Advancing-Voice-Conversion-via-Discrete-Token-Vocoders" class="headerlink" title="vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders"></a>vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders</h2><p><strong>Authors:Yiwei Guo, Zhihan Li, Junjie Li, Chenpeng Du, Hankun Wang, Shuai Wang, Xie Chen, Kai Yu</strong></p>
<p>We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis. </p>
<blockquote>
<p>我们提出了一种新的语音离散令牌编解码器vec2wav 2.0，它改进了语音转换（VC）。我们使用语音自监督模型的离散令牌作为源语音的内容特征，并将VC视为提示性编解码任务。为了解决内容令牌中扬声器音色的损失问题，vec2wav 2.0利用WavLM特征提供强大的音色相关信息。提出了一种新型的自适应Snake激活函数，以更好地将音色融入波形重建过程。通过这种方式，vec2wav 2.0能够在给定不同的参考提示时学会适当地改变演讲者的音色。此外，不需要对vec2wav 2.0进行有监督数据的训练，就能使其有效地工作。实验结果表明，在任意到任意的VC中，vec2wav 2.0在音频质量和说话人相似性方面大大超过了所有其他基线。消融研究验证了所提出技术的影响。此外，vec2wav 2.0即使在仅使用单语语料库进行训练的情况下，也实现了具有竞争力的跨语言VC。因此，vec2wav 2.0表明，只需语音令牌编解码器即可操纵音色，从而推动VC和语音合成的前沿。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01995v3">PDF</a> 5 pages, 4 figures. Demo page:   <a target="_blank" rel="noopener" href="https://cantabile-kwok.github.io/vec2wav2/">https://cantabile-kwok.github.io/vec2wav2/</a></p>
<p><strong>Summary</strong></p>
<p>新一代语音离散令牌编码器vec2wav 2.0提出，将语音自监督模型的离散令牌作为源语音的内容特征，并将语音转换（VC）视为提示编码任务。为弥补内容令牌中演讲者音色的损失，vec2wav 2.0利用WavLM特征提供强烈的音色相关信息。提出一种新型自适应Snake激活函数，更好地将音色融入波形重建过程。因此，vec2wav 2.0能够在给定不同参考提示的情况下，学习适当地改变演讲者的音色。此外，训练vec2wav 2.0无需监督数据。实验结果表明，在任意到任意的语音转换中，vec2wav 2.0在音频质量和说话人相似性方面大大优于所有其他基线。消融研究证实了所提出技术的效果。而且，仅在单语语料库上训练的vec2wav 2.0实现了有竞争力的跨语言语音转换。因此，vec2wav 2.0显示了音色可能仅通过语音令牌编码器进行操作，推动了语音转换和语音合成的前沿。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>vec2wav 2.0是一种新的语音离散令牌vocoder，用于推进语音转换（VC）技术。</li>
<li>该方法使用语音自监督模型的离散令牌作为源语音的内容特征，并将VC视为提示编码任务。</li>
<li>WavLM特征被用来提供强烈的音色相关信息，以弥补内容令牌中演讲者音色的损失。</li>
<li>引入了一种新型自适应Snake激活函数，以更好地将音色融入波形重建过程。</li>
<li>vec2wav 2.0能在给定不同参考提示的情况下学习适当改变演讲者的音色。</li>
<li>该模型无需监督数据即可进行有效训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.01995">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-86dd9465920d3500042377129434fea7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2e0b8d17c174e3b7daaabd526766acb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa91c8e5dce637dbdb885a86d0e9ff10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5981cb1d57b1df06f385dff6e7f8f131.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1e402cfd0177540f4f3424d40e20680e.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2024-12-25  RF-GML Reference-Free Generative Machine Listener
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cc8da03525039b4ae3e9044e917243e7.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-25  FaceLift Single Image to 3D Head with View Generation and GS-LRM
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">11676k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
