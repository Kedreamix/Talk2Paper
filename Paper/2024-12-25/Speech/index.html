<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  Investigating Prosodic Signatures via Speech Pre-Trained Models for   Audio Deepfake Source Attribution">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8e31971a3fc3fb4ba42d0e51ee726c39.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    74 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-25-æ›´æ–°"><a href="#2024-12-25-æ›´æ–°" class="headerlink" title="2024-12-25 æ›´æ–°"></a>2024-12-25 æ›´æ–°</h1><h2 id="Investigating-Prosodic-Signatures-via-Speech-Pre-Trained-Models-for-Audio-Deepfake-Source-Attribution"><a href="#Investigating-Prosodic-Signatures-via-Speech-Pre-Trained-Models-for-Audio-Deepfake-Source-Attribution" class="headerlink" title="Investigating Prosodic Signatures via Speech Pre-Trained Models for   Audio Deepfake Source Attribution"></a>Investigating Prosodic Signatures via Speech Pre-Trained Models for   Audio Deepfake Source Attribution</h2><p><strong>Authors:Orchid Chetia Phukan, Drishti Singh, Swarup Ranjan Behera, Arun Balaji Buduru, Rajesh Sharma</strong></p>
<p>In this work, we investigate various state-of-the-art (SOTA) speech pre-trained models (PTMs) for their capability to capture prosodic signatures of the generative sources for audio deepfake source attribution (ADSD). These prosodic characteristics can be considered one of major signatures for ADSD, which is unique to each source. So better is the PTM at capturing prosodic signs better the ADSD performance. We consider various SOTA PTMs that have shown top performance in different prosodic tasks for our experiments on benchmark datasets, ASVSpoof 2019 and CFAD. x-vector (speaker recognition PTM) attains the highest performance in comparison to all the PTMs considered despite consisting lowest model parameters. This higher performance can be due to its speaker recognition pre-training that enables it for capturing unique prosodic characteristics of the sources in a better way. Further, motivated from tasks such as audio deepfake detection and speech recognition, where fusion of PTMs representations lead to improved performance, we explore the same and propose FINDER for effective fusion of such representations. With fusion of Whisper and x-vector representations through FINDER, we achieved the topmost performance in comparison to all the individual PTMs as well as baseline fusion techniques and attaining SOTA performance. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤šç§æœ€æ–°å‰æ²¿çš„è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPTMï¼‰ï¼Œä»¥è¯„ä¼°å®ƒä»¬åœ¨æ•è·éŸ³é¢‘æ·±åº¦ä¼ªé€ æºå½’å› ï¼ˆADSDï¼‰çš„ç”Ÿæˆæºè¯­çš„éŸµå¾‹ç‰¹å¾æ–¹é¢çš„èƒ½åŠ›ã€‚è¿™äº›éŸµå¾‹ç‰¹å¾å¯è§†ä¸ºADSDçš„ä¸»è¦ç‰¹å¾ä¹‹ä¸€ï¼Œæ¯ä¸ªæºéƒ½æœ‰å…¶ç‹¬ç‰¹æ€§ã€‚å› æ­¤ï¼ŒPTMåœ¨æ•æ‰éŸµå¾‹ç¬¦å·æ–¹é¢çš„èƒ½åŠ›è¶Šå¼ºï¼ŒADSDçš„æ€§èƒ½å°±è¶Šå¥½ã€‚æˆ‘ä»¬åœ¨åŸºå‡†æ•°æ®é›†ASVSpoof 2019å’ŒCFADä¸Šè¿›è¡Œäº†å®éªŒï¼Œè€ƒè™‘äº†åœ¨ä¸åŒéŸµå¾‹ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³çš„å¤šç§æœ€æ–°å‰æ²¿PTMã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œx-vectorï¼ˆè¯­éŸ³è¯†åˆ«PTMï¼‰å°½ç®¡æ¨¡å‹å‚æ•°æœ€å°‘ï¼Œä½†å–å¾—äº†æœ€é«˜æ€§èƒ½ã€‚è¿™ç§é«˜æ€§èƒ½å¯èƒ½æ˜¯ç”±äºå…¶è¯­éŸ³è¯†åˆ«çš„é¢„è®­ç»ƒåŠŸèƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æºçš„ç‹¬ç‰¹éŸµå¾‹ç‰¹å¾ã€‚æ­¤å¤–ï¼Œå—éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹å’Œè¯­éŸ³è¯†åˆ«ç­‰ä»»åŠ¡çš„å¯å‘ï¼ŒèåˆPTMè¡¨ç¤ºå¯ä»¥æé«˜æ€§èƒ½ï¼Œå› æ­¤æˆ‘ä»¬æ¢ç´¢äº†åŒæ ·çš„æ–¹æ³•ï¼Œå¹¶æå‡ºäº†FINDERï¼Œä»¥å®ç°æ­¤ç±»è¡¨ç¤ºçš„æœ‰æ•ˆèåˆã€‚é€šè¿‡FINDERèåˆWhisperå’Œx-vectorè¡¨ç¤ºï¼Œä¸æ‰€æœ‰å•ä¸ªPTMä»¥åŠåŸºçº¿èåˆæŠ€æœ¯ç›¸æ¯”ï¼Œæˆ‘ä»¬å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶è¾¾åˆ°äº†æœ€æ–°å‰æ²¿æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17796v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å…ˆè¿›çš„è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹åœ¨æ•æ‰éŸ³é¢‘æ·±åº¦ä¼ªé€ æºå½’å› çš„éŸµå¾‹ç‰¹å¾æ–¹é¢çš„èƒ½åŠ›ã€‚è¿™äº›éŸµå¾‹ç‰¹å¾æ˜¯æ¯ä¸ªæºçš„ç‹¬ç‰¹æ ‡å¿—ï¼Œå› æ­¤é¢„è®­ç»ƒæ¨¡å‹åœ¨æ•æ‰éŸµå¾‹ç‰¹å¾æ–¹é¢çš„èƒ½åŠ›è¶Šå¼ºï¼ŒADSDæ€§èƒ½è¶Šå¥½ã€‚æœ¬æ–‡è€ƒè™‘äº†å¤šä¸ªåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°çªå‡ºçš„å…ˆè¿›é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå®éªŒï¼Œå‘ç°x-vectorï¼ˆè¯­éŸ³è¯†åˆ«é¢„è®­ç»ƒæ¨¡å‹ï¼‰ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºå…¶è¯­éŸ³è¯†åˆ«é¢„è®­ç»ƒèƒ½æ›´å¥½åœ°æ•æ‰æºç‹¬ç‰¹éŸµå¾‹ç‰¹å¾æ‰€è‡´ã€‚æ­¤å¤–ï¼Œå—éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹å’Œè¯­éŸ³è¯†åˆ«ä»»åŠ¡çš„å¯å‘ï¼Œé€šè¿‡èåˆå¤šç§é¢„è®­ç»ƒæ¨¡å‹çš„è¡¨ç¤ºå½¢å¼å¯ä»¥æé«˜æ€§èƒ½ï¼Œå› æ­¤æœ¬æ–‡æå‡ºäº†FINDERè¿›è¡Œæœ‰æ•ˆèåˆã€‚é€šè¿‡èåˆWhisperå’Œx-vectorçš„è¡¨ç¤ºå½¢å¼ï¼Œç›¸è¾ƒäºæ‰€æœ‰å•ä¸ªé¢„è®­ç»ƒæ¨¡å‹å’ŒåŸºå‡†èåˆæŠ€æœ¯ï¼Œå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¾¾åˆ°äº†ç›®å‰æœ€å…ˆè¿›çš„æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†å…ˆè¿›è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹åœ¨æ•æ‰éŸ³é¢‘æ·±åº¦ä¼ªé€ æºå½’å› çš„éŸµå¾‹ç‰¹å¾æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>éŸµå¾‹ç‰¹å¾æ˜¯æ¯ä¸ªæºçš„ç‹¬ç‰¹æ ‡å¿—ï¼Œå¯¹ADSDæ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>x-vectoråœ¨å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œè¿™å¾—ç›Šäºå…¶è¯­éŸ³è¯†åˆ«é¢„è®­ç»ƒèƒ½æ›´å¥½åœ°æ•æ‰æºçš„ç‹¬ç‰¹éŸµå¾‹ç‰¹å¾ã€‚</li>
<li>èåˆå¤šç§é¢„è®­ç»ƒæ¨¡å‹çš„è¡¨ç¤ºå½¢å¼å¯ä»¥æé«˜æ€§èƒ½ï¼Œå—éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹å’Œè¯­éŸ³è¯†åˆ«ä»»åŠ¡çš„å¯å‘ã€‚</li>
<li>æå‡ºäº†FINDERæ–¹æ³•ï¼Œç”¨äºæœ‰æ•ˆèåˆé¢„è®­ç»ƒæ¨¡å‹çš„è¡¨ç¤ºå½¢å¼ã€‚</li>
<li>é€šè¿‡èåˆWhisperå’Œx-vectorçš„è¡¨ç¤ºå½¢å¼ï¼Œå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¾¾åˆ°äº†ç›®å‰æœ€å…ˆè¿›çš„æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17796">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-91bb710ffb8c415605251c34126a575e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f47304da8914c144cff7dd88be15b00e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7731638cea1b34495c60e8d9f565e9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c07d5af9d1f57da812826ea760980275.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VERSA-A-Versatile-Evaluation-Toolkit-for-Speech-Audio-and-Music"><a href="#VERSA-A-Versatile-Evaluation-Toolkit-for-Speech-Audio-and-Music" class="headerlink" title="VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music"></a>VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music</h2><p><strong>Authors:Jiatong Shi, Hye-jin Shim, Jinchuan Tian, Siddhant Arora, Haibin Wu, Darius Petermann, Jia Qi Yip, You Zhang, Yuxun Tang, Wangyou Zhang, Dareen Safar Alharthi, Yichen Huang, Koichi Saito, Jionghao Han, Yiwen Zhao, Chris Donahue, Shinji Watanabe</strong></p>
<p>In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals. The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient. With full installation, VERSA offers 63 metrics with 711 metric variations based on different configurations. These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions. As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios. To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation. The toolkit is available at <a target="_blank" rel="noopener" href="https://github.com/shinjiwlab/versa">https://github.com/shinjiwlab/versa</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VERSAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå„ç§è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·è®¾è®¡çš„ç»Ÿä¸€ã€æ ‡å‡†åŒ–çš„è¯„ä¼°å·¥å…·åŒ…ã€‚è¯¥å·¥å…·åŒ…å…·æœ‰Pythoné£æ ¼çš„æ¥å£ï¼Œå…·æœ‰çµæ´»çš„é…ç½®å’Œä¾èµ–æ§åˆ¶ï¼Œä½¿å…¶æ˜“äºä½¿ç”¨ä¸”é«˜æ•ˆã€‚å®Œæˆå®‰è£…åï¼ŒVERSAæä¾›åŸºäºä¸åŒé…ç½®çš„63ç§æŒ‡æ ‡å’Œ711ç§æŒ‡æ ‡å˜ä½“ã€‚è¿™äº›æŒ‡æ ‡åŒ…æ‹¬åˆ©ç”¨å¤šç§å¤–éƒ¨èµ„æºçš„è¯„ä¼°ï¼ŒåŒ…æ‹¬åŒ¹é…å’ŒéåŒ¹é…çš„å‚è€ƒéŸ³é¢‘ã€æ–‡æœ¬è½¬å½•å’Œæ–‡æœ¬æ ‡é¢˜ã€‚ä½œä¸ºä¸€ä¸ªè½»ä¾¿è€Œå…¨é¢çš„å·¥å…·åŒ…ï¼ŒVERSAæ”¯æŒå¯¹å„ç§ä¸‹æ¸¸åœºæ™¯çš„è¯„ä¼°ã€‚ä¸ºäº†å±•ç¤ºå…¶èƒ½åŠ›ï¼Œè¿™é¡¹å·¥ä½œé‡ç‚¹ä»‹ç»äº†VERSAçš„ç¤ºä¾‹ç”¨ä¾‹ï¼ŒåŒ…æ‹¬éŸ³é¢‘ç¼–ç ã€è¯­éŸ³åˆæˆã€è¯­éŸ³å¢å¼ºã€æ­Œå”±åˆæˆå’ŒéŸ³ä¹ç”Ÿæˆã€‚è¯¥å·¥å…·åŒ…å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shinjiwlab/versa%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/shinjiwlab/versaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17667v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>è¯¥å·¥ä½œæ¨å‡ºVERSAå·¥å…·åŒ…ï¼Œä¸€ä¸ªç»Ÿä¸€æ ‡å‡†åŒ–çš„ç”¨äºè¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·çš„è¯„ä¼°å·¥å…·åŒ…ã€‚è¯¥å·¥å…·åŒ…é‡‡ç”¨Pythonæ¥å£ï¼Œé…ç½®çµæ´»ï¼Œä¾èµ–æ§åˆ¶æ€§å¼ºï¼Œä½¿ç”¨æ–¹ä¾¿ä¸”æ•ˆç‡é«˜ã€‚å®Œæˆå®‰è£…åï¼Œæ ¹æ®é…ç½®ä¸åŒï¼ŒVERSAæä¾›63ç§æŒ‡æ ‡çš„711ç§å˜åŒ–ã€‚è¿™äº›æŒ‡æ ‡æ¶µç›–åˆ©ç”¨å¤šç§å¤–éƒ¨èµ„æºçš„è¯„ä¼°ï¼ŒåŒ…æ‹¬åŒ¹é…å’ŒéåŒ¹é…å‚è€ƒéŸ³é¢‘ã€æ–‡æœ¬è½¬å½•å’Œæ–‡æœ¬å­—å¹•ç­‰ã€‚ä½œä¸ºä¸€ä¸ªè½»ä¾¿è€Œå…¨é¢çš„å·¥å…·åŒ…ï¼ŒVERSAæ”¯æŒå¤šç§ä¸‹æ¸¸åœºæ™¯çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬éŸ³é¢‘ç¼–ç ã€è¯­éŸ³åˆæˆã€è¯­éŸ³å¢å¼ºã€æ­Œå”±åˆæˆå’ŒéŸ³ä¹ç”Ÿæˆç­‰ã€‚ç›¸å…³èµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shinjiwlab/versa%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/shinjiwlab/versaè·å–ã€‚</a></p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>VERSAæ˜¯ä¸€ä¸ªç»Ÿä¸€æ ‡å‡†åŒ–çš„è¯„ä»·å·¥å…·åŒ…ï¼Œé€‚ç”¨äºå¤šç§è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·çš„è¯„ä¼°ã€‚</li>
<li>VERSAæä¾›Pythonicæ¥å£ï¼Œé…ç½®çµæ´»ï¼Œä¾èµ–æ§åˆ¶æ€§å¼ºï¼Œæ–¹ä¾¿ç”¨æˆ·ä½¿ç”¨ã€‚</li>
<li>æ ¹æ®ä¸åŒçš„é…ç½®ï¼ŒVERSAæä¾›ä¸°å¯Œçš„è¯„ä»·æŒ‡æ ‡ã€‚</li>
<li>VERSAæ¶µç›–å¤šç§å¤–éƒ¨èµ„æºçš„è¯„ä¼°ï¼Œå¦‚åŒ¹é…å’ŒéåŒ¹é…å‚è€ƒéŸ³é¢‘ã€æ–‡æœ¬è½¬å½•å’Œæ–‡æœ¬å­—å¹•ç­‰ã€‚</li>
<li>VERSAæ”¯æŒå¤šç§ä¸‹æ¸¸åœºæ™¯çš„è¯„ä¼°ï¼Œå¦‚éŸ³é¢‘ç¼–ç ã€è¯­éŸ³åˆæˆç­‰ã€‚</li>
<li>VERSAåœ¨GitHubä¸Šæœ‰è¯¦ç»†çš„èµ„æºå’Œç›¸å…³é“¾æ¥ï¼Œæ–¹ä¾¿è·å–å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca5810467e21064aad7ef23ae592e12a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f43ca7b05cd368a4682ed95008e06032.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-105333784beee2cd2b863f3f3fd741b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00d391b42d6acbe6ed460f7bb19e84c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-081514bb790a0108b7e0dde5a0b7d40d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HumanVBench-Exploring-Human-Centric-Video-Understanding-Capabilities-of-MLLMs-with-Synthetic-Benchmark-Data"><a href="#HumanVBench-Exploring-Human-Centric-Video-Understanding-Capabilities-of-MLLMs-with-Synthetic-Benchmark-Data" class="headerlink" title="HumanVBench: Exploring Human-Centric Video Understanding Capabilities of   MLLMs with Synthetic Benchmark Data"></a>HumanVBench: Exploring Human-Centric Video Understanding Capabilities of   MLLMs with Synthetic Benchmark Data</h2><p><strong>Authors:Ting Zhou, Daoyuan Chen, Qirui Jiao, Bolin Ding, Yaliang Li, Ying Shen</strong></p>
<p>In the domain of Multimodal Large Language Models (MLLMs), achieving human-centric video understanding remains a formidable challenge. Existing benchmarks primarily emphasize object and action recognition, often neglecting the intricate nuances of human emotions, behaviors, and speech visual alignment within video content. We present HumanVBench, an innovative benchmark meticulously crafted to bridge these gaps in the evaluation of video MLLMs. HumanVBench comprises 17 carefully designed tasks that explore two primary dimensions: inner emotion and outer manifestations, spanning static and dynamic, basic and complex, as well as single-modal and cross-modal aspects. With two advanced automated pipelines for video annotation and distractor-included QA generation, HumanVBench utilizes diverse state-of-the-art (SOTA) techniques to streamline benchmark data synthesis and quality assessment, minimizing human annotation dependency tailored to human-centric multimodal attributes. A comprehensive evaluation across 16 SOTA video MLLMs reveals notable limitations in current performance, especially in cross-modal and temporal alignment, underscoring the necessity for further refinement toward achieving more human-like understanding. HumanVBench is open-sourced to facilitate future advancements and real-world applications in video MLLMs. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢†åŸŸï¼Œå®ç°ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å¼ºè°ƒå¯¹è±¡å’ŒåŠ¨ä½œè¯†åˆ«ï¼Œå¾€å¾€å¿½è§†äº†è§†é¢‘å†…å®¹ä¸­äººç±»æƒ…ç»ªã€è¡Œä¸ºå’Œè¯­éŸ³è§†è§‰å¯¹é½çš„ç»†å¾®å·®åˆ«ã€‚æˆ‘ä»¬æå‡ºäº†HumanVBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¼¥è¡¥è§†é¢‘MLLMsè¯„ä¼°ä¸­çš„è¿™äº›å·®è·ã€‚HumanVBenchåŒ…å«17é¡¹ç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ï¼Œæ¢ç´¢ä¸¤ä¸ªä¸»è¦ç»´åº¦ï¼šå†…åœ¨æƒ…ç»ªå’Œå¤–åœ¨è¡¨ç°ï¼Œæ¶µç›–é™æ€å’ŒåŠ¨æ€ã€åŸºæœ¬å’Œå¤æ‚ä»¥åŠå•æ¨¡æ€å’Œè·¨æ¨¡æ€æ–¹é¢ã€‚HumanVBenché‡‡ç”¨ä¸¤ä¸ªå…ˆè¿›çš„è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œè§†é¢‘æ ‡æ³¨å’ŒåŒ…å«å¹²æ‰°é¡¹çš„QAç”Ÿæˆï¼Œåˆ©ç”¨å¤šç§æœ€æ–°æŠ€æœ¯ä¼˜åŒ–åŸºå‡†æ•°æ®åˆæˆå’Œè´¨é‡è¯„ä¼°ï¼Œå‡å°‘å¯¹äººç±»æ ‡æ³¨çš„ä¾èµ–ï¼Œä¸“é—¨é’ˆå¯¹ä»¥äººç±»ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å±æ€§ã€‚å¯¹16ç§æœ€æ–°è§†é¢‘MLLMsçš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æ€§èƒ½å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå°¤å…¶åœ¨è·¨æ¨¡æ€å’Œæ—¶é—´å¯¹é½æ–¹é¢ï¼Œè¿™å¼ºè°ƒäº†å¯¹è¿›ä¸€æ­¥æ”¹è¿›å®ç°æ›´äººæ€§åŒ–çš„ç†è§£çš„å¿…è¦æ€§ã€‚HumanVBenchå¼€æºï¼Œä»¥ä¿ƒè¿›è§†é¢‘MLLMsçš„æœªæ¥å‘å±•å’Œå®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17574v1">PDF</a> 22 pages, 24 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸï¼ˆMLLMsï¼‰ï¼Œå®ç°ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£ä»ç„¶æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºå¯¹è±¡å’ŒåŠ¨ä½œè¯†åˆ«ï¼Œå¾€å¾€å¿½è§†äº†äººç±»æƒ…ç»ªã€è¡Œä¸ºå’Œè§†é¢‘å†…è¯­éŸ³è§†è§‰å¯¹é½çš„ç»†å¾®å·®åˆ«ã€‚æœ¬æ–‡æå‡ºäº†HumanVBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¼¥è¡¥è§†é¢‘MLLMè¯„ä¼°ä¸­çš„è¿™äº›å·®è·ã€‚HumanVBenchåŒ…å«17ä¸ªç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ï¼Œæ¢ç´¢ä¸¤ä¸ªä¸»è¦ç»´åº¦ï¼šå†…åœ¨æƒ…ç»ªå’Œå¤–åœ¨è¡¨ç°ï¼Œæ¶µç›–é™æ€å’ŒåŠ¨æ€ã€åŸºæœ¬å’Œå¤æ‚ä»¥åŠå•æ¨¡æ€å’Œè·¨æ¨¡æ€æ–¹é¢ã€‚å®ƒé‡‡ç”¨ä¸¤ä¸ªå…ˆè¿›çš„è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œè§†é¢‘æ³¨é‡Šå’ŒåŒ…å«å¹²æ‰°é¡¹çš„QAç”Ÿæˆï¼Œåˆ©ç”¨å¤šç§æœ€å…ˆè¿›æŠ€æœ¯æ¥ç®€åŒ–åŸºå‡†æµ‹è¯•æ•°æ®åˆæˆå’Œè´¨é‡è¯„ä¼°ï¼Œå‡å°‘äººå·¥æ³¨é‡Šçš„ä¾èµ–ï¼Œä¸“æ³¨äºä»¥äººç±»ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€å±æ€§ã€‚å¯¹16ä¸ªæœ€æ–°è§†é¢‘MLLMçš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨è·¨æ¨¡æ€å’Œæ—¶é—´å¯¹é½æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼Œå¼ºè°ƒéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›æ‰èƒ½å®ç°æ›´äººæ€§åŒ–çš„ç†è§£ã€‚HumanVBenchå·²å¼€æºï¼Œä»¥ä¿ƒè¿›è§†é¢‘MLLMçš„æœªæ¥å‘å±•å’Œå®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Human-centric video understanding remains a challenge in Multimodal Large Language Models (MLLMs).</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å¯¹è±¡å’ŒåŠ¨ä½œè¯†åˆ«ï¼Œå¿½è§†äº†äººç±»æƒ…ç»ªã€è¡Œä¸ºå’Œè¯­éŸ³è§†è§‰å¯¹é½çš„ç»†å¾®å·®åˆ«ã€‚</li>
<li>HumanVBenchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¼¥è¡¥è§†é¢‘MLLMè¯„ä¼°ä¸­çš„å·®è·ã€‚</li>
<li>HumanVBenchåŒ…å«17ä¸ªä»»åŠ¡ï¼Œæ¢ç´¢å†…åœ¨æƒ…ç»ªå’Œå¤–åœ¨è¡¨ç°ä¸¤ä¸ªä¸»è¦ç»´åº¦ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•é‡‡ç”¨è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œè§†é¢‘æ³¨é‡Šå’ŒQAç”Ÿæˆï¼Œå‡å°‘äººå·¥æ³¨é‡Šçš„ä¾èµ–ã€‚</li>
<li>æœ€æ–°è§†é¢‘MLLMåœ¨è·¨æ¨¡æ€å’Œæ—¶é—´å¯¹é½æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53cc7f9a310986678d9cd3b641524e89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a0631a4f5c771b320709a53dfbac91c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9430fe6c748ae3cc3954141adc32df25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-baf8cb80579d87d8c190e8f8c910e76f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="UME-Upcycling-Mixture-of-Experts-for-Scalable-and-Efficient-Automatic-Speech-Recognition"><a href="#UME-Upcycling-Mixture-of-Experts-for-Scalable-and-Efficient-Automatic-Speech-Recognition" class="headerlink" title="UME: Upcycling Mixture-of-Experts for Scalable and Efficient Automatic   Speech Recognition"></a>UME: Upcycling Mixture-of-Experts for Scalable and Efficient Automatic   Speech Recognition</h2><p><strong>Authors:Li Fu, Shanyong Yu, Siqi Li, Lu Fan, Youzheng Wu, Xiaodong He</strong></p>
<p>Recent advancements in scaling up models have significantly improved performance in Automatic Speech Recognition (ASR) tasks. However, training large ASR models from scratch remains costly. To address this issue, we introduce UME, a novel method that efficiently Upcycles pretrained dense ASR checkpoints into larger Mixture-of-Experts (MoE) architectures. Initially, feed-forward networks are converted into MoE layers. By reusing the pretrained weights, we establish a robust foundation for the expanded model, significantly reducing optimization time. Then, layer freezing and expert balancing strategies are employed to continue training the model, further enhancing performance. Experiments on a mixture of 170k-hour Mandarin and English datasets show that UME: 1) surpasses the pretrained baseline by a margin of 11.9% relative error rate reduction while maintaining comparable latency; 2) reduces training time by up to 86.7% and achieves superior accuracy compared to training models of the same size from scratch. </p>
<blockquote>
<p>æœ€è¿‘æ¨¡å‹æ‰©å±•æ–¹é¢çš„è¿›å±•æ˜¾è‘—æé«˜äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä»å¤´å¼€å§‹è®­ç»ƒå¤§å‹ASRæ¨¡å‹æˆæœ¬ä»ç„¶å¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†UMEï¼Œä¸€ç§å°†é¢„è®­ç»ƒçš„å¯†é›†ASRæ£€æŸ¥ç‚¹é«˜æ•ˆå‡çº§ä¸ºæ›´å¤§çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„çš„æ–°æ–¹æ³•ã€‚æœ€åˆï¼Œå‰é¦ˆç½‘ç»œè¢«è½¬æ¢ä¸ºMoEå±‚ã€‚é€šè¿‡é‡ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œæˆ‘ä»¬ä¸ºæ‰©å±•æ¨¡å‹å»ºç«‹äº†ç¨³å¥çš„åŸºç¡€ï¼Œæ˜¾è‘—å‡å°‘äº†ä¼˜åŒ–æ—¶é—´ã€‚ç„¶åï¼Œé‡‡ç”¨å±‚å†»ç»“å’Œä¸“å®¶å¹³è¡¡ç­–ç•¥ç»§ç»­è®­ç»ƒæ¨¡å‹ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚åœ¨170kå°æ—¶çš„æ™®é€šè¯å’Œè‹±è¯­æ•°æ®é›†æ··åˆè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒUMEï¼š1ï¼‰ç›¸å¯¹äºé¢„è®­ç»ƒåŸºçº¿ï¼Œç›¸å¯¹è¯¯å·®ç‡é™ä½äº†11.9%ï¼ŒåŒæ—¶ä¿æŒå¯æ¯”è¾ƒçš„å»¶è¿Ÿï¼›2ï¼‰è®­ç»ƒæ—¶é—´æœ€å¤šå‡å°‘86.7%ï¼Œå¹¶ä¸”åœ¨ç›¸åŒå¤§å°çš„æ¨¡å‹ä¸­å®ç°äº†ä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒçš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17507v1">PDF</a> ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>ASRæ¨¡å‹æ‰©å±•çš„æ–°æ–¹æ³•UMEé€šè¿‡å°†é¢„è®­ç»ƒçš„å¯†é›†ASRæ£€æŸ¥ç‚¹é«˜æ•ˆåœ°è½¬åŒ–ä¸ºæ›´å¤§çš„æ··åˆä¸“å®¶æ¶æ„ï¼ˆMoEï¼‰æ¥è§£å†³ä»å¤´è®­ç»ƒå¤§å‹ASRæ¨¡å‹çš„æˆæœ¬é—®é¢˜ã€‚è¯¥æ–¹æ³•å…ˆå°†å‰é¦ˆç½‘ç»œè½¬æ¢ä¸ºMoEå±‚å¹¶åˆ©ç”¨é¢„è®­ç»ƒæƒé‡è¿›è¡Œæ¨¡å‹æ‰©å±•ï¼Œä»è€Œå‡å°‘ä¼˜åŒ–æ—¶é—´ã€‚å†é€šè¿‡å†»ç»“å±‚å’Œä¸“å®¶å¹³è¡¡ç­–ç•¥è¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒUMEåœ¨ä¿æŒå»¶è¿Ÿçš„åŒæ—¶ï¼Œç›¸å¯¹äºé¢„è®­ç»ƒåŸºçº¿é™ä½äº†11.9%çš„ç›¸å¯¹è¯¯å·®ç‡ï¼ŒåŒæ—¶å‡å°‘äº†é«˜è¾¾86.7%çš„è®­ç»ƒæ—¶é—´ï¼Œå¹¶åœ¨ç›¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸­å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UMEæ˜¯ä¸€ç§è§£å†³ASRæ¨¡å‹è®­ç»ƒæˆæœ¬é«˜æ˜‚çš„æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡æœ‰æ•ˆåœ°å°†é¢„è®­ç»ƒçš„å¯†é›†ASRæ£€æŸ¥ç‚¹å‡çº§ä¸ºæ›´å¤§çš„æ··åˆä¸“å®¶æ¶æ„æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</li>
<li>UMEé€šè¿‡å°†å‰é¦ˆç½‘ç»œè½¬åŒ–ä¸ºMoEå±‚å¹¶åˆ©ç”¨é¢„è®­ç»ƒæƒé‡å»ºç«‹ç¨³å›ºåŸºç¡€æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå‡å°‘ä¼˜åŒ–æ—¶é—´ã€‚</li>
<li>åœ¨å†»ç»“å±‚å’Œé‡‡ç”¨ä¸“å®¶å¹³è¡¡ç­–ç•¥åï¼Œæ¨¡å‹çš„æ€§èƒ½å¾—åˆ°è¿›ä¸€æ­¥æå‡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒUMEåœ¨ä¿æŒå»¶è¿Ÿä¸å˜çš„æƒ…å†µä¸‹ï¼Œç›¸å¯¹äºé¢„è®­ç»ƒåŸºçº¿é™ä½äº†æ˜¾è‘—æ¯”ä¾‹çš„ç›¸å¯¹è¯¯å·®ç‡ã€‚</li>
<li>UMEæ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ—¶é—´ï¼Œç›¸æ¯”ä»å¤´å¼€å§‹è®­ç»ƒç›¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼Œå…¶æ•ˆç‡æ›´é«˜ã€‚</li>
<li>UMEæ–¹æ³•ç»“åˆäº†é¢„è®­ç»ƒæƒé‡å’Œæ··åˆä¸“å®¶æ¶æ„çš„ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜æ€§èƒ½å’Œé«˜æ•ˆèƒ½çš„ç»“åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-42aa5d91386d02bdf9ac469e76828788.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74e3f6fb9e3103bdb2525609221f1487.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8107cdaa88ba3b15b476073f2e7c9ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdc32e9ef09616f3b2bdda574fd81c2b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Uncovering-the-Visual-Contribution-in-Audio-Visual-Speech-Recognition"><a href="#Uncovering-the-Visual-Contribution-in-Audio-Visual-Speech-Recognition" class="headerlink" title="Uncovering the Visual Contribution in Audio-Visual Speech Recognition"></a>Uncovering the Visual Contribution in Audio-Visual Speech Recognition</h2><p><strong>Authors:Zhaofeng Lin, Naomi Harte</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) combines auditory and visual speech cues to enhance the accuracy and robustness of speech recognition systems. Recent advancements in AVSR have improved performance in noisy environments compared to audio-only counterparts. However, the true extent of the visual contribution, and whether AVSR systems fully exploit the available cues in the visual domain, remains unclear. This paper assesses AVSR systems from a different perspective, by considering human speech perception. We use three systems: Auto-AVSR, AVEC and AV-RelScore. We first quantify the visual contribution using effective SNR gains at 0 dB and then investigate the use of visual information in terms of its temporal distribution and word-level informativeness. We show that low WER does not guarantee high SNR gains. Our results suggest that current methods do not fully exploit visual information, and we recommend future research to report effective SNR gains alongside WERs. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆäº†å¬è§‰å’Œè§†è§‰è¯­éŸ³çº¿ç´¢ï¼Œä»¥æé«˜è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚ä¸ä»…ä½¿ç”¨éŸ³é¢‘çš„åŒè¡Œç›¸æ¯”ï¼ŒAVSRçš„æœ€æ–°è¿›å±•åœ¨å˜ˆæ‚ç¯å¢ƒä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè§†è§‰è´¡çŒ®çš„çœŸæ­£ç¨‹åº¦ï¼Œä»¥åŠAVSRç³»ç»Ÿæ˜¯å¦å……åˆ†åˆ©ç”¨è§†è§‰é¢†åŸŸä¸­çš„å¯ç”¨çº¿ç´¢ï¼Œä»ç„¶ä¸æ¸…æ¥šã€‚æœ¬æ–‡é€šè¿‡è€ƒè™‘äººç±»è¯­éŸ³æ„ŸçŸ¥ï¼Œä»ä¸åŒè§’åº¦è¯„ä¼°AVSRç³»ç»Ÿã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ç§ç³»ç»Ÿï¼šAuto-AVSRã€AVECå’ŒAV-RelScoreã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨æœ‰æ•ˆä¿¡å™ªæ¯”å¢ç›Šï¼ˆåœ¨0åˆ†è´å¤„ï¼‰é‡åŒ–è§†è§‰è´¡çŒ®ï¼Œç„¶åç ”ç©¶è§†è§‰ä¿¡æ¯çš„ä½¿ç”¨ä¸å…¶æ—¶é—´åˆ†å¸ƒå’Œè¯çº§ä¿¡æ¯é‡æœ‰å…³ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½è¯é”™è¯¯ç‡å¹¶ä¸ä¿è¯é«˜ä¿¡å™ªæ¯”å¢ç›Šã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“å‰æ–¹æ³•å¹¶æ²¡æœ‰å……åˆ†åˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œæˆ‘ä»¬å»ºè®®æœªæ¥çš„ç ”ç©¶åœ¨æŠ¥å‘Šè¯é”™è¯¯ç‡çš„åŒæ—¶ï¼Œä¹Ÿè¦æŠ¥å‘Šæœ‰æ•ˆçš„ä¿¡å™ªæ¯”å¢ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17129v1">PDF</a> 5 pages, 2 figures. Accepted to ICASSP 2025</p>
<p><strong>Summary</strong><br>     éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆäº†å¬è§‰å’Œè§†è§‰è¯­éŸ³çº¿ç´¢ï¼Œæé«˜äº†è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚æœ€æ–°è¿›å±•è¡¨æ˜ï¼ŒAVSRåœ¨å˜ˆæ‚ç¯å¢ƒä¸­çš„æ€§èƒ½ä¼˜äºä»…ä½¿ç”¨éŸ³é¢‘çš„ç³»ç»Ÿã€‚ç„¶è€Œï¼Œè§†è§‰ä¿¡æ¯çš„çœŸæ­£è´¡çŒ®ä»¥åŠAVSRç³»ç»Ÿæ˜¯å¦å……åˆ†åˆ©ç”¨è§†è§‰é¢†åŸŸçš„å¯ç”¨çº¿ç´¢ä»ä¸æ˜ç¡®ã€‚æœ¬æ–‡é‡‡ç”¨è‡ªåŠ¨AVSRã€AVECå’ŒAV-RelScoreä¸‰ä¸ªç³»ç»Ÿè¿›è¡Œè¯„ä¼°ï¼Œé¦–å…ˆé‡åŒ–è§†è§‰è´¡çŒ®ï¼Œç„¶åç ”ç©¶è§†è§‰ä¿¡æ¯çš„æ—¶åºåˆ†å¸ƒå’Œè¯çº§ä¿¡æ¯é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½è¯é”™è¯¯ç‡å¹¶ä¸ä¿è¯é«˜ä¿¡å™ªæ¯”å¢ç›Šã€‚å½“å‰æ–¹æ³•æœªå……åˆ†åˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œå»ºè®®æœªæ¥ç ”ç©¶åœ¨æŠ¥å‘Šè¯é”™è¯¯ç‡çš„åŒæ—¶ä¹Ÿè¦æŠ¥å‘Šæœ‰æ•ˆçš„ä¿¡å™ªæ¯”å¢ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AVSRç»“åˆäº†å¬è§‰å’Œè§†è§‰è¯­éŸ³çº¿ç´¢ï¼Œæé«˜äº†è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚</li>
<li>æœ€æ–°AVSRæŠ€æœ¯æ”¹è¿›äº†åœ¨å˜ˆæ‚ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚</li>
<li>è§†è§‰ä¿¡æ¯å¯¹AVSRçš„è´¡çŒ®å°šæœªå®Œå…¨æ˜ç¡®ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ä¸‰ä¸ªç³»ç»Ÿï¼ˆAuto-AVSRã€AVECå’ŒAV-RelScoreï¼‰è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°è§†è§‰ä¿¡æ¯çš„é‡åŒ–æ˜¯é‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œä½è¯é”™è¯¯ç‡å¹¶ä¸ä¿è¯é«˜ä¿¡å™ªæ¯”å¢ç›Šï¼Œè¿™æš—ç¤ºå½“å‰æ–¹æ³•æœªå……åˆ†åˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚</li>
<li>è§†è§‰ä¿¡æ¯åœ¨æ—¶ç©ºåˆ†å¸ƒå’Œè¯çº§ä¿¡æ¯é‡æ–¹é¢çš„ä½¿ç”¨æ˜¯è¯„ä¼°AVSRç³»ç»Ÿæ€§èƒ½çš„å…³é”®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8947305fb35207209e5a8fcb9c15bf6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0060dad4ae9fa14d93223b8518ca324.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a715eef15e77a331d0276aa3ba9a089a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-464af9fc67d376bc83735961f31f27ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84cfb859102f8af4e3ffb64688ae83c4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Scalable-Speech-Enhancement-with-Dynamic-Channel-Pruning"><a href="#Scalable-Speech-Enhancement-with-Dynamic-Channel-Pruning" class="headerlink" title="Scalable Speech Enhancement with Dynamic Channel Pruning"></a>Scalable Speech Enhancement with Dynamic Channel Pruning</h2><p><strong>Authors:Riccardo Miccini, Clement Laroche, Tobias Piechowiak, Luca Pezzarossa</strong></p>
<p>Speech Enhancement (SE) is essential for improving productivity in remote collaborative environments. Although deep learning models are highly effective at SE, their computational demands make them impractical for embedded systems. Furthermore, acoustic conditions can change significantly in terms of difficulty, whereas neural networks are usually static with regard to the amount of computation performed. To this end, we introduce Dynamic Channel Pruning to the audio domain for the first time and apply it to a custom convolutional architecture for SE. Our approach works by identifying unnecessary convolutional channels at runtime and saving computational resources by not computing the activations for these channels and retrieving their filters. When trained to only use 25% of channels, we save 29.6% of MACs while only causing a 0.75% drop in PESQ. Thus, DynCP offers a promising path toward deploying larger and more powerful SE solutions on resource-constrained devices. </p>
<blockquote>
<p>è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰å¯¹äºæé«˜è¿œç¨‹åä½œç¯å¢ƒä¸­çš„ç”Ÿäº§åŠ›è‡³å…³é‡è¦ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨SEæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†å…¶è®¡ç®—éœ€æ±‚ä½¿å…¶ä¸é€‚ç”¨äºåµŒå…¥å¼ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œå£°å­¦æ¡ä»¶åœ¨éš¾åº¦æ–¹é¢å¯èƒ½ä¼šå‘ç”Ÿé‡å¤§å˜åŒ–ï¼Œè€Œç¥ç»ç½‘ç»œé€šå¸¸åœ¨è¿›è¡Œè®¡ç®—æ—¶ä¿æŒé™æ€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–æ¬¡å°†åŠ¨æ€é€šé“è£å‰ªå¼•å…¥åˆ°éŸ³é¢‘é¢†åŸŸï¼Œå¹¶å°†å…¶åº”ç”¨äºç”¨äºSEçš„è‡ªå®šä¹‰å·ç§¯æ¶æ„ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨è¿è¡Œæ—¶è¯†åˆ«ä¸å¿…è¦çš„å·ç§¯é€šé“ï¼Œå¹¶é€šè¿‡ä¸è®¡ç®—è¿™äº›é€šé“çš„æ¿€æ´»å’Œæ£€ç´¢å…¶è¿‡æ»¤å™¨æ¥èŠ‚çœè®¡ç®—èµ„æºã€‚å½“ä»…è®­ç»ƒä½¿ç”¨25%çš„é€šé“æ—¶ï¼Œæˆ‘ä»¬åœ¨èŠ‚çœ29.6%çš„ä¹˜åŠ æ“ä½œçš„åŒæ—¶ï¼Œä»…å¯¼è‡´PESQä¸‹é™0.75%ã€‚å› æ­¤ï¼ŒDynCPä¸ºåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²æ›´å¤§ã€æ›´å¼ºå¤§çš„SEè§£å†³æ–¹æ¡ˆæä¾›äº†ä¸€æ¡æœ‰å‰é€”çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17121v1">PDF</a> Accepted for publication at the 2025 IEEE International Conference on   Acoustics, Speech and Signal Processing (ICASSP)</p>
<p><strong>æ€»ç»“</strong><br>    æ–‡ç« ä»‹ç»äº†é’ˆå¯¹è¿œç¨‹åä½œç¯å¢ƒä¸­çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰é—®é¢˜ï¼Œè™½ç„¶æ·±åº¦å­¦ä¹ æ¨¡å‹æ•ˆæœæ˜¾è‘—ï¼Œä½†åœ¨åµŒå…¥å¼ç³»ç»Ÿä¸­å› è®¡ç®—éœ€æ±‚è¿‡é«˜è€Œä¸å®ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œé¦–æ¬¡å°†åŠ¨æ€é€šé“è£å‰ªæŠ€æœ¯å¼•å…¥éŸ³é¢‘é¢†åŸŸï¼Œå¹¶åº”ç”¨äºå®šåˆ¶çš„å·ç§¯æ¶æ„è¿›è¡ŒSEã€‚è¯¥æ–¹æ³•åœ¨è¿è¡Œè¿‡ç¨‹ä¸­è¯†åˆ«ä¸å¿…è¦çš„å·ç§¯é€šé“ï¼Œé€šè¿‡ä¸è®¡ç®—è¿™äº›é€šé“çš„æ¿€æ´»å’Œæ£€ç´¢å…¶è¿‡æ»¤å™¨æ¥èŠ‚çœè®¡ç®—èµ„æºã€‚å½“è®­ç»ƒä»…ä½¿ç”¨25%çš„é€šé“æ—¶ï¼Œå¯åœ¨é™ä½0.75%çš„PESQçš„åŒæ—¶ï¼ŒèŠ‚çœ29.6%çš„MACsã€‚åŠ¨æ€é€šé“è£å‰ªæŠ€æœ¯ä¸ºå®ç°èµ„æºå—é™è®¾å¤‡ä¸Šéƒ¨ç½²æ›´å¤§ã€æ›´å¼ºå¤§çš„SEè§£å†³æ–¹æ¡ˆæä¾›äº†å¯è¡Œçš„è·¯å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰åœ¨è¿œç¨‹åä½œç¯å¢ƒä¸­å¯¹æå‡ç”Ÿäº§åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨SEæ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œä½†åœ¨åµŒå…¥å¼ç³»ç»Ÿä¸­å› è®¡ç®—éœ€æ±‚è¿‡é«˜è€Œä¸å®ç”¨ã€‚</li>
<li>å¼•å…¥åŠ¨æ€é€šé“è£å‰ªæŠ€æœ¯ï¼Œé¦–æ¬¡å°†å…¶åº”ç”¨äºéŸ³é¢‘é¢†åŸŸçš„å·ç§¯æ¶æ„è¿›è¡ŒSEã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è¯†åˆ«å¹¶å¿½ç•¥ä¸å¿…è¦çš„å·ç§¯é€šé“æ¥èŠ‚çœè®¡ç®—èµ„æºã€‚</li>
<li>ä½¿ç”¨ä»…25%çš„é€šé“è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥åœ¨é™ä½å°‘è®¸æ€§èƒ½æŸå¤±ï¼ˆ0.74%ï¼‰çš„å‰æä¸‹ï¼Œå®ç°æ˜¾è‘—çš„èµ„æºèŠ‚çº¦ï¼ˆèŠ‚çœ29.6%çš„MACsï¼‰ã€‚</li>
<li>åŠ¨æ€é€šé“è£å‰ªæŠ€æœ¯ä¸ºèµ„æºå—é™è®¾å¤‡ä¸Šçš„SEè§£å†³æ–¹æ¡ˆéƒ¨ç½²æä¾›äº†å¯è¡Œçš„è·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17121">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4be88cb121823d9dc02e10317f8be99d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1eb1009def48219ee5adbecb97d793a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43018a3285599a03e901e2626e17b25a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e40bc8c118bc8beacac1f74a8c121b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d935ff7654e9b9b03810a5285961f56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae8784ae8a4fe82f5069f970b46427d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44e199a715c405fcd4eb500c914e2a3b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Incremental-Disentanglement-for-Environment-Aware-Zero-Shot-Text-to-Speech-Synthesis"><a href="#Incremental-Disentanglement-for-Environment-Aware-Zero-Shot-Text-to-Speech-Synthesis" class="headerlink" title="Incremental Disentanglement for Environment-Aware Zero-Shot   Text-to-Speech Synthesis"></a>Incremental Disentanglement for Environment-Aware Zero-Shot   Text-to-Speech Synthesis</h2><p><strong>Authors:Ye-Xin Lu, Hui-Peng Du, Zheng-Yan Sheng, Yang Ai, Zhen-Hua Ling</strong></p>
<p>This paper proposes an Incremental Disentanglement-based Environment-Aware zero-shot text-to-speech (TTS) method, dubbed IDEA-TTS, that can synthesize speech for unseen speakers while preserving the acoustic characteristics of a given environment reference speech. IDEA-TTS adopts VITS as the TTS backbone. To effectively disentangle the environment, speaker, and text factors, we propose an incremental disentanglement process, where an environment estimator is designed to first decompose the environmental spectrogram into an environment mask and an enhanced spectrogram. The environment mask is then processed by an environment encoder to extract environment embeddings, while the enhanced spectrogram facilitates the subsequent disentanglement of the speaker and text factors with the condition of the speaker embeddings, which are extracted from the environmental speech using a pretrained environment-robust speaker encoder. Finally, both the speaker and environment embeddings are conditioned into the decoder for environment-aware speech generation. Experimental results demonstrate that IDEA-TTS achieves superior performance in the environment-aware TTS task, excelling in speech quality, speaker similarity, and environmental similarity. Additionally, IDEA-TTS is also capable of the acoustic environment conversion task and achieves state-of-the-art performance. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¢é‡è§£è€¦çš„ç¯å¢ƒæ„ŸçŸ¥é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ–¹æ³•ï¼Œç§°ä¸ºIDEA-TTSã€‚è¯¥æ–¹æ³•å¯ä»¥åœ¨ä¸ºæœªè§è¿‡çš„è¯´è¯äººåˆæˆè¯­éŸ³çš„åŒæ—¶ï¼Œä¿ç•™ç»™å®šç¯å¢ƒå‚è€ƒè¯­éŸ³çš„å£°å­¦ç‰¹å¾ã€‚IDEA-TTSé‡‡ç”¨VITSä½œä¸ºTTSçš„éª¨å¹²ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è§£å¼€ç¯å¢ƒã€è¯´è¯äººå’Œæ–‡æœ¬å› ç´ ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¢é‡è§£è€¦è¿‡ç¨‹ï¼Œå…¶ä¸­ç¯å¢ƒä¼°è®¡å™¨è¢«è®¾è®¡ç”¨æ¥é¦–å…ˆå°†ç¯å¢ƒå…‰è°±å›¾åˆ†è§£æˆç¯å¢ƒæ©è†œå’Œå¢å¼ºå…‰è°±å›¾ã€‚ç„¶åï¼Œç¯å¢ƒæ©è†œè¢«ç¯å¢ƒç¼–ç å™¨å¤„ç†ä»¥æå–ç¯å¢ƒåµŒå…¥ï¼Œè€Œå¢å¼ºå…‰è°±å›¾æœ‰åŠ©äºåœ¨ç¯å¢ƒè¯­éŸ³çš„è¯´è¯äººåµŒå…¥çš„æ¡ä»¶ä¸‹ï¼Œéšåè§£å¼€è¯´è¯äººå’Œæ–‡æœ¬å› ç´ ï¼Œè¿™äº›åµŒå…¥æ˜¯ä»ç¯å¢ƒè¯­éŸ³ä¸­ä½¿ç”¨é¢„è®­ç»ƒçš„ç¯å¢ƒç¨³å¥è¯´è¯äººç¼–ç å™¨æå–çš„ã€‚æœ€åï¼Œè¯´è¯äººå’Œç¯å¢ƒåµŒå…¥éƒ½è¢«è¾“å…¥åˆ°è§£ç å™¨ä¸­ï¼Œä»¥è¿›è¡Œç¯å¢ƒæ„ŸçŸ¥çš„è¯­éŸ³ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIDEA-TTSåœ¨ç¯å¢ƒæ„ŸçŸ¥TTSä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œåœ¨è¯­éŸ³è´¨é‡ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œç¯å¢ƒç›¸ä¼¼åº¦æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼ŒIDEA-TTSè¿˜å…·å¤‡å£°éŸ³ç¯å¢ƒè½¬æ¢ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16977v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¢é‡è§£è€¦çš„ç¯å¢ƒæ„ŸçŸ¥é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ–¹æ³•ï¼Œåä¸ºIDEA-TTSã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æœªè§è¿‡è¯´è¯äººçš„æƒ…å†µä¸‹åˆæˆè¯­éŸ³ï¼ŒåŒæ—¶ä¿ç•™ç»™å®šç¯å¢ƒå‚è€ƒè¯­éŸ³çš„å£°å­¦ç‰¹å¾ã€‚IDEA-TTSé‡‡ç”¨VITSä½œä¸ºTTSéª¨æ¶ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è§£å¼€ç¯å¢ƒã€è¯´è¯äººå’Œæ–‡æœ¬å› ç´ ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¢é‡è§£è€¦è¿‡ç¨‹ï¼Œå…¶ä¸­ç¯å¢ƒä¼°è®¡å™¨è¢«è®¾è®¡æ¥é¦–å…ˆåˆ†è§£ç¯å¢ƒå…‰è°±å›¾æˆä¸€ä¸ªç¯å¢ƒæ©è†œå’Œä¸€ä¸ªå¢å¼ºå…‰è°±å›¾ã€‚ç¯å¢ƒæ©è†œéšåè¢«ç¯å¢ƒç¼–ç å™¨å¤„ç†ä»¥æå–ç¯å¢ƒåµŒå…¥ï¼ŒåŒæ—¶å¢å¼ºå…‰è°±å›¾æœ‰åŠ©äºéšåè§£å¼€è¯´è¯äººå’Œæ–‡æœ¬å› ç´ ï¼Œä»¥è¯´è¯äººåµŒå…¥ä¸ºæ¡ä»¶ï¼Œè¿™äº›åµŒå…¥æ˜¯ä»å¸¦æœ‰é¢„è®­ç»ƒçš„ç¯å¢ƒç¨³å¥è¯´è¯äººç¼–ç å™¨ä»ç¯å¢ƒè¯­éŸ³ä¸­æå–çš„ã€‚æœ€åï¼Œè¯´è¯äººå’Œç¯å¢ƒåµŒå…¥éƒ½è¢«è¾“å…¥åˆ°è§£ç å™¨ä¸­è¿›è¡Œç¯å¢ƒæ„ŸçŸ¥çš„è¯­éŸ³ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIDEA-TTSåœ¨ç¯å¢ƒæ„ŸçŸ¥TTSä»»åŠ¡ä¸Šå–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œåœ¨è¯­éŸ³è´¨é‡ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œç¯å¢ƒç›¸ä¼¼æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼ŒIDEA-TTSè¿˜å…·å¤‡å£°éŸ³ç¯å¢ƒè½¬æ¢ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå¹¶å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>IDEA-TTSæ˜¯ä¸€ç§åŸºäºå¢é‡è§£è€¦çš„ç¯å¢ƒæ„ŸçŸ¥é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ–¹æ³•ã€‚</li>
<li>IDEA-TTSèƒ½å¤Ÿåˆæˆæœªè§è¿‡è¯´è¯äººçš„è¯­éŸ³ï¼ŒåŒæ—¶ä¿ç•™ç»™å®šç¯å¢ƒå‚è€ƒè¯­éŸ³çš„å£°å­¦ç‰¹å¾ã€‚</li>
<li>ç¯å¢ƒä¼°è®¡å™¨ç”¨äºåˆ†è§£ç¯å¢ƒå…‰è°±å›¾ï¼Œäº§ç”Ÿç¯å¢ƒæ©è†œå’Œå¢å¼ºå…‰è°±å›¾ã€‚</li>
<li>ç¯å¢ƒç¼–ç å™¨ç”¨äºæå–ç¯å¢ƒåµŒå…¥ã€‚</li>
<li>IDEA-TTSåˆ©ç”¨é¢„è®­ç»ƒçš„è¯´è¯äººç¼–ç å™¨ä»ç¯å¢ƒè¯­éŸ³ä¸­æå–è¯´è¯äººåµŒå…¥ã€‚</li>
<li>IDEA-TTSåœ¨ç¯å¢ƒæ„ŸçŸ¥TTSä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­éŸ³è´¨é‡ã€è¯´è¯äººç›¸ä¼¼æ€§ã€ç¯å¢ƒç›¸ä¼¼æ€§æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-281a5d9690211cc4f718b2a9083136cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9eb575ca2c3c0ececfdc76e7e109d20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8437713026e3aa8d147083e65ab6fb41.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Temporal-Frequency-State-Space-Duality-An-Efficient-Paradigm-for-Speech-Emotion-Recognition"><a href="#Temporal-Frequency-State-Space-Duality-An-Efficient-Paradigm-for-Speech-Emotion-Recognition" class="headerlink" title="Temporal-Frequency State Space Duality: An Efficient Paradigm for Speech   Emotion Recognition"></a>Temporal-Frequency State Space Duality: An Efficient Paradigm for Speech   Emotion Recognition</h2><p><strong>Authors:Jiaqi Zhao, Fei Wang, Kun Li, Yanyan Wei, Shengeng Tang, Shu Zhao, Xiao Sun</strong></p>
<p>Speech Emotion Recognition (SER) plays a critical role in enhancing user experience within human-computer interaction. However, existing methods are overwhelmed by temporal domain analysis, overlooking the valuable envelope structures of the frequency domain that are equally important for robust emotion recognition. To overcome this limitation, we propose TF-Mamba, a novel multi-domain framework that captures emotional expressions in both temporal and frequency dimensions.Concretely, we propose a temporal-frequency mamba block to extract temporal- and frequency-aware emotional features, achieving an optimal balance between computational efficiency and model expressiveness. Besides, we design a Complex Metric-Distance Triplet (CMDT) loss to enable the model to capture representative emotional clues for SER. Extensive experiments on the IEMOCAP and MELD datasets show that TF-Mamba surpasses existing methods in terms of model size and latency, providing a more practical solution for future SER applications. </p>
<blockquote>
<p>è¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼ˆSERï¼‰åœ¨äººæœºäº¤äº’ä¸­æå‡ç”¨æˆ·ä½“éªŒæ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¢«æ—¶é—´åŸŸåˆ†ææ‰€å›°æ‰°ï¼Œå¿½ç•¥äº†é¢‘ç‡åŸŸçš„å®è´µåŒ…ç»œç»“æ„ï¼Œè¿™äº›ç»“æ„å¯¹äºç¨³å¥çš„æƒ…ç»ªè¯†åˆ«åŒæ ·é‡è¦ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†TF-Mambaï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¤šåŸŸæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ—¶é—´å’Œé¢‘ç‡ä¸¤ä¸ªç»´åº¦ä¸Šæ•æ‰æƒ…ç»ªè¡¨è¾¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶ç©ºmambaå—ï¼Œä»¥æå–æ—¶ç©ºæƒ…æ„Ÿç‰¹å¾ï¼Œåœ¨è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹è¡¨ç°åŠ›ä¹‹é—´è¾¾åˆ°æœ€ä¼˜å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤æ‚åº¦é‡è·ç¦»ä¸‰å…ƒç»„ï¼ˆCMDTï¼‰æŸå¤±ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•è·ç”¨äºSERçš„å…¸å‹æƒ…æ„Ÿçº¿ç´¢ã€‚åœ¨IEMOCAPå’ŒMELDæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTF-Mambaåœ¨æ¨¡å‹å¤§å°å’Œå»¶è¿Ÿæ–¹é¢è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºæœªæ¥SERåº”ç”¨æä¾›äº†æ›´å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16904v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰åœ¨äººæœºäº¤äº’ä¸­å¯¹äºæå‡ç”¨æˆ·ä½“éªŒå…·æœ‰é‡è¦ä½œç”¨ã€‚ç°æœ‰æ–¹æ³•è¿‡äºæ³¨é‡æ—¶åŸŸåˆ†æï¼Œå¿½ç•¥äº†é¢‘åŸŸåŒ…ç»œç»“æ„å¯¹äºç¨³å¥æƒ…æ„Ÿè¯†åˆ«çš„åŒç­‰é‡è¦æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†TF-Mambaè¿™ä¸€æ–°å‹å¤šåŸŸæ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶åœ¨æ—¶åŸŸå’Œé¢‘åŸŸæ•æ‰æƒ…æ„Ÿè¡¨è¾¾ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ—¶é¢‘mambaæ¨¡å—ï¼Œä»¥æå–æ—¶é¢‘æƒ…æ„Ÿç‰¹å¾ï¼Œåœ¨è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹è¡¨ç°åŠ›ä¹‹é—´è¾¾åˆ°æœ€ä¼˜å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†å¤æ‚åº¦é‡è·ç¦»ä¸‰å…ƒç»„ï¼ˆCMDTï¼‰æŸå¤±ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰ä»£è¡¨æ€§çš„æƒ…æ„Ÿçº¿ç´¢ç”¨äºSERã€‚åœ¨IEMOCAPå’ŒMELDæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTF-Mambaåœ¨æ¨¡å‹å¤§å°å’Œå»¶è¿Ÿæ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œä¸ºæœªæ¥SERåº”ç”¨æä¾›äº†æ›´å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å¯¹æå‡äººæœºäº¤äº’ä¸­çš„ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰SERæ–¹æ³•è¿‡äºä¾èµ–æ—¶åŸŸåˆ†æï¼Œå¿½ç•¥äº†é¢‘åŸŸçš„é‡è¦æ€§ã€‚</li>
<li>TF-Mambaæ˜¯ä¸€ä¸ªæ–°å‹å¤šåŸŸæ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶æ•æ‰æ—¶åŸŸå’Œé¢‘åŸŸçš„æƒ…æ„Ÿè¡¨è¾¾ã€‚</li>
<li>TF-Mambaé€šè¿‡æ—¶é¢‘mambaæ¨¡å—æå–æ—¶é¢‘æƒ…æ„Ÿç‰¹å¾ï¼Œå®ç°è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹è¡¨ç°åŠ›çš„å¹³è¡¡ã€‚</li>
<li>CMDTæŸå¤±çš„è®¾è®¡ä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰ä»£è¡¨æ€§çš„æƒ…æ„Ÿçº¿ç´¢ï¼Œæé«˜SERæ€§èƒ½ã€‚</li>
<li>åœ¨IEMOCAPå’ŒMELDæ•°æ®é›†ä¸Šï¼ŒTF-Mambaçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16904">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3444b7e3efbf67a3bd4cffc6d0ad7df9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afef9c64cbbc9b8c622bf71dc1f39338.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-614f2fd87ee29f1169ade004ca694a64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad087ddf902943d30a2ef09b0dbf55cc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Time-Graph-Frequency-Representation-with-Singular-Value-Decomposition-for-Neural-Speech-Enhancement"><a href="#Time-Graph-Frequency-Representation-with-Singular-Value-Decomposition-for-Neural-Speech-Enhancement" class="headerlink" title="Time-Graph Frequency Representation with Singular Value Decomposition   for Neural Speech Enhancement"></a>Time-Graph Frequency Representation with Singular Value Decomposition   for Neural Speech Enhancement</h2><p><strong>Authors:Tingting Wang, Tianrui Wang, Meng Ge, Qiquan Zhang, Zirui Ge, Zhen Yang</strong></p>
<p>Time-frequency (T-F) domain methods for monaural speech enhancement have benefited from the success of deep learning. Recently, focus has been put on designing two-stream network models to predict amplitude mask and phase separately, or, coupling the amplitude and phase into Cartesian coordinates and constructing real and imaginary pairs. However, most methods suffer from the alignment modeling of amplitude and phase (real and imaginary pairs) in a two-stream network framework, which inevitably incurs performance restrictions. In this paper, we introduce a graph Fourier transform defined with the singular value decomposition (GFT-SVD), resulting in real-valued time-graph representation for neural speech enhancement. This real-valued representation-based GFT-SVD provides an ability to align the modeling of amplitude and phase, leading to avoiding recovering the target speech phase information. Our findings demonstrate the effects of real-valued time-graph representation based on GFT-SVD for neutral speech enhancement. The extensive speech enhancement experiments establish that the combination of GFT-SVD and DNN outperforms the combination of GFT with the eigenvector decomposition (GFT-EVD) and magnitude estimation UNet, and outperforms the short-time Fourier transform (STFT) and DNN, regarding objective intelligibility and perceptual quality. We release our source code at: <a target="_blank" rel="noopener" href="https://github.com/Wangfighting0015/GFT/_project">https://github.com/Wangfighting0015/GFT\_project</a>. </p>
<blockquote>
<p>å¯¹äºå•å£°é“è¯­éŸ³å¢å¼ºçš„æ—¶é¢‘åŸŸï¼ˆT-Fï¼‰æ–¹æ³•ï¼Œæ·±åº¦å­¦ä¹ å·²ç»å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚æœ€è¿‘çš„ç ”ç©¶é‡ç‚¹ä¸»è¦æ”¾åœ¨è®¾è®¡åŒæµç½‘ç»œæ¨¡å‹ä¸Šï¼Œä»¥åˆ†åˆ«é¢„æµ‹æŒ¯å¹…æ©æ¨¡å’Œç›¸ä½ï¼Œæˆ–è€…å°†æŒ¯å¹…å’Œç›¸ä½è€¦åˆåˆ°ç¬›å¡å°”åæ ‡ä¸­å¹¶æ„å»ºå®è™šå¯¹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•åœ¨åŒæµç½‘ç»œæ¡†æ¶ä¸‹çš„æŒ¯å¹…å’Œç›¸ä½ï¼ˆå®è™šå¯¹ï¼‰å¯¹é½å»ºæ¨¡æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™ä¸å¯é¿å…åœ°å¯¼è‡´äº†æ€§èƒ½é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰å®šä¹‰çš„å›¾å‚…é‡Œå¶å˜æ¢ï¼ˆGFT-SVDï¼‰ï¼Œä»è€Œå®ç°äº†ç”¨äºç¥ç»è¯­éŸ³å¢å¼ºçš„å®æ•°æ—¶é—´å›¾è¡¨ç¤ºã€‚åŸºäºå®æ•°è¡¨ç¤ºçš„GFT-SVDæä¾›äº†å¯¹é½æŒ¯å¹…å’Œç›¸ä½å»ºæ¨¡çš„èƒ½åŠ›ï¼Œé¿å…äº†æ¢å¤ç›®æ ‡è¯­éŸ³ç›¸ä½ä¿¡æ¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºGFT-SVDçš„å®æ•°æ—¶é—´å›¾è¡¨ç¤ºå¯¹äºä¸­æ€§è¯­éŸ³å¢å¼ºå…·æœ‰æ˜¾è‘—æ•ˆæœã€‚å¤§é‡çš„è¯­éŸ³å¢å¼ºå®éªŒè¯å®ï¼ŒGFT-SVDä¸æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„ç»“åˆä¼˜äºGFTä¸ç‰¹å¾å‘é‡åˆ†è§£ï¼ˆGFT-EVDï¼‰çš„ç»“åˆä»¥åŠå¹…åº¦ä¼°è®¡UNetï¼Œå¹¶ä¸”åœ¨å®¢è§‚å¯æ‡‚åº¦å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¼˜äºçŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆSTFTï¼‰å’ŒDNNã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹é“¾æ¥å‘å¸ƒäº†æˆ‘ä»¬çš„æºä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/Wangfighting0015/GFT_project">https://github.com/Wangfighting0015/GFT_project</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16823v1">PDF</a> 5 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„æ—¶é¢‘åŸŸæ–¹æ³•åœ¨å•å£°é“è¯­éŸ³å¢å¼ºæ–¹é¢å–å¾—äº†æˆåŠŸã€‚è¿‘æœŸç ”ç©¶å¤šé›†ä¸­äºè®¾è®¡åŒæµç½‘ç»œæ¨¡å‹ä»¥åˆ†åˆ«é¢„æµ‹å¹…åº¦æ©ç å’Œç›¸ä½æ©ç ã€‚ä½†å¤§éƒ¨åˆ†æ–¹æ³•å—åˆ°åŒæµç½‘ç»œä¸­å¹…åº¦ä¸ç›¸ä½å»ºæ¨¡å¯¹ä½ä¸è‰¯çš„å½±å“ï¼Œå­˜åœ¨æ€§èƒ½é™åˆ¶ã€‚æœ¬æ–‡å¼•å…¥åŸºäºå¥‡å¼‚å€¼åˆ†è§£çš„å›¾å‚…é‡Œå¶å˜æ¢ï¼ˆGFT-SVDï¼‰ï¼Œäº§ç”Ÿç”¨äºç¥ç»è¯­éŸ³å¢å¼ºçš„å®å€¼æ—¶é—´å›¾è¡¨ç¤ºã€‚è¿™ç§å®å€¼è¡¨ç¤ºæ³•èƒ½å¤Ÿè‰¯å¥½åœ°è§£å†³å¹…åº¦ä¸ç›¸ä½çš„å»ºæ¨¡å¯¹ä½é—®é¢˜ï¼Œé¿å…ç›®æ ‡è¯­éŸ³ç›¸ä½ä¿¡æ¯çš„æ¢å¤ã€‚å®éªŒè¯æ˜ï¼ŒåŸºäºå®å€¼æ—¶é—´å›¾è¡¨ç¤ºçš„GFT-SVDæ–¹æ³•èƒ½æå‡ä¸­æ€§è¯­éŸ³å¢å¼ºçš„æ•ˆæœï¼Œä¸”åœ¨å®¢è§‚å¯ç†è§£æ€§å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢è¡¨ç°ä¼˜ç§€ã€‚ç›¸å…³ç ”ç©¶ä»£ç å·²å‘å¸ƒäºGitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒæµç½‘ç»œæ¨¡å‹å¹¿æ³›åº”ç”¨äºå•å£°é“è¯­éŸ³å¢å¼ºä¸­çš„æ—¶é¢‘åŸŸæ–¹æ³•ã€‚</li>
<li>åŒæµç½‘ç»œæ¨¡å‹é¢ä¸´å¹…åº¦ä¸ç›¸ä½å»ºæ¨¡çš„å¯¹ä½é—®é¢˜ï¼Œå½±å“æ€§èƒ½ã€‚</li>
<li>å¼•å…¥åŸºäºå¥‡å¼‚å€¼åˆ†è§£çš„å›¾å‚…é‡Œå¶å˜æ¢ï¼ˆGFT-SVDï¼‰ï¼Œç”Ÿæˆå®å€¼æ—¶é—´å›¾è¡¨ç¤ºç”¨äºè§£å†³å¹…åº¦ä¸ç›¸ä½å»ºæ¨¡é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æé«˜è¯­éŸ³å¢å¼ºæ€§èƒ½ï¼Œå°¤å…¶è¡¨ç°åœ¨å®¢è§‚å¯ç†è§£æ€§å’Œæ„ŸçŸ¥è´¨é‡ä¸Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-342a2cdc041a8f73ca9b8df3fc3d7cd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffa49783ef187100a7c44402b5a13b30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43d52ce4c19efb35169d29f18f9fae5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d301f59d6408a6a46145b1b4ba9a69b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d5e6b6abebea4d5dd31b482a22ad596.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SilVar-Speech-Driven-Multimodal-Model-for-Reasoning-Visual-Question-Answering-and-Object-Localization"><a href="#SilVar-Speech-Driven-Multimodal-Model-for-Reasoning-Visual-Question-Answering-and-Object-Localization" class="headerlink" title="SilVar: Speech Driven Multimodal Model for Reasoning Visual Question   Answering and Object Localization"></a>SilVar: Speech Driven Multimodal Model for Reasoning Visual Question   Answering and Object Localization</h2><p><strong>Authors:Tan-Hanh Pham, Hoang-Nam Le, Phu-Vinh Nguyen, Chris Ngo, Truong-Son Hy</strong></p>
<p>Visual Language Models have demonstrated remarkable capabilities across tasks, including visual question answering and image captioning. However, most models rely on text-based instructions, limiting their effectiveness in human-machine interactions. Moreover, the quality of language models depends on reasoning and prompting techniques, such as COT, which remain underexplored when using speech instructions. To address these challenges, we propose SilVar, a novel end-to-end multimodal model that uses speech instructions for reasoning in visual question answering. In addition, we investigate reasoning techniques with levels including conversational, simple, and complex speech instruction. SilVar is built upon CLIP, Whisper, and LLaMA 3.1-8B, enabling intuitive interactions by allowing users to provide verbal or text instructions. To this end, we introduce a dataset designed to challenge models with speech-based reasoning tasks for object localization. This dataset enhances the model ability to process and explain visual scenes from spoken input, moving beyond object recognition to reasoning-based interactions. The experiments show that SilVar achieves SOTA performance on the MMMU and ScienceQA benchmarks despite the challenge of speech-based instructions. We believe SilVar will inspire next-generation multimodal reasoning models, toward expert artificial general intelligence. Our code and dataset are available here. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨¡å‹ä¾èµ–äºæ–‡æœ¬æŒ‡ä»¤ï¼Œè¿™åœ¨äººæœºäº¤äº’ä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¯­è¨€æ¨¡å‹çš„è´¨é‡ä¾èµ–äºæ¨ç†å’Œæç¤ºæŠ€æœ¯ï¼Œå¦‚COTï¼ˆè®¤çŸ¥ç†è®ºï¼‰ï¼Œåœ¨ä½¿ç”¨è¯­éŸ³æŒ‡ä»¤æ—¶ä»è¢«æ¢ç´¢ä¸è¶³ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SilVarï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯çš„è·¨æ¨¡æ€æ¨¡å‹ï¼Œå®ƒä½¿ç”¨è¯­éŸ³æŒ‡ä»¤è¿›è¡Œè§†è§‰é—®ç­”ä¸­çš„æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŒ…æ‹¬å¯¹è¯ã€ç®€å•å’Œå¤æ‚è¯­éŸ³æŒ‡ä»¤åœ¨å†…çš„æ¨ç†æŠ€æœ¯ã€‚SilVarå»ºç«‹åœ¨CLIPã€Whisperå’ŒLLaMA 3.1-8Bçš„åŸºç¡€ä¸Šï¼Œå…è®¸ç”¨æˆ·æä¾›å£å¤´æˆ–æ–‡æœ¬æŒ‡ä»¤ï¼Œä»è€Œå®ç°ç›´è§‚äº¤äº’ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡åŸºäºè¯­éŸ³çš„æ¨ç†ä»»åŠ¡æ¥æŒ‘æˆ˜æ¨¡å‹è¿›è¡Œç›®æ ‡å®šä½ã€‚è¯¥æ•°æ®é›†æé«˜äº†æ¨¡å‹ä»å£è¯­è¾“å…¥å¤„ç†å¹¶è§£é‡Šè§†è§‰åœºæ™¯çš„èƒ½åŠ›ï¼Œè¶…è¶Šäº†ç›®æ ‡è¯†åˆ«ï¼Œå®ç°äº†åŸºäºæ¨ç†çš„äº¤äº’ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡é¢ä¸´åŸºäºè¯­éŸ³çš„æŒ‡ä»¤æŒ‘æˆ˜ï¼ŒSilVaråœ¨MMMUå’ŒScienceQAåŸºå‡†æµ‹è¯•ä¸­ä»è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡SilVarå°†å¯å‘ä¸‹ä¸€ä»£è·¨æ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œæœç€ä¸“å®¶é€šç”¨äººå·¥æ™ºèƒ½çš„æ–¹å‘å‘å±•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a href="%E4%BD%A0%E7%9A%84%E7%BD%91%E5%9D%80%E9%93%BE%E6%8E%A5">æ­¤å¤„è®¿é—®</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16771v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ç­‰ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨¡å‹ä¾èµ–æ–‡æœ¬æŒ‡ä»¤ï¼Œåœ¨äººæœºäº¤äº’ä¸­æ•ˆæœæœ‰é™ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºSilVarï¼Œä¸€ç§ä½¿ç”¨è¯­éŸ³æŒ‡ä»¤è¿›è¡Œè§†è§‰é—®ç­”æ¨ç†çš„æ–°å‹ç«¯åˆ°ç«¯å¤šæ¨¡æ€æ¨¡å‹ã€‚SilVarç»“åˆCLIPã€Whisperå’ŒLLaMA 3.1-8Bæ„å»ºï¼Œå…è®¸ç”¨æˆ·é€šè¿‡å£å¤´æˆ–æ–‡æœ¬æŒ‡ä»¤è¿›è¡Œç›´è§‚äº¤äº’ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œæ—¨åœ¨é€šè¿‡è¯­éŸ³æ¨ç†ä»»åŠ¡æŒ‘æˆ˜æ¨¡å‹åœ¨ç‰©ä½“å®šä½æ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡å­˜åœ¨è¯­éŸ³æŒ‡ä»¤çš„æŒ‘æˆ˜ï¼ŒSilVaråœ¨MMMUå’ŒScienceQAåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å“è¶Šè¡¨ç°ã€‚æˆ‘ä»¬ç›¸ä¿¡SilVarå°†å¯å‘ä¸‹ä¸€ä»£å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½å‘ä¸“å®¶çº§é€šç”¨æ™ºèƒ½å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>å¤§å¤šæ•°è§†è§‰è¯­è¨€æ¨¡å‹ä¾èµ–æ–‡æœ¬æŒ‡ä»¤ï¼Œé™åˆ¶äº†å…¶åœ¨äººæœºäº¤äº’ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯­éŸ³æŒ‡ä»¤ä¸ºè§†è§‰é—®ç­”æ¨ç†æä¾›äº†æ–°é€”å¾„ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„äº¤äº’æ€§å’Œå®ç”¨æ€§ã€‚</li>
<li>SilVaræ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œç»“åˆäº†CLIPã€Whisperå’ŒLLaMAæŠ€æœ¯ï¼Œæ”¯æŒå£å¤´æˆ–æ–‡æœ¬æŒ‡ä»¤ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œç”¨äºæŒ‘æˆ˜æ¨¡å‹åœ¨è¯­éŸ³æ¨ç†ä»»åŠ¡ä¸­çš„ç‰©ä½“å®šä½èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSilVaråœ¨MMMUå’ŒScienceQAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°½ç®¡é¢ä¸´è¯­éŸ³æŒ‡ä»¤çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-702c880f40660d5be0da75439badbd14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae30b21009d06e894795606c86ed0dfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18b2877f64abc26fa6d429b8377107b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f5ddc01e87c027a115f7c1688389a61.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Mamba-SEUNet-Mamba-UNet-for-Monaural-Speech-Enhancement"><a href="#Mamba-SEUNet-Mamba-UNet-for-Monaural-Speech-Enhancement" class="headerlink" title="Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement"></a>Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement</h2><p><strong>Authors:Junyu Wang, Zizhen Lin, Tianrui Wang, Meng Ge, Longbiao Wang, Jianwu Dang</strong></p>
<p>In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73. </p>
<blockquote>
<p>åœ¨æœ€è¿‘çš„è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ç ”ç©¶ä¸­ï¼ŒTransformeråŠå…¶å˜ä½“å·²ç»æˆä¸ºä¸»è¦çš„æ–¹æ³•è®ºã€‚ç„¶è€Œï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§å¯¹å®é™…éƒ¨ç½²æ–½åŠ äº†ä¸€å®šçš„é™åˆ¶ã€‚Mambaä½œä¸ºä¸€ç§æ–°å‹çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œç”±äºå…¶å¼ºå¤§çš„é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›å’Œç›¸å¯¹è¾ƒä½çš„è®¡ç®—å¤æ‚æ€§ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Mamba-SEUNetï¼Œè¿™æ˜¯ä¸€ç§å°†Mambaä¸U-Netç»“åˆç”¨äºSEä»»åŠ¡çš„åˆ›æ–°æ¶æ„ã€‚é€šè¿‡åˆ©ç”¨åŒå‘Mambaæ¥å»ºæ¨¡è¯­éŸ³ä¿¡å·åœ¨ä¸åŒåˆ†è¾¨ç‡ä¸Šçš„å‰å‘å’Œåå‘ä¾èµ–å…³ç³»ï¼Œå¹¶ç»“åˆè·³è¿‡è¿æ¥æ¥æ•è·å¤šå°ºåº¦ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°ï¼ˆSOTAï¼‰çš„æ€§èƒ½ã€‚åœ¨VCTK+DEMANDæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMamba-SEUNetçš„PESQå¾—åˆ†ä¸º3.59ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—å¤æ‚æ€§ã€‚å½“ä¸æ„ŸçŸ¥å¯¹æ¯”åº¦æ‹‰ä¼¸æŠ€æœ¯ç›¸ç»“åˆæ—¶ï¼ŒMamba-SEUNetçš„PESQå¾—åˆ†è¿›ä¸€æ­¥æé«˜åˆ°3.73ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16626v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Mamba-SEUNetç»“åˆMambaå’ŒU-Netï¼Œåˆ©ç”¨åŒå‘Mambaå¯¹è¯­éŸ³ä¿¡å·çš„å‰å‘å’Œåå‘ä¾èµ–è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°è¯­éŸ³å¢å¼ºä»»åŠ¡çš„é«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMamba-SEUNetåœ¨VCTK+DEMANDæ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒé«˜çš„PESQè¯„åˆ†ï¼Œä¸”è®¡ç®—å¤æ‚åº¦è¾ƒä½ã€‚ä¸æ„ŸçŸ¥å¯¹æ¯”åº¦æ‹‰ä¼¸æŠ€æœ¯ç»“åˆåï¼Œæ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸè¯­éŸ³å¢å¼ºç ”ç©¶ä¸­ï¼ŒTransformeråŠå…¶å˜ä½“æ˜¯ä¸»è¦çš„æ–¹æ³•è®ºã€‚</li>
<li>Mambaä½œä¸ºä¸€ç§æ–°å‹çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œåœ¨å¤©ç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚</li>
<li>Mamba-SEUNetç»“åˆäº†Mambaå’ŒU-Netï¼Œç”¨äºè¯­éŸ³å¢å¼ºä»»åŠ¡ã€‚</li>
<li>Mamba-SEUNetåˆ©ç”¨åŒå‘Mambaå¯¹è¯­éŸ³ä¿¡å·çš„ä¸åŒåˆ†è¾¨ç‡è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶åŠ å…¥è·³è·ƒè¿æ¥æ¥æ•æ‰å¤šå°ºåº¦ä¿¡æ¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMamba-SEUNetåœ¨VCTK+DEMANDæ•°æ®é›†ä¸Šå–å¾—äº†å…ˆè¿›æ€§èƒ½ï¼ŒPESQè¯„åˆ†ä¸º3.59ã€‚</li>
<li>Mamba-SEUNetè®¡ç®—å¤æ‚åº¦è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7271332620fa42bcf82d619279ffd185.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0b610ceff8eb4f9f4a2970cddbff175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4be3fd2343b6225fc8e3f924d60f5dfa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12a4844837943400195f9ce9b520a496.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41f4e90603b828568e3af194b361b54e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SemTalk-Holistic-Co-speech-Motion-Generation-with-Frame-level-Semantic-Emphasis"><a href="#SemTalk-Holistic-Co-speech-Motion-Generation-with-Frame-level-Semantic-Emphasis" class="headerlink" title="SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic   Emphasis"></a>SemTalk: Holistic Co-speech Motion Generation with Frame-level Semantic   Emphasis</h2><p><strong>Authors:Xiangyue Zhang, Jiangfang Li, Jiaxu Zhang, Ziqiang Dang, Jianqiang Ren, Liefeng Bo, Zhigang Tu</strong></p>
<p>A good co-speech motion generation cannot be achieved without a careful integration of common rhythmic motion and rare yet essential semantic motion. In this work, we propose SemTalk for holistic co-speech motion generation with frame-level semantic emphasis. Our key insight is to separately learn general motions and sparse motions, and then adaptively fuse them. In particular, rhythmic consistency learning is explored to establish rhythm-related base motion, ensuring a coherent foundation that synchronizes gestures with the speech rhythm. Subsequently, textit{semantic emphasis learning is designed to generate semantic-aware sparse motion, focusing on frame-level semantic cues. Finally, to integrate sparse motion into the base motion and generate semantic-emphasized co-speech gestures, we further leverage a learned semantic score for adaptive synthesis. Qualitative and quantitative comparisons on two public datasets demonstrate that our method outperforms the state-of-the-art, delivering high-quality co-speech motion with enhanced semantic richness over a stable base motion. </p>
<blockquote>
<p>åœ¨å…±åŒè¯­è¨€è¿åŠ¨ç”Ÿæˆä¸­ï¼Œå¦‚æœä¸ä»”ç»†èåˆå¸¸è§çš„èŠ‚å¥è¿åŠ¨å’Œç½•è§ä½†è‡³å…³é‡è¦çš„è¯­ä¹‰è¿åŠ¨ï¼Œå°±æ— æ³•å®ç°è‰¯å¥½çš„å…±åŒè¯­è¨€è¿åŠ¨ç”Ÿæˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºæ•´ä½“å…±åŒè¯­è¨€è¿åŠ¨ç”Ÿæˆçš„SemTalkæ–¹æ³•ï¼Œå…·æœ‰å¸§çº§è¯­ä¹‰å¼ºè°ƒã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯åˆ†åˆ«å­¦ä¹ å¸¸è§„è¿åŠ¨å’Œç¨€ç–è¿åŠ¨ï¼Œç„¶åè‡ªé€‚åº”åœ°èåˆå®ƒä»¬ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ¢ç´¢äº†èŠ‚å¥ä¸€è‡´æ€§å­¦ä¹ ä»¥å»ºç«‹ä¸èŠ‚å¥ç›¸å…³çš„åŸºæœ¬è¿åŠ¨ï¼Œç¡®ä¿æ‰‹åŠ¿ä¸è¯­éŸ³èŠ‚å¥çš„åŒæ­¥åè°ƒåŸºç¡€ã€‚éšåï¼Œè®¾è®¡äº†è¯­ä¹‰é‡ç‚¹å­¦ä¹ ä»¥ç”Ÿæˆå…·æœ‰è¯­ä¹‰æ„ŸçŸ¥çš„ç¨€ç–è¿åŠ¨ï¼Œä¾§é‡äºå¸§çº§è¯­ä¹‰çº¿ç´¢ã€‚æœ€åï¼Œä¸ºäº†å°†ç¨€ç–è¿åŠ¨èå…¥åŸºæœ¬è¿åŠ¨å¹¶ç”Ÿæˆå…·æœ‰è¯­ä¹‰å¼ºè°ƒçš„å…±åŒè¯­è¨€æ‰‹åŠ¿ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åˆ©ç”¨å­¦ä¹ åˆ°çš„è¯­ä¹‰åˆ†æ•°è¿›è¡Œè‡ªé€‚åº”åˆæˆã€‚åœ¨ä¸¤é¡¹å…¬å…±æ•°æ®é›†ä¸Šçš„å®šæ€§å’Œå®šé‡æ¯”è¾ƒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæä¾›é«˜è´¨é‡çš„å…±åŒè¯­è¨€è¿åŠ¨ï¼Œåœ¨ç¨³å®šçš„åŸºæœ¬è¿åŠ¨ä¸Šå¢åŠ äº†è¯­ä¹‰ä¸°å¯Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16563v1">PDF</a> 11 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè¯­ä¹‰å¼ºè°ƒçš„ååŒè¯­éŸ³åŠ¨ä½œç”Ÿæˆæ–¹æ³•SemTalkï¼Œé€šè¿‡åˆ†ç¦»å­¦ä¹ é€šç”¨åŠ¨ä½œå’Œç¨€ç–åŠ¨ä½œï¼Œå¹¶è‡ªé€‚åº”èåˆã€‚é€šè¿‡èŠ‚å¥ä¸€è‡´æ€§å­¦ä¹ å»ºç«‹ä¸èŠ‚å¥ç›¸å…³çš„åŸºæœ¬åŠ¨ä½œï¼Œç¡®ä¿åŠ¨ä½œä¸è¯­éŸ³èŠ‚å¥çš„åŒæ­¥ã€‚è¯­ä¹‰å¼ºè°ƒå­¦ä¹ ç”¨äºç”Ÿæˆè¯­ä¹‰æ„ŸçŸ¥çš„ç¨€ç–åŠ¨ä½œï¼Œä¸“æ³¨äºå¸§çº§è¯­ä¹‰çº¿ç´¢ã€‚æœ€åï¼Œåˆ©ç”¨å­¦ä¹ çš„è¯­ä¹‰åˆ†æ•°å°†ç¨€ç–åŠ¨ä½œé›†æˆåˆ°åŸºæœ¬åŠ¨ä½œä¸­ï¼Œç”Ÿæˆå…·æœ‰è¯­ä¹‰å¼ºè°ƒçš„ååŒè¯­éŸ³åŠ¨ä½œã€‚æ­¤æ–¹æ³•åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œèƒ½ç”Ÿæˆé«˜è´¨é‡ã€è¯­ä¹‰ä¸°å¯Œçš„ååŒè¯­éŸ³åŠ¨ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ååŒè¯­éŸ³åŠ¨ä½œç”Ÿæˆéœ€ç»“åˆé€šç”¨èŠ‚å¥åŠ¨ä½œå’Œå…³é”®è¯­ä¹‰åŠ¨ä½œã€‚</li>
<li>æå‡ºSemTalkæ–¹æ³•ï¼Œé€šè¿‡åˆ†ç¦»å­¦ä¹ é€šç”¨åŠ¨ä½œå’Œç¨€ç–åŠ¨ä½œï¼Œå®ç°å…¨æ¯ååŒè¯­éŸ³åŠ¨ä½œç”Ÿæˆã€‚</li>
<li>èŠ‚å¥ä¸€è‡´æ€§å­¦ä¹ ç¡®ä¿åŠ¨ä½œä¸è¯­éŸ³èŠ‚å¥çš„åŒæ­¥ã€‚</li>
<li>è¯­ä¹‰å¼ºè°ƒå­¦ä¹ å…³æ³¨å¸§çº§è¯­ä¹‰çº¿ç´¢ï¼Œç”Ÿæˆè¯­ä¹‰æ„ŸçŸ¥çš„ç¨€ç–åŠ¨ä½œã€‚</li>
<li>åˆ©ç”¨å­¦ä¹ çš„è¯­ä¹‰åˆ†æ•°è‡ªé€‚åº”åˆæˆååŒè¯­éŸ³åŠ¨ä½œã€‚</li>
<li>åœ¨å…¬å…±æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜äºç°æœ‰æŠ€æœ¯çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9a8ad306b21de3cd31a2555c99485585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ea2f7d8193e1f32cf6e1cb15de3fa79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dd2a2dfae00d7c17206de4de1469f07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dc0da43cbf30e7013e1c4f64e6acd90.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Improving-Lip-synchrony-in-Direct-Audio-Visual-Speech-to-Speech-Translation"><a href="#Improving-Lip-synchrony-in-Direct-Audio-Visual-Speech-to-Speech-Translation" class="headerlink" title="Improving Lip-synchrony in Direct Audio-Visual Speech-to-Speech   Translation"></a>Improving Lip-synchrony in Direct Audio-Visual Speech-to-Speech   Translation</h2><p><strong>Authors:Lucas Goncalves, Prashant Mathur, Xing Niu, Brady Houston, Chandrashekhar Lavania, Srikanth Vishnubhotla, Lijia Sun, Anthony Ferritto</strong></p>
<p>Audio-Visual Speech-to-Speech Translation typically prioritizes improving translation quality and naturalness. However, an equally critical aspect in audio-visual content is lip-synchrony-ensuring that the movements of the lips match the spoken content-essential for maintaining realism in dubbed videos. Despite its importance, the inclusion of lip-synchrony constraints in AVS2S models has been largely overlooked. This study addresses this gap by integrating a lip-synchrony loss into the training process of AVS2S models. Our proposed method significantly enhances lip-synchrony in direct audio-visual speech-to-speech translation, achieving an average LSE-D score of 10.67, representing a 9.2% reduction in LSE-D over a strong baseline across four language pairs. Additionally, it maintains the naturalness and high quality of the translated speech when overlaid onto the original video, without any degradation in translation quality. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘é€šå¸¸ä¼˜å…ˆè€ƒè™‘æé«˜ç¿»è¯‘è´¨é‡å’Œè‡ªç„¶åº¦ã€‚ç„¶è€Œï¼Œåœ¨è§†å¬å†…å®¹ä¸­ï¼ŒåŒæ ·å…³é”®çš„ä¸€ä¸ªæ–¹é¢æ˜¯å”‡åŒæ­¥é—®é¢˜â€”â€”ç¡®ä¿å˜´å”‡çš„åŠ¨ä½œä¸æ‰€è¯´è¯çš„å†…å®¹ç›¸åŒ¹é…ï¼Œè¿™å¯¹äºä¿æŒé…éŸ³è§†é¢‘çš„çœŸå®æ€§è‡³å…³é‡è¦ã€‚å°½ç®¡å…¶é‡è¦æ€§ä¸å®¹å¿½è§†ï¼Œä½†åœ¨AVS2Sæ¨¡å‹ä¸­èå…¥å”‡åŒæ­¥çº¦æŸå´è¢«å¤§å¤§å¿½è§†äº†ã€‚æœ¬ç ”ç©¶é€šè¿‡å‘AVS2Sæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å¼•å…¥å”‡åŒæ­¥æŸå¤±æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ˜¾è‘—æé«˜äº†ç›´æ¥è§†å¬è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ä¸­çš„å”‡åŒæ­¥æ•ˆæœï¼Œå¹³å‡LSE-Då¾—åˆ†ä¸º10.67ï¼Œåœ¨å››ç§è¯­è¨€å¯¹ä¸Šç›¸å¯¹äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ï¼ŒLSE-Dé™ä½äº†9.2%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç¿»è¯‘è¯­éŸ³çš„è‡ªç„¶æ€§å’Œé«˜è´¨é‡çš„åŒæ—¶ï¼Œå°†å…¶è¦†ç›–åœ¨åŸå§‹è§†é¢‘ä¸Šï¼Œæ²¡æœ‰é™ä½ç¿»è¯‘è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16530v1">PDF</a> Accepted at ICASSP, 4 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†éŸ³é¢‘è§†è§‰è¯­éŸ³åˆ°è¯­éŸ³çš„ç¿»è¯‘é—®é¢˜ï¼Œå¼ºè°ƒäº†å”‡åŒæ­¥çš„é‡è¦æ€§ï¼Œå³åœ¨éŸ³é¢‘è§†è§‰å†…å®¹ä¸­ç¡®ä¿å˜´å”‡åŠ¨ä½œä¸å£è¯­å†…å®¹ç›¸åŒ¹é…ï¼Œå¯¹äºä¿æŒè§†é¢‘é…éŸ³çš„çœŸå®æ€§è‡³å…³é‡è¦ã€‚è¯¥ç ”ç©¶é€šè¿‡å°†å”‡åŒæ­¥æŸå¤±çº³å…¥AVS2Sæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨ç›´æ¥éŸ³é¢‘è§†è§‰è¯­éŸ³åˆ°è¯­éŸ³çš„ç¿»è¯‘ä¸­æ˜¾è‘—æé«˜äº†å”‡åŒæ­¥æ€§ï¼Œåœ¨å››ç§è¯­è¨€å¯¹ä¸Šå¹³å‡LSE-Då¾—åˆ†ä¸º10.67ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿é™ä½äº†9.2%ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç¿»è¯‘è¯­éŸ³çš„è‡ªç„¶æ€§å’Œé«˜è´¨é‡æ—¶ï¼Œä¸ä¼šé™ä½ç¿»è¯‘è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘è§†è§‰è¯­éŸ³åˆ°è¯­éŸ³çš„ç¿»è¯‘ä¸­ï¼Œå”‡åŒæ­¥çš„é‡è¦æ€§è¢«å¼ºè°ƒï¼Œå®ƒå¯¹äºä¿æŒè§†é¢‘é…éŸ³çš„çœŸå®æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸­ï¼Œå°†å”‡åŒæ­¥çº¦æŸçº³å…¥AVS2Sæ¨¡å‹çš„åšæ³•è¢«å¿½è§†ã€‚</li>
<li>ç ”ç©¶é€šè¿‡æ•´åˆå”‡åŒæ­¥æŸå¤±åˆ°AVS2Sæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæé«˜äº†å”‡åŒæ­¥æ€§ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨å››ç§è¯­è¨€å¯¹ä¸Šå¹³å‡LSE-Då¾—åˆ†ä¸º10.67ï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿é™ä½äº†9.2%ã€‚</li>
<li>æ–¹æ³•åœ¨æ”¹å–„å”‡åŒæ­¥æ€§çš„åŒæ—¶ï¼Œä¸å½±å“ç¿»è¯‘è¯­éŸ³çš„è‡ªç„¶æ€§å’Œé«˜è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿä¸ºç›´æ¥éŸ³é¢‘è§†è§‰è¯­éŸ³åˆ°è¯­éŸ³çš„ç¿»è¯‘å¸¦æ¥æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16530">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-49a85b33226ba3d48c4315d74f2bdd49.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16530v1/page_1_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4934d23aafad627f0fb11e4d0509e7cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0dacfea3c0766ca8ca46af277ca2af6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcbbd84322d9da92143cc8124c0994ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4253973140ef00a18dcc37b5a149ce4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Adapting-Whisper-for-Code-Switching-through-Encoding-Refining-and-Language-Aware-Decoding"><a href="#Adapting-Whisper-for-Code-Switching-through-Encoding-Refining-and-Language-Aware-Decoding" class="headerlink" title="Adapting Whisper for Code-Switching through Encoding Refining and   Language-Aware Decoding"></a>Adapting Whisper for Code-Switching through Encoding Refining and   Language-Aware Decoding</h2><p><strong>Authors:Jiahui Zhao, Hao Shi, Chenrui Cui, Tianrui Wang, Hexin Liu, Zhaoheng Ni, Lingxuan Ye, Longbiao Wang</strong></p>
<p>Code-switching (CS) automatic speech recognition (ASR) faces challenges due to the language confusion resulting from accents, auditory similarity, and seamless language switches. Adaptation on the pre-trained multi-lingual model has shown promising performance for CS-ASR. In this paper, we adapt Whisper, which is a large-scale multilingual pre-trained speech recognition model, to CS from both encoder and decoder parts. First, we propose an encoder refiner to enhance the encoderâ€™s capacity of intra-sentence swithching. Second, we propose using two sets of language-aware adapters with different language prompt embeddings to achieve language-specific decoding information in each decoder layer. Then, a fusion module is added to fuse the language-aware decoding. The experimental results using the SEAME dataset show that, compared with the baseline model, the proposed approach achieves a relative MER reduction of 4.1% and 7.2% on the dev_man and dev_sge test sets, respectively, surpassing state-of-the-art methods. Through experiments, we found that the proposed method significantly improves the performance on non-native language in CS speech, indicating that our approach enables Whisper to better distinguish between the two languages. </p>
<blockquote>
<p>ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é¢ä¸´ç€ç”±äºå£éŸ³ã€å¬è§‰ç›¸ä¼¼æ€§å’Œæ— ç¼è¯­è¨€åˆ‡æ¢å¯¼è‡´çš„è¯­è¨€æ··æ·†æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚é¢„è®­ç»ƒçš„å¤šå…ƒè¯­è¨€æ¨¡å‹çš„è°ƒæ•´å¯¹CS-ASRè¡¨ç°å‡ºäº†æœ‰å‰æ™¯çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€‚åº”äº†å¤§è§„æ¨¡çš„å¤šå…ƒè¯­è¨€é¢„è®­ç»ƒè¯­éŸ³è¯†åˆ«æ¨¡å‹Whisperï¼Œæ—¢é€‚åº”äºç¼–ç å™¨éƒ¨åˆ†ä¹Ÿé€‚åº”äºè§£ç å™¨éƒ¨åˆ†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç¼–ç å™¨ç²¾ç‚¼å™¨ï¼Œä»¥æé«˜ç¼–ç å™¨åœ¨å¥å­å†…åˆ‡æ¢çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ä¸¤ç»„å¸¦æœ‰ä¸åŒè¯­è¨€æç¤ºåµŒå…¥çš„è¯­è¨€æ„ŸçŸ¥é€‚é…å™¨æ¥å®ç°æ¯ä¸ªè§£ç å™¨å±‚ä¸­çš„è¯­è¨€ç‰¹å®šè§£ç ä¿¡æ¯ã€‚ç„¶åï¼Œæ·»åŠ ä¸€ä¸ªèåˆæ¨¡å—æ¥èåˆè¯­è¨€æ„ŸçŸ¥è§£ç ã€‚ä½¿ç”¨SEAMEæ•°æ®é›†çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨dev_manå’Œdev_sgeæµ‹è¯•é›†ä¸Šç›¸å¯¹MERåˆ†åˆ«é™ä½äº†4.1%å’Œ7.2%ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°è¯¥æ–¹æ³•åœ¨éæ¯è¯­åœ¨CSè¯­éŸ³ä¸Šçš„è¡¨ç°æ˜¾è‘—æé«˜ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•ä½¿Whisperèƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†ä¸¤ç§è¯­è¨€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16507v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨å¤„ç†ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰æ—¶çš„æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡å¯¹å¤§å‹é¢„è®­ç»ƒçš„å¤šè¯­è¨€è¯­éŸ³æ¨¡å‹Whisperè¿›è¡Œé€‚åº”å–å¾—äº†è¿›å±•ã€‚è®ºæ–‡é€šè¿‡å¢å¼ºç¼–ç å™¨å†…çš„å¥å­åˆ‡æ¢èƒ½åŠ›å¹¶å¼•å…¥è¯­è¨€æ„ŸçŸ¥è§£ç å™¨æ¥æé«˜CSè¯­éŸ³è¯†åˆ«çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹åœ¨ç›¸å¯¹è¯é”™è¯¯ç‡ä¸Šæœ‰æ‰€é™ä½ï¼Œä¸”åœ¨å¤„ç†éæ¯è¯­è¯­è¨€æ—¶è¡¨ç°æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é¢ä¸´ç”±å£éŸ³ã€å¬è§‰ç›¸ä¼¼æ€§å’Œæ— ç¼è¯­è¨€åˆ‡æ¢å¼•èµ·çš„è¯­è¨€æ··æ·†æŒ‘æˆ˜ã€‚</li>
<li>é€‚åº”é¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹å¯¹äºCS-ASRå…·æœ‰å‰æ™¯ã€‚</li>
<li>å¯¹Whisperæ¨¡å‹çš„ç¼–ç å™¨è¿›è¡Œæ”¹è¿›ï¼Œå¢å¼ºå…¶å¥å­å†…åˆ‡æ¢èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥è¯­è¨€æ„ŸçŸ¥é€‚é…å™¨ä¸è¯­è¨€æç¤ºåµŒå…¥ï¼Œå®ç°è§£ç å™¨çš„è¯­è¨€ç‰¹å¼‚æ€§ã€‚</li>
<li>é€šè¿‡èåˆæ¨¡å—èåˆè¯­è¨€æ„ŸçŸ¥è§£ç ç»“æœã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿æ¨¡å‹åœ¨SEAMEæ•°æ®é›†ä¸Šçš„æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6c84e9aa5557cb4d4e76a680e605e901.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a30302356c17d7ddd2d5b0cd9dad213.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a09f11164dca0020d9cea40b4582ee9.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Speech-Retrieval-Augmented-Generation-without-Automatic-Speech-Recognition"><a href="#Speech-Retrieval-Augmented-Generation-without-Automatic-Speech-Recognition" class="headerlink" title="Speech Retrieval-Augmented Generation without Automatic Speech   Recognition"></a>Speech Retrieval-Augmented Generation without Automatic Speech   Recognition</h2><p><strong>Authors:Do June Min, Karel Mundnich, Andy Lapastora, Erfan Soltanmohammadi, Srikanth Ronanki, Kyu Han</strong></p>
<p>One common approach for question answering over speech data is to first transcribe speech using automatic speech recognition (ASR) and then employ text-based retrieval-augmented generation (RAG) on the transcriptions. While this cascaded pipeline has proven effective in many practical settings, ASR errors can propagate to the retrieval and generation steps. To overcome this limitation, we introduce SpeechRAG, a novel framework designed for open-question answering over spoken data. Our proposed approach fine-tunes a pre-trained speech encoder into a speech adapter fed into a frozen large language model (LLM)â€“based retrieval model. By aligning the embedding spaces of text and speech, our speech retriever directly retrieves audio passages from text-based queries, leveraging the retrieval capacity of the frozen text retriever. Our retrieval experiments on spoken question answering datasets show that direct speech retrieval does not degrade over the text-based baseline, and outperforms the cascaded systems using ASR. For generation, we use a speech language model (SLM) as a generator, conditioned on audio passages rather than transcripts. Without fine-tuning of the SLM, this approach outperforms cascaded text-based models when there is high WER in the transcripts. </p>
<blockquote>
<p>é’ˆå¯¹è¯­éŸ³æ•°æ®çš„é—®ç­”çš„ä¸€ä¸ªå¸¸è§æ–¹æ³•æ˜¯é¦–å…ˆä½¿ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è¿›è¡Œè¯­éŸ³è½¬å½•ï¼Œç„¶ååœ¨è½¬å½•ä¸Šåº”ç”¨åŸºäºæ–‡æœ¬çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚è™½ç„¶è¿™ç§çº§è”ç®¡é“å·²åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ASRé”™è¯¯å¯èƒ½ä¼šä¼ æ’­åˆ°æ£€ç´¢å’Œç”Ÿæˆæ­¥éª¤ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpeechRAGï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¼€æ”¾å¼é—®é¢˜å›ç­”è®¾è®¡çš„å…¨æ–°æ¡†æ¶ï¼Œç”¨äºå¤„ç†å£è¯­æ•°æ®ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ˜¯å¯¹é¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”äºåŸºäºå†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢æ¨¡å‹çš„è¯­éŸ³é€‚é…å™¨ã€‚é€šè¿‡å¯¹æ–‡æœ¬å’Œè¯­éŸ³çš„åµŒå…¥ç©ºé—´è¿›è¡Œå¯¹é½ï¼Œæˆ‘ä»¬çš„è¯­éŸ³æ£€ç´¢å™¨å¯ä»¥ç›´æ¥ä»åŸºäºæ–‡æœ¬çš„æŸ¥è¯¢ä¸­æ£€ç´¢éŸ³é¢‘ç‰‡æ®µï¼Œåˆ©ç”¨å†»ç»“çš„æ–‡æœ¬æ£€ç´¢å™¨çš„æ£€ç´¢èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨å£è¯­é—®ç­”æ•°æ®é›†ä¸Šçš„æ£€ç´¢å®éªŒè¡¨æ˜ï¼Œç›´æ¥è¯­éŸ³æ£€ç´¢å¹¶ä¸äºšäºåŸºäºæ–‡æœ¬çš„åŸºçº¿ï¼Œå¹¶ä¸”è¡¨ç°ä¼˜äºä½¿ç”¨ASRçš„çº§è”ç³»ç»Ÿã€‚å¯¹äºç”Ÿæˆï¼Œæˆ‘ä»¬ä½¿ç”¨è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä½œä¸ºç”Ÿæˆå™¨ï¼Œä»¥éŸ³é¢‘ç‰‡æ®µä¸ºæ¡ä»¶ï¼Œè€Œä¸æ˜¯æ–‡æœ¬ã€‚åœ¨ä¸å¾®è°ƒSLMçš„æƒ…å†µä¸‹ï¼Œå½“è½¬å½•ä¸­çš„å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰è¾ƒé«˜æ—¶ï¼Œæ­¤æ–¹æ³•çš„è¡¨ç°ä¼˜äºçº§è”çš„åŸºäºæ–‡æœ¬æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16500v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šä¸ºæé«˜è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é”™è¯¯åœ¨é—®ç­”ç³»ç»Ÿä¸­çš„å½±å“ï¼Œå¼•å…¥SpeechRAGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨å¹¶å°†å…¶è¾“å…¥åˆ°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢æ¨¡å‹ä¸­ï¼Œå®ç°ç›´æ¥è¯­éŸ³æ£€ç´¢ã€‚é€šè¿‡æ–‡æœ¬å’Œè¯­éŸ³åµŒå…¥ç©ºé—´çš„å¯¹é½ï¼Œè¯¥è¯­éŸ³æ£€ç´¢å™¨å¯ç›´æ¥ä»åŸºäºæ–‡æœ¬çš„æŸ¥è¯¢ä¸­æ£€ç´¢éŸ³é¢‘ç‰‡æ®µã€‚å®éªŒè¡¨æ˜ï¼Œç›´æ¥è¯­éŸ³æ£€ç´¢åœ¨å£è¯­é—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸äºšäºåŸºäºæ–‡æœ¬çš„åŸºçº¿ç³»ç»Ÿï¼Œä¸”ä¼˜äºçº§è”ç³»ç»Ÿã€‚ç”Ÿæˆéƒ¨åˆ†åˆ™é‡‡ç”¨è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä½œä¸ºç”Ÿæˆå™¨ï¼Œä»¥éŸ³é¢‘ç‰‡æ®µä¸ºæ¡ä»¶ï¼Œæ— éœ€å¯¹SLMè¿›è¡Œå¾®è°ƒï¼Œå½“å­—å¹•é”™è¯¯ç‡è¾ƒé«˜æ—¶ï¼Œè¡¨ç°ä¼˜äºçº§è”çš„åŸºäºæ–‡æœ¬æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SpeechRAGæ˜¯ä¸€ä¸ªä¸ºå£è¯­é—®ç­”è®¾è®¡çš„å…¨æ–°æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨ä»¥è¿›è¡Œè¯­éŸ³é€‚é…ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ£€ç´¢ï¼Œå®ç°ç›´æ¥è¯­éŸ³æ£€ç´¢ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬å’Œè¯­éŸ³åµŒå…¥ç©ºé—´çš„å¯¹é½ï¼ŒSpeechRAGèƒ½å¤Ÿä»åŸºäºæ–‡æœ¬çš„æŸ¥è¯¢ä¸­ç›´æ¥æ£€ç´¢éŸ³é¢‘ç‰‡æ®µã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œç›´æ¥è¯­éŸ³æ£€ç´¢åœ¨å£è¯­é—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸åŸºäºæ–‡æœ¬çš„åŸºçº¿ç³»ç»Ÿç›¸å½“ã€‚</li>
<li>ä¸çº§è”ç³»ç»Ÿç›¸æ¯”ï¼ŒSpeechRAGå…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>åœ¨ç”Ÿæˆé˜¶æ®µï¼Œé‡‡ç”¨è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ä½œä¸ºç”Ÿæˆå™¨ï¼Œä»¥éŸ³é¢‘ç‰‡æ®µä¸ºæ¡ä»¶ï¼Œæ— éœ€å¯¹SLMè¿›è¡Œå¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e7f31e227c45d8188e738cc50544676.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-beeeea3271dca8686eaa3dddb32c34d1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16500v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16500v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16500v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Enhancing-Multilingual-ASR-for-Unseen-Languages-via-Language-Embedding-Modeling"><a href="#Enhancing-Multilingual-ASR-for-Unseen-Languages-via-Language-Embedding-Modeling" class="headerlink" title="Enhancing Multilingual ASR for Unseen Languages via Language Embedding   Modeling"></a>Enhancing Multilingual ASR for Unseen Languages via Language Embedding   Modeling</h2><p><strong>Authors:Shao-Syuan Huang, Kuan-Po Huang, Andy T. Liu, Hung-yi Lee</strong></p>
<p>Multilingual Automatic Speech Recognition (ASR) aims to recognize and transcribe speech from multiple languages within a single system. Whisper, one of the most advanced ASR models, excels in this domain by handling 99 languages effectively, leveraging a vast amount of data and incorporating language tags as prefixes to guide the recognition process. However, despite its success, Whisper struggles with unseen languages, those not included in its pre-training. Motivated by the observation that many languages share linguistic characteristics, we propose methods that exploit these relationships to enhance ASR performance on unseen languages. Specifically, we introduce a weighted sum method, which computes a weighted sum of the embeddings of language tags, using Whisperâ€™s predicted language probabilities. In addition, we develop a predictor-based approach that refines the weighted sum embedding to more closely approximate the true embedding for unseen languages. Experimental results demonstrate substantial improvements in ASR performance, both in zero-shot and fine-tuning settings. Our proposed methods outperform baseline approaches, providing an effective solution for addressing unseen languages in multilingual ASR. </p>
<blockquote>
<p>å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ—¨åœ¨åœ¨ä¸€ä¸ªç³»ç»Ÿä¸­è¯†åˆ«å’Œè½¬å½•å¤šç§è¯­è¨€çš„è¯­éŸ³ã€‚whisperæ˜¯æœ€å…ˆè¿›çš„ASRæ¨¡å‹ä¹‹ä¸€ï¼Œé€šè¿‡å¤„ç†99ç§è¯­è¨€ï¼Œåˆ©ç”¨å¤§é‡æ•°æ®å¹¶èå…¥è¯­è¨€æ ‡ç­¾ä½œä¸ºå‰ç¼€æ¥å¼•å¯¼è¯†åˆ«è¿‡ç¨‹ï¼Œåœ¨è¯¥é¢†åŸŸè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå°½ç®¡å–å¾—äº†æˆåŠŸï¼Œwhisperä»é¢ä¸´ç€æœªè§è¿‡çš„è¯­è¨€çš„æŒ‘æˆ˜ï¼Œè¿™äº›è¯­è¨€å¹¶æœªåŒ…å«åœ¨é¢„è®­ç»ƒä¹‹ä¸­ã€‚å—å¤šç§è¯­è¨€å…·æœ‰å…±åŒè¯­è¨€ç‰¹æ€§çš„è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ©ç”¨è¿™äº›å…³ç³»æé«˜æœªè§è¯­è¨€çš„ASRæ€§èƒ½çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ æƒå’Œçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è®¡ç®—è¯­è¨€æ ‡ç­¾åµŒå…¥çš„åŠ æƒå’Œï¼Œä½¿ç”¨whisperé¢„æµ‹çš„è¯­éŸ³æ¦‚ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºé¢„æµ‹çš„æ–¹æ³•ï¼Œå¯¹åŠ æƒå’ŒåµŒå…¥è¿›è¡Œå¾®è°ƒï¼Œä»¥æ›´ç²¾ç¡®åœ°é€¼è¿‘æœªè§è¯­è¨€çš„çœŸå®åµŒå…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é›¶æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸­ï¼ŒASRæ€§èƒ½å‡æœ‰æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œä¸ºè§£å†³å¤šè¯­ç§ASRä¸­çš„æœªè§è¯­è¨€é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16474v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ—¨åœ¨åœ¨ä¸€ä¸ªç³»ç»Ÿä¸­è¯†åˆ«å’Œè½¬å½•å¤šç§è¯­è¨€çš„è¯­éŸ³ã€‚æœ€å…ˆè¿›çš„ASRæ¨¡å‹ä¹‹ä¸€Whisperèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†99ç§è¯­è¨€ï¼Œåˆ©ç”¨å¤§é‡æ•°æ®å’Œå¼•å…¥è¯­è¨€æ ‡ç­¾å‰ç¼€æ¥å¼•å¯¼è¯†åˆ«è¿‡ç¨‹ã€‚ç„¶è€Œï¼ŒWhisperåœ¨æœªè§è¯­è¨€ï¼ˆå³é¢„è®­ç»ƒé˜¶æ®µæœªåŒ…å«çš„è¯­è¨€ï¼‰æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚å—å¤šç§è¯­è¨€å…·æœ‰å…±äº«è¯­è¨€ç‰¹æ€§çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºåˆ©ç”¨è¿™äº›å…³ç³»æé«˜æœªè§è¯­è¨€çš„ASRæ€§èƒ½çš„æ–¹æ³•ã€‚å…·ä½“æå‡ºäº†åŠ æƒå’Œæ–¹æ³•å’ŒåŸºäºé¢„æµ‹å™¨çš„æ–¹æ³•ï¼Œå‰è€…è®¡ç®—è¯­è¨€æ ‡ç­¾åµŒå…¥çš„åŠ æƒå’Œï¼Œä½¿ç”¨Whisperé¢„æµ‹çš„è¯­è¨€æ¦‚ç‡ï¼›åè€…å¯¹åŠ æƒå’ŒåµŒå…¥è¿›è¡Œç²¾ç»†åŒ–ï¼Œä»¥æ›´æ¥è¿‘äºæœªè§è¯­è¨€çš„çœŸå®åµŒå…¥ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨é›¶æ ·æœ¬å’Œå¾®è°ƒè®¾ç½®ä¸­ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•åœ¨ASRæ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æé«˜ï¼Œå¹¶ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œä¸ºå¤„ç†å¤šè¯­ç§ASRä¸­çš„æœªè§è¯­è¨€æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰èƒ½åœ¨ä¸€ä¸ªç³»ç»Ÿä¸­è¯†åˆ«å’Œè½¬å½•å¤šç§è¯­è¨€çš„è¯­éŸ³ã€‚</li>
<li>Whisperæ˜¯å¤„ç†å¤šç§è¯­è¨€çš„æœ‰æ•ˆASRæ¨¡å‹ï¼Œä½†é¢ä¸´æœªè§è¯­è¨€çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤šç§è¯­è¨€å…·æœ‰å…±äº«è¯­è¨€ç‰¹æ€§ï¼Œå¯ä»¥åˆ©ç”¨è¿™äº›å…³ç³»æé«˜ASRæ€§èƒ½ã€‚</li>
<li>æå‡ºäº†åŠ æƒå’Œæ–¹æ³•å’ŒåŸºäºé¢„æµ‹å™¨çš„æ–¹æ³•æ¥æé«˜ASRå¯¹æœªè§è¯­è¨€çš„æ€§èƒ½ã€‚</li>
<li>åŠ æƒå’Œæ–¹æ³•é€šè¿‡è®¡ç®—è¯­è¨€æ ‡ç­¾åµŒå…¥çš„åŠ æƒå’Œæ¥å·¥ä½œï¼Œåˆ©ç”¨é¢„æµ‹çš„è¯­è¨€æ¦‚ç‡ã€‚</li>
<li>åŸºäºé¢„æµ‹å™¨çš„æ–¹æ³•å¯¹åŠ æƒå’ŒåµŒå…¥è¿›è¡Œç²¾ç»†åŒ–ï¼Œä»¥æ›´æ¥è¿‘æœªè§è¯­è¨€çš„çœŸå®åµŒå…¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16474v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16474v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16474v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Transducer-Llama-Integrating-LLMs-into-Streamable-Transducer-based-Speech-Recognition"><a href="#Transducer-Llama-Integrating-LLMs-into-Streamable-Transducer-based-Speech-Recognition" class="headerlink" title="Transducer-Llama: Integrating LLMs into Streamable Transducer-based   Speech Recognition"></a>Transducer-Llama: Integrating LLMs into Streamable Transducer-based   Speech Recognition</h2><p><strong>Authors:Keqi Deng, Jinxi Guo, Yingyi Ma, Niko Moritz, Philip C. Woodland, Ozlem Kalinli, Mike Seltzer</strong></p>
<p>While large language models (LLMs) have been applied to automatic speech recognition (ASR), the task of making the model streamable remains a challenge. This paper proposes a novel model architecture, Transducer-Llama, that integrates LLMs into a Factorized Transducer (FT) model, naturally enabling streaming capabilities. Furthermore, given that the large vocabulary of LLMs can cause data sparsity issue and increased training costs for spoken language systems, this paper introduces an efficient vocabulary adaptation technique to align LLMs with speech system vocabularies. The results show that directly optimizing the FT model with a strong pre-trained LLM-based predictor using the RNN-T loss yields some but limited improvements over a smaller pre-trained LM predictor. Therefore, this paper proposes a weak-to-strong LM swap strategy, using a weak LM predictor during RNN-T loss training and then replacing it with a strong LLM. After LM replacement, the minimum word error rate (MWER) loss is employed to finetune the integration of the LLM predictor with the Transducer-Llama model. Experiments on the LibriSpeech and large-scale multi-lingual LibriSpeech corpora show that the proposed streaming Transducer-Llama approach gave a 17% relative WER reduction (WERR) over a strong FT baseline and a 32% WERR over an RNN-T baseline. </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åº”ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æƒ…å†µä¸‹ï¼Œå®ç°æ¨¡å‹çš„æµå¼ä¼ è¾“ä»»åŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¨¡å‹æ¶æ„Transducer-Llamaï¼Œè¯¥æ¶æ„å°†LLMé›†æˆåˆ°åˆ†è§£å¼è½¬æ¢å™¨ï¼ˆFTï¼‰æ¨¡å‹ä¸­ï¼Œè‡ªç„¶åœ°å®ç°äº†æµå¼ä¼ è¾“åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œé‰´äºLLMçš„å¤§é‡è¯æ±‡å¯èƒ½å¯¼è‡´æ•°æ®ç¨€ç–é—®é¢˜ä»¥åŠå¢åŠ å£è¯­ç³»ç»Ÿçš„è®­ç»ƒæˆæœ¬ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§æœ‰æ•ˆçš„è¯æ±‡é€‚åº”æŠ€æœ¯ï¼Œä»¥ä½¿LLMä¸è¯­éŸ³ç³»ç»Ÿè¯æ±‡å¯¹é½ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒLLMé¢„æµ‹å™¨ç›´æ¥ä¼˜åŒ–FTæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨RNN-TæŸå¤±è¿›è¡Œè®­ç»ƒï¼Œç›¸è¾ƒäºè¾ƒå°çš„é¢„è®­ç»ƒLMé¢„æµ‹å™¨è™½ç„¶æœ‰ä¸€å®šæ”¹è¿›ï¼Œä½†æ”¹è¿›æœ‰é™ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä»å¼±åˆ°å¼ºçš„LMæ›¿æ¢ç­–ç•¥ï¼Œå³åœ¨RNN-TæŸå¤±è®­ç»ƒæœŸé—´ä½¿ç”¨å¼±LMé¢„æµ‹å™¨ï¼Œç„¶åå°†å…¶æ›¿æ¢ä¸ºå¼ºå¤§çš„LLMã€‚åœ¨LMæ›¿æ¢ä¹‹åï¼Œé‡‡ç”¨æœ€å°å•è¯é”™è¯¯ç‡ï¼ˆMWERï¼‰æŸå¤±å¯¹LLMé¢„æµ‹å™¨ä¸Transducer-Llamaæ¨¡å‹çš„é›†æˆè¿›è¡Œå¾®è°ƒã€‚åœ¨LibriSpeechå’Œå¤§è§„æ¨¡å¤šè¯­ç§LibriSpeechè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æµå¼ä¼ è¾“Transducer-Llamaæ–¹æ³•ç›¸è¾ƒäºå¼ºå¤§çš„FTåŸºå‡†çº¿å’ŒRNN-TåŸºå‡†çº¿åˆ†åˆ«å®ç°äº†17%å’Œ32%çš„ç›¸å¯¹å­—è¯é”™è¯¯ç‡å‡å°‘ï¼ˆWERRï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16464v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æµå¼è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹Transducer-Llamaã€‚é’ˆå¯¹LLMsè¯æ±‡é‡å¤§å¯¼è‡´çš„è®­ç»ƒå’Œè¯†åˆ«é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„è¯æ±‡è¡¨é€‚åº”æŠ€æœ¯ã€‚é€šè¿‡å¯¹åŸºäºå¼ºé¢„è®­ç»ƒLLMé¢„æµ‹å™¨çš„æµå¼è½¬å¯¼å™¨æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œé‡‡ç”¨æ›¿æ¢ç­–ç•¥ä»¥æé«˜è¯†åˆ«æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºå‡†æ¨¡å‹æ˜¾è‘—æé«˜äº†è¯†åˆ«æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transducer-Llamaæ¨¡å‹ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå®ç°æµå¼è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚</li>
<li>é’ˆå¯¹LLMsè¯æ±‡é‡å¤§å¸¦æ¥çš„æ•°æ®ç¨€ç–å’Œè®­ç»ƒæˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†æœ‰æ•ˆçš„è¯æ±‡è¡¨é€‚åº”æŠ€æœ¯ã€‚</li>
<li>ç›´æ¥ä¼˜åŒ–åŸºäºå¼ºé¢„è®­ç»ƒLLMé¢„æµ‹å™¨çš„æµå¼è½¬å¯¼å™¨æ¨¡å‹æœ‰æ‰€æå‡ï¼Œä½†æ•ˆæœæœ‰é™ã€‚</li>
<li>é‡‡ç”¨å¼±åˆ°å¼ºçš„LMæ›¿æ¢ç­–ç•¥ï¼Œå…ˆç”¨å¼±LMé¢„æµ‹å™¨è¿›è¡Œè®­ç»ƒï¼Œå†æ›¿æ¢ä¸ºå¼ºLLMé¢„æµ‹å™¨ã€‚</li>
<li>å¼•å…¥æœ€å°è¯é”™è¯¯ç‡ï¼ˆMWERï¼‰æŸå¤±æ¥å¾®è°ƒLLMé¢„æµ‹å™¨ä¸Transducer-Llamaæ¨¡å‹çš„é›†æˆã€‚</li>
<li>åœ¨LibriSpeechæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTransducer-Llamaæ–¹æ³•ç›¸è¾ƒäºå¼ºåŸºçº¿æ¨¡å‹å’ŒRNN-TåŸºçº¿æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16464v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16464v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16464v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16464v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16464v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.16464v1/page_3_1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LAMA-UT-Language-Agnostic-Multilingual-ASR-through-Orthography-Unification-and-Language-Specific-Transliteration"><a href="#LAMA-UT-Language-Agnostic-Multilingual-ASR-through-Orthography-Unification-and-Language-Specific-Transliteration" class="headerlink" title="LAMA-UT: Language Agnostic Multilingual ASR through Orthography   Unification and Language-Specific Transliteration"></a>LAMA-UT: Language Agnostic Multilingual ASR through Orthography   Unification and Language-Specific Transliteration</h2><p><strong>Authors:Sangmin Lee, Woo-Jin Chung, Hong-Goo Kang</strong></p>
<p>Building a universal multilingual automatic speech recognition (ASR) model that performs equitably across languages has long been a challenge due to its inherent difficulties. To address this task we introduce a Language-Agnostic Multilingual ASR pipeline through orthography Unification and language-specific Transliteration (LAMA-UT). LAMA-UT operates without any language-specific modules while matching the performance of state-of-the-art models trained on a minimal amount of data. Our pipeline consists of two key steps. First, we utilize a universal transcription generator to unify orthographic features into Romanized form and capture common phonetic characteristics across diverse languages. Second, we utilize a universal converter to transform these universal transcriptions into language-specific ones. In experiments, we demonstrate the effectiveness of our proposed method leveraging universal transcriptions for massively multilingual ASR. Our pipeline achieves a relative error reduction rate of 45% when compared to Whisper and performs comparably to MMS, despite being trained on only 0.1% of Whisperâ€™s training data. Furthermore, our pipeline does not rely on any language-specific modules. However, it performs on par with zero-shot ASR approaches which utilize additional language-specific lexicons and language models. We expect this framework to serve as a cornerstone for flexible multilingual ASR systems that are generalizable even to unseen languages. </p>
<blockquote>
<p>æ„å»ºä¸€ä¸ªé€‚ç”¨äºå¤šç§è¯­è¨€çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å„ç§è¯­è¨€ä¹‹é—´å®ç°å…¬å¹³è¡¨ç°ï¼Œé•¿æœŸä»¥æ¥ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå…¶å›ºæœ‰çš„å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ä»»åŠ¡ï¼Œæˆ‘ä»¬é€šè¿‡æ­£å­—æ³•ç»Ÿä¸€å’Œé’ˆå¯¹ç‰¹å®šè¯­è¨€çš„è½¬å†™ï¼ˆLAMA-UTï¼‰ï¼Œå¼•å…¥äº†è¯­è¨€æ— å…³çš„å¤šè¯­è¨€ASRç®¡é“ã€‚LAMA-UTåœ¨æ²¡æœ‰ä»»ä½•é’ˆå¯¹ç‰¹å®šè¯­è¨€çš„æ¨¡å—çš„æƒ…å†µä¸‹è¿è¡Œï¼ŒåŒæ—¶åŒ¹é…åœ¨å°‘é‡æ•°æ®ä¸Šè®­ç»ƒçš„æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç®¡é“åŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ­¥éª¤ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨é€šç”¨è½¬å½•ç”Ÿæˆå™¨å°†æ­£å­—ç‰¹å¾ç»Ÿä¸€ä¸ºç½—é©¬åŒ–å½¢å¼ï¼Œå¹¶æ•æ‰ä¸åŒè¯­è¨€çš„å…±åŒè¯­éŸ³ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨é€šç”¨è½¬æ¢å™¨å°†è¿™äº›é€šç”¨è½¬å½•è½¬æ¢ä¸ºç‰¹å®šè¯­è¨€çš„è½¬å½•ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åˆ©ç”¨é€šç”¨è½¬å½•è¿›è¡Œå¤§è§„æ¨¡å¤šè¯­è¨€ASRçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸whisperç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç®¡é“åœ¨ç›¸å¯¹è¯¯å·®å‡å°‘ç‡æ–¹é¢å®ç°äº†45%çš„é™ä½ï¼Œå°½ç®¡å®ƒåªæ¥å—äº†whisper 0.1%çš„è®­ç»ƒæ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç®¡é“ä¸ä¾èµ–äºä»»ä½•é’ˆå¯¹ç‰¹å®šè¯­è¨€çš„æ¨¡å—ï¼Œä½†å…¶æ€§èƒ½ä¸é›¶é•œå¤´ASRæ–¹æ³•ç›¸å½“ï¼Œåè€…åˆ©ç”¨é¢å¤–çš„é’ˆå¯¹ç‰¹å®šè¯­è¨€çš„è¯æ±‡å’Œè¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªæ¡†æ¶èƒ½æˆä¸ºçµæ´»çš„å¤šè¯­è¨€ASRç³»ç»Ÿçš„åŸºçŸ³ï¼Œå³ä½¿å¯¹äºæœªè§è¿‡çš„è¯­è¨€ï¼Œå®ƒä¹Ÿå…·æœ‰é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15299v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨è¯­è¨€çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œå³è¯­è¨€æ— å…³çš„å¤šè¯­è¨€ASRç®¡é“ï¼Œé€šè¿‡æ­£å­—æ³•ç»Ÿä¸€å’Œè¯­è¨€ç‰¹å®šè½¬å†™ï¼ˆLAMA-UTï¼‰æ¥è§£å†³åœ¨ä¸åŒè¯­è¨€ä¸­è¡¨ç°å‡è¡¡çš„æŒ‘æˆ˜ã€‚è¯¥ç®¡é“æ— éœ€ä»»ä½•ç‰¹å®šè¯­è¨€çš„æ¨¡å—ï¼Œå³å¯åŒ¹é…åœ¨å°‘é‡æ•°æ®ä¸Šè®­ç»ƒçš„å…ˆè¿›æ¨¡å‹æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é€šç”¨è½¬å½•æ¥å®ç°å¤§è§„æ¨¡å¤šè¯­è¨€ASRçš„æœ‰æ•ˆæ€§ï¼Œç›¸è¾ƒäºWhisperï¼Œç›¸å¯¹è¯¯å·®å‡å°‘ç‡è¾¾åˆ°45%ï¼Œå¹¶åœ¨è®­ç»ƒæ•°æ®é‡ä»…ä¸ºå…¶ååˆ†ä¹‹ä¸€çš„æƒ…å†µä¸‹è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚è™½ç„¶è¯¥ç®¡é“ä¸ä¾èµ–ä»»ä½•ç‰¹å®šè¯­è¨€çš„æ¨¡å—ï¼Œä½†å…¶æ€§èƒ½ä¸é›¶æ ·æœ¬ASRæ–¹æ³•ç›¸å½“ï¼Œåè€…ä½¿ç”¨é¢å¤–çš„ç‰¹å®šè¯­è¨€è¯å…¸å’Œè¯­è¨€æ¨¡å‹ã€‚è¯¥æ¡†æ¶æœ‰æœ›æˆä¸ºçµæ´»çš„å¤šè¯­è¨€ASRç³»ç»Ÿçš„åŸºçŸ³ï¼Œèƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„è¯­è¨€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è·¨è¯­è¨€ASRæ¨¡å‹â€”â€”è¯­è¨€æ— å…³çš„å¤šè¯­è¨€ASRç®¡é“ï¼ˆLAMA-UTï¼‰ã€‚</li>
<li>é€šè¿‡æ­£å­—æ³•ç»Ÿä¸€å’Œè¯­è¨€ç‰¹å®šè½¬å†™ä¸¤ä¸ªå…³é”®æ­¥éª¤å®ç°è¯¥æ¨¡å‹ã€‚</li>
<li>æ— éœ€ç‰¹å®šè¯­è¨€çš„æ¨¡å—ï¼Œå³èƒ½åŒ¹é…åœ¨å°‘é‡æ•°æ®ä¸Šè®­ç»ƒçš„å…ˆè¿›æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç›¸å¯¹è¯¯å·®å‡å°‘ç‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºWhisperè¾¾åˆ°45%ã€‚</li>
<li>åœ¨è®­ç»ƒæ•°æ®é‡ä»…ä¸ºå…¶ååˆ†ä¹‹ä¸€çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥ç®¡é“æ€§èƒ½ä¸é›¶æ ·æœ¬ASRæ–¹æ³•ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.15299v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.15299v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.15299v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.15299v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.15299v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.15299v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2412.15299v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="BEST-STD-Bidirectional-Mamba-Enhanced-Speech-Tokenization-for-Spoken-Term-Detection"><a href="#BEST-STD-Bidirectional-Mamba-Enhanced-Speech-Tokenization-for-Spoken-Term-Detection" class="headerlink" title="BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken   Term Detection"></a>BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken   Term Detection</h2><p><strong>Authors:Anup Singh, Kris Demuynck, Vipul Arora</strong></p>
<p>Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient. </p>
<blockquote>
<p>è¯­éŸ³æœ¯è¯­æ£€æµ‹ï¼ˆSTDï¼‰é€šå¸¸å—åˆ°ä¾èµ–äºå¸§çº§ç‰¹å¾å’Œè®¡ç®—å¯†é›†å‹çš„åŸºäºDTWçš„æ¨¡æ¿åŒ¹é…çš„é˜»ç¢ï¼Œè¿™é™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†è¯­éŸ³ç¼–ç ä¸ºç¦»æ•£ã€ç‹¬ç«‹äºè¯´è¯äººçš„è¯­ä¹‰ä»¤ç‰Œçš„æ–°æ–¹æ³•ã€‚è¿™æœ‰åŠ©äºä½¿ç”¨åŸºäºæ–‡æœ¬çš„æœç´¢ç®—æ³•è¿›è¡Œå¿«é€Ÿæ£€ç´¢ï¼Œå¹¶æœ‰æ•ˆåœ°å¤„ç†è¯æ±‡è¡¨ä¹‹å¤–çš„æœ¯è¯­ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¾§é‡äºç”ŸæˆåŒä¸€æœ¯è¯­ä¸åŒè¡¨è¾¾ä¹‹é—´ä¸€è‡´çš„ä»¤ç‰Œåºåˆ—ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åœ¨Mambaç¼–ç å™¨å†…éƒ¨çš„åŒå‘çŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼Œåœ¨è‡ªæˆ‘ç›‘ç£çš„å­¦ä¹ æ¡†æ¶ä¸­è¿›è¡Œè®­ç»ƒï¼Œä»¥å­¦ä¹ ä¸Šä¸‹æ–‡å¸§çº§ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾è¿›ä¸€æ­¥è¢«ç¼–ç æˆç¦»æ•£ä»¤ç‰Œã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è¯­éŸ³ä»¤ç‰Œè¡¨ç°å‡ºæ¯”ç°æœ‰åˆ†è¯å™¨æ›´é«˜çš„è¯´è¯äººä¸å˜æ€§ï¼Œä½¿å…¶æ›´é€‚åˆäºSTDä»»åŠ¡ã€‚åœ¨LibriSpeechå’ŒTIMITæ•°æ®åº“ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„STDåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶æ•ˆç‡æ›´é«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14100v2">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>é’ˆå¯¹å½“å‰è¯­éŸ³æœ¯è¯­æ£€æµ‹ï¼ˆSTDï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ä¾èµ–å¸§çº§ç‰¹å¾å’ŒåŸºäºDTWçš„æ¨¡æ¿åŒ¹é…å¯¼è‡´çš„è®¡ç®—é‡å¤§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å°†è¯­éŸ³ç¼–ç ä¸ºç¦»æ•£ã€ç‹¬ç«‹äºè¯´è¯è€…çš„è¯­ä¹‰ä»¤ç‰Œï¼Œä¿ƒè¿›ä½¿ç”¨æ–‡æœ¬æœç´¢ç®—æ³•è¿›è¡Œå¿«é€Ÿæ£€ç´¢ï¼Œå¹¶æœ‰æ•ˆå¤„ç†è¯æ±‡è¡¨å¤–çš„æœ¯è¯­ã€‚è¯¥æ–¹æ³•ä¸“æ³¨äºç”ŸæˆåŒä¸€æœ¯è¯­ä¸åŒè¡¨è¾¾å½¢å¼çš„ä¸€è‡´ä»¤ç‰Œåºåˆ—ï¼Œå¹¶æå‡ºåœ¨Mambaç¼–ç å™¨å†…ä½¿ç”¨åŒå‘çŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼Œä»¥åœ¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ¡†æ¶ä¸­å­¦ä¹ ç¯å¢ƒçº§åˆ«çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç”Ÿæˆçš„è¯­éŸ³ä»¤ç‰Œè¡¨ç°å‡ºæ›´é«˜çš„è¯´è¯äººæ— å…³æ€§ï¼Œä½¿å¾—å®ƒä»¬æ›´é€‚åˆç”¨äºSTDä»»åŠ¡ã€‚åœ¨LibriSpeechå’ŒTIMITæ•°æ®åº“ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¼˜äºç°æœ‰çš„STDåŸºçº¿ï¼Œè€Œä¸”æ›´é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­éŸ³æœ¯è¯­æ£€æµ‹ï¼ˆSTDï¼‰é¢ä¸´ä¾èµ–å¸§çº§ç‰¹å¾å’Œè®¡ç®—å¯†é›†å‹DTWæ¨¡æ¿åŒ¹é…çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡å°†è¯­éŸ³ç¼–ç ä¸ºç‹¬ç«‹äºè¯´è¯è€…çš„è¯­ä¹‰ä»¤ç‰Œæ¥ä¿ƒè¿›å¿«é€Ÿæ£€ç´¢å’Œå¤„ç†è¯æ±‡è¡¨å¤–çš„æœ¯è¯­ã€‚</li>
<li>æ–¹æ³•æ³¨é‡ç”ŸæˆåŒä¸€æœ¯è¯­ä¸åŒè¡¨è¾¾å½¢å¼çš„ä¸€è‡´ä»¤ç‰Œåºåˆ—ã€‚</li>
<li>å¼•å…¥äº†åŒå‘çŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼Œåœ¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ¡†æ¶ä¸­å­¦ä¹ ç¯å¢ƒçº§åˆ«çš„ç‰¹å¾ã€‚</li>
<li>ç”Ÿæˆçš„è¯­éŸ³ä»¤ç‰Œå±•ç°å‡ºè¾ƒé«˜çš„è¯´è¯äººæ— å…³æ€§ï¼Œæé«˜äº†å…¶åœ¨STDä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚</li>
<li>åœ¨LibriSpeechå’ŒTIMITæ•°æ®åº“ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„STDåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.14100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2411.14100v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2411.14100v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2411.14100v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2411.14100v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2411.14100v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Multi-Source-Spatial-Knowledge-Understanding-for-Immersive-Visual-Text-to-Speech"><a href="#Multi-Source-Spatial-Knowledge-Understanding-for-Immersive-Visual-Text-to-Speech" class="headerlink" title="Multi-Source Spatial Knowledge Understanding for Immersive Visual   Text-to-Speech"></a>Multi-Source Spatial Knowledge Understanding for Immersive Visual   Text-to-Speech</h2><p><strong>Authors:Shuwei He, Rui Liu</strong></p>
<p>Visual Text-to-Speech (VTTS) aims to take the environmental image as the prompt to synthesize reverberant speech for the spoken content. Previous works focus on the RGB modality for global environmental modeling, overlooking the potential of multi-source spatial knowledge like depth, speaker position, and environmental semantics. To address these issues, we propose a novel multi-source spatial knowledge understanding scheme for immersive VTTS, termed MS2KU-VTTS. Specifically, we first prioritize RGB image as the dominant source and consider depth image, speaker position knowledge from object detection, and Gemini-generated semantic captions as supplementary sources. Afterwards, we propose a serial interaction mechanism to effectively integrate both dominant and supplementary sources. The resulting multi-source knowledge is dynamically integrated based on the respective contributions of each source.This enriched interaction and integration of multi-source spatial knowledge guides the speech generation model, enhancing the immersive speech experience. Experimental results demonstrate that the MS$^2$KU-VTTS surpasses existing baselines in generating immersive speech. Demos and code are available at: <a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/MS2KU-VTTS">https://github.com/AI-S2-Lab/MS2KU-VTTS</a>. </p>
<blockquote>
<p>è§†è§‰æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆVTTSï¼‰æ—¨åœ¨ä»¥ç¯å¢ƒå›¾åƒä¸ºæç¤ºï¼Œåˆæˆå“äº®çš„å†…å®¹è¯­éŸ³ã€‚ä»¥å‰çš„ç ”ç©¶å·¥ä½œä¸»è¦é›†ä¸­åœ¨RGBæ¨¡æ€è¿›è¡Œå…¨å±€ç¯å¢ƒå»ºæ¨¡ï¼Œå¿½ç•¥äº†æ·±åº¦ã€è¯´è¯è€…ä½ç½®å’Œç¯å¢ƒè¯­ä¹‰ç­‰å¤šæºç©ºé—´çŸ¥è¯†çš„æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ²‰æµ¸å¼VTTSçš„å¤šæºç©ºé—´çŸ¥è¯†ç†è§£æ–¹æ¡ˆï¼Œç§°ä¸ºMS2KU-VTTSã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä»¥RGBå›¾åƒä½œä¸ºä¸»è¦æ¥æºï¼Œå¹¶å°†æ·±åº¦å›¾åƒã€æ¥è‡ªå¯¹è±¡æ£€æµ‹çš„è¯´è¯è€…ä½ç½®çŸ¥è¯†ä»¥åŠGeminiç”Ÿæˆçš„è¯­ä¹‰å­—å¹•ä½œä¸ºè¾…åŠ©æ¥æºã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸²è¡Œäº¤äº’æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆåœ°æ•´åˆä¸»è¦æ¥æºå’Œè¾…åŠ©æ¥æºã€‚æ‰€å¾—çš„å¤šæºçŸ¥è¯†æ˜¯åŠ¨æ€æ•´åˆçš„ï¼ŒåŸºäºæ¯ä¸ªæ¥æºçš„ç›¸å¯¹è´¡çŒ®ã€‚è¿™ç§ä¸°å¯Œçš„å¤šæºç©ºé—´çŸ¥è¯†çš„äº¤äº’å’Œæ•´åˆå¼•å¯¼è¯­éŸ³ç”Ÿæˆæ¨¡å‹ï¼Œå¢å¼ºäº†æ²‰æµ¸å¼è¯­éŸ³ä½“éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMS$^2$KU-VTTSåœ¨ç”Ÿæˆæ²‰æµ¸å¼è¯­éŸ³æ–¹é¢è¶…è¿‡äº†ç°æœ‰åŸºçº¿ã€‚ç›¸å…³æ¼”ç¤ºå’Œä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/AI-S2-Lab/MS2KU-VTTS">https://github.com/AI-S2-Lab/MS2KU-VTTS</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14101v2">PDF</a> 5 pages, 1 figure, Accepted by ICASSPâ€™2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰æ–‡æœ¬è¯­éŸ³è½¬æ¢ï¼ˆVTTSï¼‰çš„ç›®æ ‡æ˜¯åˆ©ç”¨ç¯å¢ƒå›¾åƒåˆæˆè¯­éŸ³å†…å®¹ã€‚é’ˆå¯¹ä»¥å¾€ç ”ç©¶ä»…å…³æ³¨RGBæ¨¡æ€è¿›è¡Œå…¨å±€ç¯å¢ƒå»ºæ¨¡ï¼Œå¿½è§†äº†æ·±åº¦ã€è¯´è¯è€…ä½ç½®å’Œè¯­ä¹‰ç­‰å¤šæºç©ºé—´çŸ¥è¯†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMS^2KU-VTTSçš„å¤šæºç©ºé—´çŸ¥è¯†ç†è§£æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆä»¥RGBå›¾åƒä¸ºä¸»è¦æ¥æºï¼Œè€ƒè™‘æ·±åº¦å›¾åƒã€ä»å¯¹è±¡æ£€æµ‹ä¸­è·å¾—çš„è¯´è¯è€…ä½ç½®çŸ¥è¯†ä»¥åŠGeminiç”Ÿæˆçš„è¯­ä¹‰å­—å¹•ä½œä¸ºè¾…åŠ©æ¥æºã€‚é€šè¿‡ä¸²è¡Œäº¤äº’æœºåˆ¶æœ‰æ•ˆåœ°æ•´åˆäº†ä¸»è¦å’Œè¾…åŠ©æ¥æºçš„çŸ¥è¯†ï¼Œå¹¶æ ¹æ®å„æ¥æºçš„è´¡çŒ®åŠ¨æ€é›†æˆå¤šæºçŸ¥è¯†ã€‚è¿™ä¸°å¯Œäº†å¤šæºç©ºé—´çŸ¥è¯†çš„äº¤äº’å’Œæ•´åˆï¼ŒæŒ‡å¯¼è¯­éŸ³ç”Ÿæˆæ¨¡å‹ï¼Œæé«˜äº†æ²‰æµ¸å¼è¯­éŸ³ä½“éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMS^2KU-VTTSåœ¨ç”Ÿæˆæ²‰æµ¸å¼è¯­éŸ³æ–¹é¢è¶…è¿‡äº†ç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VTTSæ—¨åœ¨åˆ©ç”¨ç¯å¢ƒå›¾åƒåˆæˆè¯­éŸ³å†…å®¹ã€‚</li>
<li>ä»¥å¾€ç ”ç©¶ä¸»è¦å…³æ³¨RGBæ¨¡æ€è¿›è¡Œå…¨å±€ç¯å¢ƒå»ºæ¨¡ï¼Œå¿½è§†äº†å¤šæºç©ºé—´çŸ¥è¯†ã€‚</li>
<li>MS^2KU-VTTSæ–¹æ¡ˆæå‡ºä»¥RGBå›¾åƒä¸ºä¸»è¦æ¥æºï¼ŒåŒæ—¶è€ƒè™‘æ·±åº¦å›¾åƒã€è¯´è¯è€…ä½ç½®çŸ¥è¯†å’Œè¯­ä¹‰å­—å¹•ä½œä¸ºè¾…åŠ©æ¥æºã€‚</li>
<li>é€šè¿‡ä¸²è¡Œäº¤äº’æœºåˆ¶æ•´åˆäº†ä¸»è¦å’Œè¾…åŠ©æ¥æºçš„çŸ¥è¯†ã€‚</li>
<li>å¤šæºçŸ¥è¯†æ ¹æ®å„æ¥æºçš„è´¡çŒ®åŠ¨æ€é›†æˆã€‚</li>
<li>ä¸°å¯Œçš„å¤šæºç©ºé—´çŸ¥è¯†äº¤äº’å’Œæ•´åˆæé«˜äº†æ²‰æµ¸å¼è¯­éŸ³ä½“éªŒã€‚</li>
<li>MS^2KU-VTTSåœ¨ç”Ÿæˆæ²‰æµ¸å¼è¯­éŸ³æ–¹é¢è¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2410.14101v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2410.14101v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2410.14101v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2024-12-25\./crop_Speech/2410.14101v2/page_3_1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-79cb91f79ce8ba808f6f8ec8ee38ef59.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  Singular Value Scaling Efficient Generative Model Compression via   Pruned Weights Refinement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b2763a185660a9986e98a89699314c52.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  PINN-EMFNet PINN-based and Enhanced Multi-Scale Feature Fusion Network   for Breast Ultrasound Images Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27197.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
