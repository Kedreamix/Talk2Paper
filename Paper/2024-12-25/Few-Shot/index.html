<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  Knowledge Editing through Chain-of-Thought">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-ef3c89546934a25924e4a0910914e013.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    56 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-25-æ›´æ–°"><a href="#2024-12-25-æ›´æ–°" class="headerlink" title="2024-12-25 æ›´æ–°"></a>2024-12-25 æ›´æ–°</h1><h2 id="Knowledge-Editing-through-Chain-of-Thought"><a href="#Knowledge-Editing-through-Chain-of-Thought" class="headerlink" title="Knowledge Editing through Chain-of-Thought"></a>Knowledge Editing through Chain-of-Thought</h2><p><strong>Authors:Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated exceptional capabilities across a wide range of natural language processing (NLP) tasks. However, keeping these models up-to-date with evolving world knowledge remains a significant challenge due to the high costs of frequent retraining. To address this challenge, knowledge editing techniques have emerged to update LLMs with new information without rebuilding the model from scratch. Among these, the in-context editing paradigm stands out for its effectiveness in integrating new knowledge while preserving the modelâ€™s original capabilities. Despite its potential, existing in-context knowledge editing methods are often task-specific, focusing primarily on multi-hop QA tasks using structured knowledge triples. Moreover, their reliance on few-shot prompting for task decomposition makes them unstable and less effective in generalizing across diverse tasks.   In response to these limitations, we propose EditCoT, a novel knowledge editing framework that flexibly and efficiently updates LLMs across various tasks without retraining. EditCoT works by generating a chain-of-thought (CoT) for a given input and then iteratively refining this CoT process using a CoT editor based on updated knowledge. We evaluate EditCoT across a diverse range of benchmarks, covering multiple languages and tasks. The results demonstrate that our approach achieves state-of-the-art performance while offering superior generalization, effectiveness, and stability compared to existing methods, marking a significant advancement in the field of knowledge updating. Code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/bebr2/EditCoT">https://github.com/bebr2/EditCoT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºé‡æ–°è®­ç»ƒçš„é«˜æˆæœ¬ï¼Œéšç€ä¸–ç•ŒçŸ¥è¯†çš„ä¸æ–­å‘å±•ï¼Œå¦‚ä½•ä¿æŒè¿™äº›æ¨¡å‹çš„æœ€æ–°çŠ¶æ€ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼ŒçŸ¥è¯†ç¼–è¾‘æŠ€æœ¯åº”è¿è€Œç”Ÿï¼Œå¯ä»¥åœ¨ä¸é‡å»ºæ¨¡å‹çš„æƒ…å†µä¸‹æ›´æ–°LLMçš„æ–°ä¿¡æ¯ã€‚å…¶ä¸­ï¼Œä¸Šä¸‹æ–‡ç¼–è¾‘èŒƒå¼å› å…¶åœ¨æ–°çŸ¥è¯†é›†æˆçš„åŒæ—¶ä¿ç•™æ¨¡å‹åŸå§‹èƒ½åŠ›è€Œè„±é¢–è€Œå‡ºã€‚å°½ç®¡å…·æœ‰æ½œåŠ›ï¼Œä½†ç°æœ‰çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ç¼–è¾‘æ–¹æ³•å¾€å¾€æ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ï¼Œä¸»è¦ä¾§é‡äºä½¿ç”¨ç»“æ„åŒ–çŸ¥è¯†ä¸‰å…ƒç»„çš„å¤šè·³é—®ç­”ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¯¹å°‘é‡æç¤ºçš„ä»»åŠ¡åˆ†è§£çš„ä¾èµ–ï¼Œä½¿å¾—å®ƒä»¬åœ¨è·¨ä¸åŒä»»åŠ¡æ³›åŒ–æ—¶å˜å¾—ä¸ç¨³å®šä¸”æ•ˆæœè¾ƒå·®ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†EditCoTï¼Œè¿™æ˜¯ä¸€ä¸ªçµæ´»é«˜æ•ˆçš„çŸ¥è¯†ç¼–è¾‘æ¡†æ¶ï¼Œå¯åœ¨å„ç§ä»»åŠ¡ä¸­æ›´æ–°LLMè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚EditCoTé€šè¿‡ä¸ºç»™å®šè¾“å…¥ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œç„¶åä½¿ç”¨åŸºäºæœ€æ–°çŸ¥è¯†çš„CoTç¼–è¾‘å™¨è¿­ä»£ä¼˜åŒ–æ­¤æ€ç»´é“¾è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨æ¶µç›–å¤šç§è¯­è¨€å’Œä»»åŠ¡çš„å¤šç§åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†EditCoTã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨æ³›åŒ–èƒ½åŠ›ã€æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œæ ‡å¿—ç€çŸ¥è¯†æ›´æ–°é¢†åŸŸçš„ä¸€ä¸ªé‡å¤§è¿›å±•ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/bebr2/EditCoT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/bebr2/EditCoTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17727v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å¦‚ä½•ä½¿è¿™äº›æ¨¡å‹è·Ÿä¸Šä¸æ–­å‘å±•çš„ä¸–ç•ŒçŸ¥è¯†æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºé¢‘ç¹å†è®­ç»ƒçš„æˆæœ¬å¾ˆé«˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå‡ºç°äº†çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯ï¼Œå¯ä»¥åœ¨ä¸é‡å»ºæ¨¡å‹çš„æƒ…å†µä¸‹æ›´æ–°LLMçš„æ–°ä¿¡æ¯ã€‚å…¶ä¸­ï¼Œä¸Šä¸‹æ–‡ç¼–è¾‘èŒƒå¼å› å…¶æ•´åˆæ–°çŸ¥è¯†çš„åŒæ—¶ä¿ç•™æ¨¡å‹çš„åŸå§‹èƒ½åŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ç¼–è¾‘æ–¹æ³•å¾€å¾€æ˜¯é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ï¼Œä¸»è¦é›†ä¸­åœ¨ä½¿ç”¨ç»“æ„åŒ–çŸ¥è¯†ä¸‰å…ƒç»„çš„å¤šè·³é—®ç­”ä»»åŠ¡ä¸Šã€‚è€Œä¸”ï¼Œå®ƒä»¬å¯¹å°‘æ•°æ¡ˆä¾‹æç¤ºçš„ä»»åŠ¡åˆ†è§£çš„ä¾èµ–ï¼Œä½¿å®ƒä»¬åœ¨è·¨ä¸åŒä»»åŠ¡çš„é€šç”¨æ€§æ–¹é¢è¡¨ç°ä¸ç¨³å®šä¸”æ•ˆæœè¾ƒå·®ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†EditCoTï¼Œä¸€ä¸ªçµæ´»é«˜æ•ˆçš„çŸ¥è¯†ç¼–è¾‘æ¡†æ¶ï¼Œå¯åœ¨å„ç§ä»»åŠ¡ä¸­æ›´æ–°LLMè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚EditCoTé€šè¿‡ä¸ºç»™å®šè¾“å…¥ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œç„¶åä½¿ç”¨åŸºäºæ›´æ–°çŸ¥è¯†çš„CoTç¼–è¾‘å™¨è¿­ä»£ä¼˜åŒ–è¿™ä¸ªæ€ç»´è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨æ¶µç›–å¤šç§è¯­è¨€å’Œä»»åŠ¡çš„å¹¿æ³›åŸºå‡†ä¸Šè¯„ä¼°äº†EditCoTã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°æœ€æ–°æ€§èƒ½çš„åŒæ—¶ï¼Œåœ¨é€šç”¨æ€§ã€æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§æ–¹é¢ç›¸æ¯”ç°æœ‰æ–¹æ³•å…·æœ‰ä¼˜åŠ¿ï¼Œæ ‡å¿—ç€çŸ¥è¯†æ›´æ–°é¢†åŸŸçš„ä¸€ä¸ªé‡å¤§è¿›å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„å“è¶Šæ€§èƒ½ä»¥åŠæ›´æ–°æ¨¡å‹ä»¥è·Ÿä¸Šä¸–ç•ŒçŸ¥è¯†å‘å±•çš„æŒ‘æˆ˜ã€‚</li>
<li>çŸ¥è¯†ç¼–è¾‘æŠ€æœ¯çš„å…´èµ·ï¼Œä½¿å¾—èƒ½å¤Ÿåœ¨ä¸é‡å»ºæ¨¡å‹çš„æƒ…å†µä¸‹æ›´æ–°LLMçš„æ–°ä¿¡æ¯ã€‚</li>
<li>ä¸Šä¸‹æ–‡ç¼–è¾‘èŒƒå¼çš„ä¼˜åŠ¿åœ¨äºæ•´åˆæ–°çŸ¥è¯†çš„åŒæ—¶ä¿ç•™æ¨¡å‹çš„åŸå§‹èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰ä¸Šä¸‹æ–‡çŸ¥è¯†ç¼–è¾‘æ–¹æ³•çš„å±€é™æ€§ï¼Œå¦‚ä»»åŠ¡ç‰¹å®šæ€§ä»¥åŠå¯¹å°‘æ•°æ¡ˆä¾‹æç¤ºçš„ä¾èµ–ï¼Œå¯¼è‡´è·¨ä»»åŠ¡é€šç”¨æ€§ä¸è¶³ã€‚</li>
<li>EditCoTçŸ¥è¯†ç¼–è¾‘æ¡†æ¶çš„æå‡ºï¼Œæ—¨åœ¨çµæ´»ã€é«˜æ•ˆåœ°åœ¨å„ç§ä»»åŠ¡ä¸­æ›´æ–°LLMï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>EditCoTé€šè¿‡ç”Ÿæˆå¹¶ä¼˜åŒ–ç»™å®šè¾“å…¥çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¥å·¥ä½œï¼Œä½¿ç”¨åŸºäºæ›´æ–°çŸ¥è¯†çš„CoTç¼–è¾‘å™¨ã€‚</li>
<li>åœ¨å¤šä¸ªè¯­è¨€å’Œä»»åŠ¡çš„å¹¿æ³›åŸºå‡†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒEditCoTå®ç°äº†æœ€æ–°æ€§èƒ½ï¼Œå¹¶åœ¨é€šç”¨æ€§ã€æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f472e31e6ac9de51829b7b63d25d818a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bfb8cd6183b6f8fdb3ba4a079571b5e6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a7f525fb1d898a3b9e60bd906a249150.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-56026f175b786530e630070f421a79a1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="COBRA-COmBinatorial-Retrieval-Augmentation-for-Few-Shot-Learning"><a href="#COBRA-COmBinatorial-Retrieval-Augmentation-for-Few-Shot-Learning" class="headerlink" title="COBRA: COmBinatorial Retrieval Augmentation for Few-Shot Learning"></a>COBRA: COmBinatorial Retrieval Augmentation for Few-Shot Learning</h2><p><strong>Authors:Arnav M. Das, Gantavya Bhatt, Lilly Kumari, Sahil Verma, Jeff Bilmes</strong></p>
<p>Retrieval augmentation, the practice of retrieving additional data from large auxiliary pools, has emerged as an effective technique for enhancing model performance in the low-data regime, e.g. few-shot learning. Prior approaches have employed only nearest-neighbor based strategies for data selection, which retrieve auxiliary samples with high similarity to instances in the target task. However, these approaches are prone to selecting highly redundant samples, since they fail to incorporate any notion of diversity. In our work, we first demonstrate that data selection strategies used in prior retrieval-augmented few-shot learning settings can be generalized using a class of functions known as Combinatorial Mutual Information (CMI) measures. We then propose COBRA (COmBinatorial Retrieval Augmentation), which employs an alternative CMI measure that considers both diversity and similarity to a target dataset. COBRA consistently outperforms previous retrieval approaches across image classification tasks and few-shot learning techniques when used to retrieve samples from LAION-2B. COBRA introduces negligible computational overhead to the cost of retrieval while providing significant gains in downstream model performance. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºï¼ˆretrieval augmentationï¼‰æ˜¯é€šè¿‡ä»å¤§å‹è¾…åŠ©æ± ä¸­æ£€ç´¢é¢å¤–æ•°æ®æ¥æé«˜ä½æ•°æ®ç¯å¢ƒä¸‹æ¨¡å‹æ€§èƒ½çš„ä¸€ç§æœ‰æ•ˆæŠ€æœ¯ï¼Œä¾‹å¦‚åœ¨å°‘æ ·æœ¬å­¦ä¹ ï¼ˆfew-shot learningï¼‰ä¸­ã€‚å…ˆå‰çš„æ–¹æ³•ä»…é‡‡ç”¨åŸºäºæœ€è¿‘é‚»çš„ç­–ç•¥è¿›è¡Œæ•°æ®é€‰æ‹©ï¼Œå³æ£€ç´¢ä¸ç›®æ ‡ä»»åŠ¡å®ä¾‹é«˜åº¦ç›¸ä¼¼çš„è¾…åŠ©æ ·æœ¬ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å®¹æ˜“é€‰æ‹©é«˜åº¦é‡å¤çš„æ ·æœ¬ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰èå…¥å¤šæ ·æ€§çš„æ¦‚å¿µã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¯æ˜åœ¨å…ˆå‰æ£€ç´¢å¢å¼ºå°‘æ ·æœ¬å­¦ä¹ çš„æ•°æ®é€‰æ‹©ç­–ç•¥å¯ä»¥é€šè¿‡ä¸€ç±»ç§°ä¸ºç»„åˆäº’ä¿¡æ¯ï¼ˆCombinatorial Mutual Informationï¼ŒCMIï¼‰åº¦é‡çš„å‡½æ•°è¿›è¡Œæ¦‚æ‹¬ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºCOBRAï¼ˆç»„åˆæ£€ç´¢å¢å¼ºï¼ŒCOmBinatorial Retrieval Augmentationï¼‰ï¼Œå®ƒé‡‡ç”¨ä¸€ç§æ›¿ä»£çš„CMIåº¦é‡æ–¹æ³•ï¼ŒåŒæ—¶è€ƒè™‘å¤šæ ·æ€§å’Œå¯¹ç›®æ ‡æ•°æ®é›†çš„ç›¸ä¼¼æ€§ã€‚åœ¨LAION-2Bçš„æ ·æœ¬æ£€ç´¢ä¸­ï¼ŒCOBRAåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡å’Œå°‘æ ·æœ¬å­¦ä¹ æŠ€æœ¯æ–¹é¢å§‹ç»ˆä¼˜äºå…ˆå‰çš„æ£€ç´¢æ–¹æ³•ã€‚COBRAåœ¨æ£€ç´¢è¿‡ç¨‹ä¸­å¢åŠ äº†å¾®ä¸è¶³é“çš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶ä¸ºä¸‹æ¸¸æ¨¡å‹æ€§èƒ½æä¾›äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17684v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ•°æ®å¢å¼ºæ£€ç´¢å·²æˆä¸ºæé«˜ä½æ•°æ®ç¯å¢ƒä¸‹æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ é¢†åŸŸã€‚è¿‡å»çš„æ–¹æ³•ä»…ä½¿ç”¨åŸºäºæœ€è¿‘é‚»çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå€¾å‘äºé€‰æ‹©ä¸ç›®æ ‡ä»»åŠ¡é«˜åº¦ç›¸ä¼¼çš„è¾…åŠ©æ ·æœ¬ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æ˜“é€‰å–å†—ä½™æ ·æœ¬ï¼Œå¿½ç•¥äº†å¤šæ ·æ€§ã€‚æœ¬ç ”ç©¶é€šè¿‡ç»„åˆäº’ä¿¡æ¯ï¼ˆCMIï¼‰åº¦é‡æ–¹æ³•ï¼Œæ³›åŒ–æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œå¹¶æå‡ºCOBRAï¼ˆç»„åˆæ£€ç´¢å¢å¼ºï¼‰ã€‚COBRAé‡‡ç”¨è€ƒè™‘å¤šæ ·æ€§å’Œä¸ç›®æ ‡æ•°æ®é›†ç›¸ä¼¼æ€§çš„æ–°å‹CMIåº¦é‡æ–¹å¼ï¼Œåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡å’Œå°æ ·æœ¬å­¦ä¹ æŠ€æœ¯ä¸­å‡è¡¨ç°å‡ºå¯¹ä¹‹å‰æ£€ç´¢æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚å®ƒå¸¦æ¥çš„è®¡ç®—å¼€é”€å¾®ä¹å…¶å¾®ï¼Œä½†å¯¹ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½çš„æå‡æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€ç´¢å¢å¼ºå·²æˆä¸ºæé«˜æ¨¡å‹åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹æ€§èƒ½çš„æœ‰æ•ˆæ‰‹æ®µï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ é¢†åŸŸã€‚</li>
<li>è¿‡å»çš„æ–¹æ³•ä¸»è¦åŸºäºæœ€è¿‘é‚»ç­–ç•¥é€‰æ‹©æ•°æ®ï¼Œè¿™å¯èƒ½å¯¼è‡´é€‰å–çš„æ ·æœ¬é«˜åº¦å†—ä½™ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡ç»„åˆäº’ä¿¡æ¯ï¼ˆCMIï¼‰åº¦é‡æ–¹æ³•æ³›åŒ–æ•°æ®é€‰æ‹©ç­–ç•¥ã€‚</li>
<li>COBRAæ˜¯ä¸€ç§æ–°å‹æ£€ç´¢å¢å¼ºæ–¹æ³•ï¼Œè€ƒè™‘äº†å¤šæ ·æ€§å’Œä¸ç›®æ ‡æ•°æ®é›†çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>COBRAåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡å’Œå°æ ·æœ¬å­¦ä¹ æŠ€æœ¯ä¸­å‡ä¼˜äºä¹‹å‰çš„æ£€ç´¢æ–¹æ³•ã€‚</li>
<li>COBRAå¸¦æ¥çš„è®¡ç®—å¼€é”€å¾ˆå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-21dafc1597d4b241c0140d8a400b7dc3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f958588aa1d713116b338f3b34ee632c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8e60b0dac9d877e89760dc5a253cf10d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-adb77f9399de31bda66dc78de47355d7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Kernel-Aware-Graph-Prompt-Learning-for-Few-Shot-Anomaly-Detection"><a href="#Kernel-Aware-Graph-Prompt-Learning-for-Few-Shot-Anomaly-Detection" class="headerlink" title="Kernel-Aware Graph Prompt Learning for Few-Shot Anomaly Detection"></a>Kernel-Aware Graph Prompt Learning for Few-Shot Anomaly Detection</h2><p><strong>Authors:Fenfang Tao, Guo-Sen Xie, Fang Zhao, Xiangbo Shu</strong></p>
<p>Few-shot anomaly detection (FSAD) aims to detect unseen anomaly regions with the guidance of very few normal support images from the same class. Existing FSAD methods usually find anomalies by directly designing complex text prompts to align them with visual features under the prevailing large vision-language model paradigm. However, these methods, almost always, neglect intrinsic contextual information in visual features, e.g., the interaction relationships between different vision layers, which is an important clue for detecting anomalies comprehensively. To this end, we propose a kernel-aware graph prompt learning framework, termed as KAG-prompt, by reasoning the cross-layer relations among visual features for FSAD. Specifically, a kernel-aware hierarchical graph is built by taking the different layer features focusing on anomalous regions of different sizes as nodes, meanwhile, the relationships between arbitrary pairs of nodes stand for the edges of the graph. By message passing over this graph, KAG-prompt can capture cross-layer contextual information, thus leading to more accurate anomaly prediction. Moreover, to integrate the information of multiple important anomaly signals in the prediction map, we propose a novel image-level scoring method based on multi-level information fusion. Extensive experiments on MVTecAD and VisA datasets show that KAG-prompt achieves state-of-the-art FSAD results for image-level&#x2F;pixel-level anomaly detection. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CVL-hub/KAG-prompt.git">https://github.com/CVL-hub/KAG-prompt.git</a>. </p>
<blockquote>
<p>å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆFSADï¼‰æ—¨åœ¨åˆ©ç”¨åŒä¸€ç±»åˆ«ä¸­å°‘é‡çš„æ­£å¸¸æ”¯æŒå›¾åƒæ¥æ£€æµ‹æœªè§è¿‡çš„å¼‚å¸¸åŒºåŸŸã€‚ç°æœ‰çš„FSADæ–¹æ³•é€šå¸¸é€šè¿‡ç›´æ¥è®¾è®¡å¤æ‚çš„æ–‡æœ¬æç¤ºæ¥ä¸æµè¡Œçš„è§†è§‰è¯­è¨€æ¨¡å‹èŒƒå¼ä¸‹çš„è§†è§‰ç‰¹å¾å¯¹é½æ¥å‘ç°å¼‚å¸¸ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å‡ ä¹æ€»æ˜¯å¿½ç•¥äº†è§†è§‰ç‰¹å¾ä¸­çš„å†…åœ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¾‹å¦‚ä¸åŒè§†è§‰å±‚ä¹‹é—´çš„äº¤äº’å…³ç³»ï¼Œè¿™æ˜¯å…¨é¢æ£€æµ‹å¼‚å¸¸çš„é‡è¦çº¿ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ ¸å¿ƒæ„ŸçŸ¥å›¾æç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºKAG-promptï¼Œé€šè¿‡æ¨ç†è§†è§‰ç‰¹å¾ä¹‹é—´çš„è·¨å±‚å…³ç³»æ¥è¿›è¡ŒFSADã€‚å…·ä½“æ¥è¯´ï¼Œä»¥å…³æ³¨ä¸åŒå¤§å°å¼‚å¸¸åŒºåŸŸçš„ä¸åŒå±‚ç‰¹å¾ä½œä¸ºèŠ‚ç‚¹ï¼Œæ„å»ºäº†ä¸€ä¸ªæ ¸å¿ƒæ„ŸçŸ¥åˆ†å±‚å›¾ï¼ŒåŒæ—¶ï¼Œä»»æ„èŠ‚ç‚¹å¯¹ä¹‹é—´çš„å…³ç³»ä»£è¡¨å›¾çš„è¾¹ã€‚é€šè¿‡åœ¨æ­¤å›¾ä¸Šè¿›è¡Œæ¶ˆæ¯ä¼ é€’ï¼ŒKAG-promptå¯ä»¥æ•è·è·¨å±‚ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„å¼‚å¸¸é¢„æµ‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ•´åˆé¢„æµ‹å›¾ä¸­å¤šä¸ªé‡è¦å¼‚å¸¸ä¿¡å·çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šå±‚æ¬¡ä¿¡æ¯èåˆçš„æ–°å‹å›¾åƒçº§è¯„åˆ†æ–¹æ³•ã€‚åœ¨MVTecADå’ŒVisAæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒKAG-promptåœ¨å›¾åƒçº§&#x2F;åƒç´ çº§çš„å¼‚å¸¸æ£€æµ‹ä¸­è¾¾åˆ°äº†æœ€æ–°çš„FSADç»“æœã€‚ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/CVLhub/KAG-prompt.git">https://github.com/CVLhub/KAG-prompt.git</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17619v1">PDF</a> Accepted to AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºè·¨å±‚å…³ç³»æ¨ç†çš„é¢å‘å°æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„æ ¸æ„ŸçŸ¥å›¾æç¤ºå­¦ä¹ æ¡†æ¶ï¼ˆKAG-promptï¼‰ã€‚è¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªä»¥ä¸åŒå±‚ç‰¹å¾ä¸ºåŸºç¡€çš„æ ¸æ„ŸçŸ¥å±‚æ¬¡å›¾ï¼Œèƒ½æœ‰æ•ˆæ•æ‰å¼‚å¸¸åŒºåŸŸçš„è·¨å±‚ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿›è€Œæé«˜å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åŸºäºå¤šå±‚æ¬¡ä¿¡æ¯èåˆçš„æ–°å›¾åƒçº§è¯„åˆ†æ–¹æ³•ï¼Œç”¨äºæ•´åˆé¢„æµ‹å›¾ä¸­çš„å¤šä¸ªé‡è¦å¼‚å¸¸ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼ŒKAG-promptåœ¨MVTecADå’ŒVisAæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å›¾åƒçº§å’Œåƒç´ çº§å¼‚å¸¸æ£€æµ‹ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>KAG-promptæ˜¯ä¸€ä¸ªé¢å‘å°æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„æ ¸æ„ŸçŸ¥å›¾æç¤ºå­¦ä¹ æ¡†æ¶ã€‚</li>
<li>é€šè¿‡æ„å»ºæ ¸æ„ŸçŸ¥å±‚æ¬¡å›¾ï¼ŒKAG-promptèƒ½æ•æ‰è·¨å±‚ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»¥æé«˜å¼‚å¸¸æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨å¤šå±‚æ¬¡ä¿¡æ¯èåˆçš„æ–¹æ³•ï¼Œæ•´åˆé¢„æµ‹å›¾ä¸­çš„å¤šä¸ªé‡è¦å¼‚å¸¸ä¿¡å·ã€‚</li>
<li>KAG-promptåœ¨MVTecADå’ŒVisAæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å›¾åƒçº§å’Œåƒç´ çº§å¼‚å¸¸æ£€æµ‹ç»“æœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-200a9d4b6eb0852c88449fc457abdabf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f64c6213e92bb025a53b9832ee5139cd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-da0b28b54c0b8692f44b391df54435db.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AFANet-Adaptive-Frequency-Aware-Network-for-Weakly-Supervised-Few-Shot-Semantic-Segmentation"><a href="#AFANet-Adaptive-Frequency-Aware-Network-for-Weakly-Supervised-Few-Shot-Semantic-Segmentation" class="headerlink" title="AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot   Semantic Segmentation"></a>AFANet: Adaptive Frequency-Aware Network for Weakly-Supervised Few-Shot   Semantic Segmentation</h2><p><strong>Authors:Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li</strong></p>
<p>Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5\textsuperscript{i} and COCO-20\textsuperscript{i} datasets demonstrate that AFANet has achieved state-of-the-art performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/jarch-ma/AFANet">https://github.com/jarch-ma/AFANet</a>. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ æ—¨åœ¨é€šè¿‡ä»å°‘é‡æ ·æœ¬ä¸­å­¦ä¹ åˆ°çš„å…ˆéªŒçŸ¥è¯†æ¥è¯†åˆ«æ–°æ¦‚å¿µã€‚ç„¶è€Œï¼Œå¯¹äºè§†è§‰å¯†é›†å‹ä»»åŠ¡ï¼ˆå¦‚å°‘é‡è¯­ä¹‰åˆ†å‰²ï¼‰è€Œè¨€ï¼Œåƒç´ çº§æ³¨é‡Šæ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ã€‚å› æ­¤ï¼Œæœ¬æ–‡é‡‡ç”¨æ›´å…·æŒ‘æˆ˜æ€§çš„å›¾åƒçº§æ³¨é‡Šï¼Œå¹¶æå‡ºä¸€ç§è‡ªé€‚åº”é¢‘ç‡æ„ŸçŸ¥ç½‘ç»œï¼ˆAFANetï¼‰ç”¨äºå¼±ç›‘ç£å°‘é‡è¯­ä¹‰åˆ†å‰²ï¼ˆWFSSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºä¸€ç§è·¨ç²’åº¦é¢‘ç‡æ„ŸçŸ¥æ¨¡å—ï¼ˆCFMï¼‰ï¼Œè¯¥æ¨¡å—å°†RGBå›¾åƒåˆ†è§£ä¸ºé«˜é¢‘å’Œä½é¢‘åˆ†å¸ƒï¼Œå¹¶é€šè¿‡é‡æ–°å¯¹é½è¿›ä¸€æ­¥ä¼˜åŒ–è¯­ä¹‰ç»“æ„ä¿¡æ¯ã€‚ä¸å¤§å¤šæ•°ç°æœ‰ä½¿ç”¨å¤šæ¨¡æ€è¯­è¨€è§†è§‰æ¨¡å‹çš„æ–‡æœ¬ä¿¡æ¯çš„WFSSæ–¹æ³•ä¸åŒï¼ˆä¾‹å¦‚CLIPç¦»çº¿å­¦ä¹ æ–¹å¼ï¼‰ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºä¸€ä¸ªCLIPå¼•å¯¼çš„ç©ºé—´é€‚é…å™¨æ¨¡å—ï¼ˆCSMï¼‰ï¼Œè¯¥æ¨¡å—é€šè¿‡åœ¨çº¿å­¦ä¹ å¯¹æ–‡æœ¬ä¿¡æ¯è¿›è¡Œç©ºé—´åŸŸè‡ªé€‚åº”è½¬æ¢ï¼Œä»è€Œä¸ºCFMæä¾›ä¸°å¯Œçš„è·¨æ¨¡æ€è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨Pascal-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAFANetå·²ç»è¾¾åˆ°äº†ä¸šç•Œæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jarch-ma/AFANet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jarch-ma/AFANetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17601v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¼±ç›‘ç£å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆWFSSï¼‰é—®é¢˜ï¼Œåˆ©ç”¨å›¾åƒçº§æ ‡æ³¨ä¿¡æ¯ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”é¢‘ç‡æ„ŸçŸ¥ç½‘ç»œï¼ˆAFANetï¼‰ã€‚è¯¥ç½‘ç»œåŒ…æ‹¬ä¸€ä¸ªè·¨ç²’åº¦é¢‘ç‡æ„ŸçŸ¥æ¨¡å—ï¼ˆCFMï¼‰ï¼Œå¯å°†å›¾åƒåˆ†è§£ä¸ºé«˜é¢‘å’Œä½é¢‘åˆ†å¸ƒï¼Œå¹¶é‡æ–°å¯¹é½è¯­ä¹‰ç»“æ„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§CLIPå¼•å¯¼çš„ç©ºé—´é€‚é…å™¨æ¨¡å—ï¼ˆCSMï¼‰ï¼Œé€šè¿‡åœ¨çº¿å­¦ä¹ æ–¹å¼å¯¹æ–‡æœ¬ä¿¡æ¯è¿›è¡Œç©ºé—´åŸŸè‡ªé€‚åº”è½¬æ¢ï¼Œä¸ºCFMæä¾›ä¸°å¯Œçš„è·¨æ¨¡æ€è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨Pascal-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAFANetè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬å­¦ä¹ æ—¨åœ¨é€šè¿‡ä»å°‘é‡æ ·æœ¬ä¸­å­¦ä¹ åˆ°çš„å…ˆéªŒçŸ¥è¯†æ¥è¯†åˆ«æ–°æ¦‚å¿µã€‚</li>
<li>é’ˆå¯¹è§†è§‰å¯†é›†å‹ä»»åŠ¡å¦‚å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼Œåƒç´ çº§æ ‡æ³¨æ˜¯è€—æ—¶ä¸”æ˜‚è´µçš„ã€‚</li>
<li>æœ¬æ–‡åˆ©ç”¨æ›´å…·æŒ‘æˆ˜æ€§çš„å›¾åƒçº§æ ‡æ³¨ä¿¡æ¯ï¼Œæå‡ºè‡ªé€‚åº”é¢‘ç‡æ„ŸçŸ¥ç½‘ç»œï¼ˆAFANetï¼‰è¿›è¡Œå¼±ç›‘ç£å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>AFANetåŒ…æ‹¬è·¨ç²’åº¦é¢‘ç‡æ„ŸçŸ¥æ¨¡å—ï¼ˆCFMï¼‰ï¼Œå¯å°†å›¾åƒåˆ†è§£ä¸ºé«˜é¢‘å’Œä½é¢‘åˆ†å¸ƒï¼Œå¹¶ä¼˜åŒ–è¯­ä¹‰ç»“æ„ä¿¡æ¯ã€‚</li>
<li>ä¸å¤§å¤šæ•°ä½¿ç”¨ç¦»çº¿å­¦ä¹ æ–¹å¼çš„å¤šæ¨¡æ€è¯­è¨€è§†è§‰æ¨¡å‹ä¸åŒï¼Œæœ¬æ–‡æå‡ºäº†CLIPå¼•å¯¼çš„ç©ºé—´é€‚é…å™¨æ¨¡å—ï¼ˆCSMï¼‰ï¼Œé€šè¿‡åœ¨çº¿å­¦ä¹ æ–¹å¼å¯¹æ–‡æœ¬ä¿¡æ¯è¿›è¡Œç©ºé—´åŸŸè‡ªé€‚åº”è½¬æ¢ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒAFANetåœ¨Pascal-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c286b546a7b8b112f49e70c8c692a052.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-92b12f4ef3edec5343b33d63af5f2e4b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ef3c89546934a25924e4a0910914e013.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-912009cb34665b008f15f63397f9a37e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Revisiting-Multimodal-Fusion-for-3D-Anomaly-Detection-from-an-Architectural-Perspective"><a href="#Revisiting-Multimodal-Fusion-for-3D-Anomaly-Detection-from-an-Architectural-Perspective" class="headerlink" title="Revisiting Multimodal Fusion for 3D Anomaly Detection from an   Architectural Perspective"></a>Revisiting Multimodal Fusion for 3D Anomaly Detection from an   Architectural Perspective</h2><p><strong>Authors:Kaifang Long, Guoyang Xie, Lianbo Ma, Jiaqi Liu, Zhichao Lu</strong></p>
<p>Existing efforts to boost multimodal fusion of 3D anomaly detection (3D-AD) primarily concentrate on devising more effective multimodal fusion strategies. However, little attention was devoted to analyzing the role of multimodal fusion architecture (topology) design in contributing to 3D-AD. In this paper, we aim to bridge this gap and present a systematic study on the impact of multimodal fusion architecture design on 3D-AD. This work considers the multimodal fusion architecture design at the intra-module fusion level, i.e., independent modality-specific modules, involving early, middle or late multimodal features with specific fusion operations, and also at the inter-module fusion level, i.e., the strategies to fuse those modules. In both cases, we first derive insights through theoretically and experimentally exploring how architectural designs influence 3D-AD. Then, we extend SOTA neural architecture search (NAS) paradigm and propose 3D-ADNAS to simultaneously search across multimodal fusion strategies and modality-specific modules for the first time.Extensive experiments show that 3D-ADNAS obtains consistent improvements in 3D-AD across various model capacities in terms of accuracy, frame rate, and memory usage, and it exhibits great potential in dealing with few-shot 3D-AD tasks. </p>
<blockquote>
<p>ç°æœ‰çš„ä¿ƒè¿›3Då¼‚å¸¸æ£€æµ‹ï¼ˆ3D-ADï¼‰çš„å¤šæ¨¡æ€èåˆå·¥ä½œä¸»è¦é›†ä¸­åœ¨è®¾è®¡æ›´æœ‰æ•ˆçš„å¤šæ¨¡æ€èåˆç­–ç•¥ä¸Šã€‚ç„¶è€Œï¼Œå¾ˆå°‘æœ‰äººå…³æ³¨å¤šæ¨¡æ€èåˆæ¶æ„ï¼ˆæ‹“æ‰‘ï¼‰è®¾è®¡å¯¹3D-ADçš„è´¡çŒ®ã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œå¹¶ç³»ç»Ÿåœ°ç ”ç©¶äº†å¤šæ¨¡æ€èåˆæ¶æ„è®¾è®¡å¯¹3D-ADçš„å½±å“ã€‚è¿™é¡¹å·¥ä½œè€ƒè™‘äº†æ¨¡å—å†…éƒ¨èåˆå±‚é¢çš„å¤šæ¨¡æ€èåˆæ¶æ„è®¾è®¡ï¼Œå³ç‹¬ç«‹çš„æ¨¡æ€ç‰¹å®šæ¨¡å—ï¼Œæ¶‰åŠæ—©æœŸã€ä¸­æœŸæˆ–æ™šæœŸçš„å¤šæ¨¡æ€ç‰¹å¾ä»¥åŠç‰¹å®šçš„èåˆæ“ä½œï¼Œè¿˜è€ƒè™‘äº†æ¨¡å—é—´èåˆå±‚é¢çš„ç­–ç•¥ï¼Œå³èåˆè¿™äº›æ¨¡å—çš„æ–¹æ³•ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é¦–å…ˆè¦é€šè¿‡ç†è®ºå’Œå®éªŒæ¢ç´¢æ¶æ„è®¾è®¡å¦‚ä½•å½±å“3D-ADï¼Œä»è€Œè·å¾—å¯ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬æ‰©å±•äº†æœ€å…ˆè¿›çš„ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰èŒƒå¼ï¼Œé¦–æ¬¡æå‡ºäº†ç”¨äºåŒæ—¶æœç´¢å¤šæ¨¡æ€èåˆç­–ç•¥å’Œæ¨¡æ€ç‰¹å®šæ¨¡å—çš„3D-ADNASã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å‡†ç¡®æ€§ã€å¸§é€Ÿç‡å’Œå†…å­˜ä½¿ç”¨æ–¹é¢ï¼Œ3D-ADNASåœ¨å¤šç§æ¨¡å‹å®¹é‡çš„3D-ADä¸­å–å¾—äº†æŒç»­çš„æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨å¤„ç†å°‘æ ·æœ¬çš„3D-ADä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17297v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨å¡«è¡¥ç°æœ‰ç ”ç©¶ä¸­å…³äºå¤šæ¨¡æ€èåˆæ¶æ„å¯¹ä¸‰ç»´å¼‚å¸¸æ£€æµ‹ï¼ˆ3D-ADï¼‰å½±å“ç ”ç©¶çš„ç©ºç™½ã€‚æ–‡ç« å¯¹å¤šæ¨¡æ€èåˆæ¶æ„çš„è®¾è®¡è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œæ¶µç›–äº†æ¨¡å—å†…éƒ¨å’Œæ¨¡å—é—´çš„èåˆç­–ç•¥ã€‚åŒæ—¶ï¼Œæå‡ºäº†æ”¹è¿›çš„ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰æ–¹æ³•ï¼Œç”¨äºåŒæ—¶æœç´¢å¤šæ¨¡æ€èåˆç­–ç•¥å’Œæ¨¡æ€ç‰¹å®šæ¨¡å—ã€‚å®éªŒè¡¨æ˜ï¼Œæ–°æ–¹æ³•åœ¨å¤šç§æ¨¡å‹å®¹é‡ä¸‹æé«˜äº†ä¸‰ç»´å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ã€å¸§ç‡å’Œå†…å­˜ä½¿ç”¨ç‡ï¼Œå¹¶åœ¨å¤„ç†å°‘é‡æ ·æœ¬çš„ä¸‰ç»´å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« é‡ç‚¹å…³æ³¨å¤šæ¨¡æ€èåˆæ¶æ„å¯¹ä¸‰ç»´å¼‚å¸¸æ£€æµ‹ï¼ˆ3D-ADï¼‰çš„å½±å“ï¼Œè¿™æ˜¯ç°æœ‰ç ”ç©¶çš„ç©ºç™½é¢†åŸŸã€‚</li>
<li>ç ”ç©¶æ¶‰åŠæ¨¡å—å†…éƒ¨å’Œæ¨¡å—é—´çš„å¤šæ¨¡æ€èåˆç­–ç•¥ã€‚</li>
<li>æå‡ºæ”¹è¿›çš„ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰æ–¹æ³•ï¼Œå³3D-ADNASï¼Œå¯åŒæ—¶æœç´¢å¤šæ¨¡æ€èåˆç­–ç•¥å’Œæ¨¡æ€ç‰¹å®šæ¨¡å—ã€‚</li>
<li>é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œæ­ç¤ºäº†å¤šæ¨¡æ€èåˆæ¶æ„å¯¹ä¸‰ç»´å¼‚å¸¸æ£€æµ‹çš„å½±å“ã€‚</li>
<li>3D-ADNASæ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å®¹é‡ä¸‹å‡èƒ½æé«˜ä¸‰ç»´å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ã€å¸§ç‡å’Œå†…å­˜ä½¿ç”¨ç‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å°‘é‡æ ·æœ¬çš„ä¸‰ç»´å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6a066d80b9c855bb2ca6a1935f2009d1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5aec74e030a6c2ed404e58a81381eda4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a60debd2e2f13f349de281ee4c0a1688.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f46f99b685955b5a92d853367c731b45.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-409eb83315de4a974782e85ed8950898.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-58286c5f97e5b607412b05ab063d338d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8593fb0144558bd7a5b12bb77578c91f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-42ce3fda951671838aedc60e8f6a1963.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-04e196ccf1ed90ab042139728cb3a417.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multi-Modal-Grounded-Planning-and-Efficient-Replanning-For-Learning-Embodied-Agents-with-A-Few-Examples"><a href="#Multi-Modal-Grounded-Planning-and-Efficient-Replanning-For-Learning-Embodied-Agents-with-A-Few-Examples" class="headerlink" title="Multi-Modal Grounded Planning and Efficient Replanning For Learning   Embodied Agents with A Few Examples"></a>Multi-Modal Grounded Planning and Efficient Replanning For Learning   Embodied Agents with A Few Examples</h2><p><strong>Authors:Taewoong Kim, Byeonghwi Kim, Jonghyun Choi</strong></p>
<p>Learning a perception and reasoning module for robotic assistants to plan steps to perform complex tasks based on natural language instructions often requires large free-form language annotations, especially for short high-level instructions. To reduce the cost of annotation, large language models (LLMs) are used as a planner with few data. However, when elaborating the steps, even the state-of-the-art planner that uses LLMs mostly relies on linguistic common sense, often neglecting the status of the environment at command reception, resulting in inappropriate plans. To generate plans grounded in the environment, we propose FLARE (Few-shot Language with environmental Adaptive Replanning Embodied agent), which improves task planning using both language command and environmental perception. As language instructions often contain ambiguities or incorrect expressions, we additionally propose to correct the mistakes using visual cues from the agent. The proposed scheme allows us to use a few language pairs thanks to the visual cues and outperforms state-of-the-art approaches. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/snumprlab/flare">https://github.com/snumprlab/flare</a>. </p>
<blockquote>
<p>å¯¹äºæœºå™¨äººåŠ©ç†æ¥è¯´ï¼Œå­¦ä¹ åŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„æ„ŸçŸ¥å’Œæ¨ç†æ¨¡å—é€šå¸¸éœ€è¦å¤§é‡çš„è‡ªç”±å½¢å¼è¯­è¨€æ³¨é‡Šï¼Œç‰¹åˆ«æ˜¯å¯¹äºç®€çŸ­çš„é«˜çº§æŒ‡ä»¤ã€‚ä¸ºäº†å‡å°‘æ³¨é‡Šçš„æˆæœ¬ï¼Œä½¿ç”¨å°‘é‡æ•°æ®çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè§„åˆ’å™¨ã€‚ç„¶è€Œï¼Œåœ¨è¯¦ç»†é˜è¿°æ­¥éª¤æ—¶ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„è§„åˆ’å™¨ä¹Ÿä¸»è¦ä¾èµ–äºè¯­è¨€å¸¸è¯†ï¼Œå¾€å¾€å¿½ç•¥äº†æ¥æ”¶å‘½ä»¤æ—¶ç¯å¢ƒçš„çŠ¶æ€ï¼Œå¯¼è‡´è®¡åˆ’ä¸å½“ã€‚ä¸ºäº†ç”ŸæˆåŸºäºç¯å¢ƒçš„è®¡åˆ’ï¼Œæˆ‘ä»¬æå‡ºäº†FLAREï¼ˆå…·æœ‰ç¯å¢ƒè‡ªé€‚åº”é‡æ–°è§„åˆ’åŠŸèƒ½çš„åµŒå…¥å¼ä»£ç†çš„å°‘é‡è¯­è¨€ï¼‰ï¼Œå®ƒç»“åˆäº†è¯­è¨€å‘½ä»¤å’Œç¯å¢ƒæ„ŸçŸ¥ï¼Œæ”¹è¿›äº†ä»»åŠ¡è§„åˆ’ã€‚ç”±äºè¯­è¨€æŒ‡ä»¤é€šå¸¸åŒ…å«æ¨¡ç³Šæˆ–é”™è¯¯çš„è¡¨è¾¾ï¼Œæˆ‘ä»¬è¿˜æè®®åˆ©ç”¨ä»£ç†çš„è§†è§‰çº¿ç´¢æ¥çº æ­£é”™è¯¯ã€‚è¯¥æ–¹æ¡ˆå…è®¸æˆ‘ä»¬åˆ©ç”¨å°‘é‡çš„è¯­è¨€å¯¹ï¼Œå¾—ç›Šäºè§†è§‰çº¿ç´¢ï¼Œå¹¶ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/snumprlab/flare%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/snumprlab/flareæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17288v1">PDF</a> AAAI 2025 (Project page: <a target="_blank" rel="noopener" href="https://twoongg.github.io/projects/flare/">https://twoongg.github.io/projects/flare/</a>)</p>
<p><strong>Summary</strong><br>åŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„æœºå™¨äººåŠ©æ‰‹æ‰§è¡Œä»»åŠ¡æ­¥éª¤çš„è®¡åˆ’éœ€è¦å¤§å‹è‡ªç”±å½¢å¼çš„è¯­è¨€æ³¨é‡Šï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç®€çŸ­çš„é«˜çº§æŒ‡ä»¤æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚ä¸ºå‡å°‘æ ‡æ³¨æˆæœ¬ï¼Œä½¿ç”¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè®¡åˆ’è€…è¿›è¡Œæ•°æ®å¤„ç†è¾ƒå°‘çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œåœ¨æ‰§è¡Œè¯¦ç»†æ­¥éª¤æ—¶ï¼Œå³ä½¿æ˜¯ä½¿ç”¨LLMçš„æœ€å…ˆè¿›è®¡åˆ’è€…ä¹Ÿä¸»è¦ä¾èµ–äºè¯­è¨€å¸¸è¯†ï¼Œå¾€å¾€å¿½ç•¥äº†æ¥æ”¶å‘½ä»¤æ—¶çš„ç¯å¢ƒçŠ¶æ€ï¼Œå¯¼è‡´è®¡åˆ’ä¸å½“ã€‚ä¸ºäº†ç”ŸæˆåŸºäºç¯å¢ƒçš„è®¡åˆ’ï¼Œæˆ‘ä»¬æå‡ºäº†FLAREï¼ˆå…·æœ‰ç¯å¢ƒè‡ªé€‚åº”é‡æ–°è§„åˆ’çš„å°‘æ•°è¯­è¨€ä½“ç°ä»£ç†ï¼‰ï¼Œå®ƒé€šè¿‡è¯­è¨€å‘½ä»¤å’Œç¯å¢ƒæ„ŸçŸ¥æ¥æ”¹è¿›ä»»åŠ¡è§„åˆ’ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³è¯­è¨€æŒ‡ä»¤ä¸­çš„æ¨¡ç³Šæˆ–é”™è¯¯è¡¨è¾¾é—®é¢˜ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åˆ©ç”¨ä»£ç†çš„è§†è§‰çº¿ç´¢è¿›è¡Œä¿®æ­£ã€‚è¯¥æ–¹æ¡ˆå…è®¸æˆ‘ä»¬åˆ©ç”¨å°‘æ•°è¯­è¨€é…å¯¹å¾—ç›Šäºè§†è§‰çº¿ç´¢å¹¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥å‡å°‘æ ‡æ³¨æˆæœ¬ï¼Œç”¨äºæœºå™¨äººåŠ©æ‰‹çš„è§„åˆ’ä»»åŠ¡ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„è®¡åˆ’è€…åœ¨æ‰§è¡Œè¯¦ç»†æ­¥éª¤æ—¶ä¸»è¦ä¾èµ–è¯­è¨€å¸¸è¯†ï¼Œå¿½ç•¥äº†ç¯å¢ƒçŠ¶æ€ã€‚</li>
<li>FLAREç»“åˆäº†è¯­è¨€å‘½ä»¤å’Œç¯å¢ƒæ„ŸçŸ¥ï¼Œæé«˜äº†ä»»åŠ¡è§„åˆ’çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯­è¨€æŒ‡ä»¤ä¸­å¸¸å­˜åœ¨æ¨¡ç³Šæˆ–é”™è¯¯è¡¨è¾¾ï¼Œéœ€è¦å€ŸåŠ©è§†è§‰çº¿ç´¢è¿›è¡Œä¿®æ­£ã€‚</li>
<li>FLAREåˆ©ç”¨å°‘æ•°è¯­è¨€é…å¯¹å¾—ç›Šäºè§†è§‰çº¿ç´¢è¿›è¡Œä»»åŠ¡è§„åˆ’ï¼Œè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å·²ç»å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/snumprlab/flare%E3%80%82">https://github.com/snumprlab/flareã€‚</a></li>
<li>FLAREå¯¹æœºå™¨äººç†è§£å¹¶æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„èƒ½åŠ›æœ‰æ½œåœ¨çš„æå‡ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17288">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-08be43202dd1ba7e68352fc068a526a0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-11cef0f2e7f2bb1d1b91e190c68feda1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-df879b88ac3954fa4e7aaf8622a776ef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-863fccc9136a8d0219c85e0c79bec906.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a5603783cc806b2d67987a9e9516e123.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-05e010536b8b4eaee33bab070cd88f2f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Self-Corrected-Flow-Distillation-for-Consistent-One-Step-and-Few-Step-Text-to-Image-Generation"><a href="#Self-Corrected-Flow-Distillation-for-Consistent-One-Step-and-Few-Step-Text-to-Image-Generation" class="headerlink" title="Self-Corrected Flow Distillation for Consistent One-Step and Few-Step   Text-to-Image Generation"></a>Self-Corrected Flow Distillation for Consistent One-Step and Few-Step   Text-to-Image Generation</h2><p><strong>Authors:Quan Dao, Hao Phung, Trung Dao, Dimitris Metaxas, Anh Tran</strong></p>
<p>Flow matching has emerged as a promising framework for training generative models, demonstrating impressive empirical performance while offering relative ease of training compared to diffusion-based models. However, this method still requires numerous function evaluations in the sampling process. To address these limitations, we introduce a self-corrected flow distillation method that effectively integrates consistency models and adversarial training within the flow-matching framework. This work is a pioneer in achieving consistent generation quality in both few-step and one-step sampling. Our extensive experiments validate the effectiveness of our method, yielding superior results both quantitatively and qualitatively on CelebA-HQ and zero-shot benchmarks on the COCO dataset. Our implementation is released at <a target="_blank" rel="noopener" href="https://github.com/VinAIResearch/SCFlow">https://github.com/VinAIResearch/SCFlow</a> </p>
<blockquote>
<p>æµé‡åŒ¹é…ä½œä¸ºä¸€ä¸ªæœ‰å‰æ™¯çš„ç”Ÿæˆæ¨¡å‹è®­ç»ƒæ¡†æ¶å·²ç»å´­éœ²å¤´è§’ã€‚ç›¸æ¯”åŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œå®ƒå±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»éªŒæ€§èƒ½ï¼Œå¹¶ä¸”ç›¸å¯¹æ›´å®¹æ˜“è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œè¯¥æ–¹æ³•åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ä»ç„¶éœ€è¦å¤§é‡å‡½æ•°è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªæˆ‘æ ¡æ­£æµé‡è’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°å°†ä¸€è‡´æ€§æ¨¡å‹å’Œå¯¹æŠ—æ€§è®­ç»ƒæ•´åˆåˆ°æµé‡åŒ¹é…æ¡†æ¶ä¸­ã€‚è¿™é¡¹å·¥ä½œåœ¨å°‘æ­¥å’Œä¸€æ­¥é‡‡æ ·ä¸­éƒ½å®ç°äº†ä¸€è‡´çš„ç”Ÿæˆè´¨é‡ï¼Œå…·æœ‰å¼€åˆ›æ€§ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨CelebA-HQå’ŒCOCOæ•°æ®é›†çš„é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸Šï¼Œå®šé‡å’Œå®šæ€§ç»“æœå‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®ç°å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/VinAIResearch/SCFlow%E3%80%82">https://github.com/VinAIResearch/SCFlowã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16906v1">PDF</a> Accepted at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æµåŒ¹é…æ¡†æ¶åœ¨è®­ç»ƒç”Ÿæˆæ¨¡å‹æ–¹é¢çš„æ½œåŠ›ï¼Œå®ƒç›¸è¾ƒäºåŸºäºæ‰©æ•£çš„æ¨¡å‹æ›´å®¹æ˜“è®­ç»ƒï¼Œå¹¶ä¸”åœ¨å®è¯ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œè¯¥æ–¹æ³•åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­éœ€è¦å¤šæ¬¡å‡½æ•°è¯„ä¼°ï¼Œä»å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªæ ¡æ­£æµè’¸é¦æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°å°†ä¸€è‡´æ€§æ¨¡å‹å’Œå¯¹æŠ—æ€§è®­ç»ƒé›†æˆåˆ°æµåŒ¹é…æ¡†æ¶ä¸­ã€‚è¿™é¡¹å·¥ä½œæ˜¯å°‘æ•°å‡ æ­¥å’Œä¸€æ­¥é‡‡æ ·ä¸­éƒ½èƒ½å®ç°ä¸€è‡´ç”Ÿæˆè´¨é‡çš„å…ˆé©±ã€‚æˆ‘ä»¬çš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨CelebA-HQå’ŒCOCOæ•°æ®é›†çš„é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ— è®ºæ˜¯å®šé‡è¿˜æ˜¯å®šæ€§ç»“æœå‡è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµåŒ¹é…æ¡†æ¶å·²æˆä¸ºè®­ç»ƒç”Ÿæˆæ¨¡å‹çš„æœ‰åŠ›å€™é€‰è€…ï¼Œå…¶ç›¸è¾ƒäºæ‰©æ•£æ¨¡å‹æ›´å®¹æ˜“è®­ç»ƒä¸”å®è¯è¡¨ç°çªå‡ºã€‚</li>
<li>å½“å‰æ–¹æ³•é‡‡æ ·è¿‡ç¨‹ä¸­éœ€è¦å¤šæ¬¡å‡½æ•°è¯„ä¼°ï¼Œå­˜åœ¨ä¸€å®šå±€é™æ€§ã€‚</li>
<li>ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„è‡ªæ ¡æ­£æµè’¸é¦æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†ä¸€è‡´æ€§æ¨¡å‹å’Œå¯¹æŠ—æ€§è®­ç»ƒï¼Œé›†æˆåœ¨æµåŒ¹é…æ¡†æ¶å†…ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å°‘æ•°å‡ æ­¥å’Œä¸€æ­¥é‡‡æ ·ä¸­å‡èƒ½å®ç°ä¸€è‡´çš„é«˜å“è´¨ç”Ÿæˆã€‚</li>
<li>å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9fa0c0a7280c7a37bc85f5c573286c89.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e5e030c58585d6ccff20e233808c89fa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8bf599ea76566347effbc3644c2ec082.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-db45a738c59ae6b639f80b3ddebeb3cb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fc74e4cc2adf828174d953481c02a340.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-338d1e2e26b7186150ddd9be3a2da48f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bae3a3df753a33112c3a4a87a700e65b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-99cc7feb5e4570818306aa242a83e8ad.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MVREC-A-General-Few-shot-Defect-Classification-Model-Using-Multi-View-Region-Context"><a href="#MVREC-A-General-Few-shot-Defect-Classification-Model-Using-Multi-View-Region-Context" class="headerlink" title="MVREC: A General Few-shot Defect Classification Model Using Multi-View   Region-Context"></a>MVREC: A General Few-shot Defect Classification Model Using Multi-View   Region-Context</h2><p><strong>Authors:Shuai Lyu, Fangjian Liao, Zeqi Ma, Rongchen Zhang, Dongmei Mo, Waikeung Wong</strong></p>
<p>Few-shot defect multi-classification (FSDMC) is an emerging trend in quality control within industrial manufacturing. However, current FSDMC research often lacks generalizability due to its focus on specific datasets. Additionally, defect classification heavily relies on contextual information within images, and existing methods fall short of effectively extracting this information. To address these challenges, we propose a general FSDMC framework called MVREC, which offers two primary advantages: (1) MVREC extracts general features for defect instances by incorporating the pre-trained AlphaCLIP model. (2) It utilizes a region-context framework to enhance defect features by leveraging mask region input and multi-view context augmentation. Furthermore, Few-shot Zip-Adapter(-F) classifiers within the model are introduced to cache the visual features of the support set and perform few-shot classification. We also introduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes 1228 defect images with instance-level mask annotations and 46 defect types. Extensive experiments conducted on MVTec-FS and four additional datasets demonstrate its effectiveness in general defect classification and its ability to incorporate contextual information to improve classification performance. Code: <a target="_blank" rel="noopener" href="https://github.com/ShuaiLYU/MVREC">https://github.com/ShuaiLYU/MVREC</a> </p>
<blockquote>
<p>å°‘æ•°æ ·æœ¬ç¼ºé™·å¤šåˆ†ç±»ï¼ˆFSDMCï¼‰æ˜¯å·¥ä¸šåˆ¶é€ è´¨é‡æ§åˆ¶é¢†åŸŸçš„æ–°å…´è¶‹åŠ¿ã€‚ç„¶è€Œï¼Œç›®å‰çš„FSDMCç ”ç©¶å¾€å¾€å› ä¸“æ³¨äºç‰¹å®šæ•°æ®é›†è€Œç¼ºä¹é€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œç¼ºé™·åˆ†ç±»ä¸¥é‡ä¾èµ–äºå›¾åƒä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆæå–è¿™äº›ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„FSDMCæ¡†æ¶ï¼Œåä¸ºMVRECï¼Œå®ƒæœ‰ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰MVRECé€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„AlphaCLIPæ¨¡å‹ï¼Œæå–ç¼ºé™·å®ä¾‹çš„é€šç”¨ç‰¹å¾ã€‚ï¼ˆ2ï¼‰å®ƒåˆ©ç”¨åŒºåŸŸä¸Šä¸‹æ–‡æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨æ©è†œåŒºåŸŸè¾“å…¥å’Œå¤šè§†å›¾ä¸Šä¸‹æ–‡å¢å¼ºæ¥å¢å¼ºç¼ºé™·ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæ¨¡å‹ä¸­å¼•å…¥äº†å°‘æ•°æ ·æœ¬Zip-Adapterï¼ˆ-Fï¼‰åˆ†ç±»å™¨ï¼Œä»¥ç¼“å­˜æ”¯æŒé›†çš„è§†è§‰ç‰¹å¾å¹¶æ‰§è¡Œå°‘æ•°æ ·æœ¬åˆ†ç±»ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†åŸºäºMVTec ADçš„FSDMCæ–°åŸºå‡†MVTec-FSï¼Œå…¶ä¸­åŒ…æ‹¬å…·æœ‰å®ä¾‹çº§æ©è†œæ³¨é‡Šçš„1228ä¸ªç¼ºé™·å›¾åƒå’Œ46ç§ç¼ºé™·ç±»å‹ã€‚åœ¨MVTec-FSå’Œå¦å¤–å››ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†å…¶åœ¨é€šç”¨ç¼ºé™·åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ä»¥åŠå…¶ç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯æé«˜åˆ†ç±»æ€§èƒ½çš„èƒ½åŠ›ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/ShuaiLYU/MVREC">https://github.com/ShuaiLYU/MVREC</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16897v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢å‘å·¥ä¸šåˆ¶é€ ä¸­è´¨é‡æ§åˆ¶çš„æ–°å…´è¶‹åŠ¿â€”â€”å°æ ·æœ¬ç¼ºé™·å¤šåˆ†ç±»ï¼ˆFSDMCï¼‰ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶ç¼ºä¹é€šç”¨æ€§å’Œåœ¨ç¼ºé™·åˆ†ç±»ä¸­æ— æ³•æœ‰æ•ˆæå–å›¾åƒä¸Šä¸‹æ–‡ä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºMVRECçš„é€šç”¨FSDMCæ¡†æ¶ã€‚è¯¥æ¡†æ¶å…·æœ‰ä¸¤å¤§ä¼˜åŠ¿ï¼šä¸€æ˜¯é€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„AlphaCLIPæ¨¡å‹æå–ç¼ºé™·å®ä¾‹çš„é€šç”¨ç‰¹å¾ï¼›äºŒæ˜¯åˆ©ç”¨åŒºåŸŸä¸Šä¸‹æ–‡æ¡†æ¶ï¼Œé€šè¿‡æ©è†œåŒºåŸŸè¾“å…¥å’Œå¤šè§†å›¾ä¸Šä¸‹æ–‡å¢å¼ºæ¥æå‡ç¼ºé™·ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å°æ ·æœ¬Zip-Adapterï¼ˆ-Fï¼‰åˆ†ç±»å™¨æ¥ç¼“å­˜æ”¯æŒé›†çš„è§†è§‰ç‰¹å¾å¹¶æ‰§è¡Œå°æ ·æœ¬åˆ†ç±»ã€‚åŒæ—¶ï¼ŒåŸºäºMVTec ADæ¨å‡ºäº†æ–°çš„FSDMCåŸºå‡†æµ‹è¯•MVTec-FSï¼ŒåŒ…å«1228å¼ ç¼ºé™·å›¾åƒå’Œå®ä¾‹çº§æ©è†œæ³¨é‡Šï¼Œä»¥åŠ46ç§ç¼ºé™·ç±»å‹ã€‚åœ¨MVTec-FSå’Œå…¶ä»–å››ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†å…¶åœ¨é€šç”¨ç¼ºé™·åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ä»¥åŠåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯æé«˜åˆ†ç±»æ€§èƒ½çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSDMCæ˜¯å·¥ä¸šåˆ¶é€ è´¨é‡æ§åˆ¶ä¸­çš„æ–°å…´è¶‹åŠ¿ï¼Œä½†ç°æœ‰ç ”ç©¶ç¼ºä¹é€šç”¨æ€§ã€‚</li>
<li>MVRECæ¡†æ¶é€šè¿‡ç»“åˆAlphaCLIPæ¨¡å‹æå–ç¼ºé™·å®ä¾‹çš„é€šç”¨ç‰¹å¾ã€‚</li>
<li>MVRECåˆ©ç”¨åŒºåŸŸä¸Šä¸‹æ–‡æ¡†æ¶ï¼Œé€šè¿‡æ©è†œåŒºåŸŸè¾“å…¥å’Œå¤šè§†å›¾ä¸Šä¸‹æ–‡å¢å¼ºæå‡ç¼ºé™·ç‰¹å¾ã€‚</li>
<li>å°æ ·æœ¬Zip-Adapterï¼ˆ-Fï¼‰åˆ†ç±»å™¨ç”¨äºç¼“å­˜æ”¯æŒé›†çš„è§†è§‰ç‰¹å¾å¹¶æ‰§è¡Œå°æ ·æœ¬åˆ†ç±»ã€‚</li>
<li>æ¨å‡ºäº†æ–°çš„FSDMCåŸºå‡†æµ‹è¯•MVTec-FSï¼ŒåŒ…å«å¤§é‡ç¼ºé™·å›¾åƒå’Œå®ä¾‹çº§æ©è†œæ³¨é‡Šã€‚</li>
<li>MVTec-FSå’Œå…¶ä»–æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†MVRECåœ¨é€šç”¨ç¼ºé™·åˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c3a7dee66974de16098575a36af33061.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-efc26c50b53ccb4c4c62e89939705e26.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-54b259f6b391328178ce97ba7a19d598.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0fdc1e952c355e9e185fd9114f2a9ecf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d0535c179bb76a7233830e0f3d92fc08.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="OpenRFT-Adapting-Reasoning-Foundation-Model-for-Domain-specific-Tasks-with-Reinforcement-Fine-Tuning"><a href="#OpenRFT-Adapting-Reasoning-Foundation-Model-for-Domain-specific-Tasks-with-Reinforcement-Fine-Tuning" class="headerlink" title="OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks   with Reinforcement Fine-Tuning"></a>OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks   with Reinforcement Fine-Tuning</h2><p><strong>Authors:Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Yuhang Wang, Jinlin Xiao, Jitao Sang</strong></p>
<p>OpenAIâ€™s recent introduction of Reinforcement Fine-Tuning (RFT) showcases the potential of reasoning foundation model and offers a new paradigm for fine-tuning beyond simple pattern imitation. This technical report presents \emph{OpenRFT}, our attempt to fine-tune generalist reasoning models for domain-specific tasks under the same settings as RFT. OpenRFT addresses two key challenges of lacking reasoning step data and the limited quantity of training samples, by leveraging the domain-specific samples in three ways: question augmentation, synthesizing reasoning-process data, and few-shot ICL. The evaluation is conducted on SciKnowEval, where OpenRFT achieves notable performance gains with only $100$ domain-specific samples for each task. More experimental results will be updated continuously in later versions. Source codes, datasets, and models are disclosed at: <a target="_blank" rel="noopener" href="https://github.com/ADaM-BJTU/OpenRFT">https://github.com/ADaM-BJTU/OpenRFT</a> </p>
<blockquote>
<p>OpenAIæœ€è¿‘æ¨å‡ºçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰å±•ç¤ºäº†æ¨ç†åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œå¹¶ä¸ºè¶…è¶Šç®€å•æ¨¡å¼æ¨¡ä»¿çš„å¾®è°ƒæä¾›äº†æ–°èŒƒå¼ã€‚æœ¬æŠ€æœ¯æŠ¥å‘Šä»‹ç»äº†æˆ‘ä»¬çš„å°è¯•æ–¹æ³•<em>OpenRFT</em>ï¼Œåœ¨æ¨¡ä»¿RFTè®¾ç½®çš„æƒ…å†µä¸‹ï¼Œå¯¹é€šç”¨æ¨ç†æ¨¡å‹è¿›è¡Œç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡å¾®è°ƒã€‚OpenRFTé€šè¿‡ä¸‰ç§æ–¹å¼åˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„æ ·æœ¬ï¼Œè§£å†³äº†ç¼ºä¹æ¨ç†æ­¥éª¤æ•°æ®å’Œè®­ç»ƒæ ·æœ¬æ•°é‡æœ‰é™è¿™ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œè¿™ä¸‰ç§æ–¹å¼ä¸ºï¼šé—®é¢˜æ‰©å……ã€åˆæˆæ¨ç†è¿‡ç¨‹æ•°æ®å’Œå°‘é‡ICLã€‚åœ¨SciKnowEvalä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒOpenRFTä»…éœ€æ¯ä¸ªä»»åŠ¡$100$ä¸ªç‰¹å®šé¢†åŸŸçš„æ ·æœ¬å°±å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åç»­ç‰ˆæœ¬å°†æŒç»­æ›´æ–°æ›´å¤šçš„å®éªŒç»“æœã€‚ç›¸å…³æºä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ADaM-BJTU/OpenRFT">https://github.com/ADaM-BJTU/OpenRFT</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16849v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§æ¨¡å‹çš„å¼ºåŒ–å¾®è°ƒæŠ€æœ¯æŠ¥å‘Šä»‹ç»äº†OpenRFTï¼Œä¸€ç§é’ˆå¯¹ç‰¹å®šé¢†åŸŸä»»åŠ¡çš„é€šç”¨æ¨ç†æ¨¡å‹å¾®è°ƒæ–¹æ³•ã€‚OpenRFTé€šè¿‡ä¸‰ç§æ–¹å¼åˆ©ç”¨é¢†åŸŸç‰¹å®šæ ·æœ¬åº”å¯¹ç¼ºå°‘æ¨ç†æ­¥éª¤æ•°æ®å’Œè®­ç»ƒæ ·æœ¬æ•°é‡æœ‰é™ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚å…¶åœ¨SciKnowEvalä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œåªéœ€æ¯ä¸ªä»»åŠ¡100ä¸ªé¢†åŸŸç‰¹å®šæ ·æœ¬ï¼Œå³å¯å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æºä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenRFTæ˜¯OpenAIæ¨å‡ºçš„é’ˆå¯¹ç‰¹å®šé¢†åŸŸä»»åŠ¡çš„é€šç”¨æ¨ç†æ¨¡å‹å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>OpenRFTè§£å†³äº†ç¼ºå°‘æ¨ç†æ­¥éª¤æ•°æ®å’Œè®­ç»ƒæ ·æœ¬æ•°é‡æœ‰é™çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>OpenRFTé€šè¿‡ä¸‰ç§æ–¹å¼åˆ©ç”¨é¢†åŸŸç‰¹å®šæ ·æœ¬ï¼šé—®é¢˜å¢å¼ºã€åˆæˆæ¨ç†è¿‡ç¨‹æ•°æ®å’Œå°‘æ ·æœ¬ICLã€‚</li>
<li>åœ¨SciKnowEvalä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨OpenRFTåœ¨åªéœ€å°‘é‡é¢†åŸŸç‰¹å®šæ ·æœ¬çš„æƒ…å†µä¸‹å³å¯å®ç°æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>OpenRFTçš„æºä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å·²ç»å…¬å¼€ï¼Œä¾¿äºç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
<li>OpenRFTæä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’æ¥çœ‹å¾…å¤§æ¨¡å‹çš„å¾®è°ƒï¼Œå±•ç¤ºäº†è¶…è¶Šç®€å•æ¨¡å¼æ¨¡ä»¿çš„æ–°èŒƒå¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16849">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a22d61d17bb5eb4b51233c1995c1a2a7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8da167c5c2dc614b2e61e118bbb78c9f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LEARN-A-Unified-Framework-for-Multi-Task-Domain-Adapt-Few-Shot-Learning"><a href="#LEARN-A-Unified-Framework-for-Multi-Task-Domain-Adapt-Few-Shot-Learning" class="headerlink" title="LEARN: A Unified Framework for Multi-Task Domain Adapt Few-Shot Learning"></a>LEARN: A Unified Framework for Multi-Task Domain Adapt Few-Shot Learning</h2><p><strong>Authors:Bharadwaj Ravichandran, Alexander Lynch, Sarah Brockman, Brandon RichardWebster, Dawei Du, Anthony Hoogs, Christopher Funk</strong></p>
<p>Both few-shot learning and domain adaptation sub-fields in Computer Vision have seen significant recent progress in terms of the availability of state-of-the-art algorithms and datasets. Frameworks have been developed for each sub-field; however, building a common system or framework that combines both is something that has not been explored. As part of our research, we present the first unified framework that combines domain adaptation for the few-shot learning setting across 3 different tasks - image classification, object detection and video classification. Our framework is highly modular with the capability to support few-shot learning with&#x2F;without the inclusion of domain adaptation depending on the algorithm. Furthermore, the most important configurable feature of our framework is the on-the-fly setup for incremental $n$-shot tasks with the optional capability to configure the system to scale to a traditional many-shot task. With more focus on Self-Supervised Learning (SSL) for current few-shot learning approaches, our system also supports multiple SSL pre-training configurations. To test our frameworkâ€™s capabilities, we provide benchmarks on a wide range of algorithms and datasets across different task and problem settings. The code is open source has been made publicly available here: <a target="_blank" rel="noopener" href="https://gitlab.kitware.com/darpa_learn/learn">https://gitlab.kitware.com/darpa_learn/learn</a> </p>
<blockquote>
<p>è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„å°æ ·æœ¬å­¦ä¹ å’ŒåŸŸé€‚åº”è¿™ä¸¤ä¸ªå­é¢†åŸŸåœ¨æœ€æ–°ç®—æ³•å’Œæ•°æ®é›†çš„å¯ç”¨æ€§æ–¹é¢æœ€è¿‘å–å¾—äº†é‡å¤§è¿›å±•ã€‚è™½ç„¶æ¯ä¸ªå­é¢†åŸŸéƒ½å·²ç»å¼€å‘å‡ºäº†ç›¸åº”çš„æ¡†æ¶ï¼Œä½†æ„å»ºä¸€ä¸ªç»“åˆä¸¤è€…çš„é€šç”¨ç³»ç»Ÿæˆ–æ¡†æ¶å°šæœªè¢«æ¢ç´¢ã€‚ä½œä¸ºæˆ‘ä»¬ç ”ç©¶çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ä¸‰ç§ä¸åŒä»»åŠ¡ï¼ˆå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè§†é¢‘åˆ†ç±»ï¼‰çš„å°æ ·æœ¬å­¦ä¹ è®¾ç½®ä¸­çš„åŸŸé€‚åº”ã€‚æˆ‘ä»¬çš„æ¡†æ¶å…·æœ‰é«˜åº¦çš„æ¨¡å—åŒ–ç‰¹æ€§ï¼Œæ”¯æŒåœ¨æœ‰æˆ–æ— åŸŸé€‚åº”çš„æƒ…å†µä¸‹è¿›è¡Œå°æ ·æœ¬å­¦ä¹ ï¼Œè¿™å–å†³äºç®—æ³•çš„é€‰æ‹©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¡†æ¶æœ€é‡è¦çš„å¯é…ç½®ç‰¹æ€§æ˜¯ä¸ºå¢é‡næ¬¡ä»»åŠ¡æä¾›å³æ—¶è®¾ç½®ï¼Œå¹¶ä¸”å¯é€‰æ‹©é…ç½®ç³»ç»Ÿä»¥æ‰©å±•åˆ°ä¼ ç»Ÿçš„å¤šæ¬¡ä»»åŠ¡ã€‚å½“å‰çš„å°æ ·æœ¬å­¦ä¹ æ–¹æ³•æ›´åŠ å…³æ³¨è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿè¿˜æ”¯æŒå¤šç§SSLé¢„è®­ç»ƒé…ç½®ã€‚ä¸ºäº†æµ‹è¯•æˆ‘ä»¬æ¡†æ¶çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„ä»»åŠ¡å’Œé—®é¢˜è§£å†³è®¾ç½®ä¸Šæä¾›äº†å¹¿æ³›çš„ç®—æ³•å’Œæ•°æ®é›†çš„åŸºå‡†æµ‹è¯•ã€‚ä»£ç æ˜¯å¼€æºçš„ï¼Œå·²ç»åœ¨è¿™é‡Œå…¬å¼€å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://gitlab.kitware.com/darpa_learn/learn">https://gitlab.kitware.com/darpa_learn/learn</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16275v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡ä»‹ç»äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„å°æ ·æœ¬å­¦ä¹ å’ŒåŸŸé€‚åº”å­é¢†åŸŸçš„æ–°è¿›å±•ã€‚é’ˆå¯¹è¿™ä¸¤è€…çš„èåˆéœ€æ±‚ï¼Œæœ¬æ–‡é¦–æ¬¡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæ”¯æŒä¸åŒä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè§†é¢‘åˆ†ç±»ï¼‰ä¸­çš„å°æ ·æœ¬å­¦ä¹ å’ŒåŸŸé€‚åº”åŠŸèƒ½ã€‚è¯¥æ¡†æ¶é«˜åº¦æ¨¡å—åŒ–ï¼Œå¯ä»¥æŒ‰éœ€å¯ç”¨æˆ–ç¦ç”¨åŸŸé€‚åº”åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œå…¶è¿˜å…·æœ‰å¯é…ç½®ç‰¹æ€§ï¼Œæ”¯æŒå®æ—¶è®¾ç½®å¢é‡n-shotä»»åŠ¡ï¼Œå¹¶å¯é€‰é…ç½®ç³»ç»Ÿä»¥æ‰©å±•åˆ°ä¼ ç»Ÿçš„å¤§è§„æ¨¡å­¦ä¹ ä»»åŠ¡ã€‚è¯¥æ¡†æ¶è¿˜æ”¯æŒå¤šç§è‡ªç›‘ç£é¢„è®­ç»ƒé…ç½®ï¼Œå¹¶åœ¨ä¸åŒçš„ç®—æ³•å’Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•éªŒè¯ã€‚ä»£ç å·²å¼€æºå¹¶æä¾›é“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ¡†æ¶é¦–æ¬¡å®ç°äº†å°æ ·æœ¬å­¦ä¹ å’ŒåŸŸé€‚åº”çš„ç»Ÿä¸€ç»“åˆï¼Œæ¶µç›–å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œè§†é¢‘åˆ†ç±»ä¸‰å¤§ä»»åŠ¡ã€‚</li>
<li>æ¡†æ¶å…·æœ‰é«˜åº¦çš„æ¨¡å—åŒ–è®¾è®¡ï¼Œå¯ä»¥æ ¹æ®éœ€æ±‚çµæ´»é€‰æ‹©æ˜¯å¦å¯ç”¨åŸŸé€‚åº”åŠŸèƒ½ã€‚</li>
<li>å¯å®æ—¶é…ç½®å¢é‡n-shotä»»åŠ¡ï¼Œå¹¶å…·å¤‡å‘ä¼ ç»Ÿå¤šæ ·æœ¬ä»»åŠ¡æ‰©å±•çš„èƒ½åŠ›ã€‚</li>
<li>æ”¯æŒå¤šç§è‡ªç›‘ç£é¢„è®­ç»ƒé…ç½®ï¼Œä»¥é€‚åº”å½“å‰å°æ ·æœ¬å­¦ä¹ æ–¹æ³•çš„è¶‹åŠ¿ã€‚</li>
<li>æ¡†æ¶åœ¨å¹¿æ³›çš„ç®—æ³•å’Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•éªŒè¯å…¶æ•ˆèƒ½ã€‚</li>
<li>ä»£ç ä»¥å¼€æºå½¢å¼æä¾›ï¼Œæ–¹ä¾¿ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-062794af0242bd048085ae57748adc1a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-872d1f844cbea99d8ab94b01b2e28308.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4e6e4a7008dfa4429c4883a6af726520.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6c1b850760713833857d7314dd0cec6f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MR-GDINO-Efficient-Open-World-Continual-Object-Detection"><a href="#MR-GDINO-Efficient-Open-World-Continual-Object-Detection" class="headerlink" title="MR-GDINO: Efficient Open-World Continual Object Detection"></a>MR-GDINO: Efficient Open-World Continual Object Detection</h2><p><strong>Authors:Bowen Dong, Zitong Huang, Guanglei Yang, Lei Zhang, Wangmeng Zuo</strong></p>
<p>Open-world (OW) recognition and detection models show strong zero- and few-shot adaptation abilities, inspiring their use as initializations in continual learning methods to improve performance. Despite promising results on seen classes, such OW abilities on unseen classes are largely degenerated due to catastrophic forgetting. To tackle this challenge, we propose an open-world continual object detection task, requiring detectors to generalize to old, new, and unseen categories in continual learning scenarios. Based on this task, we present a challenging yet practical OW-COD benchmark to assess detection abilities. The goal is to motivate OW detectors to simultaneously preserve learned classes, adapt to new classes, and maintain open-world capabilities under few-shot adaptations. To mitigate forgetting in unseen categories, we propose MR-GDINO, a strong, efficient and scalable baseline via memory and retrieval mechanisms within a highly scalable memory pool. Experimental results show that existing continual detectors suffer from severe forgetting for both seen and unseen categories. In contrast, MR-GDINO largely mitigates forgetting with only 0.1% activated extra parameters, achieving state-of-the-art performance for old, new, and unseen categories. </p>
<blockquote>
<p>å¼€æ”¾ä¸–ç•Œï¼ˆOWï¼‰è¯†åˆ«å’Œæ£€æµ‹æ¨¡å‹æ˜¾ç¤ºå‡ºå¼ºå¤§çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬é€‚åº”åŠ›ï¼Œè¿™æ¿€å‘äº†å°†å…¶ä½œä¸ºåˆå§‹åŒ–ç”¨äºæŒç»­å­¦ä¹ æ–¹æ³•ä»¥æé«˜æ€§èƒ½ã€‚å°½ç®¡åœ¨å¯è§ç±»åˆ«ä¸Šå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†æ­¤ç±»OWæ¨¡å‹åœ¨æœªè§è¿‡ç±»åˆ«ä¸Šçš„èƒ½åŠ›ç”±äºç¾éš¾æ€§é—å¿˜è€Œå¤§å¤§é€€åŒ–ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¼€æ”¾ä¸–ç•ŒæŒç»­ç›®æ ‡æ£€æµ‹ä»»åŠ¡ï¼Œè¦æ±‚æ£€æµ‹å™¨åœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸­æ¨å¹¿åˆ°æ—§ã€æ–°å’Œæœªè§ç±»åˆ«ã€‚åŸºäºè¿™é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ä½†å®ç”¨çš„OW-CODåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°æ£€æµ‹èƒ½åŠ›ã€‚ç›®æ ‡æ˜¯æ¿€åŠ±OWæ£€æµ‹å™¨åœ¨ä¿ç•™å·²å­¦ç±»åˆ«ã€é€‚åº”æ–°ç±»åˆ«ä»¥åŠç»´æŒå¼€æ”¾ä¸–ç•Œèƒ½åŠ›æ–¹é¢è¿›è¡Œå°‘æ ·æœ¬é€‚åº”ã€‚ä¸ºäº†ç¼“è§£åœ¨æœªè§è¿‡ç±»åˆ«çš„é—å¿˜é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MR-GDINOï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§çš„ã€é«˜æ•ˆçš„ã€å¯æ‰©å±•çš„åŸºçº¿ï¼Œé€šè¿‡å†…å­˜å’Œæ£€ç´¢æœºåˆ¶åœ¨ä¸€ä¸ªé«˜åº¦å¯æ‰©å±•çš„å†…å­˜æ± å†…å®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„æŒç»­æ£€æµ‹å™¨åœ¨å¯è§å’Œæœªè§è¿‡ç±»åˆ«ä¸Šéƒ½å­˜åœ¨ä¸¥é‡çš„é—å¿˜é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMR-GDINOåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šç¼“è§£äº†é—å¿˜é—®é¢˜ï¼Œå¹¶ä¸”åªéœ€æ¿€æ´»0.1%çš„é¢å¤–å‚æ•°ï¼Œå³å¯å®ç°å¯¹æ—§ã€æ–°å’Œæœªè§ç±»åˆ«çš„æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15979v2">PDF</a> Website: <a target="_blank" rel="noopener" href="https://m1saka.moe/owcod/">https://m1saka.moe/owcod/</a> . Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/DongSky/MR-GDINO">https://github.com/DongSky/MR-GDINO</a></p>
<p><strong>Summary</strong></p>
<p>å¼€æ”¾å¼ä¸–ç•Œï¼ˆOWï¼‰è¯†åˆ«ä¸æ£€æµ‹æ¨¡å‹åœ¨é›¶æ¬¡å’Œå‡ æ¬¡å°„å‡»é€‚åº”æ–¹é¢å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œå¯åœ¨æŒç»­å­¦ä¹ æ–¹æ³•ä¸­ä½œä¸ºåˆå§‹åŒ–ä½¿ç”¨ä»¥æé«˜æ€§èƒ½ã€‚å°½ç®¡åœ¨å·²çŸ¥ç±»åˆ«ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æœªçŸ¥ç±»åˆ«ä¸Šçš„èƒ½åŠ›å› ç¾éš¾æ€§é—å¿˜è€Œé€€åŒ–ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†å¼€æ”¾å¼ä¸–ç•ŒæŒç»­å¯¹è±¡æ£€æµ‹ä»»åŠ¡ï¼Œè¦æ±‚æ£€æµ‹å™¨åœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸­æ³›åŒ–åˆ°æ—§ã€æ–°å’ŒæœªçŸ¥ç±»åˆ«ã€‚åŸºäºæ­¤ä»»åŠ¡ï¼Œæœ¬æ–‡æå‡ºäº†å…·æœ‰æŒ‘æˆ˜æ€§çš„å¼€æ”¾å¼ä¸–ç•ŒCODåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ£€æµ‹èƒ½åŠ›ã€‚ç›®æ ‡æ˜¯æ¿€åŠ±å¼€æ”¾å¼ä¸–ç•Œæ£€æµ‹å™¨åœ¨ä¿ç•™å·²å­¦ç±»åˆ«ã€é€‚åº”æ–°ç±»åˆ«çš„åŒæ—¶ï¼Œåœ¨å°‘é•œå¤´é€‚åº”ä¸‹ä¿æŒå¼€æ”¾å¼ä¸–ç•Œèƒ½åŠ›ã€‚ä¸ºç¼“è§£å¯¹æœªçŸ¥ç±»åˆ«çš„é—å¿˜ï¼Œæœ¬æ–‡æå‡ºäº†MR-GDINOæ–¹æ³•ï¼Œé€šè¿‡å†…å­˜å’Œæ£€ç´¢æœºåˆ¶æ„å»ºé«˜åº¦å¯æ‰©å±•çš„å†…å­˜æ± å®ç°åŸºçº¿æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ‰©å±•æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æŒç»­æ£€æµ‹å™¨å¯¹å·²çŸ¥å’ŒæœªçŸ¥ç±»åˆ«çš„é—å¿˜ä¸¥é‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMR-GDINOæå¤§åœ°ç¼“è§£äº†é—å¿˜é—®é¢˜ï¼Œä»…æ¿€æ´»é¢å¤–å‚æ•°çš„0.1%ï¼Œåœ¨æ—§ã€æ–°å’ŒæœªçŸ¥ç±»åˆ«ä¸Šå‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾ä¸–ç•Œè¯†åˆ«ä¸æ£€æµ‹æ¨¡å‹å…·å¤‡å¼ºå¤§çš„é›¶æ¬¡å’Œå‡ æ¬¡å°„å‡»é€‚åº”èƒ½åŠ›ï¼Œé€‚ç”¨äºæŒç»­å­¦ä¹ æ–¹æ³•ä¸­çš„åˆå§‹åŒ–é˜¶æ®µä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸­ï¼Œæ£€æµ‹å™¨éœ€è¦æ³›åŒ–åˆ°æ—§ã€æ–°å’ŒæœªçŸ¥ç±»åˆ«ã€‚</li>
<li>æå‡ºäº†å¼€æ”¾å¼ä¸–ç•ŒæŒç»­å¯¹è±¡æ£€æµ‹ä»»åŠ¡å’Œå¼€æ”¾å¼ä¸–ç•ŒCODåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æŒç»­æ£€æµ‹å™¨åœ¨å·²çŸ¥å’ŒæœªçŸ¥ç±»åˆ«ä¸Šå‡å­˜åœ¨ä¸¥é‡çš„é—å¿˜é—®é¢˜ã€‚</li>
<li>MR-GDINOæ–¹æ³•é€šè¿‡å†…å­˜å’Œæ£€ç´¢æœºåˆ¶æ„å»ºé«˜åº¦å¯æ‰©å±•çš„å†…å­˜æ± ï¼Œæœ‰æ•ˆç¼“è§£äº†å¯¹æœªçŸ¥ç±»åˆ«çš„é—å¿˜é—®é¢˜ã€‚</li>
<li>MR-GDINOæ–¹æ³•ä»…æ¿€æ´»é¢å¤–å‚æ•°çš„0.1%ï¼Œåœ¨æ—§ã€æ–°å’ŒæœªçŸ¥ç±»åˆ«ä¸Šå‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-34b3cdefd9b3ec179cb8c2e5014b64e7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c67f270d92a0b582e0f8224b537b87e4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5f259829d5f680f4587202efbf032300.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-32354e702dcbc3bf1998e6351f27fe5c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#FAMNet-Frequency-aware-Matching-Network-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation"></a>FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot   Medical Image Segmentation</h2><p><strong>Authors:Yuntian Bo, Yazhou Zhu, Lunbo Li, Haofeng Zhang</strong></p>
<p>Existing few-shot medical image segmentation (FSMIS) models fail to address a practical issue in medical imaging: the domain shift caused by different imaging techniques, which limits the applicability to current FSMIS tasks. To overcome this limitation, we focus on the cross-domain few-shot medical image segmentation (CD-FSMIS) task, aiming to develop a generalized model capable of adapting to a broader range of medical image segmentation scenarios with limited labeled data from the novel target domain. Inspired by the characteristics of frequency domain similarity across different domains, we propose a Frequency-aware Matching Network (FAMNet), which includes two key components: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion (MSF) module. The FAM module tackles two problems during the meta-learning phase: 1) intra-domain variance caused by the inherent support-query bias, due to the different appearances of organs and lesions, and 2) inter-domain variance caused by different medical imaging techniques. Additionally, we design an MSF module to integrate the different frequency features decoupled by the FAM module, and further mitigate the impact of inter-domain variance on the modelâ€™s segmentation performance. Combining these two modules, our FAMNet surpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation models on three cross-domain datasets, achieving state-of-the-art performance in the CD-FSMIS task. </p>
<blockquote>
<p>ç°æœ‰çš„ä¸€æ¬¡æ€§å°‘é‡åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼ˆFSMISï¼‰æ— æ³•è§£å†³åŒ»å­¦æˆåƒä¸­çš„ä¸€ä¸ªå®é™…é—®é¢˜ï¼šç”±äºä¸åŒæˆåƒæŠ€æœ¯å¯¼è‡´çš„åŸŸåç§»é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å½“å‰FSMISä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸“æ³¨äºè·¨åŸŸä¸€æ¬¡æ€§å°‘é‡åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å¼€å‘ä¸€ç§é€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„æ–°ç›®æ ‡åŸŸæ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹é€‚åº”æ›´å¹¿æ³›çš„åŒ»å­¦å›¾åƒåˆ†å‰²åœºæ™¯ã€‚æˆ‘ä»¬å—åˆ°ä¸åŒé¢†åŸŸé¢‘ç‡åŸŸç›¸ä¼¼æ€§ç‰¹å¾çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šé¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰æ¨¡å—å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰æ¨¡å—ã€‚FAMæ¨¡å—è§£å†³äº†å…ƒå­¦ä¹ é˜¶æ®µçš„ä¸¤ä¸ªé—®é¢˜ï¼šä¸€æ˜¯ç”±äºå™¨å®˜å’Œç—…ç¶å¤–è§‚ä¸åŒé€ æˆçš„å›ºæœ‰æ”¯æŒæŸ¥è¯¢åå·®å¯¼è‡´çš„åŸŸå†…æ–¹å·®ï¼›äºŒæ˜¯ç”±äºä¸åŒåŒ»å­¦æˆåƒæŠ€æœ¯å¯¼è‡´çš„è·¨åŸŸæ–¹å·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªMSFæ¨¡å—æ¥æ•´åˆè¢«FAMæ¨¡å—åˆ†ç¦»çš„ä¸åŒçš„é¢‘ç‡ç‰¹å¾ï¼Œå¹¶è¿›ä¸€æ­¥å‡è½»è·¨åŸŸæ–¹å·®å¯¹æ¨¡å‹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚ç»“åˆè¿™ä¸¤ä¸ªæ¨¡å—ï¼Œæˆ‘ä»¬çš„FAMNetåœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œåœ¨CD-FSMISä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09319v3">PDF</a> Accepted by the 39th Annual AAAI Conference on Artificial   Intelligence (AAAI-25)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨è·¨åŸŸå°‘æ ·æœ¬åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„æ–°ç›®æ ‡åŸŸæ ‡è®°æ•°æ®ä¸‹ï¼Œé€‚åº”æ›´å¹¿æ³›çš„åŒ»ç–—å›¾åƒåˆ†å‰²åœºæ™¯ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ä¸ªé¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼ŒåŒ…æ‹¬é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰æ¨¡å—å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰æ¨¡å—ã€‚FAMæ¨¡å—è§£å†³å…ƒå­¦ä¹ é˜¶æ®µçš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šå™¨å®˜å’Œç—…å˜å¤–è§‚å¼•èµ·çš„åŸŸå†…å·®å¼‚ä»¥åŠä¸åŒåŒ»å­¦æˆåƒæŠ€æœ¯å¯¼è‡´çš„åŸŸé—´å·®å¼‚ã€‚MSFæ¨¡å—è¿›ä¸€æ­¥æ•´åˆFAMæ¨¡å—è§£è€¦çš„ä¸åŒé¢‘ç‡ç‰¹å¾ï¼Œå‡è½»è·¨åŸŸå·®å¼‚å¯¹æ¨¡å‹åˆ†å‰²æ€§èƒ½çš„å½±å“ã€‚åœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šï¼ŒFAMNetè¶…è¶Šç°æœ‰FSMISæ¨¡å‹å’Œè·¨åŸŸå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œå®ç°CD-FSMISä»»åŠ¡çš„æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å°‘æ ·æœ¬åŒ»ç–—å›¾åƒåˆ†å‰²æ¨¡å‹é¢ä¸´å› ä¸åŒæˆåƒæŠ€æœ¯å¯¼è‡´çš„åŸŸåç§»é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>è·¨åŸŸå°‘æ ·æœ¬åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰æ—¨åœ¨å¼€å‘ä¸€ä¸ªèƒ½å¤Ÿé€‚åº”æ›´å¹¿æ³›åŒ»ç–—å›¾åƒåˆ†å‰²åœºæ™¯çš„é€šç”¨æ¨¡å‹ã€‚</li>
<li>æå‡ºé¢‘ç‡æ„ŸçŸ¥åŒ¹é…ç½‘ç»œï¼ˆFAMNetï¼‰ï¼ŒåŒ…å«é¢‘ç‡æ„ŸçŸ¥åŒ¹é…ï¼ˆFAMï¼‰æ¨¡å—å’Œå¤šå…‰è°±èåˆï¼ˆMSFï¼‰æ¨¡å—ä»¥åº”å¯¹è·¨åŸŸé—®é¢˜ã€‚</li>
<li>FAMæ¨¡å—è§£å†³å…ƒå­¦ä¹ é˜¶æ®µçš„åŸŸå†…å’ŒåŸŸé—´å·®å¼‚é—®é¢˜ã€‚</li>
<li>MSFæ¨¡å—æ•´åˆä¸åŒé¢‘ç‡ç‰¹å¾ï¼Œè¿›ä¸€æ­¥å‡è½»è·¨åŸŸå·®å¼‚å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</li>
<li>FAMNetåœ¨ä¸‰ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¶…è¿‡ç°æœ‰æ¨¡å‹ï¼Œå®ç°CD-FSMISä»»åŠ¡çš„æœ€æ–°è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bb17c5e92f42f7d91e3164ec653e424b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d0796f64fbec939deef49b781663f0ee.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-def190b9c5343be34695a04abbf24490.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence"><a href="#Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence" class="headerlink" title="Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence"></a>Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence</h2><p><strong>Authors:Wenbo Huang, Jinghui Zhang, Guang Li, Lei Zhang, Shuoyuan Wang, Fang Dong, Jiahui Jin, Takahiro Ogawa, Miki Haseyama</strong></p>
<p>In few-shot action recognition (FSAR), long sub-sequences of video naturally express entire actions more effectively. However, the high computational complexity of mainstream Transformer-based methods limits their application. Recent Mamba demonstrates efficiency in modeling long sequences, but directly applying Mamba to FSAR overlooks the importance of local feature modeling and alignment. Moreover, long sub-sequences within the same class accumulate intra-class variance, which adversely impacts FSAR performance. To solve these challenges, we propose a Matryoshka MAmba and CoNtrasTive LeArning framework (Manta). Firstly, the Matryoshka Mamba introduces multiple Inner Modules to enhance local feature representation, rather than directly modeling global features. An Outer Module captures dependencies of timeline between these local features for implicit temporal alignment. Secondly, a hybrid contrastive learning paradigm, combining both supervised and unsupervised methods, is designed to mitigate the negative effects of intra-class variance accumulation. The Matryoshka Mamba and the hybrid contrastive learning paradigm operate in two parallel branches within Manta, enhancing Mamba for FSAR of long sub-sequence. Manta achieves new state-of-the-art performance on prominent benchmarks, including SSv2, Kinetics, UCF101, and HMDB51. Extensive empirical studies prove that Manta significantly improves FSAR of long sub-sequence from multiple perspectives. </p>
<blockquote>
<p>åœ¨å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰ä¸­ï¼Œè§†é¢‘çš„é•¿å­åºåˆ—æ›´è‡ªç„¶åœ°è¡¨è¾¾äº†æ•´ä¸ªåŠ¨ä½œã€‚ç„¶è€Œï¼Œä¸»æµåŸºäºTransformerçš„æ–¹æ³•çš„é«˜è®¡ç®—å¤æ‚åº¦é™åˆ¶äº†å…¶åº”ç”¨ã€‚æœ€è¿‘çš„Mambaåœ¨å»ºæ¨¡é•¿åºåˆ—æ–¹é¢å±•ç¤ºäº†æ•ˆç‡ï¼Œä½†ç›´æ¥å°†Mambaåº”ç”¨äºFSARå¿½è§†äº†å±€éƒ¨ç‰¹å¾å»ºæ¨¡å’Œå¯¹é½çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼ŒåŒä¸€ç±»åˆ«å†…çš„é•¿å­åºåˆ—ä¼šç§¯ç´¯ç±»å†…å·®å¼‚ï¼Œè¿™å¯¹FSARæ€§èƒ½äº§ç”Ÿä¸åˆ©å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªMatryoshka Mambaå’Œå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼ˆMantaï¼‰ã€‚é¦–å…ˆï¼ŒMatryoshka Mambaå¼•å…¥å¤šä¸ªå†…éƒ¨æ¨¡å—æ¥å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç›´æ¥å¯¹å…¨å±€ç‰¹å¾è¿›è¡Œå»ºæ¨¡ã€‚å¤–éƒ¨æ¨¡å—æ•è·è¿™äº›å±€éƒ¨ç‰¹å¾çš„æ—¶é—´çº¿ä¾èµ–æ€§ï¼Œä»¥è¿›è¡Œéšå¼çš„æ—¶é—´å¯¹é½ã€‚å…¶æ¬¡ï¼Œç»“åˆæœ‰ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•çš„æ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼è¢«è®¾è®¡ç”¨äºç¼“è§£ç±»å†…å·®å¼‚ç§¯ç´¯å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚Matryoshka Mambaå’Œæ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼åœ¨Mantaçš„ä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯ä¸­è¿è¡Œï¼Œå¢å¼ºäº†Mambaå¯¹é•¿åºåˆ—FSARçš„èƒ½åŠ›ã€‚Mantaåœ¨åŒ…æ‹¬SSv2ã€Kineticsã€UCF101å’ŒHMDB51ç­‰ä¸»æµåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€ä½³æ€§èƒ½ã€‚å¤§é‡çš„å®è¯ç ”ç©¶è¯æ˜ï¼ŒMantaä»å¤šä¸ªè§’åº¦æ˜¾è‘—æé«˜äº†é•¿åºåˆ—FSARçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07481v3">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åœ¨å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰ä¸­ï¼Œé•¿è§†é¢‘å­åºåˆ—èƒ½æ›´è‡ªç„¶åœ°è¡¨è¾¾å®Œæ•´åŠ¨ä½œï¼Œä½†ä¸»æµåŸºäºTransformerçš„æ–¹æ³•è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚æœ€è¿‘æå‡ºçš„Mambaæ¨¡å‹å¯¹é•¿åºåˆ—å»ºæ¨¡æœ‰æ•ˆï¼Œä½†ç›´æ¥åº”ç”¨äºFSARæ—¶å¿½ç•¥äº†å±€éƒ¨ç‰¹å¾å»ºæ¨¡å’Œå¯¹é½çš„é‡è¦æ€§ã€‚åŒä¸€ç±»åˆ«å†…çš„é•¿å­åºåˆ—ä¼šç§¯ç´¯ç±»å†…æ–¹å·®ï¼Œå½±å“FSARæ€§èƒ½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†Matryoshka Mambaå’Œå¯¹æ¯”å­¦ä¹ æ¡†æ¶Mantaã€‚Matryoshka Mambaé€šè¿‡å¤šä¸ªå†…éƒ¨æ¨¡å—å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç›´æ¥å»ºæ¨¡å…¨å±€ç‰¹å¾ã€‚å¤–éƒ¨æ¨¡å—æ•è·è¿™äº›å±€éƒ¨ç‰¹å¾çš„æ—¶åºä¾èµ–æ€§ï¼Œè¿›è¡Œéšå¼æ—¶é—´å¯¹é½ã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§ç»“åˆæœ‰ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•çš„æ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼ï¼Œä»¥å‡è½»ç±»å†…æ–¹å·®ç§¯ç´¯å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚Mantaåœ¨SSv2ã€Kineticsã€UCF101å’ŒHMDB51ç­‰ä¸»æµåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„ä¸€æµæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ä¸­ï¼Œé•¿è§†é¢‘å­åºåˆ—æ›´è‡ªç„¶åœ°è¡¨è¾¾å®Œæ•´åŠ¨ä½œï¼Œä½†è®¡ç®—å¤æ‚åº¦æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>Mambaæ¨¡å‹æœ‰æ•ˆå¤„ç†é•¿åºåˆ—ï¼Œä½†ç›´æ¥åº”ç”¨äºFSARæ—¶å¿½ç•¥äº†å±€éƒ¨ç‰¹å¾å»ºæ¨¡å’Œå¯¹é½ã€‚</li>
<li>åŒä¸€ç±»åˆ«å†…çš„é•¿å­åºåˆ—ä¼šç§¯ç´¯ç±»å†…æ–¹å·®ï¼Œå½±å“FSARæ€§èƒ½ã€‚</li>
<li>Matryoshka Mambaé€šè¿‡å¤šä¸ªå†…éƒ¨æ¨¡å—å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶è¿›è¡Œéšå¼æ—¶é—´å¯¹é½ã€‚</li>
<li>Mantaé‡‡ç”¨æ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼ï¼Œç»“åˆæœ‰ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•ï¼Œå‡è½»ç±»å†…æ–¹å·®ç§¯ç´¯çš„å½±å“ã€‚</li>
<li>Mantaåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°çš„ä¸€æµæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-205bf618fd373d39d44b1240914ceef5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d16c2681d6549d6322cdf09a77cba898.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b7850902a9c3912646aa3a165d6fc9b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d1d20f12b2ec1bc52f161cf0f0f664e5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d3efc2920012bc9e302b18d829345f0e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-27e490f47ca16f70e9a33df1c71fe152.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-34c6e325a9bcc51cecd7512552bf34d5.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GR-MG-Leveraging-Partially-Annotated-Data-via-Multi-Modal-Goal-Conditioned-Policy"><a href="#GR-MG-Leveraging-Partially-Annotated-Data-via-Multi-Modal-Goal-Conditioned-Policy" class="headerlink" title="GR-MG: Leveraging Partially Annotated Data via Multi-Modal   Goal-Conditioned Policy"></a>GR-MG: Leveraging Partially Annotated Data via Multi-Modal   Goal-Conditioned Policy</h2><p><strong>Authors:Peiyan Li, Hongtao Wu, Yan Huang, Chilam Cheang, Liang Wang, Tao Kong</strong></p>
<p>The robotics community has consistently aimed to achieve generalizable robot manipulation with flexible natural language instructions. One primary challenge is that obtaining robot trajectories fully annotated with both actions and texts is time-consuming and labor-intensive. However, partially-annotated data, such as human activity videos without action labels and robot trajectories without text labels, are much easier to collect. Can we leverage these data to enhance the generalization capabilities of robots? In this paper, we propose GR-MG, a novel method which supports conditioning on a text instruction and a goal image. During training, GR-MG samples goal images from trajectories and conditions on both the text and the goal image or solely on the image when text is not available. During inference, where only the text is provided, GR-MG generates the goal image via a diffusion-based image-editing model and conditions on both the text and the generated image. This approach enables GR-MG to leverage large amounts of partially-annotated data while still using languages to flexibly specify tasks. To generate accurate goal images, we propose a novel progress-guided goal image generation model which injects task progress information into the generation process. In simulation experiments, GR-MG improves the average number of tasks completed in a row of 5 from 3.35 to 4.04. In real-robot experiments, GR-MG is able to perform 58 different tasks and improves the success rate from 68.7% to 78.1% and 44.4% to 60.6% in simple and generalization settings, respectively. It also outperforms comparing baseline methods in few-shot learning of novel skills. Video demos, code, and checkpoints are available on the project page: <a target="_blank" rel="noopener" href="https://gr-mg.github.io/">https://gr-mg.github.io/</a>. </p>
<blockquote>
<p>æœºå™¨äººæŠ€æœ¯ç¤¾åŒºä¸€ç›´è‡´åŠ›äºå®ç°ä½¿ç”¨çµæ´»çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œé€šç”¨æœºå™¨äººæ“ä½œã€‚ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯ï¼Œè·å¾—åŒæ—¶å¸¦æœ‰åŠ¨ä½œå’Œæ–‡æœ¬æ³¨è§£çš„æœºå™¨äººè½¨è¿¹æ˜¯éå¸¸è€—æ—¶å’ŒåŠ³åŠ¨å¯†é›†å‹çš„ã€‚ç„¶è€Œï¼Œéƒ¨åˆ†æ ‡æ³¨çš„æ•°æ®ï¼Œå¦‚æ²¡æœ‰åŠ¨ä½œæ ‡ç­¾çš„äººç±»æ´»åŠ¨è§†é¢‘å’Œæ²¡æœ‰æ–‡æœ¬æ ‡ç­¾çš„æœºå™¨äººè½¨è¿¹ï¼Œæ›´å®¹æ˜“æ”¶é›†ã€‚æˆ‘ä»¬èƒ½å¦åˆ©ç”¨è¿™äº›æ•°æ®æ¥å¢å¼ºæœºå™¨äººçš„æ³›åŒ–èƒ½åŠ›ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•GR-MGï¼Œå®ƒæ”¯æŒåŸºäºæ–‡æœ¬æŒ‡ä»¤å’Œç›®æ ‡å›¾åƒçš„æ¡ä»¶è®¾ç½®ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒGR-MGä»è½¨è¿¹ä¸­é‡‡æ ·ç›®æ ‡å›¾åƒï¼Œå¹¶æ ¹æ®æ–‡æœ¬å’Œç›®æ ‡å›¾åƒæˆ–åœ¨æ²¡æœ‰æ–‡æœ¬æ—¶ä»…æ ¹æ®å›¾åƒè¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä»…æä¾›æ–‡æœ¬æ—¶ï¼ŒGR-MGä¼šé€šè¿‡åŸºäºæ‰©æ•£çš„å›¾åƒç¼–è¾‘æ¨¡å‹ç”Ÿæˆç›®æ ‡å›¾åƒï¼Œå¹¶æ ¹æ®æ–‡æœ¬å’Œç”Ÿæˆå›¾åƒè¿›è¡Œæ¡ä»¶è®¾ç½®ã€‚è¿™ç§æ–¹æ³•ä½¿GR-MGèƒ½å¤Ÿåˆ©ç”¨å¤§é‡éƒ¨åˆ†æ ‡æ³¨çš„æ•°æ®ï¼ŒåŒæ—¶ä½¿ç”¨è¯­è¨€çµæ´»åœ°æŒ‡å®šä»»åŠ¡ã€‚ä¸ºäº†ç”Ÿæˆå‡†ç¡®çš„ç›®æ ‡å›¾åƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¿›åº¦å¼•å¯¼ç›®æ ‡å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå°†ä»»åŠ¡è¿›åº¦ä¿¡æ¯æ³¨å…¥ç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨æ¨¡æ‹Ÿå®éªŒä¸­ï¼ŒGR-MGå°†ä¸€æ’5ä¸ªä»»åŠ¡å®Œæˆçš„å¹³å‡æ•°é‡ä»3.35æé«˜åˆ°4.04ã€‚åœ¨çœŸå®æœºå™¨äººå®éªŒä¸­ï¼ŒGR-MGèƒ½å¤Ÿæ‰§è¡Œ58ç§ä¸åŒçš„ä»»åŠ¡ï¼Œå¹¶åœ¨ç®€å•å’Œæ³›åŒ–è®¾ç½®ä¸­åˆ†åˆ«å°†æˆåŠŸç‡ä»68.7%æé«˜åˆ°78.1%å’Œä»44.4%æé«˜åˆ°60.6%ã€‚å®ƒè¿˜åœ¨å°‘é‡å­¦ä¹ æ–°æŠ€èƒ½æ–¹é¢ä¼˜äºå…¶ä»–å¯¹æ¯”æ–¹æ³•ã€‚è§†é¢‘æ¼”ç¤ºã€ä»£ç å’Œæ£€æŸ¥ç‚¹å¯åœ¨é¡¹ç›®é¡µé¢è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://gr-mg.github.io/">https://gr-mg.github.io/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.14368v2">PDF</a> 8 pages, 5 figures, RA-L</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§åä¸ºGR-MGçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨åˆ©ç”¨æ–‡æœ¬æŒ‡ä»¤å’Œå›¾åƒç›®æ ‡æ¥å®ç°æœºå™¨äººæ“ä½œçš„é€šç”¨åŒ–ã€‚æ–¹æ³•èƒ½å¤Ÿåœ¨éƒ¨åˆ†æ ‡æ³¨æ•°æ®åŸºç¡€ä¸Šå·¥ä½œï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬å’Œå›¾åƒæˆ–ä»…åˆ©ç”¨å›¾åƒè¿›è¡Œè®­ç»ƒã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå½“ä»…æä¾›æ–‡æœ¬æ—¶ï¼ŒGR-MGé€šè¿‡æ‰©æ•£å¼å›¾åƒç¼–è¾‘æ¨¡å‹ç”Ÿæˆç›®æ ‡å›¾åƒï¼Œå¹¶ç»“åˆæ–‡æœ¬å’Œç”Ÿæˆå›¾åƒè¿›è¡Œæ¡ä»¶å¤„ç†ã€‚æ­¤æ–¹æ³•æé«˜äº†ä»»åŠ¡çš„å®Œæˆç‡ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æœºå™¨äººå®éªŒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GR-MGæ–¹æ³•æ”¯æŒç»“åˆæ–‡æœ¬æŒ‡ä»¤å’Œå›¾åƒç›®æ ‡è¿›è¡Œæœºå™¨äººæ“ä½œã€‚</li>
<li>æ–¹æ³•èƒ½å¤Ÿåœ¨éƒ¨åˆ†æ ‡æ³¨æ•°æ®ä¸Šå·¥ä½œï¼Œåˆ©ç”¨æ‰©æ•£å¼å›¾åƒç¼–è¾‘æ¨¡å‹ç”Ÿæˆç›®æ ‡å›¾åƒã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿå®éªŒä¸­ï¼ŒGR-MGæé«˜äº†ä»»åŠ¡çš„å®Œæˆæ•°é‡ã€‚</li>
<li>åœ¨çœŸå®æœºå™¨äººå®éªŒä¸­ï¼ŒGR-MGæé«˜äº†ä»»åŠ¡çš„å®Œæˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³›åŒ–è®¾ç½®ä¸­ã€‚</li>
<li>GR-MGåœ¨å°‘é‡å­¦ä¹ æ–°æŠ€èƒ½æ–¹é¢ä¼˜äºå¯¹æ¯”çš„åŸºçº¿æ–¹æ³•ã€‚</li>
<li>é¡¹ç›®é¡µé¢æä¾›äº†è§†é¢‘æ¼”ç¤ºã€ä»£ç å’Œæ£€æŸ¥ç‚¹ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç»“åˆæ–‡æœ¬å’Œå›¾åƒï¼Œæé«˜äº†æœºå™¨äººçš„é€šç”¨æ“ä½œèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.14368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-858219dfb1ff123d71a65ddee921098f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-994655b8b265cc1e3e7a6b8f1ca0a723.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e307d898355c2304fbe13ce4a6b32bff.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-91908f912c8ff81de3ca2087ebdf4170.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-fb6a9bcadf96155d981b56d53cf18d75.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4c5f31e90688e29197cc2ad238e5236f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-25/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-724f134c00e0aea7909b0b554903f24f.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  VarAD Lightweight High-Resolution Image Anomaly Detection via Visual   Autoregressive Modeling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-25/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fced7d64cd21b5b085a70d33764b6aba.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-25  Multi-Modal Grounded Planning and Efficient Replanning For Learning   Embodied Agents with A Few Examples
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">7652.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    


        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
