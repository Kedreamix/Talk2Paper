<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  STACK Adversarial Attacks on LLM Safeguard Pipelines">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-16540274011f2d1c616a2978dc5fcede.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-03-æ›´æ–°"><a href="#2025-07-03-æ›´æ–°" class="headerlink" title="2025-07-03 æ›´æ–°"></a>2025-07-03 æ›´æ–°</h1><h2 id="STACK-Adversarial-Attacks-on-LLM-Safeguard-Pipelines"><a href="#STACK-Adversarial-Attacks-on-LLM-Safeguard-Pipelines" class="headerlink" title="STACK: Adversarial Attacks on LLM Safeguard Pipelines"></a>STACK: Adversarial Attacks on LLM Safeguard Pipelines</h2><p><strong>Authors:Ian R. McKenzie, Oskar J. Hollinsworth, Tom Tseng, Xander Davies, Stephen Casper, Aaron D. Tucker, Robert Kirk, Adam Gleave</strong></p>
<p>Frontier AI developers are relying on layers of safeguards to protect against catastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus model using one such defense pipeline, and other frontier developers including Google DeepMind and OpenAI pledge to soon deploy similar defenses. However, the security of such pipelines is unclear, with limited prior work evaluating or attacking these pipelines. We address this gap by developing and red-teaming an open-source defense pipeline. First, we find that a novel few-shot-prompted input and output classifier outperforms state-of-the-art open-weight safeguard model ShieldGemma across three attacks and two datasets, reducing the attack success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second, we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on ClearHarm in a black-box attack against the few-shot-prompted classifier pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33% ASR, providing initial evidence that it is feasible to design attacks with no access to the target pipeline. We conclude by suggesting specific mitigations that developers could use to thwart staged attacks. </p>
<blockquote>
<p>å‰æ²¿AIå¼€å‘è€…ä»¬æ­£åœ¨ä¾é å¤šå±‚ä¿éšœæœºåˆ¶æ¥é˜²æ­¢AIç³»ç»Ÿçš„ç¾éš¾æ€§æ»¥ç”¨ã€‚Antropicä½¿ç”¨å…¶ä¸­ä¸€ç§é˜²å¾¡ç®¡é“ä¿æŠ¤å…¶æœ€æ–°çš„Claude 4 Opusæ¨¡å‹ï¼Œå…¶ä»–å‰æ²¿å¼€å‘è€…åŒ…æ‹¬Google DeepMindå’ŒOpenAIæ‰¿è¯ºå°†å¾ˆå¿«éƒ¨ç½²ç±»ä¼¼çš„é˜²å¾¡æªæ–½ã€‚ç„¶è€Œï¼Œè¿™äº›ç®¡é“çš„å®‰å…¨æ€§å°šä¸æ¸…æ¥šï¼Œä¹‹å‰çš„å·¥ä½œå¯¹å…¶è¯„ä¼°æˆ–æ”»å‡»éƒ½æœ‰é™ã€‚æˆ‘ä»¬é€šè¿‡å¼€å‘å’Œç»„å»ºä¸€ä¸ªå¼€æºçš„é˜²å¾¡ç®¡é“æ¥è§£å†³è¿™ä¸€ç©ºç™½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å‘ç°ä¸€ç§æ–°å‹å°‘æ ·æœ¬æç¤ºçš„è¾“å…¥å’Œè¾“å‡ºåˆ†ç±»å™¨ï¼Œåœ¨ä¸‰ç§æ”»å‡»å’Œä¸¤ä¸ªæ•°æ®é›†ä¸Šä¼˜äºæœ€æ–°çš„å¼€æºé‡é‡çº§é˜²æŠ¤æ¨¡å‹ShieldGemmaï¼Œå°†æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰é™ä½åˆ°ClearHarmç¾éš¾æ€§æ»¥ç”¨æ•°æ®é›†çš„0%ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåˆ†é˜¶æ®µæ”»å‡»ï¼ˆSTACKï¼‰ç¨‹åºï¼Œåœ¨é’ˆå¯¹å°‘æ ·æœ¬æç¤ºåˆ†ç±»å™¨ç®¡é“çš„é»‘ç›’æ”»å‡»ä¸­ï¼ŒClearHarmä¸Šçš„ASRè¾¾åˆ°äº†71%ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜å¯¹STACKè¿›è¡Œäº†è¿ç§»è®¾ç½®è¯„ä¼°ï¼Œå®ç°äº†33%çš„ASRï¼Œåˆæ­¥è¯æ˜åœ¨æ²¡æœ‰è®¿é—®ç›®æ ‡ç®¡é“çš„æƒ…å†µä¸‹è®¾è®¡æ”»å‡»æ˜¯å¯è¡Œçš„ã€‚æˆ‘ä»¬æœ€åå»ºè®®å¼€å‘è€…ä½¿ç”¨ç‰¹å®šçš„ç¼“è§£æªæ–½æ¥é˜»æ­¢åˆ†é˜¶æ®µæ”»å‡»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.24068v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ€æ–°å‰æ²¿äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å®‰å…¨é˜²æŠ¤æ­£åœ¨å—åˆ°é‡è§†ã€‚å¼€å‘äººå‘˜é‡‡ç”¨å¤šå±‚ä¿éšœæœºåˆ¶æ¥é˜²æ­¢äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç¾éš¾æ€§è¯¯ç”¨ã€‚ä¾‹å¦‚ï¼ŒAnthropicä¸ºå…¶æœ€æ–°çš„Claude 4 Opusæ¨¡å‹éƒ¨ç½²äº†é˜²å¾¡ç®¡é“ï¼Œå…¶ä»–å‰æ²¿å¼€å‘è€…å¦‚Google DeepMindå’ŒOpenAIä¹Ÿæ‰¿è¯ºå°†å¾ˆå¿«æ¨å‡ºç±»ä¼¼çš„é˜²æŠ¤æªæ–½ã€‚é’ˆå¯¹æ­¤ç±»ç®¡é“çš„å®‰å…¨æ€§å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªå¼€æºçš„é˜²å¾¡ç®¡é“å¹¶è¿›è¡Œçº¢é˜Ÿæµ‹è¯•ã€‚ç ”ç©¶æ˜¾ç¤ºï¼Œä¸€ç§æ–°å‹çš„åŸºäºfew-shotæç¤ºçš„è¾“å…¥è¾“å‡ºåˆ†ç±»å™¨åœ¨ä¸‰ç§æ”»å‡»å’Œä¸¤ç§æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¼€æ”¾æƒé‡å®‰å…¨ä¿æŠ¤æ¨¡å‹ShieldGemmaï¼Œèƒ½å°†ç¾éš¾æ€§è¯¯ç”¨æ•°æ®é›†ClearHarmä¸Šçš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰é™è‡³0%ã€‚ç„¶è€Œï¼Œç ”ç©¶å›¢é˜Ÿä¹Ÿè®¾è®¡äº†ä¸€ç§åˆ†é˜¶æ®µæ”»å‡»ï¼ˆSTACKï¼‰ç¨‹åºï¼Œèƒ½åœ¨ä¸äº†è§£ç›®æ ‡ç®¡é“çš„æƒ…å†µä¸‹å¯¹å…¶è¿›è¡Œæ”»å‡»ï¼Œå¹¶åœ¨ClearHarmä¸Šå®ç°71%çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚æœ¬æ–‡è¿˜ä¸ºå¼€å‘è€…æå‡ºäº†åº”å¯¹åˆ†é˜¶æ®µæ”»å‡»çš„ç¼“è§£æªæ–½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‰æ²¿äººå·¥æ™ºèƒ½å¼€å‘è€…æ­£ä½¿ç”¨å¤šå±‚ä¿éšœæœºåˆ¶æ¥é˜²æ­¢AIç³»ç»Ÿçš„ç¾éš¾æ€§è¯¯ç”¨ã€‚</li>
<li>ä¸€ç§æ–°å‹çš„åŸºäºfew-shotæç¤ºçš„è¾“å…¥è¾“å‡ºåˆ†ç±»å™¨è¡¨ç°å‡ºä¼˜å¼‚çš„é˜²æŠ¤æ•ˆæœï¼Œèƒ½å°†æ”»å‡»æˆåŠŸç‡é™è‡³0%ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªå¼€æºçš„é˜²å¾¡ç®¡é“æ¥è¯„ä¼°AIç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç§åˆ†é˜¶æ®µæ”»å‡»ï¼ˆSTACKï¼‰ç¨‹åºï¼Œèƒ½åœ¨ä¸äº†è§£ç›®æ ‡ç®¡é“çš„æƒ…å†µä¸‹è¿›è¡Œæ”»å‡»ã€‚</li>
<li>STACKç¨‹åºåœ¨ClearHarmä¸Šçš„æ”»å‡»æˆåŠŸç‡è¾¾åˆ°71%ï¼Œå¹¶æä¾›åˆæ­¥è¯æ®è¡¨æ˜åœ¨æ— ç›®æ ‡ç®¡é“è®¿é—®çš„æƒ…å†µä¸‹è®¾è®¡æ”»å‡»æ˜¯å¯è¡Œçš„ã€‚</li>
<li>ç ”ç©¶ç»“æœæŒ‡å‡ºäº†AIç³»ç»Ÿå­˜åœ¨çš„å®‰å…¨éšæ‚£å’Œæ¼æ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.24068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e819a50aa980fba34bb8d84be3eb627f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a96cf801f1b53631c23415bb6054c08c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-032c9a747904fab321be5e1121e1800d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5ec741c3283e7ef86dc1dd13e3abd29.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MadCLIP-Few-shot-Medical-Anomaly-Detection-with-CLIP"><a href="#MadCLIP-Few-shot-Medical-Anomaly-Detection-with-CLIP" class="headerlink" title="MadCLIP: Few-shot Medical Anomaly Detection with CLIP"></a>MadCLIP: Few-shot Medical Anomaly Detection with CLIP</h2><p><strong>Authors:Mahshid Shiri, Cigdem Beyan, Vittorio Murino</strong></p>
<p>An innovative few-shot anomaly detection approach is presented, leveraging the pre-trained CLIP model for medical data, and adapting it for both image-level anomaly classification (AC) and pixel-level anomaly segmentation (AS). A dual-branch design is proposed to separately capture normal and abnormal features through learnable adapters in the CLIP vision encoder. To improve semantic alignment, learnable text prompts are employed to link visual features. Furthermore, SigLIP loss is applied to effectively handle the many-to-one relationship between images and unpaired text prompts, showcasing its adaptation in the medical field for the first time. Our approach is validated on multiple modalities, demonstrating superior performance over existing methods for AC and AS, in both same-dataset and cross-dataset evaluations. Unlike prior work, it does not rely on synthetic data or memory banks, and an ablation study confirms the contribution of each component. The code is available at <a target="_blank" rel="noopener" href="https://github.com/mahshid1998/MadCLIP">https://github.com/mahshid1998/MadCLIP</a>. </p>
<blockquote>
<p>ä»‹ç»äº†ä¸€ç§åˆ›æ–°çš„åŸºäºå°‘é‡æ ·æœ¬çš„å¼‚å¸¸æ£€æµ‹æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåˆ©ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹å¤„ç†åŒ»ç–—æ•°æ®ï¼Œå¹¶é€‚åº”äºå›¾åƒçº§åˆ«çš„å¼‚å¸¸åˆ†ç±»ï¼ˆACï¼‰å’Œåƒç´ çº§åˆ«çš„å¼‚å¸¸åˆ†å‰²ï¼ˆASï¼‰ã€‚æå‡ºäº†ä¸€ç§åŒåˆ†æ”¯è®¾è®¡ï¼Œé€šè¿‡CLIPè§†è§‰ç¼–ç å™¨çš„å¯å­¦ä¹ é€‚é…å™¨åˆ†åˆ«æ•è·æ­£å¸¸å’Œå¼‚å¸¸ç‰¹å¾ã€‚ä¸ºäº†æ”¹å–„è¯­ä¹‰å¯¹é½ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„æ–‡æœ¬æç¤ºæ¥é“¾æ¥è§†è§‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œåº”ç”¨SigLIPæŸå¤±æ¥æœ‰æ•ˆå¤„ç†å›¾åƒå’Œæœªé…å¯¹çš„æ–‡æœ¬æç¤ºä¹‹é—´çš„å¤šå¯¹ä¸€å…³ç³»ï¼Œé¦–æ¬¡å±•ç¤ºäº†å…¶åœ¨åŒ»ç–—é¢†åŸŸçš„é€‚åº”æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæ¨¡æ€ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œåœ¨ç›¸åŒæ•°æ®é›†å’Œè·¨æ•°æ®é›†è¯„ä¼°ä¸­ï¼Œç›¸è¾ƒäºç°æœ‰çš„ACå’ŒASæ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¸ä¹‹å‰çš„å·¥ä½œä¸åŒï¼Œå®ƒä¸ä¾èµ–äºåˆæˆæ•°æ®æˆ–å†…å­˜é“¶è¡Œï¼Œå¹¶ä¸”é€šè¿‡æ¶ˆèç ”ç©¶è¯å®äº†æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/mahshid1998/MadCLIP%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mahshid1998/MadCLIPè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23810v1">PDF</a> Accepted to MICCAI 2025 (this version is not peer-reviewed; it is the   submitted version). MICCAI proceedings DOI will appear here</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§åˆ›æ–°çš„åŸºäºå°‘é‡æ ·æœ¬çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•è¢«æå‡ºï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹å¤„ç†åŒ»ç–—æ•°æ®ï¼Œå¹¶é€‚åº”äºå›¾åƒçº§å¼‚å¸¸åˆ†ç±»å’Œåƒç´ çº§å¼‚å¸¸åˆ†å‰²ã€‚è¯¥æ–¹æ³•é€šè¿‡CLIPè§†è§‰ç¼–ç å™¨çš„å¯å­¦ä¹ é€‚é…å™¨ï¼Œé‡‡ç”¨åŒåˆ†æ”¯è®¾è®¡æ¥åˆ†åˆ«æ•æ‰æ­£å¸¸å’Œå¼‚å¸¸ç‰¹å¾ã€‚ä¸ºäº†æ”¹è¿›è¯­ä¹‰å¯¹é½ï¼Œä½¿ç”¨äº†å¯å­¦ä¹ çš„æ–‡æœ¬æç¤ºæ¥é“¾æ¥è§†è§‰ç‰¹å¾ã€‚æ­¤å¤–ï¼Œé¦–æ¬¡åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨SigLIPæŸå¤±ï¼Œä»¥æœ‰æ•ˆå¤„ç†å›¾åƒå’Œæœªé…å¯¹æ–‡æœ¬æç¤ºä¹‹é—´çš„å¤šå¯¹ä¸€å…³ç³»ã€‚è¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€éªŒè¯ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œåœ¨ç›¸åŒæ•°æ®é›†å’Œè·¨æ•°æ®é›†è¯„ä¼°ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä¸å…ˆå‰å·¥ä½œä¸åŒï¼Œå®ƒä¸éœ€è¦ä¾èµ–åˆæˆæ•°æ®æˆ–å†…å­˜åº“ï¼Œå¹¶ä¸”é€šè¿‡æ¶ˆèç ”ç©¶è¯å®äº†æ¯ä¸ªç»„ä»¶çš„è´¡çŒ®ã€‚ä»£ç å·²å…¬å¼€äº <a target="_blank" rel="noopener" href="https://github.com/mahshid1998/MadCLIP%E3%80%82">https://github.com/mahshid1998/MadCLIPã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹è¿›è¡ŒåŒ»ç–—æ•°æ®çš„å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>é€‚åº”äºå›¾åƒçº§å¼‚å¸¸åˆ†ç±»å’Œåƒç´ çº§å¼‚å¸¸åˆ†å‰²ã€‚</li>
<li>é‡‡ç”¨åŒåˆ†æ”¯è®¾è®¡ï¼Œé€šè¿‡å¯å­¦ä¹ é€‚é…å™¨æ•æ‰æ­£å¸¸å’Œå¼‚å¸¸ç‰¹å¾ã€‚</li>
<li>ä½¿ç”¨å¯å­¦ä¹ æ–‡æœ¬æç¤ºæ”¹è¿›è¯­ä¹‰å¯¹é½ã€‚</li>
<li>åº”ç”¨SigLIPæŸå¤±å¤„ç†å›¾åƒå’Œæœªé…å¯¹æ–‡æœ¬æç¤ºä¹‹é—´çš„å¤šå¯¹ä¸€å…³ç³»ã€‚</li>
<li>åœ¨å¤šæ¨¡æ€éªŒè¯ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffb09bfba3bab440c1483d1e99b0d0c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f5eb91e7de82e8d365d0dc2b7da4710.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Visual-Textualization-for-Image-Prompted-Object-Detection"><a href="#Visual-Textualization-for-Image-Prompted-Object-Detection" class="headerlink" title="Visual Textualization for Image Prompted Object Detection"></a>Visual Textualization for Image Prompted Object Detection</h2><p><strong>Authors:Yongjian Wu, Yang Zhou, Jiya Saiyin, Bingzheng Wei, Yan Xu</strong></p>
<p>We propose VisTex-OVLM, a novel image prompted object detection method that introduces visual textualization â€“ a process that projects a few visual exemplars into the text feature space to enhance Object-level Vision-Language Modelsâ€™ (OVLMs) capability in detecting rare categories that are difficult to describe textually and nearly absent from their pre-training data, while preserving their pre-trained object-text alignment. Specifically, VisTex-OVLM leverages multi-scale textualizing blocks and a multi-stage fusion strategy to integrate visual information from visual exemplars, generating textualized visual tokens that effectively guide OVLMs alongside text prompts. Unlike previous methods, our method maintains the original architecture of OVLM, maintaining its generalization capabilities while enhancing performance in few-shot settings. VisTex-OVLM demonstrates superior performance across open-set datasets which have minimal overlap with OVLMâ€™s pre-training data and achieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/WitGotFlg/VisTex-OVLM">https://github.com/WitGotFlg/VisTex-OVLM</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†VisTex-OVLMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å›¾åƒæç¤ºç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œå®ƒå¼•å…¥äº†è§†è§‰æ–‡æœ¬åŒ–â€”â€”ä¸€ç§å°†å°‘é‡è§†è§‰æ ·æœ¬æŠ•å½±åˆ°æ–‡æœ¬ç‰¹å¾ç©ºé—´çš„è¿‡ç¨‹ï¼Œä»¥å¢å¼ºç›®æ ‡çº§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆOVLMï¼‰åœ¨æ£€æµ‹æ–‡æœ¬æè¿°å›°éš¾ä¸”å‡ ä¹ä¸å‡ºç°åœ¨å…¶é¢„è®­ç»ƒæ•°æ®ä¸­çš„ç¨€æœ‰ç±»åˆ«æ—¶çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶é¢„è®­ç»ƒçš„å¯¹è±¡æ–‡æœ¬å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼ŒVisTex-OVLMåˆ©ç”¨å¤šå°ºåº¦æ–‡æœ¬åŒ–å—å’Œå¤šé˜¶æ®µèåˆç­–ç•¥æ¥æ•´åˆè§†è§‰æ ·æœ¬ä¸­çš„è§†è§‰ä¿¡æ¯ï¼Œç”Ÿæˆæ–‡æœ¬åŒ–è§†è§‰ä»¤ç‰Œï¼Œè¿™äº›ä»¤ç‰Œå¯ä»¥æœ‰æ•ˆåœ°å¼•å¯¼OVLMä¸æ–‡æœ¬æç¤ºä¸€èµ·å·¥ä½œã€‚ä¸ä»¥å‰çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿æŒäº†OVLMçš„åŸå§‹æ¶æ„ï¼Œä¿æŒäº†å…¶æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶æé«˜äº†å°æ ·æœ¬åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚VisTex-OVLMåœ¨å¼€æ”¾æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¿™äº›æ•°æ®é›†ä¸OVLMçš„é¢„è®­ç»ƒæ•°æ®é‡å æœ€å°‘ï¼Œå¹¶ä¸”åœ¨PASCAL VOCå’ŒMSCOCOç­‰å°æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/WitGotFlg/VisTex-OVLM%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/WitGotFlg/VisTex-OVLMå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23785v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>VisTex-OVLMæ˜¯ä¸€ç§æ–°å‹å›¾åƒæç¤ºç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡è§†è§‰æ–‡æœ¬åŒ–è¿‡ç¨‹æå‡Object-level Vision-Language Modelsï¼ˆOVLMsï¼‰å¯¹éš¾ä»¥æ–‡æœ¬æè¿°ä¸”é¢„è®­ç»ƒæ•°æ®å‡ ä¹ç¼ºå¤±çš„ç½•è§ç±»åˆ«çš„æ£€æµ‹èƒ½åŠ›ã€‚VisTex-OVLMé€šè¿‡å¤šå°ºåº¦æ–‡æœ¬åŒ–å—å’Œå¤šé˜¶æ®µèåˆç­–ç•¥ï¼Œå°†è§†è§‰å®ä¾‹çš„è§†è§‰ä¿¡æ¯é›†æˆåˆ°æ–‡æœ¬ç‰¹å¾ç©ºé—´ä¸­ï¼Œç”Ÿæˆæ–‡æœ¬åŒ–è§†è§‰ä»¤ç‰Œï¼Œæœ‰æ•ˆå¼•å¯¼OVLMsä¸æ–‡æœ¬æç¤ºååŒå·¥ä½œã€‚è¯¥æ–¹æ³•ä¿æŒäº†OVLMçš„åŸå§‹æ¶æ„ï¼Œåœ¨æå‡å°‘æ ·æœ¬åœºæ™¯æ€§èƒ½çš„åŒæ—¶ç»´æŒäº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚VisTex-OVLMåœ¨å¼€æ”¾æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯ä¸é¢„è®­ç»ƒæ•°æ®é‡å è¾ƒå°‘çš„æ•°æ®é›†ä¸Šï¼Œå¹¶åœ¨PASCAL VOCå’ŒMSCOCOå°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisTex-OVLMæ˜¯ä¸€ç§æ–°çš„å›¾åƒæç¤ºç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œå®ƒé€šè¿‡è§†è§‰æ–‡æœ¬åŒ–å¢å¼ºäº†OVLMså¯¹ç½•è§ç±»åˆ«çš„æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>VisTex-OVLMå°†è§†è§‰å®ä¾‹çš„è§†è§‰ä¿¡æ¯æŠ•å½±åˆ°æ–‡æœ¬ç‰¹å¾ç©ºé—´ï¼Œç”Ÿæˆæ–‡æœ¬åŒ–çš„è§†è§‰ä»¤ç‰Œã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨å¤šå°ºåº¦æ–‡æœ¬åŒ–å—å’Œå¤šé˜¶æ®µèåˆç­–ç•¥æ¥é›†æˆè§†è§‰ä¿¡æ¯ã€‚</li>
<li>VisTex-OVLMä¿ç•™äº†OVLMçš„åŸå§‹æ¶æ„ï¼ŒåŒæ—¶æé«˜äº†å°‘æ ·æœ¬åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>VisTex-OVLMåœ¨å¼€æ”¾æ•°æ®é›†ä¸Šçš„è¡¨ç°çªå‡ºï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸é¢„è®­ç»ƒæ•°æ®é‡å è¾ƒå°‘çš„æ•°æ®é›†ä¸Šã€‚</li>
<li>VisTex-OVLMåœ¨PASCAL VOCå’ŒMSCOCOå°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-005f31e18c029e46a2deb180649bc941.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4ba9b2ce8b0dad5672ccb509571a48a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9afb17e02ef94a3b1c21b9920b1f167d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04a11874a0698c5c60f93faa6f77ccf0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AI-Generated-Lecture-Slides-for-Improving-Slide-Element-Detection-and-Retrieval"><a href="#AI-Generated-Lecture-Slides-for-Improving-Slide-Element-Detection-and-Retrieval" class="headerlink" title="AI-Generated Lecture Slides for Improving Slide Element Detection and   Retrieval"></a>AI-Generated Lecture Slides for Improving Slide Element Detection and   Retrieval</h2><p><strong>Authors:Suyash Maniyar, Vishvesh Trivedi, Ajoy Mondal, Anand Mishra, C. V. Jawahar</strong></p>
<p>Lecture slide element detection and retrieval are key problems in slide understanding. Training effective models for these tasks often depends on extensive manual annotation. However, annotating large volumes of lecture slides for supervised training is labor intensive and requires domain expertise. To address this, we propose a large language model (LLM)-guided synthetic lecture slide generation pipeline, SynLecSlideGen, which produces high-quality, coherent and realistic slides. We also create an evaluation benchmark, namely RealSlide by manually annotating 1,050 real lecture slides. To assess the utility of our synthetic slides, we perform few-shot transfer learning on real data using models pre-trained on them. Experimental results show that few-shot transfer learning with pretraining on synthetic slides significantly improves performance compared to training only on real data. This demonstrates that synthetic data can effectively compensate for limited labeled lecture slides. The code and resources of our work are publicly available on our project website: <a target="_blank" rel="noopener" href="https://synslidegen.github.io/">https://synslidegen.github.io/</a>. </p>
<blockquote>
<p>è®²åº§å¹»ç¯ç‰‡å…ƒç´ æ£€æµ‹å’Œæ£€ç´¢æ˜¯å¹»ç¯ç‰‡ç†è§£ä¸­çš„å…³é”®é—®é¢˜ã€‚é’ˆå¯¹è¿™äº›ä»»åŠ¡è®­ç»ƒæœ‰æ•ˆæ¨¡å‹é€šå¸¸ä¾èµ–äºå¤§é‡çš„äººå·¥æ ‡æ³¨ã€‚ç„¶è€Œï¼Œä¸ºç›‘ç£è®­ç»ƒæ ‡æ³¨å¤§é‡è®²åº§å¹»ç¯ç‰‡æ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ï¼Œéœ€è¦é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•å¯¼çš„åˆæˆè®²åº§å¹»ç¯ç‰‡ç”Ÿæˆç®¡é“ï¼ˆSynLecSlideGenï¼‰ï¼Œç”Ÿæˆé«˜è´¨é‡ã€è¿è´¯ä¸”é€¼çœŸçš„å¹»ç¯ç‰‡ã€‚æˆ‘ä»¬è¿˜é€šè¿‡æ‰‹åŠ¨æ ‡æ³¨1050å¼ çœŸå®è®²åº§å¹»ç¯ç‰‡åˆ›å»ºäº†ä¸€ä¸ªè¯„ä¼°åŸºå‡†ï¼Œå³RealSlideã€‚ä¸ºäº†è¯„ä¼°åˆæˆå¹»ç¯ç‰‡çš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬åœ¨çœŸå®æ•°æ®ä¸Šæ‰§è¡Œäº†åŸºäºå°‘é‡æ ·æœ¬çš„è¿ç§»å­¦ä¹ ï¼Œå¹¶ä½¿ç”¨åœ¨åˆæˆå¹»ç¯ç‰‡ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ä»…åœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒç›¸æ¯”ï¼Œåœ¨åˆæˆå¹»ç¯ç‰‡ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„å°‘é‡æ ·æœ¬è¿ç§»å­¦ä¹ å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚è¿™è¯æ˜åˆæˆæ•°æ®å¯ä»¥æœ‰æ•ˆåœ°å¼¥è¡¥æ ‡è®°è®²åº§å¹»ç¯ç‰‡çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œä»£ç å’Œèµ„æºå¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™ä¸Šå…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://synslidegen.github.io/">https://synslidegen.github.io/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23605v1">PDF</a> 40 pages including supplementary, accepted at ICDAR 2025</p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒç”¨äºå¹»ç¯ç‰‡ç†è§£çš„å…³é”®ä»»åŠ¡â€”â€”è®²åº§å¹»ç¯ç‰‡å…ƒç´ æ£€æµ‹å’Œæ£€ç´¢çš„æœ‰æ•ˆæ¨¡å‹ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„æ‰‹åŠ¨æ ‡æ³¨æ•°æ®ã€‚ä¸ºè§£å†³æ ‡æ³¨åŠ³åŠ¨å¯†é›†ä¸”éœ€é¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹(LLM)å¼•å¯¼çš„è®²åº§å¹»ç¯ç‰‡ç”Ÿæˆæµæ°´çº¿ï¼Œå‘½åä¸ºSynLecSlideGenï¼Œå®ƒèƒ½ç”Ÿæˆé«˜è´¨é‡ã€è¿è´¯ä¸”é€¼çœŸçš„å¹»ç¯ç‰‡ã€‚æˆ‘ä»¬è¿˜é€šè¿‡æ‰‹åŠ¨æ ‡æ³¨1050å¼ çœŸå®è®²åº§å¹»ç¯ç‰‡ï¼Œåˆ›å»ºäº†ä¸€ä¸ªè¯„ä¼°åŸºå‡†ï¼Œå³RealSlideã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨çœŸå®æ•°æ®ä¸Šè¿›è¡Œå‡ æ¬¡è¿ç§»å­¦ä¹ ï¼Œå¹¶åœ¨åˆæˆå¹»ç¯ç‰‡ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚è¿™è¯æ˜äº†åˆæˆæ•°æ®å¯ä»¥æœ‰æ•ˆåœ°å¼¥è¡¥æœ‰é™çš„æ ‡è®°è®²åº§å¹»ç¯ç‰‡ã€‚æˆ‘ä»¬çš„å·¥ä½œå’Œèµ„æºä»£ç å¯åœ¨é¡¹ç›®ç½‘ç«™ä¸Šå…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://synslidegen.github.io/%E3%80%82">https://synslidegen.github.io/ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®²åº§å¹»ç¯ç‰‡å…ƒç´ æ£€æµ‹å’Œæ£€ç´¢æ˜¯å¹»ç¯ç‰‡ç†è§£ä¸­çš„å…³é”®é—®é¢˜ã€‚</li>
<li>è®­ç»ƒè¿™äº›ä»»åŠ¡çš„æœ‰æ•ˆæ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„æ‰‹åŠ¨æ ‡æ³¨æ•°æ®ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ³åŠ¨å¯†é›†ä¸”éœ€è¦é¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„è¿‡ç¨‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºSynLecSlideGençš„å¤§å‹è¯­è¨€æ¨¡å‹å¼•å¯¼çš„è®²åº§å¹»ç¯ç‰‡ç”Ÿæˆæµæ°´çº¿ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€è¿è´¯ä¸”é€¼çœŸçš„å¹»ç¯ç‰‡ã€‚</li>
<li>åˆ›å»ºäº†ä¸€ä¸ªè¯„ä¼°åŸºå‡†RealSlideï¼Œé€šè¿‡æ‰‹åŠ¨æ ‡æ³¨1050å¼ çœŸå®è®²åº§å¹»ç¯ç‰‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åˆæˆå¹»ç¯ç‰‡ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åè¿›è¡Œå‡ æ¬¡è¿ç§»å­¦ä¹ ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>åˆæˆæ•°æ®å¯ä»¥æœ‰æ•ˆåœ°å¼¥è¡¥æœ‰é™çš„æ ‡è®°è®²åº§å¹»ç¯ç‰‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6c3aa5abeebd4ff59246754276e6f730.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0f90bf235e5684fa2fa13f086e485c8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AdFair-CLIP-Adversarial-Fair-Contrastive-Language-Image-Pre-training-for-Chest-X-rays"><a href="#AdFair-CLIP-Adversarial-Fair-Contrastive-Language-Image-Pre-training-for-Chest-X-rays" class="headerlink" title="AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training   for Chest X-rays"></a>AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training   for Chest X-rays</h2><p><strong>Authors:Chenlang Yi, Zizhan Xiong, Qi Qi, Xiyuan Wei, Girish Bathla, Ching-Long Lin, Bobak Jack Mortazavi, Tianbao Yang</strong></p>
<p>Contrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive experiments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, particularly for CXR analysis. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹åœ¨åŒ…æ‹¬åŒ»å­¦å›¾åƒåˆ†ç±»åœ¨å†…çš„å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…³äºCLIPæ¨¡å‹çš„å…¬å¹³æ€§é—®é¢˜ï¼ŒåŒ…æ‹¬äººå£ç»Ÿè®¡åè§ï¼Œå¹¶æœªå—åˆ°è¶³å¤Ÿçš„é‡è§†ã€‚è¿™ç§ç–å¿½å¯¼è‡´äº†å…³é”®é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä¸ç§æ—å’Œæ€§åˆ«æœ‰å…³çš„é—®é¢˜ï¼Œè¿›è€Œå¯¼è‡´è¯Šæ–­ç»“æœçš„ä¸å…¬å¹³å’Œå¯¹ä»£è¡¨æ€§ä¸è¶³çš„ç¾¤ä½“çš„å¯é æ€§é™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AdFair-CLIPï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨å¯¹æŠ—æ€§ç‰¹å¾å¹²é¢„æ¥æŠ‘åˆ¶æ•æ„Ÿå±æ€§çš„æ–°å‹æ¡†æ¶ï¼Œä»è€Œå‡è½»è™šå‡å…³è”ï¼Œæé«˜é¢„æµ‹å…¬å¹³æ€§ã€‚æˆ‘ä»¬åœ¨èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œç»“æœè¡¨æ˜AdFair-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸­ï¼Œæ˜¾è‘—æé«˜äº†å…¬å¹³æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœä¸ºCLIPåŸºåŒ»å­¦è¯Šæ–­æ¨¡å‹ä¸­çš„å…¬å¹³æ„è¯†å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯CXRåˆ†æï¼Œæ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23467v1">PDF</a> This preprint has been accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>CLIPæ¨¡å‹åœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬åŒ»å­¦å½±åƒåˆ†ç±»ã€‚ç„¶è€Œï¼Œé’ˆå¯¹CLIPæ¨¡å‹çš„å…¬å¹³æ€§å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯ç§æ—å’Œæ€§åˆ«ç›¸å…³çš„é—®é¢˜å´è¢«å¿½è§†ã€‚è¿™å¯èƒ½å¯¼è‡´è¯Šæ–­ç»“æœçš„ä¸å…¬å¹³å’Œå¯¹å°‘æ•°ç¾¤ä½“çš„è¯Šæ–­å¯é æ€§é™ä½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºAdFair-CLIPæ¡†æ¶ï¼Œåˆ©ç”¨å¯¹æŠ—ç‰¹å¾å¹²é¢„æŠ‘åˆ¶æ•æ„Ÿå±æ€§ï¼Œä»¥å‡å°‘å¶ç„¶è”ç³»å¹¶æå‡é¢„æµ‹å…¬å¹³æ€§ã€‚åœ¨èƒ¸éƒ¨Xå…‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAdFair-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹æ˜¾è‘—æé«˜äº†å…¬å¹³æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸ºCLIPåŸºç¡€çš„åŒ»å­¦å½±åƒè¯Šæ–­æ¨¡å‹çš„å…¬å¹³æ€§æ„è¯†å­¦ä¹ æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹åœ¨åŒ»å­¦å½±åƒåˆ†ç±»ç­‰è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>é’ˆå¯¹CLIPæ¨¡å‹çš„å…¬å¹³æ€§å…³æ³¨è¢«å¿½è§†ï¼Œå¯èƒ½å¯¼è‡´è¯Šæ–­ç»“æœçš„ä¸å…¬å¹³å’Œå¯¹å°‘æ•°ç¾¤ä½“çš„è¯Šæ–­å¯é æ€§é™ä½ã€‚</li>
<li>AdFair-CLIPæ¡†æ¶é€šè¿‡é‡‡ç”¨å¯¹æŠ—ç‰¹å¾å¹²é¢„æ¥æŠ‘åˆ¶æ•æ„Ÿå±æ€§ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>åœ¨èƒ¸éƒ¨Xå…‰æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒAdFair-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹èƒ½æ˜¾è‘—æé«˜å…¬å¹³æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>AdFair-CLIPæ¡†æ¶èƒ½ç»´æŒè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºCLIPåŸºç¡€çš„åŒ»å­¦å½±åƒè¯Šæ–­æ¨¡å‹çš„å…¬å¹³æ€§æ„è¯†å­¦ä¹ æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a694931f373977ed867e26a7a91742c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3b099463c7072cddecb002adc2e6032.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a56d0f31bb5da36140ab15e2ebae7779.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Exposing-and-Mitigating-Calibration-Biases-and-Demographic-Unfairness-in-MLLM-Few-Shot-In-Context-Learning-for-Medical-Image-Classification"><a href="#Exposing-and-Mitigating-Calibration-Biases-and-Demographic-Unfairness-in-MLLM-Few-Shot-In-Context-Learning-for-Medical-Image-Classification" class="headerlink" title="Exposing and Mitigating Calibration Biases and Demographic Unfairness in   MLLM Few-Shot In-Context Learning for Medical Image Classification"></a>Exposing and Mitigating Calibration Biases and Demographic Unfairness in   MLLM Few-Shot In-Context Learning for Medical Image Classification</h2><p><strong>Authors:Xing Shen, Justin Szeto, Mingyang Li, Hengguan Huang, Tal Arbel</strong></p>
<p>Multimodal large language models (MLLMs) have enormous potential to perform few-shot in-context learning in the context of medical image analysis. However, safe deployment of these models into real-world clinical practice requires an in-depth analysis of the accuracies of their predictions, and their associated calibration errors, particularly across different demographic subgroups. In this work, we present the first investigation into the calibration biases and demographic unfairness of MLLMsâ€™ predictions and confidence scores in few-shot in-context learning for medical image classification. We introduce CALIN, an inference-time calibration method designed to mitigate the associated biases. Specifically, CALIN estimates the amount of calibration needed, represented by calibration matrices, using a bi-level procedure: progressing from the population level to the subgroup level prior to inference. It then applies this estimation to calibrate the predicted confidence scores during inference. Experimental results on three medical imaging datasets: PAPILA for fundus image classification, HAM10000 for skin cancer classification, and MIMIC-CXR for chest X-ray classification demonstrate CALINâ€™s effectiveness at ensuring fair confidence calibration in its prediction, while improving its overall prediction accuracies and exhibiting minimum fairness-utility trade-off. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—å›¾åƒåˆ†æçš„æƒ…å¢ƒä¸‹æ‰§è¡Œå°‘é‡æƒ…å¢ƒå­¦ä¹ æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹å®‰å…¨éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œçš„ä¸´åºŠå®è·µéœ€è¦æ·±å…¥åˆ†æå…¶é¢„æµ‹çš„å‡†ç¡®æ€§ä»¥åŠç›¸å…³çš„æ ¡å‡†è¯¯å·®ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒçš„äººå£äºšç»„ä¹‹é—´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡è°ƒæŸ¥äº†MLLMsçš„é¢„æµ‹å’Œç½®ä¿¡åº¦å¾—åˆ†çš„æ ¡å‡†åè§å’Œäººå£ç»Ÿè®¡å­¦ä¸Šçš„ä¸å…¬å¹³ç°è±¡ï¼Œé’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†ç±»çš„å°‘é‡æƒ…å¢ƒå­¦ä¹ ã€‚æˆ‘ä»¬ä»‹ç»äº†CALINï¼Œè¿™æ˜¯ä¸€ç§æ¨ç†æ—¶é—´æ ¡å‡†æ–¹æ³•ï¼Œæ—¨åœ¨å‡è½»ç›¸å…³çš„åè§ã€‚å…·ä½“æ¥è¯´ï¼ŒCALINä½¿ç”¨ä¸¤çº§ç¨‹åºä¼°è®¡æ‰€éœ€çš„æ ¡å‡†é‡ï¼Œè¯¥æ ¡å‡†é‡ç”±æ ¡å‡†çŸ©é˜µè¡¨ç¤ºï¼šä»äººç¾¤å±‚é¢æ¨è¿›åˆ°äºšç»„å±‚é¢è¿›è¡Œæ¨ç†ã€‚ç„¶åï¼Œå®ƒåœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†è¿™ä¸ªä¼°è®¡åº”ç”¨äºæ ¡å‡†é¢„æµ‹çš„ç½®ä¿¡åº¦å¾—åˆ†ã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†CALINçš„æœ‰æ•ˆæ€§ï¼šç”¨äºçœ¼åº•å›¾åƒåˆ†ç±»çš„PAPILAæ•°æ®é›†ã€ç”¨äºçš®è‚¤ç™Œåˆ†ç±»çš„HAM10000æ•°æ®é›†ä»¥åŠç”¨äºèƒ¸éƒ¨Xå°„çº¿åˆ†ç±»çš„MIMIC-CXRæ•°æ®é›†ã€‚è¿™äº›ç»“æœè¯æ˜äº†CALINåœ¨ä¿è¯é¢„æµ‹ç½®ä¿¡åº¦æ ¡å‡†çš„å…¬æ­£æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶æé«˜äº†å…¶æ•´ä½“é¢„æµ‹å‡†ç¡®æ€§å¹¶è¡¨ç°å‡ºæœ€å°çš„å…¬å¹³æ€§ä¸å®ç”¨æ€§çš„æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23298v2">PDF</a> Preprint version. The peer-reviewed version of this paper has been   accepted to MICCAI 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†æé¢†åŸŸå…·æœ‰å·¨å¤§çš„è¿›è¡Œå°‘é‡æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹å®‰å…¨éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œçš„ä¸´åºŠå®è·µéœ€è¦æ·±å…¥åˆ†æå…¶é¢„æµ‹çš„å‡†ç¡®æ€§åŠå…¶ç›¸å…³çš„æ ¡å‡†è¯¯å·®ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒçš„äººå£äºšç¾¤ä¹‹é—´ã€‚æœ¬ç ”ç©¶é¦–æ¬¡è°ƒæŸ¥äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é¢„æµ‹å’Œä¿¡å¿ƒåˆ†æ•°åœ¨å°‘é‡æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ ¡å‡†åè§å’Œäººå£ä¸å…¬å¹³é—®é¢˜ã€‚æˆ‘ä»¬ä»‹ç»äº†CALINï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç¼“è§£ç›¸å…³åè§çš„è®¾è®¡çš„æ¨ç†æ—¶é—´æ ¡å‡†æ–¹æ³•ã€‚CALINé€šè¿‡ä¸¤çº§ç¨‹åºä¼°è®¡æ‰€éœ€çš„æ ¡å‡†é‡ï¼Œä»æ€»ä½“å±‚é¢è¿›æ­¥åˆ°å°ç»„å±‚é¢ï¼Œç„¶ååº”ç”¨äºæ¨ç†è¿‡ç¨‹ä¸­çš„é¢„æµ‹ä¿¡å¿ƒåˆ†æ•°æ ¡å‡†ã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCALINåœ¨ç¡®ä¿å…¬å¹³ä¿¡å¿ƒæ ¡å‡†çš„åŒæ—¶ï¼Œæé«˜äº†å…¶é¢„æµ‹çš„æ€»ä½“å‡†ç¡®æ€§ï¼Œå¹¶å®ç°äº†æœ€å°çš„å…¬å¹³æ•ˆç”¨æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†æä¸­å…·æœ‰è¿›è¡Œå°‘é‡æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ½œåŠ›ã€‚</li>
<li>æ¨¡å‹éƒ¨ç½²åˆ°ä¸´åºŠå®è·µéœ€è¦æ·±å…¥åˆ†æå…¶é¢„æµ‹å‡†ç¡®æ€§å’Œæ ¡å‡†è¯¯å·®ï¼Œå°¤å…¶æ˜¯ä¸åŒäººå£äºšç¾¤ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡è°ƒæŸ¥äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é¢„æµ‹å’Œä¿¡å¿ƒåˆ†æ•°åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­çš„æ ¡å‡†åè§å’Œäººå£ä¸å…¬å¹³é—®é¢˜ã€‚</li>
<li>ä»‹ç»äº†CALINæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ä¸¤çº§ç¨‹åºä¼°è®¡å¹¶åº”ç”¨æ‰€éœ€çš„æ ¡å‡†é‡ï¼Œä»¥æé«˜é¢„æµ‹çš„ä¿¡å¿ƒåˆ†æ•°æ ¡å‡†ã€‚</li>
<li>CALINæ–¹æ³•å¯æœ‰æ•ˆç¡®ä¿å…¬å¹³ä¿¡å¿ƒæ ¡å‡†ï¼Œæé«˜é¢„æµ‹çš„æ€»ä½“å‡†ç¡®æ€§ï¼Œå¹¶å®ç°æœ€å°çš„å…¬å¹³æ•ˆç”¨æƒè¡¡ã€‚</li>
<li>å®éªŒç»“æœåœ¨ä¸‰ä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸ŠéªŒè¯äº†CALINçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9e857695606482ba339ca9e05355684.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be07ec8609fee399a8ec0b106a03b8d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b8df457de984bf3851fc28313584916.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Probabilistic-Prototype-Calibration-of-Vision-Language-Models-for-Generalized-Few-shot-Semantic-Segmentation"><a href="#Probabilistic-Prototype-Calibration-of-Vision-Language-Models-for-Generalized-Few-shot-Semantic-Segmentation" class="headerlink" title="Probabilistic Prototype Calibration of Vision-Language Models for   Generalized Few-shot Semantic Segmentation"></a>Probabilistic Prototype Calibration of Vision-Language Models for   Generalized Few-shot Semantic Segmentation</h2><p><strong>Authors:Jie Liu, Jiayi Shen, Pan Zhou, Jan-Jakob Sonke, Efstratios Gavves</strong></p>
<p>Generalized Few-Shot Semantic Segmentation (GFSS) aims to extend a segmentation model to novel classes with only a few annotated examples while maintaining performance on base classes. Recently, pretrained vision-language models (VLMs) such as CLIP have been leveraged in GFSS to improve generalization on novel classes through multi-modal prototypes learning. However, existing prototype-based methods are inherently deterministic, limiting the adaptability of learned prototypes to diverse samples, particularly for novel classes with scarce annotations. To address this, we propose FewCLIP, a probabilistic prototype calibration framework over multi-modal prototypes from the pretrained CLIP, thus providing more adaptive prototype learning for GFSS. Specifically, FewCLIP first introduces a prototype calibration mechanism, which refines frozen textual prototypes with learnable visual calibration prototypes, leading to a more discriminative and adaptive representation. Furthermore, unlike deterministic prototype learning techniques, FewCLIP introduces distribution regularization over these calibration prototypes. This probabilistic formulation ensures structured and uncertainty-aware prototype learning, effectively mitigating overfitting to limited novel class data while enhancing generalization. Extensive experimental results on PASCAL-5$^i$ and COCO-20$^i$ datasets demonstrate that our proposed FewCLIP significantly outperforms state-of-the-art approaches across both GFSS and class-incremental setting. The code is available at <a target="_blank" rel="noopener" href="https://github.com/jliu4ai/FewCLIP">https://github.com/jliu4ai/FewCLIP</a>. </p>
<blockquote>
<p>å¹¿ä¹‰å°æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆGFSSï¼‰æ—¨åœ¨å°†åˆ†å‰²æ¨¡å‹æ‰©å±•åˆ°åªæœ‰å°‘é‡æ ‡æ³¨ç¤ºä¾‹çš„æ–°ç±»åˆ«ä¸Šï¼ŒåŒæ—¶ä¿æŒå¯¹åŸºç¡€ç±»åˆ«çš„æ€§èƒ½ã€‚æœ€è¿‘ï¼Œäººä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨GFSSä¸­é€šè¿‡å¤šæ¨¡æ€åŸå‹å­¦ä¹ æ¥æé«˜å¯¹æ–°ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºåŸå‹çš„æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯ç¡®å®šçš„ï¼Œé™åˆ¶äº†å­¦ä¹ åˆ°çš„åŸå‹å¯¹å¤šæ ·æ ·æœ¬çš„é€‚åº”æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ ‡æ³¨ç¨€ç¼ºçš„æ–°ç±»åˆ«ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FewCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºé¢„è®­ç»ƒCLIPçš„å¤šæ¨¡æ€åŸå‹çš„æ¦‚ç‡åŸå‹æ ¡å‡†æ¡†æ¶ï¼Œä¸ºGFSSæä¾›æ›´è‡ªé€‚åº”çš„åŸå‹å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼ŒFewCLIPé¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªåŸå‹æ ¡å‡†æœºåˆ¶ï¼Œç”¨å¯å­¦ä¹ çš„è§†è§‰æ ¡å‡†åŸå‹æ¥ç²¾ç»†è°ƒæ•´å†»ç»“çš„æ–‡æœ¬åŸå‹ï¼Œä»è€Œå¾—åˆ°æ›´å…·åŒºåˆ†æ€§å’Œè‡ªé€‚åº”çš„è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œä¸åŒäºç¡®å®šçš„åŸå‹å­¦ä¹ æŠ€æœ¯ï¼ŒFewCLIPå¯¹è¿™äº›æ ¡å‡†åŸå‹è¿›è¡Œäº†åˆ†å¸ƒæ­£åˆ™åŒ–ã€‚è¿™ç§æ¦‚ç‡å…¬å¼ç¡®ä¿ç»“æ„åŒ–ä¸”äº†è§£ä¸ç¡®å®šæ€§çš„åŸå‹å­¦ä¹ ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†å¯¹æœ‰é™æ–°ç±»åˆ«æ•°æ®çš„è¿‡åº¦æ‹Ÿåˆï¼ŒåŒæ—¶æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åœ¨PASCAL-5iå’ŒCOCO-20iæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„FewCLIPåœ¨GFSSå’Œç±»å¢é‡è®¾ç½®æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jliu4ai/FewCLIP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jliu4ai/FewCLIPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22979v1">PDF</a> ICCV2025 Proceeding</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„å¹¿ä¹‰å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆGFSSï¼‰æ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€åŸå‹å­¦ä¹ æ‰©å±•åˆ†å‰²æ¨¡å‹åœ¨å°‘é‡æ³¨é‡Šç¤ºä¾‹ä¸‹å¯¹æ–°é¢–ç±»çš„è¡¨ç°èƒ½åŠ›ã€‚ç°æœ‰åŸå‹æ–¹æ³•å›ºæœ‰ç¡®å®šæ€§é™åˆ¶äº†å…¶å¯¹æ–°ç±»å­¦ä¹ é€‚åº”æ€§ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§åŸºäºCLIPæ¦‚ç‡åŸå‹æ ¡å‡†æ¡†æ¶çš„FewCLIPæ¥è§£å†³è¯¥é—®é¢˜ã€‚å®ƒé€šè¿‡æ ¡å‡†åŸå‹å’Œè°ƒæ•´æ ¡å‡†åŸå‹çš„åˆ†å¸ƒæ¥å¢å¼ºæ¨¡å‹å¯¹æ–°ç±»çš„é€‚åº”æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFewCLIPåœ¨GFSSå’Œç±»å¢é‡è®¾ç½®ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GFSSæ—¨åœ¨æ‰©å±•åˆ†å‰²æ¨¡å‹åœ¨ä»…æœ‰å°‘é‡æ³¨é‡Šç¤ºä¾‹çš„æƒ…å†µä¸‹å¯¹æ–°é¢–ç±»çš„è¡¨ç°èƒ½åŠ›ã€‚</li>
<li>CLIPç­‰é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹è¢«ç”¨äºGFSSä¸­ä»¥æé«˜åœ¨æ–°é¢–ç±»ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡å¤šæ¨¡æ€åŸå‹å­¦ä¹ å®ç°ã€‚</li>
<li>ç°æœ‰åŸå‹æ–¹æ³•å­˜åœ¨å›ºæœ‰ç¡®å®šæ€§é—®é¢˜ï¼Œæ— æ³•é€‚åº”å¤šæ ·æ ·æœ¬ï¼Œç‰¹åˆ«æ˜¯æ–°ç±»æ ·æœ¬ç¨€ç¼ºçš„æƒ…å†µã€‚</li>
<li>FewCLIPæå‡ºæ¦‚ç‡åŸå‹æ ¡å‡†æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥åŸå‹æ ¡å‡†æœºåˆ¶å’Œåˆ†å¸ƒæ­£åˆ™åŒ–æ¥æ”¹è¿›ç°æœ‰æ–¹æ³•ã€‚</li>
<li>FewCLIPé‡‡ç”¨åŸºäºCLIPçš„å¤šæ¨¡æ€åŸå‹è¿›è¡Œæ ¡å‡†ï¼Œå¹¶å¼•å…¥å­¦ä¹ å¾—åˆ°çš„è§†è§‰æ ¡å‡†åŸå‹å¯¹å›ºå®šçš„æ–‡æœ¬åŸå‹è¿›è¡Œç»†åŒ–ã€‚</li>
<li>ä¸ç¡®å®šæ€§åŸå‹å­¦ä¹ æ–¹æ³•ä¸åŒï¼ŒFewCLIPå¼•å…¥äº†ç»“æ„åŒ–ã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„åŸå‹å­¦ä¹ ï¼Œæœ‰æ•ˆç¼“è§£äº†å¯¹æœ‰é™æ–°ç±»æ•°æ®çš„è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-16540274011f2d1c616a2978dc5fcede.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be03280edcd591322794625bc005f6f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-010686a380d48d795aa41a975f85a012.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Assessing-the-feasibility-of-Large-Language-Models-for-detecting-micro-behaviors-in-team-interactions-during-space-missions"><a href="#Assessing-the-feasibility-of-Large-Language-Models-for-detecting-micro-behaviors-in-team-interactions-during-space-missions" class="headerlink" title="Assessing the feasibility of Large Language Models for detecting   micro-behaviors in team interactions during space missions"></a>Assessing the feasibility of Large Language Models for detecting   micro-behaviors in team interactions during space missions</h2><p><strong>Authors:Ankush Raut, Projna Paromita, Sydney Begerowski, Suzanne Bell, Theodora Chaspari</strong></p>
<p>We explore the feasibility of large language models (LLMs) in detecting subtle expressions of micro-behaviors in team conversations using transcripts collected during simulated space missions. Specifically, we examine zero-shot classification, fine-tuning, and paraphrase-augmented fine-tuning with encoder-only sequence classification LLMs, as well as few-shot text generation with decoder-only causal language modeling LLMs, to predict the micro-behavior associated with each conversational turn (i.e., dialogue). Our findings indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to detect underrepresented micro-behaviors, particularly discouraging speech, even with weighted fine-tuning. In contrast, the instruction fine-tuned version of Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best models achieving macro F1-scores of 44% for 3-way classification and 68% for binary classification. These results have implications for the development of speech technologies aimed at analyzing team communication dynamics and enhancing training interventions in high-stakes environments such as space missions, particularly in scenarios where text is the only accessible data. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ©ç”¨æ¨¡æ‹Ÿå¤ªç©ºä»»åŠ¡æœŸé—´æ”¶é›†çš„è½¬å½•æœ¬æ¥æ£€æµ‹å›¢é˜Ÿå¯¹è¯ä¸­çš„å¾®å¦™å¾®è§‚è¡Œä¸ºè¡¨è¾¾çš„å¯è¡Œæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç ”ç©¶äº†é›¶æ ·æœ¬åˆ†ç±»ã€å¾®è°ƒä»¥åŠåŸºäºç¼–ç å™¨åºåˆ—åˆ†ç±»LLMçš„å¢è¯‘å¾®è°ƒï¼Œä»¥åŠåŸºäºè§£ç å™¨å› æœè¯­è¨€å»ºæ¨¡LLMçš„å°‘é‡æ–‡æœ¬ç”Ÿæˆï¼Œä»¥é¢„æµ‹ä¸æ¯ä¸ªå¯¹è¯å›åˆï¼ˆå³å¯¹è¯ï¼‰ç›¸å…³çš„å¾®è§‚è¡Œä¸ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯åŠ æƒå¾®è°ƒåï¼Œå¦‚RoBERTaå’ŒDistilBERTç­‰ç¼–ç å™¨LLMåœ¨æ£€æµ‹ä»£è¡¨æ€§ä¸è¶³çš„å¾®è§‚è¡Œä¸ºæ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨åŠé˜»æ€§è¨€è¯­æ–¹é¢ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç»è¿‡æŒ‡ä»¤å¾®è°ƒåçš„è§£ç å™¨LLM Llama-3.1è¡¨ç°ä¼˜å¼‚ï¼Œæœ€ä½³æ¨¡å‹çš„å®F1å¾—åˆ†ç‡è¾¾åˆ°äº†ä¸‰ç±»åˆ†ç±»çš„44%å’ŒäºŒå…ƒåˆ†ç±»çš„68%ã€‚è¿™äº›ç»“æœå¯¹äºå¼€å‘æ—¨åœ¨åˆ†æå›¢é˜Ÿæ²Ÿé€šåŠ¨æ€å’Œåœ¨é«˜é£é™©ç¯å¢ƒä¸­å¢å¼ºè®­ç»ƒå¹²é¢„çš„è¯­éŸ³æŠ€æœ¯å…·æœ‰é‡è¦æ„ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨åªèƒ½è®¿é—®æ–‡æœ¬æ•°æ®çš„åœºæ™¯ä¸­ï¼Œå¦‚å¤ªç©ºä»»åŠ¡ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22679v1">PDF</a> 5 pages, 4 figures. Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨¡æ‹Ÿå¤ªç©ºä»»åŠ¡ä¸­çš„å›¢é˜Ÿå¯¹è¯å¾®è¡Œä¸ºæ£€æµ‹å¯è¡Œæ€§ç ”ç©¶ã€‚æ¢è®¨äº†é›¶æ ·æœ¬åˆ†ç±»ã€å¾®è°ƒã€åŸºäºç¼–ç å™¨çš„åºåˆ—åˆ†ç±»LLMä»¥åŠåŸºäºè§£ç å™¨çš„å› æœè¯­è¨€å»ºæ¨¡LLMåœ¨é¢„æµ‹å¯¹è¯å¾®è¡Œä¸ºæ–¹é¢çš„è¡¨ç°ã€‚å‘ç°ç¼–ç å™¨LLMåœ¨æ£€æµ‹æ¬ ä»£è¡¨çš„å¾®è¡Œä¸ºæ–¹é¢è¡¨ç°ä¸ä½³ï¼Œè€ŒæŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬çš„è§£ç å™¨LLMåˆ™è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæœ€ä½³æ¨¡å‹çš„ä¸‰ç±»åˆ†ç±»å®F1åˆ†æ•°è¾¾åˆ°44%ï¼ŒäºŒå…ƒåˆ†ç±»è¾¾åˆ°68%ã€‚å¯¹å¼€å‘é’ˆå¯¹å›¢é˜Ÿæ²Ÿé€šåŠ¨åŠ›åˆ†æçš„è¯­éŸ³æŠ€æœ¯ä»¥åŠå¤ªç©ºä»»åŠ¡ç­‰é«˜å‹åŠ›ç¯å¢ƒä¸­çš„è®­ç»ƒå¹²é¢„å…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚æ–‡æœ¬ä¾§é‡äºè®­ç»ƒå’Œä½¿ç”¨LLMå¯¹æ¨¡æ‹Ÿå¤ªç©ºä»»åŠ¡å¯¹è¯å¾®è¡Œä¸ºçš„è¯†åˆ«å’Œåˆ†æèƒ½åŠ›çš„é‡è¦æ€§åŠå…¶æ½œåœ¨çš„ä¼˜åŒ–å’Œæ”¹è¿›ç©ºé—´ã€‚ç ”ç©¶ç»“æœæœ‰åŠ©äºæé«˜å›¢é˜Ÿæˆå‘˜ä¹‹é—´äº’åŠ¨çš„æœ‰æ•ˆæ€§å’Œæå‡æœªæ¥åœ¨å¤æ‚ç¯å¢ƒä¸‹ï¼ˆå¦‚å¤ªç©ºä»»åŠ¡ï¼‰çš„åˆ†æå’Œç†è§£èƒ½åŠ›ã€‚è¿™ä¸ä»…ä»…å±€é™äºæ–‡æœ¬ç†è§£æŠ€æœ¯çš„å®é™…åº”ç”¨èŒƒå›´å’Œæå‡è¿™ç±»æŠ€æœ¯åº”ç”¨çš„è´¨é‡å’Œä»·å€¼æ½œåŠ›ï¼Œè€Œä¸”è¿˜æ‹“å±•åˆ°äº†äººæœºäº¤äº’è®¾è®¡å’Œç”¨æˆ·åˆ†æç­‰æ–¹é¢çš„å‰æ™¯å’Œå‘å±•è¶‹åŠ¿ã€‚è¿™é¡¹ç ”ç©¶çš„ç»“æœå°†æœ‰åŠ©äºæé«˜è¯­éŸ³è¯†åˆ«å’Œå¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½å’Œå¯é æ€§ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆã€æ›´å‡†ç¡®çš„è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿå¼€è¾Ÿæ–°çš„é“è·¯ã€‚è¿™é¡¹ç ”ç©¶å°†å¼€å¯ä¸€ç§å…¨æ–°çš„å¯¹è¯åˆ†æå’Œé¢„æµ‹æœºåˆ¶çš„æ–°çºªå…ƒï¼Œä»è€Œå®ç°åŸºäºå¤æ‚ç³»ç»Ÿå’Œç¤¾ä¼šç°è±¡çš„ç†è®ºæ„å»ºå’ŒæŠ€æœ¯å®ç°ç›¸ç»“åˆçš„ç ”ç©¶ä»·å€¼å’Œå‘å±•å‰æ™¯ã€‚è¿™ä¸ºç ”ç©¶äººå‘˜åœ¨ç†è§£å¤æ‚çš„ç¤¾äº¤å’Œå›¢é˜Ÿåˆä½œåœºæ™¯ä¸­åº”ç”¨çš„æ·±åº¦è¯­è¨€å¤„ç†æä¾›äº†æ–°çš„è§†è§’å’Œæœºé‡ã€‚æ€»ç»“ä¸Šè¿°ç ”ç©¶å†…å®¹å’Œç»“æœï¼Œå¯ä»¥å‘ç°è¯¥ç ”ç©¶çš„é‡ç‚¹åœ¨äºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨¡æ‹Ÿå¤ªç©ºä»»åŠ¡ä¸­æ£€æµ‹å›¢é˜Ÿå¯¹è¯çš„å¾®è¡Œä¸ºè¡¨è¾¾å¯è¡Œæ€§åˆ†æï¼Œå…¶æ¢ç´¢äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶æ­ç¤ºäº†ç°æœ‰æŠ€æœ¯çš„å±€é™æ€§ï¼Œä¹Ÿæä¾›äº†å®è´µçš„æ´å¯Ÿå’Œæ”¹è¿›å»ºè®®ã€‚æ­¤ç ”ç©¶çš„æ´å¯Ÿå…·æœ‰æå…¶é‡è¦çš„æ„ä¹‰å’Œä»·å€¼æ½œåŠ›ï¼Œå¹¶ä¸ºç›¸å…³é¢†åŸŸå¼€è¾Ÿäº†æ–°çš„å‘å±•è·¯å¾„å’Œæœºé‡ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åŸºäºè§£ç å™¨çš„LLMçš„ä¼˜å¼‚æ€§èƒ½ä»¥åŠå…¶åœ¨æœªæ¥è®­ç»ƒå’Œæ”¹è¿›æ–¹é¢çš„æ½œåŠ›ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶ä¹ŸæŒ‡å‡ºäº†åœ¨ç‰¹å®šåœºæ™¯ä¸‹ï¼ˆå¦‚é«˜å‹åŠ›ç¯å¢ƒï¼‰çš„é€‚ç”¨æ€§æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚è¿™ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å®è´µçš„å¯ç¤ºå’ŒæŒ‡å¼•æ–¹å‘ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶è¿˜å¼ºè°ƒäº†å…¶åœ¨å›¢é˜Ÿæ²Ÿé€šåˆ†æã€è¯­è¨€æŠ€æœ¯åº”ç”¨ç­‰å¤šä¸ªé¢†åŸŸçš„å®é™…åº”ç”¨ä»·å€¼å’Œæœªæ¥å‰æ™¯åˆ†æçš„å¯èƒ½æ€§ç­‰ä¸°å¯Œçš„å†…å®¹å’Œå«ä¹‰ã€‚ã€‚å¼ºè°ƒæŠ€æœ¯æ”¹è¿›çš„æ˜¾è‘—æˆæ•ˆåŠå…¶åœ¨å›¢é˜Ÿåä½œå’Œæœªæ¥ä»¿çœŸç¯å¢ƒä¸‹çš„å¹¿é˜”åº”ç”¨å‰æ™¯ã€‚å±•æœ›æœªæ¥ï¼Œè¯¥ç ”ç©¶å°†æ¨åŠ¨å›¢é˜Ÿæ²Ÿé€šåˆ†ææŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œå¹¶æœ‰æœ›ä¸ºæœªæ¥çš„ç©ºé—´ä»»åŠ¡å’Œå…¶ä»–é«˜å‹åŠ›ç¯å¢ƒæä¾›æ›´å¥½çš„å†³ç­–æ”¯æŒå’Œç³»ç»Ÿå¹²é¢„æ–¹æ³•ç­‰æä¾›æ”¯æŒå’Œå»ºè®®çš„ç ”ç©¶å®è·µå±•æœ›å‘å±•è¶‹è¢«å¤§å†™çš„ç¨‹åº¦å¸¦æ¥äº†å¸®åŠ©ä½œç”¨å’Œä¼˜è¶Šæ€§å¾—ä»¥å±•ç°ä»¥åŠåç»­å‘å±•çš„æ½œåœ¨ç©ºé—´å’Œä»·å€¼ç‚¹æŒ–æ˜ä¸ºå…¶ä»–é¢†åŸŸæä¾›äº†é‡è¦å¯ç¤ºå’Œæ¨å¹¿åº”ç”¨å‰æ™¯å±•ç°äº†é‡è¦çš„æ½œåŠ›å’Œå½±å“ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒå’Œå€Ÿé‰´ä»·å€¼<strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºæ£€æµ‹å›¢é˜Ÿå¯¹è¯ä¸­çš„å¾®è¡Œä¸ºè¡¨è¾¾ã€‚</li>
<li>ç¼–ç å™¨LLMåœ¨æ£€æµ‹æ¬ ä»£è¡¨çš„å¾®è¡Œä¸ºæ–¹é¢è¡¨ç°ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯å¦‚æŠ‘åˆ¶æ€§è¨€è¯­ç­‰ç»†å¾®çš„è¡¨è¾¾ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬çš„è§£ç å™¨LLMï¼ˆå¦‚Llama-3.1ï¼‰åœ¨é¢„æµ‹å¾®è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>æœ€ä½³æ¨¡å‹çš„å®F1åˆ†æ•°åœ¨ä¸‰ç±»å’ŒäºŒå…ƒåˆ†ç±»ä¸­åˆ†åˆ«è¾¾åˆ°44%å’Œ68%ï¼Œæ˜¾ç¤ºå‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹å¼€å‘åˆ†æå›¢é˜Ÿæ²Ÿé€šåŠ¨åŠ›çš„è¯­éŸ³æŠ€æœ¯æœ‰å¯ç¤ºæ„ä¹‰ï¼Œå°¤å…¶æ˜¯åœ¨é«˜å‹åŠ›ç¯å¢ƒä¸‹çš„è®­ç»ƒå¹²é¢„ï¼Œå¦‚å¤ªç©ºä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†ç‰¹å®šåœºæ™¯ä¸‹æ–‡æœ¬æ•°æ®çš„å”¯ä¸€æ€§å’Œå…¶åœ¨æŠ€æœ¯æ”¹è¿›æ–¹é¢çš„æŒ‘æˆ˜åŠæœªæ¥å‘å±•æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a8bd9be35cfd1648769564a79ad8f1d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-481461c46540ac09255d9e75f00076b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cdf082f4ae258989e4602052b4c6158.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a45c02e724ef839dbefdf7eb3c1b9cf7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Exploiting-Vision-Language-Model-for-Training-Free-3D-Point-Cloud-OOD-Detection-via-Graph-Score-Propagation"><a href="#Exploiting-Vision-Language-Model-for-Training-Free-3D-Point-Cloud-OOD-Detection-via-Graph-Score-Propagation" class="headerlink" title="Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD   Detection via Graph Score Propagation"></a>Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD   Detection via Graph Score Propagation</h2><p><strong>Authors:Tiankai Chen, Yushu Li, Adam Goodge, Fei Teng, Xulei Yang, Tianrui Li, Xun Xu</strong></p>
<p>Out-of-distribution (OOD) detection in 3D point cloud data remains a challenge, particularly in applications where safe and robust perception is critical. While existing OOD detection methods have shown progress for 2D image data, extending these to 3D environments involves unique obstacles. This paper introduces a training-free framework that leverages Vision-Language Models (VLMs) for effective OOD detection in 3D point clouds. By constructing a graph based on class prototypes and testing data, we exploit the data manifold structure to enhancing the effectiveness of VLMs for 3D OOD detection. We propose a novel Graph Score Propagation (GSP) method that incorporates prompt clustering and self-training negative prompting to improve OOD scoring with VLM. Our method is also adaptable to few-shot scenarios, providing options for practical applications. We demonstrate that GSP consistently outperforms state-of-the-art methods across synthetic and real-world datasets 3D point cloud OOD detection. </p>
<blockquote>
<p>åœ¨3Dç‚¹äº‘æ•°æ®ä¸­è¿›è¡Œç¦»ç¾¤å€¼æ£€æµ‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å’Œç¨³å¥æ„ŸçŸ¥è‡³å…³é‡è¦çš„åº”ç”¨ä¸­ã€‚å°½ç®¡ç°æœ‰çš„ç¦»ç¾¤å€¼æ£€æµ‹æ–¹æ³•åœ¨äºŒç»´å›¾åƒæ•°æ®æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å°†å…¶æ‰©å±•åˆ°ä¸‰ç»´ç¯å¢ƒä¼šé¢ä¸´ç‹¬ç‰¹çš„éšœç¢ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ä¸‰ç»´ç‚¹äº‘ä¸­å®ç°æœ‰æ•ˆçš„ç¦»ç¾¤å€¼æ£€æµ‹ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºç±»åˆ«åŸå‹å’Œæµ‹è¯•æ•°æ®æ„å»ºå›¾å½¢ï¼Œåˆ©ç”¨æ•°æ®æµå½¢ç»“æ„æé«˜VLMç”¨äºä¸‰ç»´ç¦»ç¾¤å€¼æ£€æµ‹çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å›¾å¾—åˆ†ä¼ æ’­ï¼ˆGSPï¼‰æ–¹æ³•ï¼Œå®ƒç»“åˆäº†æç¤ºèšç±»å’Œè‡ªè®­ç»ƒè´Ÿæç¤ºï¼Œä»¥æé«˜VLMçš„ç¦»ç¾¤å¾—åˆ†ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿé€‚ç”¨äºå°æ ·ä¾‹åœºæ™¯ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†é€‰æ‹©ã€‚æˆ‘ä»¬è¯æ˜äº†GSPåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºæœ€æ–°çš„ä¸‰ç»´ç‚¹äº‘ç¦»ç¾¤å€¼æ£€æµ‹æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22375v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºVision-Language Modelsï¼ˆVLMsï¼‰çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºåœ¨ä¸‰ç»´ç‚¹äº‘æ•°æ®ä¸­å®ç°æœ‰æ•ˆçš„Out-of-Distributionï¼ˆOODï¼‰æ£€æµ‹ã€‚é€šè¿‡æ„å»ºåŸºäºç±»åŸå‹å’Œæµ‹è¯•æ•°æ®çš„å›¾ï¼Œåˆ©ç”¨æ•°æ®æµå½¢ç»“æ„æé«˜VLMsåœ¨ä¸‰ç»´OODæ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„Graph Score Propagationï¼ˆGSPï¼‰æ–¹æ³•ï¼Œé€šè¿‡æç¤ºèšç±»å’Œè‡ªè®­ç»ƒè´Ÿæç¤ºæ¥æé«˜OODè¯„åˆ†ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé€‚åº”å°‘æ ·æœ¬åœºæ™¯ï¼Œå¹¶åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šå‡è¡¨ç°å‡ºå¯¹ä¸‰ç»´ç‚¹äº‘OODæ£€æµ‹çš„æœ€ä¼˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡è§£å†³äº†åœ¨ä¸‰ç»´ç‚¹äº‘æ•°æ®ä¸­è¿›è¡ŒOut-of-Distribution (OOD) æ£€æµ‹çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºVision-Language Models (VLMs) çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œé€‚ç”¨äºä¸‰ç»´OODæ£€æµ‹ã€‚</li>
<li>é€šè¿‡æ„å»ºåŸºäºç±»åŸå‹å’Œæµ‹è¯•æ•°æ®çš„å›¾ï¼Œåˆ©ç”¨æ•°æ®æµå½¢ç»“æ„æé«˜VLMsçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„Graph Score Propagation (GSP) æ–¹æ³•ï¼Œç»“åˆæç¤ºèšç±»å’Œè‡ªè®­ç»ƒè´Ÿæç¤ºæ¥æé«˜OODè¯„åˆ†ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿé€‚åº”å°‘æ ·æœ¬åœºæ™¯ï¼Œæä¾›äº†å®é™…åº”ç”¨çš„å¯èƒ½æ€§ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGSPåœ¨ä¸‰ç»´ç‚¹äº‘OODæ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a933743d8619a4654c3a521154d78fe6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54bc12e0fd2b4efae233300c14586a44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bc152406f92be50fa762f059551abc36.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Attention-disentangled-Uniform-Orthogonal-Feature-Space-Optimization-for-Few-shot-Object-Detection"><a href="#Attention-disentangled-Uniform-Orthogonal-Feature-Space-Optimization-for-Few-shot-Object-Detection" class="headerlink" title="Attention-disentangled Uniform Orthogonal Feature Space Optimization for   Few-shot Object Detection"></a>Attention-disentangled Uniform Orthogonal Feature Space Optimization for   Few-shot Object Detection</h2><p><strong>Authors:Taijin Zhao, Heqian Qiu, Yu Dai, Lanxiao Wang, Fanman Meng, Qingbo Wu, Hongliang Li</strong></p>
<p>Few-shot object detection (FSOD) aims to detect objects with limited samples for novel classes, while relying on abundant data for base classes. Existing FSOD approaches, predominantly built on the Faster R-CNN detector, entangle objectness recognition and foreground classification within shared feature spaces. This paradigm inherently establishes class-specific objectness criteria and suffers from unrepresentative novel class samples. To resolve this limitation, we propose a Uniform Orthogonal Feature Space (UOFS) optimization framework. First, UOFS decouples the feature space into two orthogonal components, where magnitude encodes objectness and angle encodes classification. This decoupling enables transferring class-agnostic objectness knowledge from base classes to novel classes. Moreover, implementing the disentanglement requires careful attention to two challenges: (1) Base set images contain unlabeled foreground instances, causing confusion between potential novel class instances and backgrounds. (2) Angular optimization depends exclusively on base class foreground instances, inducing overfitting of angular distributions to base classes. To address these challenges, we propose a Hybrid Background Optimization (HBO) strategy: (1) Constructing a pure background base set by removing unlabeled instances in original images to provide unbiased magnitude-based objectness supervision. (2) Incorporating unlabeled foreground instances in the original base set into angular optimization to enhance distribution uniformity. Additionally, we propose a Spatial-wise Attention Disentanglement and Association (SADA) module to address task conflicts between class-agnostic and class-specific tasks. Experiments demonstrate that our method significantly outperforms existing approaches based on entangled feature spaces. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFSODï¼‰æ—¨åœ¨åˆ©ç”¨æœ‰é™çš„æ ·æœ¬å¯¹æ–°å‹ç±»åˆ«è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼ŒåŒæ—¶ä¾èµ–äºåŸºæœ¬ç±»åˆ«çš„ä¸°å¯Œæ•°æ®è¿›è¡Œè¾…åŠ©ã€‚ç°æœ‰çš„FSODæ–¹æ³•ä¸»è¦åŸºäºFaster R-CNNæ£€æµ‹å™¨ï¼Œåœ¨å…±äº«ç‰¹å¾ç©ºé—´å†…çº ç¼ ç›®æ ‡è¯†åˆ«å’Œå‰æ™¯åˆ†ç±»ã€‚è¿™ç§èŒƒå¼å›ºæœ‰çš„å»ºç«‹äº†ç‰¹å®šç±»åˆ«çš„ç›®æ ‡æ€§å‡†åˆ™ï¼Œå¹¶å—åˆ°æ–°å‹ç±»åˆ«æ ·æœ¬ä»£è¡¨æ€§ä¸è¶³çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€æ­£äº¤ç‰¹å¾ç©ºé—´ï¼ˆUOFSï¼‰ä¼˜åŒ–æ¡†æ¶ã€‚é¦–å…ˆï¼ŒUOFSå°†ç‰¹å¾ç©ºé—´è§£è€¦ä¸ºä¸¤ä¸ªæ­£äº¤ç»„ä»¶ï¼Œå…¶ä¸­å¹…åº¦ç¼–ç ç›®æ ‡æ€§ï¼Œè§’åº¦ç¼–ç åˆ†ç±»ã€‚è¿™ç§è§£è€¦ä½¿å¾—å¯ä»¥ä»åŸºæœ¬ç±»åˆ«å‘æ–°å‹ç±»åˆ«è½¬ç§»ç±»åˆ«æ— å…³çš„ç›®æ ‡æ€§çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œå®ç°è§£è€¦éœ€è¦æ³¨æ„ä¸¤ä¸ªæŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰åŸºæœ¬é›†å›¾åƒåŒ…å«æœªæ ‡è®°çš„å‰æ™¯å®ä¾‹ï¼Œå¯¼è‡´æ½œåœ¨çš„æ–°å‹ç±»åˆ«å®ä¾‹å’ŒèƒŒæ™¯ä¹‹é—´çš„æ··æ·†ã€‚ï¼ˆ2ï¼‰è§’åº¦ä¼˜åŒ–å®Œå…¨ä¾èµ–äºåŸºæœ¬ç±»åˆ«çš„å‰æ™¯å®ä¾‹ï¼Œå¯¼è‡´è§’åº¦åˆ†å¸ƒå¯¹åŸºæœ¬ç±»åˆ«è¿‡åº¦æ‹Ÿåˆã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ··åˆèƒŒæ™¯ä¼˜åŒ–ï¼ˆHBOï¼‰ç­–ç•¥ï¼šï¼ˆ1ï¼‰é€šè¿‡å»é™¤åŸå§‹å›¾åƒä¸­çš„æœªæ ‡è®°å®ä¾‹æ¥æ„å»ºçº¯èƒŒæ™¯åŸºæœ¬é›†ï¼Œä»¥æä¾›æ— åçš„åŸºäºå¹…åº¦çš„ç›®æ ‡æ€§ç›‘ç£ã€‚ï¼ˆ2ï¼‰å°†åŸå§‹åŸºæœ¬é›†ä¸­çš„æœªæ ‡è®°å‰æ™¯å®ä¾‹çº³å…¥è§’åº¦ä¼˜åŒ–ï¼Œä»¥æé«˜åˆ†å¸ƒå‡åŒ€æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³ç±»åˆ«æ— å…³ä»»åŠ¡å’Œç‰¹å®šç±»åˆ«ä»»åŠ¡ä¹‹é—´çš„ä»»åŠ¡å†²çªï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ç©ºé—´æ³¨æ„åŠ›è§£è€¦å’Œå…³è”ï¼ˆSADAï¼‰æ¨¡å—ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºäºçº ç¼ ç‰¹å¾ç©ºé—´çš„æ–¹æ³•ä¸Šæ˜¾è‘—ä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22161v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFew-Shot Object Detection, FSODï¼‰çš„ä¼˜åŒ–æ¡†æ¶â€”â€”ç»Ÿä¸€æ­£äº¤ç‰¹å¾ç©ºé—´ï¼ˆUOFSï¼‰ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨æ–°ç±»åˆ«æ ·æœ¬æœ‰é™çš„æƒ…å†µä¸‹å­˜åœ¨çš„å±€é™æ€§ã€‚é€šè¿‡è§£è€¦ç‰¹å¾ç©ºé—´ï¼Œä½¿å¹…åº¦ç¼–ç å¯¹è±¡æ€§ï¼Œè§’åº¦ç¼–ç åˆ†ç±»ï¼Œå®ç°äº†ä»åŸºç¡€ç±»åˆ«åˆ°æ–°é¢–ç±»åˆ«çš„ç±»æ— å…³å¯¹è±¡æ€§çŸ¥è¯†çš„è¿ç§»ã€‚åŒæ—¶ï¼Œæå‡ºäº†æ··åˆèƒŒæ™¯ä¼˜åŒ–ï¼ˆHBOï¼‰ç­–ç•¥å’Œç©ºé—´æ³¨æ„åŠ›è§£è€¦ä¸å…³è”ï¼ˆSADAï¼‰æ¨¡å—æ¥åº”å¯¹ç›¸å…³æŒ‘æˆ˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºäºçº ç¼ ç‰¹å¾ç©ºé—´çš„ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹çš„æ”¹è¿›æ¡†æ¶â€”â€”ç»Ÿä¸€æ­£äº¤ç‰¹å¾ç©ºé—´ï¼ˆUOFSï¼‰ã€‚</li>
<li>UOFSé€šè¿‡è§£è€¦ç‰¹å¾ç©ºé—´å®ç°å¯¹è±¡æ€§å’Œåˆ†ç±»çš„åˆ†ç¦»ã€‚</li>
<li>æå‡ºäº†æ··åˆèƒŒæ™¯ä¼˜åŒ–ï¼ˆHBOï¼‰ç­–ç•¥æ¥è§£å†³èƒŒæ™¯å¹²æ‰°å’Œè¿‡åº¦æ‹Ÿåˆçš„é—®é¢˜ã€‚</li>
<li>ä»‹ç»äº†ç©ºé—´æ³¨æ„åŠ›è§£è€¦ä¸å…³è”ï¼ˆSADAï¼‰æ¨¡å—ä»¥å¤„ç†ä»»åŠ¡å†²çªã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•åœ¨å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿè¿ç§»ç±»æ— å…³çš„å¯¹è±¡æ€§çŸ¥è¯†ä»åŸºç¡€ç±»åˆ«åˆ°æ–°é¢–ç±»åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6bd41f3294b9b936b3bd8d2eac0c1a59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5b7e76458b7ffc01893fc74bb04609c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6bab1e334b17698808aa02e48e94dd5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SAGE-Spliced-Audio-Generated-Data-for-Enhancing-Foundational-Models-in-Low-Resource-Arabic-English-Code-Switched-Speech-Recognition"><a href="#SAGE-Spliced-Audio-Generated-Data-for-Enhancing-Foundational-Models-in-Low-Resource-Arabic-English-Code-Switched-Speech-Recognition" class="headerlink" title="SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in   Low-Resource Arabic-English Code-Switched Speech Recognition"></a>SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in   Low-Resource Arabic-English Code-Switched Speech Recognition</h2><p><strong>Authors:Muhammad Umar Farooq, Oscar Saz</strong></p>
<p>This paper investigates the performance of various speech SSL models on dialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address data scarcity, a modified audio-splicing approach is introduced to generate artificial CS speech data. Fine-tuning an already fine-tuned SSL model with the proposed Spliced-Audio Generated (SAGE) data results in an absolute improvement on Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks. Additionally, an Experience Replay (ER) inspired approach is proposed to enhance generalisation across DA and CS speech while mitigating catastrophic forgetting. Integrating an out-of-domain 3-gram language model reduces the overall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching benchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS benchmarks surpasses large-scale multilingual models, including USM and Whisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and 8.4%, respectively. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†å„ç§è¯­éŸ³SSLæ¨¡å‹åœ¨æ–¹è¨€é˜¿æ‹‰ä¼¯è¯­ï¼ˆDAï¼‰å’Œé˜¿æ‹‰ä¼¯è¯­-è‹±è¯­ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰è¯­éŸ³ä¸Šçš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„éŸ³é¢‘æ‹¼æ¥æ–¹æ³•æ¥ç”Ÿæˆäººå·¥CSè¯­éŸ³æ•°æ®ã€‚ä½¿ç”¨æå‡ºçš„æ‹¼æ¥éŸ³é¢‘ç”Ÿæˆï¼ˆSAGEï¼‰æ•°æ®å¯¹å·²ç»å¾®è°ƒè¿‡çš„SSLæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç»“æœåœ¨é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­CSåŸºå‡†æµ‹è¯•ä¸Šçš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ç»å¯¹æ”¹è¿›äº†7.8%ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§å—ç»éªŒå›æ”¾ï¼ˆERï¼‰å¯å‘çš„æ–¹æ³•æ¥æé«˜DAå’ŒCSè¯­éŸ³çš„é€šç”¨æ€§ï¼ŒåŒæ—¶å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚é›†æˆåŸŸå¤–3-gramè¯­è¨€æ¨¡å‹å°†æ•´ä½“å¹³å‡WERä»31.7%é™ä½åˆ°26.6%ã€‚é’ˆå¯¹ä»£ç åˆ‡æ¢åŸºå‡†æµ‹è¯•çš„å°‘é‡å¾®è°ƒè¿›ä¸€æ­¥å°†WERæé«˜äº†4.9%ã€‚åœ¨é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­CSåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°çš„WERä¸º31.1%ï¼Œè¶…è¶Šäº†å¤§è§„æ¨¡å¤šè¯­ç§æ¨¡å‹ï¼ŒåŒ…æ‹¬USMå’ŒWhisper-large-v2ï¼ˆä¸¤è€…éƒ½è¶…è¿‡åå€å¤§å°ï¼‰ï¼Œåˆ†åˆ«ç»å¯¹ä¼˜äº5.5%å’Œ8.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22143v1">PDF</a> Accepted for IEEE MLSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¸åŒè¯­éŸ³SSLæ¨¡å‹åœ¨æ–¹è¨€é˜¿æ‹‰ä¼¯è¯­å’Œé˜¿æ‹‰ä¼¯è¯­-è‹±è¯­è½¬ç è¯­éŸ³ä¸Šçš„è¡¨ç°ã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§æ”¹è¿›å‹éŸ³é¢‘æ‹¼æ¥æ–¹æ³•æ¥ç”Ÿæˆäººå·¥è½¬ç è¯­éŸ³æ•°æ®ã€‚ä½¿ç”¨ç”Ÿæˆçš„SAGEæ•°æ®å¯¹å·²ç»ç²¾ç»†è°ƒæ•´çš„SSLæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåœ¨é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­è½¬ç åŸºå‡†æµ‹è¯•ä¸Šï¼Œå•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ç»å¯¹æ”¹å–„äº†7.8%ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§å—ç»éªŒå›æ”¾å¯å‘çš„ç­–ç•¥ï¼Œä»¥æé«˜åœ¨æ–¹è¨€é˜¿æ‹‰ä¼¯è¯­å’Œè½¬ç è¯­éŸ³ä¹‹é—´çš„é€šç”¨æ€§ï¼ŒåŒæ—¶å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚ç»“åˆé¢†åŸŸå¤–çš„ä¸‰å…ƒè¯­è¨€æ¨¡å‹ï¼Œå°†æ•´ä½“å¹³å‡WERä»31.7%é™ä½åˆ°26.6%ã€‚å¯¹è½¬ç åŸºå‡†æµ‹è¯•è¿›è¡Œå°æ ·æœ¬å¾®è°ƒè¿›ä¸€æ­¥å°†WERæé«˜äº†4.9%ã€‚åœ¨é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­è½¬ç åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†31.1%çš„WERï¼Œè¾ƒUSMå’ŒWhisper-large-v2ç­‰å¤§å‹å¤šè¯­ç§æ¨¡å‹åˆ†åˆ«æé«˜äº†5.5%å’Œ8.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ç ”ç©¶äº†ä¸åŒè¯­éŸ³SSLæ¨¡å‹åœ¨æ–¹è¨€é˜¿æ‹‰ä¼¯è¯­å’Œé˜¿æ‹‰ä¼¯è¯­-è‹±è¯­è½¬ç è¯­éŸ³ä¸Šçš„è¡¨ç°ã€‚</li>
<li>ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œé‡‡ç”¨äº†æ”¹è¿›å‹éŸ³é¢‘æ‹¼æ¥æ–¹æ³•ç”Ÿæˆäººå·¥è½¬ç è¯­éŸ³æ•°æ®ã€‚</li>
<li>é€šè¿‡å¾®è°ƒSSLæ¨¡å‹å¹¶èåˆç”Ÿæˆæ•°æ®ï¼Œå•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰åœ¨é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­è½¬ç åŸºå‡†æµ‹è¯•ä¸Šæœ‰æ‰€æ”¹å–„ã€‚</li>
<li>æå‡ºå—ç»éªŒå›æ”¾å¯å‘çš„ç­–ç•¥ä»¥æé«˜æ¨¡å‹çš„é€šç”¨æ€§å¹¶å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>ç»“åˆé¢†åŸŸå¤–çš„ä¸‰å…ƒè¯­è¨€æ¨¡å‹ï¼Œè¿›ä¸€æ­¥é™ä½WERã€‚</li>
<li>åœ¨é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­è½¬ç åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°è¾ƒä½WERï¼Œè¡¨ç°ä¼˜äºæŸäº›å¤§å‹å¤šè¯­ç§æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-37784a5af802b900535a4e90bf4a636b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4adefd8817f7e1d9443d3ca011e04b00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f15420747815e3f873aec122e4e83efd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-859392e010e9345738f6aa37c0bcaf11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bc939a013385cde86d7d7c6907c6841.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18883875c032b8aac34d4d1072b07322.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Tied-Prototype-Model-for-Few-Shot-Medical-Image-Segmentation"><a href="#Tied-Prototype-Model-for-Few-Shot-Medical-Image-Segmentation" class="headerlink" title="Tied Prototype Model for Few-Shot Medical Image Segmentation"></a>Tied Prototype Model for Few-Shot Medical Image Segmentation</h2><p><strong>Authors:Hyeongji Kim, Stine Hansen, Michael Kampffmeyer</strong></p>
<p>Common prototype-based medical image few-shot segmentation (FSS) methods model foreground and background classes using class-specific prototypes. However, given the high variability of the background, a more promising direction is to focus solely on foreground modeling, treating the background as an anomaly â€“ an approach introduced by ADNet. Yet, ADNet faces three key limitations: dependence on a single prototype per class, a focus on binary classification, and fixed thresholds that fail to adapt to patient and organ variability. To address these shortcomings, we propose the Tied Prototype Model (TPM), a principled reformulation of ADNet with tied prototype locations for foreground and background distributions. Building on its probabilistic foundation, TPM naturally extends to multiple prototypes and multi-class segmentation while effectively separating non-typical background features. Notably, both extensions lead to improved segmentation accuracy. Finally, we leverage naturally occurring class priors to define an ideal target for adaptive thresholds, boosting segmentation performance. Taken together, TPM provides a fresh perspective on prototype-based FSS for medical image segmentation. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/hjk92g/TPM-FSS">https://github.com/hjk92g/TPM-FSS</a>. </p>
<blockquote>
<p>å¸¸è§çš„åŸºäºåŸå‹çš„åŒ»å­¦å›¾åƒå°‘æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰æ–¹æ³•ä½¿ç”¨é’ˆå¯¹ç‰¹å®šç±»åˆ«çš„åŸå‹å¯¹å‰æ™¯å’ŒèƒŒæ™¯ç±»åˆ«è¿›è¡Œå»ºæ¨¡ã€‚ç„¶è€Œï¼Œè€ƒè™‘åˆ°èƒŒæ™¯çš„å·¨å¤§å˜åŒ–æ€§ï¼Œä¸€ä¸ªæ›´æœ‰å‰æ™¯çš„æ–¹å‘æ˜¯ä¸“æ³¨äºå‰æ™¯å»ºæ¨¡ï¼Œå°†èƒŒæ™¯è§†ä¸ºå¼‚å¸¸å€¼ï¼ŒADNetå°±é‡‡ç”¨äº†è¿™ç§æ€è·¯ã€‚ç„¶è€Œï¼ŒADNeté¢ä¸´ä¸‰ä¸ªä¸»è¦å±€é™ï¼šæ¯ä¸ªç±»åˆ«åªä¾èµ–äºä¸€ä¸ªåŸå‹ã€ä¸“æ³¨äºäºŒåˆ†ç±»ä»¥åŠæ— æ³•é€‚åº”æ‚£è€…å’Œå™¨å®˜å¯å˜æ€§çš„å›ºå®šé˜ˆå€¼ã€‚ä¸ºäº†è§£å†³è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†è”åˆåŸå‹æ¨¡å‹ï¼ˆTPMï¼‰ï¼Œå®ƒæ˜¯ADNetçš„æœ‰åŸåˆ™çš„é‡è¿°ï¼Œå…·æœ‰ç”¨äºå‰æ™¯å’ŒèƒŒæ™¯åˆ†å¸ƒçš„å…±åŒåŸå‹ä½ç½®ã€‚åŸºäºæ¦‚ç‡è®ºåŸºç¡€ï¼ŒTPMè‡ªç„¶åœ°æ‰©å±•åˆ°å¤šä¸ªåŸå‹å’Œå¤šç±»åˆ†å‰²ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°åˆ†ç¦»éå…¸å‹èƒŒæ™¯ç‰¹å¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸¤ä¸ªæ‰©å±•éƒ½æé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨è‡ªç„¶å‘ç”Ÿçš„ç±»åˆ«å…ˆéªŒæ¥å®šä¹‰è‡ªé€‚åº”é˜ˆå€¼çš„ç†æƒ³ç›®æ ‡ï¼Œä»è€Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚ç»¼ä¸Šæ‰€è¿°ï¼ŒTPMä¸ºåŸºäºåŸå‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²FSSæä¾›äº†å…¨æ–°çš„è§†è§’ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hjk92g/TPM-FSS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hjk92g/TPM-FSSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22101v1">PDF</a> Submitted version (MICCAI). Accepted at MICCAI 2025. The code repo   will be made publicly available soon</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºåŸå‹çš„åŒ»ç–—å›¾åƒå°‘æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰æ–¹æ³•çš„å¸¸è§åšæ³•ï¼Œè¿™äº›æ–¹æ³•é€šè¿‡ç±»åˆ«ç‰¹å®šçš„åŸå‹å¯¹å‰æ™¯å’ŒèƒŒæ™¯ç±»åˆ«è¿›è¡Œå»ºæ¨¡ã€‚é‰´äºèƒŒæ™¯çš„é«˜å¯å˜æ€§ï¼Œä¸€ç§æ›´æœ‰å‰é€”çš„æ–¹å‘æ˜¯ä»…ä¸“æ³¨äºå‰æ™¯å»ºæ¨¡ï¼Œå°†èƒŒæ™¯è§†ä¸ºå¼‚å¸¸å€¼ã€‚ADNeté¦–æ¬¡é‡‡ç”¨äº†è¿™ç§æ€è·¯ï¼Œä½†é¢ä¸´ä¸‰ä¸ªå…³é”®å±€é™ï¼šä¾èµ–å•ä¸€åŸå‹ã€å…³æ³¨äºŒå…ƒåˆ†ç±»ä»¥åŠæ— æ³•é€‚åº”æ‚£è€…å’Œå™¨å®˜å¯å˜æ€§çš„å›ºå®šé˜ˆå€¼ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Tied Prototype Modelï¼ˆTPMï¼‰ï¼Œå®ƒæ˜¯ADNetçš„ç†è®ºé‡æ„ï¼Œä¸ºå‰æ™¯å’ŒèƒŒæ™¯åˆ†å¸ƒæä¾›äº†ç»‘å®šçš„åŸå‹ä½ç½®ã€‚åŸºäºæ¦‚ç‡è®ºï¼ŒTPMè‡ªç„¶åœ°æ‰©å±•åˆ°å¤šä¸ªåŸå‹å’Œå¤šç±»åˆ†å‰²ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°åˆ†ç¦»äº†éå…¸å‹èƒŒæ™¯ç‰¹å¾ã€‚ä¸¤ä¸ªæ‰©å±•å‡æé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨è‡ªç„¶å‘ç”Ÿçš„ç±»åˆ«å…ˆéªŒæ¥å®šä¹‰è‡ªé€‚åº”é˜ˆå€¼çš„ç†æƒ³ç›®æ ‡ï¼Œæé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚æ€»çš„æ¥è¯´ï¼ŒTPMä¸ºåŸºäºåŸå‹çš„FSSåŒ»ç–—å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„è§†è§’ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/hjk92g/TPM-FSS%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/hjk92g/TPM-FSSè®¿é—®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰åŸºäºåŸå‹çš„FSSæ–¹æ³•ä¸»è¦ä¾èµ–ç±»åˆ«ç‰¹å®šçš„åŸå‹å¯¹å‰æ™¯å’ŒèƒŒæ™¯è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>ADNetæ–¹æ³•è™½ç„¶å°è¯•è§£å†³èƒŒæ™¯å¤æ‚æ€§é—®é¢˜ï¼Œä½†å­˜åœ¨å•ä¸€åŸå‹ä¾èµ–ã€äºŒå…ƒåˆ†ç±»ç„¦ç‚¹å’Œå›ºå®šé˜ˆå€¼ä¸é€‚åº”æ€§çš„å±€é™ã€‚</li>
<li>TPMæ¨¡å‹æ˜¯å¯¹ADNetçš„ç†è®ºé‡æ„ï¼Œé€šè¿‡ç»‘å®šå‰æ™¯å’ŒèƒŒæ™¯çš„åŸå‹ä½ç½®æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>TPMæ¨¡å‹èƒ½å¤Ÿè‡ªç„¶åœ°æ‰©å±•åˆ°å¤šä¸ªåŸå‹å’Œå¤šç±»åˆ†å‰²ï¼Œæœ‰æ•ˆåˆ†ç¦»éå…¸å‹ç‰¹å¾ã€‚</li>
<li>TPMåˆ©ç”¨è‡ªé€‚åº”é˜ˆå€¼å’Œç±»åˆ«å…ˆéªŒæ¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>TPMæ¨¡å‹æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’æ¥çœ‹å¾…åŸºäºåŸå‹çš„FSSåœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc5104ccf10c1aee59a8de8499353553.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c6755f5e30e4835fc02d46f0c1bfc14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95ee4d4ca64d8b3ca76a0a3f0418725a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Identity-Adaptation-for-3D-Talking-Heads-via-Global-Gaussian-Field"><a href="#Few-Shot-Identity-Adaptation-for-3D-Talking-Heads-via-Global-Gaussian-Field" class="headerlink" title="Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian   Field"></a>Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian   Field</h2><p><strong>Authors:Hong Nie, Fuyuan Cao, Lu Chen, Fengxin Chen, Yuefeng Zou, Jun Yu</strong></p>
<p>Reconstruction and rendering-based talking head synthesis methods achieve high-quality results with strong identity preservation but are limited by their dependence on identity-specific models. Each new identity requires training from scratch, incurring high computational costs and reduced scalability compared to generative model-based approaches. To overcome this limitation, we propose FIAG, a novel 3D speaking head synthesis framework that enables efficient identity-specific adaptation using only a few training footage. FIAG incorporates Global Gaussian Field, which supports the representation of multiple identities within a shared field, and Universal Motion Field, which captures the common motion dynamics across diverse identities. Benefiting from the shared facial structure information encoded in the Global Gaussian Field and the general motion priors learned in the motion field, our framework enables rapid adaptation from canonical identity representations to specific ones with minimal data. Extensive comparative and ablation experiments demonstrate that our method outperforms existing state-of-the-art approaches, validating both the effectiveness and generalizability of the proposed framework. Code is available at: \textit{<a target="_blank" rel="noopener" href="https://github.com/gme-hong/FIAG%7D">https://github.com/gme-hong/FIAG}</a>. </p>
<blockquote>
<p>åŸºäºé‡å»ºå’Œæ¸²æŸ“çš„è¯´è¯äººå¤´éƒ¨åˆæˆæ–¹æ³•èƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„ç»“æœå¹¶å…·æœ‰å¾ˆå¼ºçš„èº«ä»½ä¿ç•™æ€§ï¼Œä½†å—é™äºå…¶ä¾èµ–äºç‰¹å®šèº«ä»½çš„æ¨¡å‹ã€‚æ¯ä¸ªæ–°èº«ä»½éƒ½éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒï¼Œè¿™å¢åŠ äº†è®¡ç®—æˆæœ¬ï¼Œä¸åŸºäºç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”ï¼Œå¯æ‰©å±•æ€§é™ä½ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FIAGï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„3Dè¯´è¯äººå¤´åˆæˆæ¡†æ¶ï¼Œå®ƒä»…ä½¿ç”¨å°‘é‡è®­ç»ƒé•œå¤´å°±èƒ½å®ç°é«˜æ•ˆçš„èº«ä»½ç‰¹å®šé€‚åº”ã€‚FIAGç»“åˆäº†å…¨å±€é«˜æ–¯åœºï¼Œæ”¯æŒåœ¨å…±äº«åœºå†…è¡¨ç¤ºå¤šä¸ªèº«ä»½ï¼Œä»¥åŠé€šç”¨è¿åŠ¨åœºï¼Œèƒ½æ•æ‰ä¸åŒèº«ä»½ä¹‹é—´çš„å…±åŒè¿åŠ¨åŠ¨æ€ã€‚å—ç›Šäºç¼–ç åœ¨å…¨å±€é«˜æ–¯åœºä¸­çš„å…±äº«é¢éƒ¨ç»“æ„ä¿¡æ¯å’Œè¿åŠ¨åœºä¸­å­¦ä¹ çš„é€šç”¨è¿åŠ¨å…ˆéªŒçŸ¥è¯†ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿè¿…é€Ÿä»è§„èŒƒèº«ä»½è¡¨ç¤ºé€‚åº”åˆ°ç‰¹å®šèº«ä»½è¡¨ç¤ºï¼Œæ‰€éœ€æ•°æ®æœ€å°‘ã€‚å¹¿æ³›çš„å¯¹æ¯”å’Œæ¶ˆèå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†æ‰€æå‡ºæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/gme-hong/FIAG%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/gme-hong/FIAGè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22044v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§åŸºäºé‡å»ºå’Œæ¸²æŸ“çš„è¯´è¯äººå¤´éƒ¨åˆæˆæ–¹æ³•èƒ½å¤Ÿè¾¾æˆé«˜è´¨é‡çš„ç»“æœå¹¶å¼ºçƒˆä¿ç•™èº«ä»½ç‰¹å¾ï¼Œä½†å—é™äºç‰¹å®šèº«ä»½æ¨¡å‹çš„ä¾èµ–ã€‚æ¯ä¸ªæ–°èº«ä»½éƒ½éœ€è¦ä»é›¶å¼€å§‹è®­ç»ƒï¼Œå¯¼è‡´äº†é«˜çš„è®¡ç®—æˆæœ¬å’Œä¸åŸºäºç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ç›¸æ¯”çš„å¯æ‰©å±•æ€§é™ä½ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†FIAGï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„3Dè¯´è¯äººå¤´éƒ¨åˆæˆæ¡†æ¶ï¼Œå®ƒä»…ä½¿ç”¨å°‘é‡è®­ç»ƒé•œå¤´å°±èƒ½å®ç°ç‰¹å®šèº«ä»½çš„é€‚åº”ã€‚FIAGç»“åˆäº†å…¨å±€é«˜æ–¯åœºï¼Œæ”¯æŒåœ¨å…±äº«åœºå†…è¡¨ç¤ºå¤šä¸ªèº«ä»½ï¼Œä»¥åŠé€šç”¨è¿åŠ¨åœºï¼Œèƒ½æ•æ‰ä¸åŒèº«ä»½çš„é€šç”¨è¿åŠ¨åŠ¨æ€ã€‚å—ç›Šäºç¼–ç åœ¨å…¨å±€é«˜æ–¯åœºä¸­çš„å…±äº«é¢éƒ¨ç»“æ„ä¿¡æ¯å’Œè¿åŠ¨åœºä¸­å­¦ä¹ åˆ°çš„ä¸€èˆ¬è¿åŠ¨å…ˆéªŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿä»æ ‡å‡†èº«ä»½è¡¨ç¤ºè¿…é€Ÿé€‚åº”ç‰¹å®šèº«ä»½ï¼Œæ‰€éœ€æ•°æ®æå°‘ã€‚å¹¿æ³›çš„å¯¹æ¯”å’Œæ¶ˆèå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒéªŒè¯äº†æ‰€æå‡ºæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FIAGæ˜¯ä¸€ä¸ªæ–°é¢–çš„3Dè¯´è¯äººå¤´éƒ¨åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆç‰¹å®šèº«ä»½é€‚åº”ã€‚</li>
<li>å®ƒç»“åˆå…¨å±€é«˜æ–¯åœºå’Œé€šç”¨è¿åŠ¨åœºï¼Œåˆ†åˆ«æ”¯æŒå¤šèº«ä»½è¡¨ç¤ºå’Œé€šç”¨è¿åŠ¨åŠ¨æ€çš„æ•æ‰ã€‚</li>
<li>FIAGèƒ½å¤Ÿä»æ ‡å‡†èº«ä»½è¡¨ç¤ºè¿…é€Ÿé€‚åº”ç‰¹å®šèº«ä»½ï¼Œåªéœ€å°‘é‡æ•°æ®ã€‚</li>
<li>å¹¿æ³›çš„å¯¹æ¯”å’Œæ¶ˆèå®éªŒè¯æ˜ï¼ŒFIAGåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>FIAGæ¡†æ¶å…·æœ‰æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11f50ad98fffcbb550b2200e3d643b38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f29f7f387ccdcf4893e33be9de8b3c64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-479a0618e3a00e4840a52ab7953b5d0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-459fba3b6dca2b7686c5ad42ee62a64e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7787627c70d8d372e310b21edbf13e96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d3c4fd13861273e7797567c579a8849.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fabcb31d8c9c8938cc81c2f4937e5d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5adf561684b870784ab2333bb58d56df.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Segmentation-of-Historical-Maps-via-Linear-Probing-of-Vision-Foundation-Models"><a href="#Few-Shot-Segmentation-of-Historical-Maps-via-Linear-Probing-of-Vision-Foundation-Models" class="headerlink" title="Few-Shot Segmentation of Historical Maps via Linear Probing of Vision   Foundation Models"></a>Few-Shot Segmentation of Historical Maps via Linear Probing of Vision   Foundation Models</h2><p><strong>Authors:Rafael Sterzinger, Marco Peer, Robert Sablatnig</strong></p>
<p>As rich sources of history, maps provide crucial insights into historical changes, yet their diverse visual representations and limited annotated data pose significant challenges for automated processing. We propose a simple yet effective approach for few-shot segmentation of historical maps, leveraging the rich semantic embeddings of large vision foundation models combined with parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on the Siegfried benchmark dataset in vineyard and railway segmentation, achieving +5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20% in the more challenging 5-shot setting. Additionally, it demonstrates strong performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3% for building block segmentation, despite not being optimized for this shape-sensitive metric, underscoring its generalizability. Notably, our approach maintains high performance even in extremely low-data regimes (10- &amp; 5-shot), while requiring only 689k trainable parameters - just 0.21% of the total model size. Our approach enables precise segmentation of diverse historical maps while drastically reducing the need for manual annotations, advancing automated processing and analysis in the field. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/RafaelSterzinger/few-shot-map-segmentation">https://github.com/RafaelSterzinger/few-shot-map-segmentation</a>. </p>
<blockquote>
<p>åœ°å›¾ä½œä¸ºå†å²çš„ä¸°å¯Œæ¥æºï¼Œä¸ºå†å²å˜è¿æä¾›äº†å…³é”®è§è§£ï¼Œç„¶è€Œï¼Œå…¶å¤šæ ·çš„è§†è§‰è¡¨ç¤ºå’Œæœ‰é™çš„æ ‡æ³¨æ•°æ®ç»™è‡ªåŠ¨åŒ–å¤„ç†å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºå†å²åœ°å›¾çš„å°‘é‡æ ·æœ¬åˆ†å‰²ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹çš„ä¸°å¯Œè¯­ä¹‰åµŒå…¥ï¼Œå¹¶ç»“åˆå‚æ•°é«˜æ•ˆçš„å¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨SiegfriedåŸºå‡†æ•°æ®é›†ä¸Šçš„è‘¡è„å›­å’Œé“è·¯åˆ†å‰²æ–¹é¢è¶…è¶Šäº†æœ€æ–°æŠ€æœ¯ï¼Œåœ¨10æ¬¡å°„å‡»åœºæ™¯ä¸­mIoUç›¸å¯¹æé«˜äº†+5%å’Œ+13%ï¼Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„5æ¬¡å°„å‡»è®¾ç½®ä¸­æé«˜äº†çº¦+20%ã€‚æ­¤å¤–ï¼Œåœ¨ICDAR 2021ç«èµ›æ•°æ®é›†ä¸Šï¼Œå®ƒåœ¨å»ºç­‘å—åˆ†å‰²æ–¹é¢è¾¾åˆ°äº†67.3%çš„å¹³å‡PQï¼Œå°½ç®¡è¿™ä¸€å½¢çŠ¶æ•æ„Ÿçš„æŒ‡æ ‡å¹¶æœªç»è¿‡ä¼˜åŒ–ï¼Œä½†å‡¸æ˜¾äº†å…¶é€šç”¨æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å³ä½¿åœ¨æä½æ•°æ®ç¯å¢ƒï¼ˆ10æ¬¡å’Œ5æ¬¡å°„å‡»ï¼‰ä¸‹ä¹Ÿèƒ½ä¿æŒé«˜æ€§èƒ½ï¼Œå¹¶ä¸”åªéœ€è¦689kä¸ªå¯è®­ç»ƒå‚æ•°â€”â€”ä»…å æ¨¡å‹æ€»å¤§å°çš„0.21%ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°å„ç§å†å²åœ°å›¾çš„ç²¾ç¡®åˆ†å‰²ï¼Œå¹¶å¤§å¤§é™ä½äº†å¯¹æ‰‹åŠ¨æ³¨é‡Šçš„éœ€æ±‚ï¼Œæ¨åŠ¨äº†è¯¥é¢†åŸŸçš„è‡ªåŠ¨åŒ–å¤„ç†å’Œåˆ†æçš„å‘å±•ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/RafaelSterzinger/few-shot-map-segmentation">https://github.com/RafaelSterzinger/few-shot-map-segmentation</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21826v1">PDF</a> 18 pages, accepted at ICDAR2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å†å²åœ°å›¾çš„Few-Shotåˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹çš„ä¸°å¯Œè¯­ä¹‰åµŒå…¥å’Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œå®ç°äº†å¯¹å†å²åœ°å›¾çš„ç²¾å‡†åˆ†å‰²ã€‚è¯¥æ–¹æ³•åœ¨Siegfriedæ•°æ®é›†ä¸Šçš„è‘¡è„å›­å’Œé“è·¯åˆ†å‰²ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œåœ¨10æ¬¡æ‹æ‘„åœºæ™¯ä¸­ç›¸å¯¹æé«˜äº†5%å’Œ13%çš„mIoUã€‚æ­¤å¤–ï¼Œåœ¨ICDAR 2021ç«èµ›æ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å¯¹å»ºç­‘åŒºå—åˆ†å‰²çš„å¹³å‡PQè¾¾åˆ°äº†67.3%ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨æä½æ•°æ®é‡ç¯å¢ƒä¸‹ä»èƒ½ä¿æŒé«˜æ€§èƒ½ï¼Œå¹¶ä¸”åªéœ€è¦è®­ç»ƒå°‘é‡çš„å‚æ•°ã€‚è¯¥æ–¹æ³•çš„å®æ–½å¯ä»¥å…¬å¼€è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹å†å²åœ°å›¾çš„Few-Shotåˆ†å‰²æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹çš„ä¸°å¯Œè¯­ä¹‰åµŒå…¥ã€‚</li>
<li>é€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯å®ç°ç²¾å‡†åˆ†å‰²ã€‚</li>
<li>åœ¨Siegfriedæ•°æ®é›†ä¸Šçš„è‘¡è„å›­å’Œé“è·¯åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>åœ¨ICDAR 2021ç«èµ›æ•°æ®é›†ä¸Šï¼Œå¯¹å»ºç­‘åŒºå—åˆ†å‰²æ€§èƒ½å¼ºå¤§ã€‚</li>
<li>åœ¨æä½æ•°æ®é‡ç¯å¢ƒä¸‹ä¿æŒé«˜æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•æ‰€éœ€è®­ç»ƒå‚æ•°å°‘ï¼Œå®æ–½å…¬å¼€å¯è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0f3afaa8719eb228d3ae0c31f73eb06f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6557f66be181124f08858093a652207e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6367c47527df8a9bdcdc17e6793848d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="INP-Former-Advancing-Universal-Anomaly-Detection-via-Intrinsic-Normal-Prototypes-and-Residual-Learning"><a href="#INP-Former-Advancing-Universal-Anomaly-Detection-via-Intrinsic-Normal-Prototypes-and-Residual-Learning" class="headerlink" title="INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal   Prototypes and Residual Learning"></a>INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal   Prototypes and Residual Learning</h2><p><strong>Authors:Wei Luo, Haiming Yao, Yunkang Cao, Qiyu Chen, Ang Gao, Weiming Shen, Wenyong Yu</strong></p>
<p>Anomaly detection (AD) is essential for industrial inspection and medical diagnosis, yet existing methods typically rely on &#96;&#96;comparingâ€™â€™ test images to normal references from a training set. However, variations in appearance and positioning often complicate the alignment of these references with the test image, limiting detection accuracy. We observe that most anomalies manifest as local variations, meaning that even within anomalous images, valuable normal information remains. We argue that this information is useful and may be more aligned with the anomalies since both the anomalies and the normal information originate from the same image. Therefore, rather than relying on external normality from the training set, we propose INP-Former, a novel method that extracts Intrinsic Normal Prototypes (INPs) directly from the test image. Specifically, we introduce the INP Extractor, which linearly combines normal tokens to represent INPs. We further propose an INP Coherence Loss to ensure INPs can faithfully represent normality for the testing image. These INPs then guide the INP-guided Decoder to reconstruct only normal tokens, with reconstruction errors serving as anomaly scores. Additionally, we propose a Soft Mining Loss to prioritize hard-to-optimize samples during training. INP-Former achieves state-of-the-art performance in single-class, multi-class, and few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a versatile and universal solution for AD. Remarkably, INP-Former also demonstrates some zero-shot AD capability. Furthermore, we propose a soft version of the INP Coherence Loss and enhance INP-Former by incorporating residual learning, leading to the development of INP-Former++. The proposed method significantly improves detection performance across single-class, multi-class, semi-supervised, few-shot, and zero-shot settings. </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰å¯¹äºå·¥ä¸šæ£€æµ‹å’ŒåŒ»ç–—è¯Šæ–­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå°†æµ‹è¯•å›¾åƒä¸è®­ç»ƒé›†ä¸­çš„æ­£å¸¸å‚è€ƒå›¾åƒè¿›è¡Œæ¯”å¯¹ã€‚ä½†å¤–è§‚å’Œä½ç½®çš„å˜åŒ–ç»å¸¸ä½¿è¿™äº›å‚è€ƒå›¾åƒä¸æµ‹è¯•å›¾åƒçš„å¯¹é½å˜å¾—å¤æ‚ï¼Œä»è€Œé™åˆ¶äº†æ£€æµ‹ç²¾åº¦ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå¤§å¤šæ•°å¼‚å¸¸è¡¨ç°ä¸ºå±€éƒ¨å˜åŒ–ï¼Œè¿™æ„å‘³ç€å³ä½¿åœ¨å¼‚å¸¸å›¾åƒå†…éƒ¨ï¼Œä»æœ‰å®è´µçš„æ­£å¸¸ä¿¡æ¯å­˜åœ¨ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›ä¿¡æ¯æ˜¯æœ‰ç”¨çš„ï¼Œå¹¶ä¸”å¯èƒ½ä¸å¼‚å¸¸æ›´åŠ å¯¹é½ï¼Œå› ä¸ºå¼‚å¸¸å’Œæ­£å¸¸ä¿¡æ¯éƒ½æ¥è‡ªåŒä¸€å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•INP-Formerï¼Œå®ƒç›´æ¥ä»æµ‹è¯•å›¾åƒä¸­æå–å†…åœ¨æ­£å¸¸åŸå‹ï¼ˆINPsï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†INPæå–å™¨ï¼Œè¯¥æå–å™¨é€šè¿‡çº¿æ€§ç»„åˆæ­£å¸¸ä»¤ç‰Œæ¥è¡¨ç¤ºINPsã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§INPä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ç¡®ä¿INPsèƒ½å¤Ÿå¿ å®åœ°ä»£è¡¨æµ‹è¯•å›¾åƒçš„æ­£å¸¸æ€§ã€‚è¿™äº›INPsç„¶åå¼•å¯¼INPå¼•å¯¼è§£ç å™¨ä»…é‡å»ºæ­£å¸¸ä»¤ç‰Œï¼Œé‡å»ºè¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è½¯æŒ–æ˜æŸå¤±ï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†éš¾ä»¥ä¼˜åŒ–çš„æ ·æœ¬ã€‚INP-Formeråœ¨MVTec-ADã€VisAå’ŒReal-IADä¸Šçš„å•ç±»ã€å¤šç±»ä»¥åŠå°‘æ ·æœ¬ADä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œæˆä¸ºADçš„é€šç”¨è§£å†³æ–¹æ¡ˆã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒINP-Formerè¿˜å±•ç°å‡ºäº†ä¸€äº›é›¶æ ·æœ¬ADçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†INPä¸€è‡´æ€§æŸå¤±çš„è½¯ç‰ˆæœ¬ï¼Œå¹¶é€šè¿‡å¼•å…¥æ®‹å·®å­¦ä¹ å¢å¼ºäº†INP-Formerï¼Œä»è€Œå¼€å‘å‡ºINP-Former++ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨å•ç±»ã€å¤šç±»ã€åŠç›‘ç£ã€å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬è®¾ç½®ä¸­æ˜¾è‘—æé«˜äº†æ£€æµ‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03660v2">PDF</a> 15 pages, 11 figures, 13 tables. arXiv admin note: substantial text   overlap with arXiv:2503.02424</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºINP-Formerçš„æ–°å‹å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»æµ‹è¯•å›¾åƒä¸­æå–å†…åœ¨æ­£å¸¸åŸå‹ï¼ˆINPsï¼‰æ¥æ£€æµ‹å¼‚å¸¸ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥INPæå–å™¨å’ŒINPå¼•å¯¼è§£ç å™¨ï¼Œä»¥åŠINPä¸€è‡´æ€§æŸå¤±å’Œè½¯æŒ–æ˜æŸå¤±ï¼Œå®ç°äº†å¯¹å›¾åƒçš„æ­£å¸¸æ€§è¿›è¡Œé‡å»ºå¹¶è¯†åˆ«å¼‚å¸¸ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶å…·æœ‰é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚è¿›ä¸€æ­¥é€šè¿‡å¼•å…¥è½¯ç‰ˆæœ¬çš„INPä¸€è‡´æ€§æŸå¤±å¹¶ç»“åˆæ®‹å·®å­¦ä¹ ï¼Œæå‡ºäº†æ”¹è¿›çš„INP-Former++æ–¹æ³•ï¼Œè¿›ä¸€æ­¥æå‡äº†æ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå¼‚å¸¸æ£€æµ‹æ–¹æ³•å¸¸ä¾èµ–è®­ç»ƒé›†ä¸­çš„æ­£å¸¸å‚è€ƒå›¾åƒè¿›è¡Œæ¯”è¾ƒï¼Œä½†å¤–è§‚å’Œä½ç½®çš„å˜åŒ–ä½¿å¾—å¯¹é½å˜å¾—å›°éš¾ã€‚</li>
<li>INP-Formeræ–¹æ³•ç›´æ¥ä»æµ‹è¯•å›¾åƒä¸­æå–å†…åœ¨æ­£å¸¸åŸå‹ï¼ˆINPsï¼‰æ¥æ£€æµ‹å¼‚å¸¸ï¼Œä¸éœ€è¦å¤–éƒ¨æ­£å¸¸æ€§å‚è€ƒã€‚</li>
<li>INPæå–å™¨é€šè¿‡çº¿æ€§ç»„åˆæ­£å¸¸æ ‡è®°æ¥ä»£è¡¨INPsã€‚</li>
<li>INPä¸€è‡´æ€§æŸå¤±ç¡®ä¿INPsèƒ½å¿ å®ä»£è¡¨æµ‹è¯•å›¾åƒçš„æ­£å¸¸æ€§ã€‚</li>
<li>INP-guidedè§£ç å™¨åˆ©ç”¨INPsåªé‡å»ºæ­£å¸¸æ ‡è®°ï¼Œé‡å»ºè¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°ã€‚</li>
<li>SoftMining Lossåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆä¼˜åŒ–éš¾ä»¥ä¼˜åŒ–çš„æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5b752c6fdcb834ebd6f3fa808a02b96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b6ddbfcd961f66c281772f79031dd20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be4b43ad242a977ed0538f66f3bab015.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98001f37b2b498380406311590aab93e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Chameleon-A-MatMul-Free-Temporal-Convolutional-Network-Accelerator-for-End-to-End-Few-Shot-and-Continual-Learning-from-Sequential-Data"><a href="#Chameleon-A-MatMul-Free-Temporal-Convolutional-Network-Accelerator-for-End-to-End-Few-Shot-and-Continual-Learning-from-Sequential-Data" class="headerlink" title="Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for   End-to-End Few-Shot and Continual Learning from Sequential Data"></a>Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for   End-to-End Few-Shot and Continual Learning from Sequential Data</h2><p><strong>Authors:Douwe den Blanken, Charlotte Frenkel</strong></p>
<p>On-device learning at the edge enables low-latency, private personalization with improved long-term robustness and reduced maintenance costs. Yet, achieving scalable, low-power end-to-end on-chip learning, especially from real-world sequential data with a limited number of examples, is an open challenge. Indeed, accelerators supporting error backpropagation optimize for learning performance at the expense of inference efficiency, while simplified learning algorithms often fail to reach acceptable accuracy targets. In this work, we present Chameleon, leveraging three key contributions to solve these challenges. (i) A unified learning and inference architecture supports few-shot learning (FSL), continual learning (CL) and inference at only 0.5% area overhead to the inference logic. (ii) Long temporal dependencies are efficiently captured with temporal convolutional networks (TCNs), enabling the first demonstration of end-to-end on-chip FSL and CL on sequential data and inference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free compute array allows either matching the power consumption of state-of-the-art inference-only keyword spotting (KWS) accelerators or enabling $4.3\times$ higher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records on Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way 5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots), while maintaining an inference accuracy of 93.3% on the 12-class Google Speech Commands dataset at an extreme-edge power budget of 3.1 $\mu$W. </p>
<blockquote>
<p>åœ¨è®¾å¤‡è¾¹ç¼˜è¿›è¡Œåœ¨çº¿å­¦ä¹ å¯ä»¥å®ç°ä½å»¶è¿Ÿã€éšç§ä¿æŠ¤çš„ä¸ªæ€§åŒ–ï¼ŒåŒæ—¶æé«˜é•¿æœŸç¨³å¥æ€§å¹¶é™ä½ç»´æŠ¤æˆæœ¬ã€‚ç„¶è€Œï¼Œå®ç°å¯æ‰©å±•çš„ã€ä½åŠŸè€—çš„ç«¯åˆ°ç«¯ç‰‡ä¸Šå­¦ä¹ ï¼Œå°¤å…¶æ˜¯ä»ç°å®ä¸–ç•Œä¸­çš„åºåˆ—æ•°æ®ä¸­å­¦ä¹ ä¸”æ ·æœ¬æ•°é‡æœ‰é™ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚å®é™…ä¸Šï¼Œæ”¯æŒåå‘ä¼ æ’­çš„åŠ é€Ÿå™¨ä»¥æ¨ç†æ•ˆç‡ä¸ºä»£ä»·ä¼˜åŒ–äº†å­¦ä¹ æ€§èƒ½ï¼Œè€Œç®€åŒ–çš„å­¦ä¹ ç®—æ³•å¾€å¾€æ— æ³•è¾¾åˆ°å¯æ¥å—çš„å‡†ç¡®æ€§ç›®æ ‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Chameleonï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®è´¡çŒ®æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚ï¼ˆiï¼‰ç»Ÿä¸€çš„å­¦ä¹ å’Œæ¨ç†æ¶æ„æ”¯æŒå°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ã€æŒç»­å­¦ä¹ ï¼ˆCLï¼‰å’Œæ¨ç†ï¼Œä»…å¯¹æ¨ç†é€»è¾‘äº§ç”Ÿ0.5%çš„é¢ç§¯å¼€é”€ã€‚ï¼ˆiiï¼‰é€šè¿‡æ—¶åºå·ç§¯ç½‘ç»œï¼ˆTCNsï¼‰æœ‰æ•ˆåœ°æ•è·äº†é•¿æ—¶é—´ä¾èµ–æ€§ï¼Œå®ç°äº†åºåˆ—æ•°æ®çš„ç«¯åˆ°ç«¯ç‰‡ä¸ŠFSLå’ŒCLçš„é¦–æ¬¡æ¼”ç¤ºä»¥åŠå¯¹16kHzåŸå§‹éŸ³é¢‘çš„æ¨ç†ã€‚ï¼ˆiiiï¼‰ä¸€ç§åŒæ¨¡å¼ã€æ— çŸ©é˜µä¹˜æ³•çš„è®¡ç®—é˜µåˆ—ï¼Œæ—¢å¯ä»¥åŒ¹é…å½“å‰æœ€å…ˆè¿›çš„ä»…ç”¨äºå…³é”®å­—è¯†åˆ«ï¼ˆKWSï¼‰çš„æ¨ç†åŠ é€Ÿå™¨çš„åŠŸè€—ï¼Œåˆå¯ä»¥æé«˜4.3å€çš„è®¡ç®—å³°å€¼ã€‚Chameleoné‡‡ç”¨40çº³ç±³CMOSå·¥è‰ºåˆ¶é€ ï¼Œåœ¨Omniglotæ•°æ®é›†ä¸Šåˆ›é€ äº†ç«¯åˆ°ç«¯ç‰‡ä¸ŠFSLçš„æ–°å‡†ç¡®æ€§è®°å½•ï¼ˆ5è·¯1æ¬¡å°„å‡»96.8%ï¼Œ5è·¯5æ¬¡å°„å‡»98.8%ï¼‰ï¼Œåœ¨æŒç»­å­¦ä¹ æ–¹é¢ï¼ˆå­¦ä¹ 250ä¸ªç±»åˆ«ï¼Œä½¿ç”¨10æ¬¡å°„å‡»çš„æœ€ç»ˆå‡†ç¡®æ€§ä¸º82.2%ï¼‰ï¼ŒåŒæ—¶åœ¨Googleè¯­éŸ³å‘½ä»¤æ•°æ®é›†ä¸Šä¿æŒ12ç±»çš„æ¨ç†å‡†ç¡®æ€§ä¸º93.3%ï¼Œåœ¨æç«¯è¾¹ç¼˜çš„åŠŸç‡é¢„ç®—ä¸º3.1Î¼Wçš„æƒ…å†µä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24852v2">PDF</a> 14 pages, 7 figures; added FSL power consumption measurements at 100   kHz clock speed, fixed typos</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºChameleonæ–¹æ¡ˆï¼Œè§£å†³äº†åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°å®æ—¶å­¦ä¹ çš„æŒ‘æˆ˜ã€‚Chameleoné€šè¿‡ä¸‰ä¸ªå…³é”®è´¡çŒ®å®ç°äº†åœ¨ä½å»¶è¿Ÿã€éšç§ä¿æŠ¤ä¸‹çš„ä¸ªæ€§åŒ–å­¦ä¹ ï¼Œå¹¶æé«˜äº†é•¿æœŸç¨³å¥æ€§å’Œé™ä½äº†ç»´æŠ¤æˆæœ¬ã€‚é¦–å…ˆï¼Œå®ƒæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„å­¦ä¹ å’Œæ¨ç†æ¶æ„ï¼Œæ”¯æŒå°‘é‡å­¦ä¹ ã€æŒç»­å­¦ä¹ å’Œæ¨ç†ï¼Œä»…åœ¨æ¨ç†é€»è¾‘ä¸Šå¢åŠ äº†0.5%çš„é¢ç§¯å¼€é”€ã€‚å…¶æ¬¡ï¼Œåˆ©ç”¨æ—¶åºå·ç§¯ç½‘ç»œï¼ˆTCNsï¼‰æœ‰æ•ˆåœ°æ•æ‰é•¿æœŸæ—¶é—´ä¾èµ–å…³ç³»ï¼Œå®ç°äº†å¯¹é¡ºåºæ•°æ®çš„ç«¯åˆ°ç«¯åœ¨çº¿å°‘é‡å­¦ä¹ å’ŒæŒç»­å­¦ä¹ ä»¥åŠ16kHzåŸå§‹éŸ³é¢‘çš„æ¨ç†ã€‚æœ€åï¼Œä¸€ç§åŒæ¨¡å¼ã€æ— çŸ©é˜µä¹˜æ³•çš„è®¡ç®—é˜µåˆ—å…è®¸åŒ¹é…å½“å‰å…ˆè¿›çš„ä»…æ¨ç†å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰åŠ é€Ÿå™¨çš„åŠŸè€—æˆ–æé«˜4.3å€å³°å€¼GOPSã€‚åœ¨Omniglotæ•°æ®é›†ä¸Šï¼ŒChameleonåœ¨ç«¯åˆ°ç«¯çš„åœ¨çº¿å°‘é‡å­¦ä¹ ï¼ˆ96.8%ï¼Œ5é€‰1ï¼›98.8%ï¼Œ5é€‰5ï¼‰å’ŒæŒç»­å­¦ä¹ ï¼ˆå­¦ä¹ 250ç±»ï¼Œæ¯æ¬¡å­¦ä¹ 10ä¸ªæ ·æœ¬çš„æœ€ç»ˆå‡†ç¡®åº¦ä¸º82.2%ï¼‰æ–¹é¢åˆ›é€ äº†æ–°çš„å‡†ç¡®åº¦è®°å½•ï¼ŒåŒæ—¶åœ¨Googleè¯­éŸ³å‘½ä»¤æ•°æ®é›†ä¸Šä¿æŒ93.3%çš„æ¨ç†å‡†ç¡®åº¦ï¼Œåœ¨æç«¯è¾¹ç¼˜è®¾å¤‡çš„åŠŸè€—é¢„ç®—ä¸º3.1Î¼Wçš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Chameleonæ–¹æ¡ˆé€šè¿‡ç»Ÿä¸€çš„å­¦ä¹ å’Œæ¨ç†æ¶æ„æ”¯æŒå°‘é‡å­¦ä¹ ã€æŒç»­å­¦ä¹ å’Œæ¨ç†ï¼Œä¸”ä»…å¢åŠ å¾®å°çš„é¢ç§¯å¼€é”€ã€‚</li>
<li>åˆ©ç”¨æ—¶åºå·ç§¯ç½‘ç»œï¼ˆTCNsï¼‰æœ‰æ•ˆæ•æ‰é•¿æœŸæ—¶é—´ä¾èµ–å…³ç³»ï¼Œå®ç°åœ¨èŠ¯ç‰‡ä¸Šå¯¹é¡ºåºæ•°æ®çš„å°‘é‡å­¦ä¹ å’ŒæŒç»­å­¦ä¹ çš„é¦–æ¬¡æ¼”ç¤ºã€‚</li>
<li>æå‡ºä¸€ç§åŒæ¨¡å¼ã€æ— çŸ©é˜µä¹˜æ³•çš„è®¡ç®—é˜µåˆ—ï¼Œå¯åœ¨ä¿æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶æé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>åœ¨Omniglotæ•°æ®é›†ä¸Šå®ç°äº†ç«¯åˆ°ç«¯çš„åœ¨çº¿å°‘é‡å­¦ä¹ å’ŒæŒç»­å­¦ä¹ çš„é«˜å‡†ç¡®åº¦ã€‚</li>
<li>åœ¨Googleè¯­éŸ³å‘½ä»¤æ•°æ®é›†ä¸Šä¿æŒäº†é«˜æ¨ç†å‡†ç¡®åº¦ï¼ŒåŒæ—¶æ»¡è¶³äº†æç«¯è¾¹ç¼˜è®¾å¤‡çš„ä½åŠŸè€—è¦æ±‚ã€‚</li>
<li>Chameleonæ–¹æ¡ˆåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œä¸ºæœªæ¥çš„è¾¹ç¼˜å­¦ä¹ æä¾›äº†é‡è¦å¯ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ed1d61b6b495628eb42349ec37022c35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0023412cecb155477cf18926cd7b631.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa269f0b70426dfcd2874486ebcf9c7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-293d57f648de833cfb006a28207d5017.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17e6205da49b6124f9db30bc3b0dd5b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-494009e11d94f057a1ba81f09a60502a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AdaReasoner-Adaptive-Reasoning-Enables-More-Flexible-Thinking-in-Large-Language-Models"><a href="#AdaReasoner-Adaptive-Reasoning-Enables-More-Flexible-Thinking-in-Large-Language-Models" class="headerlink" title="AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking in Large   Language Models"></a>AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking in Large   Language Models</h2><p><strong>Authors:Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang</strong></p>
<p>LLMs often need effective configurations, like temperature and reasoning steps, to handle tasks requiring sophisticated reasoning and problem-solving, ranging from joke generation to mathematical reasoning. Existing prompting approaches usually adopt general-purpose, fixed configurations that work â€˜well enoughâ€™ across tasks but seldom achieve task-specific optimality. To address this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM to automate adaptive reasoning configurations for tasks requiring different types of thinking. AdaReasoner is trained using a reinforcement learning (RL) framework, combining a factorized action space with a targeted exploration strategy, along with a pretrained reward model to optimize the policy model for reasoning configurations with only a few-shot guide. AdaReasoner is backed by theoretical guarantees and experiments of fast convergence and a sublinear policy gap. Across six different LLMs and a variety of reasoning tasks, it consistently outperforms standard baselines, preserves out-of-distribution robustness, and yield gains on knowledge-intensive tasks through tailored prompts. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸éœ€è¦æœ‰æ•ˆçš„é…ç½®ï¼Œå¦‚æ¸©åº¦å’Œæ¨ç†æ­¥éª¤ï¼Œæ¥å¤„ç†ä»ç¬‘è¯ç”Ÿæˆåˆ°æ•°å­¦æ¨ç†ç­‰éœ€è¦å¤æ‚æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›çš„ä»»åŠ¡ã€‚ç°æœ‰çš„æç¤ºæ–¹æ³•é€šå¸¸é‡‡ç”¨é€šç”¨ã€å›ºå®šçš„é…ç½®ï¼Œè¿™äº›é…ç½®åœ¨å„é¡¹ä»»åŠ¡ä¸­â€œè¡¨ç°è¶³å¤Ÿå¥½â€ï¼Œä½†å¾ˆå°‘å®ç°é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†AdaReasonerï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹çš„é€šç”¨æ’ä»¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–é€‚åº”éœ€è¦ä¸åŒç±»å‹æ€è€ƒçš„æ¨ç†ä»»åŠ¡çš„é…ç½®ã€‚AdaReasonerä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œç»“åˆåˆ†è§£çš„åŠ¨ä½œç©ºé—´å’Œæœ‰é’ˆå¯¹æ€§çš„æ¢ç´¢ç­–ç•¥ï¼Œä»¥åŠé¢„è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œä»¥ä¼˜åŒ–ç­–ç•¥æ¨¡å‹è¿›è¡Œæ¨ç†é…ç½®ï¼Œåªéœ€å°‘æ•°é•œå¤´æŒ‡å¯¼ã€‚AdaReasoneræœ‰ç†è®ºä¿è¯å’Œå®éªŒæ”¯æŒå¿«é€Ÿæ”¶æ•›å’Œæ¬¡çº¿æ€§ç­–ç•¥å·®è·ã€‚åœ¨å…­ä¸ªä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šç§æ¨ç†ä»»åŠ¡ä¸Šï¼Œå®ƒå§‹ç»ˆè¶…è¶Šæ ‡å‡†åŸºçº¿ï¼Œä¿æŒè¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„ç¨³å¥æ€§ï¼Œå¹¶é€šè¿‡å®šåˆ¶æç¤ºåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šè·å¾—æ”¶ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17312v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LLMsåœ¨å¤„ç†éœ€è¦å¤æ‚æ¨ç†å’Œé—®é¢˜è§£å†³çš„ä»»åŠ¡æ—¶ï¼Œéœ€è¦æœ‰æ•ˆçš„é…ç½®ï¼Œå¦‚æ¸©åº¦å’Œæ¨ç†æ­¥éª¤ã€‚ç°æœ‰æç¤ºæ–¹æ³•é€šå¸¸é‡‡ç”¨é€šç”¨ã€å›ºå®šçš„é…ç½®ï¼Œè¿™äº›é…ç½®åœ¨è·¨ä»»åŠ¡æ—¶è¡¨ç°â€œè¶³å¤Ÿå¥½â€ï¼Œä½†å¾ˆå°‘è¾¾åˆ°ä»»åŠ¡ç‰¹å®šçš„æœ€ä¼˜çŠ¶æ€ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†AdaReasonerï¼Œè¿™æ˜¯ä¸€ç§ä¸ºä»»ä½•LLMè®¾è®¡çš„LLMæ— å…³æ’ä»¶ï¼Œå¯è‡ªåŠ¨é€‚åº”éœ€è¦ä¸åŒç±»å‹æ€è€ƒçš„æ¨ç†ä»»åŠ¡çš„é…ç½®ã€‚AdaReasonerä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œç»“åˆåˆ†è§£åŠ¨ä½œç©ºé—´ä¸æœ‰é’ˆå¯¹æ€§çš„æ¢ç´¢ç­–ç•¥ï¼Œä»¥åŠé¢„è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹æ¥ä¼˜åŒ–ç­–ç•¥æ¨¡å‹ï¼Œä»…é€šè¿‡å‡ æ¬¡æç¤ºå°±èƒ½å¯¹æ¨ç†é…ç½®è¿›è¡Œä¼˜åŒ–ã€‚AdaReasonerå¾—åˆ°äº†ç†è®ºä¸Šçš„ä¿è¯å’Œå®éªŒç»“æœçš„å¿«é€Ÿæ”¶æ•›ä»¥åŠç­–ç•¥å·®è·çš„æ¬¡çº¿æ€§è¯æ˜ã€‚åœ¨ä¸åŒLLMså’Œå„ç§æ¨ç†ä»»åŠ¡ä¸Šï¼Œå®ƒå§‹ç»ˆä¼˜äºæ ‡å‡†åŸºçº¿ï¼Œä¿æŒå¯¹åˆ†å¸ƒå¤–çš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šé€šè¿‡å®šåˆ¶æç¤ºè·å¾—æ”¶ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMséœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æœ‰æ•ˆé…ç½®ä»¥å¤„ç†å¤æ‚æ¨ç†å’Œé—®é¢˜è§£å†³ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æç¤ºæ–¹æ³•é‡‡ç”¨é€šç”¨é…ç½®ï¼Œéš¾ä»¥è¾¾åˆ°ä»»åŠ¡ç‰¹å®šæœ€ä¼˜çŠ¶æ€ã€‚</li>
<li>AdaReasoneræ˜¯ä¸€ç§LLMæ— å…³çš„æ’ä»¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–é€‚åº”ä¸åŒä»»åŠ¡çš„æ¨ç†é…ç½®ã€‚</li>
<li>AdaReasonerä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è®­ç»ƒï¼Œç»“åˆåˆ†è§£åŠ¨ä½œç©ºé—´ä¸æ¢ç´¢ç­–ç•¥åŠé¢„è®­ç»ƒå¥–åŠ±æ¨¡å‹ä¼˜åŒ–ç­–ç•¥æ¨¡å‹ã€‚</li>
<li>AdaReasonerå…·æœ‰ç†è®ºä¿è¯å’Œå®éªŒç»“æœçš„å¿«é€Ÿæ”¶æ•›å’Œæ¬¡çº¿æ€§ç­–ç•¥å·®è·è¯æ˜ã€‚</li>
<li>AdaReasoneråœ¨ä¸åŒLLMså’Œå„ç§æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºæ ‡å‡†åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6680c2d762c362d1876cf6323feeae2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c925ca695a570faf035e9318317b1afa.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Mitigating-Knowledge-Discrepancies-among-Multiple-Datasets-for-Task-agnostic-Unified-Face-Alignment"><a href="#Mitigating-Knowledge-Discrepancies-among-Multiple-Datasets-for-Task-agnostic-Unified-Face-Alignment" class="headerlink" title="Mitigating Knowledge Discrepancies among Multiple Datasets for   Task-agnostic Unified Face Alignment"></a>Mitigating Knowledge Discrepancies among Multiple Datasets for   Task-agnostic Unified Face Alignment</h2><p><strong>Authors:Jiahao Xia, Min Xu, Wenjian Huang, Jianguo Zhang, Haimin Zhang, Chunxia Xiao</strong></p>
<p>Despite the similar structures of human faces, existing face alignment methods cannot learn unified knowledge from multiple datasets with different landmark annotations. The limited training samples in a single dataset commonly result in fragile robustness in this field. To mitigate knowledge discrepancies among different datasets and train a task-agnostic unified face alignment (TUFA) framework, this paper presents a strategy to unify knowledge from multiple datasets. Specifically, we calculate a mean face shape for each dataset. To explicitly align these mean shapes on an interpretable plane based on their semantics, each shape is then incorporated with a group of semantic alignment embeddings. The 2D coordinates of these aligned shapes can be viewed as the anchors of the plane. By encoding them into structure prompts and further regressing the corresponding facial landmarks using image features, a mapping from the plane to the target faces is finally established, which unifies the learning target of different datasets. Consequently, multiple datasets can be utilized to boost the generalization ability of the model. The successful mitigation of discrepancies also enhances the efficiency of knowledge transferring to a novel dataset, significantly boosts the performance of few-shot face alignment. Additionally, the interpretable plane endows TUFA with a task-agnostic characteristic, enabling it to locate landmarks unseen during training in a zero-shot manner. Extensive experiments are carried on seven benchmarks and the results demonstrate an impressive improvement in face alignment brought by knowledge discrepancies mitigation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Jiahao-UTS/TUFA">https://github.com/Jiahao-UTS/TUFA</a>. </p>
<blockquote>
<p>å°½ç®¡äººè„¸ç»“æ„ç›¸ä¼¼ï¼Œä½†ç°æœ‰çš„é¢éƒ¨å¯¹é½æ–¹æ³•æ— æ³•ä»å…·æœ‰ä¸åŒç‰¹å¾ç‚¹æ ‡æ³¨çš„å¤šä¸ªæ•°æ®é›†ä¸­å­¦ä¹ ç»Ÿä¸€çš„çŸ¥è¯†ã€‚å•ä¸€æ•°æ®é›†ä¸­çš„æœ‰é™è®­ç»ƒæ ·æœ¬é€šå¸¸ä¼šå¯¼è‡´è¯¥é¢†åŸŸçš„è„†å¼±ç¨³å¥æ€§ã€‚ä¸ºäº†ç¼“è§£ä¸åŒæ•°æ®é›†ä¹‹é—´çš„çŸ¥è¯†å·®å¼‚å¹¶è®­ç»ƒä¸€ä¸ªä»»åŠ¡é€šç”¨çš„ç»Ÿä¸€é¢éƒ¨å¯¹é½ï¼ˆTUFAï¼‰æ¡†æ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç­–ç•¥æ¥ç»Ÿä¸€å¤šä¸ªæ•°æ®é›†çš„çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªæ•°æ®é›†è®¡ç®—å¹³å‡è„¸éƒ¨å½¢çŠ¶ã€‚ä¸ºäº†æ ¹æ®è¯­ä¹‰åœ¨å¯è§£é‡Šå¹³é¢ä¸Šæ˜¾å¼å¯¹é½è¿™äº›å¹³å‡å½¢çŠ¶ï¼Œç„¶åå°†æ¯ä¸ªå½¢çŠ¶ä¸ä¸€ç»„è¯­ä¹‰å¯¹é½åµŒå…¥ç›¸ç»“åˆã€‚è¿™äº›å¯¹é½å½¢çŠ¶çš„2Dåæ ‡å¯ä»¥è¢«è§†ä¸ºå¹³é¢çš„é”šç‚¹ã€‚é€šè¿‡å°†å®ƒä»¬ç¼–ç ä¸ºç»“æ„æç¤ºå¹¶ä½¿ç”¨å›¾åƒç‰¹å¾è¿›ä¸€æ­¥å›å½’ç›¸åº”çš„é¢éƒ¨ç‰¹å¾ç‚¹ï¼Œæœ€ç»ˆå»ºç«‹äº†ä»å¹³é¢åˆ°ç›®æ ‡é¢éƒ¨çš„æ˜ å°„ï¼Œè¿™ç»Ÿä¸€äº†ä¸åŒæ•°æ®é›†çš„å­¦ä¹ ç›®æ ‡ã€‚å› æ­¤ï¼Œå¯ä»¥åˆ©ç”¨å¤šä¸ªæ•°æ®é›†æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆåŠŸç¼“è§£å·®å¼‚è¿˜å¯ä»¥æé«˜çŸ¥è¯†è½¬ç§»åˆ°æ–°æ•°æ®é›†çš„æ•ˆç‡ï¼Œæå¤§åœ°æé«˜äº†å°‘æ ·æœ¬é¢éƒ¨å¯¹é½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯è§£é‡Šçš„å¹³é¢èµ‹äºˆäº†TUFAä»»åŠ¡é€šç”¨çš„ç‰¹æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿä»¥é›¶æ ·æœ¬çš„æ–¹å¼å®šä½è®­ç»ƒæœŸé—´æœªè§è¿‡çš„ç‰¹å¾ç‚¹ã€‚åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œé€šè¿‡ç¼“è§£çŸ¥è¯†å·®å¼‚ï¼Œé¢éƒ¨å¯¹é½å¸¦æ¥äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ”¹è¿›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Jiahao-UTS/TUFA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Jiahao-UTS/TUFAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22359v2">PDF</a> 24 Pages, 9 Figures, accepted to IJCV-2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»Ÿä¸€çŸ¥è¯†ä»å¤šä¸ªæ•°æ®é›†çš„ç­–ç•¥ï¼Œä»¥è§£å†³ä¸åŒæ•°æ®é›†é—´çŸ¥è¯†å·®å¼‚çš„é—®é¢˜ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªä»»åŠ¡æ— å…³çš„ç»Ÿä¸€é¢éƒ¨å¯¹é½ï¼ˆTUFAï¼‰æ¡†æ¶ã€‚é€šè¿‡è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„å¹³å‡é¢éƒ¨å½¢çŠ¶ï¼Œå¹¶ç»“åˆè¯­ä¹‰å¯¹é½åµŒå…¥ï¼Œæ˜¾å¼åœ°å°†è¿™äº›å¹³å‡å½¢çŠ¶å¯¹é½åœ¨ä¸€ä¸ªå¯è§£é‡Šçš„å¹³é¢ä¸Šã€‚è¿™å»ºç«‹äº†ä»å¹³é¢åˆ°ç›®æ ‡é¢éƒ¨çš„æ˜ å°„ï¼Œç»Ÿä¸€äº†ä¸åŒæ•°æ®é›†çš„å­¦ä¹ ç›®æ ‡ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç­–ç•¥æˆåŠŸç¼“è§£äº†çŸ¥è¯†å·®å¼‚ï¼Œæé«˜äº†çŸ¥è¯†è½¬ç§»åˆ°æ–°æ•°æ®é›†çš„æ•ˆç‡ï¼Œå¹¶æ˜¾è‘—æå‡äº†å°‘æ ·æœ¬é¢éƒ¨å¯¹é½çš„æ€§èƒ½ã€‚TUFAæ¡†æ¶å…·æœ‰ä»»åŠ¡æ— å…³çš„ç‰¹æ€§ï¼Œèƒ½åœ¨é›¶æ ·æœ¬çš„æƒ…å†µä¸‹å®šä½è®­ç»ƒæ—¶æœªè§è¿‡çš„åœ°æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºç»Ÿä¸€çŸ¥è¯†ä»å¤šä¸ªæ•°æ®é›†çš„ç­–ç•¥ï¼Œè§£å†³ä¸åŒæ•°æ®é›†é—´çŸ¥è¯†å·®å¼‚çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡è®¡ç®—å¹³å‡é¢éƒ¨å½¢çŠ¶å¹¶ç»“åˆè¯­ä¹‰å¯¹é½åµŒå…¥ï¼Œæ˜¾å¼å¯¹é½è¿™äº›å½¢çŠ¶åœ¨ä¸€ä¸ªå¯è§£é‡Šçš„å¹³é¢ä¸Šã€‚</li>
<li>å»ºç«‹ä»å¹³é¢åˆ°ç›®æ ‡é¢éƒ¨çš„æ˜ å°„ï¼Œç»Ÿä¸€ä¸åŒæ•°æ®é›†çš„å­¦ä¹ ç›®æ ‡ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æˆåŠŸç¼“è§£çŸ¥è¯†å·®å¼‚ï¼Œæé«˜çŸ¥è¯†è½¬ç§»åˆ°æ–°æ•°æ®é›†çš„æ•ˆç‡ã€‚</li>
<li>æå‡å°‘æ ·æœ¬é¢éƒ¨å¯¹é½çš„æ€§èƒ½ã€‚</li>
<li>TUFAæ¡†æ¶å…·æœ‰ä»»åŠ¡æ— å…³çš„ç‰¹æ€§ï¼Œèƒ½åœ¨é›¶æ ·æœ¬çš„æƒ…å†µä¸‹å®šä½è®­ç»ƒæ—¶æœªè§è¿‡çš„åœ°æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22359">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26c3d17f1f9488753e776ba8edadb46d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-023a329216c4e7619c16f5d1ed3943f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ca1c19389d3f2e16668e6fe8eb3aa04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3abbaea0314092aacca627ac245184fb.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Edit-Transfer-Learning-Image-Editing-via-Vision-In-Context-Relations"><a href="#Edit-Transfer-Learning-Image-Editing-via-Vision-In-Context-Relations" class="headerlink" title="Edit Transfer: Learning Image Editing via Vision In-Context Relations"></a>Edit Transfer: Learning Image Editing via Vision In-Context Relations</h2><p><strong>Authors:Lan Chen, Qi Mao, Yuchao Gu, Mike Zheng Shou</strong></p>
<p>We introduce a new setting, Edit Transfer, where a model learns a transformation from just a single source-target example and applies it to a new query image. While text-based methods excel at semantic manipulations through textual prompts, they often struggle with precise geometric details (e.g., poses and viewpoint changes). Reference-based editing, on the other hand, typically focuses on style or appearance and fails at non-rigid transformations. By explicitly learning the editing transformation from a source-target pair, Edit Transfer mitigates the limitations of both text-only and appearance-centric references. Drawing inspiration from in-context learning in large language models, we propose a visual relation in-context learning paradigm, building upon a DiT-based text-to-image model. We arrange the edited example and the query image into a unified four-panel composite, then apply lightweight LoRA fine-tuning to capture complex spatial transformations from minimal examples. Despite using only 42 training samples, Edit Transfer substantially outperforms state-of-the-art TIE and RIE methods on diverse non-rigid scenarios, demonstrating the effectiveness of few-shot visual relation learning. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°è®¾ç½®ï¼Œåä¸ºâ€œç¼–è¾‘ä¼ è¾“â€ï¼ˆEdit Transferï¼‰ï¼Œåœ¨è¯¥è®¾ç½®ä¸­ï¼Œæ¨¡å‹ä»…ä»ä¸€ä¸ªæºç›®æ ‡ç¤ºä¾‹ä¸­å­¦ä¹ è½¬æ¢ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ–°çš„æŸ¥è¯¢å›¾åƒã€‚è™½ç„¶åŸºäºæ–‡æœ¬çš„æ–¹æ³•åœ¨é€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œè¯­ä¹‰æ“ä½œæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬é€šå¸¸éš¾ä»¥å¤„ç†ç²¾ç¡®çš„å‡ ä½•ç»†èŠ‚ï¼ˆä¾‹å¦‚å§¿åŠ¿å’Œè§†ç‚¹å˜åŒ–ï¼‰ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŸºäºå‚è€ƒçš„ç¼–è¾‘é€šå¸¸ä¾§é‡äºé£æ ¼æˆ–å¤–è§‚ï¼Œè€Œåœ¨éåˆšä½“è½¬æ¢æ–¹é¢è¡¨ç°ä¸ä½³ã€‚é€šè¿‡ä»æºç›®æ ‡å¯¹ä¸­å­¦ä¹ ç¼–è¾‘è½¬æ¢ï¼ŒEdit Transferç¼“è§£äº†ä»…ä½¿ç”¨æ–‡æœ¬å’Œå¤–è§‚ä¸­å¿ƒå‚è€ƒçš„å±€é™æ€§ã€‚æˆ‘ä»¬ä»å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­æ±²å–çµæ„Ÿï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºDiTæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è§†è§‰å…³ç³»ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ã€‚æˆ‘ä»¬å°†ç¼–è¾‘ç¤ºä¾‹å’ŒæŸ¥è¯¢å›¾åƒæ’åˆ—æˆä¸€ä¸ªç»Ÿä¸€çš„å››é¢æ¿ç»„åˆï¼Œç„¶ååº”ç”¨è½»é‡çº§çš„LoRAå¾®è°ƒæŠ€æœ¯ï¼Œä»æœ€å°‘çš„ç¤ºä¾‹ä¸­å­¦ä¹ å¤æ‚çš„ç©ºé—´è½¬æ¢ã€‚å°½ç®¡åªä½¿ç”¨äº†42ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œä½†åœ¨å¤šç§éåˆšä½“åœºæ™¯ä¸‹ï¼ŒEdit Transferå¤§å¹…è¶…è¶Šäº†æœ€æ–°çš„TIEå’ŒRIEæ–¹æ³•ï¼Œè¯æ˜äº†å°‘è§†è§‰å…³ç³»å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13327v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¼–è¾‘è½¬ç§»æŠ€æœ¯é€šè¿‡å­¦ä¹ ä»å•ä¸€æºç›®æ ‡ç¤ºä¾‹çš„è½¬æ¢ï¼Œå¹¶åº”ç”¨äºæ–°æŸ¥è¯¢å›¾åƒï¼Œå®ç°äº†å›¾åƒç¼–è¾‘çš„æ–°è®¾ç½®ã€‚è¯¥æŠ€æœ¯å¼¥è¡¥äº†çº¯æ–‡æœ¬å’Œå¤–è§‚ä¸ºä¸­å¿ƒçš„å‚è€ƒæ–¹æ³•çš„å±€é™æ€§ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„ç©ºé—´è½¬æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¼–è¾‘è½¬ç§»æŠ€æœ¯é€šè¿‡å­¦ä¹ ä»æºåˆ°ç›®æ ‡çš„è½¬æ¢ï¼Œå¹¶åº”ç”¨äºæ–°æŸ¥è¯¢å›¾åƒï¼Œåˆ›é€ äº†æ–°çš„å›¾åƒç¼–è¾‘è®¾ç½®ã€‚</li>
<li>ä¼ ç»Ÿæ–‡æœ¬æ–¹æ³•å’ŒåŸºäºå‚è€ƒçš„ç¼–è¾‘æ–¹æ³•åœ¨ç‰¹å®šæƒ…å†µä¸‹å­˜åœ¨å±€é™æ€§ï¼Œè€Œç¼–è¾‘è½¬ç§»æŠ€æœ¯å¯ä»¥å…‹æœè¿™äº›å±€é™æ€§ã€‚</li>
<li>ç¼–è¾‘è½¬ç§»æŠ€æœ¯èƒ½å¤Ÿå¤„ç†å¤æ‚çš„ç©ºé—´è½¬æ¢ï¼Œå¦‚å§¿åŠ¿å’Œè§†è§’å˜åŒ–ã€‚</li>
<li>è¯¥æŠ€æœ¯ä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ è§†è§‰å…³ç³»ï¼Œå¹¶åº”ç”¨äºå›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>ä½¿ç”¨ä»…42ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œç¼–è¾‘è½¬ç§»æŠ€æœ¯åœ¨å¤šç§éåˆšæ€§åœºæ™¯ä¸Šæ˜¾è‘—ä¼˜äºå½“å‰å…ˆè¿›çš„TIEå’ŒRIEæ–¹æ³•ã€‚</li>
<li>ç¼–è¾‘è½¬ç§»æŠ€æœ¯å—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¸Šä¸‹æ–‡å­¦ä¹ çš„å¯å‘ï¼Œå»ºç«‹äº†åŸºäºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è§†è§‰å…³ç³»ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f696d0abc332980cc2e65ec059b04686.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd2d433aae67ac0e2c4865eddbce06e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70d35a37e025975f895ccee2ddeb0a6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93737b98caefc0a3fa940e57236f0217.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f4c77fe1ab0d654a5570e7b5ea52361.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01de7e1045a60242bfe0d9b967cb3457.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TabNSA-Native-Sparse-Attention-for-Efficient-Tabular-Data-Learning"><a href="#TabNSA-Native-Sparse-Attention-for-Efficient-Tabular-Data-Learning" class="headerlink" title="TabNSA: Native Sparse Attention for Efficient Tabular Data Learning"></a>TabNSA: Native Sparse Attention for Efficient Tabular Data Learning</h2><p><strong>Authors:Ali Eslamian, Qiang Cheng</strong></p>
<p>Tabular data poses unique challenges for deep learning due to its heterogeneous feature types, lack of spatial structure, and often limited sample sizes. We propose TabNSA, a novel deep learning framework that integrates Native Sparse Attention (NSA) with a TabMixer backbone to efficiently model tabular data. TabNSA tackles computational and representational challenges by dynamically focusing on relevant feature subsets per instance. The NSA module employs a hierarchical sparse attention mechanism, including token compression, selective preservation, and localized sliding windows, to significantly reduce the quadratic complexity of standard attention operations while addressing feature heterogeneity. Complementing this, the TabMixer backbone captures complex, non-linear dependencies through parallel multilayer perceptron (MLP) branches with independent parameters. These modules are synergistically combined via element-wise summation and mean pooling, enabling TabNSA to model both global context and fine-grained interactions. Extensive experiments across supervised and transfer learning settings show that TabNSA consistently outperforms state-of-the-art deep learning models. Furthermore, by augmenting TabNSA with a fine-tuned large language model (LLM), we enable it to effectively address Few-Shot Learning challenges through language-guided generalization on diverse tabular benchmarks. </p>
<blockquote>
<p>è¡¨æ ¼æ•°æ®å› å…¶å¤šæ ·åŒ–çš„ç‰¹å¾ç±»å‹ã€ç¼ºä¹ç©ºé—´ç»“æ„ä»¥åŠé€šå¸¸æœ‰é™çš„æ ·æœ¬é‡è€Œç»™æ·±åº¦å­¦ä¹ å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†TabNSAï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé›†æˆäº†åŸç”Ÿç¨€ç–æ³¨æ„åŠ›ï¼ˆNSAï¼‰å’ŒTabMixeréª¨å¹²ç½‘ï¼Œä»¥æœ‰æ•ˆåœ°å¯¹è¡¨æ ¼æ•°æ®è¿›è¡Œå»ºæ¨¡ã€‚TabNSAé€šè¿‡åŠ¨æ€å…³æ³¨æ¯ä¸ªå®ä¾‹çš„ç›¸å…³ç‰¹å¾å­é›†æ¥è§£å†³è®¡ç®—å’Œè¡¨å¾æŒ‘æˆ˜ã€‚NSAæ¨¡å—é‡‡ç”¨åˆ†å±‚ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬ä»¤ç‰Œå‹ç¼©ã€é€‰æ‹©æ€§ä¿ç•™å’Œå±€éƒ¨æ»‘åŠ¨çª—å£ï¼Œä»¥æ˜¾è‘—é™ä½æ ‡å‡†æ³¨æ„åŠ›æ“ä½œçš„äºŒæ¬¡å¤æ‚æ€§ï¼ŒåŒæ—¶è§£å†³ç‰¹å¾å¼‚è´¨æ€§é—®é¢˜ã€‚ä½œä¸ºè¡¥å……ï¼ŒTabMixeréª¨å¹²ç½‘é€šè¿‡å…·æœ‰ç‹¬ç«‹å‚æ•°çš„å¹¶è¡Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰åˆ†æ”¯æ•è·å¤æ‚çš„éçº¿æ€§ä¾èµ–å…³ç³»ã€‚è¿™äº›æ¨¡å—é€šè¿‡é€å…ƒç´ æ±‚å’Œå’Œå¹³å‡æ± åŒ–è¿›è¡ŒååŒç»„åˆï¼Œä½¿TabNSAèƒ½å¤ŸåŒæ—¶å¯¹å…¨å±€ä¸Šä¸‹æ–‡å’Œç²¾ç»†äº¤äº’è¿›è¡Œå»ºæ¨¡ã€‚åœ¨ç›‘ç£å­¦ä¹ å’Œè¿ç§»å­¦ä¹ ç¯å¢ƒä¸‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTabNSAå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç”¨å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºTabNSAï¼Œæˆ‘ä»¬ä½¿å…¶èƒ½å¤Ÿé€šè¿‡è¯­è¨€å¼•å¯¼åœ¨å¤šç§è¡¨æ ¼åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œæ³›åŒ–ï¼Œä»è€Œæœ‰æ•ˆè§£å†³å°æ ·æœ¬å­¦ä¹ æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09850v2">PDF</a> 26 pages, 11 tables</p>
<p><strong>Summary</strong></p>
<p>TabNSAæ˜¯ä¸€ç§é’ˆå¯¹è¡¨æ ¼æ•°æ®çš„æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆåŸç”Ÿç¨€ç–æ³¨æ„åŠ›ï¼ˆNSAï¼‰å’ŒTabMixeréª¨å¹²ç½‘ï¼Œå®ç°å¯¹è¡¨æ ¼æ•°æ®çš„é«˜æ•ˆå»ºæ¨¡ã€‚å®ƒè§£å†³äº†è®¡ç®—å’Œè¡¨å¾æŒ‘æˆ˜ï¼Œé€šè¿‡åŠ¨æ€å…³æ³¨æ¯ä¸ªå®ä¾‹çš„ç›¸å…³ç‰¹å¾å­é›†æ¥å‡å°‘æ ‡å‡†æ³¨æ„åŠ›æ“ä½œçš„äºŒæ¬¡å¤æ‚æ€§ï¼Œå¹¶å¤„ç†ç‰¹å¾å¼‚è´¨æ€§ã€‚åœ¨ç›‘ç£å’Œè¿ç§»å­¦ä¹ ç¯å¢ƒä¸­è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTabNSAæŒç»­è¶…è¶Šæœ€æ–°æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¡¨ç°ã€‚é€šè¿‡ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒç›¸ç»“åˆï¼ŒTabNSAèƒ½å¤Ÿæœ‰æ•ˆè§£å†³å°æ ·å­¦ä¹ æŒ‘æˆ˜ï¼Œåœ¨å¤šæ ·åŒ–çš„è¡¨æ ¼åŸºå‡†æµ‹è¯•ä¸Šå®ç°è¯­è¨€å¼•å¯¼æ³›åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TabNSAæ˜¯ä¸€ä¸ªé’ˆå¯¹è¡¨æ ¼æ•°æ®çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†åŸç”Ÿç¨€ç–æ³¨æ„åŠ›ï¼ˆNSAï¼‰å’ŒTabMixeréª¨å¹²ç½‘ã€‚</li>
<li>NSAæ¨¡å—é‡‡ç”¨åˆ†å±‚ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒ…æ‹¬ä»¤ç‰Œå‹ç¼©ã€é€‰æ‹©æ€§ä¿ç•™å’Œå±€éƒ¨æ»‘åŠ¨çª—å£ï¼Œä»¥å‡å°‘æ ‡å‡†æ³¨æ„åŠ›æ“ä½œçš„äºŒæ¬¡å¤æ‚æ€§ï¼Œå¹¶å¤„ç†ç‰¹å¾å¼‚è´¨æ€§ã€‚</li>
<li>TabMixeréª¨å¹²ç½‘é€šè¿‡å¹¶è¡Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰åˆ†æ”¯æ•è·å¤æ‚çš„éçº¿æ€§ä¾èµ–å…³ç³»ã€‚</li>
<li>TabNSAé€šè¿‡åŠ¨æ€å…³æ³¨ç›¸å…³ç‰¹å¾å­é›†ï¼Œè§£å†³äº†è¡¨æ ¼æ•°æ®å¸¦æ¥çš„è®¡ç®—å’Œè¡¨å¾æŒ‘æˆ˜ã€‚</li>
<li>TabNSAåœ¨ç›‘ç£å’Œè¿ç§»å­¦ä¹ ç¯å¢ƒä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒæŒç»­è¶…è¶Šæœ€æ–°æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>é€šè¿‡ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆï¼ŒTabNSAèƒ½å¤Ÿè§£å†³å°æ ·å­¦ä¹ æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b2af6fc3bb712b4e865d8d3562858c28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-241c2f9e4a454f917da8fd980e418ddb.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-81188eccb11f963c31609e5b758aae9f.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Subjective Camera Bridging Human Cognition and Visual Reconstruction   through Sequence-Aware Sketch-Guided Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e906d407c81f51945c0316d5fc30ddd0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Agent.xpu Efficient Scheduling of Agentic LLM Workloads on   Heterogeneous SoC
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
