<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Supervised Diffusion-Model-Based PET Image Reconstruction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-51ed996d0656e43ac56aed689e67752c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-03-æ›´æ–°"><a href="#2025-07-03-æ›´æ–°" class="headerlink" title="2025-07-03 æ›´æ–°"></a>2025-07-03 æ›´æ–°</h1><h2 id="Supervised-Diffusion-Model-Based-PET-Image-Reconstruction"><a href="#Supervised-Diffusion-Model-Based-PET-Image-Reconstruction" class="headerlink" title="Supervised Diffusion-Model-Based PET Image Reconstruction"></a>Supervised Diffusion-Model-Based PET Image Reconstruction</h2><p><strong>Authors:George Webber, Alexander Hammers, Andrew P King, Andrew J Reader</strong></p>
<p>Diffusion models (DMs) have recently been introduced as a regularizing prior for PET image reconstruction, integrating DMs trained on high-quality PET images with unsupervised schemes that condition on measured data. While these approaches have potential generalization advantages due to their independence from the scanner geometry and the injected activity level, they forgo the opportunity to explicitly model the interaction between the DM prior and noisy measurement data, potentially limiting reconstruction accuracy. To address this, we propose a supervised DM-based algorithm for PET reconstruction. Our method enforces the non-negativity of PETâ€™s Poisson likelihood model and accommodates the wide intensity range of PET images. Through experiments on realistic brain PET phantoms, we demonstrate that our approach outperforms or matches state-of-the-art deep learning-based methods quantitatively across a range of dose levels. We further conduct ablation studies to demonstrate the benefits of the proposed components in our model, as well as its dependence on training data, parameter count, and number of diffusion steps. Additionally, we show that our approach enables more accurate posterior sampling than unsupervised DM-based methods, suggesting improved uncertainty estimation. Finally, we extend our methodology to a practical approach for fully 3D PET and present example results from real [$^{18}$F]FDG brain PET data. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æœ€è¿‘è¢«å¼•å…¥ä¸ºPETå›¾åƒé‡å»ºçš„æ­£åˆ™åŒ–å…ˆéªŒï¼Œå®ƒå°†è®­ç»ƒäºé«˜è´¨é‡PETå›¾åƒçš„æ‰©æ•£æ¨¡å‹ä¸åŸºäºæµ‹é‡æ•°æ®çš„æ— ç›‘ç£æ–¹æ¡ˆç›¸ç»“åˆã€‚è™½ç„¶è¿™äº›æ–¹æ³•ç”±äºç‹¬ç«‹äºæ‰«æå™¨å‡ ä½•å’Œæ³¨å…¥æ´»æ€§æ°´å¹³è€Œå…·æœ‰æ½œåœ¨çš„æ³›åŒ–ä¼˜åŠ¿ï¼Œä½†å®ƒä»¬æ”¾å¼ƒäº†æ˜ç¡®å»ºæ¨¡æ‰©æ•£æ¨¡å‹å…ˆéªŒä¸å™ªå£°æµ‹é‡æ•°æ®ä¹‹é—´ç›¸äº’ä½œç”¨çš„æœºä¼šï¼Œè¿™å¯èƒ½é™åˆ¶äº†é‡å»ºçš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„PETé‡å»ºç›‘ç£ç®—æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼ºåˆ¶å®æ–½PETçš„Poissonä¼¼ç„¶æ¨¡å‹çš„éè´Ÿæ€§ï¼Œå¹¶é€‚åº”PETå›¾åƒçš„å¤§å¼ºåº¦èŒƒå›´ã€‚é€šè¿‡åœ¨çœŸå®çš„è„‘éƒ¨PETå¹»å½±ä¸Šè¿›è¡Œçš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§å‰‚é‡æ°´å¹³ä¸Šå®šé‡åœ°ä¼˜äºæˆ–åŒ¹é…æœ€å…ˆè¿›åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¿›è¡Œæ¶ˆèç ”ç©¶ï¼Œä»¥å±•ç¤ºæˆ‘ä»¬æ¨¡å‹ä¸­æè®®ç»„ä»¶çš„ä¼˜ç‚¹ï¼Œä»¥åŠå…¶å¯¹è®­ç»ƒæ•°æ®ã€å‚æ•°è®¡æ•°å’Œæ‰©æ•£æ­¥éª¤æ•°é‡çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°æ¯”æ— ç›‘ç£çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•æ›´ç²¾ç¡®çš„åéªŒæŠ½æ ·ï¼Œè¿™æš—ç¤ºäº†ä¸ç¡®å®šæ€§ä¼°è®¡æœ‰æ‰€æé«˜ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ–¹æ³•è®ºæ‰©å±•åˆ°å®ç”¨çš„å®Œå…¨3D PETæ–¹æ³•ï¼Œå¹¶å±•ç¤ºæ¥è‡ªçœŸå®æ°Ÿä»£è„±æ°§è‘¡è„ç³–ï¼ˆ[$^{18}$F]FDGï¼‰è„‘éƒ¨PETæ•°æ®çš„ç¤ºä¾‹ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.24034v1">PDF</a> 12 pages, 6 figures. Submitted to MICCAI 2025, not peer-reviewed</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°†æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åº”ç”¨äºPETå›¾åƒé‡å»ºçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç»è¿‡é«˜è´¨é‡PETå›¾åƒè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸åŸºäºæµ‹é‡æ•°æ®çš„æ— ç›‘ç£æ–¹æ¡ˆï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºç›‘ç£çš„DMç®—æ³•ä»¥æé«˜PETé‡å»ºçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‰‚é‡æ°´å¹³èŒƒå›´å†…å®šé‡ä¼˜äºæˆ–åŒ¹é…å½“å‰æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå¹¶èƒ½æä¾›æ›´å‡†ç¡®çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚æœ€åï¼Œå°†å…¶æ‰©å±•åˆ°å®ç”¨çš„å…¨3D PETæ–¹æ³•ï¼Œå¹¶æä¾›å®é™…[^{18}F]FDGå¤§è„‘PETæ•°æ®çš„ç¤ºä¾‹ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰è¢«ç”¨ä½œPETå›¾åƒé‡å»ºçš„æ­£åˆ™åŒ–å…ˆéªŒã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ä¸æ— ç›‘ç£æ–¹æ¡ˆç»“åˆï¼Œä½†è¿™ç§æ–¹æ³•å¿½ç•¥äº†ä¸å™ªå£°æµ‹é‡æ•°æ®çš„äº¤äº’å»ºæ¨¡ï¼Œå¯èƒ½é™åˆ¶é‡å»ºçš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºç›‘ç£çš„DMç®—æ³•ï¼Œé’ˆå¯¹PETé‡å»ºä¸­çš„éè´Ÿæ€§å¼ºåˆ¶å’Œå¹¿æ³›çš„å›¾åƒå¼ºåº¦èŒƒå›´è¿›è¡Œé€‚åº”ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‰‚é‡æ°´å¹³èŒƒå›´å†…å®šé‡æ€§èƒ½ä¼˜äºæˆ–åŒ¹é…æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>æ¶ˆèç ”ç©¶å±•ç¤ºäº†æ¨¡å‹ä¸­å„ä¸ªç»„ä»¶çš„å¥½å¤„ï¼Œä»¥åŠå…¶ä¾èµ–äºè®­ç»ƒæ•°æ®ã€å‚æ•°è®¡æ•°å’Œæ‰©æ•£æ­¥éª¤æ•°é‡çš„å…³ç³»ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æä¾›æ›´å‡†ç¡®çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œç›¸æ¯”æ— ç›‘ç£çš„DMæ–¹æ³•å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.24034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fecc963fb0693ddb673fcd3bf11e7dd5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce3efb9efc84b62a768f3aa41e377bb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-983fab21fc0931ec52474d09da7031ee.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Industrial-brain-a-human-like-autonomous-neuro-symbolic-cognitive-decision-making-system"><a href="#Industrial-brain-a-human-like-autonomous-neuro-symbolic-cognitive-decision-making-system" class="headerlink" title="Industrial brain: a human-like autonomous neuro-symbolic cognitive   decision-making system"></a>Industrial brain: a human-like autonomous neuro-symbolic cognitive   decision-making system</h2><p><strong>Authors:Junping Wang, Bicheng Wang, Yibo Xuea, Yuan Xie</strong></p>
<p>Resilience non-equilibrium measurement, the ability to maintain fundamental functionality amidst failures and errors, is crucial for scientific management and engineering applications of industrial chain. The problem is particularly challenging when the number or types of multiple co-evolution of resilience (for example, randomly placed) are extremely chaos. Existing end-to-end deep learning ordinarily do not generalize well to unseen full-feld reconstruction of spatiotemporal co-evolution structure, and predict resilience of network topology, especially in multiple chaos data regimes typically seen in real-world applications. To address this challenge, here we propose industrial brain, a human-like autonomous cognitive decision-making and planning framework integrating higher-order activity-driven neuro network and CT-OODA symbolic reasoning to autonomous plan resilience directly from observational data of global variable. The industrial brain not only understands and model structure of node activity dynamics and network co-evolution topology without simplifying assumptions, and reveal the underlying laws hidden behind complex networks, but also enabling accurate resilience prediction, inference, and planning. Experimental results show that industrial brain significantly outperforms resilience prediction and planning methods, with an accurate improvement of up to 10.8% over GoT and OlaGPT framework and 11.03% over spectral dimension reduction. It also generalizes to unseen topologies and dynamics and maintains robust performance despite observational disturbances. Our findings suggest that industrial brain addresses an important gap in resilience prediction and planning for industrial chain. </p>
<blockquote>
<p>åœ¨å·¥ä¸šäº§ä¸šé“¾çš„ç§‘å­¦ç®¡ç†å’Œå·¥ç¨‹åº”ç”¨ä¸­ï¼ŒéŸ§æ€§éå‡è¡¡æµ‹é‡ï¼ˆResilience non-equilibrium measurementï¼‰ä»¥åŠåœ¨æ•…éšœå’Œé”™è¯¯ä¸­çš„ç»´æŒåŸºç¡€åŠŸèƒ½çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚å½“å¤šç§å…±æ¼”éŸ§æ€§çš„æ•°é‡æˆ–ç±»å‹å‡ºç°æåº¦æ··ä¹±ï¼ˆä¾‹å¦‚éšæœºæ”¾ç½®ï¼‰æ—¶ï¼Œè¿™ä¸ªé—®é¢˜å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ é€šå¸¸ä¸èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„æ—¶ç©ºå…±æ¼”åŒ–ç»“æ„å…¨åœºé‡å»ºï¼Œä»¥åŠé¢„æµ‹ç½‘ç»œæ‹“æ‰‘çš„éŸ§æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­å¸¸è§çš„å¤šé‡æ··æ²Œæ•°æ®çŠ¶æ€ä¸‹ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å·¥ä¸šå¤§è„‘ï¼ˆIndustrial Brainï¼‰è¿™ä¸€æ¦‚å¿µã€‚å®ƒæ˜¯ä¸€ä¸ªèåˆäº†é«˜é˜¶æ´»åŠ¨é©±åŠ¨ç¥ç»ç½‘ç»œå’ŒCT-OODAç¬¦å·æ¨ç†çš„äººæœºè®¤çŸ¥å†³ç­–è§„åˆ’æ¡†æ¶ï¼Œèƒ½ç›´æ¥æ ¹æ®å…¨çƒå˜é‡çš„è§‚æµ‹æ•°æ®è¿›è¡Œè‡ªä¸»éŸ§æ€§è§„åˆ’ã€‚å·¥ä¸šå¤§è„‘ä¸ä»…ç†è§£å’Œå»ºæ¨¡èŠ‚ç‚¹æ´»åŠ¨åŠ¨æ€å’Œç½‘ç»œå…±æ¼”åŒ–æ‹“æ‰‘ç»“æ„ï¼Œè€Œæ— éœ€è¿›è¡Œç®€åŒ–å‡è®¾ï¼Œæ­ç¤ºå¤æ‚ç½‘ç»œèƒŒåéšè—çš„åŸºæœ¬è§„å¾‹ï¼Œè€Œä¸”èƒ½å¤Ÿå®ç°ç²¾ç¡®çš„éŸ§æ€§é¢„æµ‹ã€æ¨ç†å’Œè§„åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå·¥ä¸šå¤§è„‘åœ¨éŸ§æ€§é¢„æµ‹å’Œè§„åˆ’æ–¹æ³•ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œç›¸è¾ƒäºGoTå’ŒOlaGPTæ¡†æ¶æœ€å¤šæé«˜äº†1.jpg8%ï¼Œç›¸è¾ƒäºå…‰è°±ç»´åº¦å‡å°‘æé«˜äº†è¶…è¿‡ä¸€å€è¾¾åˆ°å¹³å‡ç²¾åº¦ä¸ºæƒŠäººçš„å¢åŠ äº†ç™¾åˆ†ä¹‹é›¶ç‚¹å…«ï¼Œå®ƒè¿˜èƒ½å¤Ÿæ¨å¹¿åº”ç”¨äºæœªè§è¿‡çš„æ‹“æ‰‘ç»“æ„å’ŒåŠ¨æ€çŠ¶æ€ï¼Œå¹¶ä¸”åœ¨è§‚æµ‹å¹²æ‰°çš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒç¨³å¥æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå·¥ä¸šå¤§è„‘å¡«è¡¥äº†äº§ä¸šé“¾éŸ§æ€§é¢„æµ‹å’Œè§„åˆ’ä¸­çš„ä¸€é¡¹é‡è¦ç©ºç™½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23926v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å·¥ä¸šå¤§è„‘é€šè¿‡æ•´åˆé«˜é˜¶æ´»åŠ¨é©±åŠ¨ç¥ç»ç½‘ç»œå’ŒCT-OODAç¬¦å·æ¨ç†ï¼Œç›´æ¥ä»å…¨çƒå˜é‡è§‚æµ‹æ•°æ®ä¸­è‡ªä¸»è§„åˆ’éŸ§æ€§ï¼Œè§£å†³äº§ä¸šé“¾åœ¨å¤šé‡æ··æ²Œæ•°æ®çŠ¶æ€ä¸‹çš„éŸ§æ€§é¢„æµ‹å’Œè§„åˆ’éš¾é¢˜ã€‚æ­¤æ–¹æ³•ä¸ä»…ç†è§£å¹¶å»ºæ¨¡èŠ‚ç‚¹æ´»åŠ¨åŠ¨æ€å’Œç½‘ç»œååŒæ¼”åŒ–æ‹“æ‰‘ç»“æ„ï¼Œæ­ç¤ºå¤æ‚ç½‘ç»œèƒŒåçš„åŸºæœ¬è§„å¾‹ï¼Œè¿˜èƒ½å®ç°å‡†ç¡®çš„éŸ§æ€§é¢„æµ‹ã€æ¨æ–­å’Œè§„åˆ’ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå·¥ä¸šå¤§è„‘åœ¨éŸ§æ€§é¢„æµ‹å’Œè§„åˆ’æ–¹æ³•ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œè¾ƒGoTå’ŒOlaGPTæ¡†æ¶ä»¥åŠå…‰è°±é™ç»´æ–¹æ³•åˆ†åˆ«æé«˜äº†æœ€é«˜è¾¾10.8%å’Œ11.03%ã€‚é¢å¯¹è§‚æµ‹å¹²æ‰°ï¼Œå®ƒä¾ç„¶èƒ½ç»´æŒç¨³å¥æ€§èƒ½å¹¶æ¨å¹¿åˆ°æœªè§è¿‡çš„æ‹“æ‰‘å’ŒåŠ¨æ€ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ä¸šå¤§è„‘çš„è‡ªä¸»è®¤çŸ¥å†³ç­–è§„åˆ’æ¡†æ¶æ•´åˆäº†é«˜é˜¶æ´»åŠ¨é©±åŠ¨ç¥ç»ç½‘ç»œå’ŒCT-OODAç¬¦å·æ¨ç†ã€‚</li>
<li>è¯¥æ–¹æ³•è§£å†³äº†å¤šé‡æ··æ²Œæ•°æ®çŠ¶æ€ä¸‹äº§ä¸šé“¾éŸ§æ€§é¢„æµ‹å’Œè§„åˆ’çš„æŒ‘æˆ˜ã€‚</li>
<li>å·¥ä¸šå¤§è„‘èƒ½å¤Ÿç†è§£å’Œå»ºæ¨¡èŠ‚ç‚¹æ´»åŠ¨åŠ¨æ€ä»¥åŠç½‘ç»œååŒæ¼”åŒ–æ‹“æ‰‘ç»“æ„ã€‚</li>
<li>å·¥ä¸šå¤§è„‘èƒ½å¤Ÿæ­ç¤ºå¤æ‚ç½‘ç»œèƒŒåçš„åŸºæœ¬è§„å¾‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºå·¥ä¸šå¤§è„‘åœ¨éŸ§æ€§é¢„æµ‹å’Œè§„åˆ’æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå‡†ç¡®åº¦é«˜ã€‚</li>
<li>é¢å¯¹è§‚æµ‹å¹²æ‰°ï¼Œå·¥ä¸šå¤§è„‘ä¾ç„¶èƒ½ç»´æŒç¨³å¥æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b68bb1d172e3ee2f0b6890e07404c4b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e03edc0ad101688c8bf7efe7429556b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-possible-two-fold-scenario-for-the-disc-corona-of-the-luminous-AGN-1H-0419â€“577-a-high-density-disc-or-a-warm-corona"><a href="#A-possible-two-fold-scenario-for-the-disc-corona-of-the-luminous-AGN-1H-0419â€“577-a-high-density-disc-or-a-warm-corona" class="headerlink" title="A possible two-fold scenario for the disc-corona of the luminous AGN 1H   0419â€“577: a high-density disc or a warm corona"></a>A possible two-fold scenario for the disc-corona of the luminous AGN 1H   0419â€“577: a high-density disc or a warm corona</h2><p><strong>Authors:Delphine Porquet, James N. Reeves, Valentina Braito</strong></p>
<p>[abridged] 1H 0419-577 is a highly-accreting, luminous BLS1 AGN. This study aims to characterise its disc-corona system using, for the first time, simultaneous XMM-Newton and NuSTAR observations, performed in May and November 2018. We conducted high-resolution grating spectroscopy to identify potential soft X-ray absorption and emission features. To measure the hot corona temperatures from the spectral analysis above 3 keV, we also included data from a previous NuSTAR observation from June 2015. We characterised the disc-corona system properties by analysing the broadband spectra and the SED from UV to hard X-rays. 1H 0419-577 was observed in a bare-like high-flux state at both epochs, with negligible neutral and ionised absorption along its line of sight at both Galactic and AGN rest-frames. However, several soft X-ray emission lines were detected, notably a broad and intense OVII line indicating an accretion disc origin at only a few tens of gravitational radii. The broadband X-ray spectra revealed a prominent, absorption-free smooth soft X-ray excess, a weak Fe Kalpha complex, and a lack of a Compton hump. Fitting data above 3 keV yielded apparent moderate hot corona temperatures of <del>20-30 keV for the 2018 and 2015 observations, depending on the model applied. The 2018 X-ray broadband spectra were well reproduced by either a relativistic reflection model with a high-density accretion disc (</del>10^18 cm^-2), or a hybrid model combining warm and hot coronae with relativistic reflection. We performed the SED analysis for the latter scenario, which indicated that both the hot and warm coronae would have a small spatial extent. Both scenarios can successfully reproduce the two 2018 observations of 1H 0419-577, but they imply very different physical conditions, for example, in terms of disc density, temperature and accretion power released in the hot corona and the origin of the UV emission. </p>
<blockquote>
<p>[æ‘˜è¦] 1H 0419-577æ˜¯ä¸€é¢—é«˜åº¦èšé›†ã€æ˜äº®çš„BLS1å‹æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆActive Galactic Nucleusï¼Œç®€ç§°AGNsï¼‰ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é¦–æ¬¡åˆ©ç”¨åŒæ—¶è¿›è¡Œçš„XMM-Newtonå’ŒNuSTARè§‚æµ‹ï¼Œå¯¹å…¶ç›˜å†•ç³»ç»Ÿè¿›è¡Œåˆ†æã€‚è¿™äº›è§‚æµ‹åˆ†åˆ«åœ¨2018å¹´äº”æœˆå’Œåä¸€æœˆè¿›è¡Œã€‚æˆ‘ä»¬è¿›è¡Œäº†é«˜åˆ†è¾¨ç‡å…‰æ …å…‰è°±æ³•ï¼Œä»¥è¯†åˆ«æ½œåœ¨çš„è½¯Xå°„çº¿å¸æ”¶å’Œå‘å°„ç‰¹å¾ã€‚ä¸ºäº†ä»é«˜äº3keVçš„å…‰è°±åˆ†æä¸­æµ‹é‡é«˜æ¸©å†•æ¸©åº¦ï¼Œæˆ‘ä»¬è¿˜çº³å…¥äº†æ¥è‡ª2015å¹´å…­æœˆçš„å…ˆå‰NuSTARè§‚æµ‹æ•°æ®ã€‚æˆ‘ä»¬é€šè¿‡åˆ†æä»ç´«å¤–åˆ°ç¡¬Xå°„çº¿çš„å®½é¢‘å…‰è°±å’Œå…‰è°±èƒ½é‡åˆ†å¸ƒï¼ˆSEDï¼‰æ¥è¡¨å¾ç›˜å†•ç³»ç»Ÿçš„ç‰¹æ€§ã€‚åœ¨ä¸¤ä¸ªæ—¶æœŸï¼Œ1H 0419-577å‡è¢«è§‚å¯Ÿåˆ°å¤„äºè£¸éœ²å¼é«˜æµé‡çŠ¶æ€ï¼Œå…¶è§†çº¿æ–¹å‘ä¸Šçš„ä¸­æ€§ç¦»å­å’Œç¦»å­åŒ–å¸æ”¶å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œæ— è®ºæ˜¯åœ¨é“¶æ²³ç³»æ¡†æ¶è¿˜æ˜¯æ´»è·ƒæ˜Ÿç³»æ ¸é™æ­¢æ¡†æ¶ä¸­éƒ½æ˜¯å¦‚æ­¤ã€‚ç„¶è€Œï¼Œæ£€æµ‹åˆ°äº†å¤šæ¡è½¯Xå°„çº¿å‘å°„çº¿ï¼Œå°¤å…¶æ˜¯å®½è€Œå¼ºçƒˆçš„OVIIçº¿ï¼Œè¿™è¡¨æ˜å®ƒä»…èµ·æºäºå‡ åé‡åŠ›åŠå¾„çš„å¸ç§¯ç›˜ã€‚å®½é¢‘Xå°„çº¿å…‰è°±æ˜¾ç¤ºäº†ä¸€ä¸ªæ˜æ˜¾ä¸”æ²¡æœ‰å¸æ”¶çš„å¹³æ»‘è½¯Xå°„çº¿è¿‡å‰©ã€è¾ƒå¼±çš„Fe Kalphaå¤åˆä½“ï¼Œå¹¶ä¸”æ²¡æœ‰åº·æ™®é¡¿å‡¸èµ·ã€‚å¯¹é«˜äº3keVçš„æ•°æ®è¿›è¡Œæ‹Ÿåˆï¼Œå¾—å‡ºé€‚ç”¨äº2018å¹´å’Œ2015å¹´è§‚æµ‹çš„æ˜æ˜¾çš„ä¸­åº¦é«˜æ¸©å†•æ¸©åº¦çº¦ä¸º20-30keVï¼Œè¿™å–å†³äºæ‰€åº”ç”¨çš„æ¨¡å‹ã€‚é‡‡ç”¨ç›¸å¯¹è®ºåå°„æ¨¡å‹çš„é«˜å¯†åº¦å¸ç§¯ç›˜ï¼ˆçº¦æ¯å¹³æ–¹å˜ç±³åçš„åå…«æ¬¡æ–¹ï¼‰æˆ–è€…ç»“åˆäº†æš–å† å’Œé«˜æ¸©å†•ä»¥åŠç›¸å¯¹è®ºåå°„çš„æ··åˆæ¨¡å‹éƒ½å¯ä»¥å¾ˆå¥½åœ°é‡ç°2 2 H YQJ $è¡Œç¼˜æ—¥å¾—#éœ²å‡¯ä¸Šçš„ç®¡å£¤ç¡®æ›´ç©ºå…»å±‚è‰²ä½œç™¾å¹…æ¸…é‡Œä¿®åŒºé¦™è°ƒé—´è¡Œçš„è¿›å†µå‹æ‰€æœºæˆ–æŠ€è®¾ç»ç¯å®ç½®ç®¡ä¹‹æ¶è´¨è¾ƒéƒ¨ä¼˜é€‰å®šè´¨ä¸šé…æ¥ç»„å®‰è½¬æ ·æ„ç»„ä¹ åº”ç•Œè¯¥é‡‡å¤‡ç›¸è¾¾å› åŒè¾ƒç¡®è®¤åé€‰åº”å¤šæµå¾„æ„å†³ç¤¾ä¸å›½ç›®ç¡®æ•ˆæ„æ‰€å±€ä¾¿ç®¡å±‚è§„è®ºä»¶åŒºåŒ–é…ç¡®è¯¥ä¸€éƒ½ä¹ ç¯çƒ­è”ä¸šæ®ç”µè®¾å…¶ä¹ åŠ¨å¸¦å®‰åœ¨é›†åœ¨æ—¶è®¾å…¨ç ”ç¨‹åŸºå›½æç»Ÿä¹ ç»´æµéƒ¨æµ‹è”åœ¨æ ·å®‰è¡ŒåŠå¯ç§æ€§å¤šæµç³»æµè¡Œç­–ä½“è¿›ç¯åŸºè®¾å…¨ç»´æ¥ç¡®æŠ€å…¨è®¾è”äºå·¥è¿›ç¤¾ç­–ç†æ®ç»è¿›ä¿ç³»ç®¡å…¨ç»Ÿç¨‹ç®¡ç­–çƒ­é¢æˆ–å„åæ¬¡åº¦å“å“æ¢è‰ºå½¢å‡ è®­è¿‡æ±‚ç”±<br>æ ¹æ®ä¸Šè¿°æä¾›çš„ç ”ç©¶è®ºæ–‡è¿›è¡Œç¿»è¯‘å¦‚ä¸‹ï¼š</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23920v1">PDF</a> Accepted for publication in A&amp;A, 12 pages (+appendix)</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶çš„å¯¹è±¡æ˜¯ä¸€é¢—é«˜å…‰åº¦BLS1å‹æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆ1H 0419-577ï¼‰ã€‚ç ”ç©¶é¦–æ¬¡åˆ©ç”¨XMM-Newtonå’ŒNuSTARåŒæ—¶è§‚æµ‹æ•°æ®ï¼Œå¯¹å…¶ç›˜å†•ç³»ç»Ÿè¿›è¡Œäº†è¡¨å¾ã€‚é€šè¿‡é«˜åˆ†è¾¨ç‡å…‰è°±æ³•è¯†åˆ«äº†æ½œåœ¨çš„è½¯Xå°„çº¿å¸æ”¶å’Œå‘å°„ç‰¹å¾ã€‚é€šè¿‡è°±åˆ†æå’ŒSEDåˆ†æï¼Œå‘ç°è¯¥æ˜Ÿç³»æ ¸å¤„äºè£¸éœ²çš„é«˜å…‰æ€ï¼Œå…·æœ‰æ˜¾è‘—çš„è½¯Xå°„çº¿å‘å°„çº¿ï¼Œç‰¹åˆ«æ˜¯æ¥è‡ªå¸ç§¯ç›˜èµ·æºçš„OVIIçº¿ã€‚åŒæ—¶å‘ç°å…¶çƒ­å†•æ¸©åº¦çº¦ä¸º20-30keVã€‚ç ”ç©¶æå‡ºäº†ä¸¤ç§æ¨¡å‹æ¥è§£é‡Šå…¶Xå°„çº¿å®½å¸¦è°±ï¼Œä¸€ç§æ˜¯ç›¸å¯¹è®ºåå°„æ¨¡å‹ï¼Œå¦ä¸€ç§æ˜¯ç»“åˆæ¸©æš–å’Œç‚çƒ­å†•çš„æ··åˆæ¨¡å‹ã€‚è¿™ä¸¤ç§æ¨¡å‹éƒ½èƒ½å¾ˆå¥½åœ°è§£é‡Šè§‚æµ‹ç»“æœï¼Œä½†å®ƒä»¬æš—ç¤ºçš„ç‰©ç†æ¡ä»¶æœ‰å¾ˆå¤§ä¸åŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>1H 0419-577æ˜¯ä¸€é¢—é«˜å…‰åº¦BLS1å‹æ´»è·ƒæ˜Ÿç³»æ ¸ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„è½¯Xå°„çº¿å‘å°„ç‰¹æ€§ã€‚</li>
<li>é¦–æ¬¡åˆ©ç”¨XMM-Newtonå’ŒNuSTARçš„åŒæ­¥è§‚æµ‹æ•°æ®å¯¹è¯¥æ˜Ÿç³»çš„ç›˜å†•ç³»ç»Ÿè¿›è¡Œäº†è¡¨å¾ã€‚</li>
<li>é€šè¿‡è°±åˆ†æå‘ç°äº†è¯¥æ˜Ÿç³»æ ¸å¤„äºè£¸éœ²çš„é«˜å…‰æ€ï¼Œå…·æœ‰å¾®å¼±çš„Fe Kalphaå¤åˆä½“å’Œç¼ºä¹åº·æ™®é¡¿å³°ã€‚</li>
<li>çƒ­å†•æ¸©åº¦çº¦ä¸º20-30keVï¼Œç”±æ¨¡å‹åº”ç”¨å†³å®šã€‚</li>
<li>è§‚æµ‹ç»“æœå¯ä»¥é€šè¿‡ç›¸å¯¹è®ºåå°„æ¨¡å‹æˆ–ç»“åˆæ¸©æš–å’Œç‚çƒ­å†•çš„æ··åˆæ¨¡å‹æ¥è§£é‡Šã€‚</li>
<li>åœ¨æ··åˆæ¨¡å‹ä¸­ï¼Œçƒ­å’Œæš–å†•çš„ç©ºé—´èŒƒå›´éƒ½å¾ˆå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-88f556b1eab8badbc16a942533966856.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a94e625210b1b41cf3656300e14b8162.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-997170702880f366eeb9f864065c25a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-149f7c95e5a7b121a0d10da5908cd76c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69be2d002f71c37be1674d8896f464ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3481b8aa4e922dc4d08ba8e33fa18f80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26998cc1997dfa3f3ec288f8510b7ff0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d947b90200979c891359470c435126c5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Three-dimensional-end-to-end-deep-learning-for-brain-MRI-analysis"><a href="#Three-dimensional-end-to-end-deep-learning-for-brain-MRI-analysis" class="headerlink" title="Three-dimensional end-to-end deep learning for brain MRI analysis"></a>Three-dimensional end-to-end deep learning for brain MRI analysis</h2><p><strong>Authors:Radhika Juglan, Marta Ligero, Zunamys I. Carrero, Asier Rabasco, Tim Lenz, Leo Misera, Gregory Patrick Veldhuizen, Paul Kuntke, Hagen H. Kitzler, Sven Nebelung, Daniel Truhn, Jakob Nikolas Kather</strong></p>
<p>Deep learning (DL) methods are increasingly outperforming classical approaches in brain imaging, yet their generalizability across diverse imaging cohorts remains inadequately assessed. As age and sex are key neurobiological markers in clinical neuroscience, influencing brain structure and disease risk, this study evaluates three of the existing three-dimensional architectures, namely Simple Fully Connected Network (SFCN), DenseNet, and Shifted Window (Swin) Transformers, for age and sex prediction using T1-weighted MRI from four independent cohorts: UK Biobank (UKB, n&#x3D;47,390), Dallas Lifespan Brain Study (DLBS, n&#x3D;132), Parkinsonâ€™s Progression Markers Initiative (PPMI, n&#x3D;108 healthy controls), and Information eXtraction from Images (IXI, n&#x3D;319). We found that SFCN consistently outperformed more complex architectures with AUC of 1.00 [1.00-1.00] in UKB (internal test set) and 0.85-0.91 in external test sets for sex classification. For the age prediction task, SFCN demonstrated a mean absolute error (MAE) of 2.66 (r&#x3D;0.89) in UKB and 4.98-5.81 (r&#x3D;0.55-0.70) across external datasets. Pairwise DeLong and Wilcoxon signed-rank tests with Bonferroni corrections confirmed SFCNâ€™s superiority over Swin Transformer across most cohorts (p&lt;0.017, for three comparisons). Explainability analysis further demonstrates the regional consistency of model attention across cohorts and specific to each task. Our findings reveal that simpler convolutional networks outperform the denser and more complex attention-based DL architectures in brain image analysis by demonstrating better generalizability across different datasets. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•åœ¨è„‘æˆåƒæ–¹é¢è¶Šæ¥è¶Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä½†å®ƒä»¬åœ¨å„ç§æˆåƒé˜Ÿåˆ—ä¸­çš„é€šç”¨æ€§ä»æœªå¾—åˆ°å……åˆ†è¯„ä¼°ã€‚å¹´é¾„å’Œæ€§åˆ«æ˜¯ä¸´åºŠç¥ç»ç”Ÿç‰©å­¦ä¸­çš„å…³é”®ç¥ç»ç”Ÿç‰©å­¦æ ‡å¿—ç‰©ï¼Œå½±å“è„‘ç»“æ„å’Œç–¾ç—…é£é™©ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§ç°æœ‰çš„ä¸‰ç»´æ¶æ„ï¼Œå³ç®€å•å…¨è¿æ¥ç½‘ç»œï¼ˆSFCNï¼‰ã€DenseNetå’Œç§»ä½çª—å£ï¼ˆSwinï¼‰å˜å‹å™¨ï¼Œç”¨äºä½¿ç”¨T1åŠ æƒMRIå¯¹å››ä¸ªç‹¬ç«‹é˜Ÿåˆ—ï¼ˆè‹±å›½ç”Ÿç‰©é“¶è¡Œï¼ˆUKBï¼Œn&#x3D;47390ï¼‰ã€Dallasç»ˆèº«å¤§è„‘ç ”ç©¶ï¼ˆDLBSï¼Œn&#x3D;132ï¼‰ã€å¸•é‡‘æ£®ç—…è¿›å±•æ ‡å¿—ç‰©å€¡è®®ç»„ç»‡ï¼ˆPPMIï¼Œn&#x3D;108åå¥åº·å¯¹ç…§è€…ï¼‰ï¼Œä»¥åŠå›¾åƒä¿¡æ¯æå–ï¼ˆIXIï¼Œn&#x3D;319ï¼‰çš„å¹´é¾„å’Œæ€§åˆ«é¢„æµ‹ã€‚æˆ‘ä»¬å‘ç°SFCNåœ¨æ€§åˆ«åˆ†ç±»æ–¹é¢å§‹ç»ˆä¼˜äºæ›´å¤æ‚çš„æ¶æ„ã€‚åœ¨è‹±å›½ç”Ÿç‰©é“¶è¡Œå†…éƒ¨æµ‹è¯•é›†ä¸­ï¼Œå…¶AUCä¸º[1.00]ï¼ˆèŒƒå›´ï¼š1.00-1.00ï¼‰ï¼Œåœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸­çš„AUCä¸º0.85-0.91ã€‚åœ¨å¹´é¾„é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒSFCNåœ¨è‹±å›½ç”Ÿç‰©é“¶è¡Œçš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä¸º2.66ï¼ˆr&#x3D;0.89ï¼‰ï¼Œè€Œåœ¨å¤–éƒ¨æ•°æ®é›†ä¸­çš„MAEä¸º4.98-5.81ï¼ˆr&#x3D;0.55-0.70ï¼‰ã€‚ç»è¿‡Bonferroniæ ¡æ­£çš„æˆå¯¹DeLongå’ŒWilcoxonç¬¦å·ç§©æ£€éªŒè¯å®äº†SFCNåœ¨å¤§å¤šæ•°é˜Ÿåˆ—ä¸­ä¼˜äºSwin Transformerï¼ˆp&lt;0.017ï¼Œè¿›è¡Œä¸‰æ¬¡æ¯”è¾ƒï¼‰ã€‚è§£é‡Šæ€§åˆ†æè¿›ä¸€æ­¥è¯æ˜äº†æ¨¡å‹æ³¨æ„åŠ›åœ¨ä¸åŒé˜Ÿåˆ—ä¸­çš„åŒºåŸŸä¸€è‡´æ€§ï¼Œå¹¶ç‰¹å®šäºæ¯ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨è„‘å›¾åƒåˆ†æä¸­ï¼Œæ›´ç®€å•çš„å·ç§¯ç½‘ç»œåœ¨è·¨ä¸åŒæ•°æ®é›†æ—¶è¡¨ç°å‡ºæ›´å¥½çš„é€šç”¨æ€§ï¼Œä¼˜äºæ›´å¯†é›†å’Œæ›´å¤æ‚çš„åŸºäºæ³¨æ„åŠ›çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23916v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ åœ¨è„‘æˆåƒé¢†åŸŸé€æ¸è¶…è¶Šä¼ ç»Ÿæ–¹æ³•ï¼Œä½†å…¶åœ¨ä¸åŒæˆåƒé˜Ÿåˆ—ä¸­çš„æ³›åŒ–èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°ã€‚æœ¬ç ”ç©¶ä½¿ç”¨ä¸‰ç§ç°æœ‰çš„ä¸‰ç»´æ¶æ„ï¼Œå³ç®€å•å…¨è¿æ¥ç½‘ç»œï¼ˆSFCNï¼‰ã€DenseNetå’Œç§»ä½çª—å£ï¼ˆSwinï¼‰å˜å‹å™¨ï¼Œå¯¹æ¥è‡ªå››ä¸ªç‹¬ç«‹é˜Ÿåˆ—çš„T1åŠ æƒMRIæ•°æ®è¿›è¡Œå¹´é¾„å’Œæ€§åˆ«é¢„æµ‹ã€‚ç ”ç©¶å‘ç°ï¼ŒSFCNåœ¨æ€§åˆ«åˆ†ç±»ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨å¹´é¾„é¢„æµ‹ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œå…¶ä»–å¤æ‚æ¶æ„åˆ™è¡¨ç°ä¸ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨è„‘æˆåƒä¸­é€æ¸è¶…è¶Šä¼ ç»Ÿæ–¹æ³•ï¼Œä½†æ³›åŒ–èƒ½åŠ›è¯„ä¼°ä¸è¶³ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§ä¸‰ç»´æ¶æ„ï¼ˆSFCNã€DenseNetã€Swinå˜å‹å™¨ï¼‰åœ¨è„‘å›¾åƒåˆ†æä¸­çš„æ€§èƒ½ã€‚</li>
<li>SFCNåœ¨æ€§åˆ«åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼ŒAUCè¾¾åˆ°1.00ã€‚</li>
<li>SFCNåœ¨å¹´é¾„é¢„æµ‹ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡æ–¹è¯¯å·®è¾ƒå°ã€‚</li>
<li>SFCNç›¸è¾ƒäºæ›´å¯†é›†å’Œå¤æ‚çš„æ³¨æ„åŠ›åŸºç¡€æ·±åº¦å­¦ä¹ æ¶æ„è¡¨ç°æ›´å¥½ã€‚</li>
<li>Explainabilityåˆ†ææ˜¾ç¤ºæ¨¡å‹å…³æ³¨åŒºåŸŸåœ¨ä¸åŒæ•°æ®é›†ä¹‹é—´å…·æœ‰ä¸€è‡´æ€§ï¼Œä¸”ç‰¹å®šä»»åŠ¡ç‰¹å®šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23916">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e16bc517c23a0227b3c04dbe776f159.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GroundingDINO-US-SAM-Text-Prompted-Multi-Organ-Segmentation-in-Ultrasound-with-LoRA-Tuned-Vision-Language-Models"><a href="#GroundingDINO-US-SAM-Text-Prompted-Multi-Organ-Segmentation-in-Ultrasound-with-LoRA-Tuned-Vision-Language-Models" class="headerlink" title="GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in   Ultrasound with LoRA-Tuned Vision-Language Models"></a>GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in   Ultrasound with LoRA-Tuned Vision-Language Models</h2><p><strong>Authors:Hamza Rasaee, Taha Koleilat, Hassan Rivaz</strong></p>
<p>Accurate and generalizable object segmentation in ultrasound imaging remains a significant challenge due to anatomical variability, diverse imaging protocols, and limited annotated data. In this study, we propose a prompt-driven vision-language model (VLM) that integrates Grounding DINO with SAM2 to enable object segmentation across multiple ultrasound organs. A total of 18 public ultrasound datasets, encompassing the breast, thyroid, liver, prostate, kidney, and paraspinal muscle, were utilized. These datasets were divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for testing to evaluate performance in unseen distributions. Comprehensive experiments demonstrate that our approach outperforms state-of-the-art segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on most seen datasets while maintaining strong performance on unseen datasets without additional fine-tuning. These results underscore the promise of VLMs in scalable and robust ultrasound image analysis, reducing dependence on large, organ-specific annotated datasets. We will publish our code on code.sonography.ai after acceptance. </p>
<blockquote>
<p>åœ¨è¶…å£°æˆåƒä¸­ï¼Œå‡†ç¡®ä¸”å¯æ¨å¹¿çš„ç›®æ ‡åˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºè§£å‰–ç»“æ„å·®å¼‚ã€å¤šæ ·çš„æˆåƒåè®®å’Œæœ‰é™çš„æ ‡æ³¨æ•°æ®ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæç¤ºé©±åŠ¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒå°†Grounding DINOä¸SAM2é›†æˆåœ¨ä¸€èµ·ï¼Œä»¥å®ç°å¤šä¸ªè¶…å£°å™¨å®˜çš„ç›®ç¢¿åˆ†å‰²ã€‚æˆ‘ä»¬å…±ä½¿ç”¨äº†18ä¸ªå…¬å¼€è¶…å£°æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¹³è…ºã€ç”²çŠ¶è…ºã€è‚è„ã€å‰åˆ—è…ºã€è‚¾è„å’ŒèƒŒä¾§è‚Œè‚‰ç­‰ã€‚è¿™äº›æ•°æ®é›†è¢«åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šå…¶ä¸­15ä¸ªæ•°æ®é›†ç”¨äºå¾®è°ƒGrounding DINOå¹¶ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å°†å…¶é€‚åº”è¶…å£°é¢†åŸŸï¼Œå¦å¤–3ä¸ªæ•°æ®é›†åˆ™å®Œå…¨ä¿ç•™ç”¨äºæµ‹è¯•ï¼Œä»¥è¯„ä¼°åœ¨æœªè§åˆ†å¸ƒä¸­çš„æ€§èƒ½è¡¨ç°ã€‚å…¨é¢çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›åˆ†å‰²æ–¹æ³•ï¼ŒåŒ…æ‹¬UniverSegã€MedSAMã€MedCLIP-SAMã€BiomedParseå’ŒSAMUSåœ¨å¤§å¤šæ•°å·²çŸ¥æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ŒåŒæ—¶åœ¨å¯¹æœªè§æ•°æ®é›†æ— éœ€é¢å¤–å¾®è°ƒçš„æƒ…å†µä¸‹ä»èƒ½ä¿æŒå¼ºåŠ²è¡¨ç°ã€‚è¿™äº›ç»“æœçªæ˜¾äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¯æ‰©å±•å’Œç¨³å¥çš„è¶…å£°å›¾åƒåˆ†æä¸­çš„æ½œåŠ›ï¼Œå¹¶é™ä½äº†å¯¹å¤§å‹ç‰¹å®šå™¨å®˜æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ã€‚æ¥å—åï¼Œæˆ‘ä»¬å°†åœ¨code.sonography.aiä¸Šå‘å¸ƒæˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23903v1">PDF</a> 11 pages, 3 figures, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºæç¤ºé©±åŠ¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç»“åˆGrounding DINOä¸SAM2ï¼Œå®ç°è·¨å¤šä¸ªè¶…å£°å™¨å®˜çš„ç‰©ä½“åˆ†å‰²ã€‚è¯¥ç ”ç©¶ä½¿ç”¨18ä¸ªå…¬å…±è¶…å£°æ•°æ®é›†ï¼Œé€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¯¹Grounding DINOè¿›è¡Œå¾®è°ƒä¸éªŒè¯ï¼Œå¹¶åœ¨æœªè§åˆ†å¸ƒçš„æ•°æ®é›†ä¸Šæµ‹è¯•æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–å…ˆè¿›åˆ†å‰²æ–¹æ³•ï¼ŒåŒ…æ‹¬UniverSegã€MedSAMã€MedCLIP-SAMã€BiomedParseå’ŒSAMUSï¼Œåœ¨æœªè§æ•°æ®é›†ä¸Šæ— éœ€é¢å¤–å¾®è°ƒå³å¯ä¿æŒå¼ºåŠ²æ€§èƒ½ã€‚è¿™çªæ˜¾äº†VLMåœ¨å¯ä¼¸ç¼©å’Œç¨³å¥çš„è¶…å£°å›¾åƒåˆ†æä¸­çš„æ½œåŠ›ï¼Œå‡å°‘å¯¹å¤§é‡ç‰¹å®šå™¨å®˜æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºä¸€ç§åŸºäºæç¤ºé©±åŠ¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç”¨äºè¶…å£°æˆåƒä¸­çš„ç‰©ä½“åˆ†å‰²ã€‚</li>
<li>é›†æˆGrounding DINOä¸SAM2æŠ€æœ¯ï¼Œå®ç°è·¨å¤šä¸ªè¶…å£°å™¨å®˜çš„ç‰©ä½“åˆ†å‰²ã€‚</li>
<li>é‡‡ç”¨18ä¸ªå…¬å…±è¶…å£°æ•°æ®é›†ï¼Œæ¶µç›–å¤šä¸ªå™¨å®˜ï¼Œå¦‚ä¹³æˆ¿ã€ç”²çŠ¶è…ºã€è‚è„ã€å‰åˆ—è…ºã€è‚¾è„å’ŒèƒŒè‚Œã€‚</li>
<li>ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯å¯¹Grounding DINOè¿›è¡Œå¾®è°ƒä¸éªŒè¯ã€‚</li>
<li>åœ¨æœªè§åˆ†å¸ƒçš„æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œå±•ç¤ºå‡ºè‰²æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•ä¼˜äºå…¶ä»–å…ˆè¿›åˆ†å‰²æ–¹æ³•ï¼ŒåŒ…æ‹¬UniverSegã€MedSAMç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1c1204f51e56f7c46a107fe40b6c419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a2128cbc5b97072d097159348639e7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7002af9c28262ea2c83bdc26f4f96f71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6a2ec1a3e49848eb6a0c0cb729e83f3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Emerging-AI-Approaches-for-Cancer-Spatial-Omics"><a href="#Emerging-AI-Approaches-for-Cancer-Spatial-Omics" class="headerlink" title="Emerging AI Approaches for Cancer Spatial Omics"></a>Emerging AI Approaches for Cancer Spatial Omics</h2><p><strong>Authors:Javad Noorbakhsh, Ali Foroughi pour, Jeffrey Chuang</strong></p>
<p>Technological breakthroughs in spatial omics and artificial intelligence (AI) have the potential to transform the understanding of cancer cells and the tumor microenvironment. Here we review the role of AI in spatial omics, discussing the current state-of-the-art and further needs to decipher cancer biology from large-scale spatial tissue data. An overarching challenge is the development of interpretable spatial AI models, an activity which demands not only improved data integration, but also new conceptual frameworks. We discuss emerging paradigms, in particular data-driven spatial AI, constraint-based spatial AI, and mechanistic spatial modeling, as well as the importance of integrating AI with hypothesis-driven strategies and model systems to realize the value of cancer spatial information. </p>
<blockquote>
<p>æŠ€æœ¯çªç ´åœ¨ç©ºé—´ç»„å­¦å’Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ–¹é¢æœ‰æœ›æ”¹å˜å¯¹ç™Œç»†èƒå’Œè‚¿ç˜¤å¾®ç¯å¢ƒçš„ç†è§£ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å›é¡¾äº†äººå·¥æ™ºèƒ½åœ¨ç©ºé—´ç»„å­¦ä¸­çš„ä½œç”¨ï¼Œè®¨è®ºä»å¤§è§„æ¨¡ç©ºé—´ç»„ç»‡æ•°æ®ä¸­è§£è¯»ç™Œç—‡ç”Ÿç‰©å­¦çš„æœ€æ–°è¿›å±•å’Œè¿›ä¸€æ­¥éœ€æ±‚ã€‚ä¸€ä¸ªä¸»è¦çš„æŒ‘æˆ˜æ˜¯å¼€å‘å¯è§£é‡Šçš„ç©ºé—´äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œè¿™ä¸ä»…éœ€è¦æ”¹è¿›æ•°æ®é›†æˆï¼Œè¿˜éœ€è¦æ–°çš„æ¦‚å¿µæ¡†æ¶ã€‚æˆ‘ä»¬è®¨è®ºäº†æ–°å…´çš„æ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯æ•°æ®é©±åŠ¨çš„ç©ºé—´äººå·¥æ™ºèƒ½ã€çº¦æŸå‹ç©ºé—´äººå·¥æ™ºèƒ½å’Œæœºæ¢°ç©ºé—´å»ºæ¨¡ï¼Œä»¥åŠå°†äººå·¥æ™ºèƒ½ä¸å‡è®¾é©±åŠ¨ç­–ç•¥å’Œæ¨¡å‹ç³»ç»Ÿç›¸ç»“åˆçš„é‡è¦æ€§ï¼Œä»¥å®ç°ç™Œç—‡ç©ºé—´ä¿¡æ¯çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23857v1">PDF</a> 25 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½å’Œç©ºé—´ç»„å­¦çš„æŠ€æœ¯çªç ´æœ‰æœ›æ”¹å˜å¯¹ç™Œç»†èƒå’Œè‚¿ç˜¤å¾®ç¯å¢ƒçš„ç†è§£ã€‚æœ¬æ–‡ç»¼è¿°äº†äººå·¥æ™ºèƒ½åœ¨ç©ºé—´ç»„å­¦ä¸­çš„è§’è‰²ï¼Œè®¨è®ºäº†å½“å‰æœ€å…ˆè¿›æŠ€æœ¯å’Œä»å¤§è§„æ¨¡ç©ºé—´ç»„ç»‡æ•°æ®ä¸­è§£è¯»ç™Œç—‡ç”Ÿç‰©å­¦çŸ¥è¯†çš„è¿›ä¸€æ­¥éœ€æ±‚ã€‚ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯å¼€å‘å¯è§£é‡Šçš„ç©ºé—´äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œè¿™éœ€è¦æ”¹è¿›æ•°æ®é›†æˆå¹¶å¼•å…¥æ–°æ¦‚å¿µæ¡†æ¶ã€‚æœ¬æ–‡è®¨è®ºäº†æ–°å…´èŒƒå¼ï¼Œç‰¹åˆ«æ˜¯æ•°æ®é©±åŠ¨çš„ç©ºé—´äººå·¥æ™ºèƒ½ã€çº¦æŸé©±åŠ¨çš„ç©ºé—´äººå·¥æ™ºèƒ½å’Œæœºæ¢°ç©ºé—´å»ºæ¨¡çš„é‡è¦æ€§ï¼Œä»¥åŠå°†äººå·¥æ™ºèƒ½ä¸å‡è®¾é©±åŠ¨ç­–ç•¥å’Œæ¨¡å‹ç³»ç»Ÿç›¸ç»“åˆä»¥ä½“ç°ç™Œç—‡ç©ºé—´ä¿¡æ¯ä»·å€¼çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æŠ€æœ¯çªç ´åœ¨äººå·¥æ™ºèƒ½å’Œç©ºé—´ç»„å­¦é¢†åŸŸä¸ºç†è§£ç™Œç—‡ç»†èƒå’Œè‚¿ç˜¤å¾®ç¯å¢ƒå¸¦æ¥å˜é©æ½œåŠ›ã€‚</li>
<li>å½“å‰æŒ‘æˆ˜åœ¨äºå¼€å‘å¯è§£é‡Šçš„ç©ºé—´äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œéœ€è¦æ”¹è¿›æ•°æ®é›†æˆå’Œæ–°æ¦‚å¿µæ¡†æ¶ã€‚</li>
<li>äººå·¥æ™ºèƒ½åœ¨ç©ºé—´ç»„å­¦ä¸­çš„è§’è‰²è¢«é‡ç‚¹è®¨è®ºï¼ŒåŒ…æ‹¬å½“å‰æŠ€æœ¯å’Œè§£è¯»ç™Œç—‡ç”Ÿç‰©å­¦çš„éœ€æ±‚ã€‚</li>
<li>æ–°å…´èŒƒå¼å¦‚æ•°æ®é©±åŠ¨å’Œçº¦æŸé©±åŠ¨çš„ç©ºé—´äººå·¥æ™ºèƒ½ä»¥åŠæœºæ¢°ç©ºé—´å»ºæ¨¡å—åˆ°å…³æ³¨ã€‚</li>
<li>äººå·¥æ™ºèƒ½ä¸å‡è®¾é©±åŠ¨ç­–ç•¥å’Œæ¨¡å‹ç³»ç»Ÿçš„ç»“åˆå¯¹äºä½“ç°ç™Œç—‡ç©ºé—´ä¿¡æ¯ä»·å€¼è‡³å…³é‡è¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5d9f7bfa23be10f26b53774090037a77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c8d065e3e5caf650703cef0231e0926.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MDPG-Multi-domain-Diffusion-Prior-Guidance-for-MRI-Reconstruction"><a href="#MDPG-Multi-domain-Diffusion-Prior-Guidance-for-MRI-Reconstruction" class="headerlink" title="MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction"></a>MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction</h2><p><strong>Authors:Lingtong Zhang, Mengdie Song, Xiaohan Hao, Huayu Mai, Bensheng Qiu</strong></p>
<p>Magnetic Resonance Imaging (MRI) reconstruction is essential in medical diagnostics. As the latest generative models, diffusion models (DMs) have struggled to produce high-fidelity images due to their stochastic nature in image domains. Latent diffusion models (LDMs) yield both compact and detailed prior knowledge in latent domains, which could effectively guide the model towards more effective learning of the original data distribution. Inspired by this, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by pre-trained LDMs to enhance data consistency in MRI reconstruction tasks. Specifically, we first construct a Visual-Mamba-based backbone, which enables efficient encoding and reconstruction of under-sampled images. Then pre-trained LDMs are integrated to provide conditional priors in both latent and image domains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion in multi-level latent domains. Simultaneously, to effectively utilize a prior in both the k-space and image domain, under-sampled images are fused with generated full-sampled images by the Dual-domain Fusion Branch (DFB) for self-adaption guidance. Lastly, to further enhance the data consistency, we propose a k-space regularization strategy based on the non-auto-calibration signal (NACS) set. Extensive experiments on two public MRI datasets fully demonstrate the effectiveness of the proposed methodology. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Zolento/MDPG">https://github.com/Zolento/MDPG</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰é‡å»ºåœ¨åŒ»å­¦è¯Šæ–­ä¸­è‡³å…³é‡è¦ã€‚ä½œä¸ºæœ€æ–°çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ç”±äºå…¶å›¾åƒåŸŸçš„éšæœºæ€§ï¼Œéš¾ä»¥äº§ç”Ÿé«˜ä¿çœŸåº¦çš„å›¾åƒã€‚æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨æ½œåœ¨åŸŸä¸­äº§ç”Ÿäº†ç´§å‡‘ä¸”è¯¦ç»†çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¼•å¯¼æ¨¡å‹æ›´æœ‰æ•ˆåœ°å­¦ä¹ åŸå§‹æ•°æ®åˆ†å¸ƒã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ç”±é¢„è®­ç»ƒçš„LDMsæä¾›çš„å¤šåŸŸæ‰©æ•£å…ˆéªŒæŒ‡å¯¼ï¼ˆMDPGï¼‰ï¼Œä»¥æé«˜MRIé‡å»ºä»»åŠ¡ä¸­çš„æ•°æ®ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŸºäºVisual-Mambaçš„éª¨å¹²ç½‘ï¼Œä½¿æ¬ é‡‡æ ·å›¾åƒçš„ç¼–ç å’Œé‡å»ºæ›´åŠ é«˜æ•ˆã€‚ç„¶åï¼Œå°†é¢„è®­ç»ƒçš„LDMsé›†æˆåˆ°æ½œåœ¨åŸŸå’Œå›¾åƒåŸŸä¸­ï¼Œä»¥æä¾›æ¡ä»¶å…ˆéªŒã€‚æå‡ºäº†ä¸€ç§æ–°çš„æ½œåœ¨å¼•å¯¼æ³¨æ„åŠ›ï¼ˆLGAï¼‰ï¼Œç”¨äºå¤šçº§æ½œåœ¨åŸŸä¸­çš„æœ‰æ•ˆèåˆã€‚åŒæ—¶ï¼Œä¸ºäº†æœ‰æ•ˆåœ°åˆ©ç”¨kç©ºé—´å’Œå›¾åƒåŸŸä¸­çš„å…ˆéªŒä¿¡æ¯ï¼Œæ¬ é‡‡æ ·å›¾åƒé€šè¿‡åŒåŸŸèåˆåˆ†æ”¯ï¼ˆDFBï¼‰ä¸ç”Ÿæˆçš„å®Œå…¨é‡‡æ ·å›¾åƒèåˆï¼Œä»¥å®ç°è‡ªé€‚åº”å¼•å¯¼ã€‚æœ€åï¼Œä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºæ•°æ®ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéè‡ªåŠ¨æ ¡å‡†ä¿¡å·ï¼ˆNACSï¼‰é›†çš„kç©ºé—´æ­£åˆ™åŒ–ç­–ç•¥ã€‚åœ¨ä¸¤ä¸ªå…¬å…±MRIæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒå……åˆ†è¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Zolento/MDPG">https://github.com/Zolento/MDPG</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23701v1">PDF</a> Accept by MICCAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¤šåŸŸæ‰©æ•£å…ˆéªŒæŒ‡å¯¼ï¼ˆMDPGï¼‰çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰é‡å»ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰æä¾›æ¡ä»¶å…ˆéªŒï¼Œé€šè¿‡æ„å»ºè§†è§‰MambaåŸºç¡€æ¶æ„å¹¶åœ¨æ½œåœ¨å’Œå›¾åƒåŸŸä¸­æ•´åˆLDMï¼Œå®ç°é«˜æ•ˆç¼–ç å’Œé‡å»ºã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºä¸€ç§æ–°é¢–çš„åŒåŸŸèåˆåˆ†æ”¯ï¼ˆDFBï¼‰å’Œkç©ºé—´æ­£åˆ™åŒ–ç­–ç•¥æ¥è¿›ä¸€æ­¥å¢å¼ºæ•°æ®ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ‰©æ•£æ¨¡å‹åœ¨MRIé‡å»ºä¸­é¢ä¸´é«˜ä¿çœŸå›¾åƒç”ŸæˆæŒ‘æˆ˜ã€‚</li>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨æ½œåœ¨åŸŸæä¾›ç´§å‡‘ä¸”è¯¦ç»†çš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>æå‡ºå¤šåŸŸæ‰©æ•£å…ˆéªŒæŒ‡å¯¼ï¼ˆMDPGï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒLDMå¢å¼ºMRIé‡å»ºçš„æ•°æ®ä¸€è‡´æ€§ã€‚</li>
<li>æ„å»ºè§†è§‰MambaåŸºç¡€æ¶æ„å®ç°é«˜æ•ˆç¼–ç å’Œé‡å»ºã€‚</li>
<li>æ•´åˆé¢„è®­ç»ƒLDMåœ¨æ½œåœ¨å’Œå›¾åƒåŸŸæä¾›æ¡ä»¶å…ˆéªŒã€‚</li>
<li>æå‡ºæ–°é¢–çš„åŒåŸŸèåˆåˆ†æ”¯ï¼ˆDFBï¼‰è¿›è¡Œè‡ªé€‚åº”æŒ‡å¯¼ã€‚</li>
<li>å®æ–½åŸºäºéæ ¡å‡†ä¿¡å·çš„kç©ºé—´æ­£åˆ™åŒ–ç­–ç•¥æ¥å¢å¼ºæ•°æ®ä¸€è‡´æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23701">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b156120b0d5bd5512c9e5c364dee62e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-835586027be10aa53b8c30174edaa4c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b211d62e7ce29b3504d19d80533492d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GUSL-A-Novel-and-Efficient-Machine-Learning-Model-for-Prostate-Segmentation-on-MRI"><a href="#GUSL-A-Novel-and-Efficient-Machine-Learning-Model-for-Prostate-Segmentation-on-MRI" class="headerlink" title="GUSL: A Novel and Efficient Machine Learning Model for Prostate   Segmentation on MRI"></a>GUSL: A Novel and Efficient Machine Learning Model for Prostate   Segmentation on MRI</h2><p><strong>Authors:Jiaxin Yang, Vasileios Magoulianitis, Catherine Aurelia Christie Alexander, Jintang Xue, Masatomo Kaneko, Giovanni Cacciamani, Andre Abreu, Vinay Duddalwar, C. -C. Jay Kuo, Inderbir S. Gill, Chrysostomos Nikias</strong></p>
<p>Prostate and zonal segmentation is a crucial step for clinical diagnosis of prostate cancer (PCa). Computer-aided diagnosis tools for prostate segmentation are based on the deep learning (DL) paradigm. However, deep neural networks are perceived as â€œblack-boxâ€ solutions by physicians, thus making them less practical for deployment in the clinical setting. In this paper, we introduce a feed-forward machine learning model, named Green U-shaped Learning (GUSL), suitable for medical image segmentation without backpropagation. GUSL introduces a multi-layer regression scheme for coarse-to-fine segmentation. Its feature extraction is based on a linear model, which enables seamless interpretability during feature extraction. Also, GUSL introduces a mechanism for attention on the prostate boundaries, which is an error-prone region, by employing regression to refine the predictions through residue correction. In addition, a two-step pipeline approach is used to mitigate the class imbalance, an issue inherent in medical imaging problems. After conducting experiments on two publicly available datasets and one private dataset, in both prostate gland and zonal segmentation tasks, GUSL achieves state-of-the-art performance among other DL-based models. Notably, GUSL features a very energy-efficient pipeline, since it has a model size several times smaller and less complexity than the rest of the solutions. In all datasets, GUSL achieved a Dice Similarity Coefficient (DSC) performance greater than $0.9$ for gland segmentation. Considering also its lightweight model size and transparency in feature extraction, it offers a competitive and practical package for medical imaging applications. </p>
<blockquote>
<p>å‰åˆ—è…ºç™Œï¼ˆPCaï¼‰çš„ä¸´åºŠè¯Šæ–­ä¸­ï¼Œå‰åˆ—è…ºåŠå…¶åˆ†åŒºçš„åˆ†å‰²æ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„å‰åˆ—è…ºåˆ†å‰²è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å·¥å…·è¢«å¹¿æ³›é‡‡ç”¨ã€‚ç„¶è€Œï¼Œæ·±åº¦ç¥ç»ç½‘ç»œè¢«åŒ»ç”Ÿè§†ä¸ºâ€œé»‘ç®±â€è§£å†³æ–¹æ¡ˆï¼Œå› æ­¤åœ¨ä¸´åºŠç¯å¢ƒä¸­å®ç”¨æ€§è¾ƒä½ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å‰é¦ˆæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œåä¸ºç»¿è‰²Uå½¢å­¦ä¹ ï¼ˆGUSLï¼‰ï¼Œé€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œæ— éœ€åå‘ä¼ æ’­ã€‚GUSLå¼•å…¥äº†ä¸€ç§å¤šå±‚å›å½’æ–¹æ¡ˆï¼Œç”¨äºä»ç²—åˆ°ç»†è¿›è¡Œåˆ†å‰²ã€‚å…¶ç‰¹æ€§æå–åŸºäºçº¿æ€§æ¨¡å‹ï¼Œå¯åœ¨ç‰¹æ€§æå–è¿‡ç¨‹ä¸­å®ç°æ— ç¼è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼ŒGUSLé€šè¿‡åœ¨å›å½’ä¸­é‡‡ç”¨ä¸€ç§å…³æ³¨å‰åˆ—è…ºè¾¹ç•Œï¼ˆä¸€ä¸ªæ˜“å‡ºé”™åŒºåŸŸï¼‰çš„æœºåˆ¶ï¼Œé€šè¿‡æ®‹å·®ä¿®æ­£æ¥ä¼˜åŒ–é¢„æµ‹ã€‚åŒæ—¶ï¼Œé‡‡ç”¨ä¸¤æ­¥ç®¡é“æ–¹æ³•ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œè¿™æ˜¯åŒ»å­¦æˆåƒé—®é¢˜æ‰€å›ºæœ‰çš„ã€‚åœ¨å…¬å¼€æ•°æ®é›†å’Œç§æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œçš„å‰åˆ—è…ºå’Œåˆ†åŒºåˆ†å‰²ä»»åŠ¡å®éªŒè¡¨æ˜ï¼ŒGUSLåœ¨ä¸å…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸æ¯”æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒGUSLçš„ç®¡é“éå¸¸èŠ‚èƒ½ï¼Œå› ä¸ºå…¶æ¨¡å‹å¤§å°è¾ƒå°ä¸”å¤æ‚æ€§ä½äºå…¶ä»–è§£å†³æ–¹æ¡ˆã€‚åœ¨æ‰€æœ‰æ•°æ®é›†ä¸­ï¼ŒGUSLçš„è…ºä½“åˆ†å‰²ç‹„å…‹ç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰æ€§èƒ½å‡å¤§äº0.9ã€‚è€ƒè™‘åˆ°å…¶è½»é‡çº§çš„æ¨¡å‹å¤§å°å’Œç‰¹æ€§æå–çš„é€æ˜åº¦ï¼Œå®ƒä¸ºåŒ»å­¦æˆåƒåº”ç”¨æä¾›äº†ä¸€ä¸ªæœ‰ç«äº‰åŠ›çš„å®ç”¨è½¯ä»¶åŒ…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23688v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGreen U-shaped Learningï¼ˆGUSLï¼‰çš„å‰é¦ˆæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œé€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œæ— éœ€åå‘ä¼ æ’­ã€‚GUSLé‡‡ç”¨å¤šå±‚å›å½’æ–¹æ¡ˆå®ç°ç”±ç²—åˆ°ç»†çš„åˆ†å‰²ï¼Œç‰¹å¾æå–åŸºäºçº¿æ€§æ¨¡å‹ï¼Œå®ç°æ— ç¼è§£è¯»ã€‚é€šè¿‡å›å½’å¯¹å‰åˆ—è…ºè¾¹ç•Œè¿›è¡Œå…³æ³¨ï¼Œå¹¶é‡‡ç”¨ä¸¤æ­¥ç®¡é“æ–¹æ³•ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGUSLåœ¨å‰åˆ—è…ºè…ºä½“åŠåŒºåŸŸåˆ†å‰²ä»»åŠ¡ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä¸”å…·å¤‡é«˜èƒ½æ•ˆã€æ¨¡å‹ä½“ç§¯å°ã€é€æ˜åº¦é«˜ç­‰ä¼˜ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUSLæ˜¯ä¸€ç§é€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„å‰é¦ˆæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ— éœ€åå‘ä¼ æ’­ï¼Œé€‚åˆåœ¨ä¸´åºŠç¯å¢ƒä¸­åº”ç”¨ã€‚</li>
<li>GUSLé‡‡ç”¨å¤šå±‚å›å½’æ–¹æ¡ˆï¼Œå®ç°ä»ç²—åˆ°ç»†çš„åˆ†å‰²ã€‚</li>
<li>ç‰¹å¾æå–åŸºäºçº¿æ€§æ¨¡å‹ï¼Œå¢å¼ºæ¨¡å‹çš„é€æ˜åº¦ï¼Œä¾¿äºåŒ»ç”Ÿç†è§£ã€‚</li>
<li>GUSLé€šè¿‡å…³æ³¨å‰åˆ—è…ºè¾¹ç•Œï¼Œä½¿ç”¨å›å½’è¿›è¡Œé¢„æµ‹ä¿®æ­£ï¼Œæé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>é‡‡ç”¨ä¸¤æ­¥ç®¡é“æ–¹æ³•ç¼“è§£åŒ»å­¦æˆåƒä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒGUSLåœ¨å‰åˆ—è…ºè…ºä½“åŠåŒºåŸŸåˆ†å‰²ä»»åŠ¡ä¸­æ€§èƒ½ä¼˜è¶Šï¼ŒDice Similarity Coefficientï¼ˆDSCï¼‰å¤§äº0.9ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2bf8fe82f4faa71ff28e347f810b2b97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-791846c86c7fc5f5febe894ea6830463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-957fa2acdb9be115ea5738e6b95f7d52.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a12978c3d33cd0b686efb4c8d9debc06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f95857daff61d9b0fab2fbbabab41708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adb8550512e1f866db2515fe1eb743b8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-based-Data-Augmentation-Method-for-Fetal-Head-Ultrasound-Segmentation"><a href="#Diffusion-Model-based-Data-Augmentation-Method-for-Fetal-Head-Ultrasound-Segmentation" class="headerlink" title="Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound   Segmentation"></a>Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound   Segmentation</h2><p><strong>Authors:Fangyijie Wang, Kevin Whelan, FÃ©lix Balado, GuÃ©nolÃ© Silvestre, Kathleen M. Curran</strong></p>
<p>Medical image data is less accessible than in other domains due to privacy and regulatory constraints. In addition, labeling requires costly, time-intensive manual image annotation by clinical experts. To overcome these challenges, synthetic medical data generation offers a promising solution. Generative AI (GenAI), employing generative deep learning models, has proven effective at producing realistic synthetic images. This study proposes a novel mask-guided GenAI approach using diffusion models to generate synthetic fetal head ultrasound images paired with segmentation masks. These synthetic pairs augment real datasets for supervised fine-tuning of the Segment Anything Model (SAM). Our results show that the synthetic data captures real image features effectively, and this approach reaches state-of-the-art fetal head segmentation, especially when trained with a limited number of real image-mask pairs. In particular, the segmentation reaches Dice Scores of 94.66% and 94.38% using a handful of ultrasound images from the Spanish and African cohorts, respectively. Our code, models, and data are available on GitHub. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæ•°æ®ç”±äºéšç§å’Œç›‘ç®¡é™åˆ¶ï¼Œç›¸è¾ƒäºå…¶ä»–é¢†åŸŸæ›´éš¾ä»¥è·å–ã€‚æ­¤å¤–ï¼Œæ ‡æ³¨éœ€è¦ä¸´åºŠä¸“å®¶è¿›è¡Œè€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚çš„æ‰‹åŠ¨å›¾åƒæ³¨é‡Šã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼ŒåˆæˆåŒ»å­¦æ•°æ®ç”Ÿæˆæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚é‡‡ç”¨ç”Ÿæˆå¼æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰å·²è¢«è¯æ˜èƒ½å¤Ÿäº§ç”Ÿé€¼çœŸçš„åˆæˆå›¾åƒã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„æ©è†œå¼•å¯¼GenAIæ–¹æ³•ï¼Œç”¨äºç”Ÿæˆé…æœ‰åˆ†å‰²æ©è†œçš„åˆæˆèƒå„¿å¤´éƒ¨è¶…å£°å›¾åƒã€‚è¿™äº›åˆæˆå›¾åƒå¯¹ç”¨äºæ‰©å……çœŸå®æ•°æ®é›†ï¼Œä»¥ç›‘ç£å¾®è°ƒåˆ†æ®µä»»ä½•äº‹æƒ…æ¨¡å‹ï¼ˆSAMï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåˆæˆæ•°æ®æœ‰æ•ˆåœ°æ•æ‰äº†çœŸå®å›¾åƒçš„ç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨æœ‰é™æ•°é‡çš„çœŸå®å›¾åƒ-æ©è†œå¯¹è¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¿™ç§æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„èƒå„¿å¤´éƒ¨åˆ†å‰²æ•ˆæœã€‚ç‰¹åˆ«æ˜¯ï¼Œä½¿ç”¨æ¥è‡ªè¥¿ç­ç‰™å’Œéæ´²é˜Ÿåˆ—çš„å°‘é‡è¶…å£°å›¾åƒï¼Œåˆ†å‰²è¾¾åˆ°Diceåˆ†æ•°åˆ†åˆ«ä¸º94.66%å’Œ94.38%ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å‡å¯åœ¨GitHubä¸Šè·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23664v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒæ•°æ®ç”±äºéšç§å’Œç›‘ç®¡é™åˆ¶è€Œéš¾ä»¥è·å–ï¼Œä¸”æ ‡æ³¨éœ€è¦ä¸´åºŠä¸“å®¶è¿›è¡Œè€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚çš„æ‰‹åŠ¨å›¾åƒæ ‡æ³¨ã€‚ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹é®ç½©å¼•å¯¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGenAIï¼‰æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆé…æœ‰åˆ†å‰²é®ç½©çš„åˆæˆèƒå„¿å¤´éƒ¨è¶…å£°å›¾åƒã€‚è¿™äº›åˆæˆæ•°æ®å¯¹çœŸå®æ•°æ®é›†è¿›è¡Œå¢å¼ºï¼Œç”¨äºç›‘ç£å¾®è°ƒSAMï¼ˆSegment Anything Modelï¼‰æ¨¡å‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œåˆæˆæ•°æ®èƒ½æœ‰æ•ˆæ•æ‰çœŸå®å›¾åƒç‰¹å¾ï¼Œä¸”åœ¨æœ‰é™çœŸå®å›¾åƒ-é®ç½©å¯¹è®­ç»ƒä¸‹è¾¾åˆ°å…ˆè¿›çš„èƒå„¿å¤´éƒ¨åˆ†å‰²æ•ˆæœï¼ŒDiceå¾—åˆ†åˆ†åˆ«ä¸º94.66%å’Œ94.38%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ•°æ®å› éšç§å’Œç›‘ç®¡é—®é¢˜è€Œéš¾ä»¥è·å–ã€‚</li>
<li>æ ‡æ³¨åŒ»å­¦å›¾åƒéœ€è¦ä¸´åºŠä¸“å®¶æ‰‹åŠ¨è¿›è¡Œï¼Œæˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ–°å‹é®ç½©å¼•å¯¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGenAIï¼‰åˆ›é€ åˆæˆåŒ»å­¦å›¾åƒã€‚</li>
<li>åˆæˆèƒå„¿å¤´éƒ¨è¶…å£°å›¾åƒä¸çœŸå®å›¾åƒç»“åˆï¼Œç”¨äºè®­ç»ƒSAMæ¨¡å‹ã€‚</li>
<li>åˆæˆæ•°æ®èƒ½æœ‰æ•ˆæ¨¡æ‹ŸçœŸå®åŒ»å­¦å›¾åƒç‰¹å¾ã€‚</li>
<li>åœ¨æœ‰é™çœŸå®å›¾åƒ-é®ç½©å¯¹è®­ç»ƒä¸‹ï¼Œè¾¾åˆ°å…ˆè¿›çš„èƒå„¿å¤´éƒ¨åˆ†å‰²æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2423c23496a4f78b83a54ddaecfd87c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7dce93e8b19db2c3d8d9015eb56e0908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7b6a8a7abfb2aabf66b7f083e5bb127.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FD-DiT-Frequency-Domain-Directed-Diffusion-Transformer-for-Low-Dose-CT-Reconstruction"><a href="#FD-DiT-Frequency-Domain-Directed-Diffusion-Transformer-for-Low-Dose-CT-Reconstruction" class="headerlink" title="FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT   Reconstruction"></a>FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT   Reconstruction</h2><p><strong>Authors:Qiqing Liu, Guoquan Wei, Zekun Zhou, Yiyang Wen, Liu Shi, Qiegen Liu</strong></p>
<p>Low-dose computed tomography (LDCT) reduces radiation exposure but suffers from image artifacts and loss of detail due to quantum and electronic noise, potentially impacting diagnostic accuracy. Transformer combined with diffusion models has been a promising approach for image generation. Nevertheless, existing methods exhibit limitations in preserving finegrained image details. To address this issue, frequency domain-directed diffusion transformer (FD-DiT) is proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy that progressively introduces noise until the distribution statistically aligns with that of LDCT data, followed by denoising processing. Furthermore, we employ a frequency decoupling technique to concentrate noise primarily in high-frequency domain, thereby facilitating effective capture of essential anatomical structures and fine details. A hybrid denoising network is then utilized to optimize the overall data reconstruction process. To enhance the capability in recognizing high-frequency noise, we incorporate sliding sparse local attention to leverage the sparsity and locality of shallow-layer information, propagating them via skip connections for improving feature representation. Finally, we propose a learnable dynamic fusion strategy for optimal component integration. Experimental results demonstrate that at identical dose levels, LDCT images reconstructed by FD-DiT exhibit superior noise and artifact suppression compared to state-of-the-art methods. </p>
<blockquote>
<p>ä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰é™ä½äº†è¾å°„æš´éœ²ï¼Œä½†ç”±äºé‡å­å’Œç”µå­å™ªå£°çš„å½±å“ï¼Œä¼šå‡ºç°å›¾åƒä¼ªå½±å’Œç»†èŠ‚æŸå¤±ï¼Œä»è€Œå¯èƒ½å½±å“è¯Šæ–­å‡†ç¡®æ€§ã€‚ç»“åˆæ‰©æ•£æ¨¡å‹çš„å˜å‹å™¨åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„å‰æ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨ä¿ç•™ç²¾ç»†å›¾åƒç»†èŠ‚æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†é¢‘ç‡åŸŸå¯¼å‘æ‰©æ•£å˜å‹å™¨ï¼ˆFD-DiTï¼‰ç”¨äºLDCTé‡å»ºã€‚FD-DiTä¸“æ³¨äºä¸€ç§æ‰©æ•£ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€æ­¥å¼•å…¥å™ªå£°ï¼Œç›´åˆ°ç»Ÿè®¡åˆ†å¸ƒä¸LDCTæ•°æ®åˆ†å¸ƒå¯¹é½ï¼Œç„¶åè¿›è¡Œå»å™ªå¤„ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨é¢‘ç‡è§£è€¦æŠ€æœ¯ï¼Œå°†å™ªå£°ä¸»è¦é›†ä¸­åœ¨é«˜é¢‘åŸŸï¼Œä»è€Œä¾¿äºæœ‰æ•ˆæ•è·é‡è¦çš„è§£å‰–ç»“æ„å’Œç»†èŠ‚ã€‚ç„¶åï¼Œä½¿ç”¨æ··åˆå»å™ªç½‘ç»œä¼˜åŒ–æ•´ä½“æ•°æ®é‡å»ºè¿‡ç¨‹ã€‚ä¸ºäº†æé«˜è¯†åˆ«é«˜é¢‘å™ªå£°çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ»‘åŠ¨ç¨€ç–å±€éƒ¨æ³¨æ„åŠ›ï¼Œåˆ©ç”¨æµ…å±‚ä¿¡æ¯çš„ç¨€ç–æ€§å’Œå±€éƒ¨æ€§ï¼Œé€šè¿‡è·³è¿‡è¿æ¥ä¼ æ’­å®ƒä»¬ï¼Œä»¥æ”¹è¿›ç‰¹å¾è¡¨ç¤ºã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯å­¦ä¹ çš„åŠ¨æ€èåˆç­–ç•¥ï¼Œä»¥å®ç°æœ€ä½³ç»„ä»¶é›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒå‰‚é‡æ°´å¹³ä¸‹ï¼Œé‡‡ç”¨FD-DiTé‡å»ºçš„LDCTå›¾åƒä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„å™ªå£°å’Œä¼ªå½±æŠ‘åˆ¶æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23466v1">PDF</a> 11pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>ä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰è™½èƒ½å‡å°‘è¾å°„æš´éœ²ï¼Œä½†å­˜åœ¨å›¾åƒä¼ªå½±å’Œç»†èŠ‚ä¸¢å¤±é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§é¢‘ç‡åŸŸå¯¼å‘çš„æ‰©æ•£å˜å‹å™¨ï¼ˆFD-DiTï¼‰æ–¹æ³•ç”¨äºLDCTé‡å»ºï¼Œé€šè¿‡æ‰©æ•£ç­–ç•¥é€æ­¥å¼•å…¥å™ªå£°ï¼Œå†é‡‡ç”¨å»å™ªå¤„ç†ã€‚ç»“åˆé¢‘ç‡è§£è€¦æŠ€æœ¯å’Œæ··åˆå»å™ªç½‘ç»œï¼Œæé«˜å›¾åƒè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒå‰‚é‡æ°´å¹³ä¸‹ï¼ŒFD-DiTé‡å»ºçš„LDCTå›¾åƒå…·æœ‰æ›´å¥½çš„å™ªå£°å’Œä¼ªå½±æŠ‘åˆ¶æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDCTå­˜åœ¨å›¾åƒä¼ªå½±å’Œç»†èŠ‚ä¸¢å¤±çš„é—®é¢˜ã€‚</li>
<li>é¢‘ç‡åŸŸå¯¼å‘çš„æ‰©æ•£å˜å‹å™¨ï¼ˆFD-DiTï¼‰è¢«æå‡ºæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>FD-DiTé‡‡ç”¨æ‰©æ•£ç­–ç•¥é€æ­¥å¼•å…¥å™ªå£°ï¼Œå¹¶æ³¨é‡é«˜é¢‘åŸŸçš„å™ªå£°å¤„ç†ã€‚</li>
<li>ç»“åˆé¢‘ç‡è§£è€¦æŠ€æœ¯å’Œæ··åˆå»å™ªç½‘ç»œæ¥æé«˜å›¾åƒè´¨é‡ã€‚</li>
<li>é‡‡ç”¨æ»‘åŠ¨ç¨€ç–å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶æ¥è¯†åˆ«é«˜é¢‘å™ªå£°ã€‚</li>
<li>æå‡ºä¸€ç§å¯å­¦ä¹ çš„åŠ¨æ€èåˆç­–ç•¥æ¥å®ç°æœ€ä½³ç»„ä»¶é›†æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23466">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a880236d7447a486a0963a14f3a8275f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-232793d4acf81f8499034232e47a3793.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80a28a8922bfe48da6f62334adcea8be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67842c61c81aef2e0862ce1c99caeb96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69bfea0ee7478f78d0c2cce6351c76c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ab1794c96b11d7131d9b21d55bedab0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Contrastive-Learning-with-Diffusion-Features-for-Weakly-Supervised-Medical-Image-Segmentation"><a href="#Contrastive-Learning-with-Diffusion-Features-for-Weakly-Supervised-Medical-Image-Segmentation" class="headerlink" title="Contrastive Learning with Diffusion Features for Weakly Supervised   Medical Image Segmentation"></a>Contrastive Learning with Diffusion Features for Weakly Supervised   Medical Image Segmentation</h2><p><strong>Authors:Dewen Zeng, Xinrong Hu, Yu-Jen Chen, Yawen Wu, Xiaowei Xu, Yiyu Shi</strong></p>
<p>Weakly supervised semantic segmentation (WSSS) methods using class labels often rely on class activation maps (CAMs) to localize objects. However, traditional CAM-based methods struggle with partial activations and imprecise object boundaries due to optimization discrepancies between classification and segmentation. Recently, the conditional diffusion model (CDM) has been used as an alternative for generating segmentation masks in WSSS, leveraging its strong image generation capabilities tailored to specific class distributions. By modifying or perturbing the condition during diffusion sampling, the related objects can be highlighted in the generated images. Yet, the saliency maps generated by CDMs are prone to noise from background alterations during reverse diffusion. To alleviate the problem, we introduce Contrastive Learning with Diffusion Features (CLDF), a novel method that uses contrastive learning to train a pixel decoder to map the diffusion features from a frozen CDM to a low-dimensional embedding space for segmentation. Specifically, we integrate gradient maps generated from CDM external classifier with CAMs to identify foreground and background pixels with fewer false positives&#x2F;negatives for contrastive learning, enabling robust pixel embedding learning. Experimental results on four segmentation tasks from two public medical datasets demonstrate that our method significantly outperforms existing baselines. </p>
<blockquote>
<p>åŸºäºå¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰çš„æ–¹æ³•å¸¸å¸¸åˆ©ç”¨ç±»åˆ«æ ‡ç­¾è¿›è¡Œå®šä½å¯¹è±¡çš„æ–¹æ³•ï¼Œé‡‡ç”¨ç±»æ¿€æ´»æ˜ å°„ï¼ˆCAMsï¼‰ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºCAMçš„æ–¹æ³•ç”±äºåˆ†ç±»å’Œåˆ†å‰²ä¹‹é—´çš„ä¼˜åŒ–å·®å¼‚ï¼Œå¯¹äºéƒ¨åˆ†æ¿€æ´»å’Œä¸ç²¾ç¡®çš„å¯¹è±¡è¾¹ç•Œæ„Ÿåˆ°å›°æ‰°ã€‚æœ€è¿‘ï¼Œæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆCDMï¼‰å·²è¢«ç”¨ä½œç”ŸæˆWSSSä¸­çš„åˆ†å‰²æ©ç çš„æ›¿ä»£æ–¹æ³•ï¼Œåˆ©ç”¨å…¶é’ˆå¯¹ç‰¹å®šç±»åˆ«åˆ†å¸ƒå®šåˆ¶çš„å¼ºå¤§å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡åœ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­ä¿®æ”¹æˆ–æ‰°åŠ¨æ¡ä»¶ï¼Œå¯ä»¥åœ¨ç”Ÿæˆçš„å›¾åƒä¸­çªå‡ºæ˜¾ç¤ºç›¸å…³å¯¹è±¡ã€‚ç„¶è€Œï¼Œç”±CDMç”Ÿæˆçš„æ˜¾è‘—æ€§æ˜ å°„å®¹æ˜“åœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­å—åˆ°èƒŒæ™¯æ”¹å˜è€Œäº§ç”Ÿçš„å™ªå£°å¹²æ‰°ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯¹æ¯”å­¦ä¹ æ‰©æ•£ç‰¹å¾ï¼ˆCLDFï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒåƒç´ è§£ç å™¨çš„æ–°æ–¹æ³•ï¼Œå°†æ¥è‡ªå†»ç»“CDMçš„æ‰©æ•£ç‰¹å¾æ˜ å°„åˆ°ä½ç»´åµŒå…¥ç©ºé—´è¿›è¡Œåˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ç”±CDMå¤–éƒ¨åˆ†ç±»å™¨ç”Ÿæˆçš„æ¢¯åº¦å›¾ä¸CAMç›¸ç»“åˆï¼Œä»¥ç¡®å®šç”¨äºå¯¹æ¯”å­¦ä¹ çš„å‰æ™¯å’ŒèƒŒæ™¯åƒç´ ï¼Œä»¥å‡å°‘è¯¯æŠ¥&#x2F;æ¼æŠ¥çš„æ•°é‡ï¼Œä»è€Œå®ç°ç¨³å¥çš„åƒç´ åµŒå…¥å­¦ä¹ ã€‚åœ¨æ¥è‡ªä¸¤ä¸ªå…¬å…±åŒ»ç–—æ•°æ®é›†çš„å››ä¸ªåˆ†å‰²ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23460v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆCDMï¼‰çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒåƒç´ è§£ç å™¨ï¼Œå°†æ‰©æ•£ç‰¹å¾æ˜ å°„åˆ°ä½ç»´åµŒå…¥ç©ºé—´è¿›è¡Œåˆ†å‰²ã€‚æ–°æ–¹æ³•ç»“åˆCDMå¤–éƒ¨åˆ†ç±»å™¨ç”Ÿæˆçš„æ¢¯åº¦å›¾å’Œç±»æ¿€æ´»æ˜ å°„ï¼ˆCAMsï¼‰ï¼Œæé«˜äº†å‰æ™¯å’ŒèƒŒæ™¯åƒç´ çš„è¯†åˆ«èƒ½åŠ›ï¼Œå‡å°‘äº†è¯¯æŠ¥&#x2F;æ¼æŠ¥ï¼Œå®ç°äº†ç¨³å¥çš„åƒç´ åµŒå…¥å­¦ä¹ ã€‚åœ¨å…¬å…±åŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰åœ¨åˆ©ç”¨ç±»æ ‡ç­¾è¿›è¡Œå¯¹è±¡å®šä½æ—¶å¸¸å¸¸ä¾èµ–äºç±»æ¿€æ´»æ˜ å°„ï¼ˆCAMsï¼‰ã€‚</li>
<li>ä¼ ç»ŸåŸºäºCAMçš„æ–¹æ³•åœ¨éƒ¨åˆ†æ¿€æ´»å’Œä¸ç²¾ç¡®çš„å¯¹è±¡è¾¹ç•Œä¸Šå­˜åœ¨é—®é¢˜ï¼Œå› ä¸ºåˆ†ç±»å’Œåˆ†å‰²ä¹‹é—´çš„ä¼˜åŒ–å·®å¼‚ã€‚</li>
<li>æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆCDMï¼‰å·²è¢«ç”¨äºç”ŸæˆWSSSä¸­çš„åˆ†å‰²æ©è†œï¼Œåˆ©ç”¨å…¶é’ˆå¯¹ç‰¹å®šç±»åˆ†å¸ƒçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ä¿®æ”¹æˆ–æ‰°åŠ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­çš„æ¡ä»¶ï¼Œå¯ä»¥çªå‡ºä¸ç±»ç›¸å…³çš„å¯¹è±¡ã€‚</li>
<li>CDMç”Ÿæˆçš„æ˜¾è‘—æ€§æ˜ å°„å®¹æ˜“å—åˆ°åå‘æ‰©æ•£è¿‡ç¨‹ä¸­èƒŒæ™¯æ”¹å˜äº§ç”Ÿçš„å™ªå£°å½±å“ã€‚</li>
<li>æ–°æ–¹æ³•Contrastive Learning with Diffusion Features (CLDF)ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ¥è®­ç»ƒåƒç´ è§£ç å™¨ï¼Œå°†æ‰©æ•£ç‰¹å¾æ˜ å°„åˆ°ä½ç»´åµŒå…¥ç©ºé—´è¿›è¡Œåˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-455f35321a7f25148ed921842a7f3af6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-073432a04132db70cb746f7ace48a58b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9402e69e63a67620e791f8b87e50943.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-239f00fe2f56718ae174e285877ecb66.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Exposing-and-Mitigating-Calibration-Biases-and-Demographic-Unfairness-in-MLLM-Few-Shot-In-Context-Learning-for-Medical-Image-Classification"><a href="#Exposing-and-Mitigating-Calibration-Biases-and-Demographic-Unfairness-in-MLLM-Few-Shot-In-Context-Learning-for-Medical-Image-Classification" class="headerlink" title="Exposing and Mitigating Calibration Biases and Demographic Unfairness in   MLLM Few-Shot In-Context Learning for Medical Image Classification"></a>Exposing and Mitigating Calibration Biases and Demographic Unfairness in   MLLM Few-Shot In-Context Learning for Medical Image Classification</h2><p><strong>Authors:Xing Shen, Justin Szeto, Mingyang Li, Hengguan Huang, Tal Arbel</strong></p>
<p>Multimodal large language models (MLLMs) have enormous potential to perform few-shot in-context learning in the context of medical image analysis. However, safe deployment of these models into real-world clinical practice requires an in-depth analysis of the accuracies of their predictions, and their associated calibration errors, particularly across different demographic subgroups. In this work, we present the first investigation into the calibration biases and demographic unfairness of MLLMsâ€™ predictions and confidence scores in few-shot in-context learning for medical image classification. We introduce CALIN, an inference-time calibration method designed to mitigate the associated biases. Specifically, CALIN estimates the amount of calibration needed, represented by calibration matrices, using a bi-level procedure: progressing from the population level to the subgroup level prior to inference. It then applies this estimation to calibrate the predicted confidence scores during inference. Experimental results on three medical imaging datasets: PAPILA for fundus image classification, HAM10000 for skin cancer classification, and MIMIC-CXR for chest X-ray classification demonstrate CALINâ€™s effectiveness at ensuring fair confidence calibration in its prediction, while improving its overall prediction accuracies and exhibiting minimum fairness-utility trade-off. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›è¿›è¡Œå°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹å®‰å…¨éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠå®è·µéœ€è¦æ·±å…¥åˆ†æå…¶é¢„æµ‹çš„å‡†ç¡®æ€§ä»¥åŠç›¸å…³çš„æ ¡å‡†è¯¯å·®ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒçš„ç§æ—äººå£äºšç»„ä¹‹é—´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡è°ƒæŸ¥äº†MLLMsé¢„æµ‹ç»“æœçš„æ ¡å‡†åå·®ä»¥åŠäººå£ç»Ÿè®¡å­¦ä¸å…¬å¹³ç°è±¡ï¼Œä»¥åŠå…¶åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»çš„å°‘é‡ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„ç½®ä¿¡åº¦è¯„åˆ†ã€‚æˆ‘ä»¬å¼•å…¥äº†CALINï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå‡å°‘ç›¸å…³åå·®çš„æ¨ç†æ—¶é—´æ ¡å‡†æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒCALINä½¿ç”¨ä¸€ç§åŒå±‚è¿‡ç¨‹æ¥ä¼°è®¡æ‰€éœ€çš„æ ¡å‡†é‡ï¼Œè¯¥è¿‡ç¨‹ç”±æ ¡å‡†çŸ©é˜µè¡¨ç¤ºï¼šä»æ€»ä½“å±‚é¢æ¨è¿›åˆ°äºšç»„å±‚é¢è¿›è¡Œæ¨ç†ã€‚ç„¶åï¼Œå®ƒå°†è¿™äº›ä¼°è®¡åº”ç”¨äºæ¨ç†è¿‡ç¨‹ä¸­çš„é¢„æµ‹ç½®ä¿¡åº¦è¯„åˆ†æ ¡å‡†ã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼šç”¨äºçœ¼åº•å›¾åƒåˆ†ç±»çš„PAPILAæ•°æ®é›†ã€ç”¨äºçš®è‚¤ç™Œåˆ†ç±»çš„HAM10000æ•°æ®é›†ä»¥åŠç”¨äºèƒ¸éƒ¨Xå°„çº¿åˆ†ç±»çš„MIMIC-CXRæ•°æ®é›†ï¼ŒCALINåœ¨ä¿è¯é¢„æµ‹ç½®ä¿¡åº¦å…¬å¹³æ ¡å‡†æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼ŒåŒæ—¶æé«˜äº†æ€»ä½“é¢„æµ‹ç²¾åº¦å¹¶å±•ç°äº†æœ€å°çš„å…¬å¹³æ•ˆç”¨æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23298v2">PDF</a> Preprint version. The peer-reviewed version of this paper has been   accepted to MICCAI 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†å…¶é¢„æµ‹å‡†ç¡®æ€§ã€æ ¡å‡†è¯¯å·®ä»¥åŠä¸åŒäººç¾¤ä¹‹é—´çš„å·®å¼‚ä»éœ€æ·±å…¥åˆ†æã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†è¯¥æ¨¡å‹åœ¨å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ ¡å‡†åè§å’Œäººç¾¤ä¸å…¬å¹³æ€§ã€‚ä¸ºç¼“è§£ç›¸å…³é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†CALINæ ¡å‡†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µè®¾è®¡ï¼Œé€šè¿‡ä¸¤çº§ç¨‹åºä»æ€»ä½“åˆ°åˆ†ç»„å±‚é¢ä¼°è®¡æ‰€éœ€çš„æ ¡å‡†é‡ï¼Œå¹¶åº”ç”¨äºæ¨ç†è¿‡ç¨‹ä¸­çš„é¢„æµ‹ç½®ä¿¡åº¦å¾—åˆ†æ ¡å‡†ã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCALINæ–¹æ³•èƒ½æœ‰æ•ˆç¡®ä¿å…¬å¹³ç½®ä¿¡åº¦æ ¡å‡†ï¼Œæé«˜é¢„æµ‹å‡†ç¡®æ€§ï¼Œä¸”å…¬å¹³æ€§æ•ˆç”¨æƒè¡¡è¡¨ç°ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†é¢„æµ‹å‡†ç¡®æ€§å’Œæ ¡å‡†è¯¯å·®éœ€æ·±å…¥åˆ†æã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†æ¨¡å‹åœ¨å°‘æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ ¡å‡†åè§å’Œäººç¾¤ä¸å…¬å¹³æ€§é—®é¢˜ã€‚</li>
<li>ä¸ºç¼“è§£è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†CALINæ ¡å‡†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†é˜¶æ®µè®¾è®¡ç”¨äºæ ¡å‡†é¢„æµ‹ç½®ä¿¡åº¦å¾—åˆ†ã€‚</li>
<li>CALINé€šè¿‡ä¸¤çº§ç¨‹åºä»æ€»ä½“åˆ°åˆ†ç»„å±‚é¢ä¼°è®¡æ‰€éœ€çš„æ ¡å‡†é‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCALINæ–¹æ³•èƒ½æœ‰æ•ˆç¡®ä¿å…¬å¹³ç½®ä¿¡åº¦æ ¡å‡†ï¼Œæé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>CALINæ–¹æ³•åœ¨å®ç°é«˜é¢„æµ‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå±•ç°å‡ºè‰¯å¥½çš„å…¬å¹³æ€§æ•ˆç”¨æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e9e857695606482ba339ca9e05355684.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be07ec8609fee399a8ec0b106a03b8d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b8df457de984bf3851fc28313584916.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Inpainting-is-All-You-Need-A-Diffusion-based-Augmentation-Method-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Inpainting-is-All-You-Need-A-Diffusion-based-Augmentation-Method-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Inpainting is All You Need: A Diffusion-based Augmentation Method for   Semi-supervised Medical Image Segmentation"></a>Inpainting is All You Need: A Diffusion-based Augmentation Method for   Semi-supervised Medical Image Segmentation</h2><p><strong>Authors:Xinrong Hu, Yiyu Shi</strong></p>
<p>Collecting pixel-level labels for medical datasets can be a laborious and expensive process, and enhancing segmentation performance with a scarcity of labeled data is a crucial challenge. This work introduces AugPaint, a data augmentation framework that utilizes inpainting to generate image-label pairs from limited labeled data. AugPaint leverages latent diffusion models, known for their ability to generate high-quality in-domain images with low overhead, and adapts the sampling process for the inpainting task without need for retraining. Specifically, given a pair of image and label mask, we crop the area labeled with the foreground and condition on it during reversed denoising process for every noise level. Masked background area would gradually be filled in, and all generated images are paired with the label mask. This approach ensures the accuracy of match between synthetic images and label masks, setting it apart from existing dataset generation methods. The generated images serve as valuable supervision for training downstream segmentation models, effectively addressing the challenge of limited annotations. We conducted extensive evaluations of our data augmentation method on four public medical image segmentation datasets, including CT, MRI, and skin imaging. Results across all datasets demonstrate that AugPaint outperforms state-of-the-art label-efficient methodologies, significantly improving segmentation performance. </p>
<blockquote>
<p>æ”¶é›†åŒ»å­¦æ•°æ®é›†çš„åƒç´ çº§æ ‡ç­¾å¯èƒ½æ˜¯ä¸€ä¸ªæ—¢ç¹çåˆæ˜‚è´µçš„è¿‡ç¨‹ï¼Œè€Œåœ¨æ ‡ç­¾æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹æé«˜åˆ†å‰²æ€§èƒ½æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†AugPaintæ•°æ®å¢å¼ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å›¾åƒä¿®å¤æŠ€æœ¯ä»æœ‰é™çš„æ ‡è®°æ•°æ®ä¸­ç”Ÿæˆå›¾åƒ-æ ‡ç­¾å¯¹ã€‚AugPaintåˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œè¿™ç§æ¨¡å‹ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å†…éƒ¨åŸŸå›¾åƒå’Œä½å¼€é”€è€Œé—»åï¼Œå¹¶é€‚åº”é‡‡æ ·è¿‡ç¨‹è¿›è¡Œå›¾åƒä¿®å¤ä»»åŠ¡è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šå›¾åƒå’Œæ ‡ç­¾æ©è†œå¯¹ï¼Œæˆ‘ä»¬è£å‰ªå‰æ™¯æ ‡è®°åŒºåŸŸå¹¶åœ¨æ¯ä¸ªå™ªå£°æ°´å¹³çš„åå‘å»å™ªè¿‡ç¨‹ä¸­å¯¹å…¶è¿›è¡Œæ¡ä»¶å¤„ç†ã€‚è¢«é®ç½©çš„èƒŒæ™¯åŒºåŸŸä¼šé€æ¸è¢«å¡«å……ï¼Œæ‰€æœ‰ç”Ÿæˆçš„å›¾åƒéƒ½ä¸æ ‡ç­¾æ©è†œé…å¯¹ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†åˆæˆå›¾åƒä¸æ ‡ç­¾æ©è†œä¹‹é—´çš„åŒ¹é…å‡†ç¡®æ€§ï¼Œä¸ç°æœ‰çš„æ•°æ®é›†ç”Ÿæˆæ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚ç”Ÿæˆçš„å›¾åƒä½œä¸ºè®­ç»ƒä¸‹æ¸¸åˆ†å‰²æ¨¡å‹çš„å®è´µç›‘ç£æ•°æ®ï¼Œæœ‰æ•ˆè§£å†³äº†æ ‡æ³¨æœ‰é™çš„é—®é¢˜ã€‚æˆ‘ä»¬åœ¨å››ä¸ªå…¬å…±åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ•°æ®å¢å¼ºæ–¹æ³•è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬CTã€MRIå’Œçš®è‚¤æˆåƒã€‚åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„ç»“æœå‡è¡¨æ˜ï¼ŒAugPaintä¼˜äºæœ€æ–°çš„æ ‡ç­¾æ•ˆç‡æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23038v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒæ•°æ®é›†æ ‡æ³¨åŠ³åŠ¨å¼ºåº¦å¤§ä¸”æˆæœ¬é«˜ï¼Œç¼ºä¹æ ‡æ³¨æ•°æ®æé«˜åˆ†å‰²æ€§èƒ½æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºAugPaintæ•°æ®å¢å¼ºæ¡†æ¶ï¼Œåˆ©ç”¨inpaintingæŠ€æœ¯ä»æœ‰é™æ ‡æ³¨æ•°æ®ä¸­ç”Ÿæˆå›¾åƒ-æ ‡ç­¾å¯¹ã€‚AugPainté‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œä»¥ä½å¼€é”€ç”Ÿæˆé«˜è´¨é‡åŸŸå†…å›¾åƒï¼Œå¹¶é€‚åº”inpaintingä»»åŠ¡çš„é‡‡æ ·è¿‡ç¨‹è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¯¥æ–¹æ³•ç¡®ä¿åˆæˆå›¾åƒä¸æ ‡ç­¾æ©è†œä¹‹é—´çš„å‡†ç¡®åŒ¹é…ï¼Œä¸ç°æœ‰æ•°æ®é›†ç”Ÿæˆæ–¹æ³•åŒºåˆ†å¼€æ¥ã€‚ç”Ÿæˆçš„å›¾åƒä½œä¸ºä¸‹æ¸¸åˆ†å‰²æ¨¡å‹è®­ç»ƒçš„å®è´µç›‘ç£èµ„æ–™ï¼Œæœ‰æ•ˆè§£å†³æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚å®éªŒåœ¨å››ä¸ªå…¬å…±åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¯„ä¼°è¯¥æ–¹æ³•ï¼ŒåŒ…æ‹¬CTã€MRIå’Œçš®è‚¤æˆåƒï¼Œç»“æœæ˜¾ç¤ºAugPaintä¼˜äºå½“å‰æœ€å…ˆè¿›çš„é«˜æ•ˆæ ‡ç­¾æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AugPaintæ˜¯ä¸€ä¸ªç”¨äºåŒ»å­¦å›¾åƒçš„æ•°æ®å¢å¼ºæ¡†æ¶ï¼Œåˆ©ç”¨inpaintingæŠ€æœ¯ä»æœ‰é™çš„æ ‡æ³¨æ•°æ®ä¸­ç”Ÿæˆå›¾åƒ-æ ‡ç­¾å¯¹ã€‚</li>
<li>AugPainté‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡åŸŸå†…å›¾åƒï¼Œæ— éœ€é‡æ–°è®­ç»ƒå³å¯é€‚åº”inpaintingä»»åŠ¡çš„é‡‡æ ·è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•ç¡®ä¿åˆæˆå›¾åƒä¸æ ‡ç­¾æ©è†œä¹‹é—´çš„å‡†ç¡®åŒ¹é…ã€‚</li>
<li>AugPainté€šè¿‡ç”Ÿæˆå›¾åƒä½œä¸ºä¸‹æ¸¸åˆ†å‰²æ¨¡å‹çš„å®è´µç›‘ç£èµ„æ–™ï¼Œæœ‰æ•ˆè§£å†³æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>åœ¨å››ä¸ªå…¬å…±åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAugPaintæ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>AugPaintæ¡†æ¶é€‚ç”¨äºå¤šç§åŒ»å­¦å›¾åƒç±»å‹ï¼ŒåŒ…æ‹¬CTã€MRIå’Œçš®è‚¤æˆåƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23038">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1dccb6205d9bd89d465f54f864ae097f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce68d6e0a78d83a9ff6e2305690ac9c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-551e617aad68dfdf9fb47984ee7ecbdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13beb7dd3feaa4d9a2bfe3260f7cba04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51ed996d0656e43ac56aed689e67752c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94f6682833aa1617a4a7adc5c43299dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3bb6408b2ec18062c536eac591e8f0d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="YM-WML-A-new-Yolo-based-segmentation-Model-with-Weighted-Multi-class-Loss-for-medical-imaging"><a href="#YM-WML-A-new-Yolo-based-segmentation-Model-with-Weighted-Multi-class-Loss-for-medical-imaging" class="headerlink" title="YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class   Loss for medical imaging"></a>YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class   Loss for medical imaging</h2><p><strong>Authors:Haniyeh Nikkhah, Jafar Tanha, Mahdi Zarrin, SeyedEhsan Roshan, Amin Kazempour</strong></p>
<p>Medical image segmentation poses significant challenges due to class imbalance and the complex structure of medical images. To address these challenges, this study proposes YM-WML, a novel model for cardiac image segmentation. The model integrates a robust backbone for effective feature extraction, a YOLOv11 neck for multi-scale feature aggregation, and an attention-based segmentation head for precise and accurate segmentation. To address class imbalance, we introduce the Weighted Multi-class Exponential (WME) loss function. On the ACDC dataset, YM-WML achieves a Dice Similarity Coefficient of 91.02, outperforming state-of-the-art methods. The model demonstrates stable training, accurate segmentation, and strong generalization, setting a new benchmark in cardiac segmentation tasks. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´ç€ç±»ä¸å¹³è¡¡å’ŒåŒ»å­¦å›¾åƒå¤æ‚ç»“æ„å¸¦æ¥çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†YM-WMLè¿™ä¸€æ–°å‹å¿ƒè„å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚è¯¥æ¨¡å‹é›†æˆäº†ç¨³å¥çš„éª¨å¹²ç½‘è¿›è¡Œé«˜æ•ˆç‰¹å¾æå–ã€YOLOv11é¢ˆéƒ¨è¿›è¡Œå¤šå°ºåº¦ç‰¹å¾èšåˆå’ŒåŸºäºæ³¨æ„åŠ›çš„åˆ†å‰²å¤´ï¼Œä»¥å®ç°ç²¾ç¡®å’Œå‡†ç¡®çš„åˆ†å‰²ã€‚ä¸ºäº†è§£å†³ç±»ä¸å¹³è¡¡é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ æƒå¤šç±»æŒ‡æ•°ï¼ˆWMEï¼‰æŸå¤±å‡½æ•°ã€‚åœ¨ACDCæ•°æ®é›†ä¸Šï¼ŒYM-WMLçš„Diceç›¸ä¼¼ç³»æ•°è¾¾åˆ°91.02%ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¯¥æ¨¡å‹å±•ç°å‡ºç¨³å®šçš„è®­ç»ƒã€å‡†ç¡®çš„åˆ†å‰²å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¿ƒè„åˆ†å‰²ä»»åŠ¡ä¸­æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22955v1">PDF</a> Accepted at The 7th International conference on Pattern Recognition   and Image Analysis (IPRIA 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡å’Œå¤æ‚ç»“æ„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å¿ƒè„å›¾åƒåˆ†å‰²æ¨¡å‹YM-WMLã€‚è¯¥æ¨¡å‹èåˆäº†ç¨³å¥çš„éª¨å¹²ç½‘ç»œä»¥å®ç°æœ‰æ•ˆçš„ç‰¹å¾æå–ï¼Œå€ŸåŠ©YOLOv11é¢ˆéƒ¨è¿›è¡Œå¤šå°ºåº¦ç‰¹å¾èšåˆï¼Œä»¥åŠåŸºäºæ³¨æ„åŠ›çš„åˆ†å‰²å¤´å®ç°ç²¾ç¡®å’Œå‡†ç¡®çš„åˆ†å‰²ã€‚ä¸ºè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¼•å…¥äº†åŠ æƒå¤šç±»æŒ‡æ•°ï¼ˆWMEï¼‰æŸå¤±å‡½æ•°ã€‚åœ¨ACDCæ•°æ®é›†ä¸Šï¼ŒYM-WMLçš„Diceç›¸ä¼¼ç³»æ•°è¾¾åˆ°äº†91.02%ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºç¨³å®šçš„è®­ç»ƒã€å‡†ç¡®çš„åˆ†å‰²å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå¿ƒè„åˆ†å‰²ä»»åŠ¡è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´ç±»åˆ«ä¸å¹³è¡¡å’Œå¤æ‚ç»“æ„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºæ–°å‹å¿ƒè„å›¾åƒåˆ†å‰²æ¨¡å‹YM-WMLã€‚</li>
<li>YM-WMLæ¨¡å‹èåˆç¨³å¥éª¨å¹²ç½‘ç»œã€YOLOv11é¢ˆéƒ¨å’Œæ³¨æ„åŠ›åˆ†å‰²å¤´ã€‚</li>
<li>å¼•å…¥åŠ æƒå¤šç±»æŒ‡æ•°ï¼ˆWMEï¼‰æŸå¤±å‡½æ•°ä»¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>åœ¨ACDCæ•°æ®é›†ä¸Šï¼ŒYM-WMLçš„Diceç›¸ä¼¼ç³»æ•°è¾¾91.02%ã€‚</li>
<li>YM-WMLæ€§èƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da130ab1bd41ef1b5da9d19920b9b043.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3927ba750a3d341528c7c8057216523.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fc4a9ae25d4fec7163b476004f498ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcd00c2144fdfba13c242e694337e195.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-571b5c7d54703465787c7bf6ab45c026.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Missing-Modality-Aware-Graph-Neural-Network-for-Cancer-Classification"><a href="#Missing-Modality-Aware-Graph-Neural-Network-for-Cancer-Classification" class="headerlink" title="Missing-Modality-Aware Graph Neural Network for Cancer Classification"></a>Missing-Modality-Aware Graph Neural Network for Cancer Classification</h2><p><strong>Authors:Sina Tabakhi, Haiping Lu</strong></p>
<p>A key challenge in learning from multimodal biological data is missing modalities, where all data from some modalities are missing for some patients. Current fusion methods address this by excluding patients with missing modalities, imputing missing modalities, or making predictions directly with partial modalities. However, they often struggle with diverse missing-modality patterns and the exponential growth of the number of such patterns as the number of modalities increases. To address these limitations, we propose MAGNET (Missing-modality-Aware Graph neural NETwork) for direct prediction with partial modalities, which introduces a patient-modality multi-head attention mechanism to fuse lower-dimensional modality embeddings based on their importance and missingness. MAGNETâ€™s complexity increases linearly with the number of modalities while adapting to missing-pattern variability. To generate predictions, MAGNET further constructs a patient graph with fused multimodal embeddings as node features and the connectivity determined by the modality missingness, followed by a conventional graph neural network. Experiments on three public multiomics datasets for cancer classification, with real-world instead of artificial missingness, show that MAGNET outperforms the state-of-the-art fusion methods. The data and code are available at <a target="_blank" rel="noopener" href="https://github.com/SinaTabakhi/MAGNET">https://github.com/SinaTabakhi/MAGNET</a>. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€ç”Ÿç‰©æ•°æ®å­¦ä¹ ä¸­ï¼Œç¼ºå¤±æ¨¡æ€æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå…¶ä¸­æŸäº›æ‚£è€…çš„æŸäº›æ¨¡æ€æ•°æ®å…¨éƒ¨ç¼ºå¤±ã€‚ç›®å‰çš„èåˆæ–¹æ³•é€šè¿‡æ’é™¤å…·æœ‰ç¼ºå¤±æ¨¡æ€çš„æ‚£è€…ã€ä¼°ç®—ç¼ºå¤±æ¨¡æ€æˆ–ä»…ä½¿ç”¨éƒ¨åˆ†æ¨¡æ€è¿›è¡Œç›´æ¥é¢„æµ‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œéšç€æ¨¡æ€æ•°é‡çš„å¢åŠ ï¼Œå®ƒä»¬é€šå¸¸é¢ä¸´å¤šæ ·çš„ç¼ºå¤±æ¨¡æ€æ¨¡å¼ä»¥åŠæ­¤ç±»æ¨¡å¼æ•°é‡å‘ˆæŒ‡æ•°å¢é•¿çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MAGNETï¼ˆåŸºäºç¼ºå¤±æ¨¡æ€æ„ŸçŸ¥çš„å›¾ç¥ç»ç½‘ç»œï¼‰ï¼Œé‡‡ç”¨éƒ¨åˆ†æ¨¡æ€è¿›è¡Œç›´æ¥é¢„æµ‹ã€‚MAGNETå¼•å…¥äº†æ‚£è€…-æ¨¡æ€å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ ¹æ®é‡è¦æ€§åŠå…¶ç¼ºå¤±æƒ…å†µèåˆä½ç»´æ¨¡æ€åµŒå…¥ã€‚MAGNETçš„å¤æ‚æ€§éšæ¨¡æ€æ•°é‡çš„å¢åŠ è€Œçº¿æ€§å¢é•¿ï¼ŒåŒæ—¶é€‚åº”ç¼ºå¤±æ¨¡å¼çš„å¯å˜æ€§ã€‚ä¸ºäº†ç”Ÿæˆé¢„æµ‹ï¼ŒMAGNETè¿›ä¸€æ­¥æ„å»ºäº†ä¸€ä¸ªæ‚£è€…å›¾ï¼Œä»¥èåˆçš„å¤šæ¨¡æ€åµŒå…¥ä½œä¸ºèŠ‚ç‚¹ç‰¹å¾ï¼Œè¿æ¥æ€§ç”±æ¨¡æ€ç¼ºå¤±æ€§å†³å®šï¼Œç„¶åä½¿ç”¨ä¼ ç»Ÿçš„å›¾ç¥ç»ç½‘ç»œã€‚åœ¨ä¸‰ä¸ªå…¬å¼€çš„å¤šç»„å­¦æ•°æ®é›†ä¸Šè¿›è¡Œç™Œç—‡åˆ†ç±»çš„å®éªŒï¼ˆä½¿ç”¨ç°å®ä¸–ç•Œä¸­çš„ç¼ºå¤±æ•°æ®è€Œéäººä¸ºç¼ºå¤±æ•°æ®ï¼‰è¡¨æ˜ï¼ŒMAGNETåœ¨èåˆæ–¹æ³•ä¸Šè¶…è¶Šäº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SinaTabakhi/MAGNET%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SinaTabakhi/MAGNETæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22901v1">PDF</a> 15 pages, 7 figures</p>
<p><strong>Summary</strong><br>     å¤„ç†å¤šæ¨¡æ€ç”Ÿç‰©æ•°æ®æ—¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ç¼ºå¤±æ¨¡æ€é—®é¢˜ã€‚é’ˆå¯¹è¯¥é—®é¢˜ï¼Œç°æœ‰èåˆæ–¹æ³•å¸¸é€šè¿‡æ’é™¤ç¼ºå¤±æ¨¡æ€çš„æ‚£è€…ã€è¡¥é½ç¼ºå¤±æ¨¡æ€æˆ–ç›´æ¥ç”¨éƒ¨åˆ†æ¨¡æ€è¿›è¡Œé¢„æµ‹æ¥åº”å¯¹ã€‚ç„¶è€Œï¼Œéšç€æ¨¡æ€æ•°é‡çš„å¢åŠ ï¼Œå®ƒä»¬é¢ä¸´å¤šæ ·ä¸”å¤æ‚çš„ç¼ºå¤±æ¨¡æ€æ¨¡å¼æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºMAGNETï¼ˆç¼ºå¤±æ¨¡æ€æ„ŸçŸ¥å›¾ç¥ç»ç½‘ç»œï¼‰è¿›è¡Œç›´æ¥é¢„æµ‹ï¼Œå¼•å…¥äº†æ‚£è€…-æ¨¡æ€å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¥èåˆåŸºäºé‡è¦æ€§è€Œå¹¶éç¼ºå¤±ç¨‹åº¦çš„ä½ç»´æ¨¡æ€åµŒå…¥ã€‚MAGNETçš„å¤æ‚åº¦éšæ¨¡æ€æ•°é‡çº¿æ€§å¢é•¿ï¼ŒåŒæ—¶é€‚åº”ç¼ºå¤±æ¨¡å¼çš„å¤šæ ·æ€§ã€‚MAGNETé€šè¿‡æ„å»ºæ‚£è€…å›¾ç”Ÿæˆé¢„æµ‹ç»“æœï¼Œå›¾ä¸­èåˆçš„å¤šæ¨¡æ€åµŒå…¥ä½œä¸ºèŠ‚ç‚¹ç‰¹å¾ï¼Œè¿æ¥æ€§ç”±ç¼ºå¤±çš„æ¨¡æ€å†³å®šï¼Œç„¶åä½¿ç”¨å¸¸è§„çš„å›¾ç¥ç»ç½‘ç»œå¤„ç†ã€‚åœ¨ä¸‰ä¸ªå…¬å…±å¤šç»„å­¦æ•°æ®é›†ä¸Šçš„ç™Œç—‡åˆ†ç±»å®éªŒè¡¨æ˜ï¼ŒMAGNETä¼˜äºç°æœ‰èåˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤„ç†å¤šæ¨¡æ€ç”Ÿç‰©æ•°æ®æ—¶å­˜åœ¨ç¼ºå¤±æ¨¡æ€çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰èåˆæ–¹æ³•å¦‚æ’é™¤ç¼ºå¤±æ¨¡æ€æ‚£è€…ã€è¡¥é½ç¼ºå¤±æ¨¡æ€æˆ–ç›´æ¥ä½¿ç”¨éƒ¨åˆ†æ¨¡æ€é¢„æµ‹éƒ½æœ‰å…¶å±€é™æ€§ã€‚</li>
<li>MAGNETæ–¹æ³•é€šè¿‡å¼•å…¥æ‚£è€…-æ¨¡æ€å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶è§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œèåˆäº†åŸºäºé‡è¦æ€§çš„ä½ç»´æ¨¡æ€åµŒå…¥ã€‚</li>
<li>MAGNETèƒ½éšç€æ¨¡æ€æ•°é‡çš„å¢é•¿çº¿æ€§å¢é•¿å¤æ‚åº¦å¹¶é€‚åº”å¤šç§ç¼ºå¤±æ¨¡å¼ã€‚</li>
<li>MAGNETæ„å»ºäº†ä¸€ä¸ªæ‚£è€…å›¾ï¼Œé€šè¿‡ç»“åˆèåˆçš„å¤šæ¨¡æ€åµŒå…¥å’ŒèŠ‚ç‚¹è¿æ¥æ€§æ¥ç”Ÿæˆé¢„æµ‹ç»“æœã€‚</li>
<li>å®éªŒè¯æ˜MAGNETåœ¨ç™Œç—‡åˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–èåˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f12eb53793019ba7e4ee434425fba81d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c1cfdc97054b397792d97126a3a4f47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5a319125948b70264367337050da6b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-588133fed298c83d09f1f076fe786b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e4ff6bc36934ee8b70b4a2218c6418e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CA-Diff-Collaborative-Anatomy-Diffusion-for-Brain-Tissue-Segmentation"><a href="#CA-Diff-Collaborative-Anatomy-Diffusion-for-Brain-Tissue-Segmentation" class="headerlink" title="CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation"></a>CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation</h2><p><strong>Authors:Qilong Xing, Zikai Song, Yuteng Ye, Yuke Chen, Youjia Zhang, Na Feng, Junqing Yu, Wei Yang</strong></p>
<p>Segmentation of brain structures from MRI is crucial for evaluating brain morphology, yet existing CNN and transformer-based methods struggle to delineate complex structures accurately. While current diffusion models have shown promise in image segmentation, they are inadequate when applied directly to brain MRI due to neglecting anatomical information. To address this, we propose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating spatial anatomical features to enhance segmentation accuracy of the diffusion model. Specifically, we introduce distance field as an auxiliary anatomical condition to provide global spatial context, alongside a collaborative diffusion process to model its joint distribution with anatomical structures, enabling effective utilization of anatomical features for segmentation. Furthermore, we introduce a consistency loss to refine relationships between the distance field and anatomical structures and design a time adapted channel attention module to enhance the U-Net feature fusion procedure. Extensive experiments show that CA-Diff outperforms state-of-the-art (SOTA) methods. </p>
<blockquote>
<p>ä»MRIä¸­åˆ†å‰²è„‘ç»“æ„å¯¹äºè¯„ä¼°è„‘å½¢æ€è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„åŸºäºCNNå’Œtransformerçš„æ–¹æ³•åœ¨å‡†ç¡®æç»˜å¤æ‚ç»“æ„ä¸Šå­˜åœ¨å›°éš¾ã€‚è™½ç„¶å½“å‰çš„æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆ†å‰²æ–¹é¢æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œä½†ç›´æ¥åº”ç”¨äºè„‘MRIæ—¶å´è¡¨ç°ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬å¿½ç•¥äº†è§£å‰–ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åä½œè§£å‰–æ‰©æ•£ï¼ˆCA-Diffï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç©ºé—´è§£å‰–ç‰¹å¾ï¼Œä»¥æé«˜æ‰©æ•£æ¨¡å‹çš„åˆ†å‰²ç²¾åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥è·ç¦»åœºä½œä¸ºè¾…åŠ©è§£å‰–æ¡ä»¶ï¼Œä»¥æä¾›å…¨å±€ç©ºé—´ä¸Šä¸‹æ–‡ï¼Œä»¥åŠä¸€ä¸ªåä½œæ‰©æ•£è¿‡ç¨‹ï¼Œä»¥æ¨¡æ‹Ÿå…¶ä¸è§£å‰–ç»“æ„çš„è”åˆåˆ†å¸ƒï¼Œä»è€Œå®ç°è§£å‰–ç‰¹å¾çš„æœ‰æ•ˆåˆ©ç”¨è¿›è¡Œåˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€è‡´æ€§æŸå¤±æ¥ä¼˜åŒ–è·ç¦»åœºä¸è§£å‰–ç»“æ„ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ—¶é—´é€‚åº”çš„é€šé“æ³¨æ„åŠ›æ¨¡å—æ¥å¢å¼ºU-Netç‰¹å¾èåˆè¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCA-Diffä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22882v1">PDF</a> ICME 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆç©ºé—´è§£å‰–ç‰¹å¾çš„åä½œå¼è§£å‰–å­¦æ‰©æ•£ï¼ˆCA-Diffï¼‰æ¡†æ¶ï¼Œç”¨äºæé«˜æ‰©æ•£æ¨¡å‹åœ¨è„‘ç»“æ„MRIå›¾åƒåˆ†å‰²ä¸­çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¼•å…¥è·ç¦»åœºä½œä¸ºè¾…åŠ©è§£å‰–æ¡ä»¶ï¼Œå¹¶ç»“åˆåä½œæ‰©æ•£è¿‡ç¨‹ï¼Œæœ‰æ•ˆè¿ç”¨è§£å‰–ç‰¹å¾è¿›è¡Œåˆ†å‰²ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€è‡´æ€§æŸå¤±æ¥ä¼˜åŒ–è·ç¦»åœºä¸è§£å‰–ç»“æ„ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶è®¾è®¡äº†æ—¶é—´é€‚åº”æ€§é€šé“æ³¨æ„åŠ›æ¨¡å—æ¥å¢å¼ºU-Netç‰¹å¾èåˆè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCA-Diffæ¡†æ¶çš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰CNNå’Œtransformer-basedæ–¹æ³•åœ¨è„‘ç»“æ„MRIå›¾åƒåˆ†å‰²ä¸Šå­˜åœ¨å‡†ç¡®æ€§é—®é¢˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆ†å‰²ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç›´æ¥åº”ç”¨äºè„‘MRIæ—¶å› å¿½è§†è§£å‰–ä¿¡æ¯è€Œä¸è¶³ã€‚</li>
<li>æå‡ºçš„CA-Diffæ¡†æ¶é›†æˆäº†ç©ºé—´è§£å‰–ç‰¹å¾ï¼Œä»¥æé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥è·ç¦»åœºä½œä¸ºè¾…åŠ©è§£å‰–æ¡ä»¶ï¼Œæä¾›å…¨å±€ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>åä½œæ‰©æ•£è¿‡ç¨‹èƒ½å¤Ÿå»ºæ¨¡è·ç¦»åœºä¸è§£å‰–ç»“æ„çš„è”åˆåˆ†å¸ƒã€‚</li>
<li>ä¸€è‡´æ€§æŸå¤±ç”¨äºä¼˜åŒ–è·ç¦»åœºä¸è§£å‰–ç»“æ„ä¹‹é—´çš„å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22882">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-86c9470ff0e72f379d5b94e254d6ee8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3d39c50d9f375b6c4c5951d1c9b1dda.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4510a8b9b66fbaeeb893e9f57f08cd5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8baa43f860d2a9400384cab1603ba3e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e48ca9fa83db613ac561bf17bcf2358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0e63302a9ef8c26dc1bf52f8271ce19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6bc4d56927883f3d16a70f6157d2fdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1499a6684e4c5c4a8a8ddbc10e047dde.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Decoupled-Seg-Tokens-Make-Stronger-Reasoning-Video-Segmenter-and-Grounder"><a href="#Decoupled-Seg-Tokens-Make-Stronger-Reasoning-Video-Segmenter-and-Grounder" class="headerlink" title="Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and   Grounder"></a>Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and   Grounder</h2><p><strong>Authors:Dang Jisheng, Wu Xudong, Wang Bimei, Lv Ning, Chen Jiayu, Jingwen Zhao, Yichu liu, Jizhao Liu, Juncheng Li, Teng Wang</strong></p>
<p>Existing video segmenter and grounder approaches, exemplified by Sa2VA, directly fuse features within segmentation models. This often results in an undesirable entanglement of dynamic visual information and static semantics, thereby degrading segmentation accuracy. To systematically mitigate this issue, we propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text pre-training and a linear decoupling module to address the information processing limitations inherent in SAM-2. Specifically, first, we devise a pre-training paradigm that converts textual ground-truth labels into point-level prompts while generating corresponding text masks. These masks are refined through a hybrid loss function to strengthen the modelâ€™s semantic grounding capabilities. Next, we employ linear projection to disentangle hidden states that generated by a large language model into distinct textual and visual feature subspaces. Finally, a dynamic mask fusion strategy synergistically combines these decoupled features through triple supervision from predicted text&#x2F;visual masks and ground-truth annotations. Extensive experiments demonstrate state-of-the-art performance across diverse tasks, including image segmentation, image question answering, video segmentation, and video question answering. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/longmalongma/DeSa2VA">https://github.com/longmalongma/DeSa2VA</a>. </p>
<blockquote>
<p>ç°æœ‰çš„è§†é¢‘åˆ†å‰²å™¨å’Œæ‰“æ ‡å™¨æ–¹æ³•ï¼Œä»¥Sa2VAä¸ºä¾‹ï¼Œç›´æ¥åœ¨åˆ†å‰²æ¨¡å‹ä¸­èåˆç‰¹å¾ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´åŠ¨æ€è§†è§‰ä¿¡æ¯å’Œé™æ€è¯­ä¹‰çš„çº ç¼ ï¼Œä»è€Œé™ä½åˆ†å‰²ç²¾åº¦ã€‚ä¸ºäº†ç³»ç»Ÿåœ°è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeSa2VAï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºè§£è€¦çš„æç¤ºæ–¹æ¡ˆï¼Œå®ƒç»“åˆäº†æ–‡æœ¬é¢„è®­ç»ƒå’Œéçº¿æ€§è§£è€¦æ¨¡å—ï¼Œä»¥è§£å†³SAM-2å›ºæœ‰çš„ä¿¡æ¯å¤„ç†é™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é¢„è®­ç»ƒæ¨¡å¼ï¼Œå°†æ–‡æœ¬çœŸå®æ ‡ç­¾è½¬æ¢ä¸ºç‚¹çº§æç¤ºï¼ŒåŒæ—¶ç”Ÿæˆç›¸åº”çš„æ–‡æœ¬æ©ç ã€‚è¿™äº›æ©ç é€šè¿‡æ··åˆæŸå¤±å‡½æ•°è¿›è¡Œç²¾ç‚¼ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„è¯­ä¹‰å®šä½èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é‡‡ç”¨çº¿æ€§æŠ•å½±æŠ€æœ¯ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„éšè—çŠ¶æ€åˆ†è§£æˆä¸åŒçš„æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾å­ç©ºé—´ã€‚æœ€åï¼Œé€šè¿‡é¢„æµ‹æ–‡æœ¬&#x2F;è§†è§‰æ©ç å’ŒçœŸå®æ³¨é‡Šçš„ä¸‰é‡ç›‘ç£ï¼Œé‡‡ç”¨åŠ¨æ€æ©ç èåˆç­–ç•¥ååŒç»“åˆè¿™äº›è§£è€¦ç‰¹å¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨åŒ…æ‹¬å›¾åƒåˆ†å‰²ã€å›¾åƒé—®ç­”ã€è§†é¢‘åˆ†å‰²å’Œè§†é¢‘é—®ç­”ç­‰å¤šé¡¹ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„çŠ¶æ€ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/longmalongma/DeSa2VA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/longmalongma/DeSa2VAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22880v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°æœ‰è§†é¢‘åˆ†å‰²å™¨å’Œåœ°é¢æ ‡è®°å™¨æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚Sa2VAç­‰æ–¹æ³•ç›´æ¥èåˆåˆ†å‰²æ¨¡å‹ä¸­çš„ç‰¹å¾ï¼Œå¯¼è‡´åŠ¨æ€è§†è§‰ä¿¡æ¯å’Œé™æ€è¯­ä¹‰çš„çº ç¼ ï¼Œé™ä½åˆ†å‰²ç²¾åº¦ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DeSa2VAæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡å¢å¼ºè§£è€¦æç¤ºå’Œæ•´åˆæ–‡æœ¬é¢„è®­ç»ƒä¸çº¿æ€§è§£è€¦æ¨¡å—æ¥è§£å†³SAM-2çš„ä¿¡æ¯å¤„ç†é™åˆ¶ã€‚å…·ä½“åŒ…æ‹¬è½¬æ¢ä¸ºç‚¹çº§æç¤ºçš„æ–‡æœ¬é¢„è®­ç»ƒã€çº¿æ€§æŠ•å½±å®ç°çš„æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾å­ç©ºé—´çš„åŒºåˆ†ä»¥åŠåŠ¨æ€æ©ç èåˆç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å›¾åƒåˆ†å‰²ã€å›¾åƒé—®ç­”ã€è§†é¢‘åˆ†å‰²å’Œè§†é¢‘é—®ç­”ç­‰å¤šé¡¹ä»»åŠ¡ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è§†é¢‘åˆ†å‰²å™¨å’Œåœ°é¢æ ‡è®°å™¨æ–¹æ³•å­˜åœ¨ä¿¡æ¯çº ç¼ é—®é¢˜ï¼Œå½±å“åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>DeSa2VAé€šè¿‡å¢å¼ºè§£è€¦æç¤ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ—¨åœ¨æ”¹å–„SAM-2çš„ä¿¡æ¯å¤„ç†é™åˆ¶ã€‚</li>
<li>é¢„è®­ç»ƒå°†æ–‡æœ¬åœ°é¢çœŸå®æ ‡ç­¾è½¬æ¢ä¸ºç‚¹çº§æç¤ºï¼Œå¹¶ä½¿ç”¨æ··åˆæŸå¤±å‡½æ•°ä¼˜åŒ–æ–‡æœ¬æ©ç ä»¥å¢å¼ºæ¨¡å‹è¯­ä¹‰å®šä½èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡çº¿æ€§æŠ•å½±æŠ€æœ¯ï¼ŒDeSa2VAèƒ½å°†å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„éšè—çŠ¶æ€åˆ†è§£æˆç‹¬ç«‹çš„æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾å­ç©ºé—´ã€‚</li>
<li>åŠ¨æ€æ©ç èåˆç­–ç•¥ç»“åˆäº†è¿™äº›è§£è€¦ç‰¹å¾ï¼Œé€šè¿‡æ¥è‡ªé¢„æµ‹æ–‡æœ¬&#x2F;è§†è§‰æ©ç å’Œåœ°é¢çœŸå®æ³¨è§£çš„ä¸‰é‡ç›‘ç£è¿›è¡ŒååŒä½œç”¨ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDeSa2VAåœ¨å¤šç§ä»»åŠ¡ä¸Šè¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†å‰²ã€å›¾åƒé—®ç­”ã€è§†é¢‘åˆ†å‰²å’Œè§†é¢‘é—®ç­”ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e502d8c319ca47ec509cffb4d829e37b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f308a70f0d02ef8f5181d8d6da02a8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2506531d0bc4eb49cf54d4a50be35fb5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Degradation-Modeled-Multipath-Diffusion-for-Tunable-Metalens-Photography"><a href="#Degradation-Modeled-Multipath-Diffusion-for-Tunable-Metalens-Photography" class="headerlink" title="Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography"></a>Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography</h2><p><strong>Authors:Jianing Zhang, Jiayi Zhu, Feiyu Ji, Xiaokang Yang, Xiaoyun Yuan</strong></p>
<p>Metalenses offer significant potential for ultra-compact computational imaging but face challenges from complex optical degradation and computational restoration difficulties. Existing methods typically rely on precise optical calibration or massive paired datasets, which are non-trivial for real-world imaging systems. Furthermore, a lack of control over the inference process often results in undesirable hallucinated artifacts. We introduce Degradation-Modeled Multipath Diffusion for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. Our framework uses positive, neutral, and negative-prompt paths to balance high-frequency detail generation, structural fidelity, and suppression of metalens-specific degradation, alongside \textit{pseudo} data augmentation. A tunable decoder enables controlled trade-offs between fidelity and perceptual quality. Additionally, a spatially varying degradation-aware attention (SVDA) module adaptively models complex optical and sensor-induced degradation. Finally, we design and build a millimeter-scale MetaCamera for real-world validation. Extensive results show that our approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction. More materials: <a target="_blank" rel="noopener" href="https://dmdiff.github.io/">https://dmdiff.github.io/</a>. </p>
<blockquote>
<p>é‡‘å±é€é•œåœ¨è¶…ç´§å‡‘è®¡ç®—æˆåƒæ–¹é¢å­˜åœ¨å·¨å¤§æ½œåŠ›ï¼Œä½†é¢ä¸´ç€å¤æ‚å…‰å­¦é€€åŒ–å’Œè®¡ç®—ä¿®å¤å›°éš¾ç­‰æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç²¾ç¡®çš„å…‰å­¦æ ¡å‡†æˆ–å¤§é‡çš„é…å¯¹æ•°æ®é›†ï¼Œå¯¹äºç°å®ä¸–ç•Œæˆåƒç³»ç»Ÿè€Œè¨€ï¼Œè¿™äº›æ–¹æ³•å¹¶ä¸ç®€å•ã€‚æ­¤å¤–ï¼Œç¼ºä¹å¯¹æ¨ç†è¿‡ç¨‹çš„æ§åˆ¶å¾€å¾€ä¼šå¯¼è‡´å‡ºç°ä¸ç†æƒ³çš„äººå·¥åˆæˆä¼ªå½±ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºé€€åŒ–æ¨¡å‹çš„å¤šè·¯æ‰©æ•£æŠ€æœ¯ï¼Œç”¨äºå¯è°ƒé‡‘å±é€é•œæ‘„å½±ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å¼ºå¤§è‡ªç„¶å›¾åƒå…ˆéªŒçŸ¥è¯†ï¼Œè€Œä¸æ˜¯å¤§å‹æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨æ­£å‘ã€ä¸­æ€§å’Œè´Ÿå‘æç¤ºè·¯å¾„æ¥å¹³è¡¡é«˜é¢‘ç»†èŠ‚ç”Ÿæˆã€ç»“æ„ä¿çœŸåº¦å’ŒæŠ‘åˆ¶é‡‘å±é€é•œç‰¹æœ‰çš„é€€åŒ–ç°è±¡ï¼ŒåŒæ—¶è¾…ä»¥ä¼ªæ•°æ®å¢å¼ºã€‚å¯è°ƒè§£ç å™¨èƒ½å¤Ÿåœ¨ä¿çœŸåº¦å’Œæ„ŸçŸ¥è´¨é‡ä¹‹é—´è¿›è¡Œå¯æ§çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œç©ºé—´å˜åŒ–çš„é€€åŒ–æ„ŸçŸ¥æ³¨æ„åŠ›ï¼ˆSVDAï¼‰æ¨¡å—èƒ½å¤Ÿè‡ªé€‚åº”åœ°æ¨¡æ‹Ÿå¤æ‚çš„å…‰å­¦é€€åŒ–å’Œä¼ æ„Ÿå™¨å¼•èµ·çš„é€€åŒ–ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡å¹¶åˆ¶ä½œäº†ä¸€æ¬¾ç”¨äºå®é™…éªŒè¯çš„æ¯«ç±³çº§MetaCameraã€‚å¤§é‡ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå®ç°äº†é«˜ä¿çœŸå’Œæ¸…æ™°çš„å›¾åƒé‡å»ºã€‚æ›´å¤šææ–™è¯·å‚è§ï¼š<a target="_blank" rel="noopener" href="https://dmdiff.github.io/">https://dmdiff.github.io/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22753v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é‡‘å±é€é•œåœ¨è¶…ç´§å‡‘è®¡ç®—æˆåƒæ–¹é¢å…·å·¨å¤§æ½œåŠ›ï¼Œä½†é¢ä¸´å¤æ‚å…‰å­¦é™è§£å’Œè®¡ç®—ä¿®å¤éš¾é¢˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–ç²¾ç¡®å…‰å­¦æ ¡å‡†æˆ–å¤§é‡é…å¯¹æ•°æ®é›†ï¼Œä¸é€‚ç”¨äºç°å®ä¸–ç•Œæˆåƒç³»ç»Ÿã€‚ç ”ç©¶å¼•å…¥åŸºäºé™è§£å»ºæ¨¡çš„å¤šè·¯å¾„æ‰©æ•£æ–¹æ³•ï¼Œç”¨äºå¯è°ƒé‡‘å±é€é•œæ‘„å½±ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å¤©ç„¶å›¾åƒå…ˆéªŒè€Œéå¤§å‹æ•°æ®é›†ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ­£ã€ä¸­æ€§åŠè´Ÿæç¤ºè·¯å¾„æ¥å¹³è¡¡é«˜é¢‘ç»†èŠ‚ç”Ÿæˆã€ç»“æ„ä¿çœŸå’ŒæŠ‘åˆ¶é‡‘å±é€é•œç‰¹å®šé™è§£ï¼Œé…åˆä¼ªæ•°æ®å¢å¼ºã€‚å¯è°ƒè§£ç å™¨å¯åœ¨ä¿çœŸåº¦å’Œæ„ŸçŸ¥è´¨é‡ä¹‹é—´å®ç°å¯æ§æƒè¡¡ã€‚æ­¤å¤–ï¼Œç©ºé—´å˜åŒ–é™è§£æ„ŸçŸ¥æ³¨æ„åŠ›æ¨¡å—å¯è‡ªé€‚åº”åœ°æ¨¡æ‹Ÿå¤æ‚å…‰å­¦å’Œä¼ æ„Ÿå™¨å¼•èµ·çš„é™è§£ã€‚æœ€åï¼Œç ”ç©¶è®¾è®¡å¹¶åˆ¶ä½œäº†ä¸€æ¬¾ç”¨äºçœŸå®ä¸–ç•ŒéªŒè¯çš„æ¯«ç±³çº§MetaCameraã€‚å¤§é‡ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¯å®ç°é«˜ä¿çœŸå’Œæ¸…æ™°çš„å›¾åƒé‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘å±é€é•œåœ¨è®¡ç®—æˆåƒä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†é¢ä¸´å…‰å­¦é™è§£å’Œè®¡ç®—ä¿®å¤çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºç²¾ç¡®å…‰å­¦æ ¡å‡†æˆ–å¤§è§„æ¨¡é…å¯¹æ•°æ®é›†ï¼Œä¸é€‚ç”¨äºç°å®ä¸–ç•Œçš„æˆåƒç³»ç»Ÿã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºé™è§£å»ºæ¨¡çš„å¤šè·¯å¾„æ‰©æ•£æ–¹æ³•ï¼Œç”¨äºå¯è°ƒé‡‘å±é€é•œæ‘„å½±ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å¤©ç„¶å›¾åƒå…ˆéªŒï¼Œé€šè¿‡æ­£ã€ä¸­æ€§åŠè´Ÿæç¤ºè·¯å¾„å®ç°ç»†èŠ‚ç”Ÿæˆã€ç»“æ„ä¿çœŸå’Œé™è§£æŠ‘åˆ¶çš„å¹³è¡¡ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†ä¼ªæ•°æ®å¢å¼ºã€å¯è°ƒè§£ç å™¨å’Œç©ºé—´å˜åŒ–é™è§£æ„ŸçŸ¥æ³¨æ„åŠ›æ¨¡å—ç­‰æŠ€æœ¯ï¼Œä»¥æé«˜æˆåƒè´¨é‡ã€‚</li>
<li>ç ”ç©¶è®¾è®¡å¹¶åˆ¶ä½œäº†ä¸€æ¬¾ç”¨äºçœŸå®ä¸–ç•ŒéªŒè¯çš„æ¯«ç±³çº§MetaCameraã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3396a6cffc50d63d7b7511b7f0a72d2a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17dfab62daedd9d25405e3e16e535b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f70199b31516f60f60213d4c97303c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04e0dfc85fe8d048394186ac688e9a6c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="UniFuse-A-Unified-All-in-One-Framework-for-Multi-Modal-Medical-Image-Fusion-Under-Diverse-Degradations-and-Misalignments"><a href="#UniFuse-A-Unified-All-in-One-Framework-for-Multi-Modal-Medical-Image-Fusion-Under-Diverse-Degradations-and-Misalignments" class="headerlink" title="UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image   Fusion Under Diverse Degradations and Misalignments"></a>UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image   Fusion Under Diverse Degradations and Misalignments</h2><p><strong>Authors:Dayong Su, Yafei Zhang, Huafeng Li, Jinxing Li, Yu Liu</strong></p>
<p>Current multimodal medical image fusion typically assumes that source images are of high quality and perfectly aligned at the pixel level. Its effectiveness heavily relies on these conditions and often deteriorates when handling misaligned or degraded medical images. To address this, we propose UniFuse, a general fusion framework. By embedding a degradation-aware prompt learning module, UniFuse seamlessly integrates multi-directional information from input images and correlates cross-modal alignment with restoration, enabling joint optimization of both tasks within a unified framework. Additionally, we design an Omni Unified Feature Representation scheme, which leverages Spatial Mamba to encode multi-directional features and mitigate modality differences in feature alignment. To enable simultaneous restoration and fusion within an All-in-One configuration, we propose a Universal Feature Restoration &amp; Fusion module, incorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA principles. By leveraging ALSNâ€™s adaptive feature representation along with degradation-type guidance, we enable joint restoration and fusion within a single-stage framework. Compared to staged approaches, UniFuse unifies alignment, restoration, and fusion within a single framework. Experimental results across multiple datasets demonstrate the methodâ€™s effectiveness and significant advantages over existing approaches. </p>
<blockquote>
<p>å½“å‰çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆé€šå¸¸å‡è®¾æºå›¾åƒè´¨é‡é«˜ä¸”åœ¨åƒç´ çº§åˆ«å®Œç¾å¯¹é½ã€‚å…¶æœ‰æ•ˆæ€§ä¸¥é‡ä¾èµ–äºè¿™äº›æ¡ä»¶ï¼Œè€Œåœ¨å¤„ç†é”™ä½æˆ–é€€åŒ–åŒ»å­¦å›¾åƒæ—¶ï¼Œå…¶æ•ˆæœå¾€å¾€ä¼šä¸‹é™ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniFuseé€šç”¨èåˆæ¡†æ¶ã€‚é€šè¿‡åµŒå…¥æ„ŸçŸ¥é€€åŒ–æç¤ºå­¦ä¹ æ¨¡å—ï¼ŒUniFuseæ— ç¼é›†æˆäº†è¾“å…¥å›¾åƒçš„å¤šæ–¹å‘ä¿¡æ¯ï¼Œå¹¶å°†è·¨æ¨¡æ€å¯¹é½ä¸æ¢å¤ç›¸å…³è”ï¼Œåœ¨ç»Ÿä¸€æ¡†æ¶å†…å®ç°ä¸¤ä¸ªä»»åŠ¡çš„è”åˆä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§Omniç»Ÿä¸€ç‰¹å¾è¡¨ç¤ºæ–¹æ¡ˆï¼Œåˆ©ç”¨ç©ºé—´æ›¼å·´ç¼–ç å¤šæ–¹å‘ç‰¹å¾ï¼Œå‡è½»ç‰¹å¾å¯¹é½ä¸­çš„æ¨¡æ€å·®å¼‚ã€‚ä¸ºäº†å®ç°All-in-Oneé…ç½®ä¸‹çš„åŒæ—¶æ¢å¤å’Œèåˆï¼Œæˆ‘ä»¬æå‡ºäº†é€šç”¨ç‰¹å¾æ¢å¤ä¸èåˆæ¨¡å—ï¼Œç»“åˆäº†åŸºäºLoRAåŸç†çš„è‡ªé€‚åº”LoRAååŒç½‘ç»œï¼ˆALSNï¼‰ã€‚é€šè¿‡åˆ©ç”¨ALSNçš„è‡ªé€‚åº”ç‰¹å¾è¡¨ç¤ºå’Œé€€åŒ–ç±»å‹æŒ‡å¯¼ï¼Œæˆ‘ä»¬åœ¨å•é˜¶æ®µæ¡†æ¶å†…å®ç°äº†è”åˆæ¢å¤å’Œèåˆã€‚ä¸åˆ†é˜¶æ®µæ–¹æ³•ç›¸æ¯”ï¼ŒUniFuseå°†å¯¹é½ã€æ¢å¤å’Œèåˆç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶å†…ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯¹ç°æœ‰æ–¹æ³•çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22736v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºUniFuseçš„é€šç”¨èåˆæ¡†æ¶ï¼Œç”¨äºè§£å†³å½“å‰å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆä¸­é‡åˆ°çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸåµŒå…¥é€€åŒ–æ„ŸçŸ¥æç¤ºå­¦ä¹ æ¨¡å—ï¼Œæ— ç¼é›†æˆè¾“å…¥å›¾åƒçš„å¤šæ–¹å‘ä¿¡æ¯ï¼Œå¹¶é€šè¿‡è·¨æ¨¡æ€å¯¹é½ä¸æ¢å¤çš„ç›¸å…³æ€§ï¼Œåœ¨ç»Ÿä¸€æ¡†æ¶å†…è”åˆä¼˜åŒ–ä¸¤ä¸ªä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†Omniç»Ÿä¸€ç‰¹å¾è¡¨ç¤ºæ–¹æ¡ˆï¼Œåˆ©ç”¨ç©ºé—´Mambaç¼–ç å¤šæ–¹å‘ç‰¹å¾ï¼Œå‡è½»ç‰¹å¾å¯¹é½ä¸­çš„æ¨¡æ€å·®å¼‚ã€‚é€šè¿‡è‡ªé€‚åº”LoRAååŒç½‘ç»œï¼ˆALSNï¼‰åœ¨å•ä¸ªé˜¶æ®µå®ç°æ¢å¤å’Œèåˆçš„è”åˆæ“ä½œï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡æœ‰æ•ˆï¼Œä¸”ç›¸è¾ƒäºåˆ†æœŸå¤„ç†æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤šæ¨¡æ€åŒ»å­¦å›¾åƒèåˆå‡è®¾æºå›¾åƒé«˜è´¨é‡ä¸”åƒç´ çº§å¯¹é½ï¼Œä½†å¤„ç†é”™ä½æˆ–é€€åŒ–å›¾åƒæ—¶æ•ˆæœä¸‹é™ã€‚</li>
<li>UniFuseæ¡†æ¶é€šè¿‡åµŒå…¥é€€åŒ–æ„ŸçŸ¥æç¤ºå­¦ä¹ æ¨¡å—ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>UniFuseæ¡†æ¶èƒ½å¤Ÿæ— ç¼é›†æˆè¾“å…¥å›¾åƒçš„å¤šæ–¹å‘ä¿¡æ¯ï¼Œå¹¶å…³è”è·¨æ¨¡æ€å¯¹é½ä¸æ¢å¤ã€‚</li>
<li>Omniç»Ÿä¸€ç‰¹å¾è¡¨ç¤ºæ–¹æ¡ˆåˆ©ç”¨ç©ºé—´Mambaç¼–ç å¤šæ–¹å‘ç‰¹å¾ï¼Œå‡è½»æ¨¡æ€å·®å¼‚ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”LoRAååŒç½‘ç»œï¼ˆALSNï¼‰ï¼Œå®ç°åœ¨å•ä¸ªé˜¶æ®µçš„æ¢å¤å’Œèåˆè”åˆæ“ä½œã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜UniFuseæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡æœ‰æ•ˆï¼Œä¸”è¾ƒä¼ ç»Ÿæ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4fe0fc2c4b0273ed0868a039f8e73262.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd3d7971ab94ff320f3e8a1cd97aeae9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21e698c32a63cafc90762f4f138d0bb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddbf6e99a1aca26b126d343ff477f3e3.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="FedCLAM-Client-Adaptive-Momentum-with-Foreground-Intensity-Matching-for-Federated-Medical-Image-Segmentation"><a href="#FedCLAM-Client-Adaptive-Momentum-with-Foreground-Intensity-Matching-for-Federated-Medical-Image-Segmentation" class="headerlink" title="FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for   Federated Medical Image Segmentation"></a>FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for   Federated Medical Image Segmentation</h2><p><strong>Authors:Vasilis Siomos, Jonathan Passerat-Palmbach, Giacomo Tarroni</strong></p>
<p>Federated learning is a decentralized training approach that keeps data under stakeholder control while achieving superior performance over isolated training. While inter-institutional feature discrepancies pose a challenge in all federated settings, medical imaging is particularly affected due to diverse imaging devices and population variances, which can diminish the global modelâ€™s effectiveness. Existing aggregation methods generally fail to adapt across varied circumstances. To address this, we propose FedCLAM, which integrates \textit{client-adaptive momentum} terms derived from each clientâ€™s loss reduction during local training, as well as a \textit{personalized dampening factor} to curb overfitting. We further introduce a novel \textit{intensity alignment} loss that matches predicted and ground-truth foreground distributions to handle heterogeneous image intensity profiles across institutions and devices. Extensive evaluations on two datasets show that FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks, underscoring its efficacy. The code is available at <a target="_blank" rel="noopener" href="https://github.com/siomvas/FedCLAM">https://github.com/siomvas/FedCLAM</a>. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ æ˜¯ä¸€ç§å»ä¸­å¿ƒåŒ–çš„è®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ•°æ®å—åˆ©ç›Šç›¸å…³è€…æ§åˆ¶çš„åŒæ—¶ï¼Œå®ç°ä¼˜äºç‹¬ç«‹è®­ç»ƒçš„æ€§èƒ½ã€‚è™½ç„¶è·¨æœºæ„ç‰¹å¾å·®å¼‚åœ¨æ‰€æœ‰è”é‚¦ç¯å¢ƒä¸­éƒ½æ„æˆæŒ‘æˆ˜ï¼Œä½†ç”±äºå„ç§æˆåƒè®¾å¤‡å’Œäººç¾¤å·®å¼‚ï¼ŒåŒ»å­¦å½±åƒå—åˆ°çš„å½±å“å°¤ä¸ºçªå‡ºï¼Œè¿™å¯èƒ½ä¼šé™ä½å…¨å±€æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç°æœ‰çš„èšåˆæ–¹æ³•ä¸€èˆ¬æ— æ³•é€‚åº”å„ç§æƒ…å†µçš„å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FedCLAMï¼Œå®ƒç»“åˆäº†æ¯ä¸ªå®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒè¿‡ç¨‹ä¸­æŸå¤±å‡å°‘æ‰€è¡ç”Ÿå‡ºçš„â€œå®¢æˆ·ç«¯è‡ªé€‚åº”åŠ¨é‡â€é¡¹ï¼Œä»¥åŠä¸€ä¸ªæŠ‘åˆ¶è¿‡æ‹Ÿåˆçš„â€œä¸ªæ€§åŒ–é˜»å°¼å› å­â€ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„â€œå¼ºåº¦å¯¹é½â€æŸå¤±ï¼Œç”¨äºåŒ¹é…é¢„æµ‹å‰æ™¯å’ŒçœŸå®å‰æ™¯åˆ†å¸ƒï¼Œä»¥å¤„ç†ä¸åŒæœºæ„å’Œè®¾å¤‡ä¹‹é—´å¼‚è´¨å›¾åƒå¼ºåº¦åˆ†å¸ƒã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒFedCLAMåœ¨åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸Šè¶…è¶Šäº†å…«ç§å‰æ²¿æ–¹æ³•ï¼Œå‡¸æ˜¾äº†å…¶æœ‰æ•ˆæ€§ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/siomvas/FedCLAM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/siomvas/FedCLAMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22580v1">PDF</a> 10 pages, 2 figures, Accepted at MICCAI 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒè”é‚¦å­¦ä¹ ä¸­çš„æœºæ„é—´ç‰¹å¾å·®å¼‚æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ ·åŒ–çš„æˆåƒè®¾å¤‡å’Œäººå£å·®å¼‚çš„æƒ…å†µä¸‹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼ŒFedCLAMé€šè¿‡å¼•å…¥å®¢æˆ·ç«¯é€‚åº”æ€§åŠ¨é‡å’Œä¸ªæ€§åŒ–æŠ‘åˆ¶å› å­æ¥é€‚åº”ä¸åŒå®¢æˆ·ç«¯çš„æŸå¤±å‡å°‘æƒ…å†µï¼ŒåŒæ—¶é‡‡ç”¨å¼ºåº¦å¯¹é½æŸå¤±æ¥åŒ¹é…é¢„æµ‹å’ŒçœŸå®å‰æ™¯åˆ†å¸ƒï¼Œå¤„ç†ä¸åŒæœºæ„å’Œè®¾å¤‡çš„å¼‚æ„å›¾è±¡å¼ºåº¦åˆ†å¸ƒã€‚FedCLAMåœ¨åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­è¶…è¶Šå…«ç§å‰æ²¿æ–¹æ³•ï¼Œè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è”é‚¦å­¦ä¹ æ˜¯ä¸€ç§å»ä¸­å¿ƒåŒ–çš„è®­ç»ƒæ–¹æ³•ï¼Œå®ç°äº†æ•°æ®åœ¨åˆ©ç›Šç›¸å…³è€…æ§åˆ¶ä¸‹çš„é«˜æ€§èƒ½è®­ç»ƒã€‚</li>
<li>åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸï¼Œä¸åŒæœºæ„é—´çš„ç‰¹å¾å·®å¼‚æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¤šæ ·åŒ–çš„æˆåƒè®¾å¤‡å’Œäººå£å·®å¼‚ä¼šå½±å“å…¨çƒæ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>FedCLAMé€šè¿‡å¼•å…¥å®¢æˆ·ç«¯é€‚åº”æ€§åŠ¨é‡å’Œä¸ªæ€§åŒ–æŠ‘åˆ¶å› å­æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>FedCLAMé‡‡ç”¨å¼ºåº¦å¯¹é½æŸå¤±æ¥å¤„ç†ä¸åŒæœºæ„å’Œè®¾å¤‡çš„å¼‚æ„å›¾è±¡å¼ºåº¦åˆ†å¸ƒé—®é¢˜ã€‚</li>
<li>FedCLAMåœ¨åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dcd5e41a33dbad9cd2241f2978217e9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e28d940778035433e118bb5555a2c311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9a7d10b977abae6c626693cfe9ae638.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-56c1499ca1498945a60e4450eaf36f45.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  JAM-Flow Joint Audio-Motion Synthesis with Flow Matching
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4cf9c2bc6af6f33a9d63db6f06aefbd6.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Imagine for Me Creative Conceptual Blending of Real Images and Text via   Blended Attention
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
