<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-07-03  Supervised Diffusion-Model-Based PET Image Reconstruction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-51ed996d0656e43ac56aed689e67752c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-03-更新"><a href="#2025-07-03-更新" class="headerlink" title="2025-07-03 更新"></a>2025-07-03 更新</h1><h2 id="Supervised-Diffusion-Model-Based-PET-Image-Reconstruction"><a href="#Supervised-Diffusion-Model-Based-PET-Image-Reconstruction" class="headerlink" title="Supervised Diffusion-Model-Based PET Image Reconstruction"></a>Supervised Diffusion-Model-Based PET Image Reconstruction</h2><p><strong>Authors:George Webber, Alexander Hammers, Andrew P King, Andrew J Reader</strong></p>
<p>Diffusion models (DMs) have recently been introduced as a regularizing prior for PET image reconstruction, integrating DMs trained on high-quality PET images with unsupervised schemes that condition on measured data. While these approaches have potential generalization advantages due to their independence from the scanner geometry and the injected activity level, they forgo the opportunity to explicitly model the interaction between the DM prior and noisy measurement data, potentially limiting reconstruction accuracy. To address this, we propose a supervised DM-based algorithm for PET reconstruction. Our method enforces the non-negativity of PET’s Poisson likelihood model and accommodates the wide intensity range of PET images. Through experiments on realistic brain PET phantoms, we demonstrate that our approach outperforms or matches state-of-the-art deep learning-based methods quantitatively across a range of dose levels. We further conduct ablation studies to demonstrate the benefits of the proposed components in our model, as well as its dependence on training data, parameter count, and number of diffusion steps. Additionally, we show that our approach enables more accurate posterior sampling than unsupervised DM-based methods, suggesting improved uncertainty estimation. Finally, we extend our methodology to a practical approach for fully 3D PET and present example results from real [$^{18}$F]FDG brain PET data. </p>
<blockquote>
<p>扩散模型（DMs）最近被引入为PET图像重建的正则化先验，它将训练于高质量PET图像的扩散模型与基于测量数据的无监督方案相结合。虽然这些方法由于独立于扫描器几何和注入活性水平而具有潜在的泛化优势，但它们放弃了明确建模扩散模型先验与噪声测量数据之间相互作用的机会，这可能限制了重建的准确性。为了解决这一问题，我们提出了一种基于扩散模型的PET重建监督算法。我们的方法强制实施PET的Poisson似然模型的非负性，并适应PET图像的大强度范围。通过在真实的脑部PET幻影上进行的实验，我们证明了我们的方法在多种剂量水平上定量地优于或匹配最先进基于深度学习的方法。我们进一步进行消融研究，以展示我们模型中提议组件的优点，以及其对训练数据、参数计数和扩散步骤数量的依赖。此外，我们表明，我们的方法能够实现比无监督的扩散模型方法更精确的后验抽样，这暗示了不确定性估计有所提高。最后，我们将方法论扩展到实用的完全3D PET方法，并展示来自真实氟代脱氧葡萄糖（[$^{18}$F]FDG）脑部PET数据的示例结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.24034v1">PDF</a> 12 pages, 6 figures. Submitted to MICCAI 2025, not peer-reviewed</p>
<p><strong>Summary</strong></p>
<p>本文介绍了将扩散模型（DMs）应用于PET图像重建的方法。该方法结合了经过高质量PET图像训练的扩散模型与基于测量数据的无监督方案，并提出了一种基于监督的DM算法以提高PET重建的准确性。实验结果表明，该方法在剂量水平范围内定量优于或匹配当前最先进的深度学习方法，并能提供更准确的不确定性估计。最后，将其扩展到实用的全3D PET方法，并提供实际[^{18}F]FDG大脑PET数据的示例结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型（DMs）被用作PET图像重建的正则化先验。</li>
<li>扩散模型与无监督方案结合，但这种方法忽略了与噪声测量数据的交互建模，可能限制重建的准确性。</li>
<li>提出了一种基于监督的DM算法，针对PET重建中的非负性强制和广泛的图像强度范围进行适应。</li>
<li>实验表明，该方法在剂量水平范围内定量性能优于或匹配最先进的深度学习方法。</li>
<li>消融研究展示了模型中各个组件的好处，以及其依赖于训练数据、参数计数和扩散步骤数量的关系。</li>
<li>该方法能提供更准确的不确定性估计，相比无监督的DM方法具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.24034">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fecc963fb0693ddb673fcd3bf11e7dd5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce3efb9efc84b62a768f3aa41e377bb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-983fab21fc0931ec52474d09da7031ee.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Industrial-brain-a-human-like-autonomous-neuro-symbolic-cognitive-decision-making-system"><a href="#Industrial-brain-a-human-like-autonomous-neuro-symbolic-cognitive-decision-making-system" class="headerlink" title="Industrial brain: a human-like autonomous neuro-symbolic cognitive   decision-making system"></a>Industrial brain: a human-like autonomous neuro-symbolic cognitive   decision-making system</h2><p><strong>Authors:Junping Wang, Bicheng Wang, Yibo Xuea, Yuan Xie</strong></p>
<p>Resilience non-equilibrium measurement, the ability to maintain fundamental functionality amidst failures and errors, is crucial for scientific management and engineering applications of industrial chain. The problem is particularly challenging when the number or types of multiple co-evolution of resilience (for example, randomly placed) are extremely chaos. Existing end-to-end deep learning ordinarily do not generalize well to unseen full-feld reconstruction of spatiotemporal co-evolution structure, and predict resilience of network topology, especially in multiple chaos data regimes typically seen in real-world applications. To address this challenge, here we propose industrial brain, a human-like autonomous cognitive decision-making and planning framework integrating higher-order activity-driven neuro network and CT-OODA symbolic reasoning to autonomous plan resilience directly from observational data of global variable. The industrial brain not only understands and model structure of node activity dynamics and network co-evolution topology without simplifying assumptions, and reveal the underlying laws hidden behind complex networks, but also enabling accurate resilience prediction, inference, and planning. Experimental results show that industrial brain significantly outperforms resilience prediction and planning methods, with an accurate improvement of up to 10.8% over GoT and OlaGPT framework and 11.03% over spectral dimension reduction. It also generalizes to unseen topologies and dynamics and maintains robust performance despite observational disturbances. Our findings suggest that industrial brain addresses an important gap in resilience prediction and planning for industrial chain. </p>
<blockquote>
<p>在工业产业链的科学管理和工程应用中，韧性非均衡测量（Resilience non-equilibrium measurement）以及在故障和错误中的维持基础功能的能力至关重要。当多种共演韧性的数量或类型出现极度混乱（例如随机放置）时，这个问题尤其具有挑战性。现有的端到端深度学习通常不能很好地推广到未见过的时空共演化结构全场重建，以及预测网络拓扑的韧性，特别是在现实世界应用中常见的多重混沌数据状态下。为了应对这一挑战，我们提出了工业大脑（Industrial Brain）这一概念。它是一个融合了高阶活动驱动神经网络和CT-OODA符号推理的人机认知决策规划框架，能直接根据全球变量的观测数据进行自主韧性规划。工业大脑不仅理解和建模节点活动动态和网络共演化拓扑结构，而无需进行简化假设，揭示复杂网络背后隐藏的基本规律，而且能够实现精确的韧性预测、推理和规划。实验结果表明，工业大脑在韧性预测和规划方法上表现出显著优势，相较于GoT和OlaGPT框架最多提高了1.jpg8%，相较于光谱维度减少提高了超过一倍达到平均精度为惊人的增加了百分之零点八，它还能够推广应用于未见过的拓扑结构和动态状态，并且在观测干扰的情况下仍能保持稳健性能。我们的研究结果表明，工业大脑填补了产业链韧性预测和规划中的一项重要空白。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23926v1">PDF</a> </p>
<p><strong>Summary</strong><br>     工业大脑通过整合高阶活动驱动神经网络和CT-OODA符号推理，直接从全球变量观测数据中自主规划韧性，解决产业链在多重混沌数据状态下的韧性预测和规划难题。此方法不仅理解并建模节点活动动态和网络协同演化拓扑结构，揭示复杂网络背后的基本规律，还能实现准确的韧性预测、推断和规划。实验结果显示，工业大脑在韧性预测和规划方法上表现出显著优势，较GoT和OlaGPT框架以及光谱降维方法分别提高了最高达10.8%和11.03%。面对观测干扰，它依然能维持稳健性能并推广到未见过的拓扑和动态中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>工业大脑的自主认知决策规划框架整合了高阶活动驱动神经网络和CT-OODA符号推理。</li>
<li>该方法解决了多重混沌数据状态下产业链韧性预测和规划的挑战。</li>
<li>工业大脑能够理解和建模节点活动动态以及网络协同演化拓扑结构。</li>
<li>工业大脑能够揭示复杂网络背后的基本规律。</li>
<li>实验结果显示工业大脑在韧性预测和规划方面表现出显著优势，准确度高。</li>
<li>面对观测干扰，工业大脑依然能维持稳健性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23926">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b68bb1d172e3ee2f0b6890e07404c4b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e03edc0ad101688c8bf7efe7429556b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-possible-two-fold-scenario-for-the-disc-corona-of-the-luminous-AGN-1H-0419–577-a-high-density-disc-or-a-warm-corona"><a href="#A-possible-two-fold-scenario-for-the-disc-corona-of-the-luminous-AGN-1H-0419–577-a-high-density-disc-or-a-warm-corona" class="headerlink" title="A possible two-fold scenario for the disc-corona of the luminous AGN 1H   0419–577: a high-density disc or a warm corona"></a>A possible two-fold scenario for the disc-corona of the luminous AGN 1H   0419–577: a high-density disc or a warm corona</h2><p><strong>Authors:Delphine Porquet, James N. Reeves, Valentina Braito</strong></p>
<p>[abridged] 1H 0419-577 is a highly-accreting, luminous BLS1 AGN. This study aims to characterise its disc-corona system using, for the first time, simultaneous XMM-Newton and NuSTAR observations, performed in May and November 2018. We conducted high-resolution grating spectroscopy to identify potential soft X-ray absorption and emission features. To measure the hot corona temperatures from the spectral analysis above 3 keV, we also included data from a previous NuSTAR observation from June 2015. We characterised the disc-corona system properties by analysing the broadband spectra and the SED from UV to hard X-rays. 1H 0419-577 was observed in a bare-like high-flux state at both epochs, with negligible neutral and ionised absorption along its line of sight at both Galactic and AGN rest-frames. However, several soft X-ray emission lines were detected, notably a broad and intense OVII line indicating an accretion disc origin at only a few tens of gravitational radii. The broadband X-ray spectra revealed a prominent, absorption-free smooth soft X-ray excess, a weak Fe Kalpha complex, and a lack of a Compton hump. Fitting data above 3 keV yielded apparent moderate hot corona temperatures of <del>20-30 keV for the 2018 and 2015 observations, depending on the model applied. The 2018 X-ray broadband spectra were well reproduced by either a relativistic reflection model with a high-density accretion disc (</del>10^18 cm^-2), or a hybrid model combining warm and hot coronae with relativistic reflection. We performed the SED analysis for the latter scenario, which indicated that both the hot and warm coronae would have a small spatial extent. Both scenarios can successfully reproduce the two 2018 observations of 1H 0419-577, but they imply very different physical conditions, for example, in terms of disc density, temperature and accretion power released in the hot corona and the origin of the UV emission. </p>
<blockquote>
<p>[摘要] 1H 0419-577是一颗高度聚集、明亮的BLS1型活跃星系核（Active Galactic Nucleus，简称AGNs）。本研究旨在首次利用同时进行的XMM-Newton和NuSTAR观测，对其盘冕系统进行分析。这些观测分别在2018年五月和十一月进行。我们进行了高分辨率光栅光谱法，以识别潜在的软X射线吸收和发射特征。为了从高于3keV的光谱分析中测量高温冕温度，我们还纳入了来自2015年六月的先前NuSTAR观测数据。我们通过分析从紫外到硬X射线的宽频光谱和光谱能量分布（SED）来表征盘冕系统的特性。在两个时期，1H 0419-577均被观察到处于裸露式高流量状态，其视线方向上的中性离子和离子化吸收可以忽略不计，无论是在银河系框架还是活跃星系核静止框架中都是如此。然而，检测到了多条软X射线发射线，尤其是宽而强烈的OVII线，这表明它仅起源于几十重力半径的吸积盘。宽频X射线光谱显示了一个明显且没有吸收的平滑软X射线过剩、较弱的Fe Kalpha复合体，并且没有康普顿凸起。对高于3keV的数据进行拟合，得出适用于2018年和2015年观测的明显的中度高温冕温度约为20-30keV，这取决于所应用的模型。采用相对论反射模型的高密度吸积盘（约每平方厘米十的十八次方）或者结合了暖冠和高温冕以及相对论反射的混合模型都可以很好地重现2 2 H YQJ $行缘日得#露凯上的管壤确更空养层色作百幅清里修区香调间行的进况型所机或技设经环实置管之架质较部优选定质业配接组安转样意组习应界该采备相达因同较确认后选应多济径构决社与国目确效构所局便管层规论件区化配确该一都习环热联业据电设其习动带安在集在时设全研程基国提统习维济部测联在样安行及可种性多济系济行策体进环基设全维接确技全设联于工进社策理据经进保系管全统程管策热面或各协次度品响探艺形几训过求由<br>根据上述提供的研究论文进行翻译如下：</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23920v1">PDF</a> Accepted for publication in A&amp;A, 12 pages (+appendix)</p>
<p><strong>Summary</strong></p>
<p>该研究的对象是一颗高光度BLS1型活跃星系核（1H 0419-577）。研究首次利用XMM-Newton和NuSTAR同时观测数据，对其盘冕系统进行了表征。通过高分辨率光谱法识别了潜在的软X射线吸收和发射特征。通过谱分析和SED分析，发现该星系核处于裸露的高光态，具有显著的软X射线发射线，特别是来自吸积盘起源的OVII线。同时发现其热冕温度约为20-30keV。研究提出了两种模型来解释其X射线宽带谱，一种是相对论反射模型，另一种是结合温暖和炎热冕的混合模型。这两种模型都能很好地解释观测结果，但它们暗示的物理条件有很大不同。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>1H 0419-577是一颗高光度BLS1型活跃星系核，表现出显著的软X射线发射特性。</li>
<li>首次利用XMM-Newton和NuSTAR的同步观测数据对该星系的盘冕系统进行了表征。</li>
<li>通过谱分析发现了该星系核处于裸露的高光态，具有微弱的Fe Kalpha复合体和缺乏康普顿峰。</li>
<li>热冕温度约为20-30keV，由模型应用决定。</li>
<li>观测结果可以通过相对论反射模型或结合温暖和炎热冕的混合模型来解释。</li>
<li>在混合模型中，热和暖冕的空间范围都很小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23920">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-88f556b1eab8badbc16a942533966856.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a94e625210b1b41cf3656300e14b8162.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-997170702880f366eeb9f864065c25a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-149f7c95e5a7b121a0d10da5908cd76c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69be2d002f71c37be1674d8896f464ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3481b8aa4e922dc4d08ba8e33fa18f80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26998cc1997dfa3f3ec288f8510b7ff0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d947b90200979c891359470c435126c5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Three-dimensional-end-to-end-deep-learning-for-brain-MRI-analysis"><a href="#Three-dimensional-end-to-end-deep-learning-for-brain-MRI-analysis" class="headerlink" title="Three-dimensional end-to-end deep learning for brain MRI analysis"></a>Three-dimensional end-to-end deep learning for brain MRI analysis</h2><p><strong>Authors:Radhika Juglan, Marta Ligero, Zunamys I. Carrero, Asier Rabasco, Tim Lenz, Leo Misera, Gregory Patrick Veldhuizen, Paul Kuntke, Hagen H. Kitzler, Sven Nebelung, Daniel Truhn, Jakob Nikolas Kather</strong></p>
<p>Deep learning (DL) methods are increasingly outperforming classical approaches in brain imaging, yet their generalizability across diverse imaging cohorts remains inadequately assessed. As age and sex are key neurobiological markers in clinical neuroscience, influencing brain structure and disease risk, this study evaluates three of the existing three-dimensional architectures, namely Simple Fully Connected Network (SFCN), DenseNet, and Shifted Window (Swin) Transformers, for age and sex prediction using T1-weighted MRI from four independent cohorts: UK Biobank (UKB, n&#x3D;47,390), Dallas Lifespan Brain Study (DLBS, n&#x3D;132), Parkinson’s Progression Markers Initiative (PPMI, n&#x3D;108 healthy controls), and Information eXtraction from Images (IXI, n&#x3D;319). We found that SFCN consistently outperformed more complex architectures with AUC of 1.00 [1.00-1.00] in UKB (internal test set) and 0.85-0.91 in external test sets for sex classification. For the age prediction task, SFCN demonstrated a mean absolute error (MAE) of 2.66 (r&#x3D;0.89) in UKB and 4.98-5.81 (r&#x3D;0.55-0.70) across external datasets. Pairwise DeLong and Wilcoxon signed-rank tests with Bonferroni corrections confirmed SFCN’s superiority over Swin Transformer across most cohorts (p&lt;0.017, for three comparisons). Explainability analysis further demonstrates the regional consistency of model attention across cohorts and specific to each task. Our findings reveal that simpler convolutional networks outperform the denser and more complex attention-based DL architectures in brain image analysis by demonstrating better generalizability across different datasets. </p>
<blockquote>
<p>深度学习（DL）方法在脑成像方面越来越优于传统方法，但它们在各种成像队列中的通用性仍未得到充分评估。年龄和性别是临床神经生物学中的关键神经生物学标志物，影响脑结构和疾病风险。因此，本研究评估了三种现有的三维架构，即简单全连接网络（SFCN）、DenseNet和移位窗口（Swin）变压器，用于使用T1加权MRI对四个独立队列（英国生物银行（UKB，n&#x3D;47390）、Dallas终身大脑研究（DLBS，n&#x3D;132）、帕金森病进展标志物倡议组织（PPMI，n&#x3D;108名健康对照者），以及图像信息提取（IXI，n&#x3D;319）的年龄和性别预测。我们发现SFCN在性别分类方面始终优于更复杂的架构。在英国生物银行内部测试集中，其AUC为[1.00]（范围：1.00-1.00），在外部测试集中的AUC为0.85-0.91。在年龄预测任务中，SFCN在英国生物银行的平均绝对误差（MAE）为2.66（r&#x3D;0.89），而在外部数据集中的MAE为4.98-5.81（r&#x3D;0.55-0.70）。经过Bonferroni校正的成对DeLong和Wilcoxon符号秩检验证实了SFCN在大多数队列中优于Swin Transformer（p&lt;0.017，进行三次比较）。解释性分析进一步证明了模型注意力在不同队列中的区域一致性，并特定于每个任务。我们的研究结果表明，在脑图像分析中，更简单的卷积网络在跨不同数据集时表现出更好的通用性，优于更密集和更复杂的基于注意力的深度学习架构。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23916v1">PDF</a> </p>
<p><strong>Summary</strong><br>     深度学习在脑成像领域逐渐超越传统方法，但其在不同成像队列中的泛化能力尚未得到充分评估。本研究使用三种现有的三维架构，即简单全连接网络（SFCN）、DenseNet和移位窗口（Swin）变压器，对来自四个独立队列的T1加权MRI数据进行年龄和性别预测。研究发现，SFCN在性别分类上表现优异，且在年龄预测任务上也表现出良好的泛化能力，而其他复杂架构则表现不佳。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习在脑成像中逐渐超越传统方法，但泛化能力评估不足。</li>
<li>研究评估了三种三维架构（SFCN、DenseNet、Swin变压器）在脑图像分析中的性能。</li>
<li>SFCN在性别分类任务上表现最佳，AUC达到1.00。</li>
<li>SFCN在年龄预测任务上也表现出良好的泛化能力，均方误差较小。</li>
<li>SFCN相较于更密集和复杂的注意力基础深度学习架构表现更好。</li>
<li>Explainability分析显示模型关注区域在不同数据集之间具有一致性，且特定任务特定。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23916">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4e16bc517c23a0227b3c04dbe776f159.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GroundingDINO-US-SAM-Text-Prompted-Multi-Organ-Segmentation-in-Ultrasound-with-LoRA-Tuned-Vision-Language-Models"><a href="#GroundingDINO-US-SAM-Text-Prompted-Multi-Organ-Segmentation-in-Ultrasound-with-LoRA-Tuned-Vision-Language-Models" class="headerlink" title="GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in   Ultrasound with LoRA-Tuned Vision-Language Models"></a>GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in   Ultrasound with LoRA-Tuned Vision-Language Models</h2><p><strong>Authors:Hamza Rasaee, Taha Koleilat, Hassan Rivaz</strong></p>
<p>Accurate and generalizable object segmentation in ultrasound imaging remains a significant challenge due to anatomical variability, diverse imaging protocols, and limited annotated data. In this study, we propose a prompt-driven vision-language model (VLM) that integrates Grounding DINO with SAM2 to enable object segmentation across multiple ultrasound organs. A total of 18 public ultrasound datasets, encompassing the breast, thyroid, liver, prostate, kidney, and paraspinal muscle, were utilized. These datasets were divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for testing to evaluate performance in unseen distributions. Comprehensive experiments demonstrate that our approach outperforms state-of-the-art segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse, and SAMUS on most seen datasets while maintaining strong performance on unseen datasets without additional fine-tuning. These results underscore the promise of VLMs in scalable and robust ultrasound image analysis, reducing dependence on large, organ-specific annotated datasets. We will publish our code on code.sonography.ai after acceptance. </p>
<blockquote>
<p>在超声成像中，准确且可推广的目标分割仍然是一个重大挑战，这主要是由于解剖结构差异、多样的成像协议和有限的标注数据。在这项研究中，我们提出了一种基于提示驱动的视觉语言模型（VLM），它将Grounding DINO与SAM2集成在一起，以实现多个超声器官的目碿分割。我们共使用了18个公开超声数据集，包括乳腺、甲状腺、肝脏、前列腺、肾脏和背侧肌肉等。这些数据集被分为两部分：其中15个数据集用于微调Grounding DINO并使用低秩适应（LoRA）将其适应超声领域，另外3个数据集则完全保留用于测试，以评估在未见分布中的性能表现。全面的实验表明，我们的方法优于最先进分割方法，包括UniverSeg、MedSAM、MedCLIP-SAM、BiomedParse和SAMUS在大多数已知数据集上的表现，同时在对未见数据集无需额外微调的情况下仍能保持强劲表现。这些结果突显了视觉语言模型在可扩展和稳健的超声图像分析中的潜力，并降低了对大型特定器官标注数据集的依赖。接受后，我们将在code.sonography.ai上发布我们的代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23903v1">PDF</a> 11 pages, 3 figures, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本研究提出一种基于提示驱动的视觉语言模型（VLM），结合Grounding DINO与SAM2，实现跨多个超声器官的物体分割。该研究使用18个公共超声数据集，通过低秩适应（LoRA）对Grounding DINO进行微调与验证，并在未见分布的数据集上测试性能。实验表明，该方法优于其他先进分割方法，包括UniverSeg、MedSAM、MedCLIP-SAM、BiomedParse和SAMUS，在未见数据集上无需额外微调即可保持强劲性能。这突显了VLM在可伸缩和稳健的超声图像分析中的潜力，减少对大量特定器官标注数据集的依赖。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出一种基于提示驱动的视觉语言模型（VLM），用于超声成像中的物体分割。</li>
<li>集成Grounding DINO与SAM2技术，实现跨多个超声器官的物体分割。</li>
<li>采用18个公共超声数据集，涵盖多个器官，如乳房、甲状腺、肝脏、前列腺、肾脏和背肌。</li>
<li>使用低秩适应（LoRA）技术对Grounding DINO进行微调与验证。</li>
<li>在未见分布的数据集上进行测试，展示出色性能。</li>
<li>该方法优于其他先进分割方法，包括UniverSeg、MedSAM等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23903">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d1c1204f51e56f7c46a107fe40b6c419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a2128cbc5b97072d097159348639e7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7002af9c28262ea2c83bdc26f4f96f71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6a2ec1a3e49848eb6a0c0cb729e83f3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Emerging-AI-Approaches-for-Cancer-Spatial-Omics"><a href="#Emerging-AI-Approaches-for-Cancer-Spatial-Omics" class="headerlink" title="Emerging AI Approaches for Cancer Spatial Omics"></a>Emerging AI Approaches for Cancer Spatial Omics</h2><p><strong>Authors:Javad Noorbakhsh, Ali Foroughi pour, Jeffrey Chuang</strong></p>
<p>Technological breakthroughs in spatial omics and artificial intelligence (AI) have the potential to transform the understanding of cancer cells and the tumor microenvironment. Here we review the role of AI in spatial omics, discussing the current state-of-the-art and further needs to decipher cancer biology from large-scale spatial tissue data. An overarching challenge is the development of interpretable spatial AI models, an activity which demands not only improved data integration, but also new conceptual frameworks. We discuss emerging paradigms, in particular data-driven spatial AI, constraint-based spatial AI, and mechanistic spatial modeling, as well as the importance of integrating AI with hypothesis-driven strategies and model systems to realize the value of cancer spatial information. </p>
<blockquote>
<p>技术突破在空间组学和人工智能（AI）方面有望改变对癌细胞和肿瘤微环境的理解。在这里，我们回顾了人工智能在空间组学中的作用，讨论从大规模空间组织数据中解读癌症生物学的最新进展和进一步需求。一个主要的挑战是开发可解释的空间人工智能模型，这不仅需要改进数据集成，还需要新的概念框架。我们讨论了新兴的模式，特别是数据驱动的空间人工智能、约束型空间人工智能和机械空间建模，以及将人工智能与假设驱动策略和模型系统相结合的重要性，以实现癌症空间信息的价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23857v1">PDF</a> 25 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>人工智能和空间组学的技术突破有望改变对癌细胞和肿瘤微环境的理解。本文综述了人工智能在空间组学中的角色，讨论了当前最先进技术和从大规模空间组织数据中解读癌症生物学知识的进一步需求。一个主要挑战是开发可解释的空间人工智能模型，这需要改进数据集成并引入新概念框架。本文讨论了新兴范式，特别是数据驱动的空间人工智能、约束驱动的空间人工智能和机械空间建模的重要性，以及将人工智能与假设驱动策略和模型系统相结合以体现癌症空间信息价值的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>技术突破在人工智能和空间组学领域为理解癌症细胞和肿瘤微环境带来变革潜力。</li>
<li>当前挑战在于开发可解释的空间人工智能模型，需要改进数据集成和新概念框架。</li>
<li>人工智能在空间组学中的角色被重点讨论，包括当前技术和解读癌症生物学的需求。</li>
<li>新兴范式如数据驱动和约束驱动的空间人工智能以及机械空间建模受到关注。</li>
<li>人工智能与假设驱动策略和模型系统的结合对于体现癌症空间信息价值至关重要。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23857">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5d9f7bfa23be10f26b53774090037a77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c8d065e3e5caf650703cef0231e0926.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MDPG-Multi-domain-Diffusion-Prior-Guidance-for-MRI-Reconstruction"><a href="#MDPG-Multi-domain-Diffusion-Prior-Guidance-for-MRI-Reconstruction" class="headerlink" title="MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction"></a>MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction</h2><p><strong>Authors:Lingtong Zhang, Mengdie Song, Xiaohan Hao, Huayu Mai, Bensheng Qiu</strong></p>
<p>Magnetic Resonance Imaging (MRI) reconstruction is essential in medical diagnostics. As the latest generative models, diffusion models (DMs) have struggled to produce high-fidelity images due to their stochastic nature in image domains. Latent diffusion models (LDMs) yield both compact and detailed prior knowledge in latent domains, which could effectively guide the model towards more effective learning of the original data distribution. Inspired by this, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by pre-trained LDMs to enhance data consistency in MRI reconstruction tasks. Specifically, we first construct a Visual-Mamba-based backbone, which enables efficient encoding and reconstruction of under-sampled images. Then pre-trained LDMs are integrated to provide conditional priors in both latent and image domains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion in multi-level latent domains. Simultaneously, to effectively utilize a prior in both the k-space and image domain, under-sampled images are fused with generated full-sampled images by the Dual-domain Fusion Branch (DFB) for self-adaption guidance. Lastly, to further enhance the data consistency, we propose a k-space regularization strategy based on the non-auto-calibration signal (NACS) set. Extensive experiments on two public MRI datasets fully demonstrate the effectiveness of the proposed methodology. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Zolento/MDPG">https://github.com/Zolento/MDPG</a>. </p>
<blockquote>
<p>磁共振成像（MRI）重建在医学诊断中至关重要。作为最新的生成模型，扩散模型（DMs）由于其图像域的随机性，难以产生高保真度的图像。潜在扩散模型（LDMs）在潜在域中产生了紧凑且详细的先验知识，可以有效地引导模型更有效地学习原始数据分布。受此启发，我们提出了由预训练的LDMs提供的多域扩散先验指导（MDPG），以提高MRI重建任务中的数据一致性。具体来说，我们首先构建了一个基于Visual-Mamba的骨干网，使欠采样图像的编码和重建更加高效。然后，将预训练的LDMs集成到潜在域和图像域中，以提供条件先验。提出了一种新的潜在引导注意力（LGA），用于多级潜在域中的有效融合。同时，为了有效地利用k空间和图像域中的先验信息，欠采样图像通过双域融合分支（DFB）与生成的完全采样图像融合，以实现自适应引导。最后，为了进一步增强数据一致性，我们提出了一种基于非自动校准信号（NACS）集的k空间正则化策略。在两个公共MRI数据集上的广泛实验充分证明了所提出方法的有效性。代码可通过以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/Zolento/MDPG">https://github.com/Zolento/MDPG</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23701v1">PDF</a> Accept by MICCAI2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于多域扩散先验指导（MDPG）的磁共振成像（MRI）重建方法。该方法利用预训练的潜在扩散模型（LDM）提供条件先验，通过构建视觉Mamba基础架构并在潜在和图像域中整合LDM，实现高效编码和重建。此外，还提出一种新颖的双域融合分支（DFB）和k空间正则化策略来进一步增强数据一致性。实验结果证明该方法的有效性。代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>扩散模型在MRI重建中面临高保真图像生成挑战。</li>
<li>潜在扩散模型（LDM）在潜在域提供紧凑且详细的先验知识。</li>
<li>提出多域扩散先验指导（MDPG）方法，利用预训练LDM增强MRI重建的数据一致性。</li>
<li>构建视觉Mamba基础架构实现高效编码和重建。</li>
<li>整合预训练LDM在潜在和图像域提供条件先验。</li>
<li>提出新颖的双域融合分支（DFB）进行自适应指导。</li>
<li>实施基于非校准信号的k空间正则化策略来增强数据一致性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23701">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b156120b0d5bd5512c9e5c364dee62e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-835586027be10aa53b8c30174edaa4c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b211d62e7ce29b3504d19d80533492d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GUSL-A-Novel-and-Efficient-Machine-Learning-Model-for-Prostate-Segmentation-on-MRI"><a href="#GUSL-A-Novel-and-Efficient-Machine-Learning-Model-for-Prostate-Segmentation-on-MRI" class="headerlink" title="GUSL: A Novel and Efficient Machine Learning Model for Prostate   Segmentation on MRI"></a>GUSL: A Novel and Efficient Machine Learning Model for Prostate   Segmentation on MRI</h2><p><strong>Authors:Jiaxin Yang, Vasileios Magoulianitis, Catherine Aurelia Christie Alexander, Jintang Xue, Masatomo Kaneko, Giovanni Cacciamani, Andre Abreu, Vinay Duddalwar, C. -C. Jay Kuo, Inderbir S. Gill, Chrysostomos Nikias</strong></p>
<p>Prostate and zonal segmentation is a crucial step for clinical diagnosis of prostate cancer (PCa). Computer-aided diagnosis tools for prostate segmentation are based on the deep learning (DL) paradigm. However, deep neural networks are perceived as “black-box” solutions by physicians, thus making them less practical for deployment in the clinical setting. In this paper, we introduce a feed-forward machine learning model, named Green U-shaped Learning (GUSL), suitable for medical image segmentation without backpropagation. GUSL introduces a multi-layer regression scheme for coarse-to-fine segmentation. Its feature extraction is based on a linear model, which enables seamless interpretability during feature extraction. Also, GUSL introduces a mechanism for attention on the prostate boundaries, which is an error-prone region, by employing regression to refine the predictions through residue correction. In addition, a two-step pipeline approach is used to mitigate the class imbalance, an issue inherent in medical imaging problems. After conducting experiments on two publicly available datasets and one private dataset, in both prostate gland and zonal segmentation tasks, GUSL achieves state-of-the-art performance among other DL-based models. Notably, GUSL features a very energy-efficient pipeline, since it has a model size several times smaller and less complexity than the rest of the solutions. In all datasets, GUSL achieved a Dice Similarity Coefficient (DSC) performance greater than $0.9$ for gland segmentation. Considering also its lightweight model size and transparency in feature extraction, it offers a competitive and practical package for medical imaging applications. </p>
<blockquote>
<p>前列腺癌（PCa）的临床诊断中，前列腺及其分区的分割是一个关键步骤。基于深度学习的前列腺分割计算机辅助诊断工具被广泛采用。然而，深度神经网络被医生视为“黑箱”解决方案，因此在临床环境中实用性较低。本文介绍了一种前馈机器学习模型，名为绿色U形学习（GUSL），适用于医学图像分割，无需反向传播。GUSL引入了一种多层回归方案，用于从粗到细进行分割。其特性提取基于线性模型，可在特性提取过程中实现无缝解释性。此外，GUSL通过在回归中采用一种关注前列腺边界（一个易出错区域）的机制，通过残差修正来优化预测。同时，采用两步管道方法缓解类别不平衡问题，这是医学成像问题所固有的。在公开数据集和私有数据集上进行的前列腺和分区分割任务实验表明，GUSL在与其他深度学习模型相比时表现出卓越的性能。值得一提的是，GUSL的管道非常节能，因为其模型大小较小且复杂性低于其他解决方案。在所有数据集中，GUSL的腺体分割狄克相似系数（DSC）性能均大于0.9。考虑到其轻量级的模型大小和特性提取的透明度，它为医学成像应用提供了一个有竞争力的实用软件包。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23688v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为Green U-shaped Learning（GUSL）的前馈机器学习模型，适用于医学图像分割，无需反向传播。GUSL采用多层回归方案实现由粗到细的分割，特征提取基于线性模型，实现无缝解读。通过回归对前列腺边界进行关注，并采用两步管道方法缓解类别不平衡问题。实验结果显示，GUSL在前列腺腺体及区域分割任务中达到最新技术水平，且具备高能效、模型体积小、透明度高等优点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUSL是一种适用于医学图像分割的前馈机器学习模型，无需反向传播，适合在临床环境中应用。</li>
<li>GUSL采用多层回归方案，实现从粗到细的分割。</li>
<li>特征提取基于线性模型，增强模型的透明度，便于医生理解。</li>
<li>GUSL通过关注前列腺边界，使用回归进行预测修正，提高了分割精度。</li>
<li>采用两步管道方法缓解医学成像中的类别不平衡问题。</li>
<li>实验证明，GUSL在前列腺腺体及区域分割任务中性能优越，Dice Similarity Coefficient（DSC）大于0.9。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23688">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2bf8fe82f4faa71ff28e347f810b2b97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-791846c86c7fc5f5febe894ea6830463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-957fa2acdb9be115ea5738e6b95f7d52.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a12978c3d33cd0b686efb4c8d9debc06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f95857daff61d9b0fab2fbbabab41708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adb8550512e1f866db2515fe1eb743b8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-based-Data-Augmentation-Method-for-Fetal-Head-Ultrasound-Segmentation"><a href="#Diffusion-Model-based-Data-Augmentation-Method-for-Fetal-Head-Ultrasound-Segmentation" class="headerlink" title="Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound   Segmentation"></a>Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound   Segmentation</h2><p><strong>Authors:Fangyijie Wang, Kevin Whelan, Félix Balado, Guénolé Silvestre, Kathleen M. Curran</strong></p>
<p>Medical image data is less accessible than in other domains due to privacy and regulatory constraints. In addition, labeling requires costly, time-intensive manual image annotation by clinical experts. To overcome these challenges, synthetic medical data generation offers a promising solution. Generative AI (GenAI), employing generative deep learning models, has proven effective at producing realistic synthetic images. This study proposes a novel mask-guided GenAI approach using diffusion models to generate synthetic fetal head ultrasound images paired with segmentation masks. These synthetic pairs augment real datasets for supervised fine-tuning of the Segment Anything Model (SAM). Our results show that the synthetic data captures real image features effectively, and this approach reaches state-of-the-art fetal head segmentation, especially when trained with a limited number of real image-mask pairs. In particular, the segmentation reaches Dice Scores of 94.66% and 94.38% using a handful of ultrasound images from the Spanish and African cohorts, respectively. Our code, models, and data are available on GitHub. </p>
<blockquote>
<p>医学图像数据由于隐私和监管限制，相较于其他领域更难以获取。此外，标注需要临床专家进行耗时且成本高昂的手动图像注释。为了克服这些挑战，合成医学数据生成提供了一个有前景的解决方案。采用生成式深度学习模型的生成式人工智能（GenAI）已被证明能够产生逼真的合成图像。本研究提出了一种新型的基于扩散模型的掩膜引导GenAI方法，用于生成配有分割掩膜的合成胎儿头部超声图像。这些合成图像对用于扩充真实数据集，以监督微调分段任何事情模型（SAM）。我们的结果表明，合成数据有效地捕捉了真实图像的特征，特别是在使用有限数量的真实图像-掩膜对进行训练时，这种方法达到了最先进的胎儿头部分割效果。特别是，使用来自西班牙和非洲队列的少量超声图像，分割达到Dice分数分别为94.66%和94.38%。我们的代码、模型和数据均可在GitHub上获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23664v1">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像数据由于隐私和监管限制而难以获取，且标注需要临床专家进行耗时且成本高昂的手动图像标注。研究提出一种基于扩散模型的新型遮罩引导生成对抗网络（GenAI）方法，用于生成配有分割遮罩的合成胎儿头部超声图像。这些合成数据对真实数据集进行增强，用于监督微调SAM（Segment Anything Model）模型。结果显示，合成数据能有效捕捉真实图像特征，且在有限真实图像-遮罩对训练下达到先进的胎儿头部分割效果，Dice得分分别为94.66%和94.38%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像数据因隐私和监管问题而难以获取。</li>
<li>标注医学图像需要临床专家手动进行，成本高昂且耗时。</li>
<li>研究采用新型遮罩引导生成对抗网络（GenAI）创造合成医学图像。</li>
<li>合成胎儿头部超声图像与真实图像结合，用于训练SAM模型。</li>
<li>合成数据能有效模拟真实医学图像特征。</li>
<li>在有限真实图像-遮罩对训练下，达到先进的胎儿头部分割效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23664">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2423c23496a4f78b83a54ddaecfd87c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7dce93e8b19db2c3d8d9015eb56e0908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7b6a8a7abfb2aabf66b7f083e5bb127.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FD-DiT-Frequency-Domain-Directed-Diffusion-Transformer-for-Low-Dose-CT-Reconstruction"><a href="#FD-DiT-Frequency-Domain-Directed-Diffusion-Transformer-for-Low-Dose-CT-Reconstruction" class="headerlink" title="FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT   Reconstruction"></a>FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT   Reconstruction</h2><p><strong>Authors:Qiqing Liu, Guoquan Wei, Zekun Zhou, Yiyang Wen, Liu Shi, Qiegen Liu</strong></p>
<p>Low-dose computed tomography (LDCT) reduces radiation exposure but suffers from image artifacts and loss of detail due to quantum and electronic noise, potentially impacting diagnostic accuracy. Transformer combined with diffusion models has been a promising approach for image generation. Nevertheless, existing methods exhibit limitations in preserving finegrained image details. To address this issue, frequency domain-directed diffusion transformer (FD-DiT) is proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy that progressively introduces noise until the distribution statistically aligns with that of LDCT data, followed by denoising processing. Furthermore, we employ a frequency decoupling technique to concentrate noise primarily in high-frequency domain, thereby facilitating effective capture of essential anatomical structures and fine details. A hybrid denoising network is then utilized to optimize the overall data reconstruction process. To enhance the capability in recognizing high-frequency noise, we incorporate sliding sparse local attention to leverage the sparsity and locality of shallow-layer information, propagating them via skip connections for improving feature representation. Finally, we propose a learnable dynamic fusion strategy for optimal component integration. Experimental results demonstrate that at identical dose levels, LDCT images reconstructed by FD-DiT exhibit superior noise and artifact suppression compared to state-of-the-art methods. </p>
<blockquote>
<p>低剂量计算机断层扫描（LDCT）降低了辐射暴露，但由于量子和电子噪声的影响，会出现图像伪影和细节损失，从而可能影响诊断准确性。结合扩散模型的变压器在图像生成方面表现出良好的前景。然而，现有方法在保留精细图像细节方面存在局限性。为了解决这一问题，提出了频率域导向扩散变压器（FD-DiT）用于LDCT重建。FD-DiT专注于一种扩散策略，该策略逐步引入噪声，直到统计分布与LDCT数据分布对齐，然后进行去噪处理。此外，我们采用频率解耦技术，将噪声主要集中在高频域，从而便于有效捕获重要的解剖结构和细节。然后，使用混合去噪网络优化整体数据重建过程。为了提高识别高频噪声的能力，我们引入了滑动稀疏局部注意力，利用浅层信息的稀疏性和局部性，通过跳过连接传播它们，以改进特征表示。最后，我们提出了一种可学习的动态融合策略，以实现最佳组件集成。实验结果表明，在相同剂量水平下，采用FD-DiT重建的LDCT图像与最新方法相比，具有更好的噪声和伪影抑制效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23466v1">PDF</a> 11pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>低剂量计算机断层扫描（LDCT）虽能减少辐射暴露，但存在图像伪影和细节丢失问题。本文提出一种频率域导向的扩散变压器（FD-DiT）方法用于LDCT重建，通过扩散策略逐步引入噪声，再采用去噪处理。结合频率解耦技术和混合去噪网络，提高图像质量。实验结果表明，在相同剂量水平下，FD-DiT重建的LDCT图像具有更好的噪声和伪影抑制效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDCT存在图像伪影和细节丢失的问题。</li>
<li>频率域导向的扩散变压器（FD-DiT）被提出来解决这些问题。</li>
<li>FD-DiT采用扩散策略逐步引入噪声，并注重高频域的噪声处理。</li>
<li>结合频率解耦技术和混合去噪网络来提高图像质量。</li>
<li>采用滑动稀疏局部注意力机制来识别高频噪声。</li>
<li>提出一种可学习的动态融合策略来实现最佳组件集成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23466">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a880236d7447a486a0963a14f3a8275f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-232793d4acf81f8499034232e47a3793.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80a28a8922bfe48da6f62334adcea8be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67842c61c81aef2e0862ce1c99caeb96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69bfea0ee7478f78d0c2cce6351c76c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ab1794c96b11d7131d9b21d55bedab0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Contrastive-Learning-with-Diffusion-Features-for-Weakly-Supervised-Medical-Image-Segmentation"><a href="#Contrastive-Learning-with-Diffusion-Features-for-Weakly-Supervised-Medical-Image-Segmentation" class="headerlink" title="Contrastive Learning with Diffusion Features for Weakly Supervised   Medical Image Segmentation"></a>Contrastive Learning with Diffusion Features for Weakly Supervised   Medical Image Segmentation</h2><p><strong>Authors:Dewen Zeng, Xinrong Hu, Yu-Jen Chen, Yawen Wu, Xiaowei Xu, Yiyu Shi</strong></p>
<p>Weakly supervised semantic segmentation (WSSS) methods using class labels often rely on class activation maps (CAMs) to localize objects. However, traditional CAM-based methods struggle with partial activations and imprecise object boundaries due to optimization discrepancies between classification and segmentation. Recently, the conditional diffusion model (CDM) has been used as an alternative for generating segmentation masks in WSSS, leveraging its strong image generation capabilities tailored to specific class distributions. By modifying or perturbing the condition during diffusion sampling, the related objects can be highlighted in the generated images. Yet, the saliency maps generated by CDMs are prone to noise from background alterations during reverse diffusion. To alleviate the problem, we introduce Contrastive Learning with Diffusion Features (CLDF), a novel method that uses contrastive learning to train a pixel decoder to map the diffusion features from a frozen CDM to a low-dimensional embedding space for segmentation. Specifically, we integrate gradient maps generated from CDM external classifier with CAMs to identify foreground and background pixels with fewer false positives&#x2F;negatives for contrastive learning, enabling robust pixel embedding learning. Experimental results on four segmentation tasks from two public medical datasets demonstrate that our method significantly outperforms existing baselines. </p>
<blockquote>
<p>基于弱监督语义分割（WSSS）的方法常常利用类别标签进行定位对象的方法，采用类激活映射（CAMs）。然而，传统的基于CAM的方法由于分类和分割之间的优化差异，对于部分激活和不精确的对象边界感到困扰。最近，条件扩散模型（CDM）已被用作生成WSSS中的分割掩码的替代方法，利用其针对特定类别分布定制的强大图像生成能力。通过在扩散采样过程中修改或扰动条件，可以在生成的图像中突出显示相关对象。然而，由CDM生成的显著性映射容易在反向扩散过程中受到背景改变而产生的噪声干扰。为了缓解这个问题，我们引入了对比学习扩散特征（CLDF）方法，这是一种使用对比学习训练像素解码器的新方法，将来自冻结CDM的扩散特征映射到低维嵌入空间进行分割。具体来说，我们将由CDM外部分类器生成的梯度图与CAM相结合，以确定用于对比学习的前景和背景像素，以减少误报&#x2F;漏报的数量，从而实现稳健的像素嵌入学习。在来自两个公共医疗数据集的四个分割任务上的实验结果表明，我们的方法显著优于现有基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23460v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了基于条件扩散模型（CDM）的弱监督语义分割（WSSS）方法，通过使用对比学习训练像素解码器，将扩散特征映射到低维嵌入空间进行分割。新方法结合CDM外部分类器生成的梯度图和类激活映射（CAMs），提高了前景和背景像素的识别能力，减少了误报&#x2F;漏报，实现了稳健的像素嵌入学习。在公共医学数据集上的实验结果表明，该方法显著优于现有基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>弱监督语义分割（WSSS）在利用类标签进行对象定位时常常依赖于类激活映射（CAMs）。</li>
<li>传统基于CAM的方法在部分激活和不精确的对象边界上存在问题，因为分类和分割之间的优化差异。</li>
<li>条件扩散模型（CDM）已被用于生成WSSS中的分割掩膜，利用其针对特定类分布的图像生成能力。</li>
<li>通过修改或扰动扩散采样过程中的条件，可以突出与类相关的对象。</li>
<li>CDM生成的显著性映射容易受到反向扩散过程中背景改变产生的噪声影响。</li>
<li>新方法Contrastive Learning with Diffusion Features (CLDF)使用对比学习来训练像素解码器，将扩散特征映射到低维嵌入空间进行分割。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23460">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-455f35321a7f25148ed921842a7f3af6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-073432a04132db70cb746f7ace48a58b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9402e69e63a67620e791f8b87e50943.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-239f00fe2f56718ae174e285877ecb66.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Exposing-and-Mitigating-Calibration-Biases-and-Demographic-Unfairness-in-MLLM-Few-Shot-In-Context-Learning-for-Medical-Image-Classification"><a href="#Exposing-and-Mitigating-Calibration-Biases-and-Demographic-Unfairness-in-MLLM-Few-Shot-In-Context-Learning-for-Medical-Image-Classification" class="headerlink" title="Exposing and Mitigating Calibration Biases and Demographic Unfairness in   MLLM Few-Shot In-Context Learning for Medical Image Classification"></a>Exposing and Mitigating Calibration Biases and Demographic Unfairness in   MLLM Few-Shot In-Context Learning for Medical Image Classification</h2><p><strong>Authors:Xing Shen, Justin Szeto, Mingyang Li, Hengguan Huang, Tal Arbel</strong></p>
<p>Multimodal large language models (MLLMs) have enormous potential to perform few-shot in-context learning in the context of medical image analysis. However, safe deployment of these models into real-world clinical practice requires an in-depth analysis of the accuracies of their predictions, and their associated calibration errors, particularly across different demographic subgroups. In this work, we present the first investigation into the calibration biases and demographic unfairness of MLLMs’ predictions and confidence scores in few-shot in-context learning for medical image classification. We introduce CALIN, an inference-time calibration method designed to mitigate the associated biases. Specifically, CALIN estimates the amount of calibration needed, represented by calibration matrices, using a bi-level procedure: progressing from the population level to the subgroup level prior to inference. It then applies this estimation to calibrate the predicted confidence scores during inference. Experimental results on three medical imaging datasets: PAPILA for fundus image classification, HAM10000 for skin cancer classification, and MIMIC-CXR for chest X-ray classification demonstrate CALIN’s effectiveness at ensuring fair confidence calibration in its prediction, while improving its overall prediction accuracies and exhibiting minimum fairness-utility trade-off. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在医学图像分析领域具有巨大的潜力进行少样本上下文学习。然而，将这些模型安全部署到现实世界中的临床实践需要深入分析其预测的准确性以及相关的校准误差，特别是在不同的种族人口亚组之间。在这项工作中，我们首次调查了MLLMs预测结果的校准偏差以及人口统计学不公平现象，以及其在医学图像分类的少量上下文学习中的置信度评分。我们引入了CALIN，这是一种用于减少相关偏差的推理时间校准方法。具体来说，CALIN使用一种双层过程来估计所需的校准量，该过程由校准矩阵表示：从总体层面推进到亚组层面进行推理。然后，它将这些估计应用于推理过程中的预测置信度评分校准。在三个医学成像数据集上的实验结果表明：用于眼底图像分类的PAPILA数据集、用于皮肤癌分类的HAM10000数据集以及用于胸部X射线分类的MIMIC-CXR数据集，CALIN在保证预测置信度公平校准方面效果显著，同时提高了总体预测精度并展现了最小的公平效用权衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23298v2">PDF</a> Preprint version. The peer-reviewed version of this paper has been   accepted to MICCAI 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>多模态大型语言模型在医学图像分析领域具有巨大的潜力，但其预测准确性、校准误差以及不同人群之间的差异仍需深入分析。本研究首次探讨了该模型在少样本上下文学习中的校准偏见和人群不公平性。为缓解相关问题，研究提出了CALIN校准方法，该方法在推理阶段设计，通过两级程序从总体到分组层面估计所需的校准量，并应用于推理过程中的预测置信度得分校准。在三个医学图像数据集上的实验结果表明，CALIN方法能有效确保公平置信度校准，提高预测准确性，且公平性效用权衡表现优秀。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型在医学图像分析中具有巨大潜力，但预测准确性和校准误差需深入分析。</li>
<li>本研究首次探讨了模型在少样本上下文学习中的校准偏见和人群不公平性问题。</li>
<li>为缓解这些问题，提出了CALIN校准方法，该方法在推理阶段设计用于校准预测置信度得分。</li>
<li>CALIN通过两级程序从总体到分组层面估计所需的校准量。</li>
<li>实验结果表明，CALIN方法能有效确保公平置信度校准，提高预测准确性。</li>
<li>CALIN方法在实现高预测准确性的同时，展现出良好的公平性效用权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23298">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e9e857695606482ba339ca9e05355684.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be07ec8609fee399a8ec0b106a03b8d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b8df457de984bf3851fc28313584916.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Inpainting-is-All-You-Need-A-Diffusion-based-Augmentation-Method-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Inpainting-is-All-You-Need-A-Diffusion-based-Augmentation-Method-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Inpainting is All You Need: A Diffusion-based Augmentation Method for   Semi-supervised Medical Image Segmentation"></a>Inpainting is All You Need: A Diffusion-based Augmentation Method for   Semi-supervised Medical Image Segmentation</h2><p><strong>Authors:Xinrong Hu, Yiyu Shi</strong></p>
<p>Collecting pixel-level labels for medical datasets can be a laborious and expensive process, and enhancing segmentation performance with a scarcity of labeled data is a crucial challenge. This work introduces AugPaint, a data augmentation framework that utilizes inpainting to generate image-label pairs from limited labeled data. AugPaint leverages latent diffusion models, known for their ability to generate high-quality in-domain images with low overhead, and adapts the sampling process for the inpainting task without need for retraining. Specifically, given a pair of image and label mask, we crop the area labeled with the foreground and condition on it during reversed denoising process for every noise level. Masked background area would gradually be filled in, and all generated images are paired with the label mask. This approach ensures the accuracy of match between synthetic images and label masks, setting it apart from existing dataset generation methods. The generated images serve as valuable supervision for training downstream segmentation models, effectively addressing the challenge of limited annotations. We conducted extensive evaluations of our data augmentation method on four public medical image segmentation datasets, including CT, MRI, and skin imaging. Results across all datasets demonstrate that AugPaint outperforms state-of-the-art label-efficient methodologies, significantly improving segmentation performance. </p>
<blockquote>
<p>收集医学数据集的像素级标签可能是一个既繁琐又昂贵的过程，而在标签数据稀缺的情况下提高分割性能是一个关键挑战。本研究引入了AugPaint数据增强框架，该框架利用图像修复技术从有限的标记数据中生成图像-标签对。AugPaint利用潜在扩散模型，这种模型以生成高质量的内部域图像和低开销而闻名，并适应采样过程进行图像修复任务而无需重新训练。具体来说，给定图像和标签掩膜对，我们裁剪前景标记区域并在每个噪声水平的反向去噪过程中对其进行条件处理。被遮罩的背景区域会逐渐被填充，所有生成的图像都与标签掩膜配对。这种方法确保了合成图像与标签掩膜之间的匹配准确性，与现有的数据集生成方法相比具有明显优势。生成的图像作为训练下游分割模型的宝贵监督数据，有效解决了标注有限的问题。我们在四个公共医学图像分割数据集上对我们的数据增强方法进行了广泛评估，包括CT、MRI和皮肤成像。在所有数据集上的结果均表明，AugPaint优于最新的标签效率方法，显著提高了分割性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23038v1">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像数据集标注劳动强度大且成本高，缺乏标注数据提高分割性能是一大挑战。本研究提出AugPaint数据增强框架，利用inpainting技术从有限标注数据中生成图像-标签对。AugPaint采用潜在扩散模型，以低开销生成高质量域内图像，并适应inpainting任务的采样过程而无需重新训练。该方法确保合成图像与标签掩膜之间的准确匹配，与现有数据集生成方法区分开来。生成的图像作为下游分割模型训练的宝贵监督资料，有效解决标注数据有限的问题。实验在四个公共医学图像分割数据集上评估该方法，包括CT、MRI和皮肤成像，结果显示AugPaint优于当前最先进的高效标签方法，显著提高分割性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AugPaint是一个用于医学图像的数据增强框架，利用inpainting技术从有限的标注数据中生成图像-标签对。</li>
<li>AugPaint采用潜在扩散模型生成高质量域内图像，无需重新训练即可适应inpainting任务的采样过程。</li>
<li>该方法确保合成图像与标签掩膜之间的准确匹配。</li>
<li>AugPaint通过生成图像作为下游分割模型的宝贵监督资料，有效解决标注数据有限的问题。</li>
<li>在四个公共医学图像分割数据集上的实验表明，AugPaint显著提高了分割性能。</li>
<li>AugPaint框架适用于多种医学图像类型，包括CT、MRI和皮肤成像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23038">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1dccb6205d9bd89d465f54f864ae097f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce68d6e0a78d83a9ff6e2305690ac9c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-551e617aad68dfdf9fb47984ee7ecbdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13beb7dd3feaa4d9a2bfe3260f7cba04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51ed996d0656e43ac56aed689e67752c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94f6682833aa1617a4a7adc5c43299dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3bb6408b2ec18062c536eac591e8f0d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="YM-WML-A-new-Yolo-based-segmentation-Model-with-Weighted-Multi-class-Loss-for-medical-imaging"><a href="#YM-WML-A-new-Yolo-based-segmentation-Model-with-Weighted-Multi-class-Loss-for-medical-imaging" class="headerlink" title="YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class   Loss for medical imaging"></a>YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class   Loss for medical imaging</h2><p><strong>Authors:Haniyeh Nikkhah, Jafar Tanha, Mahdi Zarrin, SeyedEhsan Roshan, Amin Kazempour</strong></p>
<p>Medical image segmentation poses significant challenges due to class imbalance and the complex structure of medical images. To address these challenges, this study proposes YM-WML, a novel model for cardiac image segmentation. The model integrates a robust backbone for effective feature extraction, a YOLOv11 neck for multi-scale feature aggregation, and an attention-based segmentation head for precise and accurate segmentation. To address class imbalance, we introduce the Weighted Multi-class Exponential (WME) loss function. On the ACDC dataset, YM-WML achieves a Dice Similarity Coefficient of 91.02, outperforming state-of-the-art methods. The model demonstrates stable training, accurate segmentation, and strong generalization, setting a new benchmark in cardiac segmentation tasks. </p>
<blockquote>
<p>医学图像分割面临着类不平衡和医学图像复杂结构带来的挑战。为了解决这些挑战，本研究提出了YM-WML这一新型心脏图像分割模型。该模型集成了稳健的骨干网进行高效特征提取、YOLOv11颈部进行多尺度特征聚合和基于注意力的分割头，以实现精确和准确的分割。为了解决类不平衡问题，我们引入了加权多类指数（WME）损失函数。在ACDC数据集上，YM-WML的Dice相似系数达到91.02%，优于现有最先进的方法。该模型展现出稳定的训练、准确的分割和强大的泛化能力，在心脏分割任务中树立了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22955v1">PDF</a> Accepted at The 7th International conference on Pattern Recognition   and Image Analysis (IPRIA 2025)</p>
<p><strong>Summary</strong></p>
<p>本论文针对医学图像分割中的类别不平衡和复杂结构问题，提出了一种新型的心脏图像分割模型YM-WML。该模型融合了稳健的骨干网络以实现有效的特征提取，借助YOLOv11颈部进行多尺度特征聚合，以及基于注意力的分割头实现精确和准确的分割。为解决类别不平衡问题，引入了加权多类指数（WME）损失函数。在ACDC数据集上，YM-WML的Dice相似系数达到了91.02%，优于现有最先进的方法，显示出稳定的训练、准确的分割和强大的泛化能力，为心脏分割任务设定了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割面临类别不平衡和复杂结构挑战。</li>
<li>提出新型心脏图像分割模型YM-WML。</li>
<li>YM-WML模型融合稳健骨干网络、YOLOv11颈部和注意力分割头。</li>
<li>引入加权多类指数（WME）损失函数以解决类别不平衡问题。</li>
<li>在ACDC数据集上，YM-WML的Dice相似系数达91.02%。</li>
<li>YM-WML性能优于现有最先进的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22955">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-da130ab1bd41ef1b5da9d19920b9b043.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3927ba750a3d341528c7c8057216523.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fc4a9ae25d4fec7163b476004f498ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcd00c2144fdfba13c242e694337e195.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-571b5c7d54703465787c7bf6ab45c026.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Missing-Modality-Aware-Graph-Neural-Network-for-Cancer-Classification"><a href="#Missing-Modality-Aware-Graph-Neural-Network-for-Cancer-Classification" class="headerlink" title="Missing-Modality-Aware Graph Neural Network for Cancer Classification"></a>Missing-Modality-Aware Graph Neural Network for Cancer Classification</h2><p><strong>Authors:Sina Tabakhi, Haiping Lu</strong></p>
<p>A key challenge in learning from multimodal biological data is missing modalities, where all data from some modalities are missing for some patients. Current fusion methods address this by excluding patients with missing modalities, imputing missing modalities, or making predictions directly with partial modalities. However, they often struggle with diverse missing-modality patterns and the exponential growth of the number of such patterns as the number of modalities increases. To address these limitations, we propose MAGNET (Missing-modality-Aware Graph neural NETwork) for direct prediction with partial modalities, which introduces a patient-modality multi-head attention mechanism to fuse lower-dimensional modality embeddings based on their importance and missingness. MAGNET’s complexity increases linearly with the number of modalities while adapting to missing-pattern variability. To generate predictions, MAGNET further constructs a patient graph with fused multimodal embeddings as node features and the connectivity determined by the modality missingness, followed by a conventional graph neural network. Experiments on three public multiomics datasets for cancer classification, with real-world instead of artificial missingness, show that MAGNET outperforms the state-of-the-art fusion methods. The data and code are available at <a target="_blank" rel="noopener" href="https://github.com/SinaTabakhi/MAGNET">https://github.com/SinaTabakhi/MAGNET</a>. </p>
<blockquote>
<p>在多模态生物数据学习中，缺失模态是一个关键挑战，其中某些患者的某些模态数据全部缺失。目前的融合方法通过排除具有缺失模态的患者、估算缺失模态或仅使用部分模态进行直接预测来解决这个问题。然而，随着模态数量的增加，它们通常面临多样的缺失模态模式以及此类模式数量呈指数增长的问题。为了解决这些局限性，我们提出了MAGNET（基于缺失模态感知的图神经网络），采用部分模态进行直接预测。MAGNET引入了患者-模态多头注意力机制，根据重要性及其缺失情况融合低维模态嵌入。MAGNET的复杂性随模态数量的增加而线性增长，同时适应缺失模式的可变性。为了生成预测，MAGNET进一步构建了一个患者图，以融合的多模态嵌入作为节点特征，连接性由模态缺失性决定，然后使用传统的图神经网络。在三个公开的多组学数据集上进行癌症分类的实验（使用现实世界中的缺失数据而非人为缺失数据）表明，MAGNET在融合方法上超越了最新技术水平。数据和代码可在<a target="_blank" rel="noopener" href="https://github.com/SinaTabakhi/MAGNET%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SinaTabakhi/MAGNET找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22901v1">PDF</a> 15 pages, 7 figures</p>
<p><strong>Summary</strong><br>     处理多模态生物数据时面临的主要挑战是缺失模态问题。针对该问题，现有融合方法常通过排除缺失模态的患者、补齐缺失模态或直接用部分模态进行预测来应对。然而，随着模态数量的增加，它们面临多样且复杂的缺失模态模式挑战。为解决此问题，我们提出MAGNET（缺失模态感知图神经网络）进行直接预测，引入了患者-模态多头注意力机制来融合基于重要性而并非缺失程度的低维模态嵌入。MAGNET的复杂度随模态数量线性增长，同时适应缺失模式的多样性。MAGNET通过构建患者图生成预测结果，图中融合的多模态嵌入作为节点特征，连接性由缺失的模态决定，然后使用常规的图神经网络处理。在三个公共多组学数据集上的癌症分类实验表明，MAGNET优于现有融合方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>处理多模态生物数据时存在缺失模态的挑战。</li>
<li>当前融合方法如排除缺失模态患者、补齐缺失模态或直接使用部分模态预测都有其局限性。</li>
<li>MAGNET方法通过引入患者-模态多头注意力机制解决了上述问题，融合了基于重要性的低维模态嵌入。</li>
<li>MAGNET能随着模态数量的增长线性增长复杂度并适应多种缺失模式。</li>
<li>MAGNET构建了一个患者图，通过结合融合的多模态嵌入和节点连接性来生成预测结果。</li>
<li>实验证明MAGNET在癌症分类任务上优于其他融合方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22901">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f12eb53793019ba7e4ee434425fba81d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c1cfdc97054b397792d97126a3a4f47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5a319125948b70264367337050da6b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-588133fed298c83d09f1f076fe786b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e4ff6bc36934ee8b70b4a2218c6418e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CA-Diff-Collaborative-Anatomy-Diffusion-for-Brain-Tissue-Segmentation"><a href="#CA-Diff-Collaborative-Anatomy-Diffusion-for-Brain-Tissue-Segmentation" class="headerlink" title="CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation"></a>CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation</h2><p><strong>Authors:Qilong Xing, Zikai Song, Yuteng Ye, Yuke Chen, Youjia Zhang, Na Feng, Junqing Yu, Wei Yang</strong></p>
<p>Segmentation of brain structures from MRI is crucial for evaluating brain morphology, yet existing CNN and transformer-based methods struggle to delineate complex structures accurately. While current diffusion models have shown promise in image segmentation, they are inadequate when applied directly to brain MRI due to neglecting anatomical information. To address this, we propose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating spatial anatomical features to enhance segmentation accuracy of the diffusion model. Specifically, we introduce distance field as an auxiliary anatomical condition to provide global spatial context, alongside a collaborative diffusion process to model its joint distribution with anatomical structures, enabling effective utilization of anatomical features for segmentation. Furthermore, we introduce a consistency loss to refine relationships between the distance field and anatomical structures and design a time adapted channel attention module to enhance the U-Net feature fusion procedure. Extensive experiments show that CA-Diff outperforms state-of-the-art (SOTA) methods. </p>
<blockquote>
<p>从MRI中分割脑结构对于评估脑形态至关重要，但现有的基于CNN和transformer的方法在准确描绘复杂结构上存在困难。虽然当前的扩散模型在图像分割方面显示出了一定的潜力，但直接应用于脑MRI时却表现不足，因为它们忽略了解剖信息。为了解决这一问题，我们提出了协作解剖扩散（CA-Diff）框架，该框架结合了空间解剖特征，以提高扩散模型的分割精度。具体来说，我们引入距离场作为辅助解剖条件，以提供全局空间上下文，以及一个协作扩散过程，以模拟其与解剖结构的联合分布，从而实现解剖特征的有效利用进行分割。此外，我们还引入了一致性损失来优化距离场与解剖结构之间的关系，并设计了一个时间适应的通道注意力模块来增强U-Net特征融合过程。大量实验表明，CA-Diff优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22882v1">PDF</a> ICME 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种结合空间解剖特征的协作式解剖学扩散（CA-Diff）框架，用于提高扩散模型在脑结构MRI图像分割中的准确性。通过引入距离场作为辅助解剖条件，并结合协作扩散过程，有效运用解剖特征进行分割。此外，还引入了一致性损失来优化距离场与解剖结构之间的关系，并设计了时间适应性通道注意力模块来增强U-Net特征融合过程。实验表明，CA-Diff框架的性能超越了现有先进技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有CNN和transformer-based方法在脑结构MRI图像分割上存在准确性问题。</li>
<li>扩散模型在图像分割中展现出潜力，但直接应用于脑MRI时因忽视解剖信息而不足。</li>
<li>提出的CA-Diff框架集成了空间解剖特征，以提高分割准确性。</li>
<li>引入距离场作为辅助解剖条件，提供全局空间上下文信息。</li>
<li>协作扩散过程能够建模距离场与解剖结构的联合分布。</li>
<li>一致性损失用于优化距离场与解剖结构之间的关系。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22882">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-86c9470ff0e72f379d5b94e254d6ee8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3d39c50d9f375b6c4c5951d1c9b1dda.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4510a8b9b66fbaeeb893e9f57f08cd5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8baa43f860d2a9400384cab1603ba3e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e48ca9fa83db613ac561bf17bcf2358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0e63302a9ef8c26dc1bf52f8271ce19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6bc4d56927883f3d16a70f6157d2fdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1499a6684e4c5c4a8a8ddbc10e047dde.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Decoupled-Seg-Tokens-Make-Stronger-Reasoning-Video-Segmenter-and-Grounder"><a href="#Decoupled-Seg-Tokens-Make-Stronger-Reasoning-Video-Segmenter-and-Grounder" class="headerlink" title="Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and   Grounder"></a>Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and   Grounder</h2><p><strong>Authors:Dang Jisheng, Wu Xudong, Wang Bimei, Lv Ning, Chen Jiayu, Jingwen Zhao, Yichu liu, Jizhao Liu, Juncheng Li, Teng Wang</strong></p>
<p>Existing video segmenter and grounder approaches, exemplified by Sa2VA, directly fuse features within segmentation models. This often results in an undesirable entanglement of dynamic visual information and static semantics, thereby degrading segmentation accuracy. To systematically mitigate this issue, we propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text pre-training and a linear decoupling module to address the information processing limitations inherent in SAM-2. Specifically, first, we devise a pre-training paradigm that converts textual ground-truth labels into point-level prompts while generating corresponding text masks. These masks are refined through a hybrid loss function to strengthen the model’s semantic grounding capabilities. Next, we employ linear projection to disentangle hidden states that generated by a large language model into distinct textual and visual feature subspaces. Finally, a dynamic mask fusion strategy synergistically combines these decoupled features through triple supervision from predicted text&#x2F;visual masks and ground-truth annotations. Extensive experiments demonstrate state-of-the-art performance across diverse tasks, including image segmentation, image question answering, video segmentation, and video question answering. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/longmalongma/DeSa2VA">https://github.com/longmalongma/DeSa2VA</a>. </p>
<blockquote>
<p>现有的视频分割器和打标器方法，以Sa2VA为例，直接在分割模型中融合特征。这通常会导致动态视觉信息和静态语义的纠缠，从而降低分割精度。为了系统地解决这一问题，我们提出了DeSa2VA，这是一种增强解耦的提示方案，它结合了文本预训练和非线性解耦模块，以解决SAM-2固有的信息处理限制。具体来说，首先，我们设计了一种预训练模式，将文本真实标签转换为点级提示，同时生成相应的文本掩码。这些掩码通过混合损失函数进行精炼，以增强模型的语义定位能力。其次，我们采用线性投影技术，将大型语言模型生成的隐藏状态分解成不同的文本和视觉特征子空间。最后，通过预测文本&#x2F;视觉掩码和真实注释的三重监督，采用动态掩码融合策略协同结合这些解耦特征。大量实验表明，我们在包括图像分割、图像问答、视频分割和视频问答等多项任务中达到了最先进的状态。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/longmalongma/DeSa2VA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/longmalongma/DeSa2VA获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22880v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了现有视频分割器和地面标记器方法存在的问题，如Sa2VA等方法直接融合分割模型中的特征，导致动态视觉信息和静态语义的纠缠，降低分割精度。为解决这一问题，本文提出了DeSa2VA方案，该方案通过增强解耦提示和整合文本预训练与线性解耦模块来解决SAM-2的信息处理限制。具体包括转换为点级提示的文本预训练、线性投影实现的文本和视觉特征子空间的区分以及动态掩码融合策略。实验证明，该方案在图像分割、图像问答、视频分割和视频问答等多项任务上达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有视频分割器和地面标记器方法存在信息纠缠问题，影响分割精度。</li>
<li>DeSa2VA通过增强解耦提示解决这一问题，旨在改善SAM-2的信息处理限制。</li>
<li>预训练将文本地面真实标签转换为点级提示，并使用混合损失函数优化文本掩码以增强模型语义定位能力。</li>
<li>通过线性投影技术，DeSa2VA能将大型语言模型生成的隐藏状态分解成独立的文本和视觉特征子空间。</li>
<li>动态掩码融合策略结合了这些解耦特征，通过来自预测文本&#x2F;视觉掩码和地面真实注解的三重监督进行协同作用。</li>
<li>实验结果显示，DeSa2VA在多种任务上达到先进水平，包括图像分割、图像问答、视频分割和视频问答等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22880">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e502d8c319ca47ec509cffb4d829e37b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f308a70f0d02ef8f5181d8d6da02a8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2506531d0bc4eb49cf54d4a50be35fb5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Degradation-Modeled-Multipath-Diffusion-for-Tunable-Metalens-Photography"><a href="#Degradation-Modeled-Multipath-Diffusion-for-Tunable-Metalens-Photography" class="headerlink" title="Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography"></a>Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography</h2><p><strong>Authors:Jianing Zhang, Jiayi Zhu, Feiyu Ji, Xiaokang Yang, Xiaoyun Yuan</strong></p>
<p>Metalenses offer significant potential for ultra-compact computational imaging but face challenges from complex optical degradation and computational restoration difficulties. Existing methods typically rely on precise optical calibration or massive paired datasets, which are non-trivial for real-world imaging systems. Furthermore, a lack of control over the inference process often results in undesirable hallucinated artifacts. We introduce Degradation-Modeled Multipath Diffusion for tunable metalens photography, leveraging powerful natural image priors from pretrained models instead of large datasets. Our framework uses positive, neutral, and negative-prompt paths to balance high-frequency detail generation, structural fidelity, and suppression of metalens-specific degradation, alongside \textit{pseudo} data augmentation. A tunable decoder enables controlled trade-offs between fidelity and perceptual quality. Additionally, a spatially varying degradation-aware attention (SVDA) module adaptively models complex optical and sensor-induced degradation. Finally, we design and build a millimeter-scale MetaCamera for real-world validation. Extensive results show that our approach outperforms state-of-the-art methods, achieving high-fidelity and sharp image reconstruction. More materials: <a target="_blank" rel="noopener" href="https://dmdiff.github.io/">https://dmdiff.github.io/</a>. </p>
<blockquote>
<p>金属透镜在超紧凑计算成像方面存在巨大潜力，但面临着复杂光学退化和计算修复困难等挑战。现有方法通常依赖于精确的光学校准或大量的配对数据集，对于现实世界成像系统而言，这些方法并不简单。此外，缺乏对推理过程的控制往往会导致出现不理想的人工合成伪影。我们引入了基于退化模型的多路扩散技术，用于可调金属透镜摄影，利用预训练模型的强大自然图像先验知识，而不是大型数据集。我们的框架使用正向、中性和负向提示路径来平衡高频细节生成、结构保真度和抑制金属透镜特有的退化现象，同时辅以伪数据增强。可调解码器能够在保真度和感知质量之间进行可控的权衡。此外，空间变化的退化感知注意力（SVDA）模块能够自适应地模拟复杂的光学退化和传感器引起的退化。最后，我们设计并制作了一款用于实际验证的毫米级MetaCamera。大量结果表明，我们的方法优于最先进的方法，实现了高保真和清晰的图像重建。更多材料请参见：<a target="_blank" rel="noopener" href="https://dmdiff.github.io/">https://dmdiff.github.io/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22753v1">PDF</a> </p>
<p><strong>Summary</strong><br>     金属透镜在超紧凑计算成像方面具巨大潜力，但面临复杂光学降解和计算修复难题。现有方法通常依赖精确光学校准或大量配对数据集，不适用于现实世界成像系统。研究引入基于降解建模的多路径扩散方法，用于可调金属透镜摄影，利用预训练模型的天然图像先验而非大型数据集。该方法采用正、中性及负提示路径来平衡高频细节生成、结构保真和抑制金属透镜特定降解，配合伪数据增强。可调解码器可在保真度和感知质量之间实现可控权衡。此外，空间变化降解感知注意力模块可自适应地模拟复杂光学和传感器引起的降解。最后，研究设计并制作了一款用于真实世界验证的毫米级MetaCamera。大量结果表明，该方法优于现有技术，可实现高保真和清晰的图像重建。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>金属透镜在计算成像中具有巨大潜力，但面临光学降解和计算修复的挑战。</li>
<li>现有方法依赖于精确光学校准或大规模配对数据集，不适用于现实世界的成像系统。</li>
<li>研究提出了一种基于降解建模的多路径扩散方法，用于可调金属透镜摄影。</li>
<li>该方法利用预训练模型的天然图像先验，通过正、中性及负提示路径实现细节生成、结构保真和降解抑制的平衡。</li>
<li>研究引入了伪数据增强、可调解码器和空间变化降解感知注意力模块等技术，以提高成像质量。</li>
<li>研究设计并制作了一款用于真实世界验证的毫米级MetaCamera。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22753">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3396a6cffc50d63d7b7511b7f0a72d2a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17dfab62daedd9d25405e3e16e535b9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f70199b31516f60f60213d4c97303c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04e0dfc85fe8d048394186ac688e9a6c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="UniFuse-A-Unified-All-in-One-Framework-for-Multi-Modal-Medical-Image-Fusion-Under-Diverse-Degradations-and-Misalignments"><a href="#UniFuse-A-Unified-All-in-One-Framework-for-Multi-Modal-Medical-Image-Fusion-Under-Diverse-Degradations-and-Misalignments" class="headerlink" title="UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image   Fusion Under Diverse Degradations and Misalignments"></a>UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image   Fusion Under Diverse Degradations and Misalignments</h2><p><strong>Authors:Dayong Su, Yafei Zhang, Huafeng Li, Jinxing Li, Yu Liu</strong></p>
<p>Current multimodal medical image fusion typically assumes that source images are of high quality and perfectly aligned at the pixel level. Its effectiveness heavily relies on these conditions and often deteriorates when handling misaligned or degraded medical images. To address this, we propose UniFuse, a general fusion framework. By embedding a degradation-aware prompt learning module, UniFuse seamlessly integrates multi-directional information from input images and correlates cross-modal alignment with restoration, enabling joint optimization of both tasks within a unified framework. Additionally, we design an Omni Unified Feature Representation scheme, which leverages Spatial Mamba to encode multi-directional features and mitigate modality differences in feature alignment. To enable simultaneous restoration and fusion within an All-in-One configuration, we propose a Universal Feature Restoration &amp; Fusion module, incorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA principles. By leveraging ALSN’s adaptive feature representation along with degradation-type guidance, we enable joint restoration and fusion within a single-stage framework. Compared to staged approaches, UniFuse unifies alignment, restoration, and fusion within a single framework. Experimental results across multiple datasets demonstrate the method’s effectiveness and significant advantages over existing approaches. </p>
<blockquote>
<p>当前的多模态医学图像融合通常假设源图像质量高且在像素级别完美对齐。其有效性严重依赖于这些条件，而在处理错位或退化医学图像时，其效果往往会下降。针对这一问题，我们提出了UniFuse通用融合框架。通过嵌入感知退化提示学习模块，UniFuse无缝集成了输入图像的多方向信息，并将跨模态对齐与恢复相关联，在统一框架内实现两个任务的联合优化。此外，我们设计了一种Omni统一特征表示方案，利用空间曼巴编码多方向特征，减轻特征对齐中的模态差异。为了实现All-in-One配置下的同时恢复和融合，我们提出了通用特征恢复与融合模块，结合了基于LoRA原理的自适应LoRA协同网络（ALSN）。通过利用ALSN的自适应特征表示和退化类型指导，我们在单阶段框架内实现了联合恢复和融合。与分阶段方法相比，UniFuse将对齐、恢复和融合统一到一个框架内。在多个数据集上的实验结果表明了该方法的有效性和对现有方法的显著优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22736v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为UniFuse的通用融合框架，用于解决当前多模态医学图像融合中遇到的问题。该框架能够嵌入退化感知提示学习模块，无缝集成输入图像的多方向信息，并通过跨模态对齐与恢复的相关性，在统一框架内联合优化两个任务。此外，还设计了Omni统一特征表示方案，利用空间Mamba编码多方向特征，减轻特征对齐中的模态差异。通过自适应LoRA协同网络（ALSN）在单个阶段实现恢复和融合的联合操作，实验结果表明，该方法在多个数据集上均有效，且相较于分期处理方法具有显著优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前多模态医学图像融合假设源图像高质量且像素级对齐，但处理错位或退化图像时效果下降。</li>
<li>UniFuse框架通过嵌入退化感知提示学习模块，解决了这一问题。</li>
<li>UniFuse框架能够无缝集成输入图像的多方向信息，并关联跨模态对齐与恢复。</li>
<li>Omni统一特征表示方案利用空间Mamba编码多方向特征，减轻模态差异。</li>
<li>通过自适应LoRA协同网络（ALSN），实现在单个阶段的恢复和融合联合操作。</li>
<li>实验结果表明UniFuse框架在多个数据集上均有效，且较传统方法有明显优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22736">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4fe0fc2c4b0273ed0868a039f8e73262.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd3d7971ab94ff320f3e8a1cd97aeae9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21e698c32a63cafc90762f4f138d0bb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddbf6e99a1aca26b126d343ff477f3e3.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="FedCLAM-Client-Adaptive-Momentum-with-Foreground-Intensity-Matching-for-Federated-Medical-Image-Segmentation"><a href="#FedCLAM-Client-Adaptive-Momentum-with-Foreground-Intensity-Matching-for-Federated-Medical-Image-Segmentation" class="headerlink" title="FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for   Federated Medical Image Segmentation"></a>FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for   Federated Medical Image Segmentation</h2><p><strong>Authors:Vasilis Siomos, Jonathan Passerat-Palmbach, Giacomo Tarroni</strong></p>
<p>Federated learning is a decentralized training approach that keeps data under stakeholder control while achieving superior performance over isolated training. While inter-institutional feature discrepancies pose a challenge in all federated settings, medical imaging is particularly affected due to diverse imaging devices and population variances, which can diminish the global model’s effectiveness. Existing aggregation methods generally fail to adapt across varied circumstances. To address this, we propose FedCLAM, which integrates \textit{client-adaptive momentum} terms derived from each client’s loss reduction during local training, as well as a \textit{personalized dampening factor} to curb overfitting. We further introduce a novel \textit{intensity alignment} loss that matches predicted and ground-truth foreground distributions to handle heterogeneous image intensity profiles across institutions and devices. Extensive evaluations on two datasets show that FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks, underscoring its efficacy. The code is available at <a target="_blank" rel="noopener" href="https://github.com/siomvas/FedCLAM">https://github.com/siomvas/FedCLAM</a>. </p>
<blockquote>
<p>联邦学习是一种去中心化的训练方法，能够在保持数据受利益相关者控制的同时，实现优于独立训练的性能。虽然跨机构特征差异在所有联邦环境中都构成挑战，但由于各种成像设备和人群差异，医学影像受到的影响尤为突出，这可能会降低全局模型的有效性。现有的聚合方法一般无法适应各种情况的变化。为了解决这一问题，我们提出了FedCLAM，它结合了每个客户端本地训练过程中损失减少所衍生出的“客户端自适应动量”项，以及一个抑制过拟合的“个性化阻尼因子”。我们还引入了一种新颖的“强度对齐”损失，用于匹配预测前景和真实前景分布，以处理不同机构和设备之间异质图像强度分布。在两个数据集上的广泛评估表明，FedCLAM在医学分割任务上超越了八种前沿方法，凸显了其有效性。代码可通过<a target="_blank" rel="noopener" href="https://github.com/siomvas/FedCLAM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/siomvas/FedCLAM获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22580v1">PDF</a> 10 pages, 2 figures, Accepted at MICCAI 2025</p>
<p><strong>Summary</strong><br>医学图像联邦学习中的机构间特征差异是一大挑战，特别是在多样化的成像设备和人口差异的情况下。为解决此问题，FedCLAM通过引入客户端适应性动量和个性化抑制因子来适应不同客户端的损失减少情况，同时采用强度对齐损失来匹配预测和真实前景分布，处理不同机构和设备的异构图象强度分布。FedCLAM在医学分割任务中超越八种前沿方法，证明其有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>联邦学习是一种去中心化的训练方法，实现了数据在利益相关者控制下的高性能训练。</li>
<li>在医学图像领域，不同机构间的特征差异是一大挑战，特别是多样化的成像设备和人口差异会影响全球模型的有效性。</li>
<li>FedCLAM通过引入客户端适应性动量和个性化抑制因子来解决这一问题。</li>
<li>FedCLAM采用强度对齐损失来处理不同机构和设备的异构图象强度分布问题。</li>
<li>FedCLAM在医学分割任务中进行了广泛的评估，证明其有效性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22580">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dcd5e41a33dbad9cd2241f2978217e9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e28d940778035433e118bb5555a2c311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9a7d10b977abae6c626693cfe9ae638.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-56c1499ca1498945a60e4450eaf36f45.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-07-03  JAM-Flow Joint Audio-Motion Synthesis with Flow Matching
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4cf9c2bc6af6f33a9d63db6f06aefbd6.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-07-03  Imagine for Me Creative Conceptual Blending of Real Images and Text via   Blended Attention
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
