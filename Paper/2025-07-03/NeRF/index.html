<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  AttentionGS Towards Initialization-Free 3D Gaussian Splatting via   Structural Attention">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-8316847a61ca9ce8a873eb8dcbaa3c8a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    39 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-03-æ›´æ–°"><a href="#2025-07-03-æ›´æ–°" class="headerlink" title="2025-07-03 æ›´æ–°"></a>2025-07-03 æ›´æ–°</h1><h2 id="AttentionGS-Towards-Initialization-Free-3D-Gaussian-Splatting-via-Structural-Attention"><a href="#AttentionGS-Towards-Initialization-Free-3D-Gaussian-Splatting-via-Structural-Attention" class="headerlink" title="AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via   Structural Attention"></a>AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via   Structural Attention</h2><p><strong>Authors:Ziao Liu, Zhenjia Li, Yifeng Shi, Xiangang Li</strong></p>
<p>3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance Fields (NeRF), excelling in complex scene reconstruction and efficient rendering. However, it relies on high-quality point clouds from Structure-from-Motion (SfM), limiting its applicability. SfM also fails in texture-deficient or constrained-view scenarios, causing severe degradation in 3DGS reconstruction. To address this limitation, we propose AttentionGS, a novel framework that eliminates the dependency on high-quality initial point clouds by leveraging structural attention for direct 3D reconstruction from randomly initialization. In the early training stage, we introduce geometric attention to rapidly recover the global scene structure. As training progresses, we incorporate texture attention to refine fine-grained details and enhance rendering quality. Furthermore, we employ opacity-weighted gradients to guide Gaussian densification, leading to improved surface reconstruction. Extensive experiments on multiple benchmark datasets demonstrate that AttentionGS significantly outperforms state-of-the-art methods, particularly in scenarios where point cloud initialization is unreliable. Our approach paves the way for more robust and flexible 3D Gaussian Splatting in real-world applications. </p>
<blockquote>
<p>3Dé«˜æ–¯æ¶‚æŠ¹ï¼ˆ3DGSï¼‰æ˜¯ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æœ‰åŠ›æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨å¤æ‚åœºæ™¯é‡å»ºå’Œé«˜æ•ˆæ¸²æŸ“æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä¾èµ–äºè¿åŠ¨ç»“æ„ï¼ˆSfMï¼‰çš„é«˜è´¨é‡ç‚¹äº‘ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚SfMåœ¨çº¹ç†ç¼ºå¤±æˆ–è§†è§’å—é™çš„åœºæ™¯ä¸­ä¹Ÿä¼šå¤±æ•ˆï¼Œå¯¼è‡´3DGSé‡å»ºä¸¥é‡é€€åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†AttentionGSè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨ç»“æ„æ³¨æ„åŠ›ï¼Œå®ç°ä»éšæœºåˆå§‹åŒ–è¿›è¡Œç›´æ¥3Dé‡å»ºï¼Œæ¶ˆé™¤äº†å¯¹é«˜è´¨é‡åˆå§‹ç‚¹äº‘çš„ä¾èµ–ã€‚åœ¨è®­ç»ƒåˆæœŸï¼Œæˆ‘ä»¬å¼•å…¥å‡ ä½•æ³¨æ„åŠ›ï¼Œå¿«é€Ÿæ¢å¤å…¨å±€åœºæ™¯ç»“æ„ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæˆ‘ä»¬ç»“åˆçº¹ç†æ³¨æ„åŠ›ï¼Œç»†åŒ–ç»†èŠ‚ï¼Œæé«˜æ¸²æŸ“è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨ä¸é€æ˜åº¦åŠ æƒæ¢¯åº¦å¼•å¯¼é«˜æ–¯ç¨ å¯†åŒ–ï¼Œæé«˜äº†è¡¨é¢é‡å»ºæ•ˆæœã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAttentionGSæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‚¹äº‘åˆå§‹åŒ–ä¸å¯é çš„åœºæ™¯ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæ›´ç¨³å¥å’Œçµæ´»çš„3Dé«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯åœ¨ç°å®ä¸–ç•Œåº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23611v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ”¹è¿›å‹3Dé«˜æ–¯æ’å€¼æ–¹æ³•ï¼ˆAttentionGSï¼‰ï¼Œç”¨äºä»éšæœºåˆå§‹åŒ–è¿›è¡Œç›´æ¥ä¸‰ç»´é‡å»ºï¼Œè§£å†³ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰å¯¹é«˜è´¨é‡åˆå§‹ç‚¹äº‘çš„ä¾èµ–é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å‡ ä½•æ³¨æ„åŠ›å’Œçº¹ç†æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨è®­ç»ƒåˆæœŸå¿«é€Ÿæ¢å¤å…¨å±€åœºæ™¯ç»“æ„ï¼Œå¹¶éšç€è®­ç»ƒè¿›å±•ä¼˜åŒ–ç»†èŠ‚ï¼Œæé«˜æ¸²æŸ“è´¨é‡ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨åŸºäºä¸é€æ˜åº¦åŠ æƒçš„æ¢¯åº¦å¼•å¯¼é«˜æ–¯å¯†é›†åŒ–æŠ€æœ¯ï¼Œæ”¹å–„è¡¨é¢é‡å»ºæ•ˆæœã€‚å®éªŒè¯æ˜ï¼ŒAttentionGSåœ¨ç‚¹äº‘åˆå§‹åŒ–ä¸å¯é çš„åœºæ™¯ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºå®é™…åº”ç”¨ä¸­æ›´ç¨³å¥çµæ´»çš„3Dé«˜æ–¯æ’å€¼æ–¹æ³•å¼€è¾Ÿé“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä»‹ç»äº†åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ”¹è¿›å‹ä¸‰ç»´é‡å»ºæ–¹æ³•AttentionGSã€‚</li>
<li>AttentionGSè§£å†³äº†å¯¹é«˜è´¨é‡åˆå§‹ç‚¹äº‘çš„ä¾èµ–é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡éšæœºåˆå§‹åŒ–è¿›è¡Œç›´æ¥ä¸‰ç»´é‡å»ºã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å‡ ä½•æ³¨æ„åŠ›å’Œçº¹ç†æ³¨æ„åŠ›æœºåˆ¶æ¢å¤åœºæ™¯ç»“æ„å¹¶ä¼˜åŒ–ç»†èŠ‚ã€‚</li>
<li>é‡‡ç”¨åŸºäºä¸é€æ˜åº¦åŠ æƒçš„æ¢¯åº¦å¼•å¯¼é«˜æ–¯å¯†é›†åŒ–æŠ€æœ¯ï¼Œæé«˜è¡¨é¢é‡å»ºè´¨é‡ã€‚</li>
<li>AttentionGSåœ¨ç‚¹äº‘åˆå§‹åŒ–ä¸å¯é çš„åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad3aa318d3a239901bdf263109cc9f16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47ffdc35cbe002cf5d2c0ebcc5dd0acd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb373851513c8a57bdbcd4a7fac1d8cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef675a5e8826902351fdf54f1f16b97e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69f58f2f818926d946b958e83ab8cac6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dynamic-View-Synthesis-from-Small-Camera-Motion-Videos"><a href="#Dynamic-View-Synthesis-from-Small-Camera-Motion-Videos" class="headerlink" title="Dynamic View Synthesis from Small Camera Motion Videos"></a>Dynamic View Synthesis from Small Camera Motion Videos</h2><p><strong>Authors:Huiqiang Sun, Xingyi Li, Juewen Peng, Liao Shen, Zhiguo Cao, Ke Xian, Guosheng Lin</strong></p>
<p>Novel view synthesis for dynamic $3$D scenes poses a significant challenge. Many notable efforts use NeRF-based approaches to address this task and yield impressive results. However, these methods rely heavily on sufficient motion parallax in the input images or videos. When the camera motion range becomes limited or even stationary (i.e., small camera motion), existing methods encounter two primary challenges: incorrect representation of scene geometry and inaccurate estimation of camera parameters. These challenges make prior methods struggle to produce satisfactory results or even become invalid. To address the first challenge, we propose a novel Distribution-based Depth Regularization (DDR) that ensures the rendering weight distribution to align with the true distribution. Specifically, unlike previous methods that use depth loss to calculate the error of the expectation, we calculate the expectation of the error by using Gumbel-softmax to differentiably sample points from discrete rendering weight distribution. Additionally, we introduce constraints that enforce the volume density of spatial points before the object boundary along the ray to be near zero, ensuring that our model learns the correct geometry of the scene. To demystify the DDR, we further propose a visualization tool that enables observing the scene geometry representation at the rendering weight level. For the second challenge, we incorporate camera parameter learning during training to enhance the robustness of our model to camera parameters. We conduct extensive experiments to demonstrate the effectiveness of our approach in representing scenes with small camera motion input, and our results compare favorably to state-of-the-art methods. </p>
<blockquote>
<p>åŠ¨æ€ä¸‰ç»´åœºæ™¯çš„æ–°å‹è§†å›¾åˆæˆæ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚è®¸å¤šå¼•äººæ³¨ç›®çš„åŠªåŠ›ä½¿ç”¨åŸºäºNeRFçš„æ–¹æ³•æ¥è§£å†³è¿™é¡¹ä»»åŠ¡ï¼Œå¹¶äº§ç”Ÿäº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºè¾“å…¥å›¾åƒæˆ–è§†é¢‘ä¸­çš„å……è¶³è¿åŠ¨è§†å·®ã€‚å½“ç›¸æœºè¿åŠ¨èŒƒå›´å˜å¾—æœ‰é™ç”šè‡³é™æ­¢ï¼ˆå³ç›¸æœºè¿åŠ¨å¾ˆå°ï¼‰æ—¶ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šåœºæ™¯å‡ ä½•è¡¨ç¤ºä¸æ­£ç¡®å’Œç›¸æœºå‚æ•°ä¼°è®¡ä¸å‡†ç¡®ã€‚è¿™äº›æŒ‘æˆ˜ä½¿å¾—ç°æœ‰æ–¹æ³•éš¾ä»¥äº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœï¼Œç”šè‡³å˜å¾—æ— æ•ˆã€‚ä¸ºäº†è§£å†³ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒçš„æ·±åº¦æ­£åˆ™åŒ–ï¼ˆDDRï¼‰æ–¹æ³•ï¼Œç¡®ä¿æ¸²æŸ“æƒé‡åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒå¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ä»¥å‰ä½¿ç”¨æ·±åº¦æŸå¤±æ¥è®¡ç®—æœŸæœ›è¯¯å·®çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬ä½¿ç”¨Gumbel-softmaxå¯å¾®é‡‡æ ·ç‚¹æ¥è®¡ç®—ç¦»æ•£æ¸²æŸ“æƒé‡åˆ†å¸ƒçš„è¯¯å·®æœŸæœ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†çº¦æŸæ¡ä»¶ï¼Œå¼ºåˆ¶æ²¿å…‰çº¿æ–¹å‘åœ¨ç‰©ä½“è¾¹ç•Œä¹‹å‰çš„ç©ºé—´ç‚¹çš„ä½“ç§¯å¯†åº¦æ¥è¿‘é›¶ï¼Œç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹å­¦ä¹ åœºæ™¯çš„æ­£ç¡®å‡ ä½•å½¢çŠ¶ã€‚ä¸ºäº†æ­ç¤ºDDRçš„ç¥ç§˜æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªå¯è§†åŒ–å·¥å…·ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°æ¸²æŸ“æƒé‡çº§åˆ«çš„åœºæ™¯å‡ ä½•è¡¨ç¤ºã€‚å¯¹äºç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ å…¥äº†ç›¸æœºå‚æ•°å­¦ä¹ ï¼Œä»¥æé«˜æ¨¡å‹å¯¹ç›¸æœºå‚æ•°çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•åœ¨è¡¨ç¤ºå…·æœ‰å°ç›¸æœºè¿åŠ¨è¾“å…¥çš„åœºæ™¯æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬çš„ç»“æœä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23153v1">PDF</a> Accepted by TVCG</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§é’ˆå¯¹åŠ¨æ€ä¸‰ç»´åœºæ™¯çš„æ–°å‹è§†å›¾åˆæˆæ–¹æ³•ï¼Œé€šè¿‡åŸºäºNeRFçš„æ–¹æ³•è§£å†³æ­¤ä»»åŠ¡å¹¶è·å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨å°èŒƒå›´ç›¸æœºè¿åŠ¨çš„æƒ…å†µä¸‹é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šåœºæ™¯å‡ ä½•çš„ä¸æ­£ç¡®è¡¨ç¤ºå’Œç›¸æœºå‚æ•°çš„ä¸å‡†ç¡®ä¼°è®¡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†åŸºäºåˆ†å¸ƒçš„æ·±åº¦æ­£åˆ™åŒ–ï¼ˆDDRï¼‰å’Œä¸€ç§æ–°çš„å¯è§†åŒ–å·¥å…·ï¼Œèƒ½å¢å¼ºæ¨¡å‹å¯¹ç›¸æœºå‚æ•°çš„é²æ£’æ€§ï¼Œå¹¶åœ¨åœºæ™¯å‡ ä½•è¡¨ç¤ºæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€ä¸‰ç»´åœºæ™¯çš„æ–°å‹è§†å›¾åˆæˆæ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–NeRFæŠ€æœ¯è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>å½“ç›¸æœºè¿åŠ¨èŒƒå›´æœ‰é™æˆ–é™æ­¢æ—¶ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šåœºæ™¯å‡ ä½•è¡¨ç¤ºä¸å‡†ç¡®å’Œç›¸æœºå‚æ•°ä¼°è®¡ä¸å‡†ç¡®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåˆ†å¸ƒçš„æ·±åº¦æ­£åˆ™åŒ–ï¼ˆDDRï¼‰æ–¹æ³•ï¼Œç¡®ä¿æ¸²æŸ“æƒé‡åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒå¯¹é½ã€‚</li>
<li>DDRé€šè¿‡Gumbel-softmaxå¯å¾®é‡‡æ ·ç‚¹æ¥è®¡ç®—è¯¯å·®çš„æœŸæœ›å€¼ï¼ŒåŒæ—¶å¼•å…¥çº¦æŸä»¥å¼ºåŒ–åœºæ™¯å‡ ä½•çš„æ­£ç¡®æ€§ã€‚</li>
<li>ä¸ºäº†æ›´å¥½åœ°ç†è§£DDRï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¯è§†åŒ–å·¥å…·æ¥è§‚å¯Ÿåœºæ™¯å‡ ä½•è¡¨ç¤ºåœ¨æ¸²æŸ“æƒé‡å±‚é¢çš„ä¿¡æ¯ã€‚</li>
<li>æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»“åˆäº†ç›¸æœºå‚æ•°å­¦ä¹ ï¼Œæé«˜äº†å¯¹ç›¸æœºå‚æ•°çš„é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1c4a3950eddfcc480f66045cc8fa1e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8dfa0d24eaac831ca190b08c16bcb69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df3478b30b33979106c1f0cc4a8b889c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d10bab0c3488ce679c7dbef773752cf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2dfca45baec77ea9bc143e7d4e68cb5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="UnMix-NeRF-Spectral-Unmixing-Meets-Neural-Radiance-Fields"><a href="#UnMix-NeRF-Spectral-Unmixing-Meets-Neural-Radiance-Fields" class="headerlink" title="UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields"></a>UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields</h2><p><strong>Authors:Fabian Perez, Sara Rojas, Carlos Hinojosa, Hoover Rueda-ChacÃ³n, Bernard Ghanem</strong></p>
<p>Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic material properties. This limitation restricts accurate material perception, which is crucial for robotics, augmented reality, simulation, and other applications. We introduce UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling joint hyperspectral novel view synthesis and unsupervised material segmentation. Our method models spectral reflectance via diffuse and specular components, where a learned dictionary of global endmembers represents pure material signatures, and per-point abundances capture their distribution. For material segmentation, we use spectral signature predictions along learned endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF enables scene editing by modifying learned endmember dictionaries for flexible material-based appearance manipulation. Extensive experiments validate our approach, demonstrating superior spectral reconstruction and material segmentation to existing methods. Project page: <a target="_blank" rel="noopener" href="https://www.factral.co/UnMix-NeRF">https://www.factral.co/UnMix-NeRF</a>. </p>
<blockquote>
<p>åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„åˆ†å‰²æ–¹æ³•ä¸»è¦å…³æ³¨å¯¹è±¡è¯­ä¹‰ï¼Œå¹¶ä»…ä¾èµ–äºRGBæ•°æ®ï¼Œç¼ºä¹å†…åœ¨ææ–™å±æ€§ã€‚è¿™ä¸€å±€é™æ€§é™åˆ¶äº†ææ–™æ„ŸçŸ¥çš„å‡†ç¡®æ€§ï¼Œå¯¹äºæœºå™¨äººã€å¢å¼ºç°å®ã€æ¨¡æ‹Ÿå’Œå…¶ä»–åº”ç”¨è€Œè¨€ï¼Œè¿™æ˜¯è‡³å…³é‡è¦çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†UnMix-NeRFæ¡†æ¶ï¼Œå®ƒå°†å…‰è°±æ··åˆæŠ€æœ¯é›†æˆåˆ°NeRFä¸­ï¼Œå®ç°äº†è¶…å…‰è°±æ–°è§†è§’åˆæˆå’Œæ— äººç›‘ç£ææ–™åˆ†å‰²çš„è”åˆå¤„ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ¼«åå°„å’Œé•œé¢åå°„æˆåˆ†å¯¹å…‰è°±åå°„è¿›è¡Œå»ºæ¨¡ï¼Œå…¶ä¸­é€šè¿‡å…¨å±€ç«¯å…ƒå­¦ä¹ å­—å…¸è¡¨ç¤ºçº¯ææ–™ç‰¹å¾ï¼Œè€Œæ¯ç‚¹çš„ä¸°åº¦åˆ™æ•æ‰å…¶åˆ†å¸ƒã€‚å¯¹äºææ–™åˆ†å‰²ï¼Œæˆ‘ä»¬ä½¿ç”¨æ²¿å­¦ä¹ ç«¯å…ƒçš„è°±ç‰¹å¾é¢„æµ‹ï¼Œå®ç°æ— äººç›‘ç£çš„ææ–™èšç±»ã€‚æ­¤å¤–ï¼ŒUnMix-NeRFé€šè¿‡ä¿®æ”¹å­¦ä¹ ç«¯å…ƒå­—å…¸ï¼Œå®ç°åœºæ™¯ç¼–è¾‘ï¼Œè¿›è¡Œçµæ´»çš„ææ–™å¤–è§‚æ“ä½œã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨å…‰è°±é‡å»ºå’Œææ–™åˆ†å‰²æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://www.factral.co/UnMix-NeRF%E3%80%82">https://www.factral.co/UnMix-NeRFã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21884v1">PDF</a> Paper accepted at ICCV 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºNeRFçš„UnMix-NeRFæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å…‰è°±æ··åˆæŠ€æœ¯ï¼Œå®ç°äº†è”åˆè¶…å…‰è°±æ–°è§†è§’åˆæˆå’Œæ— ç›‘ç£ææ–™åˆ†å‰²ã€‚é€šè¿‡æ¨¡æ‹Ÿææ–™çš„åå°„å…‰è°±ï¼ŒUnMix-NeRFä½¿ç”¨å…¨å±€ç«¯å…ƒç»„æˆçš„å­—å…¸è¡¨ç¤ºçº¯ææ–™ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ¯ç‚¹çš„ä¸°åº¦æ•æ‰å…¶åˆ†å¸ƒã€‚å¯¹äºææ–™åˆ†å‰²ï¼Œå®ƒä½¿ç”¨å…‰è°±ç‰¹å¾é¢„æµ‹å’Œå­¦ä¹ çš„ç«¯å…ƒè¿›è¡Œæ— ç›‘ç£ææ–™èšç±»ã€‚æ­¤å¤–ï¼ŒUnMix-NeRFè¿˜èƒ½é€šè¿‡ä¿®æ”¹å­¦ä¹ çš„ç«¯å…ƒå­—å…¸å®ç°åœºæ™¯ç¼–è¾‘ï¼Œè¿›è¡Œçµæ´»çš„ææ–™å¤–è§‚æ“ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UnMix-NeRFç»“åˆäº†å…‰è°±æ··åˆæŠ€æœ¯ï¼Œå®ç°äº†NeRFåœ¨è¶…å…‰è°±æ–°è§†è§’åˆæˆå’Œæ— ç›‘ç£ææ–™åˆ†å‰²æ–¹é¢çš„çªç ´ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿææ–™çš„åå°„å…‰è°±ï¼ŒUnMix-NeRFä½¿ç”¨å…¨å±€ç«¯å…ƒå­—å…¸è¡¨ç¤ºçº¯ææ–™ç‰¹å¾ã€‚</li>
<li>UnMix-NeRFé‡‡ç”¨æ¯ç‚¹çš„ä¸°åº¦æ•æ‰ææ–™åˆ†å¸ƒã€‚</li>
<li>åˆ©ç”¨å…‰è°±ç‰¹å¾é¢„æµ‹å’Œå­¦ä¹ çš„ç«¯å…ƒè¿›è¡Œæ— ç›‘ç£ææ–™èšç±»ï¼Œå®ç°ææ–™åˆ†å‰²ã€‚</li>
<li>UnMix-NeRFæ”¯æŒåœºæ™¯ç¼–è¾‘ï¼Œé€šè¿‡ä¿®æ”¹å­¦ä¹ çš„ç«¯å…ƒå­—å…¸å®ç°çµæ´»çš„ææ–™å¤–è§‚æ“ä½œã€‚</li>
<li>å®éªŒéªŒè¯ï¼ŒUnMix-NeRFåœ¨å…‰è°±é‡å»ºå’Œææ–™åˆ†å‰²æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21884">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-84614942984b56ceba631d7293e6a313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f6a7fd7c3a1f9b276086618b955c2a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-447fdaa6bf12ed632ddae3bf47bba947.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ICP-3DGS-SfM-free-3D-Gaussian-Splatting-for-Large-scale-Unbounded-Scenes"><a href="#ICP-3DGS-SfM-free-3D-Gaussian-Splatting-for-Large-scale-Unbounded-Scenes" class="headerlink" title="ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded   Scenes"></a>ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded   Scenes</h2><p><strong>Authors:Chenhao Zhang, Yezhi Shen, Fengqing Zhu</strong></p>
<p>In recent years, neural rendering methods such as NeRFs and 3D Gaussian Splatting (3DGS) have made significant progress in scene reconstruction and novel view synthesis. However, they heavily rely on preprocessed camera poses and 3D structural priors from structure-from-motion (SfM), which are challenging to obtain in outdoor scenarios. To address this challenge, we propose to incorporate Iterative Closest Point (ICP) with optimization-based refinement to achieve accurate camera pose estimation under large camera movements. Additionally, we introduce a voxel-based scene densification approach to guide the reconstruction in large-scale scenes. Experiments demonstrate that our approach ICP-3DGS outperforms existing methods in both camera pose estimation and novel view synthesis across indoor and outdoor scenes of various scales. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/Chenhao-Z/ICP-3DGS">https://github.com/Chenhao-Z/ICP-3DGS</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç¥ç»æ¸²æŸ“æ–¹æ³•ï¼Œå¦‚NeRFå’Œä¸‰ç»´é«˜æ–¯æº…å‡ºï¼ˆ3DGSï¼‰ï¼Œåœ¨åœºæ™¯é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¸¥é‡ä¾èµ–äºä»è¿åŠ¨æ¢å¤ç»“æ„ï¼ˆSfMï¼‰é¢„å¤„ç†çš„ç›¸æœºå§¿æ€å’Œä¸‰ç»´ç»“æ„å…ˆéªŒï¼Œè¿™åœ¨æˆ·å¤–åœºæ™¯ä¸­å¾ˆéš¾è·å–ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºç»“åˆè¿­ä»£æœ€è¿‘ç‚¹ï¼ˆICPï¼‰å’Œä¼˜åŒ–ç²¾ä¿®æ–¹æ³•æ¥å®ç°å¤§ç›¸æœºè¿åŠ¨ä¸‹çš„å‡†ç¡®ç›¸æœºå§¿æ€ä¼°è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§åŸºäºä½“ç´ çš„åœºæ™¯ç¨ åŒ–æ–¹æ³•ï¼Œä»¥æŒ‡å¯¼å¤§è§„æ¨¡åœºæ™¯çš„é‡å»ºã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ICP-3DGSæ–¹æ³•åœ¨å®¤å†…å¤–å„ç§è§„æ¨¡çš„åœºæ™¯çš„ç›¸æœºå§¿æ€ä¼°è®¡å’Œæ–°é¢–è§†è§’åˆæˆæ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Chenhao-Z/ICP-3DGS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Chenhao-Z/ICP-3DGSè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21629v1">PDF</a> 6 pages, Source code is available at   <a target="_blank" rel="noopener" href="https://github.com/Chenhao-Z/ICP-3DGS">https://github.com/Chenhao-Z/ICP-3DGS</a>. To appear at ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼Œç¥ç»æ¸²æŸ“æ–¹æ³•å¦‚NeRFå’Œ3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰åœ¨åœºæ™¯é‡å»ºå’Œè§†è§’åˆæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬ä¾èµ–äºè¿åŠ¨ç»“æ„ï¼ˆSfMï¼‰çš„é¢„å¤„ç†ç›¸æœºå§¿æ€å’Œ3Dç»“æ„å…ˆéªŒï¼Œè¿™åœ¨æˆ·å¤–åœºæ™¯ä¸­è·å–å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºåº”å¯¹æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç»“åˆè¿­ä»£æœ€è¿‘ç‚¹ï¼ˆICPï¼‰ä¸ä¼˜åŒ–ç²¾ä¿®æ³•ï¼Œåœ¨å¤§èŒƒå›´ç›¸æœºç§»åŠ¨ä¸‹å®ç°å‡†ç¡®çš„ç›¸æœºå§¿æ€ä¼°è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºä½“ç´ çš„åœºæ™¯å¯†é›†åŒ–æ–¹æ³•ï¼Œä»¥æŒ‡å¯¼å¤§è§„æ¨¡åœºæ™¯çš„é‡å»ºã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„ICP-3DGSæ–¹æ³•åœ¨ç›¸æœºå§¿æ€ä¼°è®¡å’Œè§†è§’åˆæˆæ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»æ¸²æŸ“æ–¹æ³•å¦‚NeRFå’Œ3DGSåœ¨åœºæ™¯é‡å»ºå’Œè§†è§’åˆæˆä¸Šå–å¾—è¿›å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–SfMçš„é¢„å¤„ç†æ•°æ®å’Œç»“æ„å…ˆéªŒï¼Œè¿™åœ¨æˆ·å¤–åœºæ™¯ä¸­æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºç»“åˆICPå’Œä¼˜åŒ–ç²¾ä¿®æ³•ï¼Œå®ç°å¤§ç§»åŠ¨ä¸‹çš„å‡†ç¡®ç›¸æœºå§¿æ€ä¼°è®¡ã€‚</li>
<li>å¼•å…¥åŸºäºä½“ç´ çš„åœºæ™¯å¯†é›†åŒ–æ–¹æ³•ï¼Œç”¨äºæŒ‡å¯¼å¤§è§„æ¨¡åœºæ™¯çš„é‡å»ºã€‚</li>
<li>ICP-3DGSæ–¹æ³•åœ¨ç›¸æœºå§¿æ€ä¼°è®¡å’Œè§†è§’åˆæˆä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>æ–¹æ³•é€‚ç”¨äºå®¤å†…å’Œå®¤å¤–å„ç§è§„æ¨¡çš„åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-539a9603c157ad7fd6c29ff2a2664f44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99fb175bf6adfd097538f639c45f23c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b5b7ffe61cd8a850dd58ef27bdbb822.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01276fb61736e52f7009b6acf2e4681b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b36fe35f156410985594ca6ef22d533.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ae44bdca555c05aa9792ddcc43d0609.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="HumanGif-Single-View-Human-Diffusion-with-Generative-Prior"><a href="#HumanGif-Single-View-Human-Diffusion-with-Generative-Prior" class="headerlink" title="HumanGif: Single-View Human Diffusion with Generative Prior"></a>HumanGif: Single-View Human Diffusion with Generative Prior</h2><p><strong>Authors:Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji</strong></p>
<p>Previous 3D human creation methods have made significant progress in synthesizing view-consistent and temporally aligned results from sparse-view images or monocular videos. However, it remains challenging to produce perpetually realistic, view-consistent, and temporally coherent human avatars from a single image, as limited information is available in the single-view input setting. Motivated by the success of 2D character animation, we propose HumanGif, a single-view human diffusion model with generative prior. Specifically, we formulate the single-view-based 3D human novel view and pose synthesis as a single-view-conditioned human diffusion process, utilizing generative priors from foundational diffusion models to complement the missing information. To ensure fine-grained and consistent novel view and pose synthesis, we introduce a Human NeRF module in HumanGif to learn spatially aligned features from the input image, implicitly capturing the relative camera and human pose transformation. Furthermore, we introduce an image-level loss during optimization to bridge the gap between latent and image spaces in diffusion models. Extensive experiments on RenderPeople, DNA-Rendering, THuman 2.1, and TikTok datasets demonstrate that HumanGif achieves the best perceptual performance, with better generalizability for novel view and pose synthesis. </p>
<blockquote>
<p>ä¹‹å‰çš„3Däººç‰©åˆ›å»ºæ–¹æ³•åœ¨åˆæˆè§†å›¾ä¸€è‡´ä¸”æ—¶é—´ä¸Šå¯¹é½çš„ç»“æœæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™äº›ç»“æœæ¥æºäºç¨€ç–è§†å›¾å›¾åƒæˆ–å•ç›®è§†é¢‘ã€‚ç„¶è€Œï¼Œä»å•å¼ å›¾åƒç”Ÿæˆæ°¸ä¹…é€¼çœŸçš„ã€è§†å›¾ä¸€è‡´ä¸”æ—¶é—´ä¸Šè¿è´¯çš„äººç‰©åŒ–èº«ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºåœ¨å•è§†å›¾è¾“å…¥è®¾ç½®ä¸­å¯ç”¨çš„ä¿¡æ¯æœ‰é™ã€‚å—åˆ°äºŒç»´è§’è‰²åŠ¨ç”»æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†HumanGifï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦æœ‰ç”Ÿæˆå…ˆéªŒçš„å•è§†å›¾äººç±»æ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†åŸºäºå•è§†å›¾çš„3Däººç‰©æ–°é¢–è§†è§’å’Œå§¿æ€åˆæˆåˆ¶å®šä¸ºå—å•è§†å›¾æ¡ä»¶çº¦æŸçš„äººç±»æ‰©æ•£è¿‡ç¨‹ï¼Œåˆ©ç”¨åŸºç¡€æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒæ¥è¡¥å……ç¼ºå¤±çš„ä¿¡æ¯ã€‚ä¸ºäº†ç¡®ä¿ç²¾ç»†ä¸”ä¸€è‡´çš„å…¨æ–°è§†è§’å’Œå§¿æ€åˆæˆï¼Œæˆ‘ä»¬åœ¨HumanGifä¸­å¼•å…¥äº†Human NeRFæ¨¡å—ï¼Œä»è¾“å…¥å›¾åƒä¸­å­¦ä¹ ç©ºé—´å¯¹é½çš„ç‰¹å¾ï¼Œéšå¼æ•è·ç›¸å¯¹ç›¸æœºå’Œäººç‰©å§¿æ€å˜æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å¼•å…¥äº†å›¾åƒçº§æŸå¤±ï¼Œä»¥å¼¥åˆæ‰©æ•£æ¨¡å‹ä¸­çš„æ½œåœ¨ç©ºé—´å’Œå›¾åƒç©ºé—´ä¹‹é—´çš„å·®è·ã€‚åœ¨RenderPeopleã€DNA-Renderingã€THuman 2.1å’ŒTikTokæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHumanGifåœ¨æ„ŸçŸ¥æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¯¹äºæ–°é¢–è§†è§’å’Œå§¿æ€åˆæˆå…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12080v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://skhu101.github.io/HumanGif/">https://skhu101.github.io/HumanGif/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå•è§†è§’çš„æ‰©æ•£æ¨¡å‹HumanGifï¼Œç”¨äºåˆ›å»ºäººç±»è§’è‰²åŠ¨ç”»ã€‚è¯¥æ¨¡å‹ç»“åˆäº†äºŒç»´è§’è‰²åŠ¨ç”»çš„æˆåŠŸç»éªŒï¼Œé€šè¿‡å¼•å…¥ç”Ÿæˆå…ˆéªŒä¿¡æ¯æ¥å¼¥è¡¥å•è§†è§’è¾“å…¥ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ã€‚HumanGifä½¿ç”¨Human NeRFæ¨¡å—å­¦ä¹ ä»è¾“å…¥å›¾åƒä¸­æå–çš„ç©ºé—´å¯¹é½ç‰¹å¾ï¼Œå¹¶éšå¼æ•æ‰ç›¸æœºå’Œäººç±»å§¿åŠ¿çš„ç›¸å¯¹å˜æ¢ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å›¾åƒçº§æŸå¤±ä¼˜åŒ–æ¨¡å‹ï¼Œç¼©å°æ½œåœ¨ç©ºé—´å’Œå›¾åƒç©ºé—´ä¹‹é—´çš„å·®è·ã€‚å®éªŒè¯æ˜ï¼ŒHumanGifåœ¨æ„ŸçŸ¥æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œå…·æœ‰å‡ºè‰²çš„æ–°é¢–è§†è§’å’Œå§¿æ€åˆæˆçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HumanGifæ˜¯ä¸€ä¸ªåŸºäºå•è§†è§’çš„äººç±»æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨ç”Ÿæˆå…ˆéªŒä¿¡æ¯åˆæˆæ–°é¢–è§†è§’å’Œå§¿æ€çš„äººç±»è§’è‰²åŠ¨ç”»ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥Human NeRFæ¨¡å—å­¦ä¹ ä»è¾“å…¥å›¾åƒä¸­æå–ç©ºé—´å¯¹é½ç‰¹å¾ï¼Œéšå¼æ•æ‰ç›¸æœºå’Œäººç±»å§¿åŠ¿çš„ç›¸å¯¹å˜æ¢ã€‚</li>
<li>HumanGifé‡‡ç”¨å›¾åƒçº§æŸå¤±ä¼˜åŒ–æ¨¡å‹ï¼Œä»¥æé«˜å¯¹æœªçŸ¥è§†è§’å’Œå§¿æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒHumanGifåœ¨æ„ŸçŸ¥æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œèƒ½å¤Ÿæä¾›é«˜è´¨é‡çš„äººç±»è§’è‰²åŠ¨ç”»ã€‚</li>
<li>è¯¥æ¨¡å‹é€‚ç”¨äºå¤šç§æ•°æ®é›†ï¼ŒåŒ…æ‹¬RenderPeopleã€DNA-Renderingã€THuman 2.1å’ŒTikTokç­‰ã€‚</li>
<li>HumanGifçš„æˆåŠŸæºäºå…¶ç»“åˆäºŒç»´è§’è‰²åŠ¨ç”»ç»éªŒå¹¶å¼•å…¥ç”Ÿæˆå…ˆéªŒä¿¡æ¯çš„åˆ›æ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e85a13c07d28886374a04ebea374f311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ef2f3e3ebf3fcfb8acb1c9334475e50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7a7845aedaf1530dc23f922b4fb86a4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Grounding-Creativity-in-Physics-A-Brief-Survey-of-Physical-Priors-in-AIGC"><a href="#Grounding-Creativity-in-Physics-A-Brief-Survey-of-Physical-Priors-in-AIGC" class="headerlink" title="Grounding Creativity in Physics: A Brief Survey of Physical Priors in   AIGC"></a>Grounding Creativity in Physics: A Brief Survey of Physical Priors in   AIGC</h2><p><strong>Authors:Siwei Meng, Yawei Luo, Ping Liu</strong></p>
<p>Recent advancements in AI-generated content have significantly improved the realism of 3D and 4D generation. However, most existing methods prioritize appearance consistency while neglecting underlying physical principles, leading to artifacts such as unrealistic deformations, unstable dynamics, and implausible objects interactions. Incorporating physics priors into generative models has become a crucial research direction to enhance structural integrity and motion realism. This survey provides a review of physics-aware generative methods, systematically analyzing how physical constraints are integrated into 3D and 4D generation. First, we examine recent works in incorporating physical priors into static and dynamic 3D generation, categorizing methods based on representation types, including vision-based, NeRF-based, and Gaussian Splatting-based approaches. Second, we explore emerging techniques in 4D generation, focusing on methods that model temporal dynamics with physical simulations. Finally, we conduct a comparative analysis of major methods, highlighting their strengths, limitations, and suitability for different materials and motion dynamics. By presenting an in-depth analysis of physics-grounded AIGC, this survey aims to bridge the gap between generative models and physical realism, providing insights that inspire future research in physically consistent content generation. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹æ–¹é¢çš„æœ€æ–°è¿›å±•æå¤§åœ°æé«˜äº†3Då’Œ4Dç”Ÿæˆçš„é€¼çœŸåº¦ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¼˜å…ˆè€ƒè™‘å¤–è§‚çš„ä¸€è‡´æ€§ï¼Œå´å¿½ç•¥äº†åŸºæœ¬çš„ç‰©ç†åŸç†ï¼Œå¯¼è‡´å‡ºç°ä¸çœŸå®çš„å˜å½¢ã€ä¸ç¨³å®šçš„åŠ¨æ€ä»¥åŠä¸åˆç†çš„ç‰©ä½“äº¤äº’ç­‰ä¼ªè¿¹ã€‚å› æ­¤ï¼Œå°†ç‰©ç†å…ˆéªŒçŸ¥è¯†èå…¥ç”Ÿæˆæ¨¡å‹å·²æˆä¸ºå¢å¼ºç»“æ„å®Œæ•´æ€§å’Œè¿åŠ¨é€¼çœŸåº¦çš„å…³é”®ç ”ç©¶æ–¹å‘ã€‚æœ¬æ–‡ç»¼è¿°äº†ç‰©ç†æ„ŸçŸ¥ç”Ÿæˆæ–¹æ³•ï¼Œç³»ç»Ÿåˆ†æäº†å¦‚ä½•å°†ç‰©ç†çº¦æŸèå…¥3Då’Œ4Dç”Ÿæˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç ”ç©¶äº†è¿‘æœŸå°†ç‰©ç†å…ˆéªŒçŸ¥è¯†èå…¥é™æ€å’ŒåŠ¨æ€3Dç”Ÿæˆçš„å·¥ä½œï¼ŒæŒ‰è¡¨ç¤ºç±»å‹å¯¹æ–¹æ³•è¿›è¡Œåˆ†ç±»ï¼ŒåŒ…æ‹¬åŸºäºè§†è§‰ã€åŸºäºNeRFå’ŒåŸºäºé«˜æ–¯æ‹¼è´´çš„æ–¹æ³•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ¢ç´¢äº†4Dç”Ÿæˆçš„æ–°å…´æŠ€æœ¯ï¼Œé‡ç‚¹å…³æ³¨åˆ©ç”¨ç‰©ç†æ¨¡æ‹Ÿå¯¹æ—¶é—´åŠ¨æ€è¿›è¡Œå»ºæ¨¡çš„æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹ä¸»è¦æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œé‡ç‚¹ä»‹ç»äº†å®ƒä»¬å„è‡ªçš„ä¼˜åŠ¿ã€å±€é™æ€§å’Œåœ¨ä¸åŒææ–™å’Œè¿åŠ¨åŠ¨åŠ›å­¦æ–¹é¢çš„é€‚ç”¨æ€§ã€‚é€šè¿‡å¯¹åŸºäºç‰©ç†åŸç†çš„äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹çš„æ·±å…¥åˆ†æï¼Œæœ¬ç»¼è¿°æ—¨åœ¨ç¼©å°ç”Ÿæˆæ¨¡å‹å’Œç‰©ç†é€¼çœŸåº¦ä¹‹é—´çš„å·®è·ï¼Œæä¾›è§è§£ï¼Œä¸ºç‰©ç†ä¸€è‡´æ€§å†…å®¹ç”Ÿæˆæ–¹é¢çš„æœªæ¥ç ”ç©¶æä¾›çµæ„Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07007v3">PDF</a> Accepted by IJCAI 2025 Survey Track</p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹çš„æŠ€æœ¯è¿›æ­¥æ˜¾è‘—æé«˜äº†ä¸‰ç»´ï¼ˆ3Dï¼‰å’Œå››ç»´ï¼ˆ4Dï¼‰å†…å®¹çš„é€¼çœŸåº¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€é‡è§†å¤–è§‚ä¸€è‡´æ€§ï¼Œå¿½è§†äº†åº•å±‚ç‰©ç†åŸç†ï¼Œå¯¼è‡´å‡ºç°ä¸çœŸå®çš„å˜å½¢ã€åŠ¨æ€ä¸ç¨³å®šå’Œç‰©ä½“äº¤äº’ä¸åˆç†ç­‰é—®é¢˜ã€‚ä¸ºäº†å¢å¼ºç»“æ„å®Œæ•´æ€§å’ŒåŠ¨ä½œé€¼çœŸæ€§ï¼Œå°†ç‰©ç†å…ˆéªŒçŸ¥è¯†èå…¥ç”Ÿæˆæ¨¡å‹æˆä¸ºé‡è¦ç ”ç©¶æ–¹å‘ã€‚æœ¬æ–‡ç»¼è¿°äº†ç‰©ç†æ„ŸçŸ¥ç”Ÿæˆæ–¹æ³•ï¼Œç³»ç»Ÿåˆ†æäº†å¦‚ä½•å°†ç‰©ç†çº¦æŸèå…¥3Då’Œ4Då†…å®¹ç”Ÿæˆã€‚æ–‡ç« é¦–å…ˆæ¢è®¨äº†å°†ç‰©ç†å…ˆéªŒçŸ¥è¯†èå…¥é™æ€å’ŒåŠ¨æ€3Dç”Ÿæˆçš„æ–¹æ³•ï¼ŒæŒ‰è¡¨ç¤ºç±»å‹åˆ†ç±»ï¼ŒåŒ…æ‹¬åŸºäºè§†è§‰ã€NeRFå’ŒGaussian Splattingçš„æ–¹æ³•ã€‚æ¥ç€ï¼Œæ–‡ç« æ¢è®¨äº†ç”¨äºå»ºæ¨¡æ—¶é—´åŠ¨æ€çš„ç‰©ç†æ¨¡æ‹Ÿçš„4Dç”Ÿæˆæ–°å…´æŠ€æœ¯ã€‚æœ€åï¼Œå¯¹ä¸»è¦æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œçªå‡ºäº†å…¶ä¼˜åŠ¿ã€å±€é™æ€§å’Œåœ¨ä¸åŒææ–™å’Œè¿åŠ¨åŠ¨æ€ä¸­çš„é€‚ç”¨æ€§ã€‚æœ¬æ–‡æ—¨åœ¨å¼¥è¡¥ç”Ÿæˆæ¨¡å‹å’Œç‰©ç†çœŸå®æ€§ä¹‹é—´çš„é¸¿æ²Ÿï¼Œæä¾›æ·±å…¥äº†è§£å¹¶ä¸ºæœªæ¥çš„ç‰©ç†ä¸€è‡´æ€§å†…å®¹ç”Ÿæˆç ”ç©¶æä¾›å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆçš„3Då’Œ4Då†…å®¹åœ¨é€¼çœŸåº¦ä¸Šæœ‰äº†æ˜¾è‘—çš„æå‡ã€‚</li>
<li>å½“å‰æ–¹æ³•è¿‡äºæ³¨é‡å¤–è§‚ä¸€è‡´æ€§ï¼Œå¿½è§†äº†ç‰©ç†åŸç†ï¼Œå¯¼è‡´å­˜åœ¨ä¸çœŸå®å˜å½¢ç­‰é—®é¢˜ã€‚</li>
<li>å°†ç‰©ç†å…ˆéªŒçŸ¥è¯†èå…¥ç”Ÿæˆæ¨¡å‹å¯ä»¥å¢å¼ºç»“æ„å®Œæ•´æ€§å’ŒåŠ¨ä½œé€¼çœŸæ€§ã€‚</li>
<li>æ–‡ç« ç»¼è¿°äº†ç‰©ç†æ„ŸçŸ¥ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶ç³»ç»Ÿåˆ†æäº†å¦‚ä½•å°†ç‰©ç†çº¦æŸèå…¥3Då’Œ4Då†…å®¹ç”Ÿæˆã€‚</li>
<li>é™æ€å’ŒåŠ¨æ€3Dç”Ÿæˆæ–¹æ³•æŒ‰è¡¨ç¤ºç±»å‹åˆ†ç±»ï¼ŒåŒ…æ‹¬åŸºäºè§†è§‰ã€NeRFå’ŒGaussian Splattingçš„æ–¹æ³•ã€‚</li>
<li>4Dç”ŸæˆæŠ€æœ¯ä¸»è¦å…³æ³¨äºä½¿ç”¨ç‰©ç†æ¨¡æ‹Ÿå»ºæ¨¡æ—¶é—´åŠ¨æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8316847a61ca9ce8a873eb8dcbaa3c8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc02149aac6500706268cff41d86f7d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcb9ba7838cf34365a0566c3c008fea1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PoI-A-Filter-to-Extract-Pixel-of-Interest-from-Novel-View-Synthesis-for-Scene-Coordinate-Regression"><a href="#PoI-A-Filter-to-Extract-Pixel-of-Interest-from-Novel-View-Synthesis-for-Scene-Coordinate-Regression" class="headerlink" title="PoI: A Filter to Extract Pixel of Interest from Novel View Synthesis for   Scene Coordinate Regression"></a>PoI: A Filter to Extract Pixel of Interest from Novel View Synthesis for   Scene Coordinate Regression</h2><p><strong>Authors:Feifei Li, Qi Song, Chi Zhang, Hui Shuai, Rui Huang</strong></p>
<p>Novel View Synthesis (NVS) techniques, notably Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), can augment camera pose estimation by extending and diversifying training data. However, images generated by these methods are often plagued by spatial artifacts such as blurring and ghosting, undermining their reliability as training data for camera pose estimation. This limitation is particularly critical for Scene Coordinate Regression (SCR) methods, which aim at pixel-level 3D coordinate estimation, because rendering artifacts directly lead to estimation inaccuracies. To address this challenge, we propose a dual-criteria filtering mechanism that dynamically identifies and discards suboptimal pixels during training. The dual-criteria filter evaluates two concurrent metrics: (1) real-time SCR reprojection error, and (2) gradient threshold, across the coordinate regression domain. In addition, for visual localization problems in sparse-input scenarios, it becomes even more necessary to use NVS-generated data to assist localization. We design a coarse-to-fine Points of Interest (PoI) variant using sparse-input NVS to solve this problem. Experiments across indoor and outdoor benchmarks confirm our methodâ€™s efficacy, achieving state-of-the-art localization accuracy while maintaining computational efficiency. </p>
<blockquote>
<p>æ–°é¢–è§†å›¾åˆæˆï¼ˆNVSï¼‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰ï¼Œå¯ä»¥é€šè¿‡æ‰©å±•å’Œå¤šæ ·åŒ–è®­ç»ƒæ•°æ®æ¥å¢å¼ºç›¸æœºå§¿æ€ä¼°è®¡ã€‚ç„¶è€Œï¼Œç”±è¿™äº›æ–¹æ³•ç”Ÿæˆçš„å›¾åƒå¾€å¾€å—åˆ°ç©ºé—´ä¼ªå½±çš„å›°æ‰°ï¼Œå¦‚æ¨¡ç³Šå’Œé‡å½±ï¼Œè¿™é™ä½äº†å®ƒä»¬ä½œä¸ºç›¸æœºå§¿æ€ä¼°è®¡è®­ç»ƒæ•°æ®çš„å¯é æ€§ã€‚è¿™ä¸€å±€é™æ€§å¯¹äºåœºæ™¯åæ ‡å›å½’ï¼ˆSCRï¼‰æ–¹æ³•å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºè¿™äº›æ–¹æ³•æ—¨åœ¨è¿›è¡Œåƒç´ çº§åˆ«çš„ä¸‰ç»´åæ ‡ä¼°è®¡ï¼Œæ¸²æŸ“ä¼ªå½±ç›´æ¥å¯¼è‡´ä¼°è®¡ä¸å‡†ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒæ ‡å‡†è¿‡æ»¤æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è¯†åˆ«å¹¶ä¸¢å¼ƒæ¬¡ä¼˜åƒç´ ã€‚åŒæ ‡å‡†è¿‡æ»¤å™¨è¯„ä¼°ä¸¤ä¸ªå¹¶å‘æŒ‡æ ‡ï¼šï¼ˆ1ï¼‰å®æ—¶SCRå†æŠ•å½±è¯¯å·®ï¼Œï¼ˆ2ï¼‰åæ ‡å›å½’åŸŸä¸­çš„æ¢¯åº¦é˜ˆå€¼ã€‚æ­¤å¤–ï¼Œåœ¨ç¨€ç–è¾“å…¥çš„è§†è§‰å®šä½é—®é¢˜ä¸­ï¼Œä½¿ç”¨NVSç”Ÿæˆçš„æ•°æ®è¾…åŠ©å®šä½å˜å¾—å°¤ä¸ºå¿…è¦ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä»ç²—åˆ°ç»†çš„æ„Ÿå…´è¶£ç‚¹ï¼ˆPoIï¼‰å˜ä½“ï¼Œä½¿ç”¨ç¨€ç–è¾“å…¥çš„NVSæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å®¤å†…å’Œå®¤å¤–åŸºå‡†æµ‹è¯•çš„å®éªŒç»“æœè¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„å®šä½ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04843v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯è´´ç‰‡ï¼ˆ3DGSï¼‰ç­‰æ–°å‹è§†å›¾åˆæˆï¼ˆNVSï¼‰æŠ€æœ¯ï¼Œå¯ä»¥å¢å¼ºç›¸æœºå§¿æ€ä¼°è®¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç”Ÿæˆçš„å›¾åƒå¸¸å­˜åœ¨ç©ºé—´ä¼ªå½±é—®é¢˜ï¼Œå¦‚æ¨¡ç³Šå’Œé‡å½±ï¼Œé™ä½äº†å®ƒä»¬ä½œä¸ºè®­ç»ƒæ•°æ®çš„å¯é æ€§ã€‚é’ˆå¯¹åœºæ™¯åæ ‡å›å½’ï¼ˆSCRï¼‰æ–¹æ³•çš„æ¸²æŸ“ä¼ªå½±å¯¼è‡´ä¼°è®¡ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºåŒé‡æ ‡å‡†çš„è¿‡æ»¤æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è¯†åˆ«å¹¶ä¸¢å¼ƒéä¼˜è´¨åƒç´ ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ç¨€ç–è¾“å…¥åœºæ™¯ä¸‹çš„è§†è§‰å®šä½é—®é¢˜ï¼Œä½¿ç”¨NVSç”Ÿæˆçš„æ•°æ®è¾…åŠ©å®šä½è‡³å…³é‡è¦ã€‚è®¾è®¡äº†ä¸€ç§ä»ç²—åˆ°ç»†çš„æ„Ÿå…´è¶£ç‚¹ï¼ˆPoIï¼‰å˜ä½“æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¹¶åœ¨å®¤å†…å’Œå®¤å¤–åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œå®ç°äº†é«˜å®šä½ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NVSæŠ€æœ¯å¦‚NeRFå’Œ3DGSèƒ½å¢å¼ºç›¸æœºå§¿æ€ä¼°è®¡çš„è®­ç»ƒæ•°æ®å¤šæ ·æ€§å’Œæ‰©å±•æ€§ã€‚</li>
<li>NVSç”Ÿæˆçš„å›¾åƒå­˜åœ¨ç©ºé—´ä¼ªå½±é—®é¢˜ï¼Œå¦‚æ¨¡ç³Šå’Œé‡å½±ï¼Œå½±å“ä½œä¸ºè®­ç»ƒæ•°æ®çš„å¯é æ€§ã€‚</li>
<li>åŒé‡æ ‡å‡†çš„è¿‡æ»¤æœºåˆ¶å¯ä»¥åŠ¨æ€è¯†åˆ«å¹¶ä¸¢å¼ƒéä¼˜è´¨åƒç´ ï¼Œæé«˜SCRæ–¹æ³•çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨ç¨€ç–è¾“å…¥åœºæ™¯ä¸‹ï¼Œä½¿ç”¨NVSç”Ÿæˆçš„æ•°æ®å¯¹äºè§†è§‰å®šä½è‡³å…³é‡è¦ã€‚</li>
<li>è®¾è®¡äº†ä»ç²—åˆ°ç»†çš„PoIå˜ä½“æ¥è§£å†³ç¨€ç–è¾“å…¥åœºæ™¯ä¸‹çš„è§†è§‰å®šä½é—®é¢˜ã€‚</li>
<li>å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨å®¤å†…å¤–åŸºå‡†æµ‹è¯•ä¸Šçš„é«˜æ•ˆæ€§å’Œé«˜å®šä½ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04843">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aba2a3338028740e67919c961813c4b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-008f4f060e48f7c770ebc5b76c85d03b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e31d40fdfb0e54ba206bd97306511e06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0e770f1dca42eeeeee8b40a3a8f2f60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39a5fd5b8a197e32326661bc38cd5e51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb21def5b5cbeec36a9a12456b99ec9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b57f2ca9edb64a288225e1626988f58.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Wavelet-Diffusion-GAN-for-Image-Super-Resolution"><a href="#A-Wavelet-Diffusion-GAN-for-Image-Super-Resolution" class="headerlink" title="A Wavelet Diffusion GAN for Image Super-Resolution"></a>A Wavelet Diffusion GAN for Image Super-Resolution</h2><p><strong>Authors:Lorenzo Aloisi, Luigi Sigillo, Aurelio Uncini, Danilo Comminiello</strong></p>
<p>In recent years, diffusion models have emerged as a superior alternative to generative adversarial networks (GANs) for high-fidelity image generation, with wide applications in text-to-image generation, image-to-image translation, and super-resolution. However, their real-time feasibility is hindered by slow training and inference speeds. This study addresses this challenge by proposing a wavelet-based conditional Diffusion GAN scheme for Single-Image Super-Resolution (SISR). Our approach utilizes the diffusion GAN paradigm to reduce the timesteps required by the reverse diffusion process and the Discrete Wavelet Transform (DWT) to achieve dimensionality reduction, decreasing training and inference times significantly. The results of an experimental validation on the CelebA-HQ dataset confirm the effectiveness of our proposed scheme. Our approach outperforms other state-of-the-art methodologies successfully ensuring high-fidelity output while overcoming inherent drawbacks associated with diffusion models in time-sensitive applications. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹å·²ç»ä½œä¸ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åœ¨é«˜ä¿çœŸå›¾åƒç”Ÿæˆæ–¹é¢çš„ä¼˜è¶Šæ›¿ä»£æ–¹æ¡ˆè€Œå‡ºç°ï¼Œå¹¿æ³›åº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å›¾åƒåˆ°å›¾åƒè½¬æ¢å’Œè¶…åˆ†è¾¨ç‡ç­‰é¢†åŸŸã€‚ç„¶è€Œï¼Œç”±äºå…¶è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦æ…¢ï¼Œå®æ—¶å¯è¡Œæ€§å—åˆ°é™åˆ¶ã€‚æœ¬ç ”ç©¶é€šè¿‡æå‡ºä¸€ç§åŸºäºå°æ³¢çš„å•ä¸€å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰æ¡ä»¶æ‰©æ•£GANæ–¹æ¡ˆæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æ‰©æ•£GANèŒƒå¼å‡å°‘åå‘æ‰©æ•£è¿‡ç¨‹æ‰€éœ€çš„æ—¶é—´æ­¥é•¿å’Œç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰æ¥å®ç°é™ç»´ï¼Œä»è€Œæ˜¾è‘—å‡å°‘è®­ç»ƒå’Œæ¨ç†æ—¶é—´ã€‚åœ¨CelebA-HQæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯ç»“æœè¯å®äº†æ‰€æå‡ºæ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸè¶…è¶Šäº†å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨ä¿è¯é«˜ä¿çœŸè¾“å‡ºçš„åŒæ—¶ï¼Œå…‹æœäº†æ‰©æ•£æ¨¡å‹åœ¨æ—¶é—´æ•æ„Ÿåº”ç”¨ä¸­çš„å›ºæœ‰ç¼ºç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17966v2">PDF</a> The paper has been accepted at Italian Workshop on Neural Networks   (WIRN) 2024</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒç”Ÿæˆé¢†åŸŸå±•ç°å‡ºä¼˜äºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¼˜åŠ¿ï¼Œå¹¿æ³›åº”ç”¨äºæ–‡æœ¬è½¬å›¾åƒã€å›¾åƒè½¬å›¾åƒä»¥åŠè¶…åˆ†è¾¨ç‡ç­‰åœºæ™¯ã€‚ä½†æ‰©æ•£æ¨¡å‹å­˜åœ¨è®­ç»ƒåŠæ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºå°æ³¢å˜æ¢çš„æ¡ä»¶æ‰©æ•£GANæ–¹æ¡ˆï¼Œç”¨äºå•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰ã€‚è¯¥ç ”ç©¶åˆ©ç”¨æ‰©æ•£GANèŒƒå¼å‡å°‘åå‘æ‰©æ•£è¿‡ç¨‹æ‰€éœ€çš„æ—¶é—´æ­¥é•¿ï¼Œå¹¶ç»“åˆç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰å®ç°é™ç»´ï¼Œæ˜¾è‘—ç¼©çŸ­è®­ç»ƒå’Œæ¨ç†æ—¶é—´ã€‚åœ¨CelebA-HQæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯ï¼Œç¡®è®¤äº†æ‰€ææ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶åœ¨ä¿è¯é«˜ä¿çœŸè¾“å‡ºçš„åŒæ—¶ï¼Œå…‹æœäº†æ‰©æ•£æ¨¡å‹åœ¨æ—¶é—´æ•æ„Ÿæ€§åº”ç”¨ä¸­çš„å›ºæœ‰ç¼ºç‚¹ï¼Œä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºé«˜ä¿çœŸå›¾åƒç”Ÿæˆé¢†åŸŸçš„é¢†å…ˆæŠ€æœ¯ï¼Œæ›¿ä»£äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºæ–‡æœ¬è½¬å›¾åƒã€å›¾åƒè½¬å›¾åƒä»¥åŠè¶…åˆ†è¾¨ç‡ç­‰åœºæ™¯ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å­˜åœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå°æ³¢å˜æ¢çš„æ¡ä»¶æ‰©æ•£GANæ–¹æ¡ˆï¼Œç”¨äºå•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰ã€‚</li>
<li>è¯¥æ–¹æ¡ˆåˆ©ç”¨æ‰©æ•£GANèŒƒå¼å‡å°‘åå‘æ‰©æ•£è¿‡ç¨‹çš„æ—¶é—´æ­¥é•¿ã€‚</li>
<li>ç»“åˆç¦»æ•£å°æ³¢å˜æ¢ï¼ˆDWTï¼‰å®ç°é™ç»´ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f8c61ed71f06e8e110baf528da95acf8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7987eb632319509c4054fc4bf32519b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-407a35b38a6f7e683a27c6fc037fc2c2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Self-supervised-Learning-of-Hybrid-Part-aware-3D-Representation-of-2D-Gaussians-and-Superquadrics"><a href="#Self-supervised-Learning-of-Hybrid-Part-aware-3D-Representation-of-2D-Gaussians-and-Superquadrics" class="headerlink" title="Self-supervised Learning of Hybrid Part-aware 3D Representation of 2D   Gaussians and Superquadrics"></a>Self-supervised Learning of Hybrid Part-aware 3D Representation of 2D   Gaussians and Superquadrics</h2><p><strong>Authors:Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, Kai Xu</strong></p>
<p>Low-level 3D representations, such as point clouds, meshes, NeRFs and 3D Gaussians, are commonly used for modeling 3D objects and scenes. However, cognitive studies indicate that human perception operates at higher levels and interprets 3D environments by decomposing them into meaningful structural parts, rather than low-level elements like points or voxels. Structured geometric decomposition enhances scene interpretability and facilitates downstream tasks requiring component-level manipulation. In this work, we introduce PartGS, a self-supervised part-aware reconstruction framework that integrates 2D Gaussians and superquadrics to parse objects and scenes into an interpretable decomposition, leveraging multi-view image inputs to uncover 3D structural information. Our method jointly optimizes superquadric meshes and Gaussians by coupling their parameters within a hybrid representation. On one hand, superquadrics enable the representation of a wide range of shape primitives, facilitating flexible and meaningful decompositions. On the other hand, 2D Gaussians capture detailed texture and geometric details, ensuring high-fidelity appearance and geometry reconstruction. Operating in a self-supervised manner, our approach demonstrates superior performance compared to state-of-the-art methods across extensive experiments on the DTU, ShapeNet, and real-world datasets. </p>
<blockquote>
<p>ä½çº§åˆ«çš„ä¸‰ç»´è¡¨ç¤ºï¼Œå¦‚ç‚¹äº‘ã€ç½‘æ ¼ã€NeRFå’Œä¸‰ç»´é«˜æ–¯ç­‰ï¼Œé€šå¸¸ç”¨äºå¯¹ä¸‰ç»´ç‰©ä½“å’Œåœºæ™¯è¿›è¡Œå»ºæ¨¡ã€‚ç„¶è€Œï¼Œè®¤çŸ¥ç ”ç©¶è¡¨æ˜ï¼Œäººç±»çš„æ„ŸçŸ¥æ˜¯åœ¨æ›´é«˜å±‚æ¬¡ä¸Šè¿›è¡Œçš„ï¼Œé€šè¿‡åˆ†è§£ä¸‰ç»´ç¯å¢ƒä¸ºæœ‰æ„ä¹‰çš„ç»“æ„éƒ¨åˆ†æ¥è¿›è¡Œè§£è¯»ï¼Œè€Œéä½çº§åˆ«çš„å…ƒç´ ï¼Œå¦‚ç‚¹æˆ–ä½“ç´ ã€‚ç»“æ„åŒ–å‡ ä½•åˆ†è§£å¢å¼ºäº†åœºæ™¯çš„å¯è§£é‡Šæ€§ï¼Œå¹¶ä¿ƒè¿›äº†éœ€è¦ç»„ä»¶çº§æ“ä½œçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10789v3">PDF</a> Accepted by ICCV 2025 Code: <a target="_blank" rel="noopener" href="https://github.com/zhirui-gao/PartGS">https://github.com/zhirui-gao/PartGS</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPartGSçš„è‡ªç›‘ç£æ„ŸçŸ¥é‡å»ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨äºŒç»´é«˜æ–¯å’Œè¶…çº§äºŒæ¬¡æ›²é¢æ¥è§£æå¯¹è±¡å’Œåœºæ™¯ï¼Œå°†å…¶åˆ†è§£ä¸ºå¯è§£é‡Šçš„ç»“æ„ã€‚é€šè¿‡å¤šè§†è§’å›¾åƒè¾“å…¥ï¼Œæ­ç¤ºä¸‰ç»´ç»“æ„ä¿¡æ¯ã€‚è¯¥æ–¹æ³•é€šè¿‡æ··åˆè¡¨ç¤ºä¸­çš„å‚æ•°è€¦åˆæ¥è”åˆä¼˜åŒ–è¶…çº§äºŒæ¬¡æ›²é¢ç½‘æ ¼å’Œé«˜æ–¯ï¼Œä»è€Œå®ç°çµæ´»å’Œæœ‰æ„ä¹‰çš„åˆ†è§£ï¼ŒåŒæ—¶ä¿è¯é«˜ä¿çœŸåº¦çš„å¤–è§‚å’Œå‡ ä½•é‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PartGSæ¡†æ¶ç»“åˆäºŒç»´é«˜æ–¯å’Œè¶…çº§äºŒæ¬¡æ›²é¢ï¼Œå®ç°å¯¹å¯¹è±¡å’Œåœºæ™¯çš„æ„ŸçŸ¥é‡å»ºã€‚</li>
<li>é€šè¿‡å¤šè§†è§’å›¾åƒè¾“å…¥ï¼Œæ­ç¤ºä¸‰ç»´ç»“æ„ä¿¡æ¯ã€‚</li>
<li>PartGSæ¡†æ¶é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹å¼ï¼Œæ— éœ€å¤§é‡æ ‡æ³¨æ•°æ®ã€‚</li>
<li>è¶…çº§äºŒæ¬¡æ›²é¢èƒ½å¤Ÿè¡¨ç¤ºå¤šç§å½¢çŠ¶åŸå§‹ï¼Œä¿ƒè¿›çµæ´»å’Œæœ‰æ„ä¹‰çš„åˆ†è§£ã€‚</li>
<li>äºŒç»´é«˜æ–¯æ•æ‰è¯¦ç»†çš„çº¹ç†å’Œå‡ ä½•ç»†èŠ‚ï¼Œç¡®ä¿é«˜ä¿çœŸåº¦çš„é‡å»ºã€‚</li>
<li>PartGSæ¡†æ¶åœ¨DTUã€ShapeNetå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-04e7cca3a50dcd816fbd5c82788ad205.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b357e318ab97e2af9b9d74f906b3b00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bf2582665fec395d86c4812b0452fd1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Simple-RF-Regularizing-Sparse-Input-Radiance-Fields-with-Simpler-Solutions"><a href="#Simple-RF-Regularizing-Sparse-Input-Radiance-Fields-with-Simpler-Solutions" class="headerlink" title="Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler   Solutions"></a>Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler   Solutions</h2><p><strong>Authors:Nagabhushan Somraj, Sai Harsha Mupparaju, Adithyan Karanayil, Rajiv Soundararajan</strong></p>
<p>Neural Radiance Fields (NeRF) show impressive performance in photo-realistic free-view rendering of scenes. Recent improvements on the NeRF such as TensoRF and ZipNeRF employ explicit models for faster optimization and rendering, as compared to the NeRF that employs an implicit representation. However, both implicit and explicit radiance fields require dense sampling of images in the given scene. Their performance degrades significantly when only a sparse set of views is available. Researchers find that supervising the depth estimated by a radiance field helps train it effectively with fewer views. The depth supervision is obtained either using classical approaches or neural networks pre-trained on a large dataset. While the former may provide only sparse supervision, the latter may suffer from generalization issues. As opposed to the earlier approaches, we seek to learn the depth supervision by designing augmented models and training them along with the main radiance field. Further, we aim to design a framework of regularizations that can work across different implicit and explicit radiance fields. We observe that certain features of these radiance field models overfit to the observed images in the sparse-input scenario. Our key finding is that reducing the capability of the radiance fields with respect to positional encoding, the number of decomposed tensor components or the size of the hash table, constrains the model to learn simpler solutions, which estimate better depth in certain regions. By designing augmented models based on such reduced capabilities, we obtain better depth supervision for the main radiance field. We achieve state-of-the-art view-synthesis performance with sparse input views on popular datasets containing forward-facing and 360$^\circ$ scenes by employing the above regularizations. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨åœºæ™¯çš„å…‰å†™å®è‡ªç”±è§†å›¾æ¸²æŸ“ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚æœ€è¿‘çš„NeRFæ”¹è¿›ï¼Œå¦‚TensoRFå’ŒZipNeRFï¼Œé‡‡ç”¨æ˜¾å¼æ¨¡å‹è¿›è¡Œæ›´å¿«çš„ä¼˜åŒ–å’Œæ¸²æŸ“ï¼Œä¸ä¹‹å‰é‡‡ç”¨éšå¼è¡¨ç¤ºçš„NeRFå½¢æˆå¯¹æ¯”ã€‚ç„¶è€Œï¼Œéšå¼å’Œæ˜¾å¼è¾å°„åœºéƒ½éœ€è¦å¯¹ç»™å®šåœºæ™¯ä¸­çš„å›¾åƒè¿›è¡Œå¯†é›†é‡‡æ ·ã€‚å½“åªæœ‰å°‘é‡ç¨€ç–è§†å›¾å¯ç”¨æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ç ”ç©¶äººå‘˜å‘ç°ï¼Œé€šè¿‡æ·±åº¦ç›‘ç£è®­ç»ƒè¾å°„åœºæœ‰åŠ©äºä½¿ç”¨è¾ƒå°‘çš„è§†å›¾è¿›è¡Œæœ‰æ•ˆçš„è®­ç»ƒã€‚æ·±åº¦ç›‘ç£æ˜¯é€šè¿‡ç»å…¸æ–¹æ³•æˆ–ä½¿ç”¨å¤§å‹æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒç¥ç»ç½‘ç»œè·å¾—çš„ã€‚è™½ç„¶å‰è€…å¯èƒ½åªèƒ½æä¾›ç¨€ç–çš„ç›‘ç£ï¼Œä½†åè€…å¯èƒ½ä¼šé­å—æ³›åŒ–é—®é¢˜ã€‚ä¸æ—©æœŸçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬è¯•å›¾é€šè¿‡è®¾è®¡å¢å¼ºæ¨¡å‹å¹¶ä¸ä¸»è¦çš„è¾å°„åœºä¸€èµ·è®­ç»ƒæ¥å­¦ä¹ æ·±åº¦ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ—¨åœ¨è®¾è®¡ä¸€ä¸ªå¯ä»¥åœ¨ä¸åŒéšå¼å’Œæ˜¾å¼è¾å°„åœºä¹‹é—´å·¥ä½œçš„æ­£åˆ™åŒ–æ¡†æ¶ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè¿™äº›è¾å°„åœºæ¨¡å‹çš„æŸäº›ç‰¹å¾åœ¨ç¨€ç–è¾“å…¥çš„æƒ…å†µä¸‹è¿‡åº¦æ‹Ÿåˆè§‚å¯Ÿåˆ°çš„å›¾åƒã€‚æˆ‘ä»¬çš„å…³é”®å‘ç°æ˜¯ï¼Œé€šè¿‡å‡å°‘è¾å°„åœºç›¸å¯¹äºä½ç½®ç¼–ç çš„èƒ½åŠ›ã€åˆ†è§£çš„å¼ é‡ç»„ä»¶çš„æ•°é‡æˆ–å“ˆå¸Œè¡¨çš„å¤§å°ï¼Œå¯ä»¥çº¦æŸæ¨¡å‹å­¦ä¹ æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™åœ¨æŸäº›åŒºåŸŸä¸­ä¼°è®¡å‡ºæ›´å¥½çš„æ·±åº¦ã€‚åŸºäºè¿™ç§å‡å°‘çš„èƒ½åŠ›è®¾è®¡å¢å¼ºæ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºä¸»è¦çš„è¾å°„åœºè·å¾—æ›´å¥½çš„æ·±åº¦ç›‘ç£ã€‚é€šè¿‡é‡‡ç”¨ä¸Šè¿°æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨åŒ…å«æ­£é¢å’Œ360Â°åœºæ™¯æµè¡Œæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è§†å›¾åˆæˆæ€§èƒ½ï¼Œå¹¶åœ¨ç¨€ç–è¾“å…¥è§†å›¾ä¸Šè¡¨ç°è‰¯å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.19015v4">PDF</a> The source code for our model can be found on our project page:   <a target="_blank" rel="noopener" href="https://nagabhushansn95.github.io/publications/2024/Simple-RF.html">https://nagabhushansn95.github.io/publications/2024/Simple-RF.html</a>. Extension   of arXiv:2309.03955</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨åœºæ™¯çš„è‡ªç”±è§†è§’æ¸²æŸ“ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ã€‚æœ€è¿‘çš„æ”¹è¿›æ–¹æ³•å¦‚TensoRFå’ŒZipNeRFé‡‡ç”¨æ˜¾å¼æ¨¡å‹å®ç°æ›´å¿«çš„ä¼˜åŒ–å’Œæ¸²æŸ“ï¼Œç›¸æ¯”äºNeRFçš„éšå¼è¡¨ç¤ºæ³•ã€‚ç„¶è€Œï¼Œéšå¼å’Œæ˜¾å¼è¾å°„åœºéƒ½éœ€è¦å¯¹åœºæ™¯è¿›è¡Œå¯†é›†çš„å›¾åƒé‡‡æ ·ã€‚å½“åªæœ‰ç¨€ç–çš„è§†å›¾å¯ç”¨æ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡æ·±åº¦ç›‘ç£è®­ç»ƒè¾å°„åœºï¼Œå³ä½¿ä½¿ç”¨è¾ƒå°‘çš„è§†å›¾ä¹Ÿèƒ½æœ‰æ•ˆè®­ç»ƒã€‚æ·±åº¦ç›‘ç£å¯é€šè¿‡ä¼ ç»Ÿæ–¹æ³•æˆ–åœ¨å¤§æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ç¥ç»ç½‘ç»œè·å¾—ã€‚å‰è€…å¯èƒ½åªèƒ½æä¾›ç¨€ç–çš„ç›‘ç£ï¼Œè€Œåè€…å¯èƒ½é¢ä¸´æ³›åŒ–é—®é¢˜ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯»æ±‚é€šè¿‡è®¾è®¡å¢å¼ºæ¨¡å‹å¹¶ä¸ä¸»è¾å°„åœºä¸€èµ·è®­ç»ƒæ¥å­¦ä¹ æ·±åº¦ç›‘ç£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ—¨åœ¨è®¾è®¡ä¸€ä¸ªå¯ä»¥åœ¨ä¸åŒéšå¼å’Œæ˜¾å¼è¾å°„åœºä¹‹é—´å·¥ä½œçš„æ­£åˆ™åŒ–æ¡†æ¶ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè¿™äº›è¾å°„åœºæ¨¡å‹çš„æŸäº›ç‰¹å¾åœ¨ç¨€ç–è¾“å…¥çš„æƒ…å†µä¸‹ä¼šå¯¹è§‚å¯Ÿåˆ°çš„å›¾åƒè¿‡åº¦æ‹Ÿåˆã€‚æˆ‘ä»¬çš„å…³é”®å‘ç°æ˜¯ï¼Œé€šè¿‡å‡å°‘è¾å°„åœºåœ¨ä½ç½®ç¼–ç ã€åˆ†è§£å¼ é‡ç»„ä»¶çš„æ•°é‡æˆ–å“ˆå¸Œè¡¨å¤§å°æ–¹é¢çš„èƒ½åŠ›ï¼Œå¯ä»¥çº¦æŸæ¨¡å‹å­¦ä¹ æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œä»è€Œåœ¨ç‰¹å®šåŒºåŸŸæ›´å¥½åœ°ä¼°è®¡æ·±åº¦ã€‚åŸºäºè¿™ç§å‡å°‘çš„èƒ½åŠ›è®¾è®¡å¢å¼ºæ¨¡å‹ï¼Œæˆ‘ä»¬ä¸ºä¸»è¦çš„è¾å°„åœºè·å¾—äº†æ›´å¥½çš„æ·±åº¦ç›‘ç£ã€‚é€šè¿‡é‡‡ç”¨ä¸Šè¿°æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨æµè¡Œçš„æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç¨€ç–è¾“å…¥è§†å›¾åˆæˆæ€§èƒ½ï¼ŒåŒ…æ‹¬æ­£é¢å’Œ360Â°åœºæ™¯ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>NeRFåŠå…¶æ”¹è¿›æ–¹æ³•åœ¨åœºæ™¯çš„è‡ªç”±è§†è§’æ¸²æŸ“ä¸­æœ‰å‡ºè‰²è¡¨ç°ï¼Œä½†ç¨€ç–è§†å›¾ä¸‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æ·±åº¦ç›‘ç£å¯¹äºè®­ç»ƒNeRFæ¨¡å‹åœ¨å°‘é‡è§†å›¾ä¸‹æœ‰æ•ˆã€‚</li>
<li>ç°æœ‰æ·±åº¦ç›‘ç£æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ç¨€ç–ç›‘ç£æˆ–æ³›åŒ–é—®é¢˜ã€‚</li>
<li>æå‡ºé€šè¿‡è®¾è®¡å…·æœ‰çº¦æŸèƒ½åŠ›çš„å¢å¼ºæ¨¡å‹æ¥å­¦ä¹ æ·±åº¦ç›‘ç£ï¼Œé€‚ç”¨äºéšå¼å’Œæ˜¾å¼è¾å°„åœºã€‚</li>
<li>å‡å°‘è¾å°„åœºæ¨¡å‹åœ¨æŸäº›æ–¹é¢çš„èƒ½åŠ›ï¼Œå¦‚ä½ç½®ç¼–ç ã€å¼ é‡ç»„ä»¶æ•°é‡æˆ–å“ˆå¸Œè¡¨å¤§å°ï¼Œæœ‰åŠ©äºåœ¨ç‰¹å®šåŒºåŸŸæ›´å¥½åœ°ä¼°è®¡æ·±åº¦ã€‚</li>
<li>é‡‡ç”¨æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå®ç°äº†åœ¨ç¨€ç–è¾“å…¥è§†å›¾ä¸‹çš„å…ˆè¿›è§†å›¾åˆæˆæ€§èƒ½ã€‚</li>
<li>åœ¨åŒ…å«æ­£é¢å’Œ360Â°åœºæ™¯çš„ä¸»æµæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.19015">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1edae626576926f90b6fba1b595dbd71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-658ed6c3dd3d2df35c4e3a36c9bb1853.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5c087f5080d1faf84cad1dd48b42a95.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4cf9c2bc6af6f33a9d63db6f06aefbd6.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Imagine for Me Creative Conceptual Blending of Real Images and Text via   Blended Attention
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1c6c7ff641e2e607d996db2ab0a9b6f8.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  MILo Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient   Surface Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
