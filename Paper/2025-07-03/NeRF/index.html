<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2025-07-03  AttentionGS Towards Initialization-Free 3D Gaussian Splatting via   Structural Attention">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-8316847a61ca9ce8a873eb8dcbaa3c8a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-03-更新"><a href="#2025-07-03-更新" class="headerlink" title="2025-07-03 更新"></a>2025-07-03 更新</h1><h2 id="AttentionGS-Towards-Initialization-Free-3D-Gaussian-Splatting-via-Structural-Attention"><a href="#AttentionGS-Towards-Initialization-Free-3D-Gaussian-Splatting-via-Structural-Attention" class="headerlink" title="AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via   Structural Attention"></a>AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via   Structural Attention</h2><p><strong>Authors:Ziao Liu, Zhenjia Li, Yifeng Shi, Xiangang Li</strong></p>
<p>3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance Fields (NeRF), excelling in complex scene reconstruction and efficient rendering. However, it relies on high-quality point clouds from Structure-from-Motion (SfM), limiting its applicability. SfM also fails in texture-deficient or constrained-view scenarios, causing severe degradation in 3DGS reconstruction. To address this limitation, we propose AttentionGS, a novel framework that eliminates the dependency on high-quality initial point clouds by leveraging structural attention for direct 3D reconstruction from randomly initialization. In the early training stage, we introduce geometric attention to rapidly recover the global scene structure. As training progresses, we incorporate texture attention to refine fine-grained details and enhance rendering quality. Furthermore, we employ opacity-weighted gradients to guide Gaussian densification, leading to improved surface reconstruction. Extensive experiments on multiple benchmark datasets demonstrate that AttentionGS significantly outperforms state-of-the-art methods, particularly in scenarios where point cloud initialization is unreliable. Our approach paves the way for more robust and flexible 3D Gaussian Splatting in real-world applications. </p>
<blockquote>
<p>3D高斯涂抹（3DGS）是神经辐射场（NeRF）的有力替代方案，在复杂场景重建和高效渲染方面表现出色。然而，它依赖于运动结构（SfM）的高质量点云，限制了其适用性。SfM在纹理缺失或视角受限的场景中也会失效，导致3DGS重建严重退化。为了解决这一局限性，我们提出了AttentionGS这一新型框架，通过利用结构注意力，实现从随机初始化进行直接3D重建，消除了对高质量初始点云的依赖。在训练初期，我们引入几何注意力，快速恢复全局场景结构。随着训练的进行，我们结合纹理注意力，细化细节，提高渲染质量。此外，我们还采用不透明度加权梯度引导高斯稠密化，提高了表面重建效果。在多个基准数据集上的大量实验表明，AttentionGS显著优于最先进的方法，特别是在点云初始化不可靠的场景中。我们的方法为更稳健和灵活的3D高斯涂抹技术在现实世界应用奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23611v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于注意力机制的改进型3D高斯插值方法（AttentionGS），用于从随机初始化进行直接三维重建，解决结构从运动（SfM）对高质量初始点云的依赖问题。该方法通过引入几何注意力和纹理注意力机制，在训练初期快速恢复全局场景结构，并随着训练进展优化细节，提高渲染质量。此外，还采用基于不透明度加权的梯度引导高斯密集化技术，改善表面重建效果。实验证明，AttentionGS在点云初始化不可靠的场景中表现优于现有方法，为实际应用中更稳健灵活的3D高斯插值方法开辟道路。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>介绍了基于注意力机制的改进型三维重建方法AttentionGS。</li>
<li>AttentionGS解决了对高质量初始点云的依赖问题，可以通过随机初始化进行直接三维重建。</li>
<li>该方法利用几何注意力和纹理注意力机制恢复场景结构并优化细节。</li>
<li>采用基于不透明度加权的梯度引导高斯密集化技术，提高表面重建质量。</li>
<li>AttentionGS在点云初始化不可靠的场景中表现优异。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23611">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ad3aa318d3a239901bdf263109cc9f16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47ffdc35cbe002cf5d2c0ebcc5dd0acd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb373851513c8a57bdbcd4a7fac1d8cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef675a5e8826902351fdf54f1f16b97e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69f58f2f818926d946b958e83ab8cac6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dynamic-View-Synthesis-from-Small-Camera-Motion-Videos"><a href="#Dynamic-View-Synthesis-from-Small-Camera-Motion-Videos" class="headerlink" title="Dynamic View Synthesis from Small Camera Motion Videos"></a>Dynamic View Synthesis from Small Camera Motion Videos</h2><p><strong>Authors:Huiqiang Sun, Xingyi Li, Juewen Peng, Liao Shen, Zhiguo Cao, Ke Xian, Guosheng Lin</strong></p>
<p>Novel view synthesis for dynamic $3$D scenes poses a significant challenge. Many notable efforts use NeRF-based approaches to address this task and yield impressive results. However, these methods rely heavily on sufficient motion parallax in the input images or videos. When the camera motion range becomes limited or even stationary (i.e., small camera motion), existing methods encounter two primary challenges: incorrect representation of scene geometry and inaccurate estimation of camera parameters. These challenges make prior methods struggle to produce satisfactory results or even become invalid. To address the first challenge, we propose a novel Distribution-based Depth Regularization (DDR) that ensures the rendering weight distribution to align with the true distribution. Specifically, unlike previous methods that use depth loss to calculate the error of the expectation, we calculate the expectation of the error by using Gumbel-softmax to differentiably sample points from discrete rendering weight distribution. Additionally, we introduce constraints that enforce the volume density of spatial points before the object boundary along the ray to be near zero, ensuring that our model learns the correct geometry of the scene. To demystify the DDR, we further propose a visualization tool that enables observing the scene geometry representation at the rendering weight level. For the second challenge, we incorporate camera parameter learning during training to enhance the robustness of our model to camera parameters. We conduct extensive experiments to demonstrate the effectiveness of our approach in representing scenes with small camera motion input, and our results compare favorably to state-of-the-art methods. </p>
<blockquote>
<p>动态三维场景的新型视图合成是一个巨大的挑战。许多引人注目的努力使用基于NeRF的方法来解决这项任务，并产生了令人印象深刻的结果。然而，这些方法在很大程度上依赖于输入图像或视频中的充足运动视差。当相机运动范围变得有限甚至静止（即相机运动很小）时，现有方法面临两个主要挑战：场景几何表示不正确和相机参数估计不准确。这些挑战使得现有方法难以产生令人满意的结果，甚至变得无效。为了解决第一个挑战，我们提出了一种基于分布的深度正则化（DDR）方法，确保渲染权重分布与真实分布对齐。具体来说，与以前使用深度损失来计算期望误差的方法不同，我们使用Gumbel-softmax可微采样点来计算离散渲染权重分布的误差期望。此外，我们引入了约束条件，强制沿光线方向在物体边界之前的空间点的体积密度接近零，确保我们的模型学习场景的正确几何形状。为了揭示DDR的神秘性，我们进一步开发了一个可视化工具，可以观察到渲染权重级别的场景几何表示。对于第二个挑战，我们在训练过程中加入了相机参数学习，以提高模型对相机参数的稳健性。我们进行了大量实验，证明了我们方法在表示具有小相机运动输入的场景方面的有效性，我们的结果与最先进的方法相比具有竞争力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23153v1">PDF</a> Accepted by TVCG</p>
<p><strong>Summary</strong></p>
<p>该文本提出了一种针对动态三维场景的新型视图合成方法，通过基于NeRF的方法解决此任务并获得了令人印象深刻的结果。然而，现有方法在小范围相机运动的情况下面临两大挑战：场景几何的不正确表示和相机参数的不准确估计。为了解决这些问题，提出了基于分布的深度正则化（DDR）和一种新的可视化工具，能增强模型对相机参数的鲁棒性，并在场景几何表示方面取得了显著成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动态三维场景的新型视图合成是一个重大挑战，现有方法主要依赖NeRF技术解决此问题。</li>
<li>当相机运动范围有限或静止时，现有方法面临两大挑战：场景几何表示不准确和相机参数估计不准确。</li>
<li>提出了一种基于分布的深度正则化（DDR）方法，确保渲染权重分布与真实分布对齐。</li>
<li>DDR通过Gumbel-softmax可微采样点来计算误差的期望值，同时引入约束以强化场景几何的正确性。</li>
<li>为了更好地理解DDR，提出了一种新的可视化工具来观察场景几何表示在渲染权重层面的信息。</li>
<li>模型在训练过程中结合了相机参数学习，提高了对相机参数的鲁棒性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23153">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c1c4a3950eddfcc480f66045cc8fa1e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8dfa0d24eaac831ca190b08c16bcb69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df3478b30b33979106c1f0cc4a8b889c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d10bab0c3488ce679c7dbef773752cf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2dfca45baec77ea9bc143e7d4e68cb5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="UnMix-NeRF-Spectral-Unmixing-Meets-Neural-Radiance-Fields"><a href="#UnMix-NeRF-Spectral-Unmixing-Meets-Neural-Radiance-Fields" class="headerlink" title="UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields"></a>UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields</h2><p><strong>Authors:Fabian Perez, Sara Rojas, Carlos Hinojosa, Hoover Rueda-Chacón, Bernard Ghanem</strong></p>
<p>Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic material properties. This limitation restricts accurate material perception, which is crucial for robotics, augmented reality, simulation, and other applications. We introduce UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling joint hyperspectral novel view synthesis and unsupervised material segmentation. Our method models spectral reflectance via diffuse and specular components, where a learned dictionary of global endmembers represents pure material signatures, and per-point abundances capture their distribution. For material segmentation, we use spectral signature predictions along learned endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF enables scene editing by modifying learned endmember dictionaries for flexible material-based appearance manipulation. Extensive experiments validate our approach, demonstrating superior spectral reconstruction and material segmentation to existing methods. Project page: <a target="_blank" rel="noopener" href="https://www.factral.co/UnMix-NeRF">https://www.factral.co/UnMix-NeRF</a>. </p>
<blockquote>
<p>基于神经辐射场（NeRF）的分割方法主要关注对象语义，并仅依赖于RGB数据，缺乏内在材料属性。这一局限性限制了材料感知的准确性，对于机器人、增强现实、模拟和其他应用而言，这是至关重要的。我们引入了UnMix-NeRF框架，它将光谱混合技术集成到NeRF中，实现了超光谱新视角合成和无人监督材料分割的联合处理。我们的方法通过漫反射和镜面反射成分对光谱反射进行建模，其中通过全局端元学习字典表示纯材料特征，而每点的丰度则捕捉其分布。对于材料分割，我们使用沿学习端元的谱特征预测，实现无人监督的材料聚类。此外，UnMix-NeRF通过修改学习端元字典，实现场景编辑，进行灵活的材料外观操作。大量实验验证了我们的方法，在光谱重建和材料分割方面优于现有方法。项目页面：<a target="_blank" rel="noopener" href="https://www.factral.co/UnMix-NeRF%E3%80%82">https://www.factral.co/UnMix-NeRF。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21884v1">PDF</a> Paper accepted at ICCV 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于NeRF的UnMix-NeRF框架，该框架结合了光谱混合技术，实现了联合超光谱新视角合成和无监督材料分割。通过模拟材料的反射光谱，UnMix-NeRF使用全局端元组成的字典表示纯材料特征，并通过每点的丰度捕捉其分布。对于材料分割，它使用光谱特征预测和学习的端元进行无监督材料聚类。此外，UnMix-NeRF还能通过修改学习的端元字典实现场景编辑，进行灵活的材料外观操作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UnMix-NeRF结合了光谱混合技术，实现了NeRF在超光谱新视角合成和无监督材料分割方面的突破。</li>
<li>通过模拟材料的反射光谱，UnMix-NeRF使用全局端元字典表示纯材料特征。</li>
<li>UnMix-NeRF采用每点的丰度捕捉材料分布。</li>
<li>利用光谱特征预测和学习的端元进行无监督材料聚类，实现材料分割。</li>
<li>UnMix-NeRF支持场景编辑，通过修改学习的端元字典实现灵活的材料外观操作。</li>
<li>实验验证，UnMix-NeRF在光谱重建和材料分割方面优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21884">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-84614942984b56ceba631d7293e6a313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f6a7fd7c3a1f9b276086618b955c2a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-447fdaa6bf12ed632ddae3bf47bba947.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ICP-3DGS-SfM-free-3D-Gaussian-Splatting-for-Large-scale-Unbounded-Scenes"><a href="#ICP-3DGS-SfM-free-3D-Gaussian-Splatting-for-Large-scale-Unbounded-Scenes" class="headerlink" title="ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded   Scenes"></a>ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded   Scenes</h2><p><strong>Authors:Chenhao Zhang, Yezhi Shen, Fengqing Zhu</strong></p>
<p>In recent years, neural rendering methods such as NeRFs and 3D Gaussian Splatting (3DGS) have made significant progress in scene reconstruction and novel view synthesis. However, they heavily rely on preprocessed camera poses and 3D structural priors from structure-from-motion (SfM), which are challenging to obtain in outdoor scenarios. To address this challenge, we propose to incorporate Iterative Closest Point (ICP) with optimization-based refinement to achieve accurate camera pose estimation under large camera movements. Additionally, we introduce a voxel-based scene densification approach to guide the reconstruction in large-scale scenes. Experiments demonstrate that our approach ICP-3DGS outperforms existing methods in both camera pose estimation and novel view synthesis across indoor and outdoor scenes of various scales. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/Chenhao-Z/ICP-3DGS">https://github.com/Chenhao-Z/ICP-3DGS</a>. </p>
<blockquote>
<p>近年来，神经渲染方法，如NeRF和三维高斯溅出（3DGS），在场景重建和新颖视角合成方面取得了显著进展。然而，它们严重依赖于从运动恢复结构（SfM）预处理的相机姿态和三维结构先验，这在户外场景中很难获取。为了应对这一挑战，我们提出结合迭代最近点（ICP）和优化精修方法来实现大相机运动下的准确相机姿态估计。此外，我们还介绍了一种基于体素的场景稠化方法，以指导大规模场景的重建。实验表明，我们的ICP-3DGS方法在室内外各种规模的场景的相机姿态估计和新颖视角合成方面都优于现有方法。源代码可在<a target="_blank" rel="noopener" href="https://github.com/Chenhao-Z/ICP-3DGS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Chenhao-Z/ICP-3DGS获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21629v1">PDF</a> 6 pages, Source code is available at   <a target="_blank" rel="noopener" href="https://github.com/Chenhao-Z/ICP-3DGS">https://github.com/Chenhao-Z/ICP-3DGS</a>. To appear at ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>近年来，神经渲染方法如NeRF和3D高斯拼贴（3DGS）在场景重建和视角合成方面取得显著进展，但它们依赖于运动结构（SfM）的预处理相机姿态和3D结构先验，这在户外场景中获取具有挑战性。为应对此挑战，我们结合迭代最近点（ICP）与优化精修法，在大范围相机移动下实现准确的相机姿态估计。此外，我们引入了基于体素的场景密集化方法，以指导大规模场景的重建。实验证明，我们的ICP-3DGS方法在相机姿态估计和视角合成方面表现优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经渲染方法如NeRF和3DGS在场景重建和视角合成上取得进展。</li>
<li>现有方法依赖SfM的预处理数据和结构先验，这在户外场景中是挑战。</li>
<li>提出结合ICP和优化精修法，实现大移动下的准确相机姿态估计。</li>
<li>引入基于体素的场景密集化方法，用于指导大规模场景的重建。</li>
<li>ICP-3DGS方法在相机姿态估计和视角合成上表现优越。</li>
<li>方法适用于室内和室外各种规模的场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-539a9603c157ad7fd6c29ff2a2664f44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99fb175bf6adfd097538f639c45f23c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b5b7ffe61cd8a850dd58ef27bdbb822.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01276fb61736e52f7009b6acf2e4681b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b36fe35f156410985594ca6ef22d533.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ae44bdca555c05aa9792ddcc43d0609.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="HumanGif-Single-View-Human-Diffusion-with-Generative-Prior"><a href="#HumanGif-Single-View-Human-Diffusion-with-Generative-Prior" class="headerlink" title="HumanGif: Single-View Human Diffusion with Generative Prior"></a>HumanGif: Single-View Human Diffusion with Generative Prior</h2><p><strong>Authors:Shoukang Hu, Takuya Narihira, Kazumi Fukuda, Ryosuke Sawata, Takashi Shibuya, Yuki Mitsufuji</strong></p>
<p>Previous 3D human creation methods have made significant progress in synthesizing view-consistent and temporally aligned results from sparse-view images or monocular videos. However, it remains challenging to produce perpetually realistic, view-consistent, and temporally coherent human avatars from a single image, as limited information is available in the single-view input setting. Motivated by the success of 2D character animation, we propose HumanGif, a single-view human diffusion model with generative prior. Specifically, we formulate the single-view-based 3D human novel view and pose synthesis as a single-view-conditioned human diffusion process, utilizing generative priors from foundational diffusion models to complement the missing information. To ensure fine-grained and consistent novel view and pose synthesis, we introduce a Human NeRF module in HumanGif to learn spatially aligned features from the input image, implicitly capturing the relative camera and human pose transformation. Furthermore, we introduce an image-level loss during optimization to bridge the gap between latent and image spaces in diffusion models. Extensive experiments on RenderPeople, DNA-Rendering, THuman 2.1, and TikTok datasets demonstrate that HumanGif achieves the best perceptual performance, with better generalizability for novel view and pose synthesis. </p>
<blockquote>
<p>之前的3D人物创建方法在合成视图一致且时间上对齐的结果方面取得了显著进展，这些结果来源于稀疏视图图像或单目视频。然而，从单张图像生成永久逼真的、视图一致且时间上连贯的人物化身仍然是一个挑战，因为在单视图输入设置中可用的信息有限。受到二维角色动画成功的启发，我们提出了HumanGif，这是一个带有生成先验的单视图人类扩散模型。具体来说，我们将基于单视图的3D人物新颖视角和姿态合成制定为受单视图条件约束的人类扩散过程，利用基础扩散模型的生成先验来补充缺失的信息。为了确保精细且一致的全新视角和姿态合成，我们在HumanGif中引入了Human NeRF模块，从输入图像中学习空间对齐的特征，隐式捕获相对相机和人物姿态变换。此外，我们在优化过程中引入了图像级损失，以弥合扩散模型中的潜在空间和图像空间之间的差距。在RenderPeople、DNA-Rendering、THuman 2.1和TikTok数据集上的大量实验表明，HumanGif在感知性能上表现最佳，对于新颖视角和姿态合成具有更好的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12080v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://skhu101.github.io/HumanGif/">https://skhu101.github.io/HumanGif/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于单视角的扩散模型HumanGif，用于创建人类角色动画。该模型结合了二维角色动画的成功经验，通过引入生成先验信息来弥补单视角输入信息不足的问题。HumanGif使用Human NeRF模块学习从输入图像中提取的空间对齐特征，并隐式捕捉相机和人类姿势的相对变换。此外，还引入了图像级损失优化模型，缩小潜在空间和图像空间之间的差距。实验证明，HumanGif在感知性能上表现最佳，具有出色的新颖视角和姿态合成的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HumanGif是一个基于单视角的人类扩散模型，利用生成先验信息合成新颖视角和姿态的人类角色动画。</li>
<li>该模型通过引入Human NeRF模块学习从输入图像中提取空间对齐特征，隐式捕捉相机和人类姿势的相对变换。</li>
<li>HumanGif采用图像级损失优化模型，以提高对未知视角和姿态的泛化能力。</li>
<li>实验证明，HumanGif在感知性能上表现最佳，能够提供高质量的人类角色动画。</li>
<li>该模型适用于多种数据集，包括RenderPeople、DNA-Rendering、THuman 2.1和TikTok等。</li>
<li>HumanGif的成功源于其结合二维角色动画经验并引入生成先验信息的创新方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e85a13c07d28886374a04ebea374f311.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ef2f3e3ebf3fcfb8acb1c9334475e50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7a7845aedaf1530dc23f922b4fb86a4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Grounding-Creativity-in-Physics-A-Brief-Survey-of-Physical-Priors-in-AIGC"><a href="#Grounding-Creativity-in-Physics-A-Brief-Survey-of-Physical-Priors-in-AIGC" class="headerlink" title="Grounding Creativity in Physics: A Brief Survey of Physical Priors in   AIGC"></a>Grounding Creativity in Physics: A Brief Survey of Physical Priors in   AIGC</h2><p><strong>Authors:Siwei Meng, Yawei Luo, Ping Liu</strong></p>
<p>Recent advancements in AI-generated content have significantly improved the realism of 3D and 4D generation. However, most existing methods prioritize appearance consistency while neglecting underlying physical principles, leading to artifacts such as unrealistic deformations, unstable dynamics, and implausible objects interactions. Incorporating physics priors into generative models has become a crucial research direction to enhance structural integrity and motion realism. This survey provides a review of physics-aware generative methods, systematically analyzing how physical constraints are integrated into 3D and 4D generation. First, we examine recent works in incorporating physical priors into static and dynamic 3D generation, categorizing methods based on representation types, including vision-based, NeRF-based, and Gaussian Splatting-based approaches. Second, we explore emerging techniques in 4D generation, focusing on methods that model temporal dynamics with physical simulations. Finally, we conduct a comparative analysis of major methods, highlighting their strengths, limitations, and suitability for different materials and motion dynamics. By presenting an in-depth analysis of physics-grounded AIGC, this survey aims to bridge the gap between generative models and physical realism, providing insights that inspire future research in physically consistent content generation. </p>
<blockquote>
<p>近年来，人工智能生成内容方面的最新进展极大地提高了3D和4D生成的逼真度。然而，大多数现有方法优先考虑外观的一致性，却忽略了基本的物理原理，导致出现不真实的变形、不稳定的动态以及不合理的物体交互等伪迹。因此，将物理先验知识融入生成模型已成为增强结构完整性和运动逼真度的关键研究方向。本文综述了物理感知生成方法，系统分析了如何将物理约束融入3D和4D生成。首先，我们研究了近期将物理先验知识融入静态和动态3D生成的工作，按表示类型对方法进行分类，包括基于视觉、基于NeRF和基于高斯拼贴的方法。其次，我们探索了4D生成的新兴技术，重点关注利用物理模拟对时间动态进行建模的方法。最后，我们对主要方法进行了比较分析，重点介绍了它们各自的优势、局限性和在不同材料和运动动力学方面的适用性。通过对基于物理原理的人工智能生成内容的深入分析，本综述旨在缩小生成模型和物理逼真度之间的差距，提供见解，为物理一致性内容生成方面的未来研究提供灵感。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07007v3">PDF</a> Accepted by IJCAI 2025 Survey Track</p>
<p><strong>Summary</strong><br>新一代人工智能生成内容的技术进步显著提高了三维（3D）和四维（4D）内容的逼真度。然而，现有方法往往重视外观一致性，忽视了底层物理原理，导致出现不真实的变形、动态不稳定和物体交互不合理等问题。为了增强结构完整性和动作逼真性，将物理先验知识融入生成模型成为重要研究方向。本文综述了物理感知生成方法，系统分析了如何将物理约束融入3D和4D内容生成。文章首先探讨了将物理先验知识融入静态和动态3D生成的方法，按表示类型分类，包括基于视觉、NeRF和Gaussian Splatting的方法。接着，文章探讨了用于建模时间动态的物理模拟的4D生成新兴技术。最后，对主要方法进行了比较分析，突出了其优势、局限性和在不同材料和运动动态中的适用性。本文旨在弥补生成模型和物理真实性之间的鸿沟，提供深入了解并为未来的物理一致性内容生成研究提供启示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI生成的3D和4D内容在逼真度上有了显著的提升。</li>
<li>当前方法过于注重外观一致性，忽视了物理原理，导致存在不真实变形等问题。</li>
<li>将物理先验知识融入生成模型可以增强结构完整性和动作逼真性。</li>
<li>文章综述了物理感知生成方法，并系统分析了如何将物理约束融入3D和4D内容生成。</li>
<li>静态和动态3D生成方法按表示类型分类，包括基于视觉、NeRF和Gaussian Splatting的方法。</li>
<li>4D生成技术主要关注于使用物理模拟建模时间动态。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07007">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8316847a61ca9ce8a873eb8dcbaa3c8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc02149aac6500706268cff41d86f7d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcb9ba7838cf34365a0566c3c008fea1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PoI-A-Filter-to-Extract-Pixel-of-Interest-from-Novel-View-Synthesis-for-Scene-Coordinate-Regression"><a href="#PoI-A-Filter-to-Extract-Pixel-of-Interest-from-Novel-View-Synthesis-for-Scene-Coordinate-Regression" class="headerlink" title="PoI: A Filter to Extract Pixel of Interest from Novel View Synthesis for   Scene Coordinate Regression"></a>PoI: A Filter to Extract Pixel of Interest from Novel View Synthesis for   Scene Coordinate Regression</h2><p><strong>Authors:Feifei Li, Qi Song, Chi Zhang, Hui Shuai, Rui Huang</strong></p>
<p>Novel View Synthesis (NVS) techniques, notably Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), can augment camera pose estimation by extending and diversifying training data. However, images generated by these methods are often plagued by spatial artifacts such as blurring and ghosting, undermining their reliability as training data for camera pose estimation. This limitation is particularly critical for Scene Coordinate Regression (SCR) methods, which aim at pixel-level 3D coordinate estimation, because rendering artifacts directly lead to estimation inaccuracies. To address this challenge, we propose a dual-criteria filtering mechanism that dynamically identifies and discards suboptimal pixels during training. The dual-criteria filter evaluates two concurrent metrics: (1) real-time SCR reprojection error, and (2) gradient threshold, across the coordinate regression domain. In addition, for visual localization problems in sparse-input scenarios, it becomes even more necessary to use NVS-generated data to assist localization. We design a coarse-to-fine Points of Interest (PoI) variant using sparse-input NVS to solve this problem. Experiments across indoor and outdoor benchmarks confirm our method’s efficacy, achieving state-of-the-art localization accuracy while maintaining computational efficiency. </p>
<blockquote>
<p>新颖视图合成（NVS）技术，特别是神经辐射场（NeRF）和三维高斯喷涂（3DGS），可以通过扩展和多样化训练数据来增强相机姿态估计。然而，由这些方法生成的图像往往受到空间伪影的困扰，如模糊和重影，这降低了它们作为相机姿态估计训练数据的可靠性。这一局限性对于场景坐标回归（SCR）方法尤为重要，因为这些方法旨在进行像素级别的三维坐标估计，渲染伪影直接导致估计不准确。为了解决这一挑战，我们提出了一种双标准过滤机制，该机制在训练过程中动态识别并丢弃次优像素。双标准过滤器评估两个并发指标：（1）实时SCR再投影误差，（2）坐标回归域中的梯度阈值。此外，在稀疏输入的视觉定位问题中，使用NVS生成的数据辅助定位变得尤为必要。我们设计了一种从粗到细的感兴趣点（PoI）变体，使用稀疏输入的NVS来解决这个问题。室内和室外基准测试的实验结果证实了我们的方法的有效性，在保持计算效率的同时实现了最先进的定位精度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04843v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>利用神经辐射场（NeRF）和三维高斯贴片（3DGS）等新型视图合成（NVS）技术，可以增强相机姿态估计。然而，这些方法生成的图像常存在空间伪影问题，如模糊和重影，降低了它们作为训练数据的可靠性。针对场景坐标回归（SCR）方法的渲染伪影导致估计不准确的问题，提出了基于双重标准的过滤机制，该机制在训练过程中动态识别并丢弃非优质像素。此外，针对稀疏输入场景下的视觉定位问题，使用NVS生成的数据辅助定位至关重要。设计了一种从粗到细的感兴趣点（PoI）变体来解决这一问题，并在室内和室外基准测试上进行了实验验证，实现了高定位精度和计算效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NVS技术如NeRF和3DGS能增强相机姿态估计的训练数据多样性和扩展性。</li>
<li>NVS生成的图像存在空间伪影问题，如模糊和重影，影响作为训练数据的可靠性。</li>
<li>双重标准的过滤机制可以动态识别并丢弃非优质像素，提高SCR方法的准确性。</li>
<li>在稀疏输入场景下，使用NVS生成的数据对于视觉定位至关重要。</li>
<li>设计了从粗到细的PoI变体来解决稀疏输入场景下的视觉定位问题。</li>
<li>实验验证了该方法在室内外基准测试上的高效性和高定位精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04843">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aba2a3338028740e67919c961813c4b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-008f4f060e48f7c770ebc5b76c85d03b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e31d40fdfb0e54ba206bd97306511e06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0e770f1dca42eeeeee8b40a3a8f2f60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39a5fd5b8a197e32326661bc38cd5e51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb21def5b5cbeec36a9a12456b99ec9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b57f2ca9edb64a288225e1626988f58.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Wavelet-Diffusion-GAN-for-Image-Super-Resolution"><a href="#A-Wavelet-Diffusion-GAN-for-Image-Super-Resolution" class="headerlink" title="A Wavelet Diffusion GAN for Image Super-Resolution"></a>A Wavelet Diffusion GAN for Image Super-Resolution</h2><p><strong>Authors:Lorenzo Aloisi, Luigi Sigillo, Aurelio Uncini, Danilo Comminiello</strong></p>
<p>In recent years, diffusion models have emerged as a superior alternative to generative adversarial networks (GANs) for high-fidelity image generation, with wide applications in text-to-image generation, image-to-image translation, and super-resolution. However, their real-time feasibility is hindered by slow training and inference speeds. This study addresses this challenge by proposing a wavelet-based conditional Diffusion GAN scheme for Single-Image Super-Resolution (SISR). Our approach utilizes the diffusion GAN paradigm to reduce the timesteps required by the reverse diffusion process and the Discrete Wavelet Transform (DWT) to achieve dimensionality reduction, decreasing training and inference times significantly. The results of an experimental validation on the CelebA-HQ dataset confirm the effectiveness of our proposed scheme. Our approach outperforms other state-of-the-art methodologies successfully ensuring high-fidelity output while overcoming inherent drawbacks associated with diffusion models in time-sensitive applications. </p>
<blockquote>
<p>近年来，扩散模型已经作为生成对抗网络（GANs）在高保真图像生成方面的优越替代方案而出现，广泛应用于文本到图像生成、图像到图像转换和超分辨率等领域。然而，由于其训练和推理速度慢，实时可行性受到限制。本研究通过提出一种基于小波的单一图像超分辨率（SISR）条件扩散GAN方案来解决这一挑战。我们的方法利用扩散GAN范式减少反向扩散过程所需的时间步长和离散小波变换（DWT）来实现降维，从而显著减少训练和推理时间。在CelebA-HQ数据集上的实验验证结果证实了所提出方案的有效性。我们的方法成功超越了其他最先进的方法，在保证高保真输出的同时，克服了扩散模型在时间敏感应用中的固有缺点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17966v2">PDF</a> The paper has been accepted at Italian Workshop on Neural Networks   (WIRN) 2024</p>
<p><strong>Summary</strong><br>扩散模型在高保真图像生成领域展现出优于生成对抗网络（GANs）的优势，广泛应用于文本转图像、图像转图像以及超分辨率等场景。但扩散模型存在训练及推理速度慢的问题。本研究提出一种基于小波变换的条件扩散GAN方案，用于单图像超分辨率（SISR）。该研究利用扩散GAN范式减少反向扩散过程所需的时间步长，并结合离散小波变换（DWT）实现降维，显著缩短训练和推理时间。在CelebA-HQ数据集上的实验验证，确认了所提方案的有效性。该研究在保证高保真输出的同时，克服了扩散模型在时间敏感性应用中的固有缺点，优于其他先进方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型已成为高保真图像生成领域的领先技术，替代了生成对抗网络（GANs）。</li>
<li>扩散模型广泛应用于文本转图像、图像转图像以及超分辨率等场景。</li>
<li>扩散模型存在训练和推理速度慢的问题。</li>
<li>本研究提出了一种基于小波变换的条件扩散GAN方案，用于单图像超分辨率（SISR）。</li>
<li>该方案利用扩散GAN范式减少反向扩散过程的时间步长。</li>
<li>结合离散小波变换（DWT）实现降维，提高了效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17966">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f8c61ed71f06e8e110baf528da95acf8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7987eb632319509c4054fc4bf32519b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-407a35b38a6f7e683a27c6fc037fc2c2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Self-supervised-Learning-of-Hybrid-Part-aware-3D-Representation-of-2D-Gaussians-and-Superquadrics"><a href="#Self-supervised-Learning-of-Hybrid-Part-aware-3D-Representation-of-2D-Gaussians-and-Superquadrics" class="headerlink" title="Self-supervised Learning of Hybrid Part-aware 3D Representation of 2D   Gaussians and Superquadrics"></a>Self-supervised Learning of Hybrid Part-aware 3D Representation of 2D   Gaussians and Superquadrics</h2><p><strong>Authors:Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, Kai Xu</strong></p>
<p>Low-level 3D representations, such as point clouds, meshes, NeRFs and 3D Gaussians, are commonly used for modeling 3D objects and scenes. However, cognitive studies indicate that human perception operates at higher levels and interprets 3D environments by decomposing them into meaningful structural parts, rather than low-level elements like points or voxels. Structured geometric decomposition enhances scene interpretability and facilitates downstream tasks requiring component-level manipulation. In this work, we introduce PartGS, a self-supervised part-aware reconstruction framework that integrates 2D Gaussians and superquadrics to parse objects and scenes into an interpretable decomposition, leveraging multi-view image inputs to uncover 3D structural information. Our method jointly optimizes superquadric meshes and Gaussians by coupling their parameters within a hybrid representation. On one hand, superquadrics enable the representation of a wide range of shape primitives, facilitating flexible and meaningful decompositions. On the other hand, 2D Gaussians capture detailed texture and geometric details, ensuring high-fidelity appearance and geometry reconstruction. Operating in a self-supervised manner, our approach demonstrates superior performance compared to state-of-the-art methods across extensive experiments on the DTU, ShapeNet, and real-world datasets. </p>
<blockquote>
<p>低级别的三维表示，如点云、网格、NeRF和三维高斯等，通常用于对三维物体和场景进行建模。然而，认知研究表明，人类的感知是在更高层次上进行的，通过分解三维环境为有意义的结构部分来进行解读，而非低级别的元素，如点或体素。结构化几何分解增强了场景的可解释性，并促进了需要组件级操作的下游任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10789v3">PDF</a> Accepted by ICCV 2025 Code: <a target="_blank" rel="noopener" href="https://github.com/zhirui-gao/PartGS">https://github.com/zhirui-gao/PartGS</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为PartGS的自监督感知重建框架，该框架利用二维高斯和超级二次曲面来解析对象和场景，将其分解为可解释的结构。通过多视角图像输入，揭示三维结构信息。该方法通过混合表示中的参数耦合来联合优化超级二次曲面网格和高斯，从而实现灵活和有意义的分解，同时保证高保真度的外观和几何重建。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PartGS框架结合二维高斯和超级二次曲面，实现对对象和场景的感知重建。</li>
<li>通过多视角图像输入，揭示三维结构信息。</li>
<li>PartGS框架采用自监督学习方式，无需大量标注数据。</li>
<li>超级二次曲面能够表示多种形状原始，促进灵活和有意义的分解。</li>
<li>二维高斯捕捉详细的纹理和几何细节，确保高保真度的重建。</li>
<li>PartGS框架在DTU、ShapeNet和真实世界数据集上的实验表现优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10789">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-04e7cca3a50dcd816fbd5c82788ad205.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b357e318ab97e2af9b9d74f906b3b00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bf2582665fec395d86c4812b0452fd1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Simple-RF-Regularizing-Sparse-Input-Radiance-Fields-with-Simpler-Solutions"><a href="#Simple-RF-Regularizing-Sparse-Input-Radiance-Fields-with-Simpler-Solutions" class="headerlink" title="Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler   Solutions"></a>Simple-RF: Regularizing Sparse Input Radiance Fields with Simpler   Solutions</h2><p><strong>Authors:Nagabhushan Somraj, Sai Harsha Mupparaju, Adithyan Karanayil, Rajiv Soundararajan</strong></p>
<p>Neural Radiance Fields (NeRF) show impressive performance in photo-realistic free-view rendering of scenes. Recent improvements on the NeRF such as TensoRF and ZipNeRF employ explicit models for faster optimization and rendering, as compared to the NeRF that employs an implicit representation. However, both implicit and explicit radiance fields require dense sampling of images in the given scene. Their performance degrades significantly when only a sparse set of views is available. Researchers find that supervising the depth estimated by a radiance field helps train it effectively with fewer views. The depth supervision is obtained either using classical approaches or neural networks pre-trained on a large dataset. While the former may provide only sparse supervision, the latter may suffer from generalization issues. As opposed to the earlier approaches, we seek to learn the depth supervision by designing augmented models and training them along with the main radiance field. Further, we aim to design a framework of regularizations that can work across different implicit and explicit radiance fields. We observe that certain features of these radiance field models overfit to the observed images in the sparse-input scenario. Our key finding is that reducing the capability of the radiance fields with respect to positional encoding, the number of decomposed tensor components or the size of the hash table, constrains the model to learn simpler solutions, which estimate better depth in certain regions. By designing augmented models based on such reduced capabilities, we obtain better depth supervision for the main radiance field. We achieve state-of-the-art view-synthesis performance with sparse input views on popular datasets containing forward-facing and 360$^\circ$ scenes by employing the above regularizations. </p>
<blockquote>
<p>神经辐射场（NeRF）在场景的光写实自由视图渲染中表现出令人印象深刻的性能。最近的NeRF改进，如TensoRF和ZipNeRF，采用显式模型进行更快的优化和渲染，与之前采用隐式表示的NeRF形成对比。然而，隐式和显式辐射场都需要对给定场景中的图像进行密集采样。当只有少量稀疏视图可用时，它们的性能会显著下降。研究人员发现，通过深度监督训练辐射场有助于使用较少的视图进行有效的训练。深度监督是通过经典方法或使用大型数据集进行预训练神经网络获得的。虽然前者可能只能提供稀疏的监督，但后者可能会遭受泛化问题。与早期的方法不同，我们试图通过设计增强模型并与主要的辐射场一起训练来学习深度监督。此外，我们旨在设计一个可以在不同隐式和显式辐射场之间工作的正则化框架。我们观察到，这些辐射场模型的某些特征在稀疏输入的情况下过度拟合观察到的图像。我们的关键发现是，通过减少辐射场相对于位置编码的能力、分解的张量组件的数量或哈希表的大小，可以约束模型学习更简单的解决方案，这在某些区域中估计出更好的深度。基于这种减少的能力设计增强模型，我们可以为主要的辐射场获得更好的深度监督。通过采用上述正则化方法，我们在包含正面和360°场景流行数据集上实现了最先进的视图合成性能，并在稀疏输入视图上表现良好。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.19015v4">PDF</a> The source code for our model can be found on our project page:   <a target="_blank" rel="noopener" href="https://nagabhushansn95.github.io/publications/2024/Simple-RF.html">https://nagabhushansn95.github.io/publications/2024/Simple-RF.html</a>. Extension   of arXiv:2309.03955</p>
<p><strong>摘要</strong></p>
<p>神经辐射场（NeRF）在场景的自由视角渲染中展现出令人印象深刻的表现。最近的改进方法如TensoRF和ZipNeRF采用显式模型实现更快的优化和渲染，相比于NeRF的隐式表示法。然而，隐式和显式辐射场都需要对场景进行密集的图像采样。当只有稀疏的视图可用时，它们的性能会显著下降。研究发现，通过深度监督训练辐射场，即使使用较少的视图也能有效训练。深度监督可通过传统方法或在大数据集上预训练的神经网络获得。前者可能只能提供稀疏的监督，而后者可能面临泛化问题。相反，我们寻求通过设计增强模型并与主辐射场一起训练来学习深度监督。此外，我们旨在设计一个可以在不同隐式和显式辐射场之间工作的正则化框架。我们观察到，这些辐射场模型的某些特征在稀疏输入的情况下会对观察到的图像过度拟合。我们的关键发现是，通过减少辐射场在位置编码、分解张量组件的数量或哈希表大小方面的能力，可以约束模型学习更简单的解决方案，从而在特定区域更好地估计深度。基于这种减少的能力设计增强模型，我们为主要的辐射场获得了更好的深度监督。通过采用上述正则化方法，我们在流行的数据集上实现了最先进的稀疏输入视图合成性能，包括正面和360°场景。</p>
<p><strong>要点</strong></p>
<ol>
<li>NeRF及其改进方法在场景的自由视角渲染中有出色表现，但稀疏视图下性能下降。</li>
<li>深度监督对于训练NeRF模型在少量视图下有效。</li>
<li>现有深度监督方法存在局限性，如稀疏监督或泛化问题。</li>
<li>提出通过设计具有约束能力的增强模型来学习深度监督，适用于隐式和显式辐射场。</li>
<li>减少辐射场模型在某些方面的能力，如位置编码、张量组件数量或哈希表大小，有助于在特定区域更好地估计深度。</li>
<li>采用正则化方法，实现了在稀疏输入视图下的先进视图合成性能。</li>
<li>在包含正面和360°场景的主流数据集上取得了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.19015">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1edae626576926f90b6fba1b595dbd71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-658ed6c3dd3d2df35c4e3a36c9bb1853.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5c087f5080d1faf84cad1dd48b42a95.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4cf9c2bc6af6f33a9d63db6f06aefbd6.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-07-03  Imagine for Me Creative Conceptual Blending of Real Images and Text via   Blended Attention
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1c6c7ff641e2e607d996db2ab0a9b6f8.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-07-03  MILo Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient   Surface Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28051.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
