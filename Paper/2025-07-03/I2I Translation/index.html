<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-07-03  Subjective Camera Bridging Human Cognition and Visual Reconstruction   through Sequence-Aware Sketch-Guided Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-81188eccb11f963c31609e5b758aae9f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-03-更新"><a href="#2025-07-03-更新" class="headerlink" title="2025-07-03 更新"></a>2025-07-03 更新</h1><h2 id="Subjective-Camera-Bridging-Human-Cognition-and-Visual-Reconstruction-through-Sequence-Aware-Sketch-Guided-Diffusion"><a href="#Subjective-Camera-Bridging-Human-Cognition-and-Visual-Reconstruction-through-Sequence-Aware-Sketch-Guided-Diffusion" class="headerlink" title="Subjective Camera: Bridging Human Cognition and Visual Reconstruction   through Sequence-Aware Sketch-Guided Diffusion"></a>Subjective Camera: Bridging Human Cognition and Visual Reconstruction   through Sequence-Aware Sketch-Guided Diffusion</h2><p><strong>Authors:Haoyang Chen, Dongfang Sun, Caoyuan Ma, Shiqin Wang, Kewei Zhang, Zheng Wang, Zhixiang Wang</strong></p>
<p>We propose Subjective Camera, a human-as-imaging-device paradigm that reconstructs real-world scenes from mental impressions through synergistic use of verbal descriptions and progressive rough sketches. This approach overcomes dual limitations of language ambiguity and sketch abstraction by treating the user’s drawing sequence as priors, effectively translating subjective perceptual expectations into photorealistic images.   Existing approaches face three fundamental barriers: (1) user-specific subjective input biases, (2) huge modality gap between planar sketch and 3D priors in diffusion, and (3) sketch quality-sensitive performance degradation. Current solutions either demand resource-intensive model adaptation or impose impractical requirements on sketch precision.   Our framework addresses these challenges through concept-sequential generation. (1) We establish robust appearance priors through text-reward optimization, and then implement sequence-aware disentangled generation that processes concepts in sketching order; these steps accommodate user-specific subjective expectation in a train-free way. (2) We employ latent optimization that effectively bridges the modality gap between planar sketches and 3D priors in diffusion. (3) Our hierarchical reward-guided framework enables the use of rough sketches without demanding artistic expertise. Comprehensive evaluation across diverse datasets demonstrates that our approach achieves state-of-the-art performance in maintaining both semantic and spatial coherence. </p>
<blockquote>
<p>我们提出了主观相机（Subjective Camera）这一概念，这是一种以人为成像设备范式，通过协同使用语言描述和渐进式粗略草图，从心理印象重建真实场景。该方法通过用户的绘图序列作为先验，克服了语言模糊和草图抽象性的双重局限性，有效地将主观感知期望转化为逼真的图像。现有方法面临三大基本障碍：（1）用户特定的主观输入偏见，（2）平面草图与扩散中三维先验之间的巨大模态差距，（3）草图质量敏感的性能下降。目前的解决方案要么需要大量资源对模型进行适配，要么对草图的精确性提出不切实际的要求。我们的框架通过概念序列生成来解决这些挑战。（1）我们通过文本奖励优化建立稳健的外观先验，然后实现序列感知的分离生成，按草图顺序处理概念；这些步骤以无训练的方式容纳用户特定的主观期望。（2）我们采用潜在优化，有效地缩小了平面草图和扩散中三维先验之间的模态差距。（3）我们的分层奖励引导框架允许使用粗略草图，无需艺术专业知识。在多个数据集上的综合评估表明，我们的方法在保持语义和空间连贯性方面达到了最新性能水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23711v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>我们提出了主观相机这一人类作为成像设备的新范式，它通过协同使用言语描述和渐进的粗略草图，从心理印象重建现实世界场景。这种方法克服了语言模糊和草图抽象性的双重局限性，通过将用户的绘图序列视为先验信息，有效地将主观感知期望转化为逼真的图像。我们的框架通过概念序列生成解决现有挑战，无需特定模型适应或高要求的草图精度要求，同时实现了文本奖励优化、序列感知分离生成以及潜在优化技术。全面评估表明，我们的方法在保持语义和空间连贯性方面达到了最新技术水平。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>提出了Subjective Camera新范式，将人类视为成像设备，结合言语描述和草图重建现实场景。</li>
<li>克服语言模糊和草图抽象性的挑战，将用户绘图序列作为先验信息，转化为真实图像。</li>
<li>现有方法面临用户主观输入偏见、草图与3D先验间的模态差距以及草图质量敏感的性能下降等三大难题。</li>
<li>通过概念序列生成解决挑战，包括建立外观先验、序列感知分离生成等。</li>
<li>采用文本奖励优化和潜在优化技术，缩小了草图与3D先验之间的模态差距。</li>
<li>框架允许使用粗略草图，无需艺术专业技能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23711">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-932ca9759a24a3d800c524f087069a63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c80c8f5289d0fd3cc83fe324d101116.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81188eccb11f963c31609e5b758aae9f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-Markerless-Intraoperative-Tracking-of-Deformable-Spine-Tissue"><a href="#Towards-Markerless-Intraoperative-Tracking-of-Deformable-Spine-Tissue" class="headerlink" title="Towards Markerless Intraoperative Tracking of Deformable Spine Tissue"></a>Towards Markerless Intraoperative Tracking of Deformable Spine Tissue</h2><p><strong>Authors:Connor Daly, Elettra Marconi, Marco Riva, Jinendra Ekanayake, Daniel S. Elson, Ferdinando Rodriguez y Baena</strong></p>
<p>Consumer-grade RGB-D imaging for intraoperative orthopedic tissue tracking is a promising method with high translational potential. Unlike bone-mounted tracking devices, markerless tracking can reduce operating time and complexity. However, its use has been limited to cadaveric studies. This paper introduces the first real-world clinical RGB-D dataset for spine surgery and develops SpineAlign, a system for capturing deformation between preoperative and intraoperative spine states. We also present an intraoperative segmentation network trained on this data and introduce CorrespondNet, a multi-task framework for predicting key regions for registration in both intraoperative and preoperative scenes. </p>
<blockquote>
<p>消费级RGB-D成像用于术中骨科组织追踪是一种具有很高应用潜力且前景光明的方法。与骨安装式追踪设备不同，无标记追踪可以减少手术时间和复杂性。然而，其应用仅限于尸体研究。本文介绍了第一个用于脊柱手术的实时临床RGB-D数据集，并开发了SpineAlign系统，用于捕获术前和术中脊柱状态之间的变形。我们还展示了基于此数据的术中分割网络，并引入了CorrespondNet多任务框架，用于预测术中情景和术前情景中的关键区域的注册。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23657v2">PDF</a> An improved version of this manuscript was accepted to MICCAI</p>
<p><strong>Summary</strong></p>
<p>消费者级RGB-D成像在术中骨科组织追踪方面展现出巨大潜力，具有高度的临床转化潜力。无标记追踪技术相较于骨头上安装的追踪设备，能够减少手术时间和复杂性。然而，其应用仅限于尸体研究。本文首次引入用于脊柱外科的真实世界临床RGB-D数据集，并开发了SpineAlign系统以捕获术前和术中脊椎状态之间的变形情况。此外，本文展示了一个基于该数据训练的术中分割网络，并引入了CorrespondNet多任务框架，用于预测术中与术前场景的注册关键区域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RGB-D成像在术中骨科组织追踪方面具有巨大潜力。</li>
<li>无标记追踪技术相较于骨头上安装的追踪设备能够减少手术时间和复杂性。</li>
<li>RGB-D成像的应用在真实世界临床环境中仍处于初步阶段，仅限于尸体研究。</li>
<li>首次引入用于脊柱外科的真实世界临床RGB-D数据集。</li>
<li>开发了SpineAlign系统以捕捉术前和术中脊椎状态之间的变形情况。</li>
<li>展示了一个基于真实世界临床数据训练的术中分割网络。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23657">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-67f7fce721c53d3885edc9c65712530c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0cf6ee806eb45b6b29f5c64be18a221.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d47ca684fdee4c488f875bf7a843e93d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CycleVAR-Repurposing-Autoregressive-Model-for-Unsupervised-One-Step-Image-Translation"><a href="#CycleVAR-Repurposing-Autoregressive-Model-for-Unsupervised-One-Step-Image-Translation" class="headerlink" title="CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step   Image Translation"></a>CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step   Image Translation</h2><p><strong>Authors:Yi Liu, Shengqian Li, Zuzeng Lin, Feng Wang, Si Liu</strong></p>
<p>The current conditional autoregressive image generation methods have shown promising results, yet their potential remains largely unexplored in the practical unsupervised image translation domain, which operates without explicit cross-domain correspondences. A critical limitation stems from the discrete quantization inherent in traditional Vector Quantization-based frameworks, which disrupts gradient flow between the Variational Autoencoder decoder and causal Transformer, impeding end-to-end optimization during adversarial training in image space. To tackle this issue, we propose using Softmax Relaxed Quantization, a novel approach that reformulates codebook selection as a continuous probability mixing process via Softmax, thereby preserving gradient propagation. Building upon this differentiable foundation, we introduce CycleVAR, which reformulates image-to-image translation as image-conditional visual autoregressive generation by injecting multi-scale source image tokens as contextual prompts, analogous to prefix-based conditioning in language models. CycleVAR exploits two modes to generate the target image tokens, including (1) serial multi-step generation, enabling iterative refinement across scales, and (2) parallel one-step generation synthesizing all resolution outputs in a single forward pass. Experimental findings indicate that the parallel one-step generation mode attains superior translation quality with quicker inference speed than the serial multi-step mode in unsupervised scenarios. Furthermore, both quantitative and qualitative results indicate that CycleVAR surpasses previous state-of-the-art unsupervised image translation models, \textit{e}.\textit{g}., CycleGAN-Turbo. </p>
<blockquote>
<p>当前的条件自回归图像生成方法已经显示出有前景的结果，但在实际的无监督图像翻译领域，其潜力在很大程度上尚未被探索，这一领域的工作不需要明确的跨域对应。一个关键的局限性来自于传统基于向量量化的框架中的离散量化，它破坏了变分自编码器解码器和因果变压器之间的梯度流，阻碍了图像空间对抗训练过程中的端到端优化。为了解决这一问题，我们提出了Softmax松弛量化方法，这是一种将码本选择重新制定为一个连续的概率混合过程的新方法，通过Softmax保留梯度传播。基于这个可微分的基础，我们引入了CycleVAR，它将图像到图像的翻译重新定义为图像条件视觉自回归生成，通过注入多尺度源图像标记作为上下文提示，类似于语言模型中的前缀条件。CycleVAR利用两种模式来生成目标图像标记，包括（1）串行多步生成，实现跨尺度的迭代细化；（2）并行一步生成，在一次前向传递中合成所有分辨率的输出。实验结果表明，在无人监督的情况下，并行一步生成模式具有更高的翻译质量和更快的推理速度，优于串行多步模式。此外，定量和定性的结果都表明，CycleVAR超越了现有的最先进的无监督图像翻译模型，例如CycleGAN-Turbo。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23347v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>当前条件自回归图像生成方法已显示出有前景的结果，但在无监督图像翻译领域的应用潜力尚未得到充分探索。针对传统基于向量量化的框架中存在的离散量化问题，我们提出使用Softmax Relaxed Quantization，将码本选择重构为连续的概略混合过程，从而保留梯度传播。在此基础上，我们引入CycleVAR，将图像到图像的翻译重构为图像条件视觉自回归生成，通过注入多尺度源图像令牌作为上下文提示。实验表明，在无人监督的场景下，并行一步生成模式在翻译质量和推理速度上优于串行多步生成模式。此外，CycleVAR在定量和定性结果上都超越了之前的无监督图像翻译模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>条件自回归图像生成方法在无监督图像翻译领域应用潜力巨大。</li>
<li>传统基于向量量化的框架存在离散量化问题，影响梯度流和端到端优化。</li>
<li>Softmax Relaxed Quantization方法将码本选择重构为连续的概略混合过程。</li>
<li>CycleVAR将图像到图像的翻译重构为图像条件视觉自回归生成。</li>
<li>CycleVAR利用多尺度源图像令牌作为上下文提示，引入两种生成模式。</li>
<li>并行一步生成模式在无人监督的场景下表现优越，翻译质量和推理速度更佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23347">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-75dacf7738d1fd1cc56b5439d0ce5391.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fc32e83fee4908553aab59432cffafd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f02173296733cda3a97d680413dae74.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Score-based-Diffusion-Model-for-Unpaired-Virtual-Histology-Staining"><a href="#Score-based-Diffusion-Model-for-Unpaired-Virtual-Histology-Staining" class="headerlink" title="Score-based Diffusion Model for Unpaired Virtual Histology Staining"></a>Score-based Diffusion Model for Unpaired Virtual Histology Staining</h2><p><strong>Authors:Anran Liu, Xiaofei Wang, Jing Cai, Chao Li</strong></p>
<p>Hematoxylin and eosin (H&amp;E) staining visualizes histology but lacks specificity for diagnostic markers. Immunohistochemistry (IHC) staining provides protein-targeted staining but is restricted by tissue availability and antibody specificity. Virtual staining, i.e., computationally translating the H&amp;E image to its IHC counterpart while preserving the tissue structure, is promising for efficient IHC generation. Existing virtual staining methods still face key challenges: 1) effective decomposition of staining style and tissue structure, 2) controllable staining process adaptable to diverse tissue and proteins, and 3) rigorous structural consistency modelling to handle the non-pixel-aligned nature of paired H&amp;E and IHC images. This study proposes a mutual-information (MI)-guided score-based diffusion model for unpaired virtual staining. Specifically, we design 1) a global MI-guided energy function that disentangles the tissue structure and staining characteristics across modalities, 2) a novel timestep-customized reverse diffusion process for precise control of the staining intensity and structural reconstruction, and 3) a local MI-driven contrastive learning strategy to ensure the cellular level structural consistency between H&amp;E-IHC images. Extensive experiments demonstrate the our superiority over state-of-the-art approaches, highlighting its biomedical potential. Codes will be open-sourced upon acceptance. </p>
<blockquote>
<p>苏木精和伊红（H＆E）染色能可视化组织形态学，但缺乏特异性诊断标志物。免疫组织化学（IHC）染色提供针对蛋白质的染色，但受到组织可用性和抗体特异性的限制。虚拟染色即通过计算将H＆E图像翻译成其IHC对应物，同时保留组织结构，对于高效生成IHC非常有前景。现有的虚拟染色方法仍面临关键挑战：1）有效分解染色风格和组织结构；2）可控的染色过程，适应各种组织和蛋白质；3）严格的结构一致性建模，以处理成对的H＆E和IHC图像的非像素对齐性质。本研究提出了一种用于非配对虚拟染色的基于互信息（MI）引导得分扩散模型。具体来说，我们设计了1）一种全局MI引导的能量函数，该函数可以解开不同模式下的组织结构和染色特性；2）一种新型的定时反向扩散过程，实现对染色强度的精确控制和结构重建；3）一种局部MI驱动的对比学习策略，以确保H＆E-IHC图像在细胞水平上的结构一致性。大量实验证明，我们的方法优于最新方法，突显了其生物医学潜力。代码将在接受后开源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23184v1">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong>：</p>
<p>该研究提出一种基于互信息指导的分数扩散模型用于非配对的虚拟染色技术。通过设计全局互信息引导的能量函数，分离不同模态下的组织结构和染色特征；采用时间步长定制的逆向扩散过程，精确控制染色强度和结构重建；使用局部互信息驱动对比学习策略，确保H&amp;E与IHC图像在细胞水平上的结构一致性。实验证明该方法优于现有技术，展现其在生物医学领域的潜力。代码接受后将开源。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>虚拟染色技术能够在计算上将H&amp;E图像转化为IHC图像，同时保留组织结构。</li>
<li>现存的虚拟染色方法面临有效分解染色风格和组织结构、适应多种组织和蛋白质的可控染色过程，以及处理H&amp;E和IHC图像非像素对齐的严格结构一致性建模等挑战。</li>
<li>该研究通过设计全局互信息引导的能量函数，实现了不同模态下组织结构和染色特征的分离。</li>
<li>采用时间步长定制的逆向扩散过程，实现对染色强度的精确控制以及结构重建。</li>
<li>通过局部互信息驱动对比学习策略，确保了H&amp;E与IHC图像在细胞水平上的结构一致性。</li>
<li>实验证明该方法优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-636acd990da94a71dfeea48bad693b37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bcaebf07dda1301a4a44fbe9dea7076.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6451f883323ab441278ee22c719ba7ea.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MatChA-Cross-Algorithm-Matching-with-Feature-Augmentation"><a href="#MatChA-Cross-Algorithm-Matching-with-Feature-Augmentation" class="headerlink" title="MatChA: Cross-Algorithm Matching with Feature Augmentation"></a>MatChA: Cross-Algorithm Matching with Feature Augmentation</h2><p><strong>Authors:Paula Carbó Cubero, Alberto Jaenal Gálvez, André Mateus, José Araújo, Patric Jensfelt</strong></p>
<p>State-of-the-art methods fail to solve visual localization in scenarios where different devices use different sparse feature extraction algorithms to obtain keypoints and their corresponding descriptors. Translating feature descriptors is enough to enable matching. However, performance is drastically reduced in cross-feature detector cases, because current solutions assume common keypoints. This means that the same detector has to be used, which is rarely the case in practice when different descriptors are used. The low repeatability of keypoints, in addition to non-discriminatory and non-distinctive descriptors, make the identification of true correspondences extremely challenging. We present the first method tackling this problem, which performs feature descriptor augmentation targeting cross-detector feature matching, and then feature translation to a latent space. We show that our method significantly improves image matching and visual localization in the cross-feature scenario and evaluate the proposed method on several benchmarks. </p>
<blockquote>
<p>当前先进技术的方法无法在处理视觉定位的场景中解决不同设备使用不同的稀疏特征提取算法获取关键点和其对应的描述符的问题。翻译特征描述符足以实现匹配。但在跨特征检测器的情况下，性能会大大降低，因为当前解决方案假设了共同的关键点。这意味着必须使用相同的检测器，但在实践中当使用不同的描述符时，这种情况很少见。关键点的低重复性以及非歧视性和非独特性描述符，使得识别真正的对应关系极具挑战性。我们提出了第一种解决此问题的方法，该方法针对跨检测器特征匹配进行特征描述符增强，然后将其转换为潜在空间。我们证明了该方法在跨特征场景中显著提高了图像匹配和视觉定位的能力，并在多个基准测试上对所提出的方法进行了评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22336v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了不同设备使用不同稀疏特征提取算法进行关键点及其对应描述符提取时，视觉定位面临的挑战。当前解决方案假设共同关键点，但在跨特征检测器情况下性能大幅降低。为此，本文提出一种解决此问题的方法，通过特征描述符增强和跨检测器特征匹配，然后将特征翻译到潜在空间。实验证明，该方法在跨特征场景中能显著提高图像匹配和视觉定位效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>不同设备使用不同的稀疏特征提取算法时，视觉定位面临挑战。</li>
<li>当前解决方案假设共同关键点，导致跨特征检测器情况下性能降低。</li>
<li>特征描述符的翻译是使匹配成为可能的关键。</li>
<li>关键点低重复率以及描述符的非歧视性和非独特性使得识别真正的对应关系极具挑战性。</li>
<li>本文提出一种解决此问题的方法，通过特征描述符增强和匹配，针对跨检测器进行特征翻译至潜在空间。</li>
<li>该方法在跨特征场景中能显著提高图像匹配和视觉定位效果。</li>
<li>本文方法在多个基准测试集上进行了评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22336">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f9ffa97727a9c930077974202df399d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee9327c08e91c7a01e25d5f5a303b4e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ee1507eb437ba2d3346e5c9c88c3834.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="3D-Telepathy-Reconstructing-3D-Objects-from-EEG-Signals"><a href="#3D-Telepathy-Reconstructing-3D-Objects-from-EEG-Signals" class="headerlink" title="3D-Telepathy: Reconstructing 3D Objects from EEG Signals"></a>3D-Telepathy: Reconstructing 3D Objects from EEG Signals</h2><p><strong>Authors:Yuxiang Ge, Jionghao Cheng, Ruiquan Ge, Zhaojie Fang, Gangyong Jia, Xiang Wan, Nannan Li, Ahmed Elazab, Changmiao Wang</strong></p>
<p>Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds significant potential for applications in Brain-Computer Interfaces (BCIs) and aiding individuals with communication disorders. Traditionally, efforts have focused on converting brain activity into 2D images, neglecting the translation of EEG data into 3D objects. This limitation is noteworthy, as the human brain inherently processes three-dimensional spatial information regardless of whether observing 2D images or the real world. The neural activities captured by EEG contain rich spatial information that is inevitably lost when reconstructing only 2D images, thus limiting its practical applications in BCI. The transition from EEG data to 3D object reconstruction faces considerable obstacles. These include the presence of extensive noise within EEG signals and a scarcity of datasets that include both EEG and 3D information, which complicates the extraction process of 3D visual data. Addressing this challenging task, we propose an innovative EEG encoder architecture that integrates a dual self-attention mechanism. We use a hybrid training strategy to train the EEG Encoder, which includes cross-attention, contrastive learning, and self-supervised learning techniques. Additionally, by employing stable diffusion as a prior distribution and utilizing Variational Score Distillation to train a neural radiation field, we successfully generate 3D objects with similar content and structure from EEG data. </p>
<blockquote>
<p>从脑电图（EEG）数据中重建3D视觉刺激在脑机接口（BCI）应用中具有显著潜力，并能帮助有交流障碍的人。传统上，人们致力于将脑活动转化为二维图像，却忽视了将脑电图数据转化为三维物体的可能性。这种局限性值得注意，因为无论观察二维图像还是真实世界，人类大脑都内在地处理三维空间信息。脑电图捕捉到的神经活动包含丰富的空间信息，在仅重建二维图像时会不可避免地丢失，从而限制了其在脑机接口中的实际应用。从脑电图数据过渡到三维物体重建面临着相当大的障碍。其中包括脑电图信号中存在大量噪声，以及缺乏同时包含脑电图和三维信息的数据集，这使得三维视觉数据的提取过程变得复杂。为了应对这一具有挑战性的任务，我们提出了一种创新的脑电图编码器架构，该架构结合了双重自注意力机制。我们使用混合训练策略来训练脑电图编码器，包括交叉注意力、对比学习和自我监督学习技术。此外，通过采用稳定扩散作为先验分布，并利用变分分数蒸馏训练神经辐射场，我们成功地从脑电图数据中生成了具有相似内容和结构的三维物体。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21843v1">PDF</a> </p>
<p><strong>Summary</strong>：利用脑电图（EEG）数据重建三维视觉刺激在脑机接口（BCI）和辅助沟通障碍个体方面具有巨大潜力。传统上，研究主要关注将脑活动转化为二维图像，但将EEG数据翻译为三维物体的研究尚未得到足够重视。本文提出了一种创新的EEG编码器架构，采用双重自注意力机制，通过混合训练策略，成功从EEG数据中生成具有相似内容和结构的三维物体。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>EEG数据重建三维视觉刺激在脑机接口及辅助沟通障碍领域具有显著潜力。</li>
<li>传统方法主要将脑活动转化为二维图像，忽视了从EEG数据翻译到三维物体的研究。</li>
<li>EEG信号中包含丰富的空间信息，在重建过程中不可避免地会丢失部分信息。</li>
<li>从EEG数据到三维物体重建存在挑战，包括EEG信号中的大量噪声和缺乏包含EEG和三维信息的数据集。</li>
<li>创新的EEG编码器架构结合了双重自注意力机制，为应对这些挑战提供了新的方向。</li>
<li>使用混合训练策略，包括交叉注意力、对比学习和自监督学习技术来训练EEG编码器。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21843">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b3ce0267bd6b8b78e2f48689f13f2839.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3befbf42cf6722e3dce22b576ad640c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-761fa84734c47207cd4726fd883036c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bed351bc97a56e5648a54710cb9c5617.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bbe14b684a0ff1d40cecd71c37c2a2da.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Score-based-Generative-Diffusion-Models-to-Synthesize-Full-dose-FDG-Brain-PET-from-MRI-in-Epilepsy-Patients"><a href="#Score-based-Generative-Diffusion-Models-to-Synthesize-Full-dose-FDG-Brain-PET-from-MRI-in-Epilepsy-Patients" class="headerlink" title="Score-based Generative Diffusion Models to Synthesize Full-dose FDG   Brain PET from MRI in Epilepsy Patients"></a>Score-based Generative Diffusion Models to Synthesize Full-dose FDG   Brain PET from MRI in Epilepsy Patients</h2><p><strong>Authors:Jiaqi Wu, Jiahong Ouyang, Farshad Moradi, Mohammad Mehdi Khalighi, Greg Zaharchuk</strong></p>
<p>Fluorodeoxyglucose (FDG) PET to evaluate patients with epilepsy is one of the most common applications for simultaneous PET&#x2F;MRI, given the need to image both brain structure and metabolism, but is suboptimal due to the radiation dose in this young population. Little work has been done synthesizing diagnostic quality PET images from MRI data or MRI data with ultralow-dose PET using advanced generative AI methods, such as diffusion models, with attention to clinical evaluations tailored for the epilepsy population. Here we compared the performance of diffusion- and non-diffusion-based deep learning models for the MRI-to-PET image translation task for epilepsy imaging using simultaneous PET&#x2F;MRI in 52 subjects (40 train&#x2F;2 validate&#x2F;10 hold-out test). We tested three different models: 2 score-based generative diffusion models (SGM-Karras Diffusion [SGM-KD] and SGM-variance preserving [SGM-VP]) and a Transformer-Unet. We report results on standard image processing metrics as well as clinically relevant metrics, including congruency measures (Congruence Index and Congruency Mean Absolute Error) that assess hemispheric metabolic asymmetry, which is a key part of the clinical analysis of these images. The SGM-KD produced the best qualitative and quantitative results when synthesizing PET purely from T1w and T2 FLAIR images with the least mean absolute error in whole-brain specific uptake value ratio (SUVR) and highest intraclass correlation coefficient. When 1% low-dose PET images are included in the inputs, all models improve significantly and are interchangeable for quantitative performance and visual quality. In summary, SGMs hold great potential for pure MRI-to-PET translation, while all 3 model types can synthesize full-dose FDG-PET accurately using MRI and ultralow-dose PET. </p>
<blockquote>
<p>使用氟脱氧葡萄糖（FDG）PET评估癫痫患者是同时PET&#x2F;MRI最常见的应用之一，由于需要同时显示大脑结构和代谢。但是由于年轻患者的辐射剂量问题，其效果并不理想。目前很少有工作使用先进的生成式人工智能方法（如扩散模型）合成具有诊断质量的PET图像或MRI数据与超低剂量PET的MRI数据，并针对癫痫人群进行临床评估。在这里，我们比较了基于扩散和非扩散深度学习模型在MRI到PET图像转换任务上的表现，使用同时PET&#x2F;MRI对52名癫痫患者（40名训练&#x2F; 2名验证&#x2F; 10名保留测试）进行成像。我们测试了三种不同的模型：两种基于分数的生成扩散模型（SGM-Karras扩散[SGM-KD]和SGM-方差保留[SGM-VP]）和一个Transformer-Unet。我们报告了标准图像处理指标的结果以及与临床相关的指标，包括评估半球代谢不对称性的符合度指标（符合度指数和符合度平均绝对误差）。符合度指数是这些图像临床分析的关键部分。SGM-KD在仅使用T1w和T2 FLAIR图像合成PET时产生了最佳的主观和客观结果，具有最低的全脑特异性摄取值比率（SUVR）平均绝对误差和最高的组内相关系数。当包含1%低剂量PET图像时，所有模型的定量性能和视觉质量均得到显着提高并且可以互换使用。总之，SGM在纯MRI到PET翻译方面显示出巨大潜力，而所有三种类型的模型都可以使用MRI和超低剂量PET准确合成全剂量FDG-PET图像。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11297v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了利用深度学习模型在MRI图像基础上合成PET图像的方法，对比了扩散模型与非扩散模型在癫痫病人成像中的表现。研究发现，基于扩散模型的SGM-KD在纯MRI图像上表现最佳，而所有模型在结合超低剂量PET时都能准确合成全剂量FDG-PET图像。</p>
<p><strong>Key Takeaways</strong></p>
<p>1.氟代脱氧葡萄糖（FDG）PET用于评估癫痫患者的应用是最常见的PET&#x2F;MRI同时应用之一，但需要同时成像大脑结构和代谢。<br>2.由于年轻人口的辐射剂量问题，当前的应用被认为是不理想的。<br>3.本研究使用深度学习模型，包括扩散模型，对MRI到PET的图像转换任务进行了比较。<br>4.在癫痫成像方面，研究对三种模型进行了测试：基于扩散的SGM-KD和SGM-VP模型以及Transformer-Unet模型。<br>5.SGM-KD模型在纯MRI图像上表现出最佳的定性和定量结果，具有最小的全脑特定摄取值比率（SUVR）平均绝对误差和最高的组内相关系数。<br>6.当加入超低剂量PET图像时，所有模型的表现都得到了显著提高，并且在定量性能和视觉质量方面变得可互换。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11297">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eaf7094ff06c73fad53595e19dbc7273.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac385a6974423ebb1624f29d8cd0913f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd6fafd44448ae724af6eddad5cf165a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Wavelet-Diffusion-GAN-for-Image-Super-Resolution"><a href="#A-Wavelet-Diffusion-GAN-for-Image-Super-Resolution" class="headerlink" title="A Wavelet Diffusion GAN for Image Super-Resolution"></a>A Wavelet Diffusion GAN for Image Super-Resolution</h2><p><strong>Authors:Lorenzo Aloisi, Luigi Sigillo, Aurelio Uncini, Danilo Comminiello</strong></p>
<p>In recent years, diffusion models have emerged as a superior alternative to generative adversarial networks (GANs) for high-fidelity image generation, with wide applications in text-to-image generation, image-to-image translation, and super-resolution. However, their real-time feasibility is hindered by slow training and inference speeds. This study addresses this challenge by proposing a wavelet-based conditional Diffusion GAN scheme for Single-Image Super-Resolution (SISR). Our approach utilizes the diffusion GAN paradigm to reduce the timesteps required by the reverse diffusion process and the Discrete Wavelet Transform (DWT) to achieve dimensionality reduction, decreasing training and inference times significantly. The results of an experimental validation on the CelebA-HQ dataset confirm the effectiveness of our proposed scheme. Our approach outperforms other state-of-the-art methodologies successfully ensuring high-fidelity output while overcoming inherent drawbacks associated with diffusion models in time-sensitive applications. </p>
<blockquote>
<p>近年来，扩散模型已经作为生成对抗网络（GANs）的优质替代方案，在高保真图像生成方面展现出优势，广泛应用于文本到图像生成、图像到图像转换和超分辨率处理。然而，由于其训练和推理速度慢，实时性能受限。本研究针对这一挑战，提出一种基于小波条件的扩散GAN方案，用于单图像超分辨率（SISR）。我们的方法利用扩散GAN范式减少反向扩散过程所需的时间步长，并利用离散小波变换（DWT）实现降维，从而显著缩短训练和推理时间。在CelebA-HQ数据集上的实验验证结果证实了我们方案的有效性。我们的方法成功超越了其他最先进的方法，既保证了高保真输出，又克服了扩散模型在时间敏感应用中的固有缺陷。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17966v2">PDF</a> The paper has been accepted at Italian Workshop on Neural Networks   (WIRN) 2024</p>
<p><strong>Summary</strong></p>
<p>近年来，扩散模型已逐渐取代生成对抗网络（GANs），成为高保真图像生成的首选方法，广泛应用于文本到图像生成、图像到图像转换以及超分辨率等领域。然而，其实时可行性受到训练速度慢和推理速度慢的阻碍。本研究通过提出一种基于小波的条件扩散GAN方案来解决这一挑战，用于单图像超分辨率（SISR）。我们的方法利用扩散GAN范式减少了反向扩散过程所需的时间步长，并利用离散小波变换（DWT）实现降维，从而显著减少训练和推理时间。在CelebA-HQ数据集上的实验验证结果表明了本方法的有效性。本方法成功超越了其他先进的方法论，确保了高保真输出，并克服了扩散模型在时间敏感应用中的固有缺陷。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型已成为高保真图像生成的主流方法，替代了生成对抗网络（GANs）。</li>
<li>扩散模型在实时应用中面临训练和推理速度慢的难题。</li>
<li>本研究提出一种基于小波的条件扩散GAN方案，旨在解决扩散模型在实时应用中的瓶颈。</li>
<li>方法利用扩散GAN范式减少时间步长，并利用离散小波变换（DWT）实现降维。</li>
<li>实验在CelebA-HQ数据集上进行，证明了该方法的有效性。</li>
<li>该方法成功超越了其他先进的方法，保证了高保真输出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17966">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f8c61ed71f06e8e110baf528da95acf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7987eb632319509c4054fc4bf32519b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-407a35b38a6f7e683a27c6fc037fc2c2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3031c37fcfad37945b20e4cbd021760e.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-07-03  Flash-VStream Efficient Real-Time Understanding for Long Video Streams
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-16540274011f2d1c616a2978dc5fcede.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-07-03  STACK Adversarial Attacks on LLM Safeguard Pipelines
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27348.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
