<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Advancing Multi-Step Mathematical Reasoning in Large Language Models   through Multi-Layered Self-Reflection with Auto-Prompting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ca1f465a0456014d848fe433316fe5f1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-03-æ›´æ–°"><a href="#2025-07-03-æ›´æ–°" class="headerlink" title="2025-07-03 æ›´æ–°"></a>2025-07-03 æ›´æ–°</h1><h2 id="Advancing-Multi-Step-Mathematical-Reasoning-in-Large-Language-Models-through-Multi-Layered-Self-Reflection-with-Auto-Prompting"><a href="#Advancing-Multi-Step-Mathematical-Reasoning-in-Large-Language-Models-through-Multi-Layered-Self-Reflection-with-Auto-Prompting" class="headerlink" title="Advancing Multi-Step Mathematical Reasoning in Large Language Models   through Multi-Layered Self-Reflection with Auto-Prompting"></a>Advancing Multi-Step Mathematical Reasoning in Large Language Models   through Multi-Layered Self-Reflection with Auto-Prompting</h2><p><strong>Authors:AndrÃ© de Souza Loureiro, Jorge Valverde-Rebaza, Julieta Noguez, David Escarcega, Ricardo Marcacini</strong></p>
<p>Recent advancements in Large Language Models (LLMs) have significantly improved their problem-solving capabilities. However, these models still struggle when faced with complex multi-step reasoning tasks. In this paper, we propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework, a novel approach designed to enhance multi-step mathematical reasoning in LLMs by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an iterative refinement process. Initially, the model generates a solution using CoT prompting. When errors are detected, an adaptive self-reflection mechanism identifies and analyzes them, generating tailored prompts to guide corrections. These dynamically adjusted prompts enable the model to iteratively refine its reasoning. Experiments on four well-established benchmarks across multiple LLMs show that MAPS significantly outperforms standard CoT and achieves competitive results with reasoning-optimized models. In addition, MAPS enables general-purpose LLMs to reach performance levels comparable to specialized reasoning models. While deeper reflection layers improve accuracy, they also increase token usage and costs. To balance this trade-off, MAPS strategically limits reflection depth, ensuring an optimal balance between cost and reasoning performance. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æå¤§åœ°æé«˜äº†å…¶è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“é¢å¯¹å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡æ—¶ï¼Œè¿™äº›æ¨¡å‹ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå±‚æ¬¡è‡ªæˆ‘åæ€ä¸è‡ªåŠ¨æç¤ºï¼ˆMAPSï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆæ€ç»´é“¾ï¼ˆCoTï¼‰ã€è‡ªæˆ‘åæ€å’Œè‡ªåŠ¨æç¤ºç­‰æŠ€æœ¯ï¼Œå¢å¼ºLLMä¸­çš„å¤šæ­¥éª¤æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„é™æ€æç¤ºæ–¹æ³•ä¸åŒï¼ŒMAPSé‡‡ç”¨è¿­ä»£ç»†åŒ–è¿‡ç¨‹ã€‚æœ€åˆï¼Œæ¨¡å‹ä½¿ç”¨CoTæç¤ºç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚å½“æ£€æµ‹åˆ°é”™è¯¯æ—¶ï¼Œè‡ªé€‚åº”çš„è‡ªæˆ‘åæ€æœºåˆ¶ä¼šè¯†åˆ«å¹¶åˆ†æè¿™äº›é”™è¯¯ï¼Œç”Ÿæˆå®šåˆ¶çš„æç¤ºæ¥æŒ‡å¯¼æ›´æ­£ã€‚è¿™äº›åŠ¨æ€è°ƒæ•´çš„æç¤ºä½¿æ¨¡å‹èƒ½å¤Ÿè¿­ä»£åœ°ä¼˜åŒ–å…¶æ¨ç†ã€‚åœ¨å¤šä¸ªLLMçš„å››ä¸ªæˆç†ŸåŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAPSæ˜¾è‘—ä¼˜äºæ ‡å‡†CoTï¼Œå¹¶ä¸ä¼˜åŒ–æ¨ç†çš„æ¨¡å‹å–å¾—å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æ­¤å¤–ï¼ŒMAPSä½¿é€šç”¨LLMèƒ½å¤Ÿè¾¾åˆ°ä¸ä¸“ç”¨æ¨ç†æ¨¡å‹ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚è™½ç„¶æ›´æ·±çš„åæ€å±‚æ¬¡æé«˜äº†å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬ä¹Ÿå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨é‡å’Œæˆæœ¬ã€‚ä¸ºäº†å¹³è¡¡è¿™ä¸€æƒè¡¡ï¼ŒMAPSæˆ˜ç•¥æ€§åœ°é™åˆ¶äº†åæ€æ·±åº¦ï¼Œç¡®ä¿æˆæœ¬ä¸æ¨ç†æ€§èƒ½ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23888v1">PDF</a> Accepted for publication in: European Conference on Machine Learning   and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD   2025). Research Track</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³æ•°å­¦é—®é¢˜æ–¹é¢çš„èƒ½åŠ›å·²ç»æ˜¾è‘—æé«˜ï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºçš„Multi-Layered Self-Reflection with Auto-Promptingï¼ˆMAPSï¼‰æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆé“¾å¼æ€ç»´ï¼ˆCoTï¼‰ã€è‡ªæˆ‘åæ€å’Œè‡ªåŠ¨æç¤ºç­‰æŠ€æœ¯ï¼Œå¢å¼ºäº†LLMçš„å¤šæ­¥éª¤æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAPSæ˜¾è‘—ä¼˜äºæ ‡å‡†CoTï¼Œä¸ç»è¿‡æ¨ç†ä¼˜åŒ–çš„æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶èƒ½ä½¿é€šç”¨LLMçš„æ€§èƒ½è¾¾åˆ°ä¸ä¸“ç”¨æ¨ç†æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è§£å†³å¤šæ­¥éª¤æ•°å­¦æ¨ç†ä»»åŠ¡æ—¶ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>MAPSæ¡†æ¶é€šè¿‡ç»“åˆCoTã€è‡ªæˆ‘åæ€å’Œè‡ªåŠ¨æç¤ºç­‰æŠ€æœ¯å¢å¼ºLLMçš„å¤šæ­¥éª¤æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MAPSé‡‡ç”¨è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œé€šè¿‡è‡ªæˆ‘åæ€æœºåˆ¶è¯†åˆ«é”™è¯¯ï¼Œå¹¶ç”Ÿæˆé’ˆå¯¹æ€§çš„æç¤ºæ¥å¼•å¯¼ä¿®æ­£ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMAPSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†CoTï¼Œå¹¶å®ç°äº†ä¸æ¨ç†ä¼˜åŒ–æ¨¡å‹çš„ç«äº‰æ€§èƒ½ã€‚</li>
<li>MAPSä½¿é€šç”¨LLMçš„æ€§èƒ½è¾¾åˆ°ä¸ä¸“ç”¨æ¨ç†æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚</li>
<li>æ›´æ·±çš„åæ€å±‚æ¬¡è™½ç„¶èƒ½æé«˜å‡†ç¡®æ€§ï¼Œä½†ä¹Ÿå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨é‡å’Œæˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23888">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64ea61165c383a0248e81921a41ffa06.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc10a5fddc2e0720b455313c0df137b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efa1f7e3508cf530f31e52e91341a83c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5dc9c221e2df811692487e5d65042aa.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AutoEvoEval-An-Automated-Framework-for-Evolving-Close-Ended-LLM-Evaluation-Data"><a href="#AutoEvoEval-An-Automated-Framework-for-Evolving-Close-Ended-LLM-Evaluation-Data" class="headerlink" title="AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM   Evaluation Data"></a>AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM   Evaluation Data</h2><p><strong>Authors:JiaRu Wu, Mingwei Liu</strong></p>
<p>Large language models (LLMs) have shown remarkable performance on various tasks, but existing evaluation benchmarks are often static and insufficient to fully assess their robustness and generalization in realistic scenarios. Prior work using evolutionary or adversarial data augmentation has improved evaluation diversity but lacks systematic control over perturbation types and multi-step complexity, limiting comprehensive robustness analysis. To address these gaps, we propose AutoEvoEval, an evolution-based evaluation framework for close-ended tasks such as multi-choice question answering. AutoEvoEval introduces 22 interpretable atomic evolution operations and supports multi-round compositions, enabling controlled generation of diverse, challenging, and realistic test samples. We conduct extensive experiments addressing four research questions on a broad set of open- and closed-source LLMs. Our results show that atomic operations cause an average accuracy drop of 7.283%, with structure-disrupting or misleading semantic edits causing the largest declines. Model sensitivities vary significantly for the same perturbation, and combining multiple evolution steps amplifies adversarial effects by up to 52.932%. These findings suggest current benchmarks may overestimate true model generalization and emphasize the need for evolution-aware robustness evaluation. Code and resources are available at: <a target="_blank" rel="noopener" href="https://github.com/SYSUSELab/AutoEvoEval">https://github.com/SYSUSELab/AutoEvoEval</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†ç°æœ‰çš„è¯„ä¼°åŸºå‡†é€šå¸¸æ˜¯é™æ€çš„ï¼Œä¸è¶³ä»¥åœ¨çœŸå®åœºæ™¯ä¸­å…¨é¢è¯„ä¼°å…¶é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å…ˆå‰ä½¿ç”¨è¿›åŒ–æˆ–å¯¹æŠ—æ€§æ•°æ®å¢å¼ºçš„å·¥ä½œæé«˜äº†è¯„ä¼°çš„å¤šæ ·æ€§ï¼Œä½†ç¼ºä¹å¯¹æ‰°åŠ¨ç±»å‹å’Œå¤šæ­¥å¤æ‚æ€§çš„ç³»ç»Ÿæ§åˆ¶ï¼Œé™åˆ¶äº†å…¨é¢çš„é²æ£’æ€§åˆ†æã€‚ä¸ºäº†è§£å†³è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†AutoEvoEvalï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè¿›åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå°é—­ä»»åŠ¡ï¼Œå¦‚å¤šé¡¹é€‰æ‹©é¢˜å›ç­”ã€‚AutoEvoEvalå¼•å…¥äº†22ä¸ªå¯è§£é‡Šçš„åŸå­è¿›åŒ–æ“ä½œï¼Œå¹¶æ”¯æŒå¤šè½®ç»„åˆï¼Œèƒ½å¤Ÿæ§åˆ¶ç”Ÿæˆå¤šæ ·ã€å…·æœ‰æŒ‘æˆ˜æ€§å’Œç°å®çš„æµ‹è¯•æ ·æœ¬ã€‚æˆ‘ä»¬å¯¹å¼€æºå’Œå°é—­æºç çš„LLMsè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå›ç­”äº†å››ä¸ªç ”ç©¶é—®é¢˜ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸå­æ“ä½œå¯¼è‡´å¹³å‡å‡†ç¡®ç‡ä¸‹é™7.283%ï¼Œå…¶ä¸­ç ´åç»“æ„æˆ–è¯¯å¯¼è¯­ä¹‰çš„ç¼–è¾‘å¯¼è‡´ä¸‹é™å¹…åº¦æœ€å¤§ã€‚åŒä¸€æ‰°åŠ¨å¯¹æ¨¡å‹çš„æ•æ„Ÿæ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç»“åˆå¤šä¸ªè¿›åŒ–æ­¥éª¤ä¼šä½¿å¯¹æŠ—æ€§æ•ˆæœæ”¾å¤§é«˜è¾¾52.932%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå½“å‰åŸºå‡†æµ‹è¯•å¯èƒ½ä¼šé«˜ä¼°æ¨¡å‹çš„çœŸæ­£æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å¼ºè°ƒéœ€è¦è¿›è¡Œè¿›åŒ–æ„ŸçŸ¥çš„é²æ£’æ€§è¯„ä¼°ã€‚ç›¸å…³ä»£ç å’Œèµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SYSUSELab/AutoEvoEval%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/SYSUSELab/AutoEvoEvalè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23735v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ç°æœ‰è¯„ä¼°åŸºå‡†æµ‹è¯•å¾€å¾€æ˜¯é™æ€çš„ä¸”ä¸è¶³ä»¥å…¨é¢è¯„ä¼°å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºåŸºäºè¿›åŒ–çš„è¯„ä¼°æ¡†æ¶AutoEvoEvalï¼Œé€‚ç”¨äºå¦‚å¤šé€‰é—®ç­”ç­‰å°é—­ä»»åŠ¡ã€‚è¯¥æ¡†æ¶å¼•å…¥22ç§å¯è§£é‡Šçš„åŸå­è¿›åŒ–æ“ä½œï¼Œæ”¯æŒå¤šè½®ç»„åˆï¼Œå¯æ§åˆ¶ç”Ÿæˆå¤šæ ·åŒ–ã€å…·æŒ‘æˆ˜æ€§å’Œç°å®çš„æµ‹è¯•æ ·æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸå­æ“ä½œå¯¼è‡´å¹³å‡å‡†ç¡®ç‡ä¸‹é™7.283%ï¼Œå…¶ä¸­ç ´åç»“æ„æˆ–è¯¯å¯¼è¯­ä¹‰çš„ç¼–è¾‘å¯¼è‡´ä¸‹é™å¹…åº¦æœ€å¤§ã€‚æ¨¡å‹å¯¹åŒä¸€æ‰°åŠ¨çš„æ•æ„Ÿåº¦å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¤šæ­¥è¿›åŒ–ç»“åˆå¯æ”¾å¤§å¯¹æŠ—æ•ˆæœé«˜è¾¾52.932%ã€‚è¿™æç¤ºæˆ‘ä»¬ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¯èƒ½é«˜ä¼°äº†æ¨¡å‹çš„çœŸå®æ³›åŒ–èƒ½åŠ›ï¼Œå¼ºè°ƒéœ€è¦è¿›è¡ŒåŸºäºè¿›åŒ–çš„ç¨³å¥æ€§è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ç°æœ‰è¯„ä¼°æ–¹æ³•ä¸è¶³ä»¥å…¨é¢è¯„ä¼°å…¶ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºçš„AutoEvoEvalæ¡†æ¶æ˜¯åŸºäºè¿›åŒ–çš„è¯„ä¼°æ–¹æ³•ï¼Œé€‚ç”¨äºå°é—­ä»»åŠ¡å¦‚å¤šé€‰é—®ç­”ã€‚</li>
<li>AutoEvoEvalé€šè¿‡å¼•å…¥22ç§åŸå­è¿›åŒ–æ“ä½œï¼Œæ”¯æŒå¤šè½®ç»„åˆï¼Œä»¥æ§åˆ¶ç”Ÿæˆå¤šæ ·åŒ–ã€å…·æŒ‘æˆ˜æ€§å’Œç°å®çš„æµ‹è¯•æ ·æœ¬ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒåŸå­æ“ä½œå¯¼è‡´æ¨¡å‹å¹³å‡å‡†ç¡®ç‡ä¸‹é™ï¼Œå…¶ä¸­æŸäº›æ“ä½œï¼ˆå¦‚ç»“æ„ç ´åæˆ–è¯­ä¹‰è¯¯å¯¼ï¼‰å½±å“æ›´å¤§ã€‚</li>
<li>æ¨¡å‹å¯¹åŒä¸€æ‰°åŠ¨çš„æ•æ„Ÿåº¦æœ‰æ˜¾è‘—å·®å¼‚ï¼Œä¸”å¤šæ­¥è¿›åŒ–ç»“åˆä¼šæ˜¾è‘—æ”¾å¤§å¯¹æŠ—æ•ˆæœã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¯èƒ½é«˜ä¼°æ¨¡å‹çš„çœŸæ­£æ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦æ›´å…¨é¢çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>AutoEvoEvalæ¡†æ¶çš„ä»£ç å’Œèµ„æºå·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7779eb109997faa03eeabf6a317ab398.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8b2bbd9fff2ec6b404a20079d2e0f87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca1f465a0456014d848fe433316fe5f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cb25f5fc3b5006f926140d1fc015bf9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DABstep-Data-Agent-Benchmark-for-Multi-step-Reasoning"><a href="#DABstep-Data-Agent-Benchmark-for-Multi-step-Reasoning" class="headerlink" title="DABstep: Data Agent Benchmark for Multi-step Reasoning"></a>DABstep: Data Agent Benchmark for Multi-step Reasoning</h2><p><strong>Authors:Alex Egg, Martin Iglesias Goyanes, Friso Kingma, Andreu Mora, Leandro von Werra, Thomas Wolf</strong></p>
<p>We introduce DABstep, a novel benchmark for evaluating AI agents on realistic multi-step data analysis tasks. DABstep comprises over 450 real-world challenges derived from a financial analytics platform, requiring models to combine code-based data processing with contextual reasoning over heterogeneous documentation. Each task demands an iterative, multi-step problem-solving approach, testing capabilities in data manipulation, cross-referencing multiple sources, and precise result reporting. The benchmark provides a factoid-style answer format with automatic correctness checks for objective scoring at scale. We evaluate leading LLM-based agents, revealing a substantial performance gap: even the best agent achieves only 14.55% accuracy on the hardest tasks. We detail our benchmarkâ€™s design, dataset composition, task formulation, evaluation protocol, report baseline results and analyze failure modes. DABstep is released with a public leaderboard and toolkit to accelerate research in autonomous data analysis. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DABstepï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°AIä»£ç†åœ¨ç°å®çš„å¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚DABstepåŒ…å«äº†æ¥è‡ªé‡‘èåˆ†æå¹³å°çš„450å¤šä¸ªç°å®æŒ‘æˆ˜ï¼Œè¦æ±‚æ¨¡å‹ç»“åˆä»£ç æ•°æ®å¤„ç†å’ŒåŸºäºä¸Šä¸‹æ–‡çš„å¼‚è´¨æ–‡æ¡£æ¨ç†ã€‚æ¯ä¸ªä»»åŠ¡éƒ½éœ€è¦è¿­ä»£ã€å¤šæ­¥éª¤çš„é—®é¢˜è§£å†³æ–¹å¼ï¼Œæµ‹è¯•æ•°æ®æ“æ§èƒ½åŠ›ã€è·¨æºäº¤å‰å¼•ç”¨èƒ½åŠ›å’Œç²¾ç¡®ç»“æœæŠ¥å‘Šèƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ä¸ªäº‹å®å‹ç­”æ¡ˆæ ¼å¼ï¼Œå¸¦æœ‰è‡ªåŠ¨æ­£ç¡®æ€§æ£€æŸ¥ï¼Œå¯å¤§è§„æ¨¡è¿›è¡Œå®¢è§‚è¯„åˆ†ã€‚æˆ‘ä»¬è¯„ä¼°äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„é¢†å…ˆä»£ç†ï¼Œç»“æœæ˜¾ç¤ºå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼šå³ä½¿åœ¨æœ€éš¾çš„ä»»åŠ¡ä¸Šï¼Œæœ€ä½³ä»£ç†ä¹Ÿä»…è¾¾åˆ°14.55%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†åŸºå‡†æµ‹è¯•çš„è®¾è®¡ã€æ•°æ®é›†ç»„æˆã€ä»»åŠ¡åˆ¶å®šã€è¯„ä¼°åè®®ï¼ŒæŠ¥å‘Šäº†åŸºçº¿ç»“æœå¹¶åˆ†æäº†å¤±è´¥æ¨¡å¼ã€‚DABstepå·²å‘å¸ƒå…¬å…±æ’è¡Œæ¦œå’Œå·¥å…·åŒ…ï¼Œä»¥åŠ å¿«è‡ªä¸»æ•°æ®åˆ†æçš„ç ”ç©¶è¿›ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23719v1">PDF</a> 13 pages, 5 figures</p>
<p><strong>Summary</strong>ï¼šæ¨å‡ºDABstepåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°AIä»£ç†åœ¨çœŸå®çš„å¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚DABstepåŒ…å«450å¤šä¸ªæ¥è‡ªé‡‘èåˆ†æå¹³å°çš„çœŸå®æŒ‘æˆ˜ï¼Œè¦æ±‚æ¨¡å‹ç»“åˆä»£ç æ•°æ®å¤„ç†å’Œå¼‚è´¨æ–‡æ¡£çš„ä¸Šä¸‹æ–‡æ¨ç†ã€‚è¯¥åŸºå‡†æµ‹è¯•é‡‡ç”¨äº‹å®å‹ç­”æ¡ˆæ ¼å¼ï¼Œæä¾›å¤§è§„æ¨¡å®¢è§‚è¯„åˆ†ï¼Œå¹¶è¯„ä¼°äº†é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œæ˜¾ç¤ºå‡ºç°å­˜æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>DABstepæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°AIåœ¨å¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>å®ƒåŒ…å«450å¤šä¸ªä»é‡‘èåˆ†æå¹³å°è¡ç”Ÿå‡ºçš„çœŸå®ä¸–ç•ŒæŒ‘æˆ˜ã€‚</li>
<li>DABstepè¦æ±‚æ¨¡å‹ç»“åˆä»£ç æ•°æ®å¤„ç†å’Œä¸Šä¸‹æ–‡æ¨ç†ï¼Œè§£å†³å¼‚è´¨æ–‡æ¡£é—®é¢˜ã€‚</li>
<li>åŸºå‡†æµ‹è¯•é‡‡ç”¨äº‹å®å‹ç­”æ¡ˆæ ¼å¼ï¼Œä¾¿äºè‡ªåŠ¨æ­£ç¡®æ€§æ£€æŸ¥å’Œå¤§è§„æ¨¡å®¢è§‚è¯„åˆ†ã€‚</li>
<li>è¯„ä¼°äº†é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œæ˜¾ç¤ºæ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>æœ€ä¼˜ç§€çš„ä»£ç†åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šä»…è¾¾åˆ°14.55%çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23719">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b6ece718e98f8241275d507acba7990.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b19baa7a56a62e9d94bdd5efe7a20970.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3550d27f9908bfff5c2d90be8b069aa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48e2c8a176efbf9b8517e278288cf0f7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="L0-Reinforcement-Learning-to-Become-General-Agents"><a href="#L0-Reinforcement-Learning-to-Become-General-Agents" class="headerlink" title="L0: Reinforcement Learning to Become General Agents"></a>L0: Reinforcement Learning to Become General Agents</h2><p><strong>Authors:Junjie Zhang, Jingyi Xi, Zhuoyang Song, Junyu Lu, Yuhua Ke, Ting Sun, Yukun Yang, Jiaxing Zhang, Songxin Zhang, Zejian Xie</strong></p>
<p>Training large language models (LLMs) to act as autonomous agents for multi-turn, long-horizon tasks remains significant challenges in scalability and training efficiency. To address this, we introduce L-Zero (L0), a scalable, end-to-end training pipeline for general-purpose agents. Featuring a low-cost, extensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier for applying reinforcement learning in complex environments. We also introduce NB-Agent, the agent scaffold within L0, which operates in a â€œcode-as-actionâ€ fashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality question-answering benchmarks. Our experiments demonstrate that a base model can develop robust problem-solving skills using solely Reinforcement Learning with Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method boosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41 %. We have open-sourced the entire L0 system, including our L0 series models, the NB-Agent, a complete training pipeline, and the corresponding training recipes on (<a target="_blank" rel="noopener" href="https://github.com/cmriat/l0">https://github.com/cmriat/l0</a>). </p>
<blockquote>
<p>è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥æ‰§è¡Œå¤šè½®ã€é•¿æœŸä»»åŠ¡ä½œä¸ºè‡ªä¸»ä»£ç†ä»ç„¶å­˜åœ¨å¯æ‰©å±•æ€§å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢çš„é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†L-Zeroï¼ˆL0ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé€šç”¨ä»£ç†çš„å¯æ‰©å±•ç«¯åˆ°ç«¯è®­ç»ƒç®¡é“ã€‚L0é‡‡ç”¨ä½æˆæœ¬ã€å¯æ‰©å±•ã€æ²™ç®±åŒ–çš„å¹¶å‘ä»£ç†å·¥ä½œè€…æ± ï¼Œé™ä½äº†åœ¨å¤æ‚ç¯å¢ƒä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ çš„é—¨æ§›ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†L0å†…çš„NB-Agentä»£ç†æ¶æ„ï¼Œå®ƒé€šè¿‡Read-Eval-Print-Loopï¼ˆREPLï¼‰ä»¥â€œä»£ç å³è¡ŒåŠ¨â€çš„æ–¹å¼è¿è¡Œã€‚æˆ‘ä»¬åœ¨äº‹å®æ€§é—®é¢˜å›ç­”åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†L0çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºç¡€æ¨¡å‹ä»…ä½¿ç”¨å¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å°±èƒ½å‘å±•å‡ºç¨³å¥çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚åœ¨Qwen2.5-7B-Instructæ¨¡å‹ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†SimpleQAçš„å‡†ç¡®ç‡ä»30%æé«˜åˆ°80%ï¼ŒHotpotQAçš„å‡†ç¡®ç‡ä»22%æé«˜åˆ°41%ã€‚æˆ‘ä»¬å·²å…¬å¼€äº†æ•´ä¸ªL0ç³»ç»Ÿï¼ŒåŒ…æ‹¬æˆ‘ä»¬çš„L0ç³»åˆ—æ¨¡å‹ã€NB-Agentã€å®Œæ•´çš„è®­ç»ƒç®¡é“å’Œç›¸åº”çš„è®­ç»ƒé…æ–¹ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/cmriat/l0%EF%BC%89%E3%80%82">https://github.com/cmriat/l0ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>L-Zeroï¼ˆL0ï¼‰æ˜¯ä¸€ä¸ªç”¨äºé€šç”¨ä»£ç†çš„å¯æ‰©å±•ç«¯åˆ°ç«¯è®­ç»ƒç®¡é“ï¼Œè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œå¤šè½®é•¿å‘¨æœŸä»»åŠ¡æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¼•å…¥ä½æˆæœ¬çš„ã€å¯æ‰©å±•çš„ã€æ²™ç®±åŒ–çš„å¹¶å‘ä»£ç†å·¥ä½œè€…æ± ï¼ŒL0é™ä½äº†åœ¨å¤æ‚ç¯å¢ƒä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ çš„é—¨æ§›ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„åŸºç¡€æ¨¡å‹å¯ä»¥å‘å±•å‡ºå¼ºå¤§çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚åœ¨Qwen2.5-7B-Instructæ¨¡å‹ä¸Šï¼ŒL0æ–¹æ³•å°†SimpleQAçš„å‡†ç¡®ç‡ä»30%æé«˜åˆ°80%ï¼ŒHotpotQAçš„å‡†ç¡®ç‡ä»22%æé«˜åˆ°41%ã€‚L0ç³»ç»Ÿå’Œç›¸å…³æ¨¡å‹å·²å¼€æºå…±äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>L-Zeroï¼ˆL0ï¼‰æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯æ‰©å±•ç«¯åˆ°ç«¯è®­ç»ƒç®¡é“ã€‚</li>
<li>L0é€šè¿‡å¼•å…¥å¹¶å‘ä»£ç†å·¥ä½œè€…æ± æ¥é™ä½å¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚ç¯å¢ƒä¸­çš„ä½¿ç”¨é—¨æ§›ã€‚</li>
<li>L0é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ç»“åˆçš„æ–¹æ³•ä½¿åŸºç¡€æ¨¡å‹å…·å¤‡å¼ºå¤§çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>åœ¨Qwen2.5-7B-Instructæ¨¡å‹ä¸Šï¼ŒL0æ˜¾è‘—æé«˜äº†é—®ç­”ä»»åŠ¡çš„å‡†ç¡®ç‡ã€‚</li>
<li>L-Zeroï¼ˆL0ï¼‰ç³»ç»Ÿå’Œç›¸å…³æ¨¡å‹å·²åœ¨GitHubä¸Šå¼€æºå…±äº«ï¼Œæ–¹ä¾¿å…¶ä»–ç ”ç©¶äººå‘˜ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
<li>L0ç³»ç»Ÿçš„å…³é”®ç»„æˆéƒ¨åˆ†ä¹‹ä¸€æ˜¯NB-Agentä»£ç†éª¨æ¶ï¼Œå®ƒä»¥â€œä»£ç å³è¡ŒåŠ¨â€çš„æ–¹å¼å·¥ä½œï¼Œé€šè¿‡Read-Eval-Print-Loopï¼ˆREPLï¼‰è¿›è¡Œäº¤äº’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-44951a14875bbfa96c69286b8c12e04a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3b048ce4e1516cb699b937da9c5ae24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdffdb834dc9781be557047c33b9a8f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15b38f0a442261e065f3d1b347947839.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MMReason-An-Open-Ended-Multi-Modal-Multi-Step-Reasoning-Benchmark-for-MLLMs-Toward-AGI"><a href="#MMReason-An-Open-Ended-Multi-Modal-Multi-Step-Reasoning-Benchmark-for-MLLMs-Toward-AGI" class="headerlink" title="MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for   MLLMs Toward AGI"></a>MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for   MLLMs Toward AGI</h2><p><strong>Authors:Huanjin Yao, Jiaxing Huang, Yawen Qiu, Michael K. Chen, Wenzheng Liu, Wei Zhang, Wenjie Zeng, Xikun Zhang, Jingyi Zhang, Yuxin Song, Wenhao Wu, Dacheng Tao</strong></p>
<p>Reasoning plays a crucial role in advancing Multimodal Large Language Models (MLLMs) toward Artificial General Intelligence. However, existing MLLM benchmarks often fall short in precisely and comprehensively evaluating long-chain reasoning abilities from three key aspects: (1) lack of difficulty and diversity, (2) susceptibility to guessability and memorization, (3) inadequate assessment of intermediate reasoning steps. To fill this gap, we introduce MMReason, a new benchmark designed to precisely and comprehensively evaluate MLLM long-chain reasoning capability with diverse, open-ended, challenging questions. First, we curate challenging questions requiring multi-step reasoning from various fields (i.e., 6 disciplines) and multiple difficulty levels (i.e., from pre-university to university, and from foundational to competition tiers). Second, these questions are reformulated into an open-ended format and filtered using a multi-model voting technique to eliminate shortcut cases related to guessing and memorization, ensuring robust reasoning evaluations. Third, we annotate the questions with detailed step-by-step solutions, and design a reference-based ternary scoring mechanism to reliably assess intermediate reasoning steps. With MMReason, we benchmark popular leading MLLMs and provide an in-depth analysis of their reasoning capabilities. We hope MMReason will serve as a valuable resource for advancing MLLM reasoning research. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/HJYao00/MMReason">https://github.com/HJYao00/MMReason</a>. </p>
<blockquote>
<p>æ¨ç†åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å‘é€šç”¨äººå·¥æ™ºèƒ½è¿ˆè¿›çš„è¿‡ç¨‹ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MLLMåŸºå‡†æµ‹è¯•åœ¨ç²¾ç¡®å…¨é¢åœ°è¯„ä¼°é•¿æœŸæ¨ç†èƒ½åŠ›ä»ä¸‰ä¸ªæ–¹é¢å¾€å¾€æœ‰æ‰€ä¸è¶³ï¼šï¼ˆ1ï¼‰ç¼ºä¹éš¾åº¦å’Œå¤šæ ·æ€§ï¼›ï¼ˆ2ï¼‰å®¹æ˜“çŒœæµ‹å’Œè®°å¿†ï¼›ï¼ˆ3ï¼‰å¯¹ä¸­é—´æ¨ç†æ­¥éª¤çš„è¯„ä¼°ä¸è¶³ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MMReasonï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç²¾ç¡®å…¨é¢åœ°è¯„ä¼°MLLMçš„é•¿æœŸæ¨ç†èƒ½åŠ›ï¼Œé‡‡ç”¨å¤šæ ·ã€å¼€æ”¾ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»å„ä¸ªå­¦ç§‘é¢†åŸŸï¼ˆå³6ä¸ªå­¦ç§‘ï¼‰æ”¶é›†éœ€è¦å¤šæ­¥éª¤æ¨ç†çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œå¹¶è®¾ç½®å¤šä¸ªéš¾åº¦çº§åˆ«ï¼ˆå³ä»å¤§å­¦é¢„ç§‘åˆ°å¤§å­¦ï¼Œä»åŸºç¡€åˆ°ç«èµ›çº§åˆ«ï¼‰ã€‚å…¶æ¬¡ï¼Œè¿™äº›é—®é¢˜è¢«é‡æ–°åˆ¶å®šä¸ºå¼€æ”¾å¼é—®é¢˜ï¼Œå¹¶ä½¿ç”¨å¤šæ¨¡å‹æŠ•ç¥¨æŠ€æœ¯è¿›è¡Œç­›é€‰ï¼Œä»¥æ¶ˆé™¤ä¸çŒœæµ‹å’Œè®°å¿†ç›¸å…³çš„æ·å¾„æ¡ˆä¾‹ï¼Œç¡®ä¿ç¨³å¥çš„æ¨ç†è¯„ä¼°ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬ä¸ºé—®é¢˜æä¾›äº†è¯¦ç»†çš„é€æ­¥è§£å†³æ–¹æ¡ˆï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºå‚è€ƒçš„ä¸‰å…ƒè¯„åˆ†æœºåˆ¶ï¼Œä»¥å¯é åœ°è¯„ä¼°ä¸­é—´æ¨ç†æ­¥éª¤ã€‚é€šè¿‡MMReasonï¼Œæˆ‘ä»¬å¯¹æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å¯¹å®ƒä»¬çš„æ¨ç†èƒ½åŠ›è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚æˆ‘ä»¬å¸Œæœ›MMReasonèƒ½æˆä¸ºæ¨åŠ¨MLLMæ¨ç†ç ”ç©¶çš„æœ‰ä»·å€¼çš„èµ„æºã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/HJYao00/MMReason%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/HJYao00/MMReasonä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23563v1">PDF</a> Technical report</p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ¨è¿›äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„è¿‡ç¨‹ä¸­ï¼Œæ¨ç†å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚ä½†ç°æœ‰MLLMåŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°é•¿æœŸæ¨ç†èƒ½åŠ›æ—¶å­˜åœ¨ç¼ºé™·ï¼Œå¦‚ç¼ºä¹éš¾åº¦å’Œå¤šæ ·æ€§ã€æ˜“äºçŒœæµ‹å’Œè®°å¿†ã€æ— æ³•å……åˆ†è¯„ä¼°ä¸­é—´æ¨ç†æ­¥éª¤ç­‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºMMReasonæ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ç²¾å‡†å…¨é¢åœ°è¯„ä¼°MLLMçš„é•¿æœŸæ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«å¤šæ ·ã€å¼€æ”¾ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚è¯¥æµ‹è¯•ä»å…­ä¸ªå­¦ç§‘é¢†åŸŸæŒ‘é€‰å…·æœ‰æŒ‘æˆ˜ã€éœ€è¦å¤šæ­¥éª¤æ¨ç†çš„é—®é¢˜ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ”¹é©åˆ¶å®šå¼€æ”¾æ€§æ ¼å¼çš„é—®é¢˜ï¼›è¿ç”¨å¤šæ¨¡å‹æŠ•ç¥¨æŠ€æœ¯æ¶ˆé™¤ä¸çŒœæµ‹å’Œè®°å¿†ç›¸å…³çš„æ·å¾„æ¡ˆä¾‹ï¼Œç¡®ä¿æ¨ç†è¯„ä¼°çš„ç¨³å¥æ€§ï¼›ä¸ºé—®é¢˜æä¾›è¯¦ç»†çš„åˆ†æ­¥è§£ç­”ï¼Œå¹¶é‡‡ç”¨åŸºäºå‚è€ƒçš„ä¸‰å…ƒè¯„åˆ†æœºåˆ¶æ¥å¯é åœ°è¯„ä¼°ä¸­é—´æ¨ç†æ­¥éª¤ã€‚MMReasonä¸ºæµè¡Œçš„é¢†å…ˆMLLMæä¾›äº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶æ·±å…¥åˆ†æäº†å®ƒä»¬çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­èµ·å…³é”®ä½œç”¨ï¼Œå¯¹äºå®ç°äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰MLLMåŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°é•¿æœŸæ¨ç†èƒ½åŠ›æ—¶å­˜åœ¨ç¼ºé™·ï¼Œä¸»è¦åŒ…æ‹¬ç¼ºä¹éš¾åº¦å’Œå¤šæ ·æ€§ã€æ˜“äºçŒœæµ‹å’Œè®°å¿†ã€å¯¹ä¸­é—´æ¨ç†æ­¥éª¤è¯„ä¼°ä¸è¶³ã€‚</li>
<li>MMReasonæ–°åŸºå‡†æµ‹è¯•æ—¨åœ¨ç²¾å‡†å…¨é¢åœ°è¯„ä¼°MLLMçš„é•¿æœŸæ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šæ ·ã€å¼€æ”¾ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚</li>
<li>MMReasonä»å…­ä¸ªå­¦ç§‘é¢†åŸŸæŒ‘é€‰é—®é¢˜ï¼Œè¦æ±‚å¤šæ­¥éª¤æ¨ç†ï¼Œå¹¶æ”¹é©åˆ¶å®šå¼€æ”¾æ€§æ ¼å¼çš„é—®é¢˜ã€‚</li>
<li>MMReasonè¿ç”¨å¤šæ¨¡å‹æŠ•ç¥¨æŠ€æœ¯å‡å°‘çŒœæµ‹å’Œè®°å¿†çš„å½±å“ï¼Œç¡®ä¿æ¨ç†è¯„ä¼°çš„ç¨³å¥æ€§ã€‚</li>
<li>MMReasonä¸ºé—®é¢˜æä¾›è¯¦ç»†çš„åˆ†æ­¥è§£ç­”ï¼Œå¹¶é‡‡ç”¨åŸºäºå‚è€ƒçš„ä¸‰å…ƒè¯„åˆ†æœºåˆ¶è¯„ä¼°ä¸­é—´æ¨ç†æ­¥éª¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-861c080d65d46e328ff450eb899d3ce6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5e648b3ff657fcbbb7b34f2ba8e350b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf6ada5ee7b35b506bd0675206a04ba5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55be7277b29bcfe0888e0a761b7ee103.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a5a5939ea3cb9911b0b932a0138c31d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Datasets-for-Fairness-in-Language-Models-An-In-Depth-Survey"><a href="#Datasets-for-Fairness-in-Language-Models-An-In-Depth-Survey" class="headerlink" title="Datasets for Fairness in Language Models: An In-Depth Survey"></a>Datasets for Fairness in Language Models: An In-Depth Survey</h2><p><strong>Authors:Jiale Zhang, Zichong Wang, Avash Palikhe, Zhipeng Yin, Wenbin Zhang</strong></p>
<p>Fairness benchmarks play a central role in shaping how we evaluate language models, yet surprisingly little attention has been given to examining the datasets that these benchmarks rely on. This survey addresses that gap by presenting a broad and careful review of the most widely used fairness datasets in current language model research, characterizing them along several key dimensions including their origin, scope, content, and intended use to help researchers better appreciate the assumptions and limitations embedded in these resources. To support more meaningful comparisons and analyses, we introduce a unified evaluation framework that reveals consistent patterns of demographic disparities across datasets and scoring methods. Applying this framework to twenty four common benchmarks, we highlight the often overlooked biases that can influence conclusions about model fairness and offer practical guidance for selecting, combining, and interpreting these datasets. We also point to opportunities for creating new fairness benchmarks that reflect more diverse social contexts and encourage more thoughtful use of these tools going forward. All code, data, and detailed results are publicly available at <a target="_blank" rel="noopener" href="https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets">https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets</a> to promote transparency and reproducibility across the research community. </p>
<blockquote>
<p>å…¬å¹³æ€§åŸºå‡†åœ¨å¡‘é€ æˆ‘ä»¬å¦‚ä½•è¯„ä¼°è¯­è¨€æ¨¡å‹æ–¹é¢èµ·ç€æ ¸å¿ƒä½œç”¨ï¼Œç„¶è€Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œäººä»¬å‡ ä¹æ²¡æœ‰æ³¨æ„åˆ°è¿™äº›åŸºå‡†æ‰€ä¾èµ–çš„æ•°æ®é›†çš„æ£€éªŒã€‚æœ¬æ–‡æ—¨åœ¨å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œå¯¹å½“å‰è¯­è¨€æ¨¡å‹ç ”ç©¶ä¸­ä½¿ç”¨æœ€å¹¿æ³›çš„å…¬å¹³æ€§æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›è€Œç»†è‡´çš„è¯„è¿°ï¼Œä»å¤šä¸ªå…³é”®ç»´åº¦å¯¹è¿™äº›æ•°æ®é›†è¿›è¡Œäº†ç‰¹å¾æè¿°ï¼ŒåŒ…æ‹¬å…¶æ¥æºã€èŒƒå›´ã€å†…å®¹å’Œé¢„æœŸç”¨é€”ç­‰ï¼Œä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°äº†è§£è¿™äº›èµ„æºä¸­çš„å‡è®¾å’Œå±€é™æ€§ã€‚ä¸ºäº†æ”¯æŒæ›´æœ‰æ„ä¹‰çš„æ¯”è¾ƒå’Œåˆ†æï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ­ç¤ºäº†ä¸åŒæ•°æ®é›†å’Œè¯„åˆ†æ–¹æ³•ä¸­çš„ä¸€è´¯çš„äººå£å·®å¼‚æ¨¡å¼ã€‚åœ¨äºŒåå››é¡¹å¸¸è§åŸºå‡†æµ‹è¯•ä¸­åº”ç”¨è¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å¯èƒ½å½±å“æ¨¡å‹å…¬å¹³æ€§çš„ç»“è®ºä¸­ç»å¸¸è¢«å¿½è§†åè§é—®é¢˜ï¼Œå¹¶ä¸ºé€‰æ‹©ã€ç»„åˆå’Œè§£é‡Šè¿™äº›æ•°æ®é›†æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚æˆ‘ä»¬è¿˜æŒ‡å‡ºäº†åˆ›å»ºåæ˜ æ›´å¤šæ ·åŒ–ç¤¾ä¼šèƒŒæ™¯çš„æ–°å…¬å¹³æ€§åŸºå‡†çš„æœºä¼šï¼Œå¹¶é¼“åŠ±åœ¨æœªæ¥çš„ç ”ç©¶ä¸­æ›´è°¨æ…åœ°ä½¿ç”¨è¿™äº›å·¥å…·ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’Œè¯¦ç»†ç»“æœéƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets%E5%85%AC%E5%BC%80%E4%BA%8C%E9%A2%84%E9%85%B7%E5%A4%BF%E6%A8%A1%E5%BC%8F%EF%BC%9B/tree/main/%E8%AE%AD%E5%A%E7%BB%83%E5%8A%9FCommunity">https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasetså…¬å¼€è®¿é—®ï¼Œä»¥ä¿ƒè¿›ç ”ç©¶ç¤¾åŒºçš„é€æ˜åº¦å’Œå¯é‡å¤æ€§ã€‚</a>%EF%BC%}æœ¬é“¾æ¥æä¾›äº†ä¸€ä¸ªå…¬å¼€é€æ˜çš„ç¯å¢ƒä»¥ä¾¿äºç§‘ç ”äººå‘˜åˆä½œè¿›æ­¥å¹¶æŒç»­æ¢ç´¢ä¸ç²¾è¿›å…³äºå¤§å‹è¯­è¨€æ¨¡å‹å…¬å¹³æ€§çš„ç ”ç©¶é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23411v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è°ƒæŸ¥äº†å½“å‰è¯­è¨€æ¨¡å‹ç ”ç©¶ä¸­å¹¿æ³›ä½¿ç”¨çš„å…¬å¹³æ€§æ•°æ®é›†ï¼Œé€šè¿‡ä»‹ç»ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶æ¥æ­ç¤ºæ•°æ®é›†å’Œè¯„åˆ†æ–¹æ³•ä¸­çš„æŒç»­äººå£ç»Ÿè®¡å·®å¼‚æ¨¡å¼ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°äº†è§£è¿™äº›èµ„æºä¸­çš„å‡è®¾å’Œå±€é™æ€§ã€‚æ–‡ç« å¼ºè°ƒäº†å¸¸è¢«å¿½è§†çš„åè§å¯¹æ¨¡å‹å…¬å¹³æ€§çš„å½±å“ï¼Œå¹¶ä¸ºé€‰æ‹©ã€ç»„åˆå’Œè§£é‡Šè¿™äº›æ•°æ®é›†æä¾›äº†å®ç”¨æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¬å¹³æ€§åŸºå‡†åœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹æ—¶èµ·åˆ°æ ¸å¿ƒä½œç”¨ï¼Œä½†äººä»¬å¾€å¾€å¿½è§†å…¶æ‰€ä¾èµ–çš„æ•°æ®é›†ã€‚</li>
<li>æœ¬æ–‡å…¨é¢å®¡æŸ¥äº†å½“å‰å¹¿æ³›ä½¿ç”¨çš„å…¬å¹³æ€§æ•°æ®é›†ï¼ŒåŒ…æ‹¬å…¶æ¥æºã€èŒƒå›´ã€å†…å®¹å’Œç”¨é€”ã€‚</li>
<li>ä»‹ç»äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œä»¥æ­ç¤ºæ•°æ®é›†å’Œè¯„åˆ†æ–¹æ³•ä¸­çš„æŒç»­äººå£ç»Ÿè®¡å·®å¼‚æ¨¡å¼ã€‚</li>
<li>é€šè¿‡åº”ç”¨æ­¤æ¡†æ¶äº24ä¸ªå¸¸è§åŸºå‡†æµ‹è¯•ï¼Œçªå‡ºäº†å¯èƒ½å½±å“æ¨¡å‹å…¬å¹³æ€§çš„å¸¸è¢«å¿½è§†çš„åè§ã€‚</li>
<li>æä¾›äº†å…³äºå¦‚ä½•é€‰æ‹©ã€ç»„åˆå’Œè§£é‡Šè¿™äº›æ•°æ®é›†çš„å®ç”¨æŒ‡å¯¼ã€‚</li>
<li>æŒ‡å‡ºåˆ›å»ºåæ˜ æ›´å¤šç¤¾ä¼šèƒŒæ™¯çš„æ–°å…¬å¹³æ€§åŸºå‡†çš„æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86a6e745ae195b719e51b4342f674f07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9317473107dc214065097edb171ab2db.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Teaching-a-Language-Model-to-Speak-the-Language-of-Tools"><a href="#Teaching-a-Language-Model-to-Speak-the-Language-of-Tools" class="headerlink" title="Teaching a Language Model to Speak the Language of Tools"></a>Teaching a Language Model to Speak the Language of Tools</h2><p><strong>Authors:Simeon Emanuilov</strong></p>
<p>External tool integration through function-calling is essential for practical language model applications, yet most multilingual models lack reliable tool-use capabilities in non-English languages. Even state-of-the-art multilingual models struggle with determining when to use tools and generating the structured outputs required for function calls, often exhibiting language confusion when prompted in lower-resource languages. This work presents a methodology for adapting existing language models to enable robust tool use in any target language, using Bulgarian as a case study. The approach involves continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a novel bilingual dataset of 10,035 function-calling examples designed to support standardized protocols like MCP (Model Context Protocol). The research introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to 28.75% improvement in function-calling accuracy over base models while preserving core language understanding, as verified on established Bulgarian benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready response formatting with clean, parsable function calls, contrasting with the verbose and inconsistent outputs of base models. The models, evaluation framework, and dataset are released to enable replication for other languages. This work demonstrates a practical approach for extending tool-augmented capabilities beyond English-centric systems. </p>
<blockquote>
<p>é€šè¿‡å‡½æ•°è°ƒç”¨è¿›è¡Œå¤–éƒ¨å·¥å…·é›†æˆå¯¹äºå®é™…è¯­è¨€æ¨¡å‹åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†å¤§å¤šæ•°å¤šè¯­ç§æ¨¡å‹åœ¨éè‹±è¯­è¯­å¢ƒä¸‹çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›å¹¶ä¸å¯é ã€‚å³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¤šè¯­ç§æ¨¡å‹ä¹Ÿéš¾ä»¥ç¡®å®šä½•æ—¶ä½¿ç”¨å·¥å…·ä»¥åŠç”Ÿæˆæ‰€éœ€çš„å‡½æ•°è°ƒç”¨ç»“æ„è¾“å‡ºï¼Œåœ¨éèµ„æºè¯­è¨€æç¤ºæ—¶ç»å¸¸å‡ºç°è¯­è¨€æ··æ·†ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é€‚åº”ç°æœ‰è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä»»ä½•ç›®æ ‡è¯­è¨€ä¸­ç¨³å¥åœ°ä½¿ç”¨å·¥å…·ï¼Œå¹¶ä»¥ä¿åŠ åˆ©äºšè¯­ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ã€‚è¯¥æ–¹æ³•æ¶‰åŠåœ¨æ–°å‹åŒè¯­æ•°æ®é›†ä¸ŠæŒç»­è®­ç»ƒBgGPTæ¨¡å‹ç³»åˆ—ï¼ˆ2.6Bã€9Bã€27Bå‚æ•°ï¼‰ï¼Œè¯¥æ•°æ®é›†åŒ…å«10,035ä¸ªå‡½æ•°è°ƒç”¨ç¤ºä¾‹ï¼Œæ”¯æŒå¦‚MCPï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰ç­‰æ ‡å‡†åŒ–åè®®ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†TUCANï¼ˆèƒ½å¤Ÿä½¿ç”¨å·¥å…·çš„åŠ©ç†å¯¼èˆªå™¨ï¼‰ï¼Œä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨å‡½æ•°è°ƒç”¨å‡†ç¡®æ€§æ–¹é¢æé«˜äº†é«˜è¾¾28.75%ï¼ŒåŒæ—¶ä¿ç•™äº†æ ¸å¿ƒè¯­è¨€ç†è§£åŠŸèƒ½ï¼Œè¿™åœ¨å·²å»ºç«‹çš„ä¿åŠ åˆ©äºšåŸºå‡†æµ‹è¯•ä¸­å¾—åˆ°äº†éªŒè¯ã€‚é™¤äº†æé«˜å‡†ç¡®æ€§å¤–ï¼ŒTUCANæ¨¡å‹è¿˜å±•ç¤ºäº†ç”Ÿäº§å°±ç»ªçš„å“åº”æ ¼å¼ï¼Œå…·æœ‰å¹²å‡€ã€å¯è§£æçš„å‡½æ•°è°ƒç”¨ï¼Œä¸åŸºç¡€æ¨¡å‹çš„å†—é•¿å’Œè¾“å‡ºä¸ä¸€è‡´å½¢æˆé²œæ˜å¯¹æ¯”ã€‚ä¸ºäº†åœ¨å…¶ä»–è¯­è¨€ä¸­å®ç°å¤åˆ¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æ¨¡å‹ã€è¯„ä¼°æ¡†æ¶å’Œæ•°æ®é›†ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä¸€ç§å°†å·¥å…·å¢å¼ºåŠŸèƒ½æ‰©å±•åˆ°è‹±è¯­ä¸­å¿ƒç³»ç»Ÿä»¥å¤–çš„å®ç”¨æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23394v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé€šè¿‡å‡½æ•°è°ƒç”¨å®ç°å¤–éƒ¨å·¥å…·é›†æˆå¯¹äºå®é™…è¯­è¨€æ¨¡å‹åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†å¤§å¤šæ•°å¤šè¯­è¨€æ¨¡å‹åœ¨éè‹±è¯­ç¯å¢ƒä¸­çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ä¸å¯é ã€‚æœ¬æ–‡æå‡ºä¸€ç§é€‚åº”ç°æœ‰è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä»»ä½•ç›®æ ‡è¯­è¨€ä¸­ç¨³å¥åœ°ä½¿ç”¨å·¥å…·ï¼Œå¹¶ä»¥ä¿åŠ åˆ©äºšè¯­ä¸ºæ¡ˆä¾‹è¿›è¡Œç ”ç©¶ã€‚è¯¥æ–¹æ³•æ¶‰åŠç»§ç»­è®­ç»ƒBgGPTæ¨¡å‹ç³»åˆ—ï¼ˆ2.6Bã€9Bã€27Bå‚æ•°ï¼‰ï¼Œåœ¨10,035ä¸ªå‡½æ•°è°ƒç”¨ç¤ºä¾‹çš„æ–°åŒè¯­æ•°æ®é›†ä¸Šæ”¯æŒæ ‡å‡†åŒ–åè®®ï¼ˆå¦‚MCPï¼‰ã€‚ç ”ç©¶ä¸­å¼•å…¥äº†TUCANï¼ˆå·¥å…·ä½¿ç”¨èƒ½åŠ›è¾…åŠ©å¯¼èˆªå™¨ï¼‰ï¼Œä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œå…¶åœ¨å‡½æ•°è°ƒç”¨å‡†ç¡®æ€§æ–¹é¢æé«˜äº†28.75%ï¼ŒåŒæ—¶ä¿ç•™äº†æ ¸å¿ƒè¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå¹¶åœ¨ä¿åŠ åˆ©äºšè¯­åŸºå‡†æµ‹è¯•ä¸Šå¾—åˆ°äº†éªŒè¯ã€‚æ­¤å¤–ï¼ŒTUCANæ¨¡å‹è¿˜å±•ç¤ºäº†æ¸…æ™°ã€å¯è§£æçš„å‡½æ•°è°ƒç”¨å“åº”æ ¼å¼ï¼Œä¸åŸºç¡€æ¨¡å‹çš„å†—é•¿å’Œè¾“å‡ºä¸ä¸€è‡´å½¢æˆé²œæ˜å¯¹æ¯”ã€‚è¯¥æ¨¡å‹ã€è¯„ä¼°æ¡†æ¶å’Œæ•°æ®é›†å·²å‘å¸ƒï¼Œä»¥æ”¯æŒåœ¨å…¶ä»–è¯­è¨€ä¸­çš„å¤åˆ¶ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†å°†å·¥å…·å¢å¼ºåŠŸèƒ½æ‰©å±•åˆ°è‹±è¯­ä»¥å¤–ç³»ç»Ÿçš„å®ç”¨æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤–éƒ¨å·¥å…·é›†æˆå¯¹äºè¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>å¤šè¯­è¨€æ¨¡å‹åœ¨éè‹±è¯­ç¯å¢ƒä¸­çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€‚åº”ç°æœ‰è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿å…¶èƒ½åœ¨ä»»ä½•ç›®æ ‡è¯­è¨€ä¸­ç¨³å¥åœ°ä½¿ç”¨å·¥å…·ã€‚</li>
<li>æ–¹æ³•æ¶‰åŠç»§ç»­è®­ç»ƒBgGPTæ¨¡å‹ç³»åˆ—ï¼Œå¹¶åœ¨æ–°åŒè¯­æ•°æ®é›†ä¸Šè¿›è¡Œæ”¯æŒæ ‡å‡†åŒ–åè®®çš„è®­ç»ƒã€‚</li>
<li>TUCANï¼ˆå·¥å…·ä½¿ç”¨èƒ½åŠ›è¾…åŠ©å¯¼èˆªå™¨ï¼‰åœ¨å‡½æ•°è°ƒç”¨å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—æé«˜ï¼ŒåŒæ—¶ä¿ç•™æ ¸å¿ƒè¯­è¨€ç†è§£èƒ½åŠ›ã€‚</li>
<li>TUCANæ¨¡å‹å…·æœ‰æ¸…æ™°ã€å¯è§£æçš„å“åº”æ ¼å¼ï¼Œä¸åŸºç¡€æ¨¡å‹çš„è¾“å‡ºå½¢æˆå¯¹æ¯”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23394">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-35a67e778201570590a127234dcc2c8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96127d8336a1712779256997be36cc34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d392f7e431f3acafc513a1f3d7f6936b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc4152bb01ef0715da27cea064226946.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GeoProg3D-Compositional-Visual-Reasoning-for-City-Scale-3D-Language-Fields"><a href="#GeoProg3D-Compositional-Visual-Reasoning-for-City-Scale-3D-Language-Fields" class="headerlink" title="GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language   Fields"></a>GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language   Fields</h2><p><strong>Authors:Shunsuke Yasuki, Taiki Miyanishi, Nakamasa Inoue, Shuhei Kurita, Koya Sakamoto, Daichi Azuma, Masato Taki, Yutaka Matsuo</strong></p>
<p>The advancement of 3D language fields has enabled intuitive interactions with 3D scenes via natural language. However, existing approaches are typically limited to small-scale environments, lacking the scalability and compositional reasoning capabilities necessary for large, complex urban settings. To overcome these limitations, we propose GeoProg3D, a visual programming framework that enables natural language-driven interactions with city-scale high-fidelity 3D scenes. GeoProg3D consists of two key components: (i) a Geography-aware City-scale 3D Language Field (GCLF) that leverages a memory-efficient hierarchical 3D model to handle large-scale data, integrated with geographic information for efficiently filtering vast urban spaces using directional cues, distance measurements, elevation data, and landmark references; and (ii) Geographical Vision APIs (GV-APIs), specialized geographic vision tools such as area segmentation and object detection. Our framework employs large language models (LLMs) as reasoning engines to dynamically combine GV-APIs and operate GCLF, effectively supporting diverse geographic vision tasks. To assess performance in city-scale reasoning, we introduce GeoEval3D, a comprehensive benchmark dataset containing 952 query-answer pairs across five challenging tasks: grounding, spatial reasoning, comparison, counting, and measurement. Experiments demonstrate that GeoProg3D significantly outperforms existing 3D language fields and vision-language models across multiple tasks. To our knowledge, GeoProg3D is the first framework enabling compositional geographic reasoning in high-fidelity city-scale 3D environments via natural language. The code is available at <a target="_blank" rel="noopener" href="https://snskysk.github.io/GeoProg3D/">https://snskysk.github.io/GeoProg3D/</a>. </p>
<blockquote>
<p>ä¸‰ç»´è¯­è¨€é¢†åŸŸçš„è¿›æ­¥å·²ç»èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€ä¸ä¸‰ç»´åœºæ™¯è¿›è¡Œç›´è§‚äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å±€é™äºå°è§„æ¨¡ç¯å¢ƒï¼Œç¼ºä¹åœ¨å¤§è§„æ¨¡ã€å¤æ‚çš„åŸå¸‚ç¯å¢ƒä¸­æ‰€éœ€çš„å¯æ‰©å±•æ€§å’Œç»„åˆæ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†GeoProg3Dï¼Œè¿™æ˜¯ä¸€ä¸ªè§†è§‰ç¼–ç¨‹æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€ä¸åŸå¸‚è§„æ¨¡çš„é«˜ä¿çœŸä¸‰ç»´åœºæ™¯è¿›è¡Œäº¤äº’ã€‚GeoProg3Dç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶æ„æˆï¼šï¼ˆiï¼‰åœ°ç†æ„ŸçŸ¥åŸå¸‚è§„æ¨¡ä¸‰ç»´è¯­è¨€é¢†åŸŸï¼ˆGCLFï¼‰ï¼Œå®ƒåˆ©ç”¨é«˜æ•ˆçš„åˆ†å±‚ä¸‰ç»´æ¨¡å‹å¤„ç†å¤§è§„æ¨¡æ•°æ®ï¼Œå¹¶ç»“åˆåœ°ç†ä¿¡æ¯ï¼Œé€šè¿‡æ–¹å‘çº¿ç´¢ã€è·ç¦»æµ‹é‡ã€æµ·æ‹”æ•°æ®å’Œåœ°æ ‡å‚è€ƒæœ‰æ•ˆåœ°è¿‡æ»¤å¹¿é˜”çš„åŸå¸‚ç©ºé—´ï¼›ï¼ˆiiï¼‰åœ°ç†è§†è§‰APIï¼ˆGV-APIsï¼‰ï¼ŒåŒ…æ‹¬åŒºåŸŸåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ç­‰ä¸“ä¸šçš„åœ°ç†è§†è§‰å·¥å…·ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºæ¨ç†å¼•æ“ï¼ŒåŠ¨æ€ç»“åˆGV-APIså¹¶æ“ä½œGCLFï¼Œæœ‰æ•ˆæ”¯æŒå„ç§åœ°ç†è§†è§‰ä»»åŠ¡ã€‚ä¸ºäº†è¯„ä¼°åœ¨åŸå¸‚è§„æ¨¡æ¨ç†ä¸­çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†GeoEval3Dï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«952ä¸ªæŸ¥è¯¢ç­”æ¡ˆå¯¹çš„ç»¼åˆåŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼šæ¥åœ°ã€ç©ºé—´æ¨ç†ã€æ¯”è¾ƒã€è®¡æ•°å’Œæµ‹é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒGeoProg3Dåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸‰ç»´è¯­è¨€é¢†åŸŸå’Œè§†è§‰è¯­è¨€æ¨¡å‹ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒGeoProg3Dæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€åœ¨é«˜ä¿çœŸåŸå¸‚è§„æ¨¡ä¸‰ç»´ç¯å¢ƒä¸­å®ç°ç»„åˆåœ°ç†æ¨ç†çš„æ¡†æ¶ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://snskysk.github.io/GeoProg3D/%E6%89%BE%E5%88%B0%E3%80%82">https://snskysk.github.io/GeoProg3D/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23352v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†GeoProg3Dæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡è‡ªç„¶è¯­è¨€é©±åŠ¨ä¸åŸå¸‚è§„æ¨¡çš„é«˜ä¿çœŸ3Dåœºæ™¯è¿›è¡Œäº¤äº’ã€‚å®ƒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šåœ°ç†æ„ŸçŸ¥åŸå¸‚è§„æ¨¡ä¸‰ç»´è¯­è¨€åœºï¼ˆGCLFï¼‰å’Œåœ°ç†è§†è§‰APIï¼ˆGV-APIsï¼‰ã€‚æ¡†æ¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ¨ç†å¼•æ“æ¥ç»“åˆGV-APIså¹¶æ“ä½œGCLFï¼Œæ”¯æŒå¤šæ ·åŒ–çš„åœ°ç†è§†è§‰ä»»åŠ¡ã€‚æ¡†æ¶å¼•å…¥GeoEval3Dæ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¼˜è¶Šæ€§èƒ½ã€‚è¿™æ˜¯é¦–ä¸ªæ”¯æŒåŸå¸‚è§„æ¨¡ä¸‰ç»´ç¯å¢ƒä¸­çš„è‡ªç„¶è¯­è¨€çš„åœ°ç†æ¨ç†æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GeoProg3Dæ˜¯ä¸€ä¸ªè§†è§‰ç¼–ç¨‹æ¡†æ¶ï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€ä¸åŸå¸‚è§„æ¨¡çš„é«˜ä¿çœŸ3Dåœºæ™¯äº¤äº’ã€‚</li>
<li>å®ƒåŒ…å«åœ°ç†æ„ŸçŸ¥åŸå¸‚è§„æ¨¡ä¸‰ç»´è¯­è¨€åœºï¼ˆGCLFï¼‰å’Œåœ°ç†è§†è§‰APIï¼ˆGV-APIsï¼‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>GCLFåˆ©ç”¨é«˜æ•ˆçš„ä¸‰ç»´æ¨¡å‹å¤„ç†å¤§è§„æ¨¡æ•°æ®ï¼Œå¹¶ç»“åˆåœ°ç†ä¿¡æ¯å®ç°å¤§è§„æ¨¡ç©ºé—´çš„è¿‡æ»¤ã€‚</li>
<li>GV-APIsæä¾›åœ°ç†è§†è§‰å·¥å…·ï¼Œå¦‚åŒºåŸŸåˆ†å‰²å’Œå¯¹è±¡æ£€æµ‹ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºæ¨ç†å¼•æ“ï¼Œå®ç°å¤šæ ·åŒ–çš„åœ°ç†è§†è§‰ä»»åŠ¡æ”¯æŒã€‚</li>
<li>å¼•å…¥GeoEval3Dæ•°æ®é›†è¯„ä¼°æ€§èƒ½ï¼Œæ¶µç›–äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>GeoProg3Dåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸‰ç»´è¯­è¨€åœºå’Œè§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23352">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-64219242968cd42f5a91bb692a6820d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91833e558e3cc013eeef577def0d1539.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f14cc1f6b09e0ab4debd0c20998cb0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0948fc42aa7ea56b9f20fa69c7d1bf6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-210e96e3c13cc17a4c9ab1b954a9d2c4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Corrupted-by-Reasoning-Reasoning-Language-Models-Become-Free-Riders-in-Public-Goods-Games"><a href="#Corrupted-by-Reasoning-Reasoning-Language-Models-Become-Free-Riders-in-Public-Goods-Games" class="headerlink" title="Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in   Public Goods Games"></a>Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in   Public Goods Games</h2><p><strong>Authors:David Guzman Piedrahita, Yongjin Yang, Mrinmaya Sachan, Giorgia Ramponi, Bernhard SchÃ¶lkopf, Zhijing Jin</strong></p>
<p>As large language models (LLMs) are increasingly deployed as autonomous agents, understanding their cooperation and social mechanisms is becoming increasingly important. In particular, how LLMs balance self-interest and collective well-being is a critical challenge for ensuring alignment, robustness, and safe deployment. In this paper, we examine the challenge of costly sanctioning in multi-agent LLM systems, where an agent must decide whether to invest its own resources to incentivize cooperation or penalize defection. To study this, we adapt a public goods game with institutional choice from behavioral economics, allowing us to observe how different LLMs navigate social dilemmas over repeated interactions. Our analysis reveals four distinct behavioral patterns among models: some consistently establish and sustain high levels of cooperation, others fluctuate between engagement and disengagement, some gradually decline in cooperative behavior over time, and others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we find that reasoning LLMs, such as the o1 series, struggle significantly with cooperation, whereas some traditional LLMs consistently achieve high levels of cooperation. These findings suggest that the current approach to improving LLMs, which focuses on enhancing their reasoning capabilities, does not necessarily lead to cooperation, providing valuable insights for deploying LLM agents in environments that require sustained collaboration. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/davidguzmanp/SanctSim">https://github.com/davidguzmanp/SanctSim</a> </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²ä¸ºè‡ªä¸»ä»£ç†ï¼Œç†è§£å®ƒä»¬çš„åˆä½œå’Œç¤¾ä¼šæœºåˆ¶å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç‰¹åˆ«æ˜¯ï¼ŒLLMå¦‚ä½•å¹³è¡¡è‡ªæˆ‘åˆ©ç›Šå’Œé›†ä½“ç¦ç¥‰æ˜¯ç¡®ä¿å…¶å¯¹é½ã€ç¨³å¥å’Œå®‰å…¨éƒ¨ç½²çš„å…³é”®æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤šä»£ç†LLMç³»ç»Ÿä¸­æˆæœ¬é«˜æ˜‚çš„åˆ¶è£æŒ‘æˆ˜ï¼Œå…¶ä¸­ä»£ç†å¿…é¡»å†³å®šæ˜¯æŠ•å…¥è‡ªå·±çš„èµ„æºæ¥æ¿€åŠ±åˆä½œè¿˜æ˜¯æƒ©ç½šè¿è§„è¡Œä¸ºã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸ªï¼Œæˆ‘ä»¬ä»è¡Œä¸ºç»æµå­¦ä¸­é‡‡ç”¨äº†å…¬å…±ç‰©å“æ¸¸æˆä¸åˆ¶åº¦é€‰æ‹©çš„æ–¹æ³•ï¼Œè®©æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ä¸åŒçš„LLMå¦‚ä½•é‡å¤äº’åŠ¨å¹¶è§£å†³ç¤¾ä¼šå›°å¢ƒã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†å››ç§ä¸åŒçš„è¡Œä¸ºæ¨¡å¼ï¼šä¸€äº›æ¨¡å‹å§‹ç»ˆå»ºç«‹å’Œç»´æŒé«˜æ°´å¹³çš„åˆä½œï¼Œå¦ä¸€äº›åˆ™åœ¨å‚ä¸å’Œä¸å‚ä¸ä¹‹é—´æ³¢åŠ¨ï¼Œè¿˜æœ‰ä¸€äº›éšç€æ—¶é—´çš„æ¨ç§»é€æ¸åœ¨åˆä½œè¡Œä¸ºä¸Šå‡å¼±ï¼Œè€Œå…¶ä»–æ¨¡å‹åˆ™åƒµåŒ–åœ°éµå¾ªå›ºå®šç­–ç•¥è€Œä¸è€ƒè™‘ç»“æœã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åƒO1ç³»åˆ—è¿™æ ·çš„æ¨ç†LLMåœ¨åˆä½œæ–¹é¢é‡åˆ°äº†é‡å¤§å›°éš¾ï¼Œè€Œä¸€äº›ä¼ ç»ŸLLMåˆ™å§‹ç»ˆå®ç°äº†é«˜æ°´å¹³çš„åˆä½œã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå½“å‰æ”¹è¿›LLMçš„æ–¹æ³•â€”â€”ä¸»è¦é›†ä¸­åœ¨æé«˜å…¶æ¨ç†èƒ½åŠ›ä¸Šâ€”â€”å¹¶ä¸ä¸€å®šå¯¼è‡´åˆä½œï¼Œè¿™ä¸ºåœ¨éœ€è¦æŒç»­åä½œçš„ç¯å¢ƒä¸­éƒ¨ç½²LLMä»£ç†æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/davidguzmanp/SanctSim">https://github.com/davidguzmanp/SanctSim</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23276v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè‡ªä¸»ä»£ç†çš„éƒ¨ç½²æ—¥ç›Šæ™®éï¼Œç†è§£å…¶åˆä½œå’Œç¤¾ä¼šæœºåˆ¶å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡ç ”ç©¶äº†å¤šä»£ç†LLMç³»ç»Ÿä¸­æˆæœ¬é«˜æ˜‚çš„åˆ¶è£æŒ‘æˆ˜ï¼Œå…¶ä¸­ä»£ç†å¿…é¡»å†³å®šæ˜¯æŠ•å…¥è‡ªèº«èµ„æºä»¥æ¿€åŠ±åˆä½œè¿˜æ˜¯æƒ©ç½šèƒŒå›ã€‚é€šè¿‡é€‚åº”è¡Œä¸ºç»æµå­¦çš„å…¬å…±ç‰©å“æ¸¸æˆä¸åˆ¶åº¦é€‰æ‹©ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸åŒLLMåœ¨é‡å¤äº’åŠ¨ä¸­å¦‚ä½•è§£å†³ç¤¾ä¼šå›°å¢ƒã€‚åˆ†ææ­ç¤ºäº†å››ç§ä¸åŒçš„è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶æ„å¤–å‘ç°æ³¨é‡æ¨ç†çš„LLMï¼Œå¦‚o1ç³»åˆ—ï¼Œåœ¨åˆä½œæ–¹é¢å­˜åœ¨æ˜æ˜¾å›°éš¾ï¼Œè€Œä¸€äº›ä¼ ç»ŸLLMåˆ™å§‹ç»ˆç»´æŒé«˜æ°´å¹³åˆä½œã€‚è¿™ä¸ºåœ¨éœ€è¦æŒç»­åä½œçš„ç¯å¢ƒä¸­éƒ¨ç½²LLMä»£ç†æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆä½œå’Œç¤¾ä¼šæœºåˆ¶åœ¨è‡ªä¸»ä»£ç†éƒ¨ç½²ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>åœ¨å¤šä»£ç†LLMç³»ç»Ÿä¸­ï¼Œä»£ç†é¢ä¸´æˆæœ¬é«˜æ˜‚çš„åˆ¶è£æŒ‘æˆ˜æ—¶éœ€è¦å¹³è¡¡è‡ªæˆ‘åˆ©ç›Šå’Œé›†ä½“ç¦ç¥‰ã€‚</li>
<li>é€šè¿‡å…¬å…±ç‰©å“æ¸¸æˆä¸åˆ¶åº¦é€‰æ‹©çš„ç ”ç©¶æ–¹æ³•ï¼Œè§‚å¯Ÿåˆ°LLMè§£å†³ç¤¾ä¼šå›°å¢ƒçš„å››ç§ä¸åŒè¡Œä¸ºæ¨¡å¼ã€‚</li>
<li>æ¨ç†å‹LLMï¼ˆå¦‚o1ç³»åˆ—ï¼‰åœ¨åˆä½œæ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>ä¼ ç»ŸLLMèƒ½æŒç»­ç»´æŒé«˜æ°´å¹³åˆä½œã€‚</li>
<li>æ”¹è¿›LLMçš„æ¨ç†èƒ½åŠ›å¹¶ä¸ä¸€å®šä¼šæé«˜å…¶åˆä½œèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23276">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0145002b0574d232ba1cf2d67e9e95d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-751bf07f9cfc820da9f98f6959156735.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2569a04b1d3e891f65e6f6001a58467b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Why-Settle-for-One-Text-to-ImageSet-Generation-and-Evaluation"><a href="#Why-Settle-for-One-Text-to-ImageSet-Generation-and-Evaluation" class="headerlink" title="Why Settle for One? Text-to-ImageSet Generation and Evaluation"></a>Why Settle for One? Text-to-ImageSet Generation and Evaluation</h2><p><strong>Authors:Chengyou Jia, Xin Shen, Zhuohang Dang, Zhuohang Dang, Changliang Xia, Weijia Wu, Xinyu Zhang, Hangwei Qian, Ivor W. Tsang, Minnan Luo</strong></p>
<p>Despite remarkable progress in Text-to-Image models, many real-world applications require generating coherent image sets with diverse consistency requirements. Existing consistent methods often focus on a specific domain with specific aspects of consistency, which significantly constrains their generalizability to broader applications. In this paper, we propose a more challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate sets of images that meet various consistency requirements based on user instructions. To systematically study this problem, we first introduce $\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories, providing comprehensive coverage for T2IS generation. Building on this, we propose $\textbf{T2IS-Eval}$, an evaluation framework that transforms user instructions into multifaceted assessment criteria and employs effective evaluators to adaptively assess consistency fulfillment between criteria and generated sets. Subsequently, we propose $\textbf{AutoT2IS}$, a training-free framework that maximally leverages pretrained Diffusion Transformersâ€™ in-context capabilities to harmonize visual elements to satisfy both image-level prompt alignment and set-level visual consistency. Extensive experiments on T2IS-Bench reveal that diverse consistency challenges all existing methods, while our AutoT2IS significantly outperforms current generalized and even specialized approaches. Our method also demonstrates the ability to enable numerous underexplored real-world applications, confirming its substantial practical value. Visit our project in <a target="_blank" rel="noopener" href="https://chengyou-jia.github.io/T2IS-Home">https://chengyou-jia.github.io/T2IS-Home</a>. </p>
<blockquote>
<p>å°½ç®¡æ–‡æœ¬åˆ°å›¾åƒï¼ˆText-to-Imageï¼‰æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨è®¸å¤šç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼Œéœ€è¦ç”Ÿæˆå…·æœ‰ä¸åŒä¸€è‡´æ€§è¦æ±‚çš„è¿è´¯å›¾åƒé›†ã€‚ç°æœ‰çš„ä¸€è‡´æ€§æ–¹æ³•é€šå¸¸ä¸“æ³¨äºå…·æœ‰ç‰¹å®šä¸€è‡´æ€§çš„ç‰¹å®šé¢†åŸŸï¼Œè¿™æ˜¾è‘—é™åˆ¶äº†å®ƒä»¬åœ¨æ›´å¹¿æ³›åº”ç”¨ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå³æ–‡æœ¬åˆ°å›¾åƒé›†ï¼ˆText-to-ImageSetï¼ŒT2ISï¼‰ç”Ÿæˆã€‚å®ƒçš„ç›®æ ‡æ˜¯åŸºäºç”¨æˆ·æŒ‡ä»¤ç”Ÿæˆæ»¡è¶³å„ç§ä¸€è‡´æ€§è¦æ±‚çš„å›¾åƒé›†ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†T2IS-Benchï¼Œå®ƒåŒ…å«26ä¸ªå­ç±»åˆ«ä¸­çš„596æ¡å¤šæ ·åŒ–æŒ‡ä»¤ï¼Œä¸ºT2ISç”Ÿæˆæä¾›äº†å…¨é¢çš„è¦†ç›–ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†T2IS-Evalè¯„ä¼°æ¡†æ¶ï¼Œå®ƒå°†ç”¨æˆ·æŒ‡ä»¤è½¬åŒ–ä¸ºå¤šæ–¹é¢çš„è¯„ä¼°æ ‡å‡†ï¼Œå¹¶é‡‡ç”¨æœ‰æ•ˆçš„è¯„ä¼°å™¨æ¥é€‚åº”æ€§åœ°è¯„ä¼°ç”Ÿæˆé›†ä¸æ ‡å‡†ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚éšåï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€è®­ç»ƒçš„AutoT2ISæ¡†æ¶ï¼Œå®ƒæœ€å¤§é™åº¦åœ°åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£å˜å‹å™¨çš„ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œåè°ƒè§†è§‰å…ƒç´ ï¼Œä»¥æ»¡è¶³å›¾åƒçº§åˆ«çš„æç¤ºå¯¹é½å’Œé›†åˆçº§åˆ«çš„è§†è§‰ä¸€è‡´æ€§ã€‚åœ¨T2IS-Benchä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå¤šæ ·åŒ–çš„ä¸€è‡´æ€§æŒ‘æˆ˜äº†æ‰€æœ‰ç°æœ‰æ–¹æ³•ï¼Œè€Œæˆ‘ä»¬çš„AutoT2ISæ˜¾è‘—ä¼˜äºå½“å‰çš„é€šç”¨ç”šè‡³ä¸“ç”¨æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜è¯æ˜äº†åœ¨æ— æ•°å°šæœªæ¢ç´¢çš„ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®ç”¨æ€§ï¼Œè¯å®äº†å…¶å·¨å¤§çš„å®é™…ä»·å€¼ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://chengyou-jia.github.io/T2IS-Home%E3%80%82">https://chengyou-jia.github.io/T2IS-Homeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23275v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªæ–°çš„æŒ‘æˆ˜æ€§é—®é¢˜â€”â€”æ–‡æœ¬åˆ°å›¾åƒé›†ç”Ÿæˆï¼ˆT2ISï¼‰ï¼Œæ—¨åœ¨æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ç”Ÿæˆæ»¡è¶³å„ç§ä¸€è‡´æ€§è¦æ±‚çš„å›¾åƒé›†ã€‚ä¸ºç³»ç»Ÿç ”ç©¶è¿™ä¸€é—®é¢˜ï¼Œä»‹ç»T2IS-Benchï¼ŒåŒ…å«596æ¡è·¨è¶Š26ä¸ªå­ç±»åˆ«çš„æŒ‡ä»¤ï¼Œä¸ºT2ISç”Ÿæˆæä¾›å…¨é¢è¦†ç›–ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºT2IS-Evalè¯„ä¼°æ¡†æ¶ï¼Œå°†ç”¨æˆ·æŒ‡ä»¤è½¬åŒ–ä¸ºå¤šå…ƒè¯„ä¼°æ ‡å‡†ï¼Œå¹¶æœ‰æ•ˆåˆ©ç”¨è¯„ä¼°å™¨è‡ªé€‚åº”è¯„ä¼°ç”Ÿæˆå›¾åƒé›†çš„ä¸€è‡´æ€§æ»¡è¶³ç¨‹åº¦ã€‚æœ€åï¼Œæå‡ºæ— éœ€è®­ç»ƒçš„AutoT2ISæ¡†æ¶ï¼Œæœ€å¤§åŒ–åˆ©ç”¨é¢„è®­ç»ƒçš„Diffusion Transformersçš„ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œåè°ƒè§†è§‰å…ƒç´ ï¼Œæ»¡è¶³å›¾åƒçº§æç¤ºå¯¹é½å’Œé›†çº§è§†è§‰ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒAutoT2ISåœ¨T2IS-Benchä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰é€šç”¨åŠä¸“ç”¨æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†åœ¨å¤šä¸ªæœªæ¢ç´¢çš„å®é™…åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒé›†ç”Ÿæˆï¼ˆT2ISï¼‰æ˜¯ä¸€ä¸ªæ–°çš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæ—¨åœ¨æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ç”Ÿæˆæ»¡è¶³å¤šç§ä¸€è‡´æ€§è¦æ±‚çš„å›¾åƒé›†ã€‚</li>
<li>T2IS-BenchåŒ…å«596æ¡æŒ‡ä»¤ï¼Œè¦†ç›–26ä¸ªå­ç±»åˆ«ï¼Œä¸ºT2ISç”Ÿæˆæä¾›å…¨é¢è¦†ç›–ã€‚</li>
<li>T2IS-Evalæ¡†æ¶èƒ½å¤Ÿå°†ç”¨æˆ·æŒ‡ä»¤è½¬åŒ–ä¸ºå¤šå…ƒè¯„ä¼°æ ‡å‡†ï¼Œå¹¶è‡ªé€‚åº”è¯„ä¼°ç”Ÿæˆå›¾åƒé›†çš„ä¸€è‡´æ€§ã€‚</li>
<li>AutoT2ISæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„Diffusion Transformersçš„ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œæ»¡è¶³å›¾åƒçº§å’Œé›†çº§çš„ä¸€è‡´æ€§è¦æ±‚ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒAutoT2ISåœ¨T2IS-Benchä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>AutoT2ISå…·æœ‰å®ç°å¤šä¸ªæœªæ¢ç´¢çš„å®é™…åº”ç”¨çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e1cca6f28efacac851a85d2c68d5ad19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bec34dc8593d52eb38ea07fec001780.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04388d2b8f18ae44495d119239c03313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45a8f0719b4a08c6eb085c246c587138.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UrbanLLaVA-A-Multi-modal-Large-Language-Model-for-Urban-Intelligence-with-Spatial-Reasoning-and-Understanding"><a href="#UrbanLLaVA-A-Multi-modal-Large-Language-Model-for-Urban-Intelligence-with-Spatial-Reasoning-and-Understanding" class="headerlink" title="UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence   with Spatial Reasoning and Understanding"></a>UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence   with Spatial Reasoning and Understanding</h2><p><strong>Authors:Jie Feng, Shengyuan Wang, Tianhui Liu, Yanxin Xi, Yong Li</strong></p>
<p>Urban research involves a wide range of scenarios and tasks that require the understanding of multi-modal data. Current methods often focus on specific data types and lack a unified framework in urban field for processing them comprehensively. The recent success of multi-modal large language models (MLLMs) presents a promising opportunity to overcome this limitation. In this paper, we introduce $\textit{UrbanLLaVA}$, a multi-modal large language model designed to process these four types of data simultaneously and achieve strong performance across diverse urban tasks compared with general MLLMs. In $\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset encompassing both single-modal and cross-modal urban data, spanning from location view to global view of urban environment. Additionally, we propose a multi-stage training framework that decouples spatial reasoning enhancement from domain knowledge learning, thereby improving the compatibility and downstream performance of $\textit{UrbanLLaVA}$ across diverse urban tasks. Finally, we also extend existing benchmark for urban research to assess the performance of MLLMs across a wide range of urban tasks. Experimental results from three cities demonstrate that $\textit{UrbanLLaVA}$ outperforms open-source and proprietary MLLMs in both single-modal tasks and complex cross-modal tasks and shows robust generalization abilities across cities. Source codes and data are openly accessible to the research community via <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/UrbanLLaVA">https://github.com/tsinghua-fib-lab/UrbanLLaVA</a>. </p>
<blockquote>
<p>åŸå¸‚ç ”ç©¶æ¶‰åŠå¤šç§åœºæ™¯å’Œä»»åŠ¡ï¼Œéœ€è¦ç†è§£å¤šæ¨¡æ€æ•°æ®ã€‚å½“å‰çš„æ–¹æ³•å¾€å¾€ä¾§é‡äºç‰¹å®šçš„æ•°æ®ç±»å‹ï¼Œç¼ºä¹åŸå¸‚é¢†åŸŸçš„ç»Ÿä¸€æ¡†æ¶æ¥è¿›è¡Œç»¼åˆå¤„ç†ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æœ€æ–°æˆåŠŸä¸ºè§£å†³è¿™ä¸€é™åˆ¶æä¾›äº†æœ‰å‰æ™¯çš„æœºä¼šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†<em>UrbanLLaVA</em>ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨åŒæ—¶å¤„ç†è¿™å››ç§ç±»å‹çš„æ•°æ®ï¼Œå¹¶åœ¨å¤šç§åŸå¸‚ä»»åŠ¡ä¸­ç›¸å¯¹äºä¸€èˆ¬çš„MLLMsè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚åœ¨<em>UrbanLLaVA</em>ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«å•æ¨¡æ€å’Œè·¨æ¨¡æ€åŸå¸‚æ•°æ®çš„å¤šæ ·åŒ–åŸå¸‚æŒ‡ä»¤æ•°æ®é›†ï¼Œæ¶µç›–äº†ä»å±€éƒ¨åˆ°å…¨å±€çš„åŸå¸‚ç¯å¢ƒè§†è§’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œå°†ç©ºé—´æ¨ç†å¢å¼ºä¸é¢†åŸŸçŸ¥è¯†å­¦ä¹ è§£è€¦ï¼Œä»è€Œæé«˜äº†<em>UrbanLLaVA</em>åœ¨å¤šç§åŸå¸‚ä»»åŠ¡ä¸­çš„å…¼å®¹æ€§å’Œä¸‹æ¸¸æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜æ‰©å±•äº†åŸå¸‚ç ”ç©¶çš„ç°æœ‰åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°MLLMsåœ¨å¹¿æ³›çš„åŸå¸‚ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚æ¥è‡ªä¸‰ä¸ªåŸå¸‚çš„å®éªŒç»“æœè¯æ˜ï¼Œ<em>UrbanLLaVA</em>åœ¨å•æ¨¡æ€ä»»åŠ¡å’Œå¤æ‚çš„è·¨æ¨¡æ€ä»»åŠ¡ä¸­éƒ½ä¼˜äºå¼€æºå’Œä¸“æœ‰MLLMsï¼Œå¹¶æ˜¾ç¤ºå‡ºè·¨åŸå¸‚çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚ç›¸å…³æºä»£ç å’Œæ•°æ®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/UrbanLLaVA%E5%AF%B9%E7%A0%94%E7%A9%B6%E7%A4%BE%E5%8C%BA%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/tsinghua-fib-lab/UrbanLLaVAå¯¹ç ”ç©¶ç¤¾åŒºå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23219v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>    åŸå¸‚ç ”ç©¶æ¶‰åŠå¤šç§åœºæ™¯å’Œä»»åŠ¡ï¼Œéœ€è¦ç†è§£å¤šæ¨¡æ€æ•°æ®ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€å…³æ³¨ç‰¹å®šæ•°æ®ç±»å‹ï¼Œç¼ºä¹ç»Ÿä¸€æ¡†æ¶è¿›è¡Œç»¼åˆå¤„ç†ã€‚è¿‘æœŸå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æˆåŠŸä¸ºè§£å†³è¿™ä¸€å±€é™æä¾›äº†æœºä¼šã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé’ˆå¯¹å››ç§æ•°æ®ç±»å‹è®¾è®¡çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹UrbanLLaVAï¼Œç›¸è¾ƒäºä¸€èˆ¬MLLMsï¼Œåœ¨å¤šç§åŸå¸‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚UrbanLLaVAæ„å»ºäº†ä¸€ä¸ªåŒ…å«å•æ¨¡æ€å’Œè·¨æ¨¡æ€åŸå¸‚æ•°æ®çš„å¤šæ ·åŒ–åŸå¸‚æŒ‡ä»¤æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåˆ†é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œä»¥æé«˜å…¶åœ¨å¤šç§åŸå¸‚ä»»åŠ¡ä¸­çš„å…¼å®¹æ€§å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜æ‰©å±•äº†ç°æœ‰åŸå¸‚ç ”ç©¶åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°MLLMsåœ¨å¹¿æ³›çš„åŸå¸‚ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUrbanLLaVAåœ¨å•æ¨¡æ€ä»»åŠ¡å’Œå¤æ‚çš„è·¨æ¨¡æ€ä»»åŠ¡ä¸­å‡è¶…è¶Šäº†å¼€æºå’Œä¸“æœ‰MLLMsï¼Œå¹¶å±•ç°å‡ºè·¨åŸå¸‚çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Urban researchéœ€è¦å¤„ç†å¤šæ¨¡æ€æ•°æ®ï¼Œä½†ç°æœ‰æ–¹æ³•ç¼ºä¹ç»¼åˆå¤„ç†è¿™äº›æ•°æ®çš„ç»Ÿä¸€æ¡†æ¶ã€‚</li>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æˆåŠŸä¸ºåŸå¸‚ç ”ç©¶é¢†åŸŸå¸¦æ¥äº†æ–°çš„æœºé‡ã€‚</li>
<li>UrbanLLaVAæ˜¯ä¸€ä¸ªè®¾è®¡ç”¨äºå¤„ç†å››ç§ç±»å‹æ•°æ®çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œç›¸è¾ƒäºä¸€èˆ¬MLLMsåœ¨å¤šç§åŸå¸‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>UrbanLLaVAæ„å»ºäº†åŒ…å«å•æ¨¡æ€å’Œè·¨æ¨¡æ€æ•°æ®çš„å¤šæ ·åŒ–åŸå¸‚æŒ‡ä»¤æ•°æ®é›†ã€‚</li>
<li>UrbanLLaVAé‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œä»¥æé«˜å…¶åœ¨ä¸åŒåŸå¸‚ä»»åŠ¡ä¸­çš„å…¼å®¹æ€§å’Œæ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒUrbanLLaVAåœ¨å¤šç§ä»»åŠ¡ä¸­è¶…è¶Šäº†å…¶ä»–MLLMsï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-97ea4b5ff46d2708ea2dce0e64830111.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5119ac949e1f54b418a0e4f1bc532dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccb449f12809b0eb8e506b551e8e6041.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239d49d6018fced368263557de9aac9e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-30763d0eea64e409d20378aab02d5420.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c3a3e79c0d2a2dc893f5d7b05e0479f1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Are-Large-Language-Models-Capable-of-Deep-Relational-Reasoning-Insights-from-DeepSeek-R1-and-Benchmark-Comparisons"><a href="#Are-Large-Language-Models-Capable-of-Deep-Relational-Reasoning-Insights-from-DeepSeek-R1-and-Benchmark-Comparisons" class="headerlink" title="Are Large Language Models Capable of Deep Relational Reasoning? Insights   from DeepSeek-R1 and Benchmark Comparisons"></a>Are Large Language Models Capable of Deep Relational Reasoning? Insights   from DeepSeek-R1 and Benchmark Comparisons</h2><p><strong>Authors:Chi Chiu So, Yueyue Sun, Jun-Min Wang, Siu Pang Yung, Anthony Wai Keung Loh, Chun Pong Chau</strong></p>
<p>How far are Large Language Models (LLMs) in performing deep relational reasoning? In this paper, we evaluate and compare the reasoning capabilities of three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a suite of carefully designed benchmark tasks in family tree and general graph reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the highest F1-scores across multiple tasks and problem sizes, demonstrating strong aptitude in logical deduction and relational inference. However, all evaluated models, including DeepSeek-R1, struggle significantly as problem complexity increases, largely due to token length limitations and incomplete output structures. A detailed analysis of DeepSeek-R1â€™s long Chain-of-Thought responses uncovers its unique planning and verification strategies, but also highlights instances of incoherent or incomplete reasoning, calling attention to the need for deeper scrutiny into LLMsâ€™ internal inference dynamics. We further discuss key directions for future work, including the role of multimodal reasoning and the systematic examination of reasoning failures. Our findings provide both empirical insights and theoretical implications for advancing LLMsâ€™ reasoning abilities, particularly in tasks that demand structured, multi-step logical inference. Our code repository will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/kelvinhkcs/Deep-Relational-Reasoning">https://github.com/kelvinhkcs/Deep-Relational-Reasoning</a>. </p>
<blockquote>
<p>å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ·±åº¦å…³ç³»æ¨ç†æ–¹é¢çš„èƒ½åŠ›æœ‰å¤šå¼ºï¼Ÿåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å®¶åº­æ ‘å’Œé€šç”¨å›¾æ¨ç†çš„ç²¾å¿ƒè®¾è®¡çš„åŸºå‡†ä»»åŠ¡ï¼Œè¯„ä¼°å¹¶æ¯”è¾ƒäº†ä¸‰ä¸ªå°–ç«¯LLMï¼ˆå³DeepSeek-R1ã€DeepSeek-V3å’ŒGPT-4oï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDeepSeek-R1åœ¨å¤šä»»åŠ¡å’Œä¸åŒé—®é¢˜è§„æ¨¡ä¸Šå§‹ç»ˆè·å¾—æœ€é«˜çš„F1åˆ†æ•°ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„é€»è¾‘æ¼”ç»å’Œå…³ç³»æ¨æ–­èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬DeepSeek-R1ï¼Œéšç€é—®é¢˜å¤æ‚æ€§çš„å¢åŠ ï¼Œéƒ½é¢ä¸´ç€æ˜¾è‘—çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºä»¤ç‰Œé•¿åº¦é™åˆ¶å’Œè¾“å‡ºç»“æ„ä¸å®Œæ•´æ‰€å¯¼è‡´çš„ã€‚å¯¹DeepSeek-R1çš„é•¿ç¯‡æ€ç»´é“¾ååº”çš„è¯¦ç»†åˆ†ææ­ç¤ºäº†å…¶ç‹¬ç‰¹çš„è§„åˆ’å’ŒéªŒè¯ç­–ç•¥ï¼Œä½†ä¹Ÿçªå‡ºäº†ä¸è¿è´¯æˆ–ä¸å®Œå…¨æ¨ç†çš„å®ä¾‹ï¼Œè¿™æé†’æˆ‘ä»¬éœ€è¦å¯¹LLMçš„å†…éƒ¨æ¨ç†åŠ¨æ€è¿›è¡Œæ›´æ·±å…¥çš„ç ”ç©¶ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è®¨è®ºäº†æœªæ¥çš„å…³é”®ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬å¤šæ¨¡å¼æ¨ç†çš„ä½œç”¨å’Œæ¨ç†å¤±è´¥çš„ç³»ç»Ÿæ£€æŸ¥ã€‚æˆ‘ä»¬çš„å‘ç°ä¸ºæ¨è¿›LLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†å®è¯è§è§£å’Œç†è®ºå¯ç¤ºï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»“æ„åŒ–ã€å¤šæ­¥éª¤é€»è¾‘æ¨æ–­çš„ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬çš„ä»£ç ä»“åº“å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/kelvinhkcs/Deep-Relational-Reasoning%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/kelvinhkcs/Deep-Relational-Reasoningä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23128v1">PDF</a> 10 pages, 0 figures, accepted by 2025 IEEE international conference   on artificial intelligence testing (AITest)</p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†æ·±åº¦å…³ç³»æ¨ç†æ—¶çš„èƒ½åŠ›å¦‚ä½•ï¼Ÿæ–‡ç« å¯¹ä¸‰æ¬¾å°–ç«¯LLMæ¨¡å‹ï¼ˆDeepSeek-R1ã€DeepSeek-V3å’ŒGPT-4oï¼‰è¿›è¡Œäº†è¯„ä¼°ä¸æ¯”è¾ƒã€‚å®éªŒè¡¨æ˜ï¼ŒDeepSeek-R1åœ¨å¤šä»»åŠ¡ä¸ä¸åŒé—®é¢˜è§„æ¨¡ä¸­å‡å–å¾—æœ€é«˜F1åˆ†æ•°ï¼Œå±•ç°äº†å¼ºå¤§çš„é€»è¾‘æ¼”ç»ä¸å…³ç³»æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œéšç€é—®é¢˜å¤æ‚åº¦çš„æå‡ï¼Œæ‰€æœ‰è¯„ä¼°æ¨¡å‹å‡é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œä¸»è¦åŸå› æ˜¯ä»¤ç‰Œé•¿åº¦é™åˆ¶ä¸è¾“å‡ºç»“æ„ä¸å®Œæ•´ã€‚å¯¹DeepSeek-R1çš„é•¿ç¯‡æ¨ç†å›åº”çš„æ·±å…¥åˆ†ææ­ç¤ºäº†å…¶ç‹¬ç‰¹çš„è§„åˆ’ä¸éªŒè¯ç­–ç•¥ï¼Œä½†ä¹Ÿæš´éœ²å‡ºæ¨ç†ä¸ä¸€è‡´æˆ–ä¸å®Œæ•´çš„æƒ…å†µï¼Œè¿™å¼•èµ·äº†äººä»¬å¯¹LLMå†…éƒ¨æ¨ç†åŠ¨æ€çš„æ›´æ·±å…¥å®¡æŸ¥çš„éœ€è¦ã€‚æ–‡ç« è¿˜è®¨è®ºäº†æœªæ¥çš„å…³é”®ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æ¨ç†å’Œæ¨ç†å¤±è´¥çš„ç³»ç»Ÿå®¡æŸ¥ã€‚æœ¬æ–‡çš„å‘ç°ä¸ºæ¨è¿›LLMçš„æ¨ç†èƒ½åŠ›æä¾›äº†å®è¯è§è§£å’Œç†è®ºå¯ç¤ºï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»“æ„åŒ–ã€å¤šæ­¥éª¤é€»è¾‘æ¨ç†çš„ä»»åŠ¡ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯„ä¼°äº†ä¸‰ç§é¡¶å°–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ·±åº¦å…³ç³»æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>DeepSeek-R1åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ€ä½³çš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é€»è¾‘æ¼”ç»å’Œå…³ç³»æ¨ç†æ–¹é¢ã€‚</li>
<li>æ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹åœ¨é—®é¢˜å¤æ‚åº¦å¢åŠ æ—¶éƒ½é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºä»¤ç‰Œé•¿åº¦é™åˆ¶å’Œè¾“å‡ºç»“æ„çš„ä¸å®Œæ•´æ€§ã€‚</li>
<li>DeepSeek-R1çš„æ¨ç†å›åº”åˆ†ææ­ç¤ºäº†å…¶ç‹¬ç‰¹çš„è§„åˆ’å’ŒéªŒè¯ç­–ç•¥ï¼Œä½†ä¹Ÿæš´éœ²å‡ºæ¨ç†è¿‡ç¨‹ä¸­çš„ä¸ä¸€è‡´æ€§å’Œä¸å®Œæ•´æ€§ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†æ·±å…¥å®¡æŸ¥LLMå†…éƒ¨æ¨ç†åŠ¨æ€çš„å¿…è¦æ€§ã€‚</li>
<li>æ–‡ç« è®¨è®ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æ¨ç†å’Œæ¨ç†å¤±è´¥çš„ç³»ç»Ÿå®¡æŸ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23128">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-412d407d8e1d96aad72f2b9870a465c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d65eb87720bc0ba229bfdb831c90ead1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c227c516de060482cbb4fc47864eb503.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Text2VectorSQL-Bridging-Text-to-SQL-and-Vector-Search-for-Unified-Natural-Language-Queries"><a href="#Text2VectorSQL-Bridging-Text-to-SQL-and-Vector-Search-for-Unified-Natural-Language-Queries" class="headerlink" title="Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified   Natural Language Queries"></a>Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified   Natural Language Queries</h2><p><strong>Authors:Zhengren Wang, Bozhou Li, Dongwen Yao, Wentao Zhang</strong></p>
<p>While Text-to-SQL enables natural language interaction with structured databases, its effectiveness diminishes with unstructured data or ambiguous queries due to rigid syntax and limited expressiveness. Concurrently, vector search has emerged as a powerful paradigm for semantic retrieval, particularly for unstructured data. However, existing VectorSQL implementations still rely heavily on manual crafting and lack tailored evaluation frameworks, leaving a significant gap between theoretical potential and practical deployment. To bridge these complementary paradigms, we introduces Text2VectorSQL, a novel framework unifying Text-to-SQL and vector search to overcome expressiveness constraints and support more diverse and holistical natural language queries. Specifically, Text2VectorSQL enables semantic filtering, multi-modal matching, and retrieval acceleration. For evaluation, we build vector index on appropriate columns, extend user queries with semantic search, and annotate ground truths via an automatic pipeline with expert review. Furthermore, we develop dedicated Text2VectorSQL models with synthetic data, demonstrating significant performance improvements over baseline methods. Our work establishes the foundation for the Text2VectorSQL task, paving the way for more versatile and intuitive database interfaces. The repository will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/Open-DataFlow/Text2VectorSQL">https://github.com/Open-DataFlow/Text2VectorSQL</a>. </p>
<blockquote>
<p>è™½ç„¶æ–‡æœ¬åˆ°SQLï¼ˆText-to-SQLï¼‰æŠ€æœ¯èƒ½å¤Ÿå®ç°ä¸ç»“æ„åŒ–æ•°æ®åº“çš„è‡ªç„¶è¯­è¨€äº¤äº’ï¼Œä½†ç”±äºå…¶è¯­æ³•è¿‡äºåƒµåŒ–ã€è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œåœ¨å¤„ç†éç»“æ„åŒ–æ•°æ®æˆ–æ¨¡ç³ŠæŸ¥è¯¢æ—¶ï¼Œå…¶æ•ˆæœä¼šå¤§æ‰“æŠ˜æ‰£ã€‚åŒæ—¶ï¼Œå‘é‡æœç´¢ä½œä¸ºä¸€ç§å¼ºå¤§çš„è¯­ä¹‰æ£€ç´¢èŒƒå¼ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†éç»“æ„åŒ–æ•°æ®æ—¶ï¼Œè¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„VectorSQLå®ç°ä»ç„¶ä¸¥é‡ä¾èµ–äºæ‰‹åŠ¨æ„å»ºï¼Œå¹¶ä¸”ç¼ºä¹é’ˆå¯¹æ€§çš„è¯„ä¼°æ¡†æ¶ï¼Œå¯¼è‡´ç†è®ºæ½œåŠ›ä¸å®é™…éƒ¨ç½²ä¹‹é—´å­˜åœ¨å·¨å¤§å·®è·ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸¤ç§èŒƒå¼çš„é¸¿æ²Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†Text2VectorSQLè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒèåˆäº†æ–‡æœ¬åˆ°SQLå’Œå‘é‡æœç´¢æŠ€æœ¯ï¼Œå…‹æœäº†è¡¨è¾¾èƒ½åŠ›çš„é™åˆ¶ï¼Œæ”¯æŒæ›´å¤šæ ·åŒ–ã€æ›´å…¨é¢çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚å…·ä½“æ¥è¯´ï¼ŒText2VectorSQLèƒ½å¤Ÿå®ç°è¯­ä¹‰è¿‡æ»¤ã€å¤šæ¨¡æ€åŒ¹é…å’Œæ£€ç´¢åŠ é€Ÿã€‚åœ¨è¯„ä¼°æ–¹é¢ï¼Œæˆ‘ä»¬åœ¨é€‚å½“çš„åˆ—ä¸Šæ„å»ºå‘é‡ç´¢å¼•ï¼Œé€šè¿‡è‡ªåŠ¨ç®¡é“æ‰©å±•ç”¨æˆ·æŸ¥è¯¢å¹¶è¿›è¡Œè¯­ä¹‰æœç´¢ï¼Œå¹¶é€šè¿‡ä¸“å®¶è¯„å®¡è¿›è¡ŒçœŸå®æ ‡æ³¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨åˆæˆæ•°æ®å¼€å‘äº†ä¸“ç”¨çš„Text2VectorSQLæ¨¡å‹ï¼Œåœ¨åŸºå‡†æ–¹æ³•ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºText2VectorSQLä»»åŠ¡å¥ å®šäº†åŸºç¡€ï¼Œä¸ºæ›´é€šç”¨ã€æ›´ç›´è§‚çš„æ•°æ®åº“æ¥å£é“ºå¹³äº†é“è·¯ã€‚è¯¥ä»“åº“å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Open-DataFlow/Text2VectorSQL%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/Open-DataFlow/Text2VectorSQLä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23071v1">PDF</a> Work in progess</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†Text-to-SQLåœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®åº“æ—¶çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†éç»“æ„åŒ–æ•°æ®æˆ–æ¨¡ç³ŠæŸ¥è¯¢æ—¶çš„æ•ˆæœå‡å¼±ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„æ¡†æ¶Text2VectorSQLï¼Œå®ƒå°†Text-to-SQLä¸å‘é‡æœç´¢ç›¸ç»“åˆï¼Œæé«˜äº†è¡¨è¾¾æ€§ï¼Œæ”¯æŒæ›´å¤šæ ·åŒ–å’Œå…¨é¢çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚è¯¥æ¡†æ¶å®ç°äº†è¯­ä¹‰è¿‡æ»¤ã€å¤šæ¨¡å¼åŒ¹é…å’Œæ£€ç´¢åŠ é€Ÿç­‰åŠŸèƒ½ã€‚åŒæ—¶ï¼Œè¿˜ä»‹ç»äº†å¯¹è¯„ä¼°æ–¹æ³•çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬å»ºç«‹å‘é‡ç´¢å¼•ã€æ‰©å±•ç”¨æˆ·æŸ¥è¯¢è¯­ä¹‰æœç´¢å’Œè‡ªåŠ¨ç®¡é“åŒ–æ ‡æ³¨çœŸå®æƒ…å†µç­‰å†…å®¹ã€‚Text2VectorSQLçš„æ¨å‡ºä¸ºå»ºç«‹æ›´é€šç”¨å’Œç›´è§‚æ•°æ®åº“æ¥å£å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-to-SQLåœ¨å¤„ç†éç»“æ„åŒ–æ•°æ®æˆ–æ¨¡ç³ŠæŸ¥è¯¢æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Text2VectorSQLæ¡†æ¶ç»“åˆäº†Text-to-SQLå’Œå‘é‡æœç´¢ï¼Œæé«˜äº†è¡¨è¾¾æ€§å’Œå¯¹å¤šæ ·åŒ–è‡ªç„¶è¯­è¨€æŸ¥è¯¢çš„æ”¯æŒã€‚</li>
<li>Text2VectorSQLå®ç°äº†è¯­ä¹‰è¿‡æ»¤ã€å¤šæ¨¡å¼åŒ¹é…å’Œæ£€ç´¢åŠ é€ŸåŠŸèƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶å¼•å…¥äº†æ”¹è¿›çš„è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬å»ºç«‹å‘é‡ç´¢å¼•ã€æ‰©å±•ç”¨æˆ·æŸ¥è¯¢çš„è¯­ä¹‰æœç´¢å’Œè‡ªåŠ¨ç®¡é“åŒ–æ ‡æ³¨çœŸå®æƒ…å†µç­‰ã€‚</li>
<li>Text2VectorSQLæ¡†æ¶çš„å»ºç«‹ä¸ºæ•°æ®åº“æ¥å£çš„å‘å±•å¥ å®šäº†åŸºç¡€ï¼Œä½¿å…¶æ›´åŠ é€šç”¨å’Œç›´è§‚ã€‚</li>
<li>Text2VectorSQLæ¨¡å‹é€šè¿‡åˆæˆæ•°æ®è¿›è¡Œäº†å¼€å‘ï¼Œç›¸è¾ƒäºåŸºå‡†æ–¹æ³•ï¼Œæ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f3172949b25a960b4b63a8e0d5e0578.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-114387384b86220ed95f03f9b202a292.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9e7dcd8c1b7b4609a3bdc30fb708c13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72258329062ce5f821da3ee58144d659.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Boosting-LLMâ€™s-Molecular-Structure-Elucidation-with-Knowledge-Enhanced-Tree-Search-Reasoning"><a href="#Boosting-LLMâ€™s-Molecular-Structure-Elucidation-with-Knowledge-Enhanced-Tree-Search-Reasoning" class="headerlink" title="Boosting LLMâ€™s Molecular Structure Elucidation with Knowledge Enhanced   Tree Search Reasoning"></a>Boosting LLMâ€™s Molecular Structure Elucidation with Knowledge Enhanced   Tree Search Reasoning</h2><p><strong>Authors:Xiang Zhuang, Bin Wu, Jiyu Cui, Kehua Feng, Xiaotong Li, Huabin Xing, Keyan Ding, Qiang Zhang, Huajun Chen</strong></p>
<p>Molecular structure elucidation involves deducing a moleculeâ€™s structure from various types of spectral data, which is crucial in chemical experimental analysis. While large language models (LLMs) have shown remarkable proficiency in analyzing and reasoning through complex tasks, they still encounter substantial challenges in molecular structure elucidation. We identify that these challenges largely stem from LLMsâ€™ limited grasp of specialized chemical knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search for test-time scaling as a plugin. Specifically, we construct an external molecular substructure knowledge base to extend the LLMsâ€™ coverage of the chemical structure space. Furthermore, we design a specialized molecule-spectrum scorer to act as a reward model for the reasoning process, addressing the issue of inaccurate solution evaluation in LLMs. Experimental results show that our approach significantly boosts performance, particularly gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/HICAI-ZJU/K-MSE">https://github.com/HICAI-ZJU/K-MSE</a>. </p>
<blockquote>
<p>åˆ†å­ç»“æ„é˜é‡Šæ˜¯ä»å„ç§ç±»å‹çš„å…‰è°±æ•°æ®ä¸­æ¨æ–­åˆ†å­çš„ç»“æ„ï¼Œè¿™åœ¨åŒ–å­¦å®éªŒåˆ†æä¸­è‡³å…³é‡è¦ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ†ææ¨ç†å¤æ‚ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨åˆ†å­ç»“æ„é˜é‡Šæ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°è¿™äº›æŒ‘æˆ˜ä¸»è¦æºäºLLMså¯¹ä¸“ä¸šåŒ–å­¦çŸ¥è¯†çš„æœ‰é™æŒæ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†çŸ¥è¯†å¢å¼ºåˆ†å­ç»“æ„é˜é‡Šæ¨ç†æ¡†æ¶ï¼ˆK-MSEï¼‰ï¼Œåˆ©ç”¨è’™ç‰¹å¡ç½—æ ‘æœç´¢ä½œä¸ºæ’ä»¶è¿›è¡Œå®æ—¶æµ‹è¯•æ‰©å±•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤–éƒ¨åˆ†å­å­ç»“æ„çŸ¥è¯†åº“ï¼Œä»¥æ‰©å±•LLMså¯¹åŒ–å­¦ç»“æ„ç©ºé—´çš„è¦†ç›–èŒƒå›´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªä¸“é—¨çš„åˆ†å­å…‰è°±è¯„åˆ†å™¨ï¼Œä½œä¸ºæ¨ç†è¿‡ç¨‹çš„å¥–åŠ±æ¨¡å‹ï¼Œè§£å†³LLMsä¸­è§£å†³æ–¹æ¡ˆè¯„ä¼°ä¸å‡†ç¡®çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨GPT-4o-miniå’ŒGPT-4oä¸Šéƒ½å–å¾—äº†è¶…è¿‡20%çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HICAI-ZJU/K-MSE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HICAI-ZJU/K-MSEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23056v1">PDF</a> ACL 2025 Main</p>
<p><strong>Summary</strong></p>
<p>åˆ†å­ç»“æ„è§£ææ˜¯ä»å„ç§å…‰è°±æ•°æ®ä¸­æ¨æ–­åˆ†å­ç»“æ„çš„è¿‡ç¨‹ï¼Œå¯¹åŒ–å­¦å®éªŒåˆ†æè‡³å…³é‡è¦ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ†ææ¨ç†å¤æ‚ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨åˆ†å­ç»“æ„è§£ææ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æºäºå…¶å¯¹ä¸“ä¸šåŒ–å­¦çŸ¥è¯†çš„æœ‰é™æŒæ¡ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§çŸ¥è¯†å¢å¼ºåˆ†å­ç»“æ„è§£ææ¨ç†æ¡†æ¶ï¼ˆK-MSEï¼‰ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ä½œä¸ºæµ‹è¯•æ—¶çš„å¯æ‰©å±•æ’ä»¶ã€‚é€šè¿‡æ„å»ºå¤–éƒ¨åˆ†å­å­ç»“æ„çŸ¥è¯†åº“ï¼Œæ‰©å±•LLMså¯¹åŒ–å­¦ç»“æ„ç©ºé—´çš„è¦†ç›–ã€‚åŒæ—¶ï¼Œè®¾è®¡ä¸“é—¨çš„åˆ†å­å…‰è°±è¯„åˆ†å™¨ï¼Œä½œä¸ºæ¨ç†è¿‡ç¨‹çš„å¥–åŠ±æ¨¡å‹ï¼Œè§£å†³LLMsä¸­è§£å†³æ–¹æ¡ˆè¯„ä»·ä¸å‡†ç¡®çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨GPT-4o-miniå’ŒGPT-4oä¸Šæå‡è¶…è¿‡20%ã€‚ç›¸å…³ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/HICAI-ZJU/K-MSE%E3%80%82">https://github.com/HICAI-ZJU/K-MSEã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†å­ç»“æ„è§£ææ˜¯åŒ–å­¦å®éªŒä¸­é‡è¦çš„åˆ†æè¿‡ç¨‹ï¼Œæ¶‰åŠä»å…‰è°±æ•°æ®ä¸­æ¨æ–­åˆ†å­ç»“æ„ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡åˆ†ææ¨ç†æ–¹é¢å…·æœ‰æ˜¾è‘—èƒ½åŠ›ï¼Œä½†åœ¨åˆ†å­ç»“æ„è§£æé¢†åŸŸé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æŒ‘æˆ˜ä¸»è¦æºäºLLMså¯¹ä¸“ä¸šåŒ–å­¦çŸ¥è¯†çš„æœ‰é™æŒæ¡ï¼Œéœ€è¦çŸ¥è¯†å¢å¼ºæ¥æå‡å…¶åœ¨è¯¥é¢†åŸŸçš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥K-MSEçŸ¥è¯†å¢å¼ºæ¨ç†æ¡†æ¶ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢è¿›è¡Œæµ‹è¯•æ—¶æ‰©å±•ã€‚</li>
<li>é€šè¿‡æ„å»ºå¤–éƒ¨åˆ†å­å­ç»“æ„çŸ¥è¯†åº“ï¼Œæ‰©å±•LLMså¯¹åŒ–å­¦ç»“æ„ç©ºé—´çš„è¦†ç›–ï¼Œæé«˜è§£æå‡†ç¡®æ€§ã€‚</li>
<li>è®¾è®¡ä¸“é—¨çš„åˆ†å­å…‰è°±è¯„åˆ†å™¨ï¼Œè§£å†³LLMsä¸­è§£å†³æ–¹æ¡ˆè¯„ä»·ä¸å‡†ç¡®çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6dec422c422dfc49306c74f158c07be2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0695c0a8a15bfd5ca2b51793b4358365.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ecdd2a0c5ff588e00ebe2ca21f96212.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1c1a435ae03c12c199a78cd7ccadf76.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Towards-Explainable-Bilingual-Multimodal-Misinformation-Detection-and-Localization"><a href="#Towards-Explainable-Bilingual-Multimodal-Misinformation-Detection-and-Localization" class="headerlink" title="Towards Explainable Bilingual Multimodal Misinformation Detection and   Localization"></a>Towards Explainable Bilingual Multimodal Misinformation Detection and   Localization</h2><p><strong>Authors:Yiwei He, Xiangtai Li, Zhenglin Huang, Yi Dong, Hao Fei, Jiangning Zhang, Baoyuan Wu, Guangliang Cheng</strong></p>
<p>The increasing realism of multimodal content has made misinformation more subtle and harder to detect, especially in news media where images are frequently paired with bilingual (e.g., Chinese-English) subtitles. Such content often includes localized image edits and cross-lingual inconsistencies that jointly distort meaning while remaining superficially plausible. We introduce BiMi, a bilingual multimodal framework that jointly performs region-level localization, cross-modal and cross-lingual consistency detection, and natural language explanation for misinformation analysis. To support generalization, BiMi integrates an online retrieval module that supplements model reasoning with up-to-date external context. We further release BiMiBench, a large-scale and comprehensive benchmark constructed by systematically editing real news images and subtitles, comprising 104,000 samples with realistic manipulations across visual and linguistic modalities. To enhance interpretability, we apply Group Relative Policy Optimization (GRPO) to improve explanation quality, marking the first use of GRPO in this domain. Extensive experiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in classification accuracy, +15.9 in localization accuracy, and +2.5 in explanation BERTScore, advancing state-of-the-art performance in realistic, multilingual misinformation detection. Code, models, and datasets will be released. </p>
<blockquote>
<p>éšç€å¤šåª’ä½“å†…å®¹ç°å®æ„Ÿçš„å¢å¼ºï¼Œé”™è¯¯ä¿¡æ¯å˜å¾—æ›´åŠ å¾®å¦™å’Œéš¾ä»¥æ£€æµ‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°é—»åª’ä½“ä¸­ï¼Œå›¾åƒç»å¸¸é…æœ‰åŒè¯­ï¼ˆå¦‚ä¸­è‹±æ–‡ï¼‰å­—å¹•ã€‚æ­¤ç±»å†…å®¹é€šå¸¸åŒ…æ‹¬æœ¬åœ°åŒ–å›¾åƒç¼–è¾‘å’Œè·¨è¯­è¨€ä¸ä¸€è‡´ä¹‹å¤„ï¼Œå®ƒä»¬å…±åŒæ‰­æ›²äº†æ„ä¹‰ï¼ŒåŒæ—¶åœ¨è¡¨é¢ä¸Šä¼¼ä¹åˆç†ã€‚æˆ‘ä»¬æ¨å‡ºäº†BiMiï¼Œè¿™æ˜¯ä¸€ä¸ªåŒè¯­å¤šåª’ä½“æ¡†æ¶ï¼Œå®ƒè”åˆæ‰§è¡ŒåŒºåŸŸçº§å®šä½ã€è·¨æ¨¡æ€å’Œè·¨è¯­è¨€ä¸€è‡´æ€§æ£€æµ‹ï¼Œä»¥åŠé”™è¯¯ä¿¡æ¯åˆ†æçš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚ä¸ºäº†æ”¯æŒé€šç”¨æ€§ï¼ŒBiMié›†æˆäº†ä¸€ä¸ªåœ¨çº¿æ£€ç´¢æ¨¡å—ï¼Œè¯¥æ¨¡å—ç”¨æœ€æ–°çš„å¤–éƒ¨ä¸Šä¸‹æ–‡è¡¥å……æ¨¡å‹æ¨ç†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å‘å¸ƒäº†BiMiBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ç³»ç»Ÿç¼–è¾‘çœŸå®æ–°é—»å›¾åƒå’Œå­—å¹•æ„å»ºçš„å¤§è§„æ¨¡ç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«104000ä¸ªæ ·æœ¬ï¼Œåœ¨è§†è§‰å’Œè¯­è¨€æ¨¡å¼æ–¹é¢è¿›è¡Œäº†ç°å®æ“ä½œã€‚ä¸ºäº†æé«˜å¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬åº”ç”¨äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥æé«˜è§£é‡Šè´¨é‡ï¼Œæ ‡å¿—ç€GRPOé¦–æ¬¡åœ¨è¿™ä¸ªé¢†åŸŸçš„ä½¿ç”¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBiMiåœ¨åˆ†ç±»ç²¾åº¦ä¸Šæ¯”å¼ºåŸºçº¿é«˜å‡º+8.9ï¼Œå®šä½ç²¾åº¦é«˜å‡º+15.9ï¼Œè§£é‡ŠBERTScoreé«˜å‡º+2.5ï¼Œåœ¨çœŸå®ã€å¤šè¯­è¨€é”™è¯¯ä¿¡æ¯æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®é›†å°†äºˆä»¥å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22930v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€å†…å®¹çš„ç°å®æ€§å¢å¼ºä½¿å¾—è™šå‡ä¿¡æ¯æ›´ä¸ºå¾®å¦™ä¸”éš¾ä»¥æ£€æµ‹ï¼Œå°¤å…¶æ˜¯åœ¨é…æ­åŒè¯­å­—å¹•çš„æ–°é—»ä¸­ã€‚é’ˆå¯¹è¿™ç§æƒ…å†µï¼Œæå‡ºBilingual Multimodalï¼ˆBiMiï¼‰æ¡†æ¶ï¼Œç”¨äºåœ°åŒºçº§åˆ«å®šä½ã€è·¨æ¨¡æ€ä¸è·¨è¯­è¨€ä¸€è‡´æ€§æ£€æµ‹ä»¥åŠè‡ªç„¶è¯­è¨€è§£é‡Šçš„åˆ†æã€‚æ¡†æ¶é›†æˆäº†åœ¨çº¿æ£€ç´¢æ¨¡å—ï¼Œä»¥æ”¯æŒæ™®åŠå¹¶æä¾›æœ€æ–°å¤–éƒ¨èƒŒæ™¯ä¿¡æ¯ã€‚åŒæ—¶å‘å¸ƒäº†å¤§å‹ç»¼åˆåŸºå‡†æµ‹è¯•BiMiBenchï¼Œé€šè¿‡ç³»ç»Ÿç¼–è¾‘çœŸå®æ–°é—»å›¾åƒå’Œå­—å¹•ç”ŸæˆåŒ…å«è§†è§‰å’Œè¯­è¨€æ¨¡æ€çœŸå®æ“ä½œçš„æ ·æœ¬ã€‚ä¸ºæé«˜è§£é‡Šè´¨é‡ï¼Œé¦–æ¬¡é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ã€‚å®éªŒè¯æ˜BiMiåœ¨åˆ†ç±»å‡†ç¡®æ€§ä¸Šè¶…è¶Šå¼ºå¤§åŸºçº¿è¾¾+8.9ï¼Œå®šä½å‡†ç¡®æ€§è¾¾+15.9ï¼Œè§£é‡ŠBERTScoreè¾¾+2.5ï¼Œä¸ºçœŸå®çš„å¤šè¯­è¨€è™šå‡ä¿¡æ¯æ£€æµ‹å¸¦æ¥æœ€æ–°æŠ€æœ¯è¿›å±•ã€‚æ¨¡å‹å’Œæ•°æ®é›†å°†ä¼šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„ä¸ƒä¸ªå…³é”®æ´å¯Ÿç‚¹ï¼š</p>
<ul>
<li>å¤šæ¨¡æ€å†…å®¹çš„å¢åŠ ä½¿è™šå‡ä¿¡æ¯æ›´éš¾ä»¥æ£€æµ‹ã€‚å°¤å…¶æ˜¯åŒè¯­å­—å¹•æ–°é—»ä¸­ï¼Œè™šå‡ä¿¡æ¯æ›´ä¸ºå¾®å¦™ã€‚</li>
<li>BiMiæ¡†æ¶èƒ½è¿›è¡Œåœ°åŒºçº§åˆ«å®šä½ã€è·¨æ¨¡æ€å’Œè·¨è¯­è¨€ä¸€è‡´æ€§æ£€æµ‹ä»¥åŠè‡ªç„¶è¯­è¨€è§£é‡Šçš„åˆ†æã€‚</li>
<li>BiMiæ¡†æ¶é›†æˆäº†åœ¨çº¿æ£€ç´¢æ¨¡å—ä»¥æ”¯æŒæ™®åŠå¹¶æä¾›æœ€æ–°å¤–éƒ¨èƒŒæ™¯ä¿¡æ¯ã€‚</li>
<li>BiMiBenchæ˜¯ä¸€ä¸ªå¤§å‹ç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«çœŸå®æ–°é—»å›¾åƒå’Œå­—å¹•çš„ç³»ç»Ÿç¼–è¾‘æ ·æœ¬ã€‚</li>
<li>é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰æé«˜è§£é‡Šè´¨é‡ã€‚</li>
<li>BiMiåœ¨åˆ†ç±»å‡†ç¡®æ€§ã€å®šä½å‡†ç¡®æ€§å’Œè§£é‡ŠBERTScoreæ–¹é¢è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f6bb68381c8d6c2cbef6502551ff3b20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-613f3f32ec3434c62c0d81b635ec4597.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dc70e8eec7157cf77f68ad29b44486b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d20762c80303e3965d52e79d9f84a35.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00ccb617aee1a78662547f0997f9bcb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7a1486cdf8b52ab0e18c477e21b3ab0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Improving-Rationality-in-the-Reasoning-Process-of-Language-Models-through-Self-playing-Game"><a href="#Improving-Rationality-in-the-Reasoning-Process-of-Language-Models-through-Self-playing-Game" class="headerlink" title="Improving Rationality in the Reasoning Process of Language Models   through Self-playing Game"></a>Improving Rationality in the Reasoning Process of Language Models   through Self-playing Game</h2><p><strong>Authors:Pinzheng Wang, Juntao Li, Zecheng Tang, Haijia Gui, Min zhang</strong></p>
<p>Large language models (LLMs) have demonstrated considerable reasoning abilities in various tasks such as mathematics and coding. However, recent studies indicate that even the best models lack true comprehension of their reasoning processes. In this paper, we explore how self-play can enhance the rationality of models in the reasoning process without supervision from humans or superior models. We design a Critic-Discernment Game(CDG) in which a prover first provides a solution to a given problem and is subsequently challenged by critiques of its solution. These critiques either aim to assist or mislead the prover. The objective of the prover is to maintain the correct answer when faced with misleading comments, while correcting errors in response to constructive feedback. Our experiments on tasks involving mathematical reasoning, stepwise error detection, self-correction, and long-chain reasoning demonstrate that CDG training can significantly improve the ability of well-aligned LLMs to comprehend their reasoning process. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ç›¸å½“å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œå¦‚æ•°å­¦å’Œç¼–ç¨‹ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å¥½çš„æ¨¡å‹ä¹Ÿç¼ºä¹å¯¹æ¨ç†è¿‡ç¨‹çš„çœŸæ­£ç†è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æ— éœ€äººç±»æˆ–æ›´é«˜çº§æ¨¡å‹çš„ç›‘ç£ï¼Œè‡ªæˆ‘æ¸¸æˆå¦‚ä½•å¢å¼ºæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„åˆç†æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ‰¹åˆ¤é‰´åˆ«æ¸¸æˆï¼ˆCDGï¼‰ï¼Œå…¶ä¸­è¯æ˜è€…é¦–å…ˆæä¾›ç»™å®šé—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œéšåå—åˆ°æ‰¹åˆ¤è€…çš„æŒ‘æˆ˜ã€‚è¿™äº›æ‰¹åˆ¤æ—¨åœ¨å¸®åŠ©æˆ–è¯¯å¯¼è¯æ˜è€…ã€‚è¯æ˜è€…çš„ç›®æ ‡æ˜¯é¢å¯¹è¯¯å¯¼æ€§è¯„è®ºæ—¶ä¿æŒæ­£ç¡®ç­”æ¡ˆï¼ŒåŒæ—¶æ ¹æ®å»ºè®¾æ€§åé¦ˆçº æ­£é”™è¯¯ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠæ•°å­¦æ¨ç†ã€é€æ­¥é”™è¯¯æ£€æµ‹ã€è‡ªæˆ‘ä¿®æ­£å’Œé•¿é“¾æ¨ç†çš„ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCDGè®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜è‰¯å¥½å¯¹é½çš„LLMç†è§£å…¶æ¨ç†è¿‡ç¨‹çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22920v1">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç±»ä»»åŠ¡ä¸­å±•ç°å‡ºç›¸å½“çš„æ¨ç†èƒ½åŠ›ï¼Œå¦‚æ•°å­¦å’Œç¼–ç¨‹ã€‚ä½†æœ€æ–°ç ”ç©¶æ˜¾ç¤ºï¼Œé¡¶å°–æ¨¡å‹ä»ç¼ºä¹å¯¹è‡ªèº«æ¨ç†è¿‡ç¨‹çœŸæ­£ç†è§£çš„èƒ½åŠ›ã€‚æœ¬æ–‡æ¢ç´¢äº†æ— éœ€äººç±»æˆ–æ›´é«˜çº§æ¨¡å‹ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•é€šè¿‡è‡ªæˆ‘åšå¼ˆå¢å¼ºæ¨¡å‹çš„æ¨ç†ç†æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ‰¹åˆ¤é‰´åˆ«æ¸¸æˆï¼ˆCDGï¼‰ï¼Œå…¶ä¸­è¯æ˜è€…é¦–å…ˆæä¾›é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œéšåæ¥å—æ‰¹åˆ¤è€…çš„æŒ‘æˆ˜ã€‚è¿™äº›æ‰¹åˆ¤æ—¨åœ¨ååŠ©æˆ–è¯¯å¯¼è¯æ˜è€…ã€‚è¯æ˜è€…çš„ç›®æ ‡æ˜¯é¢å¯¹è¯¯å¯¼æ€§è¯„è®ºæ—¶ä¿æŒæ­£ç¡®å›ç­”ï¼Œå¹¶åœ¨å»ºè®¾æ€§åé¦ˆä¸­çº æ­£é”™è¯¯ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠæ•°å­¦æ¨ç†ã€é€æ­¥é”™è¯¯æ£€æµ‹ã€è‡ªæˆ‘ä¿®æ­£å’Œé•¿é“¾æ¨ç†çš„ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCDGè®­ç»ƒèƒ½æ˜¾è‘—æé«˜å…·æœ‰è‰¯å¥½å¯¹é½æ€§çš„LLMå¯¹è‡ªèº«æ¨ç†è¿‡ç¨‹çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰ç ”ç©¶æŒ‡å‡ºï¼ŒLLMç¼ºä¹å¯¹è‡ªèº«æ¨ç†è¿‡ç¨‹çš„çœŸæ­£ç†è§£ã€‚</li>
<li>æ‰¹åˆ¤é‰´åˆ«æ¸¸æˆï¼ˆCDGï¼‰è¢«è®¾è®¡ç”¨äºå¢å¼ºæ¨¡å‹çš„æ¨ç†ç†æ€§ã€‚</li>
<li>åœ¨CDGä¸­ï¼Œè¯æ˜è€…éœ€é¢å¯¹æ‰¹åˆ¤è€…çš„æŒ‘æˆ˜ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨é¢å¯¹è¯¯å¯¼æ—¶çš„åˆ¤æ–­åŠ›å’Œå¯¹æ­£ç¡®è§£ç­”çš„åšæŒã€‚</li>
<li>CDGè®­ç»ƒæ˜¾è‘—æé«˜äº†LLMåœ¨å¤šæ–¹é¢çš„èƒ½åŠ›ï¼Œå¦‚æ•°å­¦æ¨ç†ã€é”™è¯¯æ£€æµ‹ä¸è‡ªæˆ‘ä¿®æ­£ã€é•¿é“¾æ¨ç†ç­‰ã€‚</li>
<li>CDGè®­ç»ƒèƒ½å¸®åŠ©LLMæ›´å¥½åœ°ç†è§£å…¶æ¨ç†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cc59f6f1d6c9621fa96a1585cafbef1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55a44cd6db0a5e9831b8a7cca8d4f01c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-113d3f967e1ad7ce1a8a30f3cc1d6c07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23a1992c1e5ed1983aaa933fb5fe9606.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-019087ae534100520453f2fe93d2660e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Decoupled-Seg-Tokens-Make-Stronger-Reasoning-Video-Segmenter-and-Grounder"><a href="#Decoupled-Seg-Tokens-Make-Stronger-Reasoning-Video-Segmenter-and-Grounder" class="headerlink" title="Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and   Grounder"></a>Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and   Grounder</h2><p><strong>Authors:Dang Jisheng, Wu Xudong, Wang Bimei, Lv Ning, Chen Jiayu, Jingwen Zhao, Yichu liu, Jizhao Liu, Juncheng Li, Teng Wang</strong></p>
<p>Existing video segmenter and grounder approaches, exemplified by Sa2VA, directly fuse features within segmentation models. This often results in an undesirable entanglement of dynamic visual information and static semantics, thereby degrading segmentation accuracy. To systematically mitigate this issue, we propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text pre-training and a linear decoupling module to address the information processing limitations inherent in SAM-2. Specifically, first, we devise a pre-training paradigm that converts textual ground-truth labels into point-level prompts while generating corresponding text masks. These masks are refined through a hybrid loss function to strengthen the modelâ€™s semantic grounding capabilities. Next, we employ linear projection to disentangle hidden states that generated by a large language model into distinct textual and visual feature subspaces. Finally, a dynamic mask fusion strategy synergistically combines these decoupled features through triple supervision from predicted text&#x2F;visual masks and ground-truth annotations. Extensive experiments demonstrate state-of-the-art performance across diverse tasks, including image segmentation, image question answering, video segmentation, and video question answering. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/longmalongma/DeSa2VA">https://github.com/longmalongma/DeSa2VA</a>. </p>
<blockquote>
<p>ç°æœ‰çš„è§†é¢‘åˆ†å‰²å™¨å’Œæ‰“åº•å™¨æ–¹æ³•ï¼Œä»¥Sa2VAä¸ºä¾‹ï¼Œç›´æ¥åœ¨åˆ†å‰²æ¨¡å‹ä¸­èåˆç‰¹å¾ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´åŠ¨æ€è§†è§‰ä¿¡æ¯å’Œé™æ€è¯­ä¹‰çš„çº ç¼ ï¼Œä»è€Œé™ä½åˆ†å‰²ç²¾åº¦ã€‚ä¸ºäº†ç³»ç»Ÿåœ°ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeSa2VAï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºè§£è€¦çš„æç¤ºæ–¹æ¡ˆï¼Œå®ƒç»“åˆäº†æ–‡æœ¬é¢„è®­ç»ƒå’Œéçº¿æ€§è§£è€¦æ¨¡å—ï¼Œä»¥è§£å†³SAM-2å›ºæœ‰çš„ä¿¡æ¯å¤„ç†é™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é¢„è®­ç»ƒæ¨¡å¼ï¼Œå°†æ–‡æœ¬çœŸå®æ ‡ç­¾è½¬æ¢ä¸ºç‚¹çº§æç¤ºï¼ŒåŒæ—¶ç”Ÿæˆç›¸åº”çš„æ–‡æœ¬æ©ç ã€‚è¿™äº›æ©ç é€šè¿‡æ··åˆæŸå¤±å‡½æ•°è¿›è¡Œç²¾ç‚¼ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„è¯­ä¹‰å®šä½èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é‡‡ç”¨çº¿æ€§æŠ•å½±å°†å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„éšè—çŠ¶æ€åˆ†è§£åˆ°ä¸åŒçš„æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾å­ç©ºé—´ä¸­ã€‚æœ€åï¼Œé€šè¿‡é¢„æµ‹æ–‡æœ¬&#x2F;è§†è§‰æ©ç å’ŒçœŸå®æ³¨é‡Šçš„ä¸‰é‡ç›‘ç£ï¼ŒåŠ¨æ€æ©èåˆç­–ç•¥ååŒåœ°ç»“åˆäº†è¿™äº›è§£è€¦çš„ç‰¹å¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨åŒ…æ‹¬å›¾åƒåˆ†å‰²ã€å›¾åƒé—®ç­”ã€è§†é¢‘åˆ†å‰²å’Œè§†é¢‘é—®ç­”ç­‰å¤šç§ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/longmalongma/DeSa2VA%E3%80%82">https://github.com/longmalongma/DeSa2VAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22880v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDeSa2VAçš„æ”¹è¿›æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘åˆ†å‰²å™¨å’Œæ¥åœ°å™¨ï¼ˆå¦‚Sa2VAï¼‰åœ¨èåˆç‰¹å¾æ—¶å‡ºç°çš„åŠ¨æ€è§†è§‰ä¿¡æ¯ä¸é™æ€è¯­ä¹‰çº ç¼ é—®é¢˜ï¼Œä»è€Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡é¢„è®­ç»ƒå°†æ–‡æœ¬æ ‡ç­¾è½¬æ¢ä¸ºç‚¹çº§æç¤ºå¹¶ç”Ÿæˆæ–‡æœ¬æ©ç ï¼Œä½¿ç”¨æ··åˆæŸå¤±å‡½æ•°å¢å¼ºæ¨¡å‹çš„è¯­ä¹‰æ¥åœ°èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé‡‡ç”¨çº¿æ€§æŠ•å½±æŠ€æœ¯å°†å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„éšè—çŠ¶æ€åˆ†è§£æˆç‹¬ç«‹çš„æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾å­ç©ºé—´ã€‚æœ€åï¼Œé€šè¿‡æ¥è‡ªé¢„æµ‹æ–‡æœ¬&#x2F;è§†è§‰æ©ç å’ŒçœŸå®æ³¨é‡Šçš„ä¸‰é‡ç›‘ç£ï¼Œé‡‡ç”¨åŠ¨æ€æ©ç èåˆç­–ç•¥ååŒç»“åˆè¿™äº›è§£è€¦ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å›¾åƒåˆ†å‰²ã€å›¾åƒé—®ç­”ã€è§†é¢‘åˆ†å‰²å’Œè§†é¢‘é—®ç­”ç­‰å¤šé¡¹ä»»åŠ¡ä¸Šå‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeSa2VAè§£å†³äº†ç°æœ‰è§†é¢‘åˆ†å‰²å™¨å’Œæ¥åœ°å™¨ï¼ˆå¦‚Sa2VAï¼‰åœ¨èåˆç‰¹å¾æ—¶åŠ¨æ€è§†è§‰ä¿¡æ¯ä¸é™æ€è¯­ä¹‰çº ç¼ çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒå°†æ–‡æœ¬æ ‡ç­¾è½¬æ¢ä¸ºç‚¹çº§æç¤ºå¹¶ç”Ÿæˆæ–‡æœ¬æ©ç ï¼Œå¢å¼ºæ¨¡å‹çš„è¯­ä¹‰æ¥åœ°èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨çº¿æ€§æŠ•å½±æŠ€æœ¯åˆ†è§£éšè—çŠ¶æ€ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºåˆ†ä¸ºæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾å­ç©ºé—´ã€‚</li>
<li>ä½¿ç”¨æ··åˆæŸå¤±å‡½æ•°ä¼˜åŒ–æ–‡æœ¬æ©ç ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ä¸‰é‡ç›‘ç£ï¼ˆé¢„æµ‹æ–‡æœ¬&#x2F;è§†è§‰æ©ç å’ŒçœŸå®æ³¨é‡Šï¼‰å®ç°åŠ¨æ€æ©ç èåˆç­–ç•¥ã€‚</li>
<li>DeSa2VAåœ¨å›¾åƒåˆ†å‰²ã€å›¾åƒé—®ç­”ã€è§†é¢‘åˆ†å‰²å’Œè§†é¢‘é—®ç­”ç­‰å¤šé¡¹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e502d8c319ca47ec509cffb4d829e37b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f308a70f0d02ef8f5181d8d6da02a8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2506531d0bc4eb49cf54d4a50be35fb5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ReasonBridge-Efficient-Reasoning-Transfer-from-Closed-to-Open-Source-Language-Models"><a href="#ReasonBridge-Efficient-Reasoning-Transfer-from-Closed-to-Open-Source-Language-Models" class="headerlink" title="ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source   Language Models"></a>ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source   Language Models</h2><p><strong>Authors:Ziqi Zhong, Xunzhu Tang</strong></p>
<p>Recent advancements in Large Language Models (LLMs) have revealed a significant performance gap between closed-source and open-source models, particularly in tasks requiring complex reasoning and precise instruction following. This paper introduces ReasonBridge, a methodology that efficiently transfers reasoning capabilities from powerful closed-source to open-source models through a novel hierarchical knowledge distillation framework. We develop a tailored dataset Reason1K with only 1,000 carefully curated reasoning traces emphasizing difficulty, diversity, and quality. These traces are filtered from across multiple domains using a structured multi-criteria selection algorithm. Our transfer learning approach incorporates: (1) a hierarchical distillation process capturing both strategic abstraction and tactical implementation patterns, (2) a sparse reasoning-focused adapter architecture requiring only 0.3% additional trainable parameters, and (3) a test-time compute scaling mechanism using guided inference interventions. Comprehensive evaluations demonstrate that ReasonBridge improves reasoning capabilities in open-source models by up to 23% on benchmark tasks, significantly narrowing the gap with closed-source models. Notably, the enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its performance on competition-level AIME problems. Our methodology generalizes effectively across diverse reasoning domains and model architectures, establishing a sample-efficient approach to reasoning enhancement for instruction following. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ­ç¤ºäº†é—­æºæ¨¡å‹å’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤æ‚æ¨ç†å’Œç²¾ç¡®æŒ‡ä»¤æ‰§è¡Œçš„ä»»åŠ¡ä¸­ã€‚æœ¬æ–‡ä»‹ç»äº†ReasonBridgeæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ–°å‹åˆ†å±‚çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œæœ‰æ•ˆåœ°å°†ä»å¼ºå¤§é—­æºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°å¼€æºæ¨¡å‹ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªä¸“ç”¨æ•°æ®é›†Reason1Kï¼Œå…¶ä¸­ä»…åŒ…å«1000ä¸ªç²¾å¿ƒç­–åˆ’çš„æ¨ç†è½¨è¿¹ï¼Œå¼ºè°ƒéš¾åº¦ã€å¤šæ ·æ€§å’Œè´¨é‡ã€‚è¿™äº›è½¨è¿¹æ˜¯é€šè¿‡ç»“æ„åŒ–å¤šæ ‡å‡†é€‰æ‹©ç®—æ³•ä»å¤šä¸ªé¢†åŸŸç­›é€‰å‡ºæ¥çš„ã€‚æˆ‘ä»¬çš„è¿ç§»å­¦ä¹ æ–¹æ³•ç»“åˆäº†ä»¥ä¸‹å‡ ç‚¹ï¼šï¼ˆ1ï¼‰åˆ†å±‚è’¸é¦è¿‡ç¨‹ï¼Œæ•æ‰æˆ˜ç•¥æŠ½è±¡å’Œæˆ˜æœ¯å®æ–½æ¨¡å¼ï¼›ï¼ˆ2ï¼‰ç¨€ç–æ¨ç†èšç„¦é€‚é…å™¨æ¶æ„ï¼Œåªéœ€é¢å¤–çš„0.3%å¯è®­ç»ƒå‚æ•°ï¼›ï¼ˆ3ï¼‰æµ‹è¯•æ—¶çš„è®¡ç®—ç¼©æ”¾æœºåˆ¶ï¼Œé‡‡ç”¨æŒ‡å¯¼æ¨ç†å¹²é¢„ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒReasonBridgeåœ¨åŸºå‡†æµ‹è¯•ä»»åŠ¡ä¸Šæé«˜äº†å¼€æºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæœ€å¤šå¯æé«˜23%ï¼Œæ˜¾è‘—ç¼©å°äº†ä¸é—­æºæ¨¡å‹çš„å·®è·ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç»è¿‡æ”¹è¿›çš„Qwen2.5-14Båœ¨MATH500ä¸Šè¡¨ç°ä¼˜äºClaude-Sonnet3.5ï¼Œå¹¶åœ¨ç«èµ›çº§åˆ«çš„AIMEé—®é¢˜ä¸Šä¸ä¹‹ç›¸åŒ¹é…ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„æ¨ç†é¢†åŸŸå’Œæ¨¡å‹æ¶æ„ä¸­æœ‰æ•ˆåœ°é€šç”¨åŒ–ï¼Œå»ºç«‹äº†ä¸€ç§æ ·æœ¬é«˜æ•ˆçš„æ¨ç†å¢å¼ºæ–¹æ³•ï¼Œç”¨äºæŒ‡ä»¤æ‰§è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22865v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•æ­ç¤ºäº†é—­æºå’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤æ‚æ¨ç†å’Œç²¾ç¡®æŒ‡ä»¤æ‰§è¡Œçš„ä»»åŠ¡ä¸­ã€‚æœ¬æ–‡æå‡ºäº†ReasonBridgeæ–¹æ³•ï¼Œé€šè¿‡ä¸€ç§æ–°å‹åˆ†å±‚çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œæœ‰æ•ˆåœ°å°†å¼ºå¤§çš„é—­æºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°å¼€æºæ¨¡å‹ä¸Šã€‚å¼€å‘äº†ä¸€ä¸ªä¸“ä¸ºæ¨ç†èƒ½åŠ›è®¾è®¡çš„æ•°æ®é›†Reason1Kï¼ŒåŒ…å«1000ä¸ªç²¾å¿ƒç­›é€‰çš„æ¨ç†è½¨è¿¹ï¼Œå¼ºè°ƒéš¾åº¦ã€å¤šæ ·æ€§å’Œè´¨é‡ã€‚é€šè¿‡åˆ†å±‚è’¸é¦è¿‡ç¨‹ã€ç¨€ç–æ¨ç†é€‚é…å™¨æ¶æ„å’Œæµ‹è¯•æ—¶è®¡ç®—ç¼©æ”¾æœºåˆ¶ï¼Œæˆ‘ä»¬çš„è½¬ç§»å­¦ä¹ æ–¹æ³•æ˜¾è‘—æé«˜äº†å¼€æºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒReasonBridgeåœ¨åŸºå‡†ä»»åŠ¡ä¸Šæé«˜äº†å¼€æºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›é«˜è¾¾23%ï¼Œæ˜¾è‘—ç¼©å°äº†ä¸é—­æºæ¨¡å‹çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†å’Œç²¾ç¡®æŒ‡ä»¤æ‰§è¡Œä»»åŠ¡ä¸Šï¼Œé—­æºä¸å¼€æºæ¨¡å‹å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ã€‚</li>
<li>ReasonBridgeæ–¹æ³•é€šè¿‡åˆ†å±‚çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œå°†é—­æºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›æœ‰æ•ˆè½¬ç§»åˆ°å¼€æºæ¨¡å‹ä¸Šã€‚</li>
<li>å¼€å‘äº†Reason1Kæ•°æ®é›†ï¼ŒåŒ…å«ç²¾é€‰çš„æ¨ç†è½¨è¿¹ï¼Œå¼ºè°ƒéš¾åº¦ã€å¤šæ ·æ€§å’Œè´¨é‡ã€‚</li>
<li>è½¬ç§»å­¦ä¹ æ–¹æ³•åŒ…æ‹¬åˆ†å±‚è’¸é¦ã€ç¨€ç–æ¨ç†é€‚é…å™¨æ¶æ„å’Œæµ‹è¯•æ—¶è®¡ç®—ç¼©æ”¾æœºåˆ¶ã€‚</li>
<li>ReasonBridgeæé«˜äº†å¼€æºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæœ€é«˜è¾¾23%ï¼Œæ˜¾è‘—ç¼©å°ä¸é—­æºæ¨¡å‹çš„å·®è·ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå¢å¼ºåçš„å¼€æºæ¨¡å‹åœ¨åŸºå‡†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸é«˜çº§é—­æºæ¨¡å‹ç›¸åŒ¹æ•Œã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22865">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c1338e426103697b0bc3ed0d98510e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b1edc30bf2b1bd6b6d9c69d106ec307.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a102749dfa360834721fd8dc176ab14.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Listener-Rewarded-Thinking-in-VLMs-for-Image-Preferences"><a href="#Listener-Rewarded-Thinking-in-VLMs-for-Image-Preferences" class="headerlink" title="Listener-Rewarded Thinking in VLMs for Image Preferences"></a>Listener-Rewarded Thinking in VLMs for Image Preferences</h2><p><strong>Authors:Alexander Gambashidze, Li Pengyi, Matvey Skripkin, Andrey Galichin, Anton Gusarov, Konstantin Sobolev, Andrey Kuznetsov, Ivan Oseledets</strong></p>
<p>Training robust and generalizable reward models for human visual preferences is essential for aligning text-to-image and text-to-video generative models with human intent. However, current reward models often fail to generalize, and supervised fine-tuning leads to memorization, demanding complex annotation pipelines. While reinforcement learning (RL), specifically Group Relative Policy Optimization (GRPO), improves generalization, we uncover a key failure mode: a significant drop in reasoning accuracy occurs when a modelâ€™s reasoning trace contradicts that of an independent, frozen vision-language model (â€œlistenerâ€) evaluating the same output. To address this, we introduce a listener-augmented GRPO framework. Here, the listener re-evaluates the reasonerâ€™s chain-of-thought to provide a dense, calibrated confidence score, shaping the RL reward signal. This encourages the reasoner not only to answer correctly, but to produce explanations that are persuasive to an independent model. Our listener-shaped reward scheme achieves best accuracy on the ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD) performance on a large-scale human preference dataset (1.2M votes, up to +6% over naive reasoner), and reduces reasoning contradictions compared to strong GRPO and SFT baselines. These results demonstrate that listener-based rewards provide a scalable, data-efficient path to aligning vision-language models with nuanced human preferences. We will release our reasoning model here: <a target="_blank" rel="noopener" href="https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner">https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner</a>. </p>
<blockquote>
<p>è®­ç»ƒå¯¹äººç±»è§†è§‰åå¥½å…·æœ‰é²æ£’æ€§å’Œé€šç”¨æ€§çš„å¥–åŠ±æ¨¡å‹å¯¹äºä½¿æ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸äººç±»æ„å›¾ä¿æŒä¸€è‡´è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¥–åŠ±æ¨¡å‹é€šå¸¸æ— æ³•æ¨å¹¿ï¼Œè€Œç›‘ç£å¾®è°ƒä¼šå¯¼è‡´è®°å¿†ï¼Œéœ€è¦å¤æ‚çš„æ³¨é‡Šç®¡é“ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œç‰¹åˆ«æ˜¯ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯ä»¥æ”¹å–„é€šç”¨æ€§ï¼Œä½†æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªå…³é”®çš„å¤±è´¥æ¨¡å¼ï¼šå½“æ¨¡å‹çš„æ¨ç†è½¨è¿¹ä¸ç‹¬ç«‹ã€å†»ç»“çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆâ€œç›‘å¬å™¨â€ï¼‰è¯„ä¼°çš„ç›¸åŒè¾“å‡ºç›¸çŸ›ç›¾æ—¶ï¼Œæ¨ç†ç²¾åº¦ä¼šå‡ºç°å¤§å¹…ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¢å¼ºç›‘å¬å™¨çš„GRPOæ¡†æ¶ã€‚åœ¨è¿™é‡Œï¼Œç›‘å¬å™¨é‡æ–°è¯„ä¼°æ¨ç†è€…çš„æ€ç»´é“¾ï¼Œä»¥æä¾›å¯†é›†ã€æ ¡å‡†çš„ä¿¡å¿ƒåˆ†æ•°ï¼Œå¡‘é€ RLå¥–åŠ±ä¿¡å·ã€‚è¿™é¼“åŠ±æ¨ç†è€…ä¸ä»…å›ç­”æ­£ç¡®ï¼Œè€Œä¸”äº§ç”Ÿå¯¹ç‹¬ç«‹æ¨¡å‹æœ‰è¯´æœåŠ›çš„è§£é‡Šã€‚æˆ‘ä»¬åŸºäºç›‘å¬å™¨çš„å¥–åŠ±æ–¹æ¡ˆåœ¨ImageRewardåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€ä½³ç²¾åº¦ï¼ˆ67.4%ï¼‰ï¼Œåœ¨å¤§å‹äººç±»åå¥½æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†è¶…å‡ºåˆ†å¸ƒçš„æ€§èƒ½ï¼ˆ120ä¸‡ç¥¨ï¼Œä¼˜äºç®€å•æ¨ç†å™¨é«˜è¾¾+6%ï¼‰ï¼Œå¹¶å‡å°‘äº†ä¸å¼ºå¤§çš„GRPOå’ŒSFTåŸºå‡†æµ‹è¯•çš„æ¨ç†çŸ›ç›¾ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŸºäºç›‘å¬å™¨çš„å¥–åŠ±ä¸ºå°†è§†è§‰è¯­è¨€æ¨¡å‹ä¸å¾®å¦™çš„äººç±»åå¥½ä¿æŒä¸€è‡´æä¾›äº†ä¸€æ¡å¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®è·¯å¾„ã€‚æˆ‘ä»¬çš„æ¨ç†æ¨¡å‹å°†åœ¨æ­¤å¤„å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner%E3%80%82">https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasonerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22832v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒè®­ç»ƒèƒ½å¤Ÿåæ˜ äººç±»è§†è§‰åå¥½çš„å¥–åŠ±æ¨¡å‹å¯¹äºæ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆæ¨¡å‹ä¸äººç±»æ„å›¾å¯¹é½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰å¥–åŠ±æ¨¡å‹å­˜åœ¨é€šç”¨åŒ–ä¸è¶³å’Œé€šè¿‡ç›‘ç£å¾®è°ƒå¯¼è‡´çš„è®°å¿†åŒ–é—®é¢˜ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è™½èƒ½æé«˜é€šç”¨æ€§ï¼Œä½†åœ¨æ¨¡å‹æ¨ç†è½¨è¿¹ä¸ç‹¬ç«‹å†»ç»“çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆâ€œç›‘å¬å™¨â€ï¼‰è¯„ä¼°ç›¸åŒè¾“å‡ºæ—¶å­˜åœ¨æ¨ç†å‡†ç¡®åº¦å¤§å¹…ä¸‹é™çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªç›‘å¬å™¨å¢å¼ºçš„GRPOæ¡†æ¶ã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œç›‘å¬å™¨é‡æ–°è¯„ä¼°æ¨ç†è€…çš„æ€ç»´é“¾ï¼Œæä¾›å¯†é›†çš„æ ¡å‡†ç½®ä¿¡åº¦åˆ†æ•°ï¼Œå¡‘é€ RLå¥–åŠ±ä¿¡å·ï¼Œé¼“åŠ±æ¨ç†è€…ä¸ä»…å›ç­”æ­£ç¡®ï¼Œè€Œä¸”äº§ç”Ÿå¯¹ç‹¬ç«‹æ¨¡å‹æœ‰è¯´æœåŠ›çš„è§£é‡Šã€‚è¯¥ç›‘å¬å™¨å½¢çŠ¶çš„å¥–åŠ±æ–¹æ¡ˆåœ¨ImageRewardåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€ä½³å‡†ç¡®åº¦ï¼ˆ67.4%ï¼‰ï¼Œåœ¨å¤§è§„æ¨¡äººç±»åå¥½æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜ç¦»ç¾¤å€¼æ€§èƒ½ï¼ˆæœ€å¤š+6%ï¼‰ï¼Œå¹¶å‡å°‘äº†ä¸å¼ºGRPOå’ŒSFTåŸºå‡†æµ‹è¯•çš„æ¨ç†çŸ›ç›¾ã€‚è¿™äº›ç»“æœè¯æ˜ï¼ŒåŸºäºç›‘å¬å™¨çš„å¥–åŠ±ä¸ºå¯¹é½è§†è§‰è¯­è¨€æ¨¡å‹ä¸å¾®å¦™äººç±»åå¥½æä¾›äº†ä¸€æ¡å¯æ‰©å±•ä¸”æ•°æ®é«˜æ•ˆçš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®­ç»ƒåæ˜ äººç±»è§†è§‰åå¥½çš„å¥–åŠ±æ¨¡å‹å¯¹äºæ–‡æœ¬åˆ°å›¾åƒå’Œæ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆæ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å¥–åŠ±æ¨¡å‹å­˜åœ¨é€šç”¨åŒ–ä¸è¶³å’Œè®°å¿†åŒ–é—®é¢˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¸­çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰èƒ½æé«˜æ¨¡å‹çš„é€šç”¨æ€§ï¼Œä½†å­˜åœ¨æ¨ç†å‡†ç¡®åº¦ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>ç›‘å¬å™¨å¢å¼ºçš„GRPOæ¡†æ¶é€šè¿‡é‡æ–°è¯„ä¼°æ¨ç†è€…çš„æ€ç»´é“¾æ¥è§£å†³GRPOçš„é—®é¢˜ã€‚</li>
<li>ç›‘å¬å™¨æä¾›å¯†é›†çš„æ ¡å‡†ç½®ä¿¡åº¦åˆ†æ•°ï¼Œå¡‘é€ RLå¥–åŠ±ä¿¡å·ï¼Œé¼“åŠ±äº§ç”Ÿå¯¹ç‹¬ç«‹æ¨¡å‹æœ‰è¯´æœåŠ›çš„è§£é‡Šã€‚</li>
<li>ç›‘å¬å™¨å½¢çŠ¶çš„å¥–åŠ±æ–¹æ¡ˆåœ¨ImageRewardåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€ä½³å‡†ç¡®åº¦ï¼Œå¹¶æ˜¾è‘—æé«˜ç¦»ç¾¤å€¼æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-639ede54d5fab4b1a57c2400d2de25b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68463187baaa47aacc9ce00819cd8a9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b03f31995f98f6b40bf5db1c4086c86c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4be1685a7f50729f5006b877c3628b81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f9eb0b563032ed2b668c36a6bea2a40.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MetaCipher-A-General-and-Extensible-Reinforcement-Learning-Framework-for-Obfuscation-Based-Jailbreak-Attacks-on-Black-Box-LLMs"><a href="#MetaCipher-A-General-and-Extensible-Reinforcement-Learning-Framework-for-Obfuscation-Based-Jailbreak-Attacks-on-Black-Box-LLMs" class="headerlink" title="MetaCipher: A General and Extensible Reinforcement Learning Framework   for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs"></a>MetaCipher: A General and Extensible Reinforcement Learning Framework   for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs</h2><p><strong>Authors:Boyuan Chen, Minghao Shao, Abdul Basit, Siddharth Garg, Muhammad Shafique</strong></p>
<p>The growing capabilities of large language models (LLMs) have exposed them to increasingly sophisticated jailbreak attacks. Among these, obfuscation-based attacks â€“ which encrypt malicious content to evade detection â€“ remain highly effective. By leveraging the reasoning ability of advanced LLMs to interpret encrypted prompts, such attacks circumvent conventional defenses that rely on keyword detection or context filtering. These methods are very difficult to defend against, as existing safety mechanisms are not designed to interpret or decode ciphered content. In this work, we propose \textbf{MetaCipher}, a novel obfuscation-based jailbreak framework, along with a reinforcement learning-based dynamic cipher selection mechanism that adaptively chooses optimal encryption strategies from a cipher pool. This approach enhances jailbreak effectiveness and generalizability across diverse task types, victim LLMs, and safety guardrails. Our framework is modular and extensible by design, supporting arbitrary cipher families and accommodating evolving adversarial strategies. We complement our method with a large-scale empirical analysis of cipher performance across multiple victim LLMs. Within as few as 10 queries, MetaCipher achieves over 92% attack success rate (ASR) on most recent standard malicious prompt benchmarks against state-of-the-art non-reasoning LLMs, and over 74% ASR against reasoning-capable LLMs, outperforming all existing obfuscation-based jailbreak methods. These results highlight the long-term robustness and adaptability of our approach, making it more resilient than prior methods in the face of advancing safety measures. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ä¸æ–­å¢å¼ºï¼Œå®ƒä»¬é¢ä¸´ç€è¶Šæ¥è¶Šå¤æ‚çš„è¶Šç‹±æ”»å‡»ã€‚å…¶ä¸­ï¼ŒåŸºäºæ¨¡ç³ŠæŠ€æœ¯çš„æ”»å‡»â€”â€”é€šè¿‡åŠ å¯†æ¶æ„å†…å®¹æ¥èº²é¿æ£€æµ‹â€”â€”ä»ç„¶éå¸¸æœ‰æ•ˆã€‚è¿™äº›æ”»å‡»åˆ©ç”¨å…ˆè¿›LLMçš„æ¨ç†èƒ½åŠ›æ¥è§£é‡ŠåŠ å¯†æç¤ºï¼Œä»è€Œç»•è¿‡ä¾èµ–å…³é”®è¯æ£€æµ‹æˆ–ä¸Šä¸‹æ–‡è¿‡æ»¤çš„ä¼ ç»Ÿé˜²å¾¡æªæ–½ã€‚ç”±äºç°æœ‰çš„å®‰å…¨æœºåˆ¶æ²¡æœ‰è¢«è®¾è®¡æˆè§£é‡Šæˆ–è§£ç åŠ å¯†å†…å®¹ï¼Œå› æ­¤è¿™äº›æ”»å‡»æ–¹æ³•å¾ˆéš¾é˜²èŒƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>MetaCipher</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºäºæ¨¡ç³ŠæŠ€æœ¯çš„è¶Šç‹±æ¡†æ¶ï¼Œä»¥åŠä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„åŠ¨æ€å¯†ç é€‰æ‹©æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥ä»å¯†ç æ± ä¸­è‡ªé€‚åº”é€‰æ‹©æœ€ä½³åŠ å¯†ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•æé«˜äº†è¶Šç‹±æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œè·¨ä¸åŒä»»åŠ¡ç±»å‹ã€å—å®³è€…LLMå’Œå®‰å…¨é˜²æŠ¤æªæ–½çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶æŒ‰è®¾è®¡æ˜¯æ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„ï¼Œæ”¯æŒä»»æ„å¯†ç å®¶æ—ï¼Œå¹¶èƒ½é€‚åº”ä¸æ–­å˜åŒ–çš„å¯¹æŠ—ç­–ç•¥ã€‚æˆ‘ä»¬å¯¹å¤šä¸ªå—å®³è€…LLMçš„å¯†ç æ€§èƒ½è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯åˆ†æï¼Œä»¥è¡¥å……æˆ‘ä»¬çš„æ–¹æ³•ã€‚åœ¨ä»…10æ¬¡æŸ¥è¯¢å†…ï¼ŒMetaCipheråœ¨æœ€æ–°çš„æ ‡å‡†æ¶æ„æç¤ºåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†è¶…è¿‡92%çš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ï¼Œé’ˆå¯¹æœ€æ–°éæ¨ç†å‹LLMï¼Œä»¥åŠè¶…è¿‡7.å¯¹å…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMè¾¾åˆ°çº¦4%çš„ASRã€‚è¿™ä¸€è¡¨ç°è¶…è¿‡äº†æ‰€æœ‰ç°æœ‰çš„åŸºäºæ¨¡ç³ŠæŠ€æœ¯çš„è¶Šç‹±æ–¹æ³•ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„é•¿æœŸç¨³å¥æ€§å’Œé€‚åº”æ€§ï¼Œä½¿å…¶åœ¨é¢å¯¹æ—¥ç›Šè¿›æ­¥çš„å®‰å…¨æªæ–½æ—¶æ›´å…·éŸ§æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22557v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›å¢é•¿ä½¿å…¶é¢ä¸´è¶Šæ¥è¶Šå¤æ‚çš„è¶Šç‹±æ”»å‡»ï¼Œå…¶ä¸­åŸºäºæ¨¡ç³Šçš„æ”»å‡»â€”â€”é€šè¿‡åŠ å¯†æ¶æ„å†…å®¹æ¥èº²é¿æ£€æµ‹â€”â€”ä»ç„¶éå¸¸æœ‰æ•ˆã€‚åˆ©ç”¨å…ˆè¿›LLMsçš„æ¨ç†èƒ½åŠ›æ¥è§£é‡ŠåŠ å¯†æç¤ºï¼Œè¿™äº›æ”»å‡»ç»•è¿‡äº†ä¾èµ–å…³é”®è¯æ£€æµ‹æˆ–ä¸Šä¸‹æ–‡è¿‡æ»¤çš„ä¼ ç»Ÿé˜²å¾¡æ‰‹æ®µã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºMetaCipherçš„æ–°å‹åŸºäºæ¨¡ç³ŠæŠ€æœ¯çš„è¶Šç‹±æ¡†æ¶ï¼Œä»¥åŠä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„åŠ¨æ€å¯†ç é€‰æ‹©æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥ä»å¯†ç æ± ä¸­è‡ªé€‚åº”é€‰æ‹©æœ€ä½³åŠ å¯†ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•æé«˜äº†è¶Šç‹±æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œè·¨ä¸åŒä»»åŠ¡ç±»å‹ã€å—å®³è€…LLMså’Œå®‰å…¨é˜²æŠ¤æªæ–½çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶æŒ‰æ¨¡å—åŒ–è®¾è®¡ï¼Œå…·æœ‰å¯æ‰©å±•æ€§ï¼Œæ”¯æŒä»»æ„å¯†ç å®¶æ—ï¼Œå¹¶èƒ½é€‚åº”ä¸æ–­å‘å±•çš„å¯¹æŠ—ç­–ç•¥ã€‚æˆ‘ä»¬ç»“åˆå¤§é‡å®è¯ç ”ç©¶ï¼Œå¯¹å¤šä¸ªå—å®³è€…LLMsçš„å¯†ç æ€§èƒ½è¿›è¡Œäº†è¡¥å……åˆ†æã€‚åœ¨ä»…10æ¬¡æŸ¥è¯¢å†…ï¼ŒMetaCipheråœ¨æœ€æ–°æ ‡å‡†æ¶æ„æç¤ºåŸºå‡†æµ‹è¯•ä¸Šçš„æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰è¶…è¿‡92%ï¼Œé’ˆå¯¹ç°æœ‰éæ¨ç†å‹LLMsçš„ASRè¶…è¿‡74%ï¼Œæ˜¾ç¤ºå‡ºæˆ‘ä»¬æ–¹æ³•çš„é•¿æœŸç¨³å¥æ€§å’Œé€‚åº”æ€§ï¼Œä½¿å…¶åœ¨é¢å¯¹ä¸æ–­å‘å±•çš„å®‰å…¨æªæ–½æ—¶æ¯”ç°æœ‰æ–¹æ³•æ›´å…·æŠ—æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢ä¸´æ—¥ç›Šå¤æ‚çš„æ”»å‡»ï¼Œå…¶ä¸­åŸºäºæ¨¡ç³Šçš„åŠ å¯†æ”»å‡»å°¤ä¸ºæœ‰æ•ˆã€‚</li>
<li>åŸºäºæ¨¡ç³ŠæŠ€æœ¯çš„MetaCipheræ¡†æ¶è¢«æå‡ºï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ åŠ¨æ€é€‰æ‹©åŠ å¯†ç­–ç•¥ã€‚</li>
<li>MetaCipherå¢å¼ºäº†è¶Šç‹±æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œå¯é€‚åº”ä¸åŒçš„ä»»åŠ¡ç±»å‹ã€LLMså’Œå®‰å…¨æªæ–½ã€‚</li>
<li>MetaCipheræ¡†æ¶æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒå¤šç§å¯†ç å®¶æ—å’Œé€‚åº”ä¸æ–­å‘å±•çš„å¯¹æŠ—ç­–ç•¥ã€‚</li>
<li>MetaCipheråœ¨æ ‡å‡†æ¶æ„æç¤ºåŸºå‡†æµ‹è¯•ä¸Šçš„æ”»å‡»æˆåŠŸç‡è¶…è¿‡92%ï¼Œæ˜¾ç¤ºå‡ºå…¶é•¿æœŸç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>ä¸éæ¨ç†å‹LLMsç›¸æ¯”ï¼ŒMetaCipherå¯¹å…·æœ‰æ¨ç†èƒ½åŠ›çš„LLMsçš„æ”»å‡»æˆåŠŸç‡è¶…è¿‡74%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdd546f726da10b2420b6388bff8da77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99910d5e59664bbde212e50300900223.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a33b0ddb4380f586c4690fc734e61b4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c53698f7a7cd23c9816e39967f3dbc3f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-47b30a4ff914de705cc022b85fe30c9c.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Positional Bias in Binary Question Answering How Uncertainty Shapes   Model Preferences
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-afe56b34b0fa28b9fec39311c219549c.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-29  MambaMorph a Mamba-based Framework for Medical MR-CT Deformable   Registration
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31987.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
