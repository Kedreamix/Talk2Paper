<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Visual Textualization for Image Prompted Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-66df7122ad8bb673f007c253fe7b5fc0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    60 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-03-æ›´æ–°"><a href="#2025-07-03-æ›´æ–°" class="headerlink" title="2025-07-03 æ›´æ–°"></a>2025-07-03 æ›´æ–°</h1><h2 id="Visual-Textualization-for-Image-Prompted-Object-Detection"><a href="#Visual-Textualization-for-Image-Prompted-Object-Detection" class="headerlink" title="Visual Textualization for Image Prompted Object Detection"></a>Visual Textualization for Image Prompted Object Detection</h2><p><strong>Authors:Yongjian Wu, Yang Zhou, Jiya Saiyin, Bingzheng Wei, Yan Xu</strong></p>
<p>We propose VisTex-OVLM, a novel image prompted object detection method that introduces visual textualization â€“ a process that projects a few visual exemplars into the text feature space to enhance Object-level Vision-Language Modelsâ€™ (OVLMs) capability in detecting rare categories that are difficult to describe textually and nearly absent from their pre-training data, while preserving their pre-trained object-text alignment. Specifically, VisTex-OVLM leverages multi-scale textualizing blocks and a multi-stage fusion strategy to integrate visual information from visual exemplars, generating textualized visual tokens that effectively guide OVLMs alongside text prompts. Unlike previous methods, our method maintains the original architecture of OVLM, maintaining its generalization capabilities while enhancing performance in few-shot settings. VisTex-OVLM demonstrates superior performance across open-set datasets which have minimal overlap with OVLMâ€™s pre-training data and achieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/WitGotFlg/VisTex-OVLM">https://github.com/WitGotFlg/VisTex-OVLM</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†VisTex-OVLMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å›¾ç‰‡å¼•å¯¼ç›®æ ‡æ£€æµ‹æ³•ã€‚å®ƒé€šè¿‡è§†è§‰æ–‡æœ¬åŒ–è¿™ä¸€æµç¨‹â€”â€”å°†å°‘é‡è§†è§‰æ ·æœ¬æŠ•å°„åˆ°æ–‡æœ¬ç‰¹å¾ç©ºé—´æ¥å¢å¼ºå¯¹è±¡çº§åˆ«çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆOVLMsï¼‰æ£€æµ‹é‚£äº›éš¾ä»¥ç”¨æ–‡å­—æè¿°ã€å‡ ä¹ä¸å­˜åœ¨äºä»–ä»¬çš„é¢„è®­ç»ƒæ•°æ®ä¸­çš„ç¨€æœ‰ç±»åˆ«çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå®ƒä»¬é¢„å…ˆè®­ç»ƒçš„å¯¹è±¡æ–‡æœ¬å¯¹é½æ–¹å¼ã€‚å…·ä½“æ¥è¯´ï¼ŒVisTex-OVLMåˆ©ç”¨å¤šå°ºåº¦æ–‡æœ¬åŒ–å—å’Œå¤šé˜¶æ®µèåˆç­–ç•¥æ•´åˆæ¥è‡ªè§†è§‰æ ·æœ¬çš„è§†è§‰ä¿¡æ¯ï¼Œç”Ÿæˆå¸¦æœ‰æ–‡æœ¬çš„è§†è§‰ä»¤ç‰Œï¼Œæœ‰æ•ˆåœ°æŒ‡å¯¼OVLMä¸æ–‡æœ¬æç¤ºä¸€èµ·å‘æŒ¥ä½œç”¨ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿æŒäº†OVLMçš„åŸå§‹æ¶æ„ï¼Œä¿æŒäº†å…¶æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶åœ¨å°‘æ ·æœ¬ç¯å¢ƒä¸­å¢å¼ºäº†æ€§èƒ½ã€‚VisTex-OVLMåœ¨å¼€æ”¾æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸OVLMçš„é¢„è®­ç»ƒæ•°æ®é‡å æå°ï¼Œå¹¶åœ¨PASCAL VOCå’ŒMSCOCOçš„å°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/WitGotFlg/VisTex-OVLM%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/WitGotFlg/VisTex-OVLMå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23785v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>VisTex-OVLMæ˜¯ä¸€ç§æ–°å‹å›¾åƒæç¤ºç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œå®ƒé€šè¿‡è§†è§‰æ–‡æœ¬åŒ–æŠ€æœ¯å¢å¼ºObject-level Vision-Language Modelsï¼ˆOVLMï¼‰åœ¨æ£€æµ‹ç½•è§ç±»åˆ«ç›®æ ‡çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å°†è§†è§‰å®ä¾‹æŠ•å½±åˆ°æ–‡æœ¬ç‰¹å¾ç©ºé—´ï¼Œæé«˜æ¨¡å‹åœ¨æ£€æµ‹éš¾ä»¥æ–‡æœ¬æè¿°çš„ç½•è§ç±»åˆ«ç›®æ ‡çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™é¢„è®­ç»ƒçš„å¯¹è±¡æ–‡æœ¬å¯¹é½ã€‚VisTex-OVLMé‡‡ç”¨å¤šå°ºåº¦æ–‡æœ¬åŒ–å—å’Œå¤šé˜¶æ®µèåˆç­–ç•¥ï¼Œé›†æˆè§†è§‰ä¿¡æ¯ç”Ÿæˆæ–‡æœ¬åŒ–è§†è§‰æ ‡è®°ï¼Œæœ‰æ•ˆå¼•å¯¼OVLMä¸æ–‡æœ¬æç¤ºååŒå·¥ä½œã€‚è¯¥æ–¹æ³•åœ¨å¼€æ”¾æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œä¸OVLMé¢„è®­ç»ƒæ•°æ®é‡å åº¦æå°ï¼Œå¹¶åœ¨PASCAL VOCå’ŒMSCOCOå°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/WitGotFlg/VisTex-OVLM%E3%80%82">https://github.com/WitGotFlg/VisTex-OVLMã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisTex-OVLMæ˜¯ä¸€ç§æ–°å‹å›¾åƒæç¤ºç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡è§†è§‰æ–‡æœ¬åŒ–æŠ€æœ¯å¢å¼ºæ¨¡å‹æ£€æµ‹ç½•è§ç±»åˆ«ç›®æ ‡çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•å°†è§†è§‰å®ä¾‹æŠ•å½±åˆ°æ–‡æœ¬ç‰¹å¾ç©ºé—´ï¼Œä»¥æé«˜æ¨¡å‹æè¿°å’Œæ£€æµ‹éš¾ä»¥æ–‡æœ¬æè¿°çš„ç½•è§ç±»åˆ«ç›®æ ‡çš„æ€§èƒ½ã€‚</li>
<li>VisTex-OVLMä¿ç•™äº†é¢„è®­ç»ƒçš„å¯¹è±¡æ–‡æœ¬å¯¹é½ï¼ŒåŒæ—¶å¼•å…¥äº†å¤šå°ºåº¦æ–‡æœ¬åŒ–å—å’Œå¤šé˜¶æ®µèåˆç­–ç•¥æ¥é›†æˆè§†è§‰ä¿¡æ¯ã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆæ–‡æœ¬åŒ–è§†è§‰æ ‡è®°ï¼Œæœ‰æ•ˆå¼•å¯¼OVLMä¸æ–‡æœ¬æç¤ºååŒå·¥ä½œï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>VisTex-OVLMåœ¨å¼€æ”¾æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜è¶Šï¼Œä¸é¢„è®­ç»ƒæ•°æ®çš„é‡å åº¦ä½ã€‚</li>
<li>VisTex-OVLMåœ¨PASCAL VOCå’ŒMSCOCOå°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-005f31e18c029e46a2deb180649bc941.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4ba9b2ce8b0dad5672ccb509571a48a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9afb17e02ef94a3b1c21b9920b1f167d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04a11874a0698c5c60f93faa6f77ccf0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PBCAT-Patch-based-composite-adversarial-training-against-physically-realizable-attacks-on-object-detection"><a href="#PBCAT-Patch-based-composite-adversarial-training-against-physically-realizable-attacks-on-object-detection" class="headerlink" title="PBCAT: Patch-based composite adversarial training against physically   realizable attacks on object detection"></a>PBCAT: Patch-based composite adversarial training against physically   realizable attacks on object detection</h2><p><strong>Authors:Xiao Li, Yiming Zhu, Yifan Huang, Wei Zhang, Yingzhe He, Jie Shi, Xiaolin Hu</strong></p>
<p>Object detection plays a crucial role in many security-sensitive applications. However, several recent studies have shown that object detectors can be easily fooled by physically realizable attacks, \eg, adversarial patches and recent adversarial textures, which pose realistic and urgent threats. Adversarial Training (AT) has been recognized as the most effective defense against adversarial attacks. While AT has been extensively studied in the $l_\infty$ attack settings on classification models, AT against physically realizable attacks on object detectors has received limited exploration. Early attempts are only performed to defend against adversarial patches, leaving AT against a wider range of physically realizable attacks under-explored. In this work, we consider defending against various physically realizable attacks with a unified AT method. We propose PBCAT, a novel Patch-Based Composite Adversarial Training strategy. PBCAT optimizes the model by incorporating the combination of small-area gradient-guided adversarial patches and imperceptible global adversarial perturbations covering the entire image. With these designs, PBCAT has the potential to defend against not only adversarial patches but also unseen physically realizable attacks such as adversarial textures. Extensive experiments in multiple settings demonstrated that PBCAT significantly improved robustness against various physically realizable attacks over state-of-the-art defense methods. Notably, it improved the detection accuracy by 29.7% over previous defense methods under one recent adversarial texture attack. </p>
<blockquote>
<p>å¯¹è±¡æ£€æµ‹åœ¨è®¸å¤šå®‰å…¨æ•æ„Ÿåº”ç”¨ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ä¸€äº›ç ”ç©¶è¡¨æ˜ï¼Œå¯¹è±¡æ£€æµ‹å™¨å¾ˆå®¹æ˜“å—åˆ°ç‰©ç†å¯å®ç°æ”»å‡»ï¼ˆä¾‹å¦‚å¯¹æŠ—æ€§è¡¥ä¸å’Œæœ€æ–°çš„å¯¹æŠ—æ€§çº¹ç†ï¼‰çš„æ¬ºéª—ï¼Œè¿™äº›æ”»å‡»å¸¦æ¥äº†ç°å®è€Œç´§è¿«çš„å¨èƒã€‚å¯¹æŠ—è®­ç»ƒï¼ˆATï¼‰å·²è¢«å…¬è®¤ä¸ºå¯¹æŠ—å¯¹æŠ—æ€§æ”»å‡»çš„æœ€æœ‰æ•ˆé˜²å¾¡æ‰‹æ®µã€‚è™½ç„¶ATåœ¨$l_\infty$æ”»å‡»åˆ†ç±»æ¨¡å‹çš„è®¾ç½®ä¸­å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†é’ˆå¯¹å¯¹è±¡æ£€æµ‹å™¨çš„ç‰©ç†å¯å®ç°æ”»å‡»çš„ATå´é²œæœ‰ç ”ç©¶ã€‚æ—©æœŸçš„å°è¯•ä»…ç”¨äºé˜²å¾¡å¯¹æŠ—æ€§è¡¥ä¸ï¼Œè€Œå¯¹æ›´å¹¿æ³›çš„ç‰©ç†å¯å®ç°æ”»å‡»çš„ATæ¢ç´¢ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘ä½¿ç”¨ç»Ÿä¸€çš„ATæ–¹æ³•æ¥é˜²å¾¡å„ç§ç‰©ç†å¯å®ç°æ”»å‡»ã€‚æˆ‘ä»¬æå‡ºäº†PBCATï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¡¥ä¸çš„å¤åˆå¯¹æŠ—è®­ç»ƒç­–ç•¥ã€‚PBCATé€šè¿‡ç»“åˆå°åŒºåŸŸæ¢¯åº¦å¼•å¯¼å¯¹æŠ—è¡¥ä¸å’Œè¦†ç›–æ•´ä¸ªå›¾åƒçš„éš¾ä»¥å¯Ÿè§‰çš„å…¨å±€å¯¹æŠ—æ‰°åŠ¨æ¥ä¼˜åŒ–æ¨¡å‹ã€‚é€šè¿‡è¿™äº›è®¾è®¡ï¼ŒPBCATä¸ä»…æœ‰æ½œåŠ›é˜²å¾¡å¯¹æŠ—è¡¥ä¸ï¼Œè¿˜æœ‰æ½œåŠ›é˜²å¾¡æœªè§è¿‡çš„ç‰©ç†å¯å®ç°æ”»å‡»ï¼Œå¦‚å¯¹æŠ—çº¹ç†ã€‚åœ¨å¤šä¸ªè®¾ç½®ä¸­çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„é˜²å¾¡æ–¹æ³•ç›¸æ¯”ï¼ŒPBCATåœ¨æŠµæŠ—å„ç§ç‰©ç†å¯å®ç°æ”»å‡»æ–¹é¢æ˜¾è‘—æé«˜äº†é²æ£’æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨æœ€è¿‘çš„ä¸€æ¬¡å¯¹æŠ—çº¹ç†æ”»å‡»ä¸‹ï¼Œç›¸è¾ƒäºä»¥å‰çš„æ–¹æ³•æé«˜äº†29.7%çš„æ£€æµ‹å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23581v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç›®æ ‡æ£€æµ‹åœ¨å®‰å…¨æ€§æ•æ„Ÿåº”ç”¨ä¸­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é¢ä¸´ç‰©ç†å¯å®ç°æ”»å‡»æ—¶çš„é—®é¢˜ã€‚è™½ç„¶å¯¹æŠ—è®­ç»ƒï¼ˆATï¼‰åœ¨åˆ†ç±»æ¨¡å‹ä¸­çš„å¯¹æŠ—æ”»å‡»ä¸­å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†åœ¨ç›®æ ‡æ£€æµ‹å™¨çš„ç‰©ç†å¯å®ç°æ”»å‡»ä¸Šçš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¡¥ä¸çš„å¤åˆå¯¹æŠ—è®­ç»ƒç­–ç•¥ï¼ˆPBCATï¼‰ï¼Œèƒ½å¤Ÿé˜²å¾¡å„ç§ç‰©ç†å¯å®ç°æ”»å‡»ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPBCATåœ¨å„ç§ç‰©ç†å¯å®ç°æ”»å‡»ä¸­å…·æœ‰æ›´é«˜çš„é²æ£’æ€§ã€‚ç‰¹åˆ«æ˜¯é’ˆå¯¹ä¸€ç§æœ€æ–°çš„å¯¹æŠ—çº¹ç†æ”»å‡»ï¼Œå…¶æ£€æµ‹å‡†ç¡®ç‡æé«˜äº†29.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›®æ ‡æ£€æµ‹åœ¨å®‰å…¨æ€§æ•æ„Ÿåº”ç”¨ä¸­é¢ä¸´ç‰©ç†å¯å®ç°æ”»å‡»çš„æŒ‘æˆ˜ã€‚</li>
<li>å¯¹æŠ—è®­ç»ƒï¼ˆATï¼‰æ˜¯é˜²å¾¡å¯¹æŠ—æ”»å‡»çš„æœ‰æ•ˆæ‰‹æ®µï¼Œä½†åœ¨ç›®æ ‡æ£€æµ‹å™¨çš„ç‰©ç†å¯å®ç°æ”»å‡»æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºè¡¥ä¸çš„å¤åˆå¯¹æŠ—è®­ç»ƒç­–ç•¥ï¼ˆPBCATï¼‰ï¼Œç”¨äºé˜²å¾¡å¤šç§ç‰©ç†å¯å®ç°æ”»å‡»ã€‚</li>
<li>PBCATé€šè¿‡ç»“åˆå°åŒºåŸŸæ¢¯åº¦å¼•å¯¼çš„å¯¹æŠ—è¡¥ä¸å’Œè¦†ç›–æ•´ä¸ªå›¾åƒçš„ä¸æ˜“å¯Ÿè§‰çš„å…¨å±€å¯¹æŠ—æ‰°åŠ¨æ¥ä¼˜åŒ–æ¨¡å‹ã€‚</li>
<li>PBCATä¸ä»…èƒ½é˜²å¾¡å¯¹æŠ—è¡¥ä¸æ”»å‡»ï¼Œè¿˜èƒ½é˜²å¾¡æœªçŸ¥çš„ç‰©ç†å¯å®ç°æ”»å‡»ï¼Œå¦‚å¯¹æŠ—çº¹ç†æ”»å‡»ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPBCATåœ¨å„ç§ç‰©ç†å¯å®ç°æ”»å‡»ä¸­å…·æœ‰æ›´é«˜çš„é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6b273d9334ad3853b38f5fb6c003b1bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1bfb32119131cd411f50b81ad3957d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28046a8fd365bda3272c920eee933ff7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94b99aae7ab97238de9cbb0a03c01416.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DGE-YOLO-Dual-Branch-Gathering-and-Attention-for-Accurate-UAV-Object-Detection"><a href="#DGE-YOLO-Dual-Branch-Gathering-and-Attention-for-Accurate-UAV-Object-Detection" class="headerlink" title="DGE-YOLO: Dual-Branch Gathering and Attention for Accurate UAV Object   Detection"></a>DGE-YOLO: Dual-Branch Gathering and Attention for Accurate UAV Object   Detection</h2><p><strong>Authors:Kunwei Lv, Ping Lan</strong></p>
<p>The rapid proliferation of unmanned aerial vehicles (UAVs) has highlighted the importance of robust and efficient object detection in diverse aerial scenarios. Detecting small objects under complex conditions, however, remains a significant challenge. Existing approaches often prioritize inference speed, leading to degraded performance when handling multi-modal inputs. To address this, we present DGE-YOLO, an enhanced YOLO-based detection framework designed to effectively fuse multi-modal information. Specifically, we introduce a dual-branch architecture for modality-specific feature extraction, enabling the model to process both infrared and visible images. To further enrich semantic representation, we propose an Efficient Multi-scale Attention (EMA) mechanism that enhances feature learning across spatial scales. Additionally, we replace the conventional neck with a Gather-and-Distribute module to mitigate information loss during feature aggregation. Extensive experiments on the Drone Vehicle dataset demonstrate that DGE-YOLO achieves superior performance over state-of-the-art methods, validating its effectiveness in multi-modal UAV object detection tasks. </p>
<blockquote>
<p>éšç€æ— äººæœºï¼ˆUAVsï¼‰çš„å¿«é€Ÿæ™®åŠï¼Œåœ¨å„ç§ç©ºä¸­åœºæ™¯ä¸­å®ç°ç¨³å¥é«˜æ•ˆçš„ç›®æ ‡æ£€æµ‹æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚ç„¶è€Œï¼Œåœ¨å¤æ‚æ¡ä»¶ä¸‹æ£€æµ‹å°ç›®æ ‡ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€ä¼˜å…ˆæ³¨é‡æ¨ç†é€Ÿåº¦ï¼Œåœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DGE-YOLOï¼Œè¿™æ˜¯ä¸€ç§åŸºäºYOLOçš„å¢å¼ºæ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°èåˆå¤šæ¨¡æ€ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºç‰¹å®šæ¨¡æ€ç‰¹å¾æå–çš„åŒåˆ†æ”¯æ¶æ„ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†çº¢å¤–å’Œå¯è§å…‰å›¾åƒã€‚ä¸ºäº†ä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¤šå°ºåº¦æ³¨æ„åŠ›ï¼ˆEMAï¼‰æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥åœ¨ä¸åŒçš„ç©ºé—´å°ºåº¦ä¸Šå¢å¼ºç‰¹å¾å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç”¨Gather-and-Distributeæ¨¡å—æ›¿æ¢äº†ä¼ ç»Ÿçš„é¢ˆéƒ¨ï¼Œä»¥å‡è½»ç‰¹å¾èšåˆè¿‡ç¨‹ä¸­çš„ä¿¡æ¯æŸå¤±ã€‚åœ¨Drone Vehicleæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDGE-YOLOåœ¨æ— äººæœºå¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šæ¨¡æ€æ— äººæœºç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23252v1">PDF</a> 8 pages, 5 figures</p>
<p><strong>Summary</strong><br>æ— äººæœºæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ä½¿å¾—åœ¨å¤šæ ·åŒ–çš„ç©ºä¸­åœºæ™¯ä¸‹å®ç°ç¨³å¥é«˜æ•ˆçš„ç›®æ ‡æ£€æµ‹è‡³å…³é‡è¦ã€‚å¤æ‚æ¡ä»¶ä¸‹çš„å°ç›®æ ‡æ£€æµ‹ä»æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ³¨é‡æ¨ç†é€Ÿåº¦ï¼Œåœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶æ€§èƒ½ä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåŸºäºYOLOå¢å¼ºçš„æ£€æµ‹æ¡†æ¶DGE-YOLOï¼Œæœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯ã€‚é€šè¿‡å¼•å…¥åŒåˆ†æ”¯æ¶æ„è¿›è¡Œæ¨¡æ€ç‰¹å®šç‰¹å¾æå–ï¼Œèƒ½å¤Ÿå¤„ç†çº¢å¤–å’Œå¯è§å…‰å›¾åƒã€‚ä¸ºä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºï¼Œæˆ‘ä»¬æå‡ºé«˜æ•ˆå¤šå°ºåº¦æ³¨æ„åŠ›ï¼ˆEMAï¼‰æœºåˆ¶ï¼Œå¢å¼ºè·¨ç©ºé—´å°ºåº¦çš„ç‰¹å¾å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨Gather-and-Distributeæ¨¡å—æ›¿ä»£ä¼ ç»Ÿé¢ˆéƒ¨ç»“æ„ï¼Œå‡å°‘ç‰¹å¾èšåˆè¿‡ç¨‹ä¸­çš„ä¿¡æ¯æŸå¤±ã€‚åœ¨Drone Vehicleæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDGE-YOLOåœ¨å¤šåª’ä½“æ— äººæœºç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— äººæœºæŠ€æœ¯çš„å¿«é€Ÿå‘å±•çªå‡ºäº†åœ¨å¤šæ ·åŒ–ç©ºä¸­åœºæ™¯ä¸‹å®ç°ç¨³å¥é«˜æ•ˆç›®æ ‡æ£€æµ‹çš„å¿…è¦æ€§ã€‚</li>
<li>å¤æ‚æ¡ä»¶ä¸‹çš„å°ç›®æ ‡æ£€æµ‹æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šæ¨¡æ€è¾“å…¥æ—¶æ€§èƒ½ä¸‹é™ï¼Œä¸»è¦å› ä¸ºæ³¨é‡æ¨ç†é€Ÿåº¦è€Œå¿½è§†å‡†ç¡®æ€§ã€‚</li>
<li>DGE-YOLOæ˜¯ä¸€ä¸ªåŸºäºYOLOçš„å¢å¼ºæ£€æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯ã€‚</li>
<li>DGE-YOLOå¼•å…¥åŒåˆ†æ”¯æ¶æ„è¿›è¡Œæ¨¡æ€ç‰¹å®šç‰¹å¾æå–ï¼Œèƒ½å¤„ç†çº¢å¤–å’Œå¯è§å…‰å›¾åƒã€‚</li>
<li>EMAæœºåˆ¶ç”¨äºä¸°å¯Œè¯­ä¹‰è¡¨ç¤ºå¹¶å¢å¼ºè·¨ç©ºé—´å°ºåº¦çš„ç‰¹å¾å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23252">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e09afc251679a7c2ac0ffb6ffb14e7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68c5b83c0cdfbd1d1444aa3684ebfbc6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aeb058aead06301351692380df9d0bee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f43f42bfdb586a31afb7a526b2dcbee9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fe718f33f4b46860804f25a32c1160f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-399b639e38cab3ef6519325ffb09c062.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CRISP-SAM2-SAM2-with-Cross-Modal-Interaction-and-Semantic-Prompting-for-Multi-Organ-Segmentation"><a href="#CRISP-SAM2-SAM2-with-Cross-Modal-Interaction-and-Semantic-Prompting-for-Multi-Organ-Segmentation" class="headerlink" title="CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for   Multi-Organ Segmentation"></a>CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for   Multi-Organ Segmentation</h2><p><strong>Authors:Xinlei Yu, Chanmiao Wang, Hui Jin, Ahmed Elazab, Gangyong Jia, Xiang Wan, Changqing Zou, Ruiquan Ge</strong></p>
<p>Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/YU-deep/CRISP/_SAM2.git">https://github.com/YU-deep/CRISP\_SAM2.git</a>. </p>
<blockquote>
<p>åŒ»å­¦å¤šå™¨å®˜åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒå¤„ç†çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¯¹äºåŒ»ç”Ÿè¿›è¡Œå‡†ç¡®è¯Šæ–­å’Œåˆ¶å®šæœ‰æ•ˆæ²»ç–—æ–¹æ¡ˆè‡³å…³é‡è¦ã€‚å°½ç®¡è¯¥é¢†åŸŸå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰çš„å¤šå™¨å®˜åˆ†å‰²æ¨¡å‹é€šå¸¸å­˜åœ¨ç»†èŠ‚ä¸å‡†ç¡®ã€ä¾èµ–å‡ ä½•æç¤ºä»¥åŠç©ºé—´ä¿¡æ¯ä¸¢å¤±ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºCRISP-SAM2çš„æ–°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºSAM2å…·æœ‰è·¨æ¨¡æ€äº¤äº’å’Œè¯­ä¹‰æç¤ºåŠŸèƒ½ã€‚è¯¥æ¨¡å‹æ˜¯ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œé€šè¿‡å™¨å®˜çš„æ–‡å­—æè¿°æ¥å¼•å¯¼å¤šå™¨å®˜åŒ»å­¦åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡æ¸è¿›çš„äº¤å‰æ³¨æ„åŠ›äº¤äº’æœºåˆ¶å°†è§†è§‰å’Œæ–‡å­—è¾“å…¥è½¬æ¢ä¸ºè·¨æ¨¡æ€ä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚ç„¶åï¼Œè¿™äº›è¯­ä¹‰è¢«æ³¨å…¥å›¾åƒç¼–ç å™¨ï¼Œä»¥æé«˜å¯¹è§†è§‰ä¿¡æ¯çš„è¯¦ç»†ç†è§£ã€‚ä¸ºäº†å‡å°‘å¯¹å‡ ä½•æç¤ºçš„ä¾èµ–ï¼Œæˆ‘ä»¬ä½¿ç”¨è¯­ä¹‰æç¤ºç­–ç•¥ï¼Œä»¥æ›¿ä»£åŸå§‹æç¤ºç¼–ç å™¨ï¼Œæé«˜éš¾ä»¥è¯†åˆ«ç›®æ ‡çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†ç”¨äºè®°å¿†çš„ç›¸ä¼¼åº¦æ’åºè‡ªæ›´æ–°ç­–ç•¥å’Œæ©è†œç»†åŒ–è¿‡ç¨‹ï¼Œä»¥è¿›ä¸€æ­¥é€‚åº”åŒ»å­¦å½±åƒå¹¶å¢å¼ºå±€éƒ¨ç»†èŠ‚ã€‚åœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¯¹æ¯”å®éªŒè¡¨æ˜ï¼ŒCRISP-SAM2ä¼˜äºç°æœ‰æ¨¡å‹ã€‚å¹¿æ³›çš„åˆ†æä¹Ÿè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä»è€Œè¯å®äº†å…¶å“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³ä¸Šè¿°é™åˆ¶æ–¹é¢ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/YU-deep/CRISP_SAM2.git">https://github.com/YU-deep/CRISP_SAM2.git</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23121v1">PDF</a> 19 pages, 9 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCRISP-SAM2çš„æ–°å‹å¤šå™¨å®˜åŒ»å­¦åˆ†å‰²æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€äº¤äº’å’Œè¯­ä¹‰æç¤ºæŠ€æœ¯ï¼Œè§£å†³äº†å½“å‰æ¨¡å‹åœ¨ç»†èŠ‚å‡†ç¡®æ€§ã€å¯¹å‡ ä½•æç¤ºçš„ä¾èµ–ä»¥åŠç©ºé—´ä¿¡æ¯ä¸¢å¤±ç­‰æ–¹é¢çš„é—®é¢˜ã€‚CRISP-SAM2æ¨¡å‹é€šè¿‡æ¸è¿›å¼è·¨æ³¨æ„åŠ›äº¤äº’æœºåˆ¶ï¼Œå°†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºè·¨æ¨¡æ€ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œç„¶åå°†å…¶æ³¨å…¥å›¾åƒç¼–ç å™¨ï¼Œæé«˜è§†è§‰ä¿¡æ¯çš„è¯¦ç»†ç†è§£ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨è¯­ä¹‰æç¤ºç­–ç•¥æ¶ˆé™¤äº†å¯¹å‡ ä½•æç¤ºçš„ä¾èµ–ï¼ŒåŒæ—¶é‡‡ç”¨ç›¸ä¼¼åº¦æ’åºè‡ªæ›´æ–°ç­–ç•¥å’Œæ©è†œç»†åŒ–è¿‡ç¨‹ï¼Œè¿›ä¸€æ­¥é€‚åº”åŒ»å­¦æˆåƒå¹¶å¢å¼ºå±€éƒ¨ç»†èŠ‚ã€‚åœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¯¹æ¯”å®éªŒè¡¨æ˜ï¼ŒCRISP-SAM2æ¨¡å‹ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå™¨å®˜åŒ»å­¦åˆ†å‰²æ˜¯åŒ»ç–—å›¾åƒå¤„ç†ä¸­çš„å…³é”®éƒ¨åˆ†ï¼Œå¯¹åŒ»ç”Ÿè¿›è¡Œå‡†ç¡®è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å¤šå™¨å®˜åˆ†å‰²æ¨¡å‹å­˜åœ¨ç»†èŠ‚ä¸å‡†ç¡®ã€ä¾èµ–å‡ ä½•æç¤ºå’Œç©ºé—´ä¿¡æ¯ä¸¢å¤±ç­‰æŒ‘æˆ˜ã€‚</li>
<li>CRISP-SAM2æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€äº¤äº’å’Œè¯­ä¹‰æç¤ºæŠ€æœ¯è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>CRISP-SAM2å°†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºè·¨æ¨¡æ€ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œæé«˜è§†è§‰ä¿¡æ¯ç†è§£ã€‚</li>
<li>è¯­ä¹‰æç¤ºç­–ç•¥æ¶ˆé™¤å¯¹å‡ ä½•æç¤ºçš„ä¾èµ–ï¼Œå¹¶é‡‡ç”¨ç›¸ä¼¼åº¦æ’åºè‡ªæ›´æ–°ç­–ç•¥å’Œæ©è†œç»†åŒ–è¿‡ç¨‹ï¼Œé€‚åº”åŒ»å­¦æˆåƒå¹¶å¢å¼ºå±€éƒ¨ç»†èŠ‚ã€‚</li>
<li>åœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¯¹æ¯”å®éªŒæ˜¾ç¤ºï¼ŒCRISP-SAM2æ¨¡å‹æ€§èƒ½ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23121">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-00334622fd2ff0eb05557b1dd80dffb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8db8a3d6ff0ef75ecbfba86f9076f4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6595d78001f04a7058b24c04f7c1ab2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8427ee27dc7b238598ed38a9cc350d6c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Dual-Atrous-Separable-Convolution-for-Improving-Agricultural-Semantic-Segmentation"><a href="#Dual-Atrous-Separable-Convolution-for-Improving-Agricultural-Semantic-Segmentation" class="headerlink" title="Dual Atrous Separable Convolution for Improving Agricultural Semantic   Segmentation"></a>Dual Atrous Separable Convolution for Improving Agricultural Semantic   Segmentation</h2><p><strong>Authors:Chee Mei Ling, Thangarajah Akilan, Aparna Ravinda Phalke</strong></p>
<p>Agricultural image semantic segmentation is a pivotal component of modern agriculture, facilitating accurate visual data analysis to improve crop management, optimize resource utilization, and boost overall productivity. This study proposes an efficient image segmentation method for precision agriculture, focusing on accurately delineating farmland anomalies to support informed decision-making and proactive interventions. A novel Dual Atrous Separable Convolution (DAS Conv) module is integrated within the DeepLabV3-based segmentation framework. The DAS Conv module is meticulously designed to achieve an optimal balance between dilation rates and padding size, thereby enhancing model performance without compromising efficiency. The study also incorporates a strategic skip connection from an optimal stage in the encoder to the decoder to bolster the modelâ€™s capacity to capture fine-grained spatial features. Despite its lower computational complexity, the proposed model outperforms its baseline and achieves performance comparable to highly complex transformer-based state-of-the-art (SOTA) models on the Agriculture Vision benchmark dataset. It achieves more than 66% improvement in efficiency when considering the trade-off between model complexity and performance, compared to the SOTA model. This study highlights an efficient and effective solution for improving semantic segmentation in remote sensing applications, offering a computationally lightweight model capable of high-quality performance in agricultural imagery. </p>
<blockquote>
<p>å†œä¸šå›¾åƒè¯­ä¹‰åˆ†å‰²æ˜¯ç°ä»£å†œä¸šä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œé€šè¿‡ç²¾ç¡®çš„è§†è§‰æ•°æ®åˆ†æï¼Œæœ‰åŠ©äºæ”¹å–„ä½œç‰©ç®¡ç†ã€ä¼˜åŒ–èµ„æºåˆ©ç”¨ï¼Œå¹¶æé«˜æ•´ä½“ç”Ÿäº§åŠ›ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨ç²¾å‡†å†œä¸šä¸­å‡†ç¡®æç»˜å†œç”°å¼‚å¸¸ç°è±¡ï¼Œä»¥æ”¯æŒå†³ç­–åˆ¶å®šå’Œä¸»åŠ¨å¹²é¢„ã€‚åœ¨åŸºäºDeepLabV3çš„åˆ†å‰²æ¡†æ¶ä¸­é›†æˆäº†æ–°å‹çš„åŒè†¨èƒ€å¯åˆ†ç¦»å·ç§¯ï¼ˆDAS Convï¼‰æ¨¡å—ã€‚DAS Convæ¨¡å—ç²¾å¿ƒè®¾è®¡ï¼Œä»¥å®ç°è†¨èƒ€ç‡å’Œå¡«å……å¤§å°ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½è€Œä¸æŸå¤±æ•ˆç‡ã€‚è¯¥ç ”ç©¶è¿˜ä»ç¼–ç å™¨çš„æœ€ä½³é˜¶æ®µåˆ°è§£ç å™¨åŠ å…¥äº†ç­–ç•¥æ€§è·³è¿‡è¿æ¥ï¼Œä»¥å¢å¼ºæ¨¡å‹æ•è·ç²¾ç»†ç©ºé—´ç‰¹å¾çš„èƒ½åŠ›ã€‚å°½ç®¡è®¡ç®—å¤æ‚åº¦è¾ƒä½ï¼Œè¯¥æ¨¡å‹åœ¨å†œä¸šè§†è§‰åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†åŸºçº¿æ¨¡å‹ï¼Œå¹¶è¾¾åˆ°äº†ä¸åŸºäºå¤æ‚å˜æ¢å™¨çš„å‰æ²¿æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ä¸å‰æ²¿æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨è€ƒè™‘æ¨¡å‹å¤æ‚æ€§ä¸æ€§èƒ½ä¹‹é—´çš„æƒè¡¡æ—¶ï¼Œè¯¥æ¨¡å‹çš„æ•ˆç‡æé«˜äº†66%ä»¥ä¸Šã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†æé«˜é¥æ„Ÿåº”ç”¨ä¸­è¯­ä¹‰åˆ†å‰²æ•ˆç‡çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œæä¾›äº†ä¸€ä¸ªè®¡ç®—é‡è½»ã€æ€§èƒ½é«˜çš„å†œä¸šå›¾åƒæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22570v1">PDF</a> 17 pages, 7 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>å†œä¸šå›¾åƒè¯­ä¹‰åˆ†å‰²æ˜¯ç°ä»£å†œä¸šä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œé€šè¿‡å¯¹è§†è§‰æ•°æ®çš„ç²¾ç¡®åˆ†æï¼Œæ”¹è¿›ä½œç‰©ç®¡ç†ã€ä¼˜åŒ–èµ„æºé…ç½®å¹¶æé«˜æ•´ä½“ç”Ÿäº§åŠ›ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œä¸“æ³¨äºç²¾ç¡®æç»˜å†œç”°å¼‚å¸¸ï¼Œä»¥æ”¯æŒå†³ç­–åˆ¶å®šå’Œä¸»åŠ¨å¹²é¢„ã€‚è¯¥ç ”ç©¶åœ¨æ·±LabV3åˆ†å‰²æ¡†æ¶ä¸­é›†æˆäº†æ–°å‹çš„åŒè†¨èƒ€å¯åˆ†ç¦»å·ç§¯ï¼ˆDAS Convï¼‰æ¨¡å—ï¼Œæ—¨åœ¨å®ç°è†¨èƒ€ç‡å’Œå¡«å……å¤§å°ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½è€Œä¸æŸå¤±æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ä»ç¼–ç å™¨ä¸­çš„æœ€ä½³é˜¶æ®µåˆ°è§£ç å™¨çš„æˆ˜ç•¥è·³è·ƒè¿æ¥ï¼Œå¢å¼ºäº†æ¨¡å‹æ•è·ç²¾ç»†ç©ºé—´ç‰¹å¾çš„èƒ½åŠ›ã€‚ä¸å…¶ä»–é«˜åº¦å¤æ‚çš„åŸºäºè½¬æ¢å™¨çš„æœ€æ–°æ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨å†œä¸šè§†è§‰åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨è®¡ç®—å¤æ‚æ€§å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡æ–¹é¢å®ç°äº†è¶…è¿‡66%çš„æ•ˆç‡æå‡ã€‚æœ¬ç ”ç©¶ä¸ºè§£å†³é¥æ„Ÿåº”ç”¨ä¸­è¯­ä¹‰åˆ†å‰²é—®é¢˜æä¾›äº†é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†œä¸šå›¾åƒè¯­ä¹‰åˆ†å‰²åœ¨ç°ä»£å†œä¸šä¸­è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºè§†è§‰æ•°æ®åˆ†æä»¥æé«˜å†œä¸šç”Ÿäº§åŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºDeepLabV3çš„é«˜æ•ˆå›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œç”¨äºç²¾ç¡®æç»˜å†œç”°å¼‚å¸¸ã€‚</li>
<li>é›†æˆæ–°å‹åŒè†¨èƒ€å¯åˆ†ç¦»å·ç§¯ï¼ˆDAS Convï¼‰æ¨¡å—ï¼Œå®ç°æ¨¡å‹æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ã€‚</li>
<li>é€šè¿‡æˆ˜ç•¥è·³è·ƒè¿æ¥å¢å¼ºæ¨¡å‹æ•è·ç²¾ç»†ç©ºé—´ç‰¹å¾çš„èƒ½åŠ›ã€‚</li>
<li>æ‰€æå‡ºæ¨¡å‹åœ¨å†œä¸šè§†è§‰åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå®ç°äº†é«˜æ•ˆç‡ä¸è®¡ç®—å¤æ‚åº¦çš„æƒè¡¡ä¼˜åŒ–ã€‚</li>
<li>ä¸å½“å‰å¤æ‚çš„åŸºäºè½¬æ¢å™¨çš„æœ€æ–°æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥ç ”ç©¶æå‡ºçš„æ¨¡å‹æ›´å…·ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22570">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a9ff3aeeb1688e1447b58ee491db7e3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39dd8da2c13c84e8819e60b8f867a6e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68efa9b96b82e00aafb2d8a7a7d3a8f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2d5a86c6b83d2ccbec3ea26eb2590fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-168f7d6f74e52d2ec3860490f6810663.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improving-Token-based-Object-Detection-with-Video"><a href="#Improving-Token-based-Object-Detection-with-Video" class="headerlink" title="Improving Token-based Object Detection with Video"></a>Improving Token-based Object Detection with Video</h2><p><strong>Authors:Abhineet Singh, Nilanjan Ray</strong></p>
<p>This paper improves upon the Pix2Seq object detector by extending it for videos. In the process, it introduces a new way to perform end-to-end video object detection that improves upon existing video detectors in two key ways. First, by representing objects as variable-length sequences of discrete tokens, we can succinctly represent widely varying numbers of video objects, with diverse shapes and locations, without having to inject any localization cues in the training process. This eliminates the need to sample the space of all possible boxes that constrains conventional detectors and thus solves the dual problems of loss sparsity during training and heuristics-based postprocessing during inference. Second, it conceptualizes and outputs the video objects as fully integrated and indivisible 3D boxes or tracklets instead of generating image-specific 2D boxes and linking these boxes together to construct the video object, as done in most conventional detectors. This allows it to scale effortlessly with available computational resources by simply increasing the length of the video subsequence that the network takes as input, even generalizing to multi-object tracking if the subsequence can span the entire video. We compare our video detector with the baseline Pix2Seq static detector on several datasets and demonstrate consistent improvement, although with strong signs of being bottlenecked by our limited computational resources. We also compare it with several video detectors on UA-DETRAC to show that it is competitive with the current state of the art even with the computational bottleneck. We make our code and models publicly available. </p>
<blockquote>
<p>æœ¬æ–‡æ”¹è¿›äº†Pix2Seqç›®æ ‡æ£€æµ‹å™¨ï¼Œå°†å…¶æ‰©å±•è‡³è§†é¢‘é¢†åŸŸã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œå®ƒå¼•å…¥äº†ä¸€ç§æ–°çš„ç«¯åˆ°ç«¯è§†é¢‘ç›®æ ‡æ£€æµ‹æ–¹å¼ï¼Œåœ¨ä¸¤ä¸ªå…³é”®æ–¹é¢å¯¹ç°æœ‰è§†é¢‘æ£€æµ‹å™¨è¿›è¡Œäº†æ”¹è¿›ã€‚é¦–å…ˆï¼Œé€šè¿‡å°†ç›®æ ‡è¡¨ç¤ºä¸ºç¦»æ•£æ ‡è®°çš„å¯å˜é•¿åº¦åºåˆ—ï¼Œæˆ‘ä»¬å¯ä»¥ç®€æ´åœ°è¡¨ç¤ºæ•°é‡ä¼—å¤šã€å½¢çŠ¶å’Œä½ç½®å„å¼‚çš„è§†é¢‘ç›®æ ‡ï¼Œè€Œæ— éœ€åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ³¨å…¥ä»»ä½•å®šä½çº¿ç´¢ã€‚è¿™æ¶ˆé™¤äº†éœ€è¦å¯¹æ‰€æœ‰å¯èƒ½æ¡†çš„ç©ºé—´è¿›è¡Œé‡‡æ ·çš„éœ€è¦ï¼Œè¿™é™åˆ¶äº†ä¼ ç»Ÿæ£€æµ‹å™¨ï¼Œä»è€Œè§£å†³äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±ç¨€ç–æ€§å’Œæ¨ç†è¿‡ç¨‹ä¸­çš„åŸºäºå¯å‘å¼æ–¹æ³•çš„åå¤„ç†è¿™ä¸¤ä¸ªé—®é¢˜ã€‚å…¶æ¬¡ï¼Œå®ƒæ¦‚å¿µåŒ–å¹¶å°†è§†é¢‘ç›®æ ‡è¾“å‡ºä¸ºå®Œæ•´ä¸”ä¸å¯åˆ†çš„3Dæ¡†æˆ–è½¨è¿¹ï¼Œè€Œä¸æ˜¯ç”Ÿæˆç‰¹å®šçš„å›¾åƒ2Dæ¡†ï¼Œå¹¶å°†è¿™äº›æ¡†è¿æ¥èµ·æ¥æ„å»ºè§†é¢‘ç›®æ ‡ï¼Œè¿™æ˜¯å¤§å¤šæ•°ä¼ ç»Ÿæ£€æµ‹å™¨æ‰€åšçš„äº‹æƒ…ã€‚è¿™å…è®¸å®ƒè½»æ¾æ‰©å±•å¯ç”¨çš„è®¡ç®—èµ„æºï¼Œåªéœ€é€šè¿‡å¢åŠ ç½‘ç»œæ‰€æ¥å—çš„è§†é¢‘å­åºåˆ—çš„é•¿åº¦ï¼Œå³ä½¿å­åºåˆ—èƒ½è·¨è¶Šæ•´ä¸ªè§†é¢‘ï¼Œä¹Ÿèƒ½æ¨å¹¿åˆ°å¤šç›®æ ‡è·Ÿè¸ªã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå°†æˆ‘ä»¬çš„è§†é¢‘æ£€æµ‹å™¨ä¸åŸºçº¿Pix2Seqé™æ€æ£€æµ‹å™¨è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶è¡¨ç°å‡ºäº†æŒç»­çš„æ”¹è¿›ï¼Œå°½ç®¡å—åˆ°æˆ‘ä»¬æœ‰é™è®¡ç®—èµ„æºçš„é™åˆ¶ã€‚æˆ‘ä»¬è¿˜ä¸UA-DETRACä¸Šçš„å…¶ä»–è§†é¢‘æ£€æµ‹å™¨è¿›è¡Œäº†æ¯”è¾ƒï¼Œä»¥è¯æ˜å³ä½¿åœ¨è®¡ç®—ç“¶é¢ˆçš„æƒ…å†µä¸‹ï¼Œå®ƒä¹Ÿå…·æœ‰ä¸æœ€æ–°æŠ€æœ¯ç«äº‰çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22562v1">PDF</a> Under review for publication in IEEE Access</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ”¹è¿›äº†Pix2Seqç›®æ ‡æ£€æµ‹å™¨ï¼Œå°†å…¶æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸã€‚é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„ç«¯åˆ°ç«¯è§†é¢‘ç›®æ ‡æ£€æµ‹æ–¹å¼ï¼Œå®ƒåœ¨ä¸¤ä¸ªå…³é”®æ–¹é¢è¶…è¶Šäº†ç°æœ‰è§†é¢‘æ£€æµ‹å™¨ã€‚é¦–å…ˆï¼Œé€šè¿‡å°†ç›®æ ‡è¡¨ç¤ºä¸ºç¦»æ•£ä»¤ç‰Œçš„å¯å˜é•¿åº¦åºåˆ—ï¼Œå¯ä»¥ç®€æ´åœ°è¡¨ç¤ºæ•°é‡ä¼—å¤šã€å½¢çŠ¶å’Œä½ç½®å„å¼‚çš„ç›®æ ‡ï¼Œè€Œæ— éœ€åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ³¨å…¥ä»»ä½•å®šä½çº¿ç´¢ã€‚è¿™è§£å†³äº†ä¼ ç»Ÿæ£€æµ‹å™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±ç¨€ç–é—®é¢˜å’Œæ¨ç†è¿‡ç¨‹ä¸­çš„åŸºäºå¯å‘å¼è§„åˆ™çš„åå¤„ç†ã€‚å…¶æ¬¡ï¼Œå®ƒå°†è§†é¢‘ç›®æ ‡æ¦‚å¿µåŒ–ä¸ºå®Œå…¨é›†æˆå’Œä¸å¯åˆ†å‰²çš„3Dæ¡†æˆ–è½¨è¿¹ï¼Œè€Œä¸æ˜¯ç”Ÿæˆé’ˆå¯¹å›¾åƒç‰¹å®šçš„2Dæ¡†å¹¶è¿æ¥è¿™äº›æ¡†æ¥æ„å»ºè§†é¢‘ç›®æ ‡ï¼ˆè¿™æ˜¯å¤§å¤šæ•°ä¼ ç»Ÿæ£€æµ‹å™¨æ‰€åšçš„ï¼‰ã€‚è¿™ä½¿å¾—å®ƒèƒ½å¤Ÿè½»æ¾æ‰©å±•å¯ç”¨çš„è®¡ç®—èµ„æºï¼Œé€šè¿‡ç®€å•åœ°å¢åŠ ç½‘ç»œä½œä¸ºè¾“å…¥çš„è§†é¢‘å­åºåˆ—çš„é•¿åº¦æ¥è¿›è¡Œç¼©æ”¾ï¼Œç”šè‡³åœ¨å­åºåˆ—èƒ½å¤Ÿè·¨è¶Šæ•´ä¸ªè§†é¢‘æ—¶æ¨å¹¿åˆ°å¤šç›®æ ‡è·Ÿè¸ªã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå°†æˆ‘ä»¬çš„è§†é¢‘æ£€æµ‹å™¨ä¸åŸºçº¿Pix2Seqé™æ€æ£€æµ‹å™¨è¿›è¡Œæ¯”è¾ƒï¼Œè¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æ”¹è¿›ï¼Œå°½ç®¡å—åˆ°äº†æœ‰é™è®¡ç®—èµ„æºçš„é™åˆ¶ã€‚åœ¨UA-DETRACä¸Šä¸å‡ ç§è§†é¢‘æ£€æµ‹å™¨çš„æ¯”è¾ƒè¡¨æ˜ï¼Œå³ä½¿åœ¨è®¡ç®—ç“¶é¢ˆçš„æƒ…å†µä¸‹ï¼Œå®ƒä¹Ÿæ˜¯å½“å‰æœ€å…ˆè¿›çš„ã€‚æˆ‘ä»¬å…¬å¼€æä¾›äº†ä»£ç å’Œæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ”¹è¿›äº†Pix2Seqç›®æ ‡æ£€æµ‹å™¨ä»¥åº”ç”¨äºè§†é¢‘é¢†åŸŸã€‚</li>
<li>é€šè¿‡å°†ç›®æ ‡è¡¨ç¤ºä¸ºç¦»æ•£ä»¤ç‰Œçš„å¯å˜é•¿åº¦åºåˆ—ï¼Œè§£å†³äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±ç¨€ç–é—®é¢˜å’Œæ¨ç†è¿‡ç¨‹ä¸­çš„å¯å‘å¼åå¤„ç†éœ€æ±‚ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„è§†é¢‘ç›®æ ‡æ£€æµ‹æ–¹å¼ï¼Œå°†è§†é¢‘ç›®æ ‡æ¦‚å¿µåŒ–ä¸ºå®Œå…¨é›†æˆå’Œä¸å¯åˆ†å‰²çš„3Dæ¡†æˆ–è½¨è¿¹ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿè½»æ¾æ‰©å±•è®¡ç®—èµ„æºï¼Œé€šè¿‡ç®€å•åœ°å¢åŠ ç½‘ç»œä½œä¸ºè¾“å…¥çš„è§†é¢‘å­åºåˆ—çš„é•¿åº¦æ¥è¿›è¡Œç¼©æ”¾ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¸åŸºçº¿Pix2Seqé™æ€æ£€æµ‹å™¨è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¡¨ç°å‡ºæ€§èƒ½æ”¹è¿›ã€‚</li>
<li>åœ¨UA-DETRACä¸Šä¸å½“å‰æœ€å…ˆè¿›çš„è§†é¢‘æ£€æµ‹å™¨å…·æœ‰ç«äº‰åŠ›ï¼Œå³ä½¿å­˜åœ¨è®¡ç®—ç“¶é¢ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38906fc4953e24facdd73e1ab82a4c7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5bd9dee7c9068a4ccd05ad027d1e608.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d268f6ae032aead1536ff94845d0d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-894c68bb5be7817d230d3bb3af609d5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b888f97356a4c3cfb632350415ac2c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Attention-disentangled-Uniform-Orthogonal-Feature-Space-Optimization-for-Few-shot-Object-Detection"><a href="#Attention-disentangled-Uniform-Orthogonal-Feature-Space-Optimization-for-Few-shot-Object-Detection" class="headerlink" title="Attention-disentangled Uniform Orthogonal Feature Space Optimization for   Few-shot Object Detection"></a>Attention-disentangled Uniform Orthogonal Feature Space Optimization for   Few-shot Object Detection</h2><p><strong>Authors:Taijin Zhao, Heqian Qiu, Yu Dai, Lanxiao Wang, Fanman Meng, Qingbo Wu, Hongliang Li</strong></p>
<p>Few-shot object detection (FSOD) aims to detect objects with limited samples for novel classes, while relying on abundant data for base classes. Existing FSOD approaches, predominantly built on the Faster R-CNN detector, entangle objectness recognition and foreground classification within shared feature spaces. This paradigm inherently establishes class-specific objectness criteria and suffers from unrepresentative novel class samples. To resolve this limitation, we propose a Uniform Orthogonal Feature Space (UOFS) optimization framework. First, UOFS decouples the feature space into two orthogonal components, where magnitude encodes objectness and angle encodes classification. This decoupling enables transferring class-agnostic objectness knowledge from base classes to novel classes. Moreover, implementing the disentanglement requires careful attention to two challenges: (1) Base set images contain unlabeled foreground instances, causing confusion between potential novel class instances and backgrounds. (2) Angular optimization depends exclusively on base class foreground instances, inducing overfitting of angular distributions to base classes. To address these challenges, we propose a Hybrid Background Optimization (HBO) strategy: (1) Constructing a pure background base set by removing unlabeled instances in original images to provide unbiased magnitude-based objectness supervision. (2) Incorporating unlabeled foreground instances in the original base set into angular optimization to enhance distribution uniformity. Additionally, we propose a Spatial-wise Attention Disentanglement and Association (SADA) module to address task conflicts between class-agnostic and class-specific tasks. Experiments demonstrate that our method significantly outperforms existing approaches based on entangled feature spaces. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFew-shot Object Detectionï¼ŒFSODï¼‰æ—¨åœ¨é’ˆå¯¹æ–°ç±»åˆ«ä½¿ç”¨æœ‰é™æ ·æœ¬è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼ŒåŒæ—¶ä¾èµ–äºåŸºæœ¬ç±»åˆ«çš„ä¸°å¯Œæ•°æ®ã€‚ç°æœ‰çš„FSODæ–¹æ³•ä¸»è¦åŸºäºFaster R-CNNæ£€æµ‹å™¨ï¼Œåœ¨å…±äº«ç‰¹å¾ç©ºé—´å†…çº ç¼ ç›®æ ‡è¯†åˆ«å’Œå‰æ™¯åˆ†ç±»ã€‚è¿™ç§èŒƒå¼å›ºæœ‰çš„å»ºç«‹äº†ç‰¹å®šç±»åˆ«çš„ç›®æ ‡æ€§æ ‡å‡†ï¼Œå¹¶å—åˆ°æ–°ç±»åˆ«æ ·æœ¬ä»£è¡¨æ€§çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€æ­£äº¤ç‰¹å¾ç©ºé—´ï¼ˆUniform Orthogonal Feature Spaceï¼ŒUOFSï¼‰ä¼˜åŒ–æ¡†æ¶ã€‚é¦–å…ˆï¼ŒUOFSå°†ç‰¹å¾ç©ºé—´è§£è€¦ä¸ºä¸¤ä¸ªæ­£äº¤ç»„ä»¶ï¼Œå…¶ä¸­å¹…åº¦ç¼–ç ç›®æ ‡æ€§ï¼Œè§’åº¦ç¼–ç åˆ†ç±»ã€‚è¿™ç§è§£è€¦ä½¿å¾—ä»åŸºæœ¬ç±»åˆ«è½¬ç§»åˆ°æ–°ç±»åˆ«çš„ç±»åˆ«æ— å…³çš„ç›®æ ‡æ€§çŸ¥è¯†æˆä¸ºå¯èƒ½ã€‚æ­¤å¤–ï¼Œå®ç°è§£è€¦éœ€è¦æ³¨æ„ä¸¤ä¸ªæŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰åŸºç¡€é›†å›¾åƒåŒ…å«æœªæ ‡è®°çš„å‰æ™¯å®ä¾‹ï¼Œå¯¼è‡´æ½œåœ¨çš„æ–°ç±»åˆ«å®ä¾‹å’ŒèƒŒæ™¯ä¹‹é—´çš„æ··æ·†ã€‚ï¼ˆ2ï¼‰è§’åº¦ä¼˜åŒ–å®Œå…¨ä¾èµ–äºåŸºæœ¬ç±»åˆ«çš„å‰æ™¯å®ä¾‹ï¼Œå¯¼è‡´è§’åº¦åˆ†å¸ƒå¯¹åŸºæœ¬ç±»åˆ«äº§ç”Ÿè¿‡åº¦æ‹Ÿåˆã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ··åˆèƒŒæ™¯ä¼˜åŒ–ï¼ˆHybrid Background Optimizationï¼ŒHBOï¼‰ç­–ç•¥ï¼šï¼ˆ1ï¼‰é€šè¿‡å»é™¤åŸå§‹å›¾åƒä¸­çš„æœªæ ‡è®°å®ä¾‹æ¥æ„å»ºçº¯å‡€çš„èƒŒæ™¯åŸºç¡€é›†ï¼Œä»¥æä¾›æ— åçš„åŸºäºå¹…åº¦çš„ç›®æ ‡æ€§ç›‘ç£ã€‚ï¼ˆ2ï¼‰å°†åŸå§‹åŸºç¡€é›†ä¸­çš„æœªæ ‡è®°å‰æ™¯å®ä¾‹çº³å…¥è§’åº¦ä¼˜åŒ–ï¼Œä»¥å¢å¼ºåˆ†å¸ƒå‡åŒ€æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç©ºé—´å…³æ³¨è§£è€¦ä¸å…³è”ï¼ˆSpatial-wise Attention Disentanglement and Associationï¼ŒSADAï¼‰æ¨¡å—ï¼Œä»¥è§£å†³ç±»åˆ«æ— å…³å’Œç‰¹å®šç±»åˆ«ä»»åŠ¡ä¹‹é—´çš„ä»»åŠ¡å†²çªã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºäºçº ç¼ ç‰¹å¾ç©ºé—´çš„æ–¹æ³•ä¸Šå®ç°äº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22161v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨æœ‰é™çš„æ ·æœ¬ä¸‹å¯¹æ–°å‹ç±»åˆ«è¿›è¡Œç›®æ ‡æ£€æµ‹æ˜¯å°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFSODï¼‰çš„ä»»åŠ¡ã€‚ç°æœ‰çš„FSODæ–¹æ³•ä¸»è¦åŸºäºFaster R-CNNæ£€æµ‹å™¨ï¼Œåœ¨å…±äº«ç‰¹å¾ç©ºé—´ä¸­çº ç¼ ç›®æ ‡è¯†åˆ«å’Œå‰æ™¯åˆ†ç±»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ­£äº¤ç‰¹å¾ç©ºé—´ï¼ˆUOFSï¼‰çš„ä¼˜åŒ–æ¡†æ¶ï¼Œå°†ç‰¹å¾ç©ºé—´è§£è€¦ä¸ºä¸¤ä¸ªæ­£äº¤ç»„ä»¶ï¼šå¹…åº¦ç¼–ç ç›®æ ‡æ€§å’Œè§’åº¦ç¼–ç åˆ†ç±»ã€‚è§£å†³äº†ç±»ç‰¹å®šç›®æ ‡æ€§æ ‡å‡†çš„å»ºç«‹å’Œä¸ä»£è¡¨æ€§æ–°å‹ç±»åˆ«æ ·æœ¬çš„é—®é¢˜ã€‚é€šè¿‡æ··åˆèƒŒæ™¯ä¼˜åŒ–ï¼ˆHBOï¼‰ç­–ç•¥è§£å†³äº†è§£è€¦å®ç°çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ„å»ºçº¯èƒŒæ™¯åŸºç¡€é›†å’Œå¢å¼ºåˆ†å¸ƒå‡åŒ€æ€§ã€‚åŒæ—¶ï¼Œæå‡ºäº†ç©ºé—´å…³æ³¨è§£çº ç¼ å’Œå…³è”ï¼ˆSADAï¼‰æ¨¡å—æ¥è§£å†³ç±»é€šç”¨å’Œç±»ç‰¹å®šä»»åŠ¡ä¹‹é—´çš„å†²çªã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºäºçº ç¼ ç‰¹å¾ç©ºé—´çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FSODæ—¨åœ¨ä½¿ç”¨æœ‰é™æ ·æœ¬å¯¹æ–°å‹ç±»åˆ«è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚</li>
<li>ç°æœ‰FSODæ–¹æ³•ä¸»è¦åŸºäºFaster R-CNNæ£€æµ‹å™¨ï¼Œåœ¨å…±äº«ç‰¹å¾ç©ºé—´ä¸­å¤„ç†ç›®æ ‡è¯†åˆ«å’Œå‰æ™¯åˆ†ç±»ï¼Œå­˜åœ¨ç±»ç‰¹å®šç›®æ ‡æ€§æ ‡å‡†å’Œæ ·æœ¬ä¸ä»£è¡¨æ€§æŒ‘æˆ˜ã€‚</li>
<li>UOFSæ¡†æ¶å°†ç‰¹å¾ç©ºé—´è§£è€¦ä¸ºä¸¤ä¸ªæ­£äº¤ç»„ä»¶ï¼šå¹…åº¦ç”¨äºç¼–ç ç›®æ ‡æ€§ï¼Œè§’åº¦ç”¨äºç¼–ç åˆ†ç±»ã€‚</li>
<li>HBOç­–ç•¥è§£å†³äº†ç‰¹å¾è§£è€¦çš„ä¸¤ä¸ªæŒ‘æˆ˜ï¼šæ„å»ºçº¯èƒŒæ™¯åŸºç¡€é›†ä»¥æä¾›æ— åçš„ç›®æ ‡æ€§ç›‘ç£ï¼Œå¹¶å¢å¼ºåˆ†å¸ƒå‡åŒ€æ€§ã€‚</li>
<li>SADAæ¨¡å—è§£å†³äº†ç±»é€šç”¨å’Œç±»ç‰¹å®šä»»åŠ¡ä¹‹é—´çš„å†²çªã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºäºçº ç¼ ç‰¹å¾ç©ºé—´çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6bd41f3294b9b936b3bd8d2eac0c1a59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5b7e76458b7ffc01893fc74bb04609c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6bab1e334b17698808aa02e48e94dd5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Object-detection-in-adverse-weather-conditions-for-autonomous-vehicles-using-Instruct-Pix2Pix"><a href="#Object-detection-in-adverse-weather-conditions-for-autonomous-vehicles-using-Instruct-Pix2Pix" class="headerlink" title="Object detection in adverse weather conditions for autonomous vehicles   using Instruct Pix2Pix"></a>Object detection in adverse weather conditions for autonomous vehicles   using Instruct Pix2Pix</h2><p><strong>Authors:Unai Gurbindo, Axel Brando, Jaume Abella, Caroline KÃ¶nig</strong></p>
<p>Enhancing the robustness of object detection systems under adverse weather conditions is crucial for the advancement of autonomous driving technology. This study presents a novel approach leveraging the diffusion model Instruct Pix2Pix to develop prompting methodologies that generate realistic datasets with weather-based augmentations aiming to mitigate the impact of adverse weather on the perception capabilities of state-of-the-art object detection models, including Faster R-CNN and YOLOv10. Experiments were conducted in two environments, in the CARLA simulator where an initial evaluation of the proposed data augmentation was provided, and then on the real-world image data sets BDD100K and ACDC demonstrating the effectiveness of the approach in real environments.   The key contributions of this work are twofold: (1) identifying and quantifying the performance gap in object detection models under challenging weather conditions, and (2) demonstrating how tailored data augmentation strategies can significantly enhance the robustness of these models. This research establishes a solid foundation for improving the reliability of perception systems in demanding environmental scenarios, and provides a pathway for future advancements in autonomous driving. </p>
<blockquote>
<p>æé«˜æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹ç‰©ä½“æ£€æµ‹ç³»ç»Ÿçš„ç¨³å¥æ€§å¯¹äºè‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å‘å±•è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹Instruct Pix2Pixå¼€å‘æç¤ºæ–¹æ³•çš„æ–°æ–¹æ³•ï¼Œç”Ÿæˆå…·æœ‰å¤©æ°”å¢å¼ºçš„ç°å®æ•°æ®é›†ï¼Œæ—¨åœ¨å‡è½»æ¶åŠ£å¤©æ°”å¯¹æœ€æ–°ç‰©ä½“æ£€æµ‹æ¨¡å‹æ„ŸçŸ¥èƒ½åŠ›çš„å½±å“ï¼ŒåŒ…æ‹¬Faster R-CNNå’ŒYOLOv10ã€‚å®éªŒåœ¨ä¸¤ä¸ªç¯å¢ƒä¸­è¿›è¡Œï¼Œåœ¨CARLAæ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œæ‰€æå‡ºçš„æ•°æ®å¢å¼ºçš„åˆæ­¥è¯„ä¼°ï¼Œç„¶ååœ¨çœŸå®ä¸–ç•Œå›¾åƒæ•°æ®é›†BDD100Kå’ŒACDCä¸Šå±•ç¤ºè¯¥æ–¹æ³•åœ¨çœŸå®ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œçš„å…³é”®è´¡çŒ®æœ‰ä¸¤ç‚¹ï¼šï¼ˆ1ï¼‰è¯†åˆ«å’Œé‡åŒ–ç‰©ä½“æ£€æµ‹æ¨¡å‹åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„æ€§èƒ½å·®è·ï¼›ï¼ˆ2ï¼‰å±•ç¤ºæœ‰é’ˆå¯¹æ€§çš„æ•°æ®å¢å¼ºç­–ç•¥å¦‚ä½•æ˜¾è‘—å¢å¼ºè¿™äº›æ¨¡å‹çš„ç¨³å¥æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæé«˜éœ€æ±‚ç¯å¢ƒåœºæ™¯ä¸­æ„ŸçŸ¥ç³»ç»Ÿçš„å¯é æ€§å¥ å®šäº†åšå®çš„åŸºç¡€ï¼Œå¹¶ä¸ºè‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„æœªæ¥å‘å±•æä¾›äº†é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08228v2">PDF</a> 8 pages, 5 figures. Accepted at the International Joint Conference on   Neural Networks (IJCNN) 2025 (to appear)</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹Instruct Pix2Pixå‘å±•å‡ºæ–°çš„æç¤ºæ–¹æ³•ï¼Œç”Ÿæˆå…·æœ‰å¤©æ°”å¢å¼ºçš„ç°å®æ•°æ®é›†ï¼Œæ—¨åœ¨å‡è½»æ¶åŠ£å¤©æ°”å¯¹æœ€å…ˆè¿›ç‰©ä½“æ£€æµ‹æ¨¡å‹æ„ŸçŸ¥èƒ½åŠ›çš„å½±å“ï¼ŒåŒ…æ‹¬Faster R-CNNå’ŒYOLOv10ã€‚ç ”ç©¶åœ¨CARLAæ¨¡æ‹Ÿå™¨ä¸BDD100KåŠACDCçœŸå®å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶å…³é”®è´¡çŒ®åœ¨äºè¯†åˆ«å¹¶é‡åŒ–ç‰©ä½“æ£€æµ‹æ¨¡å‹åœ¨æ¶åŠ£æ¡ä»¶ä¸‹çš„æ€§èƒ½å·®è·ï¼Œå¹¶å±•ç¤ºé‡èº«å®šåˆ¶çš„æ•°æ®å¢å¼ºç­–ç•¥å¦‚ä½•æ˜¾è‘—æå‡æ¨¡å‹çš„ç¨³å¥æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæ”¹è¿›éœ€æ±‚ç¯å¢ƒä¸‹æ„ŸçŸ¥ç³»ç»Ÿçš„å¯é æ€§å¥ å®šåšå®åŸºç¡€ï¼Œå¹¶ä¸ºè‡ªåŠ¨é©¾é©¶çš„æœªæ¥å‘å±•é“ºå¹³é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨Instruct Pix2Pixæ‰©æ•£æ¨¡å‹ï¼Œæå‡ºä¸€ç§æ–°é¢–çš„æ–¹æ³•ç”Ÿæˆå¸¦æœ‰å¤©æ°”å¢å¼ºçš„ç°å®æ•°æ®é›†ã€‚</li>
<li>è¯¥æ–¹æ³•æ—¨åœ¨å‡è½»æ¶åŠ£å¤©æ°”å¯¹ç‰©ä½“æ£€æµ‹æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›çš„å½±å“ã€‚</li>
<li>å®éªŒåœ¨CARLAæ¨¡æ‹Ÿå™¨ä¸Šè¿›è¡Œåˆæ­¥è¯„ä¼°ï¼Œå¹¶åœ¨BDD100KåŠACDCçœŸå®å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç ”ç©¶è¯†åˆ«å¹¶é‡åŒ–ç‰©ä½“æ£€æµ‹æ¨¡å‹åœ¨æ¶åŠ£æ¡ä»¶ä¸‹çš„æ€§èƒ½å·®è·ã€‚</li>
<li>å±•ç¤ºé‡èº«å®šåˆ¶çš„æ•°æ®å¢å¼ºç­–ç•¥èƒ½æ˜¾è‘—æå‡ç‰©ä½“æ£€æµ‹æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºæ”¹è¿›æ„ŸçŸ¥ç³»ç»Ÿåœ¨éœ€æ±‚ç¯å¢ƒä¸‹çš„å¯é æ€§å¥ å®šåšå®åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-739a0b831b73dcbddc21bee42be3920f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-363d6e00b239a37beb20eff28e2ae720.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f6bd6eea63376b53ec2482b74727555.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3fc3cd889849877f499467f17eaeb420.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba6e4714986ed45478cc2cbdf6cef4a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cebb960c3997ce9f322cacae2d410dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5335cdcd125ad772b7155427a903557.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Accelerate-3D-Object-Detection-Models-via-Zero-Shot-Attention-Key-Pruning"><a href="#Accelerate-3D-Object-Detection-Models-via-Zero-Shot-Attention-Key-Pruning" class="headerlink" title="Accelerate 3D Object Detection Models via Zero-Shot Attention Key   Pruning"></a>Accelerate 3D Object Detection Models via Zero-Shot Attention Key   Pruning</h2><p><strong>Authors:Lizhen Xu, Xiuxiu Bai, Xiaojun Jia, Jianwu Fang, Shanmin Pang</strong></p>
<p>Query-based methods with dense features have demonstrated remarkable success in 3D object detection tasks. However, the computational demands of these models, particularly with large image sizes and multiple transformer layers, pose significant challenges for efficient running on edge devices. Existing pruning and distillation methods either need retraining or are designed for ViT models, which are hard to migrate to 3D detectors. To address this issue, we propose a zero-shot runtime pruning method for transformer decoders in 3D object detection models. The method, termed tgGBC (trim keys gradually Guided By Classification scores), systematically trims keys in transformer modules based on their importance. We expand the classification score to multiply it with the attention map to get the importance score of each key and then prune certain keys after each transformer layer according to their importance scores. Our method achieves a 1.99x speedup in the transformer decoder of the latest ToC3D model, with only a minimal performance loss of less than 1%. Interestingly, for certain models, our method even enhances their performance. Moreover, we deploy 3D detectors with tgGBC on an edge device, further validating the effectiveness of our method. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/iseri27/tg_gbc">https://github.com/iseri27/tg_gbc</a>. </p>
<blockquote>
<p>åŸºäºæŸ¥è¯¢çš„æ–¹æ³•å’Œå¯†é›†ç‰¹å¾åœ¨3Dç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¯¹è®¡ç®—çš„éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å›¾åƒå°ºå¯¸å’Œå¤šä¸ªè½¬æ¢å™¨å±‚çš„æƒ…å†µä¸‹ï¼Œå¯¹äºåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œé«˜æ•ˆè¿è¡Œæ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„ä¿®å‰ªå’Œè’¸é¦æ–¹æ³•éƒ½éœ€è¦é‡æ–°è®­ç»ƒï¼Œæˆ–è€…é’ˆå¯¹ViTæ¨¡å‹è®¾è®¡ï¼Œå¾ˆéš¾è¿ç§»åˆ°3Dæ£€æµ‹å™¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äº3Dç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸­å˜å‹å™¨è§£ç å™¨çš„é›¶å°„å‡»è¿è¡Œæ—¶ä¿®å‰ªæ–¹æ³•ã€‚è¯¥æ–¹æ³•è¢«ç§°ä¸ºtgGBCï¼ˆé€šè¿‡åˆ†ç±»åˆ†æ•°é€æ­¥å¼•å¯¼ä¿®å‰ªå…³é”®ï¼‰ï¼Œå®ƒæ ¹æ®å…³é”®çš„é‡è¦æ€§ç³»ç»Ÿåœ°ä¿®å‰ªå˜å‹å™¨æ¨¡å—ä¸­çš„å…³é”®ã€‚æˆ‘ä»¬å°†åˆ†ç±»åˆ†æ•°æ‰©å±•åˆ°ä¸æ³¨æ„åŠ›å›¾ç›¸ä¹˜ï¼Œä»¥å¾—åˆ°æ¯ä¸ªå…³é”®çš„é‡è¦æ€§åˆ†æ•°ï¼Œç„¶åæ ¹æ®å…¶é‡è¦æ€§åˆ†æ•°åœ¨æ¯ä¸ªè½¬æ¢å™¨å±‚ä¹‹åä¿®å‰ªæŸäº›å…³é”®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ€æ–°çš„ToC3Dæ¨¡å‹çš„å˜å‹å™¨è§£ç å™¨ä¸­å®ç°äº†1.99å€çš„é€Ÿåº¦æå‡ï¼Œæ€§èƒ½æŸå¤±ä»…ä½äº1%ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå¯¹äºæŸäº›æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”šè‡³æé«˜äº†å…¶æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²äº†å¸¦æœ‰tgGBCçš„3Dæ£€æµ‹å™¨ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/iseri27/tg_gbc%E3%80%82">https://github.com/iseri27/tg_gbcã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08101v3">PDF</a> Accepted by ICCV2025. The code can be found at   <a target="_blank" rel="noopener" href="https://github.com/iseri27/tg_gbc">https://github.com/iseri27/tg_gbc</a></p>
<p><strong>Summary</strong><br>åŸºäºæŸ¥è¯¢æ–¹æ³•å’Œå¯†é›†ç‰¹å¾çš„3Då¯¹è±¡æ£€æµ‹ä»»åŠ¡å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†è®¡ç®—éœ€æ±‚è¾ƒé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§å›¾åƒå’Œå¤šå±‚Transformeræ—¶ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºtgGBCçš„é›¶è¿è¡Œæ—¶ä¿®å‰ªæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºåˆ†ç±»åˆ†æ•°è®¡ç®—å…³é”®é‡è¦æ€§ï¼Œå¹¶é€å±‚ä¿®å‰ªTransformeræ¨¡å—ä¸­çš„å…³é”®ã€‚tgGBCåœ¨æœ€æ–°ToC3Dæ¨¡å‹çš„Transformerè§£ç å™¨ä¸­å®ç°äº†1.99å€åŠ é€Ÿï¼Œæ€§èƒ½æŸå¤±æå°ï¼ˆä¸åˆ°1%ï¼‰ï¼Œå¹¶åœ¨æŸäº›æ¨¡å‹ä¸­ç”šè‡³æé«˜äº†æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²äº†å¸¦æœ‰tgGBCçš„3Dæ£€æµ‹å™¨ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŸ¥è¯¢æ–¹æ³•å’Œå¯†é›†ç‰¹å¾åœ¨3Då¯¹è±¡æ£€æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è®¡ç®—éœ€æ±‚é«˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å›¾åƒå’Œå¤šå±‚Transformerã€‚</li>
<li>æå‡ºä¸€ç§é›¶è¿è¡Œæ—¶ä¿®å‰ªæ–¹æ³•â€”â€”tgGBCï¼Œé’ˆå¯¹Transformerè§£ç å™¨è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>tgGBCåŸºäºåˆ†ç±»åˆ†æ•°è®¡ç®—å…³é”®é‡è¦æ€§å¹¶è¿›è¡Œä¿®å‰ªã€‚</li>
<li>åœ¨æœ€æ–°ToC3Dæ¨¡å‹çš„Transformerè§£ç å™¨ä¸­å®ç°äº†1.99å€åŠ é€Ÿã€‚</li>
<li>æ€§èƒ½æŸå¤±æå°ï¼ˆä¸åˆ°1%ï¼‰ï¼Œåœ¨æŸäº›æ¨¡å‹ä¸­ç”šè‡³æé«˜äº†æ€§èƒ½ã€‚</li>
<li>åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²äº†å¸¦æœ‰tgGBCçš„3Dæ£€æµ‹å™¨ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d60d3b5f82d1877b51327c97d6a6d361.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fcd960e8fe7aab4b760823fb766eb29e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6e8d1be338c3e06befe5e87361353df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff785287fc071e6f8215097c93de42d9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Mitigating-Hallucinations-in-YOLO-based-Object-Detection-Models-A-Revisit-to-Out-of-Distribution-Detection"><a href="#Mitigating-Hallucinations-in-YOLO-based-Object-Detection-Models-A-Revisit-to-Out-of-Distribution-Detection" class="headerlink" title="Mitigating Hallucinations in YOLO-based Object Detection Models: A   Revisit to Out-of-Distribution Detection"></a>Mitigating Hallucinations in YOLO-based Object Detection Models: A   Revisit to Out-of-Distribution Detection</h2><p><strong>Authors:Weicheng He, Changshun Wu, Chih-Hong Cheng, Xiaowei Huang, Saddek Bensalem</strong></p>
<p>Object detection systems must reliably perceive objects of interest without being overly confident to ensure safe decision-making in dynamic environments. Filtering techniques based on out-of-distribution (OoD) detection are commonly added as an extra safeguard to filter hallucinations caused by overconfidence in novel objects. Nevertheless, evaluating YOLO-family detectors and their filters under existing OoD benchmarks often leads to unsatisfactory performance. This paper studies the underlying reasons for performance bottlenecks and proposes a methodology to improve performance fundamentally. Our first contribution is a calibration of all existing evaluation results: Although images in existing OoD benchmark datasets are claimed not to have objects within in-distribution (ID) classes (i.e., categories defined in the training dataset), around 13% of objects detected by the object detector are actually ID objects. Dually, the ID dataset containing OoD objects can also negatively impact the decision boundary of filters. These ultimately lead to a significantly imprecise performance estimation. Our second contribution is to consider the task of hallucination reduction as a joint pipeline of detectors and filters. By developing a methodology to carefully synthesize an OoD dataset that semantically resembles the objects to be detected, and using the crafted OoD dataset in the fine-tuning of YOLO detectors to suppress the objectness score, we achieve a 88% reduction in overall hallucination error with a combined fine-tuned detection and filtering system on the self-driving benchmark BDD-100K. Our code and dataset are available at: <a target="_blank" rel="noopener" href="https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood">https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood</a>. </p>
<blockquote>
<p>å¯¹è±¡æ£€æµ‹ç³»ç»Ÿå¿…é¡»å¯é åœ°æ„ŸçŸ¥åŠ¨æ€ç¯å¢ƒä¸­çš„æ„Ÿå…´è¶£å¯¹è±¡ï¼ŒåŒæ—¶é¿å…è¿‡äºè‡ªä¿¡ï¼Œä»¥ç¡®ä¿åšå‡ºå®‰å…¨å†³ç­–ã€‚åŸºäºç¦»ç¾¤åˆ†å¸ƒï¼ˆOut-of-Distributionï¼ŒOoDï¼‰æ£€æµ‹çš„è¿‡æ»¤æŠ€æœ¯é€šå¸¸è¢«ç”¨ä½œé¢å¤–çš„å®‰å…¨ä¿æŠ¤æªæ–½ï¼Œä»¥è¿‡æ»¤ç”±äºè¿‡äºè‡ªä¿¡è€Œäº§ç”Ÿçš„æ–°å‹å¯¹è±¡çš„å¹»è§‰ã€‚ç„¶è€Œï¼Œåœ¨ç°æœ‰çš„OoDåŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°YOLOç³»åˆ—æ£€æµ‹å™¨åŠå…¶è¿‡æ»¤å™¨é€šå¸¸ä¼šå¾—åˆ°ä»¤äººä¸æ»¡æ„çš„è¡¨ç°ã€‚æœ¬æ–‡ç ”ç©¶äº†æ€§èƒ½ç“¶é¢ˆçš„åº•å±‚åŸå› ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä»æ ¹æœ¬ä¸Šæé«˜æ€§èƒ½çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªè´¡çŒ®æ˜¯æ ¡å‡†äº†æ‰€æœ‰ç°æœ‰çš„è¯„ä¼°ç»“æœï¼šå°½ç®¡ç°æœ‰OoDåŸºå‡†æ•°æ®é›†å£°ç§°åœ¨å†…éƒ¨åˆ†å¸ƒï¼ˆIDï¼‰ç±»ï¼ˆå³è®­ç»ƒæ•°æ®é›†ä¸­å®šä¹‰çš„ç±»åˆ«ï¼‰ä¸­æ²¡æœ‰å¯¹è±¡ï¼Œä½†å¯¹è±¡æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å¯¹è±¡ä¸­å¤§çº¦æœ‰13%å®é™…ä¸Šæ˜¯IDå¯¹è±¡ã€‚åŒæ ·ï¼ŒåŒ…å«OoDå¯¹è±¡çš„IDæ•°æ®é›†ä¹Ÿå¯èƒ½å¯¹è¿‡æ»¤å™¨çš„å†³ç­–è¾¹ç•Œäº§ç”Ÿè´Ÿé¢å½±å“ã€‚è¿™äº›æœ€ç»ˆå¯¼è‡´äº†æ€§èƒ½ä¼°è®¡çš„ä¸¥é‡ä¸å‡†ç¡®ã€‚æˆ‘ä»¬çš„ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯å°†å¹»è§‰å‡å°‘ä»»åŠ¡è§†ä¸ºæ£€æµ‹å™¨å’Œè¿‡æ»¤å™¨çš„è”åˆç®¡é“ã€‚é€šè¿‡å¼€å‘ä¸€ç§ä»”ç»†åˆæˆåœ¨è¯­ä¹‰ä¸Šç±»ä¼¼äºå¾…æ£€æµ‹å¯¹è±¡çš„OoDæ•°æ®é›†çš„æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ç²¾ç»†è°ƒæ•´çš„YOLOæ£€æµ‹å™¨æŠ‘åˆ¶å¯¹è±¡åˆ†æ•°æ¥ç²¾å¿ƒåˆ¶ä½œçš„OoDæ•°æ®é›†ï¼Œæˆ‘ä»¬åœ¨è‡ªåŠ¨é©¾é©¶åŸºå‡†BDD-100Kä¸Šå®ç°äº†æ•´ä½“å¹»è§‰è¯¯å·®é™ä½äº†88%ï¼Œè¿™æ˜¯é€šè¿‡ç»“åˆç²¾ç»†è°ƒæ•´çš„æ£€æµ‹å’Œè¿‡æ»¤ç³»ç»Ÿå®ç°çš„ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood%E3%80%82">https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hoodã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07330v2">PDF</a> Camera-ready version for IROS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¯¹è±¡æ£€æµ‹ç³»ç»Ÿéœ€åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯é è¯†åˆ«ç›®æ ‡å¯¹è±¡ï¼ŒåŒæ—¶é¿å…è¿‡åº¦è‡ªä¿¡å¯¼è‡´çš„å¹»è§‰ã€‚æœ¬æ–‡ç ”ç©¶äº†åŸºäºå¼‚å¸¸å€¼æ£€æµ‹æŠ€æœ¯çš„æ»¤æ³¢æŠ€æœ¯åœ¨YOLOç³»åˆ—æ£€æµ‹å™¨ä¸­çš„åº”ç”¨ç“¶é¢ˆï¼Œå¹¶æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆã€‚ç ”ç©¶å‘ç°ç°æœ‰å¼‚å¸¸å€¼æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å­˜åœ¨é—®é¢˜ï¼Œæ£€æµ‹å‡ºçš„IDå¯¹è±¡ä¸­çº¦å­˜åœ¨é«˜è¾¾ç™¾åˆ†ä¹‹åä¸‰çš„éIDå¯¹è±¡ã€‚é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†æ–°çš„è¯„ä»·æ ‡å‡†å’Œåˆæˆå¼‚å¸¸å€¼æ•°æ®é›†çš„æ–¹æ³•è®ºï¼Œé€šè¿‡å¾®è°ƒYOLOæ£€æµ‹å™¨æŠ‘åˆ¶å¯¹è±¡å¾—åˆ†ï¼Œå®ç°äº†åœ¨BDD-100Kè‡ªåŠ¨é©¾é©¶åŸºå‡†æµ‹è¯•ä¸­å¹»è§‰è¯¯å·®å‡å°‘ç™¾åˆ†ä¹‹å…«åå…«ã€‚ç ”ç©¶ä»£ç åŠæ•°æ®é›†å·²å…¬å¼€åˆ†äº«ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹è±¡æ£€æµ‹ç³»ç»Ÿåº”é¿å…è¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ä»¥ç¡®ä¿å†³ç­–å®‰å…¨æ€§ã€‚åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­ä½¿ç”¨æ€§èƒ½è¯„ä¼°å’Œå¢å¼ºæ»¤æ³¢å™¨æ˜¯è‡³å…³é‡è¦çš„ã€‚åœ¨ç°å­˜åœ¨çš„ç³»ç»Ÿå¯é æ€§ä¸­æé«˜å’Œæ”¹è¿›ç¨³å®šæ€§æ›´ä¸ºå…³é”®ã€‚å·²æœ‰é‡‡ç”¨ä»¥è¯„ä¼°å‡½æ•°çš„æ–°å¼€å‘æ–¹å¼å¯¹æ—¢æˆä¼ æ„Ÿå™¨è®¾è®¡å’Œè¦æ±‚è®¤è¯†ç²¾å‡†å’Œç†è§£æ¥è¿›è¡Œæ£€æŸ¥èŒƒä¾‹å¼€å‘å’Œè®¾å¤‡åˆå§‹åŒ–æ ¡æ­£æ‰‹æ®µç­‰æ–¹å¼æ¥ä¿è¯å’Œæé«˜æµ‹è¯•ç²¾å‡†åº¦çš„é—®é¢˜ä¸ä¿éšœï¼›åŸºäºæ­¤èƒŒæ™¯ä¸‹å®æ–½å…ˆè¿›è§£å†³æ–¹æ¡ˆå…·æœ‰é‡è¦çš„å®ç”¨ä»·å€¼å’Œå‘å±•å‰æ™¯å¹¿é˜”çš„ç ”ç©¶é¢†åŸŸå‰æ™¯å±•æœ›é‡è¦æ€§æ˜¾è€Œæ˜“è§ï¼›æ–°æµ‹è¯•ç²¾å‡†åº¦å°†å…·æœ‰æ˜¾è‘—çš„æ¨å¹¿ä»·å€¼å’Œäº§ä¸šæ½œåŠ›åŠç°å®æ„ä¹‰ï¼Œè¯¥ç ”ç©¶æˆæœçš„åº”ç”¨åœºæ™¯æå…¶å¹¿æ³›ï¼›ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€åˆ†äº«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07330">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-20b8d1c4359d3c138318a1f8dc121c11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-845163e6f2780c7c4ec7fad2f4caf04f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2ebf3e3c0be11be4a90a1bf6dd81b91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5c8dc581fc524fab46abf9199f5a5c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5ada4ed4faf9bfee4f19a1b04bb48b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25e15ac377405eef55cf124120d3f3d7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Semi-supervised-Semantic-Segmentation-for-Remote-Sensing-Images-via-Multi-scale-Uncertainty-Consistency-and-Cross-Teacher-Student-Attention"><a href="#Semi-supervised-Semantic-Segmentation-for-Remote-Sensing-Images-via-Multi-scale-Uncertainty-Consistency-and-Cross-Teacher-Student-Attention" class="headerlink" title="Semi-supervised Semantic Segmentation for Remote Sensing Images via   Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention"></a>Semi-supervised Semantic Segmentation for Remote Sensing Images via   Multi-scale Uncertainty Consistency and Cross-Teacher-Student Attention</h2><p><strong>Authors:Shanwen Wang, Xin Sun, Changrui Chen, Danfeng Hong, Jungong Han</strong></p>
<p>Semi-supervised learning offers an appealing solution for remote sensing (RS) image segmentation to relieve the burden of labor-intensive pixel-level labeling. However, RS images pose unique challenges, including rich multi-scale features and high inter-class similarity. To address these problems, this paper proposes a novel semi-supervised Multi-Scale Uncertainty and Cross-Teacher-Student Attention (MUCA) model for RS image semantic segmentation tasks. Specifically, MUCA constrains the consistency among feature maps at different layers of the network by introducing a multi-scale uncertainty consistency regularization. It improves the multi-scale learning capability of semi-supervised algorithms on unlabeled data. Additionally, MUCA utilizes a Cross-Teacher-Student attention mechanism to guide the student network, guiding the student network to construct more discriminative feature representations through complementary features from the teacher network. This design effectively integrates weak and strong augmentations (WA and SA) to further boost segmentation performance. To verify the effectiveness of our model, we conduct extensive experiments on ISPRS-Potsdam and LoveDA datasets. The experimental results show the superiority of our method over state-of-the-art semi-supervised methods. Notably, our model excels in distinguishing highly similar objects, showcasing its potential for advancing semi-supervised RS image segmentation tasks. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ ä¸ºé¥æ„Ÿï¼ˆRSï¼‰å›¾åƒåˆ†å‰²æä¾›äº†ä¸€ç§å¸å¼•äººçš„è§£å†³æ–¹æ¡ˆï¼Œå‡è½»äº†åŠ³åŠ¨å¯†é›†å‹çš„åƒç´ çº§æ ‡æ³¨çš„è´Ÿæ‹…ã€‚ç„¶è€Œï¼Œé¥æ„Ÿå›¾åƒå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸°å¯Œçš„å¤šå°ºåº¦ç‰¹å¾å’Œé«˜çš„ç±»é—´ç›¸ä¼¼æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŠç›‘ç£å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸äº¤å‰æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›ï¼ˆMUCAï¼‰æ¨¡å‹ï¼Œç”¨äºé¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒMUCAé€šè¿‡å¼•å…¥å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œçº¦æŸç½‘ç»œä¸åŒå±‚ç‰¹å¾å›¾ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®ƒæé«˜äº†åŠç›‘ç£ç®—æ³•åœ¨æœªæ ‡è®°æ•°æ®ä¸Šçš„å¤šå°ºåº¦å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒMUCAåˆ©ç”¨è·¨æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶æ¥å¼•å¯¼å­¦ç”Ÿç½‘ç»œï¼Œé€šè¿‡æ•™å¸ˆç½‘ç»œçš„äº’è¡¥ç‰¹å¾æ„å»ºæ›´å…·åˆ¤åˆ«åŠ›çš„ç‰¹å¾è¡¨ç¤ºã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆåœ°ç»“åˆäº†å¼±å¢å¼ºå’Œå¼ºå¢å¼ºï¼ˆWAå’ŒSAï¼‰ï¼Œè¿›ä¸€æ­¥æé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨ISPRS-Potsdamå’ŒLoveDAæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°çš„åŠç›‘ç£æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åŒºåˆ†é«˜åº¦ç›¸ä¼¼ç‰©ä½“æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨è¿›åŠç›‘ç£é¥æ„Ÿå›¾åƒåˆ†å‰²ä»»åŠ¡æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.10736v3">PDF</a> </p>
<p><strong>Summary</strong><br>åŠç›‘ç£å­¦ä¹ ä¸ºé¥æ„Ÿå›¾åƒåˆ†å‰²æä¾›äº†ä¸€ç§å¸å¼•äººçš„è§£å†³æ–¹æ¡ˆï¼Œå‡è½»äº†åŠ³åŠ¨å¯†é›†å‹çš„åƒç´ çº§æ ‡æ³¨è´Ÿæ‹…ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„åŠç›‘ç£å¤šå°ºåº¦ä¸ç¡®å®šæ€§åŠäº¤å‰æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æ¨¡å‹ï¼ˆMUCAï¼‰ï¼Œè§£å†³é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­ç‰¹æœ‰çš„ä¸°å¯Œå¤šå°ºåº¦ç‰¹å¾å’Œé«˜åº¦ç±»åˆ«ç›¸ä¼¼æ€§æŒ‘æˆ˜ã€‚MUCAé€šè¿‡å¼•å…¥å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œæé«˜åŠç›‘ç£ç®—æ³•åœ¨æœªæ ‡æ³¨æ•°æ®ä¸Šçš„å¤šå°ºåº¦å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒMUCAåˆ©ç”¨äº¤å‰æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶å¼•å¯¼å­¦ç”Ÿç½‘ç»œæ„å»ºæ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ISPRS-Potsdamå’ŒLoveDAæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨åŒºåˆ†é«˜åº¦ç›¸ä¼¼å¯¹è±¡æ–¹é¢æ½œåŠ›å·¨å¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ æ˜¯è§£å†³é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­åŠ³åŠ¨å¯†é›†å‹æ ‡æ³¨è´Ÿæ‹…çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„MUCAæ¨¡å‹è§£å†³äº†é¥æ„Ÿå›¾åƒåˆ†å‰²ç‰¹æœ‰çš„ä¸°å¯Œå¤šå°ºåº¦ç‰¹å¾å’Œé«˜åº¦ç±»åˆ«ç›¸ä¼¼æ€§æŒ‘æˆ˜ã€‚</li>
<li>MUCAé€šè¿‡å¼•å…¥å¤šå°ºåº¦ä¸ç¡®å®šæ€§ä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œæé«˜åŠç›‘ç£ç®—æ³•åœ¨æœªæ ‡æ³¨æ•°æ®ä¸Šçš„å¤šå°ºåº¦å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>MUCAåˆ©ç”¨äº¤å‰æ•™å¸ˆå­¦ç”Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡æ•™å¸ˆç½‘ç»œçš„äº’è¡¥ç‰¹å¾å¼•å¯¼å­¦ç”Ÿç½‘ç»œæ„å»ºæ›´å…·åŒºåˆ†æ€§çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>æ¨¡å‹åœ¨ISPRS-Potsdamå’ŒLoveDAæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMUCAæ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–å…ˆè¿›çš„åŠç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.10736">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac237e5f98558cdf40e112c011a60874.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0c5331caa60f7e8d315d9de686cebf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7c03d48be5bd01f423539faf724de64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc037fe583e31dd05abb88309799578b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34995eae5323ebc1f8e4f519cd1ddb64.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="UAV-DETR-Efficient-End-to-End-Object-Detection-for-Unmanned-Aerial-Vehicle-Imagery"><a href="#UAV-DETR-Efficient-End-to-End-Object-Detection-for-Unmanned-Aerial-Vehicle-Imagery" class="headerlink" title="UAV-DETR: Efficient End-to-End Object Detection for Unmanned Aerial   Vehicle Imagery"></a>UAV-DETR: Efficient End-to-End Object Detection for Unmanned Aerial   Vehicle Imagery</h2><p><strong>Authors:Huaxiang Zhang, Kai Liu, Zhongxue Gan, Guo-Niu Zhu</strong></p>
<p>Unmanned aerial vehicle object detection (UAV-OD) has been widely used in various scenarios. However, most existing UAV-OD algorithms rely on manually designed components, which require extensive tuning. End-to-end models that do not depend on such manually designed components are mainly designed for natural images, which are less effective for UAV imagery. To address such challenges, this paper proposes an efficient detection transformer (DETR) framework tailored for UAV imagery, i.e., UAV-DETR. The framework includes a multi-scale feature fusion with frequency enhancement module, which captures both spatial and frequency information at different scales. In addition, a frequency-focused down-sampling module is presented to retain critical spatial details during down-sampling. A semantic alignment and calibration module is developed to align and fuse features from different fusion paths. Experimental results demonstrate the effectiveness and generalization of our approach across various UAV imagery datasets. On the VisDrone dataset, our method improves AP by 3.1% and $\text{AP}_{50}$ by 4.2% over the baseline. Similar enhancements are observed on the UAVVaste dataset. The project page: <a target="_blank" rel="noopener" href="https://github.com/ValiantDiligent/UAV-DETR">https://github.com/ValiantDiligent/UAV-DETR</a> </p>
<blockquote>
<p>æ— äººæœºç›®æ ‡æ£€æµ‹ï¼ˆUAV-ODï¼‰å·²åœ¨å„ç§åœºæ™¯ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„UAV-ODç®—æ³•ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡çš„ç»„ä»¶ï¼Œéœ€è¦å¤§é‡çš„è°ƒæ•´ã€‚ä¸ä¾èµ–äºæ­¤ç±»æ‰‹åŠ¨è®¾è®¡ç»„ä»¶çš„ç«¯åˆ°ç«¯æ¨¡å‹ä¸»è¦è®¾è®¡ç”¨äºè‡ªç„¶å›¾åƒï¼Œä½†å¯¹æ— äººæœºå›¾åƒçš„æ•ˆæœè¾ƒå·®ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ— äººæœºå›¾åƒçš„çš„é«˜æ•ˆæ£€æµ‹è½¬æ¢å™¨ï¼ˆDETRï¼‰æ¡†æ¶ï¼Œå³UAV-DETRã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å…·æœ‰é¢‘ç‡å¢å¼ºæ¨¡å—çš„å¤šå°ºåº¦ç‰¹å¾èåˆï¼Œèƒ½å¤Ÿæ•è·ä¸åŒå°ºåº¦çš„ç©ºé—´å’Œä¿¡æ¯é¢‘ç‡ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†é¢‘ç‡èšç„¦çš„ä¸‹é‡‡æ ·æ¨¡å—ï¼Œä»¥åœ¨ä¸‹é‡‡æ ·è¿‡ç¨‹ä¸­ä¿ç•™å…³é”®çš„ç©ºé—´ç»†èŠ‚ã€‚å¼€å‘äº†ä¸€ä¸ªè¯­ä¹‰å¯¹é½å’Œæ ¡å‡†æ¨¡å—ï¼Œä»¥å¯¹é½å’Œèåˆæ¥è‡ªä¸åŒèåˆè·¯å¾„çš„ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æ— äººæœºå›¾åƒæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚åœ¨VisDroneæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾ƒåŸºçº¿æé«˜äº†3.1%çš„APå’Œ4.2%çš„AP50ã€‚åœ¨UAVVasteæ•°æ®é›†ä¸Šä¹Ÿè§‚å¯Ÿåˆ°äº†ç±»ä¼¼çš„æ”¹è¿›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/ValiantDiligent/UAV-DETR">https://github.com/ValiantDiligent/UAV-DETR</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01855v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹æ— äººæœºå›¾åƒçš„é«˜æ•ˆæ£€æµ‹å˜å‹å™¨ï¼ˆUAV-DETRï¼‰æ¡†æ¶ï¼Œç”¨äºè§£å†³ç°æœ‰æ— äººæœºç›®æ ‡æ£€æµ‹ç®—æ³•ä¾èµ–æ‰‹åŠ¨è®¾è®¡ç»„ä»¶çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å¤šå°ºåº¦ç‰¹å¾èåˆä¸é¢‘ç‡å¢å¼ºæ¨¡å—ã€é¢‘ç‡èšç„¦ä¸‹é‡‡æ ·æ¨¡å—ä»¥åŠè¯­ä¹‰å¯¹é½ä¸æ ¡å‡†æ¨¡å—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨ä¸åŒæ— äººæœºå›¾åƒæ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•åœ¨VisDroneæ•°æ®é›†ä¸Šçš„APæé«˜äº†3.1%ï¼ŒAP50æé«˜äº†4.2%ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/ValiantDiligent/UAV-DETR%E3%80%82">https://github.com/ValiantDiligent/UAV-DETRã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>UAV-ODç®—æ³•å¹¿æ³›åº”ç”¨åœ¨å„ç§åœºæ™¯ä¸­ï¼Œä½†ä¾èµ–æ‰‹åŠ¨è®¾è®¡ç»„ä»¶çš„ç®—æ³•éœ€è¦å¤§é‡è°ƒæ•´ã€‚</li>
<li>ç«¯åˆ°ç«¯æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒä¸Šçš„è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨æ— äººæœºå›¾åƒä¸Šçš„æ•ˆæœè¾ƒå·®ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ— äººæœºå›¾åƒè®¾è®¡çš„æ£€æµ‹å˜å‹å™¨ï¼ˆDETRï¼‰æ¡†æ¶ï¼Œå³UAV-DETRã€‚</li>
<li>UAV-DETRæ¡†æ¶åŒ…æ‹¬å¤šå°ºåº¦ç‰¹å¾èåˆä¸é¢‘ç‡å¢å¼ºæ¨¡å—ï¼Œç”¨äºæ•æ‰ä¸åŒå°ºåº¦çš„ç©ºé—´å’Œé¢‘ç‡ä¿¡æ¯ã€‚</li>
<li>æ— äººæœºå›¾åƒæ•°æ®é›†ä¸­ï¼ŒUAV-DETRç®—æ³•è¡¨ç°å‡ºè‰¯å¥½çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨VisDroneæ•°æ®é›†ä¸Šï¼ŒUAV-DETRç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æé«˜äº†æ£€æµ‹æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-57863538b2919e17e13345377e4b7827.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-acff33d5b3771d7feb96b1222d8cafdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e60bdad392dc44598fac5fd2639dcdf1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13a83c372dbe73e1cbcc42d8c481f8dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66df7122ad8bb673f007c253fe7b5fc0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbc055665e061348e3de1adae91f208a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e2363820845cfe1b054cb9c4ddf035fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3556e126c2addcd791dcbb1439405580.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1780cd6c2e3c0691ded763a1c5cedf46.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Leveraging-Semantic-Asymmetry-for-Precise-Gross-Tumor-Volume-Segmentation-of-Nasopharyngeal-Carcinoma-in-Planning-CT"><a href="#Leveraging-Semantic-Asymmetry-for-Precise-Gross-Tumor-Volume-Segmentation-of-Nasopharyngeal-Carcinoma-in-Planning-CT" class="headerlink" title="Leveraging Semantic Asymmetry for Precise Gross Tumor Volume   Segmentation of Nasopharyngeal Carcinoma in Planning CT"></a>Leveraging Semantic Asymmetry for Precise Gross Tumor Volume   Segmentation of Nasopharyngeal Carcinoma in Planning CT</h2><p><strong>Authors:Zi Li, Ying Chen, Zeli Chen, Yanzhou Su, Tai Ma, Tony C. W. Mok, Yan-Jie Zhou, Yunhai Bai, Zhinlin Zheng, Le Lu, Yirui Wang, Jia Ge, Xianghua Ye, Senxiang Yan, Dakai Jin</strong></p>
<p>In the radiation therapy of nasopharyngeal carcinoma (NPC), clinicians typically delineate the gross tumor volume (GTV) using non-contrast planning computed tomography to ensure accurate radiation dose delivery. However, the low contrast between tumors and adjacent normal tissues necessitates that radiation oncologists manually delineate the tumors, often relying on diagnostic MRI for guidance. % In this study, we propose a novel approach to directly segment NPC gross tumors on non-contrast planning CT images, circumventing potential registration errors when aligning MRI or MRI-derived tumor masks to planning CT. To address the low contrast issues between tumors and adjacent normal structures in planning CT, we introduce a 3D Semantic Asymmetry Tumor segmentation (SATs) method. Specifically, we posit that a healthy nasopharyngeal region is characteristically bilaterally symmetric, whereas the emergence of nasopharyngeal carcinoma disrupts this symmetry. Then, we propose a Siamese contrastive learning segmentation framework that minimizes the voxel-wise distance between original and flipped areas without tumor and encourages a larger distance between original and flipped areas with tumor. Thus, our approach enhances the sensitivity of features to semantic asymmetries. % Extensive experiments demonstrate that the proposed SATs achieves the leading NPC GTV segmentation performance in both internal and external testing, \emph{e.g.}, with at least 2% absolute Dice score improvement and 12% average distance error reduction when compared to other state-of-the-art methods in the external testing. </p>
<blockquote>
<p>åœ¨é¼»å’½ç™Œï¼ˆNPCï¼‰çš„æ”¾å°„æ²»ç–—è¿‡ç¨‹ä¸­ï¼Œä¸´åºŠåŒ»ç”Ÿé€šå¸¸ä½¿ç”¨éå¯¹æ¯”è®¡åˆ’è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆplanning CTï¼‰æ¥åˆ’å®šå¤§ä½“è‚¿ç˜¤ä½“ç§¯ï¼ˆGTVï¼‰ï¼Œä»¥ç¡®ä¿å‡†ç¡®çš„è¾å°„å‰‚é‡ä¼ é€’ã€‚ç„¶è€Œï¼Œè‚¿ç˜¤ä¸ç›¸é‚»æ­£å¸¸ç»„ç»‡ä¹‹é—´çš„å¯¹æ¯”åº¦è¾ƒä½ï¼Œè¿«ä½¿æ”¾ç–—ç§‘åŒ»ç”Ÿæ‰‹åŠ¨åˆ’å®šè‚¿ç˜¤ï¼Œé€šå¸¸ä¾èµ–è¯Šæ–­ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¿›è¡Œå¼•å¯¼ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç›´æ¥åœ¨éå¯¹æ¯”è®¡åˆ’CTå›¾åƒä¸Šåˆ†å‰²é¼»å’½ç™Œå¤§ä½“è‚¿ç˜¤çš„æ–°æ–¹æ³•ï¼Œé¿å…äº†å°†MRIæˆ–MRIè¡ç”Ÿçš„è‚¿ç˜¤æ©è†œä¸è®¡åˆ’CTå¯¹é½æ—¶å¯èƒ½å‡ºç°çš„æ³¨å†Œè¯¯å·®ã€‚ä¸ºäº†è§£å†³è®¡åˆ’CTä¸­è‚¿ç˜¤ä¸ç›¸é‚»æ­£å¸¸ç»“æ„ä¹‹é—´å¯¹æ¯”åº¦ä½çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†3Dè¯­ä¹‰ä¸å¯¹ç§°è‚¿ç˜¤åˆ†å‰²ï¼ˆSATsï¼‰æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¤ä¸ºå¥åº·çš„é¼»å’½åŒºåŸŸå…·æœ‰åŒä¾§å¯¹ç§°æ€§ï¼Œè€Œé¼»å’½ç™Œçš„å‡ºç°ä¼šç ´åè¿™ç§å¯¹ç§°æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Siameseå¯¹æ¯”å­¦ä¹ åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æœ€å°åŒ–åŸå§‹å’Œç¿»è½¬çš„æ— è‚¿ç˜¤åŒºåŸŸçš„ä½“ç´ çº§è·ç¦»æ¥è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶é¼“åŠ±åŸå§‹å’Œå¸¦æœ‰è‚¿ç˜¤çš„ç¿»è½¬åŒºåŸŸä¹‹é—´çš„è·ç¦»æ›´å¤§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†å¯¹è¯­ä¹‰ä¸å¯¹ç§°æ€§çš„ç‰¹å¾æ•æ„Ÿæ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SATsåœ¨å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•ä¸­å‡å®ç°äº†é¢†å…ˆçš„NPC GTVåˆ†å‰²æ€§èƒ½ï¼Œä¾‹å¦‚ä¸å¤–éƒ¨æµ‹è¯•ä¸­çš„å…¶ä»–æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œè‡³å°‘æé«˜äº†2ï¼…çš„ç»å¯¹Diceå¾—åˆ†å¹¶é™ä½äº†12ï¼…çš„å¹³å‡è·ç¦»è¯¯å·®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18290v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåœ¨é¼»å’½ç™Œæ”¾å°„æ²»ç–—è¿‡ç¨‹ä¸­ï¼Œè‚¿ç˜¤ä¸é‚»è¿‘æ­£å¸¸ç»„ç»‡å¯¹æ¯”åº¦ä½ï¼Œéœ€åŒ»ç”Ÿæ‰‹åŠ¨åŒºåˆ†ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºéå¯¹æ¯”å‰‚è§„åˆ’è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ç›´æ¥åˆ†å‰²é¼»å’½ç™Œå¤§ä½“è‚¿ç˜¤ä½“ç§¯ï¼ˆGTVï¼‰çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å¥åº·çš„é¼»å’½åŒºåŸŸå…·æœ‰ç‰¹å¾æ€§åŒä¾§å¯¹ç§°æ€§è´¨æ¥åŒºåˆ†è‚¿ç˜¤ï¼Œå¹¶é‡‡ç”¨Siameseå¯¹æ¯”å­¦ä¹ åˆ†å‰²æ¡†æ¶ï¼Œæé«˜äº†è¯­ä¹‰ä¸å¯¹ç§°çš„æ•æ„Ÿæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å†…å¤–æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†é¢†å…ˆçš„NPC GTVåˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é¼»å’½ç™Œæ”¾å°„æ²»ç–—éœ€è¦å‡†ç¡®åŒºåˆ†è‚¿ç˜¤å’Œé‚»è¿‘æ­£å¸¸ç»„ç»‡ã€‚</li>
<li>ç”±äºè‚¿ç˜¤ä¸é‚»è¿‘ç»„ç»‡çš„å¯¹æ¯”åº¦ä½ï¼Œé€šå¸¸éœ€ä¾èµ–åŒ»ç”Ÿæ‰‹åŠ¨åˆ†å‰²è‚¿ç˜¤ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„NPCè‚¿ç˜¤åˆ†å‰²æ–¹æ³•ï¼ŒåŸºäºéå¯¹æ¯”å‰‚è§„åˆ’CTå½±åƒã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å¥åº·çš„é¼»å’½åŒºåŸŸå…·æœ‰ç‰¹å¾æ€§åŒä¾§å¯¹ç§°æ€§è´¨æ¥è¯†åˆ«è‚¿ç˜¤ã€‚</li>
<li>é‡‡ç”¨Siameseå¯¹æ¯”å­¦ä¹ åˆ†å‰²æ¡†æ¶ï¼Œæé«˜äº†è¯­ä¹‰ä¸å¯¹ç§°çš„æ•æ„Ÿæ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å†…å¤–æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œç»å¯¹Diceåˆ†æ•°è‡³å°‘æé«˜äº†2%ï¼Œå¹³å‡è·ç¦»è¯¯å·®å‡å°‘äº†12%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0d886c414ddad89946e4dd15634c7637.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ec48313fe6dfe79a08c7af7994c88df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-529c18e271b61d703b3713ce277c75eb.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Multimodal-Object-Detection-using-Depth-and-Image-Data-for-Manufacturing-Parts"><a href="#Multimodal-Object-Detection-using-Depth-and-Image-Data-for-Manufacturing-Parts" class="headerlink" title="Multimodal Object Detection using Depth and Image Data for Manufacturing   Parts"></a>Multimodal Object Detection using Depth and Image Data for Manufacturing   Parts</h2><p><strong>Authors:Nazanin Mahjourian, Vinh Nguyen</strong></p>
<p>Manufacturing requires reliable object detection methods for precise picking and handling of diverse types of manufacturing parts and components. Traditional object detection methods utilize either only 2D images from cameras or 3D data from lidars or similar 3D sensors. However, each of these sensors have weaknesses and limitations. Cameras do not have depth perception and 3D sensors typically do not carry color information. These weaknesses can undermine the reliability and robustness of industrial manufacturing systems. To address these challenges, this work proposes a multi-sensor system combining an red-green-blue (RGB) camera and a 3D point cloud sensor. The two sensors are calibrated for precise alignment of the multimodal data captured from the two hardware devices. A novel multimodal object detection method is developed to process both RGB and depth data. This object detector is based on the Faster R-CNN baseline that was originally designed to process only camera images. The results show that the multimodal model significantly outperforms the depth-only and RGB-only baselines on established object detection metrics. More specifically, the multimodal model improves mAP by 13% and raises Mean Precision by 11.8% in comparison to the RGB-only baseline. Compared to the depth-only baseline, it improves mAP by 78% and raises Mean Precision by 57%. Hence, this method facilitates more reliable and robust object detection in service to smart manufacturing applications. </p>
<blockquote>
<p>åˆ¶é€ éœ€è¦å¯é çš„ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œç”¨äºç²¾ç¡®æ‹¾å–å’Œå¤„ç†å„ç§ç±»å‹çš„åˆ¶é€ é›¶éƒ¨ä»¶ã€‚ä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ–¹æ³•ä»…ä½¿ç”¨ç›¸æœºæ‹æ‘„çš„2Då›¾åƒæˆ–ä½¿ç”¨æ¿€å…‰é›·è¾¾æˆ–ç±»ä¼¼çš„ä¸‰ç»´ä¼ æ„Ÿå™¨è·å–çš„æ•°æ®ã€‚ç„¶è€Œï¼Œæ¯ç§ä¼ æ„Ÿå™¨éƒ½æœ‰å…¶ç¼ºç‚¹å’Œå±€é™æ€§ã€‚æ‘„åƒæœºæ²¡æœ‰æ·±åº¦æ„ŸçŸ¥èƒ½åŠ›ï¼Œè€Œä¸‰ç»´ä¼ æ„Ÿå™¨é€šå¸¸ä¸åŒ…å«é¢œè‰²ä¿¡æ¯ã€‚è¿™äº›ç¼ºç‚¹å¯èƒ½ä¼šç ´åå·¥ä¸šåˆ¶é€ ç³»ç»Ÿçš„å¯é æ€§å’Œç¨³å¥æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ç§ç»“åˆçº¢è‰²ç»¿è‰²è“è‰²ï¼ˆRGBï¼‰ç›¸æœºå’Œä¸‰ç»´ç‚¹äº‘ä¼ æ„Ÿå™¨çš„å¤šä¼ æ„Ÿå™¨ç³»ç»Ÿã€‚è¿™ä¸¤ä¸ªä¼ æ„Ÿå™¨ç»è¿‡æ ¡å‡†ï¼Œä»¥å¯¹ä¸¤ä¸ªç¡¬ä»¶è®¾å¤‡æ•è·çš„å¤šæ¨¡æ€æ•°æ®è¿›è¡Œç²¾ç¡®å¯¹é½ã€‚å¼€å‘äº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œå¯åŒæ—¶å¤„ç†RGBå’Œæ·±åº¦æ•°æ®ã€‚è¯¥ç›®æ ‡æ£€æµ‹å™¨åŸºäºFaster R-CNNåŸºçº¿å¼€å‘ï¼Œæœ€åˆè®¾è®¡ä»…ç”¨äºå¤„ç†ç›¸æœºå›¾åƒã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å…¬è®¤çš„ç›®æ ‡æ£€æµ‹æŒ‡æ ‡ä¸Šï¼Œå¤šæ¨¡æ€æ¨¡å‹æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨æ·±åº¦ä¿¡æ¯å’Œä»…ä½¿ç”¨RGBä¿¡æ¯çš„åŸºçº¿æ¨¡å‹ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œä¸ä»…ä½¿ç”¨RGBçš„åŸºçº¿ç›¸æ¯”ï¼Œå¤šæ¨¡æ€æ¨¡å‹çš„å¹³å‡ç²¾åº¦å‡å€¼æé«˜äº†13%ï¼Œå¹³å‡ç²¾åº¦æé«˜äº†11.8%ã€‚ä¸ä»…ä½¿ç”¨æ·±åº¦ä¿¡æ¯çš„åŸºçº¿ç›¸æ¯”ï¼Œå¹³å‡ç²¾åº¦å‡å€¼æé«˜äº†78%ï¼Œå¹³å‡ç²¾åº¦æé«˜äº†57%ã€‚å› æ­¤ï¼Œè¯¥æ–¹æ³•åœ¨æ™ºèƒ½åˆ¶é€ åº”ç”¨ä¸­ä¿ƒè¿›äº†æ›´å¯é å’Œç¨³å¥çš„ç›®æ ‡æ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09062v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>  åˆ¶é€ ä¸šéœ€è¦å¯é çš„ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œå¯¹å¤šç§åˆ¶é€ é›¶ä»¶å’Œç»„ä»¶è¿›è¡Œç²¾ç¡®æŠ“å–å’Œå¤„ç†ã€‚æœ¬å·¥ä½œæå‡ºä¸€ç§ç»“åˆRGBç›¸æœºå’Œä¸‰ç»´ç‚¹äº‘ä¼ æ„Ÿå™¨çš„å¤šä¼ æ„Ÿå™¨ç³»ç»Ÿï¼Œå…‹æœä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ–¹æ³•çš„å±€é™ã€‚é€šè¿‡å¼€å‘æ–°å‹çš„å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹æ–¹æ³•ï¼Œå¤„ç†RGBå’Œæ·±åº¦æ•°æ®ï¼Œæ˜¾è‘—æé«˜ç›®æ ‡æ£€æµ‹çš„å¯é æ€§å’Œé²æ£’æ€§ï¼Œä¸ºæ™ºèƒ½åˆ¶é€ åº”ç”¨æä¾›æœåŠ¡ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åˆ¶é€ ä¸šä¸­ç›®æ ‡æ£€æµ‹çš„å¯é æ€§å¯¹äºç²¾ç¡®æŠ“å–å’Œå¤„ç†å„ç±»éƒ¨ä»¶è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å•ä¸€ä¼ æ„Ÿå™¨ï¼ˆå¦‚ç›¸æœºæˆ–3Dä¼ æ„Ÿå™¨ï¼‰è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬å·¥ä½œæå‡ºä¸€ç§å¤šä¼ æ„Ÿå™¨ç³»ç»Ÿï¼Œç»“åˆRGBç›¸æœºå’Œä¸‰ç»´ç‚¹äº‘ä¼ æ„Ÿå™¨ï¼Œæé«˜æ•°æ®ç²¾åº¦å’Œå¯é æ€§ã€‚</li>
<li>æ–°å‹å¤šæ¨¡æ€ç›®æ ‡æ£€æµ‹æ–¹æ³•ç»“åˆäº†RGBå’Œæ·±åº¦æ•°æ®ï¼Œæ˜¾è‘—æé«˜ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åŸºäºFaster R-CNNåŸºçº¿æ¨¡å‹å¼€å‘ï¼Œå¯å¤„ç†å¤šæ¨¡æ€æ•°æ®ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šæ¨¡æ€æ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨æ·±åº¦æˆ–RGBçš„åŸºçº¿æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæ™ºèƒ½åˆ¶é€ ä¸šä¸­çš„ç›®æ ‡æ£€æµ‹å…·æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.09062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3155b8b7069fb4af2d7e22897a9422e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a824fa36cf4f6459f6bb1a7938145bda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df3e2df2120f686997d636ee82544f5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2236d2944416e35cfbc261f12c4b6bd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6303834884038d850993ca5cfcbb63f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-903bd839f6edded625dd94b879f61224.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3cd850588343746dfe71ce4be331b862.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0128cd990e3e381ad6f8f2065f699292.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Spatially Gene Expression Prediction using Dual-Scale Contrastive   Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-790fdbfb1dddae2979a6ff3f4cd43ab8.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  A Closer Look at Conditional Prompt Tuning for Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25219.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
