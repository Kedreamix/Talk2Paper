<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Positional Bias in Binary Question Answering How Uncertainty Shapes   Model Preferences">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-47b30a4ff914de705cc022b85fe30c9c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-03-æ›´æ–°"><a href="#2025-07-03-æ›´æ–°" class="headerlink" title="2025-07-03 æ›´æ–°"></a>2025-07-03 æ›´æ–°</h1><h2 id="Positional-Bias-in-Binary-Question-Answering-How-Uncertainty-Shapes-Model-Preferences"><a href="#Positional-Bias-in-Binary-Question-Answering-How-Uncertainty-Shapes-Model-Preferences" class="headerlink" title="Positional Bias in Binary Question Answering: How Uncertainty Shapes   Model Preferences"></a>Positional Bias in Binary Question Answering: How Uncertainty Shapes   Model Preferences</h2><p><strong>Authors:Tiziano Labruna, Simone Gallo, Giovanni Da San Martino</strong></p>
<p>Positional bias in binary question answering occurs when a model systematically favors one choice over another based solely on the ordering of presented options. In this study, we quantify and analyze positional bias across five large language models under varying degrees of answer uncertainty. We re-adapted the SQuAD-it dataset by adding an extra incorrect answer option and then created multiple versions with progressively less context and more out-of-context answers, yielding datasets that range from low to high uncertainty. Additionally, we evaluate two naturally higher-uncertainty benchmarks: (1) WebGPT - question pairs with unequal human-assigned quality scores, and (2) Winning Arguments - where models predict the more persuasive argument in Redditâ€™s r&#x2F;ChangeMyView exchanges. Across each dataset, the order of the â€œcorrectâ€ (or higher-quality&#x2F;persuasive) option is systematically flipped (first placed in position 1, then in position 2) to compute both Preference Fairness and Position Consistency. We observe that positional bias is nearly absent under low-uncertainty conditions, but grows exponentially when it becomes doubtful to decide which option is correct. </p>
<blockquote>
<p>äºŒå…ƒé—®é¢˜å›ç­”ä¸­çš„ä½ç½®åè§å‘ç”Ÿåœ¨æ¨¡å‹ä»…åŸºäºå‘ˆç°çš„é€‰é¡¹é¡ºåºè€Œç³»ç»Ÿåœ°åçˆ±æŸä¸€é€‰æ‹©çš„æƒ…å†µã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åœ¨ä¸åŒç­‰çº§çš„ç­”æ¡ˆä¸ç¡®å®šæ€§ä¸‹ï¼Œå¯¹äº”ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„ä½ç½®åè§è¿›è¡Œäº†é‡åŒ–å’Œåˆ†æã€‚æˆ‘ä»¬é€šè¿‡æ·»åŠ é¢å¤–çš„é”™è¯¯ç­”æ¡ˆé€‰é¡¹é‡æ–°é€‚åº”äº†SQuAD-itæ•°æ®é›†ï¼Œç„¶ååˆ›å»ºäº†å¤šä¸ªç‰ˆæœ¬ï¼Œéšç€ä¸Šä¸‹æ–‡é€æ¸å‡å°‘ã€è„±ç¦»ä¸Šä¸‹æ–‡çš„ç­”æ¡ˆé€æ¸å¢å¤šï¼Œä»ä½ä¸ç¡®å®šæ€§åˆ°é«˜ä¸ç¡®å®šæ€§ï¼Œç”Ÿæˆäº†æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ä¸¤ä¸ªè‡ªç„¶æ›´é«˜ä¸ç¡®å®šæ€§çš„åŸºå‡†æµ‹è¯•ï¼šï¼ˆ1ï¼‰WebGPTâ€”â€”å…·æœ‰ä¸å¹³ç­‰äººç±»åˆ†é…è´¨é‡å¾—åˆ†çš„é—®é¢˜å¯¹ï¼Œï¼ˆ2ï¼‰Winning Argumentsâ€”â€”æ¨¡å‹é¢„æµ‹Redditçš„r&#x2F;ChangeMyViewäº¤æµä¸­æ›´æœ‰è¯´æœåŠ›çš„è®ºç‚¹ã€‚åœ¨æ¯ä¸ªæ•°æ®é›†ä¸­ï¼Œâ€œæ­£ç¡®â€ï¼ˆæˆ–æ›´é«˜è´¨é‡&#x2F;æ›´æœ‰è¯´æœåŠ›çš„ï¼‰é€‰é¡¹çš„é¡ºåºéƒ½è¢«ç³»ç»Ÿåœ°æ”¹å˜ï¼ˆå…ˆæ”¾åœ¨ä½ç½®1ï¼Œç„¶åæ”¾åœ¨ä½ç½®2ï¼‰ï¼Œä»¥è®¡ç®—åå¥½å…¬å¹³æ€§å’Œä½ç½®ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨ä¸ç¡®å®šæ€§ä½çš„æƒ…å†µä¸‹ï¼Œä½ç½®åè§å‡ ä¹ä¸å­˜åœ¨ï¼Œä½†åœ¨éš¾ä»¥å†³å®šå“ªä¸ªé€‰é¡¹æ˜¯æ­£ç¡®çš„æ—¶ï¼Œä½ç½®åè§å‘ˆæŒ‡æ•°å¢é•¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23743v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†äºŒå…ƒé—®é¢˜å›ç­”ä¸­çš„ä½ç½®åè§ç°è±¡ï¼Œå³åœ¨ä¸ç¡®å®šç­”æ¡ˆçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯¹é€‰é¡¹çš„åå¥½ä¼šå—åˆ°é€‰é¡¹é¡ºåºçš„å½±å“ã€‚å®éªŒé€šè¿‡æ”¹ç¼–SQuAD-itæ•°æ®é›†å¹¶æ·»åŠ é”™è¯¯ç­”æ¡ˆé€‰é¡¹ï¼Œåˆ›å»ºäº†ä¸åŒä¸ç¡®å®šç¨‹åº¦çš„ç‰ˆæœ¬ã€‚åŒæ—¶è¯„ä¼°äº†ä¸¤ä¸ªè‡ªç„¶æ›´é«˜ä¸ç¡®å®šæ€§çš„åŸºå‡†æµ‹è¯•ï¼šWebGPTå’ŒWinning Argumentsã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ä½ä¸ç¡®å®šæ€§æ¡ä»¶ä¸‹ï¼Œä½ç½®åè§å‡ ä¹ä¸å­˜åœ¨ï¼Œä½†åœ¨éš¾ä»¥ç¡®å®šå“ªä¸ªé€‰é¡¹æ­£ç¡®çš„æƒ…å†µä¸‹ï¼Œä½ç½®åè§ä¼šå‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½ç½®åè§åœ¨äºŒå…ƒé—®é¢˜å›ç­”ä¸­å‡ºç°ï¼Œæ¨¡å‹ä¼šåŸºäºé€‰é¡¹é¡ºåºç³»ç»Ÿæ€§åœ°åå¥½æŸä¸€ç­”æ¡ˆã€‚</li>
<li>å®éªŒé€šè¿‡æ”¹ç¼–SQuAD-itæ•°æ®é›†å¹¶æ·»åŠ ä¸åŒéš¾åº¦çš„ç­”æ¡ˆé€‰é¡¹ï¼Œåˆ›å»ºäº†ä¸åŒä¸ç¡®å®šæ€§çš„æ•°æ®é›†ç‰ˆæœ¬ã€‚</li>
<li>åœ¨ä½ä¸ç¡®å®šæ€§æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹å‡ ä¹ä¸å—ä½ç½®åè§å½±å“ã€‚</li>
<li>åœ¨é«˜ä¸ç¡®å®šæ€§æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹çš„ä½ç½®åè§ä¼šæ˜¾è‘—å¢é•¿ã€‚</li>
<li>ä¸¤ä¸ªè‡ªç„¶æ›´é«˜ä¸ç¡®å®šæ€§çš„åŸºå‡†æµ‹è¯•æ˜¯WebGPTå’ŒWinning Argumentsã€‚</li>
<li>é€šè¿‡ç¿»è½¬â€œæ­£ç¡®â€é€‰é¡¹çš„ä½ç½®ï¼Œå¯ä»¥è®¡ç®—åå¥½å…¬å¹³æ€§å’Œä½ç½®ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23743">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-612f6726ef1454a6bd5b1cc37f840a28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3df711e15f9e328bf0e5e4c79b53031c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Systematic-Study-of-Compositional-Syntactic-Transformer-Language-Models"><a href="#A-Systematic-Study-of-Compositional-Syntactic-Transformer-Language-Models" class="headerlink" title="A Systematic Study of Compositional Syntactic Transformer Language   Models"></a>A Systematic Study of Compositional Syntactic Transformer Language   Models</h2><p><strong>Authors:Yida Zhao, Hao Xve, Xiang Hu, Kewei Tu</strong></p>
<p>Syntactic language models (SLMs) enhance Transformers by incorporating syntactic biases through the modeling of linearized syntactic parse trees alongside surface sentences. This paper focuses on compositional SLMs that are based on constituency parse trees and contain explicit bottom-up composition of constituent representations. We identify key aspects of design choices in existing compositional SLMs and propose a unified framework encompassing both existing models and novel variants. We conduct a comprehensive empirical evaluation of all the variants in our framework across language modeling, syntactic generalization, summarization, dialogue, and inference efficiency. Based on the experimental results, we make multiple recommendations on the design of compositional SLMs. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/zhaoyd1/compositional_SLMs">https://github.com/zhaoyd1/compositional_SLMs</a>. </p>
<blockquote>
<p>å¥æ³•è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰é€šè¿‡å»ºæ¨¡çº¿æ€§åŒ–çš„å¥æ³•è§£ææ ‘å’Œè¡¨å±‚å¥å­ï¼Œå¼•å…¥äº†å¥æ³•åå·®ï¼Œä»è€Œå¢å¼ºäº†Transformerçš„åŠŸèƒ½ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»åŸºäºæˆåˆ†è§£ææ ‘çš„ç»„åˆå‹SLMï¼Œå…¶ä¸­åŒ…å«æ˜ç¡®çš„è‡ªä¸‹è€Œä¸Šçš„æˆåˆ†è¡¨ç¤ºç»„åˆã€‚æˆ‘ä»¬ç¡®å®šäº†ç°æœ‰ç»„åˆå‹SLMä¸­è®¾è®¡é€‰æ‹©çš„å…³é”®æ–¹é¢ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬ç°æœ‰æ¨¡å‹å’Œæ–°å‹å˜ä½“ã€‚æˆ‘ä»¬å¯¹æ¡†æ¶ä¸­æ‰€æœ‰å˜ä½“è¿›è¡Œäº†å…¨é¢çš„ç»éªŒè¯„ä¼°ï¼ŒåŒ…æ‹¬è¯­è¨€å»ºæ¨¡ã€å¥æ³•æ³›åŒ–ã€æ‘˜è¦ã€å¯¹è¯å’Œæ¨ç†æ•ˆç‡ç­‰æ–¹é¢ã€‚åŸºäºå®éªŒç»“æœï¼Œæˆ‘ä»¬å¯¹ç»„åˆå‹SLMçš„è®¾è®¡æå‡ºäº†å¤šé¡¹å»ºè®®ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/zhaoyd1/compositional_SLMs%E3%80%82">https://github.com/zhaoyd1/compositional_SLMsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22978v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡ç»“åˆçº¿æ€§åŒ–çš„å¥æ³•è§£ææ ‘å’Œè¡¨é¢å¥å­ï¼Œå¢å¼ºTransformerçš„å¥æ³•è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ã€‚æ–‡ç« é‡ç‚¹ç ”ç©¶åŸºäºæˆåˆ†å¥æ³•è§£ææ ‘çš„ç»„åˆSLMsï¼Œå…¶åŒ…å«æ˜ç¡®çš„è‡ªä¸‹è€Œä¸Šçš„æˆåˆ†è¡¨ç¤ºç»„åˆã€‚æ–‡ç« åˆ†æäº†ç°æœ‰ç»„åˆSLMçš„å…³é”®è®¾è®¡é€‰æ‹©ï¼Œæå‡ºäº†æ¶µç›–ç°æœ‰æ¨¡å‹å’Œæ–°å‹å˜ä½“çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå¹¶å…¨é¢è¯„ä¼°äº†æ‰€æœ‰å˜ä½“åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯­è¨€å»ºæ¨¡ã€å¥æ³•æ³›åŒ–ã€æ‘˜è¦ã€å¯¹è¯å’Œæ¨ç†æ•ˆç‡ã€‚åŸºäºå®éªŒç»“æœï¼Œå¯¹ç»„åˆSLMçš„è®¾è®¡æå‡ºäº†å¤šé¡¹å»ºè®®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLMsé€šè¿‡ç»“åˆçº¿æ€§åŒ–çš„å¥æ³•è§£ææ ‘å’Œè¡¨é¢å¥å­å¢å¼ºTransformerçš„æ€§èƒ½ã€‚</li>
<li>æ–‡ç« èšç„¦äºåŸºäºæˆåˆ†å¥æ³•è§£ææ ‘çš„ç»„åˆSLMsã€‚</li>
<li>æ–‡ç« æå‡ºäº†æ¶µç›–ç°æœ‰æ¨¡å‹å’Œæ–°å‹å˜ä½“çš„ç»Ÿä¸€æ¡†æ¶ã€‚</li>
<li>å¯¹æ‰€æœ‰å˜ä½“è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œæ¶µç›–è¯­è¨€å»ºæ¨¡ã€å¥æ³•æ³›åŒ–ã€æ‘˜è¦ã€å¯¹è¯å’Œæ¨ç†æ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒæŸäº›è®¾è®¡é€‰æ‹©å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>æ–‡ç« æä¾›äº†å…³äºå¦‚ä½•è®¾è®¡ç»„åˆSLMsçš„å¤šä¸ªå»ºè®®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9598b12eaeea5eadea035213681e2f09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df9826f7495e6cd8b3b4e358d48f97f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74f8ab79b25fc26a7f7c058bfcc076ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d85dfbc8d277229eb6b7546ab84615dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58f77b5915837969049cf12c57fa15e4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="What-Makes-ChatGPT-Effective-for-Software-Issue-Resolution-An-Empirical-Study-of-Developer-ChatGPT-Conversations-in-GitHub"><a href="#What-Makes-ChatGPT-Effective-for-Software-Issue-Resolution-An-Empirical-Study-of-Developer-ChatGPT-Conversations-in-GitHub" class="headerlink" title="What Makes ChatGPT Effective for Software Issue Resolution? An Empirical   Study of Developer-ChatGPT Conversations in GitHub"></a>What Makes ChatGPT Effective for Software Issue Resolution? An Empirical   Study of Developer-ChatGPT Conversations in GitHub</h2><p><strong>Authors:Ramtin Ehsani, Sakshi Pathak, Esteban Parra, Sonia Haiduc, Preetha Chatterjee</strong></p>
<p>Conversational large-language models are extensively used for issue resolution tasks. However, not all developer-LLM conversations are useful for effective issue resolution. In this paper, we analyze 686 developer-ChatGPT conversations shared within GitHub issue threads to identify characteristics that make these conversations effective for issue resolution. First, we analyze the conversations and their corresponding issues to distinguish helpful from unhelpful conversations. We begin by categorizing the types of tasks developers seek help with to better understand the scenarios in which ChatGPT is most effective. Next, we examine a wide range of conversational, project, and issue-related metrics to uncover factors associated with helpful conversations. Finally, we identify common deficiencies in unhelpful ChatGPT responses to highlight areas that could inform the design of more effective developer-facing tools. We found that only 62% of the ChatGPT conversations were helpful for successful issue resolution. ChatGPT is most effective for code generation and tools&#x2F;libraries&#x2F;APIs recommendations, but struggles with code explanations. Helpful conversations tend to be shorter, more readable, and exhibit stronger semantic and linguistic alignment. Larger, more popular projects and more experienced developers benefit more from ChatGPT. At the issue level, ChatGPT performs best on simpler problems with limited developer activity and faster resolution, typically well-scoped tasks like compilation errors. The most common deficiencies in unhelpful ChatGPT responses include incorrect information and lack of comprehensiveness. Our findings have wide implications including guiding developers on effective interaction strategies for issue resolution, informing the development of tools or frameworks to support optimal prompt design, and providing insights on fine-tuning LLMs for issue resolution tasks. </p>
<blockquote>
<p>å¯¹è¯å¼å¤§å‹è¯­è¨€æ¨¡å‹å¹¿æ³›åº”ç”¨äºé—®é¢˜è§£å†³æ–¹æ¡ˆä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰å¼€å‘è€…ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹è¯éƒ½æœ‰åŠ©äºæœ‰æ•ˆåœ°è§£å†³é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†åœ¨GitHubé—®é¢˜çº¿ç¨‹ä¸­å…±äº«çš„686æ¡å¼€å‘è€…ä¸ChatGPTçš„å¯¹è¯ï¼Œä»¥è¯†åˆ«ä½¿è¿™äº›å¯¹è¯åœ¨è§£å†³é—®é¢˜æ–¹é¢æœ‰æ•ˆçš„ç‰¹å¾ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ†æè¿™äº›å¯¹è¯åŠå…¶ç›¸åº”çš„é—®é¢˜ï¼Œä»¥åŒºåˆ†æœ‰å¸®åŠ©å’Œæ— å¸®åŠ©çš„å¯¹è¯ã€‚æˆ‘ä»¬é€šè¿‡åˆ†ç±»å¼€å‘è€…å¯»æ±‚å¸®åŠ©çš„ä»»åŠ¡ç±»å‹ï¼Œä»¥æ›´å¥½åœ°äº†è§£ChatGPTåœ¨å“ªäº›æƒ…å†µä¸‹æœ€ä¸ºæœ‰æ•ˆã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ£€æŸ¥å„ç§ä¸å¯¹è¯ã€é¡¹ç›®å’Œé—®é¢˜ç›¸å…³çš„æŒ‡æ ‡ï¼Œä»¥å‘ç°ä¸æœ‰å¸®åŠ©çš„å¯¹è¯ç›¸å…³çš„å› ç´ ã€‚æœ€åï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸æœ‰å¸®åŠ©çš„ChatGPTå“åº”ä¸­çš„å¸¸è§ç¼ºé™·ï¼Œä»¥å¼ºè°ƒå¯ä»¥è®¾è®¡é¢å‘å¼€å‘è€…çš„æ›´æœ‰æ•ˆå·¥å…·çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å‘ç°ï¼Œåªæœ‰62%çš„ChatGPTå¯¹è¯å¯¹äºæˆåŠŸè§£å†³é—®é¢˜æ˜¯æœ‰å¸®åŠ©çš„ã€‚ChatGPTåœ¨ä»£ç ç”Ÿæˆå’Œå·¥å…·&#x2F;åº“&#x2F;APIæ¨èæ–¹é¢æœ€ä¸ºæœ‰æ•ˆï¼Œä½†åœ¨ä»£ç è§£é‡Šæ–¹é¢å´è¡¨ç°ä¸ä½³ã€‚æœ‰å¸®åŠ©çš„å¯¹è¯å¾€å¾€è¾ƒçŸ­ã€å¯è¯»æ€§æ›´å¼ºï¼Œå¹¶ä¸”è¡¨ç°å‡ºæ›´å¼ºçš„è¯­ä¹‰å’Œè¯­è¨€å¯¹é½ã€‚è¾ƒå¤§çš„ã€è¾ƒå—æ¬¢è¿çš„é¡¹ç›®å’Œæ›´æœ‰ç»éªŒçš„å¼€å‘è€…æ›´èƒ½ä»ChatGPTä¸­å—ç›Šã€‚åœ¨é—®é¢˜å±‚é¢ï¼ŒChatGPTåœ¨æœ€ç®€å•ã€å¼€å‘è€…æ´»åŠ¨æœ‰é™ã€è§£å†³é€Ÿåº¦æ›´å¿«çš„é—®é¢˜ä¸Šè¡¨ç°æœ€ä½³ï¼Œé€šå¸¸æ˜¯èŒƒå›´æ˜ç¡®çš„ä»»åŠ¡ï¼Œå¦‚ç¼–è¯‘é”™è¯¯ã€‚ä¸æœ‰å¸®åŠ©çš„ChatGPTå“åº”ä¸­æœ€å¸¸è§çš„ç¼ºé™·åŒ…æ‹¬ä¿¡æ¯ä¸æ­£ç¡®å’Œç¼ºä¹å…¨é¢æ€§ã€‚æˆ‘ä»¬çš„å‘ç°å…·æœ‰å¹¿æ³›çš„æ„ä¹‰ï¼ŒåŒ…æ‹¬æŒ‡å¯¼å¼€å‘è€…æœ‰æ•ˆçš„äº’åŠ¨ç­–ç•¥æ¥è§£å†³é—®é¢˜ï¼Œå¼€å‘å·¥å…·å’Œæ¡†æ¶æ¥æ”¯æŒæœ€ä½³æç¤ºè®¾è®¡ï¼Œä»¥åŠä¸ºé—®é¢˜è§£å†³æ–¹æ¡ˆä»»åŠ¡å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æä¾›è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22390v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡åˆ†æäº†åœ¨GitHubé—®é¢˜çº¿ç¨‹ä¸­åˆ†äº«çš„686ä¸ªå¼€å‘è€…ä¸ChatGPTçš„å¯¹è¯ï¼Œä»¥è¯†åˆ«å“ªäº›ç‰¹å¾ä½¿å¾—è¿™äº›å¯¹è¯åœ¨è§£å†³é—®é¢˜æ—¶æ›´æœ‰æ•ˆã€‚ç ”ç©¶å‘ç°ï¼Œåªæœ‰62%çš„ChatGPTå¯¹è¯æœ‰åŠ©äºæˆåŠŸè§£å†³é—®é¢˜ã€‚ChatGPTåœ¨ä»£ç ç”Ÿæˆã€å·¥å…·&#x2F;åº“&#x2F;APIæ¨èæ–¹é¢æœ€ä¸ºæœ‰æ•ˆï¼Œä½†åœ¨ä»£ç è§£é‡Šæ–¹é¢è¡¨ç°è¾ƒå·®ã€‚æœ‰ç›Šçš„å¯¹è¯å¾€å¾€æ›´çŸ­ã€å¯è¯»æ€§æ›´å¼ºï¼Œå¹¶è¡¨ç°å‡ºæ›´å¼ºçš„è¯­ä¹‰å’Œè¯­è¨€å­¦å¯¹é½ã€‚å¤§å‹ã€æ›´å—æ¬¢è¿çš„é¡¹ç›®çš„å¼€å‘è€…ä»¥åŠæ›´æœ‰ç»éªŒçš„å¼€å‘è€…æ›´èƒ½ä»ChatGPTä¸­å—ç›Šã€‚å¯¹äºé—®é¢˜è¾ƒå°‘çš„ç®€å•é—®é¢˜ï¼ŒChatGPTè¡¨ç°æœ€ä½³ï¼Œè§£å†³é€Ÿåº¦æ›´å¿«ï¼Œé€šå¸¸æ˜¯é™å®šèŒƒå›´çš„ä»»åŠ¡ï¼Œå¦‚ç¼–è¯‘é”™è¯¯ã€‚æœªèµ·ä½œç”¨çš„ChatGPTå›åº”ä¸­æœ€å¸¸è§çš„ç¼ºé™·åŒ…æ‹¬ä¿¡æ¯ä¸æ­£ç¡®å’Œç¼ºä¹å…¨é¢æ€§ç­‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹äºæŒ‡å¯¼å¼€å‘è€…åˆ¶å®šæœ‰æ•ˆçš„äº’åŠ¨ç­–ç•¥è¿›è¡Œé—®é¢˜è§£ç­”ï¼Œæ”¯æŒå·¥å…·æˆ–æ¡†æ¶çš„ä¼˜åŒ–æç¤ºè®¾è®¡ä»¥åŠå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé—®é¢˜è§£ç­”ä»»åŠ¡å…·æœ‰å¯ç¤ºæ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹å¼€å‘è€…ä¸ChatGPTçš„å¯¹è¯è¿›è¡Œåˆ†æï¼Œå‘ç°åªæœ‰62%çš„å¯¹è¯å¯¹è§£å†³é—®é¢˜æœ‰å¸®åŠ©ã€‚</li>
<li>ChatGPTåœ¨ä»£ç ç”Ÿæˆå’Œæ¨èä»»åŠ¡ä¸­æœ€ä¸ºæœ‰æ•ˆã€‚</li>
<li>æœ‰ç›Šçš„å¯¹è¯å¾€å¾€æ›´çŸ­ã€æ›´æ˜“è¯»ï¼Œå¹¶å±•ç°å‡ºæ›´å¼ºçš„è¯­ä¹‰å’Œè¯­è¨€å­¦å¯¹é½ã€‚</li>
<li>å¤§å‹å’Œå—æ¬¢è¿çš„é¡¹ç›®çš„å¼€å‘è€…ä»¥åŠæœ‰ç»éªŒçš„å¼€å‘è€…ä»ChatGPTä¸­è·ç›Šæ›´å¤šã€‚</li>
<li>ChatGPTåœ¨è§£å†³ç®€å•é—®é¢˜ã€å¿«é€Ÿè§£å†³å’Œé™å®šèŒƒå›´çš„ä»»åŠ¡ï¼ˆå¦‚ç¼–è¯‘é”™è¯¯ï¼‰ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>æœªèµ·ä½œç”¨çš„ChatGPTå›åº”ä¸­æœ€å¸¸è§çš„ç¼ºé™·åŒ…æ‹¬ä¿¡æ¯ä¸æ­£ç¡®å’Œç¼ºä¹å…¨é¢æ€§ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4d8b4e06eba226709aed716e0db99c5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90e747326c040310d7cf9afc5b6ff913.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-426d0beabad908e02924dc346754feb1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Projected-Compression-Trainable-Projection-for-Efficient-Transformer-Compression"><a href="#Projected-Compression-Trainable-Projection-for-Efficient-Transformer-Compression" class="headerlink" title="Projected Compression: Trainable Projection for Efficient Transformer   Compression"></a>Projected Compression: Trainable Projection for Efficient Transformer   Compression</h2><p><strong>Authors:Maciej Stefaniak, MichaÅ‚ Krutul, Jan MaÅ‚aÅ›nicki, Maciej PiÃ³ro, Jakub Krajewski, Sebastian Jaszczur, Marek Cygan, Kamil Adamczewski, Jan Ludziejewski</strong></p>
<p>Large language models have steadily increased in size to achieve improved performance; however, this growth has also led to greater inference time and computational demands. Consequently, there is rising interest in model size reduction methods. To address this issue, we propose Projected Compression, a novel model compression technique, that reduces model weights by utilizing projection modules. Specifically, we first train additional trainable projections weights and preserve access to all the original model parameters. Subsequently, these projections are merged into a lower-dimensional product matrix, resulting in a reduced-size standard Transformer-based model. Unlike alternative approaches that require additional computational overhead, our method matches the base modelâ€™s per-token computation step in FLOPs. Experimental results show that Projected Compression outperforms the comparable hard pruning and retraining approach on higher quality models. Moreover, the performance margin scales well with the number of tokens. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å°ºå¯¸ä¸€ç›´åœ¨å¢é•¿ä»¥å®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œä½†è¿™ç§å¢é•¿ä¹Ÿå¯¼è‡´äº†æ¨ç†æ—¶é—´å’Œè®¡ç®—éœ€æ±‚çš„å¢åŠ ã€‚å› æ­¤ï¼Œäººä»¬å¯¹æ¨¡å‹å°ºå¯¸ç¼©å‡æ–¹æ³•äº§ç”Ÿäº†è¶Šæ¥è¶Šæµ“åšçš„å…´è¶£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Projected Compressionï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œå®ƒé€šè¿‡åˆ©ç”¨æŠ•å½±æ¨¡å—æ¥å‡å°‘æ¨¡å‹æƒé‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®­ç»ƒé¢å¤–çš„å¯è®­ç»ƒæŠ•å½±æƒé‡å¹¶ä¿ç•™è®¿é—®æ‰€æœ‰åŸå§‹æ¨¡å‹å‚æ•°çš„èƒ½åŠ›ã€‚éšåï¼Œè¿™äº›æŠ•å½±è¢«åˆå¹¶æˆä¸€ä¸ªä½ç»´äº§å“çŸ©é˜µï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªåŸºäºæ ‡å‡†Transformerçš„ç¼©å‡å°ºå¯¸æ¨¡å‹ã€‚ä¸å…¶ä»–éœ€è¦é¢å¤–è®¡ç®—å¼€é”€çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPsï¼‰ä¸Šä¸åŸºç¡€æ¨¡å‹çš„æ¯ä¸ªä»¤ç‰Œè®¡ç®—æ­¥éª¤ç›¸åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è¾ƒé«˜è´¨é‡çš„æ¨¡å‹ä¸Šï¼ŒProjected Compressionä¼˜äºç±»ä¼¼çš„ç¡¬å‰ªæå’Œå†è®­ç»ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ€§èƒ½å·®è·éšç€ä»¤ç‰Œæ•°é‡çš„å¢åŠ è€Œå¢åŠ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22255v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¢é•¿æ¨¡å‹è§„æ¨¡æ¥æå‡æ€§èƒ½ï¼Œä½†è¿™ä¹Ÿå¯¼è‡´äº†æ¨ç†æ—¶é—´å’Œè®¡ç®—éœ€æ±‚çš„å¢åŠ ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Projected Compressionè¿™ä¸€æ–°å‹æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡åˆ©ç”¨æŠ•å½±æ¨¡å—æ¥å‡å°‘æ¨¡å‹æƒé‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºå…¶ä»–ç¡¬å‰ªæå’Œå†è®­ç»ƒçš„æ–¹æ³•ï¼ŒProjected Compressionåœ¨æ›´é«˜è´¨é‡çš„æ¨¡å‹ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚ä¸”éšç€tokenæ•°é‡çš„å¢åŠ ï¼Œæ€§èƒ½å·®è·ä¹Ÿåœ¨ä¸æ–­æ‰©å¤§ã€‚è¯¥æ–¹æ³•å¯åœ¨ä¸å¢åŠ è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œå°†æ¨¡å‹å‚æ•°ç¼©å‡ä¸ºä¸€ä¸ªæ›´ä½ç»´åº¦çš„äº§å“çŸ©é˜µï¼Œä»è€Œå®ç°åŸºäºæ ‡å‡†Transformerçš„æ¨¡å‹å°ºå¯¸ç¼©å‡ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶æ€§èƒ½æœ‰æ‰€æå‡ï¼Œä½†ä¹Ÿé¢ä¸´æ¨ç†æ—¶é—´å’Œè®¡ç®—éœ€æ±‚çš„æŒ‘æˆ˜ã€‚</li>
<li>Projected Compressionæ˜¯ä¸€ç§æ–°å‹çš„æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œé€šè¿‡åˆ©ç”¨æŠ•å½±æ¨¡å—å‡å°‘æ¨¡å‹æƒé‡ã€‚</li>
<li>è¯¥æŠ€æœ¯ä¿ç•™äº†åŸå§‹æ¨¡å‹çš„å…¨éƒ¨å‚æ•°å¹¶å¼•å…¥äº†å¯è®­ç»ƒçš„æŠ•å½±æƒé‡ã€‚</li>
<li>é€šè¿‡å°†æŠ•å½±åˆå¹¶ä¸ºä¸€ä¸ªè¾ƒä½ç»´åº¦çš„äº§å“çŸ©é˜µï¼Œå®ç°äº†åŸºäºæ ‡å‡†Transformerçš„æ¨¡å‹å°ºå¯¸ç¼©å‡ã€‚</li>
<li>Projected Compressionä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´é«˜çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶ä¸”æ€§èƒ½å·®è·éštokenæ•°é‡çš„å¢åŠ è€Œæ‰©å¤§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fc1585727782cddeac716ce7e9c0b0df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d0f0bfe8fa4dd4f946266721e63a029.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a5b148565ca7d6636fc3f2e7b10cf4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb46b75c5e72bf48e351d032bf886b0c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Dataset-for-Enhancing-MLLMs-in-Visualization-Understanding-and-Reconstruction"><a href="#A-Dataset-for-Enhancing-MLLMs-in-Visualization-Understanding-and-Reconstruction" class="headerlink" title="A Dataset for Enhancing MLLMs in Visualization Understanding and   Reconstruction"></a>A Dataset for Enhancing MLLMs in Visualization Understanding and   Reconstruction</h2><p><strong>Authors:Can Liu, Chunlin Da, Xiaoxiao Long, Yuxiao Yang, Yu Zhang, Yong Wang</strong></p>
<p>Current multimodal large language models (MLLMs), while effective in natural image understanding, struggle with visualization understanding due to their inability to decode the data-to-visual mapping and extract structured information. To address these challenges, we propose SimVec, a compact and structured vector format that encodes chart elements, including mark types, positions, and sizes. Then, we present a new visualization dataset, which consists of bitmap images of charts, their corresponding SimVec representations, and data-centric question-answering pairs, each accompanied by explanatory chain-of-thought sentences. We fine-tune state-of-the-art MLLMs using our dataset. The experimental results show that fine-tuning leads to substantial improvements in data-centric reasoning tasks compared to their zero-shot versions. SimVec also enables MLLMs to accurately and compactly reconstruct chart structures from images. Our dataset and code are available at: <a target="_blank" rel="noopener" href="https://github.com/VIDA-Lab/MLLM4VIS">https://github.com/VIDA-Lab/MLLM4VIS</a>. </p>
<blockquote>
<p>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªç„¶å›¾åƒç†è§£æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†åœ¨å¯è§†åŒ–ç†è§£æ–¹é¢å´å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•è§£ç æ•°æ®åˆ°è§†è§‰çš„æ˜ å°„å¹¶æå–ç»“æ„åŒ–ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SimVecï¼Œè¿™æ˜¯ä¸€ç§ç´§å‡‘ä¸”ç»“æ„åŒ–çš„å‘é‡æ ¼å¼ï¼Œå¯ä»¥ç¼–ç å›¾è¡¨å…ƒç´ ï¼ŒåŒ…æ‹¬æ ‡è®°ç±»å‹ã€ä½ç½®å’Œå¤§å°ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªæ–°çš„å¯è§†åŒ–æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å›¾è¡¨çš„ä½å›¾å›¾åƒã€ç›¸åº”çš„SimVecè¡¨ç¤ºå½¢å¼ä»¥åŠä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„é—®ç­”å¯¹ï¼Œæ¯ä¸ªé—®ç­”å¯¹éƒ½é™„æœ‰è§£é‡Šæ€§çš„æ€ç»´é“¾å¥å­ã€‚æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†å¯¹æœ€æ–°MLLMsè¿›è¡Œäº†å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸é›¶æ ·æœ¬ç‰ˆæœ¬ç›¸æ¯”ï¼Œå¾®è°ƒåœ¨æ•°æ®ä¸ºä¸­å¿ƒçš„é€»è¾‘ä»»åŠ¡ä¸­å¸¦æ¥äº†æ˜¾è‘—æ”¹è¿›ã€‚SimVecè¿˜ä½¿MLLMsèƒ½å¤Ÿå‡†ç¡®ä¸”ç´§å‡‘åœ°ä»å›¾åƒä¸­é‡å»ºå›¾è¡¨ç»“æ„ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/VIDA-Lab/MLLM4VIS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/VIDA-Lab/MLLM4VISæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21319v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SimVecæ˜¯ä¸€ç§ç”¨äºè§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯è§†åŒ–ç†è§£æ–¹é¢çš„ä¸è¶³çš„åˆ›æ–°æ€§æ–¹æ¡ˆã€‚å®ƒæå‡ºäº†ä¸€ä¸ªç´§å‡‘ä¸”ç»“æ„åŒ–çš„å‘é‡æ ¼å¼ï¼Œèƒ½ç¼–ç å›¾è¡¨å…ƒç´ ï¼ŒåŒ…æ‹¬æ ‡è®°ç±»å‹ã€ä½ç½®å’Œå¤§å°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ¨å‡ºäº†ä¸€ä¸ªæ–°çš„å¯è§†åŒ–æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒè¿™äº›æ¨¡å‹ä»¥æ”¹è¿›å…¶åœ¨æ•°æ®ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°æ®ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—æ”¹å–„ï¼Œå¹¶ä¸”SimVecè¿˜èƒ½ä½¿è¿™äº›æ¨¡å‹å‡†ç¡®ä¸”ç´§å‡‘åœ°ä»å›¾åƒä¸­é‡å»ºå›¾è¡¨ç»“æ„ã€‚ç›¸å…³æ•°æ®å’Œä»£ç å¯åœ¨VIDAå®éªŒå®¤çš„MLLM4VISç½‘ç«™æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SimVecæ˜¯ä¸€ç§è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¯è§†åŒ–ç†è§£æ–¹é¢ä¸è¶³çš„æ–¹æ¡ˆã€‚</li>
<li>SimVecèƒ½ç¼–ç å›¾è¡¨å…ƒç´ ï¼ŒåŒ…æ‹¬æ ‡è®°ç±»å‹ã€ä½ç½®å’Œå¤§å°ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†ä¸€ä¸ªæ–°çš„å¯è§†åŒ–æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥æ”¹è¿›æ•°æ®ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ä½å›¾å›¾åƒã€SimVecè¡¨ç¤ºå’Œé—®ç­”å¯¹ï¼Œæ¯ä¸ªé—®ç­”å¯¹éƒ½é™„æœ‰è§£é‡Šæ€§çš„æ€ç»´é“¾å¥å­ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°æ®ä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>SimVecä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®ä¸”ç´§å‡‘åœ°ä»å›¾åƒä¸­é‡å»ºå›¾è¡¨ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33d8b5af359a5989d00d1107ee8da902.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cb3a6963e04294ded55b0a301d4dbbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05cd270227e6c0480cabbf96bdf5bd49.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e158771b73037d7c13fbd7e5f63e1e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd6e0655a678473fdb9c154de355bdb3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Arabic-Dialect-Classification-using-RNNs-Transformers-and-Large-Language-Models-A-Comparative-Analysis"><a href="#Arabic-Dialect-Classification-using-RNNs-Transformers-and-Large-Language-Models-A-Comparative-Analysis" class="headerlink" title="Arabic Dialect Classification using RNNs, Transformers, and Large   Language Models: A Comparative Analysis"></a>Arabic Dialect Classification using RNNs, Transformers, and Large   Language Models: A Comparative Analysis</h2><p><strong>Authors:Omar A. Essameldin, Ali O. Elbeih, Wael H. Gomaa, Wael F. Elsersy</strong></p>
<p>The Arabic language is among the most popular languages in the world with a huge variety of dialects spoken in 22 countries. In this study, we address the problem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets. RNN models, Transformer models, and large language models (LLMs) via prompt engineering are created and tested. Among these, MARBERTv2 performed best with 65% accuracy and 64% F1-score. Through the use of state-of-the-art preprocessing techniques and the latest NLP models, this paper identifies the most significant linguistic issues in Arabic dialect identification. The results corroborate applications like personalized chatbots that respond in usersâ€™ dialects, social media monitoring, and greater accessibility for Arabic communities. </p>
<blockquote>
<p>é˜¿æ‹‰ä¼¯è¯­æ˜¯ä¸–ç•Œä¸Šä½¿ç”¨æœ€å¹¿æ³›çš„è¯­ç§ä¹‹ä¸€ï¼Œåœ¨22ä¸ªå›½å®¶æœ‰ä¼—å¤šä¸åŒçš„æ–¹è¨€ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†å¯¹é˜¿æ‹‰ä¼¯æ¨ç‰¹æ•°æ®é›†ä¸­çš„åå…«ç§é˜¿æ‹‰ä¼¯æ–¹è¨€è¿›è¡Œåˆ†ç±»çš„é—®é¢˜ã€‚é€šè¿‡æç¤ºå·¥ç¨‹æŠ€æœ¯åˆ›å»ºå¹¶æµ‹è¯•äº†å¾ªç¯ç¥ç»ç½‘ç»œæ¨¡å‹ã€Transformeræ¨¡å‹ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å…¶ä¸­ï¼ŒMARBERTv2è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡ä¸ºç™¾åˆ†ä¹‹å…­åäº”ï¼ŒF1åˆ†æ•°ä¸ºç™¾åˆ†ä¹‹å…­åå››ã€‚é€šè¿‡è¿ç”¨æœ€å…ˆè¿›çš„é¢„å¤„ç†æŠ€æœ¯å’Œæœ€æ–°çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼Œæœ¬æ–‡ç¡®å®šäº†é˜¿æ‹‰ä¼¯æ–¹è¨€è¯†åˆ«ä¸­æœ€å…³é”®çš„è¯­è¨€é—®é¢˜ã€‚ç»“æœè¯å®äº†ä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººå“åº”ç”¨æˆ·æ–¹è¨€ã€ç¤¾äº¤åª’ä½“ç›‘æ§ä»¥åŠæé«˜é˜¿æ‹‰ä¼¯ç¤¾åŒºçš„è®¿é—®é‡ç­‰åº”ç”¨ç¨‹åºçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19753v2">PDF</a> Email Typo Update</p>
<p><strong>Summary</strong></p>
<p>é˜¿æ‹‰ä¼¯è¯­è¨€æ˜¯ä¸–ç•Œä¸Šæœ€æµè¡Œçš„è¯­è¨€ä¹‹ä¸€ï¼Œæ‹¥æœ‰åœ¨22ä¸ªå›½å®¶ä½¿ç”¨çš„ä¼—å¤šæ–¹è¨€ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³QADIæ•°æ®é›†é˜¿æ‹‰ä¼¯æ¨ç‰¹æ–¹è¨€åˆ†ç±»çš„é—®é¢˜ã€‚åˆ›å»ºäº†å¹¶æµ‹è¯•äº†RNNæ¨¡å‹ã€Transformeræ¨¡å‹ä»¥åŠé€šè¿‡æç¤ºå·¥ç¨‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚å…¶ä¸­ï¼ŒMARBERTv2çš„è¡¨ç°æœ€ä½³ï¼Œå‡†ç¡®ç‡å’ŒF1åˆ†æ•°åˆ†åˆ«ä¸º65%å’Œ64%ã€‚é€šè¿‡ä½¿ç”¨æœ€æ–°çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹å’ŒæŠ€æœ¯ï¼Œæœ¬ç ”ç©¶ç¡®å®šäº†é˜¿æ‹‰ä¼¯æ–¹è¨€è¯†åˆ«ä¸­æœ€æ˜¾è‘—çš„è¯­è¨€é—®é¢˜ã€‚å…¶ç»“æœè¯å®äº†ä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººã€ç¤¾äº¤åª’ä½“ç›‘æ§å’Œå¢å¼ºé˜¿æ‹‰ä¼¯ç¤¾åŒºå¯åŠæ€§çš„åº”ç”¨ç¨‹åºä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿æ‹‰ä¼¯è¯­è¨€åœ¨å…¨çƒå…·æœ‰å¹¿æ³›çš„å½±å“åŠ›ï¼Œå­˜åœ¨å¤šç§æ–¹è¨€ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸“æ³¨äºQADIæ•°æ®é›†çš„18ç§é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€åˆ†ç±»é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨äº†RNNæ¨¡å‹ã€Transformeræ¨¡å‹å’Œé€šè¿‡æç¤ºå·¥ç¨‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè§£å†³å°è¯•ã€‚</li>
<li>MARBERTv2æ¨¡å‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œå‡†ç¡®ç‡å’ŒF1åˆ†æ•°åˆ†åˆ«ä¸º65%å’Œ64%ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å…ˆè¿›çš„é¢„å¤„ç†æŠ€æœ¯å’Œæœ€æ–°çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶æŒ‡å‡ºäº†é˜¿æ‹‰ä¼¯æ–¹è¨€è¯†åˆ«ä¸­æœ€é‡è¦çš„è¯­è¨€å­¦é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53e2b56a25403420f924c615f566e98c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-baf5b71f259aa88b98ca70fb81f805bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd1fbfbef681402fce3c5cd9210ef516.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b517abceca84845cdecafd94f769ee86.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5711d5e7fa85e7d1a25d63fadeb14c58.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Benchmarking-the-Pedagogical-Knowledge-of-Large-Language-Models"><a href="#Benchmarking-the-Pedagogical-Knowledge-of-Large-Language-Models" class="headerlink" title="Benchmarking the Pedagogical Knowledge of Large Language Models"></a>Benchmarking the Pedagogical Knowledge of Large Language Models</h2><p><strong>Authors:Maxime LeliÃ¨vre, Amy Waldock, Meng Liu, Natalia ValdÃ©s Aspillaga, Alasdair Mackintosh, MarÃ­a JosÃ© Ogando Portela, Jared Lee, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod</strong></p>
<p>Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AIâ€™s knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing modelsâ€™ understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at <a target="_blank" rel="noopener" href="https://rebrand.ly/pedagogy">https://rebrand.ly/pedagogy</a> which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure modelsâ€™ capacities to understand pedagogical concepts, respond appropriately to learnersâ€™ needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions. </p>
<blockquote>
<p>åƒå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰è¿™æ ·çš„åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½åœ¨ä¸åŒé¢†åŸŸçš„çŸ¥è¯†å’Œèƒ½åŠ›æ–¹é¢å‘æŒ¥äº†è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨å†…å®¹çŸ¥è¯†ä¸Šï¼Œåœ¨è¯„ä¼°æ¨¡å‹å¯¹æ•™å­¦æ–¹æ³•çš„ç†è§£æ–¹é¢å­˜åœ¨é‡å¤§ç©ºç™½â€”â€”æ•™å­¦æ–¹æ³•å’Œå®è·µã€‚æœ¬æ–‡ä»‹ç»äº†ã€Šæ•™å­¦åŸºå‡†æµ‹è¯•ã€‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨åŸŸæ•™å­¦çŸ¥è¯†ï¼ˆCDPKï¼‰å’Œç‰¹æ®Šæ•™è‚²éœ€æ±‚ä¸æ®‹ç–¾ï¼ˆSENDï¼‰æ•™å­¦çŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›ã€‚è¿™äº›åŸºå‡†æµ‹è¯•å»ºç«‹åœ¨ä»æ•™å¸ˆèŒä¸šå‘å±•è€ƒè¯•ä¸­ç²¾å¿ƒæŒ‘é€‰çš„é—®é¢˜é›†ä¸Šï¼Œæ¶µç›–äº†ä¸€ç³»åˆ—æ•™å­¦å­åŸŸï¼Œå¦‚æ•™å­¦ç­–ç•¥å’Œè¯„ä¼°æ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¦‚è¿°äº†è¿™äº›åŸºå‡†æµ‹è¯•çš„æ–¹æ³•è®ºå’Œå‘å±•ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†97ä¸ªæ¨¡å‹çš„ç»“æœï¼Œåœ¨æ•™å­¦çŸ¥è¯†é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ä»28%åˆ°89%ä¸ç­‰ã€‚æˆ‘ä»¬è€ƒè™‘äº†æˆæœ¬ä¸å‡†ç¡®ç‡ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ç»˜åˆ¶äº†éšæ—¶é—´æ¨ç§»çš„å¸•ç´¯æ‰˜ä»·å€¼å‰æ²¿çš„è¿›å±•ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://rebrand.ly/pedagogy">https://rebrand.ly/pedagogy</a>æä¾›äº†åœ¨çº¿æ’è¡Œæ¦œï¼Œæ’è¡Œæ¦œä¼šæ›´æ–°æ–°çš„æ¨¡å‹ï¼Œå¹¶å¯æ ¹æ®å„ç§æ¨¡å‹å±æ€§è¿›è¡Œäº¤äº’æ¢ç´¢å’Œè¿‡æ»¤ï¼Œå¦‚æ¯ä»¤ç‰Œæˆæœ¬ä»¥åŠå¼€æ”¾ä¸å°é—­æƒé‡ï¼Œè¿˜å¯ä»¥æŸ¥çœ‹ä¸åŒç§‘ç›®çš„è¡¨ç°ã€‚å¤§å‹è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¹æ•™è‚²å’Œå¸®åŠ©åº”å¯¹å…¨çƒå­¦ä¹ å±æœºå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ä»¥æ•™è‚²ä¸ºé‡ç‚¹çš„åŸºå‡†æµ‹è¯•å¯¹äºè¡¡é‡æ¨¡å‹ç†è§£æ•™å­¦æ¦‚å¿µçš„èƒ½åŠ›ã€é€‚å½“å“åº”å­¦ä¹ è€…çš„éœ€æ±‚ä»¥åŠåœ¨å„ç§èƒŒæ™¯ä¸‹æ”¯æŒæœ‰æ•ˆçš„æ•™å­¦ç­–ç•¥è‡³å…³é‡è¦ã€‚å®ƒä»¬å¯¹äºåœ¨æ•™è‚²ç¯å¢ƒä¸­è´Ÿè´£ä»»å’ŒåŸºäºè¯æ®åœ°éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒLLMå·¥å…·ï¼Œä»¥åŠæŒ‡å¯¼å¼€å‘å’Œæ”¿ç­–å†³ç­–æ˜¯å¿…è¦çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18710v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åä¸ºâ€œæ•™è‚²åŸºå‡†æµ‹è¯•â€çš„æ–°æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„è·¨å­¦ç§‘æ•™è‚²çŸ¥è¯†ï¼ˆCDPKï¼‰ä»¥åŠç‰¹æ®Šæ•™è‚²å’Œæ®‹ç–¾æ•™è‚²çš„æ•™è‚²çŸ¥è¯†ã€‚è¯¥åŸºå‡†æµ‹è¯•å»ºç«‹åœ¨æ•™å¸ˆèŒä¸šå‘å±•è€ƒè¯•é—®é¢˜çš„ç²¾é€‰é›†ä¸Šï¼Œæ¶µç›–äº†æ•™å­¦æ–¹æ³•ã€è¯„ä¼°æ–¹æ³•ç­‰æ•™å­¦å­é¢†åŸŸã€‚æ–‡ç« æ¦‚è¿°äº†è¿™äº›åŸºå‡†æµ‹è¯•çš„æ–¹æ³•è®ºå’Œå‘å±•ï¼ŒæŠ¥å‘Šäº†97ä¸ªæ¨¡å‹çš„å‡†ç¡®æ€§ï¼ŒèŒƒå›´ä»28%åˆ°89%ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ç§å…³ç³»ç ”ç©¶ï¼Œæ­ç¤ºäº†æˆæœ¬å’Œå‡†ç¡®æ€§ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶éšç€æ—¶é—´æ¨ç§»å±•ç¤ºäº†å¸•ç´¯æ‰˜ä»·å€¼å‰æ²¿çš„è¿›å±•ã€‚æ–‡ç« å¼ºè°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¹æ•™è‚²çš„å·¨å¤§æ½œåŠ›åŠå…¶å¯¹å…¨çƒå­¦ä¹ å±æœºçš„æ½œåœ¨å½±å“ã€‚æ•™è‚²åŸºå‡†æµ‹è¯•å¯¹äºè¡¡é‡æ¨¡å‹ç†è§£æ•™è‚²æ¦‚å¿µçš„èƒ½åŠ›ã€é€‚å½“å“åº”å­¦ä¹ è€…éœ€æ±‚å’Œæ”¯æŒå„ç§èƒŒæ™¯ä¸‹çš„æœ‰æ•ˆæ•™å­¦å®è·µè‡³å…³é‡è¦ã€‚è¿™å¯¹äºåœ¨å­¦æœ¯ç¯å¢ƒä¸­è´Ÿè´£ä»»åœ°ã€æœ‰æ ¹æ®åœ°éƒ¨ç½²LLMä»¥åŠLLMå·¥å…·å¹¶å¼•å¯¼å¼€å‘å’Œæ”¿ç­–å†³ç­–å‡éå¸¸é‡è¦ã€‚å…·ä½“æ•°æ®å¯é€šè¿‡åœ¨çº¿é¢†å¯¼è€…æ¦œæŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://rebrand.ly/pedagogy%E3%80%82">https://rebrand.ly/pedagogyã€‚</a></p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹AIåœ¨è·¨é¢†åŸŸçŸ¥è¯†è¯„ä¼°ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œä½†ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨å†…å®¹çŸ¥è¯†ä¸Šï¼Œç¼ºä¹å¯¹æ•™å­¦æ–¹æ³•çš„ç†è§£è¯„ä¼°ã€‚</li>
<li>å¼•å…¥â€œæ•™è‚²åŸºå‡†æµ‹è¯•â€æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„è·¨å­¦ç§‘æ•™è‚²çŸ¥è¯†å’Œç‰¹æ®Šæ•™è‚²ä¸æ®‹ç–¾æ•™è‚²çŸ¥è¯†ã€‚</li>
<li>æ•°æ®é›†é€šè¿‡æ•™å¸ˆèŒä¸šå‘å±•è€ƒè¯•çš„é—®é¢˜æ„å»ºè€Œæˆï¼Œæ¶µç›–å¤šç§æ•™å­¦æ–¹æ³•å’Œè¯„ä¼°æ–¹æ³•ã€‚</li>
<li>å¯¹97ä¸ªæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå‡†ç¡®æ€§å·®å¼‚è¾ƒå¤§ï¼Œæ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²çŸ¥è¯†ç†è§£ä¸Šçš„ä¸åŒèƒ½åŠ›ã€‚</li>
<li>æä¾›äº†æˆæœ¬å’Œå‡†ç¡®æ€§ä¹‹é—´çš„å…³ç³»ç ”ç©¶ï¼Œå¹¶å±•ç¤ºäº†å¸•ç´¯æ‰˜ä»·å€¼å‰æ²¿çš„è¿›å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-009a6cfb6021917b105e6bad307aff67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15e1813648a35e69eafbf500776edd07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93cf348b124c0af8bc92c811d4a7c3dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af413a311489c9357a17bfe9e51cfa95.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TyphoFormer-Language-Augmented-Transformer-for-Accurate-Typhoon-Track-Forecasting"><a href="#TyphoFormer-Language-Augmented-Transformer-for-Accurate-Typhoon-Track-Forecasting" class="headerlink" title="TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track   Forecasting"></a>TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track   Forecasting</h2><p><strong>Authors:Lincan Li, Eren Erman Ozguven, Yue Zhao, Guang Wang, Yiqun Xie, Yushun Dong</strong></p>
<p>Accurate typhoon track forecasting is crucial for early system warning and disaster response. While Transformer-based models have demonstrated strong performance in modeling the temporal dynamics of dense trajectories of humans and vehicles in smart cities, they usually lack access to broader contextual knowledge that enhances the forecasting reliability of sparse meteorological trajectories, such as typhoon tracks. To address this challenge, we propose TyphoFormer, a novel framework that incorporates natural language descriptions as auxiliary prompts to improve typhoon trajectory forecasting. For each time step, we use Large Language Model (LLM) to generate concise textual descriptions based on the numerical attributes recorded in the North Atlantic hurricane database. The language descriptions capture high-level meteorological semantics and are embedded as auxiliary special tokens prepended to the numerical time series input. By integrating both textual and sequential information within a unified Transformer encoder, TyphoFormer enables the model to leverage contextual cues that are otherwise inaccessible through numerical features alone. Extensive experiments are conducted on HURDAT2 benchmark, results show that TyphoFormer consistently outperforms other state-of-the-art baseline methods, particularly under challenging scenarios involving nonlinear path shifts and limited historical observations. </p>
<blockquote>
<p>ç²¾ç¡®é¢„æµ‹å°é£è·¯å¾„å¯¹äºæ—©æœŸç³»ç»Ÿé¢„è­¦å’Œç¾å®³åº”å¯¹è‡³å…³é‡è¦ã€‚è™½ç„¶åŸºäºTransformerçš„æ¨¡å‹åœ¨æ¨¡æ‹Ÿæ™ºèƒ½åŸå¸‚ä¸­äººç±»å’Œè½¦è¾†çš„å¯†é›†è½¨è¿¹çš„æ—¶é—´åŠ¨æ€æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸æ— æ³•è·å–æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œè¿™æœ‰åŠ©äºæé«˜ç¨€ç–æ°”è±¡è½¨è¿¹çš„é¢„æµ‹å¯é æ€§ï¼Œä¾‹å¦‚å°é£è·¯å¾„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TyphFormerï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨è‡ªç„¶è¯­è¨€æè¿°ä½œä¸ºè¾…åŠ©æç¤ºæ¥æé«˜å°é£è½¨è¿¹é¢„æµ‹ã€‚å¯¹äºæ¯ä¸ªæ—¶é—´ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºäºåŒ—å¤§è¥¿æ´‹é£“é£æ•°æ®åº“ä¸­çš„æ•°å€¼å±æ€§ç”Ÿæˆç®€æ´çš„æ–‡æœ¬æè¿°ã€‚è¯­è¨€æè¿°æ•æ‰äº†é«˜çº§æ°”è±¡è¯­ä¹‰ï¼Œå¹¶è¢«åµŒå…¥ä½œä¸ºè¾…åŠ©ç‰¹æ®Šä»¤ç‰Œé™„åŠ åˆ°æ•°å€¼æ—¶é—´åºåˆ—è¾“å…¥ä¸­ã€‚é€šè¿‡å°†æ–‡æœ¬å’Œåºåˆ—ä¿¡æ¯é›†æˆåœ¨ä¸€ä¸ªç»Ÿä¸€çš„Transformerç¼–ç å™¨å†…ï¼ŒTyphFormerä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨æ— æ³•é€šè¿‡æ•°å€¼ç‰¹å¾è·å–çš„ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚åœ¨HURDAT2åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒTyphFormerå§‹ç»ˆä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠéçº¿æ€§è·¯å¾„å˜åŒ–å’Œæœ‰é™å†å²è§‚æµ‹çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17609v2">PDF</a> Short research paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTyphoFormerçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†è‡ªç„¶è¯­è¨€æè¿°ä½œä¸ºè¾…åŠ©æç¤ºæ¥æ”¹å–„å°é£è½¨è¿¹é¢„æŠ¥ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ¯ä¸€æ­¥ç”ŸæˆåŸºäºåŒ—å¤§è¥¿æ´‹é£“é£æ•°æ®åº“ä¸­æ•°å€¼å±æ€§çš„ç®€æ´æ–‡æœ¬æè¿°ï¼Œè¿™äº›æè¿°æ•æ‰åˆ°é«˜çº§æ°”è±¡è¯­ä¹‰å¹¶è¢«åµŒå…¥ä½œä¸ºè¾…åŠ©ç‰¹æ®Šç¬¦å·æ·»åŠ åˆ°æ•°å€¼æ—¶é—´åºåˆ—è¾“å…¥ä¸­ã€‚é€šè¿‡åœ¨ç»Ÿä¸€çš„Transformerç¼–ç å™¨ä¸­æ•´åˆæ–‡æœ¬å’Œæ—¶åºä¿¡æ¯ï¼ŒTyphoFormerä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨ä»…é€šè¿‡æ•°å€¼ç‰¹å¾æ— æ³•è·å¾—çš„ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚åœ¨HURDAT2åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTyphoFormerå§‹ç»ˆä¼˜äºå…¶ä»–å…ˆè¿›çš„åŸºç¡€æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠéçº¿æ€§è·¯å¾„å˜åŒ–å’Œæœ‰é™å†å²è§‚æµ‹çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°é£è½¨è¿¹å‡†ç¡®é¢„æŠ¥å¯¹æ—©æœŸé¢„è­¦ç³»ç»Ÿå’Œç¾å®³åº”å¯¹è‡³å…³é‡è¦ã€‚</li>
<li>Transformeræ¨¡å‹åœ¨æ™ºèƒ½åŸå¸‚äººç±»å’Œè½¦è¾†å¯†é›†è½¨è¿¹çš„æ—¶é—´åŠ¨æ€å»ºæ¨¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨æ°”è±¡è½¨è¿¹é¢„æŠ¥ä¸Šç¼ºä¹å¹¿æ³›çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶TyphoFormerï¼Œç»“åˆè‡ªç„¶è¯­è¨€æè¿°ä½œä¸ºè¾…åŠ©æç¤ºæ¥æ”¹å–„å°é£è½¨è¿¹é¢„æŠ¥ã€‚</li>
<li>TyphoFormerä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”ŸæˆåŸºäºæ•°å€¼å±æ€§çš„æ–‡æœ¬æè¿°ï¼Œæ•æ‰é«˜çº§æ°”è±¡è¯­ä¹‰ã€‚</li>
<li>TyphoFormerå°†æ–‡æœ¬å’Œæ—¶åºä¿¡æ¯æ•´åˆåœ¨ç»Ÿä¸€çš„Transformerç¼–ç å™¨ä¸­ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡çº¿ç´¢æé«˜é¢„æŠ¥å¯é æ€§ã€‚</li>
<li>åœ¨HURDAT2åŸºå‡†æµ‹è¯•ä¸Šï¼ŒTyphoFormerç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ–¹æ³•è¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4ea904418177c54744958eedf077c02b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f91dc869fea8c2244f4006bfbfe9a256.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-233a492f88ca14033b6bd23c9efb83e2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Privacy-Preserving-LLM-Interaction-with-Socratic-Chain-of-Thought-Reasoning-and-Homomorphically-Encrypted-Vector-Databases"><a href="#Privacy-Preserving-LLM-Interaction-with-Socratic-Chain-of-Thought-Reasoning-and-Homomorphically-Encrypted-Vector-Databases" class="headerlink" title="Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought   Reasoning and Homomorphically Encrypted Vector Databases"></a>Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought   Reasoning and Homomorphically Encrypted Vector Databases</h2><p><strong>Authors:Yubeen Bae, Minchan Kim, Jaejin Lee, Sangbum Kim, Jaehyung Kim, Yejin Choi, Niloofar Mireshghallah</strong></p>
<p>Large language models (LLMs) are increasingly used as personal agents, accessing sensitive user data such as calendars, emails, and medical records. Users currently face a trade-off: They can send private records, many of which are stored in remote databases, to powerful but untrusted LLM providers, increasing their exposure risk. Alternatively, they can run less powerful models locally on trusted devices. We bridge this gap. Our Socratic Chain-of-Thought Reasoning first sends a generic, non-private user query to a powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and detailed sub-queries without accessing user data. Next, we embed these sub-queries and perform encrypted sub-second semantic search using our Homomorphically Encrypted Vector Database across one million entries of a single userâ€™s private data. This represents a realistic scale of personal documents, emails, and records accumulated over years of digital activity. Finally, we feed the CoT prompt and the decrypted records to a local language model and generate the final response. On the LoCoMo long-context QA benchmark, our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model, outperforms using GPT-4o alone by up to 7.1 percentage points. This demonstrates a first step toward systems where tasks are decomposed and split between untrusted strong LLMs and weak local ones, preserving user privacy. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨ä½œä¸ªäººä»£ç†ï¼Œè®¿é—®ç”¨æˆ·çš„æ•æ„Ÿæ•°æ®ï¼Œå¦‚æ—¥å†ã€ç”µå­é‚®ä»¶å’ŒåŒ»ç–—è®°å½•ã€‚ç›®å‰ï¼Œç”¨æˆ·é¢ä¸´ä¸€ç§æƒè¡¡ï¼šä»–ä»¬å¯ä»¥å°†ç§äººè®°å½•ï¼ˆå…¶ä¸­è®¸å¤šå­˜å‚¨åœ¨è¿œç¨‹æ•°æ®åº“ä¸­ï¼‰å‘é€åˆ°åŠŸèƒ½å¼ºå¤§ä½†ä¸å—ä¿¡ä»»çš„LLMæä¾›å•†ï¼Œä»è€Œå¢åŠ å…¶æš´éœ²é£é™©ã€‚æˆ–è€…ï¼Œä»–ä»¬å¯ä»¥åœ¨å¯ä¿¡çš„è®¾å¤‡ä¸Šè¿è¡ŒåŠŸèƒ½è¾ƒå¼±çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¼¥è¡¥äº†è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬çš„è‹æ ¼æ‹‰åº•æ€ç»´é“¾æ¨ç†é¦–å…ˆå‘åŠŸèƒ½å¼ºå¤§ã€ä¸å—ä¿¡ä»»çš„å¤§å‹è¯­è¨€æ¨¡å‹å‘é€é€šç”¨çš„éç§äººç”¨æˆ·æŸ¥è¯¢ï¼Œè¯¥æ¨¡å‹ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºå’Œè¯¦ç»†çš„å­æŸ¥è¯¢ï¼Œæ— éœ€è®¿é—®ç”¨æˆ·æ•°æ®ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™äº›å­æŸ¥è¯¢åµŒå…¥å…¶ä¸­ï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬çš„åŒæ€åŠ å¯†å‘é‡æ•°æ®åº“åœ¨ç”¨æˆ·ç§äººæ•°æ®çš„ä¸€ç™¾ä¸‡ä¸ªæ¡ç›®ä¸Šæ‰§è¡ŒåŠ å¯†çš„å­ç§’è¯­ä¹‰æœç´¢ã€‚è¿™ä»£è¡¨äº†åœ¨å¤šå¹´çš„æ•°å­—æ´»åŠ¨ä¸­ç§¯ç´¯çš„ä¸ªäººæ–‡æ¡£ã€ç”µå­é‚®ä»¶å’Œè®°å½•çš„ç°å®è§„æ¨¡ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ€ç»´é“¾æç¤ºå’Œè§£å¯†çš„è®°å½•æä¾›ç»™æœ¬åœ°è¯­è¨€æ¨¡å‹ï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆå“åº”ã€‚åœ¨LoCoMoé•¿æ–‡æœ¬é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬æ··åˆæ¡†æ¶ç»“åˆäº†GPT-4oå’Œæœ¬åœ°Llama-3.2-1Bæ¨¡å‹ï¼Œæ¯”ä»…ä½¿ç”¨GPT-4oé«˜å‡º7.1ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™æœç€ä»»åŠ¡åœ¨ä¸å—ä¿¡ä»»çš„å¼ºå¤§LLMå’Œè„†å¼±çš„æœ¬åœ°LLMä¹‹é—´è¿›è¡Œåˆ†è§£å’Œåˆ†å‰²çš„ç³»ç»Ÿè¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ï¼ŒåŒæ—¶ä¿æŠ¤äº†ç”¨æˆ·éšç§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17336v2">PDF</a> 29 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä¸ªäººä»£ç†ï¼Œè®¿é—®ç”¨æˆ·æ•æ„Ÿæ•°æ®å¦‚æ—¥å†ã€ç”µå­é‚®ä»¶å’ŒåŒ»ç–—è®°å½•ã€‚ç”¨æˆ·é¢ä¸´æƒè¡¡ï¼šå¯ä»¥å°†ç§æœ‰è®°å½•å‘é€åˆ°å¼ºå¤§çš„ä½†ä¸å¯ä¿¡çš„LLMæä¾›å•†ï¼Œæˆ–å°†è®°å½•ä¿å­˜åœ¨æœ¬åœ°å¯ä¿¡è®¾å¤‡ä¸Šã€‚æˆ‘ä»¬çš„è‹æ ¼æ‹‰åº•æ€ç»´é“¾æŠ€æœ¯è§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚è¯¥æŠ€æœ¯é¦–å…ˆå‘å¼ºå¤§çš„ä¸å¯ä¿¡çš„LLMå‘é€é€šç”¨éç§æœ‰ç”¨æˆ·æŸ¥è¯¢ï¼Œç”Ÿæˆæ€ç»´é“¾æç¤ºå’Œè¯¦ç»†çš„å­æŸ¥è¯¢ï¼Œæ— éœ€è®¿é—®ç”¨æˆ·æ•°æ®ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›å­æŸ¥è¯¢åµŒå…¥åˆ°æˆ‘ä»¬çš„åŒæ€åŠ å¯†å‘é‡æ•°æ®åº“ä¸­ï¼Œå¹¶åœ¨å•ä¸ªç”¨æˆ·çš„ç§äººæ•°æ®çš„ä¸€ç™¾ä¸‡æ¡è®°å½•ä¸­è¿›è¡ŒåŠ å¯†å­ç§’è¯­ä¹‰æœç´¢ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ€ç»´é“¾æç¤ºå’Œè§£å¯†çš„è®°å½•æä¾›ç»™æœ¬åœ°è¯­è¨€æ¨¡å‹ï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆå“åº”ã€‚åœ¨LoCoMoé•¿æœŸä¸Šä¸‹æ–‡é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ··åˆæ¡†æ¶ç»“åˆGPT-4oå’Œæœ¬åœ°Llama-3.2-1Bæ¨¡å‹ï¼Œæ¯”ä»…ä½¿ç”¨GPT-4oé«˜å‡º7.1ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™å±•ç¤ºäº†ä»»åŠ¡åœ¨ä¸å¯ä¿¡çš„å¼ºå¤§çš„LLMå’Œå¼±æœ¬åœ°LLMä¹‹é—´åˆ†è§£å’Œåˆ†å‰²çš„ç¬¬ä¸€æ­¥ï¼ŒåŒæ—¶ä¿æŠ¤ç”¨æˆ·éšç§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsè¢«ç”¨ä½œä¸ªäººä»£ç†ï¼Œèƒ½å¤Ÿè®¿é—®ç”¨æˆ·çš„æ•æ„Ÿæ•°æ®ã€‚</li>
<li>ç”¨æˆ·é¢ä¸´å°†ç§æœ‰è®°å½•å‘é€åˆ°å¼ºå¤§çš„ä½†ä¸å¯ä¿¡çš„LLMæä¾›å•†çš„é£é™©ï¼Œæˆ–é€‰æ‹©è¿è¡Œè¾ƒå¼±çš„æœ¬åœ°æ¨¡å‹ä»¥ç»´æŠ¤éšç§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‹æ ¼æ‹‰åº•æ€ç»´é“¾æŠ€æœ¯ï¼Œé€šè¿‡å‘å¼ºå¤§çš„LLMå‘é€é€šç”¨æŸ¥è¯¢å¹¶ç”Ÿæˆæ€ç»´é“¾æç¤ºå’Œå­æŸ¥è¯¢æ¥å¹³è¡¡éšç§å’Œæ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨åŒæ€åŠ å¯†æŠ€æœ¯ä¿æŠ¤ç”¨æˆ·æ•°æ®ï¼Œè¿›è¡Œè¯­ä¹‰æœç´¢ã€‚</li>
<li>æ··åˆæ¡†æ¶ç»“åˆäº†è¿œç¨‹å’Œæœ¬åœ°LLMçš„ä¼˜åŠ¿ï¼Œæé«˜äº†æ€§èƒ½å¹¶ä¿æŠ¤äº†ç”¨æˆ·éšç§ã€‚</li>
<li>åœ¨LoCoMoåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ··åˆæ¡†æ¶çš„æ€§èƒ½è¶…è¿‡äº†ä»…ä½¿ç”¨è¿œç¨‹LLMçš„æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17336">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1cb21d8acda60b62154794317cbc86f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Beyond-Attention-or-Similarity-Maximizing-Conditional-Diversity-for-Token-Pruning-in-MLLMs"><a href="#Beyond-Attention-or-Similarity-Maximizing-Conditional-Diversity-for-Token-Pruning-in-MLLMs" class="headerlink" title="Beyond Attention or Similarity: Maximizing Conditional Diversity for   Token Pruning in MLLMs"></a>Beyond Attention or Similarity: Maximizing Conditional Diversity for   Token Pruning in MLLMs</h2><p><strong>Authors:Qizhe Zhang, Mengzhen Liu, Lichen Li, Ming Lu, Yuan Zhang, Junwen Pan, Qi She, Shanghang Zhang</strong></p>
<p>In multimodal large language models (MLLMs), the length of input visual tokens is often significantly greater than that of their textual counterparts, leading to a high inference cost. Many works aim to address this issue by removing redundant visual tokens. However, current approaches either rely on attention-based pruning, which retains numerous duplicate tokens, or use similarity-based pruning, overlooking the instruction relevance, consequently causing suboptimal performance. In this paper, we go beyond attention or similarity by proposing a novel visual token pruning method named CDPruner, which maximizes the conditional diversity of retained tokens. We first define the conditional similarity between visual tokens conditioned on the instruction, and then reformulate the token pruning problem with determinantal point process (DPP) to maximize the conditional diversity of the selected subset. The proposed CDPruner is training-free and model-agnostic, allowing easy application to various MLLMs. Extensive experiments across diverse MLLMs show that CDPruner establishes new state-of-the-art on various vision-language benchmarks. By maximizing conditional diversity through DPP, the selected subset better represents the input images while closely adhering to user instructions, thereby preserving strong performance even with high reduction ratios. When applied to LLaVA, CDPruner reduces FLOPs by 95% and CUDA latency by 78%, while maintaining 94% of the original accuracy. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Theia-4869/CDPruner">https://github.com/Theia-4869/CDPruner</a>. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ï¼Œè¾“å…¥è§†è§‰æ ‡è®°çš„é•¿åº¦å¾€å¾€è¿œé•¿äºæ–‡æœ¬æ ‡è®°çš„é•¿åº¦ï¼Œå¯¼è‡´æ¨ç†æˆæœ¬å¾ˆé«˜ã€‚è®¸å¤šç ”ç©¶æ—¨åœ¨é€šè¿‡æ¶ˆé™¤å†—ä½™çš„è§†è§‰æ ‡è®°æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•è¦ä¹ˆä¾èµ–äºåŸºäºæ³¨æ„åŠ›çš„ä¿®å‰ªï¼Œè¿™ç§æ–¹æ³•ä¼šä¿ç•™å¤§é‡é‡å¤çš„æ ‡è®°ï¼Œè¦ä¹ˆä½¿ç”¨åŸºäºç›¸ä¼¼åº¦çš„ä¿®å‰ªï¼Œå¿½è§†äº†æŒ‡ä»¤çš„ç›¸å…³æ€§ï¼Œä»è€Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¶…è¶Šæ³¨æ„åŠ›å’Œç›¸ä¼¼åº¦çš„æ–°å‹è§†è§‰æ ‡è®°ä¿®å‰ªæ–¹æ³•ï¼Œåä¸ºCDPrunerï¼Œå®ƒæ—¨åœ¨æœ€å¤§åŒ–ä¿ç•™æ ‡è®°çš„æ¡ä»¶å¤šæ ·æ€§ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰äº†åŸºäºæŒ‡ä»¤çš„è§†è§‰æ ‡è®°ä¹‹é—´çš„æ¡ä»¶ç›¸ä¼¼æ€§ï¼Œç„¶åä½¿ç”¨ç¡®å®šæ€§è¿‡ç¨‹ï¼ˆDPPï¼‰é‡æ–°è¡¨è¿°æ ‡è®°ä¿®å‰ªé—®é¢˜ï¼Œä»¥æœ€å¤§åŒ–æ‰€é€‰å­é›†çš„æ¡ä»¶å¤šæ ·æ€§ã€‚æ‰€æå‡ºçš„CDPruneræ— éœ€è®­ç»ƒä¸”æ¨¡å‹æ— å…³ï¼Œå¯è½»æ¾åº”ç”¨äºå„ç§MLLMsã€‚åœ¨å¤šç§MLLMsä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCDPruneråœ¨å„é¡¹è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šåˆ›ä¸‹äº†æœ€æ–°è®°å½•ã€‚é€šè¿‡DPPæœ€å¤§åŒ–æ¡ä»¶å¤šæ ·æ€§ï¼Œæ‰€é€‰å­é›†èƒ½æ›´å¥½åœ°ä»£è¡¨è¾“å…¥å›¾åƒï¼ŒåŒæ—¶ç´§å¯†éµå¾ªç”¨æˆ·æŒ‡ä»¤ï¼Œå³ä½¿åœ¨é«˜åº¦å‡å°‘çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒå¼ºå¤§çš„æ€§èƒ½ã€‚å°†CDPruneråº”ç”¨äºLLaVAæ—¶ï¼Œå¯å‡å°‘95%çš„FLOPså’Œ78%çš„CUDAå»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒ94%çš„åŸå§‹å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Theia-4869/CDPruner%E3%80%82">https://github.com/Theia-4869/CDPrunerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10967v2">PDF</a> 22 pages, 5 figures, code: <a target="_blank" rel="noopener" href="https://github.com/Theia-4869/CDPruner">https://github.com/Theia-4869/CDPruner</a>,   project page: <a target="_blank" rel="noopener" href="https://theia-4869.github.io/CDPruner">https://theia-4869.github.io/CDPruner</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCDPrunerçš„è§†è§‰ç¬¦å·ä¿®å‰ªæ–¹æ³•ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä¿ç•™ç¬¦å·çš„æ¡ä»¶å¤šæ ·æ€§ä»¥è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­è¾“å…¥è§†è§‰ç¬¦å·é•¿åº¦è¿‡é•¿å¯¼è‡´çš„é«˜æ¨ç†æˆæœ¬é—®é¢˜ã€‚è¯¥æ–¹æ³•å®šä¹‰æŒ‡ä»¤æ¡ä»¶ä¸‹çš„è§†è§‰ç¬¦å·é—´æ¡ä»¶ç›¸ä¼¼æ€§ï¼Œå¹¶ä½¿ç”¨è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPï¼‰é‡æ–°è¡¨è¿°ç¬¦å·ä¿®å‰ªé—®é¢˜ï¼Œä»¥æœ€å¤§åŒ–æ‰€é€‰å­é›†çš„æ¡ä»¶å¤šæ ·æ€§ã€‚CDPruneræ–¹æ³•æ— éœ€è®­ç»ƒå’Œæ¨¡å‹ç‰¹å®šï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§MLLMsã€‚å®éªŒè¡¨æ˜ï¼ŒCDPruneråœ¨å¤šç§è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚é€šè¿‡æœ€å¤§åŒ–æ¡ä»¶å¤šæ ·æ€§ï¼Œæ‰€é€‰å­é›†èƒ½æ›´å¥½åœ°ä»£è¡¨è¾“å…¥å›¾åƒå¹¶ä¸¥æ ¼éµå¾ªç”¨æˆ·æŒ‡ä»¤ï¼Œåœ¨é«˜ç¼©å‡ç‡ä¸‹ä»ä¿æŒè‰¯å¥½æ€§èƒ½ã€‚åº”ç”¨äºLLaVAæ—¶ï¼ŒCDPrunerå°†æµ®ç‚¹è¿ç®—å‡å°‘95%ï¼ŒCUDAå»¶è¿Ÿå‡å°‘78%ï¼ŒåŒæ—¶ä¿æŒ94%çš„åŸå§‹å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´è¾“å…¥è§†è§‰ç¬¦å·é•¿åº¦è¿‡é•¿å¯¼è‡´çš„é«˜æ¨ç†æˆæœ¬é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡å»é™¤å†—ä½™è§†è§‰ç¬¦å·æ¥è§£å†³æ­¤é—®é¢˜ï¼Œä½†å­˜åœ¨ä¿ç•™è¿‡å¤šé‡å¤ç¬¦å·æˆ–å¿½è§†æŒ‡ä»¤ç›¸å…³æ€§çš„é—®é¢˜ã€‚</li>
<li>CDPruneræ–¹æ³•æ—¨åœ¨æœ€å¤§åŒ–ä¿ç•™ç¬¦å·çš„æ¡ä»¶å¤šæ ·æ€§ï¼Œé€šè¿‡å®šä¹‰æŒ‡ä»¤æ¡ä»¶ä¸‹çš„è§†è§‰ç¬¦å·é—´æ¡ä»¶ç›¸ä¼¼æ€§æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹ï¼ˆDPPï¼‰é‡æ–°è¡¨è¿°ç¬¦å·ä¿®å‰ªé—®é¢˜ï¼Œä½¿æ–¹æ³•èƒ½å¤Ÿæ›´å®¹æ˜“åœ°åº”ç”¨äºå„ç§MLLMsã€‚</li>
<li>CDPruneræ–¹æ³•åœ¨å¤šç§è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿåœ¨é«˜ç¼©å‡ç‡ä¸‹ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>åº”ç”¨äºLLaVAæ—¶ï¼ŒCDPrunerèƒ½æ˜¾è‘—å‡å°‘æµ®ç‚¹è¿ç®—å’ŒCUDAå»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7c26b47bcd284a4e183d865ea837d24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-847badb8da0daa52ad6186866570b4dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45e2878ebca8f2e6b8aaec9a41dbb7fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41da0335f18516dd6c57eeb496574ad4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a166bd06126ff11a8ed63e7b5caec8ed.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Prompt-Guided-Latent-Diffusion-with-Predictive-Class-Conditioning-for-3D-Prostate-MRI-Generation"><a href="#Prompt-Guided-Latent-Diffusion-with-Predictive-Class-Conditioning-for-3D-Prostate-MRI-Generation" class="headerlink" title="Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D   Prostate MRI Generation"></a>Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D   Prostate MRI Generation</h2><p><strong>Authors:Emerson P. Grabke, Masoom A. Haider, Babak Taati</strong></p>
<p>Objective: Latent diffusion models (LDM) could alleviate data scarcity challenges affecting machine learning development for medical imaging. However, medical LDM strategies typically rely on short-prompt text encoders, non-medical LDMs, or large data volumes. These strategies can limit performance and scientific accessibility. We propose a novel LDM conditioning approach to address these limitations. Methods: We propose Class-Conditioned Efficient Large Language model Adapter (CCELLA), a novel dual-head conditioning approach that simultaneously conditions the LDM U-Net with free-text clinical reports and radiology classification. We also propose a data-efficient LDM framework centered around CCELLA and a proposed joint loss function. We first evaluate our method on 3D prostate MRI against state-of-the-art. We then augment a downstream classifier model training dataset with synthetic images from our method. Results: Our method achieves a 3D FID score of 0.025 on a size-limited 3D prostate MRI dataset, significantly outperforming a recent foundation model with FID 0.071. When training a classifier for prostate cancer prediction, adding synthetic images generated by our method during training improves classifier accuracy from 69% to 74%. Training a classifier solely on our methodâ€™s synthetic images achieved comparable performance to training on real images alone. Conclusion: We show that our method improved both synthetic image quality and downstream classifier performance using limited data and minimal human annotation. Significance: The proposed CCELLA-centric framework enables radiology report and class-conditioned LDM training for high-quality medical image synthesis given limited data volume and human data annotation, improving LDM performance and scientific accessibility. Code from this study will be available at <a target="_blank" rel="noopener" href="https://github.com/grabkeem/CCELLA">https://github.com/grabkeem/CCELLA</a> </p>
<blockquote>
<p>ç›®æ ‡ï¼šæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å¯ä»¥ç¼“è§£åŒ»å­¦æˆåƒé¢†åŸŸæœºå™¨å­¦ä¹ å¼€å‘ä¸­çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ã€‚ç„¶è€Œï¼ŒåŒ»å­¦LDMç­–ç•¥é€šå¸¸ä¾èµ–äºçŸ­æç¤ºæ–‡æœ¬ç¼–ç å™¨ã€éåŒ»å­¦LDMæˆ–å¤§é‡æ•°æ®ã€‚è¿™äº›ç­–ç•¥å¯èƒ½ä¼šé™åˆ¶æ€§èƒ½å’Œç§‘å­¦å¯åŠæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„LDMæ¡ä»¶å¤„ç†æ–¹æ³•æ¥è§£å†³è¿™äº›é™åˆ¶ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºClass-Conditioned Efficient Large Languageæ¨¡å‹é€‚é…å™¨ï¼ˆCCELLAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŒå¤´æ¡ä»¶å¤„ç†æ–¹æ³•ï¼Œå¯ä»¥åŒæ—¶ä½¿ç”¨è‡ªç”±æ–‡æœ¬ä¸´åºŠæŠ¥å‘Šå’Œæ”¾å°„å­¦åˆ†ç±»å¯¹LDM U-Netè¿›è¡Œæ¡ä»¶å¤„ç†ã€‚æˆ‘ä»¬è¿˜å›´ç»•CCELLAå’Œä¸€ä¸ªæå‡ºçš„è”åˆæŸå¤±å‡½æ•°æ„å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„æ•°æ®LDMæ¡†æ¶ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨æœ€å…ˆè¿›çš„3Då‰åˆ—è…ºMRIæ•°æ®è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¯¥æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒæ‰©å……ä¸‹æ¸¸åˆ†ç±»å™¨æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ã€‚</p>
<p>ç»“æœï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§„æ¨¡æœ‰é™çš„å‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šå®ç°äº†0.025çš„3D FIDå¾—åˆ†ï¼Œæ˜¾è‘—ä¼˜äºå…·æœ‰FID 0.071çš„æœ€æ–°åŸºç¡€æ¨¡å‹ã€‚åœ¨è®­ç»ƒå‰åˆ—è…ºç™Œé¢„æµ‹åˆ†ç±»å™¨æ—¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ æˆ‘ä»¬æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒå°†åˆ†ç±»å™¨çš„å‡†ç¡®åº¦ä»69%æé«˜åˆ°74%ã€‚ä»…ä½¿ç”¨æˆ‘ä»¬æ–¹æ³•çš„åˆæˆå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œå…¶æ€§èƒ½ä¸ä»…ä½¿ç”¨çœŸå®å›¾åƒè¿›è¡Œè®­ç»ƒç›¸å½“ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10230v2">PDF</a> MAH and BT are co-senior authors on the work. This work has been   submitted to the IEEE for possible publication</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨åŒ»ç–—å½±åƒæœºå™¨å­¦ä¹ å¼€å‘ä¸­é¢ä¸´çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„LDMæ¡ä»¶åŒ–æ–¹æ³•ã€‚é€šè¿‡Class-Conditioned Efficient Large Language model Adapterï¼ˆCCELLAï¼‰è¿™ä¸€æ–°å‹åŒå¤´æ¡ä»¶åŒ–æ–¹æ³•ï¼ŒåŒæ—¶ä½¿ç”¨è‡ªç”±æ–‡æœ¬ä¸´åºŠæŠ¥å‘Šå’Œæ”¾å°„å­¦åˆ†ç±»å¯¹LDM U-Netè¿›è¡Œæ¡ä»¶æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†ä»¥CCELLAä¸ºæ ¸å¿ƒçš„æ•°æ®é«˜æ•ˆLDMæ¡†æ¶å’Œè”åˆæŸå¤±å‡½æ•°ã€‚ç ”ç©¶é¦–å…ˆå¯¹ä¸‰ç»´å‰åˆ—è…ºMRIå›¾åƒè¿›è¡Œäº†è¯„ä»·å¹¶ä¸å½“å‰å…ˆè¿›æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒã€‚ç„¶åï¼Œä½¿ç”¨æœ¬æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒæ‰©å……ä¸‹æ¸¸åˆ†ç±»å™¨æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–¹æ³•åœ¨è§„æ¨¡æœ‰é™çš„ä¸‰ç»´å‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šå®ç°äº†0.025çš„3D FIDå¾—åˆ†ï¼Œæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºç¡€æ¨¡å‹çš„0.071 FIDå¾—åˆ†ã€‚åœ¨è®­ç»ƒå‰åˆ—è…ºç™Œé¢„æµ‹åˆ†ç±»å™¨æ—¶ï¼Œé€šè¿‡æœ¬æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œå°†åˆ†ç±»å™¨çš„å‡†ç¡®æ€§ä»69%æé«˜åˆ°74%ã€‚ä»…ä½¿ç”¨æœ¬æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒè¿›è¡Œè®­ç»ƒçš„åˆ†ç±»å™¨ï¼Œå…¶æ€§èƒ½ä¸ä»…ä½¿ç”¨çœŸå®å›¾åƒè¿›è¡Œè®­ç»ƒç›¸å½“ã€‚æ€»ä¹‹ï¼Œæœ¬ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æœ‰é™æ•°æ®å’Œæœ€å°‘äººå·¥æ³¨é‡Šï¼Œæœ¬æ–¹æ³•èƒ½åŒæ—¶æé«˜åˆæˆå›¾åƒè´¨é‡å’Œä¸‹æ¸¸åˆ†ç±»å™¨æ€§èƒ½ã€‚æ‰€æå‡ºçš„ä»¥CCELLAä¸ºä¸­å¿ƒçš„æ–¹æ³•æ¡†æ¶å®ç°äº†ä»¥æœ‰é™æ•°æ®å’Œæ³¨é‡Šè¿›è¡Œæ”¾å°„å­¦æŠ¥å‘Šå’Œç±»åˆ«æ¡ä»¶åŒ–çš„LDMè®­ç»ƒï¼Œæé«˜äº†é«˜è´¨é‡åŒ»å­¦å›¾åƒåˆæˆçš„èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/grabkeem/CCELLA">é“¾æ¥</a>è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰æ¡ä»¶åŒ–æ–¹æ³•ï¼Œåä¸ºClass-Conditioned Efficient Large Language model Adapterï¼ˆCCELLAï¼‰ï¼Œç”¨äºè§£å†³åŒ»å­¦æˆåƒä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>CCELLAç»“åˆäº†è‡ªç”±æ–‡æœ¬ä¸´åºŠæŠ¥å‘Šå’Œæ”¾å°„å­¦åˆ†ç±»çš„åŒé‡å¤´æ¡ä»¶åŒ–æ–¹æ³•ï¼Œæé«˜äº†LDMçš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ•°æ®é«˜æ•ˆLDMæ¡†æ¶å’Œè”åˆæŸå¤±å‡½æ•°ï¼Œç”¨äºæé«˜åˆæˆå›¾åƒçš„è´¨é‡å’Œä¸‹æ¸¸åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸‰ç»´å‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ¬æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶å®ç°äº†è¾ƒé«˜çš„å›¾åƒç”Ÿæˆè´¨é‡ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨æœ¬æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†ä¸‹æ¸¸åˆ†ç±»å™¨çš„å‡†ç¡®æ€§ã€‚</li>
<li>æœ¬æ–¹æ³•ä½¿ç”¨æœ‰é™æ•°æ®å’Œæœ€å°‘äººå·¥æ³¨é‡Šï¼Œè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œè¡¨æ˜å…¶åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­å…·æœ‰å®é™…åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fa153fde8db063073e73121572a1cbcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33ec0c5d1da94b2a27041ce179d44a8b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Learning-from-Videos-for-3D-World-Enhancing-MLLMs-with-3D-Vision-Geometry-Priors"><a href="#Learning-from-Videos-for-3D-World-Enhancing-MLLMs-with-3D-Vision-Geometry-Priors" class="headerlink" title="Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision   Geometry Priors"></a>Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision   Geometry Priors</h2><p><strong>Authors:Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang</strong></p>
<p>Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Birdâ€™s-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations. </p>
<blockquote>
<p>å…ˆå‰çš„ç ”ç©¶å·²ç»æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å°†3Dåœºæ™¯è§£é‡Šä¸ºè§†é¢‘æ–¹é¢çš„åº”ç”¨ã€‚è¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºå…¨é¢çš„3Dæ•°æ®è¾“å…¥ï¼Œå¦‚ç‚¹äº‘æˆ–é‡å»ºçš„é¸Ÿç°å›¾ï¼ˆBEVï¼‰åœ°å›¾ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹MLLMsè¿›è¡Œæ”¹è¿›ï¼Œæé«˜äº†å…¶ç›´æ¥ä»è§†é¢‘æ•°æ®ç†è§£å¹¶åœ¨3Dç©ºé—´ä¸­æ¨ç†çš„èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„3Dè¾“å…¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–è€Œé«˜æ•ˆçš„æ–¹æ³•â€”â€”è§†é¢‘3Då‡ ä½•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVG LLMï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ä¸ª3Dè§†è§‰å‡ ä½•ç¼–ç å™¨ï¼Œå®ƒä»è§†é¢‘åºåˆ—ä¸­æå–3Då…ˆéªŒä¿¡æ¯ã€‚è¿™äº›ä¿¡æ¯ä¸è§†è§‰ä»¤ç‰Œç›¸ç»“åˆå¹¶è¾“å…¥åˆ°MLLMä¸­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¶‰åŠ3Dåœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†çš„å„ç§ä»»åŠ¡ä¸­éƒ½å–å¾—äº†å®è´¨æ€§çš„æ”¹è¿›ï¼Œè¿™äº›æ”¹è¿›éƒ½ç›´æ¥ä»è§†é¢‘æºä¸­å­¦ä¹ å¾—åˆ°ã€‚ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸ä¾èµ–æ˜ç¡®çš„3Dæ•°æ®è¾“å…¥ï¼Œå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¯„ä¼°ç»“æœï¼Œä¸å…¶ä»–ç°æœ‰çš„å…ˆè¿›æ–¹æ³•ç›¸æ¯”å…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›ï¼Œç”šè‡³åœ¨VSI-Benchè¯„ä¼°ä¸­è¶…è¿‡äº†Gemini-Pro 1.5ç‰ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24625v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£ä¸‰ç»´åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¼ ç»Ÿçš„MLLMséœ€è¦å¤§é‡ä¸‰ç»´æ•°æ®è¾“å…¥å¦‚ç‚¹äº‘æˆ–é‡å»ºé¸Ÿç°å›¾ç­‰ã€‚ä½†æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„åŸºäºè§†é¢‘æ•°æ®ç†è§£ä¸‰ç»´ç©ºé—´çš„æ–¹æ³•ï¼Œåä¸ºè§†é¢‘ä¸‰ç»´å‡ ä½•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVG LLMï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰ç»´è§†è§‰å‡ ä½•ç¼–ç å™¨æå–è§†é¢‘åºåˆ—ä¸­çš„ä¸‰ç»´å…ˆéªŒä¿¡æ¯ï¼Œå¹¶ä¸è§†è§‰ä»¤ç‰Œç»“åˆè¾“å…¥åˆ°MLLMä¸­ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ç»´åœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä¸”ç›´æ¥ä»è§†é¢‘æºå­¦ä¹ ã€‚åœ¨ä¸ä¾èµ–æ˜¾å¼ä¸‰ç»´æ•°æ®è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œæœ¬æ–‡çš„4Bæ¨¡å‹åœ¨VSI-Benchè¯„ä¼°ä¸­å–å¾—äº†ä¸ç°æœ‰å…ˆè¿›æŠ€æœ¯æ–¹æ³•ç«äº‰çš„ç»“æœï¼Œç”šè‡³è¶…è¶Šäº†Gemini-1.5-Proã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¹¿æ³›åº”ç”¨äºä¸‰ç»´åœºæ™¯ç†è§£ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå…¨é¢çš„ä¸‰ç»´æ•°æ®è¾“å…¥å¦‚ç‚¹äº‘æˆ–é¸Ÿç°å›¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†é¢‘ä¸‰ç»´å‡ ä½•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVG LLMï¼‰ï¼Œç›´æ¥ä»è§†é¢‘æ•°æ®ç†è§£ä¸‰ç»´ç©ºé—´ã€‚</li>
<li>VG LLMé‡‡ç”¨ä¸‰ç»´è§†è§‰å‡ ä½•ç¼–ç å™¨æå–è§†é¢‘ä¸­çš„ä¸‰ç»´å…ˆéªŒä¿¡æ¯ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ä¸‰ç»´åœºæ™¯ç†è§£å’Œç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šå–å¾—æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>ä¸ä¾èµ–æ˜¾å¼ä¸‰ç»´æ•°æ®è¾“å…¥ï¼Œ4Bæ¨¡å‹è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œç”šè‡³è¶…è¿‡äº†æŸäº›ç°æœ‰å…ˆè¿›æŠ€æœ¯æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4aeae131dff908335a0d0a06d965be3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aeaeaf3cad1c0601636b1c74a1dd8a5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d0432485ec90cd0f136d8957782b13d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Program-of-Equations-Thoughts-to-Solve-Algebra-Word-Problems"><a href="#Program-of-Equations-Thoughts-to-Solve-Algebra-Word-Problems" class="headerlink" title="Program of Equations Thoughts to Solve Algebra Word Problems"></a>Program of Equations Thoughts to Solve Algebra Word Problems</h2><p><strong>Authors:Yunze Lin</strong></p>
<p>Solving algebraic word problems (AWPs) has recently emerged as an important natural language processing task. Recently, large language models (LLMs) have demonstrated powerful mathematical capabilities, and the Chain-of-Thought technique, which guides LLMs through step-by-step reasoning, has yielded impressive results. However, this reasoning ability is limited by the computational weaknesses of LLMs themselves, where calculation errors can accumulate, leading to incorrect final answers. To address this, we propose Program of Equations Thoughts (POET), which transforms the task of generating step-by-step reasoning answers into a two-stage task of predicting equations and generating code, offloading complex computations to a Python interpreter to avoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which utilizes a manually designed template to enable LLMs to directly generate Python code for one-step solving. Our method achieves accuracies of 95.3% and 98.0% on the PEN and ALG514 datasets, respectively, setting a new state-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5% on the DRAW-1K dataset. </p>
<blockquote>
<p>è§£å†³ä»£æ•°æ–‡å­—é¢˜ï¼ˆAWPsï¼‰æœ€è¿‘æˆä¸ºä¸€ä¸ªé‡è¦çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç¤ºäº†å¼ºå¤§çš„æ•°å­¦èƒ½åŠ›ï¼Œè€Œâ€œæ€ç»´é“¾â€æŠ€æœ¯é€šè¿‡é€æ­¥æ¨ç†å¼•å¯¼LLMï¼Œå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™ç§æ¨ç†èƒ½åŠ›å—åˆ°LLMè‡ªèº«è®¡ç®—å¼±ç‚¹çš„é™åˆ¶ï¼Œè®¡ç®—é”™è¯¯å¯èƒ½ç´¯ç§¯ï¼Œå¯¼è‡´æœ€ç»ˆç­”æ¡ˆé”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ–¹ç¨‹ç¨‹åºæ€ç»´ï¼ˆPOETï¼‰ï¼Œå°†ç”Ÿæˆé€æ­¥æ¨ç†ç­”æ¡ˆçš„ä»»åŠ¡è½¬å˜ä¸ºé¢„æµ‹æ–¹ç¨‹å’Œç”Ÿæˆä»£ç çš„ä¸¤é˜¶æ®µä»»åŠ¡ï¼Œå°†å¤æ‚è®¡ç®—å¸è½½ç»™Pythonè§£é‡Šå™¨ï¼Œä»¥é¿å…LLMä¸­çš„è®¡ç®—é”™è¯¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†é›¶æ ·æœ¬POETï¼Œå®ƒåˆ©ç”¨æ‰‹åŠ¨è®¾è®¡çš„æ¨¡æ¿ï¼Œä½¿LLMèƒ½å¤Ÿç›´æ¥ç”Ÿæˆä¸€æ­¥è§£å†³çš„Pythonä»£ç ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨PENå’ŒALG514æ•°æ®é›†ä¸Šåˆ†åˆ«è¾¾åˆ°äº†95.3%å’Œ98.0%çš„å‡†ç¡®ç‡ï¼Œåˆ›é€ äº†æ–°çš„æœ€å…ˆè¿›çš„è®°å½•ã€‚é›¶æ ·æœ¬POETåœ¨DRAW-1Kæ•°æ®é›†ä¸Šä¹Ÿè¾¾åˆ°äº†95.5%çš„æœ€å…ˆè¿›ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20170v2">PDF</a> Withdrawn pending institutional authorization and core revisions to   address methodological inconsistencies in Sections 3-4</p>
<p><strong>Summary</strong></p>
<p>è§£å†³ä»£æ•°å­—é—®é¢˜ï¼ˆAWPsï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„é‡è¦ä»»åŠ¡ä¹‹ä¸€ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦èƒ½åŠ›ï¼Œä¸”é“¾å¼æ€ç»´æ³•èƒ½é€šè¿‡é€æ­¥æ¨ç†å¾—åˆ°ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œç”±äºLLMè‡ªèº«çš„è®¡ç®—å¼±ç‚¹ï¼Œå…¶æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œè®¡ç®—é”™è¯¯å¯èƒ½ç´¯ç§¯å¯¼è‡´æœ€ç»ˆç­”æ¡ˆé”™è¯¯ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºæ–¹ç¨‹å¼ç¨‹åºæ€ç»´ï¼ˆPOETï¼‰æ–¹æ³•ï¼Œå°†ç”Ÿæˆé€æ­¥æ¨ç†ç­”æ¡ˆçš„ä»»åŠ¡è½¬åŒ–ä¸ºé¢„æµ‹æ–¹ç¨‹å¼å’Œç”Ÿæˆä»£ç çš„ä¸¤é˜¶æ®µä»»åŠ¡ï¼Œå°†å¤æ‚è®¡ç®—äº¤ç”±Pythonè§£é‡Šå™¨æ‰§è¡Œï¼Œé¿å…åœ¨LLMä¸­å‡ºç°è®¡ç®—é”™è¯¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºé›¶æ ·æœ¬POETæ–¹æ³•ï¼Œåˆ©ç”¨äººå·¥è®¾è®¡çš„æ¨¡æ¿ä½¿LLMèƒ½ç›´æ¥ç”Ÿæˆä¸€æ­¥æ±‚è§£çš„Pythonä»£ç ã€‚è¯¥æ–¹æ³•åœ¨PENå’ŒALG514æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°95.3%å’Œ98.0%ï¼Œåˆ›é€ äº†æ–°çš„æœ€å¥½æˆç»©ã€‚é›¶æ ·æœ¬POETåœ¨DRAW-1Kæ•°æ®é›†ä¸Šä¹Ÿè¾¾åˆ°äº†95.5%çš„æœ€ä½³æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§£å†³ä»£æ•°å­—é—®é¢˜æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„é‡è¦ä»»åŠ¡ä¹‹ä¸€ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å¼ºå¤§çš„æ•°å­¦èƒ½åŠ›ï¼Œä½†å­˜åœ¨è®¡ç®—å¼±ç‚¹ã€‚</li>
<li>é“¾å¼æ€ç»´æ³•é€šè¿‡é€æ­¥æ¨ç†å–å¾—æ˜¾è‘—æˆæœã€‚</li>
<li>æ–¹ç¨‹å¼ç¨‹åºæ€ç»´ï¼ˆPOETï¼‰æ–¹æ³•å°†ä»»åŠ¡è½¬åŒ–ä¸ºé¢„æµ‹æ–¹ç¨‹å¼å’Œç”Ÿæˆä»£ç çš„ä¸¤é˜¶æ®µä»»åŠ¡ã€‚</li>
<li>POETæ–¹æ³•å°†å¤æ‚è®¡ç®—äº¤ç»™Pythonè§£é‡Šå™¨æ‰§è¡Œï¼Œé¿å…åœ¨LLMä¸­çš„è®¡ç®—é”™è¯¯ã€‚</li>
<li>é›¶æ ·æœ¬POETæ–¹æ³•åˆ©ç”¨äººå·¥è®¾è®¡çš„æ¨¡æ¿ä½¿LLMèƒ½ç›´æ¥ç”Ÿæˆä¸€æ­¥æ±‚è§£çš„Pythonä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20170">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-14da5b544a6e01dfccd39a3a18dd447f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09b7828c9b32afafcd7772e602d0804d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72e82b23e978a47c2800c607c65e582b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96174021544e73edf15abe1e17c1ad03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47b30a4ff914de705cc022b85fe30c9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd657888767ade163ecf8e1e19bffd59.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AudioTrust-Benchmarking-the-Multifaceted-Trustworthiness-of-Audio-Large-Language-Models"><a href="#AudioTrust-Benchmarking-the-Multifaceted-Trustworthiness-of-Audio-Large-Language-Models" class="headerlink" title="AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large   Language Models"></a>AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large   Language Models</h2><p><strong>Authors:Kai Li, Can Shen, Yile Liu, Jirui Han, Kelong Zheng, Xuechao Zou, Zhe Wang, Xingjian Du, Shun Zhang, Hanjun Luo, Yingbin Jin, Xinxin Xing, Ziyang Ma, Yue Liu, Xiaojun Jia, Yifan Zhang, Junfeng Fang, Kun Wang, Yibo Yan, Haoyang Li, Yiming Li, Xiaobin Zhuang, Yang Liu, Haibo Hu, Zhizheng Wu, Xiaolin Hu, Eng-Siong Chng, XiaoFeng Wang, Wenyuan Xu, Wei Dong, Xinfeng Li</strong></p>
<p>The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio&#x2F;text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at <a target="_blank" rel="noopener" href="https://github.com/JusperLee/AudioTrust">https://github.com/JusperLee/AudioTrust</a>. </p>
<blockquote>
<p>éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆALLMï¼‰çš„å¿«é€Ÿè¿›æ­¥å’Œåº”ç”¨çš„ä¸æ–­æ‰©å±•ï¼Œè¦æ±‚å¯¹å…¶å¯é æ€§æœ‰ä¸¥æ ¼çš„ç†è§£ã€‚ç„¶è€Œï¼Œå…³äºè¯„ä¼°è¿™äº›æ¨¡å‹çš„ç ”ç©¶ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹éŸ³é¢‘æ¨¡æ€ç‰¹æœ‰çš„é£é™©ï¼Œä»ç„¶è¿œè¿œæ²¡æœ‰å¾—åˆ°å……åˆ†çš„æ¢ç´¢ã€‚ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸»è¦å…³æ³¨æ–‡æœ¬æ¨¡æ€æˆ–åªè§£å†³æœ‰é™çš„å®‰å…¨ç»´åº¦ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘åˆ°éŸ³é¢‘æ¨¡æ€çš„å›ºæœ‰ç‰¹æ€§å’Œåº”ç”¨åœºæ™¯ã€‚æˆ‘ä»¬å¼•å…¥äº†AudioTrustâ€”â€”é¦–ä¸ªä¸“ä¸ºALLMè®¾è®¡çš„å¤šæ–¹é¢å¯é æ€§è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ã€‚AudioTrustä¿ƒè¿›äº†å¯¹å…­ä¸ªå…³é”®ç»´åº¦çš„è¯„ä¼°ï¼šå…¬å¹³æ€§ã€å¹»è§‰ã€å®‰å…¨æ€§ã€éšç§ã€ç¨³å¥æ€§å’Œè®¤è¯ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°è¿™äº›ç»´åº¦ï¼ŒAudioTrustå›´ç»•18ä¸ªç‹¬ç‰¹çš„å®éªŒè®¾ç½®æ„å»ºã€‚å…¶æ ¸å¿ƒæ˜¯ç²¾å¿ƒæ„å»ºçš„æ•°æ®é›†ï¼ŒåŒ…å«4420ä¸ªéŸ³é¢‘&#x2F;æ–‡æœ¬æ ·æœ¬ï¼Œå–è‡ªçœŸå®åœºæ™¯ï¼ˆä¾‹å¦‚æ—¥å¸¸å¯¹è¯ã€ç´§æ€¥å‘¼å«ã€è¯­éŸ³åŠ©æ‰‹äº¤äº’ï¼‰ï¼Œä¸“é—¨ç”¨äºæ¢æµ‹ALLMçš„å¤šæ–¹é¢å¯é æ€§ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œè¯¥åŸºå‡†æµ‹è¯•ç²¾å¿ƒè®¾è®¡äº†9ä¸ªéŸ³é¢‘ç‰¹å®šè¯„ä¼°æŒ‡æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§è§„æ¨¡è‡ªåŠ¨åŒ–ç®¡é“å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œå®¢è§‚å’Œå¯æ‰©å±•çš„è¯„åˆ†ã€‚å®éªŒç»“æœæ­ç¤ºäº†å½“å‰æœ€å…ˆè¿›çš„å¼€æºå’Œé—­æºALLMåœ¨é¢å¯¹å„ç§é«˜é£é™©éŸ³é¢‘åœºæ™¯æ—¶çš„å¯é æ€§è¾¹ç•Œå’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥éŸ³é¢‘æ¨¡å‹çš„å®‰å…¨å’Œå¯é éƒ¨ç½²æä¾›äº†å®è´µçš„è§è§£ã€‚æˆ‘ä»¬çš„å¹³å°å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JusperLee/AudioTrust">https://github.com/JusperLee/AudioTrust</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.16211v2">PDF</a> Technical Report</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆALLMï¼‰çš„å¿«é€Ÿè¿›æ­¥å’Œåº”ç”¨çš„ä¸æ–­æ‹“å±•éœ€è¦å¯¹å…¶å¯é æ€§è¿›è¡Œä¸¥æ ¼çš„ç†è§£ã€‚ç„¶è€Œï¼Œé’ˆå¯¹è¿™äº›æ¨¡å‹çš„è¯„ä¼°ç ”ç©¶ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹éŸ³é¢‘æ¨¡æ€ç‰¹æœ‰é£é™©çš„è¯„ä¼°ï¼Œä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«æ¢ç´¢ã€‚ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ä¸»è¦å…³æ³¨æ–‡æœ¬æ¨¡æ€æˆ–ä»…è§£å†³æœ‰é™çš„å®‰å…¨ç»´åº¦ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘åˆ°éŸ³é¢‘æ¨¡æ€çš„å›ºæœ‰ç‰¹æ€§å’Œåº”ç”¨åœºæ™¯çš„ç‹¬ç‰¹æ€§ã€‚æˆ‘ä»¬æ¨å‡ºAudioTrustï¼Œè¿™æ˜¯é¦–ä¸ªä¸“ä¸ºALLMè®¾è®¡çš„å¤šå…ƒåŒ–å¯é æ€§è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ã€‚AudioTrusté€šè¿‡å…­ä¸ªå…³é”®ç»´åº¦è¿›è¡Œè¯„ä¼°ï¼šå…¬å¹³æ€§ã€å¹»è§‰ã€å®‰å…¨æ€§ã€éšç§ã€ç¨³å¥æ€§å’Œèº«ä»½éªŒè¯ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°è¿™äº›ç»´åº¦ï¼ŒAudioTrustå›´ç»•18ä¸ªç‹¬ç‰¹çš„å®éªŒè®¾ç½®æ„å»ºã€‚å…¶æ ¸å¿ƒæ˜¯ç²¾å¿ƒæ„å»ºçš„æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡4420ä¸ªéŸ³é¢‘&#x2F;æ–‡æœ¬æ ·æœ¬ï¼Œå–è‡ªçœŸå®åœºæ™¯ï¼ˆå¦‚æ—¥å¸¸å¯¹è¯ã€ç´§æ€¥å‘¼å«ã€è¯­éŸ³åŠ©æ‰‹äº¤äº’ï¼‰ï¼Œä¸“ä¸ºæ¢æµ‹ALLMçš„å¤šå…ƒåŒ–å¯é æ€§ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œè¯¥åŸºå‡†æµ‹è¯•ç²¾å¿ƒè®¾è®¡äº†9ä¸ªéŸ³é¢‘ç‰¹å®šè¯„ä¼°æŒ‡æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§è§„æ¨¡è‡ªåŠ¨åŒ–ç®¡é“å¯¹æ¨¡å‹è¾“å‡ºçš„å®¢è§‚å’Œå¯é‡åŒ–çš„è¯„åˆ†ã€‚å®éªŒç»“æœæ­ç¤ºäº†å½“å‰å¼€æºå’Œé—­æºALLMåœ¨é¢å¯¹å„ç§é«˜é£é™©éŸ³é¢‘åœºæ™¯æ—¶çš„å¯é æ€§è¾¹ç•Œå’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥éŸ³é¢‘æ¨¡å‹çš„å¯é éƒ¨ç½²æä¾›äº†å®è´µè§è§£ã€‚æˆ‘ä»¬çš„å¹³å°å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨[é“¾æ¥åœ°å€]æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Audio Large Language Models (ALLMs)çš„å¯é æ€§è¯„ä¼°è‡³å…³é‡è¦ï¼Œä½†é’ˆå¯¹éŸ³é¢‘æ¨¡æ€ç‰¹æœ‰é£é™©çš„è¯„ä¼°ä»å¾…æ¢ç´¢ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ¡†æ¶ä¸»è¦å…³æ³¨æ–‡æœ¬æ¨¡æ€æˆ–æœ‰é™çš„å®‰å…¨ç»´åº¦ï¼Œå¿½ç•¥äº†éŸ³é¢‘æ¨¡æ€çš„ç‹¬ç‰¹ç‰¹æ€§å’Œåº”ç”¨åœºæ™¯ã€‚</li>
<li>AudioTrustæ˜¯é¦–ä¸ªä¸“ä¸ºALLMè®¾è®¡çš„å¤šå…ƒåŒ–å¯é æ€§è¯„ä¼°æ¡†æ¶å’ŒåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å…¬å¹³æ€§ã€å¹»è§‰ã€å®‰å…¨æ€§ã€éšç§ã€ç¨³å¥æ€§å’Œèº«ä»½éªŒè¯ç­‰å…­ä¸ªå…³é”®ç»´åº¦ã€‚</li>
<li>AudioTrustä½¿ç”¨18ä¸ªç‹¬ç‰¹çš„å®éªŒè®¾ç½®è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå¹¶ç²¾å¿ƒæ„å»ºäº†åŒ…å«çœŸå®åœºæ™¯éŸ³é¢‘&#x2F;æ–‡æœ¬æ ·æœ¬çš„æ•°æ®é›†ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬9ä¸ªéŸ³é¢‘ç‰¹å®šè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶é‡‡ç”¨å¤§è§„æ¨¡è‡ªåŠ¨åŒ–ç®¡é“è¿›è¡Œæ¨¡å‹è¾“å‡ºçš„è¯„åˆ†ã€‚</li>
<li>å®éªŒç»“æœæ­ç¤ºäº†å½“å‰ALLMåœ¨é¢å¯¹é«˜é£é™©éŸ³é¢‘åœºæ™¯æ—¶çš„å¯é æ€§å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.16211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-965a3c6c3182e37cf0fb1d8561887476.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54dda451559d9c58e799f8a262e64784.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d346566c2faa744dfd923d0964abbe3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63c7c018d38cdd0a2c9fd1f3d8429317.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59513d5791662d028a87b020cb84cc40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-628cedea84d9c16b05cd0b4e5b33f78e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Reasoning-by-Superposition-A-Theoretical-Perspective-on-Chain-of-Continuous-Thought"><a href="#Reasoning-by-Superposition-A-Theoretical-Perspective-on-Chain-of-Continuous-Thought" class="headerlink" title="Reasoning by Superposition: A Theoretical Perspective on Chain of   Continuous Thought"></a>Reasoning by Superposition: A Theoretical Perspective on Chain of   Continuous Thought</h2><p><strong>Authors:Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, Yuandong Tian</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable performance in many applications, including challenging reasoning problems via chain-of-thoughts (CoTs) techniques that generate &#96;&#96;thinking tokensâ€™â€™ before answering the questions. While existing theoretical works demonstrate that CoTs with discrete tokens boost the capability of LLMs, recent work on continuous CoTs lacks a theoretical understanding of why it outperforms discrete counterparts in various reasoning tasks such as directed graph reachability, a fundamental graph reasoning problem that includes many practical domain applications as special cases. In this paper, we prove that a two-layer transformer with $D$ steps of continuous CoTs can solve the directed graph reachability problem, where $D$ is the diameter of the graph, while the best known result of constant-depth transformers with discrete CoTs requires $O(n^2)$ decoding steps where $n$ is the number of vertices ($D&lt;n$). In our construction, each continuous thought vector is a superposition state that encodes multiple search frontiers simultaneously (i.e., parallel breadth-first search (BFS)), while discrete CoTs must choose a single path sampled from the superposition state, which leads to sequential search that requires many more steps and may be trapped into local solutions. We also performed extensive experiments to verify that our theoretical construction aligns well with the empirical solution obtained via training dynamics. Notably, encoding of multiple search frontiers as a superposition state automatically emerges in training continuous CoTs, without explicit supervision to guide the model to explore multiple paths simultaneously. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šåº”ç”¨ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é€šè¿‡æ€ç»´é“¾ï¼ˆCoTsï¼‰æŠ€æœ¯è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†é—®é¢˜ï¼Œåœ¨å›ç­”é—®é¢˜ä¹‹å‰ç”Ÿæˆâ€œæ€ç»´æ ‡è®°â€ã€‚è™½ç„¶ç°æœ‰çš„ç†è®ºå·¥ä½œè¡¨æ˜ï¼Œä½¿ç”¨ç¦»æ•£æ ‡è®°çš„CoTså¯ä»¥å¢å¼ºLLMçš„èƒ½åŠ›ï¼Œä½†å…³äºè¿ç»­CoTsçš„æœ€æ–°å·¥ä½œç¼ºä¹ç†è®ºç†è§£ï¼Œå³åœ¨æœ‰å‘å›¾å¯è¾¾æ€§ç­‰å„ç§æ¨ç†ä»»åŠ¡ä¸­ï¼Œä¸ºä»€ä¹ˆå®ƒä¼˜äºç¦»æ•£åŒè¡Œã€‚æœ‰å‘å›¾å¯è¾¾æ€§æ˜¯ä¸€ä¸ªåŸºæœ¬çš„å›¾å½¢æ¨ç†é—®é¢˜ï¼ŒåŒ…æ‹¬è®¸å¤šå®é™…åº”ç”¨é¢†åŸŸçš„ç‰¹æ®Šæƒ…å†µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸€ä¸ªä¸¤å±‚å˜å‹å™¨é€šè¿‡è¿ç»­CoTsçš„Dæ­¥å¯ä»¥è§£å†³æœ‰å‘å›¾å¯è¾¾æ€§é—®é¢˜ï¼Œå…¶ä¸­Dæ˜¯å›¾çš„ç›´å¾„ï¼Œè€Œç¦»æ•£CoTsçš„å·²çŸ¥æœ€ä½³ç»“æœçš„å¸¸æ•°æ·±åº¦å˜å‹å™¨éœ€è¦O(n^2)ä¸ªè§£ç æ­¥éª¤ï¼Œå…¶ä¸­næ˜¯é¡¶ç‚¹æ•°ï¼ˆD&lt;nï¼‰ã€‚åœ¨æˆ‘ä»¬çš„æ„å»ºä¸­ï¼Œæ¯ä¸ªè¿ç»­çš„æ€ç»´å‘é‡éƒ½æ˜¯ä¸€ä¸ªå åŠ æ€ï¼Œå¯ä»¥åŒæ—¶ç¼–ç å¤šä¸ªæœç´¢å‰æ²¿ï¼ˆå³å¹¶è¡Œå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰ï¼‰ï¼Œè€Œç¦»æ•£CoTså¿…é¡»é€‰æ‹©ä»å åŠ æ€ä¸­é‡‡æ ·çš„å•ä¸ªè·¯å¾„ï¼Œä»è€Œå¯¼è‡´éœ€è¦æ›´å¤šæ­¥éª¤çš„é¡ºåºæœç´¢ï¼Œå¹¶å¯èƒ½é™·å…¥å±€éƒ¨è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„ç†è®ºæ„å»ºä¸é€šè¿‡è®­ç»ƒåŠ¨åŠ›å­¦è·å¾—çš„ç»éªŒè§£å†³æ–¹æ¡ˆä¹‹é—´çš„è‰¯å¥½å¯¹é½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è®­ç»ƒè¿ç»­CoTsè¿‡ç¨‹ä¸­ï¼Œæ— éœ€æ˜ç¡®ç›‘ç£å³å¯è‡ªåŠ¨å‡ºç°å°†å¤šä¸ªæœç´¢å‰æ²¿ç¼–ç ä¸ºå åŠ æ€çš„æƒ…å†µï¼Œè¿™æŒ‡å¯¼æ¨¡å‹åŒæ—¶æ¢ç´¢å¤šæ¡è·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12514v2">PDF</a> 26 pages, 7 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆCoTsï¼‰æŠ€æœ¯å±•ç°å‡ºå“è¶Šçš„åº”ç”¨æ€§èƒ½ï¼Œè¯¥æŠ€æœ¯èƒ½åœ¨å›ç­”é—®é¢˜å‰ç”Ÿæˆâ€œæ€è€ƒä»¤ç‰Œâ€ã€‚å°½ç®¡ç¦»æ•£ä»¤ç‰Œçš„CoTsç†è®ºå·²ç»è¯æ˜äº†å…¶æå‡LLMæ€§èƒ½çš„èƒ½åŠ›ï¼Œä½†å¯¹äºè¿ç»­CoTsåœ¨è¯¸å¦‚å®šå‘å›¾å¯è¾¾æ€§ä¹‹ç±»çš„æ¨ç†ä»»åŠ¡ä¸­ä¸ºä½•èƒ½è¶…è¶Šç¦»æ•£CoTsçš„ç†è®ºç†è§£ä»ç„¶ç¼ºä¹ã€‚æœ¬æ–‡è¯æ˜äº†ä¸¤å±‚transformeré€šè¿‡Dæ­¥è¿ç»­CoTså¯ä»¥è§£å†³å®šå‘å›¾å¯è¾¾æ€§é—®é¢˜ï¼Œå…¶ä¸­Dä¸ºå›¾çš„ç›´å¾„ï¼Œè€Œç¦»æ•£CoTsçš„å·²çŸ¥æœ€ä½³ç»“æœéœ€è¦O(n^2)è§£ç æ­¥éª¤ï¼Œå…¶ä¸­nä¸ºé¡¶ç‚¹æ•°ä¸”D&lt;nã€‚åœ¨æˆ‘ä»¬çš„æ„å»ºä¸­ï¼Œæ¯ä¸ªè¿ç»­çš„æ€ç»´å‘é‡éƒ½æ˜¯ç¼–ç äº†å¤šä¸ªæœç´¢å‰æ²¿çš„å åŠ æ€ï¼ˆå³å¹¶è¡Œå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼‰ï¼Œè€Œç¦»æ•£CoTså¿…é¡»ä»å åŠ æ€ä¸­é€‰æ‹©å•ä¸€è·¯å¾„ï¼Œå¯¼è‡´éœ€è¦æ›´å¤šæ­¥éª¤çš„åºåˆ—æœç´¢ï¼Œå¹¶å¯èƒ½é™·å…¥å±€éƒ¨è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒéªŒè¯äº†ç†è®ºæ„å»ºä¸é€šè¿‡è®­ç»ƒåŠ¨æ€è·å¾—çš„ç»éªŒè§£å†³æ–¹æ¡ˆçš„ä¸€è‡´æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è®­ç»ƒè¿ç»­CoTsè¿‡ç¨‹ä¸­ï¼Œç¼–ç å¤šä¸ªæœç´¢å‰æ²¿çš„å åŠ æ€ä¼šè‡ªåŠ¨å‡ºç°ï¼Œæ— éœ€æ˜ç¡®ç›‘ç£æ¥æŒ‡å¯¼æ¨¡å‹åŒæ—¶æ¢ç´¢å¤šæ¡è·¯å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ€ç»´é“¾ï¼ˆCoTsï¼‰æŠ€æœ¯è§£å†³æ¨ç†é—®é¢˜ã€‚</li>
<li>ç°æœ‰ç†è®ºè¯æ˜ç¦»æ•£ä»¤ç‰Œçš„CoTså¯ä»¥æå‡LLMæ€§èƒ½ã€‚</li>
<li>è¿ç»­CoTsåœ¨è§£å†³å®šå‘å›¾å¯è¾¾æ€§ç­‰é—®é¢˜ä¸Šè¡¨ç°å‡ºè¶…è¶Šç¦»æ•£CoTsçš„æ€§èƒ½ã€‚</li>
<li>ä¸¤å±‚transformeré€šè¿‡è¿ç»­CoTsçš„Dæ­¥ï¼ˆDä¸ºå›¾ç›´å¾„ï¼‰èƒ½è§£å†³å®šå‘å›¾å¯è¾¾æ€§é—®é¢˜ã€‚</li>
<li>ç¦»æ•£CoTséœ€è¦æ›´å¤šçš„è§£ç æ­¥éª¤æ¥è§£å†³åŒæ ·çš„é—®é¢˜ã€‚</li>
<li>è¿ç»­CoTsçš„æ¯ä¸ªæ€ç»´å‘é‡èƒ½ç¼–ç å¤šä¸ªæœç´¢å‰æ²¿çš„å åŠ æ€ï¼Œå®ç°å¹¶è¡Œæœç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-358bc4991e42e1f01dcd64d6f479c742.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3ba3101064cad0058560df95f34c248.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d52b9346f498b5109a03cc68cbf06d7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1925bbc90b98e577800cb23125d17d4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="T2I-R1-Reinforcing-Image-Generation-with-Collaborative-Semantic-level-and-Token-level-CoT"><a href="#T2I-R1-Reinforcing-Image-Generation-with-Collaborative-Semantic-level-and-Token-level-CoT" class="headerlink" title="T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT"></a>T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level   and Token-level CoT</h2><p><strong>Authors:Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, Hongsheng Li</strong></p>
<p>Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/CaraJ7/T2I-R1">https://github.com/CaraJ7/T2I-R1</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›å±•å±•ç¤ºäº†æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¦‚ä½•æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†æ­¤ç±»æ¨ç†ç­–ç•¥åº”ç”¨äºè§†è§‰ç”Ÿæˆé¢†åŸŸä»ç„¶é²œæœ‰ç ”ç©¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†T2I-R1ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å¢å¼ºæ¨ç†çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡å…·æœ‰ä¸¤çº§æ€ç»´é“¾æ¨ç†è¿‡ç¨‹çš„å¼ºåŒ–å­¦ä¹ é©±åŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç¡®å®šäº†å¯ä»¥ç”¨äºå¢å¼ºç”Ÿæˆä¸åŒé˜¶æ®µçš„ä¸¤ä¸ªçº§åˆ«çš„CoTï¼šï¼ˆ1ï¼‰ç”¨äºé«˜çº§æç¤ºè§„åˆ’çš„è¯­ä¹‰çº§CoTå’Œï¼ˆ2ï¼‰ç”¨äºé€å—ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä½çº§åƒç´ å¤„ç†çš„æ ‡è®°çº§CoTã€‚ä¸ºäº†æ›´å¥½åœ°åè°ƒè¿™ä¸¤ä¸ªçº§åˆ«çš„CoTï¼Œæˆ‘ä»¬å¼•å…¥äº†BiCoT-GRPOï¼Œé€šè¿‡ç”Ÿæˆå¥–åŠ±çš„ç»„åˆï¼Œå¯ä»¥åœ¨åŒä¸€è®­ç»ƒæ­¥éª¤ä¸­æ— ç¼ä¼˜åŒ–ä¸¤ç§ç”ŸæˆCoTã€‚æˆ‘ä»¬å°†æ¨ç†ç­–ç•¥åº”ç”¨äºåŸºçº¿æ¨¡å‹Janus-Proï¼Œåœ¨T2I-CompBenchä¸Šå®ç°äº†13%çš„æ€§èƒ½æå‡ï¼Œåœ¨WISEåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†19%çš„æ€§èƒ½æå‡ï¼Œç”šè‡³è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹FLUX.1ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/CaraJ7/T2I-R1">https://github.com/CaraJ7/T2I-R1</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00703v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://github.com/CaraJ7/T2I-R1">https://github.com/CaraJ7/T2I-R1</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•å±•ç¤ºäº†å¦‚ä½•é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡æ€§èƒ½ã€‚æœ¬æ–‡å°†è¿™ç§æ¨ç†ç­–ç•¥åº”ç”¨äºå›¾åƒç”Ÿæˆé¢†åŸŸï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨ç†å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹T2I-R1ã€‚é€šè¿‡å¼•å…¥åŒå±‚æ¬¡çš„é“¾å¼æ€ç»´å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–è¿‡ç¨‹ï¼Œå®ç°åœ¨å›¾åƒç”Ÿæˆçš„ä¸åŒé˜¶æ®µè¿ç”¨ä¸åŒå±‚æ¬¡çš„é“¾å¼æ€ç»´ï¼Œä»è€Œæé«˜å›¾åƒç”Ÿæˆçš„è´¨é‡ã€‚åœ¨åŸºå‡†æ¨¡å‹Janus-Proä¸Šåº”ç”¨æ­¤æ¨ç†ç­–ç•¥ï¼Œå®ç°äº†åœ¨T2I-CompBenchä¸Šçš„æ€§èƒ½æå‡è¾¾åˆ°13%ï¼Œåœ¨WISEä¸Šçš„æ€§èƒ½æå‡è¾¾åˆ°è¿‘ä¸€åŠçš„æ•ˆæœï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„æ¨¡å‹FLUXã€‚ç›®å‰ç›¸å…³ä»£ç å·²ç»å¼€æºä¾›å¤§ä¼—ä½¿ç”¨ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå¯ä»¥å¼•å…¥æ¨ç†ç­–ç•¥æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29dbadaeb3fd6e966b1b702995cd2ae7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87106e321c0c7dbf62f2d91f6f1a8da6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08eabbf38647b2f502d8c75f7317f14f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-218f1bd484d3599f4c38006168abdf72.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Transformer-Encoder-and-Multi-features-Time2Vec-for-Financial-Prediction"><a href="#Transformer-Encoder-and-Multi-features-Time2Vec-for-Financial-Prediction" class="headerlink" title="Transformer Encoder and Multi-features Time2Vec for Financial Prediction"></a>Transformer Encoder and Multi-features Time2Vec for Financial Prediction</h2><p><strong>Authors:Nguyen Kim Hai Bui, Nguyen Duy Chien, PÃ©ter KovÃ¡cs, GergÅ‘ BognÃ¡r</strong></p>
<p>Financial prediction is a complex and challenging task of time series analysis and signal processing, expected to model both short-term fluctuations and long-term temporal dependencies. Transformers have remarkable success mostly in natural language processing using attention mechanism, which also influenced the time series community. The ability to capture both short and long-range dependencies helps to understand the financial market and to recognize price patterns, leading to successful applications of Transformers in stock prediction. Although, the previous research predominantly focuses on individual features and singular predictions, that limits the modelâ€™s ability to understand broader market trends. In reality, within sectors such as finance and technology, companies belonging to the same industry often exhibit correlated stock price movements.   In this paper, we develop a novel neural network architecture by integrating Time2Vec with the Encoder of the Transformer model. Based on the study of different markets, we propose a novel correlation feature selection method. Through a comprehensive fine-tuning of multiple hyperparameters, we conduct a comparative analysis of our results against benchmark models. We conclude that our method outperforms other state-of-the-art encoding methods such as positional encoding, and we also conclude that selecting correlation features enhance the accuracy of predicting multiple stock prices. </p>
<blockquote>
<p>é‡‘èé¢„æµ‹æ˜¯ä¸€é¡¹å¤æ‚ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„æ—¶é—´åºåˆ—åˆ†æå’Œä¿¡å·å¤„ç†ä»»åŠ¡ï¼Œè¦æ±‚å¯¹çŸ­æœŸæ³¢åŠ¨å’Œé•¿æœŸæ—¶é—´ä¾èµ–è¿›è¡Œå»ºæ¨¡ã€‚Transformerä¸»è¦åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œè¿™ä¹Ÿå½±å“äº†æ—¶é—´åºåˆ—ç¤¾åŒºã€‚æ•æ‰çŸ­æœŸå’Œé•¿æœŸä¾èµ–çš„èƒ½åŠ›æœ‰åŠ©äºç†è§£é‡‘èå¸‚åœºå¹¶è¯†åˆ«ä»·æ ¼æ¨¡å¼ï¼Œä»è€Œå¯¼è‡´Transformeråœ¨è‚¡ç¥¨é¢„æµ‹ä¸­çš„æˆåŠŸåº”ç”¨ã€‚ç„¶è€Œï¼Œä»¥å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¸ªåˆ«ç‰¹å¾å’Œå•ä¸€é¢„æµ‹ä¸Šï¼Œè¿™é™åˆ¶äº†æ¨¡å‹äº†è§£æ›´å¹¿æ³›å¸‚åœºè¶‹åŠ¿çš„èƒ½åŠ›ã€‚å®é™…ä¸Šï¼Œåœ¨è¯¸å¦‚é‡‘èå’ŒæŠ€æœ¯ç­‰è¡Œä¸šä¸­ï¼Œå±äºåŒä¸€è¡Œä¸šçš„å…¬å¸é€šå¸¸è¡¨ç°å‡ºç›¸å…³çš„è‚¡ç¥¨ä»·æ ¼èµ°åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ•´åˆTime2Vecä¸Transformeræ¨¡å‹çš„ç¼–ç å™¨ï¼Œå¼€å‘äº†ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ã€‚åŸºäºå¯¹ä¸åŒå¸‚åœºçš„ç ”ç©¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç›¸å…³æ€§ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚é€šè¿‡å…¨é¢å¾®è°ƒå¤šä¸ªè¶…å‚æ•°ï¼Œæˆ‘ä»¬å¯¹æˆ‘ä»¬çš„ç»“æœä¸åŸºå‡†æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„ç¼–ç æ–¹æ³•ï¼Œå¦‚ä½ç½®ç¼–ç ï¼Œæˆ‘ä»¬è¿˜å¾—å‡ºç»“è®ºï¼Œé€‰æ‹©ç›¸å…³æ€§ç‰¹å¾æé«˜äº†é¢„æµ‹å¤šä¸ªè‚¡ç¥¨ä»·æ ¼çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13801v2">PDF</a> 5 pages, Eusipco 2025</p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡å¼€å‘äº†ä¸€ç§ç»“åˆTime2Vecä¸Transformerç¼–ç å™¨çš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”¨äºé‡‘èé¢†åŸŸçš„è‚¡ç¥¨é¢„æµ‹ã€‚ç ”ç©¶ä¸åŒå¸‚åœºåï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç›¸å…³æ€§ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚é€šè¿‡ç»¼åˆè°ƒæ•´å¤šä¸ªè¶…å‚æ•°ï¼Œä¸åŸºå‡†æ¨¡å‹è¿›è¡Œæ¯”è¾ƒåˆ†æï¼Œå‘ç°è¯¥æ–¹æ³•ä¼˜äºå…¶ä»–å…ˆè¿›çš„ç¼–ç æ–¹æ³•ï¼Œå¦‚ä½ç½®ç¼–ç ã€‚é€‰æ‹©ç›¸å…³æ€§ç‰¹å¾å¯ä»¥æé«˜é¢„æµ‹å¤šä¸ªè‚¡ç¥¨ä»·æ ¼çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬æ–‡ç»“åˆTime2Vecä¸Transformerç¼–ç å™¨å¼€å‘äº†ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”¨äºé‡‘èé¢†åŸŸçš„è‚¡ç¥¨é¢„æµ‹ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥æ•æ‰çŸ­æœŸæ³¢åŠ¨å’Œé•¿æœŸæ—¶é—´ä¾èµ–æ€§ï¼Œæœ‰åŠ©äºç†è§£é‡‘èå¸‚åœºå’Œè¯†åˆ«ä»·æ ¼æ¨¡å¼ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç›¸å…³æ€§ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œé€‚ç”¨äºé‡‘èå¸‚åœºçš„ç‰¹ç‚¹ã€‚</li>
<li>é€šè¿‡ç»¼åˆè°ƒæ•´å¤šä¸ªè¶…å‚æ•°ï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹è‚¡ç¥¨ä»·æ ¼æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›çš„ç¼–ç æ–¹æ³•ç›¸æ¯”ï¼Œå¦‚ä½ç½®ç¼–ç ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>é€‰æ‹©ç›¸å…³æ€§ç‰¹å¾å¯ä»¥æé«˜é¢„æµ‹å¤šä¸ªè‚¡ç¥¨ä»·æ ¼çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b189018f66021a977d29457c2103c7df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ecba87996e296853ad6b82b257a5b4d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cdf60bded56950453358ffbdee8d9f83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c668ff1abc60b0a5237e98fe011c4f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7002f547704730e0ff38d2f5d530384.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9d7e6c0abf77d5d6411f67a9e502221.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Interpret-and-Leverage-Structured-Linguistic-Representations-A-Case-Study-with-AMRs"><a href="#Can-LLMs-Interpret-and-Leverage-Structured-Linguistic-Representations-A-Case-Study-with-AMRs" class="headerlink" title="Can LLMs Interpret and Leverage Structured Linguistic Representations? A   Case Study with AMRs"></a>Can LLMs Interpret and Leverage Structured Linguistic Representations? A   Case Study with AMRs</h2><p><strong>Authors:Ankush Raut, Xiaofeng Zhu, Maria Leonor Pacheco</strong></p>
<p>This paper evaluates the ability of Large Language Models (LLMs) to leverage contextual information in the form of structured linguistic representations. Specifically, we examine the impact of encoding both short and long contexts using Abstract Meaning Representation (AMR) structures across a diverse set of language tasks. We perform our analysis using 8-bit quantized and instruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our results indicate that, for tasks involving short contexts, augmenting the prompt with the AMR of the original language context often degrades the performance of the underlying LLM. However, for tasks that involve long contexts, such as dialogue summarization in the SAMSum dataset, this enhancement improves LLM performance, for example, by increasing the zero-shot cosine similarity score of Llama 3.1 from 66% to 76%. This improvement is more evident in the newer and larger LLMs, but does not extend to the older or smaller ones. In addition, we observe that LLMs can effectively reconstruct the original text from a linearized AMR, achieving a cosine similarity of 81% in the best-case scenario. </p>
<blockquote>
<p>æœ¬æ–‡è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ©ç”¨ç»“æ„åŒ–è¯­è¨€è¡¨ç¤ºå½¢å¼ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨æŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰ç»“æ„å¯¹çŸ­é•¿å’Œä¸åŒè¯­è¨€ä»»åŠ¡çš„ä¸Šä¸‹æ–‡è¿›è¡Œç¼–ç çš„å½±å“ã€‚æˆ‘ä»¬ä½¿ç”¨8ä½é‡åŒ–å’ŒæŒ‡ä»¤è°ƒæ•´çš„Llama 3.1ï¼ˆ8Bï¼‰ã€Phi-3å’ŒMistral 7Bç‰ˆæœ¬è¿›è¡Œåˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œå¯¹äºæ¶‰åŠçŸ­ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ï¼Œé€šè¿‡æç¤ºå¢å¼ºåŸå§‹è¯­è¨€ä¸Šä¸‹æ–‡çš„AMRå¾€å¾€ä¼šé™ä½åŸºç¡€LLMçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºæ¶‰åŠé•¿ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ï¼Œå¦‚åœ¨SAMSumæ•°æ®é›†ä¸­çš„å¯¹è¯æ‘˜è¦ï¼Œè¿™ç§å¢å¼ºä¼šæé«˜LLMçš„æ€§èƒ½ï¼Œä¾‹å¦‚å°†Llama 3. 1çš„é›¶å°„ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†ä»66%æé«˜åˆ°76%ã€‚è¿™ä¸€æ”¹è¿›åœ¨æ›´æ–°ã€æ›´å¤§çš„LLMä¸­æ›´ä¸ºæ˜æ˜¾ï¼Œä½†å¹¶ä¸é€‚ç”¨äºæ—§ç‰ˆæˆ–è¾ƒå°çš„LLMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°LLMå¯ä»¥æœ‰æ•ˆåœ°ä»çº¿æ€§åŒ–çš„AMRä¸­é‡å»ºåŸå§‹æ–‡æœ¬ï¼Œåœ¨æœ€ä½³æƒ…å†µä¸‹è¾¾åˆ°81%çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04745v4">PDF</a> 13 pages, 23 figures. Accepted to XLLM Workshop at ACL 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ©ç”¨ç»“æ„åŒ–è¯­è¨€è¡¨ç¤ºå½¢å¼è¿›è¡Œä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶é€šè¿‡æŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰ç»“æ„ç¼–ç çŸ­é•¿å’Œä¸åŒè¯­å¢ƒï¼Œå¯¹ä¸€ç³»åˆ—è¯­è¨€ä»»åŠ¡è¿›è¡Œäº†è¯„ä¼°ã€‚åˆ†æè¡¨æ˜ï¼Œå¯¹äºçŸ­è¯­å¢ƒä»»åŠ¡ï¼Œæ·»åŠ åŸå§‹è¯­å¢ƒçš„AMRå¾€å¾€ä¼šé™ä½LLMæ€§èƒ½ï¼›è€Œå¯¹äºé•¿è¯­å¢ƒä»»åŠ¡ï¼Œå¦‚å¯¹è¯æ‘˜è¦ï¼Œè¿™ç§å¢å¼ºåˆ™èƒ½æé«˜LLMæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°ä¸”å¤§å‹çš„LLMä¸­è¡¨ç°æ›´ä¸ºæ˜æ˜¾ã€‚æ­¤å¤–ï¼ŒLLMè¿˜èƒ½æœ‰æ•ˆåœ°ä»çº¿æ€§åŒ–çš„AMRä¸­é‡å»ºåŸå§‹æ–‡æœ¬ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåˆ©ç”¨ç»“æ„åŒ–è¯­è¨€è¡¨ç¤ºï¼ˆå¦‚æŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰ï¼‰æ¥å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯å¤„ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨çŸ­è¯­å¢ƒä»»åŠ¡ä¸­ï¼Œæ·»åŠ AMRä¿¡æ¯å¯èƒ½ä¼šé™ä½LLMçš„æ€§èƒ½ã€‚</li>
<li>å¯¹äºé•¿è¯­å¢ƒä»»åŠ¡ï¼Œå¦‚å¯¹è¯æ‘˜è¦ï¼Œç»“åˆAMRä¿¡æ¯èƒ½æé«˜LLMçš„æ€§èƒ½ã€‚</li>
<li>æ–°ä¸€ä»£çš„å¤§å‹LLMåœ¨ç»“åˆAMRä¿¡æ¯æ–¹é¢çš„æ€§èƒ½æå‡æ›´ä¸ºæ˜¾è‘—ã€‚</li>
<li>LLMèƒ½ä»çº¿æ€§åŒ–çš„AMRæœ‰æ•ˆåœ°é‡å»ºåŸå§‹æ–‡æœ¬ã€‚</li>
<li>åœ¨å¤„ç†ä¸åŒé•¿åº¦çš„è¯­å¢ƒæ—¶ï¼ŒLLMçš„æ€§èƒ½ä¼šå—åˆ°æ¨¡å‹å¤§å°å’Œæ—¶ä»£çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c705a7e07127105980f19b438768030.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2876cad9086bbf6e797d1ec62315f518.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e910886f66f98fb042a4a850007dfe3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b038b015a028b0bad607c72cf974316.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e6030c7386ed715da1172957f4e455f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0625426b1f81ffa2a742d52923fa0f9d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Edit-Transfer-Learning-Image-Editing-via-Vision-In-Context-Relations"><a href="#Edit-Transfer-Learning-Image-Editing-via-Vision-In-Context-Relations" class="headerlink" title="Edit Transfer: Learning Image Editing via Vision In-Context Relations"></a>Edit Transfer: Learning Image Editing via Vision In-Context Relations</h2><p><strong>Authors:Lan Chen, Qi Mao, Yuchao Gu, Mike Zheng Shou</strong></p>
<p>We introduce a new setting, Edit Transfer, where a model learns a transformation from just a single source-target example and applies it to a new query image. While text-based methods excel at semantic manipulations through textual prompts, they often struggle with precise geometric details (e.g., poses and viewpoint changes). Reference-based editing, on the other hand, typically focuses on style or appearance and fails at non-rigid transformations. By explicitly learning the editing transformation from a source-target pair, Edit Transfer mitigates the limitations of both text-only and appearance-centric references. Drawing inspiration from in-context learning in large language models, we propose a visual relation in-context learning paradigm, building upon a DiT-based text-to-image model. We arrange the edited example and the query image into a unified four-panel composite, then apply lightweight LoRA fine-tuning to capture complex spatial transformations from minimal examples. Despite using only 42 training samples, Edit Transfer substantially outperforms state-of-the-art TIE and RIE methods on diverse non-rigid scenarios, demonstrating the effectiveness of few-shot visual relation learning. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è®¾ç½®ï¼Œç§°ä¸ºâ€œç¼–è¾‘ä¼ è¾“â€(Edit Transfer)ï¼Œåœ¨è¯¥è®¾ç½®ä¸­ï¼Œæ¨¡å‹ä»…ä»ä¸€ä¸ªæºç›®æ ‡ç¤ºä¾‹ä¸­å­¦ä¹ è½¬æ¢ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ–°çš„æŸ¥è¯¢å›¾åƒã€‚è™½ç„¶åŸºäºæ–‡æœ¬çš„æ–¹æ³•åœ¨é€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œè¯­ä¹‰æ“ä½œæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†å®ƒä»¬é€šå¸¸å¯¹äºç²¾ç¡®çš„å‡ ä½•ç»†èŠ‚ï¼ˆä¾‹å¦‚å§¿åŠ¿å’Œè§†ç‚¹å˜åŒ–ï¼‰æ„Ÿåˆ°å›°éš¾ã€‚å¦ä¸€æ–¹é¢ï¼ŒåŸºäºå‚è€ƒçš„ç¼–è¾‘é€šå¸¸ä¾§é‡äºé£æ ¼æˆ–å¤–è§‚ï¼Œè€Œåœ¨éåˆšæ€§è½¬æ¢æ–¹é¢è¡¨ç°ä¸ä½³ã€‚é€šè¿‡ä»æºç›®æ ‡å¯¹ä¸­å­¦ä¹ ç¼–è¾‘è½¬æ¢ï¼ŒEdit Transferå‡è½»äº†ä»…ä½¿ç”¨æ–‡æœ¬å’Œå¤–è§‚å‚è€ƒçš„é™åˆ¶ã€‚æˆ‘ä»¬ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ±²å–çµæ„Ÿï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºDiTçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è§†è§‰ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ã€‚æˆ‘ä»¬å°†ç¼–è¾‘ç¤ºä¾‹å’ŒæŸ¥è¯¢å›¾åƒæ’åˆ—æˆä¸€ä¸ªç»Ÿä¸€çš„å››é¢æ¿ç»„åˆï¼Œç„¶ååº”ç”¨è½»é‡çº§çš„LoRAå¾®è°ƒæ¥ä»æœ€å°‘çš„ç¤ºä¾‹ä¸­å­¦ä¹ å¤æ‚çš„ç©ºé—´è½¬æ¢ã€‚å°½ç®¡åªä½¿ç”¨äº†42ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œä½†Edit Transferåœ¨å¤šç§éåˆšæ€§åœºæ™¯ä¸Šå¤§å¤§ä¼˜äºæœ€æ–°çš„TIEå’ŒRIEæ–¹æ³•ï¼Œè¯æ˜äº†å°‘æ•°è§†è§‰å…³ç³»å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13327v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨Edit Transferæ–°è®¾å®šä¸­ï¼Œæ¨¡å‹ä»…é€šè¿‡å•ä¸ªæºç›®æ ‡ç¤ºä¾‹å­¦ä¹ è½¬æ¢ï¼Œå¹¶åº”ç”¨äºæ–°æŸ¥è¯¢å›¾åƒã€‚è¯¥æ–¹æ³•å¼¥è¡¥äº†çº¯æ–‡æœ¬å’Œå¤–è§‚å‚è€ƒæ–¹æ³•çš„å±€é™ã€‚å®ƒå€Ÿé‰´äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæå‡ºäº†åŸºäºDiTæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„è§†è§‰å…³ç³»ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ã€‚é€šè¿‡ç¼–è¾‘ç¤ºä¾‹å’ŒæŸ¥è¯¢å›¾åƒç»„æˆå››é¢æ¿åˆæˆå›¾åƒï¼Œç„¶ååº”ç”¨è½»é‡çº§LoRAå¾®è°ƒï¼Œä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ å¤æ‚ç©ºé—´è½¬æ¢ã€‚å°½ç®¡åªä½¿ç”¨42ä¸ªè®­ç»ƒæ ·æœ¬ï¼ŒEdit Transferåœ¨å¤šç§éåˆšæ€§åœºæ™¯ä¸Šå¤§å¹…è¶…è¶Šäº†æœ€æ–°çš„TIEå’ŒRIEæ–¹æ³•ï¼Œå±•ç¤ºäº†å°‘æ•°è§†è§‰å…³ç³»å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Edit Transferæ–°è®¾å®šå…è®¸æ¨¡å‹ä»å•ä¸ªæºç›®æ ‡ç¤ºä¾‹å­¦ä¹ è½¬æ¢ï¼Œå¹¶åº”ç”¨äºæ–°çš„æŸ¥è¯¢å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•èåˆäº†æ–‡æœ¬æ–¹æ³•å’Œå‚è€ƒç¼–è¾‘æ–¹æ³•çš„ä¼˜ç‚¹ï¼Œå…‹æœäº†å®ƒä»¬å„è‡ªçš„å±€é™ã€‚</li>
<li>å€Ÿé‰´å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæå‡ºäº†è§†è§‰å…³ç³»çš„ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ã€‚</li>
<li>é€šè¿‡ç»„åˆç¼–è¾‘ç¤ºä¾‹å’ŒæŸ¥è¯¢å›¾åƒå½¢æˆå››é¢æ¿åˆæˆå›¾åƒè¿›è¡Œå­¦ä¹ ã€‚</li>
<li>ä½¿ç”¨è½»é‡çº§LoRAå¾®è°ƒæŠ€æœ¯ï¼Œä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ å¤æ‚çš„ç©ºé—´è½¬æ¢ã€‚</li>
<li>Edit Transferåœ¨éåˆšæ€§åœºæ™¯ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¤§å¹…è¶…è¶Šäº†TIEå’ŒRIEæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f696d0abc332980cc2e65ec059b04686.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd2d433aae67ac0e2c4865eddbce06e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70d35a37e025975f895ccee2ddeb0a6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93737b98caefc0a3fa940e57236f0217.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f4c77fe1ab0d654a5570e7b5ea52361.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01de7e1045a60242bfe0d9b967cb3457.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LangTime-A-Language-Guided-Unified-Model-for-Time-Series-Forecasting-with-Proximal-Policy-Optimization"><a href="#LangTime-A-Language-Guided-Unified-Model-for-Time-Series-Forecasting-with-Proximal-Policy-Optimization" class="headerlink" title="LangTime: A Language-Guided Unified Model for Time Series Forecasting   with Proximal Policy Optimization"></a>LangTime: A Language-Guided Unified Model for Time Series Forecasting   with Proximal Policy Optimization</h2><p><strong>Authors:Wenzhe Niu, Zongxia Xie, Yanru Sun, Wei He, Man Xu, Chao Hao</strong></p>
<p>Recent research has shown an increasing interest in utilizing pre-trained large language models (LLMs) for a variety of time series applications. However, there are three main challenges when using LLMs as foundational models for time series forecasting: (1) Cross-domain generalization. (2) Cross-modality alignment. (3) Error accumulation in autoregressive frameworks. To address these challenges, we proposed LangTime, a language-guided unified model for time series forecasting that incorporates cross-domain pre-training with reinforcement learning-based fine-tuning. Specifically, LangTime constructs Temporal Comprehension Prompts (TCPs), which include dataset-wise and channel-wise instructions, to facilitate domain adaptation and condense time series into a single token, enabling LLMs to understand better and align temporal data. To improve autoregressive forecasting, we introduce TimePPO, a reinforcement learning-based fine-tuning algorithm. TimePPO mitigates error accumulation by leveraging a multidimensional rewards function tailored for time series and a repeat-based value estimation strategy. Extensive experiments demonstrate that LangTime achieves state-of-the-art cross-domain forecasting performance, while TimePPO fine-tuning effectively enhances the stability and accuracy of autoregressive forecasting. </p>
<blockquote>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œè¶Šæ¥è¶Šå¤šçš„å…´è¶£é›†ä¸­åœ¨åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå„ç§æ—¶é—´åºåˆ—åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“ä½¿ç”¨LLMä½œä¸ºæ—¶é—´åºåˆ—é¢„æµ‹çš„åŸºç¡€æ¨¡å‹æ—¶ï¼Œå­˜åœ¨ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰è·¨åŸŸæ³›åŒ–ï¼›ï¼ˆ2ï¼‰è·¨æ¨¡æ€å¯¹é½ï¼›ï¼ˆ3ï¼‰è‡ªå›å½’æ¡†æ¶ä¸­çš„è¯¯å·®ç´¯ç§¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LangTimeï¼Œè¿™æ˜¯ä¸€ä¸ªè¯­è¨€å¼•å¯¼çš„æ—¶é—´åºåˆ—é¢„æµ‹ç»Ÿä¸€æ¨¡å‹ï¼Œå®ƒç»“åˆäº†è·¨åŸŸé¢„è®­ç»ƒå’Œä½¿ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒã€‚å…·ä½“æ¥è¯´ï¼ŒLangTimeæ„å»ºæ—¶é—´ç†è§£æç¤ºï¼ˆTCPï¼‰ï¼ŒåŒ…æ‹¬æ•°æ®é›†å’Œé€šé“çº§çš„æŒ‡ä»¤ï¼Œä»¥ä¿ƒè¿›åŸŸé€‚åº”å¹¶å°†æ—¶é—´åºåˆ—æµ“ç¼©ä¸ºå•ä¸ªä»¤ç‰Œï¼Œä½¿LLMèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œå¯¹é½æ—¶é—´åºåˆ—æ•°æ®ã€‚ä¸ºäº†æé«˜è‡ªå›å½’é¢„æµ‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒç®—æ³•TimePPOã€‚TimePPOé€šè¿‡åˆ©ç”¨é’ˆå¯¹æ—¶é—´åºåˆ—çš„å¤šç»´å¥–åŠ±å‡½æ•°å’ŒåŸºäºé‡å¤çš„ä¼°å€¼ç­–ç•¥ï¼Œç¼“è§£è¯¯å·®ç´¯ç§¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLangTimeå®ç°äº†æœ€å…ˆè¿›çš„è·¨åŸŸé¢„æµ‹æ€§èƒ½ï¼Œè€ŒTimePPOå¾®è°ƒæœ‰æ•ˆæé«˜è‡ªå›å½’é¢„æµ‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08271v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶é—´åºåˆ—åº”ç”¨ä¸­çš„ä½¿ç”¨æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œä½†å­˜åœ¨è·¨åŸŸæ³›åŒ–ã€è·¨æ¨¡æ€å¯¹é½å’Œè‡ªå›å½’æ¡†æ¶ä¸­çš„è¯¯å·®ç´¯ç§¯ç­‰ä¸‰å¤§æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†LangTimeæ¨¡å‹ï¼Œé€šè¿‡è·¨åŸŸé¢„è®­ç»ƒä¸åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒï¼Œå®ç°è¯­è¨€å¼•å¯¼çš„æ—¶é—´åºåˆ—é¢„æµ‹ã€‚LangTimeæ„å»ºæ—¶åºç†è§£æç¤ºï¼ˆTCPï¼‰ï¼ŒåŒ…å«æ•°æ®é›†çº§å’Œé€šé“çº§çš„æŒ‡ä»¤ï¼Œä¿ƒè¿›åŸŸé€‚åº”å¹¶ç®€åŒ–æ—¶é—´åºåˆ—ä¸ºå•ä¸€ä»¤ç‰Œï¼Œæé«˜LLMçš„ç†è§£èƒ½åŠ›ã€‚ä¸ºæ”¹å–„è‡ªå›å½’é¢„æµ‹ï¼Œå¼•å…¥TimePPOç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡å¤šå°ºåº¦å¥–åŠ±å‡½æ•°å’ŒåŸºäºé‡å¤çš„ä¼°å€¼ç­–ç•¥å‡è½»è¯¯å·®ç´¯ç§¯é—®é¢˜ã€‚å®éªŒè¯æ˜LangTimeæ¨¡å‹åœ¨è·¨åŸŸé¢„æµ‹é¢†åŸŸå–å¾—äº†æœ€æ–°æˆæœï¼Œè€ŒTimePPOå¾®è°ƒç®—æ³•æœ‰æ•ˆæé«˜äº†è‡ªå›å½’é¢„æµ‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsåœ¨æ—¶é—´åºåˆ—åº”ç”¨ä¸­çš„ä¸‰å¤§æŒ‘æˆ˜åŒ…æ‹¬è·¨åŸŸæ³›åŒ–ã€è·¨æ¨¡æ€å¯¹é½å’Œè‡ªå›å½’æ¡†æ¶ä¸­çš„è¯¯å·®ç´¯ç§¯ã€‚</li>
<li>LangTimeæ¨¡å‹ç»“åˆäº†è·¨åŸŸé¢„è®­ç»ƒå’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒï¼Œæ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>LangTimeé€šè¿‡æ„å»ºæ—¶åºç†è§£æç¤ºï¼ˆTCPï¼‰ä¿ƒè¿›åŸŸé€‚åº”å¹¶ç®€åŒ–æ—¶é—´åºåˆ—æ•°æ®ã€‚</li>
<li>TimePPOç®—æ³•ç”¨äºæ”¹å–„è‡ªå›å½’é¢„æµ‹ï¼Œé€šè¿‡å¤šå°ºåº¦å¥–åŠ±å‡½æ•°å’ŒåŸºäºé‡å¤çš„ä¼°å€¼ç­–ç•¥å‡è½»è¯¯å·®ç´¯ç§¯ã€‚</li>
<li>LangTimeæ¨¡å‹åœ¨è·¨åŸŸé¢„æµ‹é¢†åŸŸå–å¾—äº†æœ€æ–°æˆæœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ae0451b20cfe81d376229ef135a9c67c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8951c3ccb710373b7a352e37bbb6ea23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c39df895fae9da54149ee14a4ac6470.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-470487888ab9b3d133676a6b4744e2ce.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e906d407c81f51945c0316d5fc30ddd0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Agent.xpu Efficient Scheduling of Agentic LLM Workloads on   Heterogeneous SoC
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ca1f465a0456014d848fe433316fe5f1.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Advancing Multi-Step Mathematical Reasoning in Large Language Models   through Multi-Layered Self-Reflection with Auto-Prompting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
