<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Imagine for Me Creative Conceptual Blending of Real Images and Text via   Blended Attention">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4cf9c2bc6af6f33a9d63db6f06aefbd6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-03-æ›´æ–°"><a href="#2025-07-03-æ›´æ–°" class="headerlink" title="2025-07-03 æ›´æ–°"></a>2025-07-03 æ›´æ–°</h1><h2 id="Imagine-for-Me-Creative-Conceptual-Blending-of-Real-Images-and-Text-via-Blended-Attention"><a href="#Imagine-for-Me-Creative-Conceptual-Blending-of-Real-Images-and-Text-via-Blended-Attention" class="headerlink" title="Imagine for Me: Creative Conceptual Blending of Real Images and Text via   Blended Attention"></a>Imagine for Me: Creative Conceptual Blending of Real Images and Text via   Blended Attention</h2><p><strong>Authors:Wonwoong Cho, Yanxia Zhang, Yan-Ying Chen, David I. Inouye</strong></p>
<p>Blending visual and textual concepts into a new visual concept is a unique and powerful trait of human beings that can fuel creativity. However, in practice, cross-modal conceptual blending for humans is prone to cognitive biases, like design fixation, which leads to local minima in the design space. In this paper, we propose a T2I diffusion adapter â€œIT-Blenderâ€ that can automate the blending process to enhance human creativity. Prior works related to cross-modal conceptual blending are limited in encoding a real image without loss of details or in disentangling the image and text inputs. To address these gaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend the latent representations of a clean reference image with those of the noisy generated image. Combined with our novel blended attention, IT-Blender encodes the real reference image without loss of details and blends the visual concept with the object specified by the text in a disentangled way. Our experiment results show that IT-Blender outperforms the baselines by a large margin in blending visual and textual concepts, shedding light on the new application of image generative models to augment human creativity. </p>
<blockquote>
<p>å°†è§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µèåˆä¸ºä¸€ä¸ªæ–°çš„è§†è§‰æ¦‚å¿µæ˜¯äººç±»ç‹¬ç‰¹ä¸”å¼ºå¤§çš„ç‰¹è´¨ï¼Œèƒ½å¤Ÿæ¿€å‘åˆ›é€ åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œäººç±»çš„è·¨æ¨¡æ€æ¦‚å¿µèåˆå®¹æ˜“å—åˆ°è®¤çŸ¥åè§çš„å½±å“ï¼Œå¦‚è®¾è®¡åƒµåŒ–ï¼Œè¿™ä¼šå¯¼è‡´è®¾è®¡ç©ºé—´ä¸­å‡ºç°å±€éƒ¨æœ€å°å€¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§T2Iæ‰©æ•£é€‚é…å™¨â€œIT-Blenderâ€ï¼Œå¯ä»¥è‡ªåŠ¨åŒ–èåˆè¿‡ç¨‹ä»¥å¢å¼ºäººç±»çš„åˆ›é€ åŠ›ã€‚ä»¥å¾€ä¸è·¨æ¨¡æ€æ¦‚å¿µèåˆç›¸å…³çš„å·¥ä½œåœ¨ç¼–ç çœŸå®å›¾åƒæ—¶è¦ä¹ˆä¼šæŸå¤±ç»†èŠ‚ï¼Œè¦ä¹ˆæ— æ³•è§£å¼€å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ã€‚ä¸ºäº†è§£å†³è¿™äº›ç¼ºé™·ï¼ŒIT-Blenderåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆSDå’ŒFLUXï¼‰æ¥èåˆå¹²å‡€å‚è€ƒå›¾åƒå’Œå™ªå£°ç”Ÿæˆå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºã€‚ç»“åˆæˆ‘ä»¬æ–°é¢–çš„èåˆæ³¨æ„åŠ›æœºåˆ¶ï¼ŒIT-Blenderèƒ½å¤Ÿç¼–ç çœŸå®å‚è€ƒå›¾åƒè€Œä¸æŸå¤±ç»†èŠ‚ï¼Œå¹¶ä»¥è§£å¼€çš„æ–¹å¼å°†è§†è§‰æ¦‚å¿µä¸æ–‡æœ¬æŒ‡å®šçš„å¯¹è±¡èåˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨èåˆè§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µæ–¹é¢ï¼ŒIT-Blenderå¤§å¹…åº¦è¶…è¶Šäº†åŸºçº¿æ–¹æ³•ï¼Œä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¢å¼ºäººç±»åˆ›é€ åŠ›æ–¹é¢çš„æ–°åº”ç”¨æä¾›äº†å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.24085v1">PDF</a> Project website is available at <a target="_blank" rel="noopener" href="https://imagineforme.github.io/">https://imagineforme.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>äººç±»èåˆè§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µå½¢æˆæ–°çš„è§†è§‰æ¦‚å¿µæ˜¯ä¸€ä¸ªç‹¬ç‰¹è€Œå¼ºå¤§çš„ç‰¹è´¨ï¼Œèƒ½å¤Ÿæ¿€å‘åˆ›é€ åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œäººç±»çš„è·¨æ¨¡æ€æ¦‚å¿µèåˆå®¹æ˜“å—åˆ°è®¤çŸ¥åè§çš„å½±å“ï¼Œå¦‚è®¾è®¡åƒµåŒ–ç—‡ï¼Œè¿™ä¼šå¯¼è‡´è®¾è®¡ç©ºé—´çš„å±€éƒ¨æœ€å°å€¼ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§T2Iæ‰©æ•£é€‚é…å™¨IT-Blenderï¼Œå¯ä»¥è‡ªåŠ¨åŒ–èåˆè¿‡ç¨‹ä»¥å¢å¼ºäººç±»çš„åˆ›é€ åŠ›ã€‚å…ˆå‰ä¸è·¨æ¨¡æ€æ¦‚å¿µèåˆç›¸å…³çš„å·¥ä½œåœ¨ç¼–ç çœŸå®å›¾åƒæ—¶å­˜åœ¨ç»†èŠ‚æŸå¤±æˆ–åœ¨åˆ†ç¦»å›¾åƒå’Œæ–‡æœ¬è¾“å…¥æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒIT-Blenderåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆSDå’ŒFLUXï¼‰å°†å¹²å‡€å‚è€ƒå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºä¸å™ªå£°ç”Ÿæˆå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºç›¸èåˆã€‚ç»“åˆæˆ‘ä»¬æ–°é¢–çš„èåˆæ³¨æ„åŠ›æœºåˆ¶ï¼ŒIT-Blenderèƒ½å¤Ÿæ— æŸç¼–ç çœŸå®å‚è€ƒå›¾åƒï¼Œå¹¶ä»¥åˆ†ç¦»çš„æ–¹å¼å°†è§†è§‰æ¦‚å¿µä¸æ–‡æœ¬æŒ‡å®šçš„å¯¹è±¡ç›¸èåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIT-Blenderåœ¨èåˆè§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µæ–¹é¢å¤§å¤§ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¢å¼ºäººç±»åˆ›é€ åŠ›æ–¹é¢çš„æ–°åº”ç”¨æä¾›äº†æ–°çš„å¯ç¤ºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>äººç±»èåˆè§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µçš„èƒ½åŠ›æ˜¯ç‹¬ç‰¹ä¸”å¼ºå¤§çš„ï¼Œèƒ½å¤Ÿæ¿€å‘åˆ›é€ åŠ›ã€‚</li>
<li>è·¨æ¨¡æ€æ¦‚å¿µèåˆåœ¨å®è·µä¸­æ˜“å—åˆ°è®¤çŸ¥åè§çš„å½±å“ï¼Œå¦‚è®¾è®¡åƒµåŒ–ç—‡ã€‚</li>
<li>IT-Blenderæ˜¯ä¸€ç§T2Iæ‰©æ•£é€‚é…å™¨ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–è·¨æ¨¡æ€æ¦‚å¿µèåˆè¿‡ç¨‹ä»¥å¢å¼ºäººç±»åˆ›é€ åŠ›ã€‚</li>
<li>IT-Blenderåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œæ–°é¢–çš„èåˆæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå›¾åƒå’Œæ–‡æœ¬çš„èåˆã€‚</li>
<li>IT-Blenderèƒ½å¤Ÿåœ¨ç¼–ç çœŸå®å›¾åƒæ—¶ä¿æŒç»†èŠ‚æ— æŸã€‚</li>
<li>IT-Blenderå°†è§†è§‰æ¦‚å¿µä¸æ–‡æœ¬æŒ‡å®šçš„å¯¹è±¡ä»¥åˆ†ç¦»çš„æ–¹å¼èåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.24085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f9fd27a4991095a0a90d9945317928c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb92fb35af6ba9972a192a2f4103f99c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cf9c2bc6af6f33a9d63db6f06aefbd6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Supervised-Diffusion-Model-Based-PET-Image-Reconstruction"><a href="#Supervised-Diffusion-Model-Based-PET-Image-Reconstruction" class="headerlink" title="Supervised Diffusion-Model-Based PET Image Reconstruction"></a>Supervised Diffusion-Model-Based PET Image Reconstruction</h2><p><strong>Authors:George Webber, Alexander Hammers, Andrew P King, Andrew J Reader</strong></p>
<p>Diffusion models (DMs) have recently been introduced as a regularizing prior for PET image reconstruction, integrating DMs trained on high-quality PET images with unsupervised schemes that condition on measured data. While these approaches have potential generalization advantages due to their independence from the scanner geometry and the injected activity level, they forgo the opportunity to explicitly model the interaction between the DM prior and noisy measurement data, potentially limiting reconstruction accuracy. To address this, we propose a supervised DM-based algorithm for PET reconstruction. Our method enforces the non-negativity of PETâ€™s Poisson likelihood model and accommodates the wide intensity range of PET images. Through experiments on realistic brain PET phantoms, we demonstrate that our approach outperforms or matches state-of-the-art deep learning-based methods quantitatively across a range of dose levels. We further conduct ablation studies to demonstrate the benefits of the proposed components in our model, as well as its dependence on training data, parameter count, and number of diffusion steps. Additionally, we show that our approach enables more accurate posterior sampling than unsupervised DM-based methods, suggesting improved uncertainty estimation. Finally, we extend our methodology to a practical approach for fully 3D PET and present example results from real [$^{18}$F]FDG brain PET data. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æœ€è¿‘è¢«å¼•å…¥ä¸ºæ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒé‡å»ºçš„æ­£åˆ™åŒ–å…ˆéªŒï¼Œå°†ç»è¿‡é«˜è´¨é‡PETå›¾åƒè®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸åŸºäºæµ‹é‡æ•°æ®çš„æ— ç›‘ç£æ–¹æ¡ˆç›¸ç»“åˆã€‚å°½ç®¡è¿™äº›æ–¹æ³•ç”±äºç‹¬ç«‹äºæ‰«æå™¨å‡ ä½•å’Œæ³¨å°„æ´»åº¦æ°´å¹³è€Œå…·æœ‰æ½œåœ¨çš„æ³›åŒ–ä¼˜åŠ¿ï¼Œä½†å®ƒä»¬æ”¾å¼ƒäº†æ˜¾å¼å»ºæ¨¡æ‰©æ•£æ¨¡å‹å…ˆéªŒä¸å™ªå£°æµ‹é‡æ•°æ®ä¹‹é—´äº¤äº’çš„æœºä¼šï¼Œå¯èƒ½ä¼šé™åˆ¶é‡å»ºçš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„PETé‡å»ºç›‘ç£ç®—æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼ºåˆ¶å®æ–½PETçš„Poissonä¼¼ç„¶æ¨¡å‹çš„éè´Ÿæ€§ï¼Œå¹¶é€‚åº”PETå›¾åƒçš„å¹¿æ³›å¼ºåº¦èŒƒå›´ã€‚é€šè¿‡å¯¹ç°å®çš„å¤§è„‘PETå¹»å½±è¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨é«˜å‰‚é‡æ°´å¹³èŒƒå›´å†…å®šé‡åœ°ä¼˜äºæˆ–åŒ¹é…äº†åŸºäºæœ€æ–°æ·±åº¦å­¦ä¹ çš„æŠ€æœ¯ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¿›è¡Œæ¶ˆèç ”ç©¶ï¼Œä»¥å±•ç¤ºæˆ‘ä»¬æ¨¡å‹ä¸­æå‡ºçš„ç»„ä»¶çš„ä¼˜åŠ¿ï¼Œä»¥åŠå…¶ä¾èµ–äºè®­ç»ƒæ•°æ®ã€å‚æ•°æ•°é‡å’Œæ‰©æ•£æ­¥éª¤æ•°é‡çš„ç¨‹åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ¯”æ— ç›‘ç£çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•æä¾›æ›´å‡†ç¡®çš„åéªŒé‡‡æ ·ï¼Œè¿™è¡¨æ˜ä¸ç¡®å®šæ€§ä¼°è®¡æœ‰æ‰€æé«˜ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•è®ºæ‰©å±•ä¸ºå®Œå…¨å®ç”¨çš„3D PETæ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†æ¥è‡ªå®é™…ä½¿ç”¨çš„${}^{18}$FDGå¤§è„‘PETæ•°æ®çš„ç¤ºä¾‹ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.24034v1">PDF</a> 12 pages, 6 figures. Submitted to MICCAI 2025, not peer-reviewed</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ä½œä¸ºPETå›¾åƒé‡å»ºçš„æ­£åˆ™åŒ–å…ˆéªŒæœ€è¿‘å·²è¢«å¼•å…¥ï¼Œé€šè¿‡å°†è®­ç»ƒäºé«˜è´¨é‡PETå›¾åƒçš„DMsä¸åŸºäºæµ‹é‡æ•°æ®çš„æ— ç›‘ç£æ–¹æ¡ˆç›¸ç»“åˆã€‚è™½ç„¶è¿™äº›æ–¹æ³•ç”±äºç‹¬ç«‹äºæ‰«æå™¨å‡ ä½•å’Œæ³¨å…¥æ´»åº¦æ°´å¹³è€Œå…·æœ‰æ½œåœ¨çš„ä¼˜åŠ¿ï¼Œä½†å®ƒä»¬æ²¡æœ‰æ˜¾å¼å»ºæ¨¡DMå…ˆéªŒä¸å™ªå£°æµ‹é‡æ•°æ®ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¯èƒ½é™åˆ¶äº†é‡å»ºçš„å‡†ç¡®æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç›‘ç£å¼PETé‡å»ºç®—æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼ºåˆ¶å®æ–½PETçš„Poissonä¼¼ç„¶æ¨¡å‹çš„éè´Ÿæ€§ï¼Œå¹¶é€‚åº”PETå›¾åƒçš„å®½å¼ºåº¦èŒƒå›´ã€‚åœ¨çœŸå®è„‘PET Phantomä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªå‰‚é‡æ°´å¹³ä¸Šå®šé‡ä¼˜äºæˆ–åŒ¹é…æœ€å…ˆè¿›åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œä»¥å±•ç¤ºæˆ‘ä»¬æ¨¡å‹ä¸­æè®®ç»„ä»¶çš„ä¼˜åŠ¿ï¼Œä»¥åŠå…¶ä¾èµ–äºè®­ç»ƒæ•°æ®ã€å‚æ•°è®¡æ•°å’Œæ‰©æ•£æ­¥éª¤æ•°é‡çš„ç¨‹åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ˜¾ç¤ºæˆ‘ä»¬çš„æ–¹æ³•æ¯”æ— ç›‘ç£çš„DMæ–¹æ³•èƒ½æ›´å‡†ç¡®åœ°ä¼°è®¡åéªŒåˆ†å¸ƒï¼Œè¿™æš—ç¤ºäº†æ”¹è¿›çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ–¹æ³•è®ºæ‰©å±•åˆ°ç”¨äºå…¨3D PETçš„å®é™…æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†æ¥è‡ªçœŸå®è„‘PETæ•°æ®çš„ç¤ºä¾‹ç»“æœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å·²ä½œä¸ºPETå›¾åƒé‡å»ºçš„æ­£åˆ™åŒ–å…ˆéªŒå¼•å…¥ã€‚</li>
<li>DMsç»“åˆäº†æ— ç›‘ç£æ–¹æ¡ˆå’ŒåŸºäºæµ‹é‡æ•°æ®çš„æ–¹æ¡ˆã€‚</li>
<li>è™½ç„¶DMså…·æœ‰æ½œåœ¨ä¼˜åŠ¿ï¼Œä½†å®ƒä»¬æœªæ˜¾å¼å»ºæ¨¡ä¸å™ªå£°æµ‹é‡æ•°æ®çš„ç›¸äº’ä½œç”¨ï¼Œå¯èƒ½é™åˆ¶äº†é‡å»ºå‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç›‘ç£å¼PETé‡å»ºç®—æ³•ï¼Œè¯¥ç®—æ³•å¼ºåˆ¶å®æ–½PETçš„Poissonä¼¼ç„¶æ¨¡å‹çš„éè´Ÿæ€§ï¼Œé€‚åº”PETå›¾åƒçš„å®½å¼ºåº¦èŒƒå›´ã€‚</li>
<li>åœ¨çœŸå®è„‘PET Phantomä¸Šçš„å®éªŒæ˜¾ç¤ºæ–°æ–¹æ³•ä¼˜äºæˆ–åŒ¹é…ç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ¶ˆèç ”ç©¶å±•ç¤ºäº†æ¨¡å‹ä¸­å„ç»„ä»¶çš„ä¼˜åŠ¿åŠå…¶ä¾èµ–æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.24034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fecc963fb0693ddb673fcd3bf11e7dd5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce3efb9efc84b62a768f3aa41e377bb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-983fab21fc0931ec52474d09da7031ee.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Controllable-Reference-Based-Real-World-Remote-Sensing-Image-Super-Resolution-with-Generative-Diffusion-Priors"><a href="#Controllable-Reference-Based-Real-World-Remote-Sensing-Image-Super-Resolution-with-Generative-Diffusion-Priors" class="headerlink" title="Controllable Reference-Based Real-World Remote Sensing Image   Super-Resolution with Generative Diffusion Priors"></a>Controllable Reference-Based Real-World Remote Sensing Image   Super-Resolution with Generative Diffusion Priors</h2><p><strong>Authors:Ce Wang, Wanjie Sun</strong></p>
<p>Super-resolution (SR) techniques can enhance the spatial resolution of remote sensing images by utilizing low-resolution (LR) images to reconstruct high-resolution (HR) images, enabling more efficient large-scale earth observation applications. While single-image super-resolution (SISR) methods have shown progress, reference-based super-resolution (RefSR) offers superior performance by incorporating historical HR images alongside current LR observations. However, existing RefSR methods struggle with real-world complexities, such as cross-sensor resolution gap and significant land cover changes, often leading to under-generation or over-reliance on reference image. To address these challenges, we propose CRefDiff, a novel controllable reference-based diffusion model for real-world remote sensing image SR. To address the under-generation problem, CRefDiff is built upon the pretrained Stable Diffusion model, leveraging its powerful generative prior to produce accurate structures and textures. To mitigate over-reliance on the reference, we introduce a dual-branch fusion mechanism that adaptively integrates both local and global information from the reference image. Moreover, this novel dual-branch design enables reference strength control during inference, enhancing interactivity and flexibility of the model. Finally, a strategy named Better Start is proposed to significantly reduce the number of denoising steps, thereby accelerating the inference process. To support further research, we introduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing images, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land cover changes and significant temporal gaps. Extensive experiments on Real-RefRSSRD show that CRefDiff achieves state-of-the-art performance across various metrics and improves downstream tasks such as scene classification and semantic segmentation. </p>
<blockquote>
<p>è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æŠ€æœ¯å¯ä»¥åˆ©ç”¨ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒé‡å»ºé«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å›¾åƒï¼Œä»è€Œæé«˜é¥æ„Ÿå›¾åƒçš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œä½¿å¤§è§„æ¨¡åœ°çƒè§‚æµ‹åº”ç”¨æ›´åŠ é«˜æ•ˆã€‚è™½ç„¶å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰æ–¹æ³•å·²ç»å–å¾—äº†è¿›å±•ï¼Œä½†åŸºäºå‚è€ƒçš„è¶…åˆ†è¾¨ç‡ï¼ˆRefSRï¼‰æ–¹æ³•é€šè¿‡ç»“åˆå†å²é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå½“å‰ä½åˆ†è¾¨ç‡è§‚æµ‹ï¼Œè¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RefSRæ–¹æ³•é¢ä¸´ç°å®ä¸–ç•Œçš„å¤æ‚æ€§æŒ‘æˆ˜ï¼Œå¦‚è·¨ä¼ æ„Ÿå™¨åˆ†è¾¨ç‡å·®è·å’ŒåœŸåœ°è¦†ç›–å˜åŒ–æ˜¾è‘—ç­‰é—®é¢˜ï¼Œè¿™å¸¸å¸¸å¯¼è‡´ç”Ÿæˆä¸è¶³æˆ–è¿‡åº¦ä¾èµ–å‚è€ƒå›¾åƒã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CRefDiffï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¯æ§å‚è€ƒçš„æ–°å‹é¥æ„Ÿå›¾åƒè¶…åˆ†è¾¨ç‡çš„æ‰©æ•£æ¨¡å‹ã€‚ä¸ºè§£å†³ç”Ÿæˆä¸è¶³çš„é—®é¢˜ï¼ŒCRefDiffå»ºç«‹åœ¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ä¹‹ä¸Šï¼Œåˆ©ç”¨å¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒæ¥äº§ç”Ÿå‡†ç¡®çš„ç»“æ„å’Œçº¹ç†ã€‚ä¸ºäº†å‡è½»å¯¹å‚è€ƒå›¾åƒçš„è¿‡åº¦ä¾èµ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒåˆ†æ”¯èåˆæœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°é›†æˆå‚è€ƒå›¾åƒçš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿™ç§æ–°å‹çš„åŒåˆ†æ”¯è®¾è®¡ä½¿å¾—åœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿè¿›è¡Œå‚è€ƒå¼ºåº¦æ§åˆ¶ï¼Œå¢å¼ºäº†æ¨¡å‹çš„äº¤äº’æ€§å’Œçµæ´»æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œæ›´å¥½å¼€å§‹â€çš„ç­–ç•¥ï¼Œä»¥å¤§å¤§å‡å°‘å»å™ªæ­¥éª¤çš„æ•°é‡ï¼Œä»è€ŒåŠ å¿«æ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†æ”¯æŒè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†Real-RefRSSRDï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„é¥æ„Ÿå›¾åƒç°å®ä¸–ç•Œçš„RefSRæ•°æ®é›†ï¼Œç”±é«˜åˆ†è¾¨ç‡çš„NAIPå’Œä½åˆ†è¾¨ç‡çš„Sentinel-2å›¾åƒå¯¹ç»„æˆï¼Œå…·æœ‰å¤šæ ·åŒ–çš„åœŸåœ°è¦†ç›–å˜åŒ–å’Œæ˜¾è‘—çš„æ—¶é—´é—´éš”ã€‚åœ¨Real-RefRSSRDä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCRefDiffåœ¨å„ç§æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ”¹è¿›äº†ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚åœºæ™¯åˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23801v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºé¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ§çš„å‚è€ƒåŸºç¡€æ‰©æ•£æ¨¡å‹CRefDiffï¼Œç”¨äºè§£å†³çœŸå®é¥æ„Ÿå›¾åƒSRä¸­çš„åˆ†è¾¨ç‡æå‡é—®é¢˜ã€‚CRefDiffé€šè¿‡å¼•å…¥åŒåˆ†æ”¯èåˆæœºåˆ¶å’ŒBetter Startç­–ç•¥ï¼Œè§£å†³äº†ç°æœ‰RefSRæ–¹æ³•é¢ä¸´çš„å‚è€ƒå›¾åƒä¾èµ–è¿‡åº¦å’Œç”Ÿæˆä¸è¶³çš„é—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹çš„äº¤äº’æ€§å’Œçµæ´»æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†æ–°çš„çœŸå®é¥æ„Ÿå›¾åƒRefSRæ•°æ®é›†Real-RefRSSRDï¼Œå®éªŒè¡¨æ˜CRefDiffåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶æé«˜äº†åœºæ™¯åˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä¸‹æ¸¸ä»»åŠ¡çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CRefDiffæ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆå…ˆéªŒï¼Œè§£å†³çœŸå®é¥æ„Ÿå›¾åƒSRä¸­çš„ç”Ÿæˆé—®é¢˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŒåˆ†æ”¯èåˆæœºåˆ¶ï¼ŒCRefDiffå‡è½»äº†è¿‡åº¦ä¾èµ–å‚è€ƒå›¾åƒçš„é—®é¢˜ï¼Œå¹¶è‡ªé€‚åº”åœ°èåˆäº†å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ã€‚</li>
<li>CRefDiffé¦–æ¬¡å®ç°äº†å‚è€ƒå¼ºåº¦çš„å¯æ§æ€§ï¼Œå¢å¼ºäº†æ¨¡å‹çš„äº¤äº’æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>æå‡ºçš„Better Startç­–ç•¥æ˜¾è‘—å‡å°‘äº†å»å™ªæ­¥éª¤çš„æ•°é‡ï¼Œä»è€ŒåŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„çœŸå®é¥æ„Ÿå›¾åƒRefSRæ•°æ®é›†Real-RefRSSRDï¼ŒåŒ…å«å…·æœ‰å„ç§åœŸåœ°è¦†ç›–å˜åŒ–å’Œæ˜¾è‘—æ—¶é—´é—´éš”çš„HR NAIPå’ŒLR Sentinel-2å›¾åƒå¯¹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒCRefDiffåœ¨å„ç§æŒ‡æ ‡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-676036f6b6276a638430b327dc2cd493.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-535df51da62a899bf23fc8c3debe8cf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-613ba235b910dc4365a56a1590bd14d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-670339906e4d1b049a339ca8b7add6de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc7579f50813aa11a35ab19ebc3393af.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Proteus-ID-ID-Consistent-and-Motion-Coherent-Video-Customization"><a href="#Proteus-ID-ID-Consistent-and-Motion-Coherent-Video-Customization" class="headerlink" title="Proteus-ID: ID-Consistent and Motion-Coherent Video Customization"></a>Proteus-ID: ID-Consistent and Motion-Coherent Video Customization</h2><p><strong>Authors:Guiyu Zhang, Chen Shi, Zijian Jiang, Xunzhi Xiang, Jingjing Qian, Shaoshuai Shi, Li Jiang</strong></p>
<p>Video identity customization seeks to synthesize realistic, temporally coherent videos of a specific subject, given a single reference image and a text prompt. This task presents two core challenges: (1) maintaining identity consistency while aligning with the described appearance and actions, and (2) generating natural, fluid motion without unrealistic stiffness. To address these challenges, we introduce Proteus-ID, a novel diffusion-based framework for identity-consistent and motion-coherent video customization. First, we propose a Multimodal Identity Fusion (MIF) module that unifies visual and textual cues into a joint identity representation using a Q-Former, providing coherent guidance to the diffusion model and eliminating modality imbalance. Second, we present a Time-Aware Identity Injection (TAII) mechanism that dynamically modulates identity conditioning across denoising steps, improving fine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a self-supervised strategy that reweights the training loss based on optical-flow-derived motion heatmaps, enhancing motion realism without requiring additional inputs. To support this task, we construct Proteus-Bench, a high-quality dataset comprising 200K curated clips for training and 150 individuals from diverse professions and ethnicities for evaluation. Extensive experiments demonstrate that Proteus-ID outperforms prior methods in identity preservation, text alignment, and motion quality, establishing a new benchmark for video identity customization. Codes and data are publicly available at <a target="_blank" rel="noopener" href="https://grenoble-zhang.github.io/Proteus-ID/">https://grenoble-zhang.github.io/Proteus-ID/</a>. </p>
<blockquote>
<p>è§†é¢‘èº«ä»½å®šåˆ¶æ—¨åœ¨æ ¹æ®å•å¼ å‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºåˆæˆç‰¹å®šä¸»ä½“çš„ç°å®ä¸”æ—¶é—´ä¸Šè¿è´¯çš„è§†é¢‘ã€‚æ­¤ä»»åŠ¡é¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š(1)åœ¨ç¬¦åˆæè¿°çš„å¤–è²Œå’Œè¡Œä¸ºçš„åŒæ—¶ä¿æŒèº«ä»½ä¸€è‡´æ€§ï¼›(2)ç”Ÿæˆè‡ªç„¶æµç•…çš„åŠ¨ä½œï¼Œé¿å…ä¸ç°å®çš„åƒµç¡¬ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Proteus-IDï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„èº«ä»½ä¸€è‡´ä¸”è¿åŠ¨è¿è´¯çš„è§†é¢‘å®šåˆ¶æ–°å‹æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€èº«ä»½èåˆï¼ˆMIFï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨Q-Formerå°†è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ç»Ÿä¸€ä¸ºè”åˆèº«ä»½è¡¨ç¤ºï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›è¿è´¯çš„æŒ‡å¯¼ï¼Œå¹¶æ¶ˆé™¤æ¨¡æ€ä¸å¹³è¡¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶é—´æ„ŸçŸ¥èº«ä»½æ³¨å…¥ï¼ˆTAIIï¼‰æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åŠ¨æ€è°ƒèŠ‚å»å™ªæ­¥éª¤ä¸­çš„èº«ä»½æ¡ä»¶ï¼Œæ”¹è¿›äº†ç²¾ç»†ç»†èŠ‚çš„é‡æ„ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”è¿åŠ¨å­¦ä¹ ï¼ˆAMLï¼‰ç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§è‡ªç›‘ç£ç­–ç•¥ï¼Œæ ¹æ®å…‰å­¦æµåŠ¨æ´¾ç”Ÿçš„è¿åŠ¨çƒ­å›¾é‡æ–°è®­ç»ƒæŸå¤±æƒé‡ï¼Œå¢å¼ºè¿åŠ¨çœŸå®æ„Ÿï¼Œè€Œæ— éœ€é¢å¤–çš„è¾“å…¥ã€‚ä¸ºäº†æ”¯æŒè¿™é¡¹ä»»åŠ¡ï¼Œæˆ‘ä»¬æ„å»ºäº†Proteus-Benchæ•°æ®é›†ï¼ŒåŒ…å«20ä¸‡ä¸ªé«˜è´¨é‡ç²¾é€‰ç‰‡æ®µç”¨äºè®­ç»ƒå’Œæ¥è‡ªä¸åŒèŒä¸šå’Œç§æ—èƒŒæ™¯çš„150ä¸ªä¸ªä½“ç”¨äºè¯„ä¼°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒProteus-IDåœ¨èº«ä»½ä¿ç•™ã€æ–‡æœ¬å¯¹é½å’Œè¿åŠ¨è´¨é‡æ–¹é¢ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œä¸ºè§†é¢‘èº«ä»½å®šåˆ¶æ ‘ç«‹äº†æ–°åŸºå‡†ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://grenoble-zhang.github.io/Proteus-ID/%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://grenoble-zhang.github.io/Proteus-ID/ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23729v1">PDF</a> Preprint. Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹è§†é¢‘èº«ä»½å®šåˆ¶æ¡†æ¶Proteus-IDï¼Œç”¨äºåˆæˆä¸ç»™å®šå‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¸€è‡´çš„è§†é¢‘ã€‚å®ƒé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ä¿æŒèº«ä»½ä¸€è‡´æ€§å¹¶ç”Ÿæˆè‡ªç„¶ã€æµç•…çš„è¿åŠ¨ã€‚é€šè¿‡å¼•å…¥Multimodal Identity Fusionï¼ˆMIFï¼‰æ¨¡å—ã€Time-Aware Identity Injectionï¼ˆTAIIï¼‰æœºåˆ¶å’ŒAdaptive Motion Learningï¼ˆAMLï¼‰ç­–ç•¥ï¼ŒProteus-IDåœ¨èº«ä»½ä¿ç•™ã€æ–‡æœ¬å¯¹é½å’Œè¿åŠ¨è´¨é‡æ–¹é¢å‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æ„å»ºäº†Proteus-Benchæ•°æ®é›†ä»¥æ”¯æŒæ­¤ä»»åŠ¡ï¼Œå®éªŒç»“æœè¯æ˜äº†å…¶æ€§èƒ½ä¼˜è¶Šæ€§ã€‚è¯¥æ–¹æ³•çš„ä»£ç å’Œæ•°æ®å·²åœ¨ç½‘ä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡ä¸­æ¶‰åŠçš„å…³é”®ç‚¹æ¦‚è§ˆï¼š</p>
<ul>
<li>è§†é¢‘èº«ä»½å®šåˆ¶çš„ç›®æ ‡æ˜¯åˆæˆä¸ç»™å®šå‚è€ƒå›¾åƒå’Œæ–‡æœ¬æç¤ºä¸€è‡´çš„è§†é¢‘ã€‚</li>
<li>ä¸»è¦æŒ‘æˆ˜åœ¨äºä¿æŒèº«ä»½ä¸€è‡´æ€§å¹¶ç”Ÿæˆè‡ªç„¶ã€æµç•…çš„è¿åŠ¨ã€‚</li>
<li>Proteus-IDæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹è§†é¢‘èº«ä»½å®šåˆ¶æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å¤šç§ç­–ç•¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>MIFæ¨¡å—ç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼Œå½¢æˆè”åˆèº«ä»½è¡¨ç¤ºï¼Œä¸ºæ‰©æ•£æ¨¡å‹æä¾›è¿è´¯æŒ‡å¯¼å¹¶æ¶ˆé™¤æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>TAIIæœºåˆ¶åŠ¨æ€è°ƒèŠ‚èº«ä»½æ¡ä»¶åœ¨é™å™ªæ­¥éª¤ä¸­çš„åº”ç”¨ï¼Œæ”¹è¿›äº†ç²¾ç»†ç»†èŠ‚çš„é‡å»ºã€‚</li>
<li>AMLæ˜¯ä¸€ç§è‡ªç›‘ç£ç­–ç•¥ï¼ŒåŸºäºå…‰å­¦æµè¡ç”Ÿçš„è¿åŠ¨çƒ­å›¾é‡æ–°è®­ç»ƒæŸå¤±æƒé‡ï¼Œæé«˜è¿åŠ¨çœŸå®æ„Ÿã€‚</li>
<li>ä¸ºæ”¯æŒæ­¤ä»»åŠ¡ï¼Œæ„å»ºäº†Proteus-Benchæ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡é«˜è´¨é‡è§†é¢‘å‰ªè¾‘ç”¨äºè®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8f5781afb75f1b9b4b77f9bfb4da2ea1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df2b01a0404c361534264ee100f7db4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ca0571759ee25b21e81bdcb71561f23.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MDPG-Multi-domain-Diffusion-Prior-Guidance-for-MRI-Reconstruction"><a href="#MDPG-Multi-domain-Diffusion-Prior-Guidance-for-MRI-Reconstruction" class="headerlink" title="MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction"></a>MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction</h2><p><strong>Authors:Lingtong Zhang, Mengdie Song, Xiaohan Hao, Huayu Mai, Bensheng Qiu</strong></p>
<p>Magnetic Resonance Imaging (MRI) reconstruction is essential in medical diagnostics. As the latest generative models, diffusion models (DMs) have struggled to produce high-fidelity images due to their stochastic nature in image domains. Latent diffusion models (LDMs) yield both compact and detailed prior knowledge in latent domains, which could effectively guide the model towards more effective learning of the original data distribution. Inspired by this, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by pre-trained LDMs to enhance data consistency in MRI reconstruction tasks. Specifically, we first construct a Visual-Mamba-based backbone, which enables efficient encoding and reconstruction of under-sampled images. Then pre-trained LDMs are integrated to provide conditional priors in both latent and image domains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion in multi-level latent domains. Simultaneously, to effectively utilize a prior in both the k-space and image domain, under-sampled images are fused with generated full-sampled images by the Dual-domain Fusion Branch (DFB) for self-adaption guidance. Lastly, to further enhance the data consistency, we propose a k-space regularization strategy based on the non-auto-calibration signal (NACS) set. Extensive experiments on two public MRI datasets fully demonstrate the effectiveness of the proposed methodology. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Zolento/MDPG">https://github.com/Zolento/MDPG</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰é‡å»ºåœ¨åŒ»å­¦è¯Šæ–­ä¸­è‡³å…³é‡è¦ã€‚ä½œä¸ºæœ€æ–°çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ç”±äºå…¶åœ¨å›¾åƒåŸŸä¸­çš„éšæœºæ€§ï¼Œéš¾ä»¥äº§ç”Ÿé«˜ä¿çœŸå›¾åƒã€‚æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨æ½œåœ¨åŸŸä¸­äº§ç”Ÿç´§å‡‘ä¸”è¯¦ç»†çš„å…ˆéªŒçŸ¥è¯†ï¼Œè¿™å¯ä»¥æœ‰æ•ˆåœ°å¼•å¯¼æ¨¡å‹æ›´æœ‰æ•ˆåœ°å­¦ä¹ åŸå§‹æ•°æ®åˆ†å¸ƒã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ç”±é¢„è®­ç»ƒçš„LDMsæä¾›çš„å¤šåŸŸæ‰©æ•£å…ˆéªŒæŒ‡å¯¼ï¼ˆMDPGï¼‰ï¼Œä»¥å¢å¼ºMRIé‡å»ºä»»åŠ¡ä¸­çš„æ•°æ®ä¸€è‡´æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŸºäºVisual-Mambaçš„éª¨å¹²ç½‘ï¼Œè¯¥éª¨å¹²ç½‘èƒ½å¤Ÿå®ç°æ¬ é‡‡æ ·å›¾åƒçš„æœ‰æ•ˆç¼–ç å’Œé‡å»ºã€‚ç„¶åï¼Œå°†é¢„è®­ç»ƒçš„LDMsé›†æˆåˆ°æ½œåœ¨åŸŸå’Œå›¾åƒåŸŸä¸­ï¼Œä»¥æä¾›æ¡ä»¶å…ˆéªŒã€‚æå‡ºäº†ä¸€ç§æ–°å‹çš„æ½œåœ¨å¼•å¯¼æ³¨æ„åŠ›ï¼ˆLGAï¼‰ï¼Œç”¨äºåœ¨å¤šå±‚æ¬¡æ½œåœ¨åŸŸä¸­è¿›è¡Œæœ‰æ•ˆèåˆã€‚åŒæ—¶ï¼Œä¸ºäº†æœ‰æ•ˆåˆ©ç”¨kç©ºé—´å’Œå›¾åƒåŸŸä¸­çš„å…ˆéªŒä¿¡æ¯ï¼Œæ¬ é‡‡æ ·å›¾åƒé€šè¿‡åŒåŸŸèåˆåˆ†æ”¯ï¼ˆDFBï¼‰ä¸ç”Ÿæˆçš„å®Œæ•´é‡‡æ ·å›¾åƒèåˆï¼Œä»¥å®ç°è‡ªé€‚åº”å¼•å¯¼ã€‚æœ€åï¼Œä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºæ•°æ®ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéè‡ªåŠ¨æ ¡å‡†ä¿¡å·ï¼ˆNACSï¼‰é›†çš„kç©ºé—´æ­£åˆ™åŒ–ç­–ç•¥ã€‚åœ¨ä¸¤ä¸ªå…¬å…±MRIæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒå……åˆ†è¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Zolento/MDPG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Zolento/MDPGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23701v1">PDF</a> Accept by MICCAI2025</p>
<p><strong>Summary</strong></p>
<pre><code> æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¤šåŸŸæ‰©æ•£å…ˆéªŒæŒ‡å¯¼ï¼ˆMDPGï¼‰çš„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰é‡å»ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰æä¾›æ¡ä»¶å…ˆéªŒï¼Œé€šè¿‡è§†è§‰ç›çˆï¼ˆVisual-Mambaï¼‰ä¸ºåŸºç¡€çš„åå¤‡æ¶æ„å®ç°é«˜æ•ˆç¼–ç å’Œé‡å»ºã€‚åŒæ—¶ï¼Œå¼•å…¥æ½œåœ¨å¼•å¯¼æ³¨æ„åŠ›ï¼ˆLGAï¼‰å®ç°å¤šçº§æ½œåœ¨åŸŸçš„æœ‰æ•ˆèåˆï¼Œå¹¶é€šè¿‡åŒåŸŸèåˆåˆ†æ”¯ï¼ˆDFBï¼‰è¿›è¡Œè‡ªé€‚åº”æŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§åŸºäºéè‡ªåŠ¨æ ¡å‡†ä¿¡å·ï¼ˆNACSï¼‰é›†çš„k-ç©ºé—´æ­£åˆ™åŒ–ç­–ç•¥ï¼Œè¿›ä¸€æ­¥æé«˜æ•°æ®ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨å…¬å…±MRIæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨ç”Ÿæˆé«˜ä¿çœŸå›¾åƒæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨å›¾åƒåŸŸçš„éšæœºæ€§å½±å“å…¶è¡¨ç°ã€‚</li>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨æ½œåœ¨åŸŸæä¾›ç´§å‡‘å’Œè¯¦ç»†çš„å…ˆéªŒçŸ¥è¯†ï¼Œèƒ½æœ‰æ•ˆæŒ‡å¯¼æ¨¡å‹å­¦ä¹ åŸå§‹æ•°æ®åˆ†å¸ƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒLDMsçš„å¤šåŸŸæ‰©æ•£å…ˆéªŒæŒ‡å¯¼ï¼ˆMDPGï¼‰æ–¹æ³•ï¼Œç”¨äºå¢å¼ºMRIé‡å»ºä»»åŠ¡ä¸­çš„æ•°æ®ä¸€è‡´æ€§ã€‚</li>
<li>åˆ©ç”¨è§†è§‰ç›çˆä¸ºåŸºç¡€çš„åå¤‡æ¶æ„å®ç°é«˜æ•ˆç¼–ç å’Œé‡å»ºã€‚</li>
<li>å¼•å…¥æ½œåœ¨å¼•å¯¼æ³¨æ„åŠ›ï¼ˆLGAï¼‰å®ç°å¤šçº§åˆ«æ½œåœ¨åŸŸçš„æœ‰æ•ˆèåˆã€‚</li>
<li>é€šè¿‡åŒåŸŸèåˆåˆ†æ”¯ï¼ˆDFBï¼‰è¿›è¡Œè‡ªé€‚åº”æŒ‡å¯¼ï¼Œåˆ©ç”¨k-ç©ºé—´å’Œéè‡ªåŠ¨æ ¡å‡†ä¿¡å·ï¼ˆNACSï¼‰çš„å…ˆéªŒä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23701">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b156120b0d5bd5512c9e5c364dee62e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-835586027be10aa53b8c30174edaa4c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b211d62e7ce29b3504d19d80533492d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Unified-Framework-for-Stealthy-Adversarial-Generation-via-Latent-Optimization-and-Transferability-Enhancement"><a href="#A-Unified-Framework-for-Stealthy-Adversarial-Generation-via-Latent-Optimization-and-Transferability-Enhancement" class="headerlink" title="A Unified Framework for Stealthy Adversarial Generation via Latent   Optimization and Transferability Enhancement"></a>A Unified Framework for Stealthy Adversarial Generation via Latent   Optimization and Transferability Enhancement</h2><p><strong>Authors:Gaozheng Pei, Ke Ma, Dongpeng Zhang, Chengzhi Sun, Qianqian Xu, Qingming Huang</strong></p>
<p>Due to their powerful image generation capabilities, diffusion-based adversarial example generation methods through image editing are rapidly gaining popularity. However, due to reliance on the discriminative capability of the diffusion model, these diffusion-based methods often struggle to generalize beyond conventional image classification tasks, such as in Deepfake detection. Moreover, traditional strategies for enhancing adversarial example transferability are challenging to adapt to these methods. To address these challenges, we propose a unified framework that seamlessly incorporates traditional transferability enhancement strategies into diffusion model-based adversarial example generation via image editing, enabling their application across a wider range of downstream tasks. Our method won first place in the â€œ1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of AI-Generated Mediaâ€ competition at ACM MM25, which validates the effectiveness of our approach. </p>
<blockquote>
<p>ç”±äºåŸºäºæ‰©æ•£æ¨¡å‹çš„å¯¹æŠ—æ ·æœ¬ç”Ÿæˆæ–¹æ³•é€šè¿‡å›¾åƒç¼–è¾‘å…·å¤‡å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå®ƒä»¬æ­£è¿…é€Ÿæµè¡Œèµ·æ¥ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›åŸºäºæ‰©æ•£çš„æ–¹æ³•ä¾èµ–äºæ‰©æ•£æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œå®ƒä»¬å¾€å¾€åœ¨è¶…è¶Šä¼ ç»Ÿå›¾åƒåˆ†ç±»ä»»åŠ¡æ—¶é‡åˆ°éš¾é¢˜ï¼Œä¾‹å¦‚åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­ã€‚æ­¤å¤–ï¼Œå°†ä¼ ç»Ÿå¢å¼ºå¯¹æŠ—æ ·æœ¬è¿ç§»æ€§çš„ç­–ç•¥åº”ç”¨äºè¿™äº›æ–¹æ³•æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡å›¾åƒç¼–è¾‘å°†ä¼ ç»Ÿè¿ç§»å¢å¼ºç­–ç•¥æ— ç¼çº³å…¥åŸºäºæ‰©æ•£æ¨¡å‹çš„å¯¹æŠ—æ ·æœ¬ç”Ÿæˆæ–¹æ³•ä¸­ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºæ›´å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ACM MM25ä¸¾åŠçš„â€œé¦–ä¸ªé’ˆå¯¹æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨çš„å¯¹æŠ—æ€§æ”»å‡»ï¼šäººå·¥æ™ºèƒ½åª’ä½“æ—¶ä»£çš„æ–°æŒ‘æˆ˜â€ç«èµ›ä¸­è·å¾—ç¬¬ä¸€åï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23676v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åŸºäºå›¾åƒç¼–è¾‘çš„å¯¹æŠ—æ ·æœ¬ç”Ÿæˆæ–¹æ³•å› å…¶å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›è€Œè¿…é€Ÿæµè¡Œã€‚ç„¶è€Œï¼Œç”±äºä¾èµ–äºæ‰©æ•£æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œè¿™äº›æ–¹æ³•åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ç­‰éå¸¸è§„å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šå¾€å¾€éš¾ä»¥æ¨å¹¿ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œé€šè¿‡å›¾åƒç¼–è¾‘å°†ä¼ ç»Ÿçš„å¯è½¬ç§»æ€§å¢å¼ºç­–ç•¥æ— ç¼åœ°èå…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸ºåŸºç¡€çš„å¯¹æŠ—æ ·æœ¬ç”Ÿæˆä¸­ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿåº”ç”¨äºæ›´å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ACM MM25çš„â€œç¬¬ä¸€å±Šé’ˆå¯¹æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨çš„å¯¹æŠ—æ€§æ”»å‡»ï¼šäººå·¥æ™ºèƒ½ç”Ÿæˆåª’ä½“æ—¶ä»£çš„æŒ‘æˆ˜â€ç«èµ›ä¸­è£è·ç¬¬ä¸€åï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åŸºäºå›¾åƒç¼–è¾‘çš„å¯¹æŠ—æ ·æœ¬ç”Ÿæˆæ–¹æ³•å…·æœ‰å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œå› æ­¤å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ç”±äºä¾èµ–æ‰©æ•£æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œè¿™äº›æ–¹æ³•åœ¨éå¸¸è§„å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„æ¨å¹¿å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†ä¼ ç»Ÿå¯è½¬ç§»æ€§å¢å¼ºç­–ç•¥èå…¥æ‰©æ•£æ¨¡å‹å¯¹æŠ—æ ·æœ¬ç”Ÿæˆä¸­ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å›¾åƒç¼–è¾‘å®ç°ä¼ ç»Ÿä¸æ‰©æ•£æ¨¡å‹æ–¹æ³•çš„ç»“åˆï¼Œæé«˜äº†æ–¹æ³•çš„é€‚ç”¨æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ACM MM25ç«èµ›ä¸­è£è·ç¬¬ä¸€åï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>æ­¤æ–¹æ³•æœ‰åŠ©äºæé«˜æ‰©æ•£æ¨¡å‹åœ¨éå¸¸è§„å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä¸ºæ·±åº¦ä¼ªé€ æ£€æµ‹ç­‰é¢†åŸŸæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df13db7e177d2d20eb74366d78b34d52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c733445e6edd2efccecc0e72bb7d61a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72f2edef43e32a49203a9f55f93c464a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3c2cec0056d8441074be8d6ce17e1c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6a67a6bca397dab61ad731cdad32d27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-202aa195e8385ea02e8e274192d44191.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Diffusion-Model-based-Data-Augmentation-Method-for-Fetal-Head-Ultrasound-Segmentation"><a href="#Diffusion-Model-based-Data-Augmentation-Method-for-Fetal-Head-Ultrasound-Segmentation" class="headerlink" title="Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound   Segmentation"></a>Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound   Segmentation</h2><p><strong>Authors:Fangyijie Wang, Kevin Whelan, FÃ©lix Balado, GuÃ©nolÃ© Silvestre, Kathleen M. Curran</strong></p>
<p>Medical image data is less accessible than in other domains due to privacy and regulatory constraints. In addition, labeling requires costly, time-intensive manual image annotation by clinical experts. To overcome these challenges, synthetic medical data generation offers a promising solution. Generative AI (GenAI), employing generative deep learning models, has proven effective at producing realistic synthetic images. This study proposes a novel mask-guided GenAI approach using diffusion models to generate synthetic fetal head ultrasound images paired with segmentation masks. These synthetic pairs augment real datasets for supervised fine-tuning of the Segment Anything Model (SAM). Our results show that the synthetic data captures real image features effectively, and this approach reaches state-of-the-art fetal head segmentation, especially when trained with a limited number of real image-mask pairs. In particular, the segmentation reaches Dice Scores of 94.66% and 94.38% using a handful of ultrasound images from the Spanish and African cohorts, respectively. Our code, models, and data are available on GitHub. </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒæ•°æ®ç”±äºéšç§å’Œç›‘ç®¡é™åˆ¶ï¼Œç›¸è¾ƒäºå…¶ä»–é¢†åŸŸæ›´éš¾ä»¥è·å–ã€‚æ­¤å¤–ï¼Œæ ‡æ³¨éœ€è¦ä¸´åºŠä¸“å®¶è¿›è¡Œæ˜‚è´µä¸”è€—æ—¶çš„æ‰‹åŠ¨å›¾åƒæ³¨é‡Šã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼ŒåˆæˆåŒ»ç–—æ•°æ®ç”Ÿæˆæä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚é‡‡ç”¨ç”Ÿæˆå¼æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ç”Ÿæˆäººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰å·²è¢«è¯æ˜åœ¨ç”Ÿæˆé€¼çœŸçš„åˆæˆå›¾åƒæ–¹é¢éå¸¸æœ‰æ•ˆã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ©è†œå¼•å¯¼GenAIæ–¹æ³•ï¼Œç”Ÿæˆåˆæˆèƒå„¿å¤´éƒ¨è¶…å£°å›¾åƒåŠå…¶åˆ†å‰²æ©è†œé…å¯¹ã€‚è¿™äº›åˆæˆé…å¯¹æ•°æ®å¢å¼ºäº†çœŸå®æ•°æ®é›†ï¼Œç”¨äºç›‘ç£å¾®è°ƒSegment Anything Modelï¼ˆSAMï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåˆæˆæ•°æ®æœ‰æ•ˆåœ°æ•æ‰äº†çœŸå®å›¾åƒçš„ç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨æœ‰é™æ•°é‡çš„çœŸå®å›¾åƒ-æ©è†œé…å¯¹è¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„èƒå„¿å¤´éƒ¨åˆ†å‰²æ•ˆæœã€‚ç‰¹åˆ«æ˜¯ï¼Œä½¿ç”¨æ¥è‡ªè¥¿ç­ç‰™å’Œéæ´²é˜Ÿåˆ—çš„å°‘é‡è¶…å£°å›¾åƒï¼Œåˆ†å‰²ç‡è¾¾åˆ°Dice Scoreçš„94.66%å’Œ94.38%ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å¯åœ¨GitHubä¸Šè·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23664v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¸¦æœ‰åˆ†å‰²æ©è†œåˆæˆèƒå„¿å¤´éƒ¨è¶…å£°å›¾åƒçš„æ–¹æ³•ã€‚åˆæˆæ•°æ®ç”¨äºå¢å¼ºçœŸå®æ•°æ®é›†ï¼Œä»¥ç›‘ç£å¾®è°ƒåˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œåˆæˆæ•°æ®èƒ½æœ‰æ•ˆæ•æ‰çœŸå®å›¾åƒç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å°‘é‡çœŸå®å›¾åƒ-æ©è†œå¯¹è¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¾¾åˆ°å…ˆè¿›çš„èƒå„¿å¤´éƒ¨åˆ†å‰²æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ•°æ®ç”±äºéšç§å’Œç›‘ç®¡é™åˆ¶è€Œéš¾ä»¥è·å–ã€‚</li>
<li>åŒ»å­¦å›¾åƒæ ‡æ³¨éœ€è¦ä¸´åºŠä¸“å®¶è€—æ—¶è€—åŠ›çš„æ‰‹åŠ¨æ ‡æ³¨ã€‚</li>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰ä½¿ç”¨ç”Ÿæˆå¼æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½æœ‰æ•ˆç”Ÿæˆé€¼çœŸçš„åˆæˆå›¾åƒã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„mask-guided GenAIæ–¹æ³•ï¼Œç”¨äºç”Ÿæˆåˆæˆèƒå„¿å¤´éƒ¨è¶…å£°å›¾åƒåŠå…¶åˆ†å‰²æ©è†œã€‚</li>
<li>åˆæˆæ•°æ®å¯¹çœŸå®æ•°æ®é›†è¿›è¡Œå¢å¼ºï¼Œç”¨äºç›‘ç£å¾®è°ƒSAMæ¨¡å‹ã€‚</li>
<li>åˆæˆæ•°æ®èƒ½æœ‰æ•ˆæ•æ‰çœŸå®å›¾åƒç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2423c23496a4f78b83a54ddaecfd87c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dce93e8b19db2c3d8d9015eb56e0908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7b6a8a7abfb2aabf66b7f083e5bb127.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TurboVSR-Fantastic-Video-Upscalers-and-Where-to-Find-Them"><a href="#TurboVSR-Fantastic-Video-Upscalers-and-Where-to-Find-Them" class="headerlink" title="TurboVSR: Fantastic Video Upscalers and Where to Find Them"></a>TurboVSR: Fantastic Video Upscalers and Where to Find Them</h2><p><strong>Authors:Zhongdao Wang, Guodongfang Zhao, Jingjing Ren, Bailan Feng, Shifeng Zhang, Wenbo Li</strong></p>
<p>Diffusion-based generative models have demonstrated exceptional promise in the video super-resolution (VSR) task, achieving a substantial advancement in detail generation relative to prior methods. However, these approaches face significant computational efficiency challenges. For instance, current techniques may require tens of minutes to super-resolve a mere 2-second, 1080p video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based video super-resolution model. Our core design comprises three key aspects: (1) We employ an autoencoder with a high compression ratio of 32$\times$32$\times$8 to reduce the number of tokens. (2) Highly compressed latents pose substantial challenges for training. We introduce factorized conditioning to mitigate the learning complexity: we first learn to super-resolve the initial frame; subsequently, we condition the super-resolution of the remaining frames on the high-resolution initial frame and the low-resolution subsequent frames. (3) We convert the pre-trained diffusion model to a shortcut model to enable fewer sampling steps, further accelerating inference. As a result, TurboVSR performs on par with state-of-the-art VSR methods, while being 100+ times faster, taking only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports image resolution by considering image as a one-frame video. Our efficient design makes SR beyond 1080p possible, results on 4K (3648$\times$2048) image SR show surprising fine details. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹åœ¨è§†é¢‘è¶…åˆ†è¾¨ç‡ï¼ˆVSRï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œç›¸å¯¹äºä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨ç»†èŠ‚ç”Ÿæˆæ–¹é¢å–å¾—äº†å®è´¨æ€§çš„è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é¢ä¸´ç€è®¡ç®—æ•ˆç‡çš„é‡å¤§æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œå½“å‰çš„æŠ€æœ¯å¯èƒ½éœ€è¦æ•°åˆ†é’Ÿæ¥å¯¹ä¸€ä¸ªä»…2ç§’ã€1080pçš„è§†é¢‘è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TurboVSRï¼Œä¸€ç§é«˜æ•ˆçš„åŸºäºæ‰©æ•£çš„è§†é¢‘è¶…åˆ†è¾¨ç‡æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒè®¾è®¡åŒ…æ‹¬ä¸‰ä¸ªæ–¹é¢ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬é‡‡ç”¨å‹ç¼©æ¯”ä¸º32Ã—32Ã—8çš„è‡ªç¼–ç å™¨æ¥å‡å°‘ä»¤ç‰Œæ•°é‡ã€‚ï¼ˆ2ï¼‰é«˜åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´ç»™è®­ç»ƒå¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†å› å­åŒ–æ¡ä»¶æ¥å‡è½»å­¦ä¹ å¤æ‚æ€§ï¼šæˆ‘ä»¬é¦–å…ˆå­¦ä¹ å¯¹åˆå§‹å¸§è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ï¼Œç„¶åå°†å…¶ä½™å¸§çš„è¶…åˆ†è¾¨ç‡æ¡ä»¶è®¾ç½®ä¸ºé«˜åˆ†è¾¨ç‡åˆå§‹å¸§å’Œä½åˆ†è¾¨ç‡åç»­å¸§ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è½¬æ¢ä¸ºå¿«æ·æ–¹å¼æ¨¡å‹ï¼Œä»¥å‡å°‘é‡‡æ ·æ­¥éª¤ï¼Œä»è€Œè¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ã€‚å› æ­¤ï¼ŒTurboVSRçš„æ€§èƒ½ä¸æœ€å…ˆè¿›çš„VSRæ–¹æ³•ç›¸å½“ï¼Œä½†é€Ÿåº¦æé«˜äº†100å€ä»¥ä¸Šï¼Œä»…éœ€7ç§’å³å¯å¤„ç†ä¸€ä¸ª2ç§’çš„1080pè§†é¢‘ã€‚TurboVSRè¿˜æ”¯æŒå°†å›¾åƒè§†ä¸ºå•å¸§è§†é¢‘è¿›è¡Œåˆ†è¾¨ç‡æå‡ã€‚æˆ‘ä»¬é«˜æ•ˆçš„è®¾è®¡ä½¿å¾—è¶…è¿‡1080pçš„SRæˆä¸ºå¯èƒ½ï¼Œåœ¨4Kï¼ˆ3648Ã—2048ï¼‰å›¾åƒSRçš„ç»“æœæ˜¾ç¤ºå‡ºæƒŠäººçš„ç»†èŠ‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23618v1">PDF</a> ICCV, 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘è¶…åˆ†è¾¨ç‡ï¼ˆVSRï¼‰ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨ç»†èŠ‚ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šé¢ä¸´æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œç°æœ‰æŠ€æœ¯å¯èƒ½éœ€è¦æ•°åˆ†é’Ÿæ‰èƒ½å¯¹ä»…2ç§’ã€1080pçš„è§†é¢‘è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ã€‚æœ¬æ–‡æå‡ºTurboVSRï¼Œä¸€ç§é«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹è§†é¢‘è¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚å…¶æ ¸å¿ƒè®¾è®¡åŒ…æ‹¬ä¸‰ä¸ªæ–¹é¢ï¼šé¦–å…ˆï¼Œé‡‡ç”¨å‹ç¼©æ¯”ä¸º32Ã—32Ã—8çš„è‡ªç¼–ç å™¨å‡å°‘ä»¤ç‰Œæ•°é‡ã€‚å…¶æ¬¡ï¼Œé’ˆå¯¹é«˜åº¦å‹ç¼©çš„æ½œåœ¨é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥å› å­åŒ–æ¡ä»¶æ¥å‡è½»å­¦ä¹ å¤æ‚æ€§ï¼šé¦–å…ˆå­¦ä¹ è¶…åˆ†è¾¨ç‡åˆå§‹å¸§ï¼Œç„¶åä»¥å…¶é«˜åˆ†è¾¨ç‡åˆå§‹å¸§å’Œä½åˆ†è¾¨ç‡åç»­å¸§ä¸ºæ¡ä»¶è¿›è¡Œå‰©ä½™å¸§çš„è¶…åˆ†è¾¨ç‡å¤„ç†ã€‚æœ€åï¼Œæˆ‘ä»¬å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è½¬æ¢ä¸ºå¿«æ·æ–¹å¼æ¨¡å‹ï¼Œä»¥å‡å°‘é‡‡æ ·æ­¥éª¤ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ã€‚å› æ­¤ï¼ŒTurboVSRæ€§èƒ½ä¸æœ€å…ˆè¿›çš„VSRæ–¹æ³•ç›¸å½“ï¼Œä½†é€Ÿåº¦æé«˜äº†100å€ä»¥ä¸Šï¼Œä»…éœ€7ç§’å³å¯å¤„ç†2ç§’é•¿çš„1080pè§†é¢‘ã€‚æ­¤å¤–ï¼ŒTurboVSRé€šè¿‡å°†å›¾åƒè§†ä¸ºå•å¸§è§†é¢‘æ¥æ”¯æŒå›¾åƒåˆ†è¾¨ç‡æå‡ï¼Œå…¶é«˜æ•ˆè®¾è®¡ä½¿å¾—è¶…è¿‡1080pçš„SRæˆä¸ºå¯èƒ½ï¼Œåœ¨4Kï¼ˆ3648Ã—2048ï¼‰å›¾åƒSRçš„ç»“æœæ˜¾ç¤ºå‡ºæƒŠäººçš„ç»†èŠ‚ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘è¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œåœ¨ç»†èŠ‚ç”Ÿæˆæ–¹é¢å–å¾—é‡å¤§è¿›å±•ã€‚</li>
<li>å½“å‰æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦ä¼˜åŒ–ã€‚</li>
<li>TurboVSRé€šè¿‡é‡‡ç”¨é«˜æ•ˆè®¾è®¡æé«˜æ‰©æ•£æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ï¼ŒåŒ…æ‹¬ä½¿ç”¨è‡ªç¼–ç å™¨ã€å› å­åŒ–æ¡ä»¶å’Œå¿«æ·æ–¹å¼æ¨¡å‹è½¬æ¢ã€‚</li>
<li>TurboVSRæ€§èƒ½ä¸æœ€å…ˆè¿›çš„VSRæ–¹æ³•ç›¸å½“ï¼Œä½†å¤„ç†é€Ÿåº¦æ˜¾è‘—æé«˜ã€‚</li>
<li>TurboVSRèƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…å¤„ç†é«˜åˆ†è¾¨ç‡è§†é¢‘ï¼Œä¾‹å¦‚ä»…éœ€7ç§’å³å¯å¤„ç†2ç§’é•¿çš„1080pè§†é¢‘ã€‚</li>
<li>TurboVSRæ”¯æŒå›¾åƒåˆ†è¾¨ç‡æå‡ï¼Œå¯å°†å›¾åƒè§†ä¸ºå•å¸§è§†é¢‘è¿›è¡Œå¤„ç†ã€‚</li>
<li>TurboVSRåœ¨4Kå›¾åƒè¶…åˆ†è¾¨ç‡æ–¹é¢çš„è¡¨ç°ä»¤äººå°è±¡æ·±åˆ»ï¼Œæ˜¾ç¤ºå‡ºæƒŠäººçš„ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23618">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-977fdab8d40b30d553579ac3b63765c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-500c72f0a93483310125ec99747a00d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d62a36a4d6dcd58c42de0e13a281088.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0087aad676bdd9b6060c3b2c80562137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a604e15dd11192c2bb2bdc5128e6ffb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f51f73bc59fdb46f5459f235c5fd30ff.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Metadata-Wavelet-and-Time-Aware-Diffusion-Models-for-Satellite-Image-Super-Resolution"><a href="#Metadata-Wavelet-and-Time-Aware-Diffusion-Models-for-Satellite-Image-Super-Resolution" class="headerlink" title="Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image   Super Resolution"></a>Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image   Super Resolution</h2><p><strong>Authors:Luigi Sigillo, Renato Giamba, Danilo Comminiello</strong></p>
<p>The acquisition of high-resolution satellite imagery is often constrained by the spatial and temporal limitations of satellite sensors, as well as the high costs associated with frequent observations. These challenges hinder applications such as environmental monitoring, disaster response, and agricultural management, which require fine-grained and high-resolution data. In this paper, we propose MWT-Diff, an innovative framework for satellite image super-resolution (SR) that combines latent diffusion models with wavelet transforms to address these challenges. At the core of the framework is a novel metadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates embeddings that capture metadata attributes, multi-scale frequency information, and temporal relationships. The embedded feature representations steer the hierarchical diffusion dynamics, through which the model progressively reconstructs high-resolution satellite imagery from low-resolution inputs. This process preserves critical spatial characteristics including textural patterns, boundary discontinuities, and high-frequency spectral components essential for detailed remote sensing analysis. The comparative analysis of MWT-Diff across multiple datasets demonstrated favorable performance compared to recent approaches, as measured by standard perceptual quality metrics including FID and LPIPS. </p>
<blockquote>
<p>è·å–é«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒé€šå¸¸å—åˆ°å«æ˜Ÿä¼ æ„Ÿå™¨ç©ºé—´å’Œæ—¶é—´ä¸Šçš„é™åˆ¶ä»¥åŠé¢‘ç¹è§‚æµ‹å¸¦æ¥çš„é«˜æˆæœ¬çš„å½±å“ã€‚è¿™äº›æŒ‘æˆ˜é˜»ç¢äº†ç¯å¢ƒç›‘æ§ã€ç¾å®³å“åº”å’Œå†œä¸šç®¡ç†ç­‰åº”ç”¨ï¼Œè¿™äº›åº”ç”¨éœ€è¦ç²¾ç»†çš„ã€é«˜åˆ†è¾¨ç‡çš„æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MWT-Diffï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„å«æ˜Ÿå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ¡†æ¶ï¼Œå®ƒå°†æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸å°æ³¢å˜æ¢ç›¸ç»“åˆï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ–°é¢–çš„é‡‘å±æ•°æ®ã€å°æ³¢å’Œæ—¶é—´æ„ŸçŸ¥ç¼–ç å™¨ï¼ˆMWT-Encoderï¼‰ï¼Œå®ƒç”ŸæˆåµŒå…¥ï¼Œæ•è·é‡‘å±æ•°æ®å±æ€§ã€å¤šå°ºåº¦é¢‘ç‡ä¿¡æ¯å’Œæ—¶é—´å…³ç³»ã€‚åµŒå…¥å¼ç‰¹å¾è¡¨ç¤ºå¼•å¯¼åˆ†å±‚æ‰©æ•£åŠ¨åŠ›å­¦ï¼Œé€šè¿‡è¯¥æ¨¡å‹ä»ä½åˆ†è¾¨ç‡è¾“å…¥é€æ­¥é‡å»ºé«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒã€‚è¿™ä¸€è¿‡ç¨‹ä¿ç•™äº†é‡è¦çš„ç©ºé—´ç‰¹å¾ï¼ŒåŒ…æ‹¬çº¹ç†æ¨¡å¼ã€è¾¹ç•Œä¸è¿ç»­æ€§å’Œé«˜é¢‘è°±æˆåˆ†ï¼Œå¯¹äºè¯¦ç»†çš„é¥æ„Ÿåˆ†æè‡³å…³é‡è¦ã€‚MWT-Diffåœ¨å¤šæ•°æ®é›†ä¸Šçš„æ¯”è¾ƒåˆ†æè¡¨æ˜ï¼Œä¸æœ€è¿‘çš„æ–¹æ³•ç›¸æ¯”ï¼Œå…¶åœ¨FIDå’ŒLPIPSç­‰æ ‡å‡†æ„ŸçŸ¥è´¨é‡æŒ‡æ ‡ä¸Šçš„è¡¨ç°è¾ƒä¸ºä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23566v1">PDF</a> ICLR 2025 Workshop on Machine Learning for Remote Sensing (ML4RS)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†MWT-Diffæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆæ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œå°æ³¢å˜æ¢ï¼Œæ—¨åœ¨è§£å†³é«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒè·å–æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å…ƒæ•°æ®ã€å°æ³¢å’Œæ—¶åºæ„ŸçŸ¥ç¼–ç å™¨ï¼ˆMWT-Encoderï¼‰ç”ŸæˆåµŒå…¥ç‰¹å¾ï¼Œæ•æ‰å…ƒæ•°æ®å±æ€§ã€å¤šå°ºåº¦é¢‘ç‡ä¿¡æ¯å’Œæ—¶åºå…³ç³»ã€‚åµŒå…¥ç‰¹å¾è¡¨ç¤ºå¼•å¯¼å±‚æ¬¡æ‰©æ•£åŠ¨åŠ›å­¦ï¼Œæ¨¡å‹ä»ä½åˆ†è¾¨ç‡è¾“å…¥é€æ­¥é‡å»ºé«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒã€‚è¿™ä¸€è¿‡ç¨‹ä¿ç•™äº†å…³é”®çš„ç©ºé—´ç‰¹å¾ï¼ŒåŒ…æ‹¬çº¹ç†æ¨¡å¼ã€è¾¹ç•Œä¸è¿ç»­æ€§å’Œé«˜é¢‘å…‰è°±æˆåˆ†ï¼Œé€‚ç”¨äºè¯¦ç»†çš„é¥æ„Ÿåˆ†æã€‚å¯¹æ¯”å¤šä¸ªæ•°æ®é›†ä¸Šçš„MWT-Diffåˆ†ææ˜¾ç¤ºï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œå…¶æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œé€šè¿‡æ ‡å‡†æ„ŸçŸ¥è´¨é‡æŒ‡æ ‡FIDå’ŒLPIPSè¿›è¡Œè¯„ä¼°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MWT-Diffæ¡†æ¶ç»“åˆäº†æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œå°æ³¢å˜æ¢ï¼Œæ—¨åœ¨è§£å†³å«æ˜Ÿå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>æ¡†æ¶æ ¸å¿ƒåœ¨äºå…ƒæ•°æ®ã€å°æ³¢å’Œæ—¶åºæ„ŸçŸ¥ç¼–ç å™¨ï¼ˆMWT-Encoderï¼‰ï¼Œèƒ½ç”Ÿæˆæ•æ‰å¤šç§ä¿¡æ¯çš„åµŒå…¥ç‰¹å¾ã€‚</li>
<li>åµŒå…¥ç‰¹å¾å¼•å¯¼å±‚æ¬¡æ‰©æ•£åŠ¨åŠ›å­¦ï¼Œä»ä½åˆ†è¾¨ç‡è¾“å…¥é‡å»ºé«˜åˆ†è¾¨ç‡å«æ˜Ÿå›¾åƒã€‚</li>
<li>è¯¥è¿‡ç¨‹ä¿ç•™äº†ç©ºé—´ç‰¹å¾ï¼ŒåŒ…æ‹¬çº¹ç†æ¨¡å¼ã€è¾¹ç•Œä¸è¿ç»­æ€§å’Œé«˜é¢‘å…‰è°±æˆåˆ†ã€‚</li>
<li>MWT-Diffåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</li>
<li>æ€§èƒ½è¯„ä¼°é‡‡ç”¨æ ‡å‡†æ„ŸçŸ¥è´¨é‡æŒ‡æ ‡ï¼Œå¦‚FIDå’ŒLPIPSã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨ç¯å¢ƒç›‘æ§ã€ç¾å®³å“åº”å’Œå†œä¸šç®¡ç†ç­‰åº”ç”¨ä¸­å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b3c0523a97b542dfdbfa0233f7d101d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f9024b9b1d6cc98a63e66d9d383863f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ce0b0d26c63b6bf1b57526a67d5e0a7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MTADiffusion-Mask-Text-Alignment-Diffusion-Model-for-Object-Inpainting"><a href="#MTADiffusion-Mask-Text-Alignment-Diffusion-Model-for-Object-Inpainting" class="headerlink" title="MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting"></a>MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting</h2><p><strong>Authors:Jun Huang, Ting Liu, Yihang Wu, Xiaochao Qu, Luoqi Liu, Xiaolin Hu</strong></p>
<p>Advancements in generative models have enabled image inpainting models to generate content within specific regions of an image based on provided prompts and masks. However, existing inpainting methods often suffer from problems such as semantic misalignment, structural distortion, and style inconsistency. In this work, we present MTADiffusion, a Mask-Text Alignment diffusion model designed for object inpainting. To enhance the semantic capabilities of the inpainting model, we introduce MTAPipeline, an automatic solution for annotating masks with detailed descriptions. Based on the MTAPipeline, we construct a new MTADataset comprising 5 million images and 25 million mask-text pairs. Furthermore, we propose a multi-task training strategy that integrates both inpainting and edge prediction tasks to improve structural stability. To promote style consistency, we present a novel inpainting style-consistency loss using a pre-trained VGG network and the Gram matrix. Comprehensive evaluations on BrushBench and EditBench demonstrate that MTADiffusion achieves state-of-the-art performance compared to other methods. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ä½¿å¾—å›¾åƒå¡«å……æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æä¾›çš„æç¤ºå’Œè’™ç‰ˆï¼Œåœ¨å›¾åƒç‰¹å®šåŒºåŸŸå†…ç”Ÿæˆå†…å®¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¡«å……æ–¹æ³•å¸¸å¸¸å­˜åœ¨è¯­ä¹‰ä¸åŒ¹é…ã€ç»“æ„å¤±çœŸå’Œé£æ ¼ä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MTADiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå¯¹è±¡å¡«å……è®¾è®¡çš„Mask-Textå¯¹é½æ‰©æ•£æ¨¡å‹ã€‚ä¸ºäº†å¢å¼ºå¡«å……æ¨¡å‹çš„è¯­ä¹‰èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†MTAPipelineï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨è§£å†³æ–¹æ¡ˆï¼Œç”¨äºå¯¹è’™ç‰ˆè¿›è¡Œè¯¦ç»†çš„æè¿°æ ‡æ³¨ã€‚åŸºäºMTAPipelineï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„MTADatasetï¼ŒåŒ…å«500ä¸‡å¼ å›¾åƒå’Œ2500ä¸‡ä¸ªè’™ç‰ˆæ–‡æœ¬å¯¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡è®­ç»ƒç­–ç•¥ï¼Œå°†å¡«å……å’Œè¾¹ç¼˜é¢„æµ‹ä»»åŠ¡ç»“åˆèµ·æ¥ï¼Œä»¥æé«˜ç»“æ„ç¨³å®šæ€§ã€‚ä¸ºäº†ä¿ƒè¿›é£æ ¼çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„VGGç½‘ç»œå’ŒGramçŸ©é˜µï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¡«å……é£æ ¼ä¸€è‡´æ€§æŸå¤±ã€‚åœ¨BrushBenchå’ŒEditBenchä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒMTADiffusionè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23482v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†MTADiffusionï¼Œä¸€ç§ç”¨äºç›®æ ‡è¡¥å…¨çš„é®ç½©æ–‡æœ¬å¯¹é½æ‰©æ•£æ¨¡å‹ã€‚ä¸ºæé«˜è¡¥å…¨æ¨¡å‹çš„è¯­ä¹‰èƒ½åŠ›ï¼Œå¼•å…¥äº†MTAPipelineï¼Œå¯è‡ªåŠ¨ä¸ºé®ç½©æä¾›è¯¦ç»†æè¿°çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶åŸºäºæ­¤æ„å»ºäº†æ–°çš„MTADatasetæ•°æ®é›†ã€‚åŒæ—¶ï¼Œé‡‡ç”¨å¤šä»»åŠ¡è®­ç»ƒç­–ç•¥ï¼Œç»“åˆè¡¥å…¨å’Œè¾¹ç¼˜é¢„æµ‹ä»»åŠ¡ï¼Œä»¥æé«˜ç»“æ„ç¨³å®šæ€§ã€‚é€šè¿‡é¢„è®­ç»ƒçš„VGGç½‘ç»œå’ŒGramçŸ©é˜µï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¡¥å…¨é£æ ¼ä¸€è‡´æ€§æŸå¤±ã€‚åœ¨BrushBenchå’ŒEditBenchä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒMTADiffusionç›¸è¾ƒäºå…¶ä»–æ–¹æ³•å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MTADiffusionæ˜¯ä¸€ä¸ªç”¨äºç›®æ ‡è¡¥å…¨çš„é®ç½©æ–‡æœ¬å¯¹é½æ‰©æ•£æ¨¡å‹ï¼Œè§£å†³äº†ç°æœ‰è¡¥å…¨æ–¹æ³•ä¸­çš„è¯­ä¹‰ä¸å¯¹é½ã€ç»“æ„æ‰­æ›²å’Œé£æ ¼ä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚</li>
<li>å¼•å…¥MTAPipelineè‡ªåŠ¨ä¸ºé®ç½©æä¾›è¯¦ç»†æè¿°çš„è§£å†³æ–¹æ¡ˆï¼Œå¢å¼ºæ¨¡å‹çš„è¯­ä¹‰èƒ½åŠ›ã€‚</li>
<li>æ„å»ºæ–°çš„MTADatasetæ•°æ®é›†ï¼ŒåŒ…å«5ç™¾ä¸‡å›¾åƒå’Œ2åƒ5ç™¾ä¸‡ä¸ªé®ç½©æ–‡æœ¬å¯¹ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒã€‚</li>
<li>é‡‡ç”¨å¤šä»»åŠ¡è®­ç»ƒç­–ç•¥ï¼Œç»“åˆè¡¥å…¨å’Œè¾¹ç¼˜é¢„æµ‹ä»»åŠ¡ï¼Œä»¥æé«˜æ¨¡å‹çš„ç»“æ„ç¨³å®šæ€§ã€‚</li>
<li>å€ŸåŠ©é¢„è®­ç»ƒçš„VGGç½‘ç»œå’ŒGramçŸ©é˜µï¼Œæå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„è¡¥å…¨é£æ ¼ä¸€è‡´æ€§æŸå¤±ï¼Œç¡®ä¿è¡¥å…¨å†…å®¹çš„é£æ ¼ä¸åŸå§‹å›¾åƒä¸€è‡´ã€‚</li>
<li>åœ¨BrushBenchå’ŒEditBenchä¸Šçš„ç»¼åˆè¯„ä¼°æ˜¾ç¤ºMTADiffusionæ€§èƒ½å“è¶Šï¼Œè¾¾åˆ°äº†å½“å‰çš„æœ€ä½³æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-596c19c75b1b75487d644322d113c7f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04adf558d8a87896de208ec07d3a106a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f61d735a1a1bb51fd2dd466fe00b5ee9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdf875699dd20ad3d05a6f44d2881c40.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="StereoDiff-Stereo-Diffusion-Synergy-for-Video-Depth-Estimation"><a href="#StereoDiff-Stereo-Diffusion-Synergy-for-Video-Depth-Estimation" class="headerlink" title="StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation"></a>StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation</h2><p><strong>Authors:Haodong Li, Chen Wang, Jiahui Lei, Kostas Daniilidis, Lingjie Liu</strong></p>
<p>Recent video depth estimation methods achieve great performance by following the paradigm of image depth estimation, i.e., typically fine-tuning pre-trained video diffusion models with massive data. However, we argue that video depth estimation is not a naive extension of image depth estimation. The temporal consistency requirements for dynamic and static regions in videos are fundamentally different. Consistent video depth in static regions, typically backgrounds, can be more effectively achieved via stereo matching across all frames, which provides much stronger global 3D cues. While the consistency for dynamic regions still should be learned from large-scale video depth data to ensure smooth transitions, due to the violation of triangulation constraints. Based on these insights, we introduce StereoDiff, a two-stage video depth estimator that synergizes stereo matching for mainly the static areas with video depth diffusion for maintaining consistent depth transitions in dynamic areas. We mathematically demonstrate how stereo matching and video depth diffusion offer complementary strengths through frequency domain analysis, highlighting the effectiveness of their synergy in capturing the advantages of both. Experimental results on zero-shot, real-world, dynamic video depth benchmarks, both indoor and outdoor, demonstrate StereoDiffâ€™s SoTA performance, showcasing its superior consistency and accuracy in video depth estimation. </p>
<blockquote>
<p>æœ€è¿‘çš„è§†é¢‘æ·±åº¦ä¼°è®¡æ–¹æ³•éµå¾ªå›¾åƒæ·±åº¦ä¼°è®¡çš„æ¨¡å¼ï¼Œå³é€šå¸¸é€šè¿‡å¯¹å¤§é‡æ•°æ®è¿›è¡Œé¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå–å¾—äº†å¾ˆå¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºè§†é¢‘æ·±åº¦ä¼°è®¡å¹¶ä¸æ˜¯å›¾åƒæ·±åº¦ä¼°è®¡çš„ç®€å•æ‰©å±•ã€‚è§†é¢‘ä¸­çš„åŠ¨æ€å’Œé™æ€åŒºåŸŸçš„æ—¶åºä¸€è‡´æ€§è¦æ±‚å­˜åœ¨æ ¹æœ¬å·®å¼‚ã€‚å¯¹äºé€šå¸¸æ˜¯èƒŒæ™¯ç­‰çš„é™æ€åŒºåŸŸçš„è§†é¢‘æ·±åº¦ä¸€è‡´æ€§ï¼Œå¯ä»¥é€šè¿‡æ‰€æœ‰å¸§çš„ç«‹ä½“åŒ¹é…æ›´æœ‰æ•ˆåœ°å®ç°ï¼Œè¿™æä¾›äº†æ›´å¼ºçš„å…¨å±€ä¸‰ç»´çº¿ç´¢ã€‚è€Œå¯¹äºåŠ¨æ€åŒºåŸŸçš„ä¸€è‡´æ€§ä»ç„¶éœ€è¦ä»å¤§è§„æ¨¡è§†é¢‘æ·±åº¦æ•°æ®ä¸­å­¦ä¹ ï¼Œä»¥ç¡®ä¿å¹³æ»‘è¿‡æ¸¡ï¼Œè¿™è¿åäº†ä¸‰è§’çº¦æŸã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†StereoDiffï¼Œè¿™æ˜¯ä¸€ç§ä¸¤é˜¶æ®µçš„è§†é¢‘æ·±åº¦ä¼°è®¡å™¨ï¼Œå®ƒå°†ä¸»è¦ç”¨äºé™æ€åŒºåŸŸçš„ç«‹ä½“åŒ¹é…ä¸ç”¨äºä¿æŒåŠ¨æ€åŒºåŸŸä¸­ä¸€è‡´æ·±åº¦è¿‡æ¸¡çš„è§†é¢‘æ·±åº¦æ‰©æ•£ç›¸ç»“åˆã€‚æˆ‘ä»¬é€šè¿‡é¢‘åŸŸåˆ†æä»æ•°å­¦ä¸Šè¯æ˜äº†ç«‹ä½“åŒ¹é…å’Œè§†é¢‘æ·±åº¦æ‰©æ•£å¦‚ä½•æä¾›äº’è¡¥ä¼˜åŠ¿ï¼Œçªæ˜¾äº†å®ƒä»¬åœ¨æ•æ‰ä¸¤è€…ä¼˜åŠ¿æ–¹é¢çš„ååŒæœ‰æ•ˆæ€§ã€‚åœ¨é›¶æ ·æœ¬ã€ç°å®ä¸–ç•Œã€å®¤å†…å’Œå®¤å¤–çš„åŠ¨æ€è§†é¢‘æ·±åº¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒStereoDiffçš„æ€§èƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†é¢‘æ·±åº¦ä¼°è®¡ä¸­çš„å“è¶Šä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20756v2">PDF</a> Work done in Nov 2024, during an internship at the University of   Pennsylvania. Project page: <a target="_blank" rel="noopener" href="https://stereodiff.github.io/">https://stereodiff.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘æ·±åº¦ä¼°è®¡çš„æ–°æ–¹æ³•StereoDiffï¼Œè¯¥æ–¹æ³•ç»“åˆç«‹ä½“åŒ¹é…å’Œè§†é¢‘æ·±åº¦æ‰©æ•£æŠ€æœ¯ï¼Œé’ˆå¯¹é™æ€å’ŒåŠ¨æ€åŒºåŸŸçš„ä¸åŒè¦æ±‚ï¼Œå®ç°äº†è§†é¢‘æ·±åº¦çš„ä¸€è‡´æ€§ä¼°è®¡ã€‚ç«‹ä½“åŒ¹é…ä¸»è¦ç”¨äºé™æ€åŒºåŸŸï¼Œè€Œè§†é¢‘æ·±åº¦æ‰©æ•£åˆ™ç”¨äºä¿æŒåŠ¨æ€åŒºåŸŸçš„æ·±åº¦è¿‡æ¸¡ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStereoDiffåœ¨é›¶æ ·æœ¬ã€çœŸå®ä¸–ç•Œã€å®¤å†…å’Œå®¤å¤–åŠ¨æ€è§†é¢‘æ·±åº¦è¯„ä¼°æ ‡å‡†ä¸Šå‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå…·æœ‰é¢†å…ˆçš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ·±åº¦ä¼°è®¡ä¸æ˜¯å›¾åƒæ·±åº¦ä¼°è®¡çš„ç®€å•æ‰©å±•ï¼Œå› ä¸ºè§†é¢‘ä¸­çš„åŠ¨æ€å’Œé™æ€åŒºåŸŸå¯¹æ—¶é—´ä¸€è‡´æ€§çš„è¦æ±‚æ ¹æœ¬ä¸åŒã€‚</li>
<li>ç«‹ä½“åŒ¹é…æŠ€æœ¯åœ¨é™æ€åŒºåŸŸï¼ˆå¦‚èƒŒæ™¯ï¼‰çš„è§†é¢‘æ·±åº¦ä¼°è®¡ä¸­æ›´æœ‰æ•ˆï¼Œèƒ½æä¾›æ›´å¼ºçš„å…¨å±€3Dçº¿ç´¢ã€‚</li>
<li>åŠ¨æ€åŒºåŸŸçš„ä¸€è‡´æ€§éœ€è¦ä»å¤§è§„æ¨¡è§†é¢‘æ·±åº¦æ•°æ®ä¸­å­¦ä¹ ï¼Œä»¥ç¡®ä¿å¹³æ»‘è¿‡æ¸¡ï¼Œè¿åä¸‰è§’çº¦æŸã€‚</li>
<li>StereoDiffæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„è§†é¢‘æ·±åº¦ä¼°è®¡å™¨ï¼Œç»“åˆç«‹ä½“åŒ¹é…å’Œè§†é¢‘æ·±åº¦æ‰©æ•£æŠ€æœ¯ã€‚</li>
<li>ç«‹ä½“åŒ¹é…å’Œè§†é¢‘æ·±åº¦æ‰©æ•£åœ¨é¢‘ç‡åŸŸåˆ†æä¸­è¡¨ç°å‡ºäº’è¡¥çš„ä¼˜åŠ¿ï¼Œæœ‰æ•ˆåœ°ç»“åˆäº†ä¸¤è€…çš„ä¼˜ç‚¹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒStereoDiffåœ¨è§†é¢‘æ·±åº¦ä¼°è®¡çš„é›¶æ ·æœ¬ã€çœŸå®ä¸–ç•Œã€å®¤å†…å’Œå®¤å¤–åŠ¨æ€è§†é¢‘ä¸Šå…·æœ‰æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c3f8dd2f826a1e20db183050ac04713.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f261418de0ffeebc8efc5822d24af98a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93f72d5e322300ee05433b61e26b0d26.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Prompt-Guided-Latent-Diffusion-with-Predictive-Class-Conditioning-for-3D-Prostate-MRI-Generation"><a href="#Prompt-Guided-Latent-Diffusion-with-Predictive-Class-Conditioning-for-3D-Prostate-MRI-Generation" class="headerlink" title="Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D   Prostate MRI Generation"></a>Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D   Prostate MRI Generation</h2><p><strong>Authors:Emerson P. Grabke, Masoom A. Haider, Babak Taati</strong></p>
<p>Objective: Latent diffusion models (LDM) could alleviate data scarcity challenges affecting machine learning development for medical imaging. However, medical LDM strategies typically rely on short-prompt text encoders, non-medical LDMs, or large data volumes. These strategies can limit performance and scientific accessibility. We propose a novel LDM conditioning approach to address these limitations. Methods: We propose Class-Conditioned Efficient Large Language model Adapter (CCELLA), a novel dual-head conditioning approach that simultaneously conditions the LDM U-Net with free-text clinical reports and radiology classification. We also propose a data-efficient LDM framework centered around CCELLA and a proposed joint loss function. We first evaluate our method on 3D prostate MRI against state-of-the-art. We then augment a downstream classifier model training dataset with synthetic images from our method. Results: Our method achieves a 3D FID score of 0.025 on a size-limited 3D prostate MRI dataset, significantly outperforming a recent foundation model with FID 0.071. When training a classifier for prostate cancer prediction, adding synthetic images generated by our method during training improves classifier accuracy from 69% to 74%. Training a classifier solely on our methodâ€™s synthetic images achieved comparable performance to training on real images alone. Conclusion: We show that our method improved both synthetic image quality and downstream classifier performance using limited data and minimal human annotation. Significance: The proposed CCELLA-centric framework enables radiology report and class-conditioned LDM training for high-quality medical image synthesis given limited data volume and human data annotation, improving LDM performance and scientific accessibility. Code from this study will be available at <a target="_blank" rel="noopener" href="https://github.com/grabkeem/CCELLA">https://github.com/grabkeem/CCELLA</a> </p>
<blockquote>
<p>ç›®æ ‡ï¼šæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼ŒLDMï¼‰å¯ä»¥ç¼“è§£åŒ»å­¦æˆåƒé¢†åŸŸæœºå™¨å­¦ä¹ å¼€å‘ä¸­çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ã€‚ç„¶è€Œï¼ŒåŒ»å­¦LDMç­–ç•¥é€šå¸¸ä¾èµ–äºçŸ­æç¤ºæ–‡æœ¬ç¼–ç å™¨ã€éåŒ»å­¦LDMsæˆ–å¤§é‡æ•°æ®ã€‚è¿™äº›ç­–ç•¥å¯èƒ½ä¼šé™åˆ¶æ€§èƒ½å’Œç§‘å­¦å¯åŠæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„LDMæ¡ä»¶å¤„ç†æ–¹æ³•æ¥è§£å†³è¿™äº›é™åˆ¶ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†Class-Conditioned Efficient Large Language model Adapterï¼ˆCCELLAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åŒå¤´æ¡ä»¶å¤„ç†æ–¹æ³•ï¼Œå¯ä»¥åŒæ—¶ä½¿ç”¨è‡ªç”±æ–‡æœ¬ä¸´åºŠæŠ¥å‘Šå’Œæ”¾å°„å­¦åˆ†ç±»å¯¹LDM U-Netè¿›è¡Œæ¡ä»¶å¤„ç†ã€‚æˆ‘ä»¬è¿˜ä»¥CCELLAä¸ºæ ¸å¿ƒï¼Œå›´ç»•å…¶æ„å»ºäº†ä¸€ä¸ªé«˜æ•ˆæ•°æ®LDMæ¡†æ¶ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè”åˆæŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨æœ€å…ˆè¿›çš„3Då‰åˆ—è…ºMRIä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¯¥æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒå¢å¼ºä¸‹æ¸¸åˆ†ç±»å™¨æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ã€‚ç»“æœï¼šæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§„æ¨¡æœ‰é™çš„ä¸‰ç»´å‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šè¾¾åˆ°äº†0.025çš„3DFIDåˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºæœ€è¿‘çš„åŸºç¡€æ¨¡å‹çš„FID 0.071ã€‚åœ¨è®­ç»ƒå‰åˆ—è…ºç™Œé¢„æµ‹åˆ†ç±»å™¨æ—¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ æˆ‘ä»¬æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒå°†åˆ†ç±»å™¨çš„å‡†ç¡®åº¦ä»69%æé«˜åˆ°74%ã€‚ä»…åœ¨æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒä¸Šè®­ç»ƒåˆ†ç±»å™¨å¯å®ç°ä¸ä»…åœ¨çœŸå®å›¾åƒä¸Šè®­ç»ƒåˆ†ç±»å™¨ç›¸å½“çš„æ€§èƒ½ã€‚ç»“è®ºï¼šæˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨æœ‰é™çš„æ•°æ®å’Œæœ€å°‘çš„äººå·¥æ ‡æ³¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æé«˜åˆæˆå›¾åƒçš„è´¨é‡å’Œä¸‹æ¸¸åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚æ„ä¹‰ï¼šä»¥CCELLAä¸ºæ ¸å¿ƒçš„æ¡†æ¶å®ç°äº†ä»¥æ”¾å°„å­¦æŠ¥å‘Šå’Œç±»åˆ«ä¸ºæ¡ä»¶çš„LDMè®­ç»ƒï¼Œå¯åœ¨æ•°æ®é‡å’Œäººå·¥æ ‡æ³¨æœ‰é™çš„æƒ…å†µä¸‹è¿›è¡Œé«˜è´¨é‡åŒ»å­¦å›¾åƒåˆæˆï¼Œæé«˜äº†LDMçš„æ€§èƒ½å’Œç§‘å­¦å¯åŠæ€§ã€‚æœ¬ç ”ç©¶çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/grabkeem/CCELLA">https://github.com/grabkeem/CCELLA</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10230v2">PDF</a> MAH and BT are co-senior authors on the work. This work has been   submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCCELLAçš„æ–°å‹LDMæ¡ä»¶åŒ–æ–¹æ³•ï¼Œç”¨äºè§£å†³åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜ã€‚é€šè¿‡åŒæ—¶ç»“åˆè‡ªç”±æ–‡æœ¬ä¸´åºŠæŠ¥å‘Šå’Œæ”¾å°„å­¦åˆ†ç±»ï¼ŒCCELLAèƒ½å¤Ÿåœ¨æœ‰é™æ•°æ®å’Œæ— éœ€å¤§é‡äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è´¨é‡åŒ»å­¦å›¾åƒï¼Œå¹¶æå‡ä¸‹æ¸¸åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨3Då‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å¯æœ‰æ•ˆæé«˜å‰åˆ—è…ºç™Œé¢„æµ‹åˆ†ç±»å™¨çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LDMå¯ä»¥ç¼“è§£åŒ»å­¦å½±åƒæœºå™¨å­¦ä¹ ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>ç°æœ‰LDMç­–ç•¥åœ¨åŒ»å­¦é¢†åŸŸåº”ç”¨æ—¶å­˜åœ¨å±€é™ã€‚</li>
<li>æå‡ºæ–°å‹LDMæ¡ä»¶åŒ–æ–¹æ³•CCELLAï¼Œç»“åˆè‡ªç”±æ–‡æœ¬ä¸´åºŠæŠ¥å‘Šå’Œæ”¾å°„å­¦åˆ†ç±»ã€‚</li>
<li>CCELLAæ–¹æ³•èƒ½å¤Ÿåœ¨æœ‰é™æ•°æ®å’Œæ— éœ€å¤§é‡äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è´¨é‡åŒ»å­¦å›¾åƒã€‚</li>
<li>åœ¨3Då‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šï¼ŒCCELLAæ–¹æ³•è¡¨ç°ä¼˜å¼‚ï¼Œ3D FIDåˆ†æ•°è¾¾åˆ°0.025ï¼Œä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨CCELLAç”Ÿæˆçš„åˆæˆå›¾åƒè®­ç»ƒä¸‹æ¸¸åˆ†ç±»å™¨ï¼Œå¯æé«˜åˆ†ç±»å™¨å‡†ç¡®æ€§ã€‚</li>
<li>CCELLAæ¡†æ¶å¯æé«˜LDMåœ¨åŒ»å­¦é¢†åŸŸçš„æ€§èƒ½åŠç§‘å­¦å¯åŠæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fa153fde8db063073e73121572a1cbcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33ec0c5d1da94b2a27041ce179d44a8b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="WeatherEdit-Controllable-Weather-Editing-with-4D-Gaussian-Field"><a href="#WeatherEdit-Controllable-Weather-Editing-with-4D-Gaussian-Field" class="headerlink" title="WeatherEdit: Controllable Weather Editing with 4D Gaussian Field"></a>WeatherEdit: Controllable Weather Editing with 4D Gaussian Field</h2><p><strong>Authors:Chenghao Qian, Wenjing Li, Yuhu Guo, Gustav Markkula</strong></p>
<p>In this work, we present WeatherEdit, a novel weather editing pipeline for generating realistic weather effects with controllable types and severity in 3D scenes. Our approach is structured into two key components: weather background editing and weather particle construction. For weather background editing, we introduce an all-in-one adapter that integrates multiple weather styles into a single pretrained diffusion model, enabling the generation of diverse weather effects in 2D image backgrounds. During inference, we design a Temporal-View (TV-) attention mechanism that follows a specific order to aggregate temporal and spatial information, ensuring consistent editing across multi-frame and multi-view images. To construct the weather particles, we first reconstruct a 3D scene using the edited images and then introduce a dynamic 4D Gaussian field to generate snowflakes, raindrops and fog in the scene. The attributes and dynamics of these particles are precisely controlled through physical-based modelling and simulation, ensuring realistic weather representation and flexible severity adjustments. Finally, we integrate the 4D Gaussian field with the 3D scene to render consistent and highly realistic weather effects. Experiments on multiple driving datasets demonstrate that WeatherEdit can generate diverse weather effects with controllable condition severity, highlighting its potential for autonomous driving simulation in adverse weather. See project page: <a target="_blank" rel="noopener" href="https://jumponthemoon.github.io/w-edit">https://jumponthemoon.github.io/w-edit</a> </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WeatherEditï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤©æ°”ç¼–è¾‘ç®¡é“ï¼Œç”¨äºåœ¨3Dåœºæ™¯ç”Ÿæˆå…·æœ‰å¯æ§ç±»å‹å’Œä¸¥é‡ç¨‹åº¦çš„ç°å®å¤©æ°”æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šå¤©æ°”èƒŒæ™¯ç¼–è¾‘å’Œå¤©æ°”ç²’å­æ„å»ºã€‚å¯¹äºå¤©æ°”èƒŒæ™¯ç¼–è¾‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨èƒ½é€‚é…å™¨ï¼Œå°†å¤šç§å¤©æ°”é£æ ¼é›†æˆåˆ°ä¸€ä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œåœ¨2Då›¾åƒèƒŒæ™¯ä¸­ç”Ÿæˆå¤šç§å¤©æ°”æ•ˆæœã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ—¶é—´è§†å›¾ï¼ˆTVï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŒ‰ç…§ç‰¹å®šé¡ºåºèšåˆæ—¶é—´å’Œç©ºé—´ä¿¡æ¯ï¼Œç¡®ä¿è·¨å¤šå¸§å’Œå¤šè§†å›¾å›¾åƒçš„ä¸€è‡´ç¼–è¾‘ã€‚ä¸ºäº†æ„å»ºå¤©æ°”ç²’å­ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ç¼–è¾‘åçš„å›¾åƒé‡å»º3Dåœºæ™¯ï¼Œç„¶åå¼•å…¥åŠ¨æ€4Dé«˜æ–¯åœºæ¥åœ¨åœºæ™¯ä¸­ç”Ÿæˆé›ªèŠ±ã€é›¨æ»´å’Œé›¾ã€‚è¿™äº›ç²’å­çš„å±æ€§å’ŒåŠ¨æ€é€šè¿‡åŸºäºç‰©ç†çš„å»ºæ¨¡å’Œæ¨¡æ‹Ÿè¿›è¡Œç²¾ç¡®æ§åˆ¶ï¼Œç¡®ä¿ç°å®çš„å¤©æ°”è¡¨ç°å’Œçµæ´»çš„ä¸¥é‡ç¨‹åº¦è°ƒæ•´ã€‚æœ€åï¼Œæˆ‘ä»¬å°†4Dé«˜æ–¯åœºä¸3Dåœºæ™¯ç›¸ç»“åˆï¼Œå‘ˆç°ä¸€è‡´ä¸”é«˜åº¦ç°å®çš„å¤©æ°”æ•ˆæœã€‚åœ¨å¤šä¸ªé©¾é©¶æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒWeatherEditå¯ä»¥ç”Ÿæˆå…·æœ‰å¯æ§æ¡ä»¶ä¸¥é‡ç¨‹åº¦çš„å¤šç§å¤©æ°”æ•ˆæœï¼Œçªæ˜¾å…¶åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹è‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿçš„æ½œåŠ›ã€‚æ›´å¤šè¯¦æƒ…è§é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://jumponthemoon.github.io/w-edit%E3%80%82">https://jumponthemoon.github.io/w-editã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20471v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬å·¥ä½œæå‡ºäº†WeatherEditï¼Œä¸€ä¸ªç”¨äºåœ¨3Dåœºæ™¯ä¸­ç”Ÿæˆå…·æœ‰å¯æ§ç±»å‹å’Œä¸¥é‡ç¨‹åº¦çš„é€¼çœŸå¤©æ°”æ•ˆæœçš„æ–°å‹å¤©æ°”ç¼–è¾‘ç®¡é“ã€‚å®ƒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå¤©æ°”èƒŒæ™¯ç¼–è¾‘å’Œå¤©æ°”ç²’å­æ„å»ºã€‚é€šè¿‡å¼•å…¥å…¨åˆä¸€é€‚é…å™¨ï¼Œå°†å¤šç§å¤©æ°”é£æ ¼é›†æˆåˆ°ä¸€ä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°2Då›¾åƒèƒŒæ™¯ä¸­å¤šç§å¤©æ°”æ•ˆæœçš„ç”Ÿæˆã€‚è®¾è®¡äº†ä¸€ç§æ—¶é—´è§†å›¾ï¼ˆTVï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ç‰¹å®šé¡ºåºèšåˆæ—¶é—´å’Œç©ºé—´ä¿¡æ¯ï¼Œç¡®ä¿è·¨å¤šå¸§å’Œå¤šè§†å›¾å›¾åƒçš„ä¸€è‡´ç¼–è¾‘ã€‚é€šè¿‡é‡å»º3Dåœºæ™¯å¹¶å¼•å…¥åŠ¨æ€4Dé«˜æ–¯åœºæ¥æ„å»ºå¤©æ°”ç²’å­ï¼Œé€šè¿‡åŸºäºç‰©ç†çš„å»ºæ¨¡å’Œæ¨¡æ‹Ÿç²¾ç¡®æ§åˆ¶è¿™äº›ç²’å­çš„å±æ€§å’ŒåŠ¨æ€ï¼Œç¡®ä¿å¤©æ°”è¡¨ç¤ºçš„çœŸå®æ€§å’Œçµæ´»è°ƒæ•´ä¸¥é‡ç¨‹åº¦ã€‚æœ€ç»ˆï¼Œå°†4Dé«˜æ–¯åœºä¸3Dåœºæ™¯é›†æˆï¼Œå‘ˆç°ä¸€è‡´ä¸”é«˜åº¦é€¼çœŸçš„å¤©æ°”æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WeatherEditæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆé€¼çœŸå¤©æ°”æ•ˆæœçš„å¤©æ°”ç¼–è¾‘ç®¡é“ï¼Œæ”¯æŒå¯æ§çš„å¤©æ°”ç±»å‹å’Œä¸¥é‡ç¨‹åº¦ã€‚</li>
<li>åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå¤©æ°”èƒŒæ™¯ç¼–è¾‘å’Œå¤©æ°”ç²’å­æ„å»ºã€‚</li>
<li>é€šè¿‡å…¨åˆä¸€é€‚é…å™¨é›†æˆå¤šç§å¤©æ°”é£æ ¼åˆ°é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­ï¼Œå®ç°2Då›¾åƒèƒŒæ™¯å¤šæ ·å¤©æ°”æ•ˆæœçš„ç”Ÿæˆã€‚</li>
<li>è®¾è®¡äº†æ—¶é—´è§†å›¾ï¼ˆTVï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œç¡®ä¿è·¨å¤šå¸§å’Œå¤šè§†å›¾å›¾åƒçš„ä¸€è‡´ç¼–è¾‘ã€‚</li>
<li>é€šè¿‡é‡å»º3Dåœºæ™¯å¹¶å¼•å…¥åŠ¨æ€4Dé«˜æ–¯åœºæ¥ç”Ÿæˆå¤©æ°”ç²’å­ï¼Œä¿è¯å¤©æ°”è¡¨ç¤ºçš„çœŸå®æ€§å’Œçµæ´»è°ƒæ•´å…¶ä¸¥é‡ç¨‹åº¦ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒWeatherEditèƒ½åœ¨å¤šä¸ªé©¾é©¶æ•°æ®é›†ä¸­ç”Ÿæˆå…·æœ‰å¯æ§æ¡ä»¶çš„å¤šæ ·å¤©æ°”æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-95d8875722cbf46f24986d08ab3f9cba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b609c5e40991c32746b408a4cc632389.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-712ee11e32146f4dc2c1efcbf2dd967b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa679ae8d1d10b638523b85675702c4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa37bda420cf4b098bd417c61234ea23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72dee27190f4bbc8310dee2c5a631d88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0c260f20b69c803f9243a0de883d788.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f0eef26cf1c28990e3e99871778e04b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Object-detection-in-adverse-weather-conditions-for-autonomous-vehicles-using-Instruct-Pix2Pix"><a href="#Object-detection-in-adverse-weather-conditions-for-autonomous-vehicles-using-Instruct-Pix2Pix" class="headerlink" title="Object detection in adverse weather conditions for autonomous vehicles   using Instruct Pix2Pix"></a>Object detection in adverse weather conditions for autonomous vehicles   using Instruct Pix2Pix</h2><p><strong>Authors:Unai Gurbindo, Axel Brando, Jaume Abella, Caroline KÃ¶nig</strong></p>
<p>Enhancing the robustness of object detection systems under adverse weather conditions is crucial for the advancement of autonomous driving technology. This study presents a novel approach leveraging the diffusion model Instruct Pix2Pix to develop prompting methodologies that generate realistic datasets with weather-based augmentations aiming to mitigate the impact of adverse weather on the perception capabilities of state-of-the-art object detection models, including Faster R-CNN and YOLOv10. Experiments were conducted in two environments, in the CARLA simulator where an initial evaluation of the proposed data augmentation was provided, and then on the real-world image data sets BDD100K and ACDC demonstrating the effectiveness of the approach in real environments.   The key contributions of this work are twofold: (1) identifying and quantifying the performance gap in object detection models under challenging weather conditions, and (2) demonstrating how tailored data augmentation strategies can significantly enhance the robustness of these models. This research establishes a solid foundation for improving the reliability of perception systems in demanding environmental scenarios, and provides a pathway for future advancements in autonomous driving. </p>
<blockquote>
<p>å¢å¼ºæ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹ç‰©ä½“æ£€æµ‹ç³»ç»Ÿç¨³å¥æ€§å¯¹äºè‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å‘å±•è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹Instruct Pix2Pixå¼€å‘æç¤ºæ–¹æ³•çš„æ–°æ–¹æ³•ï¼Œç”Ÿæˆå…·æœ‰å¤©æ°”å¢å¼ºçš„ç°å®æ•°æ®é›†ï¼Œæ—¨åœ¨å‡è½»æ¶åŠ£å¤©æ°”å¯¹æœ€æ–°ç‰©ä½“æ£€æµ‹æ¨¡å‹æ„ŸçŸ¥èƒ½åŠ›çš„å½±å“ï¼ŒåŒ…æ‹¬Faster R-CNNå’ŒYOLOv10ã€‚å®éªŒåœ¨ä¸¤ä¸ªç¯å¢ƒä¸­è¿›è¡Œï¼ŒåŒ…æ‹¬CARLAæ¨¡æ‹Ÿå™¨ï¼Œè¯¥æ¨¡æ‹Ÿå™¨æä¾›äº†æ‰€æå‡ºçš„æ•°æ®å¢å¼ºçš„åˆæ­¥è¯„ä¼°ï¼Œä»¥åŠåœ¨BDD100Kå’ŒACDCçœŸå®ä¸–ç•Œå›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨çœŸå®ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œçš„å…³é”®è´¡çŒ®æœ‰ä¸¤æ–¹é¢ï¼šï¼ˆ1ï¼‰è¯†åˆ«å’Œé‡åŒ–ç‰©ä½“æ£€æµ‹æ¨¡å‹åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„æ€§èƒ½å·®è·ï¼›ï¼ˆ2ï¼‰å±•ç¤ºé’ˆå¯¹æ€§çš„æ•°æ®å¢å¼ºç­–ç•¥å¦‚ä½•æ˜¾ç€æé«˜è¿™äº›æ¨¡å‹çš„ç¨³å¥æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæé«˜éœ€æ±‚ç¯å¢ƒåœºæ™¯ä¸­æ„ŸçŸ¥ç³»ç»Ÿçš„å¯é æ€§å¥ å®šäº†åšå®åŸºç¡€ï¼Œå¹¶ä¸ºè‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„æœªæ¥å‘å±•æä¾›äº†é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08228v2">PDF</a> 8 pages, 5 figures. Accepted at the International Joint Conference on   Neural Networks (IJCNN) 2025 (to appear)</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹Instruct Pix2Pixå‘å±•å‡ºä¸€ç§æ–°å‹æç¤ºæ–¹æ³•ï¼Œç”Ÿæˆå…·æœ‰å¤©æ°”å¢å¼ºçš„ç°å®æ•°æ®é›†ï¼Œæ—¨åœ¨å‡è½»æ¶åŠ£å¤©æ°”å¯¹å…ˆè¿›ç›®æ ‡æ£€æµ‹æ¨¡å‹æ„ŸçŸ¥èƒ½åŠ›çš„å½±å“ã€‚åœ¨CARLAæ¨¡æ‹Ÿå™¨åŠBDD100Kå’ŒACDCçœŸå®å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨çœŸå®ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶çš„å…³é”®è´¡çŒ®åœ¨äºï¼šä¸€ã€è¯†åˆ«å’Œé‡åŒ–ç›®æ ‡æ£€æµ‹æ¨¡å‹åœ¨æ¶åŠ£æ¡ä»¶ä¸‹çš„æ€§èƒ½å·®è·ï¼›äºŒã€è¯æ˜å®šåˆ¶çš„æ•°æ®å¢å¼ºç­–ç•¥èƒ½æ˜¾è‘—æé«˜è¿™äº›æ¨¡å‹çš„ç¨³å¥æ€§ã€‚è¿™ä¸ºæ”¹å–„ç¯å¢ƒéœ€æ±‚ä¸‹çš„æ„ŸçŸ¥ç³»ç»Ÿå¯é æ€§å¥ å®šäº†åŸºç¡€ï¼Œå¹¶ä¸ºè‡ªåŠ¨é©¾é©¶çš„æœªæ¥å‘å±•æä¾›äº†è·¯å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹Instruct Pix2Pixå‘å±•æ–°å‹æç¤ºæ–¹æ³•ï¼Œç”Ÿæˆç°å®æ•°æ®é›†ä»¥æ¨¡æ‹Ÿæ¶åŠ£å¤©æ°”æ¡ä»¶ã€‚</li>
<li>æ•°æ®å¢å¼ºç­–ç•¥æ—¨åœ¨å‡è½»æ¶åŠ£å¤©æ°”å¯¹ç›®æ ‡æ£€æµ‹æ¨¡å‹æ„ŸçŸ¥èƒ½åŠ›çš„å½±å“ã€‚</li>
<li>åœ¨CARLAæ¨¡æ‹Ÿå™¨ä¸Šåˆæ­¥è¯„ä¼°æ•°æ®å¢å¼ºçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨BDD100Kå’ŒACDCçœŸå®å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç ”ç©¶è¯†åˆ«å’Œé‡åŒ–ç›®æ ‡æ£€æµ‹æ¨¡å‹åœ¨æ¶åŠ£æ¡ä»¶ä¸‹çš„æ€§èƒ½å·®è·ã€‚</li>
<li>å®šåˆ¶çš„æ•°æ®å¢å¼ºç­–ç•¥èƒ½æ˜¾è‘—æé«˜ç›®æ ‡æ£€æµ‹æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08228">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-739a0b831b73dcbddc21bee42be3920f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-363d6e00b239a37beb20eff28e2ae720.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f6bd6eea63376b53ec2482b74727555.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fc3cd889849877f499467f17eaeb420.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba6e4714986ed45478cc2cbdf6cef4a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cebb960c3997ce9f322cacae2d410dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5335cdcd125ad772b7155427a903557.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Towards-Generalized-and-Training-Free-Text-Guided-Semantic-Manipulation"><a href="#Towards-Generalized-and-Training-Free-Text-Guided-Semantic-Manipulation" class="headerlink" title="Towards Generalized and Training-Free Text-Guided Semantic Manipulation"></a>Towards Generalized and Training-Free Text-Guided Semantic Manipulation</h2><p><strong>Authors:Yu Hong, Xiao Cai, Pengpeng Zeng, Shuai Zhang, Jingkuan Song, Lianli Gao, Heng Tao Shen</strong></p>
<p>Text-guided semantic manipulation refers to semantically editing an image generated from a source prompt to match a target prompt, enabling the desired semantic changes (e.g., addition, removal, and style transfer) while preserving irrelevant contents. With the powerful generative capabilities of the diffusion model, the task has shown the potential to generate high-fidelity visual content. Nevertheless, existing methods either typically require time-consuming fine-tuning (inefficient), fail to accomplish multiple semantic manipulations (poorly extensible), and&#x2F;or lack support for different modality tasks (limited generalizability). Upon further investigation, we find that the geometric properties of noises in the diffusion model are strongly correlated with the semantic changes. Motivated by this, we propose a novel $\textit{GTF}$ for text-guided semantic manipulation, which has the following attractive capabilities: 1) $\textbf{Generalized}$: our $\textit{GTF}$ supports multiple semantic manipulations (e.g., addition, removal, and style transfer) and can be seamlessly integrated into all diffusion-based methods (i.e., Plug-and-play) across different modalities (i.e., modality-agnostic); and 2) $\textbf{Training-free}$: $\textit{GTF}$ produces high-fidelity results via simply controlling the geometric relationship between noises without tuning or optimization. Our extensive experiments demonstrate the efficacy of our approach, highlighting its potential to advance the state-of-the-art in semantics manipulation. </p>
<blockquote>
<p>æ–‡æœ¬å¼•å¯¼è¯­ä¹‰æ“çºµæ˜¯æŒ‡å¯¹ç”±æºæç¤ºç”Ÿæˆçš„å›¾åƒè¿›è¡Œè¯­ä¹‰ç¼–è¾‘ï¼Œä»¥åŒ¹é…ç›®æ ‡æç¤ºï¼Œå®ç°æœŸæœ›çš„è¯­ä¹‰æ›´æ”¹ï¼ˆä¾‹å¦‚æ·»åŠ ã€åˆ é™¤å’Œé£æ ¼è½¬æ¢ï¼‰ï¼ŒåŒæ—¶ä¿ç•™æ— å…³å†…å®¹ã€‚æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ä½¿è¯¥ä»»åŠ¡æœ‰æ½œåŠ›ç”Ÿæˆé«˜ä¿çœŸè§†è§‰å†…å®¹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦è€—æ—¶çš„å¾®è°ƒï¼ˆæ•ˆç‡ä¸é«˜ï¼‰ï¼Œæ— æ³•å®Œæˆå¤šé¡¹è¯­ä¹‰æ“ä½œï¼ˆæ‰©å±•æ€§è¾ƒå·®ï¼‰ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹ä¸åŒæ¨¡æ€ä»»åŠ¡çš„æ”¯æŒï¼ˆæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼‰ã€‚é€šè¿‡è¿›ä¸€æ­¥è°ƒæŸ¥ï¼Œæˆ‘ä»¬å‘ç°æ‰©æ•£æ¨¡å‹ä¸­çš„å™ªå£°å‡ ä½•å±æ€§ä¸è¯­ä¹‰æ›´æ”¹å¯†åˆ‡ç›¸å…³ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ–‡æœ¬å¼•å¯¼è¯­ä¹‰æ“çºµçš„æ–°å‹<em>GTF</em>ï¼ˆ Guided Transformation Frameworkï¼‰ã€‚å®ƒå…·æœ‰ä»¥ä¸‹å¸å¼•äººçš„åŠŸèƒ½ï¼š1ï¼‰<em>é€šç”¨æ€§</em>ï¼šæˆ‘ä»¬çš„<em>GTF</em>æ”¯æŒå¤šé¡¹è¯­ä¹‰æ“ä½œï¼ˆä¾‹å¦‚æ·»åŠ ã€åˆ é™¤å’Œé£æ ¼è½¬æ¢ï¼‰ï¼Œå¹¶ä¸”å¯ä»¥æ— ç¼é›†æˆåˆ°æ‰€æœ‰åŸºäºæ‰©æ•£çš„æ–¹æ³•ä¸­ï¼ˆå³æ’å³ç”¨ï¼‰ï¼Œé€‚ç”¨äºä¸åŒæ¨¡æ€ï¼ˆå³æ¨¡æ€æ— å…³ï¼‰ï¼›2ï¼‰<em>å…è®­ç»ƒ</em>ï¼š<em>GTF</em>é€šè¿‡ç®€å•æ§åˆ¶å™ªå£°ä¹‹é—´çš„å‡ ä½•å…³ç³»å³å¯äº§ç”Ÿé«˜ä¿çœŸç»“æœï¼Œæ— éœ€è°ƒæ•´æˆ–ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œçªå‡ºäº†å…¶åœ¨è¯­ä¹‰æ“çºµæ–¹é¢é¢†å…ˆæŠ€æœ¯çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17269v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://ayanami-yu.github.io/GTF-Project-Page/">https://ayanami-yu.github.io/GTF-Project-Page/</a></p>
<p><strong>Summary</strong><br>æ–‡æœ¬å¼•å¯¼è¯­ä¹‰æ“æ§æ˜¯æŒ‡å¯¹ç”±æºæç¤ºç”Ÿæˆçš„å›¾åƒè¿›è¡Œè¯­ä¹‰ç¼–è¾‘ï¼Œä½¿å…¶åŒ¹é…ç›®æ ‡æç¤ºï¼Œå®ç°æ‰€éœ€è¯­ä¹‰æ›´æ”¹ï¼ˆå¦‚æ·»åŠ ã€åˆ é™¤å’Œé£æ ¼è½¬æ¢ï¼‰ï¼ŒåŒæ—¶ä¿ç•™æ— å…³å†…å®¹ã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œè¯¥ä»»åŠ¡æœ‰æ½œåŠ›ç”Ÿæˆé«˜ä¿çœŸè§†è§‰å†…å®¹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦è€—æ—¶å¾®è°ƒï¼ˆæ•ˆç‡ä½ä¸‹ï¼‰ã€æ— æ³•å®Œæˆå¤šé¡¹è¯­ä¹‰æ“æ§ï¼ˆæ‰©å±•æ€§å·®ï¼‰ä¸”ç¼ºä¹ä¸åŒæ¨¡æ€ä»»åŠ¡çš„æ”¯æŒï¼ˆæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼‰ã€‚æˆ‘ä»¬å‘ç°æ‰©æ•£æ¨¡å‹ä¸­çš„å™ªå£°å‡ ä½•å±æ€§ä¸è¯­ä¹‰æ›´æ”¹ä¹‹é—´å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ–‡æœ¬å¼•å¯¼è¯­ä¹‰æ“æ§æ–¹æ³•GTFï¼Œå…·æœ‰æ”¯æŒå¤šç§è¯­ä¹‰æ“æ§ã€å¯æ— ç¼é›†æˆåˆ°æ‰€æœ‰æ‰©æ•£æ–¹æ³•ï¼ˆå³æ’å³ç”¨ï¼‰å’Œä¸åŒæ¨¡æ€ä¸­çš„èƒ½åŠ›ï¼Œä»¥åŠæ— éœ€è®­ç»ƒå³å¯é€šè¿‡æ§åˆ¶å™ªå£°ä¹‹é—´çš„å‡ ä½•å…³ç³»äº§ç”Ÿé«˜ä¿çœŸç»“æœçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å¼•å¯¼è¯­ä¹‰æ“æ§å¯ä»¥ç¼–è¾‘å›¾åƒä»¥åŒ¹é…ç›®æ ‡æç¤ºï¼Œå®ç°å¤šç§è¯­ä¹‰æ›´æ”¹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å…·å¤‡å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¯ç”Ÿæˆé«˜ä¿çœŸè§†è§‰å†…å®¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨æ•ˆç‡ä½ä¸‹ã€æ‰©å±•æ€§å·®å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>å™ªå£°çš„å‡ ä½•å±æ€§ä¸è¯­ä¹‰æ›´æ”¹ä¹‹é—´å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ã€‚</li>
<li>GTFæ–¹æ³•æ”¯æŒå¤šç§è¯­ä¹‰æ“æ§ï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°æ‰€æœ‰æ‰©æ•£æ–¹æ³•ï¼ˆå³æ’å³ç”¨ï¼‰ã€‚</li>
<li>GTFæ–¹æ³•å…·æœ‰è·¨ä¸åŒæ¨¡æ€çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b17f1f70a879c9fb26de5120c932bad0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a4e2755939fcde4bbc745cfec835222.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-738df2761a6be3b1cf296282d5d7db5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec2e0f72d3cf6e63f4ba92898e58561a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-for-Robotic-Manipulation-A-Survey"><a href="#Diffusion-Models-for-Robotic-Manipulation-A-Survey" class="headerlink" title="Diffusion Models for Robotic Manipulation: A Survey"></a>Diffusion Models for Robotic Manipulation: A Survey</h2><p><strong>Authors:Rosa Wolf, Yitian Shi, Sheng Liu, Rania Rayyes</strong></p>
<p>Diffusion generative models have demonstrated remarkable success in visual domains such as image and video generation. They have also recently emerged as a promising approach in robotics, especially in robot manipulations. Diffusion models leverage a probabilistic framework, and they stand out with their ability to model multi-modal distributions and their robustness to high-dimensional input and output spaces. This survey provides a comprehensive review of state-of-the-art diffusion models in robotic manipulation, including grasp learning, trajectory planning, and data augmentation. Diffusion models for scene and image augmentation lie at the intersection of robotics and computer vision for vision-based tasks to enhance generalizability and data scarcity. This paper also presents the two main frameworks of diffusion models and their integration with imitation learning and reinforcement learning. In addition, it discusses the common architectures and benchmarks and points out the challenges and advantages of current state-of-the-art diffusion-based methods. </p>
<blockquote>
<p>æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆç­‰è§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚å®ƒä»¬æœ€è¿‘ä¹Ÿä½œä¸ºæœºå™¨äººæŠ€æœ¯çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•å‡ºç°ï¼Œå°¤å…¶æ˜¯æœºå™¨äººæ“æ§ã€‚æ‰©æ•£æ¨¡å‹åˆ©ç”¨æ¦‚ç‡æ¡†æ¶ï¼Œä»¥å…¶èƒ½å¤Ÿæ¨¡æ‹Ÿå¤šæ¨¡æ€åˆ†å¸ƒå’Œå¯¹é«˜ç»´è¾“å…¥å’Œè¾“å‡ºç©ºé—´çš„ç¨³å¥æ€§è€Œè„±é¢–è€Œå‡ºã€‚è¿™ç¯‡ç»¼è¿°å¯¹æœºå™¨äººæ“ä½œä¸­çš„æœ€æ–°æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†å…¨é¢å›é¡¾ï¼ŒåŒ…æ‹¬æŠ“å–å­¦ä¹ ã€è½¨è¿¹è§„åˆ’å’Œæ•°æ®å¢å¼ºã€‚åœºæ™¯å’Œå›¾åƒå¢å¼ºçš„æ‰©æ•£æ¨¡å‹ä½äºæœºå™¨äººå’Œè®¡ç®—æœºè§†è§‰çš„äº¤å‰ç‚¹ï¼Œç”¨äºåŸºäºè§†è§‰çš„ä»»åŠ¡ï¼Œä»¥æé«˜é€šç”¨æ€§å’Œè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æœ¬æ–‡è¿˜ä»‹ç»äº†æ‰©æ•£æ¨¡å‹çš„ä¸¤ä¸ªä¸»è¦æ¡†æ¶åŠå…¶ä¸æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„èåˆã€‚æ­¤å¤–ï¼Œå®ƒè¿˜è®¨è®ºäº†å¸¸è§çš„æ¶æ„å’ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•çš„æŒ‘æˆ˜å’Œä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08438v2">PDF</a> 28 pages, 2 figure, 9 tables</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œæœ€è¿‘åœ¨æœºå™¨äººé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººæ“ä½œæ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡å…¨é¢ç»¼è¿°äº†æœºå™¨äººæ“ä½œä¸­çš„æ‰©æ•£æ¨¡å‹ï¼ŒåŒ…æ‹¬æŠ“å–å­¦ä¹ ã€è½¨è¿¹è§„åˆ’å’Œæ•°æ®å¢å¼ºç­‰ã€‚æ‰©æ•£æ¨¡å‹ç»“åˆæ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥æé«˜æœºå™¨äººçš„é€šç”¨æ€§å’Œè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æœ¬æ–‡è¿˜ä»‹ç»äº†æ‰©æ•£æ¨¡å‹çš„ä¸¤ä¸ªä¸»è¦æ¡†æ¶ã€å¸¸è§æ¶æ„å’ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰å…ˆè¿›æ‰©æ•£æ–¹æ³•çš„æŒ‘æˆ˜å’Œä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨è§†è§‰é¢†åŸŸå¦‚å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸Šå–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æŠ“å–å­¦ä¹ ã€è½¨è¿¹è§„åˆ’å’Œæ•°æ®å¢å¼ºæ–¹é¢ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½ç»“åˆæ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ï¼Œæé«˜æœºå™¨äººçš„é€šç”¨æ€§ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§£å†³æœºå™¨äººæ•°æ®ç¨€ç¼ºé—®é¢˜ä¸Šå‘æŒ¥é‡è¦ä½œç”¨ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹çš„ä¸¤ä¸ªä¸»è¦æ¡†æ¶å’Œå¸¸è§æ¶æ„ã€‚</li>
<li>å½“å‰å…ˆè¿›æ‰©æ•£æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼Œä½†ä¹Ÿå…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-11fa7fe29aadeef82fe4f6c8f558e9dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b2b5eea1313260c147ae8eef6be00ef.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Identity-Preserving-3D-Head-Stylization-with-Multiview-Score-Distillation"><a href="#Identity-Preserving-3D-Head-Stylization-with-Multiview-Score-Distillation" class="headerlink" title="Identity Preserving 3D Head Stylization with Multiview Score   Distillation"></a>Identity Preserving 3D Head Stylization with Multiview Score   Distillation</h2><p><strong>Authors:Bahri Batuhan Bilecen, Ahmet Berke Gokmen, Furkan Guzelant, Aysegul Dundar</strong></p>
<p>3D head stylization transforms realistic facial features into artistic representations, enhancing user engagement across gaming and virtual reality applications. While 3D-aware generators have made significant advancements, many 3D stylization methods primarily provide near-frontal views and struggle to preserve the unique identities of original subjects, often resulting in outputs that lack diversity and individuality. This paper addresses these challenges by leveraging the PanoHead model, synthesizing images from a comprehensive 360-degree perspective. We propose a novel framework that employs negative log-likelihood distillation (LD) to enhance identity preservation and improve stylization quality. By integrating multi-view grid score and mirror gradients within the 3D GAN architecture and introducing a score rank weighing technique, our approach achieves substantial qualitative and quantitative improvements. Our findings not only advance the state of 3D head stylization but also provide valuable insights into effective distillation processes between diffusion models and GANs, focusing on the critical issue of identity preservation. Please visit the <a target="_blank" rel="noopener" href="https://three-bee.github.io/head_stylization">https://three-bee.github.io/head_stylization</a> for more visuals. </p>
<blockquote>
<p>3Då¤´éƒ¨é£æ ¼åŒ–è½¬æ¢èƒ½å°†ç°å®çš„é¢éƒ¨ç‰¹å¾è½¬åŒ–ä¸ºè‰ºæœ¯è¡¨ç°å½¢å¼ï¼Œå¢å¼ºæ¸¸æˆå’Œè™šæ‹Ÿç°å®åº”ç”¨ä¸­çš„ç”¨æˆ·å‚ä¸åº¦ã€‚è™½ç„¶3Dæ„ŸçŸ¥ç”Ÿæˆå™¨å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è®¸å¤š3Dé£æ ¼åŒ–æ–¹æ³•ä¸»è¦æä¾›è¿‘ä¹æ­£é¢çš„è§†è§’ï¼Œå¹¶ä¸”åœ¨ä¿ç•™åŸå§‹ä¸»ä½“çš„ç‹¬ç‰¹èº«ä»½æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™å¾€å¾€å¯¼è‡´è¾“å‡ºç»“æœç¼ºä¹å¤šæ ·æ€§å’Œä¸ªæ€§åŒ–ã€‚æœ¬æ–‡é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œé‡‡ç”¨PanoHeadæ¨¡å‹ï¼Œä»å…¨é¢çš„360åº¦è§†è§’åˆæˆå›¾åƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡‡ç”¨è´Ÿå¯¹æ•°ä¼¼ç„¶è’¸é¦ï¼ˆLDï¼‰çš„æ–°æ¡†æ¶ï¼Œä»¥æé«˜èº«ä»½ä¿ç•™å’Œæé«˜é£æ ¼åŒ–è´¨é‡ã€‚é€šè¿‡åœ¨3D GANæ¶æ„ä¸­æ•´åˆå¤šè§†å›¾ç½‘æ ¼è¯„åˆ†å’Œé•œåƒæ¢¯åº¦ï¼Œå¹¶å¼•å…¥è¯„åˆ†æ’ååŠ æƒæŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…æ¨åŠ¨äº†3Då¤´éƒ¨é£æ ¼åŒ–çš„ç ”ç©¶å‘å±•ï¼Œè€Œä¸”ä¸ºæ‰©æ•£æ¨¡å‹å’ŒGANsä¹‹é—´çš„æœ‰æ•ˆè’¸é¦è¿‡ç¨‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œé‡ç‚¹å…³æ³¨èº«ä»½ä¿ç•™è¿™ä¸€å…³é”®é—®é¢˜ã€‚æ›´å¤šè§†è§‰æ•ˆæœè¯·è®¿é—® <a target="_blank" rel="noopener" href="https://three-bee.github.io/head_stylization%E3%80%82">https://three-bee.github.io/head_stylizationã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13536v2">PDF</a> <a target="_blank" rel="noopener" href="https://three-bee.github.io/head_stylization">https://three-bee.github.io/head_stylization</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäº3Då¤´æ¨¡å‹æŠ€æœ¯ï¼Œè¯¥è®ºæ–‡æ¢è®¨äº†å¦‚ä½•æå‡è™šæ‹Ÿè§’è‰²å¤´éƒ¨è‰ºæœ¯é£æ ¼çš„æ¸²æŸ“æ•ˆæœå’Œç”¨æˆ·å‚ä¸åº¦ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤šè§’åº¦å±•ç¤ºå’Œä¸ªæ€§åŒ–èº«ä»½ä¿ç•™æ–¹é¢çš„ä¸è¶³ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡è´Ÿå¯¹æ•°ä¼¼ç„¶è’¸é¦æŠ€æœ¯ï¼ˆLDï¼‰å’Œå¤šè§†è§’ç½‘æ ¼è¯„åˆ†ä¸é•œåƒæ¢¯åº¦é›†æˆåœ¨3D GANæ¶æ„ä¸­ï¼Œå®ç°äº†å¤´éƒ¨é£æ ¼åŒ–çš„å®è´¨æ€§æå‡ã€‚è¯¥ç ”ç©¶ä¸ä»…æ¨åŠ¨äº†3Då¤´éƒ¨é£æ ¼åŒ–çš„è¿›å±•ï¼Œè¿˜ä¸ºæ‰©æ•£æ¨¡å‹å’ŒGANä¹‹é—´çš„æœ‰æ•ˆè’¸é¦è¿‡ç¨‹æä¾›äº†å®è´µè§è§£ã€‚æ›´å¤šè§†è§‰å±•ç¤ºè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://three-bee.github.io/head_stylization%E3%80%82">https://three-bee.github.io/head_stylizationã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-12e70152b2add82b5dc549ff7704bc76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1612f87ccdf6345ee2c899364b1ce24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec1e4c3f067c2beebf65774cf0a80e16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0afc402e0563d5c0e25c3f0b8adc57cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-136a4a25b7a3be398142db937a06e31e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Pixel-super-resolved-virtual-staining-of-label-free-tissue-using-diffusion-models"><a href="#Pixel-super-resolved-virtual-staining-of-label-free-tissue-using-diffusion-models" class="headerlink" title="Pixel super-resolved virtual staining of label-free tissue using   diffusion models"></a>Pixel super-resolved virtual staining of label-free tissue using   diffusion models</h2><p><strong>Authors:Yijie Zhang, Luzhe Huang, Nir Pillar, Yuzhu Li, Hanlong Chen, Aydogan Ozcan</strong></p>
<p>Virtual staining of tissue offers a powerful tool for transforming label-free microscopy images of unstained tissue into equivalents of histochemically stained samples. This study presents a diffusion model-based super-resolution virtual staining approach utilizing a Brownian bridge process to enhance both the spatial resolution and fidelity of label-free virtual tissue staining, addressing the limitations of traditional deep learning-based methods. Our approach integrates novel sampling techniques into a diffusion model-based image inference process to significantly reduce the variance in the generated virtually stained images, resulting in more stable and accurate outputs. Blindly applied to lower-resolution auto-fluorescence images of label-free human lung tissue samples, the diffusion-based super-resolution virtual staining model consistently outperformed conventional approaches in resolution, structural similarity and perceptual accuracy, successfully achieving a super-resolution factor of 4-5x, increasing the output space-bandwidth product by 16-25-fold compared to the input label-free microscopy images. Diffusion-based super-resolved virtual tissue staining not only improves resolution and image quality but also enhances the reliability of virtual staining without traditional chemical staining, offering significant potential for clinical diagnostics. </p>
<blockquote>
<p>ç»„ç»‡è™šæ‹ŸæŸ“è‰²ä¸ºæ— æ ‡ç­¾çš„æ˜¾å¾®é•œå›¾åƒæä¾›äº†ä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œå°†å…¶è½¬åŒ–ä¸ºä¸ç»„åŒ–æŸ“è‰²æ ·æœ¬ç›¸å½“çš„å›¾åƒã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è¶…åˆ†è¾¨ç‡è™šæ‹ŸæŸ“è‰²æ–¹æ³•ï¼Œåˆ©ç”¨å¸ƒæœ—æ¡¥è¿‡ç¨‹æé«˜äº†æ— æ ‡ç­¾è™šæ‹Ÿç»„ç»‡æŸ“è‰²çš„ç©ºé—´åˆ†è¾¨ç‡å’Œä¿çœŸåº¦ï¼Œè§£å†³äº†åŸºäºä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ–°å‹é‡‡æ ·æŠ€æœ¯é›†æˆåˆ°åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒæ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ˜¾è‘—é™ä½äº†ç”Ÿæˆè™šæ‹ŸæŸ“è‰²å›¾åƒçš„æ–¹å·®ï¼Œä»è€Œå¾—åˆ°æ›´ç¨³å®šå’Œå‡†ç¡®çš„è¾“å‡ºã€‚ç›²ç›®åº”ç”¨äºæ— æ ‡ç­¾äººè‚ºç»„ç»‡æ ·æœ¬çš„ä½åˆ†è¾¨ç‡è‡ªä½“è§å…‰å›¾åƒï¼ŒåŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡è™šæ‹ŸæŸ“è‰²æ¨¡å‹åœ¨åˆ†è¾¨ç‡ã€ç»“æ„ç›¸ä¼¼æ€§å’Œæ„ŸçŸ¥å‡†ç¡®æ€§æ–¹é¢å§‹ç»ˆä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒæˆåŠŸå®ç°4-5å€çš„è¶…åˆ†è¾¨ç‡ï¼Œä¸æ— æ ‡ç­¾çš„æ˜¾å¾®é•œå›¾åƒç›¸æ¯”ï¼Œè¾“å‡ºç©ºé—´å¸¦å®½ç§¯å¢åŠ äº†16-25å€ã€‚åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡è™šæ‹Ÿç»„ç»‡æŸ“è‰²ä¸ä»…æé«˜äº†åˆ†è¾¨ç‡å’Œå›¾åƒè´¨é‡ï¼Œè¿˜æé«˜äº†è™šæ‹ŸæŸ“è‰²çš„å¯é æ€§ï¼Œæ— éœ€ä¼ ç»Ÿçš„åŒ–å­¦æŸ“è‰²ï¼Œä¸ºä¸´åºŠè¯Šæ–­æä¾›äº†å·¨å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20073v2">PDF</a> 39 Pages, 7 Figures</p>
<p><strong>Summary</strong></p>
<p>è™šæ‹ŸæŸ“è‰²ç»„ç»‡æŠ€æœ¯é€šè¿‡æ‰©æ•£æ¨¡å‹å®ç°äº†æ— æ ‡è®°æ˜¾å¾®é•œå›¾åƒå‘ç­‰æ•ˆçš„å…ç–«ç»„ç»‡åŒ–å­¦æŸ“è‰²æ ·æœ¬çš„è½¬å˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è¶…åˆ†è¾¨ç‡è™šæ‹ŸæŸ“è‰²æ–¹æ³•ï¼Œåˆ©ç”¨å¸ƒæœ—æ¡¥è¿‡ç¨‹æé«˜äº†æ— æ ‡è®°è™šæ‹Ÿç»„ç»‡æŸ“è‰²çš„ç©ºé—´åˆ†è¾¨ç‡å’Œä¿çœŸåº¦ï¼Œè§£å†³äº†ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•å°†æ–°å‹é‡‡æ ·æŠ€æœ¯é›†æˆåˆ°æ‰©æ•£æ¨¡å‹å›¾åƒæ¨æ–­è¿‡ç¨‹ä¸­ï¼Œæ˜¾è‘—é™ä½äº†ç”Ÿæˆè™šæ‹ŸæŸ“è‰²å›¾åƒçš„æ–¹å·®ï¼Œäº§ç”Ÿäº†æ›´ç¨³å®šã€æ›´å‡†ç¡®çš„è¾“å‡ºã€‚åº”ç”¨äºæ— æ ‡è®°äººè‚ºç»„ç»‡æ ·æœ¬çš„ä½åˆ†è¾¨ç‡è‡ªä½“è§å…‰å›¾åƒæ—¶ï¼ŒåŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡è™šæ‹ŸæŸ“è‰²æ¨¡å‹åœ¨åˆ†è¾¨ç‡ã€ç»“æ„ç›¸ä¼¼æ€§å’Œæ„ŸçŸ¥å‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒæˆåŠŸå®ç°4-5å€çš„è¶…åˆ†è¾¨ç‡ï¼Œè¾“å‡ºç©ºé—´å¸¦å®½ä¹˜ç§¯æ¯”è¾“å…¥çš„æ— æ ‡è®°æ˜¾å¾®é•œå›¾åƒæé«˜äº†16-25å€ã€‚åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡è™šæ‹Ÿç»„ç»‡æŸ“è‰²æŠ€æœ¯ä¸ä»…æé«˜äº†åˆ†è¾¨ç‡å’Œå›¾åƒè´¨é‡ï¼Œè¿˜æé«˜äº†æ— ä¼ ç»ŸåŒ–å­¦æŸ“æ–™çš„è™šæ‹ŸæŸ“è‰²çš„å¯é æ€§ï¼Œä¸ºä¸´åºŠè¯Šæ–­å’Œæ²»ç–—æä¾›äº†å·¨å¤§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è™šæ‹ŸæŸ“è‰²æŠ€æœ¯èƒ½å°†æ— æ ‡è®°çš„æ˜¾å¾®é•œå›¾åƒè½¬åŒ–ä¸ºç­‰æ•ˆçš„å…ç–«ç»„ç»‡åŒ–å­¦æŸ“è‰²æ ·æœ¬ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è¶…åˆ†è¾¨ç‡è™šæ‹ŸæŸ“è‰²æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¸ƒæœ—æ¡¥è¿‡ç¨‹æé«˜äº†æ— æ ‡è®°è™šæ‹Ÿç»„ç»‡æŸ“è‰²çš„ç©ºé—´åˆ†è¾¨ç‡å’Œä¿çœŸåº¦ã€‚</li>
<li>è¯¥æ–¹æ³•é›†æˆäº†æ–°å‹é‡‡æ ·æŠ€æœ¯åˆ°æ‰©æ•£æ¨¡å‹å›¾åƒæ¨æ–­ä¸­ï¼Œæé«˜äº†è™šæ‹ŸæŸ“è‰²å›¾åƒçš„ç¨³å®šæ€§ä¸å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨æ— æ ‡è®°è‚ºç»„ç»‡æ ·æœ¬ä¸Šåº”ç”¨æ—¶ï¼ŒåŸºäºæ‰©æ•£çš„æ–¹æ³•åœ¨åˆ†è¾¨ç‡ã€ç»“æ„ç›¸ä¼¼æ€§å’Œæ„ŸçŸ¥å‡†ç¡®æ€§æ–¹é¢è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>æˆåŠŸå®ç°äº†4-5å€çš„è¶…åˆ†è¾¨ç‡ï¼Œå¤§å¹…æå‡äº†è¾“å‡ºå›¾åƒçš„ç©ºé—´å¸¦å®½ä¹˜ç§¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20073">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9df1c4597a1b86646f6e517dbb2d057d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DepthART-Monocular-Depth-Estimation-as-Autoregressive-Refinement-Task"><a href="#DepthART-Monocular-Depth-Estimation-as-Autoregressive-Refinement-Task" class="headerlink" title="DepthART: Monocular Depth Estimation as Autoregressive Refinement Task"></a>DepthART: Monocular Depth Estimation as Autoregressive Refinement Task</h2><p><strong>Authors:Bulat Gabdullin, Nina Konovalova, Nikolay Patakin, Dmitry Senushkin, Anton Konushin</strong></p>
<p>Monocular depth estimation has seen significant advances through discriminative approaches, yet their performance remains constrained by the limitations of training datasets. While generative approaches have addressed this challenge by leveraging priors from internet-scale datasets, with recent studies showing state-of-the-art results using fine-tuned text-to-image diffusion models, there is still room for improvement. Notably, autoregressive generative approaches, particularly Visual AutoRegressive modeling, have demonstrated superior results compared to diffusion models in conditioned image synthesis, while offering faster inference times. In this work, we apply Visual Autoregressive Transformer (VAR) to the monocular depth estimation problem. However, the conventional GPT-2-style training procedure (teacher forcing) inherited by VAR yields suboptimal results for depth estimation. To address this limitation, we introduce DepthART - a novel training method formulated as a Depth Autoregressive Refinement Task. Unlike traditional VAR training with static inputs and targets, our method implements a dynamic target formulation based on model outputs, enabling self-refinement. By utilizing the modelâ€™s own predictions as inputs instead of ground truth token maps during training, we frame the objective as residual minimization, effectively reducing the discrepancy between training and inference procedures. Our experimental results demonstrate that the proposed training approach significantly enhances the performance of VAR in depth estimation tasks. When trained on Hypersim dataset using our approach, the model achieves superior results across multiple unseen benchmarks compared to existing generative and discriminative baselines. </p>
<blockquote>
<p>å•ç›®æ·±åº¦ä¼°è®¡é€šè¿‡åˆ¤åˆ«æ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ€§èƒ½ä»å—åˆ°è®­ç»ƒæ•°æ®é›†å±€é™æ€§çš„é™åˆ¶ã€‚è™½ç„¶ç”Ÿæˆæ–¹æ³•é€šè¿‡åˆ©ç”¨äº’è”ç½‘è§„æ¨¡æ•°æ®é›†çš„å…ˆéªŒæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä¸”è¿‘æœŸç ”ç©¶ä½¿ç”¨å¾®è°ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ˜¾ç¤ºå‡ºæœ€å‰æ²¿ç»“æœï¼Œä½†ä»å­˜åœ¨æ”¹è¿›ç©ºé—´ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè‡ªå›å½’ç”Ÿæˆæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯è§†è§‰è‡ªå›å½’å»ºæ¨¡ï¼Œåœ¨æ¡ä»¶å›¾åƒåˆæˆæ–¹é¢æ˜¾ç¤ºå‡ºæ¯”æ‰©æ•£æ¨¡å‹æ›´ä¼˜è¶Šçš„ç»“æœï¼ŒåŒæ—¶æä¾›æ›´å¿«çš„æ¨ç†æ—¶é—´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è§†è§‰è‡ªå›å½’è½¬æ¢å™¨ï¼ˆVARï¼‰åº”ç”¨äºå•ç›®æ·±åº¦ä¼°è®¡é—®é¢˜ã€‚ç„¶è€Œï¼ŒVARç»§æ‰¿çš„å¸¸è§„GPT-2é£æ ¼è®­ç»ƒç¨‹åºï¼ˆæ•™å¸ˆå¼ºåˆ¶ï¼‰å¯¹äºæ·±åº¦ä¼°è®¡äº§ç”Ÿæ¬¡ä¼˜ç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†DepthARTâ€”â€”ä¸€ç§æ–°é¢–çš„è®­ç»ƒæ–¹æ³•ï¼Œè¢«åˆ¶å®šä¸ºæ·±åº¦è‡ªå›å½’ç»†åŒ–ä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿçš„ä½¿ç”¨é™æ€è¾“å…¥å’Œç›®æ ‡çš„VARè®­ç»ƒä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†åŸºäºæ¨¡å‹è¾“å‡ºçš„åŠ¨æ€ç›®æ ‡å…¬å¼ï¼Œå®ç°è‡ªæˆ‘å®Œå–„ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨æ¨¡å‹è‡ªå·±çš„é¢„æµ‹ä½œä¸ºè¾“å…¥ï¼Œè€Œä¸æ˜¯ä½¿ç”¨åœ°é¢çœŸå®ä»¤ç‰Œæ˜ å°„ï¼Œæˆ‘ä»¬å°†ç›®æ ‡å®šä½ä¸ºæ®‹å·®æœ€å°åŒ–ï¼Œæœ‰æ•ˆå‡å°‘äº†è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¹‹é—´çš„å·®å¼‚ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è®­ç»ƒæ–¹æ³•æ˜¾è‘—æé«˜äº†VARåœ¨æ·±åº¦ä¼°è®¡ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚åœ¨HyperSimæ•°æ®é›†ä¸Šé‡‡ç”¨æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œè®­ç»ƒåï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæœªè§è¿‡çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜äºç°æœ‰ç”Ÿæˆå’Œåˆ¤åˆ«åŸºå‡†çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15010v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æ–‡æœ¬æè¿°äº†ä½¿ç”¨ç”Ÿæˆæ¨¡å‹è¿›è¡Œå•çœ¼æ·±åº¦ä¼°è®¡çš„æŒ‘æˆ˜åŠè¿›å±•ã€‚ä¸ºæé«˜æ¨¡å‹æ€§èƒ½ï¼Œæå‡ºå°†è§†è§‰è‡ªå›å½’è½¬æ¢å™¨ï¼ˆVARï¼‰åº”ç”¨äºå•çœ¼æ·±åº¦ä¼°è®¡é—®é¢˜ï¼Œå¹¶å¼•å…¥DepthARTè®­ç»ƒæ–¹æ³•ã€‚è¯¥ç­–ç•¥ä½¿ç”¨æ¨¡å‹çš„é¢„æµ‹å€¼è€ŒéçœŸå®æ ‡ç­¾ä½œä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„è¾“å…¥ï¼Œä»è€Œå®ç°ç›®æ ‡æ®‹ä½™æœ€å°åŒ–å¹¶æå‡æ¨¡å‹æ€§èƒ½ã€‚ç»è¿‡HyperSimæ•°æ®é›†è®­ç»ƒçš„å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºç°æœ‰çš„ç”Ÿæˆå’Œåˆ¤åˆ«åŸºå‡†æ¨¡å‹ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæœªè§è¿‡çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ›´å¥½çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹å¯¹å•çœ¼æ·±åº¦ä¼°è®¡çš„æ€§èƒ½æå‡å—é™ï¼Œå°½ç®¡ä½¿ç”¨åˆ¤åˆ«æ–¹æ³•å·²ç»å–å¾—è¿›å±•ã€‚éœ€è¦æ”¹è¿›æ¨¡å‹çš„æ€§èƒ½å’Œé€‚ç”¨æ€§ã€‚</li>
<li>åˆ©ç”¨å¤§å‹äº’è”ç½‘æ•°æ®é›†ä½œä¸ºå…ˆéªŒçš„ç”Ÿæˆæ–¹æ³•å·²ç»åœ¨ä¸€å®šç¨‹åº¦ä¸Šè§£å†³äº†è®­ç»ƒæ•°æ®é›†çš„é—®é¢˜ã€‚æ‰©æ•£æ¨¡å‹å·²ç»åœ¨ç²¾ç»†è°ƒæ•´æ–‡æœ¬åˆ°å›¾åƒçš„ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚ä½†è‡ªå›å½’ç”Ÿæˆæ¨¡å‹åœ¨æ¡ä»¶å›¾åƒåˆæˆæ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15010">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8236468dae52aa20ac89891ac0ac5b7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bde48acc345b472de17370e1f1e09adb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-409ca3c6b09102ac42bff75193959c13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0238a2802bcbb4d5d33ef3636f6dd933.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0422781934800f19092489791ef3ee8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-310566b866804cf7ef341bbd148e5353.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Transformers-from-Diffusion-A-Unified-Framework-for-Neural-Message-Passing"><a href="#Transformers-from-Diffusion-A-Unified-Framework-for-Neural-Message-Passing" class="headerlink" title="Transformers from Diffusion: A Unified Framework for Neural Message   Passing"></a>Transformers from Diffusion: A Unified Framework for Neural Message   Passing</h2><p><strong>Authors:Qitian Wu, David Wipf, Junchi Yan</strong></p>
<p>Learning representations for structured data with certain geometries (e.g., observed or unobserved) is a fundamental challenge, wherein message passing neural networks (MPNNs) have become a de facto class of model solutions. In this paper, inspired by physical systems, we propose an energy-constrained diffusion model, which integrates the inductive bias of diffusion on manifolds with layer-wise constraints of energy minimization. We identify that the diffusion operators have a one-to-one correspondence with the energy functions implicitly descended by the diffusion process, and the finite-difference iteration for solving the energy-constrained diffusion system induces the propagation layers of various types of MPNNs operating on observed or latent structures. This leads to a unified mathematical framework for common neural architectures whose computational flows can be cast as message passing (or its special case), including MLPs, GNNs, and Transformers. Building on these insights, we devise a new class of neural message passing models, dubbed diffusion-inspired Transformers (DIFFormer), whose global attention layers are derived from the principled energy-constrained diffusion framework. Across diverse datasets ranging from real-world networks to images, texts, and physical particles, we demonstrate that the new model achieves promising performance in scenarios where the data structures are observed (as a graph), partially observed, or entirely unobserved. </p>
<blockquote>
<p>é’ˆå¯¹å…·æœ‰æŸäº›å‡ ä½•ç»“æ„ï¼ˆæ— è®ºæ˜¯è§‚å¯Ÿåˆ°çš„è¿˜æ˜¯æœªè§‚å¯Ÿåˆ°çš„ï¼‰çš„æ•°æ®å­¦ä¹ è¡¨ç¤ºæ˜¯ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ï¼Œå…¶ä¸­æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰å·²æˆä¸ºä¸€ç§æ¨¡å‹è§£å†³æ–¹æ¡ˆçš„å®é™…ç±»åˆ«ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å—åˆ°ç‰©ç†ç³»ç»Ÿçš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªå—èƒ½é‡çº¦æŸçš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æµå½¢ä¸Šæ‰©æ•£çš„å½’çº³åè§ä¸èƒ½é‡æœ€å°åŒ–çš„é€å±‚çº¦æŸã€‚æˆ‘ä»¬å‘ç°æ‰©æ•£ç®—å­ä¸æ‰©æ•£è¿‡ç¨‹éšå«ä¸‹é™çš„èƒ½é‡å‡½æ•°æœ‰ä¸€ä¸€å¯¹åº”å…³ç³»ï¼Œè§£å†³èƒ½é‡çº¦æŸæ‰©æ•£ç³»ç»Ÿçš„æœ‰é™å·®åˆ†è¿­ä»£å¼•å‘äº†æ“ä½œåœ¨è§‚æµ‹æˆ–æ½œåœ¨ç»“æ„ä¸Šçš„å„ç§ç±»å‹çš„MPNNçš„ä¼ æ’­å±‚ã€‚è¿™ä¸ºå¸¸è§çš„ç¥ç»ç½‘ç»œæ¶æ„æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ï¼Œå…¶è®¡ç®—æµç¨‹å¯ä»¥è¡¨ç¤ºä¸ºæ¶ˆæ¯ä¼ é€’ï¼ˆæˆ–å…¶ç‰¹æ®Šæƒ…å†µï¼‰ï¼ŒåŒ…æ‹¬MLPsã€GNNså’ŒTransformerã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹çš„ç¥ç»ç½‘ç»œæ¶ˆæ¯ä¼ é€’æ¨¡å‹ï¼Œç§°ä¸ºå—æ‰©æ•£å¯å‘çš„Transformerï¼ˆDIFFormerï¼‰ï¼Œå…¶å…¨å±€æ³¨æ„åŠ›å±‚æºäºæœ‰åŸåˆ™çš„èƒ½é‡çº¦æŸæ‰©æ•£æ¡†æ¶ã€‚åœ¨æ¶µç›–ä»çœŸå®ä¸–ç•Œç½‘ç»œåˆ°å›¾åƒã€æ–‡æœ¬å’Œç‰©ç†ç²’å­çš„å„ç§æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ–°æ¨¡å‹åœ¨æ•°æ®ç»“æ„ä¸ºå›¾ã€éƒ¨åˆ†è§‚æµ‹æˆ–å®Œå…¨æœªè§‚æµ‹çš„åœºæ™¯ä¸­å‡å–å¾—äº†æœ‰å‰æ™¯çš„æ€§èƒ½è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09111v4">PDF</a> Published in Journal of Machine Learning Research (JMLR). Extended   from DIFFormer in ICLR 2023</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå—ç‰©ç†ç³»ç»Ÿå¯å‘çš„èƒ½é‡çº¦æŸæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æ‰©æ•£æµå½¢ä¸Šçš„å½’çº³åè§ä¸èƒ½é‡æœ€å°åŒ–çš„é€å±‚çº¦æŸã€‚æ‰©æ•£ç®—å­ä¸èƒ½é‡å‡½æ•°æœ‰ä¸€ä¸€å¯¹åº”å…³ç³»ï¼Œé€šè¿‡æ±‚è§£èƒ½é‡çº¦æŸæ‰©æ•£ç³»ç»Ÿå¾—åˆ°çš„æœ‰é™å·®åˆ†è¿­ä»£ï¼Œè¯±å¯¼äº†é€‚ç”¨äºè§‚æµ‹æˆ–æ½œåœ¨ç»“æ„çš„å„ç±»æ¶ˆæ¯ä¼ é€’ç½‘ç»œï¼ˆMPNNsï¼‰çš„ä¼ æ’­å±‚ã€‚è¿™ä¸ºå¸¸è§çš„è®¡ç®—æµç¨‹å¯è½¬æ¢ä¸ºæ¶ˆæ¯ä¼ é€’æˆ–å…¶ç‰¹æ®Šæƒ…å†µçš„ç¥ç»ç½‘ç»œæ¶æ„æä¾›äº†ç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ï¼ŒåŒ…æ‹¬MLPsã€GNNså’ŒTransformerã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹çš„ç¥ç»æ¶ˆæ¯ä¼ é€’æ¨¡å‹â€”â€”æ‰©æ•£å¯å‘å¼Transformerï¼ˆDIFFormerï¼‰ï¼Œå…¶å…¨å±€æ³¨æ„åŠ›å±‚æºäºæœ‰åŸåˆ™çš„èƒ½é‡çº¦æŸæ‰©æ•£æ¡†æ¶ã€‚åœ¨å¤šç§æ•°æ®é›†ä¸Šï¼Œæ— è®ºæ˜¯ä½œä¸ºå›¾è§‚æµ‹ã€éƒ¨åˆ†è§‚æµ‹è¿˜æ˜¯å®Œå…¨æœªè§‚æµ‹çš„æ•°æ®ç»“æ„ï¼Œæ–°æ¨¡å‹å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†èƒ½é‡çº¦æŸæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†æ‰©æ•£æµå½¢ä¸Šçš„å½’çº³åè§ä¸èƒ½é‡æœ€å°åŒ–çš„é€å±‚çº¦æŸã€‚</li>
<li>æ‰©æ•£ç®—å­ä¸èƒ½é‡å‡½æ•°ä¹‹é—´å­˜åœ¨ä¸€ä¸€å¯¹åº”å…³ç³»ã€‚</li>
<li>æœ‰é™å·®åˆ†è¿­ä»£ä¸ºä¸åŒçš„MPNNsä¼ æ’­å±‚æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œè¿™äº›ç½‘ç»œå¯åº”ç”¨äºè§‚æµ‹æˆ–æ½œåœ¨ç»“æ„ã€‚</li>
<li>è¿™æä¾›äº†ç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ï¼Œæ¶µç›–è®¡ç®—æµç¨‹å¯è½¬æ¢ä¸ºæ¶ˆæ¯ä¼ é€’æˆ–å…¶ç‰¹æ®Šæƒ…å†µçš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå¦‚MLPsã€GNNså’ŒTransformerã€‚</li>
<li>åŸºäºä¸Šè¿°è§è§£ï¼Œè®ºæ–‡å¼€å‘äº†ä¸€ç§æ–°å‹çš„ç¥ç»æ¶ˆæ¯ä¼ é€’æ¨¡å‹â€”â€”æ‰©æ•£å¯å‘å¼Transformerï¼ˆDIFFormerï¼‰ã€‚</li>
<li>DIFFormerçš„å…¨å±€æ³¨æ„åŠ›å±‚æ˜¯åŸºäºæœ‰åŸåˆ™çš„èƒ½é‡çº¦æŸæ‰©æ•£æ¡†æ¶æ¨å¯¼å‡ºæ¥çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09111">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e594374d898bf068b3cecc018ae5d789.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71cf17a0d481fd4a4e8eaeb91cda418e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-51ed996d0656e43ac56aed689e67752c.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Supervised Diffusion-Model-Based PET Image Reconstruction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-8316847a61ca9ce8a873eb8dcbaa3c8a.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  AttentionGS Towards Initialization-Free 3D Gaussian Splatting via   Structural Attention
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
