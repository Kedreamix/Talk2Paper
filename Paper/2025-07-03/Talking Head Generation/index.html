<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-07-03  JAM-Flow Joint Audio-Motion Synthesis with Flow Matching">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-7787627c70d8d372e310b21edbf13e96.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    23 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-03-更新"><a href="#2025-07-03-更新" class="headerlink" title="2025-07-03 更新"></a>2025-07-03 更新</h1><h2 id="JAM-Flow-Joint-Audio-Motion-Synthesis-with-Flow-Matching"><a href="#JAM-Flow-Joint-Audio-Motion-Synthesis-with-Flow-Matching" class="headerlink" title="JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching"></a>JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching</h2><p><strong>Authors:Mingi Kwon, Joonghyuk Shin, Jaeseok Jung, Jaesik Park, Youngjung Uh</strong></p>
<p>The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: <a target="_blank" rel="noopener" href="https://joonghyuk.com/jamflow-web">https://joonghyuk.com/jamflow-web</a> </p>
<blockquote>
<p>面部动作与语音之间的内在联系在生成模型中经常被忽视，生成模型中通常将头部合成和文本到语音（TTS）视为单独的任务。本文介绍了JAM-Flow，这是一个统一框架，可以同时合成并基于面部动作和语音的条件进行建模。我们的方法利用流匹配和新颖的多模态扩散转换器（MM-DiT）架构，集成了专门的Motion-DiT和Audio-DiT模块。它们通过选择性联合注意层进行耦合，并采用了关键架构选择，如时间对齐位置嵌入和局部联合注意掩码，以实现有效的跨模态交互同时保留模态特定的优势。通过采用填充风格的目标进行训练，JAM-Flow支持广泛的条件输入，包括文本、参考音频和参考动作，促进诸如从文本生成同步头部、音频驱动动画等任务，在一个单一连贯的模型中实现更多功能。JAM-Flow通过为整体视听合成提供实用解决方案，从而极大地推动了多模态生成模型的发展。项目页面：<a target="_blank" rel="noopener" href="https://joonghyuk.com/jamflow-web">https://joonghyuk.com/jamflow-web</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23552v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://joonghyuk.com/jamflow-web">https://joonghyuk.com/jamflow-web</a> Under review.   Preprint published on arXiv</p>
<p><strong>Summary</strong></p>
<p>本文提出一个统一框架JAM-Flow，实现面部动作与语音的同步合成与条件控制。该框架采用流匹配技术和新型多模态扩散转换器（MM-DiT）架构，整合Motion-DiT和Audio-DiT模块，通过选择性联合注意力层进行耦合。JAM-Flow支持多种条件输入，包括文本、参考音频和参考动作，适用于从文本生成同步说话头像、音频驱动动画等任务。该框架为多模态生成建模提供了实用解决方案，实现了音频视觉合成的整体融合。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>JAM-Flow是一个统一框架，用于面部动作和语音的合成与条件控制。</li>
<li>该框架结合流匹配技术和Multi-Modal Diffusion Transformer（MM-DiT）架构。</li>
<li>MM-DiT架构包括Motion-DiT和Audio-DiT模块，通过选择性联合注意力层进行交互。</li>
<li>JAM-Flow支持多种条件输入，包括文本、参考音频和参考动作。</li>
<li>该框架可实现从文本生成同步说话头像、音频驱动动画等功能。</li>
<li>JAM-Flow通过采用inpainting风格的目标函数进行训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23552">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f1c60399bb279929872039df31f3b028.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf29df7073b460c0a4a11ad28902c1c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6b09d905bf5dbb272c78025f362fa9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-190a4060cc989a07869321dfbded7e81.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MirrorMe-Towards-Realtime-and-High-Fidelity-Audio-Driven-Halfbody-Animation"><a href="#MirrorMe-Towards-Realtime-and-High-Fidelity-Audio-Driven-Halfbody-Animation" class="headerlink" title="MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody   Animation"></a>MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody   Animation</h2><p><strong>Authors:Dechao Meng, Steven Xiao, Xindi Zhang, Guangyuan Wang, Peng Zhang, Qi Wang, Bang Zhang, Liefeng Bo</strong></p>
<p>Audio-driven portrait animation, which synthesizes realistic videos from reference images using audio signals, faces significant challenges in real-time generation of high-fidelity, temporally coherent animations. While recent diffusion-based methods improve generation quality by integrating audio into denoising processes, their reliance on frame-by-frame UNet architectures introduces prohibitive latency and struggles with temporal consistency. This paper introduces MirrorMe, a real-time, controllable framework built on the LTX video model, a diffusion transformer that compresses video spatially and temporally for efficient latent space denoising. To address LTX’s trade-offs between compression and semantic fidelity, we propose three innovations: 1. A reference identity injection mechanism via VAE-encoded image concatenation and self-attention, ensuring identity consistency; 2. A causal audio encoder and adapter tailored to LTX’s temporal structure, enabling precise audio-expression synchronization; and 3. A progressive training strategy combining close-up facial training, half-body synthesis with facial masking, and hand pose integration for enhanced gesture control. Extensive experiments on the EMTD Benchmark demonstrate MirrorMe’s state-of-the-art performance in fidelity, lip-sync accuracy, and temporal stability. </p>
<blockquote>
<p>音频驱动肖像动画利用音频信号从参考图像合成逼真的视频，在实时生成高保真、时间连贯的动画方面面临重大挑战。虽然最近的基于扩散的方法通过将音频集成到去噪过程中来提高生成质量，但它们对逐帧UNet架构的依赖导致了过高的延迟和时间连贯性问题。本文介绍了MirrorMe，一个基于LTX视频模型的实时可控框架。LTX是一个扩散变压器，可以在空间和时间上压缩视频，以实现有效的潜在空间去噪。为了解决LTX在压缩和语义保真之间的权衡，我们提出了三项创新：1.通过VAE编码的图像拼接和自我关注机制实现参考身份注入机制，确保身份一致性；2.针对LTX的时间结构设计的因果音频编码器和适配器，实现精确的音频表达同步；3.结合面部特写训练、面部遮挡的半体合成以及手势整合的渐进式训练策略，增强手势控制。在EMTD基准测试上的大量实验表明，MirrorMe在保真度、唇同步准确性和时间稳定性方面达到了最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22065v1">PDF</a> 8 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为MirrorMe的实时可控框架，该框架基于LTX视频模型，采用扩散变压器技术，能够在空间和时间上压缩视频，实现高效的潜在空间去噪。通过三项创新技术：通过VAE编码的图像拼接和自注意力机制实现身份一致性；针对LTX的临时结构定制因果音频编码器和适配器，实现精确的音频表达同步；以及结合面部特写训练、半身合成与面部遮挡以及手势集成等逐步训练策略，提高了手势控制效果。在EMTD基准测试上的实验表明，MirrorMe在保真度、唇同步准确性和时间稳定性方面达到了最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MirrorMe是一个基于LTX视频模型的实时可控框架，能够实现高效去噪和音频驱动的肖像动画。</li>
<li>该框架通过三项创新技术解决了LTX模型在压缩和语义保真度之间的权衡问题。</li>
<li>采用身份一致性机制确保参考身份在动画中的连续性。</li>
<li>精确的音频表达同步通过因果音频编码器和适配器实现。</li>
<li>逐步训练策略提高了手势控制和整体动画的逼真度。</li>
<li>在EMTD基准测试上，MirrorMe表现出优异的性能，达到了最新技术水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22065">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f2723feedeeb72b548764ce8fdecda33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-392c690047771c318103fe0df078422a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53d67c4f27c7e5f208a6d3c622dfdddc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1861cf53ae55b6697aadaeef7b14953d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Identity-Adaptation-for-3D-Talking-Heads-via-Global-Gaussian-Field"><a href="#Few-Shot-Identity-Adaptation-for-3D-Talking-Heads-via-Global-Gaussian-Field" class="headerlink" title="Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian   Field"></a>Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian   Field</h2><p><strong>Authors:Hong Nie, Fuyuan Cao, Lu Chen, Fengxin Chen, Yuefeng Zou, Jun Yu</strong></p>
<p>Reconstruction and rendering-based talking head synthesis methods achieve high-quality results with strong identity preservation but are limited by their dependence on identity-specific models. Each new identity requires training from scratch, incurring high computational costs and reduced scalability compared to generative model-based approaches. To overcome this limitation, we propose FIAG, a novel 3D speaking head synthesis framework that enables efficient identity-specific adaptation using only a few training footage. FIAG incorporates Global Gaussian Field, which supports the representation of multiple identities within a shared field, and Universal Motion Field, which captures the common motion dynamics across diverse identities. Benefiting from the shared facial structure information encoded in the Global Gaussian Field and the general motion priors learned in the motion field, our framework enables rapid adaptation from canonical identity representations to specific ones with minimal data. Extensive comparative and ablation experiments demonstrate that our method outperforms existing state-of-the-art approaches, validating both the effectiveness and generalizability of the proposed framework. Code is available at: \textit{<a target="_blank" rel="noopener" href="https://github.com/gme-hong/FIAG%7D">https://github.com/gme-hong/FIAG}</a>. </p>
<blockquote>
<p>基于重建和渲染的说话人头部合成方法虽然能够达成高质量的结果并强烈保留身份特征，但它们依赖于特定身份的模型，因此存在局限性。每个新身份都需要从头开始训练，这增加了计算成本，与基于生成模型的方法相比，可扩展性降低。为了克服这一限制，我们提出了FIAG，这是一个新型的3D说话人头部合成框架，它只需少量的训练镜头就能实现高效的身份特定适应。FIAG结合了全局高斯场，支持在共享场内表示多个身份，以及通用运动场，能捕捉不同身份之间的共同运动动力学。受益于编码在全局高斯场中的共享面部结构信息和运动场中学习到的一般运动先验，我们的框架能够迅速从规范身份表示适应到特定身份表示，所需数据最少。广泛的对比和消融实验表明，我们的方法优于现有的最先进方法，验证了所提出框架的有效性和通用性。代码可在：[<a target="_blank" rel="noopener" href="https://github.com/gme-hong/FIAG]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gme-hong/FIAG]上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22044v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于重建和渲染的说话人头部合成方法虽然能够达成高质量的结果并保留强烈的身份特征，但它们依赖于特定身份模型，为每个新身份都需要从头开始训练，计算成本高且可扩展性差。为了克服这一局限性，我们提出了FIAG，这是一个新型的3D说话人头合成框架，它使用少量的训练视频就能实现有效的身份特定适配。FIAG结合了全局高斯场，支持在共享场内表示多个身份，以及通用运动场，能捕捉不同身份的通用运动动态。得益于编码在全局高斯场中的共享面部结构信息和运动场中学习的通用运动先验知识，我们的框架能够迅速从规范身份表示适应特定身份，所需数据极少。广泛的对比和消融实验证明，我们的方法优于现有的最先进的方法，验证了所提出框架的有效性和通用性。相关代码已发布在：<a target="_blank" rel="noopener" href="https://github.com/gme-hong/FIAG%E3%80%82">https://github.com/gme-hong/FIAG。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FIAG是一个新型的3D说话人头合成框架，能有效进行身份特定适配。</li>
<li>FIAG使用全局高斯场和通用运动场来分别处理身份表示和运动动态。</li>
<li>框架能够迅速适应特定身份，仅需少量数据。</li>
<li>对比和消融实验证明FIAG优于现有方法。</li>
<li>FIAG框架具有有效性和通用性。</li>
<li>相关代码已发布在指定GitHub仓库。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22044">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-11f50ad98fffcbb550b2200e3d643b38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f29f7f387ccdcf4893e33be9de8b3c64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-479a0618e3a00e4840a52ab7953b5d0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-459fba3b6dca2b7686c5ad42ee62a64e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7787627c70d8d372e310b21edbf13e96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d3c4fd13861273e7797567c579a8849.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fabcb31d8c9c8938cc81c2f4937e5d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5adf561684b870784ab2333bb58d56df.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Audio-Plane-Audio-Factorization-Plane-Gaussian-Splatting-for-Real-Time-Talking-Head-Synthesis"><a href="#Audio-Plane-Audio-Factorization-Plane-Gaussian-Splatting-for-Real-Time-Talking-Head-Synthesis" class="headerlink" title="Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time   Talking Head Synthesis"></a>Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time   Talking Head Synthesis</h2><p><strong>Authors:Shuai Shen, Wanhua Li, Yunpeng Zhang, Yap-Peng Tan, Jiwen Lu</strong></p>
<p>Talking head synthesis has emerged as a prominent research topic in computer graphics and multimedia, yet most existing methods often struggle to strike a balance between generation quality and computational efficiency, particularly under real-time constraints. In this paper, we propose a novel framework that integrates Gaussian Splatting with a structured Audio Factorization Plane (Audio-Plane) to enable high-quality, audio-synchronized, and real-time talking head generation. For modeling a dynamic talking head, a 4D volume representation, which consists of three axes in 3D space and one temporal axis aligned with audio progression, is typically required. However, directly storing and processing a dense 4D grid is impractical due to the high memory and computation cost, and lack of scalability for longer durations. We address this challenge by decomposing the 4D volume representation into a set of audio-independent spatial planes and audio-dependent planes, forming a compact and interpretable representation for talking head modeling that we refer to as the Audio-Plane. This factorized design allows for efficient and fine-grained audio-aware spatial encoding, and significantly enhances the model’s ability to capture complex lip dynamics driven by speech signals. To further improve region-specific motion modeling, we introduce an audio-guided saliency splatting mechanism based on region-aware modulation, which adaptively emphasizes highly dynamic regions such as the mouth area. This allows the model to focus its learning capacity on where it matters most for accurate speech-driven animation. Extensive experiments on both the self-driven and the cross-driven settings demonstrate that our method achieves state-of-the-art visual quality, precise audio-lip synchronization, and real-time performance, outperforming prior approaches across both 2D- and 3D-based paradigms. </p>
<blockquote>
<p>头部谈话合成已成为计算机图形学和多媒体领域的一个突出研究话题。然而，大多数现有方法往往难以在生成质量和计算效率之间取得平衡，特别是在实时约束下。在本文中，我们提出了一个新颖框架，该框架结合了高斯贴图（Gaussian Splatting）与结构化音频分解平面（Audio-Plane），以实现高质量、音频同步、实时头部谈话生成。为了模拟动态谈话头部，通常需要一种4D体积表示，其由三维空间中的三个轴和与音频进展对齐的时间轴组成。然而，直接存储和处理密集的4D网格是不切实际的，因为这会消耗大量内存和计算资源，并且对于更长时间的模拟缺乏可扩展性。我们通过将4D体积表示分解为一系列音频独立的空间平面和音频依赖的平面来解决这一挑战，形成一种紧凑且可解释的谈话头部模型表示，我们称之为Audio-Plane。这种分解设计允许进行高效且精细的音频感知空间编码，并极大地提高了模型捕捉由语音信号驱动的复杂嘴唇动态的能力。为了进一步改进区域特定的运动建模，我们引入了一种基于区域感知调制的音频引导显著性贴图机制，该机制自适应地强调高度动态的区域，如嘴巴区域。这允许模型将学习能力集中在对于语音驱动动画最重要的地方。在自我驱动和交叉驱动两种设置下的广泛实验表明，我们的方法实现了最先进的视觉质量、精确的音频-嘴唇同步和实时性能，在二维和三维基于范式的方法中都超越了先前的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22605v2">PDF</a> Demo video at \url{<a target="_blank" rel="noopener" href="https://sstzal.github.io/Audio-Plane/%7D">https://sstzal.github.io/Audio-Plane/}</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的结合高斯融合与结构化音频分解平面（Audio-Plane）的框架，实现了高质量、音频同步、实时的人头生成。该框架通过分解四维体积表示，形成紧凑且可解释的音频平面表示法，有效解决了直接存储和处理密集四维网格的不切实际问题。此外，引入了音频引导显著性融合机制，自适应强调高度动态区域，如口腔区域，进一步提高区域特定运动建模的精确度。实验表明，该方法在自驱动和跨驱动设置中均实现了最佳视觉质量、精确的音频唇同步和实时性能，在2D和3D基础上均超越了先前的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了说话人头合成领域的研究现状和挑战。</li>
<li>提出了一种新的框架，结合了高斯融合和结构化音频分解平面（Audio-Plane），旨在实现高质量、音频同步和实时的说话人头生成。</li>
<li>通过分解四维体积表示法来解决直接处理密集四维网格的问题，提出了一种紧凑且可解释的音频平面表示法。</li>
<li>引入了音频引导显著性融合机制，以提高区域特定运动建模的精确度。</li>
<li>实验表明，该方法在视觉质量、音频唇同步和实时性能方面都达到了最先进的水平，且在自驱动和跨驱动设置中都优于以前的方法。</li>
<li>该方法提供了一种有效的平衡生成质量和计算效率的手段，特别是在实时约束下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22605">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b4421c32e1377871bb4076391b2450f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a354e62ab47877e903079e3a52c528a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4419f5ce49feac3a2a446fdb2cfae702.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-722d595511ab12179aecd25f16179962.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3bcbeb9dc09f309aeb2b3230feddadd.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="OmniHuman-1-Rethinking-the-Scaling-Up-of-One-Stage-Conditioned-Human-Animation-Models"><a href="#OmniHuman-1-Rethinking-the-Scaling-Up-of-One-Stage-Conditioned-Human-Animation-Models" class="headerlink" title="OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human   Animation Models"></a>OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human   Animation Models</h2><p><strong>Authors:Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, Chao Liang</strong></p>
<p>End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (<a target="_blank" rel="noopener" href="https://omnihuman-lab.github.io/">https://omnihuman-lab.github.io</a>) </p>
<blockquote>
<p>端到端的人类动画，例如音频驱动的人类谈话生成，在最近几年经历了显著的进步。然而，现有方法仍然难以扩展为大型通用视频生成模型，限制了其在真实应用中的潜力。在本文中，我们提出了OmniHuman，这是一个基于Diffusion Transformer的框架，通过混合运动相关条件来扩展数据训练阶段。为此，我们介绍了两种针对这些混合条件的训练原则，以及相应的模型结构和推理策略。这些设计使OmniHuman能够充分利用数据驱动的运动生成，最终实现高度逼真的人类视频生成。更重要的是，OmniHuman支持各种肖像内容（特写、肖像、半身、全身）、支持对话和歌唱、处理人与物体互动和具有挑战性的身体姿势，并适应不同的图像风格。与现有的端到端音频驱动方法相比，OmniHuman不仅生成更逼真的视频，而且在输入方面提供了更大的灵活性。它还支持多种驱动模式（音频驱动、视频驱动和组合驱动信号）。视频样本请参见ttfamily项目页面（<a target="_blank" rel="noopener" href="https://omnihuman-lab.github.io)./">https://omnihuman-lab.github.io）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01061v3">PDF</a> ICCV 2025, Homepage: <a target="_blank" rel="noopener" href="https://omnihuman-lab.github.io/">https://omnihuman-lab.github.io/</a></p>
<p><strong>Summary</strong><br>基于Diffusion Transformer框架的OmniHuman模型，通过混合运动条件进行训练，实现了大规模数据的扩展，提高了在真实应用场景中的潜力。该模型可以支持多种肖像内容、说话和唱歌、人机交互和复杂动作，并兼容不同的图像风格。相较于现有的端到端音频驱动方法，OmniHuman不仅能生成更逼真的视频，而且在输入上具有更大的灵活性，并支持多种驱动模式（音频驱动、视频驱动和组合驱动信号）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniHuman模型基于Diffusion Transformer框架构建。</li>
<li>通过混合运动条件进行训练，实现了数据的扩展。</li>
<li>支持多种肖像内容、说话和唱歌、人机交互和复杂动作。</li>
<li>兼容不同的图像风格。</li>
<li>相较于现有方法，OmniHuman能生成更逼真的视频。</li>
<li>输入具有更大的灵活性，支持多种驱动模式。</li>
<li>视频样本可在ttfamily项目页面查看。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01061">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-50e386dbfb3ee450b6ed0e686f85b8d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06890bffcbb8b3bac77873f402988e98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23824f9b7c6da06b77763f1e6eebbf78.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait"><a href="#FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait" class="headerlink" title="FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait"></a>FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait</h2><p><strong>Authors:Taekyung Ki, Dongchan Min, Gyeongsu Chae</strong></p>
<p>With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. Instead of a pixel-based latent space, we take advantage of a learned orthogonal motion latent space, enabling efficient generation and editing of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with an effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency. </p>
<blockquote>
<p>随着基于扩散的生成模型的快速发展，肖像图像动画已经取得了显著成果。然而，由于其迭代采样的性质，它在时间一致的视频生成和快速采样方面仍然面临挑战。本文提出了FLOAT，这是一种基于流匹配生成模型的音频驱动式肖像视频生成方法。我们利用学习得到的正交运动潜在空间，而不是基于像素的潜在空间，实现了时间一致运动的有效生成和编辑。为了实现这一点，我们引入了一个基于变压器的矢量场预测器，并配备了一种有效的帧条件机制。此外，我们的方法支持语音驱动的情感增强，能够实现表达性运动的自然融合。大量实验表明，我们的方法在视觉质量、运动保真度和效率方面优于最先进的音频驱动式肖像动画方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01064v3">PDF</a> ICCV 2025. Project page:   <a target="_blank" rel="noopener" href="https://deepbrainai-research.github.io/float/">https://deepbrainai-research.github.io/float/</a></p>
<p><strong>Summary</strong></p>
<p>随着扩散生成模型的快速发展，肖像动画已经取得了显著成果。然而，由于其迭代采样的特性，它在生成时序一致的视频和快速采样方面仍面临挑战。本文提出了基于流匹配生成模型的音频驱动肖像视频生成方法FLOAT。我们利用学习到的正交运动潜在空间，实现了高效且时序一致的运动生成和编辑。为此，我们引入了一个基于变压器的向量场预测器，并设计了一个有效的帧条件机制。此外，我们的方法还支持语音驱动的情感增强，能够自然融入表达性动作。实验证明，我们的方法在视觉质量、运动保真度和效率方面优于现有的音频驱动肖像方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散生成模型的快速发展推动了肖像动画的显著成果。</li>
<li>肖像动画在生成时序一致的视频方面仍面临挑战，尤其是快速采样问题。</li>
<li>FLOAT方法基于流匹配生成模型，解决了上述问题。</li>
<li>FLOAT利用正交运动潜在空间实现高效且时序一致的运动生成和编辑。</li>
<li>通过引入基于变压器的向量场预测器和有效的帧条件机制，FLOAT表现出优越性能。</li>
<li>FLOAT支持语音驱动的情感增强，能自然融入表达性动作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01064">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9a9a34885d0e0f3c152fbf196daea7c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f1054ceade950b1b0b3d281f1c8c346.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd0c0972287e0f2917338ea90bec8ffe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09c5c450da01da75aa22f8e69a0cc3ec.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-61ad4e9b3bd8a3c0c0d14c4da4c96e16.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-07-03  How to Move Your Dragon Text-to-Motion Synthesis for Large-Vocabulary   Objects
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cc4dd1e2d9a02e23b97b7368bd9d0bb2.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-07-03  DICE-BENCH Evaluating the Tool-Use Capabilities of Large Language   Models in Multi-Round, Multi-Party Dialogues
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28315.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
