<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  URGENT-PK Perceptually-Aligned Ranking Model Designed for Speech   Enhancement Competition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5d9fb63a9c5d31bce08d6af27fecf27c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-03-æ›´æ–°"><a href="#2025-07-03-æ›´æ–°" class="headerlink" title="2025-07-03 æ›´æ–°"></a>2025-07-03 æ›´æ–°</h1><h2 id="URGENT-PK-Perceptually-Aligned-Ranking-Model-Designed-for-Speech-Enhancement-Competition"><a href="#URGENT-PK-Perceptually-Aligned-Ranking-Model-Designed-for-Speech-Enhancement-Competition" class="headerlink" title="URGENT-PK: Perceptually-Aligned Ranking Model Designed for Speech   Enhancement Competition"></a>URGENT-PK: Perceptually-Aligned Ranking Model Designed for Speech   Enhancement Competition</h2><p><strong>Authors:Jiahe Wang, Chenda Li, Wei Wang, Wangyou Zhang, Samuele Cornell, Marvin Sach, Robin Scheibler, Kohei Saijo, Yihui Fu, Zhaoheng Ni, Anurag Kumar, Tim Fingscheidt, Shinji Watanabe, Yanmin Qian</strong></p>
<p>The Mean Opinion Score (MOS) is fundamental to speech quality assessment. However, its acquisition requires significant human annotation. Although deep neural network approaches, such as DNSMOS and UTMOS, have been developed to predict MOS to avoid this issue, they often suffer from insufficient training data. Recognizing that the comparison of speech enhancement (SE) systems prioritizes a reliable system comparison over absolute scores, we propose URGENT-PK, a novel ranking approach leveraging pairwise comparisons. URGENT-PK takes homologous enhanced speech pairs as input to predict relative quality rankings. This pairwise paradigm efficiently utilizes limited training data, as all pairwise permutations of multiple systems constitute a training instance. Experiments across multiple open test sets demonstrate URGENT-PKâ€™s superior system-level ranking performance over state-of-the-art baselines, despite its simple network architecture and limited training data. </p>
<blockquote>
<p>å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰åœ¨è¯­éŸ³è´¨é‡è¯„ä¼°ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œå®ƒçš„è·å–éœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨ã€‚å°½ç®¡å·²ç»å¼€å‘å‡ºæ·±åº¦ç¥ç»ç½‘ç»œæ–¹æ³•ï¼ˆå¦‚DNSMOSå’ŒUTMOSï¼‰æ¥é¢„æµ‹MOSä»¥é¿å…è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸å› ä¸ºè®­ç»ƒæ•°æ®ä¸è¶³è€Œå—åˆ°å½±å“ã€‚è€ƒè™‘åˆ°è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰ç³»ç»Ÿçš„æ¯”è¾ƒæ›´åŠ é‡è§†ç³»ç»Ÿä¹‹é—´çš„å¯é å¯¹æ¯”è€Œéç»å¯¹åˆ†æ•°ï¼Œæˆ‘ä»¬æå‡ºäº†URGENT-PKï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨é…å¯¹æ¯”è¾ƒçš„æ–°å‹æ’åæ–¹æ³•ã€‚URGENT-PKä»¥åŒæºå¢å¼ºè¯­éŸ³å¯¹ä½œä¸ºè¾“å…¥æ¥é¢„æµ‹ç›¸å¯¹è´¨é‡æ’åã€‚è¿™ç§é…å¯¹æ¨¡å¼æœ‰æ•ˆåœ°åˆ©ç”¨äº†æœ‰é™çš„è®­ç»ƒæ•°æ®ï¼Œå› ä¸ºå¤šä¸ªç³»ç»Ÿçš„æ‰€æœ‰é…å¯¹ç»„åˆæ„æˆä¸€ä¸ªè®­ç»ƒå®ä¾‹ã€‚åœ¨å¤šä¸ªå…¬å¼€æµ‹è¯•é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡URGEANT-PKçš„ç½‘ç»œç»“æ„ç®€å•ï¼Œè®­ç»ƒæ•°æ®æœ‰é™ï¼Œä½†å…¶ç³»ç»Ÿçº§æ’åæ€§èƒ½ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23874v1">PDF</a> Submitted to ASRU2025</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹è¯­éŸ³è´¨é‡è¯„ä¼°ä¸­çš„å‡å€¼æ„è§å¾—åˆ†ï¼ˆMOSï¼‰è·å–éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨çš„é—®é¢˜ï¼Œæå‡ºäº†URGE-PKæ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨é…å¯¹æ¯”è¾ƒçš„æ–¹å¼ï¼Œä»¥åŒæºå¢å¼ºè¯­éŸ³å¯¹ä¸ºè¾“å…¥é¢„æµ‹ç›¸å¯¹è´¨é‡æ’åï¼Œæœ‰æ•ˆåˆ©ç”¨äº†æœ‰é™çš„è®­ç»ƒæ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡ç½‘ç»œæ¶æ„ç®€å•ä¸”è®­ç»ƒæ•°æ®é‡æœ‰é™ï¼Œä½†URGE-PKåœ¨ç³»ç»Ÿçº§åˆ«æ’åæ€§èƒ½ä¸Šä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å‡å€¼æ„è§å¾—åˆ†ï¼ˆMOSï¼‰åœ¨è¯­éŸ³è´¨é‡è¯„ä¼°ä¸­è‡³å…³é‡è¦ï¼Œä½†å…¶è·å–éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨ã€‚</li>
<li>æ·±åº¦ç¥ç»ç½‘ç»œæ–¹æ³•å¦‚DNSMOSå’ŒUTMOSè¢«ç”¨æ¥é¢„æµ‹MOSï¼Œä½†ä»å­˜åœ¨è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†URGE-PKæ–¹æ³•ï¼Œé‡‡ç”¨é…å¯¹æ¯”è¾ƒçš„æ–¹å¼é¢„æµ‹ç›¸å¯¹è´¨é‡æ’åï¼Œä»¥ç¼“è§£è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>URGENT-PKæ–¹æ³•ä»¥åŒæºå¢å¼ºè¯­éŸ³å¯¹ä¸ºè¾“å…¥ï¼Œå……åˆ†åˆ©ç”¨äº†æœ‰é™çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>URGENT-PKæ–¹æ³•åœ¨ç³»ç»Ÿçº§åˆ«æ’åæ€§èƒ½ä¸Šä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</li>
<li>URGENT-PKæ–¹æ³•çš„ç½‘ç»œæ¶æ„ç›¸å¯¹ç®€å•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23874">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5417f5424c76b6d969e96e3d019742bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d9fb63a9c5d31bce08d6af27fecf27c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7696503e1e50bee9a507c8b8f41e5ae4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3aa61172dbcf03210b962d1492b6389.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33df25cc4fd73ac74cd7a24d0a9786f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-385935164e6b863af6bbb2080df16d6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f019181cb9eb88f7c567a937b94ab992.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80538bede3f4cec6e78efbefdaa249e5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Synthetically-Expressive-Evaluating-gesture-and-voice-for-emotion-and-empathy-in-VR-and-2D-scenarios"><a href="#Synthetically-Expressive-Evaluating-gesture-and-voice-for-emotion-and-empathy-in-VR-and-2D-scenarios" class="headerlink" title="Synthetically Expressive: Evaluating gesture and voice for emotion and   empathy in VR and 2D scenarios"></a>Synthetically Expressive: Evaluating gesture and voice for emotion and   empathy in VR and 2D scenarios</h2><p><strong>Authors:Haoyang Du, Kiran Chhatre, Christopher Peters, Brian Keegan, Rachel McDonnell, Cathy Ennis</strong></p>
<p>The creation of virtual humans increasingly leverages automated synthesis of speech and gestures, enabling expressive, adaptable agents that effectively engage users. However, the independent development of voice and gesture generation technologies, alongside the growing popularity of virtual reality (VR), presents significant questions about the integration of these signals and their ability to convey emotional detail in immersive environments. In this paper, we evaluate the influence of real and synthetic gestures and speech, alongside varying levels of immersion (VR vs. 2D displays) and emotional contexts (positive, neutral, negative) on user perceptions. We investigate how immersion affects the perceived match between gestures and speech and the impact on key aspects of user experience, including emotional and empathetic responses and the sense of co-presence. Our findings indicate that while VR enhances the perception of natural gesture-voice pairings, it does not similarly improve synthetic ones - amplifying the perceptual gap between them. These results highlight the need to reassess gesture appropriateness and refine AI-driven synthesis for immersive environments. Supplementary video: <a target="_blank" rel="noopener" href="https://youtu.be/WMfjIB1X-dc">https://youtu.be/WMfjIB1X-dc</a> </p>
<blockquote>
<p>éšç€è™šæ‹Ÿäººç±»çš„åˆ›å»ºè¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨è‡ªåŠ¨åˆæˆçš„è¯­éŸ³å’ŒåŠ¨ä½œï¼Œèƒ½å¤Ÿè¡¨è¾¾æƒ…æ„Ÿã€é€‚åº”ç¯å¢ƒçš„ä»£ç†ç”¨æˆ·å¾—ä»¥æœ‰æ•ˆå‚ä¸ã€‚ç„¶è€Œï¼Œè¯­éŸ³å’ŒåŠ¨ä½œç”ŸæˆæŠ€æœ¯çš„ç‹¬ç«‹å‘å±•ï¼Œä»¥åŠè™šæ‹Ÿç°å®ï¼ˆVRï¼‰çš„æ—¥ç›Šæ™®åŠï¼Œæå‡ºäº†å…³äºè¿™äº›ä¿¡å·çš„é›†æˆåŠå…¶åœ¨æ²‰æµ¸å¼ç¯å¢ƒä¸­ä¼ è¾¾æƒ…æ„Ÿç»†èŠ‚çš„èƒ½åŠ›çš„é‡å¤§é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†çœŸå®å’Œåˆæˆçš„åŠ¨ä½œä¸è¯­éŸ³çš„å½±å“ï¼Œä»¥åŠä¸åŒå±‚æ¬¡çš„æ²‰æµ¸æ„Ÿï¼ˆè™šæ‹Ÿç°å®ä¸äºŒç»´æ˜¾ç¤ºå™¨ï¼‰å’Œæƒ…æ„ŸèƒŒæ™¯ï¼ˆç§¯æã€ä¸­ç«‹ã€æ¶ˆæï¼‰å¯¹ç”¨æˆ·æ„ŸçŸ¥çš„å½±å“ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ²‰æµ¸æ„Ÿå¦‚ä½•å½±å“åŠ¨ä½œä¸è¯­éŸ³ä¹‹é—´çš„æ„ŸçŸ¥åŒ¹é…ä»¥åŠç”¨æˆ·ä½“éªŒçš„å…³é”®æ–¹é¢ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿå’Œå…±æƒ…ååº”ä»¥åŠå…±åŒå­˜åœ¨çš„æ„è¯†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶è™šæ‹Ÿç°å®å¢å¼ºäº†è‡ªç„¶åŠ¨ä½œ-è¯­éŸ³é…å¯¹çš„æ„ŸçŸ¥ï¼Œä½†å®ƒå¹¶æ²¡æœ‰å¯¹åˆæˆåŠ¨ä½œå’Œè¯­éŸ³çš„æ„ŸçŸ¥äº§ç”ŸåŒæ ·çš„å½±å“ï¼Œè¿™åŠ å¤§äº†ä¸¤è€…ä¹‹é—´çš„æ„ŸçŸ¥å·®è·ã€‚è¿™äº›ç»“æœå¼ºè°ƒéœ€è¦é‡ä¼°åŠ¨ä½œçš„é€‚å®œæ€§å¹¶å®Œå–„æ²‰æµ¸å¼ç¯å¢ƒä¸­çš„AIé©±åŠ¨åˆæˆã€‚è¡¥å……è§†é¢‘ï¼š<a target="_blank" rel="noopener" href="https://youtu.be/WMfjIB1X-dc%E3%80%82">https://youtu.be/WMfjIB1X-dcã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23777v2">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>     éšç€è™šæ‹Ÿäººç±»çš„åˆ›å»ºè¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨è‡ªåŠ¨åˆæˆçš„è¯­éŸ³å’ŒåŠ¨ä½œï¼Œè¡¨è¾¾åŠ›å¼ºã€å¯é€‚åº”çš„ä»£ç†èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¸ç”¨æˆ·è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œè¯­éŸ³å’ŒåŠ¨ä½œç”ŸæˆæŠ€æœ¯çš„ç‹¬ç«‹å‘å±•ä»¥åŠè™šæ‹Ÿç°å®ï¼ˆVRï¼‰çš„æ—¥ç›Šæ™®åŠï¼Œæå‡ºäº†å…³äºè¿™äº›ä¿¡å·çš„é›†æˆåŠå…¶åœ¨æ²‰æµ¸å¼ç¯å¢ƒä¸­ä¼ è¾¾æƒ…æ„Ÿç»†èŠ‚çš„èƒ½åŠ›çš„é‡å¤§é—®é¢˜ã€‚æœ¬æ–‡è¯„ä¼°äº†çœŸå®å’ŒåˆæˆåŠ¨ä½œä¸è¯­éŸ³çš„å½±å“ï¼Œä»¥åŠä¸åŒæ°´å¹³çš„æ²‰æµ¸å¼ï¼ˆè™šæ‹Ÿç°å®ä¸äºŒç»´æ˜¾ç¤ºå™¨ï¼‰å’Œæƒ…æ„ŸèƒŒæ™¯ï¼ˆç§¯æã€ä¸­ç«‹ã€æ¶ˆæï¼‰å¯¹ç”¨æˆ·æ„ŸçŸ¥çš„å½±å“ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ²‰æµ¸æ„Ÿå¦‚ä½•å½±å“åŠ¨ä½œä¸è¯­éŸ³ä¹‹é—´æ„ŸçŸ¥çš„åŒ¹é…ç¨‹åº¦ä»¥åŠå¯¹ç”¨æˆ·ä½“éªŒçš„å…³é”®æ–¹é¢çš„å½±å“ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿå’ŒåŒç†å“åº”ä»¥åŠå…±åŒåœ¨åœºæ„Ÿã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶è™šæ‹Ÿç°å®å¢å¼ºäº†è‡ªç„¶åŠ¨ä½œ-è¯­éŸ³é…å¯¹çš„æ„ŸçŸ¥ï¼Œä½†å®ƒå¹¶æ²¡æœ‰åŒæ ·åœ°æ”¹å–„åˆæˆçš„æ„ŸçŸ¥â€”â€”æ”¾å¤§äº†å®ƒä»¬ä¹‹é—´çš„æ„ŸçŸ¥å·®è·ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†éœ€è¦é‡æ–°è¯„ä¼°åŠ¨ä½œé€‚å½“æ€§ä»¥åŠæ”¹è¿›æ²‰æµ¸å¼ç¯å¢ƒä¸­AIé©±åŠ¨åˆæˆçš„å¿…è¦æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è™šæ‹Ÿäººç±»çš„åˆ›å»ºè¶Šæ¥è¶Šå¤šåœ°ä¾èµ–è‡ªåŠ¨åˆæˆçš„è¯­éŸ³å’ŒåŠ¨ä½œï¼Œä»¥åˆ›é€ è¡¨è¾¾æ€§å¼ºã€å¯é€‚åº”çš„ä»£ç†ã€‚</li>
<li>è™šæ‹Ÿç°å®ï¼ˆVRï¼‰å¢å¼ºäº†è‡ªç„¶åŠ¨ä½œå’Œè¯­éŸ³é…å¯¹çš„æ„ŸçŸ¥ï¼Œä½†å¯¹åˆæˆçš„é…å¯¹æ”¹å–„ä¸æ˜æ˜¾ã€‚</li>
<li>æ²‰æµ¸æ„Ÿå’Œæƒ…æ„ŸèƒŒæ™¯å¯¹ç”¨æˆ·ä½“éªŒæœ‰é‡è¦å½±å“ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿå’ŒåŒç†å¿ƒååº”ä»¥åŠå…±åŒåœ¨åœºæ„Ÿã€‚</li>
<li>ç”¨æˆ·åœ¨æ²‰æµ¸å¼ç¯å¢ƒä¸­å¯¹è™šæ‹Ÿäººç±»çš„æœŸæœ›æ›´é«˜ï¼Œéœ€è¦æ›´çœŸå®çš„åŠ¨ä½œå’Œè¯­éŸ³åˆæˆæ¥æ»¡è¶³è¿™äº›æœŸæœ›ã€‚</li>
<li>è™šæ‹Ÿç°å®æ”¾å¤§äº†åˆæˆè¯­éŸ³å’ŒåŠ¨ä½œä¹‹é—´çš„æ„ŸçŸ¥å·®è·ã€‚</li>
<li>éœ€è¦é‡æ–°è¯„ä¼°åŠ¨ä½œçš„é€‚å½“æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²‰æµ¸å¼ç¯å¢ƒä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5e5a0fac3ff391fe70f9b6d2380934e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb5aa242f3eb463f8004a497b86115ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a8c863d8f4235d75c744d8885674338.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Leveraging-a-Multi-Agent-LLM-Based-System-to-Educate-Teachers-in-Hate-Incidents-Management"><a href="#Leveraging-a-Multi-Agent-LLM-Based-System-to-Educate-Teachers-in-Hate-Incidents-Management" class="headerlink" title="Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate   Incidents Management"></a>Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate   Incidents Management</h2><p><strong>Authors:Ewelina Gajewska, Michal Wawer, Katarzyna Budzynska, JarosÅ‚aw A. Chudziak</strong></p>
<p>Computer-aided teacher training is a state-of-the-art method designed to enhance teachersâ€™ professional skills effectively while minimising concerns related to costs, time constraints, and geographical limitations. We investigate the potential of large language models (LLMs) in teacher education, using a case of teaching hate incidents management in schools. To this end, we create a multi-agent LLM-based system that mimics realistic situations of hate, using a combination of retrieval-augmented prompting and persona modelling. It is designed to identify and analyse hate speech patterns, predict potential escalation, and propose effective intervention strategies. By integrating persona modelling with agentic LLMs, we create contextually diverse simulations of hate incidents, mimicking real-life situations. The system allows teachers to analyse and understand the dynamics of hate incidents in a safe and controlled environment, providing valuable insights and practical knowledge to manage such situations confidently in real life. Our pilot evaluation demonstrates teachersâ€™ enhanced understanding of the nature of annotator disagreements and the role of context in hate speech interpretation, leading to the development of more informed and effective strategies for addressing hate in classroom settings. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©æ•™å¸ˆåŸ¹è®­æ˜¯ä¸€ç§å…ˆè¿›çš„æ–¹æ³•ï¼Œæ—¨åœ¨æœ‰æ•ˆå¢å¼ºæ•™å¸ˆçš„ä¸“ä¸šæŠ€èƒ½ï¼ŒåŒæ—¶æœ€å°åŒ–ä¸æˆæœ¬ã€æ—¶é—´é™åˆ¶å’Œåœ°ç†é™åˆ¶ç›¸å…³çš„æ‹…å¿§ã€‚æˆ‘ä»¬è°ƒæŸ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•™å¸ˆæ•™è‚²ä¸­çš„æ½œåŠ›ï¼Œä»¥å­¦æ ¡ä»‡æ¨äº‹ä»¶ç®¡ç†æ•™å­¦ä¸ºä¾‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŸºäºå¤šä»£ç†çš„LLMç³»ç»Ÿï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å¢å¼ºæç¤ºå’Œäººæ ¼å»ºæ¨¡ï¼Œæ¨¡æ‹Ÿä»‡æ¨çš„ç°å®æƒ…å¢ƒã€‚å®ƒè¢«è®¾è®¡ç”¨æ¥è¯†åˆ«å’Œåˆ†æä»‡æ¨è¨€è®ºçš„æ¨¡å¼ï¼Œé¢„æµ‹å¯èƒ½çš„å‡çº§ï¼Œå¹¶æå‡ºæœ‰æ•ˆçš„å¹²é¢„ç­–ç•¥ã€‚é€šè¿‡äººæ ¼å»ºæ¨¡ä¸æ™ºèƒ½LLMçš„æ•´åˆï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä»‡æ¨äº‹ä»¶çš„è¯­å¢ƒå¤šæ ·åŒ–æ¨¡æ‹Ÿï¼Œæ¨¡æ‹Ÿç°å®ç”Ÿæ´»ä¸­çš„æƒ…å¢ƒã€‚è¯¥ç³»ç»Ÿå…è®¸æ•™å¸ˆåœ¨å®‰å…¨å¯æ§çš„ç¯å¢ƒä¸­åˆ†æä»‡æ¨äº‹ä»¶çš„åŠ¨åŠ›å­¦ï¼Œä»è€Œè·å¾—æœ‰ä»·å€¼çš„ä¿¡æ¯å’Œå®ç”¨çŸ¥è¯†ï¼Œä»¥åœ¨ç°å®ä¸­è‡ªä¿¡åœ°ç®¡ç†æ­¤ç±»æƒ…å†µã€‚æˆ‘ä»¬çš„åˆæ­¥è¯„ä¼°æ˜¾ç¤ºï¼Œæ•™å¸ˆä»¬å¯¹æ³¨é‡Šè€…åˆ†æ­§çš„æ€§è´¨ä»¥åŠè¯­å¢ƒåœ¨ä»‡æ¨è¨€è®ºè§£é‡Šä¸­çš„ä½œç”¨æœ‰äº†æ›´æ·±çš„ç†è§£ï¼Œä»è€Œå¼€å‘å‡ºæ›´æ˜æ™ºã€æ›´æœ‰æ•ˆçš„è§£å†³è¯¾å ‚ä»‡æ¨é—®é¢˜çš„ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23774v1">PDF</a> 8 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>è®¡ç®—æœºè¾…åŠ©æ•™å­¦åŸ¹è®­æ˜¯ä¸€ç§å…ˆè¿›çš„æ–¹æ³•ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°æé«˜æ•™å¸ˆçš„ä¸“ä¸šæŠ€èƒ½ï¼ŒåŒæ—¶æœ€å°åŒ–æˆæœ¬ã€æ—¶é—´é™åˆ¶å’Œåœ°ç†é™åˆ¶ç›¸å…³çš„æ‹…å¿§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™å¸ˆæ•™è‚²çš„æ½œåŠ›ï¼Œä»¥æ ¡å›­ä»‡æ¨äº‹ä»¶ç®¡ç†ä¸ºä¾‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“çš„å¤§å‹è¯­è¨€æ¨¡å‹ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿæ¨¡æ‹Ÿä»‡æ¨çš„çœŸå®åœºæ™¯ï¼Œç»“åˆæ£€ç´¢å¢å¼ºæç¤ºå’Œä¸ªæ€§åŒ–å»ºæ¨¡ã€‚å®ƒæ—¨åœ¨è¯†åˆ«å’Œåˆ†æä»‡æ¨è¨€è®ºæ¨¡å¼ï¼Œé¢„æµ‹å¯èƒ½çš„å‡çº§ï¼Œå¹¶æå‡ºæœ‰æ•ˆçš„å¹²é¢„ç­–ç•¥ã€‚é€šè¿‡æ•´åˆä¸ªæ€§åŒ–å»ºæ¨¡ä¸æ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä»‡æ¨äº‹ä»¶çš„ä¸Šä¸‹æ–‡ä¸°å¯Œæ¨¡æ‹Ÿï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯ã€‚è¯¥ç³»ç»Ÿå…è®¸æ•™å¸ˆåœ¨å®‰å…¨å’Œå—æ§çš„ç¯å¢ƒä¸­åˆ†æä»‡æ¨äº‹ä»¶çš„åŠ¨æ€ï¼Œæä¾›å®è´µçš„è§è§£å’Œå®ç”¨çŸ¥è¯†ï¼Œå¸®åŠ©æ•™å¸ˆæ›´è‡ªä¿¡åœ°å¤„ç†çœŸå®åœºæ™¯ä¸­çš„æ­¤ç±»æƒ…å†µã€‚åˆæ­¥è¯„ä¼°æ˜¾ç¤ºï¼Œæ•™å¸ˆå¢å¼ºäº†å¯¹æ ‡æ³¨å™¨åˆ†æ­§çš„ç†è§£å’Œä»‡æ¨è¨€è®ºè§£è¯»ä¸­çš„ä¸Šä¸‹æ–‡è§’è‰²ï¼Œä»è€Œå‘å±•å‡ºæ›´æ˜æ™ºã€æ›´æœ‰æ•ˆçš„åº”å¯¹è¯¾å ‚ä»‡æ¨çš„ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©æ•™å­¦åŸ¹è®­æ—¨åœ¨æé«˜æ•™å¸ˆçš„ä¸“ä¸šæŠ€èƒ½ï¼ŒåŒæ—¶è€ƒè™‘æˆæœ¬ã€æ—¶é—´å’Œåœ°ç†é™åˆ¶ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™å¸ˆæ•™è‚²ä¸­å…·æœ‰æ½œåŠ›ï¼Œå¯ç”¨äºæ¨¡æ‹ŸçœŸå®çš„ä»‡æ¨äº‹ä»¶åœºæ™¯ã€‚</li>
<li>åŸºäºå¤šæ™ºèƒ½ä½“çš„å¤§å‹è¯­è¨€æ¨¡å‹ç³»ç»Ÿå¯è¯†åˆ«å’Œåˆ†æä»‡æ¨è¨€è®ºæ¨¡å¼ã€‚</li>
<li>è¯¥ç³»ç»Ÿå¯ä»¥é¢„æµ‹ä»‡æ¨äº‹ä»¶çš„æ½œåœ¨å‡çº§ï¼Œå¹¶æå‡ºæœ‰æ•ˆçš„å¹²é¢„ç­–ç•¥ã€‚</li>
<li>é€šè¿‡ç»“åˆä¸ªæ€§åŒ–å»ºæ¨¡å’Œæ™ºèƒ½ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåˆ›å»ºäº†ä»‡æ¨äº‹ä»¶çš„ä¸Šä¸‹æ–‡ä¸°å¯Œæ¨¡æ‹Ÿã€‚</li>
<li>æ•™å¸ˆé€šè¿‡è¯¥ç³»ç»Ÿè·å¾—å¯¹ä»‡æ¨äº‹ä»¶åŠ¨æ€çš„åˆ†æå’Œå®è´µè§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4b47b8a4d9dc79f4e6fd0a7b83b133e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d9debc3b407564fa7f7671b0bf28bb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ab5a6f8aeb71b8d0885dcaa3d236ebd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="JAM-Flow-Joint-Audio-Motion-Synthesis-with-Flow-Matching"><a href="#JAM-Flow-Joint-Audio-Motion-Synthesis-with-Flow-Matching" class="headerlink" title="JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching"></a>JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching</h2><p><strong>Authors:Mingi Kwon, Joonghyuk Shin, Jaeseok Jung, Jaesik Park, Youngjung Uh</strong></p>
<p>The intrinsic link between facial motion and speech is often overlooked in generative modeling, where talking head synthesis and text-to-speech (TTS) are typically addressed as separate tasks. This paper introduces JAM-Flow, a unified framework to simultaneously synthesize and condition on both facial motion and speech. Our approach leverages flow matching and a novel Multi-Modal Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT and Audio-DiT modules. These are coupled via selective joint attention layers and incorporate key architectural choices, such as temporally aligned positional embeddings and localized joint attention masking, to enable effective cross-modal interaction while preserving modality-specific strengths. Trained with an inpainting-style objective, JAM-Flow supports a wide array of conditioning inputs-including text, reference audio, and reference motion-facilitating tasks such as synchronized talking head generation from text, audio-driven animation, and much more, within a single, coherent model. JAM-Flow significantly advances multi-modal generative modeling by providing a practical solution for holistic audio-visual synthesis. project page: <a target="_blank" rel="noopener" href="https://joonghyuk.com/jamflow-web">https://joonghyuk.com/jamflow-web</a> </p>
<blockquote>
<p>é¢éƒ¨åŠ¨ä½œä¸è¯­éŸ³ä¹‹é—´çš„å†…åœ¨è”ç³»åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ç»å¸¸è¢«å¿½è§†ï¼Œç”Ÿæˆæ¨¡å‹ä¸­é€šå¸¸å°†å¤´éƒ¨åŠ¨ä½œåˆæˆå’Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰è§†ä¸ºå•ç‹¬çš„ä»»åŠ¡ã€‚æœ¬æ–‡ä»‹ç»äº†JAM-Flowï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶åˆæˆå’ŒåŸºäºé¢éƒ¨åŠ¨ä½œå’Œè¯­éŸ³çš„æ¡ä»¶è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨æµåŒ¹é…å’Œæ–°å‹çš„å¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆMM-DiTï¼‰æ¶æ„ï¼Œé›†æˆäº†ä¸“ä¸šåŒ–çš„è¿åŠ¨-DiTå’ŒéŸ³é¢‘-DiTæ¨¡å—ã€‚è¿™äº›æ¨¡å—é€šè¿‡é€‰æ‹©æ€§è”åˆæ³¨æ„åŠ›å±‚è¿›è¡Œè€¦åˆï¼Œå¹¶é‡‡ç”¨äº†å…³é”®æ¶æ„é€‰æ‹©ï¼Œå¦‚æ—¶é—´å¯¹é½çš„ä½ç½®åµŒå…¥å’Œå±€éƒ¨è”åˆæ³¨æ„åŠ›æ©ç ï¼Œä»¥å®ç°æœ‰æ•ˆçš„è·¨æ¨¡æ€äº¤äº’åŒæ—¶ä¿æŒæ¨¡æ€ç‰¹å®šçš„ä¼˜åŠ¿ã€‚ä½¿ç”¨å¡«å……é£æ ¼çš„ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼ŒJAM-Flowæ”¯æŒå¹¿æ³›çš„æ¡ä»¶è¾“å…¥ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å‚è€ƒéŸ³é¢‘å’Œå‚è€ƒåŠ¨ä½œï¼Œä¿ƒè¿›å¦‚ä»æ–‡æœ¬åŒæ­¥ç”Ÿæˆè¯´è¯å¤´éƒ¨ã€éŸ³é¢‘é©±åŠ¨åŠ¨ç”»ç­‰ä»»åŠ¡ï¼Œåœ¨ä¸€ä¸ªå•ä¸€ã€è¿è´¯çš„æ¨¡å‹ä¸­å®ç°æ›´å¤šåŠŸèƒ½ã€‚JAM-Flowé€šè¿‡ä¸ºæ•´ä½“éŸ³é¢‘è§†è§‰åˆæˆæä¾›å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æ¨åŠ¨äº†å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„å‘å±•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://joonghyuk.com/jamflow-web%E9%A1%B5%E9%A1%B5%E9%A2%8C%E8%AF%BB">https://joonghyuk.com/jamflow-web</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23552v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://joonghyuk.com/jamflow-web">https://joonghyuk.com/jamflow-web</a> Under review.   Preprint published on arXiv</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†JAM-Flowæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨åŒæ—¶åˆæˆå¹¶å¤„ç†é¢éƒ¨åŠ¨ä½œå’Œè¯­éŸ³ï¼Œå¼¥è¡¥ç”Ÿæˆæ¨¡å‹ä¸­å¸¸è¢«å¿½è§†çš„é¢éƒ¨åŠ¨ä½œä¸è¯­éŸ³çš„å†…åœ¨è”ç³»ã€‚é€šè¿‡é‡‡ç”¨æµåŒ¹é…æŠ€æœ¯å’Œæ–°å‹å¤šæ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆMM-DiTï¼‰æ¶æ„ï¼ŒJAM-Flowèåˆäº†Motion-DiTå’ŒAudio-DiTæ¨¡å—ã€‚å€ŸåŠ©é€‰æ‹©æ€§è”åˆæ³¨æ„å±‚ï¼Œå®ç°åœ¨ä¿ç•™å„æ¨¡æ€ç‰¹æ€§çš„åŒæ—¶ï¼Œè¿›è¡Œæœ‰æ•ˆè·¨æ¨¡æ€äº¤äº’ã€‚é€šè¿‡é‡‡ç”¨inpaintingé£æ ¼çš„ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼ŒJAM-Flowæ”¯æŒå¤šç§æ¡ä»¶è¾“å…¥ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å‚è€ƒéŸ³é¢‘å’Œå‚è€ƒåŠ¨ä½œï¼Œå¯å®Œæˆä»æ–‡æœ¬ç”ŸæˆåŒæ­¥è®²è¯è§†é¢‘ã€éŸ³é¢‘é©±åŠ¨åŠ¨ç”»ç­‰ä»»åŠ¡ã€‚JAM-Flowä¸ºéŸ³é¢‘è§†è§‰åˆæˆæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—æ¨åŠ¨äº†å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼ºè°ƒé¢éƒ¨åŠ¨ä½œä¸è¯­éŸ³åœ¨ç”Ÿæˆæ¨¡å‹ä¸­çš„å†…åœ¨è”ç³»ï¼Œå¹¶æå‡ºJAM-Flowæ¡†æ¶åŒæ—¶åˆæˆå’Œå¤„ç†ä¸¤è€…ã€‚</li>
<li>å¼•å…¥æµåŒ¹é…æŠ€æœ¯å’ŒMulti-Modal Diffusion Transformerï¼ˆMM-DiTï¼‰æ¶æ„ã€‚</li>
<li>MM-DiTæ¶æ„èåˆäº†Motion-DiTå’ŒAudio-DiTæ¨¡å—ã€‚</li>
<li>é€šè¿‡é€‰æ‹©æ€§è”åˆæ³¨æ„å±‚å®ç°è·¨æ¨¡æ€äº¤äº’ã€‚</li>
<li>æ¨¡å‹ä¿ç•™äº†æ¨¡æ€ç‰¹å¼‚æ€§ï¼ŒåŒæ—¶é€šè¿‡è®­ç»ƒæ”¯æŒå¤šç§æ¡ä»¶è¾“å…¥ï¼Œå¦‚æ–‡æœ¬ã€å‚è€ƒéŸ³é¢‘å’Œå‚è€ƒåŠ¨ä½œã€‚</li>
<li>æ”¯æŒä»æ–‡æœ¬ç”ŸæˆåŒæ­¥è®²è¯è§†é¢‘ã€éŸ³é¢‘é©±åŠ¨åŠ¨ç”»ç­‰ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f1c60399bb279929872039df31f3b028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf29df7073b460c0a4a11ad28902c1c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6b09d905bf5dbb272c78025f362fa9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-190a4060cc989a07869321dfbded7e81.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Why-Settle-for-Mid-A-Probabilistic-Viewpoint-to-Spatial-Relationship-Alignment-in-Text-to-image-Models"><a href="#Why-Settle-for-Mid-A-Probabilistic-Viewpoint-to-Spatial-Relationship-Alignment-in-Text-to-image-Models" class="headerlink" title="Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship   Alignment in Text-to-image Models"></a>Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship   Alignment in Text-to-image Models</h2><p><strong>Authors:Parham Rezaei, Arash Marioriyad, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban</strong></p>
<p>Despite the ability of text-to-image models to generate high-quality, realistic, and diverse images, they face challenges in compositional generation, often struggling to accurately represent details specified in the input prompt. A prevalent issue in compositional generation is the misalignment of spatial relationships, as models often fail to faithfully generate images that reflect the spatial configurations specified between objects in the input prompts. To address this challenge, we propose a novel probabilistic framework for modeling the relative spatial positioning of objects in a scene, leveraging the concept of Probability of Superiority (PoS). Building on this insight, we make two key contributions. First, we introduce a novel evaluation metric, PoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D spatial relationships between text and image, with improved adherence to human judgment. Second, we propose PoS-based Generation (PSG), an inference-time method that improves the alignment of 2D and 3D spatial relationships in T2I models without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based reward function that can be utilized in two distinct ways: (1) as a gradient-based guidance mechanism applied to the cross-attention maps during the denoising steps, or (2) as a search-based strategy that evaluates a set of initial noise vectors to select the best one. Extensive experiments demonstrate that the PSE metric exhibits stronger alignment with human judgment compared to traditional center-based metrics, providing a more nuanced and reliable measure of complex spatial relationship accuracy in text-image alignment. Furthermore, PSG significantly enhances the ability of text-to-image models to generate images with specified spatial configurations, outperforming state-of-the-art methods across multiple evaluation metrics and benchmarks. </p>
<blockquote>
<p>å°½ç®¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€ç°å®åŒ–å’Œå¤šæ ·åŒ–çš„å›¾åƒï¼Œä½†åœ¨ç»„åˆç”Ÿæˆæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¾€å¾€éš¾ä»¥å‡†ç¡®è¡¨ç¤ºè¾“å…¥æç¤ºä¸­æŒ‡å®šçš„ç»†èŠ‚ã€‚åœ¨ç»„åˆç”Ÿæˆä¸­çš„ä¸€ä¸ªæ™®éé—®é¢˜æ˜¯ç©ºé—´å…³ç³»çš„é”™ä½ï¼Œå› ä¸ºæ¨¡å‹å¾€å¾€æ— æ³•å¿ å®åœ°ç”Ÿæˆåæ˜ è¾“å…¥æç¤ºä¸­ç‰©ä½“ä¹‹é—´ç©ºé—´é…ç½®çš„å›¾åƒã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¦‚ç‡æ¡†æ¶ï¼Œåˆ©ç”¨ä¼˜åŠ¿æ¦‚ç‡ï¼ˆPoSï¼‰çš„æ¦‚å¿µæ¥å¯¹åœºæ™¯ä¸­ç‰©ä½“çš„ç›¸å¯¹ç©ºé—´ä½ç½®è¿›è¡Œå»ºæ¨¡ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬åšå‡ºäº†ä¸¤ä¸ªå…³é”®è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹è¯„ä¼°æŒ‡æ ‡â€”â€”åŸºäºPoSçš„è¯„ä¼°ï¼ˆPSEï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°æ–‡æœ¬å’Œå›¾åƒä¹‹é—´äºŒç»´å’Œä¸‰ç»´ç©ºé—´å…³ç³»çš„å¯¹é½æƒ…å†µï¼Œæ›´å¥½åœ°ç¬¦åˆäººç±»åˆ¤æ–­ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºPoSçš„ç”Ÿæˆï¼ˆPSGï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ¨ç†æ—¶é—´æ–¹æ³•ï¼Œå¯æé«˜æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­äºŒã€ä¸‰ç»´ç©ºé—´å…³ç³»çš„å¯¹é½ç¨‹åº¦ï¼Œæ— éœ€å¾®è°ƒã€‚PSGé‡‡ç”¨åŸºäºè¯åºçš„PoSå¥–åŠ±å‡½æ•°ï¼Œå¯ä»¥æœ‰ä¸¤ç§ä¸åŒçš„ç”¨é€”ï¼šï¼ˆ1ï¼‰ä½œä¸ºå»å™ªæ­¥éª¤ä¸­è·¨æ³¨æ„åŠ›å›¾çš„åŸºäºæ¢¯åº¦çš„å¼•å¯¼æœºåˆ¶ï¼›ï¼ˆ2ï¼‰ä½œä¸ºä¸€ç§åŸºäºæœç´¢çš„ç­–ç•¥ï¼Œè¯„ä¼°ä¸€ç»„åˆå§‹å™ªå£°å‘é‡ä»¥é€‰æ‹©æœ€ä½³å‘é‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸åŸºäºä¸­å¿ƒçš„ä¼ ç»ŸæŒ‡æ ‡ç›¸æ¯”ï¼ŒPSEæŒ‡æ ‡ä¸äººç±»åˆ¤æ–­çš„å¯¹é½ç¨‹åº¦æ›´é«˜ï¼Œä¸ºæ–‡æœ¬å›¾åƒå¯¹é½ä¸­å¤æ‚ç©ºé—´å…³ç³»çš„å‡†ç¡®æ€§æä¾›äº†æ›´ç»†è‡´å’Œå¯é çš„è¡¡é‡æ ‡å‡†ã€‚æ­¤å¤–ï¼ŒPSGæ˜¾è‘—å¢å¼ºäº†æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç”Ÿæˆå…·æœ‰æŒ‡å®šç©ºé—´é…ç½®å›¾åƒçš„èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23418v1">PDF</a> 12 main pages, 18 figures, and 16 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºæ–‡æœ¬è½¬å›¾åƒæ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸä¸”å¤šæ ·çš„å›¾åƒæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»„åˆç”Ÿæˆæ–¹é¢ã€‚æ¨¡å‹éš¾ä»¥å‡†ç¡®è¡¨ç¤ºè¾“å…¥æç¤ºä¸­æŒ‡å®šçš„ç»†èŠ‚ï¼Œå°¤å…¶åœ¨ç©ºé—´å…³ç³»çš„å¯¹é½æ–¹é¢å­˜åœ¨é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ¦‚ç‡çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä¼˜åŠ¿æ¦‚ç‡ï¼ˆPoSï¼‰æ¥å»ºæ¨¡åœºæ™¯ä¸­å¯¹è±¡çš„ç›¸å¯¹ç©ºé—´ä½ç½®ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šå¼•å…¥PoS-based Evaluationï¼ˆPSEï¼‰è¯„ä¼°æŒ‡æ ‡ï¼Œè¯„ä¼°æ–‡æœ¬å’Œå›¾åƒä¹‹é—´2Då’Œ3Dç©ºé—´å…³ç³»çš„ä¸€è‡´æ€§ï¼›æå‡ºPoS-based Generationï¼ˆPSGï¼‰ï¼Œæ˜¯ä¸€ç§æ— éœ€å¾®è°ƒå³å¯æé«˜æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹ç©ºé—´å…³ç³»å¯¹é½æ€§çš„æ¨ç†æ—¶é—´æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒPSEæŒ‡æ ‡ä¸äººç±»åˆ¤æ–­å¯¹é½æ€§æ›´å¼ºï¼ŒPSGæ˜¾è‘—æé«˜äº†æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹ç”ŸæˆæŒ‡å®šç©ºé—´é…ç½®å›¾åƒçš„èƒ½åŠ›ï¼Œä¼˜äºå¤šç§è¯„ä»·æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹åœ¨ç»„åˆç”Ÿæˆæ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥å‡†ç¡®è¡¨ç¤ºè¾“å…¥æç¤ºä¸­çš„ç»†èŠ‚ï¼Œå°¤å…¶åœ¨ç©ºé—´å…³ç³»çš„å¯¹é½æ–¹é¢ã€‚</li>
<li>å¼•å…¥ä¼˜åŠ¿æ¦‚ç‡ï¼ˆPoSï¼‰æ¦‚å¿µï¼Œæå‡ºä¸€ç§åŸºäºæ¦‚ç‡çš„æ¡†æ¶æ¥å»ºæ¨¡åœºæ™¯ä¸­å¯¹è±¡çš„ç›¸å¯¹ç©ºé—´ä½ç½®ã€‚</li>
<li>è´¡çŒ®åŒ…æ‹¬PSEè¯„ä¼°æŒ‡æ ‡å’ŒPSGæ¨ç†æ—¶é—´æ–¹æ³•ã€‚PSEè¯„ä¼°æ–‡æœ¬å’Œå›¾åƒä¹‹é—´ç©ºé—´å…³ç³»çš„ä¸€è‡´æ€§ï¼Œä¸äººç±»åˆ¤æ–­å¯¹é½æ€§æ›´å¼ºï¼›PSGæé«˜æ¨¡å‹ç”ŸæˆæŒ‡å®šç©ºé—´é…ç½®å›¾åƒçš„èƒ½åŠ›ï¼Œæ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>PSEç›¸è¾ƒäºä¼ ç»Ÿä¸­å¿ƒåŸºå‡†è¯„ä¼°æŒ‡æ ‡ï¼Œæä¾›æ›´ç²¾ç»†å’Œå¯é çš„å¤æ‚ç©ºé—´å…³ç³»å‡†ç¡®æ€§çš„è¡¡é‡ã€‚</li>
<li>PSGåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„æ¡†æ¶å’Œæ–¹æ³•ä¸ºæ”¹è¿›æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹çš„æ€§èƒ½æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bfb467cb7417474608f83b3120f5f328.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-08d5eab24fc40d3b368032ce8174461c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6b38a6f961715b3ddb42fb82b3c2239.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad1901d3447795b728a3c08fe24a8fc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d55c3d4c1489a74e04112e2eae753e7b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Entity-Preserved-Context-Aware-ASR-Structured-Transcriptions"><a href="#Mind-the-Gap-Entity-Preserved-Context-Aware-ASR-Structured-Transcriptions" class="headerlink" title="Mind the Gap: Entity-Preserved Context-Aware ASR Structured   Transcriptions"></a>Mind the Gap: Entity-Preserved Context-Aware ASR Structured   Transcriptions</h2><p><strong>Authors:Duygu Altinok</strong></p>
<p>Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high transcription accuracy but struggle with named entities and numerical data, especially when proper formatting is required. These issues increase word error rate (WER) and impair semantic understanding in critical domains like legal, financial, and medical applications. We propose a novel training approach that extends the semantic context of ASR models by adding overlapping context windows during training. By sliding 5-second overlaps on both sides of 30-second chunks, we create a 40-second â€œeffective semantic window,â€ improving entity recognition and formatting while focusing predictions on the central 30 seconds. To address entities spanning chunk boundaries, we reassign such entities entirely to the right-hand chunk, ensuring proper formatting. Additionally, enriched training data with embedded entity labels enables the model to learn both recognition and type-specific formatting. Evaluated on the Spoken Wikipedia dataset, our method improves performance across semantic tasks, including named entity recognition (NER) and entity formatting. These results highlight the effectiveness of context-aware training in addressing ASR limitations for long-form transcription and complex entity recognition tasks. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿï¼Œå¦‚Whisperï¼Œåœ¨è½¬å½•æ–¹é¢è¾¾åˆ°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä½†åœ¨å¤„ç†å‘½åå®ä½“å’Œæ•°å€¼æ•°æ®æ—¶é‡åˆ°äº†å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ­£ç¡®æ ¼å¼çš„æƒ…å†µä¸‹ã€‚è¿™äº›é—®é¢˜å¢åŠ äº†å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œå¹¶åœ¨æ³•å¾‹ã€é‡‘èå’ŒåŒ»ç–—ç­‰å…³é”®é¢†åŸŸæŸå®³äº†è¯­ä¹‰ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•æ¥æ‰©å±•ASRæ¨¡å‹çš„è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ·»åŠ é‡å çš„ä¸Šä¸‹æ–‡çª—å£æ¥å®ç°ã€‚é€šè¿‡åœ¨30ç§’ç‰‡æ®µçš„ä¸¤ä¾§æ»‘åŠ¨5ç§’é‡å çª—å£ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª40ç§’çš„â€œæœ‰æ•ˆè¯­ä¹‰çª—å£â€ï¼Œè¿™æ”¹å–„äº†å®ä½“è¯†åˆ«å’Œæ ¼å¼åŒ–ï¼ŒåŒæ—¶å°†é¢„æµ‹é‡ç‚¹æ”¾åœ¨ä¸­å¤®çš„30ç§’ä¸Šã€‚ä¸ºäº†è§£å†³è·¨è¶Šç‰‡æ®µè¾¹ç•Œçš„å®ä½“é—®é¢˜ï¼Œæˆ‘ä»¬å°†æ­¤ç±»å®ä½“å®Œå…¨é‡æ–°åˆ†é…ç»™å³æ‰‹ç‰‡æ®µï¼Œä»¥ç¡®ä¿æ­£ç¡®çš„æ ¼å¼ã€‚æ­¤å¤–ï¼Œä½¿ç”¨åµŒå…¥å®ä½“æ ‡ç­¾çš„ä¸°å¯Œè®­ç»ƒæ•°æ®ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ è¯†åˆ«å’Œç±»å‹ç‰¹å®šçš„æ ¼å¼åŒ–ã€‚åœ¨Spoken Wikipediaæ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒ…æ‹¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œå®ä½“æ ¼å¼åŒ–ç­‰è¯­ä¹‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¾—åˆ°äº†æé«˜ã€‚è¿™äº›ç»“æœçªå‡ºäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥è®­ç»ƒåœ¨è§£å†³é•¿å½¢å¼è½¬å½•å’Œå¤æ‚å®ä½“è¯†åˆ«ä»»åŠ¡çš„ASRå±€é™æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22858v1">PDF</a> This is the accepted version of an article accepted to the TSD 2025   conference, published in Springer Lecture Notes in Artificial Intelligence   (LNAI). The final authenticated version is available online at SpringerLink</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨å¤„ç†å‘½åå®ä½“å’Œæ•°å€¼æ•°æ®æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ­£ç¡®æ ¼å¼åŒ–çš„åœºæ™¯ä¸­ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•æ¥æ‰©å±•ASRæ¨¡å‹çš„è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚é€šè¿‡æ·»åŠ é‡å çš„ä¸Šä¸‹æ–‡çª—å£è¿›è¡Œè®­ç»ƒï¼Œåˆ›å»ºäº†ä¸€ä¸ªæœ‰æ•ˆçš„è¯­ä¹‰çª—å£ï¼Œæé«˜äº†å®ä½“è¯†åˆ«å’Œæ ¼å¼åŒ–çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œä½¿ç”¨å¸¦æœ‰åµŒå…¥å®ä½“æ ‡ç­¾çš„ä¸°å¯Œè®­ç»ƒæ•°æ®ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ è¯†åˆ«å’Œç±»å‹ç‰¹å®šçš„æ ¼å¼åŒ–ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯­ä¹‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰æ‰€æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ASRç³»ç»Ÿåœ¨å¤„ç†å‘½åå®ä½“å’Œæ•°å€¼æ•°æ®æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ­£ç¡®æ ¼å¼åŒ–çš„é¢†åŸŸã€‚</li>
<li>æ–°çš„è®­ç»ƒæ–¹æ³•é€šè¿‡æ·»åŠ é‡å çš„ä¸Šä¸‹æ–‡çª—å£æ¥æ‰©å±•ASRæ¨¡å‹çš„è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚</li>
<li>åˆ›å»ºä¸€ä¸ªæœ‰æ•ˆçš„è¯­ä¹‰çª—å£ï¼Œå¯ä»¥æé«˜å®ä½“è¯†åˆ«å’Œæ ¼å¼åŒ–çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å°†å®ä½“é‡æ–°åˆ†é…ç»™å³æ‰‹å—æ¥è§£å†³è·¨è¶Šå—è¾¹ç•Œçš„å®ä½“é—®é¢˜ï¼Œç¡®ä¿æ­£ç¡®çš„æ ¼å¼åŒ–ã€‚</li>
<li>ä¸°å¯Œçš„è®­ç»ƒæ•°æ®å¸¦æœ‰åµŒå…¥çš„å®ä½“æ ‡ç­¾ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ è¯†åˆ«å’Œç±»å‹ç‰¹å®šçš„æ ¼å¼åŒ–ã€‚</li>
<li>è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯­ä¹‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½æœ‰æ‰€æé«˜ï¼ŒåŒ…æ‹¬å‘½åå®ä½“è¯†åˆ«å’Œå®ä½“æ ¼å¼åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-333cec5129afe8cac64b7a1f8edbf0c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-748e05c0c2874d12d1f2dc871a4a8b36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66185f75a38de518b5537e891386fe37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f53fed26bc59d4dfc6a1423ec8c4797a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7fd3ebafde6210c979fba6ee94a3c808.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-674a7df8c062e51f9f4c6b9906094cca.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Boosting-CTC-Based-ASR-Using-LLM-Based-Intermediate-Loss-Regularization"><a href="#Boosting-CTC-Based-ASR-Using-LLM-Based-Intermediate-Loss-Regularization" class="headerlink" title="Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization"></a>Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization</h2><p><strong>Authors:Duygu Altinok</strong></p>
<p>End-to-end (E2E) automatic speech recognition (ASR) systems have revolutionized the field by integrating all components into a single neural network, with attention-based encoder-decoder models achieving state-of-the-art performance. However, their autoregressive decoding process limits inference speed, making them unsuitable for real-time applications. In contrast, CTC-based models offer faster, non-autoregressive decoding but struggle to model linguistic dependencies effectively. Addressing this challenge, we propose a novel auxiliary loss framework called Language-Aware Intermediate Loss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large language models (LLMs). By attaching connector layers to intermediate encoder layers, LAIL maps outputs to the embedding space of an LLM and computes a causal language modeling loss during training. This approach enhances linguistic modeling while preserving the computational efficiency of CTC decoding. Using the Conformer architecture and various LLaMA models, we demonstrate significant improvements in Word Error Rate (WER) on the LibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance for CTC-based ASR with minimal computational overhead. </p>
<blockquote>
<p>ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿé€šè¿‡å°†æ‰€æœ‰ç»„ä»¶é›†æˆåˆ°ä¸€ä¸ªå•ä¸€ç¥ç»ç½‘ç»œä¸­ï¼Œä»è€Œå®ç°äº†é¢†åŸŸçš„é©å‘½æ€§å˜é©ï¼ŒåŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è‡ªå›å½’è§£ç è¿‡ç¨‹é™åˆ¶äº†æ¨ç†é€Ÿåº¦ï¼Œä½¿å…¶ä¸é€‚åˆå®æ—¶åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒCTCï¼ˆè¿æ¥æ—¶åºåˆ†ç±»ï¼‰æ¨¡å‹æä¾›æ›´å¿«çš„éè‡ªå›å½’è§£ç ï¼Œä½†åœ¨æœ‰æ•ˆå»ºæ¨¡è¯­è¨€ä¾èµ–æ€§æ–¹é¢å´å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºè¯­è¨€æ„ŸçŸ¥ä¸­é—´æŸå¤±ï¼ˆLAILï¼‰çš„æ–°å‹è¾…åŠ©æŸå¤±æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­è¨€çŸ¥è¯†å¢å¼ºåŸºäºCTCçš„ASRã€‚é€šè¿‡å°†è¿æ¥å™¨å±‚é™„åŠ åˆ°ä¸­é—´ç¼–ç å™¨å±‚ï¼ŒLAILå°†è¾“å‡ºæ˜ å°„åˆ°LLMçš„åµŒå…¥ç©ºé—´ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—å› æœè¯­è¨€å»ºæ¨¡æŸå¤±ã€‚è¿™ç§æ–¹æ³•åœ¨å¢å¼ºè¯­è¨€å»ºæ¨¡çš„åŒæ—¶ï¼Œä¿æŒäº†CTCè§£ç çš„è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡ä½¿ç”¨Conformeræ¶æ„å’Œå„ç§LLaMAæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨LibriSpeechã€TEDLIUM2å’ŒWSJè¯­æ–™åº“ä¸Šæ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œåœ¨CTC-based ASRä¸­å®ç°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ï¼Œä¸”è®¡ç®—å¼€é”€æœ€å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22846v1">PDF</a> This is the accepted version of an article accepted to the TSD 2025   conference, published in Springer Lecture Notes in Artificial Intelligence   (LNAI). The final authenticated version is available online at SpringerLink</p>
<p><strong>Summary</strong>ï¼š</p>
<p>ç«¯åˆ°ç«¯çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿé€šè¿‡æ•´åˆæ‰€æœ‰ç»„ä»¶åˆ°ä¸€ä¸ªå•ä¸€ç¥ç»ç½‘ç»œä¸­å®ç°äº†é¢†åŸŸçš„é©æ–°ï¼Œå…¶ä¸­åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è‡ªå›å½’è§£ç è¿‡ç¨‹é™åˆ¶äº†æ¨ç†é€Ÿåº¦ï¼Œä¸é€‚åˆå®æ—¶åº”ç”¨ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºè¯­è¨€æ„ŸçŸ¥ä¸­é—´æŸå¤±ï¼ˆLAILï¼‰çš„æ–°å‹è¾…åŠ©æŸå¤±æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­è¨€çŸ¥è¯†å¢å¼ºCTC-based ASRã€‚LAILé€šè¿‡åœ¨ä¸­é—´ç¼–ç å™¨å±‚æ·»åŠ è¿æ¥å™¨å±‚ï¼Œå°†è¾“å‡ºæ˜ å°„åˆ°LLMçš„åµŒå…¥ç©ºé—´ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—å› æœè¯­è¨€å»ºæ¨¡æŸå¤±ã€‚æ­¤æ–¹æ³•åœ¨å¢å¼ºè¯­è¨€å»ºæ¨¡çš„åŒæ—¶ï¼Œä¿æŒäº†CTCè§£ç çš„è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡ä½¿ç”¨Conformeræ¶æ„å’Œå¤šç§LLaMAæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨LibriSpeechã€TEDLIUM2å’ŒWSJè¯­æ–™åº“ä¸Šæ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œå®ç°äº†CTC-based ASRçš„å…ˆè¿›æ€§èƒ½ï¼Œä¸”è®¡ç®—å¼€é”€æœ€å°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç«¯åˆ°ç«¯çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿé€šè¿‡æ•´åˆç¥ç»ç½‘ç»œç»„ä»¶å®ç°äº†çªç ´ã€‚</li>
<li>åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹å–å¾—æœ€å…ˆè¿›çš„ASRæ€§èƒ½ã€‚</li>
<li>è‡ªå›å½’è§£ç è¿‡ç¨‹é™åˆ¶äº†ASRç³»ç»Ÿçš„æ¨ç†é€Ÿåº¦ï¼Œä¸é€‚åˆå®æ—¶åº”ç”¨ã€‚</li>
<li>æå‡ºäº†è¯­è¨€æ„ŸçŸ¥ä¸­é—´æŸå¤±ï¼ˆLAILï¼‰æ¡†æ¶ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºCTC-based ASRã€‚</li>
<li>LAILé€šè¿‡æ˜ å°„è¾“å‡ºåˆ°LLMåµŒå…¥ç©ºé—´å¹¶è®¡ç®—å› æœè¯­è¨€å»ºæ¨¡æŸå¤±ï¼Œå¢å¼ºè¯­è¨€å»ºæ¨¡å¹¶ä¿ç•™CTCè§£ç çš„è®¡ç®—æ•ˆç‡ã€‚</li>
<li>ä½¿ç”¨Conformeræ¶æ„å’ŒLLaMAæ¨¡å‹åœ¨å¤šä¸ªè¯­æ–™åº“ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5b6b4a5b642c56440bcbaaaea33dcff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-785616c2b07250bbda7f82dfa225a9d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3195a20b862b83e285745c68bbe5fedc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Assessing-the-feasibility-of-Large-Language-Models-for-detecting-micro-behaviors-in-team-interactions-during-space-missions"><a href="#Assessing-the-feasibility-of-Large-Language-Models-for-detecting-micro-behaviors-in-team-interactions-during-space-missions" class="headerlink" title="Assessing the feasibility of Large Language Models for detecting   micro-behaviors in team interactions during space missions"></a>Assessing the feasibility of Large Language Models for detecting   micro-behaviors in team interactions during space missions</h2><p><strong>Authors:Ankush Raut, Projna Paromita, Sydney Begerowski, Suzanne Bell, Theodora Chaspari</strong></p>
<p>We explore the feasibility of large language models (LLMs) in detecting subtle expressions of micro-behaviors in team conversations using transcripts collected during simulated space missions. Specifically, we examine zero-shot classification, fine-tuning, and paraphrase-augmented fine-tuning with encoder-only sequence classification LLMs, as well as few-shot text generation with decoder-only causal language modeling LLMs, to predict the micro-behavior associated with each conversational turn (i.e., dialogue). Our findings indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to detect underrepresented micro-behaviors, particularly discouraging speech, even with weighted fine-tuning. In contrast, the instruction fine-tuned version of Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best models achieving macro F1-scores of 44% for 3-way classification and 68% for binary classification. These results have implications for the development of speech technologies aimed at analyzing team communication dynamics and enhancing training interventions in high-stakes environments such as space missions, particularly in scenarios where text is the only accessible data. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€æµ‹å›¢é˜Ÿä¼šè¯ä¸­çš„å¾®å¦™å¾®è§‚è¡Œä¸ºè¡¨è¾¾æ–¹é¢çš„å¯è¡Œæ€§ï¼Œä½¿ç”¨çš„æ˜¯åœ¨æ¨¡æ‹Ÿå¤ªç©ºä»»åŠ¡æœŸé—´æ”¶é›†çš„è½¬å½•å†…å®¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç ”ç©¶äº†é›¶æ ·æœ¬åˆ†ç±»ã€å¾®è°ƒä»¥åŠåˆ©ç”¨ç¼–ç å™¨åºåˆ—åˆ†ç±»LLMçš„æ”¹å†™å¢å¼ºå¾®è°ƒæŠ€æœ¯ï¼Œä»¥åŠåˆ©ç”¨è§£ç å™¨å› æœè¯­è¨€å»ºæ¨¡LLMçš„å°‘é‡æ–‡æœ¬ç”ŸæˆæŠ€æœ¯ï¼Œä»¥é¢„æµ‹ä¸æ¯ä¸ªä¼šè¯å›åˆï¼ˆå³å¯¹è¯ï¼‰ç›¸å…³çš„å¾®è§‚è¡Œä¸ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¦‚RoBERTaå’ŒDistilBERTç­‰ä»…ä½¿ç”¨ç¼–ç å™¨çš„LLMåœ¨æ£€æµ‹è¡¨ç°ä¸è¶³çš„å¾®è§‚è¡Œä¸ºæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åŠé˜»æ€§è¨€è¯­ï¼Œå³ä½¿ä½¿ç”¨åŠ æƒå¾®è°ƒä¹Ÿæ— æµäºäº‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒæŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬çš„Llama-3.1ï¼ˆä¸€ç§ä»…ä½¿ç”¨è§£ç å™¨çš„LLMï¼‰è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œæœ€ä½³æ¨¡å‹çš„3åˆ†ç±»å®è§‚F1åˆ†æ•°è¾¾åˆ°44%ï¼ŒäºŒå…ƒåˆ†ç±»è¾¾åˆ°68%ã€‚è¿™äº›ç»“æœå¯¹äºå¼€å‘æ—¨åœ¨åˆ†æå›¢é˜Ÿæ²Ÿé€šåŠ¨æ€çš„è¯­éŸ³æŠ€æœ¯ï¼Œä»¥åŠåœ¨å¤ªç©ºä»»åŠ¡ç­‰é«˜é£é™©ç¯å¢ƒä¸­å¢å¼ºè®­ç»ƒå¹²é¢„æªæ–½å…·æœ‰å¯ç¤ºæ„ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨åªèƒ½è®¿é—®æ–‡æœ¬æ•°æ®çš„åœºæ™¯ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22679v1">PDF</a> 5 pages, 4 figures. Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€æµ‹å›¢é˜Ÿä¼šè¯ä¸­çš„å¾®å¦™è¡¨è¾¾ä¸å¾®è¡Œä¸ºæ–¹é¢çš„å¯è¡Œæ€§ç ”ç©¶ã€‚é€šè¿‡æ¨¡æ‹Ÿå¤ªç©ºä»»åŠ¡æœŸé—´æ”¶é›†çš„è½¬å½•æœ¬ï¼Œæ¢ç´¢äº†é›¶æ ·æœ¬åˆ†ç±»ã€å¾®è°ƒã€åŸºäºå˜ä½“çš„å¾®è°ƒä¸ä»…ç¼–ç å™¨åºåˆ—åˆ†ç±»LLMï¼Œä»¥åŠä»…è§£ç å™¨å› æœè¯­è¨€å»ºæ¨¡LLMçš„å°‘é‡æ–‡æœ¬ç”Ÿæˆï¼Œä»¥é¢„æµ‹æ¯ä¸ªä¼šè¯å›åˆç›¸å…³çš„å¾®è¡Œä¸ºã€‚ç ”ç©¶å‘ç°ï¼Œç¼–ç å™¨LLMï¼ˆå¦‚RoBERTaå’ŒDistilBERTï¼‰åœ¨æ£€æµ‹ä½è¡¨è¾¾çš„å¾®è¡Œä¸ºæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€ŒæŒ‡ä»¤å¾®è°ƒç‰ˆçš„Llama-3.1è¡¨ç°ä¼˜è¶Šï¼Œæœ€ä½³æ¨¡å‹çš„ä¸‰ç±»åˆ†ç±»å®è§‚F1åˆ†æ•°ä¸º44%ï¼ŒäºŒå…ƒåˆ†ç±»ä¸º68%ã€‚è¿™å¯¹å¼€å‘åˆ†æå›¢é˜Ÿæ²Ÿé€šåŠ¨æ€å’Œæå‡å¤ªç©ºä»»åŠ¡ç­‰é«˜å‹åŠ›ç¯å¢ƒä¸‹çš„è®­ç»ƒå¹²é¢„çš„è¯­éŸ³æŠ€æœ¯å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶åœ¨åªèƒ½è·å–æ–‡æœ¬æ•°æ®çš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºæ£€æµ‹å›¢é˜Ÿä¼šè¯ä¸­çš„å¾®å¦™è¡¨è¾¾å’Œå¾®è¡Œä¸ºã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿå¤ªç©ºä»»åŠ¡è½¬å½•æœ¬è¿›è¡Œç ”ç©¶ã€‚</li>
<li>å°è¯•äº†é›¶æ ·æœ¬åˆ†ç±»ã€å¾®è°ƒåŠåŸºäºå˜ä½“çš„å¾®è°ƒç­‰å¤šç§æ–¹æ³•ã€‚</li>
<li>ç¼–ç å™¨LLMåœ¨æ£€æµ‹ä½è¡¨è¾¾çš„å¾®è¡Œä¸ºæ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒç‰ˆçš„è§£ç å™¨LLMï¼ˆå¦‚Llama-3.1ï¼‰è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>æœ€ä½³æ¨¡å‹çš„å®è§‚F1åˆ†æ•°ï¼šä¸‰ç±»åˆ†ç±»ä¸º44%ï¼ŒäºŒå…ƒåˆ†ç±»ä¸º68%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a8bd9be35cfd1648769564a79ad8f1d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-481461c46540ac09255d9e75f00076b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6cdf082f4ae258989e4602052b4c6158.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a45c02e724ef839dbefdf7eb3c1b9cf7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Speaker-Targeting-via-Self-Speaker-Adaptation-for-Multi-talker-ASR"><a href="#Speaker-Targeting-via-Self-Speaker-Adaptation-for-Multi-talker-ASR" class="headerlink" title="Speaker Targeting via Self-Speaker Adaptation for Multi-talker ASR"></a>Speaker Targeting via Self-Speaker Adaptation for Multi-talker ASR</h2><p><strong>Authors:Weiqing Wang, Taejin Park, Ivan Medennikov, Jinhan Wang, Kunal Dhawan, He Huang, Nithin Rao Koluguri, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>We propose a self-speaker adaptation method for streaming multi-talker automatic speech recognition (ASR) that eliminates the need for explicit speaker queries. Unlike conventional approaches requiring target speaker embeddings or enrollment audio, our technique dynamically adapts individual ASR instances through speaker-wise speech activity prediction. The key innovation involves injecting speaker-specific kernels generated via speaker supervision activations into selected ASR encoder layers. This enables instantaneous speaker adaptation to target speakers while handling fully overlapped speech even in a streaming scenario. Experiments show state-of-the-art performance in both offline and streaming scenarios, demonstrating that our self-adaptive method effectively addresses severe speech overlap through streamlined speaker-focused recognition. The results validate the proposed self-speaker adaptation approach as a robust solution for multi-talker ASR under severe overlapping speech conditions. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹æµå¼å¤šè¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è‡ªæˆ‘è¯´è¯äººé€‚é…æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— éœ€æ˜ç¡®çš„è¯´è¯äººæŸ¥è¯¢ã€‚ä¸åŒäºéœ€è¦ç›®æ ‡è¯´è¯äººåµŒå…¥æˆ–æ³¨å†ŒéŸ³é¢‘çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯é€šè¿‡è¯´è¯äººè¯­éŸ³æ´»åŠ¨é¢„æµ‹æ¥åŠ¨æ€é€‚é…å•ä¸ªASRå®ä¾‹ã€‚å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºå°†è¯´è¯äººç‰¹å®šå†…æ ¸æ³¨å…¥é€‰å®šçš„ASRç¼–ç å™¨å±‚ï¼Œè¿™äº›å†…æ ¸é€šè¿‡è¯´è¯äººç›‘ç£æ¿€æ´»ç”Ÿæˆã€‚è¿™èƒ½å¤Ÿå®ç°é’ˆå¯¹ç›®æ ‡è¯´è¯äººçš„å³æ—¶è¯´è¯äººé€‚é…ï¼Œå³ä½¿åœ¨æµå¼åœºæ™¯ä¸­ä¹Ÿèƒ½å¤„ç†å®Œå…¨é‡å çš„è¯­éŸ³ã€‚å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨ç¦»çº¿è¿˜æ˜¯æµå¼åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„è‡ªæˆ‘é€‚åº”æ–¹æ³•å‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜å…¶é€šè¿‡ç®€åŒ–çš„è¯´è¯äººèšç„¦è¯†åˆ«æœ‰æ•ˆåœ°è§£å†³äº†ä¸¥é‡è¯­éŸ³é‡å é—®é¢˜ã€‚ç»“æœéªŒè¯äº†æ‰€æå‡ºçš„è‡ªæˆ‘è¯´è¯äººé€‚é…æ–¹æ³•ä½œä¸ºä¸¥é‡é‡å è¯­éŸ³æ¡ä»¶ä¸‹å¤šè¯´è¯äººASRçš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22646v1">PDF</a> Accepted by INTERSPEECH 2025</p>
<p><strong>æ€»ç»“</strong><br>åœ¨æµå¼çš„å¤šè¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€æ˜ç¡®è¯´è¯äººæŸ¥è¯¢çš„è‡ªæˆ‘é€‚åº”æ–¹æ³•ã€‚ä¸åŒäºéœ€è¦ç›®æ ‡è¯´è¯äººåµŒå…¥æˆ–æ³¨å†ŒéŸ³é¢‘çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯é€šè¿‡è¯´è¯äººè¯­éŸ³æ´»åŠ¨é¢„æµ‹æ¥åŠ¨æ€é€‚åº”å„ä¸ªASRå®ä¾‹ã€‚å…³é”®åˆ›æ–°ç‚¹åœ¨äºå‘é€‰å®šASRç¼–ç å™¨å±‚æ³¨å…¥é€šè¿‡è¯´è¯äººç›‘ç£æ¿€æ´»ç”Ÿæˆçš„è¯´è¯äººç‰¹å®šæ ¸ï¼Œä»¥å®ç°å³æ—¶é€‚åº”ç›®æ ‡è¯´è¯äººï¼ŒåŒæ—¶å¤„ç†æµå¼åœºæ™¯ä¸­çš„å®Œå…¨é‡å è¯­éŸ³ã€‚å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨ç¦»çº¿è¿˜æ˜¯æµå¼åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„è‡ªæˆ‘é€‚åº”æ–¹æ³•å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæœ‰æ•ˆè§£å†³äº†ä¸¥é‡çš„è¯­éŸ³é‡å é—®é¢˜ï¼Œé€šè¿‡ç®€åŒ–çš„è¯´è¯äººè¯†åˆ«åŠŸèƒ½å®ç°é’ˆå¯¹æ€§çš„è§£å†³ç­–ç•¥ã€‚ç»“æœéªŒè¯äº†æ‰€æå‡ºçš„è‡ªæˆ‘é€‚åº”è¯´è¯äººé€‚åº”æ–¹æ³•ä½œä¸ºä¸¥é‡é‡å è¯­éŸ³æ¡ä»¶ä¸‹çš„å¤šè¯´è¯äººASRçš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§è‡ªæˆ‘é€‚åº”çš„è¯´è¯äººé€‚åº”æ–¹æ³•ï¼Œç”¨äºæµå¼çš„å¤šè¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚</li>
<li>é€šè¿‡è¯´è¯äººè¯­éŸ³æ´»åŠ¨é¢„æµ‹åŠ¨æ€é€‚åº”ASRå®ä¾‹ï¼Œæ— éœ€ç›®æ ‡è¯´è¯äººåµŒå…¥æˆ–æ³¨å†ŒéŸ³é¢‘ã€‚</li>
<li>é€šè¿‡å‘ASRç¼–ç å™¨å±‚æ³¨å…¥è¯´è¯äººç‰¹å®šæ ¸ï¼Œå®ç°å³æ—¶é€‚åº”ç›®æ ‡è¯´è¯äººã€‚</li>
<li>æŠ€æœ¯èƒ½å¤Ÿå¤„ç†æµå¼åœºæ™¯ä¸­çš„å®Œå…¨é‡å è¯­éŸ³ã€‚</li>
<li>å®éªŒç»“æœè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨ç¦»çº¿åŠæµå¼åœºæ™¯ä¸‹å‡è¾¾åˆ°ä¸šç•Œå…ˆè¿›æ°´å¹³ã€‚</li>
<li>è‡ªæˆ‘é€‚åº”æ–¹æ³•æœ‰æ•ˆè§£å†³ä¸¥é‡è¯­éŸ³é‡å é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0a045e10cf8e159dc8e20c6f85143227.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e11a7fa8e267fa92962aab2871f28328.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17f86dca59e3c39570ca8270a94b1978.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d7a59d346488756d2fa7c41b9874940.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-98c33a99c55fb9c565398113584190f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e5596f6f678ad0e0c9a0b7c580e06b0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Cross-lingual-Data-Selection-Using-Clip-level-Acoustic-Similarity-for-Enhancing-Low-resource-Automatic-Speech-Recognition"><a href="#Cross-lingual-Data-Selection-Using-Clip-level-Acoustic-Similarity-for-Enhancing-Low-resource-Automatic-Speech-Recognition" class="headerlink" title="Cross-lingual Data Selection Using Clip-level Acoustic Similarity for   Enhancing Low-resource Automatic Speech Recognition"></a>Cross-lingual Data Selection Using Clip-level Acoustic Similarity for   Enhancing Low-resource Automatic Speech Recognition</h2><p><strong>Authors:Shunsuke Mitsumori, Sara Kashiwagi, Keitaro Tanaka, Shigeo Morishima</strong></p>
<p>This paper presents a novel donor data selection method to enhance low-resource automatic speech recognition (ASR). While ASR performs well in high-resource languages, its accuracy declines in low-resource settings due to limited training data. A common solution is to leverage multilingual self-supervised learning (SSL) models with donor languages. However, existing methods rely on language-level similarity, overlooking clip-level variations. To address this limitation, we propose clip-wise acoustic token distribution similarity (CATDS), a fine-grained selection method that identifies acoustically relevant donor clips for better alignment with the target language. Unlike existing clip-level selection methods, our method aligns with the representation of SSL models and offers more challenging yet valuable samples. Experimental results show that CATDS outperforms traditional selection methods and can even utilize donor languages previously considered detrimental. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æèµ è€…æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œä»¥æé«˜ä½èµ„æºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æ€§èƒ½ã€‚è™½ç„¶ASRåœ¨é«˜èµ„æºè¯­è¨€ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä½èµ„æºç¯å¢ƒä¸­ç”±äºè®­ç»ƒæ•°æ®æœ‰é™ï¼Œå…¶å‡†ç¡®æ€§ä¼šä¸‹é™ã€‚ä¸€ç§å¸¸è§çš„è§£å†³æ–¹æ¡ˆæ˜¯åˆ©ç”¨å…·æœ‰æèµ è€…è¯­è¨€çš„å¤šè¯­è¨€è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºè¯­è¨€çº§åˆ«çš„ç›¸ä¼¼æ€§ï¼Œå¿½ç•¥äº†å‰ªè¾‘çº§åˆ«çš„å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå‰ªè¾‘çš„å£°å­¦ä»¤ç‰Œåˆ†å¸ƒç›¸ä¼¼æ€§ï¼ˆCATDSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç²¾ç»†çš„é€‰æ ·æ–¹æ³•ï¼Œå¯ä»¥è¯†åˆ«ä¸ç›®æ ‡è¯­è¨€æ›´åŒ¹é…çš„å£°å­¦ç›¸å…³æèµ å‰ªè¾‘ã€‚ä¸ä¼ ç»Ÿçš„å‰ªè¾‘çº§é€‰æ‹©æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸SSLæ¨¡å‹çš„è¡¨ç¤ºç›¸ä¸€è‡´ï¼Œå¹¶æä¾›æ›´å…·æŒ‘æˆ˜ä½†æ›´æœ‰ä»·å€¼çš„æ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCATDSä¼˜äºä¼ ç»Ÿé€‰æ‹©æ–¹æ³•ï¼Œç”šè‡³å¯ä»¥åˆ©ç”¨ä»¥å‰è¢«è®¤ä¸ºæœ‰å®³çš„æèµ è€…è¯­è¨€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22194v1">PDF</a> Accepted at INTERSPEECH 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æèµ è€…æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œä»¥æé«˜åœ¨ä½èµ„æºç¯å¢ƒä¸‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ€§èƒ½ã€‚åœ¨é«˜èµ„æºè¯­è¨€ç¯å¢ƒä¸­ï¼ŒASRè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä½èµ„æºè®¾ç½®ä¸‹ï¼Œç”±äºè®­ç»ƒæ•°æ®æœ‰é™ï¼Œå…¶å‡†ç¡®æ€§ä¼šä¸‹é™ã€‚å¸¸è§çš„è§£å†³æ–¹æ¡ˆæ˜¯åˆ©ç”¨å…·æœ‰æèµ è€…è¯­è¨€çš„å¤šè¯­è¨€è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºè¯­è¨€çº§åˆ«çš„ç›¸ä¼¼æ€§ï¼Œå¿½ç•¥äº†å‰ªè¾‘çº§åˆ«çš„å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå‰ªè¾‘çš„å£°å­¦ä»¤ç‰Œåˆ†å¸ƒç›¸ä¼¼æ€§ï¼ˆCATDSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç²¾ç»†çš„ç­›é€‰æ–¹æ³•ï¼Œå¯ä»¥è¯†åˆ«ä¸ç›®æ ‡è¯­è¨€å£°å­¦ç›¸å…³çš„æèµ å‰ªè¾‘ï¼Œä»¥å®ç°æ›´å¥½çš„å¯¹é½ã€‚ä¸ç°æœ‰çš„å‰ªè¾‘çº§åˆ«é€‰æ‹©æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¬¦åˆSSLæ¨¡å‹çš„è¡¨ç¤ºï¼Œå¹¶æä¾›æ›´å…·æŒ‘æˆ˜æ€§ä½†æ›´æœ‰ä»·å€¼çš„æ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCATDSä¼˜äºä¼ ç»Ÿé€‰æ‹©æ–¹æ³•ï¼Œç”šè‡³å¯ä»¥åˆ©ç”¨ä»¥å‰è¢«è®¤ä¸ºæœ‰å®³çš„æèµ è¯­è¨€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æèµ è€…æ•°æ®é€‰æ‹©æ–¹æ³•â€”â€”åŸºäºå‰ªè¾‘çš„å£°å­¦ä»¤ç‰Œåˆ†å¸ƒç›¸ä¼¼æ€§ï¼ˆCATDSï¼‰ï¼Œæ—¨åœ¨æé«˜ä½èµ„æºç¯å¢ƒä¸‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>ä½èµ„æºè®¾ç½®ä¸‹çš„ASRå‡†ç¡®æ€§ä¸‹é™é—®é¢˜å¾—åˆ°äº†è§£å†³ï¼Œé€šè¿‡åˆ©ç”¨å¤šè¯­è¨€è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹å’Œæèµ è€…æ•°æ®ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è¯­è¨€çº§åˆ«çš„ç›¸ä¼¼æ€§æ¥é€‰æ‹©æèµ æ•°æ®ï¼Œä½†CATDSåˆ™å…³æ³¨å‰ªè¾‘çº§åˆ«çš„å˜åŒ–ã€‚</li>
<li>CATDSèƒ½å¤Ÿè¯†åˆ«ä¸ç›®æ ‡è¯­è¨€å£°å­¦ç›¸å…³çš„æèµ å‰ªè¾‘ï¼Œå®ç°æ›´å¥½çš„å¯¹é½ã€‚</li>
<li>CATDSä¸ç°æœ‰çš„å‰ªè¾‘çº§åˆ«é€‰æ‹©æ–¹æ³•ä¸åŒï¼Œæ›´ç¬¦åˆSSLæ¨¡å‹çš„è¡¨ç¤ºæ–¹å¼ã€‚</li>
<li>CATDSæä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§ä½†å¯¹æé«˜ASRæ€§èƒ½æœ‰ä»·å€¼çš„æ ·æœ¬ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCATDSåœ¨é€‰æ‹©æèµ è¯­è¨€æ•°æ®æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç”šè‡³èƒ½å¤Ÿåˆ©ç”¨ä¹‹å‰è¢«è®¤ä¸ºæœ‰å®³çš„æèµ è¯­è¨€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e16c20853952f2c85336fcab2e5ebc52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eae75af78e3351a942d78dce1e600f67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dd34b7f61719f485aa3f0b3b3b61270.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11684a9eb7d291f0f6fc4e6022d62751.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3120d4484c77ce91e6e5c723a58c8c15.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SAGE-Spliced-Audio-Generated-Data-for-Enhancing-Foundational-Models-in-Low-Resource-Arabic-English-Code-Switched-Speech-Recognition"><a href="#SAGE-Spliced-Audio-Generated-Data-for-Enhancing-Foundational-Models-in-Low-Resource-Arabic-English-Code-Switched-Speech-Recognition" class="headerlink" title="SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in   Low-Resource Arabic-English Code-Switched Speech Recognition"></a>SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in   Low-Resource Arabic-English Code-Switched Speech Recognition</h2><p><strong>Authors:Muhammad Umar Farooq, Oscar Saz</strong></p>
<p>This paper investigates the performance of various speech SSL models on dialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address data scarcity, a modified audio-splicing approach is introduced to generate artificial CS speech data. Fine-tuning an already fine-tuned SSL model with the proposed Spliced-Audio Generated (SAGE) data results in an absolute improvement on Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks. Additionally, an Experience Replay (ER) inspired approach is proposed to enhance generalisation across DA and CS speech while mitigating catastrophic forgetting. Integrating an out-of-domain 3-gram language model reduces the overall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching benchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS benchmarks surpasses large-scale multilingual models, including USM and Whisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and 8.4%, respectively. </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†å„ç§è¯­éŸ³SSLæ¨¡å‹åœ¨æ–¹è¨€é˜¿æ‹‰ä¼¯è¯­ï¼ˆDAï¼‰å’Œé˜¿æ‹‰ä¼¯è¯­è‹±è¯­ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰è¯­éŸ³ä¸Šçš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„éŸ³é¢‘æ‹¼æ¥æ–¹æ³•æ¥ç”Ÿæˆäººå·¥CSè¯­éŸ³æ•°æ®ã€‚ä½¿ç”¨æå‡ºçš„æ‹¼æ¥éŸ³é¢‘ç”Ÿæˆï¼ˆSAGEï¼‰æ•°æ®å¯¹å·²ç»å¾®è°ƒè¿‡çš„SSLæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåœ¨é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­CSåŸºå‡†æµ‹è¯•ä¸Šï¼Œå•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ç»å¯¹é™ä½äº†7.8%ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§å—ç»éªŒå›æ”¾ï¼ˆERï¼‰å¯å‘çš„ç­–ç•¥ï¼Œä»¥æé«˜åœ¨æ–¹è¨€é˜¿æ‹‰ä¼¯è¯­å’Œä»£ç åˆ‡æ¢è¯­éŸ³ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚é›†æˆä¸€ä¸ªéåŸŸ3-gramè¯­è¨€æ¨¡å‹å°†æ•´ä½“å¹³å‡WERä»31.7%é™ä½åˆ°26.6%ã€‚é’ˆå¯¹ä»£ç åˆ‡æ¢åŸºå‡†çš„å°‘é‡å¾®è°ƒè¿›ä¸€æ­¥å°†WERæé«˜äº†4.9%ã€‚åœ¨é˜¿æ‹‰ä¼¯è¯­è‹±è¯­CSåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°çš„WERä¸º31.1%ï¼Œè¶…è¿‡äº†å¤§è§„æ¨¡å¤šè¯­ç§æ¨¡å‹ï¼ŒåŒ…æ‹¬USMå’ŒWhisper-large-v2ï¼ˆä¸¤è€…éƒ½è¶…è¿‡åå€å¤§ï¼‰ï¼Œåˆ†åˆ«ç»å¯¹é™ä½äº†5.5%å’Œ8.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22143v1">PDF</a> Accepted for IEEE MLSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†ä¸åŒè¯­éŸ³SSLæ¨¡å‹åœ¨æ–¹è¨€é˜¿æ‹‰ä¼¯è¯­ï¼ˆDAï¼‰å’Œé˜¿æ‹‰ä¼¯è¯­-è‹±è¯­ä»£ç åˆ‡æ¢ï¼ˆCSï¼‰è¯­éŸ³ä¸Šçš„è¡¨ç°ã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¼•å…¥äº†æ”¹è‰¯çš„éŸ³é¢‘æ‹¼æ¥æ–¹æ³•æ¥ç”Ÿæˆäººå·¥CSè¯­éŸ³æ•°æ®ã€‚é€šè¿‡ç”¨æå‡ºSpliced-Audio Generated (SAGE)æ•°æ®å¯¹å·²ç»å¾®è°ƒè¿‡çš„SSLæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåœ¨é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­CSåŸºå‡†æµ‹è¯•ä¸Šï¼Œå•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ç»å¯¹æ”¹å–„äº†7.8%ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å—Experience Replayï¼ˆERï¼‰å¯å‘çš„æ–¹æ³•æ¥æé«˜DAå’ŒCSè¯­éŸ³çš„é€šç”¨æ€§ï¼ŒåŒæ—¶å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚é›†æˆé¢†åŸŸå¤–çš„ä¸‰å…ƒè¯­è¨€æ¨¡å‹å°†æ€»ä½“å¹³å‡WERä»31.7%é™ä½åˆ°26.6%ã€‚é’ˆå¯¹ä»£ç åˆ‡æ¢åŸºå‡†æµ‹è¯•çš„å°‘é‡å¾®è°ƒè¿›ä¸€æ­¥å°†WERæé«˜äº†4.9%ã€‚åœ¨é˜¿æ‹‰ä¼¯è¯­-è‹±è¯­CSåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°çš„WERä¸º31.1%ï¼Œè¶…è¶Šäº†å¤§è§„æ¨¡å¤šè¯­ç§æ¨¡å‹ï¼ŒåŒ…æ‹¬USMå’ŒWhisper-large-v2ï¼ˆä¸¤è€…éƒ½è¶…è¿‡åå€å¤§ï¼‰ï¼Œåˆ†åˆ«ç»å¯¹æ”¹å–„äº†5.5%å’Œ8.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ç ”ç©¶äº†ä¸åŒè¯­éŸ³SSLæ¨¡å‹åœ¨æ–¹è¨€é˜¿æ‹‰ä¼¯è¯­å’Œé˜¿æ‹‰ä¼¯è¯­-è‹±è¯­ä»£ç åˆ‡æ¢è¯­éŸ³ä¸Šçš„è¡¨ç°ã€‚</li>
<li>å¼•å…¥æ”¹è‰¯çš„éŸ³é¢‘æ‹¼æ¥æ–¹æ³•ç”Ÿæˆäººå·¥CSè¯­éŸ³æ•°æ®ï¼Œä»¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>ä½¿ç”¨SAGEæ•°æ®å¾®è°ƒSSLæ¨¡å‹ï¼Œæ˜¾è‘—æ”¹å–„äº†å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>æå‡ºå—Experience Replayå¯å‘çš„æ–¹æ³•ï¼Œæé«˜æ¨¡å‹åœ¨DAå’ŒCSè¯­éŸ³ä¸Šçš„é€šç”¨æ€§ï¼Œå¹¶å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>é›†æˆé¢†åŸŸå¤–çš„ä¸‰å…ƒè¯­è¨€æ¨¡å‹æœ‰æ•ˆé™ä½WERã€‚</li>
<li>å°‘é‡å¾®è°ƒé’ˆå¯¹ä»£ç åˆ‡æ¢åŸºå‡†æµ‹è¯•è¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22143">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-37784a5af802b900535a4e90bf4a636b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4adefd8817f7e1d9443d3ca011e04b00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f15420747815e3f873aec122e4e83efd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-859392e010e9345738f6aa37c0bcaf11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bc939a013385cde86d7d7c6907c6841.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18883875c032b8aac34d4d1072b07322.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Robust-and-Efficient-Autoregressive-Speech-Synthesis-with-Dynamic-Chunk-wise-Prediction-Policy"><a href="#Robust-and-Efficient-Autoregressive-Speech-Synthesis-with-Dynamic-Chunk-wise-Prediction-Policy" class="headerlink" title="Robust and Efficient Autoregressive Speech Synthesis with Dynamic   Chunk-wise Prediction Policy"></a>Robust and Efficient Autoregressive Speech Synthesis with Dynamic   Chunk-wise Prediction Policy</h2><p><strong>Authors:Bohan Li, Zhihan Li, Haoran Wang, Hanglei Zhang, Yiwei Guo, Hankun Wang, Xie Chen, Kai Yu</strong></p>
<p>Recently, autoregressive (AR) language models have emerged as a dominant approach in speech synthesis, offering expressive generation and scalable training. However, conventional AR speech synthesis models relying on the next-token prediction paradigm often encounter significant challenges when handling long speech sequences. These models often struggle to construct stable frame-to-frame attention, leading to increased latency and degraded synthesis quality, thereby limiting their feasibility for real-time applications. To address these limitations, we introduce a novel dynamic chunk-wise autoregressive synthesis framework, termed DCAR, designed to enhance both efficiency and intelligibility robustness in AR speech generation. DCAR introduces a chunk-to-frame attention mechanism through training with multi-token prediction, enabling dynamic chunk prediction in variable speech contexts using a lightweight module trained on-policy. DCAR dynamically adjusts the token prediction span, significantly reducing the sequence length dependency while obtaining high synthesis quality. Comprehensive empirical evaluations demonstrate that DCAR substantially outperforms traditional next-token prediction models, achieving up to 72.27% intelligibility improvement and 2.61x inference speedup simultaneously on the test set. Furthermore, we conduct comprehensive analysis to support it as a versatile foundation for next-generation speech synthesis systems. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè‡ªå›å½’ï¼ˆARï¼‰è¯­è¨€æ¨¡å‹å·²æˆä¸ºè¯­éŸ³åˆæˆä¸­çš„ä¸»æµæ–¹æ³•ï¼Œæä¾›è¡¨è¾¾æ€§ç”Ÿæˆå’Œå¯æ‰©å±•è®­ç»ƒã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼çš„ARè¯­éŸ³åˆæˆæ¨¡å‹åœ¨å¤„ç†é•¿è¯­éŸ³åºåˆ—æ—¶ç»å¸¸é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚è¿™äº›æ¨¡å‹åœ¨æ„å»ºç¨³å®šçš„å¸§åˆ°å¸§æ³¨æ„åŠ›æ–¹é¢å¾€å¾€é‡åˆ°å›°éš¾ï¼Œä»è€Œå¯¼è‡´å»¶è¿Ÿå¢åŠ å’Œåˆæˆè´¨é‡ä¸‹é™ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬åœ¨å®æ—¶åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŠ¨æ€åˆ†æ®µè‡ªå›å½’åˆæˆæ¡†æ¶ï¼Œç§°ä¸ºDCARï¼Œæ—¨åœ¨æé«˜ARè¯­éŸ³ç”Ÿæˆä¸­çš„æ•ˆç‡å’Œæ¸…æ™°åº¦ç¨³å¥æ€§ã€‚DCARé€šè¿‡å¤šä»¤ç‰Œé¢„æµ‹è¿›è¡Œè®­ç»ƒï¼Œå¼•å…¥äº†å—åˆ°å¸§çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿ç”¨è½»é‡çº§æ¨¡å—è¿›è¡Œç­–ç•¥è®­ç»ƒï¼Œåœ¨å¯å˜è¯­éŸ³ç¯å¢ƒä¸­å®ç°åŠ¨æ€å—é¢„æµ‹ã€‚DCARåŠ¨æ€è°ƒæ•´ä»¤ç‰Œé¢„æµ‹èŒƒå›´ï¼Œæ˜¾è‘—å‡å°‘åºåˆ—é•¿åº¦ä¾èµ–æ€§ï¼ŒåŒæ—¶è·å¾—é«˜è´¨é‡åˆæˆã€‚ç»¼åˆå®è¯è¯„ä¼°è¡¨æ˜ï¼ŒDCARåœ¨æµ‹è¯•é›†ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æ¨¡å‹ï¼Œå®ç°äº†é«˜è¾¾72.27%çš„æ¸…æ™°åº¦æå‡å’Œ2.61å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œæ”¯æŒå®ƒä½œä¸ºä¸‹ä¸€ä»£è¯­éŸ³åˆæˆç³»ç»Ÿçš„å¤šåŠŸèƒ½åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22023v1">PDF</a> 17 pages, 8 figures, 5 tables</p>
<p><strong>Summary</strong><br>     è¿‘æœŸï¼Œè‡ªå›å½’è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³åˆæˆä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†ä¼ ç»Ÿè‡ªå›å½’è¯­éŸ³åˆæˆæ¨¡å‹åœ¨å¤„ç†é•¿è¯­éŸ³åºåˆ—æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæå‡æ•ˆç‡å’Œæ¸…æ™°åº¦ç¨³å¥æ€§ï¼Œæå‡ºä¸€ç§æ–°å‹åŠ¨æ€åˆ†æ®µè‡ªå›å½’åˆæˆæ¡†æ¶DCARã€‚DCARé€šè¿‡å¤šä»¤ç‰Œé¢„æµ‹è®­ç»ƒï¼Œå¼•å…¥å—åˆ°å¸§çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°åŠ¨æ€å—é¢„æµ‹ã€‚ç»¼åˆè¯„ä»·æ˜¾ç¤ºï¼ŒDCARæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼ŒåŒæ—¶æé«˜æ¸…æ™°åº¦å¹¶åŠ é€Ÿæ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªå›å½’è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³åˆæˆä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†å¤„ç†é•¿è¯­éŸ³åºåˆ—æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿè‡ªå›å½’è¯­éŸ³åˆæˆæ¨¡å‹é¢ä¸´æ„å»ºç¨³å®šå¸§åˆ°å¸§æ³¨æ„åŠ›çš„å›°éš¾ï¼Œå¯¼è‡´å»¶è¿Ÿå’Œåˆæˆè´¨é‡ä¸‹é™ã€‚</li>
<li>DCARæ¡†æ¶é€šè¿‡å¼•å…¥å—åˆ°å¸§çš„æ³¨æ„åŠ›æœºåˆ¶å’ŒåŠ¨æ€å—é¢„æµ‹ï¼Œæå‡æ•ˆç‡å’Œæ¸…æ™°åº¦ç¨³å¥æ€§ã€‚</li>
<li>DCARé€šè¿‡å¤šä»¤ç‰Œé¢„æµ‹è®­ç»ƒï¼Œå®ç°åŠ¨æ€è°ƒæ•´ä»¤ç‰Œé¢„æµ‹è·¨åº¦ï¼Œå‡å°‘åºåˆ—é•¿åº¦ä¾èµ–æ€§ã€‚</li>
<li>DCARåœ¨æµ‹è¯•é›†ä¸Šç›¸å¯¹äºä¼ ç»Ÿæ¨¡å‹å®ç°é«˜è¾¾72.27%çš„æ¸…æ™°åº¦æå‡å’Œ2.61å€çš„æ¨ç†é€Ÿåº¦åŠ å¿«ã€‚</li>
<li>ç»¼åˆè¯„ä»·æ”¯æŒDCARä½œä¸ºä¸‹ä¸€ä»£è¯­éŸ³åˆæˆç³»ç»Ÿçš„é€šç”¨åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-258c0b08b200ed02965e12c2a6d8c7fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3acefa8244e241188c2c1f94ba677c8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cab20a552dc6d3969740b9fb61eee85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91d587eec4cff4e8fcda9aa909c03554.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Analyzing-and-Fine-Tuning-Whisper-Models-for-Multilingual-Pilot-Speech-Transcription-in-the-Cockpit"><a href="#Analyzing-and-Fine-Tuning-Whisper-Models-for-Multilingual-Pilot-Speech-Transcription-in-the-Cockpit" class="headerlink" title="Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech   Transcription in the Cockpit"></a>Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech   Transcription in the Cockpit</h2><p><strong>Authors:Kartheek Kumar Reddy Nareddy, Sarah Ternus, Julia Niebling</strong></p>
<p>The developments in transformer encoder-decoder architectures have led to significant breakthroughs in machine translation, Automatic Speech Recognition (ASR), and instruction-based chat machines, among other applications. The pre-trained models were trained on vast amounts of generic data over a few epochs (fewer than five in most cases), resulting in their strong generalization capabilities. Nevertheless, the performance of these models does suffer when applied to niche domains like transcribing pilot speech in the cockpit, which involves a lot of specific vocabulary and multilingual conversations. This paper investigates and improves the transcription accuracy of cockpit conversations with Whisper models. We have collected around 85 minutes of cockpit simulator recordings and 130 minutes of interview recordings with pilots and manually labeled them. The speakers are middle aged men speaking both German and English. To improve the accuracy of transcriptions, we propose multiple normalization schemes to refine the transcripts and improve Word Error Rate (WER). We then employ fine-tuning to enhance ASR performance, utilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA). Hereby, WER decreased from 68.49 % (pretrained whisper Large model without normalization baseline) to 26.26% (finetuned whisper Large model with the proposed normalization scheme). </p>
<blockquote>
<p>éšç€Transformerç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„å‘å±•ï¼Œå…¶åœ¨æœºå™¨ç¿»è¯‘ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’ŒåŸºäºæŒ‡ä»¤çš„èŠå¤©æœºå™¨äººç­‰é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚é¢„è®­ç»ƒæ¨¡å‹ç»è¿‡å°‘é‡å‘¨æœŸï¼ˆå¤§å¤šæ•°æƒ…å†µä¸‹å°‘äºäº”ä¸ªå‘¨æœŸï¼‰çš„å¤§é‡é€šç”¨æ•°æ®è®­ç»ƒï¼Œè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“è¿™äº›æ¨¡å‹åº”ç”¨äºç‰¹å®šé¢†åŸŸï¼ˆå¦‚é©¾é©¶èˆ±å†…çš„é£è¡Œå‘˜è¯­éŸ³è½¬å½•ï¼‰æ—¶ï¼Œæ€§èƒ½ä¼šæœ‰æ‰€ä¸‹é™ï¼Œè¿™æ¶‰åŠå¤§é‡ç‰¹å®šè¯æ±‡å’Œå¤šè¯­ç§å¯¹è¯ã€‚æœ¬æ–‡ä½¿ç”¨whisperæ¨¡å‹ç ”ç©¶å¹¶æé«˜äº†é©¾é©¶èˆ±å¯¹è¯çš„è½¬å½•å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æ”¶é›†äº†å¤§çº¦85åˆ†é’Ÿçš„é©¾é©¶èˆ±æ¨¡æ‹Ÿå™¨å½•éŸ³å’Œ130åˆ†é’Ÿçš„é£è¡Œå‘˜è®¿è°ˆå½•éŸ³ï¼Œå¹¶è¿›è¡Œäº†æ‰‹åŠ¨æ ‡æ³¨ã€‚è¯´è¯äººæ˜¯ä¸­å¹´ç”·æ€§ï¼ŒåŒæ—¶ä½¿ç”¨å¾·è¯­å’Œè‹±è¯­ã€‚ä¸ºäº†æé«˜è½¬å½•çš„å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šç§å½’ä¸€åŒ–æ–¹æ¡ˆæ¥ä¼˜åŒ–è½¬å½•æœ¬å¹¶é™ä½å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨å¾®è°ƒæŠ€æœ¯æ¥æé«˜ASRæ€§èƒ½ï¼Œé‡‡ç”¨æ€§èƒ½é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•â€”â€”ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ã€‚ç”±æ­¤ï¼Œå•è¯é”™è¯¯ç‡ä»68.49%ï¼ˆæœªç»å½’ä¸€åŒ–çš„é¢„è®­ç»ƒwhisperå¤§å‹æ¨¡å‹åŸºçº¿ï¼‰é™ä½åˆ°26.26%ï¼ˆé‡‡ç”¨æ‰€æå½’ä¸€åŒ–æ–¹æ¡ˆçš„å¾®è°ƒwhisperå¤§å‹æ¨¡å‹ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21990v1">PDF</a> Computer Vision and Pattern Recognition (CVPR) 2025 Workshops</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨Whisperæ¨¡å‹æé«˜é©¾é©¶èˆ±å¯¹è¯è¯­éŸ³è½¬æ–‡å­—å‡†ç¡®æ€§ã€‚ä¸ºè§£å†³ç‰¹å®šé¢†åŸŸï¼ˆå¦‚é©¾é©¶èˆ±å¯¹è¯ï¼‰çš„ç‰¹æ®Šè¯æ±‡å’Œå¤šè¯­è¨€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ”¶é›†é©¾é©¶èˆ±æ¨¡æ‹Ÿå½•éŸ³ä¸é£è¡Œå‘˜è®¿è°ˆå½•éŸ³å¹¶è¿›è¡Œæ‰‹åŠ¨æ ‡æ³¨ã€‚é€šè¿‡å¤šé‡æ ‡å‡†åŒ–æ–¹æ¡ˆä¼˜åŒ–è½¬å½•å’Œæé«˜å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚ä½¿ç”¨æ€§èƒ½é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯Low-Rank Adaptationï¼ˆLoRAï¼‰æé«˜è¯­éŸ³è¯†åˆ«æ€§èƒ½ï¼Œä½¿WERå¤§å¹…é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜å‹å™¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„å‘å±•åœ¨æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å‹åœ¨é€šç”¨æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é©¾é©¶èˆ±è¯­éŸ³è½¬å½•é¢ä¸´ç‰¹å®šé¢†åŸŸçš„æŒ‘æˆ˜ï¼Œå¦‚ä¸“ä¸šè¯æ±‡å’Œå¤šè¯­è¨€äº¤æµã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ”¶é›†é©¾é©¶èˆ±æ¨¡æ‹Ÿå½•éŸ³å’Œé£è¡Œå‘˜è®¿è°ˆè¿›è¡Œæ‰‹åŠ¨æ ‡æ³¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æå‡ºäº†å¤šç§æ ‡å‡†åŒ–æ–¹æ¡ˆæ¥ä¼˜åŒ–è½¬å½•å’Œæé«˜å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>é‡‡ç”¨Low-Rank Adaptationï¼ˆLoRAï¼‰å¾®è°ƒæŠ€æœ¯æé«˜è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21990">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-296d816c1d838d547e699e5da6baee73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad2c94e4a930373175ffd3ed10392070.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad8796a73fba691ebcaebcdf437acd87.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb045aa8dd76a7289a01fc3c3ded1b89.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DeepTalk-Towards-Seamless-and-Smart-Speech-Interaction-with-Adaptive-Modality-Specific-MoE"><a href="#DeepTalk-Towards-Seamless-and-Smart-Speech-Interaction-with-Adaptive-Modality-Specific-MoE" class="headerlink" title="DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE"></a>DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive   Modality-Specific MoE</h2><p><strong>Authors:Hang Shao, Heting Gao, Yunhang Shen, Jiawei Chen, Lijiang Li, Zuwei Long, Bo Tong, Ke Li, Xing Sun</strong></p>
<p>Native multimodal large language models (MLLMs) restructure a single large language model (LLM) into a spoken language model (SLM) capable of both speech and text generation. Compared to modular and aligned MLLMs, native MLLMs preserve richer paralinguistic features such as emotion and prosody, and generate speech responses directly within the backbone LLM rather than using a separate speech decoder. This integration also results in lower response latency and smoother interaction. However, native MLLMs suffer from catastrophic forgetting and performance degradation because the available paired speech-text data is insufficient to support the pretraining of MLLMs compared to the vast amount of text data required to pretrain text LLMs. To address this issue, we propose DeepTalk, a framework for adaptive modality expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk first adaptively distinguishes modality experts according to their modality load within the LLM. Each modality expert then undergoes specialized single-modality training, followed by joint multimodal collaborative training. As a result, DeepTalk incurs only a 5.5% performance drop compared to the original LLM, which is significantly lower than the average performance drop of over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within 0.5 seconds, ensuring a seamless and intelligent speech interaction experience. Code and models are released at <a target="_blank" rel="noopener" href="https://github.com/talkking/DeepTalk">https://github.com/talkking/DeepTalk</a>. </p>
<blockquote>
<p>åŸç”Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å°†å•ä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡æ„ä¸ºèƒ½å¤ŸåŒæ—¶å¤„ç†è¯­éŸ³å’Œæ–‡æœ¬ç”Ÿæˆçš„å£è¯­è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ã€‚ä¸æ¨¡å—åŒ–å’Œå¯¹é½çš„MLLMsç›¸æ¯”ï¼ŒåŸç”ŸMLLMsä¿ç•™äº†æ›´ä¸°å¯Œçš„å‰¯è¯­è¨€ç‰¹å¾ï¼Œå¦‚æƒ…æ„Ÿå’Œè¯­è°ƒï¼Œå¹¶åœ¨ä¸»å¹²LLMå†…ç›´æ¥ç”Ÿæˆè¯­éŸ³å“åº”ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å•ç‹¬çš„è¯­éŸ³è§£ç å™¨ã€‚è¿™ç§é›†æˆè¿˜å¸¦æ¥äº†æ›´ä½çš„å“åº”å»¶è¿Ÿå’Œæ›´æµç•…çš„äº’åŠ¨ã€‚ç„¶è€Œï¼Œç”±äºå¯ç”¨çš„é…å¯¹è¯­éŸ³-æ–‡æœ¬æ•°æ®ä¸è¶³ä»¥æ”¯æŒMLLMsçš„é¢„è®­ç»ƒï¼Œä¸éœ€è¦é¢„è®­ç»ƒæ–‡æœ¬LLMçš„å¤§é‡æ–‡æœ¬æ•°æ®ç›¸æ¯”ï¼ŒåŸç”ŸMLLMsé­å—äº†ç¾éš¾æ€§çš„é—å¿˜å’Œæ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DeepTalkï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„çš„è‡ªé€‚åº”æ¨¡æ€ä¸“å®¶å­¦ä¹ æ¡†æ¶ã€‚DeepTalké¦–å…ˆæ ¹æ®LLMå†…çš„æ¨¡æ€è´Ÿè½½è‡ªé€‚åº”åœ°åŒºåˆ†æ¨¡æ€ä¸“å®¶ã€‚ç„¶åï¼Œæ¯ä¸ªæ¨¡æ€ä¸“å®¶è¿›è¡Œä¸“é—¨çš„å•æ¨¡æ€è®­ç»ƒï¼Œæ¥ç€è¿›è¡Œè”åˆå¤šæ¨¡æ€ååŒè®­ç»ƒã€‚å› æ­¤ï¼Œä¸åŸå§‹LLMç›¸æ¯”ï¼ŒDeepTalkåªä¼šå¯¼è‡´5.5%çš„æ€§èƒ½ä¸‹é™ï¼Œè¿™è¿œä½äºåŸç”ŸMLLMsï¼ˆå¦‚GLM-4-Voiceï¼‰é€šå¸¸å‡ºç°çš„è¶…è¿‡20%çš„å¹³å‡æ€§èƒ½ä¸‹é™ï¼Œå¹¶ä¸æ¨¡å—åŒ–MLLMsç›¸å½“ã€‚åŒæ—¶ï¼Œç«¯åˆ°ç«¯å¯¹è¯å»¶è¿Ÿä¿æŒåœ¨0.5ç§’å†…ï¼Œç¡®ä¿æ— ç¼ä¸”æ™ºèƒ½çš„è¯­éŸ³äº¤äº’ä½“éªŒã€‚ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/talkking/DeepTalk%E3%80%82">https://github.com/talkking/DeepTalkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21864v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ¨¡æ€æ··åˆä¸“å®¶å­¦ä¹ æ¶æ„ï¼Œæå‡ºDeepTalkæ¡†æ¶è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯­éŸ³å’Œæ–‡æœ¬ç”Ÿæˆæ–¹é¢çš„ç¼ºé™·ã€‚é€šè¿‡è‡ªé€‚åº”åŒºåˆ†æ¨¡æ€ä¸“å®¶ï¼Œå¹¶ç»“åˆå•ä¸€æ¨¡æ€è®­ç»ƒå’Œè”åˆå¤šæ¨¡æ€åä½œè®­ç»ƒï¼Œå®ç°äº†ä¸åŸç”Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“çš„æ•ˆèƒ½è¡¨ç°ï¼Œä½†é™ä½äº†æ¨¡å‹é—å¿˜ç°è±¡å¹¶æé«˜äº†æ€§èƒ½ç¨³å®šæ€§ã€‚é€šè¿‡è¿™ä¸€æŠ€æœ¯ä¼˜åŒ–ï¼Œç¡®ä¿äº†æµç•…çš„æ™ºèƒ½åŒ–è¯­éŸ³äº¤äº’ä½“éªŒã€‚ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŸç”Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æ•´åˆè¯­éŸ³å’Œæ–‡æœ¬ç”ŸæˆåŠŸèƒ½ï¼Œè¡¨ç°å‡ºä¸°å¯Œçš„è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚</li>
<li>DeepTalkæ¡†æ¶åŸºäºæ¨¡æ€æ··åˆä¸“å®¶å­¦ä¹ æ¶æ„æå‡ºï¼Œä»¥è§£å†³è¯­éŸ³ä¸æ–‡æœ¬ç”Ÿæˆèåˆçš„é—®é¢˜ã€‚</li>
<li>DeepTalkæ¡†æ¶åŒ…æ‹¬è‡ªé€‚åº”åŒºåˆ†æ¨¡æ€ä¸“å®¶ã€å•ä¸€æ¨¡æ€è®­ç»ƒå’Œè”åˆå¤šæ¨¡æ€åä½œè®­ç»ƒç­‰ç¯èŠ‚ã€‚</li>
<li>DeepTalkå‡å°‘äº†åŸç”Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æŸå¤±è‡³ä»…5.5%ï¼Œä½äºä¼ ç»ŸåŸç”Ÿå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹³å‡æ€§èƒ½æŸå¤±ï¼ˆè¶…è¿‡20%ï¼‰ã€‚</li>
<li>è¯¥æŠ€æœ¯ä¼˜åŒ–é™ä½äº†æ¨¡å‹é—å¿˜ç°è±¡ï¼Œæé«˜äº†æ€§èƒ½ç¨³å®šæ€§ã€‚</li>
<li>é€šè¿‡ç«¯åˆ°ç«¯çš„å¯¹è¯å»¶è¿Ÿæ§åˆ¶åœ¨0.5ç§’å†…ï¼Œä¿è¯äº†æ™ºèƒ½è¯­éŸ³äº¤äº’çš„æµç•…ä½“éªŒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b751d851e66b8602f3667e66b557295.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db036c150f14fdfbbe04b38108d8dae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa288d4ac1e5c4dd8a488e54ac864e17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d9c1c5da5d60fb6408812c5d1c19025.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="IndexTTS2-A-Breakthrough-in-Emotionally-Expressive-and-Duration-Controlled-Auto-Regressive-Zero-Shot-Text-to-Speech"><a href="#IndexTTS2-A-Breakthrough-in-Emotionally-Expressive-and-Duration-Controlled-Auto-Regressive-Zero-Shot-Text-to-Speech" class="headerlink" title="IndexTTS2: A Breakthrough in Emotionally Expressive and   Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech"></a>IndexTTS2: A Breakthrough in Emotionally Expressive and   Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</h2><p><strong>Authors:Siyi Zhou, Yiquan Zhou, Yi He, Xun Zhou, Jinchao Wang, Wei Deng, Jingchen Shu</strong></p>
<p>Large-scale text-to-speech (TTS) models are typically categorized into autoregressive and non-autoregressive systems. Although autoregressive systems exhibit certain advantages in speech naturalness, their token-by-token generation mechanism makes it difficult to precisely control the duration of synthesized speech. This is a key limitation in applications such as video dubbing that require strict audio-visual synchronization. This paper introduces IndexTTS2, which proposes a novel and autoregressive-model-friendly method for speech duration control. The method supports two generation modes: one allows explicit specification of the number of generated tokens for precise duration control; the other does not require manual input and lets the model freely generate speech while preserving prosodic characteristics from the input prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional expression and speaker identity, enabling independent control of timbre and emotion. In the zero-shot setting, the model can perfectly reproduce the emotional characteristics of the input prompt. Users may also provide a separate emotion prompt, even from a different speaker, allowing the model to reconstruct the target timbre while conveying the desired emotion. To enhance clarity during strong emotional expressions, we incorporate GPT latent representations to improve speech stability. Meanwhile, to lower the barrier for emotion control, we design a soft instruction mechanism based on textual descriptions by fine-tuning Qwen3. This enables effective guidance of speech generation with desired emotional tendencies using natural language input. Experimental results demonstrate that IndexTTS2 outperforms existing state-of-the-art zero-shot TTS models in word error rate, speaker similarity, and emotional fidelity. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹é€šå¸¸åˆ†ä¸ºè‡ªå›å½’å’Œéè‡ªå›å½’ç³»ç»Ÿä¸¤ç±»ã€‚å°½ç®¡è‡ªå›å½’ç³»ç»Ÿåœ¨è¯­éŸ³è‡ªç„¶åº¦æ–¹é¢å…·æœ‰ä¸€å®šçš„ä¼˜åŠ¿ï¼Œä½†å…¶é€ä¸ªæ ‡è®°çš„ç”Ÿæˆæœºåˆ¶ä½¿å¾—éš¾ä»¥ç²¾ç¡®æ§åˆ¶åˆæˆè¯­éŸ³çš„æŒç»­æ—¶é—´ã€‚åœ¨è¯¸å¦‚éœ€è¦ä¸¥æ ¼éŸ³è§†é¢‘åŒæ­¥çš„è§†é¢‘é…éŸ³ç­‰åº”ç”¨ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³é”®é™åˆ¶ã€‚æœ¬æ–‡ä»‹ç»äº†IndexTTS2ï¼Œå®ƒæå‡ºäº†ä¸€ç§æ–°é¢–ä¸”é€‚ç”¨äºè‡ªå›å½’æ¨¡å‹çš„è¯­éŸ³æŒç»­æ—¶é—´æ§åˆ¶æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ”¯æŒä¸¤ç§ç”Ÿæˆæ¨¡å¼ï¼šä¸€ç§å…è®¸æ˜ç¡®æŒ‡å®šç”Ÿæˆçš„æ ‡è®°æ•°é‡ä»¥å®ç°ç²¾ç¡®çš„æŒç»­æ—¶é—´æ§åˆ¶ï¼›å¦ä¸€ç§åˆ™ä¸éœ€è¦æ‰‹åŠ¨è¾“å…¥ï¼Œè®©æ¨¡å‹åœ¨ä¿ç•™è¾“å…¥æç¤ºçš„éŸµå¾‹ç‰¹å¾çš„åŒæ—¶è‡ªç”±ç”Ÿæˆè¯­éŸ³ã€‚æ­¤å¤–ï¼ŒIndexTTS2å®ç°äº†æƒ…æ„Ÿè¡¨è¾¾å’Œè¯´è¯äººèº«ä»½çš„è§£è€¦ï¼Œèƒ½å¤Ÿå®ç°éŸ³è°ƒå’Œæƒ…æ„Ÿçš„ç‹¬ç«‹æ§åˆ¶ã€‚åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œæ¨¡å‹å¯ä»¥å®Œç¾åœ°å†ç°è¾“å…¥æç¤ºçš„æƒ…æ„Ÿç‰¹å¾ã€‚ç”¨æˆ·è¿˜å¯ä»¥æä¾›æ¥è‡ªä¸åŒè¯´è¯äººçš„å•ç‹¬æƒ…æ„Ÿæç¤ºï¼Œè®©æ¨¡å‹åœ¨ä¼ è¾¾æ‰€éœ€æƒ…æ„Ÿçš„åŒæ—¶é‡å»ºç›®æ ‡éŸ³è°ƒã€‚ä¸ºäº†æé«˜å¼ºçƒˆæƒ…æ„Ÿè¡¨è¾¾æ—¶çš„æ¸…æ™°åº¦ï¼Œæˆ‘ä»¬ç»“åˆäº†GPTçš„æ½œåœ¨è¡¨ç¤ºæ¥æé«˜è¯­éŸ³çš„ç¨³å®šæ€§ã€‚åŒæ—¶ï¼Œä¸ºäº†é™ä½æƒ…æ„Ÿæ§åˆ¶çš„éšœç¢ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæ–‡æœ¬æè¿°çš„è½¯æŒ‡ä»¤æœºåˆ¶ï¼Œé€šè¿‡å¾®è°ƒQwen3æ¥å®ç°ã€‚è¿™ä½¿ç”¨è‡ªç„¶è¯­è¨€è¾“å…¥æœ‰æ•ˆåœ°å¼•å¯¼äº†å…·æœ‰æ‰€éœ€æƒ…æ„Ÿå€¾å‘çš„è¯­éŸ³ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIndexTTS2åœ¨è¯é”™è¯¯ç‡ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œæƒ…æ„Ÿä¿çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21619v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†IndexTTS2ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰çš„æ–°çš„è‡ªåŠ¨å›å½’æ¨¡å‹å‹å¥½æ–¹æ³•ã€‚è¯¥æ–¹æ³•å®ç°äº†å¯¹åˆæˆè¯­éŸ³çš„æŒç»­æ—¶é—´çš„ç²¾ç¡®æ§åˆ¶ï¼Œæ”¯æŒä¸¤ç§ä¸åŒçš„ç”Ÿæˆæ¨¡å¼ï¼Œå¯åœ¨ä¿è¯è¯­éŸ³éŸµå¾‹ç‰¹æ€§çš„åŒæ—¶è‡ªç”±ç”Ÿæˆè¯­éŸ³ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å®ç°äº†æƒ…æ„Ÿå’Œè¯´è¯äººèº«ä»½çš„è§£è€¦ï¼Œèƒ½å•ç‹¬æ§åˆ¶éŸ³è‰²å’Œæƒ…æ„Ÿè¡¨è¾¾ã€‚åœ¨é›¶æ ·æœ¬æƒ…å¢ƒä¸‹ï¼Œè¯¥æ¨¡å‹å¯å®Œç¾å†ç°è¾“å…¥æç¤ºçš„æƒ…æ„Ÿç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IndexTTS2æ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œé€‚ç”¨äºè‡ªåŠ¨å›å½’ç³»ç»Ÿã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿå®ç°ç²¾ç¡®çš„è¯­éŸ³æŒç»­æ—¶é—´æ§åˆ¶ï¼Œæ”¯æŒä¸¤ç§ç”Ÿæˆæ¨¡å¼ã€‚</li>
<li>IndexTTS2åœ¨ä¿æŒè¯­éŸ³éŸµå¾‹ç‰¹æ€§çš„åŒæ—¶ï¼Œå¯ä»¥è‡ªç”±åœ°ç”Ÿæˆè¯­éŸ³ã€‚</li>
<li>æƒ…æ„Ÿå’Œè¯´è¯äººèº«ä»½åœ¨IndexTTS2ä¸­å¾—ä»¥è§£è€¦ï¼Œå¯ä»¥ç‹¬ç«‹æ§åˆ¶ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬æƒ…å¢ƒä¸‹ï¼ŒIndexTTS2èƒ½å¤Ÿå®Œç¾å¤åˆ¶è¾“å…¥æç¤ºçš„æƒ…æ„Ÿç‰¹å¾ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥æä¾›å•ç‹¬çš„æƒ…æ„Ÿæç¤ºï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¼ è¾¾æœŸæœ›æƒ…æ„Ÿçš„åŒæ—¶é‡å»ºç›®æ ‡éŸ³è‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-832dfad02b23284234111b0536be15f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d936a9a22294c6c33abbc71fda8f44db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fea28df1538ddcf215c187aa331133b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df89d8f30b45974e1e107ac15105b64c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AI-Generated-Song-Detection-via-Lyrics-Transcripts"><a href="#AI-Generated-Song-Detection-via-Lyrics-Transcripts" class="headerlink" title="AI-Generated Song Detection via Lyrics Transcripts"></a>AI-Generated Song Detection via Lyrics Transcripts</h2><p><strong>Authors:Markus Frohmann, Elena V. Epure, Gabriel Meseguer-Brocal, Markus Schedl, Romain Hennequin</strong></p>
<p>The recent rise in capabilities of AI-based music generation tools has created an upheaval in the music industry, necessitating the creation of accurate methods to detect such AI-generated content. This can be done using audio-based detectors; however, it has been shown that they struggle to generalize to unseen generators or when the audio is perturbed. Furthermore, recent work used accurate and cleanly formatted lyrics sourced from a lyrics provider database to detect AI-generated music. However, in practice, such perfect lyrics are not available (only the audio is); this leaves a substantial gap in applicability in real-life use cases. In this work, we instead propose solving this gap by transcribing songs using general automatic speech recognition (ASR) models. We do this using several detectors. The results on diverse, multi-genre, and multi-lingual lyrics show generally strong detection performance across languages and genres, particularly for our best-performing model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that our method is more robust than state-of-the-art audio-based ones when the audio is perturbed in different ways and when evaluated on different music generators. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/deezer/robust-AI-lyrics-detection">https://github.com/deezer/robust-AI-lyrics-detection</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼ŒåŸºäºäººå·¥æ™ºèƒ½çš„éŸ³ä¹ç”Ÿæˆå·¥å…·çš„èƒ½åŠ›æå‡åœ¨éŸ³ä¹äº§ä¸šä¸­å¼•èµ·äº†å·¨å¤§å˜é©ï¼Œå› æ­¤éœ€è¦åˆ›å»ºå‡†ç¡®çš„æ–¹æ³•æ¥æ£€æµ‹è¿™ç§AIç”Ÿæˆçš„å†…å®¹ã€‚è¿™å¯ä»¥é€šè¿‡åŸºäºéŸ³é¢‘çš„æ£€æµ‹å™¨æ¥å®Œæˆï¼›ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜ï¼Œå®ƒä»¬åœ¨æ¨å¹¿åˆ°æœªè§è¿‡çš„ç”Ÿæˆå™¨æˆ–éŸ³é¢‘å—åˆ°å¹²æ‰°æ—¶è¡¨ç°æŒ£æ‰ã€‚æ­¤å¤–ï¼Œè¿‘æœŸçš„å·¥ä½œä½¿ç”¨ä»æ­Œè¯æä¾›å•†æ•°æ®åº“ä¸­çš„å‡†ç¡®ä¸”æ ¼å¼æ•´æ´çš„æ­Œè¯æ¥æ£€æµ‹AIç”Ÿæˆçš„éŸ³ä¹ã€‚ç„¶è€Œï¼Œå®é™…ä¸Šï¼Œè¿™æ ·çš„å®Œç¾æ­Œè¯å¹¶ä¸å¯ç”¨ï¼ˆåªæœ‰éŸ³é¢‘ï¼‰ï¼›è¿™åœ¨å®é™…ä½¿ç”¨åœºæ™¯ä¸­ç•™ä¸‹äº†å·¨å¤§çš„é€‚ç”¨æ€§å·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æè®®é€šè¿‡åˆ©ç”¨é€šç”¨çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹æ¥è½¬å½•æ­Œæ›²æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨äº†å¤šç§æ£€æµ‹å™¨æ¥å®Œæˆæ­¤ä»»åŠ¡ã€‚åœ¨å¤šæ ·åŒ–ã€å¤šé£æ ¼å’Œå¤šè¯­è¨€çš„æ­Œè¯ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¯­è¨€å’Œé£æ ¼ä¸Šéƒ½æœ‰å¾ˆå¼ºçš„æ£€æµ‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨Whisper large-v2å’ŒLLM2VecåµŒå…¥çš„æœ€ä½³æ€§èƒ½æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨éŸ³é¢‘ä»¥ä¸åŒæ–¹å¼å—åˆ°å¹²æ‰°æ—¶ä»¥åŠåœ¨ä¸åŒçš„éŸ³ä¹ç”Ÿæˆå™¨ä¸Šè¿›è¡Œè¯„ä¼°æ—¶ï¼Œæ¯”æœ€æ–°çš„éŸ³é¢‘æ£€æµ‹æ–¹æ³•æ›´ç¨³å¥ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/deezer/robust-AI-lyrics-detection%E3%80%82">https://github.com/deezer/robust-AI-lyrics-detectionã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18488v2">PDF</a> Accepted to ISMIR 2025</p>
<p><strong>Summary</strong><br>AIéŸ³ä¹ç”Ÿæˆå·¥å…·çš„å´›èµ·å¯¹éŸ³ä¹äº§ä¸šé€ æˆäº†å†²å‡»ï¼Œå› æ­¤éœ€è¦å¼€å‘å‡†ç¡®çš„æ–¹æ³•æ¥æ£€æµ‹AIç”Ÿæˆçš„å†…å®¹ã€‚ç°æœ‰éŸ³é¢‘æ£€æµ‹å™¨å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥é€‚åº”æœªè§è¿‡çš„ç”Ÿæˆå™¨æˆ–éŸ³é¢‘æ‰°åŠ¨æƒ…å†µã€‚ç ”ç©¶é‡‡ç”¨ä»æ­Œè¯æä¾›å•†æ•°æ®åº“è·å–çš„å‡†ç¡®ã€æ•´æ´çš„æ­Œè¯è¿›è¡Œæ£€æµ‹ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å®Œç¾æ­Œè¯å¹¶ä¸å¯ç”¨ï¼Œå­˜åœ¨åº”ç”¨ä¸Šçš„ç©ºç™½ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºä½¿ç”¨é€šç”¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹è½¬å½•æ­Œæ›²è¿›è¡Œæ£€æµ‹ã€‚ä½¿ç”¨å¤šä¸ªæ£€æµ‹å™¨åœ¨ä¸åŒè¯­è¨€ã€ä¸åŒæµæ´¾çš„éŸ³ä¹ä¸Šå–å¾—äº†è‰¯å¥½çš„æ£€æµ‹æ•ˆæœã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨éŸ³é¢‘å—åˆ°ä¸åŒå¹²æ‰°å’Œä¸åŒéŸ³ä¹ç”Ÿæˆå™¨ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰éŸ³é¢‘æ£€æµ‹æ–¹æ³•ã€‚ç›¸å…³ç ”ç©¶ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIéŸ³ä¹ç”Ÿæˆå·¥å…·çš„è¿›æ­¥å¯¹éŸ³ä¹äº§ä¸šäº§ç”Ÿäº†å½±å“ï¼Œéœ€è¦å¼€å‘å‡†ç¡®çš„æ–¹æ³•æ¥æ£€æµ‹AIç”Ÿæˆçš„å†…å®¹ã€‚</li>
<li>ç°æœ‰éŸ³é¢‘æ£€æµ‹å™¨åœ¨é€‚åº”ä¸åŒç”Ÿæˆå™¨å’ŒéŸ³é¢‘æ‰°åŠ¨æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨ä»æ­Œè¯æä¾›å•†æ•°æ®åº“è·å–çš„å‡†ç¡®æ­Œè¯è¿›è¡Œæ£€æµ‹ï¼Œä½†å®é™…åº”ç”¨ä¸­å®Œç¾æ­Œè¯çš„è·å–å¹¶ä¸å¯è¡Œã€‚</li>
<li>ä¸ºè§£å†³å®é™…åº”ç”¨ä¸­çš„ç©ºç™½ï¼Œç ”ç©¶æå‡ºä½¿ç”¨é€šç”¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹è¿›è¡Œæ­Œæ›²è½¬å½•å’Œæ£€æµ‹ã€‚</li>
<li>ä½¿ç”¨å¤šä¸ªæ£€æµ‹å™¨å–å¾—äº†è‰¯å¥½çš„æ£€æµ‹æ•ˆæœï¼Œé€‚ç”¨äºä¸åŒè¯­è¨€å’Œæµæ´¾çš„éŸ³ä¹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨éŸ³é¢‘å—åˆ°ä¸åŒå¹²æ‰°å’Œä¸åŒéŸ³ä¹ç”Ÿæˆå™¨ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰éŸ³é¢‘æ£€æµ‹æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18488">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c4511adea10f9ed2e46e651ff4d0cec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcc5c2b57fc1803f3eddbf4349a0fc8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed095d8a11541221e508c2162ea3dbac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea722d81e536beecaf13627941a77019.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d97d311c6beb6632a08b8f2a690e77c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd89225e8e7e04a2b3dcf7c915937310.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20f6db03d3cb4937f7653b40c8f63c6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="State-Space-Models-in-Efficient-Whispered-and-Multi-dialect-Speech-Recognition"><a href="#State-Space-Models-in-Efficient-Whispered-and-Multi-dialect-Speech-Recognition" class="headerlink" title="State-Space Models in Efficient Whispered and Multi-dialect Speech   Recognition"></a>State-Space Models in Efficient Whispered and Multi-dialect Speech   Recognition</h2><p><strong>Authors:Aref Farhadipour, Homayoon Beigi, Volker Dellwo, Hadi Veisi</strong></p>
<p>Whispered speech recognition presents significant challenges for conventional automatic speech recognition systems, particularly when combined with dialect variation. However, utilizing an efficient method to solve this problem using a low-range dataset and processing load is beneficial. This paper proposes a solution using a Mamba-based state-space model and four fine-tuned self-supervised models consisting of Wav2Vec2, WavLM, HuBERT, and Whisper to address the dual challenges of whispered speech and dialect diversity. Based on our knowledge, this represents the best performance reported on the wTIMIT and CHAINS datasets for whispered speech recognition. We trained the models using whispered and normal speech data across Singaporean, US, and Irish dialects. The findings demonstrated that utilizing the proposed Mamba-based model could work as a highly efficient model trained with low amounts of whispered data to simultaneously work on whispered and normal speech recognition. The code for this work is freely available. </p>
<blockquote>
<p>è½»å£°è¯­éŸ³è¯†åˆ«å¯¹äºä¼ ç»Ÿçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ¥è¯´å‘ˆç°å‡ºé‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å½“ä¸æ–¹è¨€å˜åŒ–ç›¸ç»“åˆæ—¶ã€‚ç„¶è€Œï¼Œåˆ©ç”¨æœ‰æ•ˆçš„æ–¹æ³•è§£å†³æ­¤é—®é¢˜ï¼Œä½¿ç”¨ä½èŒƒå›´æ•°æ®é›†å’Œå¤„ç†è´Ÿè½½æ˜¯éå¸¸æœ‰ç›Šçš„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºMambaçš„çŠ¶æ€ç©ºé—´æ¨¡å‹å’Œå››ä¸ªç»è¿‡å¾®è°ƒçš„è‡ªç›‘ç£æ¨¡å‹ï¼ˆåŒ…æ‹¬Wav2Vec2ã€WavLMã€HuBERTå’ŒWhisperï¼‰çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥è§£å†³è½»å£°å’Œæ–¹è¨€å¤šæ ·æ€§æ‰€å¸¦æ¥çš„åŒé‡æŒ‘æˆ˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™åœ¨wTIMITå’ŒCHAINSæ•°æ®é›†ä¸Šä»£è¡¨äº†è½»å£°è¯†åˆ«çš„æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨æ–°åŠ å¡ã€ç¾å›½å’Œçˆ±å°”å…°æ–¹è¨€çš„è½»å£°å’Œæ­£å¸¸è¯­éŸ³æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œäº†è®­ç»ƒã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨åŸºäºMambaçš„æ¨¡å‹å¯ä»¥ä½œä¸ºç”¨å°‘é‡è½»å£°æ•°æ®è®­ç»ƒçš„é«˜æ•ˆæ¨¡å‹ï¼Œå¯åŒæ—¶ç”¨äºè½»å£°å’Œæ­£å¸¸è¯­éŸ³è¯†åˆ«ã€‚è¯¥å·¥ä½œçš„ä»£ç å¯å…è´¹è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16969v2">PDF</a> paper is in 4+1 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºMambaçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œç»“åˆå››ç§ç²¾ç»†è°ƒæ•´çš„è‡ªç›‘ç£æ¨¡å‹ï¼ˆWav2Vec2ã€WavLMã€HuBERTå’ŒWhisperï¼‰ï¼Œè§£å†³ä½å£°è¯­éŸ³å’Œæ–¹è¨€å¤šæ ·æ€§å¸¦æ¥çš„åŒé‡æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹åœ¨wTIMITå’ŒCHAINSæ•°æ®é›†ä¸Šçš„ä½å£°è¯­éŸ³è¯†åˆ«è¡¨ç°æœ€ä½³ã€‚é€šè¿‡åœ¨æ–°åŠ å¡æ–¹è¨€ã€ç¾å›½æ–¹è¨€å’Œçˆ±å°”å…°æ–¹è¨€çš„è½»å£°å’Œæ­£å¸¸è¯­éŸ³æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨åŸºäºMambaçš„æ¨¡å‹èƒ½ä»¥é«˜æ•ˆç‡ä½¿ç”¨å°‘é‡çš„è½»å£°æ•°æ®ï¼ŒåŒæ—¶å¤„ç†è½»å£°å’Œæ­£å¸¸è¯­éŸ³è¯†åˆ«ã€‚è¯¥å·¥ä½œçš„ä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä½å£°è¯­éŸ³è¯†åˆ«å¯¹äºä¼ ç»Ÿçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ä¸æ–¹è¨€å˜åŒ–ç›¸ç»“åˆæ—¶ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºMambaçš„çŠ¶æ€ç©ºé—´æ¨¡å‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ¨¡å‹åœ¨è§£å†³ä½å£°è¯­éŸ³å’Œæ–¹è¨€å¤šæ ·æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>åœ¨wTIMITå’ŒCHAINSæ•°æ®é›†ä¸Šï¼Œè¯¥æ¨¡å‹çš„è¡¨ç°æ˜¯ç›®å‰å·²çŸ¥çš„æœ€ä½³ã€‚</li>
<li>æ¨¡å‹åœ¨è½»å£°å’Œæ­£å¸¸è¯­éŸ³æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬æ–°åŠ å¡æ–¹è¨€ã€ç¾å›½æ–¹è¨€å’Œçˆ±å°”å…°æ–¹è¨€ã€‚</li>
<li>åˆ©ç”¨å°‘é‡è½»å£°æ•°æ®è®­ç»ƒçš„æ¨¡å‹å¯ä»¥éå¸¸é«˜æ•ˆã€‚</li>
<li>è¯¥æ¨¡å‹å¯ä»¥åŒæ—¶å¤„ç†ä½å£°å’Œæ­£å¸¸è¯­éŸ³è¯†åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16969">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f7834d88195e84fd47be9f39328eebf4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb99dff26cfc3e5cdc5d81899b576660.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e98d4c76bf2eacd25bb9af1234564937.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27676ef516c2e380ba8f137801c84950.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Double-Entendre-Robust-Audio-Based-AI-Generated-Lyrics-Detection-via-Multi-View-Fusion"><a href="#Double-Entendre-Robust-Audio-Based-AI-Generated-Lyrics-Detection-via-Multi-View-Fusion" class="headerlink" title="Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via   Multi-View Fusion"></a>Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via   Multi-View Fusion</h2><p><strong>Authors:Markus Frohmann, Gabriel Meseguer-Brocal, Markus Schedl, Elena V. Epure</strong></p>
<p>The rapid advancement of AI-based music generation tools is revolutionizing the music industry but also posing challenges to artists, copyright holders, and providers alike. This necessitates reliable methods for detecting such AI-generated content. However, existing detectors, relying on either audio or lyrics, face key practical limitations: audio-based detectors fail to generalize to new or unseen generators and are vulnerable to audio perturbations; lyrics-based methods require cleanly formatted and accurate lyrics, unavailable in practice. To overcome these limitations, we propose a novel, practically grounded approach: a multimodal, modular late-fusion pipeline that combines automatically transcribed sung lyrics and speech features capturing lyrics-related information within the audio. By relying on lyrical aspects directly from audio, our method enhances robustness, mitigates susceptibility to low-level artifacts, and enables practical applicability. Experiments show that our method, DE-detect, outperforms existing lyrics-based detectors while also being more robust to audio perturbations. Thus, it offers an effective, robust solution for detecting AI-generated music in real-world scenarios. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/deezer/robust-AI-lyrics-detection">https://github.com/deezer/robust-AI-lyrics-detection</a>. </p>
<blockquote>
<p>åŸºäºäººå·¥æ™ºèƒ½çš„éŸ³ä¹ç”Ÿæˆå·¥å…·çš„å¿«é€Ÿå‘å±•æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜éŸ³ä¹è¡Œä¸šï¼ŒåŒæ—¶ä¹Ÿç»™è‰ºæœ¯å®¶ã€ç‰ˆæƒæ‰€æœ‰è€…å’Œæä¾›å•†å¸¦æ¥äº†æŒ‘æˆ˜ã€‚è¿™è¿«åˆ‡éœ€è¦å¯é çš„æ–¹æ³•æ¥æ£€æµ‹è¿™ç§AIç”Ÿæˆçš„å†…å®¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ£€æµ‹å™¨ï¼Œæ— è®ºæ˜¯ä¾èµ–éŸ³é¢‘è¿˜æ˜¯æ­Œè¯ï¼Œéƒ½é¢ä¸´å…³é”®çš„å®é™…å±€é™æ€§ï¼šåŸºäºéŸ³é¢‘çš„æ£€æµ‹å™¨æ— æ³•æ¨å¹¿åˆ°æ–°çš„æˆ–æœªè§è¿‡çš„ç”Ÿæˆå™¨ï¼Œå¹¶å®¹æ˜“å—åˆ°éŸ³é¢‘å¹²æ‰°çš„å½±å“ï¼›åŸºäºæ­Œè¯çš„æ–¹æ³•éœ€è¦æ•´æ´æ ¼å¼åŒ–å’Œå‡†ç¡®çš„æ­Œè¯ï¼Œè¿™åœ¨å®è·µä¸­æ˜¯å¾—ä¸åˆ°çš„ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ã€å®ç”¨çš„æ–¹æ³•ï¼šå¤šæ¨¡æ€ã€æ¨¡å—åŒ–å»¶è¿Ÿèåˆç®¡é“ï¼Œå®ƒç»“åˆäº†è‡ªåŠ¨è½¬å½•çš„æ­Œå”±æ­Œè¯å’Œæ•æ‰éŸ³é¢‘ä¸­æ­Œè¯ç›¸å…³ä¿¡æ¯çš„è¯­éŸ³ç‰¹å¾ã€‚é€šè¿‡ç›´æ¥ä»éŸ³é¢‘ä¸­ä¾èµ–æ­Œè¯æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†ç¨³å¥æ€§ï¼Œå‡è½»äº†å¯¹ä½çº§ä¼ªé€ çš„æ•æ„Ÿæ€§ï¼Œå¹¶å®ç°äº†å®é™…åº”ç”¨çš„å¯è¡Œæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•DE-detectä¼˜äºç°æœ‰çš„åŸºäºæ­Œè¯çš„æ£€æµ‹å™¨ï¼ŒåŒæ—¶å¯¹éŸ³é¢‘å¹²æ‰°æ›´ç¨³å¥ã€‚å› æ­¤ï¼Œå®ƒä¸ºæ£€æµ‹ç°å®åœºæ™¯ä¸­çš„AIç”ŸæˆéŸ³ä¹æä¾›äº†æœ‰æ•ˆä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/deezer">https://github.com/deezer</a> æ‰¾åˆ°ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„AIæ­Œè¯æ£€æµ‹ç½‘ç«™å¹¶æä¾›äº†æºä»£ç å’Œé“¾æ¥ç”¨äºè¿›ä¸€æ­¥ç ”ç©¶å’Œå®è·µåº”ç”¨ã€‚æˆ‘ä»¬çš„è®ºæ–‡ä¹Ÿè¢«æ”¾åœ¨äº†å¼€æºå¹³å°ä»¥ä¾›ä¸‹è½½é˜…è¯»å’Œç ”ç©¶ã€‚å¦‚æœæ‚¨å¯¹æˆ‘ä»¬çš„ç ”ç©¶æ„Ÿå…´è¶£å¹¶å¸Œæœ›äº†è§£æ›´å¤šå…³äºå¦‚ä½•å®æ–½æ­¤æ£€æµ‹å™¨çš„ä¿¡æ¯æˆ–æœ‰ä»»ä½•å…¶ä»–ç›¸å…³é—®é¢˜æˆ–æƒ³æ³•éœ€è¦è®¨è®ºå’Œäº¤æµçš„è¯è¯·éšæ—¶ä¸æˆ‘ä»¬è”ç³»æˆ‘ä»¬ä¼šå¾ˆä¹æ„ä¸ºæ‚¨æä¾›å¸®åŠ©å¹¶è®¨è®ºè¿›ä¸€æ­¥åˆä½œçš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15981v2">PDF</a> Accepted to ACL 2025 Findings</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½éŸ³ä¹ç”Ÿæˆå·¥å…·çš„å¿«é€Ÿå‘å±•æ­£åœ¨é¢ è¦†éŸ³ä¹è¡Œä¸šï¼Œå¯¹è‰ºæœ¯å®¶ã€ç‰ˆæƒæ‰€æœ‰è€…å’Œæä¾›å•†å¸¦æ¥æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§æ–°å‹çš„AIç”ŸæˆéŸ³ä¹å†…å®¹æ£€æµ‹æ–¹æ³•DE-detectï¼Œè¯¥æ–¹æ³•ç»“åˆäº†è‡ªåŠ¨è½¬å½•çš„æ­Œè¯å’ŒéŸ³é¢‘ä¸­çš„æ­Œè¯ç›¸å…³ä¿¡æ¯ç‰¹å¾ã€‚è¯¥æ–¹æ³•æé«˜äº†ç¨³å¥æ€§ï¼Œå‡è½»äº†ä½çº§ä¼ªé€ çš„å¹²æ‰°ï¼Œå¹¶å¯åœ¨å®è·µä¸­åº”ç”¨ã€‚DE-detectæ–¹æ³•ä¼˜äºç°æœ‰çš„æ­Œè¯æ£€æµ‹æ–¹æ³•ï¼Œå¹¶å¯¹éŸ³é¢‘æ‰°åŠ¨å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ï¼Œä¸ºçœŸå®åœºæ™¯ä¸­æ£€æµ‹AIç”Ÿæˆçš„éŸ³ä¹æä¾›äº†æœ‰æ•ˆä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIéŸ³ä¹ç”Ÿæˆå·¥å…·çš„å¿«é€Ÿå‘å±•æ­£åœ¨æ”¹å˜éŸ³ä¹è¡Œä¸šï¼Œå¯¹å„æ–¹å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>ç›®å‰å­˜åœ¨çš„æ£€æµ‹å·¥å…·å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•æœ‰æ•ˆæ£€æµ‹æ–°çš„æˆ–æœªçŸ¥çš„ç”Ÿæˆå™¨ï¼Œä¸”å®¹æ˜“å—åˆ°éŸ³é¢‘å¹²æ‰°ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼ã€æ¨¡å—åŒ–å»¶è¿Ÿèåˆç®¡é“æ–¹æ³•ï¼Œç»“åˆäº†è‡ªåŠ¨è½¬å½•çš„æ­Œè¯å’ŒéŸ³é¢‘ä¸­çš„æ­Œè¯ç›¸å…³ä¿¡æ¯ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†ç¨³å¥æ€§ï¼Œé™ä½äº†å—ä½çº§ä¼ªé€ çš„å¹²æ‰°é£é™©ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œæ–°æ–¹æ³•ä¼˜äºç°æœ‰æ­Œè¯æ£€æµ‹æ–¹æ³•ï¼Œå¯¹éŸ³é¢‘æ‰°åŠ¨å…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºæ£€æµ‹çœŸå®åœºæ™¯ä¸­çš„AIç”ŸæˆéŸ³ä¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-95ccf4a93a00172a81512ba07d40a18b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9808712ce0187a711cf09d8ab596bfe8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1139b943f1a203ea6ed1eed1606b96f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9486cb32440fa60084adfdaa2b4e587.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd24cb4e39004ae254f0c0f18d9e4684.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Audio-Plane-Audio-Factorization-Plane-Gaussian-Splatting-for-Real-Time-Talking-Head-Synthesis"><a href="#Audio-Plane-Audio-Factorization-Plane-Gaussian-Splatting-for-Real-Time-Talking-Head-Synthesis" class="headerlink" title="Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time   Talking Head Synthesis"></a>Audio-Plane: Audio Factorization Plane Gaussian Splatting for Real-Time   Talking Head Synthesis</h2><p><strong>Authors:Shuai Shen, Wanhua Li, Yunpeng Zhang, Yap-Peng Tan, Jiwen Lu</strong></p>
<p>Talking head synthesis has emerged as a prominent research topic in computer graphics and multimedia, yet most existing methods often struggle to strike a balance between generation quality and computational efficiency, particularly under real-time constraints. In this paper, we propose a novel framework that integrates Gaussian Splatting with a structured Audio Factorization Plane (Audio-Plane) to enable high-quality, audio-synchronized, and real-time talking head generation. For modeling a dynamic talking head, a 4D volume representation, which consists of three axes in 3D space and one temporal axis aligned with audio progression, is typically required. However, directly storing and processing a dense 4D grid is impractical due to the high memory and computation cost, and lack of scalability for longer durations. We address this challenge by decomposing the 4D volume representation into a set of audio-independent spatial planes and audio-dependent planes, forming a compact and interpretable representation for talking head modeling that we refer to as the Audio-Plane. This factorized design allows for efficient and fine-grained audio-aware spatial encoding, and significantly enhances the modelâ€™s ability to capture complex lip dynamics driven by speech signals. To further improve region-specific motion modeling, we introduce an audio-guided saliency splatting mechanism based on region-aware modulation, which adaptively emphasizes highly dynamic regions such as the mouth area. This allows the model to focus its learning capacity on where it matters most for accurate speech-driven animation. Extensive experiments on both the self-driven and the cross-driven settings demonstrate that our method achieves state-of-the-art visual quality, precise audio-lip synchronization, and real-time performance, outperforming prior approaches across both 2D- and 3D-based paradigms. </p>
<blockquote>
<p>è¯´è¯äººå¤´éƒ¨åˆæˆå·²æˆä¸ºè®¡ç®—æœºå›¾å½¢å­¦å’Œå¤šåª’ä½“é¢†åŸŸçš„ä¸€ä¸ªçªå‡ºç ”ç©¶ä¸»é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨ç”Ÿæˆè´¨é‡å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å®æ—¶çº¦æŸä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†é«˜æ–¯æº…å‡ºæŠ€æœ¯å’Œç»“æ„åŒ–éŸ³é¢‘å› å­åŒ–å¹³é¢ï¼ˆéŸ³é¢‘å¹³é¢ï¼‰ï¼Œä»¥å®ç°é«˜è´¨é‡ã€éŸ³é¢‘åŒæ­¥å’Œå®æ—¶çš„è¯´è¯äººå¤´éƒ¨ç”Ÿæˆã€‚ä¸ºäº†æ¨¡æ‹ŸåŠ¨æ€çš„è¯´è¯äººå¤´éƒ¨ï¼Œé€šå¸¸éœ€è¦ä¸€ç§4Dä½“ç§¯è¡¨ç¤ºï¼Œå…¶ç”±ä¸‰ä¸ªç©ºé—´è½´å’Œä¸€ä¸ªä¸éŸ³é¢‘è¿›å±•å¯¹é½çš„æ—¶é—´è½´ç»„æˆã€‚ç„¶è€Œï¼Œç”±äºç›´æ¥å­˜å‚¨å’Œå¤„ç†å¯†é›†çš„4Dç½‘æ ¼æ¶‰åŠé«˜å†…å­˜å’Œè®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸”å¯¹äºæ›´é•¿æ—¶é—´çš„æ¨¡æ‹Ÿç¼ºä¹å¯æ‰©å±•æ€§ï¼Œå› æ­¤è¿™ç§åšæ³•å¹¶ä¸å®ç”¨ã€‚æˆ‘ä»¬é€šè¿‡å°†4Dä½“ç§¯è¡¨ç¤ºåˆ†è§£ä¸ºä¸€ç³»åˆ—éŸ³é¢‘ç‹¬ç«‹çš„ç©ºé—´å¹³é¢å’ŒéŸ³é¢‘ä¾èµ–çš„å¹³é¢æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œå½¢æˆä¸€ç§ç´§å‡‘ä¸”å¯è§£é‡Šçš„è¯´è¯äººå¤´éƒ¨æ¨¡å‹çš„è¡¨ç¤ºå½¢å¼ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºéŸ³é¢‘å¹³é¢ã€‚è¿™ç§å› å­åŒ–è®¾è®¡å…è®¸é«˜æ•ˆä¸”ç²¾ç»†çš„éŸ³é¢‘æ„ŸçŸ¥ç©ºé—´ç¼–ç ï¼Œå¹¶æ˜¾ç€æé«˜äº†æ¨¡å‹æ•æ‰ç”±è¯­éŸ³ä¿¡å·é©±åŠ¨çš„å¤æ‚å˜´å”‡åŠ¨æ€çš„èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¹è¿›ç‰¹å®šåŒºåŸŸçš„è¿åŠ¨å»ºæ¨¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºåŒºåŸŸæ„ŸçŸ¥è°ƒåˆ¶çš„éŸ³é¢‘å¼•å¯¼æ˜¾è‘—æ€§æº…å‡ºæœºåˆ¶ï¼Œè¯¥æœºåˆ¶è‡ªé€‚åº”åœ°å¼ºè°ƒé«˜åº¦åŠ¨æ€çš„åŒºåŸŸï¼Œå¦‚å£è…”åŒºåŸŸã€‚è¿™ä½¿æ¨¡å‹èƒ½å¤Ÿä¸“æ³¨äºå­¦ä¹ æœ€é‡è¦éƒ¨åˆ†ä»¥å‡†ç¡®å®ç°è¯­éŸ³é©±åŠ¨åŠ¨ç”»ã€‚åœ¨è‡ªæˆ‘é©±åŠ¨å’Œäº¤å‰é©±åŠ¨ç¯å¢ƒä¸‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„è§†è§‰è´¨é‡ã€ç²¾ç¡®çš„éŸ³é¢‘-å˜´å”‡åŒæ­¥å’Œå®æ—¶æ€§èƒ½ï¼Œåœ¨äºŒç»´å’Œä¸‰ç»´åŸºäºèŒƒå¼çš„æ–¹æ³•ä¸­éƒ½ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22605v2">PDF</a> Demo video at \url{<a target="_blank" rel="noopener" href="https://sstzal.github.io/Audio-Plane/%7D">https://sstzal.github.io/Audio-Plane/}</a></p>
<p><strong>Summary</strong><br>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé›†æˆäº†é«˜æ–¯æ¶‚æ–‘æŠ€æœ¯å’Œç»“æ„åŒ–éŸ³é¢‘å› å­å¹³é¢ï¼ˆAudio-Planeï¼‰ï¼Œä»¥å®æ—¶ç”Ÿæˆé«˜è´¨é‡ã€éŸ³é¢‘åŒæ­¥çš„è¯´è¯äººå¤´åƒã€‚é’ˆå¯¹åŠ¨æ€è¯´è¯å¤´çš„å»ºæ¨¡ï¼Œé‡‡ç”¨äº†ä¸€ä¸ªåˆ›æ–°çš„å››ç»´ä½“ç§¯è¡¨ç¤ºæ³•ã€‚ä¸ºåº”å¯¹ç›´æ¥å­˜å‚¨å’Œå¤„ç†å¯†é›†å››ç»´ç½‘æ ¼å¸¦æ¥çš„é«˜å†…å­˜å’Œè®¡ç®—æˆæœ¬é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ†è§£å››ç»´ä½“ç§¯è¡¨ç¤ºæ³•çš„æ–¹æ³•ï¼Œå°†å…¶è½¬åŒ–ä¸ºä¸€ç³»åˆ—éŸ³é¢‘ç‹¬ç«‹çš„ç©ºé—´å¹³é¢å’ŒéŸ³é¢‘ä¾èµ–çš„å¹³é¢ï¼Œå½¢æˆäº†ä¸€ä¸ªç´§å‡‘ä¸”æ˜“äºç†è§£çš„è¯´è¯å¤´æ¨¡å‹è¡¨ç¤ºæ–¹æ³•â€”â€”éŸ³é¢‘å¹³é¢ã€‚è¿™ç§åˆ†è§£è®¾è®¡å®ç°äº†é«˜æ•ˆä¸”ç²¾ç»†çš„éŸ³é¢‘æ„ŸçŸ¥ç©ºé—´ç¼–ç ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹æ•æ‰å¤æ‚å”‡éƒ¨åŠ¨ä½œçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºåŒºåŸŸæ„ŸçŸ¥è°ƒåˆ¶çš„éŸ³é¢‘å¼•å¯¼æ˜¾è‘—æ€§æ¶‚æ–‘æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥è‡ªé€‚åº”åœ°å¼ºè°ƒé«˜åº¦åŠ¨æ€çš„åŒºåŸŸï¼Œå¦‚å£éƒ¨åŒºåŸŸã€‚åœ¨è‡ªæˆ‘é©±åŠ¨å’Œè·¨é©±åŠ¨ä¸¤ç§è®¾ç½®ä¸‹çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€ç²¾ç¡®çš„éŸ³é¢‘å”‡åŒæ­¥å’Œå®æ—¶æ€§èƒ½æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œåœ¨äºŒç»´å’Œä¸‰ç»´èŒƒå¼ä¸­å‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„é›†æˆGaussian Splattingä¸ç»“æ„åŒ–Audio Factorization Planeï¼ˆAudio-Planeï¼‰çš„æ¡†æ¶ï¼Œç”¨äºé«˜è´¨é‡ã€éŸ³é¢‘åŒæ­¥ã€å®æ—¶çš„è¯´è¯å¤´ç”Ÿæˆã€‚</li>
<li>é‡‡ç”¨å››ç»´ä½“ç§¯è¡¨ç¤ºæ³•æ¥å»ºæ¨¡åŠ¨æ€è¯´è¯å¤´ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªç©ºé—´è½´å’Œæ—¶é—´è½´ä¸éŸ³é¢‘è¿›å±•å¯¹é½ã€‚</li>
<li>é€šè¿‡å°†å››ç»´ä½“ç§¯è¡¨ç¤ºæ³•åˆ†è§£ä¸ºéŸ³é¢‘ç‹¬ç«‹çš„ç©ºé—´å¹³é¢å’ŒéŸ³é¢‘ä¾èµ–çš„å¹³é¢ï¼Œè§£å†³äº†ç›´æ¥å¤„ç†å¯†é›†å››ç»´ç½‘æ ¼å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†éŸ³é¢‘å¹³é¢è¿™ä¸€ç´§å‡‘ä¸”å¯è§£é‡Šçš„è¡¨ç¤ºæ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆä¸”ç²¾ç»†çš„éŸ³é¢‘æ„ŸçŸ¥ç©ºé—´ç¼–ç ã€‚</li>
<li>å¼•å…¥äº†éŸ³é¢‘å¼•å¯¼çš„æ˜¾è‘—æ€§æ¶‚æ–‘æœºåˆ¶ï¼Œå¯è‡ªé€‚åº”å¼ºè°ƒé«˜åº¦åŠ¨æ€åŒºåŸŸï¼Œå¦‚å£éƒ¨åŒºåŸŸï¼Œä»¥æé«˜åŒºåŸŸç‰¹å®šçš„è¿åŠ¨å»ºæ¨¡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€éŸ³é¢‘å”‡åŒæ­¥å’Œå®æ—¶æ€§èƒ½ä¸Šè¾¾åˆ°äº†å…ˆè¿›æ°´å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b4421c32e1377871bb4076391b2450f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a354e62ab47877e903079e3a52c528a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4419f5ce49feac3a2a446fdb2cfae702.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-722d595511ab12179aecd25f16179962.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3bcbeb9dc09f309aeb2b3230feddadd.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait"><a href="#FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait" class="headerlink" title="FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait"></a>FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait</h2><p><strong>Authors:Taekyung Ki, Dongchan Min, Gyeongsu Chae</strong></p>
<p>With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. Instead of a pixel-based latent space, we take advantage of a learned orthogonal motion latent space, enabling efficient generation and editing of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with an effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency. </p>
<blockquote>
<p>éšç€åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œè‚–åƒå›¾åƒåŠ¨ç”»å·²ç»å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œç”±äºå®ƒè¿­ä»£é‡‡æ ·çš„ç‰¹æ€§ï¼Œå®ƒåœ¨æ—¶é—´ä¸€è‡´çš„è§†é¢‘ç”Ÿæˆå’Œå¿«é€Ÿé‡‡æ ·æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†FLOATï¼Œä¸€ç§åŸºäºæµåŒ¹é…ç”Ÿæˆæ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨å¯¹è¯è‚–åƒè§†é¢‘ç”Ÿæˆæ–¹æ³•ã€‚æˆ‘ä»¬åˆ©ç”¨å­¦ä¹ åˆ°çš„æ­£äº¤è¿åŠ¨æ½œåœ¨ç©ºé—´ï¼Œè€Œä¸æ˜¯åŸºäºåƒç´ çš„æ½œåœ¨ç©ºé—´ï¼Œå®ç°äº†æ—¶é—´ä¸€è‡´è¿åŠ¨çš„æœ‰æ•ˆç”Ÿæˆå’Œç¼–è¾‘ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„çŸ¢é‡åœºé¢„æµ‹å™¨ï¼Œå¹¶é…å¤‡äº†ä¸€ä¸ªæœ‰æ•ˆçš„å¸§æ¡ä»¶æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒè¯­éŸ³é©±åŠ¨çš„æƒ…æ„Ÿå¢å¼ºï¼Œèƒ½å¤Ÿå®ç°è¡¨è¾¾æ€§è¿åŠ¨çš„è‡ªç„¶èåˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€è¿åŠ¨ä¿çœŸåº¦å’Œæ•ˆç‡æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„éŸ³é¢‘é©±åŠ¨å¯¹è¯è‚–åƒæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01064v3">PDF</a> ICCV 2025. Project page:   <a target="_blank" rel="noopener" href="https://deepbrainai-research.github.io/float/">https://deepbrainai-research.github.io/float/</a></p>
<p><strong>Summary</strong></p>
<p>éšç€æ‰©æ•£ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œè‚–åƒåŠ¨ç”»å·²ç»å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ä»é¢ä¸´ä¸´æ—¶ä¸€è‡´æ€§è§†é¢‘ç”Ÿæˆå’Œå¿«é€Ÿé‡‡æ ·æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºFLOATæ–¹æ³•ï¼Œä¸€ç§åŸºäºæµåŒ¹é…ç”Ÿæˆæ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨è‚–åƒè§†é¢‘ç”Ÿæˆæ³•ã€‚ä¸ä¼ ç»Ÿçš„åƒç´ çº§æ½œåœ¨ç©ºé—´ä¸åŒï¼Œæœ¬æ–‡åˆ©ç”¨å­¦ä¹ åˆ°çš„æ­£äº¤è¿åŠ¨æ½œåœ¨ç©ºé—´å®ç°é«˜æ•ˆä¸”ä¸´æ—¶ä¸€è‡´çš„åŠ¨ç”»ç”Ÿæˆä¸ç¼–è¾‘ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥åŸºäºå˜æ¢å™¨çš„çŸ¢é‡åœºé¢„æµ‹å™¨ä¸æœ‰æ•ˆçš„å¸§æ¡ä»¶æœºåˆ¶ï¼Œå®ç°è¯­éŸ³é©±åŠ¨çš„æƒ…æ„Ÿå¢å¼ºåŠŸèƒ½ï¼Œè®©è¡¨æƒ…åŠ¨ä½œçš„è‡ªç„¶èåˆæˆä¸ºå¯èƒ½ã€‚å®éªŒè¯æ˜ï¼Œç›¸è¾ƒäºå…ˆè¿›çš„éŸ³é¢‘é©±åŠ¨è‚–åƒåŠ¨ç”»æŠ€æœ¯ï¼ŒFLOATåœ¨è§†è§‰è´¨é‡ã€åŠ¨ä½œçœŸå®åº¦ä¸æ•ˆç‡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†è‚–åƒåŠ¨ç”»æŠ€æœ¯çš„æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>è‚–åƒåŠ¨ç”»ä»é¢ä¸´ä¸´æ—¶ä¸€è‡´æ€§è§†é¢‘ç”Ÿæˆå’Œå¿«é€Ÿé‡‡æ ·æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>FLOATæ–¹æ³•åˆ©ç”¨æµåŒ¹é…ç”Ÿæˆæ¨¡å‹å®ç°éŸ³é¢‘é©±åŠ¨çš„è‚–åƒè§†é¢‘ç”Ÿæˆã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿçš„åƒç´ çº§æ½œåœ¨ç©ºé—´ï¼Œåˆ©ç”¨æ­£äº¤è¿åŠ¨æ½œåœ¨ç©ºé—´æé«˜åŠ¨ç”»ç”Ÿæˆçš„æ•ˆç‡ä¸ä¸´æ—¶ä¸€è‡´æ€§ã€‚</li>
<li>åŸºäºå˜æ¢å™¨çš„çŸ¢é‡åœºé¢„æµ‹å™¨ä¸å¸§æ¡ä»¶æœºåˆ¶æ”¯æŒè¯­éŸ³é©±åŠ¨çš„æƒ…æ„Ÿå¢å¼ºåŠŸèƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºFLOATåœ¨è§†è§‰è´¨é‡ã€åŠ¨ä½œçœŸå®åº¦ä¸æ•ˆç‡æ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›éŸ³é¢‘é©±åŠ¨è‚–åƒåŠ¨ç”»æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.01064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a9a34885d0e0f3c152fbf196daea7c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f1054ceade950b1b0b3d281f1c8c346.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd0c0972287e0f2917338ea90bec8ffe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09c5c450da01da75aa22f8e69a0cc3ec.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-32931b4a1c24bcba969348e1419eacb7.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Identity Preserving 3D Head Stylization with Multiview Score   Distillation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7002af9c28262ea2c83bdc26f4f96f71.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  GroundingDINO-US-SAM Text-Prompted Multi-Organ Segmentation in   Ultrasound with LoRA-Tuned Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
