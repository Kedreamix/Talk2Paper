<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="无监督/半监督/对比学习">
    <meta name="description" content="无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-07-03  Spatially Gene Expression Prediction using Dual-Scale Contrastive   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>无监督/半监督/对比学习 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0128cd990e3e381ad6f8f2065f699292.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">无监督/半监督/对比学习</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">无监督/半监督/对比学习</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                无监督/半监督/对比学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-03-更新"><a href="#2025-07-03-更新" class="headerlink" title="2025-07-03 更新"></a>2025-07-03 更新</h1><h2 id="Spatially-Gene-Expression-Prediction-using-Dual-Scale-Contrastive-Learning"><a href="#Spatially-Gene-Expression-Prediction-using-Dual-Scale-Contrastive-Learning" class="headerlink" title="Spatially Gene Expression Prediction using Dual-Scale Contrastive   Learning"></a>Spatially Gene Expression Prediction using Dual-Scale Contrastive   Learning</h2><p><strong>Authors:Mingcheng Qu, Yuncong Wu, Donglin Di, Yue Gao, Tonghua Su, Yang Song, Lei Fan</strong></p>
<p>Spatial transcriptomics (ST) provides crucial insights into tissue micro-environments, but is limited to its high cost and complexity. As an alternative, predicting gene expression from pathology whole slide images (WSI) is gaining increasing attention. However, existing methods typically rely on single patches or a single pathology modality, neglecting the complex spatial and molecular interactions between target and neighboring information (e.g., gene co-expression). This leads to a failure in establishing connections among adjacent regions and capturing intricate cross-modal relationships. To address these issues, we propose NH2ST, a framework that integrates spatial context and both pathology and gene modalities for gene expression prediction. Our model comprises a query branch and a neighbor branch to process paired target patch and gene data and their neighboring regions, where cross-attention and contrastive learning are employed to capture intrinsic associations and ensure alignments between pathology and gene expression. Extensive experiments on six datasets demonstrate that our model consistently outperforms existing methods, achieving over 20% in PCC metrics. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/MCPathology/NH2ST">https://github.com/MCPathology/NH2ST</a> </p>
<blockquote>
<p>空间转录组学（ST）为组织微环境提供了关键见解，但其高昂的成本和复杂性限制了其应用。作为替代方案，从病理学全幻灯片图像（WSI）预测基因表达正引起越来越多的关注。然而，现有方法通常依赖于单个补丁或单一病理学模式，忽视了目标信息与邻近信息之间复杂的空间分子交互（例如基因共表达）。这导致了在建立相邻区域之间的联系和捕捉复杂的跨模式关系方面的失败。为了解决这些问题，我们提出了NH2ST框架，该框架融合了空间上下文以及病理学和基因模式来进行基因表达预测。我们的模型包括查询分支和邻居分支，用于处理成对的目标补丁和基因数据及其邻近区域，其中采用交叉注意力和对比学习来捕捉内在关联并确保病理与基因表达之间的对齐。在六个数据集上的大量实验表明，我们的模型始终优于现有方法，在PCC指标上达到20%以上的提升。代码可在<a target="_blank" rel="noopener" href="https://github.com/MCPathology/NH2ST%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MCPathology/NH2ST上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23827v1">PDF</a> Our paper has been accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>空间转录学（ST）对于研究组织微环境具有重要意义，但其高昂成本和复杂性限制了应用。因此，通过病理全切片图像预测基因表达备受关注。然而，现有方法往往依赖单一的切片或单一病理模式，忽略了目标区域与邻近信息间的复杂空间与分子交互（如基因共表达）。为解决这些问题，我们提出了NH2ST框架，它结合了空间上下文信息以及病理和基因两种模式来进行基因表达预测。该模型包含查询分支和邻近分支，用于处理目标区域和基因数据的配对以及相邻区域的信息。通过交叉注意力机制和对比学习，捕捉内在联系并确保病理与基因表达之间的对齐。在六个数据集上的广泛实验表明，我们的模型始终优于现有方法，在PCC指标上提高了超过20%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>空间转录学对于研究组织微环境具有重要意义，但存在高成本和复杂性的限制。</li>
<li>通过病理全切片图像预测基因表达是一个备受关注的研究方向。</li>
<li>现有方法忽略了目标区域与邻近信息间的复杂空间与分子交互。</li>
<li>NH2ST框架结合了空间上下文信息以及病理和基因两种模式进行基因表达预测。</li>
<li>NH2ST模型包含查询分支和邻近分支，用于处理目标区域和基因数据的配对以及相邻区域的信息。</li>
<li>NH2ST通过交叉注意力机制和对比学习来捕捉内在联系和确保病理与基因表达之间的对齐。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23827">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d52c65f59f6b90f4ae8a21c5ff6ac2fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ea594d3b602ffca82deee74d97ec227.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb14abf25fa2b5af0f3a4f7dafdd1f67.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AdFair-CLIP-Adversarial-Fair-Contrastive-Language-Image-Pre-training-for-Chest-X-rays"><a href="#AdFair-CLIP-Adversarial-Fair-Contrastive-Language-Image-Pre-training-for-Chest-X-rays" class="headerlink" title="AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training   for Chest X-rays"></a>AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training   for Chest X-rays</h2><p><strong>Authors:Chenlang Yi, Zizhan Xiong, Qi Qi, Xiyuan Wei, Girish Bathla, Ching-Long Lin, Bobak Jack Mortazavi, Tianbao Yang</strong></p>
<p>Contrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive experiments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, particularly for CXR analysis. </p>
<blockquote>
<p>对比语言图像预训练（CLIP）模型在包括医学图像分类在内的各种视觉任务中表现出了卓越的性能。然而，对于CLIP模型的公平性关注，包括人口统计偏见，并未得到足够的重视。这种疏忽导致了关键问题，特别是与种族和性别相关的问题，进而导致诊断结果的不公平和对代表性不足的群体的可靠性降低。为了解决这些挑战，我们引入了AdFair-CLIP，这是一个采用对抗性特征干预来抑制敏感属性、从而减轻虚假关联并改善预测公平性的新型框架。我们在胸部X射线（CXR）数据集上进行了全面的实验，结果表明AdFair-CLIP在零样本和少样本场景中，显著提高了公平性和诊断准确性，同时保持了稳健的泛化能力。这些结果为CLIP基础的医学诊断模型中的公平性感知学习，特别是CXR分析，建立了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23467v1">PDF</a> This preprint has been accepted by MICCAI 2025</p>
<p><strong>Summary</strong><br>     对比语言图像预训练（CLIP）模型在多种视觉任务中表现优异，但在医疗图像分类等任务中存在公平性问题，特别是与种族和性别相关的挑战。为解决这一问题，提出AdFair-CLIP框架，采用对抗特征干预抑制敏感属性，减少虚假关联，提高预测公平性。在胸片X线数据集上的实验表明，AdFair-CLIP在零样本和少样本场景下，能显著提高公平性和诊断准确性，并保持稳健的泛化能力，为CLIP基医疗诊断模型的公平性认知学习树立了新标杆。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP模型在多种视觉任务中表现优异，但在医疗图像分类等任务中存在公平性问题。</li>
<li>AdFair-CLIP框架被提出来解决CLIP模型的公平性问题。</li>
<li>AdFair-CLIP采用对抗特征干预来抑制敏感属性，从而减少虚假关联。</li>
<li>AdFair-CLIP能提高预测公平性。</li>
<li>在胸片X线数据集上的实验表明，AdFair-CLIP在零样本和少样本场景下能显著提高公平性和诊断准确性。</li>
<li>AdFair-CLIP框架的引入为CLIP基医疗诊断模型的公平性认知学习树立了新标杆。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23467">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a694931f373977ed867e26a7a91742c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3b099463c7072cddecb002adc2e6032.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a56d0f31bb5da36140ab15e2ebae7779.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Contrastive-Learning-with-Diffusion-Features-for-Weakly-Supervised-Medical-Image-Segmentation"><a href="#Contrastive-Learning-with-Diffusion-Features-for-Weakly-Supervised-Medical-Image-Segmentation" class="headerlink" title="Contrastive Learning with Diffusion Features for Weakly Supervised   Medical Image Segmentation"></a>Contrastive Learning with Diffusion Features for Weakly Supervised   Medical Image Segmentation</h2><p><strong>Authors:Dewen Zeng, Xinrong Hu, Yu-Jen Chen, Yawen Wu, Xiaowei Xu, Yiyu Shi</strong></p>
<p>Weakly supervised semantic segmentation (WSSS) methods using class labels often rely on class activation maps (CAMs) to localize objects. However, traditional CAM-based methods struggle with partial activations and imprecise object boundaries due to optimization discrepancies between classification and segmentation. Recently, the conditional diffusion model (CDM) has been used as an alternative for generating segmentation masks in WSSS, leveraging its strong image generation capabilities tailored to specific class distributions. By modifying or perturbing the condition during diffusion sampling, the related objects can be highlighted in the generated images. Yet, the saliency maps generated by CDMs are prone to noise from background alterations during reverse diffusion. To alleviate the problem, we introduce Contrastive Learning with Diffusion Features (CLDF), a novel method that uses contrastive learning to train a pixel decoder to map the diffusion features from a frozen CDM to a low-dimensional embedding space for segmentation. Specifically, we integrate gradient maps generated from CDM external classifier with CAMs to identify foreground and background pixels with fewer false positives&#x2F;negatives for contrastive learning, enabling robust pixel embedding learning. Experimental results on four segmentation tasks from two public medical datasets demonstrate that our method significantly outperforms existing baselines. </p>
<blockquote>
<p>弱监督语义分割（WSSS）方法通常使用类别标签，并依赖于类别激活图（CAMs）进行对象定位。然而，传统的基于CAM的方法在部分激活和优化分类与分割之间差异导致的精确对象边界方面存在困难。最近，条件扩散模型（CDM）已被用作WSSS中生成分割掩模的替代方法，利用其针对特定类别分布的强大图像生成能力。通过在扩散采样过程中修改或扰动条件，可以在生成的图像中突出显示相关对象。然而，由CDM生成的显著性地图容易受到反向扩散过程中背景改变产生的噪声影响。为了缓解这个问题，我们引入了对比学习与扩散特征（CLDF）相结合的新方法，该方法使用对比学习训练像素解码器，将来自冻结CDM的扩散特征映射到低维嵌入空间以进行分割。具体来说，我们将由CDM外部分类器生成的梯度图与CAM相结合，以识别前景和背景像素，并减少对比学习中误报&#x2F;漏报的数量，从而实现稳健的像素嵌入学习。在来自两个公共医疗数据集的四个分割任务上的实验结果表明，我们的方法显著优于现有基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23460v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>弱监督语义分割（WSSS）常使用类别标签并利用类激活图（CAMs）进行目标定位。然而，传统基于CAM的方法因分类与分割的优化差异而面临部分激活和不精确目标边界的问题。最近，条件扩散模型（CDM）被用于生成WSSS中的分割掩膜，利用其针对特定类别分布的图像生成能力。通过修改或扰动扩散采样过程中的条件，可以突出相关目标。然而，CDM生成的显著性图在反向扩散过程中易受背景变化产生的噪声影响。为解决此问题，我们提出使用对比学习训练像素解码器的方法（CLDF），将CDM的扩散特征映射到低维嵌入空间进行分割。我们整合CDM外部分类器生成的梯度图与CAMs，减少对比学习中的误报&#x2F;漏报，实现稳健的像素嵌入学习。在公共医疗数据集的四个分割任务上的实验结果表明，我们的方法显著优于现有基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WSSS方法常使用CAMs进行目标定位，但面临部分激活和不精确目标边界的挑战。</li>
<li>CDM用于WSSS中的分割掩膜生成，具有针对特定类别分布的图像生成能力。</li>
<li>CDM生成的显著性图在反向扩散过程中易受噪声影响。</li>
<li>提出CLDF方法，结合对比学习训练像素解码器，将CDM的扩散特征映射到低维嵌入空间进行分割。</li>
<li>整合梯度图和CAMs以提高对比学习的准确性，减少误报&#x2F;漏报。</li>
<li>CLDF方法在公共医疗数据集的四个分割任务上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23460">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-455f35321a7f25148ed921842a7f3af6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-073432a04132db70cb746f7ace48a58b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9402e69e63a67620e791f8b87e50943.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239f00fe2f56718ae174e285877ecb66.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dynamic-Contrastive-Learning-for-Hierarchical-Retrieval-A-Case-Study-of-Distance-Aware-Cross-View-Geo-Localization"><a href="#Dynamic-Contrastive-Learning-for-Hierarchical-Retrieval-A-Case-Study-of-Distance-Aware-Cross-View-Geo-Localization" class="headerlink" title="Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of   Distance-Aware Cross-View Geo-Localization"></a>Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of   Distance-Aware Cross-View Geo-Localization</h2><p><strong>Authors:Suofei Zhang, Xinxin Wang, Xiaofu Wu, Quan Zhou, Haifeng Hu</strong></p>
<p>Existing deep learning-based cross-view geo-localization methods primarily focus on improving the accuracy of cross-domain image matching, rather than enabling models to comprehensively capture contextual information around the target and minimize the cost of localization errors. To support systematic research into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem, we construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs multi-view imagery with precise distance annotations across three spatial resolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical retrieval problem across different domains. Our study further reveals that, due to the inherent complexity of spatial relationships among buildings, this problem can only be addressed via a contrastive learning paradigm, rather than conventional metric learning. To tackle this challenge, we propose Dynamic Contrastive Learning (DyCL), a novel framework that progressively aligns feature representations according to hierarchical spatial margins. Extensive experiments demonstrate that DyCL is highly complementary to existing multi-scale metric learning methods and yields substantial improvements in both hierarchical retrieval performance and overall cross-view geo-localization accuracy. Our code and benchmark are publicly available at <a target="_blank" rel="noopener" href="https://github.com/anocodetest1/DyCL">https://github.com/anocodetest1/DyCL</a>. </p>
<blockquote>
<p>现有的基于深度学习的跨视图地理定位方法主要关注改进跨域图像匹配的准确性，而不是使模型能够全面捕获目标周围的上下文信息并最小化定位错误成本。为了支持对距离感知跨视图地理定位（DACVGL）问题的系统研究，我们构建了距离感知校园（DA-Campus），这是第一个将多视图图像与三种空间分辨率的精确距离注释配对在一起的基准测试。基于DA-Campus，我们将DACVGL制定为不同领域的分层检索问题。我们的研究进一步表明，由于建筑物之间空间关系的固有复杂性，这个问题只能通过对比学习范式来解决，而不是传统的度量学习。为了应对这一挑战，我们提出了动态对比学习（DyCL）这一新颖框架，它根据分层空间边界逐步对齐特征表示。大量实验表明，DyCL与现有的多尺度度量学习方法高度互补，在分层检索性能和总体跨视图地理定位准确性方面都有显著提高。我们的代码和基准测试可在<a target="_blank" rel="noopener" href="https://github.com/anocodetest1/DyCL%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/anocodetest1/DyCL公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23077v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文构建了一个名为DA-Campus的基准测试平台，该平台结合了多视角图像和精确的距离标注，空间分辨率各异。在此基础上，将跨视图地理定位（DACVGL）问题转化为层次检索问题。研究发现，由于建筑物之间空间关系的固有复杂性，该问题只能通过对比学习模式而非传统度量学习来解决。为此，提出了动态对比学习（DyCL）框架，该框架根据层次空间边界逐步对齐特征表示。实验表明，DyCL对现有多尺度度量学习方法具有互补性，能显著提高层次检索性能和跨视图地理定位准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了Distance-Aware Cross-View Geo-Localization (DACVGL)问题，并构建了DA-Campus基准测试平台，包含多视角图像和精确距离标注。</li>
<li>将DACVGL问题转化为层次检索问题，强调在跨域图像匹配中全面捕捉目标上下文信息的重要性。</li>
<li>指出由于建筑物间空间关系的复杂性，解决DACVGL问题需要采用对比学习模式。</li>
<li>提出了Dynamic Contrastive Learning (DyCL)框架，能按层次空间边界逐步对齐特征表示。</li>
<li>DyCL框架对现有多尺度度量学习方法具有互补性，能显著提高层次检索和跨视图地理定位的准确性。</li>
<li>公开了研究代码和基准测试平台，便于其他人使用和进一步的研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23077">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1d4d84b7da50fc0441f4d519c4fe215d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ed4db0690363a7ee94aab46fe03724a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdf90f2c18d6b1617a8caebeef6dfda5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a091341ebc0ca12a368749fb3bb9901e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-470ac61b0de5937bb96ee8892367c9a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e502922a6710370db8082c5fd96a05c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7a929bd2cab12387f70cc355a6f9482.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="How-Semantically-Informative-is-an-Image-Measuring-the-Covariance-Weighted-Norm-of-Contrastive-Learning-Embeddings"><a href="#How-Semantically-Informative-is-an-Image-Measuring-the-Covariance-Weighted-Norm-of-Contrastive-Learning-Embeddings" class="headerlink" title="How Semantically Informative is an Image?: Measuring the   Covariance-Weighted Norm of Contrastive Learning Embeddings"></a>How Semantically Informative is an Image?: Measuring the   Covariance-Weighted Norm of Contrastive Learning Embeddings</h2><p><strong>Authors:Fumiya Uchiyama, Rintaro Yanagi, Shohei Taniguchi, Shota Takashiro, Masahiro Suzuki, Hirokatsu Kataoka, Yusuke Iwasawa, Yutaka Matsuo</strong></p>
<p>Contrastive learning has the capacity to model multimodal probability distributions by embedding and aligning visual representations with semantics from captions. This approach enables the estimation of relational semantic similarity; however, it remains unclear whether it can also represent absolute semantic informativeness. In this work, we introduce a semantic informativeness metric for an image calculated from text samples via a contrastive learning model; similarly, the informativeness of a text is calculated from image samples. We propose a redefinition of the concept of Information Gain, a concept previously explored in natural language processing, extending its application to the domains of vision and language. Our metric quantifies how conditioning on an image distorts the distribution of associated texts, and vice versa for text conditioning on image distributions. In OpenCLIP’s empirical results, we observe that images with the lowest Information Gain scores often correspond to placeholder icons such as “image not found.” Furthermore, we propose to measure a norm-based metric of the embedding to estimate the Information Gain, following the theoretical results for Skip-Gram with Negative Sampling (SGNS) word embedding. Information Gain can be measured using either CLIP or SigLIP, and the results demonstrate a strong correlation with a coefficient of determination ranging from 0.98 to 1.00. After obtaining the mean and the covariance of the sample embedding, the computational cost of this method is independent of the sample size, and it is compatible with publicly available, open-weight models. </p>
<blockquote>
<p>对比学习具有通过嵌入和对齐来自字幕的语义来建模多模态概率分布的能力。这种方法能够估计关系语义相似性；然而，尚不清楚它是否能代表绝对的语义信息性。在这项工作中，我们引入了一种通过对比学习模型从文本样本计算图像语义信息性的度量标准；同样，文本的语义信息性也是从图像样本中计算得出的。我们重新定义了信息增益的概念，这是一个之前在自然语言处理中探索过的概念，将其扩展到视觉和语言领域。我们的度量标准衡量了以图像为条件时相关文本分布的扭曲程度，反之亦然。在OpenCLIP的实证结果中，我们观察到信息增益得分最低的图片通常对应于占位符图标，如“图片未找到”。此外，我们提出基于嵌入的范数度量来估计信息增益，遵循Skip-Gram负采样（SGNS）词嵌入的理论结果。信息增益可以使用CLIP或SigLIP进行测量，结果显示与从0.98到1.00的决定系数有很强的相关性。在获得样本嵌入的均值和协方差之后，该方法的计算成本独立于样本大小，并且它与公开可用的开源权重模型兼容。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22881v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了对比学习在视觉和语义融合中的潜力，并引入了一种基于对比学习的语义信息量度量方法。通过计算图像与文本样本之间的信息增益，该度量方法可以评估图像或文本所携带的语义信息量。研究结果表明，信息增益与Skip-Gram负采样词嵌入理论结果相符，并且适用于公开可用的模型。此方法计算成本低，样本嵌入的均值和协方差独立于样本大小。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对比学习能够建模多模态概率分布，通过嵌入和对齐视觉表示与语义字幕。</li>
<li>引入了一种基于对比学习的语义信息量度量方法，可以计算图像和文本样本之间的信息增益。</li>
<li>信息增益的概念被重新定义并扩展到视觉和语言领域。</li>
<li>图像与文本之间的信息增益度量可以评估它们所携带的语义信息量。</li>
<li>信息增益的计算基于样本嵌入的范数，与Skip-Gram负采样词嵌入理论相符。</li>
<li>信息增益的计算方法计算成本低，并且适用于公开可用的模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22881">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3380ee73a42d96d6db45c6d627576951.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5dacaf0d9fb6a8fa46c98d91cb29d9fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27fe2272d39c17683e68418a72893a91.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MiCo-Multi-image-Contrast-for-Reinforcement-Visual-Reasoning"><a href="#MiCo-Multi-image-Contrast-for-Reinforcement-Visual-Reasoning" class="headerlink" title="MiCo: Multi-image Contrast for Reinforcement Visual Reasoning"></a>MiCo: Multi-image Contrast for Reinforcement Visual Reasoning</h2><p><strong>Authors:Xi Chen, Mingkang Zhu, Shaoteng Liu, Xiaoyang Wu, Xiaogang Xu, Yu Liu, Xiang Bai, Hengshuang Zhao</strong></p>
<p>This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks. </p>
<blockquote>
<p>本文探讨了实现Chain-of-Thought（CoT）推理，以在多张图像之间建立视觉线索的联系。一种直接的解决方案是为视觉语言模型（VLMs）使用基于规则的任务进行强化学习。然而，这种方法通常依赖于人工策划的问题答案对，当面对精细粒度的视觉细节和图像之间的复杂逻辑时，这可能会特别具有挑战性。受自我监督的视觉表示学习的启发，我们观察到图像包含可以作为监督的固有约束。基于这一见解，我们构建了包含同一张图像的两个增强视图和第三张相似但不同的图像的三重图像。在训练过程中，模型被提示生成一个推理过程来比较这些图像（即确定相同或不同）。然后我们用基于规则的任务进行强化学习来优化模型。由于高视觉相似性和增强的存在，模型必须关注微妙的视觉变化并进行逻辑推理才能成功。实验表明，尽管仅通过视觉比较任务进行训练，但所学的推理能力可以有效地推广到各种问题。我们的方法不依赖于任何人工标注的问题答案对，在跨图像推理基准测试中取得了显著的改进，并在通用视觉任务中表现出强大的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22434v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探索了如何通过Chain-of-Thought（CoT）推理将多个图像中的视觉线索联系起来。通过自我监督的视觉表示学习，观察到图像中存在内在约束可作为监督信息。为此构建了包含两个相同图像的增强视图和一个相似但不同的图像的三重图像。训练模型时，会提示模型比较这些图像（即判断相同或不同），然后使用基于规则的强化学习进行优化。由于高视觉相似性和存在的增强，模型必须关注微妙的视觉变化并进行逻辑推理才能成功。实验表明，虽然仅通过视觉比较任务进行训练，但学到的推理能力可以有效泛化到各种问题上。并且，在不依赖任何人工标注的问题答案对的情况下，该方法在多图像推理基准测试上取得了显著改进，并在一般视觉任务上表现出强劲性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>工作探索了Chain-of-Thought（CoT）推理在连接多个图像视觉线索方面的应用。</li>
<li>提出使用自我监督的视觉表示学习来利用图像中的内在约束作为监督信息。</li>
<li>构建三重图像来训练模型比较并区分图像间的微妙差异。</li>
<li>使用基于规则的强化学习优化模型，使其关注视觉变化和进行逻辑推理。</li>
<li>实验表明模型在视觉比较任务上训练后，能有效泛化至多图像推理任务。</li>
<li>方法无需依赖人工标注的问题答案对，在多图像推理基准测试上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22434">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c393203fc7c0a94f5bd878d651ebb1c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-018e0fde7deb4a78e29f0945d5245a5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b03185cace3b2a9390b8b3b6e30f434.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8db4750d9664b50155d0321261db691.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DynaCLR-Contrastive-Learning-of-Cellular-Dynamics-with-Temporal-Regularization"><a href="#DynaCLR-Contrastive-Learning-of-Cellular-Dynamics-with-Temporal-Regularization" class="headerlink" title="DynaCLR: Contrastive Learning of Cellular Dynamics with Temporal   Regularization"></a>DynaCLR: Contrastive Learning of Cellular Dynamics with Temporal   Regularization</h2><p><strong>Authors:Eduardo Hirata-Miyasaki, Soorya Pradeep, Ziwen Liu, Alishba Imran, Taylla Milena Theodoro, Ivan E. Ivanov, Sudip Khadka, See-Chi Lee, Michelle Grunberg, Hunter Woosley, Madhura Bhave, Carolina Arias, Shalin B. Mehta</strong></p>
<p>We report DynaCLR, a self-supervised method for embedding cell and organelle Dynamics via Contrastive Learning of Representations of time-lapse images. DynaCLR integrates single-cell tracking and time-aware contrastive sampling to learn robust, temporally regularized representations of cell dynamics. DynaCLR embeddings generalize effectively to in-distribution and out-of-distribution datasets, and can be used for several downstream tasks with sparse human annotations. We demonstrate efficient annotations of cell states with a human-in-the-loop using fluorescence and label-free imaging channels. DynaCLR method enables diverse downstream biological analyses: classification of cell division and infection, clustering heterogeneous cell migration patterns, cross-modal distillation of cell states from fluorescence to label-free channel, alignment of asynchronous cellular responses and broken cell tracks, and discovering organelle response due to infection. DynaCLR is a flexible method for comparative analyses of dynamic cellular responses to pharmacological, microbial, and genetic perturbations. We provide PyTorch-based implementations of the model training and inference pipeline (<a target="_blank" rel="noopener" href="https://github.com/mehta-lab/viscy">https://github.com/mehta-lab/viscy</a>) and a GUI (<a target="_blank" rel="noopener" href="https://github.com/czbiohub-sf/napari-iohub">https://github.com/czbiohub-sf/napari-iohub</a>) for the visualization and annotation of trajectories of cells in the real space and the embedding space. </p>
<blockquote>
<p>我们报告了一种名为DynaCLR的自我监督方法，该方法通过对比学习时间序列图像的表示来嵌入细胞和细胞器的动态。DynaCLR结合了单细胞追踪和时间感知对比采样，学习细胞动态稳健且时间规律化的表示。DynaCLR嵌入有效地泛化到内部分布和外部分布的数据集，并且可用于具有稀疏人工注释的多个下游任务。我们演示了使用荧光和无标记成像通道在循环中有效注释细胞状态。DynaCLR方法可用于多种下游生物学分析：细胞分裂和感染的分类，聚集异质细胞迁移模式，从荧光到无标记通道的跨模态细胞状态蒸馏，异步细胞反应的校准和断裂的细胞轨迹，以及由于感染而发现的细胞器反应。DynaCLR是一种用于比较药物学、微生物学和遗传学扰动对动态细胞反应分析的灵活方法。我们提供了基于PyTorch的模型训练和推理管道的实现（<a target="_blank" rel="noopener" href="https://github.com/mehta-lab/viscy%EF%BC%89%EF%BC%8C%E4%BB%A5%E5%8F%8A%E4%B8%80%E4%B8%AAGUI%EF%BC%88https://github.com/czbiohub-sf/napari-iohub%EF%BC%89%EF%BC%8C%E7%94%A8%E4%BA%8E%E5%8F%AF%E8%A7%86%E5%8C%96%E7%9C%9F%E5%AE%9E%E7%A9%BA%E9%97%B4%E5%92%8C%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%BB%86%E8%83%9E%E7%9A%84%E8%BD%A8%E8%BF%B9%E5%B9%B6%E8%BF%9B%E8%A1%8C%E6%B3%A8%E9%87%8A%E3%80%82">https://github.com/mehta-lab/viscy），以及一个GUI（https://github.com/czbiohub-sf/napari-iohub），用于可视化真实空间和嵌入空间中细胞的轨迹并进行注释。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11281v2">PDF</a> 30 pages, 6 figures, 13 appendix figures, 5 videos (ancillary files)</p>
<p><strong>Summary</strong>：<br>DynaCLR是一种基于对比学习的时间序列图像表示嵌入方法，用于细胞器动态的无监督学习。它通过结合单细胞追踪和时间感知对比采样，学习稳健的细胞动态表示。DynaCLR嵌入可以有效地应用于内部和外部数据集，并可在稀疏人类注释的情况下用于多种下游任务。它支持多种下游生物学分析，如细胞分裂和感染的分类、异质细胞迁移模式的聚类、从荧光到无标记成像通道的跨模态蒸馏等。DynaCLR具有灵活性，适用于动态细胞响应的药物学、微生物学和遗传学比较分析。提供了基于PyTorch的模型训练和推理管道实现以及可视化工具。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>DynaCLR是一种自监督学习方法，用于嵌入细胞器动态的无监督学习。</li>
<li>通过结合单细胞追踪和时间感知对比采样，学习稳健的细胞动态表示。</li>
<li>DynaCLR嵌入适用于内部和外部数据集，适用于多种下游任务，即使存在稀疏的人类注释也是如此。</li>
<li>DynaCLR能够高效地对具有人参与的注释流程进行分类应用等多样任务包括：如分类细胞分裂和感染以及细胞的异质迁移模式的聚类。也发现能够将细分层分析延续到感染后其他机制等方面进行应用研究以及标注协同和分析上的应用方向等的深入研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11281">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3ac34e10e6335dbb6015b9593dd2ad7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95beb88c5b6fad40453c93cb17007bbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f6b10a6ca1955ffdce47592e527233c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Unsupervised-contrastive-analysis-for-anomaly-detection-in-brain-MRIs-via-conditional-diffusion-models"><a href="#Unsupervised-contrastive-analysis-for-anomaly-detection-in-brain-MRIs-via-conditional-diffusion-models" class="headerlink" title="Unsupervised contrastive analysis for anomaly detection in brain MRIs   via conditional diffusion models"></a>Unsupervised contrastive analysis for anomaly detection in brain MRIs   via conditional diffusion models</h2><p><strong>Authors:Cristiano Patrício, Carlo Alberto Barbano, Attilio Fiandrotti, Riccardo Renzulli, Marco Grangetto, Luis F. Teixeira, João C. Neves</strong></p>
<p>Contrastive Analysis (CA) detects anomalies by contrasting patterns unique to a target group (e.g., unhealthy subjects) from those in a background group (e.g., healthy subjects). In the context of brain MRIs, existing CA approaches rely on supervised contrastive learning or variational autoencoders (VAEs) using both healthy and unhealthy data, but such reliance on target samples is challenging in clinical settings. Unsupervised Anomaly Detection (UAD) offers an alternative by learning a reference representation of healthy anatomy without the need for target samples. Deviations from this reference distribution can indicate potential anomalies. In this context, diffusion models have been increasingly adopted in UAD due to their superior performance in image generation compared to VAEs. Nonetheless, precisely reconstructing the anatomy of the brain remains a challenge. In this work, we propose an unsupervised framework to improve the reconstruction quality by training a self-supervised contrastive encoder on healthy images to extract meaningful anatomical features. These features are used to condition a diffusion model to reconstruct the healthy appearance of a given image, enabling interpretable anomaly localization via pixel-wise comparison. We validate our approach through a proof-of-concept on a facial image dataset and further demonstrate its effectiveness on four brain MRI datasets, achieving state-of-the-art anomaly localization performance on the NOVA benchmark. </p>
<blockquote>
<p>对比分析（CA）通过对比目标组（例如不健康主体）中独特的模式与背景组（例如健康主体）中的模式来检测异常。在脑部MRI的情境中，现有的CA方法依赖于有监督的对比学习或变分自编码器（VAEs），需要同时使用健康和不健康的数据，但在临床环境中对目标样本的依赖是一个挑战。无监督异常检测（UAD）通过学习健康组织的参考表示来提供一种替代方案，而无需使用目标样本。与此参考分布的偏差可能表明存在潜在异常。在这方面，由于其在图像生成方面的卓越性能，扩散模型在UAD中越来越被采用，相对于VAEs具有优势。然而，精确重建大脑的解剖结构仍然是一个挑战。在这项工作中，我们提出一个无监督框架，通过在有监督的对比编码器上训练健康图像来改善重建质量，从而提取有意义的解剖特征。这些特征用于调节扩散模型，以重建给定图像的健康外观，并通过像素级的比较实现可解释的异常定位。我们通过面部图像数据集的概念验证来证明我们的方法，并进一步在四个脑部MRI数据集上展示其有效性，在NOVA基准测试中实现了最先进的异常定位性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00772v3">PDF</a> Under consideration at Pattern Recognition Letters</p>
<p><strong>Summary</strong><br>     本论文提出了一种无监督框架，通过训练自监督对比编码器提取健康图像中的有意义解剖特征，并用这些特征调节扩散模型以重建给定图像的健康外观。此方法可实现通过像素级比较进行可解释的异常定位。在面部图像数据集上的概念验证以及在四个脑MRI数据集上的有效性展示，均在NOVA基准测试上达到了最先进的异常定位性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对比分析（CA）能够通过识别目标群体（如不健康主体）与背景群体（如健康主体）之间的独特模式来检测异常。</li>
<li>在脑MRI的上下文中，现有的CA方法依赖于有监督的对比学习或变分自编码器（VAEs），需要使用健康和不健康的数据。</li>
<li>无监督异常检测（UAD）提供了一种替代方案，通过学习健康解剖的参考表示，而无需目标样本。</li>
<li>扩散模型在UAD中越来越受欢迎，其在图像生成方面的性能优于VAEs。</li>
<li>本工作提出了一种无监督框架，通过训练自监督对比编码器提取有意义解剖特征，以改善重建质量。</li>
<li>该方法使用这些特征来调节扩散模型，以重建给定图像的健康外观，通过像素级比较实现可解释的异常定位。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.00772">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-27fccac4a2b1bfecb0dd61df49aff6bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ce763483aa4088abb60e5a6defb19b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ce8b6adc4dfa1de7c066ec3397120f9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Accurate-and-lightweight-dehazing-via-multi-receptive-field-non-local-network-and-novel-contrastive-regularization"><a href="#Accurate-and-lightweight-dehazing-via-multi-receptive-field-non-local-network-and-novel-contrastive-regularization" class="headerlink" title="Accurate and lightweight dehazing via multi-receptive-field non-local   network and novel contrastive regularization"></a>Accurate and lightweight dehazing via multi-receptive-field non-local   network and novel contrastive regularization</h2><p><strong>Authors:Zewei He, Zixuan Chen, Jinlei Li, Ziqian Lu, Xuecheng Sun, Hao Luo, Zhe-Ming Lu, Evangelos K. Markakis</strong></p>
<p>Recently, deep learning-based methods have dominated image dehazing domain. Although very competitive dehazing performance has been achieved with sophisticated models, effective solutions for extracting useful features are still under-explored. In addition, non-local network, which has made a breakthrough in many vision tasks, has not been appropriately applied to image dehazing. Thus, a multi-receptive-field non-local network (MRFNLN) consisting of the multi-stream feature attention block (MSFAB) and cross non-local block (CNLB) is presented in this paper. We start with extracting richer features for dehazing. Specifically, we design a multi-stream feature extraction (MSFE) sub-block, which contains three parallel convolutions with different receptive fields (i.e., $1\times 1$, $3\times 3$, $5\times 5$) for extracting multi-scale features. Following MSFE, we employ an attention sub-block to make the model adaptively focus on important channels&#x2F;regions. The MSFE and attention sub-blocks constitute our MSFAB. Then, we design a cross non-local block (CNLB), which can capture long-range dependencies beyond the query. Instead of the same input source of query branch, the key and value branches are enhanced by fusing more preceding features. CNLB is computation-friendly by leveraging a spatial pyramid down-sampling (SPDS) strategy to reduce the computation and memory consumption without sacrificing the performance. Last but not least, a novel detail-focused contrastive regularization (DFCR) is presented by emphasizing the low-level details and ignoring the high-level semantic information in the representation space. Comprehensive experimental results demonstrate that the proposed MRFNLN model outperforms recent state-of-the-art dehazing methods with less than 1.5 Million parameters. </p>
<blockquote>
<p>最近，基于深度学习的方法在图像去雾领域占据了主导地位。尽管利用复杂模型取得了非常有竞争力的去雾性能，但提取有用特征的有效解决方案仍受到较少的探索。此外，已经在许多视觉任务中取得突破的非局部网络尚未适当地应用于图像去雾。因此，本文提出了一种多感受野非局部网络（MRFNLN），该网络由多流特征注意力块（MSFAB）和交叉非局部块（CNLB）组成。我们从提取更丰富的去雾特征开始。具体来说，我们设计了一个多流特征提取（MSFE）子块，其中包含三个具有不同感受野（即$ 1\times 1 $、$ 3\times 3 $、$ 5\times 5 $）的并行卷积，以提取多尺度特征。在MSFE之后，我们采用注意力子块使模型自适应地关注重要的通道&#x2F;区域。MSFE和注意力子块构成了我们的MSFAB。然后，我们设计了一个交叉非局部块（CNLB），它能够捕捉超越查询的长程依赖关系。与查询分支的相同输入源不同，键和值分支通过融合更多的先前特征来增强。CNLB通过利用空间金字塔下采样（SPDS）策略在计算上更为友好，可以在不牺牲性能的情况下减少计算和内存消耗。最后但并非最不重要的是，提出了一种新的细节聚焦对比正则化（DFCR），它通过强调低层次细节而忽视表示空间中的高级语义信息。实验结果表明，所提出的MRFNLN模型在参数少于150万个的情况下，优于最新的先进去雾方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16494v2">PDF</a> submitted to the IEEE Journal for possible publication</p>
<p><strong>Summary</strong><br>     本文提出了一种多感受野非局部网络（MRFNLN），包含多流特征注意块（MSFAB）和交叉非局部块（CNLB）。旨在图像去雾领域提取更丰富特征，通过多流特征提取子块（MSFE）获取多尺度特征，并采用注意力子块使模型自适应关注重要通道&#x2F;区域。CNLB能够捕捉超出查询的长程依赖关系，并通过融合更多前期特征来提升关键值和查询分支。利用空间金字塔下采样策略，降低计算量和内存消耗而不损失性能。最后，提出了一种新的细节聚焦对比正则化（DFCR），通过强调低层次细节而忽视高层次的语义信息在表示空间内表现良好。实验结果表明，所提出的MRFNLN模型参数少于一百万五千个，但表现优于最新的去雾方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种多感受野非局部网络（MRFNLN），结合了多流特征注意块（MSFAB）和交叉非局部块（CNLB）。</li>
<li>通过多流特征提取子块（MSFE）获取多尺度特征。</li>
<li>利用注意力子块使模型能够自适应关注重要通道和区域。</li>
<li>交叉非局部块（CNLB）能捕捉超过查询的长程依赖关系。</li>
<li>采用空间金字塔下采样策略以降低计算成本和内存消耗。</li>
<li>引入细节聚焦对比正则化（DFCR），注重低层次细节的提取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.16494">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f898d4a6bd976bf30bc551f23030f49c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0128cd990e3e381ad6f8f2065f699292.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db7f39fad42b099b1bd947e1128ce910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-022fda2e467aba2a3f0a93395ef4d413.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6244c46f20e0db3480853405b8c9194f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">无监督/半监督/对比学习</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7002af9c28262ea2c83bdc26f4f96f71.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-07-03  GroundingDINO-US-SAM Text-Prompted Multi-Organ Segmentation in   Ultrasound with LoRA-Tuned Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-66df7122ad8bb673f007c253fe7b5fc0.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-07-03  Visual Textualization for Image Prompted Object Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">22950.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
