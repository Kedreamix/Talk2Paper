<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
    <meta name="description" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Spatially Gene Expression Prediction using Dual-Scale Contrastive   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0128cd990e3e381ad6f8f2065f699292.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-03-æ›´æ–°"><a href="#2025-07-03-æ›´æ–°" class="headerlink" title="2025-07-03 æ›´æ–°"></a>2025-07-03 æ›´æ–°</h1><h2 id="Spatially-Gene-Expression-Prediction-using-Dual-Scale-Contrastive-Learning"><a href="#Spatially-Gene-Expression-Prediction-using-Dual-Scale-Contrastive-Learning" class="headerlink" title="Spatially Gene Expression Prediction using Dual-Scale Contrastive   Learning"></a>Spatially Gene Expression Prediction using Dual-Scale Contrastive   Learning</h2><p><strong>Authors:Mingcheng Qu, Yuncong Wu, Donglin Di, Yue Gao, Tonghua Su, Yang Song, Lei Fan</strong></p>
<p>Spatial transcriptomics (ST) provides crucial insights into tissue micro-environments, but is limited to its high cost and complexity. As an alternative, predicting gene expression from pathology whole slide images (WSI) is gaining increasing attention. However, existing methods typically rely on single patches or a single pathology modality, neglecting the complex spatial and molecular interactions between target and neighboring information (e.g., gene co-expression). This leads to a failure in establishing connections among adjacent regions and capturing intricate cross-modal relationships. To address these issues, we propose NH2ST, a framework that integrates spatial context and both pathology and gene modalities for gene expression prediction. Our model comprises a query branch and a neighbor branch to process paired target patch and gene data and their neighboring regions, where cross-attention and contrastive learning are employed to capture intrinsic associations and ensure alignments between pathology and gene expression. Extensive experiments on six datasets demonstrate that our model consistently outperforms existing methods, achieving over 20% in PCC metrics. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/MCPathology/NH2ST">https://github.com/MCPathology/NH2ST</a> </p>
<blockquote>
<p>ç©ºé—´è½¬å½•ç»„å­¦ï¼ˆSTï¼‰ä¸ºç»„ç»‡å¾®ç¯å¢ƒæä¾›äº†å…³é”®è§è§£ï¼Œä½†å…¶é«˜æ˜‚çš„æˆæœ¬å’Œå¤æ‚æ€§é™åˆ¶äº†å…¶åº”ç”¨ã€‚ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼Œä»ç—…ç†å­¦å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰é¢„æµ‹åŸºå› è¡¨è¾¾æ­£å¼•èµ·è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå•ä¸ªè¡¥ä¸æˆ–å•ä¸€ç—…ç†å­¦æ¨¡å¼ï¼Œå¿½è§†äº†ç›®æ ‡ä¿¡æ¯ä¸é‚»è¿‘ä¿¡æ¯ä¹‹é—´å¤æ‚çš„ç©ºé—´åˆ†å­äº¤äº’ï¼ˆä¾‹å¦‚åŸºå› å…±è¡¨è¾¾ï¼‰ã€‚è¿™å¯¼è‡´äº†åœ¨å»ºç«‹ç›¸é‚»åŒºåŸŸä¹‹é—´çš„è”ç³»å’Œæ•æ‰å¤æ‚çš„è·¨æ¨¡å¼å…³ç³»æ–¹é¢çš„å¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†NH2STæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†ç©ºé—´ä¸Šä¸‹æ–‡ä»¥åŠç—…ç†å­¦å’ŒåŸºå› æ¨¡å¼æ¥è¿›è¡ŒåŸºå› è¡¨è¾¾é¢„æµ‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹åŒ…æ‹¬æŸ¥è¯¢åˆ†æ”¯å’Œé‚»å±…åˆ†æ”¯ï¼Œç”¨äºå¤„ç†æˆå¯¹çš„ç›®æ ‡è¡¥ä¸å’ŒåŸºå› æ•°æ®åŠå…¶é‚»è¿‘åŒºåŸŸï¼Œå…¶ä¸­é‡‡ç”¨äº¤å‰æ³¨æ„åŠ›å’Œå¯¹æ¯”å­¦ä¹ æ¥æ•æ‰å†…åœ¨å…³è”å¹¶ç¡®ä¿ç—…ç†ä¸åŸºå› è¡¨è¾¾ä¹‹é—´çš„å¯¹é½ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨PCCæŒ‡æ ‡ä¸Šè¾¾åˆ°20%ä»¥ä¸Šçš„æå‡ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MCPathology/NH2ST%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MCPathology/NH2STä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23827v1">PDF</a> Our paper has been accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>ç©ºé—´è½¬å½•å­¦ï¼ˆSTï¼‰å¯¹äºç ”ç©¶ç»„ç»‡å¾®ç¯å¢ƒå…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†å…¶é«˜æ˜‚æˆæœ¬å’Œå¤æ‚æ€§é™åˆ¶äº†åº”ç”¨ã€‚å› æ­¤ï¼Œé€šè¿‡ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒé¢„æµ‹åŸºå› è¡¨è¾¾å¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–å•ä¸€çš„åˆ‡ç‰‡æˆ–å•ä¸€ç—…ç†æ¨¡å¼ï¼Œå¿½ç•¥äº†ç›®æ ‡åŒºåŸŸä¸é‚»è¿‘ä¿¡æ¯é—´çš„å¤æ‚ç©ºé—´ä¸åˆ†å­äº¤äº’ï¼ˆå¦‚åŸºå› å…±è¡¨è¾¾ï¼‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†NH2STæ¡†æ¶ï¼Œå®ƒç»“åˆäº†ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥åŠç—…ç†å’ŒåŸºå› ä¸¤ç§æ¨¡å¼æ¥è¿›è¡ŒåŸºå› è¡¨è¾¾é¢„æµ‹ã€‚è¯¥æ¨¡å‹åŒ…å«æŸ¥è¯¢åˆ†æ”¯å’Œé‚»è¿‘åˆ†æ”¯ï¼Œç”¨äºå¤„ç†ç›®æ ‡åŒºåŸŸå’ŒåŸºå› æ•°æ®çš„é…å¯¹ä»¥åŠç›¸é‚»åŒºåŸŸçš„ä¿¡æ¯ã€‚é€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œå¯¹æ¯”å­¦ä¹ ï¼Œæ•æ‰å†…åœ¨è”ç³»å¹¶ç¡®ä¿ç—…ç†ä¸åŸºå› è¡¨è¾¾ä¹‹é—´çš„å¯¹é½ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨PCCæŒ‡æ ‡ä¸Šæé«˜äº†è¶…è¿‡20%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´è½¬å½•å­¦å¯¹äºç ”ç©¶ç»„ç»‡å¾®ç¯å¢ƒå…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†å­˜åœ¨é«˜æˆæœ¬å’Œå¤æ‚æ€§çš„é™åˆ¶ã€‚</li>
<li>é€šè¿‡ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒé¢„æµ‹åŸºå› è¡¨è¾¾æ˜¯ä¸€ä¸ªå¤‡å—å…³æ³¨çš„ç ”ç©¶æ–¹å‘ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†ç›®æ ‡åŒºåŸŸä¸é‚»è¿‘ä¿¡æ¯é—´çš„å¤æ‚ç©ºé—´ä¸åˆ†å­äº¤äº’ã€‚</li>
<li>NH2STæ¡†æ¶ç»“åˆäº†ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥åŠç—…ç†å’ŒåŸºå› ä¸¤ç§æ¨¡å¼è¿›è¡ŒåŸºå› è¡¨è¾¾é¢„æµ‹ã€‚</li>
<li>NH2STæ¨¡å‹åŒ…å«æŸ¥è¯¢åˆ†æ”¯å’Œé‚»è¿‘åˆ†æ”¯ï¼Œç”¨äºå¤„ç†ç›®æ ‡åŒºåŸŸå’ŒåŸºå› æ•°æ®çš„é…å¯¹ä»¥åŠç›¸é‚»åŒºåŸŸçš„ä¿¡æ¯ã€‚</li>
<li>NH2STé€šè¿‡äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’Œå¯¹æ¯”å­¦ä¹ æ¥æ•æ‰å†…åœ¨è”ç³»å’Œç¡®ä¿ç—…ç†ä¸åŸºå› è¡¨è¾¾ä¹‹é—´çš„å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d52c65f59f6b90f4ae8a21c5ff6ac2fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ea594d3b602ffca82deee74d97ec227.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb14abf25fa2b5af0f3a4f7dafdd1f67.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AdFair-CLIP-Adversarial-Fair-Contrastive-Language-Image-Pre-training-for-Chest-X-rays"><a href="#AdFair-CLIP-Adversarial-Fair-Contrastive-Language-Image-Pre-training-for-Chest-X-rays" class="headerlink" title="AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training   for Chest X-rays"></a>AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training   for Chest X-rays</h2><p><strong>Authors:Chenlang Yi, Zizhan Xiong, Qi Qi, Xiyuan Wei, Girish Bathla, Ching-Long Lin, Bobak Jack Mortazavi, Tianbao Yang</strong></p>
<p>Contrastive Language-Image Pre-training (CLIP) models have demonstrated superior performance across various visual tasks including medical image classification. However, fairness concerns, including demographic biases, have received limited attention for CLIP models. This oversight leads to critical issues, particularly those related to race and gender, resulting in disparities in diagnostic outcomes and reduced reliability for underrepresented groups. To address these challenges, we introduce AdFair-CLIP, a novel framework employing adversarial feature intervention to suppress sensitive attributes, thereby mitigating spurious correlations and improving prediction fairness. We conduct comprehensive experiments on chest X-ray (CXR) datasets, and show that AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while maintaining robust generalization in zero-shot and few-shot scenarios. These results establish new benchmarks for fairness-aware learning in CLIP-based medical diagnostic models, particularly for CXR analysis. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹åœ¨åŒ…æ‹¬åŒ»å­¦å›¾åƒåˆ†ç±»åœ¨å†…çš„å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºCLIPæ¨¡å‹çš„å…¬å¹³æ€§å…³æ³¨ï¼ŒåŒ…æ‹¬äººå£ç»Ÿè®¡åè§ï¼Œå¹¶æœªå¾—åˆ°è¶³å¤Ÿçš„é‡è§†ã€‚è¿™ç§ç–å¿½å¯¼è‡´äº†å…³é”®é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä¸ç§æ—å’Œæ€§åˆ«ç›¸å…³çš„é—®é¢˜ï¼Œè¿›è€Œå¯¼è‡´è¯Šæ–­ç»“æœçš„ä¸å…¬å¹³å’Œå¯¹ä»£è¡¨æ€§ä¸è¶³çš„ç¾¤ä½“çš„å¯é æ€§é™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AdFair-CLIPï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨å¯¹æŠ—æ€§ç‰¹å¾å¹²é¢„æ¥æŠ‘åˆ¶æ•æ„Ÿå±æ€§ã€ä»è€Œå‡è½»è™šå‡å…³è”å¹¶æ”¹å–„é¢„æµ‹å…¬å¹³æ€§çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œç»“æœè¡¨æ˜AdFair-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸­ï¼Œæ˜¾è‘—æé«˜äº†å…¬å¹³æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœä¸ºCLIPåŸºç¡€çš„åŒ»å­¦è¯Šæ–­æ¨¡å‹ä¸­çš„å…¬å¹³æ€§æ„ŸçŸ¥å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯CXRåˆ†æï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23467v1">PDF</a> This preprint has been accepted by MICCAI 2025</p>
<p><strong>Summary</strong><br>     å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹åœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ä¸­å­˜åœ¨å…¬å¹³æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ä¸ç§æ—å’Œæ€§åˆ«ç›¸å…³çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºAdFair-CLIPæ¡†æ¶ï¼Œé‡‡ç”¨å¯¹æŠ—ç‰¹å¾å¹²é¢„æŠ‘åˆ¶æ•æ„Ÿå±æ€§ï¼Œå‡å°‘è™šå‡å…³è”ï¼Œæé«˜é¢„æµ‹å…¬å¹³æ€§ã€‚åœ¨èƒ¸ç‰‡Xçº¿æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAdFair-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹ï¼Œèƒ½æ˜¾è‘—æé«˜å…¬å¹³æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ï¼Œå¹¶ä¿æŒç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºCLIPåŸºåŒ»ç–—è¯Šæ–­æ¨¡å‹çš„å…¬å¹³æ€§è®¤çŸ¥å­¦ä¹ æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹åœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ä¸­å­˜åœ¨å…¬å¹³æ€§é—®é¢˜ã€‚</li>
<li>AdFair-CLIPæ¡†æ¶è¢«æå‡ºæ¥è§£å†³CLIPæ¨¡å‹çš„å…¬å¹³æ€§é—®é¢˜ã€‚</li>
<li>AdFair-CLIPé‡‡ç”¨å¯¹æŠ—ç‰¹å¾å¹²é¢„æ¥æŠ‘åˆ¶æ•æ„Ÿå±æ€§ï¼Œä»è€Œå‡å°‘è™šå‡å…³è”ã€‚</li>
<li>AdFair-CLIPèƒ½æé«˜é¢„æµ‹å…¬å¹³æ€§ã€‚</li>
<li>åœ¨èƒ¸ç‰‡Xçº¿æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAdFair-CLIPåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹èƒ½æ˜¾è‘—æé«˜å…¬å¹³æ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>AdFair-CLIPæ¡†æ¶çš„å¼•å…¥ä¸ºCLIPåŸºåŒ»ç–—è¯Šæ–­æ¨¡å‹çš„å…¬å¹³æ€§è®¤çŸ¥å­¦ä¹ æ ‘ç«‹äº†æ–°æ ‡æ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a694931f373977ed867e26a7a91742c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3b099463c7072cddecb002adc2e6032.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a56d0f31bb5da36140ab15e2ebae7779.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Contrastive-Learning-with-Diffusion-Features-for-Weakly-Supervised-Medical-Image-Segmentation"><a href="#Contrastive-Learning-with-Diffusion-Features-for-Weakly-Supervised-Medical-Image-Segmentation" class="headerlink" title="Contrastive Learning with Diffusion Features for Weakly Supervised   Medical Image Segmentation"></a>Contrastive Learning with Diffusion Features for Weakly Supervised   Medical Image Segmentation</h2><p><strong>Authors:Dewen Zeng, Xinrong Hu, Yu-Jen Chen, Yawen Wu, Xiaowei Xu, Yiyu Shi</strong></p>
<p>Weakly supervised semantic segmentation (WSSS) methods using class labels often rely on class activation maps (CAMs) to localize objects. However, traditional CAM-based methods struggle with partial activations and imprecise object boundaries due to optimization discrepancies between classification and segmentation. Recently, the conditional diffusion model (CDM) has been used as an alternative for generating segmentation masks in WSSS, leveraging its strong image generation capabilities tailored to specific class distributions. By modifying or perturbing the condition during diffusion sampling, the related objects can be highlighted in the generated images. Yet, the saliency maps generated by CDMs are prone to noise from background alterations during reverse diffusion. To alleviate the problem, we introduce Contrastive Learning with Diffusion Features (CLDF), a novel method that uses contrastive learning to train a pixel decoder to map the diffusion features from a frozen CDM to a low-dimensional embedding space for segmentation. Specifically, we integrate gradient maps generated from CDM external classifier with CAMs to identify foreground and background pixels with fewer false positives&#x2F;negatives for contrastive learning, enabling robust pixel embedding learning. Experimental results on four segmentation tasks from two public medical datasets demonstrate that our method significantly outperforms existing baselines. </p>
<blockquote>
<p>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ–¹æ³•é€šå¸¸ä½¿ç”¨ç±»åˆ«æ ‡ç­¾ï¼Œå¹¶ä¾èµ–äºç±»åˆ«æ¿€æ´»å›¾ï¼ˆCAMsï¼‰è¿›è¡Œå¯¹è±¡å®šä½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„åŸºäºCAMçš„æ–¹æ³•åœ¨éƒ¨åˆ†æ¿€æ´»å’Œä¼˜åŒ–åˆ†ç±»ä¸åˆ†å‰²ä¹‹é—´å·®å¼‚å¯¼è‡´çš„ç²¾ç¡®å¯¹è±¡è¾¹ç•Œæ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ€è¿‘ï¼Œæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆCDMï¼‰å·²è¢«ç”¨ä½œWSSSä¸­ç”Ÿæˆåˆ†å‰²æ©æ¨¡çš„æ›¿ä»£æ–¹æ³•ï¼Œåˆ©ç”¨å…¶é’ˆå¯¹ç‰¹å®šç±»åˆ«åˆ†å¸ƒçš„å¼ºå¤§å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡åœ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­ä¿®æ”¹æˆ–æ‰°åŠ¨æ¡ä»¶ï¼Œå¯ä»¥åœ¨ç”Ÿæˆçš„å›¾åƒä¸­çªå‡ºæ˜¾ç¤ºç›¸å…³å¯¹è±¡ã€‚ç„¶è€Œï¼Œç”±CDMç”Ÿæˆçš„æ˜¾è‘—æ€§åœ°å›¾å®¹æ˜“å—åˆ°åå‘æ‰©æ•£è¿‡ç¨‹ä¸­èƒŒæ™¯æ”¹å˜äº§ç”Ÿçš„å™ªå£°å½±å“ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯¹æ¯”å­¦ä¹ ä¸æ‰©æ•£ç‰¹å¾ï¼ˆCLDFï¼‰ç›¸ç»“åˆçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒåƒç´ è§£ç å™¨ï¼Œå°†æ¥è‡ªå†»ç»“CDMçš„æ‰©æ•£ç‰¹å¾æ˜ å°„åˆ°ä½ç»´åµŒå…¥ç©ºé—´ä»¥è¿›è¡Œåˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ç”±CDMå¤–éƒ¨åˆ†ç±»å™¨ç”Ÿæˆçš„æ¢¯åº¦å›¾ä¸CAMç›¸ç»“åˆï¼Œä»¥è¯†åˆ«å‰æ™¯å’ŒèƒŒæ™¯åƒç´ ï¼Œå¹¶å‡å°‘å¯¹æ¯”å­¦ä¹ ä¸­è¯¯æŠ¥&#x2F;æ¼æŠ¥çš„æ•°é‡ï¼Œä»è€Œå®ç°ç¨³å¥çš„åƒç´ åµŒå…¥å­¦ä¹ ã€‚åœ¨æ¥è‡ªä¸¤ä¸ªå…¬å…±åŒ»ç–—æ•°æ®é›†çš„å››ä¸ªåˆ†å‰²ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23460v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰å¸¸ä½¿ç”¨ç±»åˆ«æ ‡ç­¾å¹¶åˆ©ç”¨ç±»æ¿€æ´»å›¾ï¼ˆCAMsï¼‰è¿›è¡Œç›®æ ‡å®šä½ã€‚ç„¶è€Œï¼Œä¼ ç»ŸåŸºäºCAMçš„æ–¹æ³•å› åˆ†ç±»ä¸åˆ†å‰²çš„ä¼˜åŒ–å·®å¼‚è€Œé¢ä¸´éƒ¨åˆ†æ¿€æ´»å’Œä¸ç²¾ç¡®ç›®æ ‡è¾¹ç•Œçš„é—®é¢˜ã€‚æœ€è¿‘ï¼Œæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆCDMï¼‰è¢«ç”¨äºç”ŸæˆWSSSä¸­çš„åˆ†å‰²æ©è†œï¼Œåˆ©ç”¨å…¶é’ˆå¯¹ç‰¹å®šç±»åˆ«åˆ†å¸ƒçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡ä¿®æ”¹æˆ–æ‰°åŠ¨æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­çš„æ¡ä»¶ï¼Œå¯ä»¥çªå‡ºç›¸å…³ç›®æ ‡ã€‚ç„¶è€Œï¼ŒCDMç”Ÿæˆçš„æ˜¾è‘—æ€§å›¾åœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­æ˜“å—èƒŒæ™¯å˜åŒ–äº§ç”Ÿçš„å™ªå£°å½±å“ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒåƒç´ è§£ç å™¨çš„æ–¹æ³•ï¼ˆCLDFï¼‰ï¼Œå°†CDMçš„æ‰©æ•£ç‰¹å¾æ˜ å°„åˆ°ä½ç»´åµŒå…¥ç©ºé—´è¿›è¡Œåˆ†å‰²ã€‚æˆ‘ä»¬æ•´åˆCDMå¤–éƒ¨åˆ†ç±»å™¨ç”Ÿæˆçš„æ¢¯åº¦å›¾ä¸CAMsï¼Œå‡å°‘å¯¹æ¯”å­¦ä¹ ä¸­çš„è¯¯æŠ¥&#x2F;æ¼æŠ¥ï¼Œå®ç°ç¨³å¥çš„åƒç´ åµŒå…¥å­¦ä¹ ã€‚åœ¨å…¬å…±åŒ»ç–—æ•°æ®é›†çš„å››ä¸ªåˆ†å‰²ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WSSSæ–¹æ³•å¸¸ä½¿ç”¨CAMsè¿›è¡Œç›®æ ‡å®šä½ï¼Œä½†é¢ä¸´éƒ¨åˆ†æ¿€æ´»å’Œä¸ç²¾ç¡®ç›®æ ‡è¾¹ç•Œçš„æŒ‘æˆ˜ã€‚</li>
<li>CDMç”¨äºWSSSä¸­çš„åˆ†å‰²æ©è†œç”Ÿæˆï¼Œå…·æœ‰é’ˆå¯¹ç‰¹å®šç±»åˆ«åˆ†å¸ƒçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>CDMç”Ÿæˆçš„æ˜¾è‘—æ€§å›¾åœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­æ˜“å—å™ªå£°å½±å“ã€‚</li>
<li>æå‡ºCLDFæ–¹æ³•ï¼Œç»“åˆå¯¹æ¯”å­¦ä¹ è®­ç»ƒåƒç´ è§£ç å™¨ï¼Œå°†CDMçš„æ‰©æ•£ç‰¹å¾æ˜ å°„åˆ°ä½ç»´åµŒå…¥ç©ºé—´è¿›è¡Œåˆ†å‰²ã€‚</li>
<li>æ•´åˆæ¢¯åº¦å›¾å’ŒCAMsä»¥æé«˜å¯¹æ¯”å­¦ä¹ çš„å‡†ç¡®æ€§ï¼Œå‡å°‘è¯¯æŠ¥&#x2F;æ¼æŠ¥ã€‚</li>
<li>CLDFæ–¹æ³•åœ¨å…¬å…±åŒ»ç–—æ•°æ®é›†çš„å››ä¸ªåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-455f35321a7f25148ed921842a7f3af6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-073432a04132db70cb746f7ace48a58b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9402e69e63a67620e791f8b87e50943.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239f00fe2f56718ae174e285877ecb66.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dynamic-Contrastive-Learning-for-Hierarchical-Retrieval-A-Case-Study-of-Distance-Aware-Cross-View-Geo-Localization"><a href="#Dynamic-Contrastive-Learning-for-Hierarchical-Retrieval-A-Case-Study-of-Distance-Aware-Cross-View-Geo-Localization" class="headerlink" title="Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of   Distance-Aware Cross-View Geo-Localization"></a>Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of   Distance-Aware Cross-View Geo-Localization</h2><p><strong>Authors:Suofei Zhang, Xinxin Wang, Xiaofu Wu, Quan Zhou, Haifeng Hu</strong></p>
<p>Existing deep learning-based cross-view geo-localization methods primarily focus on improving the accuracy of cross-domain image matching, rather than enabling models to comprehensively capture contextual information around the target and minimize the cost of localization errors. To support systematic research into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem, we construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs multi-view imagery with precise distance annotations across three spatial resolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical retrieval problem across different domains. Our study further reveals that, due to the inherent complexity of spatial relationships among buildings, this problem can only be addressed via a contrastive learning paradigm, rather than conventional metric learning. To tackle this challenge, we propose Dynamic Contrastive Learning (DyCL), a novel framework that progressively aligns feature representations according to hierarchical spatial margins. Extensive experiments demonstrate that DyCL is highly complementary to existing multi-scale metric learning methods and yields substantial improvements in both hierarchical retrieval performance and overall cross-view geo-localization accuracy. Our code and benchmark are publicly available at <a target="_blank" rel="noopener" href="https://github.com/anocodetest1/DyCL">https://github.com/anocodetest1/DyCL</a>. </p>
<blockquote>
<p>ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„è·¨è§†å›¾åœ°ç†å®šä½æ–¹æ³•ä¸»è¦å…³æ³¨æ”¹è¿›è·¨åŸŸå›¾åƒåŒ¹é…çš„å‡†ç¡®æ€§ï¼Œè€Œä¸æ˜¯ä½¿æ¨¡å‹èƒ½å¤Ÿå…¨é¢æ•è·ç›®æ ‡å‘¨å›´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶æœ€å°åŒ–å®šä½é”™è¯¯æˆæœ¬ã€‚ä¸ºäº†æ”¯æŒå¯¹è·ç¦»æ„ŸçŸ¥è·¨è§†å›¾åœ°ç†å®šä½ï¼ˆDACVGLï¼‰é—®é¢˜çš„ç³»ç»Ÿç ”ç©¶ï¼Œæˆ‘ä»¬æ„å»ºäº†è·ç¦»æ„ŸçŸ¥æ ¡å›­ï¼ˆDA-Campusï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†å¤šè§†å›¾å›¾åƒä¸ä¸‰ç§ç©ºé—´åˆ†è¾¨ç‡çš„ç²¾ç¡®è·ç¦»æ³¨é‡Šé…å¯¹åœ¨ä¸€èµ·çš„åŸºå‡†æµ‹è¯•ã€‚åŸºäºDA-Campusï¼Œæˆ‘ä»¬å°†DACVGLåˆ¶å®šä¸ºä¸åŒé¢†åŸŸçš„åˆ†å±‚æ£€ç´¢é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿›ä¸€æ­¥è¡¨æ˜ï¼Œç”±äºå»ºç­‘ç‰©ä¹‹é—´ç©ºé—´å…³ç³»çš„å›ºæœ‰å¤æ‚æ€§ï¼Œè¿™ä¸ªé—®é¢˜åªèƒ½é€šè¿‡å¯¹æ¯”å­¦ä¹ èŒƒå¼æ¥è§£å†³ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„åº¦é‡å­¦ä¹ ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€å¯¹æ¯”å­¦ä¹ ï¼ˆDyCLï¼‰è¿™ä¸€æ–°é¢–æ¡†æ¶ï¼Œå®ƒæ ¹æ®åˆ†å±‚ç©ºé—´è¾¹ç•Œé€æ­¥å¯¹é½ç‰¹å¾è¡¨ç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDyCLä¸ç°æœ‰çš„å¤šå°ºåº¦åº¦é‡å­¦ä¹ æ–¹æ³•é«˜åº¦äº’è¡¥ï¼Œåœ¨åˆ†å±‚æ£€ç´¢æ€§èƒ½å’Œæ€»ä½“è·¨è§†å›¾åœ°ç†å®šä½å‡†ç¡®æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬çš„ä»£ç å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/anocodetest1/DyCL%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/anocodetest1/DyCLå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23077v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªåä¸ºDA-Campusçš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°ç»“åˆäº†å¤šè§†è§’å›¾åƒå’Œç²¾ç¡®çš„è·ç¦»æ ‡æ³¨ï¼Œç©ºé—´åˆ†è¾¨ç‡å„å¼‚ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå°†è·¨è§†å›¾åœ°ç†å®šä½ï¼ˆDACVGLï¼‰é—®é¢˜è½¬åŒ–ä¸ºå±‚æ¬¡æ£€ç´¢é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œç”±äºå»ºç­‘ç‰©ä¹‹é—´ç©ºé—´å…³ç³»çš„å›ºæœ‰å¤æ‚æ€§ï¼Œè¯¥é—®é¢˜åªèƒ½é€šè¿‡å¯¹æ¯”å­¦ä¹ æ¨¡å¼è€Œéä¼ ç»Ÿåº¦é‡å­¦ä¹ æ¥è§£å†³ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†åŠ¨æ€å¯¹æ¯”å­¦ä¹ ï¼ˆDyCLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ¹æ®å±‚æ¬¡ç©ºé—´è¾¹ç•Œé€æ­¥å¯¹é½ç‰¹å¾è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒDyCLå¯¹ç°æœ‰å¤šå°ºåº¦åº¦é‡å­¦ä¹ æ–¹æ³•å…·æœ‰äº’è¡¥æ€§ï¼Œèƒ½æ˜¾è‘—æé«˜å±‚æ¬¡æ£€ç´¢æ€§èƒ½å’Œè·¨è§†å›¾åœ°ç†å®šä½å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†Distance-Aware Cross-View Geo-Localization (DACVGL)é—®é¢˜ï¼Œå¹¶æ„å»ºäº†DA-CampusåŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«å¤šè§†è§’å›¾åƒå’Œç²¾ç¡®è·ç¦»æ ‡æ³¨ã€‚</li>
<li>å°†DACVGLé—®é¢˜è½¬åŒ–ä¸ºå±‚æ¬¡æ£€ç´¢é—®é¢˜ï¼Œå¼ºè°ƒåœ¨è·¨åŸŸå›¾åƒåŒ¹é…ä¸­å…¨é¢æ•æ‰ç›®æ ‡ä¸Šä¸‹æ–‡ä¿¡æ¯çš„é‡è¦æ€§ã€‚</li>
<li>æŒ‡å‡ºç”±äºå»ºç­‘ç‰©é—´ç©ºé—´å…³ç³»çš„å¤æ‚æ€§ï¼Œè§£å†³DACVGLé—®é¢˜éœ€è¦é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ¨¡å¼ã€‚</li>
<li>æå‡ºäº†Dynamic Contrastive Learning (DyCL)æ¡†æ¶ï¼Œèƒ½æŒ‰å±‚æ¬¡ç©ºé—´è¾¹ç•Œé€æ­¥å¯¹é½ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>DyCLæ¡†æ¶å¯¹ç°æœ‰å¤šå°ºåº¦åº¦é‡å­¦ä¹ æ–¹æ³•å…·æœ‰äº’è¡¥æ€§ï¼Œèƒ½æ˜¾è‘—æé«˜å±‚æ¬¡æ£€ç´¢å’Œè·¨è§†å›¾åœ°ç†å®šä½çš„å‡†ç¡®æ€§ã€‚</li>
<li>å…¬å¼€äº†ç ”ç©¶ä»£ç å’ŒåŸºå‡†æµ‹è¯•å¹³å°ï¼Œä¾¿äºå…¶ä»–äººä½¿ç”¨å’Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1d4d84b7da50fc0441f4d519c4fe215d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ed4db0690363a7ee94aab46fe03724a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdf90f2c18d6b1617a8caebeef6dfda5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a091341ebc0ca12a368749fb3bb9901e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-470ac61b0de5937bb96ee8892367c9a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e502922a6710370db8082c5fd96a05c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7a929bd2cab12387f70cc355a6f9482.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="How-Semantically-Informative-is-an-Image-Measuring-the-Covariance-Weighted-Norm-of-Contrastive-Learning-Embeddings"><a href="#How-Semantically-Informative-is-an-Image-Measuring-the-Covariance-Weighted-Norm-of-Contrastive-Learning-Embeddings" class="headerlink" title="How Semantically Informative is an Image?: Measuring the   Covariance-Weighted Norm of Contrastive Learning Embeddings"></a>How Semantically Informative is an Image?: Measuring the   Covariance-Weighted Norm of Contrastive Learning Embeddings</h2><p><strong>Authors:Fumiya Uchiyama, Rintaro Yanagi, Shohei Taniguchi, Shota Takashiro, Masahiro Suzuki, Hirokatsu Kataoka, Yusuke Iwasawa, Yutaka Matsuo</strong></p>
<p>Contrastive learning has the capacity to model multimodal probability distributions by embedding and aligning visual representations with semantics from captions. This approach enables the estimation of relational semantic similarity; however, it remains unclear whether it can also represent absolute semantic informativeness. In this work, we introduce a semantic informativeness metric for an image calculated from text samples via a contrastive learning model; similarly, the informativeness of a text is calculated from image samples. We propose a redefinition of the concept of Information Gain, a concept previously explored in natural language processing, extending its application to the domains of vision and language. Our metric quantifies how conditioning on an image distorts the distribution of associated texts, and vice versa for text conditioning on image distributions. In OpenCLIPâ€™s empirical results, we observe that images with the lowest Information Gain scores often correspond to placeholder icons such as â€œimage not found.â€ Furthermore, we propose to measure a norm-based metric of the embedding to estimate the Information Gain, following the theoretical results for Skip-Gram with Negative Sampling (SGNS) word embedding. Information Gain can be measured using either CLIP or SigLIP, and the results demonstrate a strong correlation with a coefficient of determination ranging from 0.98 to 1.00. After obtaining the mean and the covariance of the sample embedding, the computational cost of this method is independent of the sample size, and it is compatible with publicly available, open-weight models. </p>
<blockquote>
<p>å¯¹æ¯”å­¦ä¹ å…·æœ‰é€šè¿‡åµŒå…¥å’Œå¯¹é½æ¥è‡ªå­—å¹•çš„è¯­ä¹‰æ¥å»ºæ¨¡å¤šæ¨¡æ€æ¦‚ç‡åˆ†å¸ƒçš„èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿä¼°è®¡å…³ç³»è¯­ä¹‰ç›¸ä¼¼æ€§ï¼›ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šå®ƒæ˜¯å¦èƒ½ä»£è¡¨ç»å¯¹çš„è¯­ä¹‰ä¿¡æ¯æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€šè¿‡å¯¹æ¯”å­¦ä¹ æ¨¡å‹ä»æ–‡æœ¬æ ·æœ¬è®¡ç®—å›¾åƒè¯­ä¹‰ä¿¡æ¯æ€§çš„åº¦é‡æ ‡å‡†ï¼›åŒæ ·ï¼Œæ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯æ€§ä¹Ÿæ˜¯ä»å›¾åƒæ ·æœ¬ä¸­è®¡ç®—å¾—å‡ºçš„ã€‚æˆ‘ä»¬é‡æ–°å®šä¹‰äº†ä¿¡æ¯å¢ç›Šçš„æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ä¸ªä¹‹å‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ¢ç´¢è¿‡çš„æ¦‚å¿µï¼Œå°†å…¶æ‰©å±•åˆ°è§†è§‰å’Œè¯­è¨€é¢†åŸŸã€‚æˆ‘ä»¬çš„åº¦é‡æ ‡å‡†è¡¡é‡äº†ä»¥å›¾åƒä¸ºæ¡ä»¶æ—¶ç›¸å…³æ–‡æœ¬åˆ†å¸ƒçš„æ‰­æ›²ç¨‹åº¦ï¼Œåä¹‹äº¦ç„¶ã€‚åœ¨OpenCLIPçš„å®è¯ç»“æœä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¿¡æ¯å¢ç›Šå¾—åˆ†æœ€ä½çš„å›¾ç‰‡é€šå¸¸å¯¹åº”äºå ä½ç¬¦å›¾æ ‡ï¼Œå¦‚â€œå›¾ç‰‡æœªæ‰¾åˆ°â€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºåŸºäºåµŒå…¥çš„èŒƒæ•°åº¦é‡æ¥ä¼°è®¡ä¿¡æ¯å¢ç›Šï¼Œéµå¾ªSkip-Gramè´Ÿé‡‡æ ·ï¼ˆSGNSï¼‰è¯åµŒå…¥çš„ç†è®ºç»“æœã€‚ä¿¡æ¯å¢ç›Šå¯ä»¥ä½¿ç”¨CLIPæˆ–SigLIPè¿›è¡Œæµ‹é‡ï¼Œç»“æœæ˜¾ç¤ºä¸ä»0.98åˆ°1.00çš„å†³å®šç³»æ•°æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚åœ¨è·å¾—æ ·æœ¬åµŒå…¥çš„å‡å€¼å’Œåæ–¹å·®ä¹‹åï¼Œè¯¥æ–¹æ³•çš„è®¡ç®—æˆæœ¬ç‹¬ç«‹äºæ ·æœ¬å¤§å°ï¼Œå¹¶ä¸”å®ƒä¸å…¬å¼€å¯ç”¨çš„å¼€æºæƒé‡æ¨¡å‹å…¼å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22881v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¯¹æ¯”å­¦ä¹ åœ¨è§†è§‰å’Œè¯­ä¹‰èåˆä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„è¯­ä¹‰ä¿¡æ¯é‡åº¦é‡æ–¹æ³•ã€‚é€šè¿‡è®¡ç®—å›¾åƒä¸æ–‡æœ¬æ ·æœ¬ä¹‹é—´çš„ä¿¡æ¯å¢ç›Šï¼Œè¯¥åº¦é‡æ–¹æ³•å¯ä»¥è¯„ä¼°å›¾åƒæˆ–æ–‡æœ¬æ‰€æºå¸¦çš„è¯­ä¹‰ä¿¡æ¯é‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¿¡æ¯å¢ç›Šä¸Skip-Gramè´Ÿé‡‡æ ·è¯åµŒå…¥ç†è®ºç»“æœç›¸ç¬¦ï¼Œå¹¶ä¸”é€‚ç”¨äºå…¬å¼€å¯ç”¨çš„æ¨¡å‹ã€‚æ­¤æ–¹æ³•è®¡ç®—æˆæœ¬ä½ï¼Œæ ·æœ¬åµŒå…¥çš„å‡å€¼å’Œåæ–¹å·®ç‹¬ç«‹äºæ ·æœ¬å¤§å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”å­¦ä¹ èƒ½å¤Ÿå»ºæ¨¡å¤šæ¨¡æ€æ¦‚ç‡åˆ†å¸ƒï¼Œé€šè¿‡åµŒå…¥å’Œå¯¹é½è§†è§‰è¡¨ç¤ºä¸è¯­ä¹‰å­—å¹•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„è¯­ä¹‰ä¿¡æ¯é‡åº¦é‡æ–¹æ³•ï¼Œå¯ä»¥è®¡ç®—å›¾åƒå’Œæ–‡æœ¬æ ·æœ¬ä¹‹é—´çš„ä¿¡æ¯å¢ç›Šã€‚</li>
<li>ä¿¡æ¯å¢ç›Šçš„æ¦‚å¿µè¢«é‡æ–°å®šä¹‰å¹¶æ‰©å±•åˆ°è§†è§‰å’Œè¯­è¨€é¢†åŸŸã€‚</li>
<li>å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„ä¿¡æ¯å¢ç›Šåº¦é‡å¯ä»¥è¯„ä¼°å®ƒä»¬æ‰€æºå¸¦çš„è¯­ä¹‰ä¿¡æ¯é‡ã€‚</li>
<li>ä¿¡æ¯å¢ç›Šçš„è®¡ç®—åŸºäºæ ·æœ¬åµŒå…¥çš„èŒƒæ•°ï¼Œä¸Skip-Gramè´Ÿé‡‡æ ·è¯åµŒå…¥ç†è®ºç›¸ç¬¦ã€‚</li>
<li>ä¿¡æ¯å¢ç›Šçš„è®¡ç®—æ–¹æ³•è®¡ç®—æˆæœ¬ä½ï¼Œå¹¶ä¸”é€‚ç”¨äºå…¬å¼€å¯ç”¨çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3380ee73a42d96d6db45c6d627576951.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5dacaf0d9fb6a8fa46c98d91cb29d9fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27fe2272d39c17683e68418a72893a91.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MiCo-Multi-image-Contrast-for-Reinforcement-Visual-Reasoning"><a href="#MiCo-Multi-image-Contrast-for-Reinforcement-Visual-Reasoning" class="headerlink" title="MiCo: Multi-image Contrast for Reinforcement Visual Reasoning"></a>MiCo: Multi-image Contrast for Reinforcement Visual Reasoning</h2><p><strong>Authors:Xi Chen, Mingkang Zhu, Shaoteng Liu, Xiaoyang Wu, Xiaogang Xu, Yu Liu, Xiang Bai, Hengshuang Zhao</strong></p>
<p>This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†å®ç°Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†ï¼Œä»¥åœ¨å¤šå¼ å›¾åƒä¹‹é—´å»ºç«‹è§†è§‰çº¿ç´¢çš„è”ç³»ã€‚ä¸€ç§ç›´æ¥çš„è§£å†³æ–¹æ¡ˆæ˜¯ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½¿ç”¨åŸºäºè§„åˆ™çš„ä»»åŠ¡è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•é€šå¸¸ä¾èµ–äºäººå·¥ç­–åˆ’çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œå½“é¢å¯¹ç²¾ç»†ç²’åº¦çš„è§†è§‰ç»†èŠ‚å’Œå›¾åƒä¹‹é—´çš„å¤æ‚é€»è¾‘æ—¶ï¼Œè¿™å¯èƒ½ä¼šç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å—è‡ªæˆ‘ç›‘ç£çš„è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„å¯å‘ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å›¾åƒåŒ…å«å¯ä»¥ä½œä¸ºç›‘ç£çš„å›ºæœ‰çº¦æŸã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«åŒä¸€å¼ å›¾åƒçš„ä¸¤ä¸ªå¢å¼ºè§†å›¾å’Œç¬¬ä¸‰å¼ ç›¸ä¼¼ä½†ä¸åŒçš„å›¾åƒçš„ä¸‰é‡å›¾åƒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹è¢«æç¤ºç”Ÿæˆä¸€ä¸ªæ¨ç†è¿‡ç¨‹æ¥æ¯”è¾ƒè¿™äº›å›¾åƒï¼ˆå³ç¡®å®šç›¸åŒæˆ–ä¸åŒï¼‰ã€‚ç„¶åæˆ‘ä»¬ç”¨åŸºäºè§„åˆ™çš„ä»»åŠ¡è¿›è¡Œå¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨¡å‹ã€‚ç”±äºé«˜è§†è§‰ç›¸ä¼¼æ€§å’Œå¢å¼ºçš„å­˜åœ¨ï¼Œæ¨¡å‹å¿…é¡»å…³æ³¨å¾®å¦™çš„è§†è§‰å˜åŒ–å¹¶è¿›è¡Œé€»è¾‘æ¨ç†æ‰èƒ½æˆåŠŸã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡ä»…é€šè¿‡è§†è§‰æ¯”è¾ƒä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œä½†æ‰€å­¦çš„æ¨ç†èƒ½åŠ›å¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°å„ç§é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–äºä»»ä½•äººå·¥æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œåœ¨è·¨å›¾åƒæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¹¶åœ¨é€šç”¨è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22434v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†å¦‚ä½•é€šè¿‡Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†å°†å¤šä¸ªå›¾åƒä¸­çš„è§†è§‰çº¿ç´¢è”ç³»èµ·æ¥ã€‚é€šè¿‡è‡ªæˆ‘ç›‘ç£çš„è§†è§‰è¡¨ç¤ºå­¦ä¹ ï¼Œè§‚å¯Ÿåˆ°å›¾åƒä¸­å­˜åœ¨å†…åœ¨çº¦æŸå¯ä½œä¸ºç›‘ç£ä¿¡æ¯ã€‚ä¸ºæ­¤æ„å»ºäº†åŒ…å«ä¸¤ä¸ªç›¸åŒå›¾åƒçš„å¢å¼ºè§†å›¾å’Œä¸€ä¸ªç›¸ä¼¼ä½†ä¸åŒçš„å›¾åƒçš„ä¸‰é‡å›¾åƒã€‚è®­ç»ƒæ¨¡å‹æ—¶ï¼Œä¼šæç¤ºæ¨¡å‹æ¯”è¾ƒè¿™äº›å›¾åƒï¼ˆå³åˆ¤æ–­ç›¸åŒæˆ–ä¸åŒï¼‰ï¼Œç„¶åä½¿ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚ç”±äºé«˜è§†è§‰ç›¸ä¼¼æ€§å’Œå­˜åœ¨çš„å¢å¼ºï¼Œæ¨¡å‹å¿…é¡»å…³æ³¨å¾®å¦™çš„è§†è§‰å˜åŒ–å¹¶è¿›è¡Œé€»è¾‘æ¨ç†æ‰èƒ½æˆåŠŸã€‚å®éªŒè¡¨æ˜ï¼Œè™½ç„¶ä»…é€šè¿‡è§†è§‰æ¯”è¾ƒä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œä½†å­¦åˆ°çš„æ¨ç†èƒ½åŠ›å¯ä»¥æœ‰æ•ˆæ³›åŒ–åˆ°å„ç§é—®é¢˜ä¸Šã€‚å¹¶ä¸”ï¼Œåœ¨ä¸ä¾èµ–ä»»ä½•äººå·¥æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨å¤šå›¾åƒæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åœ¨ä¸€èˆ¬è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·¥ä½œæ¢ç´¢äº†Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†åœ¨è¿æ¥å¤šä¸ªå›¾åƒè§†è§‰çº¿ç´¢æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>æå‡ºä½¿ç”¨è‡ªæˆ‘ç›‘ç£çš„è§†è§‰è¡¨ç¤ºå­¦ä¹ æ¥åˆ©ç”¨å›¾åƒä¸­çš„å†…åœ¨çº¦æŸä½œä¸ºç›‘ç£ä¿¡æ¯ã€‚</li>
<li>æ„å»ºä¸‰é‡å›¾åƒæ¥è®­ç»ƒæ¨¡å‹æ¯”è¾ƒå¹¶åŒºåˆ†å›¾åƒé—´çš„å¾®å¦™å·®å¼‚ã€‚</li>
<li>ä½¿ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å…¶å…³æ³¨è§†è§‰å˜åŒ–å’Œè¿›è¡Œé€»è¾‘æ¨ç†ã€‚</li>
<li>å®éªŒè¡¨æ˜æ¨¡å‹åœ¨è§†è§‰æ¯”è¾ƒä»»åŠ¡ä¸Šè®­ç»ƒåï¼Œèƒ½æœ‰æ•ˆæ³›åŒ–è‡³å¤šå›¾åƒæ¨ç†ä»»åŠ¡ã€‚</li>
<li>æ–¹æ³•æ— éœ€ä¾èµ–äººå·¥æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œåœ¨å¤šå›¾åƒæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c393203fc7c0a94f5bd878d651ebb1c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-018e0fde7deb4a78e29f0945d5245a5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b03185cace3b2a9390b8b3b6e30f434.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8db4750d9664b50155d0321261db691.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DynaCLR-Contrastive-Learning-of-Cellular-Dynamics-with-Temporal-Regularization"><a href="#DynaCLR-Contrastive-Learning-of-Cellular-Dynamics-with-Temporal-Regularization" class="headerlink" title="DynaCLR: Contrastive Learning of Cellular Dynamics with Temporal   Regularization"></a>DynaCLR: Contrastive Learning of Cellular Dynamics with Temporal   Regularization</h2><p><strong>Authors:Eduardo Hirata-Miyasaki, Soorya Pradeep, Ziwen Liu, Alishba Imran, Taylla Milena Theodoro, Ivan E. Ivanov, Sudip Khadka, See-Chi Lee, Michelle Grunberg, Hunter Woosley, Madhura Bhave, Carolina Arias, Shalin B. Mehta</strong></p>
<p>We report DynaCLR, a self-supervised method for embedding cell and organelle Dynamics via Contrastive Learning of Representations of time-lapse images. DynaCLR integrates single-cell tracking and time-aware contrastive sampling to learn robust, temporally regularized representations of cell dynamics. DynaCLR embeddings generalize effectively to in-distribution and out-of-distribution datasets, and can be used for several downstream tasks with sparse human annotations. We demonstrate efficient annotations of cell states with a human-in-the-loop using fluorescence and label-free imaging channels. DynaCLR method enables diverse downstream biological analyses: classification of cell division and infection, clustering heterogeneous cell migration patterns, cross-modal distillation of cell states from fluorescence to label-free channel, alignment of asynchronous cellular responses and broken cell tracks, and discovering organelle response due to infection. DynaCLR is a flexible method for comparative analyses of dynamic cellular responses to pharmacological, microbial, and genetic perturbations. We provide PyTorch-based implementations of the model training and inference pipeline (<a target="_blank" rel="noopener" href="https://github.com/mehta-lab/viscy">https://github.com/mehta-lab/viscy</a>) and a GUI (<a target="_blank" rel="noopener" href="https://github.com/czbiohub-sf/napari-iohub">https://github.com/czbiohub-sf/napari-iohub</a>) for the visualization and annotation of trajectories of cells in the real space and the embedding space. </p>
<blockquote>
<p>æˆ‘ä»¬æŠ¥å‘Šäº†ä¸€ç§åä¸ºDynaCLRçš„è‡ªæˆ‘ç›‘ç£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹ æ—¶é—´åºåˆ—å›¾åƒçš„è¡¨ç¤ºæ¥åµŒå…¥ç»†èƒå’Œç»†èƒå™¨çš„åŠ¨æ€ã€‚DynaCLRç»“åˆäº†å•ç»†èƒè¿½è¸ªå’Œæ—¶é—´æ„ŸçŸ¥å¯¹æ¯”é‡‡æ ·ï¼Œå­¦ä¹ ç»†èƒåŠ¨æ€ç¨³å¥ä¸”æ—¶é—´è§„å¾‹åŒ–çš„è¡¨ç¤ºã€‚DynaCLRåµŒå…¥æœ‰æ•ˆåœ°æ³›åŒ–åˆ°å†…éƒ¨åˆ†å¸ƒå’Œå¤–éƒ¨åˆ†å¸ƒçš„æ•°æ®é›†ï¼Œå¹¶ä¸”å¯ç”¨äºå…·æœ‰ç¨€ç–äººå·¥æ³¨é‡Šçš„å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬æ¼”ç¤ºäº†ä½¿ç”¨è§å…‰å’Œæ— æ ‡è®°æˆåƒé€šé“åœ¨å¾ªç¯ä¸­æœ‰æ•ˆæ³¨é‡Šç»†èƒçŠ¶æ€ã€‚DynaCLRæ–¹æ³•å¯ç”¨äºå¤šç§ä¸‹æ¸¸ç”Ÿç‰©å­¦åˆ†æï¼šç»†èƒåˆ†è£‚å’Œæ„ŸæŸ“çš„åˆ†ç±»ï¼Œèšé›†å¼‚è´¨ç»†èƒè¿ç§»æ¨¡å¼ï¼Œä»è§å…‰åˆ°æ— æ ‡è®°é€šé“çš„è·¨æ¨¡æ€ç»†èƒçŠ¶æ€è’¸é¦ï¼Œå¼‚æ­¥ç»†èƒååº”çš„æ ¡å‡†å’Œæ–­è£‚çš„ç»†èƒè½¨è¿¹ï¼Œä»¥åŠç”±äºæ„ŸæŸ“è€Œå‘ç°çš„ç»†èƒå™¨ååº”ã€‚DynaCLRæ˜¯ä¸€ç§ç”¨äºæ¯”è¾ƒè¯ç‰©å­¦ã€å¾®ç”Ÿç‰©å­¦å’Œé—ä¼ å­¦æ‰°åŠ¨å¯¹åŠ¨æ€ç»†èƒååº”åˆ†æçš„çµæ´»æ–¹æ³•ã€‚æˆ‘ä»¬æä¾›äº†åŸºäºPyTorchçš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ç®¡é“çš„å®ç°ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/mehta-lab/viscy%EF%BC%89%EF%BC%8C%E4%BB%A5%E5%8F%8A%E4%B8%80%E4%B8%AAGUI%EF%BC%88https://github.com/czbiohub-sf/napari-iohub%EF%BC%89%EF%BC%8C%E7%94%A8%E4%BA%8E%E5%8F%AF%E8%A7%86%E5%8C%96%E7%9C%9F%E5%AE%9E%E7%A9%BA%E9%97%B4%E5%92%8C%E5%B5%8C%E5%85%A5%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%BB%86%E8%83%9E%E7%9A%84%E8%BD%A8%E8%BF%B9%E5%B9%B6%E8%BF%9B%E8%A1%8C%E6%B3%A8%E9%87%8A%E3%80%82">https://github.com/mehta-lab/viscyï¼‰ï¼Œä»¥åŠä¸€ä¸ªGUIï¼ˆhttps://github.com/czbiohub-sf/napari-iohubï¼‰ï¼Œç”¨äºå¯è§†åŒ–çœŸå®ç©ºé—´å’ŒåµŒå…¥ç©ºé—´ä¸­ç»†èƒçš„è½¨è¿¹å¹¶è¿›è¡Œæ³¨é‡Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11281v2">PDF</a> 30 pages, 6 figures, 13 appendix figures, 5 videos (ancillary files)</p>
<p><strong>Summary</strong>ï¼š<br>DynaCLRæ˜¯ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ—¶é—´åºåˆ—å›¾åƒè¡¨ç¤ºåµŒå…¥æ–¹æ³•ï¼Œç”¨äºç»†èƒå™¨åŠ¨æ€çš„æ— ç›‘ç£å­¦ä¹ ã€‚å®ƒé€šè¿‡ç»“åˆå•ç»†èƒè¿½è¸ªå’Œæ—¶é—´æ„ŸçŸ¥å¯¹æ¯”é‡‡æ ·ï¼Œå­¦ä¹ ç¨³å¥çš„ç»†èƒåŠ¨æ€è¡¨ç¤ºã€‚DynaCLRåµŒå…¥å¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºå†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†ï¼Œå¹¶å¯åœ¨ç¨€ç–äººç±»æ³¨é‡Šçš„æƒ…å†µä¸‹ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚å®ƒæ”¯æŒå¤šç§ä¸‹æ¸¸ç”Ÿç‰©å­¦åˆ†æï¼Œå¦‚ç»†èƒåˆ†è£‚å’Œæ„ŸæŸ“çš„åˆ†ç±»ã€å¼‚è´¨ç»†èƒè¿ç§»æ¨¡å¼çš„èšç±»ã€ä»è§å…‰åˆ°æ— æ ‡è®°æˆåƒé€šé“çš„è·¨æ¨¡æ€è’¸é¦ç­‰ã€‚DynaCLRå…·æœ‰çµæ´»æ€§ï¼Œé€‚ç”¨äºåŠ¨æ€ç»†èƒå“åº”çš„è¯ç‰©å­¦ã€å¾®ç”Ÿç‰©å­¦å’Œé—ä¼ å­¦æ¯”è¾ƒåˆ†æã€‚æä¾›äº†åŸºäºPyTorchçš„æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ç®¡é“å®ç°ä»¥åŠå¯è§†åŒ–å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>DynaCLRæ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºåµŒå…¥ç»†èƒå™¨åŠ¨æ€çš„æ— ç›‘ç£å­¦ä¹ ã€‚</li>
<li>é€šè¿‡ç»“åˆå•ç»†èƒè¿½è¸ªå’Œæ—¶é—´æ„ŸçŸ¥å¯¹æ¯”é‡‡æ ·ï¼Œå­¦ä¹ ç¨³å¥çš„ç»†èƒåŠ¨æ€è¡¨ç¤ºã€‚</li>
<li>DynaCLRåµŒå…¥é€‚ç”¨äºå†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†ï¼Œé€‚ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå³ä½¿å­˜åœ¨ç¨€ç–çš„äººç±»æ³¨é‡Šä¹Ÿæ˜¯å¦‚æ­¤ã€‚</li>
<li>DynaCLRèƒ½å¤Ÿé«˜æ•ˆåœ°å¯¹å…·æœ‰äººå‚ä¸çš„æ³¨é‡Šæµç¨‹è¿›è¡Œåˆ†ç±»åº”ç”¨ç­‰å¤šæ ·ä»»åŠ¡åŒ…æ‹¬ï¼šå¦‚åˆ†ç±»ç»†èƒåˆ†è£‚å’Œæ„ŸæŸ“ä»¥åŠç»†èƒçš„å¼‚è´¨è¿ç§»æ¨¡å¼çš„èšç±»ã€‚ä¹Ÿå‘ç°èƒ½å¤Ÿå°†ç»†åˆ†å±‚åˆ†æå»¶ç»­åˆ°æ„ŸæŸ“åå…¶ä»–æœºåˆ¶ç­‰æ–¹é¢è¿›è¡Œåº”ç”¨ç ”ç©¶ä»¥åŠæ ‡æ³¨ååŒå’Œåˆ†æä¸Šçš„åº”ç”¨æ–¹å‘ç­‰çš„æ·±å…¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11281">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3ac34e10e6335dbb6015b9593dd2ad7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95beb88c5b6fad40453c93cb17007bbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f6b10a6ca1955ffdce47592e527233c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Unsupervised-contrastive-analysis-for-anomaly-detection-in-brain-MRIs-via-conditional-diffusion-models"><a href="#Unsupervised-contrastive-analysis-for-anomaly-detection-in-brain-MRIs-via-conditional-diffusion-models" class="headerlink" title="Unsupervised contrastive analysis for anomaly detection in brain MRIs   via conditional diffusion models"></a>Unsupervised contrastive analysis for anomaly detection in brain MRIs   via conditional diffusion models</h2><p><strong>Authors:Cristiano PatrÃ­cio, Carlo Alberto Barbano, Attilio Fiandrotti, Riccardo Renzulli, Marco Grangetto, Luis F. Teixeira, JoÃ£o C. Neves</strong></p>
<p>Contrastive Analysis (CA) detects anomalies by contrasting patterns unique to a target group (e.g., unhealthy subjects) from those in a background group (e.g., healthy subjects). In the context of brain MRIs, existing CA approaches rely on supervised contrastive learning or variational autoencoders (VAEs) using both healthy and unhealthy data, but such reliance on target samples is challenging in clinical settings. Unsupervised Anomaly Detection (UAD) offers an alternative by learning a reference representation of healthy anatomy without the need for target samples. Deviations from this reference distribution can indicate potential anomalies. In this context, diffusion models have been increasingly adopted in UAD due to their superior performance in image generation compared to VAEs. Nonetheless, precisely reconstructing the anatomy of the brain remains a challenge. In this work, we propose an unsupervised framework to improve the reconstruction quality by training a self-supervised contrastive encoder on healthy images to extract meaningful anatomical features. These features are used to condition a diffusion model to reconstruct the healthy appearance of a given image, enabling interpretable anomaly localization via pixel-wise comparison. We validate our approach through a proof-of-concept on a facial image dataset and further demonstrate its effectiveness on four brain MRI datasets, achieving state-of-the-art anomaly localization performance on the NOVA benchmark. </p>
<blockquote>
<p>å¯¹æ¯”åˆ†æï¼ˆCAï¼‰é€šè¿‡å¯¹æ¯”ç›®æ ‡ç»„ï¼ˆä¾‹å¦‚ä¸å¥åº·ä¸»ä½“ï¼‰ä¸­ç‹¬ç‰¹çš„æ¨¡å¼ä¸èƒŒæ™¯ç»„ï¼ˆä¾‹å¦‚å¥åº·ä¸»ä½“ï¼‰ä¸­çš„æ¨¡å¼æ¥æ£€æµ‹å¼‚å¸¸ã€‚åœ¨è„‘éƒ¨MRIçš„æƒ…å¢ƒä¸­ï¼Œç°æœ‰çš„CAæ–¹æ³•ä¾èµ–äºæœ‰ç›‘ç£çš„å¯¹æ¯”å­¦ä¹ æˆ–å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ï¼Œéœ€è¦åŒæ—¶ä½¿ç”¨å¥åº·å’Œä¸å¥åº·çš„æ•°æ®ï¼Œä½†åœ¨ä¸´åºŠç¯å¢ƒä¸­å¯¹ç›®æ ‡æ ·æœ¬çš„ä¾èµ–æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆUADï¼‰é€šè¿‡å­¦ä¹ å¥åº·ç»„ç»‡çš„å‚è€ƒè¡¨ç¤ºæ¥æä¾›ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œè€Œæ— éœ€ä½¿ç”¨ç›®æ ‡æ ·æœ¬ã€‚ä¸æ­¤å‚è€ƒåˆ†å¸ƒçš„åå·®å¯èƒ½è¡¨æ˜å­˜åœ¨æ½œåœ¨å¼‚å¸¸ã€‚åœ¨è¿™æ–¹é¢ï¼Œç”±äºå…¶åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„å“è¶Šæ€§èƒ½ï¼Œæ‰©æ•£æ¨¡å‹åœ¨UADä¸­è¶Šæ¥è¶Šè¢«é‡‡ç”¨ï¼Œç›¸å¯¹äºVAEså…·æœ‰ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œç²¾ç¡®é‡å»ºå¤§è„‘çš„è§£å‰–ç»“æ„ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªæ— ç›‘ç£æ¡†æ¶ï¼Œé€šè¿‡åœ¨æœ‰ç›‘ç£çš„å¯¹æ¯”ç¼–ç å™¨ä¸Šè®­ç»ƒå¥åº·å›¾åƒæ¥æ”¹å–„é‡å»ºè´¨é‡ï¼Œä»è€Œæå–æœ‰æ„ä¹‰çš„è§£å‰–ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾ç”¨äºè°ƒèŠ‚æ‰©æ•£æ¨¡å‹ï¼Œä»¥é‡å»ºç»™å®šå›¾åƒçš„å¥åº·å¤–è§‚ï¼Œå¹¶é€šè¿‡åƒç´ çº§çš„æ¯”è¾ƒå®ç°å¯è§£é‡Šçš„å¼‚å¸¸å®šä½ã€‚æˆ‘ä»¬é€šè¿‡é¢éƒ¨å›¾åƒæ•°æ®é›†çš„æ¦‚å¿µéªŒè¯æ¥è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¿›ä¸€æ­¥åœ¨å››ä¸ªè„‘éƒ¨MRIæ•°æ®é›†ä¸Šå±•ç¤ºå…¶æœ‰æ•ˆæ€§ï¼Œåœ¨NOVAåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„å¼‚å¸¸å®šä½æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.00772v3">PDF</a> Under consideration at Pattern Recognition Letters</p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒè‡ªç›‘ç£å¯¹æ¯”ç¼–ç å™¨æå–å¥åº·å›¾åƒä¸­çš„æœ‰æ„ä¹‰è§£å‰–ç‰¹å¾ï¼Œå¹¶ç”¨è¿™äº›ç‰¹å¾è°ƒèŠ‚æ‰©æ•£æ¨¡å‹ä»¥é‡å»ºç»™å®šå›¾åƒçš„å¥åº·å¤–è§‚ã€‚æ­¤æ–¹æ³•å¯å®ç°é€šè¿‡åƒç´ çº§æ¯”è¾ƒè¿›è¡Œå¯è§£é‡Šçš„å¼‚å¸¸å®šä½ã€‚åœ¨é¢éƒ¨å›¾åƒæ•°æ®é›†ä¸Šçš„æ¦‚å¿µéªŒè¯ä»¥åŠåœ¨å››ä¸ªè„‘MRIæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§å±•ç¤ºï¼Œå‡åœ¨NOVAåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å¼‚å¸¸å®šä½æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”åˆ†æï¼ˆCAï¼‰èƒ½å¤Ÿé€šè¿‡è¯†åˆ«ç›®æ ‡ç¾¤ä½“ï¼ˆå¦‚ä¸å¥åº·ä¸»ä½“ï¼‰ä¸èƒŒæ™¯ç¾¤ä½“ï¼ˆå¦‚å¥åº·ä¸»ä½“ï¼‰ä¹‹é—´çš„ç‹¬ç‰¹æ¨¡å¼æ¥æ£€æµ‹å¼‚å¸¸ã€‚</li>
<li>åœ¨è„‘MRIçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œç°æœ‰çš„CAæ–¹æ³•ä¾èµ–äºæœ‰ç›‘ç£çš„å¯¹æ¯”å­¦ä¹ æˆ–å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ï¼Œéœ€è¦ä½¿ç”¨å¥åº·å’Œä¸å¥åº·çš„æ•°æ®ã€‚</li>
<li>æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆUADï¼‰æä¾›äº†ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡å­¦ä¹ å¥åº·è§£å‰–çš„å‚è€ƒè¡¨ç¤ºï¼Œè€Œæ— éœ€ç›®æ ‡æ ·æœ¬ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨UADä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå…¶åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ä¼˜äºVAEsã€‚</li>
<li>æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§æ— ç›‘ç£æ¡†æ¶ï¼Œé€šè¿‡è®­ç»ƒè‡ªç›‘ç£å¯¹æ¯”ç¼–ç å™¨æå–æœ‰æ„ä¹‰è§£å‰–ç‰¹å¾ï¼Œä»¥æ”¹å–„é‡å»ºè´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨è¿™äº›ç‰¹å¾æ¥è°ƒèŠ‚æ‰©æ•£æ¨¡å‹ï¼Œä»¥é‡å»ºç»™å®šå›¾åƒçš„å¥åº·å¤–è§‚ï¼Œé€šè¿‡åƒç´ çº§æ¯”è¾ƒå®ç°å¯è§£é‡Šçš„å¼‚å¸¸å®šä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.00772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27fccac4a2b1bfecb0dd61df49aff6bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3ce763483aa4088abb60e5a6defb19b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ce8b6adc4dfa1de7c066ec3397120f9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Accurate-and-lightweight-dehazing-via-multi-receptive-field-non-local-network-and-novel-contrastive-regularization"><a href="#Accurate-and-lightweight-dehazing-via-multi-receptive-field-non-local-network-and-novel-contrastive-regularization" class="headerlink" title="Accurate and lightweight dehazing via multi-receptive-field non-local   network and novel contrastive regularization"></a>Accurate and lightweight dehazing via multi-receptive-field non-local   network and novel contrastive regularization</h2><p><strong>Authors:Zewei He, Zixuan Chen, Jinlei Li, Ziqian Lu, Xuecheng Sun, Hao Luo, Zhe-Ming Lu, Evangelos K. Markakis</strong></p>
<p>Recently, deep learning-based methods have dominated image dehazing domain. Although very competitive dehazing performance has been achieved with sophisticated models, effective solutions for extracting useful features are still under-explored. In addition, non-local network, which has made a breakthrough in many vision tasks, has not been appropriately applied to image dehazing. Thus, a multi-receptive-field non-local network (MRFNLN) consisting of the multi-stream feature attention block (MSFAB) and cross non-local block (CNLB) is presented in this paper. We start with extracting richer features for dehazing. Specifically, we design a multi-stream feature extraction (MSFE) sub-block, which contains three parallel convolutions with different receptive fields (i.e., $1\times 1$, $3\times 3$, $5\times 5$) for extracting multi-scale features. Following MSFE, we employ an attention sub-block to make the model adaptively focus on important channels&#x2F;regions. The MSFE and attention sub-blocks constitute our MSFAB. Then, we design a cross non-local block (CNLB), which can capture long-range dependencies beyond the query. Instead of the same input source of query branch, the key and value branches are enhanced by fusing more preceding features. CNLB is computation-friendly by leveraging a spatial pyramid down-sampling (SPDS) strategy to reduce the computation and memory consumption without sacrificing the performance. Last but not least, a novel detail-focused contrastive regularization (DFCR) is presented by emphasizing the low-level details and ignoring the high-level semantic information in the representation space. Comprehensive experimental results demonstrate that the proposed MRFNLN model outperforms recent state-of-the-art dehazing methods with less than 1.5 Million parameters. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åœ¨å›¾åƒå»é›¾é¢†åŸŸå æ®äº†ä¸»å¯¼åœ°ä½ã€‚å°½ç®¡åˆ©ç”¨å¤æ‚æ¨¡å‹å–å¾—äº†éå¸¸æœ‰ç«äº‰åŠ›çš„å»é›¾æ€§èƒ½ï¼Œä½†æå–æœ‰ç”¨ç‰¹å¾çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆä»å—åˆ°è¾ƒå°‘çš„æ¢ç´¢ã€‚æ­¤å¤–ï¼Œå·²ç»åœ¨è®¸å¤šè§†è§‰ä»»åŠ¡ä¸­å–å¾—çªç ´çš„éå±€éƒ¨ç½‘ç»œå°šæœªé€‚å½“åœ°åº”ç”¨äºå›¾åƒå»é›¾ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ„Ÿå—é‡éå±€éƒ¨ç½‘ç»œï¼ˆMRFNLNï¼‰ï¼Œè¯¥ç½‘ç»œç”±å¤šæµç‰¹å¾æ³¨æ„åŠ›å—ï¼ˆMSFABï¼‰å’Œäº¤å‰éå±€éƒ¨å—ï¼ˆCNLBï¼‰ç»„æˆã€‚æˆ‘ä»¬ä»æå–æ›´ä¸°å¯Œçš„å»é›¾ç‰¹å¾å¼€å§‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šæµç‰¹å¾æå–ï¼ˆMSFEï¼‰å­å—ï¼Œå…¶ä¸­åŒ…å«ä¸‰ä¸ªå…·æœ‰ä¸åŒæ„Ÿå—é‡ï¼ˆå³$ 1\times 1 $ã€$ 3\times 3 $ã€$ 5\times 5 $ï¼‰çš„å¹¶è¡Œå·ç§¯ï¼Œä»¥æå–å¤šå°ºåº¦ç‰¹å¾ã€‚åœ¨MSFEä¹‹åï¼Œæˆ‘ä»¬é‡‡ç”¨æ³¨æ„åŠ›å­å—ä½¿æ¨¡å‹è‡ªé€‚åº”åœ°å…³æ³¨é‡è¦çš„é€šé“&#x2F;åŒºåŸŸã€‚MSFEå’Œæ³¨æ„åŠ›å­å—æ„æˆäº†æˆ‘ä»¬çš„MSFABã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªäº¤å‰éå±€éƒ¨å—ï¼ˆCNLBï¼‰ï¼Œå®ƒèƒ½å¤Ÿæ•æ‰è¶…è¶ŠæŸ¥è¯¢çš„é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚ä¸æŸ¥è¯¢åˆ†æ”¯çš„ç›¸åŒè¾“å…¥æºä¸åŒï¼Œé”®å’Œå€¼åˆ†æ”¯é€šè¿‡èåˆæ›´å¤šçš„å…ˆå‰ç‰¹å¾æ¥å¢å¼ºã€‚CNLBé€šè¿‡åˆ©ç”¨ç©ºé—´é‡‘å­—å¡”ä¸‹é‡‡æ ·ï¼ˆSPDSï¼‰ç­–ç•¥åœ¨è®¡ç®—ä¸Šæ›´ä¸ºå‹å¥½ï¼Œå¯ä»¥åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹å‡å°‘è®¡ç®—å’Œå†…å­˜æ¶ˆè€—ã€‚æœ€åä½†å¹¶éæœ€ä¸é‡è¦çš„æ˜¯ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç»†èŠ‚èšç„¦å¯¹æ¯”æ­£åˆ™åŒ–ï¼ˆDFCRï¼‰ï¼Œå®ƒé€šè¿‡å¼ºè°ƒä½å±‚æ¬¡ç»†èŠ‚è€Œå¿½è§†è¡¨ç¤ºç©ºé—´ä¸­çš„é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MRFNLNæ¨¡å‹åœ¨å‚æ•°å°‘äº150ä¸‡ä¸ªçš„æƒ…å†µä¸‹ï¼Œä¼˜äºæœ€æ–°çš„å…ˆè¿›å»é›¾æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16494v2">PDF</a> submitted to the IEEE Journal for possible publication</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ„Ÿå—é‡éå±€éƒ¨ç½‘ç»œï¼ˆMRFNLNï¼‰ï¼ŒåŒ…å«å¤šæµç‰¹å¾æ³¨æ„å—ï¼ˆMSFABï¼‰å’Œäº¤å‰éå±€éƒ¨å—ï¼ˆCNLBï¼‰ã€‚æ—¨åœ¨å›¾åƒå»é›¾é¢†åŸŸæå–æ›´ä¸°å¯Œç‰¹å¾ï¼Œé€šè¿‡å¤šæµç‰¹å¾æå–å­å—ï¼ˆMSFEï¼‰è·å–å¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨æ³¨æ„åŠ›å­å—ä½¿æ¨¡å‹è‡ªé€‚åº”å…³æ³¨é‡è¦é€šé“&#x2F;åŒºåŸŸã€‚CNLBèƒ½å¤Ÿæ•æ‰è¶…å‡ºæŸ¥è¯¢çš„é•¿ç¨‹ä¾èµ–å…³ç³»ï¼Œå¹¶é€šè¿‡èåˆæ›´å¤šå‰æœŸç‰¹å¾æ¥æå‡å…³é”®å€¼å’ŒæŸ¥è¯¢åˆ†æ”¯ã€‚åˆ©ç”¨ç©ºé—´é‡‘å­—å¡”ä¸‹é‡‡æ ·ç­–ç•¥ï¼Œé™ä½è®¡ç®—é‡å’Œå†…å­˜æ¶ˆè€—è€Œä¸æŸå¤±æ€§èƒ½ã€‚æœ€åï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç»†èŠ‚èšç„¦å¯¹æ¯”æ­£åˆ™åŒ–ï¼ˆDFCRï¼‰ï¼Œé€šè¿‡å¼ºè°ƒä½å±‚æ¬¡ç»†èŠ‚è€Œå¿½è§†é«˜å±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯åœ¨è¡¨ç¤ºç©ºé—´å†…è¡¨ç°è‰¯å¥½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MRFNLNæ¨¡å‹å‚æ•°å°‘äºä¸€ç™¾ä¸‡äº”åƒä¸ªï¼Œä½†è¡¨ç°ä¼˜äºæœ€æ–°çš„å»é›¾æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§å¤šæ„Ÿå—é‡éå±€éƒ¨ç½‘ç»œï¼ˆMRFNLNï¼‰ï¼Œç»“åˆäº†å¤šæµç‰¹å¾æ³¨æ„å—ï¼ˆMSFABï¼‰å’Œäº¤å‰éå±€éƒ¨å—ï¼ˆCNLBï¼‰ã€‚</li>
<li>é€šè¿‡å¤šæµç‰¹å¾æå–å­å—ï¼ˆMSFEï¼‰è·å–å¤šå°ºåº¦ç‰¹å¾ã€‚</li>
<li>åˆ©ç”¨æ³¨æ„åŠ›å­å—ä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”å…³æ³¨é‡è¦é€šé“å’ŒåŒºåŸŸã€‚</li>
<li>äº¤å‰éå±€éƒ¨å—ï¼ˆCNLBï¼‰èƒ½æ•æ‰è¶…è¿‡æŸ¥è¯¢çš„é•¿ç¨‹ä¾èµ–å…³ç³»ã€‚</li>
<li>é‡‡ç”¨ç©ºé—´é‡‘å­—å¡”ä¸‹é‡‡æ ·ç­–ç•¥ä»¥é™ä½è®¡ç®—æˆæœ¬å’Œå†…å­˜æ¶ˆè€—ã€‚</li>
<li>å¼•å…¥ç»†èŠ‚èšç„¦å¯¹æ¯”æ­£åˆ™åŒ–ï¼ˆDFCRï¼‰ï¼Œæ³¨é‡ä½å±‚æ¬¡ç»†èŠ‚çš„æå–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.16494">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f898d4a6bd976bf30bc551f23030f49c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0128cd990e3e381ad6f8f2065f699292.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db7f39fad42b099b1bd947e1128ce910.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-022fda2e467aba2a3f0a93395ef4d413.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6244c46f20e0db3480853405b8c9194f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7002af9c28262ea2c83bdc26f4f96f71.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  GroundingDINO-US-SAM Text-Prompted Multi-Organ Segmentation in   Ultrasound with LoRA-Tuned Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-66df7122ad8bb673f007c253fe7b5fc0.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Visual Textualization for Image Prompted Object Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">22950.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
