<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  A Closer Look at Conditional Prompt Tuning for Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-790fdbfb1dddae2979a6ff3f4cd43ab8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-03-æ›´æ–°"><a href="#2025-07-03-æ›´æ–°" class="headerlink" title="2025-07-03 æ›´æ–°"></a>2025-07-03 æ›´æ–°</h1><h2 id="A-Closer-Look-at-Conditional-Prompt-Tuning-for-Vision-Language-Models"><a href="#A-Closer-Look-at-Conditional-Prompt-Tuning-for-Vision-Language-Models" class="headerlink" title="A Closer Look at Conditional Prompt Tuning for Vision-Language Models"></a>A Closer Look at Conditional Prompt Tuning for Vision-Language Models</h2><p><strong>Authors:Ji Zhang, Shihan Wu, Lianli Gao, Jingkuan Song, Nicu Sebe, Heng Tao Shen</strong></p>
<p>Despite the great promise of Prompt Tuning (PT) in adapting large Vision-Language Pretrained Models (VLPMs) to downstream tasks, they often struggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better tuned to a base task, their ability to generalize to new tasks diminishes. Recent work on conditional PT addresses this problem by replacing static prompts with dynamic Visual Image Information (VII)-conditioned prompts, improving the modelâ€™s generalization to new tasks to some extent. In this work, we first identify a critical issue with existing conditional PT methods: using VII as the â€œconditionâ€ of prompts yields suboptimal performance, and even random noise-conditioned prompts can outperform the VII-conditioned counterparts. On further analysis, we find that learning dynamic prompts conditioned on Textual Class Information (TCI) is the key to solving the BNT problem. Motivated by this, we then propose Class-adaptive Prompt Tuning (CaPT), which enables fast adaptation of tuned models to new classes by learning TCI-conditioned prompts from base classes. Remarkably, CaPT can be used as a plugin to mitigate the BNT problem for existing unconditional PT schemes. Extensive experiments on 11 datasets show that CaPT consistently improves the performance of five strong unconditional PT baselines with negligible additional computational cost. Additionally, by integrating CaPT with our recently proposed DePT framework, we devise a new conditional PT approach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art conditional PT scheme by 3.49%, averaged over the 11 datasets. Code: <a target="_blank" rel="noopener" href="https://github.com/Koorye/CaPT">https://github.com/Koorye/CaPT</a>. </p>
<blockquote>
<p>å°½ç®¡Prompt Tuningï¼ˆPTï¼‰åœ¨é€‚åº”å¤§å‹è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆVLPMsï¼‰åˆ°ä¸‹æ¸¸ä»»åŠ¡æ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥å…‹æœBase-New Tradeoffï¼ˆBNTï¼‰çš„å›°å¢ƒï¼šéšç€VLPMså¯¹åŸºç¡€ä»»åŠ¡çš„è°ƒæ•´è¶Šæ¥è¶Šå¥½ï¼Œå®ƒä»¬å¯¹æ–°ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›å°±ä¼šå‡å¼±ã€‚æœ€è¿‘å…³äºæ¡ä»¶PTçš„ç ”ç©¶é€šè¿‡ç”¨åŠ¨æ€è§†è§‰å›¾åƒä¿¡æ¯ï¼ˆVIIï¼‰æ›¿æ¢é™æ€æç¤ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜äº†æ¨¡å‹å¯¹æ–°ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå‘ç°äº†ç°æœ‰æ¡ä»¶PTæ–¹æ³•çš„ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šä½¿ç”¨VIIä½œä¸ºæç¤ºçš„â€œæ¡ä»¶â€ä¼šäº§ç”Ÿæ¬¡ä¼˜æ€§èƒ½ï¼Œç”šè‡³å™ªå£°æ¡ä»¶æç¤ºä¹Ÿèƒ½è¶…è¶ŠVIIæ¡ä»¶æç¤ºã€‚è¿›ä¸€æ­¥åˆ†æåï¼Œæˆ‘ä»¬å‘ç°å­¦ä¹ åŸºäºæ–‡æœ¬ç±»åˆ«ä¿¡æ¯ï¼ˆTCIï¼‰çš„åŠ¨æ€æç¤ºæ˜¯è§£å†³BNTé—®é¢˜çš„å…³é”®ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬éšåæå‡ºäº†ç±»è‡ªé€‚åº”æç¤ºè°ƒæ•´ï¼ˆCaPTï¼‰ï¼Œå®ƒé€šè¿‡ä»åŸºç¡€ç±»åˆ«ä¸­å­¦ä¹ TCIæ¡ä»¶æç¤ºï¼Œå®ç°äº†å¿«é€Ÿé€‚åº”æ–°ç±»åˆ«ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒCaPTå¯ä»¥ä½œä¸ºæ’ä»¶ç”¨äºç¼“è§£ç°æœ‰æ— æ¡ä»¶PTæ–¹æ¡ˆçš„BNTé—®é¢˜ã€‚åœ¨11ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCaPTèƒ½å¤ŸæŒç»­æé«˜äº”ä¸ªå¼ºå¤§çš„æ— æ¡ä»¶PTåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼Œå¹¶ä¸”åªéœ€å¾ˆå°çš„é¢å¤–è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†CaPTä¸æˆ‘ä»¬æœ€è¿‘æå‡ºçš„DePTæ¡†æ¶ç›¸ç»“åˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„æ¡ä»¶PTæ–¹æ³•ï¼Œç§°ä¸ºDeCaPTï¼Œå®ƒåœ¨11ä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡é«˜çº§å‡†ç¡®åº¦é«˜å‡ºäº†ç°æœ‰æœ€å…ˆè¿›çš„æ¡ä»¶PTæ–¹æ¡ˆ3.49%ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Koorye/CaPT">https://github.com/Koorye/CaPT</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23856v1">PDF</a> 18 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºåŸºäºè§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹çš„æç¤ºè°ƒèŠ‚ï¼ˆPTï¼‰åœ¨åŸºç¡€ä»»åŠ¡ä¸Šçš„ä¼˜åŠ¿ä¸æ–°ä»»åŠ¡ä¸Šçš„å±€é™ã€‚ä¸ºäº†è§£å†³åŸºç¡€æ–°ä»»åŠ¡æƒè¡¡ï¼ˆBNTï¼‰é—®é¢˜ï¼Œæœ¬ç ”ç©¶å‘ç°æ¡ä»¶PTæ–¹æ³•ä¸­ï¼Œå°†è§†è§‰å›¾åƒä¿¡æ¯ä½œä¸ºæç¤ºçš„æ¡ä»¶æ•ˆæœä¸ä½³ã€‚å¯¹æ­¤é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºç±»è‡ªé€‚åº”æç¤ºè°ƒèŠ‚ï¼ˆCaPTï¼‰ï¼Œé€šè¿‡ä»åŸºç¡€ç±»å­¦ä¹ æ–‡æœ¬ç±»ä¿¡æ¯ï¼ˆTCIï¼‰æ¡ä»¶åŒ–æç¤ºæ¥è§£å†³BNTé—®é¢˜ã€‚è¯¥æ–¹æ³•èƒ½å¿«é€Ÿé€‚åº”æ–°ç±»å¹¶åº”ç”¨äºæ— æ¡ä»¶PTæ–¹æ¡ˆä½œä¸ºæ’ä»¶æ¥å‡å°‘BNTé—®é¢˜çš„å½±å“ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCaPTèƒ½æœ‰æ•ˆæå‡äº”ä¸ªæ— æ¡ä»¶PTåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸”é¢å¤–è®¡ç®—æˆæœ¬è¾ƒå°ã€‚æ•´åˆCaPTä¸æœ€æ–°æå‡ºçš„DePTæ¡†æ¶åå¾—åˆ°çš„DeCaPTæ–°æ–¹æ³•ï¼Œç›¸è¾ƒäºå½“å‰å…ˆè¿›æ¡ä»¶PTæ–¹æ¡ˆçš„å¹³å‡é«˜ç²¾åº¦ç‡æå‡3.49%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æç¤ºè°ƒèŠ‚ï¼ˆPTï¼‰åœ¨è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆVLPMsï¼‰ä¸­å­˜åœ¨åŸºç¡€æ–°ä»»åŠ¡æƒè¡¡ï¼ˆBNTï¼‰é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨è§†è§‰å›¾åƒä¿¡æ¯ï¼ˆVIIï¼‰ä½œä¸ºæ¡ä»¶æç¤ºçš„æ•ˆæœä¸ä½³ï¼Œéšæœºå™ªå£°æ¡ä»¶æç¤ºæ•ˆæœæ›´ä½³ã€‚</li>
<li>è§£å†³BNTé—®é¢˜çš„å…³é”®åœ¨äºå­¦ä¹ åŸºäºæ–‡æœ¬ç±»ä¿¡æ¯ï¼ˆTCIï¼‰çš„åŠ¨æ€æç¤ºã€‚</li>
<li>æå‡ºç±»è‡ªé€‚åº”æç¤ºè°ƒèŠ‚ï¼ˆCaPTï¼‰ï¼Œèƒ½å¿«é€Ÿé€‚åº”æ–°ç±»å¹¶å¢å¼ºæ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>CaPTå¯ä»¥ä½œä¸ºä¸€ä¸ªæ’ä»¶ï¼Œç”¨äºè§£å†³æ— æ¡ä»¶PTæ–¹æ¡ˆä¸­çš„BNTé—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCaPTä¸ç°æœ‰æ— æ¡ä»¶PTåŸºçº¿ç»“åˆæ—¶ï¼Œèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ä¸”å¢åŠ çš„è®¡ç®—æˆæœ¬è¾ƒå°ã€‚</li>
<li>ç»“åˆCaPTä¸DePTæ¡†æ¶çš„æ–°æ–¹æ³•DeCaPTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰æ¡ä»¶PTæ–¹æ¡ˆçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7901edafbb32da2b7e5cd638f3eedde5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab10f4f9a70cfcbd45ee9e57dc1d0fa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cd2885a01b61a3c60ad8ba0d9f3d91f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LLM-enhanced-Action-aware-Multi-modal-Prompt-Tuning-for-Image-Text-Matching"><a href="#LLM-enhanced-Action-aware-Multi-modal-Prompt-Tuning-for-Image-Text-Matching" class="headerlink" title="LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text   Matching"></a>LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text   Matching</h2><p><strong>Authors:Mengxiao Tian, Xinxiao Wu, Shuo Yang</strong></p>
<p>Driven by large-scale contrastive vision-language pre-trained models such as CLIP, recent advancements in the image-text matching task have achieved remarkable success in representation learning. Due to image-level visual-language alignment, CLIP falls short in understanding fine-grained details such as object attributes and spatial relationships between objects. Recent efforts have attempted to compel CLIP to acquire structured visual representations by introducing prompt learning to achieve object-level alignment. While achieving promising results, they still lack the capability to perceive actions, which are crucial for describing the states or relationships between objects. Therefore, we propose to endow CLIP with fine-grained action-level understanding by introducing an LLM-enhanced action-aware multi-modal prompt-tuning method, incorporating the action-related external knowledge generated by large language models (LLMs). Specifically, we design an action triplet prompt and an action state prompt to exploit compositional semantic knowledge and state-related causal knowledge implicitly stored in LLMs. Subsequently, we propose an adaptive interaction module to aggregate attentive visual features conditioned on action-aware prompted knowledge for establishing discriminative and action-aware visual representations, which further improves the performance. Comprehensive experimental results on two benchmark datasets demonstrate the effectiveness of our method. </p>
<blockquote>
<p>ç”±å¤§å‹å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰é©±åŠ¨ï¼Œå›¾åƒæ–‡æœ¬åŒ¹é…ä»»åŠ¡çš„æœ€æ–°è¿›å±•åœ¨è¡¨ç¤ºå­¦ä¹ ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç”±äºå›¾åƒçº§åˆ«çš„è§†è§‰è¯­è¨€å¯¹é½ï¼ŒCLIPåœ¨ç†è§£å¯¹è±¡çš„å±æ€§ç­‰ç»†å¾®ç»†èŠ‚ä»¥åŠå¯¹è±¡ä¹‹é—´çš„ç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚è¿‘æœŸçš„ç ”ç©¶å°è¯•é€šè¿‡å¼•å…¥æç¤ºå­¦ä¹ æ¥è¿«ä½¿CLIPè·å¾—ç»“æ„åŒ–è§†è§‰è¡¨ç¤ºï¼Œä»¥å®ç°å¯¹è±¡çº§åˆ«çš„å¯¹é½ï¼Œå¹¶å–å¾—äº†ä¸€å®šçš„æˆæœã€‚è™½ç„¶åœ¨æè¿°å¯¹è±¡çŠ¶æ€æˆ–å…³ç³»æ–¹é¢å–å¾—äº†æœ‰å¸Œæœ›çš„æˆæœï¼Œä½†å®ƒä»¬ä»ç„¶ç¼ºä¹æ„ŸçŸ¥åŠ¨ä½œçš„èƒ½åŠ›ï¼Œè¿™å¯¹äºæè¿°å¯¹è±¡ä¹‹é—´çš„å…³ç³»è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºçš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡å¼æç¤ºè°ƒæ•´æ–¹æ³•ï¼Œä¸ºCLIPèµ‹äºˆç²¾ç»†çš„åŠ¨ä½œçº§åˆ«ç†è§£ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŠ¨ä½œä¸‰å…ƒç»„æç¤ºå’Œä¸€ä¸ªåŠ¨ä½œçŠ¶æ€æç¤ºï¼Œä»¥åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­éšå«çš„ç»„æˆè¯­ä¹‰çŸ¥è¯†å’ŒçŠ¶æ€ç›¸å…³å› æœå…³ç³»çŸ¥è¯†ã€‚éšåï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”äº¤äº’æ¨¡å—ï¼Œè¯¥æ¨¡å—æ ¹æ®åŠ¨ä½œæ„ŸçŸ¥æç¤ºçŸ¥è¯†èšåˆæ³¨æ„åŠ›è§†è§‰ç‰¹å¾ï¼Œä»¥å»ºç«‹å…·æœ‰é‰´åˆ«åŠ›å’ŒåŠ¨ä½œæ„ŸçŸ¥çš„è§†è§‰è¡¨ç¤ºï¼Œè¿™è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23502v1">PDF</a> accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPçš„å¤§å‹å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨å›¾åƒæ–‡æœ¬åŒ¹é…ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨ç†è§£å¯¹è±¡å±æ€§ã€å¯¹è±¡é—´ç©ºé—´å…³ç³»ç­‰ç»†èŠ‚æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œè¿‘æœŸç ”ç©¶å°è¯•å¼•å…¥æç¤ºå­¦ä¹ æ¥ä¿ƒä½¿CLIPè·å–ç»“æ„åŒ–è§†è§‰è¡¨å¾ï¼Œå®ç°å¯¹è±¡çº§åˆ«çš„å¯¹é½ã€‚å°½ç®¡å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†ä»ç¼ºä¹æ„ŸçŸ¥åŠ¨ä½œçš„èƒ½åŠ›ï¼Œè¿™å¯¹æè¿°å¯¹è±¡çŠ¶æ€æˆ–å…³ç³»è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºå‹çš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€æç¤ºè°ƒèŠ‚æ–¹æ³•ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº§ç”Ÿçš„åŠ¨ä½œç›¸å…³å¤–éƒ¨çŸ¥è¯†ã€‚é€šè¿‡è®¾è®¡åŠ¨ä½œä¸‰å…ƒç»„æç¤ºå’ŒåŠ¨ä½œçŠ¶æ€æç¤ºæ¥åˆ©ç”¨LLMä¸­éšå«çš„ç»„æˆè¯­ä¹‰çŸ¥è¯†å’ŒçŠ¶æ€ç›¸å…³å› æœçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”äº¤äº’æ¨¡å—ï¼Œæ ¹æ®åŠ¨ä½œæ„ŸçŸ¥æç¤ºçŸ¥è¯†èšåˆæ³¨æ„è§†è§‰ç‰¹å¾ï¼Œå»ºç«‹æœ‰é‰´åˆ«åŠ›å’ŒåŠ¨ä½œæ„ŸçŸ¥çš„è§†è§‰è¡¨å¾ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºCLIPçš„å¤§å‹å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨å›¾åƒæ–‡æœ¬åŒ¹é…ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>CLIPåœ¨ç†è§£å›¾åƒç»†èŠ‚ï¼ˆå¦‚å¯¹è±¡å±æ€§ã€ç©ºé—´å…³ç³»ï¼‰æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å¼•å…¥æç¤ºå­¦ä¹ æœ‰åŠ©äºå®ç°å¯¹è±¡çº§åˆ«çš„å¯¹é½ï¼Œæå‡ç»“æ„åŒ–è§†è§‰è¡¨å¾çš„è·å–ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹æ„ŸçŸ¥åŠ¨ä½œçš„èƒ½åŠ›ï¼Œè¿™å½±å“äº†æè¿°å¯¹è±¡çŠ¶æ€æˆ–å…³ç³»çš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹çš„LLMå¢å¼ºå‹åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡æ€æç¤ºè°ƒèŠ‚æ–¹æ³•ã€‚</li>
<li>é€šè¿‡åŠ¨ä½œä¸‰å…ƒç»„æç¤ºå’ŒåŠ¨ä½œçŠ¶æ€æç¤ºåˆ©ç”¨LLMä¸­çš„ç»„æˆè¯­ä¹‰å’ŒçŠ¶æ€ç›¸å…³å› æœçŸ¥è¯†ã€‚</li>
<li>è‡ªé€‚åº”äº¤äº’æ¨¡å—èƒ½æé«˜åŠ¨ä½œæ„ŸçŸ¥è§†è§‰è¡¨å¾çš„å»ºç«‹å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59c0d9b3229772216e371ba8be57de12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3325482bd2bd82c61a92137029be116d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b9876bf8cad9400397a8d239f85062a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61a85eae4ce00bebc69df0c960d8d4a6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Revisiting-CroPA-A-Reproducibility-Study-and-Enhancements-for-Cross-Prompt-Adversarial-Transferability-in-Vision-Language-Models"><a href="#Revisiting-CroPA-A-Reproducibility-Study-and-Enhancements-for-Cross-Prompt-Adversarial-Transferability-in-Vision-Language-Models" class="headerlink" title="Revisiting CroPA: A Reproducibility Study and Enhancements for   Cross-Prompt Adversarial Transferability in Vision-Language Models"></a>Revisiting CroPA: A Reproducibility Study and Enhancements for   Cross-Prompt Adversarial Transferability in Vision-Language Models</h2><p><strong>Authors:Atharv Mittal, Agam Pandey, Amritanshu Tiwari, Sukrit Jindal, Swadesh Swain</strong></p>
<p>Large Vision-Language Models (VLMs) have revolutionized computer vision, enabling tasks such as image classification, captioning, and visual question answering. However, they remain highly vulnerable to adversarial attacks, particularly in scenarios where both visual and textual modalities can be manipulated. In this study, we conduct a comprehensive reproducibility study of â€œAn Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on Vision-Language Modelsâ€ validating the Cross-Prompt Attack (CroPA) and confirming its superior cross-prompt transferability compared to existing baselines. Beyond replication we propose several key improvements: (1) A novel initialization strategy that significantly improves Attack Success Rate (ASR). (2) Investigate cross-image transferability by learning universal perturbations. (3) A novel loss function targeting vision encoder attention mechanisms to improve generalization. Our evaluation across prominent VLMs â€“ including Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on LLaVA validates the original results and demonstrates that our improvements consistently boost adversarial effectiveness. Our work reinforces the importance of studying adversarial vulnerabilities in VLMs and provides a more robust framework for generating transferable adversarial examples, with significant implications for understanding the security of VLMs in real-world applications. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å·²ç»å½»åº•æ”¹å˜äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œèƒ½å¤Ÿå®Œæˆå›¾åƒåˆ†ç±»ã€æè¿°å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶éå¸¸å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å¨èƒï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡å¼éƒ½å¯ä»¥è¢«æ“çºµçš„åœºæ™¯ä¸­ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹â€œä¸€å¼ å›¾ç‰‡èƒœè¿‡åƒè°ä¸‡è¯­ï¼šè§†è§‰è¯­è¨€æ¨¡å‹ä¸Šè·¨æç¤ºçš„å¯¹æŠ—æ€§è¿ç§»æ”»å‡»â€ï¼ˆAn Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on Vision-Language Modelsï¼‰è¿›è¡Œäº†å…¨é¢çš„å¯é‡å¤æ€§ç ”ç©¶ï¼ŒéªŒè¯äº†è·¨æç¤ºæ”»å‡»ï¼ˆCroPAï¼‰å¹¶ç¡®è®¤å…¶åœ¨è·¨æç¤ºè¿ç§»æ–¹é¢ç›¸æ¯”ç°æœ‰åŸºçº¿å…·æœ‰ä¼˜è¶Šæ€§ã€‚é™¤äº†å¤åˆ¶ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†å‡ é¡¹å…³é”®æ”¹è¿›ï¼šï¼ˆ1ï¼‰ä¸€ç§æ–°å‹åˆå§‹åŒ–ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚ï¼ˆ2ï¼‰é€šè¿‡å­¦ä¹ é€šç”¨æ‰°åŠ¨æ¥ç ”ç©¶è·¨å›¾åƒè¿ç§»æ€§ã€‚ï¼ˆ3ï¼‰é’ˆå¯¹è§†è§‰ç¼–ç å™¨æ³¨æ„åŠ›æœºåˆ¶çš„æ–°å‹æŸå¤±å‡½æ•°ï¼Œä»¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹çŸ¥åçš„VLMsï¼ˆåŒ…æ‹¬Flamingoã€BLIP-2å’ŒInstructBLIPï¼‰ä»¥åŠLLaVAä¸Šçš„æ‰©å±•å®éªŒè¿›è¡Œäº†è¯„ä¼°ï¼ŒéªŒè¯äº†åŸå§‹ç»“æœï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ”¹è¿›åœ¨æå‡å¯¹æŠ—æ€§æ•ˆæœæ–¹é¢è¡¨ç°ç¨³å®šã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†ç ”ç©¶VLMsçš„å¯¹æŠ—è„†å¼±æ€§çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºç”Ÿæˆå¯è¿ç§»çš„å¯¹æŠ—æ€§ç¤ºä¾‹æä¾›äº†æ›´ç¨³å¥çš„æ¡†æ¶ï¼Œå¯¹äºç†è§£VLMsåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®‰å…¨æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22982v1">PDF</a> Accepted to MLRC 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾åƒåˆ†ç±»ã€æè¿°å’Œè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸­å…·æœ‰å“è¶Šè¡¨ç°ï¼Œä½†ä¹Ÿé¢ä¸´ç€å¯¹æŠ—æ”»å‡»çš„é£é™©ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å’Œæ–‡å­—æ¨¡æ€å‡å¯è¢«æ“çºµçš„åœºæ™¯ä¸‹ã€‚æœ¬ç ”ç©¶å¯¹â€œAn Image is Worth 1000 Liesâ€ä¸­çš„Cross-Promptæ”»å‡»è¿›è¡Œäº†å¯å¤ç°æ€§ç ”ç©¶ï¼ŒéªŒè¯äº†å…¶è·¨æç¤ºçš„ä¼˜è¶Šè½¬ç§»æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†å‡ é¡¹å…³é”®æ”¹è¿›ï¼šæ–°å‹åˆå§‹åŒ–ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜æ”»å‡»æˆåŠŸç‡ï¼›é€šè¿‡å­¦ä¹ é€šç”¨æ‰°åŠ¨ç ”ç©¶è·¨å›¾åƒè½¬ç§»æ€§ï¼›é’ˆå¯¹è§†è§‰ç¼–ç å™¨æ³¨æ„åŠ›æœºåˆ¶çš„æ–°å‹æŸå¤±å‡½æ•°ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚åœ¨Flamingoã€BLIP-2ã€InstructBLIPç­‰ä¸»æµVLMsåŠLLaVAä¸Šçš„æ‰©å±•å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ”¹è¿›ï¼Œæå‡äº†å¯¹æŠ—æ”»å‡»çš„æ•ˆæœï¼Œå¯¹ç†è§£VLMsåœ¨ç°å®åº”ç”¨ä¸­çš„å®‰å…¨æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†æ˜“å—å¯¹æŠ—æ”»å‡»å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å’Œæ–‡å­—æ¨¡æ€å‡å¯æ“çºµçš„æƒ…å¢ƒä¸­ã€‚</li>
<li>æœ¬ç ”ç©¶æˆåŠŸå¤ç°äº†Cross-Promptæ”»å‡»ï¼Œå¹¶éªŒè¯äº†å…¶è·¨æç¤ºçš„ä¼˜è¶Šè½¬ç§»æ€§ã€‚</li>
<li>æå‡ºäº†æ–°å‹åˆå§‹åŒ–ç­–ç•¥ï¼Œèƒ½æ˜¾è‘—æé«˜æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>ç ”ç©¶äº†è·¨å›¾åƒè½¬ç§»æ€§ï¼Œé€šè¿‡å¼•å…¥é€šç”¨æ‰°åŠ¨å®ç°ã€‚</li>
<li>é’ˆå¯¹è§†è§‰ç¼–ç å™¨çš„æ–°å‹æŸå¤±å‡½æ•°è®¾è®¡ï¼Œè¯¥å‡½æ•°ä¸»è¦å…³æ³¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªä¸»æµVLMsä¸Šçš„å®éªŒè¯æ˜ï¼Œæ”¹è¿›åçš„æ”»å‡»ç­–ç•¥æœ‰æ•ˆæå‡äº†å¯¹æŠ—æ”»å‡»æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22982">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52ee17959fbeeba63042d2fc11806b2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1de241787c3b4bd19f93b6b0f92e4e20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-790fdbfb1dddae2979a6ff3f4cd43ab8.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Attention-to-Burstiness-Low-Rank-Bilinear-Prompt-Tuning"><a href="#Attention-to-Burstiness-Low-Rank-Bilinear-Prompt-Tuning" class="headerlink" title="Attention to Burstiness: Low-Rank Bilinear Prompt Tuning"></a>Attention to Burstiness: Low-Rank Bilinear Prompt Tuning</h2><p><strong>Authors:Yuzhu Wang, Manni Duan, Shu Kong</strong></p>
<p>Visual Prompt Tuning (VPT) is a parameter-efficient fune-tuning technique that adapts a pre-trained vision Transformer (ViT) by learning a small set of parameters in the input space, known as prompts. In VPT, we uncover <code>burstiness&#39;&#39; in the values arising from the interaction of image patch embeddings, and the key and query projectors within Transformer&#39;s self-attention module. Furthermore, the values of patch embeddings and the key and query projectors exhibit Laplacian and hyper-Laplacian distribution, respectively. Intuitively, these non-Gaussian distributions pose challenges for learning prompts. To address this, we propose whitening these data, de-correlating them and equalizing their variance towards more Gaussian before learning prompts. We derive the whitening matrix over random image patch embeddings and ViT&#39;s key and query projectors, and multiply it with the prompt to be learned in a bilinear manner. Surprisingly, this method significantly accelerates prompt tuning and boosts accuracy, e.g., $&gt;$25 accuracy points on the CUB dataset; interestingly, it learns </code>bursty promptsâ€™â€™. Extending the bilinear model which is known to introduce burstiness, we present a compact, low-rank version by learning two smaller matrices whose multiplication yields the final prompts. We call the proposed methods Bilinear Prompt Tuning (BPT). Extensive experiments across multiple benchmark datasets demonstrate that BPT methods not only outperform various VPT methods but also reduce parameter count and computation overhead. </p>
<blockquote>
<p>è§†è§‰æç¤ºå¾®è°ƒï¼ˆVPTï¼‰æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œå®ƒé€šè¿‡åœ¨å­¦ä¹ è¾“å…¥ç©ºé—´ä¸­çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼ˆç§°ä¸ºæç¤ºï¼‰æ¥é€‚åº”é¢„è®­ç»ƒçš„è§†è§‰Transformerï¼ˆViTï¼‰ã€‚åœ¨VPTä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†å›¾åƒè¡¥ä¸åµŒå…¥äº¤äº’æ‰€äº§ç”Ÿçš„å€¼ä¸­çš„â€œçªå‘æ€§â€ï¼Œä»¥åŠTransformerè‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é”®å’ŒæŸ¥è¯¢æŠ•å½±å™¨ã€‚æ­¤å¤–ï¼Œè¡¥ä¸åµŒå…¥çš„å€¼å’Œé”®ä»¥åŠæŸ¥è¯¢æŠ•å½±å™¨åˆ†åˆ«å‘ˆç°å‡ºæ‹‰æ™®æ‹‰æ–¯å’Œè¶…æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒã€‚ç›´è§‚åœ°è¯´ï¼Œè¿™äº›éé«˜æ–¯åˆ†å¸ƒç»™å­¦ä¹ æç¤ºå¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå¯¹è¿™äº›æ•°æ®è¿›è¡Œç™½åŒ–ï¼Œå»é™¤å…¶ç›¸å…³æ€§ï¼Œå¹¶åœ¨å­¦ä¹ æç¤ºä¹‹å‰ä½¿å…¶æ–¹å·®æ›´æ¥è¿‘é«˜æ–¯åˆ†å¸ƒã€‚æˆ‘ä»¬åœ¨éšæœºå›¾åƒè¡¥ä¸åµŒå…¥å’ŒViTçš„é”®å’ŒæŸ¥è¯¢æŠ•å½±å™¨ä¸Šæ¨å¯¼äº†ç™½åŒ–çŸ©é˜µï¼Œå¹¶å°†å…¶ä¸ä»¥åŒçº¿æ€§æ–¹å¼å­¦ä¹ çš„æç¤ºç›¸ä¹˜ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—åŠ é€Ÿäº†æç¤ºè°ƒæ•´ï¼Œå¹¶æé«˜äº†å‡†ç¡®æ€§ï¼Œä¾‹å¦‚åœ¨CUBæ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§æé«˜äº†è¶…è¿‡25ä¸ªç™¾åˆ†ç‚¹ï¼›æœ‰è¶£çš„æ˜¯ï¼Œå®ƒå­¦ä¹ äº†â€œçªå‘æ€§çš„æç¤ºâ€ã€‚åœ¨å·²çŸ¥ä¼šå¼•å…¥çªå‘æ€§çš„åŒçº¿æ€§æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç´§å‡‘ã€ä½é˜¶çš„ç‰ˆæœ¬ï¼Œé€šè¿‡å­¦ä¹ ä¸¤ä¸ªè¾ƒå°çš„çŸ©é˜µï¼Œå…¶ä¹˜ç§¯äº§ç”Ÿæœ€ç»ˆçš„æç¤ºã€‚æˆ‘ä»¬å°†æ‰€æå‡ºçš„æ–¹æ³•ç§°ä¸ºåŒçº¿æ€§æç¤ºå¾®è°ƒï¼ˆBPTï¼‰ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒBPTæ–¹æ³•ä¸ä»…ä¼˜äºå„ç§VPTæ–¹æ³•ï¼Œè€Œä¸”å‡å°‘äº†å‚æ•°è®¡æ•°å’Œè®¡ç®—å¼€é”€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22908v1">PDF</a> ICCV 2025</p>
<p><strong>æ‘˜è¦</strong><br>    è§†è§‰æç¤ºè°ƒæ•´ï¼ˆVPTï¼‰æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œå®ƒé€šè¿‡å­¦ä¹ è¢«ç§°ä¸ºæç¤ºçš„å°‘é‡å‚æ•°æ¥é€‚åº”é¢„è®­ç»ƒçš„è§†è§‰Transformerï¼ˆViTï¼‰ã€‚åœ¨VPTä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†å›¾åƒè¡¥ä¸åµŒå…¥å€¼çš„â€œçˆ†å‘æ€§â€ï¼Œä»¥åŠTransformerè‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­çš„é”®å’ŒæŸ¥è¯¢æŠ•å½±ä»ªä¸å…¶ä¹‹é—´çš„äº¤äº’ã€‚è¡¥ä¸åµŒå…¥å€¼ã€é”®å’ŒæŸ¥è¯¢æŠ•å½±ä»ªçš„å€¼å‘ˆç°å‡ºæ‹‰æ™®æ‹‰æ–¯å’Œè¶…æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒã€‚ä¸ºäº†åº”å¯¹è¿™äº›éé«˜æ–¯åˆ†å¸ƒå¯¹å­¦ä¹ æç¤ºçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºå¯¹æ•°æ®è¿›è¡Œç™½åŒ–ï¼Œå»ç›¸å…³å¹¶å‡è¡¡å…¶æ–¹å·®ä»¥æ›´æ¥è¿‘é«˜æ–¯åˆ†å¸ƒã€‚æˆ‘ä»¬é€šè¿‡éšæœºå›¾åƒè¡¥ä¸åµŒå…¥å’ŒViTçš„é”®ã€æŸ¥è¯¢æŠ•å½±ä»ªæ¥æ¨å¯¼ç™½åŒ–çŸ©é˜µï¼Œå¹¶ä»¥åŒçº¿æ€§æ–¹å¼å°†å…¶ä¸å¾…å­¦ä¹ çš„æç¤ºç›¸ä¹˜ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—åŠ é€Ÿäº†æç¤ºè°ƒæ•´å¹¶æé«˜äº†å‡†ç¡®æ€§ï¼Œä¾‹å¦‚åœ¨CUBæ•°æ®é›†ä¸Šæé«˜äº†è¶…è¿‡25ä¸ªå‡†ç¡®æ€§ç‚¹ã€‚é€šè¿‡æ‰©å±•å·²çŸ¥ä¼šå¼•å…¥çˆ†å‘æ€§çš„åŒçº¿æ€§æ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç´§å‡‘ã€ä½é˜¶çš„ç‰ˆæœ¬ï¼Œé€šè¿‡å­¦ä¹ ä¸¤ä¸ªè¾ƒå°çš„çŸ©é˜µï¼Œå…¶ä¹˜ç§¯äº§ç”Ÿæœ€ç»ˆçš„æç¤ºã€‚æˆ‘ä»¬å°†æ‰€æå‡ºçš„æ–¹æ³•ç§°ä¸ºåŒçº¿æ€§æç¤ºè°ƒæ•´ï¼ˆBPTï¼‰ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBPTæ–¹æ³•ä¸ä»…ä¼˜äºå„ç§VPTæ–¹æ³•ï¼Œè€Œä¸”å‡å°‘äº†å‚æ•°è®¡æ•°å’Œè®¡ç®—å¼€é”€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†è§‰æç¤ºè°ƒæ•´ï¼ˆVPTï¼‰æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œé€šè¿‡é€‚åº”é¢„è®­ç»ƒçš„è§†è§‰Transformeræ¥ä¼˜åŒ–æç¤ºã€‚</li>
<li>åœ¨VPTä¸­å‘ç°äº†å›¾åƒè¡¥ä¸åµŒå…¥å€¼çš„â€œçˆ†å‘æ€§â€ï¼Œä»¥åŠTransformerè‡ªæ³¨æ„åŠ›æ¨¡å—ä¸­é”®å’ŒæŸ¥è¯¢æŠ•å½±ä»ªä¸ä¹‹é—´çš„äº¤äº’å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>éé«˜æ–¯åˆ†å¸ƒåœ¨æç¤ºå­¦ä¹ ä¸­äº§ç”Ÿé—®é¢˜ï¼Œéœ€è¦é€šè¿‡æ•°æ®ç™½åŒ–ã€å»ç›¸å…³å’Œæ–¹å·®å‡è¡¡æ¥åº”å¯¹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºéšæœºå›¾åƒè¡¥ä¸åµŒå…¥å’ŒViTçš„é”®ã€æŸ¥è¯¢æŠ•å½±ä»ªçš„ç™½åŒ–çŸ©é˜µæ¨å¯¼æ–¹æ³•ã€‚</li>
<li>åŒçº¿æ€§æç¤ºè°ƒæ•´ï¼ˆBPTï¼‰æ˜¾è‘—åŠ é€Ÿäº†æç¤ºè°ƒæ•´è¿‡ç¨‹å¹¶æé«˜äº†å‡†ç¡®æ€§ï¼Œå¦‚CUBæ•°æ®é›†ä¸Šçš„å‡†ç¡®åº¦æå‡è¶…è¿‡25ç‚¹ã€‚</li>
<li>BPTæ‰©å±•äº†åŒçº¿æ€§æ¨¡å‹ï¼Œå¼•å…¥äº†ä¸€ä¸ªç´§å‡‘ã€ä½é˜¶çš„ç‰ˆæœ¬ï¼Œé€šè¿‡è¾ƒå°çš„çŸ©é˜µå­¦ä¹ äº§ç”Ÿæœ€ç»ˆæç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8321bfd2947707f5cff86a48e5d6382d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-165048eced147d9bbcfc189ce90fb0a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcf207d6775e7287a2cab17c9136716c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4e26906deafc8785e268b92a4088881.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SABRE-FL-Selective-and-Accurate-Backdoor-Rejection-for-Federated-Prompt-Learning"><a href="#SABRE-FL-Selective-and-Accurate-Backdoor-Rejection-for-Federated-Prompt-Learning" class="headerlink" title="SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt   Learning"></a>SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt   Learning</h2><p><strong>Authors:Momin Ahmad Khan, Yasra Chandio, Fatima Muhammad Anwar</strong></p>
<p>Federated Prompt Learning has emerged as a communication-efficient and privacy-preserving paradigm for adapting large vision-language models like CLIP across decentralized clients. However, the security implications of this setup remain underexplored. In this work, we present the first study of backdoor attacks in Federated Prompt Learning. We show that when malicious clients inject visually imperceptible, learnable noise triggers into input images, the global prompt learner becomes vulnerable to targeted misclassification while still maintaining high accuracy on clean inputs. Motivated by this vulnerability, we propose SABRE-FL, a lightweight, modular defense that filters poisoned prompt updates using an embedding-space anomaly detector trained offline on out-of-distribution data. SABRE-FL requires no access to raw client data or labels and generalizes across diverse datasets. We show, both theoretically and empirically, that malicious clients can be reliably identified and filtered using an embedding-based detector. Across five diverse datasets and four baseline defenses, SABRE-FL outperforms all baselines by significantly reducing backdoor accuracy while preserving clean accuracy, demonstrating strong empirical performance and underscoring the need for robust prompt learning in future federated systems. </p>
<blockquote>
<p>è”é‚¦æç¤ºå­¦ä¹ ï¼ˆFederated Prompt Learningï¼‰å·²æˆä¸ºåœ¨åˆ†å¸ƒå¼å®¢æˆ·ç«¯ä¸Šé€‚åº”å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„ä¸€ç§é€šä¿¡æ•ˆç‡é«˜ä¸”ä¿æŠ¤éšç§çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œè¯¥è®¾ç½®çš„å®‰å…¨å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹è”é‚¦æç¤ºå­¦ä¹ ä¸­çš„åé—¨æ”»å‡»è¿›è¡Œäº†é¦–æ¬¡ç ”ç©¶ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“æ¶æ„å®¢æˆ·ç«¯å°†è§†è§‰ä¸å¯å¯Ÿè§‰çš„å¯å­¦ä¹ å™ªå£°è§¦å‘å™¨æ³¨å…¥è¾“å…¥å›¾åƒæ—¶ï¼Œå…¨å±€æç¤ºå­¦ä¹ è€…ä¼šå®¹æ˜“å—åˆ°æœ‰é’ˆå¯¹æ€§çš„è¯¯åˆ†ç±»çš„å½±å“ï¼ŒåŒæ—¶ä»èƒ½åœ¨å¹²å‡€è¾“å…¥ä¸Šä¿æŒé«˜å‡†ç¡®æ€§ã€‚å—æ­¤æ¼æ´çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†SABRE-FLï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„æ¨¡å—åŒ–é˜²å¾¡æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨åœ¨ç¦»çº¿è®­ç»ƒäºç¦»ç¾¤åˆ†å¸ƒæ•°æ®ä¸Šçš„åµŒå…¥ç©ºé—´å¼‚å¸¸æ£€æµ‹å™¨æ¥è¿‡æ»¤ä¸­æ¯’æç¤ºæ›´æ–°ã€‚SABRE-FLæ— éœ€è®¿é—®åŸå§‹å®¢æˆ·ç«¯æ•°æ®æˆ–æ ‡ç­¾ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å„ç§æ•°æ®é›†ä¸Šè¿›è¡Œæ³›åŒ–ã€‚æˆ‘ä»¬ä»ç†è®ºå’Œå®è·µä¸¤æ–¹é¢è¯æ˜ï¼Œä½¿ç”¨åŸºäºåµŒå…¥çš„æ£€æµ‹å™¨å¯ä»¥å¯é åœ°è¯†åˆ«å’Œè¿‡æ»¤æ¶æ„å®¢æˆ·ç«¯ã€‚åœ¨äº”ä¸ªä¸åŒçš„æ•°æ®é›†å’Œå››ä¸ªåŸºçº¿é˜²å¾¡æªæ–½ä¸­ï¼ŒSABRE-FLæ˜¾è‘—å‡å°‘äº†åé—¨å‡†ç¡®æ€§åŒæ—¶ä¿æŒäº†æ¸…æ´å‡†ç¡®æ€§ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„å®é™…æ€§èƒ½å’Œåœ¨æœªæ¥è”é‚¦ç³»ç»Ÿä¸­å¯¹ç¨³å¥æç¤ºå­¦ä¹ çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22506v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è”é‚¦æç¤ºå­¦ä¹ ï¼ˆFederated Prompt Learningï¼‰æ˜¯é€‚åº”åˆ†å¸ƒå¼å®¢æˆ·ç«¯çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„ä¸€ç§é€šä¿¡é«˜æ•ˆä¸”éšç§ä¿æŠ¤çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¯¥è®¾ç½®çš„å®‰å…¨å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶é¦–æ¬¡ç ”ç©¶äº†è”é‚¦æç¤ºå­¦ä¹ ä¸­çš„åé—¨æ”»å‡»ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“æ¶æ„å®¢æˆ·ç«¯å‘è¾“å…¥å›¾åƒæ³¨å…¥è§†è§‰ä¸å¯å¯Ÿè§‰çš„å¯å­¦ä¹ å™ªå£°è§¦å‘å™¨æ—¶ï¼Œå…¨å±€æç¤ºå­¦ä¹ è€…ä»ç„¶å¯¹å¹²å‡€è¾“å…¥ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶å®¹æ˜“å—åˆ°ç›®æ ‡è¯¯åˆ†ç±»çš„æ”»å‡»ã€‚é’ˆå¯¹è¿™ä¸€æ¼æ´ï¼Œæˆ‘ä»¬æå‡ºäº†SABRE-FLï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§çš„æ¨¡å—åŒ–é˜²å¾¡æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ç¦»çº¿è®­ç»ƒäºéåˆ†å¸ƒæ•°æ®çš„åµŒå…¥ç©ºé—´å¼‚å¸¸æ£€æµ‹å™¨æ¥è¿‡æ»¤æœ‰æ¯’çš„æç¤ºæ›´æ–°ã€‚SABRE-FLæ— éœ€è®¿é—®åŸå§‹å®¢æˆ·ç«¯æ•°æ®æˆ–æ ‡ç­¾ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°é€šç”¨æ€§ã€‚æˆ‘ä»¬ç†è®ºåŠå®è¯åœ°è¯æ˜ï¼Œä½¿ç”¨åŸºäºåµŒå…¥çš„æ£€æµ‹å™¨å¯ä»¥å¯é åœ°è¯†åˆ«å¹¶è¿‡æ»¤æ¶æ„å®¢æˆ·ç«¯ã€‚åœ¨äº”ä¸ªä¸åŒçš„æ•°æ®é›†å’Œå››ä¸ªåŸºå‡†é˜²å¾¡ç­–ç•¥ä¸Šï¼ŒSABRE-FLæ˜¾è‘—é™ä½äº†åé—¨å‡†ç¡®æ€§åŒæ—¶ä¿æŒäº†å¯¹æ¸…æ´æ•°æ®çš„å‡†ç¡®æ€§ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„è¡¨ç°å¹¶å¼ºè°ƒæœªæ¥è”é‚¦ç³»ç»Ÿä¸­ç¨³å¥æç¤ºå­¦ä¹ çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦æç¤ºå­¦ä¹ æ˜¯ä¸€ç§ç”¨äºåœ¨åˆ†å¸ƒå¼å®¢æˆ·ç«¯ä¸Šé€‚åº”å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°èŒƒå¼ã€‚</li>
<li>è”é‚¦æç¤ºå­¦ä¹ ä¸­å­˜åœ¨åé—¨æ”»å‡»çš„å®‰å…¨éšæ‚£ã€‚</li>
<li>æ¶æ„å®¢æˆ·ç«¯å¯ä»¥é€šè¿‡æ³¨å…¥è§†è§‰ä¸å¯å¯Ÿè§‰çš„å™ªå£°è§¦å‘å™¨æ¥å½±å“å…¨å±€æç¤ºå­¦ä¹ è€…çš„å‡†ç¡®æ€§ã€‚</li>
<li>SABRE-FLæ˜¯ä¸€ç§æ–°çš„é˜²å¾¡ç­–ç•¥ï¼Œå®ƒé€šè¿‡åµŒå…¥ç©ºé—´å¼‚å¸¸æ£€æµ‹å™¨æ¥è¿‡æ»¤æœ‰æ¯’çš„æç¤ºæ›´æ–°ã€‚</li>
<li>SABRE-FLæ— éœ€è®¿é—®åŸå§‹å®¢æˆ·ç«¯æ•°æ®æˆ–æ ‡ç­¾ï¼Œä¸”åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå…·æœ‰é€šç”¨æ€§ã€‚</li>
<li>ä½¿ç”¨åŸºäºåµŒå…¥çš„æ£€æµ‹å™¨å¯ä»¥å¯é åœ°è¯†åˆ«å’Œè¿‡æ»¤æ¶æ„å®¢æˆ·ç«¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b6bb79ef837f891d10cea979f4fed77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57190c6660541604c21154bfb4754f97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a5f7fea60f87c3d2b4bb4080518ad1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0946b76e74f34b3b9f66fe1468bda42f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MiCo-Multi-image-Contrast-for-Reinforcement-Visual-Reasoning"><a href="#MiCo-Multi-image-Contrast-for-Reinforcement-Visual-Reasoning" class="headerlink" title="MiCo: Multi-image Contrast for Reinforcement Visual Reasoning"></a>MiCo: Multi-image Contrast for Reinforcement Visual Reasoning</h2><p><strong>Authors:Xi Chen, Mingkang Zhu, Shaoteng Liu, Xiaoyang Wu, Xiaogang Xu, Yu Liu, Xiang Bai, Hengshuang Zhao</strong></p>
<p>This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†å®ç°é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†ï¼Œä»¥è¿æ¥å¤šå¼ å›¾ç‰‡ä¸­çš„è§†è§‰çº¿ç´¢ã€‚ä¸€ç§ç›´æ¥è§£å†³æ–¹æ¡ˆæ˜¯ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Modelsï¼Œç®€ç§°VLMsï¼‰é€‚åº”åŸºäºè§„åˆ™çš„ç­–ç•¥å¼ºåŒ–å­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºäººå·¥æ•´ç†çš„é—®é¢˜å’Œç­”æ¡ˆé…å¯¹ï¼Œåœ¨å¤„ç†ç²¾ç»†ç²’åº¦çš„è§†è§‰ç»†èŠ‚å’Œè·¨å›¾åƒçš„å¤æ‚é€»è¾‘æ—¶ï¼Œå¯èƒ½ä¼šé¢ä¸´ç‰¹åˆ«å¤§çš„æŒ‘æˆ˜ã€‚å—è‡ªæˆ‘ç›‘ç£çš„è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„å¯å‘ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å›¾åƒåŒ…å«å¯ä»¥ä½œä¸ºç›‘ç£çš„å›ºæœ‰çº¦æŸã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æ„å»ºäº†å›¾åƒä¸‰å…ƒç»„ï¼ŒåŒ…æ‹¬åŒä¸€å›¾åƒçš„ä¸¤ä¸ªå¢å¼ºè§†å›¾å’Œç¬¬ä¸‰å¼ ç›¸ä¼¼ä½†ä¸åŒçš„å›¾åƒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹è¢«æç¤ºç”Ÿæˆä¸€ä¸ªæ¨ç†è¿‡ç¨‹æ¥æ¯”è¾ƒè¿™äº›å›¾åƒï¼ˆå³ï¼Œç¡®å®šæ˜¯å¦ç›¸åŒæˆ–ä¸åŒï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºè§„åˆ™çš„ç­–ç•¥å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æ¨¡å‹ã€‚ç”±äºé«˜è§†è§‰ç›¸ä¼¼æ€§å’Œå¢å¼ºæŠ€æœ¯çš„å­˜åœ¨ï¼Œæ¨¡å‹å¿…é¡»å…³æ³¨ç»†å¾®çš„è§†è§‰å˜åŒ–ï¼Œå¹¶è¿›è¡Œé€»è¾‘æ¨ç†ä»¥å–å¾—æˆåŠŸã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡ä»…é€šè¿‡è§†è§‰æ¯”è¾ƒä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œæ‰€å­¦ä¹ çš„æ¨ç†èƒ½åŠ›å¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°å„ç§é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–äºä»»ä½•äººå·¥æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆé…å¯¹ï¼Œåœ¨å¤šå›¾åƒæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¹¶åœ¨ä¸€èˆ¬è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22434v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†å¦‚ä½•é€šè¿‡Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†è·¨å¤šå¼ å›¾ç‰‡é“¾æ¥è§†è§‰çº¿ç´¢ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ çš„ç®€å•è§£å†³æ–¹æ¡ˆç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ä½†è¯¥æ–¹æ³•é€šå¸¸ä¾èµ–äºäººå·¥ç¼–è¾‘çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œå¯¹äºå¤„ç†ç»†å¾®è§†è§‰ç»†èŠ‚å’Œè·¨å›¾åƒå¤æ‚é€»è¾‘çš„æŒ‘æˆ˜æ€§è¾ƒå¤§ã€‚å—è‡ªæˆ‘ç›‘ç£çš„è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„å¯å‘ï¼Œæ–‡ç« è§‚å¯Ÿåˆ°å›¾åƒä¸­åŒ…å«çš„å›ºæœ‰çº¦æŸå¯ä»¥ä½œä¸ºç›‘ç£ä½¿ç”¨ã€‚åŸºäºæ­¤ï¼Œæ–‡ç« æ„å»ºäº†åŒ…å«åŒä¸€å¼ å›¾ç‰‡çš„ä¸¤ç§å¢å¼ºè§†å›¾å’Œä¸€å¼ ç›¸ä¼¼ä½†ä¸åŒçš„å›¾ç‰‡çš„å›¾åƒä¸‰å…ƒç»„ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹è¢«æç¤ºç”Ÿæˆä¸€ä¸ªæ¯”è¾ƒè¿™äº›å›¾åƒï¼ˆå³åˆ¤æ–­ç›¸åŒæˆ–ä¸åŒï¼‰çš„æ¨ç†è¿‡ç¨‹ã€‚éšåç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨¡å‹ã€‚ç”±äºå›¾åƒçš„é«˜åº¦è§†è§‰ç›¸ä¼¼æ€§å’Œå¢å¼ºæŠ€æœ¯çš„å­˜åœ¨ï¼Œæ¨¡å‹å¿…é¡»å…³æ³¨å¾®å¦™çš„è§†è§‰å˜åŒ–å¹¶è¿›è¡Œé€»è¾‘æ¨ç†æ‰èƒ½æˆåŠŸã€‚å®éªŒè¡¨æ˜ï¼Œè™½ç„¶ä»…é€šè¿‡è§†è§‰æ¯”è¾ƒä»»åŠ¡è¿›è¡Œè®­ç»ƒï¼Œä½†ä¹ å¾—çš„æ¨ç†èƒ½åŠ›å¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºå„ç§é—®é¢˜ã€‚æ— éœ€ä¾èµ–ä»»ä½•äººå·¥æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œè¯¥æ–¹æ³•åœ¨å¤šå›¾åƒæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åœ¨ä¸€èˆ¬è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶ä½¿Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†èƒ½å¤Ÿè·¨å¤šä¸ªå›¾åƒé“¾æ¥è§†è§‰çº¿ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè§„åˆ™å¼ºåŒ–å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆæ¥é€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚</li>
<li>æ³¨æ„åˆ°å›¾åƒä¸­åŒ…å«çš„å›ºæœ‰çº¦æŸå¯ç”¨äºè‡ªæˆ‘ç›‘ç£å­¦ä¹ ã€‚</li>
<li>é€šè¿‡æ„å»ºå›¾åƒä¸‰å…ƒç»„æ¥è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬åŒä¸€å¼ å›¾ç‰‡çš„ä¸¤ç§å¢å¼ºè§†å›¾å’Œä¸€å¼ ç›¸ä¼¼ä½†ä¸åŒçš„å›¾ç‰‡ã€‚</li>
<li>æ¨¡å‹å¿…é¡»å…³æ³¨å¾®å¦™çš„è§†è§‰å˜åŒ–å¹¶è¿›è¡Œé€»è¾‘æ¨ç†ä»¥æˆåŠŸå®Œæˆä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šå›¾åƒæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c393203fc7c0a94f5bd878d651ebb1c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-018e0fde7deb4a78e29f0945d5245a5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b03185cace3b2a9390b8b3b6e30f434.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8db4750d9664b50155d0321261db691.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="From-Ground-to-Air-Noise-Robustness-in-Vision-Transformers-and-CNNs-for-Event-Based-Vehicle-Classification-with-Potential-UAV-Applications"><a href="#From-Ground-to-Air-Noise-Robustness-in-Vision-Transformers-and-CNNs-for-Event-Based-Vehicle-Classification-with-Potential-UAV-Applications" class="headerlink" title="From Ground to Air: Noise Robustness in Vision Transformers and CNNs for   Event-Based Vehicle Classification with Potential UAV Applications"></a>From Ground to Air: Noise Robustness in Vision Transformers and CNNs for   Event-Based Vehicle Classification with Potential UAV Applications</h2><p><strong>Authors:Nouf Almesafri, Hector Figueiredo, Miguel Arana-Catania</strong></p>
<p>This study investigates the performance of the two most relevant computer vision deep learning architectures, Convolutional Neural Network and Vision Transformer, for event-based cameras. These cameras capture scene changes, unlike traditional frame-based cameras with capture static images, and are particularly suited for dynamic environments such as UAVs and autonomous vehicles. The deep learning models studied in this work are ResNet34 and ViT B16, fine-tuned on the GEN1 event-based dataset. The research evaluates and compares these models under both standard conditions and in the presence of simulated noise. Initial evaluations on the clean GEN1 dataset reveal that ResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with ResNet34 showing a slight advantage in classification accuracy. However, the ViT B16 model demonstrates notable robustness, particularly given its pre-training on a smaller dataset. Although this study focuses on ground-based vehicle classification, the methodologies and findings hold significant promise for adaptation to UAV contexts, including aerial object classification and event-based vision systems for aviation-related tasks. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ä¸¤ç§ä¸è®¡ç®—æœºè§†è§‰æ·±åº¦å­¦ä¹ æ¶æ„æœ€ç›¸å…³çš„æ¨¡å‹â€”â€”å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆVision Transformerï¼‰åœ¨äº‹ä»¶ç›¸æœºä¸Šçš„åº”ç”¨ã€‚ä¸ä¼ ç»ŸåŸºäºå¸§çš„ç›¸æœºæ•æ‰é™æ€å›¾åƒä¸åŒï¼Œè¿™äº›ç›¸æœºèƒ½å¤Ÿæ•æ‰åœºæ™¯å˜åŒ–ï¼Œç‰¹åˆ«é€‚ç”¨äºæ— äººæœºå’Œè‡ªåŠ¨é©¾é©¶è½¦è¾†ç­‰åŠ¨æ€ç¯å¢ƒã€‚æœ¬ç ”ç©¶ä¸­çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸ºResNet34å’ŒViT B16ï¼Œç»è¿‡GEN1äº‹ä»¶æ•°æ®é›†å¾®è°ƒã€‚è¯¥ç ”ç©¶åœ¨æ ‡å‡†æ¡ä»¶å’Œæ¨¡æ‹Ÿå™ªå£°å­˜åœ¨çš„æƒ…å†µä¸‹å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°æ¯”è¾ƒã€‚åœ¨å¹²å‡€çš„GEN1æ•°æ®é›†ä¸Šçš„åˆæ­¥è¯„ä¼°æ˜¾ç¤ºï¼ŒResNet34å’ŒViT B16çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º88%å’Œ86%ï¼Œå…¶ä¸­ResNet34åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šç¨å ä¼˜åŠ¿ã€‚ç„¶è€Œï¼ŒViT B16æ¨¡å‹æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ç¨³å¥æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„æƒ…å†µä¸‹ã€‚è™½ç„¶æœ¬ç ”ç©¶é‡ç‚¹å…³æ³¨åœ°é¢è½¦è¾†åˆ†ç±»ï¼Œä½†æ–¹æ³•å’Œç ”ç©¶ç»“æœå¯¹äºé€‚åº”æ— äººæœºä¸Šä¸‹æ–‡å…·æœ‰å·¨å¤§æ½œåŠ›ï¼ŒåŒ…æ‹¬ç©ºä¸­ç›®æ ‡åˆ†ç±»å’Œç”¨äºèˆªç©ºç›¸å…³ä»»åŠ¡çš„äº‹ä»¶è§†è§‰ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22360v1">PDF</a> 16 pages, 17 figures, 9 tables. To be presented in AIAA AVIATION   Forum 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ä¸¤ç§æœ€ç›¸å…³çš„è®¡ç®—æœºè§†è§‰æ·±åº¦å­¦ä¹ æ¶æ„â€”â€”å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆVision Transformerï¼‰åœ¨äº‹ä»¶ç›¸æœºä¸Šçš„åº”ç”¨ã€‚äº‹ä»¶ç›¸æœºèƒ½æ•æ‰åœºæ™¯å˜åŒ–ï¼Œé€‚åˆåŠ¨æ€ç¯å¢ƒå¦‚æ— äººæœºå’Œè‡ªåŠ¨é©¾é©¶è½¦è¾†ã€‚åœ¨GEN1äº‹ä»¶æ•°æ®é›†ä¸Šå¾®è°ƒResNet34å’ŒViT B16æ¨¡å‹ï¼Œç ”ç©¶è¯„ä¼°äº†è¿™äº›æ¨¡å‹åœ¨æ ‡å‡†æ¡ä»¶å’Œæ¨¡æ‹Ÿå™ªå£°ç¯å¢ƒä¸‹çš„æ€§èƒ½ã€‚åœ¨å¹²å‡€çš„GEN1æ•°æ®é›†ä¸Šåˆæ­¥è¯„ä¼°æ˜¾ç¤ºï¼ŒResNet34å’ŒViT B16çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º88%å’Œ86%ï¼ŒResNet34åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šç¨å ä¼˜åŠ¿ã€‚ç„¶è€Œï¼ŒViT B16æ¨¡å‹å±•ç°å‡ºæ˜¾è‘—é²æ£’æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒå°æ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒè¡¨ç°ã€‚å°½ç®¡è¯¥ç ”ç©¶ä¸»è¦å…³æ³¨åœ°é¢è½¦è¾†åˆ†ç±»ï¼Œä½†å…¶æ–¹æ³•å’Œç»“æœå¯¹äºæ— äººæœºçš„é€‚åº”æ½œåŠ›å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¦‚ç©ºä¸­ç›®æ ‡åˆ†ç±»å’ŒåŸºäºäº‹ä»¶è§†è§‰ç³»ç»Ÿçš„èˆªç©ºä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æ¯”è¾ƒäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆVision Transformerï¼‰åœ¨äº‹ä»¶ç›¸æœºä¸Šçš„æ€§èƒ½ã€‚</li>
<li>äº‹ä»¶ç›¸æœºèƒ½æ•æ‰åœºæ™¯å˜åŒ–ï¼Œé€‚åˆåŠ¨æ€ç¯å¢ƒåº”ç”¨å¦‚æ— äººæœºå’Œè‡ªåŠ¨é©¾é©¶è½¦è¾†ã€‚</li>
<li>åœ¨GEN1äº‹ä»¶æ•°æ®é›†ä¸Šå¾®è°ƒçš„ResNet34å’ŒViT B16æ¨¡å‹è¢«è¯„ä¼°ã€‚</li>
<li>åœ¨æ ‡å‡†æ¡ä»¶ä¸‹ï¼ŒResNet34åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šç¨ä¼˜äºViT B16ã€‚</li>
<li>ViT B16æ¨¡å‹å±•ç°å‡ºæ˜¾è‘—é²æ£’æ€§ï¼Œå°¤å…¶åœ¨è¾ƒå°æ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒè¡¨ç°ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹äºåœ°é¢è½¦è¾†åˆ†ç±»å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3a2f5706965156470a697a1bd322e23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec40c452483f1b732a864d434d7b6ab5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a3a951a2c1015141b4868ddad9800fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aa740197e2e8bf1cdecd70358d46470.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b887be55c9d2eac9162ed5fbe607b03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b550b118f85796959b138797bec169ed.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Evaluating-Multimodal-Large-Language-Models-on-Educational-Textbook-Question-Answering"><a href="#Evaluating-Multimodal-Large-Language-Models-on-Educational-Textbook-Question-Answering" class="headerlink" title="Evaluating Multimodal Large Language Models on Educational Textbook   Question Answering"></a>Evaluating Multimodal Large Language Models on Educational Textbook   Question Answering</h2><p><strong>Authors:Hessa A. Alawwad, Anas Zafar, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal</strong></p>
<p>Multimodal large language models (MLLMs) have recently achieved significant success in visionâ€“language tasks. However, their capacity to reason over complex, long lessons and intricate educational diagrams that cannot be represented as a single natural image remains largely untested. In this work, we present the first evaluation of state-of-the-art MLLMs on the textbook question answering (TQA) task using the CK12-QA dataset. We assess the performance of recent vision-language models, including LLaVA and LLaMA 3.2-Vision, across various input configurations. Additionally, we introduce a lightweight multimodal retrieval-augmented generation (RAG) pipeline that integrates both paragraphs and diagrams from the lesson into the prompt. Our results demonstrate the influence of retrieved educational context on model accuracy and reasoning, while also revealing current limitations in handling question-context relationships and the potential for noise, pointing to key directions for future research in multimodal AI-driven learning. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ€è¿‘åœ¨è§†è§‰ä¸è¯­è¨€ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹å¤æ‚ã€å†—é•¿çš„è¯¾ç¨‹ä¸ç»†è‡´çš„æ•™è‚²å›¾è¡¨è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ä»æœªå¾—åˆ°å¹¿æ³›æµ‹è¯•ï¼Œè¿™äº›æ•™è‚²å›¾è¡¨ä¸èƒ½è¡¨ç¤ºä¸ºå•ä¸€çš„è‡ªç„¶å›¾åƒã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨CK12-QAæ•°æ®é›†é¦–æ¬¡è¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™ç§‘ä¹¦é—®ç­”ï¼ˆTQAï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬è¯„ä¼°äº†å„ç§è¾“å…¥é…ç½®ä¸‹çš„è¿‘æœŸè§†è§‰è¯­è¨€æ¨¡å‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬LLaVAå’ŒLLaMA 3.2-Visionã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹ï¼Œå®ƒå°†è¯¾ç¨‹ä¸­çš„æ®µè½å’Œå›¾è¡¨éƒ½æ•´åˆåˆ°æç¤ºä¸­ã€‚æˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†æ£€ç´¢åˆ°çš„æ•™è‚²ä¸Šä¸‹æ–‡å¯¹æ¨¡å‹å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›çš„å½±å“ï¼ŒåŒæ—¶æ­ç¤ºäº†å¤„ç†é—®é¢˜ä¸ä¸Šä¸‹æ–‡å…³ç³»æ–¹é¢çš„å½“å‰å±€é™æ€§ä»¥åŠå¯¹å™ªå£°çš„æ½œåœ¨å½±å“ï¼ŒæŒ‡å‡ºäº†æœªæ¥å¤šæ¨¡æ€äººå·¥æ™ºèƒ½é©±åŠ¨å­¦ä¹ çš„ç ”ç©¶å…³é”®æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21596v1">PDF</a> 7 Pages</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡è¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•™ç§‘ä¹¦é—®ç­”ï¼ˆTQAï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œä½¿ç”¨äº†CK12-QAæ•°æ®é›†ã€‚ç ”ç©¶åŒ…æ‹¬å¯¹ä¸åŒè§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½è¯„ä¼°ï¼Œå¦‚LLaVAå’ŒLLaMA 3.2-Visionï¼Œå¹¶æ¢è®¨äº†å„ç§è¾“å…¥é…ç½®çš„å½±å“ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹ï¼Œè¯¥æµç¨‹å°†è¯¾ç¨‹æ®µè½å’Œå›¾è¡¨æ•´åˆåˆ°æç¤ºä¸­ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ£€ç´¢çš„æ•™è‚²èƒŒæ™¯å¯¹æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›æœ‰å½±å“ï¼ŒåŒæ—¶æ­ç¤ºäº†å¤„ç†é—®é¢˜ä¸ä¸Šä¸‹æ–‡å…³ç³»æ–¹é¢çš„å½“å‰å±€é™æ€§ä»¥åŠæ½œåœ¨å™ªå£°é—®é¢˜ï¼Œä¸ºæœªæ¥çš„å¤šæ¨¡æ€AIé©±åŠ¨å­¦ä¹ ç ”ç©¶æŒ‡å‡ºäº†å…³é”®æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•™ç§‘ä¹¦é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æˆåŠŸã€‚</li>
<li>LLaVAå’ŒLLaMA 3.2-Visionç­‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨CK12-QAæ•°æ®é›†ä¸Šçš„æ€§èƒ½å¾—åˆ°äº†è¯„ä¼°ã€‚</li>
<li>è¾“å…¥é…ç½®å¯¹æ¨¡å‹è¡¨ç°æœ‰å½±å“ã€‚</li>
<li>æ£€ç´¢çš„æ•™è‚²èƒŒæ™¯ä¿¡æ¯å¯¹æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨å¤„ç†é—®é¢˜ä¸ä¸Šä¸‹æ–‡å…³ç³»æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ¨¡å‹æ˜“å—å™ªå£°å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-30ccc766b4742d30beaf2bb3ba71acc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11aef68223a4dd59d415b94071c8ee31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8c8dd9b854f61710825722d4e2abfd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87b916c0a65f943ea250cf3c13e44758.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6da2699cc58c652bf81559ac3e9215f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f8234380fb39f81d23824ca8a6e0f0c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ff6a946e8e5fe7711bbad5ef83c0747.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AQUA20-A-Benchmark-Dataset-for-Underwater-Species-Classification-under-Challenging-Conditions"><a href="#AQUA20-A-Benchmark-Dataset-for-Underwater-Species-Classification-under-Challenging-Conditions" class="headerlink" title="AQUA20: A Benchmark Dataset for Underwater Species Classification under   Challenging Conditions"></a>AQUA20: A Benchmark Dataset for Underwater Species Classification under   Challenging Conditions</h2><p><strong>Authors:Taufikur Rahman Fuad, Sabbir Ahmed, Shahriar Ivan</strong></p>
<p>Robust visual recognition in underwater environments remains a significant challenge due to complex distortions such as turbidity, low illumination, and occlusion, which severely degrade the performance of standard vision systems. This paper introduces AQUA20, a comprehensive benchmark dataset comprising 8,171 underwater images across 20 marine species reflecting real-world environmental challenges such as illumination, turbidity, occlusions, etc., providing a valuable resource for underwater visual understanding. Thirteen state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet, MobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were evaluated to benchmark their performance in classifying marine species under challenging conditions. Our experimental results show ConvNeXt achieving the best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of 90.69%, as well as the highest overall F1-score of 88.92% with moderately large parameter size. The results obtained from our other benchmark models also demonstrate trade-offs between complexity and performance. We also provide an extensive explainability analysis using GRAD-CAM and LIME for interpreting the strengths and pitfalls of the models. Our results reveal substantial room for improvement in underwater species recognition and demonstrate the value of AQUA20 as a foundation for future research in this domain. The dataset is publicly available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/taufiktrf/AQUA20">https://huggingface.co/datasets/taufiktrf/AQUA20</a>. </p>
<blockquote>
<p>åœ¨æ°´ä¸‹ç¯å¢ƒä¸­å®ç°ç¨³å¥çš„è§†è§‰è¯†åˆ«ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºè¯¸å¦‚æµ‘æµŠã€ä½å…‰ç…§å’Œé®æŒ¡ä¹‹ç±»çš„å¤æ‚å¤±çœŸä¼šä¸¥é‡é™ä½æ ‡å‡†è§†è§‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚æœ¬æ–‡ä»‹ç»äº†AQUA20ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«8171å¼ æ°´ä¸‹å›¾åƒçš„ç»¼åˆåŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–äº†20ä¸ªæµ·æ´‹ç‰©ç§ï¼Œåæ˜ äº†ç°å®ä¸–ç•Œä¸­çš„ç¯å¢ƒæŒ‘æˆ˜ï¼Œå¦‚å…‰ç…§ã€æµ‘æµŠåº¦ã€é®æŒ¡ç­‰ï¼Œä¸ºæ°´ä¸‹è§†è§‰ç†è§£æä¾›äº†å®è´µçš„èµ„æºã€‚æˆ‘ä»¬è¯„ä¼°äº†13ç§æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬è½»é‡çº§CNNï¼ˆSqueezeNetã€MobileNetV2ï¼‰å’ŒåŸºäºtransformerçš„æ¶æ„ï¼ˆViTã€ConvNeXtï¼‰ï¼Œä»¥åŸºå‡†æµ‹è¯•å®ƒä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹å¯¹æµ·æ´‹ç‰©ç§è¿›è¡Œåˆ†ç±»çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConvNeXtè¡¨ç°æœ€ä½³ï¼Œå‰ä¸‰åå‡†ç¡®ç‡è¾¾åˆ°äº†98.82%ï¼Œç¬¬ä¸€åå‡†ç¡®ç‡ä¸º90.69%ï¼ŒåŒæ—¶æ€»ä½“F1åˆ†æ•°æœ€é«˜ï¼Œè¾¾åˆ°äº†88.92%ï¼Œä¸”å‚æ•°è§„æ¨¡é€‚ä¸­ã€‚å…¶ä»–åŸºå‡†æ¨¡å‹çš„ç»“æœä¹Ÿæ˜¾ç¤ºäº†å¤æ‚æ€§å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨GRAD-CAMå’ŒLIMEè¿›è¡Œäº†å¹¿æ³›çš„è§£é‡Šæ€§åˆ†æï¼Œä»¥è§£é‡Šæ¨¡å‹çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†æ°´ä¸‹ç‰©ç§è¯†åˆ«æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œå¹¶è¯æ˜äº†AQUA20ä½œä¸ºæœªæ¥è¯¥é¢†åŸŸç ”ç©¶åŸºç¡€çš„ä»·å€¼ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/taufiktrf/AQUA20%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/datasets/taufiktrf/AQUA20å…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17455v2">PDF</a> Submitted to AJSE Springer</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºAQUA20çš„æ°´ä¸‹å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«8,171å¼ åæ˜ çœŸå®æ°´ä¸‹ç¯å¢ƒæŒ‘æˆ˜ï¼ˆå¦‚å…‰ç…§ã€æµŠåº¦ã€é®æŒ¡ç­‰ï¼‰çš„20ç§æµ·æ´‹ç”Ÿç‰©å›¾åƒã€‚è¯„ä¼°äº†å¤šç§é¡¶çº§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆåŒ…æ‹¬è½»é‡çº§CNNå’ŒåŸºäºtransformerçš„æ¶æ„ï¼‰åœ¨æ°´ä¸‹ç”Ÿç‰©åˆ†ç±»æ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒConvNeXtæ¨¡å‹è¡¨ç°æœ€ä½³ï¼ŒTop-3å‡†ç¡®ç‡ä¸º98.82%ï¼ŒTop-1å‡†ç¡®ç‡ä¸º90.69%ï¼Œæ€»ä½“F1åˆ†æ•°ä¸º88.92%ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æä¾›äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§åˆ†æã€‚æœ¬æ–‡ç»“æœå±•ç¤ºå‡ºæ°´ä¸‹ç”Ÿç‰©è¯†åˆ«é¢†åŸŸçš„å·¨å¤§è¿›æ­¥ç©ºé—´ï¼ŒåŒæ—¶å¼ºè°ƒäº†AQUA20æ•°æ®é›†å¯¹æœªæ¥ç ”ç©¶çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AQUA20æ•°æ®é›†åŒ…å«çœŸå®æ°´ä¸‹ç¯å¢ƒæŒ‘æˆ˜çš„å›¾åƒï¼Œé€‚ç”¨äºæ°´ä¸‹è§†è§‰ç†è§£ç ”ç©¶ã€‚</li>
<li>è¯„ä¼°äº†å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ°´ä¸‹ç”Ÿç‰©åˆ†ç±»æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>ConvNeXtæ¨¡å‹åœ¨AQUA20æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ï¼Œå…·æœ‰é«˜çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚</li>
<li>æ¨¡å‹çš„å¯è§£é‡Šæ€§åˆ†ææ­ç¤ºäº†å…¶ä¼˜åŠ¿å’Œå±€é™ã€‚</li>
<li>æ°´ä¸‹ç”Ÿç‰©è¯†åˆ«é¢†åŸŸå­˜åœ¨å·¨å¤§è¿›æ­¥ç©ºé—´ã€‚</li>
<li>AQUA20æ•°æ®é›†å¯¹æœªæ¥ç ”ç©¶å…·æœ‰é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08491d2146d42f4973a6f7fa965104e1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OpenPath-Open-Set-Active-Learning-for-Pathology-Image-Classification-via-Pre-trained-Vision-Language-Models"><a href="#OpenPath-Open-Set-Active-Learning-for-Pathology-Image-Classification-via-Pre-trained-Vision-Language-Models" class="headerlink" title="OpenPath: Open-Set Active Learning for Pathology Image Classification   via Pre-trained Vision-Language Models"></a>OpenPath: Open-Set Active Learning for Pathology Image Classification   via Pre-trained Vision-Language Models</h2><p><strong>Authors:Lanfeng Zhong, Xin Liao, Shichuan Zhang, Shaoting Zhang, Guotai Wang</strong></p>
<p>Pathology image classification plays a crucial role in accurate medical diagnosis and treatment planning. Training high-performance models for this task typically requires large-scale annotated datasets, which are both expensive and time-consuming to acquire. Active Learning (AL) offers a solution by iteratively selecting the most informative samples for annotation, thereby reducing the labeling effort. However, most AL methods are designed under the assumption of a closed-set scenario, where all the unannotated images belong to target classes. In real-world clinical environments, the unlabeled pool often contains a substantial amount of Out-Of-Distribution (OOD) data, leading to low efficiency of annotation in traditional AL methods. Furthermore, most existing AL methods start with random selection in the first query round, leading to a significant waste of labeling costs in open-set scenarios. To address these challenges, we propose OpenPath, a novel open-set active learning approach for pathological image classification leveraging a pre-trained Vision-Language Model (VLM). In the first query, we propose task-specific prompts that combine target and relevant non-target class prompts to effectively select In-Distribution (ID) and informative samples from the unlabeled pool. In subsequent queries, Diverse Informative ID Sampling (DIS) that includes Prototype-based ID candidate Selection (PIS) and Entropy-Guided Stochastic Sampling (EGSS) is proposed to ensure both purity and informativeness in a query, avoiding the selection of OOD samples. Experiments on two public pathology image datasets show that OpenPath significantly enhances the modelâ€™s performance due to its high purity of selected samples, and outperforms several state-of-the-art open-set AL methods. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/HiLab-git/OpenPath%7D%7Bhttps://github.com/HiLab-git/OpenPath%7D">https://github.com/HiLab-git/OpenPath}{https://github.com/HiLab-git/OpenPath}</a>.. </p>
<blockquote>
<p>ç—…ç†å­¦å›¾åƒåˆ†ç±»åœ¨å‡†ç¡®çš„åŒ»å­¦è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­å‘æŒ¥è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¸ºæ­¤ä»»åŠ¡è®­ç»ƒé«˜æ€§èƒ½æ¨¡å‹é€šå¸¸éœ€è¦å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®çš„è·å–æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚ä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰é€šè¿‡è¿­ä»£é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œä»è€Œå‡å°‘äº†æ ‡æ³¨å·¥ä½œé‡ï¼Œä¸ºæ­¤æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ALæ–¹æ³•çš„è®¾è®¡æ˜¯åŸºäºå°é—­é›†åœºæ™¯çš„å‡è®¾ï¼Œå³æ‰€æœ‰æœªæ ‡æ³¨çš„å›¾åƒéƒ½å±äºç›®æ ‡ç±»åˆ«ã€‚åœ¨ç°å®ä¸–ç•Œä¸­çš„ä¸´åºŠç¯å¢ƒä¸­ï¼Œæœªæ ‡æ³¨æ± é€šå¸¸åŒ…å«å¤§é‡è¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰çš„æ•°æ®ï¼Œå¯¼è‡´ä¼ ç»ŸALæ–¹æ³•çš„æ ‡æ³¨æ•ˆç‡é™ä½ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç°æœ‰çš„ALæ–¹æ³•åœ¨ç¬¬ä¸€æ¬¡æŸ¥è¯¢æ—¶é‡‡ç”¨éšæœºé€‰æ‹©çš„æ–¹å¼ï¼Œè¿™åœ¨å¼€æ”¾é›†åœºæ™¯ä¸­é€ æˆäº†æ˜¾è‘—çš„æ ‡æ³¨æˆæœ¬æµªè´¹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†OpenPathï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒå¥½çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œç—…ç†å­¦å›¾åƒåˆ†ç±»çš„æ–°å‹å¼€æ”¾é›†ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ã€‚åœ¨ç¬¬ä¸€æ¬¡æŸ¥è¯¢ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“åˆç›®æ ‡å’Œç›¸å…³éç›®æ ‡ç±»åˆ«æç¤ºçš„ä»»åŠ¡ç‰¹å®šæç¤ºï¼Œä»¥æœ‰æ•ˆåœ°ä»æœªæ ‡æ³¨çš„æ± ä¸­é€‰æ‹©ç¬¦åˆåˆ†å¸ƒï¼ˆIDï¼‰å’Œå…·æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬ã€‚åœ¨éšåçš„æŸ¥è¯¢ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŒ…å«åŸºäºåŸå‹çš„IDå€™é€‰æ ·æœ¬é€‰æ‹©ï¼ˆPISï¼‰å’Œç†µå¼•å¯¼éšæœºé‡‡æ ·ï¼ˆEGSSï¼‰çš„å¤šæ ·ä¿¡æ¯IDé‡‡æ ·ï¼ˆDISï¼‰ï¼Œä»¥ç¡®ä¿æŸ¥è¯¢çš„çº¯å‡€åº¦å’Œä¿¡æ¯é‡ï¼Œé¿å…é€‰æ‹©OODæ ·æœ¬ã€‚åœ¨ä¸¤ä¸ªå…¬å…±ç—…ç†å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç”±äºæ‰€é€‰æ ·æœ¬çš„é«˜çº¯å‡€åº¦ï¼ŒOpenPathæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºå‡ ç§å…ˆè¿›çš„å¼€æ”¾é›†ALæ–¹æ³•ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/HiLab-git/OpenPath">https://github.com/HiLab-git/OpenPath</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15318v3">PDF</a> MICCAI 2025 early accept</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°å‹å¼€æ”¾é›†ä¸»åŠ¨å­¦ä¹ æ–¹æ³•OpenPathï¼Œç”¨äºç—…ç†å­¦å›¾åƒåˆ†ç±»ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»»åŠ¡ç‰¹å®šæç¤ºé€‰æ‹©åˆå§‹æ ·æœ¬ï¼Œå¹¶åœ¨åç»­æŸ¥è¯¢ä¸­ä½¿ç”¨å¤šæ ·ä¿¡æ¯é‡‡æ ·ç­–ç•¥ç¡®ä¿æ‰€é€‰æ ·æœ¬çš„çº¯åº¦å’Œä¿¡æ¯é‡ï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½å¹¶é¿å…é€‰æ‹©å¼‚å¸¸æ ·æœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…ç†å­¦å›¾åƒåˆ†ç±»åœ¨åŒ»ç–—è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œéœ€è¦å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†æ¥è®­ç»ƒé«˜æ€§èƒ½æ¨¡å‹ï¼Œä½†æ ‡æ³¨æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚</li>
<li>ä¸»åŠ¨å­¦ä¹ æ–¹æ³•é€šè¿‡è¿­ä»£é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œæœ‰åŠ©äºå‡å°‘æ ‡æ³¨å·¥ä½œé‡ã€‚</li>
<li>ç°æœ‰ä¸»åŠ¨å­¦ä¹ æ–¹æ³•å¤§å¤šå‡è®¾å°é—­é›†åœºæ™¯ï¼Œä½†åœ¨ç°å®ä¸´åºŠç¯å¢ƒä¸­ï¼Œæœªæ ‡æ³¨çš„æ± ä¸­å¸¸å¸¸åŒ…å«å¤§é‡å¼‚å¸¸æ•°æ®ï¼Œå¯¼è‡´æ ‡æ³¨æ•ˆç‡é™ä½ã€‚</li>
<li>OpenPathæ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä»»åŠ¡ç‰¹å®šæç¤ºå’Œå¤šæ ·ä¿¡æ¯é‡‡æ ·ç­–ç•¥ï¼Œæœ‰æ•ˆé€‰æ‹©ä¿¡æ¯é‡å¤§çš„æ ·æœ¬ï¼Œé¿å…é€‰æ‹©å¼‚å¸¸æ ·æœ¬ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒOpenPathæ–¹æ³•æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ä¼˜äºå…¶ä»–å…ˆè¿›çš„å¼€æ”¾é›†ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>OpenPathä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºæ›´é«˜æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æ ‡æ³¨èµ„æºï¼Œæ¨åŠ¨ç—…ç†å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡çš„è¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-310d447738977f0665830f224d12daae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cced2f395b08c2a3f503c662cec00168.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MrTrack-Register-Mamba-for-Needle-Tracking-with-Rapid-Reciprocating-Motion-during-Ultrasound-Guided-Aspiration-Biopsy"><a href="#MrTrack-Register-Mamba-for-Needle-Tracking-with-Rapid-Reciprocating-Motion-during-Ultrasound-Guided-Aspiration-Biopsy" class="headerlink" title="MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating   Motion during Ultrasound-Guided Aspiration Biopsy"></a>MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating   Motion during Ultrasound-Guided Aspiration Biopsy</h2><p><strong>Authors:Yuelin Zhang, Qingpeng Ding, Long Lei, Yongxuan Feng, Raymond Shing-Yan Tang, Shing Shin Cheng</strong></p>
<p>Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally invasive diagnostic procedure. However, an aspiration needle tracker addressing rapid reciprocating motion is still missing. MrTrack, an aspiration needle tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a Mamba-based register extractor to sequentially distill global context from each historical search map, storing these temporal cues in a register bank. The Mamba-based register retriever then retrieves temporal prompts from the register bank to provide external cues when current vision features are temporarily unusable due to rapid reciprocating motion and imaging degradation. A self-supervised register diversify loss is proposed to encourage feature diversity and dimension independence within the learned register, mitigating feature collapse. Comprehensive experiments conducted on both robotic and manual aspiration biopsy datasets demonstrate that MrTrack not only outperforms state-of-the-art trackers in accuracy and robustness but also achieves superior inference efficiency. Project page: <a target="_blank" rel="noopener" href="https://github.com/PieceZhang/MrTrack">https://github.com/PieceZhang/MrTrack</a> </p>
<blockquote>
<p>è¶…å£°å¼•å¯¼ä¸‹ç»†é’ˆç©¿åˆºï¼ˆFNAï¼‰æ´»æ£€æ˜¯ä¸€ç§å¸¸è§çš„å¾®åˆ›è¯Šæ–­ç¨‹åºã€‚ç„¶è€Œï¼Œç›®å‰å°šæœªæœ‰åº”å¯¹å¿«é€Ÿå¾€å¤è¿åŠ¨çš„ç©¿åˆºé’ˆè¿½è¸ªå™¨ã€‚é’ˆå¯¹è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†MrTrackè¿™ä¸€ä»¥éº»æ­¥ä¸ºåŸºç¡€ç™»è®°æœºåˆ¶çš„ç©¿åˆºé’ˆè¿½è¸ªå™¨ã€‚MrTrackåˆ©ç”¨åŸºäºéº»æ­¥çš„å¯„å­˜å™¨æå–å™¨ï¼Œä»æ¯ä¸ªå†å²æœç´¢åœ°å›¾ä¸­é¡ºåºæç‚¼å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå°†è¿™äº›æ—¶åºçº¿ç´¢å­˜å‚¨åœ¨å¯„å­˜å™¨é“¶è¡Œä¸­ã€‚å½“ç”±äºå¿«é€Ÿå¾€å¤è¿åŠ¨å’Œå›¾åƒé€€åŒ–å¯¼è‡´å½“å‰è§†è§‰ç‰¹å¾æš‚æ—¶æ— æ³•ä½¿ç”¨æ—¶ï¼ŒåŸºäºéº»æ­¥çš„å¯„å­˜å™¨æ£€ç´¢å™¨ä¼šä»å¯„å­˜å™¨é“¶è¡Œä¸­æå–æ—¶åºæç¤ºï¼Œæä¾›å¤–éƒ¨çº¿ç´¢ã€‚æå‡ºäº†ä¸€ç§è‡ªç›‘ç£å¯„å­˜å™¨å¤šæ ·åŒ–æŸå¤±ï¼Œä»¥é¼“åŠ±å­¦ä¹ åˆ°çš„å¯„å­˜å™¨å†…çš„ç‰¹å¾å¤šæ ·æ€§å’Œç»´åº¦ç‹¬ç«‹æ€§ï¼Œå‡è½»ç‰¹å¾å´©æºƒçš„é—®é¢˜ã€‚åœ¨æœºå™¨äººå’Œæ‰‹åŠ¨ç©¿åˆºæ´»æ£€æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒMrTrackä¸ä»…åœ¨å‡†ç¡®æ€§å’Œç¨³å¥æ€§æ–¹é¢ä¼˜äºæœ€æ–°è·Ÿè¸ªå™¨ï¼Œè€Œä¸”åœ¨æ¨ç†æ•ˆç‡æ–¹é¢ä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/PieceZhang/MrTrack">https://github.com/PieceZhang/MrTrack</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09450v2">PDF</a> Early Accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMrTrackçš„è¶…å£°å¼•å¯¼ä¸‹ç»†é’ˆç©¿åˆºï¼ˆFNAï¼‰æ´»æ£€é’ˆè¿½è¸ªæŠ€æœ¯ã€‚å®ƒé‡‡ç”¨åŸºäºmambaçš„å¯„å­˜å™¨æœºåˆ¶ï¼Œå¯å¤„ç†å¿«é€Ÿå¾€å¤è¿åŠ¨ã€‚MrTracké€šè¿‡ä»æ¯ä¸ªå†å²æœç´¢åœ°å›¾ä¸­è’¸é¦å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¹¶å°†è¿™äº›æ—¶é—´çº¿ç´¢å­˜å‚¨åœ¨å¯„å­˜å™¨é“¶è¡Œä¸­ï¼Œåˆ©ç”¨åŸºäºmambaçš„å¯„å­˜å™¨æå–å™¨æ¥æ£€ç´¢å¤–éƒ¨çº¿ç´¢ï¼Œä»è€Œåœ¨å½“å‰çš„è§†è§‰ç‰¹å¾å› å¿«é€Ÿå¾€å¤è¿åŠ¨å’Œå›¾åƒé€€åŒ–è€Œæš‚æ—¶æ— æ³•ä½¿ç”¨çš„æƒ…å†µä¸‹æä¾›è¾…åŠ©ã€‚åŒæ—¶æå‡ºäº†ä¸€ç§è‡ªç›‘ç£å¯„å­˜å™¨å¤šæ ·åŒ–æŸå¤±ï¼Œé¼“åŠ±å­¦ä¹ å¯„å­˜å™¨å†…çš„ç‰¹å¾å¤šæ ·æ€§å’Œç»´åº¦ç‹¬ç«‹æ€§ï¼Œå‡è½»ç‰¹å¾å´©æºƒé—®é¢˜ã€‚å®éªŒè¯æ˜ï¼ŒMrTrackåœ¨æœºå™¨äººå’Œæ‰‹åŠ¨ç©¿åˆºæ´»æ£€æ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§å‡ä¼˜äºæœ€æ–°è·Ÿè¸ªæŠ€æœ¯ï¼Œå¹¶å®ç°äº†è¾ƒé«˜çš„æ¨ç†æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MrTrackæ˜¯ä¸€ç§ç”¨äºè¶…å£°å¼•å¯¼ä¸‹ç»†é’ˆç©¿åˆºæ´»æ£€é’ˆè¿½è¸ªçš„æŠ€æœ¯ï¼Œæ—¨åœ¨å¤„ç†å¿«é€Ÿå¾€å¤è¿åŠ¨ã€‚</li>
<li>MrTracké‡‡ç”¨åŸºäºmambaçš„å¯„å­˜å™¨æœºåˆ¶ï¼Œé€šè¿‡è’¸é¦å†å²æœç´¢åœ°å›¾ä¸­çš„å…¨å±€ä¸Šä¸‹æ–‡æ¥è¿›è¡Œå·¥ä½œã€‚</li>
<li>æŠ€æœ¯åˆ©ç”¨å¯„å­˜å™¨æå–å™¨åœ¨è§†è§‰ç‰¹å¾å› å¿«é€Ÿè¿åŠ¨å’Œå›¾åƒé€€åŒ–è€Œæš‚æ—¶ä¸å¯ç”¨çš„æƒ…å†µä¸‹æä¾›å¤–éƒ¨çº¿ç´¢ã€‚</li>
<li>æå‡ºäº†è‡ªç›‘ç£å¯„å­˜å™¨å¤šæ ·åŒ–æŸå¤±ï¼Œä»¥ä¿ƒè¿›ç‰¹å¾å¤šæ ·æ€§å’Œç»´åº¦ç‹¬ç«‹æ€§ã€‚</li>
<li>MrTracké€šè¿‡å‡å°‘ç‰¹å¾å´©æºƒé—®é¢˜ï¼Œæé«˜äº†è·Ÿè¸ªæŠ€æœ¯çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒMrTrackåœ¨æœºå™¨äººå’Œæ‰‹åŠ¨ç©¿åˆºæ´»æ£€æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰è·Ÿè¸ªæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af27863b7bf3e5c78a03bc6d627f59c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93fc93d663638d0229cdc7d94db89bd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86113158fca5a69439912ec23009c4b7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Accelerate-3D-Object-Detection-Models-via-Zero-Shot-Attention-Key-Pruning"><a href="#Accelerate-3D-Object-Detection-Models-via-Zero-Shot-Attention-Key-Pruning" class="headerlink" title="Accelerate 3D Object Detection Models via Zero-Shot Attention Key   Pruning"></a>Accelerate 3D Object Detection Models via Zero-Shot Attention Key   Pruning</h2><p><strong>Authors:Lizhen Xu, Xiuxiu Bai, Xiaojun Jia, Jianwu Fang, Shanmin Pang</strong></p>
<p>Query-based methods with dense features have demonstrated remarkable success in 3D object detection tasks. However, the computational demands of these models, particularly with large image sizes and multiple transformer layers, pose significant challenges for efficient running on edge devices. Existing pruning and distillation methods either need retraining or are designed for ViT models, which are hard to migrate to 3D detectors. To address this issue, we propose a zero-shot runtime pruning method for transformer decoders in 3D object detection models. The method, termed tgGBC (trim keys gradually Guided By Classification scores), systematically trims keys in transformer modules based on their importance. We expand the classification score to multiply it with the attention map to get the importance score of each key and then prune certain keys after each transformer layer according to their importance scores. Our method achieves a 1.99x speedup in the transformer decoder of the latest ToC3D model, with only a minimal performance loss of less than 1%. Interestingly, for certain models, our method even enhances their performance. Moreover, we deploy 3D detectors with tgGBC on an edge device, further validating the effectiveness of our method. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/iseri27/tg_gbc">https://github.com/iseri27/tg_gbc</a>. </p>
<blockquote>
<p>åŸºäºæŸ¥è¯¢çš„æ–¹æ³•å’Œå¯†é›†ç‰¹å¾åœ¨3Dç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„è®¡ç®—éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§å›¾åƒå°ºå¯¸å’Œå¤šä¸ªè½¬æ¢å™¨å±‚æ—¶ï¼Œå¯¹äºåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šé«˜æ•ˆè¿è¡Œæ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„ä¿®å‰ªå’Œè’¸é¦æ–¹æ³•éœ€è¦é‡æ–°è®­ç»ƒï¼Œæˆ–è€…é’ˆå¯¹ViTæ¨¡å‹è®¾è®¡ï¼Œå¾ˆéš¾è¿ç§»åˆ°3Dæ£€æµ‹å™¨ä¸Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äº3Dç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸­è½¬æ¢å™¨è§£ç å™¨çš„é›¶æ‹æ‘„è¿è¡Œæ—¶ä¿®å‰ªæ–¹æ³•ã€‚è¯¥æ–¹æ³•è¢«ç§°ä¸ºtgGBCï¼ˆåˆ†ç±»åˆ†æ•°é€æ­¥å¼•å¯¼ä¿®å‰ªï¼‰ï¼Œå®ƒæ ¹æ®é‡è¦æ€§ç³»ç»Ÿåœ°ä¿®å‰ªè½¬æ¢å™¨æ¨¡å—ä¸­çš„é”®ã€‚æˆ‘ä»¬å°†åˆ†ç±»åˆ†æ•°æ‰©å±•åˆ°ä¸æ³¨æ„åŠ›å›¾ç›¸ä¹˜ï¼Œä»¥è·å¾—æ¯ä¸ªé”®çš„é‡è¦æ€§åˆ†æ•°ï¼Œç„¶åæ ¹æ®æ¯ä¸ªé”®çš„é‡è¦æ€§åˆ†æ•°ï¼Œåœ¨æ¯å±‚è½¬æ¢å™¨ä¹‹åå¯¹å…¶è¿›è¡Œä¿®å‰ªã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ€æ–°çš„ToC3Dæ¨¡å‹çš„è½¬æ¢å™¨è§£ç å™¨ä¸­å®ç°äº†1.99å€çš„åŠ é€Ÿï¼Œæ€§èƒ½æŸå¤±ä»…ä½äº1%ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå¯¹äºæŸäº›æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”šè‡³æé«˜äº†å…¶æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¸¦æœ‰tgGBCçš„3Dæ£€æµ‹å™¨ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨&lt;<a target="_blank" rel="noopener" href="https://github.com/iseri2">https://github.com/iseri2</a>_ çœç•¥å¤„æ˜¯åŸæ–‡ä¸­çš„å ä½ç¬¦ &gt;ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08101v3">PDF</a> Accepted by ICCV2025. The code can be found at   <a target="_blank" rel="noopener" href="https://github.com/iseri27/tg_gbc">https://github.com/iseri27/tg_gbc</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºtgGBCçš„é›¶è¿è¡Œæ—¶ä¿®å‰ªæ–¹æ³•ï¼Œç”¨äºåŸºäºæŸ¥è¯¢çš„ä¸‰ç»´ç›®æ ‡æ£€æµ‹æ¨¡å‹çš„è½¬æ¢å™¨è§£ç å™¨ã€‚è¯¥æ–¹æ³•æ ¹æ®å…³é”®çš„é‡è¦æ€§é€æ­¥ä¿®å‰ªè½¬æ¢æ¨¡å—ä¸­çš„é”®ï¼Œé€šè¿‡åˆ†ç±»åˆ†æ•°ä¸æ³¨æ„åŠ›å›¾çš„ä¹˜ç§¯æ¥ç¡®å®šæ¯ä¸ªé”®çš„é‡è¦æ€§åˆ†æ•°ï¼Œç„¶åé€å±‚ä¿®å‰ªé‡è¦åˆ†æ•°è¾ƒä½çš„é”®ã€‚è¯¥æ–¹æ³•åœ¨æœ€æ–°ToC3Dæ¨¡å‹çš„è½¬æ¢å™¨è§£ç å™¨ä¸­å®ç°äº†1.99å€çš„åŠ é€Ÿï¼Œæ€§èƒ½æŸå¤±ä»…ä¸ºä¸åˆ°1%ï¼Œå¹¶ä¸”å¯ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚åŒæ—¶å‘ç°å¯¹ç‰¹å®šæ¨¡å‹ç”šè‡³èƒ½æå‡æ€§èƒ½ã€‚æœ‰å…³ä»£ç å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³é”®è§è§£ï¼š</p>
<ul>
<li>æŸ¥è¯¢æ–¹æ³•åœ¨ä¸‰ç»´ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ç„¶è€Œè®¡ç®—éœ€æ±‚éå¸¸é«˜ï¼Œç‰¹åˆ«æ˜¯å¤§å‹å›¾åƒå’Œå¤šå±‚è½¬æ¢å™¨ï¼Œè¿™å¯¹äºè¾¹ç¼˜è®¾å¤‡æ¥è¯´æ˜¯ä¸ªæŒ‘æˆ˜ã€‚å› æ­¤éœ€è¦å¯¹ç°æœ‰çš„æ–¹æ³•å¦‚ä¿®å‰ªå’Œè’¸é¦è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºtgGBCçš„é›¶è¿è¡Œæ—¶ä¿®å‰ªæ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡è®¡ç®—é”®çš„é‡è¦æ€§å¾—åˆ†æ¥è¿›è¡Œä¿®å‰ªï¼Œä¸ä»…åŠ é€Ÿäº†è½¬æ¢å™¨çš„è¿è¡Œé€Ÿåº¦ï¼Œè€Œä¸”å¯¹æ€§èƒ½å½±å“å¾ˆå°ã€‚è¿™æ˜¯é’ˆå¯¹åŸºäºæŸ¥è¯¢çš„ä¸‰ç»´æ£€æµ‹æ¨¡å‹çš„è½¬æ¢å™¨è§£ç å™¨çš„ä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>tgGBCæ–¹æ³•é€šè¿‡ç»“åˆåˆ†ç±»åˆ†æ•°å’Œæ³¨æ„åŠ›å›¾æ¥ç¡®å®šæ¯ä¸ªé”®çš„é‡è¦æ€§å¾—åˆ†ï¼Œä½¿å¾—å…³é”®çš„é‡è¦ä¿¡æ¯å¾—åˆ°ä¿ç•™è€Œä¸ä¼šè¢«å¿½ç•¥æ‰ï¼Œä½¿å¾—æ¨¡å‹è¿è¡Œé€Ÿåº¦å¾—åˆ°æå‡åŒæ—¶ä¿è¯äº†æ£€æµ‹ç²¾åº¦ã€‚åŒæ—¶è¯¥æ–¹æ³•çš„å¯æ‰©å±•æ€§ä½¿å¾—å®ƒå¯ä»¥é€‚åº”å¤šç§æ¨¡å‹æ¶æ„å’Œé…ç½®ã€‚åŒæ—¶èƒ½å¤Ÿåœ¨æ¯ä¸€å±‚ä¸­å¯¹ç‰¹å®šçš„é”®è¿›è¡Œä¿®å‰ªï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚è¿™ç§ä¿®å‰ªç­–ç•¥æ˜¯åŸºäºåˆ†ç±»åˆ†æ•°çš„æ‰©å±•ï¼Œèƒ½å¤Ÿå‡†ç¡®åœ°è¯†åˆ«å‡ºå“ªäº›é”®å¯¹äºæ¨¡å‹æ€§èƒ½æ›´ä¸ºé‡è¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d60d3b5f82d1877b51327c97d6a6d361.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcd960e8fe7aab4b760823fb766eb29e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6e8d1be338c3e06befe5e87361353df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff785287fc071e6f8215097c93de42d9.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Advancing-Textual-Prompt-Learning-with-Anchored-Attributes"><a href="#Advancing-Textual-Prompt-Learning-with-Anchored-Attributes" class="headerlink" title="Advancing Textual Prompt Learning with Anchored Attributes"></a>Advancing Textual Prompt Learning with Anchored Attributes</h2><p><strong>Authors:Zheng Li, Yibing Song, Ming-Ming Cheng, Xiang Li, Jian Yang</strong></p>
<p>Textual-based prompt learning methods primarily employ multiple learnable soft prompts and hard class tokens in a cascading manner as text inputs, aiming to align image and text (category) spaces for downstream tasks. However, current training is restricted to aligning images with predefined known categories and cannot be associated with unknown categories. In this work, we propose utilizing universal attributes as a bridge to enhance the alignment between images and unknown categories. Specifically, we introduce an Attribute-anchored Textual Prompt learning method for vision-language models, named ATPrompt. This approach expands the learning space of soft prompts from the original one-dimensional category level into the multi-dimensional attribute level by incorporating multiple attribute tokens into the learnable soft prompts. Through this modification, we transform the text prompt from a category-centric form to an attribute-category hybrid form. Additionally, we introduce a straightforward differentiable attribute search method to identify representative and suitable attributes for downstream tasks. As an easy-to-use plug-in technique, ATPrompt can seamlessly replace the existing basic prompt format in textual-based methods, providing general improvements at a negligible computational cost. Extensive experiments across 11 datasets validate the effectiveness of our method. </p>
<blockquote>
<p>åŸºäºæ–‡æœ¬çš„æç¤ºå­¦ä¹ æ–¹æ³•ä¸»è¦åˆ©ç”¨å¯å­¦ä¹ çš„å¤šä¸ªè½¯æç¤ºå’Œç¡¬ç±»åˆ«ä»¤ç‰Œä»¥çº§è”æ–¹å¼ä½œä¸ºæ–‡æœ¬è¾“å…¥ï¼Œæ—¨åœ¨å¯¹é½å›¾åƒå’Œæ–‡æœ¬ï¼ˆç±»åˆ«ï¼‰ç©ºé—´ï¼Œä»¥è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰è®­ç»ƒä»…é™äºå¯¹é½å›¾åƒä¸é¢„å®šä¹‰çš„å·²çŸ¥ç±»åˆ«ï¼Œæ— æ³•ä¸æœªçŸ¥ç±»åˆ«ç›¸å…³è”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨é€šç”¨å±æ€§ä½œä¸ºæ¡¥æ¢ï¼Œä»¥å¢å¼ºå›¾åƒä¸æœªçŸ¥ç±»åˆ«ä¹‹é—´çš„å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹å¼•å…¥äº†ä¸€ç§åä¸ºATPromptçš„å±æ€§é”šå®šæ–‡æœ¬æç¤ºå­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¤šä¸ªå±æ€§ä»¤ç‰Œçº³å…¥å¯å­¦ä¹ çš„è½¯æç¤ºä¸­ï¼Œå°†è½¯æç¤ºçš„å­¦ä¹ ç©ºé—´ä»åŸå§‹çš„ä¸€ç»´ç±»åˆ«å±‚é¢æ‰©å±•åˆ°å¤šç»´å±æ€§å±‚é¢ã€‚é€šè¿‡è¿™ä¸€æ”¹è¿›ï¼Œæˆ‘ä»¬å°†æ–‡æœ¬æç¤ºä»ä»¥ç±»åˆ«ä¸ºä¸­å¿ƒçš„å½¢å¼è½¬å˜ä¸ºå±æ€§-ç±»åˆ«æ··åˆå½¢å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ç®€å•çš„å¯åŒºåˆ†å±æ€§æœç´¢æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«ä¸‹æ¸¸ä»»åŠ¡çš„ä»£è¡¨æ€§å’Œåˆé€‚å±æ€§ã€‚ä½œä¸ºæ˜“äºä½¿ç”¨çš„æ’ä»¶æŠ€æœ¯ï¼ŒATPromptå¯ä»¥æ— ç¼æ›¿æ¢åŸºäºæ–‡æœ¬æ–¹æ³•ä¸­çš„åŸºæœ¬æç¤ºæ ¼å¼ï¼Œä»¥å¾®é‡çš„è®¡ç®—æˆæœ¬æä¾›ä¸€èˆ¬æ€§çš„æ”¹è¿›ã€‚åœ¨11ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09442v2">PDF</a> ICCV 2025. Project Page: <a target="_blank" rel="noopener" href="https://zhengli97.github.io/ATPrompt/">https://zhengli97.github.io/ATPrompt/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå±æ€§é”šå®šçš„æ–‡æœ¬æç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆATPromptï¼‰ï¼Œç”¨äºå¢å¼ºå›¾åƒä¸æœªçŸ¥ç±»åˆ«ä¹‹é—´çš„å¯¹é½ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å¤šå±æ€§ä»¤ç‰Œæ¥æ‰©å±•å­¦ä¹ ç©ºé—´ï¼Œå°†åŸæœ¬ä¸€ç»´çš„ç±»åˆ«çº§åˆ«æ‰©å±•åˆ°å¤šç»´çš„å±æ€§çº§åˆ«ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æ˜“ç”¨çš„å¯å¾®å±æ€§æœç´¢æ–¹æ³•ï¼Œç”¨äºä¸ºä¸‹æ¸¸ä»»åŠ¡é€‰æ‹©ä»£è¡¨æ€§å±æ€§ã€‚ATPromptä½œä¸ºä¸€ç§æ˜“äºä½¿ç”¨çš„æ’ä»¶æŠ€æœ¯ï¼Œå¯ä»¥æ— ç¼æ›¿æ¢ç°æœ‰æ–‡æœ¬æç¤ºæ–¹æ³•ä¸­çš„åŸºæœ¬æ ¼å¼ï¼Œä»¥å¾®å°çš„è®¡ç®—æˆæœ¬æä¾›é€šç”¨çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå±æ€§é”šå®šçš„æ–‡æœ¬æç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆATPromptï¼‰ï¼Œæ—¨åœ¨å¢å¼ºå›¾åƒä¸æœªçŸ¥ç±»åˆ«ä¹‹é—´çš„å¯¹é½ã€‚</li>
<li>ATPrompté€šè¿‡å°†å­¦ä¹ ç©ºé—´æ‰©å±•åˆ°å¤šç»´å±æ€§çº§åˆ«ï¼Œæé«˜äº†æ–‡æœ¬æç¤ºå­¦ä¹ çš„èƒ½åŠ›ã€‚</li>
<li>ATPromptå¼•å…¥äº†å¯å¾®åˆ†çš„å±æ€§æœç´¢æ–¹æ³•ï¼Œä»¥è¯†åˆ«é€‚åˆä¸‹æ¸¸ä»»åŠ¡çš„ä»£è¡¨æ€§å±æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥ä½œä¸ºæ’ä»¶æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æ–‡æœ¬æç¤ºæ–¹æ³•ä¸­ã€‚</li>
<li>ATPromptæé«˜äº†æ¨¡å‹åœ¨æœªçŸ¥ç±»åˆ«ä¸Šçš„æ€§èƒ½ï¼Œæ‰©å¤§äº†æ¨¡å‹çš„é€‚ç”¨èŒƒå›´ã€‚</li>
<li>é€šè¿‡åœ¨11ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒéªŒè¯äº†ATPromptçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eadd62d913df9b383e6b96d1e1165797.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee7b6a7a45c40ee7017bfc7a2365f7b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4e9382bc2870305d7c81d8b7d2ead5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-832074a8042eb9ad649239ab4da874f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad93f77b5a9881b0d5e71313c056e0b4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Exploring-Text-Guided-Single-Image-Editing-for-Remote-Sensing-Images"><a href="#Exploring-Text-Guided-Single-Image-Editing-for-Remote-Sensing-Images" class="headerlink" title="Exploring Text-Guided Single Image Editing for Remote Sensing Images"></a>Exploring Text-Guided Single Image Editing for Remote Sensing Images</h2><p><strong>Authors:Fangzhou Han, Lingyu Si, Zhizhuo Jiang, Hongwei Dong, Lamei Zhang, Yu Liu, Hao Chen, Bo Du</strong></p>
<p>Artificial intelligence generative content (AIGC) has significantly impacted image generation in the field of remote sensing. However, the equally important area of remote sensing image (RSI) editing has not received sufficient attention. Deep learning based editing methods generally involve two sequential stages: generation and editing. For natural images, these stages primarily rely on generative backbones pre-trained on large-scale benchmark datasets and text guidance facilitated by vision-language models (VLMs). However, it become less viable for RSIs: First, existing generative RSI benchmark datasets do not fully capture the diversity of RSIs, and is often inadequate for universal editing tasks. Second, the single text semantic corresponds to multiple image semantics, leading to the introduction of incorrect semantics. To solve above problems, this paper proposes a text-guided RSI editing method and can be trained using only a single image. A multi-scale training approach is adopted to preserve consistency without the need for training on extensive benchmarks, while leveraging RSI pre-trained VLMs and prompt ensembling (PE) to ensure accuracy and controllability. Experimental results on multiple RSI editing tasks show that the proposed method offers significant advantages in both CLIP scores and subjective evaluations compared to existing methods. Additionally, we explore the ability of the edited RSIs to support disaster assessment tasks in order to validate their practicality. Codes will be released at <a target="_blank" rel="noopener" href="https://github.com/HIT-PhilipHan/remote_sensing_image_editing">https://github.com/HIT-PhilipHan/remote_sensing_image_editing</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰å¯¹é¥æ„Ÿé¢†åŸŸå›¾åƒç”Ÿæˆäº§ç”Ÿäº†é‡å¤§å½±å“ã€‚ç„¶è€Œï¼ŒåŒæ ·é‡è¦çš„é¥æ„Ÿå›¾åƒï¼ˆRSIï¼‰ç¼–è¾‘å´æ²¡æœ‰å¾—åˆ°è¶³å¤Ÿé‡è§†ã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„ç¼–è¾‘æ–¹æ³•é€šå¸¸æ¶‰åŠä¸¤ä¸ªé˜¶æ®µï¼šç”Ÿæˆå’Œç¼–è¾‘ã€‚å¯¹äºè‡ªç„¶å›¾åƒï¼Œè¿™ä¸¤ä¸ªé˜¶æ®µä¸»è¦ä¾èµ–äºåœ¨å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ä¸Šé¢„å…ˆè®­ç»ƒçš„ç”Ÿæˆä¸»å¹²å’Œç”±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¿ƒè¿›çš„æ–‡æœ¬æŒ‡å¯¼ã€‚ä½†å¯¹äºé¥æ„Ÿå›¾åƒï¼ˆRSIï¼‰æ¥è¯´ï¼Œè¿™ç§åšæ³•ä¸å¤ªå¯è¡Œï¼šé¦–å…ˆï¼Œç°æœ‰çš„ç”Ÿæˆé¥æ„Ÿå›¾åƒåŸºå‡†æ•°æ®é›†æ²¡æœ‰å®Œå…¨æ•æ‰åˆ°é¥æ„Ÿå›¾åƒçš„å¤šæ ·æ€§ï¼Œé€šå¸¸ä¸è¶³ä»¥åº”å¯¹é€šç”¨ç¼–è¾‘ä»»åŠ¡ã€‚å…¶æ¬¡ï¼Œå•ä¸€æ–‡æœ¬è¯­ä¹‰å¯¹åº”äºå¤šç§å›¾åƒè¯­ä¹‰ï¼Œè¿™å¼•å…¥äº†ä¸æ­£ç¡®çš„è¯­ä¹‰ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–‡æœ¬å¼•å¯¼çš„é¥æ„Ÿå›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åªéœ€å•å¼ å›¾åƒå³å¯è¿›è¡Œè®­ç»ƒã€‚é‡‡ç”¨å¤šå°ºåº¦è®­ç»ƒæ–¹æ³•ï¼Œåœ¨æ— éœ€å¤§è§„æ¨¡åŸºå‡†æ•°æ®è®­ç»ƒçš„æƒ…å†µä¸‹ä¿æŒä¸€è‡´æ€§ï¼ŒåŒæ—¶åˆ©ç”¨é¥æ„Ÿå›¾åƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œæç¤ºé›†æˆï¼ˆPEï¼‰ç¡®ä¿å‡†ç¡®æ€§å’Œå¯æ§æ€§ã€‚åœ¨å¤šä¸ªé¥æ„Ÿå›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€ææ–¹æ³•åœ¨CLIPå¾—åˆ†å’Œä¸»è§‚è¯„ä¼°æ–¹é¢éƒ½è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç¼–è¾‘åçš„é¥æ„Ÿå›¾åƒæ”¯æŒç¾å®³è¯„ä¼°ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä»¥éªŒè¯å…¶å®ç”¨æ€§ã€‚ç›¸å…³ä»£ç å°†å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/HIT-PhilipHan/remote_sensing_image_editing%E3%80%82">https://github.com/HIT-PhilipHan/remote_sensing_image_editingã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.05769v4">PDF</a> 17 pages, 18 figures, Accepted by IEEE Journal of Selected Topics in   Applied Earth Observations and Remote Sensing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰åœ¨é¥æ„Ÿå›¾åƒç¼–è¾‘é¢†åŸŸçš„å½±å“ã€‚é’ˆå¯¹é¥æ„Ÿå›¾åƒç¼–è¾‘å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚æ•°æ®é›†å¤šæ ·æ€§ä¸è¶³å’Œæ–‡æœ¬è¯­ä¹‰ä¸å›¾åƒè¯­ä¹‰ä¸åŒ¹é…ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–‡æœ¬å¼•å¯¼çš„é¥æ„Ÿå›¾åƒç¼–è¾‘æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å•å›¾åƒè®­ç»ƒï¼Œåˆ©ç”¨å¤šå°ºåº¦è®­ç»ƒä¿è¯ä¸€è‡´æ€§ï¼Œå€ŸåŠ©é¥æ„Ÿå›¾åƒé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œæç¤ºé›†æˆï¼ˆPEï¼‰ç¡®ä¿å‡†ç¡®æ€§å’Œå¯æ§æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé¥æ„Ÿå›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶æ”¯æŒç¾å®³è¯„ä¼°ä»»åŠ¡çš„å®ç”¨æ€§éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIGCå¯¹é¥æ„Ÿå›¾åƒç”Ÿæˆäº§ç”Ÿé‡å¤§å½±å“ï¼Œä½†é¥æ„Ÿå›¾åƒç¼–è¾‘é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†å…³æ³¨ã€‚</li>
<li>ç°æœ‰çš„é¥æ„Ÿå›¾åƒç”Ÿæˆæ•°æ®é›†ç¼ºä¹å¤šæ ·æ€§ï¼Œæ— æ³•æ»¡è¶³é€šç”¨ç¼–è¾‘ä»»åŠ¡çš„éœ€æ±‚ã€‚</li>
<li>æ–‡æœ¬è¯­ä¹‰ä¸å›¾åƒè¯­ä¹‰çš„ä¸åŒ¹é…é—®é¢˜åœ¨é¥æ„Ÿå›¾åƒç¼–è¾‘ä¸­å°¤ä¸ºçªå‡ºã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–‡æœ¬å¼•å¯¼çš„é¥æ„Ÿå›¾åƒç¼–è¾‘æ–¹æ³•ï¼Œé‡‡ç”¨å•å›¾åƒè®­ç»ƒï¼Œæ— éœ€å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ã€‚</li>
<li>å¤šå°ºåº¦è®­ç»ƒä¿è¯äº†ç¼–è¾‘çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶å€ŸåŠ©é¥æ„Ÿå›¾åƒé¢„è®­ç»ƒçš„VLMså’Œæç¤ºé›†æˆï¼ˆPEï¼‰æé«˜å‡†ç¡®æ€§å’Œå¯æ§æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé¥æ„Ÿå›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒCLIPåˆ†æ•°å’Œä¸»è§‚è¯„ä»·å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.05769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e081cb3dd8b4d27d62effa5c5548ca79.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-195acf4c01f6d4ef1fd19d5edbdc87c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f9bdce2736b02628fc315c28d508177.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c7c8175efd49103d8045087adb02a423.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dec94b5dd70de3cc5518fcd72b55cafd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3ce06d0894d976c8e2fc6477c56baa8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="D-2-ST-Adapter-Disentangled-and-Deformable-Spatio-Temporal-Adapter-for-Few-shot-Action-Recognition"><a href="#D-2-ST-Adapter-Disentangled-and-Deformable-Spatio-Temporal-Adapter-for-Few-shot-Action-Recognition" class="headerlink" title="D$^2$ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for   Few-shot Action Recognition"></a>D$^2$ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for   Few-shot Action Recognition</h2><p><strong>Authors:Wenjie Pei, Qizhong Tan, Guangming Lu, Jiandong Tian, Jun Yu</strong></p>
<p>Adapting pre-trained image models to video modality has proven to be an effective strategy for robust few-shot action recognition. In this work, we explore the potential of adapter tuning in image-to-video model adaptation and propose a novel video adapter tuning framework, called Disentangled-and-Deformable Spatio-Temporal Adapter (D$^2$ST-Adapter). It features a lightweight design, low adaptation overhead and powerful spatio-temporal feature adaptation capabilities. D$^2$ST-Adapter is structured with an internal dual-pathway architecture that enables built-in disentangled encoding of spatial and temporal features within the adapter, seamlessly integrating into the single-stream feature learning framework of pre-trained image models. In particular, we develop an efficient yet effective implementation of the D$^2$ST-Adapter, incorporating the specially devised anisotropic Deformable Spatio-Temporal Attention as its pivotal operation. This mechanism can be individually tailored for two pathways with anisotropic sampling densities along the spatial and temporal domains in 3D spatio-temporal space, enabling disentangled encoding of spatial and temporal features while maintaining a lightweight design. Extensive experiments by instantiating our method on both pre-trained ResNet and ViT demonstrate the superiority of our method over state-of-the-art methods. Our method is particularly well-suited to challenging scenarios where temporal dynamics are critical for action recognition. Code is available at <a target="_blank" rel="noopener" href="https://github.com/qizhongtan/D2ST-Adapter">https://github.com/qizhongtan/D2ST-Adapter</a>. </p>
<blockquote>
<p>å°†é¢„è®­ç»ƒå›¾åƒæ¨¡å‹é€‚é…è‡³è§†é¢‘æ¨¡å¼å·²è¢«è¯æ˜æ˜¯å®ç°ç¨³å¥çš„å°‘é‡åŠ¨ä½œè¯†åˆ«æ•°æ®çš„æœ‰æ•ˆç­–ç•¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†é€‚é…å™¨è°ƒæ•´åœ¨å›¾åƒåˆ°è§†é¢‘æ¨¡å‹é€‚é…ä¸­çš„æ½œåŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹çš„è§†é¢‘é€‚é…å™¨è°ƒæ•´æ¡†æ¶ï¼Œç§°ä¸ºè§£çº ç¼ å’Œå¯å˜å½¢çš„æ—¶ç©ºé€‚é…å™¨ï¼ˆD^2ST-Adapterï¼‰ã€‚å®ƒå…·æœ‰è½»é‡çº§è®¾è®¡ã€ä½é€‚åº”å¼€é”€å’Œå¼ºå¤§çš„æ—¶ç©ºç‰¹å¾é€‚åº”åŠ›ã€‚D^2ST-Adapteré‡‡ç”¨å†…éƒ¨åŒè·¯å¾„æ¶æ„ï¼Œä½¿é€‚é…å™¨å†…éƒ¨çš„ç©ºé—´å’Œæ—¶é—´ç‰¹å¾å¾—ä»¥è§£çº ç¼ ç¼–ç ï¼Œæ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒå›¾åƒæ¨¡å‹çš„å•æµç‰¹å¾å­¦ä¹ æ¡†æ¶ä¸­ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„D^2ST-Adapterå®ç°ï¼Œå…¶ä¸­èå…¥äº†ä¸“é—¨è®¾è®¡çš„å„å‘å¼‚æ€§å¯å˜æ—¶ç©ºæ³¨æ„åŠ›ä½œä¸ºå…¶å…³é”®æ“ä½œã€‚è¯¥æœºåˆ¶å¯ä»¥é’ˆå¯¹ä¸¤æ¡è·¯å¾„è¿›è¡Œä¸ªæ€§åŒ–è®¾ç½®ï¼Œåœ¨3Dæ—¶ç©ºç©ºé—´ä¸­çš„ç©ºé—´å’Œæ—¶é—´åŸŸä¸Šå…·æœ‰å„å‘å¼‚æ€§é‡‡æ ·å¯†åº¦ï¼Œä»è€Œå®ç°ç©ºé—´å’Œæ—¶é—´ç‰¹å¾çš„è§£çº ç¼ ç¼–ç ï¼ŒåŒæ—¶ä¿æŒè½»é‡çº§è®¾è®¡ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„æ–¹æ³•å’Œé¢„è®­ç»ƒçš„ResNetå’ŒViTä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºæ—¶é—´åŠ¨æ€å¯¹åŠ¨ä½œè¯†åˆ«è‡³å…³é‡è¦çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/qizhongtan/D2ST-Adapter%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/qizhongtan/D2ST-Adapteræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.01431v4">PDF</a> Accepted by ICCV2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬è®ºæ–‡æ¢ç´¢äº†é€‚é…å™¨è°ƒæ•´åœ¨å›¾åƒåˆ°è§†é¢‘æ¨¡å‹é€‚é…ä¸­çš„æ½œåŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§†é¢‘é€‚é…å™¨è°ƒæ•´æ¡†æ¶â€”â€”ç§°ä¸ºè§£çº ç¼ ä¸å¯å˜å½¢æ—¶ç©ºé€‚é…å™¨ï¼ˆD^2ST-Adapterï¼‰ã€‚å®ƒå…·æœ‰è½»é‡çº§è®¾è®¡ã€ä½é€‚åº”å¼€é”€å’Œå¼ºå¤§çš„æ—¶ç©ºç‰¹å¾é€‚åº”åŠ›ã€‚D^2ST-Adapteré‡‡ç”¨å†…éƒ¨åŒè·¯å¾„æ¶æ„ï¼Œå®ç°äº†é€‚é…å™¨å†…æ—¶ç©ºç‰¹å¾çš„è§£çº ç¼ ç¼–ç ï¼Œæ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒå›¾åƒæ¨¡å‹çš„å•æµç‰¹å¾å­¦ä¹ æ¡†æ¶ä¸­ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„D^2ST-Adapterå®ç°ï¼Œå°†å…¶æ ¸å¿ƒæ“ä½œæ•´åˆä¸ºä¸“é—¨è®¾è®¡çš„å¯å˜å½¢æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ã€‚è¯¥æœºåˆ¶å¯ä»¥åœ¨ç©ºé—´å’Œæ—¶é—´åŸŸä¸Šå®ç°ä¸ªæ€§åŒ–çš„é‡‡æ ·å¯†åº¦è°ƒæ•´ï¼Œä»¥æ”¯æŒä¸¤æ¡è·¯å¾„çš„ç‹¬ç«‹å®šåˆ¶ï¼Œä»è€Œå®ç°æ—¶ç©ºç‰¹å¾çš„è§£çº ç¼ ç¼–ç å¹¶ä¿æŒè½»é‡çº§è®¾è®¡ã€‚åœ¨é¢„è®­ç»ƒçš„ResNetå’ŒViTä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è¯¥æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºåŠ¨ä½œè¯†åˆ«ä¸­æ—¶é—´åŠ¨æ€è‡³å…³é‡è¦çš„æŒ‘æˆ˜æ€§åœºæ™¯ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/qizhongtan/D2ST-Adapter%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/qizhongtan/D2ST-Adapterè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„é’ˆå¯¹å›¾åƒåˆ°è§†é¢‘æ¨¡å‹çš„é€‚é…ç­–ç•¥â€”â€”è§£çº ç¼ ä¸å¯å˜å½¢æ—¶ç©ºé€‚é…å™¨ï¼ˆD^2ST-Adapterï¼‰ã€‚</li>
<li>é‡‡ç”¨è½»é‡çº§è®¾è®¡ä»¥è¿›è¡Œé€‚é…å™¨è°ƒæ•´å¹¶å…·æœ‰æ—¶ç©ºç‰¹å¾è§£çº ç¼ çš„èƒ½åŠ›ã€‚é€šè¿‡åŒè·¯å¾„æ¶æ„è¿›è¡Œå†…ç½®æ—¶ç©ºç‰¹å¾ç¼–ç ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§é«˜æ•ˆçš„D^2ST-Adapterå®ç°æ–¹æ³•ï¼Œç»“åˆäº†ä¸“é—¨çš„æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¯æŒç©ºé—´å’Œæ—¶é—´åŸŸä¸Šçš„ä¸ªæ€§åŒ–é‡‡æ ·å¯†åº¦è°ƒæ•´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.01431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aa4aa4937d374ddb7658be316021a135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90db395f7333885f68e3dfbd473668ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbb97edd4b79d0ce4e7b21f6fe76f658.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-159781a1ee6dab39f8ada19180af2e29.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-03/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-66df7122ad8bb673f007c253fe7b5fc0.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Visual Textualization for Image Prompted Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3031c37fcfad37945b20e4cbd021760e.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  Flash-VStream Efficient Real-Time Understanding for Long Video Streams
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
