<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-20  A Dual-Stage Time-Context Network for Speech-Based Alzheimer&#39;s Disease   Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-309e96d5c3a1a942cd89f6ed448e05da.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-20-æ›´æ–°"><a href="#2025-02-20-æ›´æ–°" class="headerlink" title="2025-02-20 æ›´æ–°"></a>2025-02-20 æ›´æ–°</h1><h2 id="A-Dual-Stage-Time-Context-Network-for-Speech-Based-Alzheimerâ€™s-Disease-Detection"><a href="#A-Dual-Stage-Time-Context-Network-for-Speech-Based-Alzheimerâ€™s-Disease-Detection" class="headerlink" title="A Dual-Stage Time-Context Network for Speech-Based Alzheimerâ€™s Disease   Detection"></a>A Dual-Stage Time-Context Network for Speech-Based Alzheimerâ€™s Disease   Detection</h2><p><strong>Authors:Yifan Gao, Long Guo, Hong Liu</strong></p>
<p>Alzheimerâ€™s disease (AD) is a progressive neurodegenerative disorder that leads to irreversible cognitive decline in memory and communication. Early detection of AD through speech analysis is crucial for delaying disease progression. However, existing methods mainly use pre-trained acoustic models for feature extraction but have limited ability to model both local and global patterns in long-duration speech. In this letter, we introduce a Dual-Stage Time-Context Network (DSTC-Net) for speech-based AD detection, integrating local acoustic features with global conversational context in long-duration recordings.We first partition each long-duration recording into fixed-length segments to reduce computational overhead and preserve local temporal details.Next, we feed these segments into an Intra-Segment Temporal Attention (ISTA) module, where a bidirectional Long Short-Term Memory (BiLSTM) network with frame-level attention extracts enhanced local features.Subsequently, a Cross-Segment Context Attention (CSCA) module applies convolution-based context modeling and adaptive attention to unify global patterns across all segments.Extensive experiments on the ADReSSo dataset show that our DSTC-Net outperforms state-of-the-art models, reaching 83.10% accuracy and 83.15% F1. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ˜¯ä¸€ç§è¿›è¡Œæ€§ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œä¼šå¯¼è‡´è®°å¿†å’Œæ²Ÿé€šçš„è®¤çŸ¥èƒ½åŠ›ä¸å¯é€†åœ°ä¸‹é™ã€‚é€šè¿‡è¯­éŸ³åˆ†ææ—©æœŸæ£€æµ‹ADå¯¹äºå»¶ç¼“ç–¾ç—…è¿›å±•è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨é¢„è®­ç»ƒçš„å£°å­¦æ¨¡å‹è¿›è¡Œç‰¹å¾æå–ï¼Œä½†åœ¨é•¿æ—¶è¯­éŸ³ä¸­åŒæ—¶å»ºæ¨¡å±€éƒ¨å’Œå…¨å±€æ¨¡å¼çš„èƒ½åŠ›æœ‰é™ã€‚</p>
</blockquote>
<p>åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç”¨äºåŸºäºè¯­éŸ³çš„ADæ£€æµ‹çš„åŒé˜¶æ®µæ—¶é—´ä¸Šä¸‹æ–‡ç½‘ç»œï¼ˆDSTC-Netï¼‰ï¼Œè¯¥ç½‘ç»œå°†å±€éƒ¨å£°å­¦ç‰¹å¾ä¸é•¿æ—¶å½•éŸ³ä¸­çš„å…¨å±€å¯¹è¯ä¸Šä¸‹æ–‡ç›¸ç»“åˆã€‚æˆ‘ä»¬é¦–å…ˆå°†æ‰€æœ‰é•¿æ—¶å½•éŸ³åˆ’åˆ†ä¸ºå›ºå®šé•¿åº¦çš„ç‰‡æ®µï¼Œä»¥å‡å°‘è®¡ç®—å¼€é”€å¹¶ä¿ç•™å±€éƒ¨æ—¶é—´ç»†èŠ‚ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›ç‰‡æ®µè¾“å…¥åˆ°Intra-Segment Temporal Attentionï¼ˆISTAï¼‰æ¨¡å—ä¸­ï¼Œå…¶ä¸­å…·æœ‰å¸§çº§æ³¨æ„åŠ›çš„åŒå‘é•¿çŸ­æ—¶è®°å¿†ï¼ˆBiLSTMï¼‰ç½‘ç»œæå–å¢å¼ºçš„å±€éƒ¨ç‰¹å¾ã€‚æ¥ä¸‹æ¥ï¼ŒCross-Segment Context Attentionï¼ˆCSCAï¼‰æ¨¡å—åº”ç”¨åŸºäºå·ç§¯çš„ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œè‡ªé€‚åº”æ³¨æ„åŠ›ï¼Œä»¥ç»Ÿä¸€æ‰€æœ‰ç‰‡æ®µä¸­çš„å…¨å±€æ¨¡å¼ã€‚åœ¨ADReSSoæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DSTC-Netä¼˜äºæœ€æ–°æ¨¡å‹ï¼Œè¾¾åˆ°äº†83.10%çš„å‡†ç¡®ç‡å’Œ83.15%çš„F1å€¼ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13064v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDual-Stage Time-Context Networkï¼ˆDSTC-Netï¼‰çš„æ¨¡å‹ï¼Œç”¨äºåŸºäºè¯­éŸ³çš„é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ£€æµ‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å±€éƒ¨å£°å­¦ç‰¹å¾å’Œå…¨å±€å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œä»¥åœ¨é•¿æ—¶é•¿å½•éŸ³ä¸­å®ç°æ›´å¥½çš„æ£€æµ‹æ•ˆæœã€‚é€šè¿‡åˆ†æ®µå¤„ç†é•¿å½•éŸ³ã€å¼ºåŒ–å±€éƒ¨ç‰¹å¾å’Œæ•æ‰å…¨å±€æ¨¡å¼ï¼ŒDSTC-Netåœ¨ADReSSoæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œè¾¾åˆ°83.10%çš„å‡†ç¡®ç‡å’Œ83.15%çš„F1å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ˜¯ä¸€ç§å¯¼è‡´ä¸å¯é€†è®°å¿†å’Œæ²Ÿé€šè®¤çŸ¥è¡°é€€çš„è¿›å±•æ€§ç¥ç»é€€è¡Œæ€§ç–¾ç—…ã€‚</li>
<li>æ—©æœŸæ£€æµ‹ADå¯¹å»¶ç¼“ç–¾ç—…è¿›å±•è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡é¢„è®­ç»ƒçš„å£°å­¦æ¨¡å‹è¿›è¡Œç‰¹å¾æå–ï¼Œä½†éš¾ä»¥åœ¨é•¿æ—¶é•¿è¯­éŸ³ä¸­åŒæ—¶å»ºæ¨¡å±€éƒ¨å’Œå…¨å±€æ¨¡å¼ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Dual-Stage Time-Context Networkï¼ˆDSTC-Netï¼‰æ¨¡å‹ï¼Œæ•´åˆå±€éƒ¨å£°å­¦ç‰¹å¾å’Œå…¨å±€å¯¹è¯ä¸Šä¸‹æ–‡ã€‚</li>
<li>DSTC-Neté€šè¿‡åˆ†æ®µå¤„ç†é•¿å½•éŸ³ã€å¼ºåŒ–å±€éƒ¨ç‰¹å¾å’Œæ•æ‰å…¨å±€æ¨¡å¼å®ç°æ›´å¥½çš„æ£€æµ‹æ•ˆæœã€‚</li>
<li>DSTC-Netåœ¨ADReSSoæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œè¾¾åˆ°è¾ƒé«˜çš„å‡†ç¡®ç‡å’ŒF1å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-297e8cfe8c47fdb60ef00db5db975671.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d48911d1248c6f688d40351dfd12d938.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8538f9b15e3ad980388c7ad16065093e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a60bf0526ec443753a8e23ebf50b54ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac21c00789032990b1551a9526573eb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa0cb9beacbcf1f121425dd0790835dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f12879c48e33a54b3f81e89df5cb4a1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-deep-learning-framework-for-efficient-pathology-image-analysis"><a href="#A-deep-learning-framework-for-efficient-pathology-image-analysis" class="headerlink" title="A deep learning framework for efficient pathology image analysis"></a>A deep learning framework for efficient pathology image analysis</h2><p><strong>Authors:Peter Neidlinger, Tim Lenz, Sebastian Foersch, Chiara M. L. Loeffler, Jan Clusmann, Marco Gustav, Lawrence A. Shaktah, Rupert Langer, Bastian Dislich, Lisa A. Boardman, Amy J. French, Ellen L. Goode, Andrea Gsur, Stefanie Brezina, Marc J. Gunter, Robert Steinfelder, Hans-Michael Behrens, Christoph RÃ¶cken, Tabitha Harrison, Ulrike Peters, Amanda I. Phipps, Giuseppe Curigliano, Nicola Fusco, Antonio Marra, Michael Hoffmeister, Hermann Brenner, Jakob Nikolas Kather</strong></p>
<p>Artificial intelligence (AI) has transformed digital pathology by enabling biomarker prediction from high-resolution whole slide images (WSIs). However, current methods are computationally inefficient, processing thousands of redundant tiles per WSI and requiring complex aggregator models. We introduce EAGLE (Efficient Approach for Guided Local Examination), a deep learning framework that emulates pathologists by selectively analyzing informative regions. EAGLE incorporates two foundation models: CHIEF for efficient tile selection and Virchow2 for extracting high-quality features. Benchmarking was conducted against leading slide- and tile-level foundation models across 31 tasks from four cancer types, spanning morphology, biomarker prediction and prognosis. EAGLE outperformed state-of-the-art foundation models by up to 23% and achieved the highest AUROC overall. It processed a slide in 2.27 seconds, reducing computational time by more than 99% compared to existing models. This efficiency enables real-time workflows, allows pathologists to validate all tiles which are used by the model during analysis, and eliminates dependence on high-performance computing, making AI-powered pathology more accessible. By reliably identifying meaningful regions and minimizing artifacts, EAGLE provides robust and interpretable outputs, supporting rapid slide searches, integration into multi-omics pipelines and emerging clinical foundation models. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å·²ç»é€šè¿‡ä»é«˜åˆ†è¾¨ç‡å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œç”Ÿç‰©æ ‡å¿—ç‰©é¢„æµ‹ï¼Œä»è€Œå½»åº•æ”¹å˜äº†æ•°å­—ç—…ç†å­¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œæ¯å¼ WSIéœ€è¦å¤„ç†æ•°åƒä¸ªå†—ä½™ç“¦ç‰‡ï¼Œå¹¶ä¸”éœ€è¦å¤æ‚çš„èšåˆæ¨¡å‹ã€‚æˆ‘ä»¬å¼•å…¥äº†EAGLEï¼ˆç”¨äºå¼•å¯¼å±€éƒ¨æ£€æŸ¥çš„é«˜æ•ˆæ–¹æ³•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©æ€§åˆ†æä¿¡æ¯åŒºåŸŸæ¥æ¨¡æ‹Ÿç—…ç†å­¦å®¶ã€‚EAGLEç»“åˆäº†ä¸¤ç§åŸºç¡€æ¨¡å‹ï¼šç”¨äºé«˜æ•ˆç“¦ç‰‡é€‰æ‹©çš„CHIEFå’Œç”¨äºæå–é«˜è´¨é‡ç‰¹å¾çš„Virchow2ã€‚æˆ‘ä»¬åœ¨æ¥è‡ªå››ç§ç™Œç—‡ç±»å‹ã€æ¶µç›–å½¢æ€å­¦ã€ç”Ÿç‰©æ ‡å¿—ç‰©é¢„æµ‹å’Œé¢„åçš„31é¡¹ä»»åŠ¡ä¸Šï¼Œä¸é¢†å…ˆçš„å¹»ç¯ç‰‡åŠç“¦ç‰‡çº§åˆ«çš„åŸºç¡€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚EAGLEåœ¨æœ€é«˜æ ‡å‡†ä¸Šè¶…è¶Šäº†é«˜è¾¾23%çš„ç°æœ‰å…ˆè¿›åŸºç¡€æ¨¡å‹ï¼Œå¹¶è·å¾—äº†æœ€é«˜çš„AUROCå€¼ã€‚å®ƒå¯ä»¥åœ¨2.27ç§’å†…å¤„ç†ä¸€å¼ å¹»ç¯ç‰‡ï¼Œä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œè®¡ç®—æ—¶é—´å‡å°‘äº†99%ä»¥ä¸Šã€‚è¿™ç§æ•ˆç‡å®ç°äº†å®æ—¶å·¥ä½œæµç¨‹ï¼Œå…è®¸ç—…ç†å­¦å®¶éªŒè¯æ¨¡å‹åœ¨åˆ†æè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ‰€æœ‰ç“¦ç‰‡ï¼Œå¹¶æ¶ˆé™¤äº†å¯¹é«˜æ€§èƒ½è®¡ç®—çš„ä¾èµ–ï¼Œä½¿AIé©±åŠ¨çš„ç—…ç†å­¦æ›´åŠ æ˜“äºè®¿é—®ã€‚é€šè¿‡å¯é åœ°è¯†åˆ«æœ‰æ„ä¹‰åŒºåŸŸå¹¶å°½é‡å‡å°‘ä¼ªå½±ï¼ŒEAGLEæä¾›äº†ç¨³å¥å’Œå¯è§£é‡Šçš„è¾“å‡ºç»“æœï¼Œæ”¯æŒå¿«é€Ÿå¹»ç¯ç‰‡æœç´¢ã€å¤šç»„å­¦ç®¡é“é›†æˆå’Œæ–°å…´çš„ä¸´åºŠåŸºç¡€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13027v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½é€šè¿‡é«˜æ•ˆåœ°ä»é«˜åˆ†è¾¨ç‡å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰é¢„æµ‹ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œå®ç°äº†æ•°å­—ç—…ç†å­¦é¢†åŸŸçš„å˜é©ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œå¤„ç†å¤§é‡å†—ä½™åˆ‡ç‰‡ï¼Œå¹¶éœ€è¦å¤æ‚çš„èšåˆæ¨¡å‹ã€‚æˆ‘ä»¬å¼•å…¥äº†EAGLEï¼ˆç”¨äºå¼•å¯¼å±€éƒ¨æ£€æŸ¥çš„æ•ˆç‡æ–¹æ³•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡é€‰æ‹©æ€§åˆ†æä¿¡æ¯åŒºåŸŸæ¥æ¨¡æ‹Ÿç—…ç†åŒ»å¸ˆçš„è¡Œä¸ºã€‚EAGLEåŒ…å«ä¸¤ä¸ªåŸºç¡€æ¨¡å‹ï¼šç”¨äºé«˜æ•ˆåˆ‡ç‰‡é€‰æ‹©çš„CHIEFå’Œç”¨äºæå–é«˜è´¨é‡ç‰¹å¾çš„Virchow2ã€‚è·¨å››ç§ç™Œç—‡ç±»å‹ä¸­çš„31ä¸ªä»»åŠ¡è¿›è¡Œçš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒEAGLEåœ¨å½¢æ€å­¦ã€ç”Ÿç‰©æ ‡å¿—ç‰©é¢„æµ‹å’Œé¢„åè¯„ä¼°æ–¹é¢ä¼˜äºå…¶ä»–é¢†å…ˆçš„åˆ‡ç‰‡çº§å’ŒåŸºç¡€æ¨¡å‹çº§æŠ€æœ¯ï¼Œå…¶AUROCå€¼æœ€é«˜ã€‚åŒæ—¶ï¼Œå®ƒå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­å¤§å¹…åº¦å‡å°‘è®¡ç®—æ—¶é—´å¹¶æé«˜è¯Šæ–­æ•ˆç‡ã€‚å› æ­¤ï¼Œé€šè¿‡å‡†ç¡®è¯†åˆ«æœ‰æ„ä¹‰åŒºåŸŸå¹¶æœ€å°åŒ–ä¼ªå½±ï¼ŒEAGLEèƒ½å¤Ÿæä¾›ç¨³å¥å’Œå¯è§£é‡Šçš„è¾“å‡ºæ¥æ”¯æŒå¿«é€Ÿåˆ‡ç‰‡æœç´¢å’Œå¤šç»„å­¦ç®¡é“é›†æˆç­‰åº”ç”¨ã€‚è¿™ä¸€æŠ€æœ¯çš„å‡ºç°ä½¿å¾—äººå·¥æ™ºèƒ½èµ‹èƒ½çš„ç—…ç†å­¦æ›´åŠ å®ç”¨å’Œä¾¿æ·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIåœ¨æ•°å­—ç—…ç†å­¦é¢†åŸŸä¸­å®ç°äº†ä»é«˜åˆ†è¾¨ç‡å…¨åˆ‡ç‰‡å›¾åƒé¢„æµ‹ç”Ÿç‰©æ ‡å¿—ç‰©çš„é«˜æ•ˆåŒ–å˜é©ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•å­˜åœ¨è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9280fc3d274b99514cd3ff3dd6b16fef.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Fake-It-Till-You-Make-It-Using-Synthetic-Data-and-Domain-Knowledge-for-Improved-Text-Based-Learning-for-LGE-Detection"><a href="#Fake-It-Till-You-Make-It-Using-Synthetic-Data-and-Domain-Knowledge-for-Improved-Text-Based-Learning-for-LGE-Detection" class="headerlink" title="Fake It Till You Make It: Using Synthetic Data and Domain Knowledge for   Improved Text-Based Learning for LGE Detection"></a>Fake It Till You Make It: Using Synthetic Data and Domain Knowledge for   Improved Text-Based Learning for LGE Detection</h2><p><strong>Authors:Athira J Jacob, Puneet Sharma, Daniel Rueckert</strong></p>
<p>Detection of hyperenhancement from cardiac LGE MRI images is a complex task requiring significant clinical expertise. Although deep learning-based models have shown promising results for the task, they require large amounts of data with fine-grained annotations. Clinical reports generated for cardiac MR studies contain rich, clinically relevant information, including the location, extent and etiology of any scars present. Although recently developed CLIP-based training enables pretraining models with image-text pairs, it requires large amounts of data and further finetuning strategies on downstream tasks. In this study, we use various strategies rooted in domain knowledge to train a model for LGE detection solely using text from clinical reports, on a relatively small clinical cohort of 965 patients. We improve performance through the use of synthetic data augmentation, by systematically creating scar images and associated text. In addition, we standardize the orientation of the images in an anatomy-informed way to enable better alignment of spatial and text features. We also use a captioning loss to enable fine-grained supervision and explore the effect of pretraining of the vision encoder on performance. Finally, ablation studies are carried out to elucidate the contributions of each design component to the overall performance of the model. </p>
<blockquote>
<p>ä»å¿ƒè„LGE MRIå›¾åƒä¸­æ£€æµ‹è¶…å¢å¼ºæ˜¯ä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦ä¸°å¯Œçš„ä¸´åºŠç»éªŒã€‚å°½ç®¡åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬éœ€è¦å¤§é‡çš„ç²¾ç»†æ ‡æ³¨æ•°æ®ã€‚ä¸ºå¿ƒè„MRç ”ç©¶ç”Ÿæˆçš„ä¸´åºŠæŠ¥å‘ŠåŒ…å«ä¸°å¯Œçš„ä¸ä¸´åºŠç›¸å…³çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä»»ä½•ç–¤ç—•çš„ä½ç½®ã€ç¨‹åº¦å’Œç—…å› ã€‚è™½ç„¶æœ€è¿‘å¼€å‘çš„åŸºäºCLIPçš„è®­ç»ƒæ–¹æ³•èƒ½å¤Ÿå®ç°å›¾åƒæ–‡æœ¬å¯¹çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½†å®ƒéœ€è¦å¤§é‡æ•°æ®ï¼Œå¹¶ä¸”éœ€è¦åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›ä¸€æ­¥å¾®è°ƒç­–ç•¥ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨åŸºäºé¢†åŸŸçŸ¥è¯†çš„å„ç§ç­–ç•¥ï¼Œä»…ä½¿ç”¨æ¥è‡ª965åæ‚£è€…ä¸´åºŠæŠ¥å‘Šçš„æ–‡æœ¬ï¼Œå¯¹LGEæ£€æµ‹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿåœ°åˆ›å»ºç–¤ç—•å›¾åƒå’Œç›¸å…³æ–‡æœ¬ï¼Œæé«˜é€šè¿‡åˆæˆæ•°æ®å¢å¼ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»¥è§£å‰–ä¿¡æ¯çš„æ–¹å¼æ ‡å‡†åŒ–å›¾åƒæ–¹å‘ï¼Œä»¥å®ç°ç©ºé—´ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾çš„æ›´å¥½å¯¹é½ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æè¿°æ€§æŸå¤±æ¥å®ç°ç²¾ç»†ç›‘ç£ï¼Œå¹¶æ¢ç´¢è§†è§‰ç¼–ç å™¨é¢„è®­ç»ƒå¯¹æ€§èƒ½çš„å½±å“ã€‚æœ€åï¼Œè¿›è¡Œæ¶ˆèç ”ç©¶ä»¥é˜æ˜æ¯ä¸ªè®¾è®¡ç»„ä»¶å¯¹æ¨¡å‹æ€»ä½“æ€§èƒ½çš„è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12948v1">PDF</a> Poster at Workshop on Large Language Models and Generative AI for   Health at AAAI 2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨ä¸´åºŠæŠ¥å‘Šä¸­çš„æ–‡æœ¬æ•°æ®ï¼Œç»“åˆé¢†åŸŸçŸ¥è¯†å’Œå¤šç§ç­–ç•¥ï¼Œå¦‚åˆæˆæ•°æ®å¢å¼ºã€å›¾åƒæ ‡å‡†åŒ–ã€ç²¾ç»†ç›‘ç£çš„æ ‡æ³¨æŸå¤±å’Œè§†è§‰ç¼–ç å™¨çš„é¢„è®­ç»ƒï¼Œè®­ç»ƒäº†ä¸€ä¸ªä»…ä½¿ç”¨æ–‡æœ¬æ•°æ®çš„LGE MRIå›¾åƒè¶…å¢å¼ºæ£€æµ‹æ¨¡å‹ã€‚åœ¨è¾ƒå°çš„965ä¾‹æ‚£è€…é˜Ÿåˆ—ä¸­å–å¾—äº†è‰¯å¥½æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒè„LGE MRIå›¾åƒçš„è¶…å¢å¼ºæ£€æµ‹æ˜¯ä¸€é¡¹éœ€è¦ä¸´åºŠä¸“å®¶ç»éªŒçš„å¤æ‚ä»»åŠ¡ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†éœ€è¦å¤§é‡ç²¾ç»†æ ‡æ³¨çš„æ•°æ®ã€‚</li>
<li>ä¸´åºŠæŠ¥å‘ŠåŒ…å«å…³äºå¿ƒè„MRç ”ç©¶çš„é‡è¦ä¸´åºŠä¿¡æ¯ï¼Œå¯ç”¨äºè®­ç»ƒæ£€æµ‹æ¨¡å‹ã€‚</li>
<li>æœ¬ç ”ç©¶ä½¿ç”¨åŸºäºé¢†åŸŸçŸ¥è¯†çš„ç­–ç•¥ï¼Œä»…ä½¿ç”¨ä¸´åºŠæŠ¥å‘Šçš„æ–‡æœ¬æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>é€šè¿‡åˆæˆæ•°æ®å¢å¼ºã€å›¾åƒæ ‡å‡†åŒ–å’Œç²¾ç»†ç›‘ç£çš„æ ‡æ³¨æŸå¤±ç­‰æ–¹æ³•æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¯¹è§†è§‰ç¼–ç å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b524d1d24c627dd2f2ba5a615eb20fb4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e168dd0cbd25050ab5286ce1fca2f7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-970ecbbf34603c490ff1501aea97db3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8a3ed1b563a6684d7a6007f00eddd85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54cb1e2e444cc2acc163d539396be165.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Carotid-Artery-Plaque-Analysis-in-3D-Based-on-Distance-Encoding-in-Mesh-Representations"><a href="#Carotid-Artery-Plaque-Analysis-in-3D-Based-on-Distance-Encoding-in-Mesh-Representations" class="headerlink" title="Carotid Artery Plaque Analysis in 3D Based on Distance Encoding in Mesh   Representations"></a>Carotid Artery Plaque Analysis in 3D Based on Distance Encoding in Mesh   Representations</h2><p><strong>Authors:Hinrich Rahlfs, Markus HÃ¼llebrand, Sebastian Schmitter, Christoph Strecker, Andreas Harloff, Anja Hennemuth</strong></p>
<p>Purpose: Enabling a comprehensive and robust assessment of carotid artery plaques in 3D through extraction and visualization of quantitative plaque parameters. These parameters have potential applications in stroke risk analysis, evaluation of therapy effectiveness, and plaque progression prediction. Methods: We propose a novel method for extracting a plaque mesh from 3D vessel wall segmentation using distance encoding on the inner and outer wall mesh for precise plaque structure analysis. A case-specific threshold, derived from the normal vessel wall thickness, was applied to extract plaques from a dataset of 202 T1-weighted black-blood MRI scans of subjects with up to 50% stenosis. Applied to baseline and one-year follow-up data, the method supports detailed plaque morphology analysis over time, including plaque volume quantification, aided by improved visualization via mesh unfolding. Results: We successfully extracted plaque meshes from 341 carotid arteries, capturing a wide range of plaque shapes with volumes ranging from 2.69{\mu}l to 847.7{\mu}l. The use of a case-specific threshold effectively eliminated false positives in young, healthy subjects. Conclusion: The proposed method enables precise extraction of plaque meshes from 3D vessel wall segmentation masks enabling a correspondence between baseline and one-year follow-up examinations. Unfolding the plaque meshes enhances visualization, while the mesh-based analysis allows quantification of plaque parameters independent of voxel resolution. </p>
<blockquote>
<p>ç›®çš„ï¼šé€šè¿‡æå–å’Œå¯è§†åŒ–å®šé‡æ–‘å—å‚æ•°ï¼Œå®ç°å¯¹é¢ˆåŠ¨è„‰æ–‘å—åœ¨3Dä¸­çš„å…¨é¢å’Œç¨³å¥è¯„ä¼°ã€‚è¿™äº›å‚æ•°åœ¨è„‘å’ä¸­é£é™©åˆ†æã€ç–—æ•ˆè¯„ä¼°ä»¥åŠæ–‘å—è¿›å±•é¢„æµ‹ä¸­å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è·ç¦»ç¼–ç å†…ã€å¤–å£ç½‘æ ¼ä»3Dè¡€ç®¡å£åˆ†å‰²ä¸­æå–æ–‘å—ç½‘æ ¼ï¼Œç”¨äºç²¾ç¡®åˆ†ææ–‘å—ç»“æ„ã€‚åº”ç”¨ç”±æ­£å¸¸è¡€ç®¡å£åšæ¨å¯¼å‡ºçš„é’ˆå¯¹ç‰¹å®šç—…ä¾‹çš„é˜ˆå€¼ï¼Œä»ä¸€ç»„åŒ…å«é«˜è¾¾50%ç‹­çª„çš„202ä¾‹T1åŠ æƒé»‘è¡€MRIæ‰«ææ•°æ®é›†ä¸­æå–æ–‘å—ã€‚è¯¥æ–¹æ³•åº”ç”¨äºåŸºçº¿æ•°æ®ä»¥åŠä¸€å¹´åçš„éšè®¿æ•°æ®ï¼Œæ”¯æŒéšæ—¶é—´å˜åŒ–çš„è¯¦ç»†æ–‘å—å½¢æ€åˆ†æï¼ŒåŒ…æ‹¬æ–‘å—ä½“ç§¯é‡åŒ–ï¼Œè¾…ä»¥ç½‘æ ¼å±•å¼€æ”¹è¿›çš„å¯è§†åŒ–ã€‚ç»“æœï¼šæˆ‘ä»¬æˆåŠŸåœ°ä»341ä¸ªé¢ˆåŠ¨è„‰ä¸­æå–äº†æ–‘å—ç½‘æ ¼ï¼Œæ•æ‰åˆ°äº†å„ç§æ–‘å—å½¢çŠ¶ï¼Œä½“ç§¯èŒƒå›´ä»2.69Î¼låˆ°847.7Î¼lã€‚ä½¿ç”¨ç‰¹å®šç—…ä¾‹çš„é˜ˆå€¼æœ‰æ•ˆåœ°æ¶ˆé™¤äº†å¹´è½»å¥åº·äººç¾¤ä¸­çš„å‡é˜³æ€§ç»“æœã€‚ç»“è®ºï¼šæ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿç²¾ç¡®åœ°ä»3Dè¡€ç®¡å£åˆ†å‰²è’™ç‰ˆä¸­æå–æ–‘å—ç½‘æ ¼ï¼Œå®ç°åŸºçº¿æ£€æŸ¥å’Œä¸€å¹´åçš„éšè®¿æ£€æŸ¥ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚å±•å¼€æ–‘å—ç½‘æ ¼å¢å¼ºäº†å¯è§†åŒ–æ•ˆæœï¼Œè€ŒåŸºäºç½‘æ ¼çš„åˆ†æå…è®¸ç‹¬ç«‹äºä½“ç´ åˆ†è¾¨ç‡çš„æ–‘å—å‚æ•°é‡åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12819v1">PDF</a> 13 pages, 5 Figures, Submitted to the International Journal of   Computer Assisted Radiology and Surgery</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸‰ç»´è¡€ç®¡å£åˆ†å‰²çš„æ–°å‹æ–‘å—æå–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯é€šè¿‡è·ç¦»ç¼–ç ç¡®å®šå†…å¤–å£ç½‘æ ¼è¿›è¡Œç²¾ç¡®æ–‘å—ç»“æ„åˆ†æã€‚åº”ç”¨äºåŸºäºT1åŠ æƒé»‘è¡€MRIæ‰«æçš„æ•°æ®é›†ï¼Œè¯¥æ–¹æ³•å¯ä»åŸºå‡†çº¿å’Œä¸€å¹´çš„éšè®¿æ•°æ®ä¸­åˆ†ææ–‘å—å½¢æ€ï¼ŒåŒ…æ‹¬é‡åŒ–æ–‘å—ä½“ç§¯ç­‰ã€‚æœ¬æ–‡æˆåŠŸçš„ä»341ä¸ªé¢ˆåŠ¨è„‰ä¸­æå–äº†æ–‘å—ç½‘æ ¼ï¼Œæ•æ‰åˆ°äº†å¹¿æ³›çš„æ–‘å—å½¢æ€å’Œä½“ç§¯èŒƒå›´ã€‚æ­¤æ–¹æ³•åˆ©ç”¨ç‰¹å®šçš„é˜ˆå€¼æœ‰æ•ˆåœ°æ¶ˆé™¤äº†å¹´è½»å¥åº·äººç¾¤ä¸­çš„å‡é˜³æ€§ç»“æœã€‚è¿™ç§æ–¹æ³•ä½¿å¾—ç²¾ç¡®æå–æ–‘å—ç½‘æ ¼æˆä¸ºå¯èƒ½ï¼Œä¸ºæ–‘å—å‚æ•°é‡åŒ–æä¾›äº†åŸºç¡€ï¼Œæœ‰åŠ©äºè¯„ä¼°ä¸­é£é£é™©ã€è¯„ä¼°æ²»ç–—æ•ˆæœå’Œé¢„æµ‹æ–‘å—è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§åŸºäºä¸‰ç»´è¡€ç®¡å£åˆ†å‰²çš„æ–°å‹æ–‘å—æå–æ–¹æ³•ï¼Œç”¨äºç²¾ç¡®åˆ†ææ–‘å—ç»“æ„ã€‚</li>
<li>åˆ©ç”¨è·ç¦»ç¼–ç ç¡®å®šå†…å¤–å£ç½‘æ ¼è¿›è¡Œæ–‘å—æå–å’Œå¯è§†åŒ–ã€‚</li>
<li>é€šè¿‡åº”ç”¨ç‰¹å®šçš„é˜ˆå€¼ï¼ŒæˆåŠŸä»æ•°æ®é›†ä¸­æå–äº†æ–‘å—ç½‘æ ¼ï¼Œæ•æ‰åˆ°äº†å¹¿æ³›çš„æ–‘å—å½¢æ€å’Œä½“ç§¯èŒƒå›´ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥åº”ç”¨äºåŸºå‡†çº¿å’Œä¸€å¹´çš„éšè®¿æ•°æ®ï¼Œè¿›è¡Œè¯¦ç»†çš„æ–‘å—å½¢æ€åˆ†æï¼ŒåŒ…æ‹¬é‡åŒ–æ–‘å—ä½“ç§¯ã€‚</li>
<li>å±•å¼€ç½‘æ ¼å¢å¼ºäº†æ–‘å—çš„å¯è§†åŒ–æ•ˆæœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d4c837f93ae7a37ccb2a95c3d83d487.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1caa8e9bb46cdfac3450f9848271280.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4b1217c016c4f9e0f5bf9d4e4dad0a4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="3D-Shape-to-Image-Brownian-Bridge-Diffusion-for-Brain-MRI-Synthesis-from-Cortical-Surfaces"><a href="#3D-Shape-to-Image-Brownian-Bridge-Diffusion-for-Brain-MRI-Synthesis-from-Cortical-Surfaces" class="headerlink" title="3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from   Cortical Surfaces"></a>3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from   Cortical Surfaces</h2><p><strong>Authors:Fabian Bongratz, Yitong Li, Sama Elbaroudy, Christian Wachinger</strong></p>
<p>Despite recent advances in medical image generation, existing methods struggle to produce anatomically plausible 3D structures. In synthetic brain magnetic resonance images (MRIs), characteristic fissures are often missing, and reconstructed cortical surfaces appear scattered rather than densely convoluted. To address this issue, we introduce Cor2Vox, the first diffusion model-based method that translates continuous cortical shape priors to synthetic brain MRIs. To achieve this, we leverage a Brownian bridge process which allows for direct structured mapping between shape contours and medical images. Specifically, we adapt the concept of the Brownian bridge diffusion model to 3D and extend it to embrace various complementary shape representations. Our experiments demonstrate significant improvements in the geometric accuracy of reconstructed structures compared to previous voxel-based approaches. Moreover, Cor2Vox excels in image quality and diversity, yielding high variation in non-target structures like the skull. Finally, we highlight the capability of our approach to simulate cortical atrophy at the sub-voxel level. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ai-med/Cor2Vox">https://github.com/ai-med/Cor2Vox</a>. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘åœ¨åŒ»å­¦å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä»ç„¶éš¾ä»¥ç”Ÿæˆè§£å‰–ä¸Šåˆç†çš„3Dç»“æ„ã€‚åœ¨åˆæˆçš„å¤§è„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­ï¼Œç‰¹å¾è£‚å¾€å¾€ç¼ºå¤±ï¼Œé‡å»ºçš„çš®å±‚è¡¨é¢çœ‹èµ·æ¥æ˜¯æ•£ä¹±çš„ï¼Œè€Œä¸æ˜¯å¯†é›†å·æ›²çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Cor2Voxï¼Œè¿™æ˜¯åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œé¦–æ¬¡å°†è¿ç»­çš„çš®è´¨å½¢çŠ¶å…ˆéªŒçŸ¥è¯†è½¬åŒ–ä¸ºåˆæˆçš„å¤§è„‘MRIã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åˆ©ç”¨å¸ƒæœ—æ¡¥è¿‡ç¨‹ï¼Œå…è®¸å½¢çŠ¶è½®å»“å’ŒåŒ»å­¦å›¾åƒä¹‹é—´çš„ç›´æ¥ç»“æ„åŒ–æ˜ å°„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹çš„æ¦‚å¿µé€‚åº”åˆ°3Dï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°åŒ…å«å„ç§äº’è¡¥å½¢çŠ¶è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸å…ˆå‰çš„åŸºäºä½“ç´ çš„æ–¹æ³•ç›¸æ¯”ï¼Œé‡å»ºç»“æ„çš„å‡ ä½•ç²¾åº¦æœ‰äº†æ˜¾ç€æé«˜ã€‚æ­¤å¤–ï¼ŒCor2Voxåœ¨å›¾åƒè´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨éç›®æ ‡ç»“æ„ï¼ˆå¦‚é¢…éª¨ï¼‰ä¸Šäº§ç”Ÿäº†è¾ƒé«˜çš„å˜åŒ–ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨äºšä½“ç´ çº§åˆ«æ¨¡æ‹Ÿçš®è´¨èç¼©çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ai-med/Cor2Vox%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ai-med/Cor2Voxè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12742v1">PDF</a> Accepted by Information Processing in Medical Imaging (IPMI) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Cor2Voxï¼Œé¦–ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œèƒ½å°†è¿ç»­çš„çš®è´¨å½¢çŠ¶å…ˆéªŒè½¬åŒ–ä¸ºåˆæˆçš„å¤§è„‘MRIå›¾åƒã€‚é€šè¿‡é‡‡ç”¨å¸ƒæœ—æ¡¥è¿‡ç¨‹ï¼Œå®ç°äº†å½¢çŠ¶è½®å»“ä¸åŒ»å­¦å›¾åƒä¹‹é—´çš„ç›´æ¥ç»“æ„åŒ–æ˜ å°„ï¼Œå¹¶åœ¨3Dç¯å¢ƒä¸­å¯¹å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†é€‚åº”å’Œæ‰©å±•ï¼Œä»¥å®¹çº³å„ç§äº’è¡¥å½¢çŠ¶è¡¨ç¤ºã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„ä½“ç´ æ–¹æ³•ï¼ŒCor2Voxåœ¨å‡ ä½•å‡†ç¡®æ€§ä¸Šæœ‰äº†æ˜¾è‘—æå‡ï¼ŒåŒæ—¶å›¾åƒè´¨é‡å’Œå¤šæ ·æ€§ä¹Ÿæœ‰æ‰€æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éç›®æ ‡ç»“æ„å¦‚é¢…éª¨ä¸Šè¡¨ç°å‡ºé«˜å˜å¼‚æ€§ã€‚æ­¤å¤–ï¼ŒCor2Voxè¿˜èƒ½æ¨¡æ‹Ÿå­ä½“ç´ çº§åˆ«çš„çš®è´¨èç¼©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Cor2Voxæ˜¯é¦–ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„åŒ»å­¦å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œä¸“æ³¨äºåˆæˆå¤§è„‘MRIã€‚</li>
<li>é€šè¿‡å¸ƒæœ—æ¡¥è¿‡ç¨‹å®ç°å½¢çŠ¶è½®å»“ä¸åŒ»å­¦å›¾åƒä¹‹é—´çš„ç›´æ¥ç»“æ„åŒ–æ˜ å°„ã€‚</li>
<li>Cor2Voxå°†è¿ç»­çš„çš®è´¨å½¢çŠ¶å…ˆéªŒè½¬åŒ–ä¸ºåˆæˆçš„å¤§è„‘MRIã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨3Dç¯å¢ƒä¸­å¯¹å¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†é€‚åº”å’Œæ‰©å±•ã€‚</li>
<li>Cor2Voxåœ¨å‡ ä½•å‡†ç¡®æ€§ä¸Šç›¸è¾ƒäºä¼ ç»Ÿä½“ç´ æ–¹æ³•æœ‰æ‰€æå‡ã€‚</li>
<li>Cor2Voxèƒ½æé«˜å›¾åƒè´¨é‡å’Œå¤šæ ·æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éç›®æ ‡ç»“æ„ä¸Šã€‚</li>
<li>Cor2Voxèƒ½æ¨¡æ‹Ÿå­ä½“ç´ çº§åˆ«çš„çš®è´¨èç¼©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12742">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae75490596d8975afb8a2208d53870cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82e9d522c9f13e3b8847bf831182c7c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f525489c3273d55ad672fd43b9673a5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31ad5050d5b50a4c67c501c6ffcdf6f9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Propagation-for-Echocardiography-Clinical-Metric-Estimation-via-Contour-Sampling"><a href="#Uncertainty-Propagation-for-Echocardiography-Clinical-Metric-Estimation-via-Contour-Sampling" class="headerlink" title="Uncertainty Propagation for Echocardiography Clinical Metric Estimation   via Contour Sampling"></a>Uncertainty Propagation for Echocardiography Clinical Metric Estimation   via Contour Sampling</h2><p><strong>Authors:Thierry Judge, Olivier Bernard, Woo-Jin Cho Kim, Alberto Gomez, Arian Beqiri, Agisilaos Chartsias, Pierre-Marc Jodoin</strong></p>
<p>Echocardiography plays a fundamental role in the extraction of important clinical parameters (e.g. left ventricular volume and ejection fraction) required to determine the presence and severity of heart-related conditions. When deploying automated techniques for computing these parameters, uncertainty estimation is crucial for assessing their utility. Since clinical parameters are usually derived from segmentation maps, there is no clear path for converting pixel-wise uncertainty values into uncertainty estimates in the downstream clinical metric calculation. In this work, we propose a novel uncertainty estimation method based on contouring rather than segmentation. Our method explicitly predicts contour location uncertainty from which contour samples can be drawn. Finally, the sampled contours can be used to propagate uncertainty to clinical metrics. Our proposed method not only provides accurate uncertainty estimations for the task of contouring but also for the downstream clinical metrics on two cardiac ultrasound datasets. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/ThierryJudge/contouring-uncertainty">https://github.com/ThierryJudge/contouring-uncertainty</a>. </p>
<blockquote>
<p>è¶…å£°å¿ƒåŠ¨æœ¯åœ¨æå–ç¡®å®šå¿ƒè„çŠ¶å†µå­˜åœ¨ä¸å¦åŠå…¶ä¸¥é‡ç¨‹åº¦æ‰€éœ€çš„é‡è¦ä¸´åºŠå‚æ•°ï¼ˆä¾‹å¦‚å·¦å¿ƒå®¤å®¹ç§¯å’Œå°„è¡€åˆ†æ•°ï¼‰æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å½“åº”ç”¨è‡ªåŠ¨åŒ–æŠ€æœ¯æ¥è®¡ç®—è¿™äº›å‚æ•°æ—¶ï¼Œå¯¹ä¸ç¡®å®šæ€§çš„è¯„ä¼°æ˜¯è‡³å…³é‡è¦çš„ã€‚ç”±äºä¸´åºŠå‚æ•°é€šå¸¸æ¥è‡ªåˆ†å‰²å›¾ï¼Œå› æ­¤æ²¡æœ‰æ˜ç¡®çš„é€”å¾„å°†åƒç´ çº§çš„ä¸ç¡®å®šæ€§å€¼è½¬æ¢ä¸ºä¸‹æ¸¸ä¸´åºŠæŒ‡æ ‡è®¡ç®—ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè½®å»“è€Œä¸æ˜¯åˆ†å‰²çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ˜ç¡®åœ°é¢„æµ‹è½®å»“ä½ç½®çš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œå¯ä»¥ä»ä¸­ç»˜åˆ¶è½®å»“æ ·æœ¬ã€‚æœ€åï¼Œè¿™äº›é‡‡æ ·è½®å»“å¯ç”¨äºå°†ä¸ç¡®å®šæ€§ä¼ æ’­åˆ°ä¸´åºŠæŒ‡æ ‡ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸ä»…ä¸ºè½®å»“ä»»åŠ¡æä¾›äº†å‡†ç¡®çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œè€Œä¸”åœ¨ä¸¤ä¸ªå¿ƒè„è¶…å£°æ•°æ®é›†ä¸Šå¯¹ä¸‹æ¸¸ä¸´åºŠæŒ‡æ ‡ä¹Ÿæä¾›äº†å‡†ç¡®çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚ç›¸å…³ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ThierryJudge/contouring-uncertainty">https://github.com/ThierryJudge/contouring-uncertainty</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12713v1">PDF</a> 10 pages, submitted to IEEE TMI</p>
<p><strong>Summary</strong><br>     è¶…å£°å¿ƒåŠ¨å›¾åœ¨æå–é‡è¦ä¸´åºŠå‚æ•°ï¼ˆå¦‚å·¦å¿ƒå®¤å®¹ç§¯å’Œå°„è¡€åˆ†æ•°ï¼‰ä»¥åˆ¤æ–­å¿ƒè„ç›¸å…³ç–¾ç—…çš„å­˜åœ¨å’Œä¸¥é‡ç¨‹åº¦æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚å½“ä½¿ç”¨è‡ªåŠ¨åŒ–æŠ€æœ¯è®¡ç®—è¿™äº›å‚æ•°æ—¶ï¼Œå¯¹ä¸ç¡®å®šæ€§è¿›è¡Œä¼°è®¡æ˜¯è¯„ä¼°å…¶æ•ˆç”¨çš„å…³é”®ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè½®å»“è€Œéåˆ†å‰²çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ˜ç¡®é¢„æµ‹è½®å»“ä½ç½®çš„ä¸ç¡®å®šæ€§ï¼Œä»è€Œç»˜åˆ¶è½®å»“æ ·æœ¬ã€‚æœ€åï¼Œè¿™äº›æ ·æœ¬å¯ç”¨äºå°†ä¸ç¡®å®šæ€§ä¼ æ’­åˆ°ä¸´åºŠæŒ‡æ ‡ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¸ºè½®å»“ä»»åŠ¡æä¾›äº†å‡†ç¡®çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œè€Œä¸”åœ¨ä¸¤ä¸ªå¿ƒè„è¶…å£°æ•°æ®é›†ä¸Šå¯¹ä¸‹æ¸¸ä¸´åºŠæŒ‡æ ‡ä¹Ÿæä¾›äº†å‡†ç¡®çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°å¿ƒåŠ¨å›¾åœ¨è¯Šæ–­å¿ƒè„ç›¸å…³ç–¾ç—…ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œèƒ½æå–å…³é”®ä¸´åºŠå‚æ•°ã€‚</li>
<li>è‡ªåŠ¨åŒ–æŠ€æœ¯åœ¨è®¡ç®—ä¸´åºŠå‚æ•°æ—¶ï¼Œä¸ç¡®å®šæ€§ä¼°è®¡æ˜¯è¯„ä¼°å…¶æ•ˆç”¨çš„å…³é”®ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè½®å»“çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„åˆ†å‰²æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æ˜ç¡®é¢„æµ‹è½®å»“ä½ç½®çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶ç»˜åˆ¶è½®å»“æ ·æœ¬ã€‚</li>
<li>è½®å»“æ ·æœ¬å¯ç”¨äºå°†ä¸ç¡®å®šæ€§ä¼ æ’­åˆ°ä¸´åºŠæŒ‡æ ‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªå¿ƒè„è¶…å£°æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¯¹ä¸´åºŠæŒ‡æ ‡å‡†ç¡®çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-309e96d5c3a1a942cd89f6ed448e05da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-061eb92e7a080b03cf71b45161c999ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ce4da25129128efa910fec03958edca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bddd12340f421c0d98207d9fe261876.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc4854a5f80ff2f32ddc30b0f37a1841.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VLBI-Imaging-of-Parsec-scale-Radio-Structures-in-Nearby-Low-luminosity-AGN"><a href="#VLBI-Imaging-of-Parsec-scale-Radio-Structures-in-Nearby-Low-luminosity-AGN" class="headerlink" title="VLBI Imaging of Parsec-scale Radio Structures in Nearby Low-luminosity   AGN"></a>VLBI Imaging of Parsec-scale Radio Structures in Nearby Low-luminosity   AGN</h2><p><strong>Authors:Xiaopeng Cheng, Tao An, Willem Baan, Raneri D. Baldi, David R. A. Williams-Baldwin, Bong Won Sohn, Robert Beswick, Ian M. Mchardy</strong></p>
<p>We report the results of high-resolution 5 GHz Very Long Baseline Array and European VLBI Network observations of 36 nearby galaxies, an extension of the Legacy e-MERLIN Multi-band Imaging of Nearby Galaxies (LeMMINGs) survey. Our sample includes 21 low ionization nuclear emission regions (LINERs), 4 Seyferts, 3 absorption line galaxies (ALGs), and 8 HII galaxies. We achieved an unprecedented detection rate, successfully imaging 23 out of 36 sources with a detection threshold of $\sim$20 $\mu$Jy beam$^{-1}$. The radio sizes are typically of $\leq$ 5 pc. Core identification was achieved in 16 sources, while 7 others were identified as core candidates. Radio luminosities of the sample range from 10$\rm ^{34}$ to 10$\rm ^{38}$ erg s$^{-1}$. Our analysis reveals a predominance of compact core structures, with ten sources exhibiting a one-sided core jet morphology and NGC 2146 exhibiting a rare two-sided jet structure. The study advances our understanding of the compactness of radio sources at various scales, indicating a core-dominated nature in all but one galaxy NGC2655. We find moderate to strong correlations between radio luminosity, black hole mass, optical [O III] line luminosity, and hard X-ray luminosity, suggesting a common active galactic nucleus (AGN) core origin. These results provide new insights into the fundamental plane of black hole activity and support the role of the synchrotron process in Low-luminosity AGN (LLAGN) radio emission. </p>
<blockquote>
<p>æˆ‘ä»¬æŠ¥å‘Šäº†ä½¿ç”¨é«˜è§£æåº¦5 GHzè¶…é•¿åŸºçº¿é˜µåˆ—å’Œæ¬§æ´²VLBIç½‘ç»œå¯¹é™„è¿‘36ä¸ªæ˜Ÿç³»è¿›è¡Œè§‚æµ‹çš„ç»“æœï¼Œè¿™æ˜¯å¯¹é—ç•™e-MERLINé‚»è¿‘æ˜Ÿç³»å¤šæ³¢æ®µæˆåƒï¼ˆLeMMINGsï¼‰è°ƒæŸ¥çš„æ‰©å±•ã€‚æˆ‘ä»¬çš„æ ·æœ¬åŒ…æ‹¬21ä¸ªä½ç”µç¦»æ ¸å‘å°„åŒºï¼ˆLINERsï¼‰ã€4ä¸ªå¡å¼—é‡Œç‰¹æ˜Ÿç³»ã€3ä¸ªå¸æ”¶çº¿æ˜Ÿç³»ï¼ˆALGsï¼‰å’Œ8ä¸ªHIIæ˜Ÿç³»ã€‚æˆ‘ä»¬å–å¾—äº†å‰æ‰€æœªæœ‰çš„æ£€æµ‹ç‡ï¼ŒæˆåŠŸåœ°å¯¹å…¶ä¸­23ä¸ªæºè¿›è¡Œäº†æˆåƒï¼Œæ£€æµ‹é˜ˆå€¼çº¦ä¸º$\sim$20 $\mu$Jy beam$^{-1}$ã€‚å°„ç”µå¤§å°é€šå¸¸å°äºæˆ–ç­‰äº5 pcã€‚æ ¸å¿ƒè¯†åˆ«åœ¨16ä¸ªæºä¸Šå®ç°ï¼Œè€Œå…¶ä»–7ä¸ªè¢«è¯†åˆ«ä¸ºæ ¸å¿ƒå€™é€‰è€…ã€‚æ ·æœ¬çš„å°„ç”µå…‰åº¦èŒƒå›´ä»10$^{34}$åˆ°10$^{38}$ erg s$^{-1}$ã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºç´§å‡‘çš„æ ¸å¿ƒç»“æ„å ä¸»å¯¼åœ°ä½ï¼Œæœ‰åä¸ªæºå‘ˆç°å•è¾¹æ ¸å¿ƒå–·å°„å½¢æ€ï¼Œè€ŒNGC 2146å‘ˆç°ç½•è§çš„åŒè¾¹å–·å°„ç»“æ„ã€‚è¯¥ç ”ç©¶æ¨è¿›äº†å¯¹ä¸åŒå°ºåº¦ä¸Šå°„ç”µæºç´§å‡‘æ€§çš„ç†è§£ï¼Œé™¤äº†ä¸€ä¸ªæ˜Ÿç³»NGC2655ä¹‹å¤–ï¼Œå…¶ä»–æ‰€æœ‰æ˜Ÿç³»éƒ½æ˜¾ç¤ºå‡ºæ ¸å¿ƒä¸»å¯¼çš„ç‰¹å¾ã€‚æˆ‘ä»¬å‘ç°å°„ç”µå…‰åº¦ä¸é»‘æ´è´¨é‡ã€å…‰å­¦[O III]çº¿å…‰åº¦å’Œç¡¬Xå°„çº¿å…‰åº¦ä¹‹é—´å­˜åœ¨ä¸­åº¦åˆ°å¼ºåº¦çš„ç›¸å…³æ€§ï¼Œè¿™è¡¨æ˜å®ƒä»¬éƒ½æ¥è‡ªæ´»è·ƒçš„æ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰æ ¸å¿ƒã€‚è¿™äº›ç»“æœæä¾›äº†å…³äºé»‘æ´æ´»åŠ¨åŸºæœ¬å¹³é¢çš„æ–°è§è§£ï¼Œå¹¶æ”¯æŒåŒæ­¥è¾å°„è¿‡ç¨‹åœ¨ä½å…‰åº¦æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆLLAGNï¼‰å°„ç”µå‘å°„ä¸­çš„ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12553v1">PDF</a> 25 pages, 6 figures, 4 tables, accepted for publication in ApJS</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æŠ¥å‘Šäº†ä½¿ç”¨é«˜åˆ†è¾¨ç‡5 GHzè¶…é•¿åŸºçº¿é˜µåˆ—å’Œæ¬§æ´²VLBIç½‘ç»œå¯¹é™„è¿‘æ˜Ÿç³»è¿›è¡Œè§‚æµ‹çš„ç»“æœï¼Œè¿™æ˜¯Legacy e-MERLINå¯¹é™„è¿‘æ˜Ÿç³»çš„å¤šæ³¢æ®µæˆåƒï¼ˆLeMMINGsï¼‰è°ƒæŸ¥çš„å»¶ä¼¸ã€‚æ ·æœ¬åŒ…æ‹¬ä½ç”µç¦»æ ¸å‘å°„åŒºï¼ˆLINERsï¼‰çš„21ä¸ªæ˜Ÿç³»ï¼Œèµ›å¼—ç‰¹æ˜Ÿç³»ï¼ˆSeyfertsï¼‰çš„4ä¸ªæ˜Ÿç³»ï¼Œå¸æ”¶çº¿æ˜Ÿç³»ï¼ˆALGsï¼‰çš„3ä¸ªæ˜Ÿç³»å’ŒHIIæ˜Ÿç³»çš„8ä¸ªæ˜Ÿç³»ã€‚åœ¨çº¦20å¾®å‰æŸçš„çµæ•åº¦æé™ä¸‹æˆåŠŸæ£€æµ‹äº†è¶…è¿‡ä¸‰åˆ†ä¹‹äºŒçš„æ˜Ÿç³»ã€‚æ ¸ç»“æ„æ˜æ˜¾å¹¶ä¸”å°å‹ç´§å‡‘ï¼Œé•¿åº¦ä¸€èˆ¬ä¸è¶…è¿‡â‰¤äº”åƒå…†ç§’çš„è·ç¦»å°ºåº¦ã€‚æ ¸å¿ƒé‰´å®šæˆåŠŸå®Œæˆåœ¨åå…­ä¸ªæ˜Ÿç³»ä¸­ï¼Œè€Œå¦å¤–ä¸ƒä¸ªæ˜Ÿç³»è¢«ç¡®è®¤ä¸ºæ ¸å¿ƒå€™é€‰è€…ã€‚æ ·æœ¬çš„å°„ç”µå…‰åº¦ä»‹äºæ¯å¹³æ–¹ç§’åƒå…†ç“¦ç‰¹ï¼ˆerg s-Â²ï¼‰ä¹‹é—´ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ç´§å‡‘æ ¸å¿ƒç»“æ„å ä¸»å¯¼åœ°ä½ï¼Œæœ‰åä¸ªæ˜Ÿç³»å‘ˆç°å•è¾¹æ ¸å¿ƒå–·å°„å½¢æ€ï¼Œè€ŒNGC 2146å‘ˆç°ç½•è§çš„åŒè¾¹å–·å°„ç»“æ„ã€‚ç ”ç©¶æ¨è¿›äº†å¯¹ä¸åŒå°ºåº¦ä¸Šå°„ç”µæºç´§å‡‘æ€§çš„ç†è§£ï¼Œæ­ç¤ºäº†é™¤NGC 2655å¤–æ‰€æœ‰æ˜Ÿç³»çš„ä»¥æ ¸å¿ƒä¸ºä¸»çš„ç‰¹ç‚¹ã€‚æˆ‘ä»¬å‘ç°å°„ç”µå…‰åº¦ä¸é»‘æ´è´¨é‡ã€å…‰å­¦æ°§IIIçº¿å…‰åº¦ä»¥åŠç¡¬Xå°„çº¿å…‰åº¦ä¹‹é—´å­˜åœ¨ä¸­åº¦è‡³å¼ºç›¸å…³æ€§ï¼Œæš—ç¤ºç€æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNsï¼‰æ ¸å¿ƒçš„å¸¸è§èµ·æºã€‚è¿™äº›ç»“æœæä¾›äº†å…³äºé»‘æ´æ´»åŠ¨åŸºæœ¬å¹³é¢çš„æ–°è§è§£ï¼Œå¹¶æ”¯æŒåŒæ­¥è¾å°„è¿‡ç¨‹åœ¨ä½å…‰åº¦æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆLLAGNsï¼‰å°„ç”µå‘å°„ä¸­çš„ä½œç”¨ã€‚ </p>
<p><strong>å…³é”®å‘ç°åˆ—è¡¨</strong></p>
<ul>
<li>åˆ©ç”¨é«˜åˆ†è¾¨é˜µåˆ—è§‚æµ‹æŠ€æœ¯å¯¹é™„è¿‘æ˜Ÿç³»è¿›è¡Œäº†å¹¿æ³›çš„å°„ç”µè§‚æµ‹ã€‚</li>
<li>æˆåŠŸæ£€æµ‹åˆ°è¶…è¿‡ä¸‰åˆ†ä¹‹äºŒçš„è§‚æµ‹ç›®æ ‡ï¼Œæ­ç¤ºäº†å°„ç”µæºçš„é«˜æ£€æµ‹ç‡ã€‚</li>
<li>è§‚æµ‹åˆ°çš„å°„ç”µæºæ ¸å¿ƒç»“æ„æ˜¾è‘—ä¸”è§„æ¨¡è¾ƒå°ï¼Œä¸»è¦å‘ˆç°ç´§å‡‘çš„æ ¸å¿ƒå½¢æ€ã€‚</li>
<li>åœ¨éƒ¨åˆ†æ˜Ÿç³»ä¸­å‘ç°å•è¾¹æ ¸å¿ƒå–·å°„å½¢æ€ï¼Œå¹¶åœ¨NGC 2146ä¸­å‘ç°ç½•è§çš„åŒè¾¹å–·å°„ç»“æ„ã€‚</li>
<li>é™¤NGC 2655å¤–ï¼Œæ‰€æœ‰è§‚æµ‹çš„æ˜Ÿç³»å‡è¡¨ç°å‡ºä»¥æ ¸å¿ƒä¸ºä¸»çš„ç‰¹ç‚¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37f13c416b8a5586b8b6b3b9c1518ebc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="When-Segmentation-Meets-Hyperspectral-Image-New-Paradigm-for-Hyperspectral-Image-Classification"><a href="#When-Segmentation-Meets-Hyperspectral-Image-New-Paradigm-for-Hyperspectral-Image-Classification" class="headerlink" title="When Segmentation Meets Hyperspectral Image: New Paradigm for   Hyperspectral Image Classification"></a>When Segmentation Meets Hyperspectral Image: New Paradigm for   Hyperspectral Image Classification</h2><p><strong>Authors:Weilian Zhou, Weixuan Xie, Sei-ichiro Kamata, Man Sing Wong,  Huiying,  Hou, Haipeng Wang</strong></p>
<p>Hyperspectral image (HSI) classification is a cornerstone of remote sensing, enabling precise material and land-cover identification through rich spectral information. While deep learning has driven significant progress in this task, small patch-based classifiers, which account for over 90% of the progress, face limitations: (1) the small patch (e.g., 7x7, 9x9)-based sampling approach considers a limited receptive field, resulting in insufficient spatial structural information critical for object-level identification and noise-like misclassifications even within uniform regions; (2) undefined optimal patch sizes lead to coarse label predictions, which degrade performance; and (3) a lack of multi-shape awareness around objects. To address these challenges, we draw inspiration from large-scale image segmentation techniques, which excel at handling object boundaries-a capability essential for semantic labeling in HSI classification. However, their application remains under-explored in this task due to (1) the prevailing notion that larger patch sizes degrade performance, (2) the extensive unlabeled regions in HSI groundtruth, and (3) the misalignment of input shapes between HSI data and segmentation models. Thus, in this study, we propose a novel paradigm and baseline, HSIseg, for HSI classification that leverages segmentation techniques combined with a novel Dynamic Shifted Regional Transformer (DSRT) to overcome these challenges. We also introduce an intuitive progressive learning framework with adaptive pseudo-labeling to iteratively incorporate unlabeled regions into the training process, thereby advancing the application of segmentation techniques. Additionally, we incorporate auxiliary data through multi-source data collaboration, promoting better feature interaction. Validated on five public HSI datasets, our proposal outperforms state-of-the-art methods. </p>
<blockquote>
<p>é«˜å…‰è°±å›¾åƒï¼ˆHSIï¼‰åˆ†ç±»æ˜¯é¥æ„Ÿçš„æ ¸å¿ƒï¼Œå®ƒé€šè¿‡ä¸°å¯Œçš„å…‰è°±ä¿¡æ¯å®ç°ç²¾ç¡®çš„ææ–™å’ŒåœŸåœ°è¦†ç›–ç‰©è¯†åˆ«ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ åœ¨æ­¤ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åŸºäºå°å—çš„åˆ†ç±»å™¨å æ®äº†è¶…è¿‡90%çš„è¿›å±•ï¼Œä»é¢ä¸´ä¸€äº›å±€é™æ€§ï¼šï¼ˆ1ï¼‰åŸºäºå°å—ï¼ˆä¾‹å¦‚7x7ã€9x9ï¼‰çš„é‡‡æ ·æ–¹æ³•è€ƒè™‘äº†ä¸€ä¸ªæœ‰é™çš„æ„Ÿå—é‡ï¼Œå¯¼è‡´ç¼ºä¹å¯¹äºå¯¹è±¡çº§åˆ«è¯†åˆ«è‡³å…³é‡è¦çš„ç©ºé—´ç»“æ„ä¿¡æ¯ï¼Œç”šè‡³åœ¨å‡åŒ€åŒºåŸŸå†…å‡ºç°ç±»ä¼¼å™ªå£°çš„è¯¯åˆ†ç±»ï¼›ï¼ˆ2ï¼‰ä¸ç¡®å®šçš„æœ€ä½³å—å¤§å°å¯¼è‡´æ ‡ç­¾é¢„æµ‹ç²—ç³™ï¼Œä»è€Œé™ä½äº†æ€§èƒ½ï¼›ï¼ˆ3ï¼‰ç¼ºä¹å¯¹è±¡å‘¨å›´çš„å¤šå½¢çŠ¶æ„è¯†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»å¤§è§„æ¨¡å›¾åƒåˆ†å‰²æŠ€æœ¯ä¸­æ±²å–çµæ„Ÿï¼Œå®ƒä»¬æ“…é•¿å¤„ç†å¯¹è±¡è¾¹ç•Œâ€”â€”è¿™æ˜¯HSIåˆ†ç±»ä¸­è¯­ä¹‰æ ‡è®°æ‰€å¿…éœ€çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ­¤ä»»åŠ¡ä¸­çš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ï¼Œå› ä¸ºï¼ˆ1ï¼‰æ™®éè®¤ä¸ºè¾ƒå¤§çš„å—å¤§å°ä¼šé™ä½æ€§èƒ½ï¼Œï¼ˆ2ï¼‰HSIåŸºå‡†å›¾åƒä¸­å­˜åœ¨å¤§é‡æœªæ ‡è®°åŒºåŸŸï¼Œä»¥åŠï¼ˆ3ï¼‰HSIæ•°æ®ä¸åˆ†å‰²æ¨¡å‹ä¹‹é—´è¾“å…¥å½¢çŠ¶çš„ä¸åŒ¹é…ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼å’ŒåŸºçº¿æ–¹æ³•HSIsegï¼Œç”¨äºHSIåˆ†ç±»ï¼Œå®ƒç»“åˆäº†åˆ†å‰²æŠ€æœ¯ä¸ä¸€ç§æ–°çš„åŠ¨æ€ç§»ä½åŒºåŸŸè½¬æ¢å™¨ï¼ˆDSRTï¼‰æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§ç›´è§‚çš„æ¸è¿›å­¦ä¹ æ¡†æ¶ï¼Œå…·æœ‰è‡ªé€‚åº”ä¼ªæ ‡ç­¾ï¼Œå¯ä»¥é€æ­¥å°†æœªæ ‡è®°åŒºåŸŸçº³å…¥è®­ç»ƒè¿‡ç¨‹ï¼Œä»è€Œä¿ƒè¿›åˆ†å‰²æŠ€æœ¯çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¤šæºæ•°æ®åä½œèå…¥è¾…åŠ©æ•°æ®ï¼Œä¿ƒè¿›æ›´å¥½çš„ç‰¹å¾äº¤äº’ã€‚åœ¨äº”ä¸ªå…¬å…±HSIæ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œæˆ‘ä»¬çš„ææ¡ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12541v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é«˜å…‰è°±å›¾åƒåˆ†ç±»ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å°è¡¥ä¸åˆ†ç±»å™¨åœ¨ç©ºé—´ç»“æ„ä¿¡æ¯ä¸Šçš„ä¸è¶³ã€æ ‡ç­¾é¢„æµ‹ç²—ç³™å’Œç¼ºä¹å¤šå½¢çŠ¶æ„ŸçŸ¥èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œå€Ÿé‰´å¤§è§„æ¨¡å›¾åƒåˆ†å‰²æŠ€æœ¯ï¼Œç»“åˆåŠ¨æ€å¹³ç§»åŒºåŸŸå˜æ¢å™¨ï¼ˆDSRTï¼‰å’Œè‡ªé€‚åº”ä¼ªæ ‡ç­¾çš„æ¸è¿›å­¦ä¹ æ¡†æ¶ï¼Œæå‡ºäº†HSIsegè¿™ä¸€æ–°çš„é«˜å…‰è°±å›¾åƒåˆ†ç±»èŒƒå¼å’ŒåŸºçº¿æ–¹æ³•ã€‚åŒæ—¶å¼•å…¥å¤šæºæ•°æ®åä½œæ¥æé«˜ç‰¹å¾äº¤äº’æ•ˆæœï¼Œåœ¨äº”ä¸ªå…¬å¼€çš„é«˜å…‰è°±æ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é«˜å…‰è°±å›¾åƒåˆ†ç±»é¢ä¸´æŒ‘æˆ˜ï¼šå°è¡¥ä¸åˆ†ç±»å™¨åœ¨ç©ºé—´ç»“æ„ä¿¡æ¯ä¸Šçš„ä¸è¶³å¯¼è‡´å™ªå£°å¹²æ‰°å’Œè¯¯åˆ†ç±»ã€‚</li>
<li>å€Ÿé‰´å¤§è§„æ¨¡å›¾åƒåˆ†å‰²æŠ€æœ¯æ¥å¤„ç†å¯¹è±¡è¾¹ç•Œé—®é¢˜ï¼Œæœ‰åŠ©äºè§£å†³è¯­ä¹‰æ ‡æ³¨é—®é¢˜ã€‚</li>
<li>åŠ¨æ€å¹³ç§»åŒºåŸŸå˜æ¢å™¨ï¼ˆDSRTï¼‰åœ¨åˆ†å‰²æŠ€æœ¯çš„æ•´åˆä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚</li>
<li>æå‡ºæ¸è¿›å­¦ä¹ æ¡†æ¶å’Œè‡ªé€‚åº”ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œå°†æœªæ ‡è®°åŒºåŸŸçº³å…¥è®­ç»ƒè¿‡ç¨‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-24b390076b57ef9dae91c8e97ce84791.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3e0584f7ed8aea1b73ff49adfe1e934.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Analytical-Diagonalization-of-Fermi-Gas-like-Hamiltonians-using-the-Sommerfeld-Watson-Transformation"><a href="#Analytical-Diagonalization-of-Fermi-Gas-like-Hamiltonians-using-the-Sommerfeld-Watson-Transformation" class="headerlink" title="Analytical Diagonalization of Fermi Gas-like Hamiltonians using the   Sommerfeld-Watson Transformation"></a>Analytical Diagonalization of Fermi Gas-like Hamiltonians using the   Sommerfeld-Watson Transformation</h2><p><strong>Authors:G. Diniz, F. D. Picoli, M. P. Lenzarini</strong></p>
<p>The Sommerfeld-Watson transformation is a powerful mathematical technique widely used in physics to simplify summations over discrete quantum numbers by converting them into contour integrals in the complex plane. This method has applications in scattering theory, high-energy physics, quantum field theory, and electrostatics. A lesser-known but significant use is in the analytical diagonalization of specific Hamiltonians in condensed matter physics, such as the Fermi gas Hamiltonian and the single-impurity Anderson model with vanishing Coulomb repulsion. These models are used to describe important phenomena like conductance in metals, x-ray photoemission, and aspects of the Kondo problem. In this work, we provide a comprehensive explanation of the Sommerfeld-Watson transformation and its application in diagonalization procedures for these models, using modern notation to enhance clarity for new students. The analytical results were validated against the numerical diagonalization, showing excellent agreement. Furthermore, we extend the presented method to a more generalized non-interacting single-impurity Anderson model with variable couplings and arbitrary band dispersion. The procedure presented here successfully achieved the analytical diagonalization of this more complex model, providing a unified solution that encompasses simpler cases. To our knowledge, this general solution has not been previously reported. </p>
<blockquote>
<p>Sommerfeld-Watsonè½¬æ¢æ˜¯ä¸€ç§å¼ºå¤§çš„æ•°å­¦æŠ€æœ¯ï¼Œåœ¨ç‰©ç†å­¦ä¸­å¹¿æ³›åº”ç”¨ï¼Œé€šè¿‡å°†å…¶è½¬æ¢ä¸ºå¤å¹³é¢ä¸Šçš„è½®å»“ç§¯åˆ†æ¥ç®€åŒ–ç¦»æ•£é‡å­æ•°çš„æ±‚å’Œã€‚è¯¥æ–¹æ³•åœ¨æ•£å°„ç†è®ºã€é«˜èƒ½ç‰©ç†ã€é‡å­åœºè®ºå’Œé™ç”µå­¦ç­‰é¢†åŸŸæœ‰åº”ç”¨ã€‚å…¶è¾ƒå°‘ä¸ºäººçŸ¥ä½†é‡è¦çš„åº”ç”¨æ˜¯åœ¨å‡èšæ€ç‰©ç†å­¦ä¸­å¯¹ç‰¹å®šå“ˆå¯†é¡¿é‡çš„åˆ†æå¯¹è§’åŒ–ï¼Œä¾‹å¦‚è´¹ç±³æ°”ä½“å“ˆå¯†é¡¿é‡å’Œæ— åº“ä»‘æ’æ–¥çš„å•æ‚è´¨å®‰å¾·æ£®æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹ç”¨äºæè¿°é‡‘å±ä¸­çš„ç”µå¯¼ã€Xå°„çº¿å…‰ç”µå­å‘å°„å’Œåº·å¤šé—®é¢˜çš„æŸäº›æ–¹é¢ç­‰é‡è¦ç°è±¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç°ä»£ç¬¦å·ä½“ç³»ï¼Œä¸ºæ–°çš„å­¦ç”Ÿæä¾›æ›´æ¸…æ™°çš„è§£é‡Šï¼Œå…¨é¢è§£é‡Šäº†Sommerfeld-Watsonè½¬æ¢åŠå…¶åœ¨è¿™äº›æ¨¡å‹å¯¹è§’åŒ–ç¨‹åºä¸­çš„åº”ç”¨ã€‚åˆ†æçš„ç»“æœç»è¿‡ä¸æ•°å€¼å¯¹è§’åŒ–çš„éªŒè¯ï¼Œæ˜¾ç¤ºå‡ºæå¥½çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ‰€ä»‹ç»çš„æ–¹æ³•æ‰©å±•åˆ°äº†å…·æœ‰å¯å˜è€¦åˆå’Œä»»æ„å¸¦æ•£å°„çš„æ›´é€šç”¨çš„éç›¸äº’ä½œç”¨å•æ‚è´¨å®‰å¾·æ£®æ¨¡å‹ã€‚è¿™é‡Œä»‹ç»çš„ç¨‹åºæˆåŠŸåœ°å®ç°äº†è¿™ä¸ªæ›´å¤æ‚æ¨¡å‹çš„åˆ†æå¯¹è§’åŒ–ï¼Œæä¾›äº†ä¸€ä¸ªæ¶µç›–ç®€å•æƒ…å†µçš„ç»¼åˆè§£å†³æ–¹æ¡ˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™ä¸€é€šç”¨è§£å†³æ–¹æ¡ˆå°šæœªæœ‰æŠ¥é“è¿‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12402v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç´¢æœ«è²å°”å¾·-æ²ƒæ£®å˜æ¢æ˜¯ä¸€ç§å¼ºå¤§çš„æ•°å­¦æŠ€å·§ï¼Œå¹¿æ³›åº”ç”¨äºç‰©ç†å­¦ä¸­ï¼Œé€šè¿‡å°†å…¶è½¬æ¢ä¸ºå¤å¹³é¢ä¸Šçš„è½®å»“ç§¯åˆ†æ¥ç®€åŒ–ç¦»æ•£é‡å­æ•°çš„æ±‚å’Œã€‚æ­¤æ–¹æ³•åœ¨æ•£å°„ç†è®ºã€é«˜èƒ½ç‰©ç†ã€é‡å­åœºè®ºå’Œé™ç”µå­¦ä¸­éƒ½æœ‰åº”ç”¨ã€‚å®ƒåœ¨å‡èšæ€ç‰©ç†ä¸­å¯¹ç‰¹å®šå“ˆå¯†é¡¿é‡çš„è§£æå¯¹è§’åŒ–æ–¹é¢çš„åº”ç”¨è™½ç„¶é²œä¸ºäººçŸ¥ï¼Œä½†ä¹Ÿæ˜¯é‡è¦çš„åº”ç”¨ä¹‹ä¸€ï¼Œå¦‚è´¹ç±³æ°”ä½“å“ˆå¯†é¡¿é‡å’Œæ— åº“ä»‘æ’æ–¥çš„å•æ‚è´¨å®‰å¾·æ£®æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹è¢«ç”¨æ¥æè¿°é‡‘å±ä¸­çš„å¯¼ç”µç°è±¡ã€Xå°„çº¿å…‰ç”µå­å‘å°„å’Œåº·å¤šé—®é¢˜çš„æŸäº›æ–¹é¢ã€‚æœ¬æ–‡æä¾›äº†ç´¢æœ«è²å°”å¾·-æ²ƒæ£®å˜æ¢åŠå…¶åœ¨è¿™äº›æ¨¡å‹å¯¹è§’åŒ–ç¨‹åºä¸­çš„åº”ç”¨çš„ç»¼åˆè§£é‡Šï¼Œå¹¶ä½¿ç”¨ç°ä»£ç¬¦å·æ¥æé«˜æ–°å­¦ç”Ÿçš„æ¸…æ™°åº¦ã€‚è§£æç»“æœä¸æ•°å€¼å¯¹è§’åŒ–çš„éªŒè¯ç»“æœå»åˆè‰¯å¥½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ‰€ä»‹ç»çš„æ–¹æ³•æ‰©å±•åˆ°äº†æ›´é€šç”¨çš„å…·æœ‰å¯å˜è€¦åˆå’Œä»»æ„å¸¦è‰²æ•£çš„éç›¸äº’ä½œç”¨å•æ‚è´¨å®‰å¾·æ£®æ¨¡å‹ã€‚æœ¬æ–‡æ‰€ä»‹ç»çš„ç¨‹åºæˆåŠŸåœ°å®ç°äº†è¿™ä¸€æ›´å¤æ‚æ¨¡å‹çš„è§£æå¯¹è§’åŒ–ï¼Œæä¾›äº†ä¸€ä¸ªæ¶µç›–ç®€å•æƒ…å†µçš„ç»¼åˆè§£å†³æ–¹æ¡ˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™ç§ä¸€èˆ¬è§£å†³æ–¹æ¡ˆä»¥å‰å°šæœªæŠ¥é“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code>* ç´¢æœ«è²å°”å¾·-æ²ƒæ£®å˜æ¢æ˜¯ä¸€ç§å¼ºå¤§çš„æ•°å­¦æŠ€å·§ï¼Œå¹¿æ³›åº”ç”¨äºç‰©ç†å­¦ä¸­çš„å¤šä¸ªé¢†åŸŸã€‚
* æ­¤æ–¹æ³•èƒ½å¤Ÿç®€åŒ–ç¦»æ•£é‡å­æ•°çš„æ±‚å’Œï¼Œé€šè¿‡å°†å®ƒä»¬è½¬æ¢ä¸ºå¤å¹³é¢ä¸Šçš„è½®å»“ç§¯åˆ†ã€‚
* ç´¢æœ«è²å°”å¾·-æ²ƒæ£®å˜æ¢åœ¨å‡èšæ€ç‰©ç†ä¸­å¯¹ç‰¹å®šå“ˆå¯†é¡¿é‡çš„è§£æå¯¹è§’åŒ–æ–¹é¢æœ‰é‡è¦åº”ç”¨ã€‚
* æ–‡ç« æåˆ°äº†ä¸¤ä¸ªå…·ä½“çš„æ¨¡å‹ï¼šè´¹ç±³æ°”ä½“å“ˆå¯†é¡¿é‡å’Œå•æ‚è´¨å®‰å¾·æ£®æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨æè¿°ç‰©ç†ç°è±¡æ–¹é¢å…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼ã€‚
* æ–‡ç« æä¾›äº†ç´¢æœ«è²å°”å¾·-æ²ƒæ£®å˜æ¢çš„ç»¼åˆè§£é‡Šï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å¯¹è§’åŒ–ç¨‹åºä¸­çš„åº”ç”¨ã€‚
* è§£æç»“æœä¸æ•°å€¼å¯¹è§’åŒ–çš„éªŒè¯ç»“æœä¸€è‡´ï¼Œè¯æ˜äº†æ–¹æ³•çš„å¯é æ€§ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cefeb8daeca197d2b184d5bd46eac0f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac4f25df2df752b9dc0e7edfc75bf9eb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Modelling-the-impact-of-Multi-Cancer-Early-Detection-tests-a-review-of-natural-history-of-disease-models"><a href="#Modelling-the-impact-of-Multi-Cancer-Early-Detection-tests-a-review-of-natural-history-of-disease-models" class="headerlink" title="Modelling the impact of Multi Cancer Early Detection tests: a review of   natural history of disease models"></a>Modelling the impact of Multi Cancer Early Detection tests: a review of   natural history of disease models</h2><p><strong>Authors:O Mandrik, S Whyte, N Kunst, A Rayner, M Harden, S Dias, K Payne, S Palmer, MO Soares</strong></p>
<p>Introduction: The potential for multi-cancer early detection (MCED) tests to detect cancer at earlier stages is currently being evaluated in screening clinical trials. Once trial evidence becomes available, modelling will be necessary to predict impacts on final outcomes (benefits and harms), account for heterogeneity in determining clinical and cost-effectiveness, and explore alternative screening programme specifications. The natural history of disease (NHD) component of a MCED model will use statistical, mathematical or calibration methods. Methods: Modelling approaches for MCED screening that include an NHD component were identified from the literature, reviewed and critically appraised. Purposively selected (non-MCED) cancer screening models were also reviewed. The appraisal focussed on the scope, data sources, evaluation approaches and the structure and parameterisation of the models. Results: Five different MCED NHD models were identified and reviewed, alongside four additional (non-MCED) models. The critical appraisal highlighted several features of this literature. In the absence of trial evidence, MCED effects are based on predictions derived from test accuracy. These predictions rely on simplifying assumptions with unknown impacts, such as the stage-shift assumption used to estimate mortality impacts from predicted stage-shifts. None of the MCED models fully characterised uncertainty in the NHD or examined uncertainty in the stage-shift assumption. Conclusion: MCED technologies are developing rapidly, and large and costly clinical studies are being designed and implemented across the globe. Currently there is no modelling approach that can integrate clinical study evidence and therefore, in support of policy, it is important that similar efforts are made in the development of MCED models that make best use of the available data on benefits and harms. </p>
<blockquote>
<p>å¼•è¨€ï¼šå¤šç™Œæ—©æœŸæ£€æµ‹ï¼ˆMCEDï¼‰è¯•éªŒåœ¨ç­›æŸ¥ä¸´åºŠè¯•éªŒä¸­è¯„ä¼°æ£€æµ‹ç™Œç—‡æ—©æœŸé˜¶æ®µçš„å¯èƒ½æ€§ã€‚ä¸€æ—¦è¯•éªŒè¯æ®å¯ç”¨ï¼Œå»ºæ¨¡å°†å¿…ä¸å¯å°‘ï¼Œä»¥é¢„æµ‹å¯¹æœ€ç»ˆç»“æœï¼ˆåˆ©ç›Šå’Œå±å®³ï¼‰çš„å½±å“ï¼Œè€ƒè™‘ä¸´åºŠå’Œæˆæœ¬æ•ˆç›Šçš„å¼‚è´¨æ€§ï¼Œå¹¶æ¢ç´¢æ›¿ä»£ç­›æŸ¥æ–¹æ¡ˆè§„æ ¼ã€‚MCEDæ¨¡å‹çš„ç–¾ç—…è‡ªç„¶å²ï¼ˆNHDï¼‰éƒ¨åˆ†å°†ä½¿ç”¨ç»Ÿè®¡ã€æ•°å­¦æˆ–æ ¡å‡†æ–¹æ³•ã€‚æ–¹æ³•ï¼šä»æ–‡çŒ®ä¸­ç¡®å®šäº†åŒ…å«NHDéƒ¨åˆ†çš„MCEDç­›æŸ¥å»ºæ¨¡æ–¹æ³•ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†è¯„è®ºå’Œæ‰¹åˆ¤æ€§è¯„ä¼°ã€‚è¿˜å®¡æŸ¥äº†æœ‰æ„é€‰æ‹©çš„ï¼ˆéMCEDï¼‰ç™Œç—‡ç­›æŸ¥æ¨¡å‹ã€‚è¯„ä¼°çš„é‡ç‚¹æ˜¯æ¨¡å‹çš„èŒƒå›´ã€æ•°æ®æ¥æºã€è¯„ä¼°æ–¹æ³•ä»¥åŠæ¨¡å‹å’Œå‚æ•°çš„ç»“æ„ã€‚ç»“æœï¼šè¯†åˆ«å¹¶å®¡æŸ¥äº†5ç§ä¸åŒçš„MCED NHDæ¨¡å‹ä»¥åŠ4ç§é¢å¤–çš„ï¼ˆéMCEDï¼‰æ¨¡å‹ã€‚æ‰¹åˆ¤æ€§è¯„ä¼°çªå‡ºäº†è¿™ç¯‡æ–‡çŒ®çš„å‡ ä¸ªç‰¹ç‚¹ã€‚åœ¨æ²¡æœ‰è¯•éªŒè¯æ®çš„æƒ…å†µä¸‹ï¼ŒMCEDçš„æ•ˆæœæ˜¯åŸºäºæµ‹è¯•å‡†ç¡®åº¦çš„é¢„æµ‹ã€‚è¿™äº›é¢„æµ‹ä¾èµ–äºç®€åŒ–å‡è®¾ï¼Œå…¶å½±å“æœªçŸ¥ï¼Œä¾‹å¦‚ç”¨äºä¼°è®¡é¢„æœŸé˜¶æ®µè½¬å˜å¯¹æ­»äº¡ç‡å½±å“çš„é˜¶æ®µè½¬å˜å‡è®¾ã€‚æ²¡æœ‰ä¸€ä¸ªMCEDæ¨¡å‹èƒ½å®Œå…¨åˆ»ç”»NHDä¸­çš„ä¸ç¡®å®šæ€§ï¼Œä¹Ÿæ²¡æœ‰ä¸€ä¸ªæ¨¡å‹èƒ½æ£€éªŒé˜¶æ®µè½¬å˜å‡è®¾ä¸­çš„ä¸ç¡®å®šæ€§ã€‚ç»“è®ºï¼šMCEDæŠ€æœ¯æ­£åœ¨è¿…é€Ÿå‘å±•ï¼Œå…¨çƒèŒƒå›´å†…æ­£åœ¨è®¾è®¡å’Œå®æ–½å¤§è§„æ¨¡ä¸”æ˜‚è´µçš„ä¸´åºŠç ”ç©¶ã€‚ç›®å‰å°šæ— å»ºæ¨¡æ–¹æ³•èƒ½å¤Ÿæ•´åˆä¸´åºŠç ”ç©¶è¯æ®ï¼Œå› æ­¤ï¼Œä¸ºäº†æ”¯æŒæ”¿ç­–åˆ¶å®šï¼Œåœ¨å¼€å‘MCEDæ¨¡å‹æ–¹é¢åšå‡ºç±»ä¼¼åŠªåŠ›è‡³å…³é‡è¦ï¼Œè¿™äº›æ¨¡å‹åº”æœ€å¤§é™åº¦åœ°åˆ©ç”¨å…³äºåˆ©ç›Šå’Œå±å®³çš„å¯ç”¨æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šç™Œç—‡æ—©æœŸæ£€æµ‹ï¼ˆMCEDï¼‰è¯•éªŒåœ¨ç­›æŸ¥ä¸´åºŠè¯•éªŒä¸­æ£€æµ‹ç™Œç—‡æ—©æœŸé˜¶æ®µçš„æ½œåŠ›ã€‚æ–‡ç« æŒ‡å‡ºï¼Œåœ¨è¯•éªŒè¯æ®å¯ç”¨åï¼Œå»ºæ¨¡æ˜¯å¿…è¦çš„ï¼Œä»¥é¢„æµ‹å¯¹æœ€ç»ˆç»“å±€çš„å½±å“ï¼ˆåŒ…æ‹¬åˆ©ç›Šå’Œå±å®³ï¼‰ï¼Œå¹¶æ¢è®¨äº†ä¸åŒç­›æŸ¥æ–¹æ¡ˆè§„æ ¼çš„é—®é¢˜ã€‚æ–‡ç« é€šè¿‡æ–‡çŒ®ç»¼è¿°å’Œè¯„ä¼°äº†MCEDç­›æŸ¥çš„å»ºæ¨¡æ–¹æ³•ï¼Œé‡ç‚¹å…³æ³¨äº†æ¨¡å‹çš„èŒƒå›´ã€æ•°æ®æ¥æºã€è¯„ä¼°æ–¹æ³•ç­‰ã€‚ç„¶è€Œï¼Œå½“å‰MCEDæ¨¡å‹å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶ç—…å²ï¼ˆNHDï¼‰æ–¹é¢ï¼Œå¹¶ä¸”ç¼ºä¹å¯¹é˜¶æ®µè½¬ç§»å‡è®¾çš„è€ƒå¯Ÿã€‚å› æ­¤ï¼Œéœ€è¦å‘å±•èƒ½æ›´å¥½åœ°åˆ©ç”¨å¯ç”¨æ•°æ®çš„MCEDæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MCEDè¯•éªŒå…·æœ‰åœ¨è¾ƒæ—©é˜¶æ®µæ£€æµ‹ç™Œç—‡çš„æ½œåŠ›ï¼Œç›®å‰æ­£åœ¨ç­›é€‰ä¸´åºŠè¯•éªŒä¸­è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>åœ¨è¯•éªŒè¯æ®å¯ç”¨åï¼Œå»ºæ¨¡æ˜¯å¿…è¦çš„ï¼Œä»¥é¢„æµ‹å¯¹æœ€ç»ˆç»“å±€çš„å½±å“ï¼Œå¹¶è€ƒè™‘ä¸´åºŠå’Œæˆæœ¬æ•ˆç›Šçš„å¼‚è´¨æ€§ã€‚</li>
<li>MCEDæ¨¡å‹çš„è‡ªç„¶ç—…å²ï¼ˆNHDï¼‰éƒ¨åˆ†å°†ä½¿ç”¨ç»Ÿè®¡ã€æ•°å­¦æˆ–æ ¡å‡†æ–¹æ³•ã€‚</li>
<li>æ–‡çŒ®ç»¼è¿°å’Œè¯„ä¼°å‘ç°MCEDç­›æŸ¥çš„å»ºæ¨¡æ–¹æ³•å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶ç—…å²æ–¹é¢ã€‚</li>
<li>MCEDæ•ˆåº”åŸºäºæµ‹è¯•å‡†ç¡®æ€§çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹ä¾èµ–äºæœªçŸ¥å½±å“çš„ç®€åŒ–å‡è®¾ï¼Œå¦‚é˜¶æ®µè½¬ç§»å‡è®¾ã€‚</li>
<li>ç›®å‰çš„MCEDæ¨¡å‹æœªèƒ½å……åˆ†åˆ»ç”»è‡ªç„¶ç—…å²çš„ä¸ç¡®å®šæ€§ï¼Œä¹Ÿæœªè€ƒå¯Ÿé˜¶æ®µè½¬ç§»å‡è®¾çš„ä¸ç¡®å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b0ebf0d5f80e5cc8ca638b70806e9be.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Data-Efficient-Limited-Angle-CT-Using-Deep-Priors-and-Regularization"><a href="#Data-Efficient-Limited-Angle-CT-Using-Deep-Priors-and-Regularization" class="headerlink" title="Data-Efficient Limited-Angle CT Using Deep Priors and Regularization"></a>Data-Efficient Limited-Angle CT Using Deep Priors and Regularization</h2><p><strong>Authors:Ilmari Vahteristo, Zhi-Song Liu, Andreas Rupp</strong></p>
<p>Reconstructing an image from its Radon transform is a fundamental computed tomography (CT) task arising in applications such as X-ray scans. In many practical scenarios, a full 180-degree scan is not feasible, or there is a desire to reduce radiation exposure. In these limited-angle settings, the problem becomes ill-posed, and methods designed for full-view data often leave significant artifacts. We propose a very low-data approach to reconstruct the original image from its Radon transform under severe angle limitations. Because the inverse problem is ill-posed, we combine multiple regularization methods, including Total Variation, a sinogram filter, Deep Image Prior, and a patch-level autoencoder. We use a differentiable implementation of the Radon transform, which allows us to use gradient-based techniques to solve the inverse problem. Our method is evaluated on a dataset from the Helsinki Tomography Challenge 2022, where the goal is to reconstruct a binary disk from its limited-angle sinogram. We only use a total of 12 data pointsâ€“eight for learning a prior and four for hyperparameter selectionâ€“and achieve results comparable to the best synthetic data-driven approaches. </p>
<blockquote>
<p>ä»Radonå˜æ¢é‡å»ºå›¾åƒæ˜¯è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­çš„ä¸€ä¸ªåŸºæœ¬ä»»åŠ¡ï¼Œå‡ºç°åœ¨Xå°„çº¿æ‰«æç­‰åº”ç”¨ä¸­ã€‚åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­ï¼Œå®Œæ•´çš„180åº¦æ‰«æå¹¶ä¸å¯è¡Œï¼Œæˆ–è€…å­˜åœ¨å‡å°‘è¾å°„æš´éœ²çš„éœ€æ±‚ã€‚åœ¨è¿™äº›æœ‰é™è§’åº¦çš„è®¾ç½®ä¸‹ï¼Œé—®é¢˜å˜å¾—ä¸é€‚å®šï¼Œä¸“ä¸ºå…¨è§†è§’æ•°æ®è®¾è®¡çš„æ–¹æ³•é€šå¸¸ä¼šäº§ç”Ÿæ˜æ˜¾çš„ä¼ªå½±ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨ä¸¥é‡è§’åº¦é™åˆ¶ä¸‹ä»Radonå˜æ¢é‡å»ºåŸå§‹å›¾åƒçš„ä½æ•°æ®æ–¹æ³•ã€‚ç”±äºåé—®é¢˜æ˜¯é€‚å®šçš„ï¼Œæˆ‘ä»¬ç»“åˆäº†å¤šç§æ­£åˆ™åŒ–æ–¹æ³•ï¼ŒåŒ…æ‹¬å…¨å˜å·®ã€è¾›å›¾æ»¤æ³¢å™¨ã€æ·±åº¦å›¾åƒå…ˆéªŒå’Œè¡¥ä¸çº§åˆ«çš„è‡ªåŠ¨ç¼–ç å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨Radonå˜æ¢çš„å¯å¾®å®ç°ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨åŸºäºæ¢¯åº¦çš„æ–¹æ³•æ¥è§£å†³åé—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨èµ«å°”è¾›åŸºæ–­å±‚æ‰«ææŒ‘æˆ˜èµ›2022çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯¥æ•°æ®é›†çš„ç›®æ ‡æ˜¯ä»å…¶æœ‰é™çš„è§’è¾›å›¾é‡å»ºä¸€ä¸ªäºŒè¿›åˆ¶ç£ç›˜ã€‚æˆ‘ä»¬åªä½¿ç”¨äº†æ€»å…±12ä¸ªæ•°æ®ç‚¹â€”â€”8ä¸ªç”¨äºå­¦ä¹ å…ˆéªŒçŸ¥è¯†ï¼Œ4ä¸ªç”¨äºè¶…å‚æ•°é€‰æ‹©â€”â€”å¹¶å–å¾—äº†ä¸æœ€ä½³åˆæˆæ•°æ®é©±åŠ¨æ–¹æ³•ç›¸å½“çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12293v1">PDF</a> 12 pages, 2 reference pages, 5 figures, submitted to SCIA 2024</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åœ¨CTæ‰«æï¼ˆè®¡ç®—æœºæ–­å±‚æ‰«æï¼‰ä¸­ï¼Œå¦‚ä½•ä»Radonå˜æ¢é‡å»ºå›¾åƒçš„é—®é¢˜ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç”±äºè§’åº¦é™åˆ¶æˆ–è¾å°„æš´éœ²é—®é¢˜ï¼Œé€šå¸¸æ— æ³•å®Œæˆå®Œæ•´çš„180åº¦æ‰«æã€‚ä½œè€…æå‡ºäº†ä¸€ç§åœ¨ä¸¥é‡è§’åº¦é™åˆ¶ä¸‹ï¼Œåˆ©ç”¨æå°‘é‡æ•°æ®ä»Radonå˜æ¢é‡å»ºåŸå§‹å›¾åƒçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¤šç§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨å¯å¾®åˆ†çš„Radonå˜æ¢å®ç°ï¼Œä»¥æ¢¯åº¦ä¸ºåŸºç¡€è§£å†³åé—®é¢˜ã€‚åœ¨Helsinki Tomography Challenge 2022çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œä»…ä½¿ç”¨æ€»è®¡12ä¸ªæ•°æ®ç‚¹ï¼ˆ8ä¸ªç”¨äºå­¦ä¹ å…ˆéªŒçŸ¥è¯†ï¼Œ4ä¸ªç”¨äºé€‰æ‹©è¶…å‚æ•°ï¼‰å³å¯è·å¾—ä¸æœ€ä½³åˆæˆæ•°æ®é©±åŠ¨æ–¹æ³•ç›¸å½“çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–‡æœ¬è®¨è®ºäº†CTæ‰«æä¸­ä»Radonå˜æ¢é‡å»ºå›¾åƒçš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™è§’åº¦ä¸‹çš„æŒ‘æˆ˜ã€‚</li>
<li>ä½œè€…æå‡ºäº†ä¸€ç§åœ¨ä¸¥é‡è§’åº¦é™åˆ¶ä¸‹ï¼Œåˆ©ç”¨éå¸¸æœ‰é™çš„æ•°æ®è¿›è¡Œå›¾åƒé‡å»ºçš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†å¤šç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä»¥æ”¹å–„åœ¨æœ‰é™è§’åº¦ä¸‹çš„å›¾åƒé‡å»ºè´¨é‡ã€‚</li>
<li>ä½¿ç”¨å¯å¾®åˆ†çš„Radonå˜æ¢å®ç°ï¼Œä¾¿äºä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>ä»…åœ¨Helsinki Tomography Challenge 2022çš„æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨å°‘é‡æ•°æ®ç‚¹ï¼ˆæ€»è®¡12ä¸ªï¼‰å³å¯å®ç°ä¸æœ€ä½³æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…ç”¨äºå­¦ä¹ å…ˆéªŒçŸ¥è¯†ï¼Œè¿˜ç”¨äºè¶…å‚æ•°çš„é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12293">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f1e6532f8b0de928dce096b04f9ee07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5610d74ba60455e0339750a7a8e1e81e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fd78d7f9622c23b9e99204ebecdb88c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ClusMFL-A-Cluster-Enhanced-Framework-for-Modality-Incomplete-Multimodal-Federated-Learning-in-Brain-Imaging-Analysis"><a href="#ClusMFL-A-Cluster-Enhanced-Framework-for-Modality-Incomplete-Multimodal-Federated-Learning-in-Brain-Imaging-Analysis" class="headerlink" title="ClusMFL: A Cluster-Enhanced Framework for Modality-Incomplete Multimodal   Federated Learning in Brain Imaging Analysis"></a>ClusMFL: A Cluster-Enhanced Framework for Modality-Incomplete Multimodal   Federated Learning in Brain Imaging Analysis</h2><p><strong>Authors:Xinpeng Wang, Rong Zhou, Han Xie, Xiaoying Tang, Lifang He, Carl Yang</strong></p>
<p>Multimodal Federated Learning (MFL) has emerged as a promising approach for collaboratively training multimodal models across distributed clients, particularly in healthcare domains. In the context of brain imaging analysis, modality incompleteness presents a significant challenge, where some institutions may lack specific imaging modalities (e.g., PET, MRI, or CT) due to privacy concerns, device limitations, or data availability issues. While existing work typically assumes modality completeness or oversimplifies missing-modality scenarios, we simulate a more realistic setting by considering both client-level and instance-level modality incompleteness in this study. Building on this realistic simulation, we propose ClusMFL, a novel MFL framework that leverages feature clustering for cross-institutional brain imaging analysis under modality incompleteness. Specifically, ClusMFL utilizes the FINCH algorithm to construct a pool of cluster centers for the feature embeddings of each modality-label pair, effectively capturing fine-grained data distributions. These cluster centers are then used for feature alignment within each modality through supervised contrastive learning, while also acting as proxies for missing modalities, allowing cross-modal knowledge transfer. Furthermore, ClusMFL employs a modality-aware aggregation strategy, further enhancing the modelâ€™s performance in scenarios with severe modality incompleteness. We evaluate the proposed framework on the ADNI dataset, utilizing structural MRI and PET scans. Extensive experimental results demonstrate that ClusMFL achieves state-of-the-art performance compared to various baseline methods across varying levels of modality incompleteness, providing a scalable solution for cross-institutional brain imaging analysis. </p>
<blockquote>
<p>å¤šæ¨¡æ€è”é‚¦å­¦ä¹ ï¼ˆMFLï¼‰å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œç”¨äºåœ¨åˆ†å¸ƒå¼å®¢æˆ·ç«¯ä¸ŠååŒè®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸã€‚åœ¨è„‘æˆåƒåˆ†æçš„èƒŒæ™¯ä¸‹ï¼Œæ¨¡æ€ä¸å®Œæ•´æ€§é—®é¢˜æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œä¸€äº›æœºæ„å¯èƒ½ä¼šå› éšç§æ‹…å¿§ã€è®¾å¤‡é™åˆ¶æˆ–æ•°æ®å¯ç”¨æ€§ç­‰é—®é¢˜è€Œç¼ºå°‘ç‰¹å®šçš„æˆåƒæ¨¡æ€ï¼ˆä¾‹å¦‚PETã€MRIæˆ–CTï¼‰ã€‚è™½ç„¶ç°æœ‰å·¥ä½œé€šå¸¸å‡è®¾æ¨¡æ€å®Œæ•´æ€§æˆ–è¿‡äºç®€åŒ–ç¼ºå¤±æ¨¡æ€åœºæ™¯ï¼Œæœ¬ç ”ç©¶åœ¨è€ƒè™‘å®¢æˆ·ç«¯å’Œå®ä¾‹çº§æ¨¡æ€ä¸å®Œæ•´æ€§çš„æƒ…å†µä¸‹ï¼Œæ¨¡æ‹Ÿäº†ä¸€ä¸ªæ›´ç°å®çš„ç¯å¢ƒã€‚åŸºäºè¿™ç§ç°å®æ¨¡æ‹Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ClusMFLï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„MFLæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç‰¹å¾èšç±»åœ¨æ¨¡æ€ä¸å®Œæ•´çš„æƒ…å†µä¸‹è¿›è¡Œè·¨æœºæ„è„‘æˆåƒåˆ†æã€‚å…·ä½“æ¥è¯´ï¼ŒClusMFLä½¿ç”¨FINCHç®—æ³•ä¸ºæ¯ç§æ¨¡æ€æ ‡ç­¾å¯¹æ„å»ºç‰¹å¾åµŒå…¥çš„ç°‡ä¸­å¿ƒæ± ï¼Œæœ‰æ•ˆæ•è·ç»†ç²’åº¦æ•°æ®åˆ†å¸ƒã€‚è¿™äº›ç°‡ä¸­å¿ƒéšåç”¨äºé€šè¿‡ç›‘ç£å¯¹æ¯”å­¦ä¹ è¿›è¡Œæ¯ç§æ¨¡æ€å†…çš„ç‰¹å¾å¯¹é½ï¼ŒåŒæ—¶ä½œä¸ºç¼ºå¤±æ¨¡æ€çš„ä»£ç†ï¼Œå®ç°è·¨æ¨¡æ€çŸ¥è¯†è½¬ç§»ã€‚æ­¤å¤–ï¼ŒClusMFLé‡‡ç”¨äº†ä¸€ç§æ¨¡æ€æ„ŸçŸ¥èšåˆç­–ç•¥ï¼Œè¿›ä¸€æ­¥æé«˜äº†åœ¨ä¸¥é‡æ¨¡æ€ä¸å®Œæ•´åœºæ™¯ä¸­çš„æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ADNIæ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ¡†æ¶ï¼Œåˆ©ç”¨ç»“æ„MRIå’ŒPETæ‰«æã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å„ç§åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒClusMFLåœ¨ä¸åŒç¨‹åº¦çš„æ¨¡æ€ä¸å®Œæ•´æ€§ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ºè·¨æœºæ„è„‘æˆåƒåˆ†ææä¾›äº†å¯ä¼¸ç¼©çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12180v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶è€ƒè™‘äº†åœ¨åŒ»ç–—é¢†åŸŸå°¤å…¶æ˜¯è„‘æˆåƒåˆ†æä¸­é¢ä¸´çš„å¤šæ¨¡æ€æ•°æ®ç¼ºå¤±é—®é¢˜ï¼Œæå‡ºäº†åŸºäºç‰¹å¾èšç±»çš„å¤šæ¨¡æ€è”é‚¦å­¦ä¹ æ¡†æ¶ClusMFLã€‚é’ˆå¯¹å®¢æˆ·ç«¯å’Œå®ä¾‹çº§åˆ«çš„æ¨¡æ€ç¼ºå¤±é—®é¢˜ï¼ŒClusMFLé€šè¿‡åˆ©ç”¨FINCHç®—æ³•æ„å»ºæ¨¡æ€æ ‡ç­¾å¯¹çš„ç‰¹å¾åµŒå…¥ç°‡ä¸­å¿ƒæ± ï¼Œå®ç°è·¨æœºæ„è„‘æˆåƒåˆ†æåœ¨æ¨¡æ€ç¼ºå¤±ä¸‹çš„ç²¾ç»†ç²’åº¦æ•°æ®å¤„ç†ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç›‘ç£å¯¹æ¯”å­¦ä¹ è¿›è¡Œç‰¹å¾å¯¹é½ï¼Œå¹¶é‡‡ç”¨æ¨¡æ€æ„ŸçŸ¥èšåˆç­–ç•¥ï¼Œæé«˜åœ¨ä¸¥é‡æ¨¡æ€ç¼ºå¤±åœºæ™¯ä¸‹çš„æ¨¡å‹æ€§èƒ½ã€‚åœ¨ADNIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒClusMFLåœ¨ä¸åŒæ°´å¹³çš„æ¨¡æ€ç¼ºå¤±æƒ…å†µä¸‹å‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€è”é‚¦å­¦ä¹ ï¼ˆMFLï¼‰æ˜¯å¤„ç†åˆ†å¸ƒå¼å®¢æˆ·ç«¯ä¸Šçš„å¤šæ¨¡æ€æ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸã€‚</li>
<li>ç°å®æƒ…å†µä¸‹å­˜åœ¨æ¨¡æ€ç¼ºå¤±é—®é¢˜ï¼Œæœ¬ç ”ç©¶è€ƒè™‘äº†å®¢æˆ·ç«¯å’Œå®ä¾‹çº§åˆ«çš„æ¨¡æ€ç¼ºå¤±ã€‚</li>
<li>ClusMFLåˆ©ç”¨ç‰¹å¾èšç±»è§£å†³æ¨¡æ€ç¼ºå¤±é—®é¢˜ï¼Œé€šè¿‡æ„å»ºæ¨¡æ€æ ‡ç­¾å¯¹çš„ç‰¹å¾åµŒå…¥ç°‡ä¸­å¿ƒæ± æ¥æ•æ‰ç²¾ç»†ç²’åº¦çš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>ClusMFLé‡‡ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ è¿›è¡Œç‰¹å¾å¯¹é½ï¼Œå¹¶ä½¿ç”¨è¿™äº›ç°‡ä¸­å¿ƒä½œä¸ºç¼ºå¤±æ¨¡æ€çš„ä»£ç†ï¼Œå®ç°è·¨æ¨¡æ€çŸ¥è¯†è½¬ç§»ã€‚</li>
<li>ClusMFLé‡‡ç”¨æ¨¡æ€æ„ŸçŸ¥èšåˆç­–ç•¥ï¼Œå¢å¼ºåœ¨ä¸¥é‡æ¨¡æ€ç¼ºå¤±æƒ…å†µä¸‹çš„æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨ADNIæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†ClusMFLçš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨ä¸åŒæ°´å¹³çš„æ¨¡æ€ç¼ºå¤±æƒ…å†µä¸‹å‡è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12180">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91b09b4debcc9b4dad20aed9b3ac1767.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4543902b3e2a9b1b7ec1a4f968d03d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21604b16eb137d7bfdf5c1e63d784551.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-720996b2ec36cdd66f40332467c7c8bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62505a70b758e7fece3ebeaca977e8c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80eed5f9164525dc31c70fd6372056c4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Coherent-Superconductor-Semiconductor-Epitaxy-for-Integrated-Quantum-Electronics"><a href="#Coherent-Superconductor-Semiconductor-Epitaxy-for-Integrated-Quantum-Electronics" class="headerlink" title="Coherent Superconductor-Semiconductor Epitaxy for Integrated Quantum   Electronics"></a>Coherent Superconductor-Semiconductor Epitaxy for Integrated Quantum   Electronics</h2><p><strong>Authors:Julian A. Steele, Patrick J. Strohbeen, Carla Verdi, Ardeshir Baktash, Alisa Danilenko, Yi-Hsun Chen, Jechiel van Dijk, Lianzhou Wang, Eugene Demler, Salva Salmani-Rezaie, Peter Jacobson, Javad Shabani</strong></p>
<p>Introducing superconductivity into group IV elements by doping has long promised a pathway to introduce quantum functionalities into well-established semiconductor technologies. The non-equilibrium hyperdoping of group III atoms into Si or Ge has successfully shown superconductivity can be achieved, however, the origin of superconductivity has been obscured by structural disorder and dopant clustering. Here, we report the epitaxial growth of hyperdoped Ga:Ge films by molecular beam epitaxy with extreme hole concentrations ($n_\textup{h} &#x3D; 4.15 \times 10^{21}$<del>cm$^{-3}$, ~17.9% Ga substitution) that yield superconductivity with a critical temperature of $T_{\textup{c}} &#x3D; 3.5$</del>K and an out-of-plane critical field of 1<del>T at 270</del>mK. Synchrotron-based X-ray absorption and scattering methods reveal that Ga dopants are substitutionally incorporated within the Ge lattice, introducing a tetragonal distortion to the crystal unit cell. Our findings, corroborated by first-principles calculations, suggest that the structural order of Ga dopants creates a narrow band for the emergence of superconductivity in Ge, establishing hyperdoped Ga:Ge as a low-disorder, epitaxial superconductor-semiconductor platform. </p>
<blockquote>
<p>å°†è¶…å¯¼æ€§å¼•å…¥ç¬¬å››æ—å…ƒç´ é€šè¿‡æºæ‚é•¿æœŸä»¥æ¥ä¸ºå°†é‡å­åŠŸèƒ½å¼•å…¥æˆç†Ÿçš„åŠå¯¼ä½“æŠ€æœ¯ä¸­å¼€è¾Ÿäº†ä¸€æ¡é“è·¯ã€‚ç¬¬ä¸‰æ—åŸå­å¯¹Siæˆ–Geçš„éå¹³è¡¡è¶…æºæ‚å·²æˆåŠŸè¯æ˜å¯ä»¥å®ç°è¶…å¯¼æ€§ã€‚ç„¶è€Œï¼Œç”±äºç»“æ„æ— åºå’Œæºæ‚å‰‚èšé›†ï¼Œè¶…å¯¼æ€§çš„èµ·æºå˜å¾—æ¨¡ç³Šä¸æ¸…ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†é€šè¿‡åˆ†å­æŸå¤–å»¶æ–¹æ³•å®ç°è¶…æºæ‚Ga:Geè–„è†œçš„å¤–å»¶ç”Ÿé•¿ï¼Œå…·æœ‰æé«˜çš„ç©ºç©´æµ“åº¦ï¼ˆnh&#x3D; 4.15Ã—10^21 cm^-3ï¼Œçº¦17.9%çš„Gaæ›¿ä»£ï¼‰ï¼Œäº§ç”Ÿè¶…å¯¼æ€§ï¼Œä¸´ç•Œæ¸©åº¦Tc&#x3D; 3.5 Kï¼Œåœ¨270 mKæ—¶çš„é¢å¤–ä¸´ç•Œåœºä¸º1 Tã€‚åŸºäºåŒæ­¥è¾å°„çš„Xå°„çº¿å¸æ”¶å’Œæ•£å°„æ–¹æ³•æ˜¾ç¤ºï¼ŒGaæºæ‚å‰‚è¢«æ›¿ä»£æ€§åœ°æºå…¥Geæ™¶æ ¼ä¸­ï¼Œä¸ºæ™¶èƒå¼•å…¥äº†å››æ–¹ç•¸å˜ã€‚æˆ‘ä»¬çš„å‘ç°å¾—åˆ°äº†ç¬¬ä¸€æ€§åŸç†è®¡ç®—çš„è¯å®ï¼Œè¡¨æ˜Gaæºæ‚å‰‚çš„ç»“æ„é¡ºåºä¸ºGeä¸­å‡ºç°è¶…å¯¼æ€§å¼€è¾Ÿäº†ä¸€ä¸ªç‹­çª„çš„é€šé“ï¼Œç¡®ç«‹äº†è¶…æºæ‚Ga:Geä½œä¸ºä½æ— åºã€å¤–å»¶çš„è¶…å¯¼ä½“-åŠå¯¼ä½“å¹³å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15421v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æºæ‚IIIæ—åŸå­è¿›å…¥ç¡…æˆ–é”—çš„éå¹³è¡¡è¶…æºæ‚å·²æˆåŠŸå®ç°è¶…å¯¼æ€§ï¼Œä½†ç»“æ„æ— åºå’Œæºæ‚å‰‚èšé›†ä½¿å¾—è¶…å¯¼æ€§çš„èµ·æºå˜å¾—æ¨¡ç³Šã€‚æœ¬ç ”ç©¶æŠ¥å‘Šé€šè¿‡åˆ†å­æŸå¤–å»¶æŠ€æœ¯å¤–å»¶ç”Ÿé•¿è¶…æºæ‚Ga:Geè–„è†œï¼Œå®ç°äº†æç«¯ç©ºç©´æµ“åº¦ä¸‹çš„è¶…å¯¼æ€§ï¼Œå…³é”®æ¸©åº¦è¾¾åˆ°3.5Kï¼Œå¹¶åœ¨æä½æ¸©åº¦ä¸‹å…·æœ‰ä¸´ç•Œåœºå¼ºåº¦ã€‚åŒæ­¥è¾å°„Xå°„çº¿å¸æ”¶å’Œæ•£å°„æ–¹æ³•æ­ç¤ºGaæºæ‚å‰‚åœ¨Geæ™¶æ ¼ä¸­çš„æ›¿ä»£ä½ç½®ï¼Œå¼•å…¥äº†å››æ–¹æ™¶èƒç•¸å˜ã€‚æœ¬ç ”ç©¶å»ºç«‹è¶…æºæ‚Ga:Geä½œä¸ºä½æ— åºã€å¤–å»¶è¶…å¯¼åŠå¯¼ä½“å¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šè¿‡æºæ‚IIIæ—åŸå­è¿›å…¥é›†å›¢IVå…ƒç´ ï¼ˆå¦‚Siæˆ–Geï¼‰å¯ä»¥å®ç°è¶…å¯¼æ€§ã€‚</li>
<li>è¶…æºæ‚æŠ€æœ¯æˆåŠŸåœ¨Ga:Geè–„è†œä¸­å®ç°è¶…å¯¼æ€§ï¼Œå…³é”®æ¸©åº¦è¾¾åˆ°3.5Kã€‚</li>
<li>Gaæºæ‚å‰‚åœ¨Geæ™¶æ ¼ä¸­çš„ä½ç½®å¯¹è¶…å¯¼æ€§çš„å‡ºç°æœ‰é‡è¦å½±å“ï¼Œå¼•å…¥å››æ–¹æ™¶èƒç•¸å˜ã€‚</li>
<li>é«˜æµ“åº¦çš„Gaæºæ‚å‰‚ï¼ˆ$n_\text{h} &#x3D; 4.15 \times 10^{21}$ cm$^{-3}$ï¼‰å¯¹è¶…å¯¼æ€§èƒ½çš„æå‡èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>é€šè¿‡åˆ†å­æŸå¤–å»¶æŠ€æœ¯å®ç°äº†æç«¯ç©ºç©´æµ“åº¦ä¸‹çš„Ga:Geè–„è†œçš„å¤–å»¶ç”Ÿé•¿ã€‚</li>
<li>åŒæ­¥è¾å°„Xå°„çº¿å¸æ”¶å’Œæ•£å°„æ–¹æ³•è¢«ç”¨æ¥æ­ç¤ºGaæºæ‚å‰‚åœ¨Geæ™¶æ ¼ä¸­çš„ç»“æ„ä½ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db586723b8ac3396600b2f54bfd37625.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5823296647eac077ea43ed22951ef143.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24c1bacc687655b15f104fd98c83923d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5d520d34ddcbafef3c99faa33157755.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1805d9797ac49382454f7790c02ea541.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Text4Seg-Reimagining-Image-Segmentation-as-Text-Generation"><a href="#Text4Seg-Reimagining-Image-Segmentation-as-Text-Generation" class="headerlink" title="Text4Seg: Reimagining Image Segmentation as Text Generation"></a>Text4Seg: Reimagining Image Segmentation as Text Generation</h2><p><strong>Authors:Mengcheng Lan, Chaofeng Chen, Yue Zhou, Jiaxing Xu, Yiping Ke, Xinjiang Wang, Litong Feng, Wayne Zhang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have shown exceptional capabilities in vision-language tasks; however, effectively integrating image segmentation into these models remains a significant challenge. In this paper, we introduce Text4Seg, a novel text-as-mask paradigm that casts image segmentation as a text generation problem, eliminating the need for additional decoders and significantly simplifying the segmentation process. Our key innovation is semantic descriptors, a new textual representation of segmentation masks where each image patch is mapped to its corresponding text label. This unified representation allows seamless integration into the auto-regressive training pipeline of MLLMs for easier optimization. We demonstrate that representing an image with $16\times16$ semantic descriptors yields competitive segmentation performance. To enhance efficiency, we introduce the Row-wise Run-Length Encoding (R-RLE), which compresses redundant text sequences, reducing the length of semantic descriptors by 74% and accelerating inference by $3\times$, without compromising performance. Extensive experiments across various vision tasks, such as referring expression segmentation and comprehension, show that Text4Seg achieves state-of-the-art performance on multiple datasets by fine-tuning different MLLM backbones. Our approach provides an efficient, scalable solution for vision-centric tasks within the MLLM framework. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ï¼›ç„¶è€Œï¼Œæœ‰æ•ˆåœ°å°†å›¾åƒåˆ†å‰²é›†æˆåˆ°è¿™äº›æ¨¡å‹ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Text4Segï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ–‡æœ¬ä½œä¸ºæ©è†œèŒƒå¼ï¼Œå®ƒå°†å›¾åƒåˆ†å‰²è½¬åŒ–ä¸ºæ–‡æœ¬ç”Ÿæˆé—®é¢˜ï¼Œæ— éœ€é¢å¤–çš„è§£ç å™¨ï¼Œä»è€Œæå¤§åœ°ç®€åŒ–äº†åˆ†å‰²è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºè¯­ä¹‰æè¿°ç¬¦ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åˆ†å‰²æ©è†œæ–‡æœ¬è¡¨ç¤ºï¼Œå…¶ä¸­æ¯ä¸ªå›¾åƒå—éƒ½æ˜ å°„åˆ°å…¶ç›¸åº”çš„æ–‡æœ¬æ ‡ç­¾ã€‚è¿™ç§ç»Ÿä¸€è¡¨ç¤ºå…è®¸æ— ç¼é›†æˆåˆ°MLLMsçš„è‡ªå›å½’è®­ç»ƒç®¡é“ä¸­ï¼Œæ›´æ˜“äºä¼˜åŒ–ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨$16\times16$è¯­ä¹‰æè¿°ç¬¦è¡¨ç¤ºå›¾åƒå¯ä»¥è·å¾—å…·æœ‰ç«äº‰åŠ›çš„åˆ†å‰²æ€§èƒ½ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¡Œè¿è¡Œé•¿åº¦ç¼–ç ï¼ˆR-RLEï¼‰ï¼Œå®ƒå‹ç¼©äº†å†—ä½™çš„æ–‡æœ¬åºåˆ—ï¼Œå°†è¯­ä¹‰æè¿°ç¬¦çš„é•¿åº¦å‡å°‘äº†74%ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†3å€ï¼ŒåŒæ—¶ä¸æŸå®³æ€§èƒ½ã€‚åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒï¼Œå¦‚æŒ‡ä»£è¡¨è¾¾å¼åˆ†å‰²å’Œç†è§£ï¼Œè¡¨æ˜Text4Segé€šè¿‡å¾®è°ƒä¸åŒçš„MLLMä¸»å¹²åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºMLLMæ¡†æ¶å†…çš„ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ä»»åŠ¡æä¾›äº†é«˜æ•ˆã€å¯ä¼¸ç¼©çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09855v2">PDF</a> ICLR 2025. Project page: <a target="_blank" rel="noopener" href="https://mc-lan.github.io/Text4Seg/">https://mc-lan.github.io/Text4Seg/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºText4Segçš„æ–°æ–¹æ³•ï¼Œå°†å›¾åƒåˆ†å‰²è½¬åŒ–ä¸ºæ–‡æœ¬ç”Ÿæˆé—®é¢˜ï¼Œé€šè¿‡è¯­ä¹‰æè¿°ç¬¦è¿™ä¸€æ–°çš„æ–‡æœ¬è¡¨ç¤ºæ–¹å¼ï¼Œå®ç°äº†ä¸å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç´§å¯†é›†æˆã€‚æ­¤æ–¹æ³•æ¶ˆé™¤äº†å¯¹é¢å¤–è§£ç å™¨çš„éœ€æ±‚ï¼Œç®€åŒ–äº†åˆ†å‰²è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥è¡Œçº§è¿è¡Œé•¿åº¦ç¼–ç ï¼ˆR-RLEï¼‰ï¼Œæé«˜äº†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†åœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text4Segå°†å›¾åƒåˆ†å‰²è½¬åŒ–ä¸ºæ–‡æœ¬ç”Ÿæˆé—®é¢˜ï¼Œç®€åŒ–äº†åˆ†å‰²è¿‡ç¨‹ã€‚</li>
<li>è¯­ä¹‰æè¿°ç¬¦æ˜¯Text4Segçš„å…³é”®åˆ›æ–°ï¼Œå®ƒå°†å›¾åƒè¡¥ä¸æ˜ å°„åˆ°ç›¸åº”çš„æ–‡æœ¬æ ‡ç­¾ï¼Œå®ç°äº†ç»Ÿä¸€è¡¨ç¤ºã€‚</li>
<li>è¯­ä¹‰æè¿°ç¬¦çš„åº”ç”¨ä½¿å¾—å›¾åƒèƒ½ä»¥æ›´ç´§å‡‘çš„æ–¹å¼è¡¨ç¤ºï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>Row-wise Run-Length Encodingï¼ˆR-RLEï¼‰æŠ€æœ¯å‡å°‘äº†æ–‡æœ¬åºåˆ—çš„å†—ä½™ï¼Œé™ä½äº†è¯­ä¹‰æè¿°ç¬¦çš„é•¿åº¦ï¼Œå¹¶åŠ é€Ÿäº†æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>Text4Segåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¦‚æŒ‡ä»£è¡¨è¾¾å¼åˆ†å‰²å’Œç†è§£ã€‚</li>
<li>Text4Segæ–¹æ³•å¯ä»¥é€šè¿‡å¾®è°ƒä¸åŒçš„MLLMä¸»å¹²æ¥å®ç°é«˜æ•ˆã€å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b3d4f5cd887ad3bbac3dec46166b7ba5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd4f24a015a53d710b08f545a2073b18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-858ccdacf8ca41c88f54734768047cd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab6fb150da0f35295514a7443fc789c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae88ce84cfd228285f41f3c077c9c7d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c2e661cc7095359e63d09565e5ca591.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MedCLIP-SAMv2-Towards-Universal-Text-Driven-Medical-Image-Segmentation"><a href="#MedCLIP-SAMv2-Towards-Universal-Text-Driven-Medical-Image-Segmentation" class="headerlink" title="MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation"></a>MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation</h2><p><strong>Authors:Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao</strong></p>
<p>Segmentation of anatomical structures and pathological regions in medical images is essential for modern clinical diagnosis, disease research, and treatment planning. While significant advancements have been made in deep learning-based segmentation techniques, many of these methods still suffer from limitations in data efficiency, generalizability, and interactivity. As a result, developing precise segmentation methods that require fewer labeled datasets remains a critical challenge in medical image analysis. Recently, the introduction of foundation models like CLIP and Segment-Anything-Model (SAM), with robust cross-domain representations, has paved the way for interactive and universal image segmentation. However, further exploration of these models for data-efficient segmentation in medical imaging is still needed and highly relevant. In this paper, we introduce MedCLIP-SAMv2, a novel framework that integrates the CLIP and SAM models to perform segmentation on clinical scans using text prompts, in both zero-shot and weakly supervised settings. Our approach includes fine-tuning the BiomedCLIP model with a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss, and leveraging the Multi-modal Information Bottleneck (M2IB) to create visual prompts for generating segmentation masks from SAM in the zero-shot setting. We also investigate using zero-shot segmentation labels within a weakly supervised paradigm to enhance segmentation quality further. Extensive testing across four diverse segmentation tasks and medical imaging modalities (breast tumor ultrasound, brain tumor MRI, lung X-ray, and lung CT) demonstrates the high accuracy of our proposed framework. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/MedCLIP-SAMv2">https://github.com/HealthX-Lab/MedCLIP-SAMv2</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒä¸­çš„è§£å‰–ç»“æ„å’Œç—…ç†åŒºåŸŸçš„åˆ†å‰²å¯¹äºç°ä»£ä¸´åºŠè¯Šæ–­ã€ç–¾ç—…ç ”ç©¶å’Œæ²»ç–—è®¡åˆ’åˆ¶å®šè‡³å…³é‡è¦ã€‚è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²æŠ€æœ¯å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è¿™äº›æ–¹æ³•ä¸­çš„è®¸å¤šåœ¨æ•°æ®æ•ˆç‡ã€é€šç”¨æ€§å’Œäº¤äº’æ€§æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚å› æ­¤ï¼Œå¼€å‘éœ€è¦è¾ƒå°‘æ ‡æ³¨æ•°æ®é›†çš„ç²¾ç¡®åˆ†å‰²æ–¹æ³•ä»ç„¶æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æœ€è¿‘ï¼Œå¼•å…¥å…·æœ‰ç¨³å¥è·¨åŸŸè¡¨ç¤ºèƒ½åŠ›çš„CLIPå’ŒSegment-Anything-Modelï¼ˆSAMï¼‰ç­‰åŸºç¡€æ¨¡å‹ï¼Œä¸ºäº¤äº’å¼å’Œé€šç”¨å›¾åƒåˆ†å‰²é“ºå¹³äº†é“è·¯ã€‚ç„¶è€Œï¼Œé’ˆå¯¹åŒ»å­¦æˆåƒä¸­çš„æ•°æ®é«˜æ•ˆåˆ†å‰²ï¼Œè¿™äº›æ¨¡å‹çš„è¿›ä¸€æ­¥æ¢ç´¢ä»ç„¶æ˜¯éœ€è¦ä¸”é«˜åº¦ç›¸å…³çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MedCLIP-SAMv2ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆCLIPå’ŒSAMæ¨¡å‹çš„æ–°æ¡†æ¶ï¼Œå¯ä»¥ä½¿ç”¨æ–‡æœ¬æç¤ºå¯¹ä¸´åºŠæ‰«æè¿›è¡Œé›¶æ ·æœ¬å’Œå¼±ç›‘ç£ç¯å¢ƒä¸‹çš„åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨æ–°çš„è§£è€¦ç¡¬è´Ÿå™ªå£°å¯¹æ¯”ä¼°è®¡ï¼ˆDHN-NCEï¼‰æŸå¤±å¯¹BiomedCLIPæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡å¼ä¿¡æ¯ç“¶é¢ˆï¼ˆM2IBï¼‰åˆ›å»ºè§†è§‰æç¤ºï¼Œä»¥ä¾¿åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸­ä»SAMç”Ÿæˆåˆ†å‰²æ©è†œã€‚æˆ‘ä»¬è¿˜ç ”ç©¶åœ¨å¼±ç›‘ç£èŒƒå¼ä¸­ä½¿ç”¨é›¶æ ·æœ¬åˆ†å‰²æ ‡ç­¾ï¼Œä»¥è¿›ä¸€æ­¥æé«˜åˆ†å‰²è´¨é‡ã€‚åœ¨å››ä¸ªä¸åŒçš„åˆ†å‰²ä»»åŠ¡å’ŒåŒ»å­¦æˆåƒæ¨¡å¼ï¼ˆä¹³è…ºè‚¿ç˜¤è¶…å£°ã€è„‘è‚¿ç˜¤MRIã€è‚ºéƒ¨Xå°„çº¿å’Œè‚ºéƒ¨CTï¼‰çš„å¹¿æ³›æµ‹è¯•è¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ¡†æ¶å…·æœ‰å¾ˆé«˜çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/MedCLIP-SAMv2%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HealthX-Lab/MedCLIP-SAMv2æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19483v4">PDF</a> 10 pages, 2 figures, 6 tables</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºMedCLIP-SAMv2æ¡†æ¶ï¼Œç»“åˆCLIPå’ŒSAMæ¨¡å‹ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºè¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œæ¶‰åŠé›¶æ ·æœ¬å’Œå¼±ç›‘ç£è®¾ç½®ã€‚é€šè¿‡å¾®è°ƒBiomedCLIPæ¨¡å‹å¹¶åº”ç”¨DHN-NCEæŸå¤±å’ŒM2IBæ–¹æ³•ï¼Œå®ç°åœ¨å¤šç§åŒ»å­¦å›¾åƒæ¨¡æ€å’Œåˆ†å‰²ä»»åŠ¡ä¸Šçš„é«˜å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºç°ä»£ä¸´åºŠè¯Šæ–­ã€ç–¾ç—…ç ”ç©¶å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>è™½ç„¶æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨æ•°æ®æ•ˆç‡ã€é€šç”¨æ€§å’Œäº¤äº’æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>MedCLIP-SAMv2æ¡†æ¶ç»“åˆäº†CLIPå’ŒSAMæ¨¡å‹ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>MedCLIP-SAMv2é€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œåˆ†å‰²ï¼Œé€‚ç”¨äºé›¶æ ·æœ¬å’Œå¼±ç›‘ç£è®¾ç½®ã€‚</li>
<li>æ¡†æ¶é€šè¿‡å¾®è°ƒBiomedCLIPæ¨¡å‹å¹¶åº”ç”¨DHN-NCEæŸå¤±æé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>M2IBæ–¹æ³•ç”¨äºåˆ›å»ºè§†è§‰æç¤ºï¼Œç”ŸæˆSAMçš„åˆ†å‰²æ©è†œã€‚</li>
<li>åœ¨å¤šç§åŒ»å­¦å›¾åƒæ¨¡æ€å’Œåˆ†å‰²ä»»åŠ¡ä¸Šçš„æµ‹è¯•è¯æ˜äº†è¯¥æ¡†æ¶çš„é«˜å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d657651226aaccedaa9dd3a03afefce3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd0a68de1f8a6fcfd097c9f41acedbfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b446fbe0406c4b802dc70ba573d20fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16065c2f255030fed0c5141a146a0962.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="PTQ4RIS-Post-Training-Quantization-for-Referring-Image-Segmentation"><a href="#PTQ4RIS-Post-Training-Quantization-for-Referring-Image-Segmentation" class="headerlink" title="PTQ4RIS: Post-Training Quantization for Referring Image Segmentation"></a>PTQ4RIS: Post-Training Quantization for Referring Image Segmentation</h2><p><strong>Authors:Xiaoyan Jiang, Hang Yang, Kaiying Zhu, Xihe Qiu, Shibo Zhao, Sifan Zhou</strong></p>
<p>Referring Image Segmentation (RIS), aims to segment the object referred by a given sentence in an image by understanding both visual and linguistic information. However, existing RIS methods tend to explore top-performance models, disregarding considerations for practical applications on resources-limited edge devices. This oversight poses a significant challenge for on-device RIS inference. To this end, we propose an effective and efficient post-training quantization framework termed PTQ4RIS. Specifically, we first conduct an in-depth analysis of the root causes of performance degradation in RIS model quantization and propose dual-region quantization (DRQ) and reorder-based outlier-retained quantization (RORQ) to address the quantization difficulties in visual and text encoders. Extensive experiments on three benchmarks with different bits settings (from 8 to 4 bits) demonstrates its superior performance. Importantly, we are the first PTQ method specifically designed for the RIS task, highlighting the feasibility of PTQ in RIS applications. Code and video are available at {<a target="_blank" rel="noopener" href="https://github.com/gugu511yy/PTQ4RIS%7D">https://github.com/gugu511yy/PTQ4RIS}</a>. </p>
<blockquote>
<p>å‚ç…§å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰æ—¨åœ¨é€šè¿‡ç†è§£è§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œå¯¹ç»™å®šå¥å­ä¸­æåŠçš„å¯¹è±¡è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RISæ–¹æ³•å¾€å¾€æ¢ç´¢æ€§èƒ½ä¼˜è¶Šæ¨¡å‹ï¼Œå´å¿½è§†äº†åœ¨èµ„æºæœ‰é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå®é™…åº”ç”¨çš„é—®é¢˜ã€‚è¿™ä¸€ç–å¿½ç»™è®¾å¤‡ä¸Šçš„RISæ¨ç†å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„è®­ç»ƒåé‡åŒ–æ¡†æ¶ï¼Œç§°ä¸ºPTQ4RISã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ·±å…¥åˆ†æäº†RISæ¨¡å‹é‡åŒ–ä¸­æ€§èƒ½ä¸‹é™çš„æ ¹æœ¬åŸå› ï¼Œå¹¶æå‡ºäº†åŒåŒºåŸŸé‡åŒ–ï¼ˆDRQï¼‰å’ŒåŸºäºé‡æ’çš„ä¿ç•™å¼‚å¸¸å€¼é‡åŒ–ï¼ˆRORQï¼‰æ¥è§£å†³è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨çš„é‡åŒ–éš¾é¢˜ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œé‡‡ç”¨ä¸åŒçš„æ¯”ç‰¹è®¾ç½®ï¼ˆä»8ä½åˆ°4ä½ï¼‰è¯æ˜äº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹RISä»»åŠ¡è®¾è®¡çš„PTQæ–¹æ³•ï¼Œçªæ˜¾äº†PTQåœ¨RISåº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚ä»£ç å’Œè§†é¢‘å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/gugu511yy/PTQ4RIS]%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/gugu511yy/PTQ4RIS]æŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17020v2">PDF</a> Accepted by ICRA 2025.(Update the code link.)</p>
<p><strong>Summary</strong><br>    æå‡ºä¸€ç§é’ˆå¯¹å›¾åƒåˆ†å‰²çš„æ–°æ–¹æ³•â€”â€”å¼•ç”¨å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰ï¼Œç»¼åˆè€ƒè™‘è§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œä½†åœ¨å®é™…åº”ç”¨äºèµ„æºæœ‰é™çš„è¾¹ç¼˜è®¾å¤‡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªé«˜æ•ˆä¸”å®ç”¨çš„åè®­ç»ƒé‡åŒ–æ¡†æ¶PTQ4RISï¼Œæ·±å…¥åˆ†æäº†é‡åŒ–è¿‡ç¨‹ä¸­æ€§èƒ½ä¸‹é™çš„æ ¹æœ¬åŸå› ï¼Œå¹¶é’ˆå¯¹æ€§åœ°æå‡ºäº†åŒåŒºåŸŸé‡åŒ–ï¼ˆDRQï¼‰å’ŒåŸºäºé‡æ’åºçš„å¼‚å¸¸ä¿ç•™é‡åŒ–ï¼ˆRORQï¼‰æ–¹æ³•æ¥è§£å†³è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨çš„é‡åŒ–éš¾é¢˜ã€‚åœ¨ä¸‰ä¸ªä¸åŒæ¯”ç‰¹è®¾ç½®ï¼ˆä»8ä½åˆ°4ä½ï¼‰ä¸‹çš„å¹¿æ³›å®éªŒè¯æ˜å…¶æ€§èƒ½ä¼˜è¶Šã€‚æ­¤æ–¹æ³•æ—¨åœ¨ä¸ºRISä»»åŠ¡è¿›è¡Œç‰¹å®šè®¾è®¡çš„PTQæ–¹æ³•ï¼Œçªæ˜¾PTQåœ¨RISåº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•ç”¨å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰ç»“åˆäº†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œæ—¨åœ¨æ ¹æ®å¥å­ä¸­çš„æè¿°åœ¨å›¾åƒä¸­åˆ†å‰²ç›®æ ‡å¯¹è±¡ã€‚</li>
<li>ç°å­˜çš„RISæ–¹æ³•æ›´æ³¨é‡é«˜æ€§èƒ½æ¨¡å‹ï¼Œå¿½ç•¥äº†åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å®é™…åº”ç”¨è€ƒè™‘ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åè®­ç»ƒé‡åŒ–æ¡†æ¶PTQ4RISï¼Œä»¥æé«˜æ¨¡å‹åœ¨è¾¹ç¼˜è®¾å¤‡çš„æ•ˆç‡å’Œå®ç”¨æ€§ã€‚</li>
<li>PTQ4RISé€šè¿‡æ·±å…¥åˆ†æå‘ç°ï¼Œæ€§èƒ½ä¸‹é™çš„ä¸»è¦åŸå› æ˜¯é‡åŒ–è¿‡ç¨‹ä¸­çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯è§†è§‰å’Œæ–‡æœ¬ç¼–ç å™¨çš„é‡åŒ–éš¾é¢˜ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†åŒåŒºåŸŸé‡åŒ–ï¼ˆDRQï¼‰å’ŒåŸºäºé‡æ’åºçš„å¼‚å¸¸ä¿ç•™é‡åŒ–ï¼ˆRORQï¼‰æ–¹æ³•æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>åœ¨ä¸‰ä¸ªä¸åŒæ¯”ç‰¹è®¾ç½®ä¸‹çš„å¹¿æ³›å®éªŒè¯æ˜äº†PTQ4RISçš„ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c25d89eb7a6ed7dee47e044dc0fba6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02e409cbaa629077f77a70ca6c63f421.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-643ffe8871b3219d35c10ef19d2eca9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cafe1f31ee48bf748f04227b9baaac76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6458d9f3968af4b63eb3e5a764f6329.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b47b39abfc0cf001033414f4961f299.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="IRSRMamba-Infrared-Image-Super-Resolution-via-Mamba-based-Wavelet-Transform-Feature-Modulation-Model"><a href="#IRSRMamba-Infrared-Image-Super-Resolution-via-Mamba-based-Wavelet-Transform-Feature-Modulation-Model" class="headerlink" title="IRSRMamba: Infrared Image Super-Resolution via Mamba-based Wavelet   Transform Feature Modulation Model"></a>IRSRMamba: Infrared Image Super-Resolution via Mamba-based Wavelet   Transform Feature Modulation Model</h2><p><strong>Authors:Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Shinichiro Omachi</strong></p>
<p>Infrared image super-resolution demands long-range dependency modeling and multi-scale feature extraction to address challenges such as homogeneous backgrounds, weak edges, and sparse textures. While Mamba-based state-space models (SSMs) excel in global dependency modeling with linear complexity, their block-wise processing disrupts spatial consistency, limiting their effectiveness for IR image reconstruction. We propose IRSRMamba, a novel framework integrating wavelet transform feature modulation for multi-scale adaptation and an SSMs-based semantic consistency loss to restore fragmented contextual information. This design enhances global-local feature fusion, structural coherence, and fine-detail preservation while mitigating block-induced artifacts. Experiments on benchmark datasets demonstrate that IRSRMamba outperforms state-of-the-art methods in PSNR, SSIM, and perceptual quality. This work establishes Mamba-based architectures as a promising direction for high-fidelity IR image enhancement. Code are available at <a target="_blank" rel="noopener" href="https://github.com/yongsongH/IRSRMamba">https://github.com/yongsongH/IRSRMamba</a>. </p>
<blockquote>
<p>çº¢å¤–å›¾åƒè¶…åˆ†è¾¨ç‡å¤„ç†éœ€è¦å»ºç«‹é•¿æœŸä¾èµ–å…³ç³»æ¨¡å‹å’Œå¤šå°ºåº¦ç‰¹å¾æå–ï¼Œä»¥è§£å†³èƒŒæ™¯å‡åŒ€ã€è¾¹ç¼˜æ¨¡ç³Šå’Œçº¹ç†ç¨€ç–ç­‰æŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºMambaçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨çº¿æ€§å¤æ‚åº¦ä¸‹æ“…é•¿å…¨å±€ä¾èµ–å…³ç³»å»ºæ¨¡ï¼Œä½†å…¶åˆ†å—å¤„ç†ä¼šç ´åç©ºé—´ä¸€è‡´æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨çº¢å¤–å›¾åƒé‡å»ºä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æå‡ºäº†IRSRMambaè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒç»“åˆäº†åŸºäºå°æ³¢å˜æ¢çš„ç‰¹å¾è°ƒåˆ¶è¿›è¡Œå¤šå°ºåº¦é€‚é…ï¼Œä»¥åŠåŸºäºSSMsçš„è¯­ä¹‰ä¸€è‡´æ€§æŸå¤±æ¥æ¢å¤åˆ†æ•£çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§è®¾è®¡å¢å¼ºäº†å…¨å±€å’Œå±€éƒ¨ç‰¹å¾çš„èåˆã€ç»“æ„è¿è´¯æ€§å’Œç»†èŠ‚ä¿ç•™æ€§ï¼ŒåŒæ—¶å‡è½»äº†å› åˆ†å—å¼•èµ·çš„ä¼ªå½±ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒIRSRMambaåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰å’Œæ„ŸçŸ¥è´¨é‡ä¸Šå‡ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚æœ¬ç ”ç©¶ç¡®ç«‹äº†åŸºäºMambaçš„æ¶æ„åœ¨é«˜ä¿çœŸçº¢å¤–å›¾åƒå¢å¼ºä¸­çš„ç ”ç©¶å‰æ™¯ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/yongsongH/IRSRMamba%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/yongsongH/IRSRMambaè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.09873v2">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong><br>åŒ»å­¦çº¢å¤–å›¾åƒè¶…åˆ†è¾¨ç‡éœ€è¦é•¿ç¨‹ä¾èµ–å»ºæ¨¡å’Œå¤šå°ºåº¦ç‰¹å¾æå–ï¼Œä»¥åº”å¯¹å‡åŒ€èƒŒæ™¯ã€å¼±è¾¹ç¼˜å’Œç¨€ç–çº¹ç†ç­‰æŒ‘æˆ˜ã€‚MambaåŸºçŠ¶æ€ç©ºé—´æ¨¡å‹è™½æ“…é•¿å…¨å±€ä¾èµ–å»ºæ¨¡ï¼Œä½†å…¶å—çŠ¶å¤„ç†ç ´åäº†ç©ºé—´ä¸€è‡´æ€§ï¼Œé™åˆ¶äº†çº¢å¤–å›¾åƒé‡å»ºçš„æ•ˆæœã€‚æœ¬ç ”ç©¶æå‡ºIRSRMambaæ¡†æ¶ï¼Œé›†æˆå°æ³¢å˜æ¢ç‰¹å¾è°ƒåˆ¶å®ç°å¤šå°ºåº¦è‡ªé€‚åº”ï¼Œå¹¶å¼•å…¥SSMè¯­ä¹‰ä¸€è‡´æ€§æŸå¤±æ¢å¤æ–­è£‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ­¤è®¾è®¡æé«˜äº†å…¨å±€å±€éƒ¨ç‰¹å¾èåˆã€ç»“æ„è¿è´¯æ€§å’Œç»†èŠ‚ä¿ç•™èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘äº†å—çŠ¶å¼•èµ·çš„ä¼ªå½±ã€‚å®éªŒè¡¨æ˜ï¼ŒIRSRMambaåœ¨PSNRã€SSIMå’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¼˜äºå…ˆè¿›æ–¹æ³•ï¼Œç¡®ç«‹äº†MambaåŸºæ¶æ„åœ¨é«˜ä¿çœŸçº¢å¤–å›¾åƒå¢å¼ºä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦çº¢å¤–å›¾åƒè¶…åˆ†è¾¨ç‡é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬å‡åŒ€èƒŒæ™¯ã€å¼±è¾¹ç¼˜å’Œç¨€ç–çº¹ç†ã€‚</li>
<li>MambaåŸºçŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨å…¨å±€ä¾èµ–å»ºæ¨¡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†å—çŠ¶å¤„ç†ä¼šå¯¼è‡´ç©ºé—´ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>IRSRMambaæ¡†æ¶é€šè¿‡é›†æˆå°æ³¢å˜æ¢ç‰¹å¾è°ƒåˆ¶å®ç°å¤šå°ºåº¦è‡ªé€‚åº”ã€‚</li>
<li>SSMsè¯­ä¹‰ä¸€è‡´æ€§æŸå¤±ç”¨äºæ¢å¤æ–­è£‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>IRSRMambaæé«˜äº†å…¨å±€å±€éƒ¨ç‰¹å¾èåˆã€ç»“æ„è¿è´¯æ€§å’Œç»†èŠ‚ä¿ç•™èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶å‡å°‘äº†å› å—çŠ¶å¤„ç†å¼•èµ·çš„ä¼ªå½±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.09873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-723890e940e302c7028e2a84f942eb9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cd49631d97573a5cb3cf5290e969f03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fece709d518abca7bfa63cee5bd597e6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MedIAnomaly-A-comparative-study-of-anomaly-detection-in-medical-images"><a href="#MedIAnomaly-A-comparative-study-of-anomaly-detection-in-medical-images" class="headerlink" title="MedIAnomaly: A comparative study of anomaly detection in medical images"></a>MedIAnomaly: A comparative study of anomaly detection in medical images</h2><p><strong>Authors:Yu Cai, Weiwen Zhang, Hao Chen, Kwang-Ting Cheng</strong></p>
<p>Anomaly detection (AD) aims at detecting abnormal samples that deviate from the expected normal patterns. Generally, it can be trained merely on normal data, without a requirement for abnormal samples, and thereby plays an important role in the recognition of rare diseases and health screening in the medical domain. Despite the emergence of numerous methods for medical AD, we observe a lack of a fair and comprehensive evaluation, which causes ambiguous conclusions and hinders the development of this field. To address this problem, this paper builds a benchmark with unified comparison. Seven medical datasets with five image modalities, including chest X-rays, brain MRIs, retinal fundus images, dermatoscopic images, and histopathology whole slide images, are curated for extensive evaluation. Thirty typical AD methods, including reconstruction and self-supervised learning-based methods, are involved in comparison of image-level anomaly classification and pixel-level anomaly segmentation. Furthermore, for the first time, we formally explore the effect of key components in existing methods, clearly revealing unresolved challenges and potential future directions. The datasets and code are available at <a target="_blank" rel="noopener" href="https://github.com/caiyu6666/MedIAnomaly">https://github.com/caiyu6666/MedIAnomaly</a>. </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰æ—¨åœ¨æ£€æµ‹ä¸é¢„æœŸæ­£å¸¸æ¨¡å¼åç¦»çš„å¼‚å¸¸æ ·æœ¬ã€‚é€šå¸¸ï¼Œå®ƒä»…èƒ½åœ¨æ­£å¸¸æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ— éœ€å¼‚å¸¸æ ·æœ¬ï¼Œå› æ­¤åœ¨åŒ»å­¦é¢†åŸŸçš„ç½•è§ç–¾ç—…è¯†åˆ«å’Œå¥åº·ç­›æŸ¥ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚å°½ç®¡å‡ºç°äº†è®¸å¤šåŒ»å­¦å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œä½†æˆ‘ä»¬å‘ç°ç¼ºä¹å…¬å¹³è€Œå…¨é¢çš„è¯„ä¼°ï¼Œè¿™å¯¼è‡´ç»“è®ºæ¨¡ç³Šå¹¶é˜»ç¢äº†è¯¥é¢†åŸŸçš„å‘å±•ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¯”è¾ƒåŸºå‡†ã€‚æˆ‘ä»¬ç²¾å¿ƒæŒ‘é€‰äº†ä¸ƒä¸ªåŒ»å­¦æ•°æ®é›†ï¼ŒåŒ…å«äº”ç§å›¾åƒæ¨¡æ€ï¼ŒåŒ…æ‹¬èƒ¸éƒ¨Xå°„çº¿ã€è„‘éƒ¨MRIã€çœ¼åº•è§†ç½‘è†œå›¾åƒã€çš®è‚¤é•œå›¾åƒå’Œç—…ç†å…¨åˆ‡ç‰‡å›¾åƒï¼Œè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚æœ¬æ–‡æ¯”è¾ƒäº†30ç§å…¸å‹çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼ŒåŒ…æ‹¬é‡å»ºå’ŒåŸºäºè‡ªç›‘ç£å­¦ä¹ çš„æ–¹æ³•ï¼Œæ¶‰åŠå›¾åƒçº§å¼‚å¸¸åˆ†ç±»å’Œåƒç´ çº§å¼‚å¸¸åˆ†å‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡æ­£å¼æ¢è®¨äº†ç°æœ‰æ–¹æ³•ä¸­çš„å…³é”®ç»„ä»¶çš„å½±å“ï¼Œæ¸…æ™°åœ°æ­ç¤ºäº†æœªè§£å†³çš„æŒ‘æˆ˜å’Œæ½œåœ¨çš„æœªæ¥æ–¹å‘ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/caiyu6666/MedIAnomaly%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/caiyu6666/MedIAnomalyä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.04518v3">PDF</a> Accepted to Medical Image Analysis, 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒå¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰æ—¨åœ¨å‘ç°åç¦»é¢„æœŸæ­£å¸¸æ¨¡å¼çš„å¼‚å¸¸æ ·æœ¬ï¼Œå¯¹äºè¯†åˆ«ç½•è§ç–¾ç—…å’Œå¥åº·ç­›æŸ¥å…·æœ‰é‡è¦æ„ä¹‰ã€‚æœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°ä¸åŒçš„åŒ»å­¦å›¾åƒADæ–¹æ³•ã€‚è¯¥å¹³å°æ¶µç›–äº†ä¸ƒç§åŒ»å­¦æ•°æ®é›†å’Œäº”ç§å›¾åƒæ¨¡æ€ï¼ŒåŒ…æ‹¬èƒ¸éƒ¨Xå°„çº¿ã€è„‘éƒ¨MRIã€çœ¼åº•å›¾åƒã€çš®è‚¤é•œå›¾åƒå’Œç—…ç†å…¨åˆ‡ç‰‡å›¾åƒç­‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é¦–æ¬¡æ­£å¼æ¢è®¨äº†ç°æœ‰æ–¹æ³•ä¸­çš„å…³é”®ç»„ä»¶çš„å½±å“ï¼Œæ­ç¤ºäº†æœªè§£å†³çš„æŒ‘æˆ˜å’Œæ½œåœ¨çš„æœªæ¥æ–¹å‘ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œç”¨äºè¯†åˆ«ç½•è§ç–¾ç—…å’Œå¥åº·ç­›æŸ¥ã€‚</li>
<li>åŒ»å­¦å›¾åƒADæ–¹æ³•çš„è¯„ä¼°ç¼ºä¹å…¬å¹³æ€§å’Œç»¼åˆæ€§ï¼Œå¯¼è‡´ç»“è®ºæ¨¡ç³Šï¼Œé˜»ç¢è¯¥é¢†åŸŸçš„å‘å±•ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…æ‹¬ä¸ƒä¸ªåŒ»å­¦æ•°æ®é›†å’Œäº”ç§å›¾åƒæ¨¡æ€ã€‚</li>
<li>å¹³å°æ¶µç›–äº†å¤šç§å…¸å‹çš„ADæ–¹æ³•ï¼ŒåŒ…æ‹¬é‡å»ºå’Œè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ¯”è¾ƒå›¾åƒçº§åˆ«çš„å¼‚å¸¸åˆ†ç±»å’Œåƒç´ çº§åˆ«çš„å¼‚å¸¸åˆ†å‰²ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡æ¢è®¨äº†ç°æœ‰æ–¹æ³•ä¸­çš„å…³é”®ç»„ä»¶çš„å½±å“ï¼ŒåŒ…æ‹¬å¯¹å¼‚å¸¸æ£€æµ‹æ€§èƒ½çš„å…·ä½“ä½œç”¨ã€‚</li>
<li>æ–‡ç« æ­ç¤ºäº†è¯¥é¢†åŸŸçš„æœªè§£å†³æŒ‘æˆ˜å’Œæ½œåœ¨æœªæ¥æ–¹å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.04518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15143b7a79e82290d70ad0af6f1a3913.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59974efd00943517ad418cf9283d7874.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a118f40ca2b478b1b0ea6d5822d00f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1c9a2726146103ff9bf7457588f4bb5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-692d1410a958dc1179439a43805fb873.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Joint-enhancement-of-automatic-chest-X-ray-diagnosis-and-radiological-gaze-prediction-with-multi-stage-cooperative-learning"><a href="#Joint-enhancement-of-automatic-chest-X-ray-diagnosis-and-radiological-gaze-prediction-with-multi-stage-cooperative-learning" class="headerlink" title="Joint enhancement of automatic chest X-ray diagnosis and radiological   gaze prediction with multi-stage cooperative learning"></a>Joint enhancement of automatic chest X-ray diagnosis and radiological   gaze prediction with multi-stage cooperative learning</h2><p><strong>Authors:Zirui Qiu, Hassan Rivaz, Yiming Xiao</strong></p>
<p>Purpose: As visual inspection is an inherent process during radiological screening, the associated eye gaze data can provide valuable insights into relevant clinical decisions. As deep learning has become the state-of-the-art for computer-assisted diagnosis, integrating human behavior, such as eye gaze data, into these systems is instrumental to help align machine predictions with clinical diagnostic criteria, thus enhancing the quality of automatic radiological diagnosis. Methods: We propose a novel deep learning framework for joint disease diagnosis and prediction of corresponding clinical visual attention maps for chest X-ray scans. Specifically, we introduce a new dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for visual attention map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks in multi-task learning, we proposed a multi-stage cooperative learning strategy, with contrastive learning for feature encoder pretraining to boost performance. Results: Our proposed method is shown to significantly outperform existing techniques for chest X-ray diagnosis (AUC&#x3D;0.93) and the quality of visual attention map prediction (Correlation coefficient&#x3D;0.58). Conclusion: Benefiting from the proposed multi-task multi-stage cooperative learning, our technique demonstrates the benefit of integrating cliniciansâ€™ eye gaze into clinical AI systems to boost performance and potentially explainability. </p>
<blockquote>
<p>ç›®çš„ï¼šè§†è§‰æ£€æŸ¥æ˜¯æ”¾å°„å­¦ç­›æŸ¥è¿‡ç¨‹ä¸­çš„å›ºæœ‰è¿‡ç¨‹ï¼Œç›¸å…³çš„çœ¼åŠ¨æ•°æ®å¯ä»¥ä¸ºç›¸å…³çš„ä¸´åºŠå†³ç­–æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚éšç€æ·±åº¦å­¦ä¹ æˆä¸ºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„æœ€æ–°æŠ€æœ¯ï¼Œå°†äººç±»è¡Œä¸ºï¼ˆå¦‚çœ¼åŠ¨æ•°æ®ï¼‰æ•´åˆåˆ°è¿™äº›ç³»ç»Ÿä¸­ï¼Œæœ‰åŠ©äºä½¿æœºå™¨é¢„æµ‹ä¸ä¸´åºŠè¯Šæ–­æ ‡å‡†ä¿æŒä¸€è‡´ï¼Œä»è€Œæé«˜è‡ªåŠ¨æ”¾å°„å­¦è¯Šæ–­çš„è´¨é‡ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè”åˆç–¾ç—…è¯Šæ–­å’Œé¢„æµ‹èƒ¸éƒ¨Xå°„çº¿æ‰«æå¯¹åº”çš„ä¸´åºŠè§†è§‰æ³¨æ„åŠ›å›¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åŒç¼–ç å™¨å¤šä»»åŠ¡UNetï¼Œå®ƒåˆ©ç”¨DenseNet201ä¸»å¹²å’ŒåŸºäºæ®‹å·®å’ŒæŒ¤å‹æ¿€åŠ±å—ç¼–ç å™¨æ¥é¢„æµ‹è§†è§‰æ³¨æ„åŠ›å›¾ç‰¹å¾ï¼Œä»¥åŠå¤šå°ºåº¦ç‰¹å¾èåˆåˆ†ç±»å™¨è¿›è¡Œç–¾ç—…åˆ†ç±»ã€‚ä¸ºäº†è§£å†³å¤šä»»åŠ¡å­¦ä¹ ä¸­å•ä¸ªä»»åŠ¡å¼‚æ­¥è®­ç»ƒæ—¥ç¨‹çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µåˆä½œå­¦ä¹ ç­–ç•¥ï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ å¯¹ç‰¹å¾ç¼–ç å™¨è¿›è¡Œé¢„è®­ç»ƒä»¥æé«˜æ€§èƒ½ã€‚ç»“æœï¼šæ‰€æå‡ºçš„æ–¹æ³•åœ¨èƒ¸éƒ¨Xå…‰è¯Šæ–­ï¼ˆAUC&#x3D;0.93ï¼‰å’Œè§†è§‰æ³¨æ„åŠ›å›¾é¢„æµ‹è´¨é‡ï¼ˆç›¸å…³ç³»æ•°&#x3D;0.58ï¼‰æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚ç»“è®ºï¼šå¾—ç›Šäºæ‰€æå‡ºçš„å¤šä»»åŠ¡å¤šé˜¶æ®µåˆä½œå­¦ä¹ ï¼Œæˆ‘ä»¬çš„æŠ€æœ¯è¯æ˜äº†å°†ä¸´åºŠåŒ»ç”Ÿçœ¼åŠ¨æ•°æ®æ•´åˆåˆ°ä¸´åºŠäººå·¥æ™ºèƒ½ç³»ç»Ÿä¸­çš„å¥½å¤„ï¼Œå¯ä»¥æé«˜æ€§èƒ½å’Œæ½œåœ¨çš„è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16970v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè”åˆè¯Šæ–­å…³èŠ‚ç–¾ç—…å¹¶é¢„æµ‹å¯¹åº”çš„ä¸´åºŠè§†è§‰æ³¨æ„åŠ›åœ°å›¾ã€‚é€šè¿‡å¼•å…¥åŒç¼–ç å™¨å¤šä»»åŠ¡UNetç½‘ç»œï¼Œç»“åˆDenseNet201éª¨å¹²ç½‘ç»œå’ŒåŸºäºæ®‹å·®ä¸æŒ¤å‹æ¿€åŠ±å—çš„ç¼–ç å™¨ï¼Œä»¥æå–å¤šæ ·åŒ–çš„ç‰¹å¾ç”¨äºè§†è§‰æ³¨æ„åŠ›åœ°å›¾é¢„æµ‹å’Œå¤šå°ºåº¦ç‰¹å¾èåˆåˆ†ç±»å™¨è¿›è¡Œç–¾ç—…åˆ†ç±»ã€‚ä¸ºè§£å†³å¤šä»»åŠ¡å­¦ä¹ ä¸­å•ä¸ªä»»åŠ¡è®­ç»ƒå®‰æ’ä¸åŒæ­¥çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µåä½œå­¦ä¹ ç­–ç•¥ï¼Œå¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ å¯¹ç‰¹å¾ç¼–ç å™¨è¿›è¡Œé¢„è®­ç»ƒä»¥æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨èƒ¸éƒ¨Xå…‰è¯Šæ–­ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸”è§†è§‰æ³¨æ„åŠ›åœ°å›¾é¢„æµ‹è´¨é‡è¾ƒé«˜ã€‚æ•´åˆåŒ»ç”Ÿè§†çº¿æ•°æ®æœ‰åŠ©äºæé«˜ä¸´åºŠAIç³»ç»Ÿçš„æ€§èƒ½å’Œè§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ£€æŸ¥æ˜¯æ”¾å°„å­¦ç­›æŸ¥ä¸­çš„å›ºæœ‰è¿‡ç¨‹ï¼Œç›¸å…³çš„çœ¼åŠ¨æ•°æ®èƒ½ä¸ºä¸´åºŠå†³ç­–æä¾›å®è´µè§è§£ã€‚</li>
<li>æ·±åº¦å­¦ä¹ å·²æˆä¸ºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„å°–ç«¯æŠ€æœ¯ï¼Œæ•´åˆäººç±»è¡Œä¸ºï¼ˆå¦‚çœ¼åŠ¨æ•°æ®ï¼‰æœ‰åŠ©äºä½¿æœºå™¨é¢„æµ‹ä¸ä¸´åºŠè¯Šæ–­æ ‡å‡†å¯¹é½ï¼Œä»è€Œæé«˜è‡ªåŠ¨æ”¾å°„å­¦è¯Šæ–­çš„è´¨é‡ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆåŒç¼–ç å™¨å¤šä»»åŠ¡UNetç½‘ç»œè¿›è¡Œç–¾ç—…è¯Šæ–­å’Œè§†è§‰æ³¨æ„åŠ›åœ°å›¾é¢„æµ‹ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨DenseNet201å’ŒåŸºäºæ®‹å·®ä¸æŒ¤å‹æ¿€åŠ±å—çš„ç¼–ç å™¨æå–å¤šæ ·åŒ–ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆåˆ†ç±»å™¨è¿›è¡Œç–¾ç—…åˆ†ç±»ã€‚</li>
<li>ä¸ºè§£å†³å¤šä»»åŠ¡å­¦ä¹ ä¸­ä»»åŠ¡è®­ç»ƒå®‰æ’ä¸åŒæ­¥çš„é—®é¢˜ï¼Œé‡‡ç”¨å¤šé˜¶æ®µåä½œå­¦ä¹ ç­–ç•¥ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ ç”¨äºç‰¹å¾ç¼–ç å™¨çš„é¢„è®­ç»ƒï¼Œä»¥æå‡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.16970">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-03e531c7197212bfeff092263dcf49ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c101f534c4f95b1b160641952cdef1d9.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="HyperFusion-A-Hypernetwork-Approach-to-Multimodal-Integration-of-Tabular-and-Medical-Imaging-Data-for-Predictive-Modeling"><a href="#HyperFusion-A-Hypernetwork-Approach-to-Multimodal-Integration-of-Tabular-and-Medical-Imaging-Data-for-Predictive-Modeling" class="headerlink" title="HyperFusion: A Hypernetwork Approach to Multimodal Integration of   Tabular and Medical Imaging Data for Predictive Modeling"></a>HyperFusion: A Hypernetwork Approach to Multimodal Integration of   Tabular and Medical Imaging Data for Predictive Modeling</h2><p><strong>Authors:Daniel Duenias, Brennan Nichyporuk, Tal Arbel, Tammy Riklin Raviv</strong></p>
<p>The integration of diverse clinical modalities such as medical imaging and the tabular data extracted from patientsâ€™ Electronic Health Records (EHRs) is a crucial aspect of modern healthcare. Integrative analysis of multiple sources can provide a comprehensive understanding of the clinical condition of a patient, improving diagnosis and treatment decision. Deep Neural Networks (DNNs) consistently demonstrate outstanding performance in a wide range of multimodal tasks in the medical domain. However, the complex endeavor of effectively merging medical imaging with clinical, demographic and genetic information represented as numerical tabular data remains a highly active and ongoing research pursuit.   We present a novel framework based on hypernetworks to fuse clinical imaging and tabular data by conditioning the image processing on the EHRâ€™s values and measurements. This approach aims to leverage the complementary information present in these modalities to enhance the accuracy of various medical applications. We demonstrate the strength and generality of our method on two different brain Magnetic Resonance Imaging (MRI) analysis tasks, namely, brain age prediction conditioned by subjectâ€™s sex and multi-class Alzheimerâ€™s Disease (AD) classification conditioned by tabular data. We show that our framework outperforms both single-modality models and state-of-the-art MRI tabular data fusion methods. A link to our code can be found at <a target="_blank" rel="noopener" href="https://github.com/daniel4725/HyperFusion">https://github.com/daniel4725/HyperFusion</a> </p>
<blockquote>
<p>å¤šæ ·ä¸´åºŠæ¨¡å¼å¦‚åŒ»å­¦æˆåƒä¸ä»æ‚£è€…ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä¸­æå–çš„è¡¨æ ¼æ•°æ®çš„èåˆæ˜¯ç°ä»£åŒ»ç–—ä¿å¥çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚å¯¹å¤šä¸ªæ¥æºçš„ç»¼åˆåˆ†æå¯ä»¥æä¾›å¯¹æ‚£è€…ä¸´åºŠçŠ¶å†µçš„å…¨é¢äº†è§£ï¼Œä»è€Œæé«˜è¯Šæ–­å’Œæ²»ç–—å†³ç­–ã€‚æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„å¤šæ¨¡å¼ä»»åŠ¡ä¸­å§‹ç»ˆè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆåœ°å°†åŒ»å­¦æˆåƒä¸ä¸´åºŠã€äººå£ç»Ÿè®¡å­¦å’Œé—ä¼ ä¿¡æ¯èåˆï¼Œè¿™äº›ä»¥æ•°å€¼è¡¨æ ¼æ•°æ®çš„å½¢å¼å‘ˆç°ï¼Œä»ç„¶æ˜¯ä¸€é¡¹æ´»è·ƒè€ŒæŒç»­çš„ç ”ç©¶è¿½æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¶…ç½‘ç»œçš„æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡ä»¥ç”µå­å¥åº·è®°å½•çš„æ•°å€¼ä¸ºæ¡ä»¶æ¥èåˆä¸´åºŠæˆåƒå’Œè¡¨æ ¼æ•°æ®ã€‚è¯¥æ–¹æ³•æ—¨åœ¨åˆ©ç”¨è¿™äº›æ¨¡å¼ä¸­çš„äº’è¡¥ä¿¡æ¯ï¼Œä»¥æé«˜å„ç§åŒ»ç–—åº”ç”¨çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„è„‘éƒ¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åˆ†æä»»åŠ¡ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜åŠ¿å’Œé€šç”¨æ€§ï¼Œå³æ ¹æ®å—è¯•è€…æ€§åˆ«è¿›è¡Œè„‘é¾„é¢„æµ‹å’Œæ ¹æ®è¡¨æ ¼æ•°æ®è¿›è¡Œçš„å¤šç±»é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰åˆ†ç±»ã€‚æˆ‘ä»¬æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼˜äºå•æ¨¡æ€æ¨¡å‹å’Œæœ€å…ˆè¿›çš„MRIè¡¨æ ¼æ•°æ®èåˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç é“¾æ¥ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/daniel4725/HyperFusion">https://github.com/daniel4725/HyperFusion</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.13319v2">PDF</a> 20 pages, 11 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦æˆåƒä¸ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰è¡¨æ ¼æ•°æ®çš„èåˆæ˜¯ç°ä»£åŒ»ç–—çš„é‡è¦æ–¹å‘ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè¶…ç½‘ç»œçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä»¥EHRçš„æ•°å€¼æ•°æ®å’Œæµ‹é‡ç»“æœä¸ºæ¡ä»¶è¿›è¡Œå›¾åƒå¤„ç†ï¼Œåˆ©ç”¨ä¸¤ç§æ¨¡æ€çš„äº’è¡¥ä¿¡æ¯æå‡åŒ»ç–—åº”ç”¨çš„å‡†ç¡®æ€§ã€‚æ¡†æ¶åœ¨å¤§è„‘æ ¸ç£å…±æŒ¯æˆåƒçš„ä¸¤ä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼šä»¥æ€§åˆ«ä¸ºæ¡ä»¶çš„è„‘é¾„é¢„æµ‹å’Œä»¥è¡¨æ ¼æ•°æ®ä¸ºæ¡ä»¶çš„é˜¿å°”èŒ¨æµ·é»˜ç—…å¤šç±»åˆ«åˆ†ç±»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒä¸ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰è¡¨æ ¼æ•°æ®çš„èåˆæ˜¯ç°ä»£åŒ»ç–—çš„å…³é”®éœ€æ±‚ã€‚</li>
<li>æ·±ç¥ç»ç½‘ç»œåœ¨å¤šæ¨¡æ€åŒ»å­¦ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ–°æ¡†æ¶åŸºäºè¶…ç½‘ç»œï¼Œæ—¨åœ¨èåˆåŒ»å­¦æˆåƒå’Œè¡¨æ ¼æ•°æ®ã€‚</li>
<li>æ¡†æ¶ä»¥EHRçš„æ•°å€¼æ•°æ®å’Œæµ‹é‡ç»“æœä¸ºæ¡ä»¶è¿›è¡Œå›¾åƒå¤„ç†ã€‚</li>
<li>æ¡†æ¶åœ¨å¤§è„‘æ ¸ç£å…±æŒ¯æˆåƒçš„è„‘é¾„é¢„æµ‹å’Œé˜¿å°”èŒ¨æµ·é»˜ç—…åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>æ–°æ¡†æ¶ä¼˜äºå•æ¨¡æ€æ¨¡å‹å’Œå…ˆè¿›çš„MRIè¡¨æ ¼æ•°æ®èåˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.13319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a079bc09178130aa40377f6608457005.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb63a3d0f162aec8027e0bb6095f2d14.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-40f1c3983e8b7242fabb49c350a2cb13.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-20  AV-Flow Transforming Text to Audio-Visual Human-like Interactions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-af1c7fd96c47838da7b7147b338a0e4a.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-20  Is Noise Conditioning Necessary for Denoising Generative Models?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
