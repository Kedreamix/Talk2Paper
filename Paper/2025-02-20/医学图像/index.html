<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-02-20  A Dual-Stage Time-Context Network for Speech-Based Alzheimer&#39;s Disease   Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-309e96d5c3a1a942cd89f6ed448e05da.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-20-更新"><a href="#2025-02-20-更新" class="headerlink" title="2025-02-20 更新"></a>2025-02-20 更新</h1><h2 id="A-Dual-Stage-Time-Context-Network-for-Speech-Based-Alzheimer’s-Disease-Detection"><a href="#A-Dual-Stage-Time-Context-Network-for-Speech-Based-Alzheimer’s-Disease-Detection" class="headerlink" title="A Dual-Stage Time-Context Network for Speech-Based Alzheimer’s Disease   Detection"></a>A Dual-Stage Time-Context Network for Speech-Based Alzheimer’s Disease   Detection</h2><p><strong>Authors:Yifan Gao, Long Guo, Hong Liu</strong></p>
<p>Alzheimer’s disease (AD) is a progressive neurodegenerative disorder that leads to irreversible cognitive decline in memory and communication. Early detection of AD through speech analysis is crucial for delaying disease progression. However, existing methods mainly use pre-trained acoustic models for feature extraction but have limited ability to model both local and global patterns in long-duration speech. In this letter, we introduce a Dual-Stage Time-Context Network (DSTC-Net) for speech-based AD detection, integrating local acoustic features with global conversational context in long-duration recordings.We first partition each long-duration recording into fixed-length segments to reduce computational overhead and preserve local temporal details.Next, we feed these segments into an Intra-Segment Temporal Attention (ISTA) module, where a bidirectional Long Short-Term Memory (BiLSTM) network with frame-level attention extracts enhanced local features.Subsequently, a Cross-Segment Context Attention (CSCA) module applies convolution-based context modeling and adaptive attention to unify global patterns across all segments.Extensive experiments on the ADReSSo dataset show that our DSTC-Net outperforms state-of-the-art models, reaching 83.10% accuracy and 83.15% F1. </p>
<blockquote>
<p>阿尔茨海默病（AD）是一种进行性神经退行性疾病，会导致记忆和沟通的认知能力不可逆地下降。通过语音分析早期检测AD对于延缓疾病进展至关重要。然而，现有方法主要使用预训练的声学模型进行特征提取，但在长时语音中同时建模局部和全局模式的能力有限。</p>
</blockquote>
<p>在这篇文章中，我们介绍了一种用于基于语音的AD检测的双阶段时间上下文网络（DSTC-Net），该网络将局部声学特征与长时录音中的全局对话上下文相结合。我们首先将所有长时录音划分为固定长度的片段，以减少计算开销并保留局部时间细节。然后，我们将这些片段输入到Intra-Segment Temporal Attention（ISTA）模块中，其中具有帧级注意力的双向长短时记忆（BiLSTM）网络提取增强的局部特征。接下来，Cross-Segment Context Attention（CSCA）模块应用基于卷积的上下文建模和自适应注意力，以统一所有片段中的全局模式。在ADReSSo数据集上的大量实验表明，我们的DSTC-Net优于最新模型，达到了83.10%的准确率和83.15%的F1值。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13064v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为Dual-Stage Time-Context Network（DSTC-Net）的模型，用于基于语音的阿尔茨海默病（AD）检测。该模型结合了局部声学特征和全局对话上下文，以在长时长录音中实现更好的检测效果。通过分段处理长录音、强化局部特征和捕捉全局模式，DSTC-Net在ADReSSo数据集上的表现优于现有模型，达到83.10%的准确率和83.15%的F1值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>阿尔茨海默病（AD）是一种导致不可逆记忆和沟通认知衰退的进展性神经退行性疾病。</li>
<li>早期检测AD对延缓疾病进展至关重要。</li>
<li>现有方法主要通过预训练的声学模型进行特征提取，但难以在长时长语音中同时建模局部和全局模式。</li>
<li>本文提出了Dual-Stage Time-Context Network（DSTC-Net）模型，整合局部声学特征和全局对话上下文。</li>
<li>DSTC-Net通过分段处理长录音、强化局部特征和捕捉全局模式实现更好的检测效果。</li>
<li>DSTC-Net在ADReSSo数据集上的表现优于现有模型，达到较高的准确率和F1值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13064">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-297e8cfe8c47fdb60ef00db5db975671.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d48911d1248c6f688d40351dfd12d938.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8538f9b15e3ad980388c7ad16065093e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a60bf0526ec443753a8e23ebf50b54ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac21c00789032990b1551a9526573eb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa0cb9beacbcf1f121425dd0790835dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f12879c48e33a54b3f81e89df5cb4a1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-deep-learning-framework-for-efficient-pathology-image-analysis"><a href="#A-deep-learning-framework-for-efficient-pathology-image-analysis" class="headerlink" title="A deep learning framework for efficient pathology image analysis"></a>A deep learning framework for efficient pathology image analysis</h2><p><strong>Authors:Peter Neidlinger, Tim Lenz, Sebastian Foersch, Chiara M. L. Loeffler, Jan Clusmann, Marco Gustav, Lawrence A. Shaktah, Rupert Langer, Bastian Dislich, Lisa A. Boardman, Amy J. French, Ellen L. Goode, Andrea Gsur, Stefanie Brezina, Marc J. Gunter, Robert Steinfelder, Hans-Michael Behrens, Christoph Röcken, Tabitha Harrison, Ulrike Peters, Amanda I. Phipps, Giuseppe Curigliano, Nicola Fusco, Antonio Marra, Michael Hoffmeister, Hermann Brenner, Jakob Nikolas Kather</strong></p>
<p>Artificial intelligence (AI) has transformed digital pathology by enabling biomarker prediction from high-resolution whole slide images (WSIs). However, current methods are computationally inefficient, processing thousands of redundant tiles per WSI and requiring complex aggregator models. We introduce EAGLE (Efficient Approach for Guided Local Examination), a deep learning framework that emulates pathologists by selectively analyzing informative regions. EAGLE incorporates two foundation models: CHIEF for efficient tile selection and Virchow2 for extracting high-quality features. Benchmarking was conducted against leading slide- and tile-level foundation models across 31 tasks from four cancer types, spanning morphology, biomarker prediction and prognosis. EAGLE outperformed state-of-the-art foundation models by up to 23% and achieved the highest AUROC overall. It processed a slide in 2.27 seconds, reducing computational time by more than 99% compared to existing models. This efficiency enables real-time workflows, allows pathologists to validate all tiles which are used by the model during analysis, and eliminates dependence on high-performance computing, making AI-powered pathology more accessible. By reliably identifying meaningful regions and minimizing artifacts, EAGLE provides robust and interpretable outputs, supporting rapid slide searches, integration into multi-omics pipelines and emerging clinical foundation models. </p>
<blockquote>
<p>人工智能（AI）已经通过从高分辨率全切片图像（WSI）进行生物标志物预测，从而彻底改变了数字病理学。然而，当前的方法计算效率低下，每张WSI需要处理数千个冗余瓦片，并且需要复杂的聚合模型。我们引入了EAGLE（用于引导局部检查的高效方法），这是一个深度学习框架，通过选择性分析信息区域来模拟病理学家。EAGLE结合了两种基础模型：用于高效瓦片选择的CHIEF和用于提取高质量特征的Virchow2。我们在来自四种癌症类型、涵盖形态学、生物标志物预测和预后的31项任务上，与领先的幻灯片及瓦片级别的基础模型进行了基准测试。EAGLE在最高标准上超越了高达23%的现有先进基础模型，并获得了最高的AUROC值。它可以在2.27秒内处理一张幻灯片，与现有模型相比，计算时间减少了99%以上。这种效率实现了实时工作流程，允许病理学家验证模型在分析过程中使用的所有瓦片，并消除了对高性能计算的依赖，使AI驱动的病理学更加易于访问。通过可靠地识别有意义区域并尽量减少伪影，EAGLE提供了稳健和可解释的输出结果，支持快速幻灯片搜索、多组学管道集成和新兴的临床基础模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13027v1">PDF</a> </p>
<p><strong>Summary</strong><br>     人工智能通过高效地从高分辨率全切片图像（WSIs）预测生物标志物，实现了数字病理学领域的变革。然而，当前方法计算效率低下，处理大量冗余切片，并需要复杂的聚合模型。我们引入了EAGLE（用于引导局部检查的效率方法），这是一个深度学习框架，通过选择性分析信息区域来模拟病理医师的行为。EAGLE包含两个基础模型：用于高效切片选择的CHIEF和用于提取高质量特征的Virchow2。跨四种癌症类型中的31个任务进行的基准测试表明，EAGLE在形态学、生物标志物预测和预后评估方面优于其他领先的切片级和基础模型级技术，其AUROC值最高。同时，它可以在实际应用中大幅度减少计算时间并提高诊断效率。因此，通过准确识别有意义区域并最小化伪影，EAGLE能够提供稳健和可解释的输出来支持快速切片搜索和多组学管道集成等应用。这一技术的出现使得人工智能赋能的病理学更加实用和便捷。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AI在数字病理学领域中实现了从高分辨率全切片图像预测生物标志物的高效化变革。然而，当前方法存在计算效率低下的问题。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13027">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9280fc3d274b99514cd3ff3dd6b16fef.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Fake-It-Till-You-Make-It-Using-Synthetic-Data-and-Domain-Knowledge-for-Improved-Text-Based-Learning-for-LGE-Detection"><a href="#Fake-It-Till-You-Make-It-Using-Synthetic-Data-and-Domain-Knowledge-for-Improved-Text-Based-Learning-for-LGE-Detection" class="headerlink" title="Fake It Till You Make It: Using Synthetic Data and Domain Knowledge for   Improved Text-Based Learning for LGE Detection"></a>Fake It Till You Make It: Using Synthetic Data and Domain Knowledge for   Improved Text-Based Learning for LGE Detection</h2><p><strong>Authors:Athira J Jacob, Puneet Sharma, Daniel Rueckert</strong></p>
<p>Detection of hyperenhancement from cardiac LGE MRI images is a complex task requiring significant clinical expertise. Although deep learning-based models have shown promising results for the task, they require large amounts of data with fine-grained annotations. Clinical reports generated for cardiac MR studies contain rich, clinically relevant information, including the location, extent and etiology of any scars present. Although recently developed CLIP-based training enables pretraining models with image-text pairs, it requires large amounts of data and further finetuning strategies on downstream tasks. In this study, we use various strategies rooted in domain knowledge to train a model for LGE detection solely using text from clinical reports, on a relatively small clinical cohort of 965 patients. We improve performance through the use of synthetic data augmentation, by systematically creating scar images and associated text. In addition, we standardize the orientation of the images in an anatomy-informed way to enable better alignment of spatial and text features. We also use a captioning loss to enable fine-grained supervision and explore the effect of pretraining of the vision encoder on performance. Finally, ablation studies are carried out to elucidate the contributions of each design component to the overall performance of the model. </p>
<blockquote>
<p>从心脏LGE MRI图像中检测超增强是一个复杂的任务，需要丰富的临床经验。尽管基于深度学习的模型在该任务中显示出有希望的结果，但它们需要大量的精细标注数据。为心脏MR研究生成的临床报告包含丰富的与临床相关的信息，包括任何疤痕的位置、程度和病因。虽然最近开发的基于CLIP的训练方法能够实现图像文本对的预训练模型，但它需要大量数据，并且需要在下游任务上进一步微调策略。在这项研究中，我们利用基于领域知识的各种策略，仅使用来自965名患者临床报告的文本，对LGE检测模型进行训练。我们通过系统地创建疤痕图像和相关文本，提高通过合成数据增强性能。此外，我们以解剖信息的方式标准化图像方向，以实现空间特征和文本特征的更好对齐。我们还使用描述性损失来实现精细监督，并探索视觉编码器预训练对性能的影响。最后，进行消融研究以阐明每个设计组件对模型总体性能的贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12948v1">PDF</a> Poster at Workshop on Large Language Models and Generative AI for   Health at AAAI 2025</p>
<p><strong>Summary</strong><br>     本研究利用临床报告中的文本数据，结合领域知识和多种策略，如合成数据增强、图像标准化、精细监督的标注损失和视觉编码器的预训练，训练了一个仅使用文本数据的LGE MRI图像超增强检测模型。在较小的965例患者队列中取得了良好性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>心脏LGE MRI图像的超增强检测是一项需要临床专家经验的复杂任务。</li>
<li>深度学习模型在该任务中显示出潜力，但需要大量精细标注的数据。</li>
<li>临床报告包含关于心脏MR研究的重要临床信息，可用于训练检测模型。</li>
<li>本研究使用基于领域知识的策略，仅使用临床报告的文本数据来训练模型。</li>
<li>通过合成数据增强、图像标准化和精细监督的标注损失等方法提高了模型性能。</li>
<li>通过对视觉编码器进行预训练，进一步提升了模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12948">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b524d1d24c627dd2f2ba5a615eb20fb4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e168dd0cbd25050ab5286ce1fca2f7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-970ecbbf34603c490ff1501aea97db3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8a3ed1b563a6684d7a6007f00eddd85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54cb1e2e444cc2acc163d539396be165.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Carotid-Artery-Plaque-Analysis-in-3D-Based-on-Distance-Encoding-in-Mesh-Representations"><a href="#Carotid-Artery-Plaque-Analysis-in-3D-Based-on-Distance-Encoding-in-Mesh-Representations" class="headerlink" title="Carotid Artery Plaque Analysis in 3D Based on Distance Encoding in Mesh   Representations"></a>Carotid Artery Plaque Analysis in 3D Based on Distance Encoding in Mesh   Representations</h2><p><strong>Authors:Hinrich Rahlfs, Markus Hüllebrand, Sebastian Schmitter, Christoph Strecker, Andreas Harloff, Anja Hennemuth</strong></p>
<p>Purpose: Enabling a comprehensive and robust assessment of carotid artery plaques in 3D through extraction and visualization of quantitative plaque parameters. These parameters have potential applications in stroke risk analysis, evaluation of therapy effectiveness, and plaque progression prediction. Methods: We propose a novel method for extracting a plaque mesh from 3D vessel wall segmentation using distance encoding on the inner and outer wall mesh for precise plaque structure analysis. A case-specific threshold, derived from the normal vessel wall thickness, was applied to extract plaques from a dataset of 202 T1-weighted black-blood MRI scans of subjects with up to 50% stenosis. Applied to baseline and one-year follow-up data, the method supports detailed plaque morphology analysis over time, including plaque volume quantification, aided by improved visualization via mesh unfolding. Results: We successfully extracted plaque meshes from 341 carotid arteries, capturing a wide range of plaque shapes with volumes ranging from 2.69{\mu}l to 847.7{\mu}l. The use of a case-specific threshold effectively eliminated false positives in young, healthy subjects. Conclusion: The proposed method enables precise extraction of plaque meshes from 3D vessel wall segmentation masks enabling a correspondence between baseline and one-year follow-up examinations. Unfolding the plaque meshes enhances visualization, while the mesh-based analysis allows quantification of plaque parameters independent of voxel resolution. </p>
<blockquote>
<p>目的：通过提取和可视化定量斑块参数，实现对颈动脉斑块在3D中的全面和稳健评估。这些参数在脑卒中风险分析、疗效评估以及斑块进展预测中具有潜在应用价值。方法：我们提出了一种新方法，通过距离编码内、外壁网格从3D血管壁分割中提取斑块网格，用于精确分析斑块结构。应用由正常血管壁厚推导出的针对特定病例的阈值，从一组包含高达50%狭窄的202例T1加权黑血MRI扫描数据集中提取斑块。该方法应用于基线数据以及一年后的随访数据，支持随时间变化的详细斑块形态分析，包括斑块体积量化，辅以网格展开改进的可视化。结果：我们成功地从341个颈动脉中提取了斑块网格，捕捉到了各种斑块形状，体积范围从2.69μl到847.7μl。使用特定病例的阈值有效地消除了年轻健康人群中的假阳性结果。结论：所提出的方法能够精确地从3D血管壁分割蒙版中提取斑块网格，实现基线检查和一年后的随访检查之间的对应关系。展开斑块网格增强了可视化效果，而基于网格的分析允许独立于体素分辨率的斑块参数量化。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12819v1">PDF</a> 13 pages, 5 Figures, Submitted to the International Journal of   Computer Assisted Radiology and Surgery</p>
<p><strong>Summary</strong><br>     本文提出了一种基于三维血管壁分割的新型斑块提取方法，该方法可通过距离编码确定内外壁网格进行精确斑块结构分析。应用于基于T1加权黑血MRI扫描的数据集，该方法可从基准线和一年的随访数据中分析斑块形态，包括量化斑块体积等。本文成功的从341个颈动脉中提取了斑块网格，捕捉到了广泛的斑块形态和体积范围。此方法利用特定的阈值有效地消除了年轻健康人群中的假阳性结果。这种方法使得精确提取斑块网格成为可能，为斑块参数量化提供了基础，有助于评估中风风险、评估治疗效果和预测斑块进展。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了一种基于三维血管壁分割的新型斑块提取方法，用于精确分析斑块结构。</li>
<li>利用距离编码确定内外壁网格进行斑块提取和可视化。</li>
<li>通过应用特定的阈值，成功从数据集中提取了斑块网格，捕捉到了广泛的斑块形态和体积范围。</li>
<li>该方法可以应用于基准线和一年的随访数据，进行详细的斑块形态分析，包括量化斑块体积。</li>
<li>展开网格增强了斑块的可视化效果。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12819">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d4c837f93ae7a37ccb2a95c3d83d487.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1caa8e9bb46cdfac3450f9848271280.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4b1217c016c4f9e0f5bf9d4e4dad0a4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="3D-Shape-to-Image-Brownian-Bridge-Diffusion-for-Brain-MRI-Synthesis-from-Cortical-Surfaces"><a href="#3D-Shape-to-Image-Brownian-Bridge-Diffusion-for-Brain-MRI-Synthesis-from-Cortical-Surfaces" class="headerlink" title="3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from   Cortical Surfaces"></a>3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from   Cortical Surfaces</h2><p><strong>Authors:Fabian Bongratz, Yitong Li, Sama Elbaroudy, Christian Wachinger</strong></p>
<p>Despite recent advances in medical image generation, existing methods struggle to produce anatomically plausible 3D structures. In synthetic brain magnetic resonance images (MRIs), characteristic fissures are often missing, and reconstructed cortical surfaces appear scattered rather than densely convoluted. To address this issue, we introduce Cor2Vox, the first diffusion model-based method that translates continuous cortical shape priors to synthetic brain MRIs. To achieve this, we leverage a Brownian bridge process which allows for direct structured mapping between shape contours and medical images. Specifically, we adapt the concept of the Brownian bridge diffusion model to 3D and extend it to embrace various complementary shape representations. Our experiments demonstrate significant improvements in the geometric accuracy of reconstructed structures compared to previous voxel-based approaches. Moreover, Cor2Vox excels in image quality and diversity, yielding high variation in non-target structures like the skull. Finally, we highlight the capability of our approach to simulate cortical atrophy at the sub-voxel level. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ai-med/Cor2Vox">https://github.com/ai-med/Cor2Vox</a>. </p>
<blockquote>
<p>尽管最近在医学图像生成方面取得了进展，但现有方法仍然难以生成解剖上合理的3D结构。在合成的大脑磁共振成像（MRI）中，特征裂往往缺失，重建的皮层表面看起来是散乱的，而不是密集卷曲的。为了解决这个问题，我们引入了Cor2Vox，这是基于扩散模型的方法，首次将连续的皮质形状先验知识转化为合成的大脑MRI。为了实现这一点，我们利用布朗桥过程，允许形状轮廓和医学图像之间的直接结构化映射。具体来说，我们将布朗桥扩散模型的概念适应到3D，并将其扩展到包含各种互补形状表示。我们的实验表明，与先前的基于体素的方法相比，重建结构的几何精度有了显着提高。此外，Cor2Vox在图像质量和多样性方面表现出色，在非目标结构（如颅骨）上产生了较高的变化。最后，我们强调了我们的方法在亚体素级别模拟皮质萎缩的能力。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/ai-med/Cor2Vox%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ai-med/Cor2Vox获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12742v1">PDF</a> Accepted by Information Processing in Medical Imaging (IPMI) 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Cor2Vox，首个基于扩散模型的方法，能将连续的皮质形状先验转化为合成的大脑MRI图像。通过采用布朗桥过程，实现了形状轮廓与医学图像之间的直接结构化映射，并在3D环境中对布朗桥扩散模型进行了适应和扩展，以容纳各种互补形状表示。相较于传统的体素方法，Cor2Vox在几何准确性上有了显著提升，同时图像质量和多样性也有所提高，特别是在非目标结构如颅骨上表现出高变异性。此外，Cor2Vox还能模拟子体素级别的皮质萎缩。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Cor2Vox是首个基于扩散模型的医学图像生成方法，专注于合成大脑MRI。</li>
<li>通过布朗桥过程实现形状轮廓与医学图像之间的直接结构化映射。</li>
<li>Cor2Vox将连续的皮质形状先验转化为合成的大脑MRI。</li>
<li>该方法在3D环境中对布朗桥扩散模型进行了适应和扩展。</li>
<li>Cor2Vox在几何准确性上相较于传统体素方法有所提升。</li>
<li>Cor2Vox能提高图像质量和多样性，特别是在非目标结构上。</li>
<li>Cor2Vox能模拟子体素级别的皮质萎缩。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12742">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ae75490596d8975afb8a2208d53870cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82e9d522c9f13e3b8847bf831182c7c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f525489c3273d55ad672fd43b9673a5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31ad5050d5b50a4c67c501c6ffcdf6f9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Propagation-for-Echocardiography-Clinical-Metric-Estimation-via-Contour-Sampling"><a href="#Uncertainty-Propagation-for-Echocardiography-Clinical-Metric-Estimation-via-Contour-Sampling" class="headerlink" title="Uncertainty Propagation for Echocardiography Clinical Metric Estimation   via Contour Sampling"></a>Uncertainty Propagation for Echocardiography Clinical Metric Estimation   via Contour Sampling</h2><p><strong>Authors:Thierry Judge, Olivier Bernard, Woo-Jin Cho Kim, Alberto Gomez, Arian Beqiri, Agisilaos Chartsias, Pierre-Marc Jodoin</strong></p>
<p>Echocardiography plays a fundamental role in the extraction of important clinical parameters (e.g. left ventricular volume and ejection fraction) required to determine the presence and severity of heart-related conditions. When deploying automated techniques for computing these parameters, uncertainty estimation is crucial for assessing their utility. Since clinical parameters are usually derived from segmentation maps, there is no clear path for converting pixel-wise uncertainty values into uncertainty estimates in the downstream clinical metric calculation. In this work, we propose a novel uncertainty estimation method based on contouring rather than segmentation. Our method explicitly predicts contour location uncertainty from which contour samples can be drawn. Finally, the sampled contours can be used to propagate uncertainty to clinical metrics. Our proposed method not only provides accurate uncertainty estimations for the task of contouring but also for the downstream clinical metrics on two cardiac ultrasound datasets. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/ThierryJudge/contouring-uncertainty">https://github.com/ThierryJudge/contouring-uncertainty</a>. </p>
<blockquote>
<p>超声心动术在提取确定心脏状况存在与否及其严重程度所需的重要临床参数（例如左心室容积和射血分数）方面起着至关重要的作用。当应用自动化技术来计算这些参数时，对不确定性的评估是至关重要的。由于临床参数通常来自分割图，因此没有明确的途径将像素级的不确定性值转换为下游临床指标计算中的不确定性估计。在这项工作中，我们提出了一种基于轮廓而不是分割的不确定性估计新方法。我们的方法可以明确地预测轮廓位置的不确定性，从而可以从中绘制轮廓样本。最后，这些采样轮廓可用于将不确定性传播到临床指标。我们提出的方法不仅为轮廓任务提供了准确的不确定性估计，而且在两个心脏超声数据集上对下游临床指标也提供了准确的不确定性估计。相关代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/ThierryJudge/contouring-uncertainty">https://github.com/ThierryJudge/contouring-uncertainty</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12713v1">PDF</a> 10 pages, submitted to IEEE TMI</p>
<p><strong>Summary</strong><br>     超声心动图在提取重要临床参数（如左心室容积和射血分数）以判断心脏相关疾病的存在和严重程度方面发挥着重要作用。当使用自动化技术计算这些参数时，对不确定性进行估计是评估其效用的关键。本研究提出了一种基于轮廓而非分割的不确定性估计新方法，该方法能够明确预测轮廓位置的不确定性，从而绘制轮廓样本。最后，这些样本可用于将不确定性传播到临床指标。该方法不仅为轮廓任务提供了准确的不确定性估计，而且在两个心脏超声数据集上对下游临床指标也提供了准确的不确定性估计。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超声心动图在诊断心脏相关疾病中扮演重要角色，能提取关键临床参数。</li>
<li>自动化技术在计算临床参数时，不确定性估计是评估其效用的关键。</li>
<li>本研究提出了一种基于轮廓的不确定性估计新方法，该方法优于传统的分割方法。</li>
<li>该方法能明确预测轮廓位置的不确定性，并绘制轮廓样本。</li>
<li>轮廓样本可用于将不确定性传播到临床指标。</li>
<li>该方法在两个心脏超声数据集上表现出对临床指标准确的不确定性估计。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12713">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-309e96d5c3a1a942cd89f6ed448e05da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-061eb92e7a080b03cf71b45161c999ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ce4da25129128efa910fec03958edca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bddd12340f421c0d98207d9fe261876.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc4854a5f80ff2f32ddc30b0f37a1841.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VLBI-Imaging-of-Parsec-scale-Radio-Structures-in-Nearby-Low-luminosity-AGN"><a href="#VLBI-Imaging-of-Parsec-scale-Radio-Structures-in-Nearby-Low-luminosity-AGN" class="headerlink" title="VLBI Imaging of Parsec-scale Radio Structures in Nearby Low-luminosity   AGN"></a>VLBI Imaging of Parsec-scale Radio Structures in Nearby Low-luminosity   AGN</h2><p><strong>Authors:Xiaopeng Cheng, Tao An, Willem Baan, Raneri D. Baldi, David R. A. Williams-Baldwin, Bong Won Sohn, Robert Beswick, Ian M. Mchardy</strong></p>
<p>We report the results of high-resolution 5 GHz Very Long Baseline Array and European VLBI Network observations of 36 nearby galaxies, an extension of the Legacy e-MERLIN Multi-band Imaging of Nearby Galaxies (LeMMINGs) survey. Our sample includes 21 low ionization nuclear emission regions (LINERs), 4 Seyferts, 3 absorption line galaxies (ALGs), and 8 HII galaxies. We achieved an unprecedented detection rate, successfully imaging 23 out of 36 sources with a detection threshold of $\sim$20 $\mu$Jy beam$^{-1}$. The radio sizes are typically of $\leq$ 5 pc. Core identification was achieved in 16 sources, while 7 others were identified as core candidates. Radio luminosities of the sample range from 10$\rm ^{34}$ to 10$\rm ^{38}$ erg s$^{-1}$. Our analysis reveals a predominance of compact core structures, with ten sources exhibiting a one-sided core jet morphology and NGC 2146 exhibiting a rare two-sided jet structure. The study advances our understanding of the compactness of radio sources at various scales, indicating a core-dominated nature in all but one galaxy NGC2655. We find moderate to strong correlations between radio luminosity, black hole mass, optical [O III] line luminosity, and hard X-ray luminosity, suggesting a common active galactic nucleus (AGN) core origin. These results provide new insights into the fundamental plane of black hole activity and support the role of the synchrotron process in Low-luminosity AGN (LLAGN) radio emission. </p>
<blockquote>
<p>我们报告了使用高解析度5 GHz超长基线阵列和欧洲VLBI网络对附近36个星系进行观测的结果，这是对遗留e-MERLIN邻近星系多波段成像（LeMMINGs）调查的扩展。我们的样本包括21个低电离核发射区（LINERs）、4个塞弗里特星系、3个吸收线星系（ALGs）和8个HII星系。我们取得了前所未有的检测率，成功地对其中23个源进行了成像，检测阈值约为$\sim$20 $\mu$Jy beam$^{-1}$。射电大小通常小于或等于5 pc。核心识别在16个源上实现，而其他7个被识别为核心候选者。样本的射电光度范围从10$^{34}$到10$^{38}$ erg s$^{-1}$。我们的分析显示紧凑的核心结构占主导地位，有十个源呈现单边核心喷射形态，而NGC 2146呈现罕见的双边喷射结构。该研究推进了对不同尺度上射电源紧凑性的理解，除了一个星系NGC2655之外，其他所有星系都显示出核心主导的特征。我们发现射电光度与黑洞质量、光学[O III]线光度和硬X射线光度之间存在中度到强度的相关性，这表明它们都来自活跃的星系核（AGN）核心。这些结果提供了关于黑洞活动基本平面的新见解，并支持同步辐射过程在低光度活动星系核（LLAGN）射电发射中的作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12553v1">PDF</a> 25 pages, 6 figures, 4 tables, accepted for publication in ApJS</p>
<p><strong>摘要</strong></p>
<p>报告了使用高分辨率5 GHz超长基线阵列和欧洲VLBI网络对附近星系进行观测的结果，这是Legacy e-MERLIN对附近星系的多波段成像（LeMMINGs）调查的延伸。样本包括低电离核发射区（LINERs）的21个星系，赛弗特星系（Seyferts）的4个星系，吸收线星系（ALGs）的3个星系和HII星系的8个星系。在约20微吉束的灵敏度极限下成功检测了超过三分之二的星系。核结构明显并且小型紧凑，长度一般不超过≤五千兆秒的距离尺度。核心鉴定成功完成在十六个星系中，而另外七个星系被确认为核心候选者。样本的射电光度介于每平方秒千兆瓦特（erg s-²）之间。我们的分析揭示了紧凑核心结构占主导地位，有十个星系呈现单边核心喷射形态，而NGC 2146呈现罕见的双边喷射结构。研究推进了对不同尺度上射电源紧凑性的理解，揭示了除NGC 2655外所有星系的以核心为主的特点。我们发现射电光度与黑洞质量、光学氧III线光度以及硬X射线光度之间存在中度至强相关性，暗示着活动星系核（AGNs）核心的常见起源。这些结果提供了关于黑洞活动基本平面的新见解，并支持同步辐射过程在低光度活动星系核（LLAGNs）射电发射中的作用。 </p>
<p><strong>关键发现列表</strong></p>
<ul>
<li>利用高分辨阵列观测技术对附近星系进行了广泛的射电观测。</li>
<li>成功检测到超过三分之二的观测目标，揭示了射电源的高检测率。</li>
<li>观测到的射电源核心结构显著且规模较小，主要呈现紧凑的核心形态。</li>
<li>在部分星系中发现单边核心喷射形态，并在NGC 2146中发现罕见的双边喷射结构。</li>
<li>除NGC 2655外，所有观测的星系均表现出以核心为主的特点。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12553">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-37f13c416b8a5586b8b6b3b9c1518ebc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="When-Segmentation-Meets-Hyperspectral-Image-New-Paradigm-for-Hyperspectral-Image-Classification"><a href="#When-Segmentation-Meets-Hyperspectral-Image-New-Paradigm-for-Hyperspectral-Image-Classification" class="headerlink" title="When Segmentation Meets Hyperspectral Image: New Paradigm for   Hyperspectral Image Classification"></a>When Segmentation Meets Hyperspectral Image: New Paradigm for   Hyperspectral Image Classification</h2><p><strong>Authors:Weilian Zhou, Weixuan Xie, Sei-ichiro Kamata, Man Sing Wong,  Huiying,  Hou, Haipeng Wang</strong></p>
<p>Hyperspectral image (HSI) classification is a cornerstone of remote sensing, enabling precise material and land-cover identification through rich spectral information. While deep learning has driven significant progress in this task, small patch-based classifiers, which account for over 90% of the progress, face limitations: (1) the small patch (e.g., 7x7, 9x9)-based sampling approach considers a limited receptive field, resulting in insufficient spatial structural information critical for object-level identification and noise-like misclassifications even within uniform regions; (2) undefined optimal patch sizes lead to coarse label predictions, which degrade performance; and (3) a lack of multi-shape awareness around objects. To address these challenges, we draw inspiration from large-scale image segmentation techniques, which excel at handling object boundaries-a capability essential for semantic labeling in HSI classification. However, their application remains under-explored in this task due to (1) the prevailing notion that larger patch sizes degrade performance, (2) the extensive unlabeled regions in HSI groundtruth, and (3) the misalignment of input shapes between HSI data and segmentation models. Thus, in this study, we propose a novel paradigm and baseline, HSIseg, for HSI classification that leverages segmentation techniques combined with a novel Dynamic Shifted Regional Transformer (DSRT) to overcome these challenges. We also introduce an intuitive progressive learning framework with adaptive pseudo-labeling to iteratively incorporate unlabeled regions into the training process, thereby advancing the application of segmentation techniques. Additionally, we incorporate auxiliary data through multi-source data collaboration, promoting better feature interaction. Validated on five public HSI datasets, our proposal outperforms state-of-the-art methods. </p>
<blockquote>
<p>高光谱图像（HSI）分类是遥感的核心，它通过丰富的光谱信息实现精确的材料和土地覆盖物识别。虽然深度学习在此任务中取得了显著进展，但基于小块的分类器占据了超过90%的进展，仍面临一些局限性：（1）基于小块（例如7x7、9x9）的采样方法考虑了一个有限的感受野，导致缺乏对于对象级别识别至关重要的空间结构信息，甚至在均匀区域内出现类似噪声的误分类；（2）不确定的最佳块大小导致标签预测粗糙，从而降低了性能；（3）缺乏对象周围的多形状意识。为了应对这些挑战，我们从大规模图像分割技术中汲取灵感，它们擅长处理对象边界——这是HSI分类中语义标记所必需的能力。然而，它们在此任务中的应用仍然未被充分探索，因为（1）普遍认为较大的块大小会降低性能，（2）HSI基准图像中存在大量未标记区域，以及（3）HSI数据与分割模型之间输入形状的不匹配。因此，本研究提出了一种新的范式和基线方法HSIseg，用于HSI分类，它结合了分割技术与一种新的动态移位区域转换器（DSRT）来克服这些挑战。我们还介绍了一种直观的渐进学习框架，具有自适应伪标签，可以逐步将未标记区域纳入训练过程，从而促进分割技术的应用。此外，我们通过多源数据协作融入辅助数据，促进更好的特征交互。在五个公共HSI数据集上进行验证，我们的提案优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12541v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了高光谱图像分类中的挑战，包括小补丁分类器在空间结构信息上的不足、标签预测粗糙和缺乏多形状感知能力。为此，借鉴大规模图像分割技术，结合动态平移区域变换器（DSRT）和自适应伪标签的渐进学习框架，提出了HSIseg这一新的高光谱图像分类范式和基线方法。同时引入多源数据协作来提高特征交互效果，在五个公开的高光谱数据集上的验证显示该方法优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>高光谱图像分类面临挑战：小补丁分类器在空间结构信息上的不足导致噪声干扰和误分类。</li>
<li>借鉴大规模图像分割技术来处理对象边界问题，有助于解决语义标注问题。</li>
<li>动态平移区域变换器（DSRT）在分割技术的整合中发挥关键作用。</li>
<li>提出渐进学习框架和自适应伪标签方法，将未标记区域纳入训练过程。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12541">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-24b390076b57ef9dae91c8e97ce84791.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3e0584f7ed8aea1b73ff49adfe1e934.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Analytical-Diagonalization-of-Fermi-Gas-like-Hamiltonians-using-the-Sommerfeld-Watson-Transformation"><a href="#Analytical-Diagonalization-of-Fermi-Gas-like-Hamiltonians-using-the-Sommerfeld-Watson-Transformation" class="headerlink" title="Analytical Diagonalization of Fermi Gas-like Hamiltonians using the   Sommerfeld-Watson Transformation"></a>Analytical Diagonalization of Fermi Gas-like Hamiltonians using the   Sommerfeld-Watson Transformation</h2><p><strong>Authors:G. Diniz, F. D. Picoli, M. P. Lenzarini</strong></p>
<p>The Sommerfeld-Watson transformation is a powerful mathematical technique widely used in physics to simplify summations over discrete quantum numbers by converting them into contour integrals in the complex plane. This method has applications in scattering theory, high-energy physics, quantum field theory, and electrostatics. A lesser-known but significant use is in the analytical diagonalization of specific Hamiltonians in condensed matter physics, such as the Fermi gas Hamiltonian and the single-impurity Anderson model with vanishing Coulomb repulsion. These models are used to describe important phenomena like conductance in metals, x-ray photoemission, and aspects of the Kondo problem. In this work, we provide a comprehensive explanation of the Sommerfeld-Watson transformation and its application in diagonalization procedures for these models, using modern notation to enhance clarity for new students. The analytical results were validated against the numerical diagonalization, showing excellent agreement. Furthermore, we extend the presented method to a more generalized non-interacting single-impurity Anderson model with variable couplings and arbitrary band dispersion. The procedure presented here successfully achieved the analytical diagonalization of this more complex model, providing a unified solution that encompasses simpler cases. To our knowledge, this general solution has not been previously reported. </p>
<blockquote>
<p>Sommerfeld-Watson转换是一种强大的数学技术，在物理学中广泛应用，通过将其转换为复平面上的轮廓积分来简化离散量子数的求和。该方法在散射理论、高能物理、量子场论和静电学等领域有应用。其较少为人知但重要的应用是在凝聚态物理学中对特定哈密顿量的分析对角化，例如费米气体哈密顿量和无库仑排斥的单杂质安德森模型。这些模型用于描述金属中的电导、X射线光电子发射和康多问题的某些方面等重要现象。在这项工作中，我们使用现代符号体系，为新的学生提供更清晰的解释，全面解释了Sommerfeld-Watson转换及其在这些模型对角化程序中的应用。分析的结果经过与数值对角化的验证，显示出极好的一致性。此外，我们将所介绍的方法扩展到了具有可变耦合和任意带散射的更通用的非相互作用单杂质安德森模型。这里介绍的程序成功地实现了这个更复杂模型的分析对角化，提供了一个涵盖简单情况的综合解决方案。据我们所知，这一通用解决方案尚未有报道过。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12402v1">PDF</a> </p>
<p><strong>Summary</strong><br>     索末菲尔德-沃森变换是一种强大的数学技巧，广泛应用于物理学中，通过将其转换为复平面上的轮廓积分来简化离散量子数的求和。此方法在散射理论、高能物理、量子场论和静电学中都有应用。它在凝聚态物理中对特定哈密顿量的解析对角化方面的应用虽然鲜为人知，但也是重要的应用之一，如费米气体哈密顿量和无库仑排斥的单杂质安德森模型。这些模型被用来描述金属中的导电现象、X射线光电子发射和康多问题的某些方面。本文提供了索末菲尔德-沃森变换及其在这些模型对角化程序中的应用的综合解释，并使用现代符号来提高新学生的清晰度。解析结果与数值对角化的验证结果吻合良好。此外，我们将所介绍的方法扩展到了更通用的具有可变耦合和任意带色散的非相互作用单杂质安德森模型。本文所介绍的程序成功地实现了这一更复杂模型的解析对角化，提供了一个涵盖简单情况的综合解决方案。据我们所知，这种一般解决方案以前尚未报道。</p>
<p><strong>Key Takeaways</strong></p>
<pre><code>* 索末菲尔德-沃森变换是一种强大的数学技巧，广泛应用于物理学中的多个领域。
* 此方法能够简化离散量子数的求和，通过将它们转换为复平面上的轮廓积分。
* 索末菲尔德-沃森变换在凝聚态物理中对特定哈密顿量的解析对角化方面有重要应用。
* 文章提到了两个具体的模型：费米气体哈密顿量和单杂质安德森模型，这些模型在描述物理现象方面具有重要的应用价值。
* 文章提供了索末菲尔德-沃森变换的综合解释，并展示了其在对角化程序中的应用。
* 解析结果与数值对角化的验证结果一致，证明了方法的可靠性。
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12402">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cefeb8daeca197d2b184d5bd46eac0f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac4f25df2df752b9dc0e7edfc75bf9eb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Modelling-the-impact-of-Multi-Cancer-Early-Detection-tests-a-review-of-natural-history-of-disease-models"><a href="#Modelling-the-impact-of-Multi-Cancer-Early-Detection-tests-a-review-of-natural-history-of-disease-models" class="headerlink" title="Modelling the impact of Multi Cancer Early Detection tests: a review of   natural history of disease models"></a>Modelling the impact of Multi Cancer Early Detection tests: a review of   natural history of disease models</h2><p><strong>Authors:O Mandrik, S Whyte, N Kunst, A Rayner, M Harden, S Dias, K Payne, S Palmer, MO Soares</strong></p>
<p>Introduction: The potential for multi-cancer early detection (MCED) tests to detect cancer at earlier stages is currently being evaluated in screening clinical trials. Once trial evidence becomes available, modelling will be necessary to predict impacts on final outcomes (benefits and harms), account for heterogeneity in determining clinical and cost-effectiveness, and explore alternative screening programme specifications. The natural history of disease (NHD) component of a MCED model will use statistical, mathematical or calibration methods. Methods: Modelling approaches for MCED screening that include an NHD component were identified from the literature, reviewed and critically appraised. Purposively selected (non-MCED) cancer screening models were also reviewed. The appraisal focussed on the scope, data sources, evaluation approaches and the structure and parameterisation of the models. Results: Five different MCED NHD models were identified and reviewed, alongside four additional (non-MCED) models. The critical appraisal highlighted several features of this literature. In the absence of trial evidence, MCED effects are based on predictions derived from test accuracy. These predictions rely on simplifying assumptions with unknown impacts, such as the stage-shift assumption used to estimate mortality impacts from predicted stage-shifts. None of the MCED models fully characterised uncertainty in the NHD or examined uncertainty in the stage-shift assumption. Conclusion: MCED technologies are developing rapidly, and large and costly clinical studies are being designed and implemented across the globe. Currently there is no modelling approach that can integrate clinical study evidence and therefore, in support of policy, it is important that similar efforts are made in the development of MCED models that make best use of the available data on benefits and harms. </p>
<blockquote>
<p>引言：多癌早期检测（MCED）试验在筛查临床试验中评估检测癌症早期阶段的可能性。一旦试验证据可用，建模将必不可少，以预测对最终结果（利益和危害）的影响，考虑临床和成本效益的异质性，并探索替代筛查方案规格。MCED模型的疾病自然史（NHD）部分将使用统计、数学或校准方法。方法：从文献中确定了包含NHD部分的MCED筛查建模方法，并对其进行了评论和批判性评估。还审查了有意选择的（非MCED）癌症筛查模型。评估的重点是模型的范围、数据来源、评估方法以及模型和参数的结构。结果：识别并审查了5种不同的MCED NHD模型以及4种额外的（非MCED）模型。批判性评估突出了这篇文献的几个特点。在没有试验证据的情况下，MCED的效果是基于测试准确度的预测。这些预测依赖于简化假设，其影响未知，例如用于估计预期阶段转变对死亡率影响的阶段转变假设。没有一个MCED模型能完全刻画NHD中的不确定性，也没有一个模型能检验阶段转变假设中的不确定性。结论：MCED技术正在迅速发展，全球范围内正在设计和实施大规模且昂贵的临床研究。目前尚无建模方法能够整合临床研究证据，因此，为了支持政策制定，在开发MCED模型方面做出类似努力至关重要，这些模型应最大限度地利用关于利益和危害的可用数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12351v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了多癌症早期检测（MCED）试验在筛查临床试验中检测癌症早期阶段的潜力。文章指出，在试验证据可用后，建模是必要的，以预测对最终结局的影响（包括利益和危害），并探讨了不同筛查方案规格的问题。文章通过文献综述和评估了MCED筛查的建模方法，重点关注了模型的范围、数据来源、评估方法等。然而，当前MCED模型存在不确定性，特别是在自然病史（NHD）方面，并且缺乏对阶段转移假设的考察。因此，需要发展能更好地利用可用数据的MCED模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MCED试验具有在较早阶段检测癌症的潜力，目前正在筛选临床试验中进行评估。</li>
<li>在试验证据可用后，建模是必要的，以预测对最终结局的影响，并考虑临床和成本效益的异质性。</li>
<li>MCED模型的自然病史（NHD）部分将使用统计、数学或校准方法。</li>
<li>文献综述和评估发现MCED筛查的建模方法存在不确定性，特别是在自然病史方面。</li>
<li>MCED效应基于测试准确性的预测，这些预测依赖于未知影响的简化假设，如阶段转移假设。</li>
<li>目前的MCED模型未能充分刻画自然病史的不确定性，也未考察阶段转移假设的不确定性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12351">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7b0ebf0d5f80e5cc8ca638b70806e9be.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Data-Efficient-Limited-Angle-CT-Using-Deep-Priors-and-Regularization"><a href="#Data-Efficient-Limited-Angle-CT-Using-Deep-Priors-and-Regularization" class="headerlink" title="Data-Efficient Limited-Angle CT Using Deep Priors and Regularization"></a>Data-Efficient Limited-Angle CT Using Deep Priors and Regularization</h2><p><strong>Authors:Ilmari Vahteristo, Zhi-Song Liu, Andreas Rupp</strong></p>
<p>Reconstructing an image from its Radon transform is a fundamental computed tomography (CT) task arising in applications such as X-ray scans. In many practical scenarios, a full 180-degree scan is not feasible, or there is a desire to reduce radiation exposure. In these limited-angle settings, the problem becomes ill-posed, and methods designed for full-view data often leave significant artifacts. We propose a very low-data approach to reconstruct the original image from its Radon transform under severe angle limitations. Because the inverse problem is ill-posed, we combine multiple regularization methods, including Total Variation, a sinogram filter, Deep Image Prior, and a patch-level autoencoder. We use a differentiable implementation of the Radon transform, which allows us to use gradient-based techniques to solve the inverse problem. Our method is evaluated on a dataset from the Helsinki Tomography Challenge 2022, where the goal is to reconstruct a binary disk from its limited-angle sinogram. We only use a total of 12 data points–eight for learning a prior and four for hyperparameter selection–and achieve results comparable to the best synthetic data-driven approaches. </p>
<blockquote>
<p>从Radon变换重建图像是计算机断层扫描（CT）中的一个基本任务，出现在X射线扫描等应用中。在许多实际场景中，完整的180度扫描并不可行，或者存在减少辐射暴露的需求。在这些有限角度的设置下，问题变得不适定，专为全视角数据设计的方法通常会产生明显的伪影。我们提出了一种在严重角度限制下从Radon变换重建原始图像的低数据方法。由于反问题是适定的，我们结合了多种正则化方法，包括全变差、辛图滤波器、深度图像先验和补丁级别的自动编码器。我们使用Radon变换的可微实现，这使我们能够使用基于梯度的方法来解决反问题。我们的方法在赫尔辛基断层扫描挑战赛2022的数据集上进行了评估，该数据集的目标是从其有限的角辛图重建一个二进制磁盘。我们只使用了总共12个数据点——8个用于学习先验知识，4个用于超参数选择——并取得了与最佳合成数据驱动方法相当的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12293v1">PDF</a> 12 pages, 2 reference pages, 5 figures, submitted to SCIA 2024</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了在CT扫描（计算机断层扫描）中，如何从Radon变换重建图像的问题。在实际应用中，由于角度限制或辐射暴露问题，通常无法完成完整的180度扫描。作者提出了一种在严重角度限制下，利用极少量数据从Radon变换重建原始图像的方法。该方法结合了多种正则化方法，并使用可微分的Radon变换实现，以梯度为基础解决反问题。在Helsinki Tomography Challenge 2022的数据集上进行的评估表明，仅使用总计12个数据点（8个用于学习先验知识，4个用于选择超参数）即可获得与最佳合成数据驱动方法相当的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该文本讨论了CT扫描中从Radon变换重建图像的问题，特别是在有限角度下的挑战。</li>
<li>作者提出了一种在严重角度限制下，利用非常有限的数据进行图像重建的方法。</li>
<li>该方法结合了多种正则化技术，以改善在有限角度下的图像重建质量。</li>
<li>使用可微分的Radon变换实现，便于使用梯度下降法进行优化。</li>
<li>仅在Helsinki Tomography Challenge 2022的数据集上，使用少量数据点（总计12个）即可实现与最佳方法相当的性能。</li>
<li>该方法不仅用于学习先验知识，还用于超参数的选择。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12293">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f1e6532f8b0de928dce096b04f9ee07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5610d74ba60455e0339750a7a8e1e81e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fd78d7f9622c23b9e99204ebecdb88c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ClusMFL-A-Cluster-Enhanced-Framework-for-Modality-Incomplete-Multimodal-Federated-Learning-in-Brain-Imaging-Analysis"><a href="#ClusMFL-A-Cluster-Enhanced-Framework-for-Modality-Incomplete-Multimodal-Federated-Learning-in-Brain-Imaging-Analysis" class="headerlink" title="ClusMFL: A Cluster-Enhanced Framework for Modality-Incomplete Multimodal   Federated Learning in Brain Imaging Analysis"></a>ClusMFL: A Cluster-Enhanced Framework for Modality-Incomplete Multimodal   Federated Learning in Brain Imaging Analysis</h2><p><strong>Authors:Xinpeng Wang, Rong Zhou, Han Xie, Xiaoying Tang, Lifang He, Carl Yang</strong></p>
<p>Multimodal Federated Learning (MFL) has emerged as a promising approach for collaboratively training multimodal models across distributed clients, particularly in healthcare domains. In the context of brain imaging analysis, modality incompleteness presents a significant challenge, where some institutions may lack specific imaging modalities (e.g., PET, MRI, or CT) due to privacy concerns, device limitations, or data availability issues. While existing work typically assumes modality completeness or oversimplifies missing-modality scenarios, we simulate a more realistic setting by considering both client-level and instance-level modality incompleteness in this study. Building on this realistic simulation, we propose ClusMFL, a novel MFL framework that leverages feature clustering for cross-institutional brain imaging analysis under modality incompleteness. Specifically, ClusMFL utilizes the FINCH algorithm to construct a pool of cluster centers for the feature embeddings of each modality-label pair, effectively capturing fine-grained data distributions. These cluster centers are then used for feature alignment within each modality through supervised contrastive learning, while also acting as proxies for missing modalities, allowing cross-modal knowledge transfer. Furthermore, ClusMFL employs a modality-aware aggregation strategy, further enhancing the model’s performance in scenarios with severe modality incompleteness. We evaluate the proposed framework on the ADNI dataset, utilizing structural MRI and PET scans. Extensive experimental results demonstrate that ClusMFL achieves state-of-the-art performance compared to various baseline methods across varying levels of modality incompleteness, providing a scalable solution for cross-institutional brain imaging analysis. </p>
<blockquote>
<p>多模态联邦学习（MFL）已成为一种有前途的方法，用于在分布式客户端上协同训练多模态模型，特别是在医疗领域。在脑成像分析的背景下，模态不完整性问题是一个巨大的挑战，一些机构可能会因隐私担忧、设备限制或数据可用性等问题而缺少特定的成像模态（例如PET、MRI或CT）。虽然现有工作通常假设模态完整性或过于简化缺失模态场景，本研究在考虑客户端和实例级模态不完整性的情况下，模拟了一个更现实的环境。基于这种现实模拟，我们提出了ClusMFL，这是一种新的MFL框架，它利用特征聚类在模态不完整的情况下进行跨机构脑成像分析。具体来说，ClusMFL使用FINCH算法为每种模态标签对构建特征嵌入的簇中心池，有效捕获细粒度数据分布。这些簇中心随后用于通过监督对比学习进行每种模态内的特征对齐，同时作为缺失模态的代理，实现跨模态知识转移。此外，ClusMFL采用了一种模态感知聚合策略，进一步提高了在严重模态不完整场景中的模型性能。我们在ADNI数据集上评估了所提出的框架，利用结构MRI和PET扫描。大量的实验结果表明，与各种基线方法相比，ClusMFL在不同程度的模态不完整性上达到了最先进的性能，为跨机构脑成像分析提供了可伸缩的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12180v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究考虑了在医疗领域尤其是脑成像分析中面临的多模态数据缺失问题，提出了基于特征聚类的多模态联邦学习框架ClusMFL。针对客户端和实例级别的模态缺失问题，ClusMFL通过利用FINCH算法构建模态标签对的特征嵌入簇中心池，实现跨机构脑成像分析在模态缺失下的精细粒度数据处理。此外，通过监督对比学习进行特征对齐，并采用模态感知聚合策略，提高在严重模态缺失场景下的模型性能。在ADNI数据集上的实验表明，ClusMFL在不同水平的模态缺失情况下均达到最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态联邦学习（MFL）是处理分布式客户端上的多模态模型的有效方法，特别是在医疗领域。</li>
<li>现实情况下存在模态缺失问题，本研究考虑了客户端和实例级别的模态缺失。</li>
<li>ClusMFL利用特征聚类解决模态缺失问题，通过构建模态标签对的特征嵌入簇中心池来捕捉精细粒度的数据分布。</li>
<li>ClusMFL采用监督对比学习进行特征对齐，并使用这些簇中心作为缺失模态的代理，实现跨模态知识转移。</li>
<li>ClusMFL采用模态感知聚合策略，增强在严重模态缺失情况下的模型性能。</li>
<li>在ADNI数据集上的实验验证了ClusMFL的有效性，其在不同水平的模态缺失情况下均表现最佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12180">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-91b09b4debcc9b4dad20aed9b3ac1767.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4543902b3e2a9b1b7ec1a4f968d03d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21604b16eb137d7bfdf5c1e63d784551.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-720996b2ec36cdd66f40332467c7c8bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62505a70b758e7fece3ebeaca977e8c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80eed5f9164525dc31c70fd6372056c4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Coherent-Superconductor-Semiconductor-Epitaxy-for-Integrated-Quantum-Electronics"><a href="#Coherent-Superconductor-Semiconductor-Epitaxy-for-Integrated-Quantum-Electronics" class="headerlink" title="Coherent Superconductor-Semiconductor Epitaxy for Integrated Quantum   Electronics"></a>Coherent Superconductor-Semiconductor Epitaxy for Integrated Quantum   Electronics</h2><p><strong>Authors:Julian A. Steele, Patrick J. Strohbeen, Carla Verdi, Ardeshir Baktash, Alisa Danilenko, Yi-Hsun Chen, Jechiel van Dijk, Lianzhou Wang, Eugene Demler, Salva Salmani-Rezaie, Peter Jacobson, Javad Shabani</strong></p>
<p>Introducing superconductivity into group IV elements by doping has long promised a pathway to introduce quantum functionalities into well-established semiconductor technologies. The non-equilibrium hyperdoping of group III atoms into Si or Ge has successfully shown superconductivity can be achieved, however, the origin of superconductivity has been obscured by structural disorder and dopant clustering. Here, we report the epitaxial growth of hyperdoped Ga:Ge films by molecular beam epitaxy with extreme hole concentrations ($n_\textup{h} &#x3D; 4.15 \times 10^{21}$<del>cm$^{-3}$, ~17.9% Ga substitution) that yield superconductivity with a critical temperature of $T_{\textup{c}} &#x3D; 3.5$</del>K and an out-of-plane critical field of 1<del>T at 270</del>mK. Synchrotron-based X-ray absorption and scattering methods reveal that Ga dopants are substitutionally incorporated within the Ge lattice, introducing a tetragonal distortion to the crystal unit cell. Our findings, corroborated by first-principles calculations, suggest that the structural order of Ga dopants creates a narrow band for the emergence of superconductivity in Ge, establishing hyperdoped Ga:Ge as a low-disorder, epitaxial superconductor-semiconductor platform. </p>
<blockquote>
<p>将超导性引入第四族元素通过掺杂长期以来为将量子功能引入成熟的半导体技术中开辟了一条道路。第三族原子对Si或Ge的非平衡超掺杂已成功证明可以实现超导性。然而，由于结构无序和掺杂剂聚集，超导性的起源变得模糊不清。在这里，我们报告了通过分子束外延方法实现超掺杂Ga:Ge薄膜的外延生长，具有极高的空穴浓度（nh&#x3D; 4.15×10^21 cm^-3，约17.9%的Ga替代），产生超导性，临界温度Tc&#x3D; 3.5 K，在270 mK时的面外临界场为1 T。基于同步辐射的X射线吸收和散射方法显示，Ga掺杂剂被替代性地掺入Ge晶格中，为晶胞引入了四方畸变。我们的发现得到了第一性原理计算的证实，表明Ga掺杂剂的结构顺序为Ge中出现超导性开辟了一个狭窄的通道，确立了超掺杂Ga:Ge作为低无序、外延的超导体-半导体平台。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15421v2">PDF</a> </p>
<p><strong>Summary</strong><br>     掺杂III族原子进入硅或锗的非平衡超掺杂已成功实现超导性，但结构无序和掺杂剂聚集使得超导性的起源变得模糊。本研究报告通过分子束外延技术外延生长超掺杂Ga:Ge薄膜，实现了极端空穴浓度下的超导性，关键温度达到3.5K，并在极低温度下具有临界场强度。同步辐射X射线吸收和散射方法揭示Ga掺杂剂在Ge晶格中的替代位置，引入了四方晶胞畸变。本研究建立超掺杂Ga:Ge作为低无序、外延超导半导体平台。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>通过掺杂III族原子进入集团IV元素（如Si或Ge）可以实现超导性。</li>
<li>超掺杂技术成功在Ga:Ge薄膜中实现超导性，关键温度达到3.5K。</li>
<li>Ga掺杂剂在Ge晶格中的位置对超导性的出现有重要影响，引入四方晶胞畸变。</li>
<li>高浓度的Ga掺杂剂（$n_\text{h} &#x3D; 4.15 \times 10^{21}$ cm$^{-3}$）对超导性能的提升起到关键作用。</li>
<li>通过分子束外延技术实现了极端空穴浓度下的Ga:Ge薄膜的外延生长。</li>
<li>同步辐射X射线吸收和散射方法被用来揭示Ga掺杂剂在Ge晶格中的结构位置。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15421">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-db586723b8ac3396600b2f54bfd37625.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5823296647eac077ea43ed22951ef143.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24c1bacc687655b15f104fd98c83923d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5d520d34ddcbafef3c99faa33157755.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1805d9797ac49382454f7790c02ea541.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Text4Seg-Reimagining-Image-Segmentation-as-Text-Generation"><a href="#Text4Seg-Reimagining-Image-Segmentation-as-Text-Generation" class="headerlink" title="Text4Seg: Reimagining Image Segmentation as Text Generation"></a>Text4Seg: Reimagining Image Segmentation as Text Generation</h2><p><strong>Authors:Mengcheng Lan, Chaofeng Chen, Yue Zhou, Jiaxing Xu, Yiping Ke, Xinjiang Wang, Litong Feng, Wayne Zhang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have shown exceptional capabilities in vision-language tasks; however, effectively integrating image segmentation into these models remains a significant challenge. In this paper, we introduce Text4Seg, a novel text-as-mask paradigm that casts image segmentation as a text generation problem, eliminating the need for additional decoders and significantly simplifying the segmentation process. Our key innovation is semantic descriptors, a new textual representation of segmentation masks where each image patch is mapped to its corresponding text label. This unified representation allows seamless integration into the auto-regressive training pipeline of MLLMs for easier optimization. We demonstrate that representing an image with $16\times16$ semantic descriptors yields competitive segmentation performance. To enhance efficiency, we introduce the Row-wise Run-Length Encoding (R-RLE), which compresses redundant text sequences, reducing the length of semantic descriptors by 74% and accelerating inference by $3\times$, without compromising performance. Extensive experiments across various vision tasks, such as referring expression segmentation and comprehension, show that Text4Seg achieves state-of-the-art performance on multiple datasets by fine-tuning different MLLM backbones. Our approach provides an efficient, scalable solution for vision-centric tasks within the MLLM framework. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在视觉语言任务中表现出了卓越的能力；然而，有效地将图像分割集成到这些模型中仍然是一个巨大的挑战。在本文中，我们介绍了Text4Seg，这是一种新的文本作为掩膜范式，它将图像分割转化为文本生成问题，无需额外的解码器，从而极大地简化了分割过程。我们的关键创新之处在于语义描述符，这是一种新的分割掩膜文本表示，其中每个图像块都映射到其相应的文本标签。这种统一表示允许无缝集成到MLLMs的自回归训练管道中，更易于优化。我们证明，使用$16\times16$语义描述符表示图像可以获得具有竞争力的分割性能。为了提高效率，我们引入了行运行长度编码（R-RLE），它压缩了冗余的文本序列，将语义描述符的长度减少了74%，推理速度提高了3倍，同时不损害性能。在各种视觉任务上的大量实验，如指代表达式分割和理解，表明Text4Seg通过微调不同的MLLM主干在多个数据集上实现了最先进的性能。我们的方法为MLLM框架内的以视觉为中心的任务提供了高效、可伸缩的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09855v2">PDF</a> ICLR 2025. Project page: <a target="_blank" rel="noopener" href="https://mc-lan.github.io/Text4Seg/">https://mc-lan.github.io/Text4Seg/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Text4Seg的新方法，将图像分割转化为文本生成问题，通过语义描述符这一新的文本表示方式，实现了与多模态大型语言模型（MLLMs）的紧密集成。此方法消除了对额外解码器的需求，简化了分割过程。通过引入行级运行长度编码（R-RLE），提高了效率，同时保持了在多种视觉任务上的卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text4Seg将图像分割转化为文本生成问题，简化了分割过程。</li>
<li>语义描述符是Text4Seg的关键创新，它将图像补丁映射到相应的文本标签，实现了统一表示。</li>
<li>语义描述符的应用使得图像能以更紧凑的方式表示，提高了计算效率。</li>
<li>Row-wise Run-Length Encoding（R-RLE）技术减少了文本序列的冗余，降低了语义描述符的长度，并加速了推理过程。</li>
<li>Text4Seg在多种视觉任务上实现了最先进的性能，如指代表达式分割和理解。</li>
<li>Text4Seg方法可以通过微调不同的MLLM主干来实现高效、可扩展的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09855">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b3d4f5cd887ad3bbac3dec46166b7ba5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd4f24a015a53d710b08f545a2073b18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-858ccdacf8ca41c88f54734768047cd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab6fb150da0f35295514a7443fc789c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae88ce84cfd228285f41f3c077c9c7d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c2e661cc7095359e63d09565e5ca591.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MedCLIP-SAMv2-Towards-Universal-Text-Driven-Medical-Image-Segmentation"><a href="#MedCLIP-SAMv2-Towards-Universal-Text-Driven-Medical-Image-Segmentation" class="headerlink" title="MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation"></a>MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation</h2><p><strong>Authors:Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao</strong></p>
<p>Segmentation of anatomical structures and pathological regions in medical images is essential for modern clinical diagnosis, disease research, and treatment planning. While significant advancements have been made in deep learning-based segmentation techniques, many of these methods still suffer from limitations in data efficiency, generalizability, and interactivity. As a result, developing precise segmentation methods that require fewer labeled datasets remains a critical challenge in medical image analysis. Recently, the introduction of foundation models like CLIP and Segment-Anything-Model (SAM), with robust cross-domain representations, has paved the way for interactive and universal image segmentation. However, further exploration of these models for data-efficient segmentation in medical imaging is still needed and highly relevant. In this paper, we introduce MedCLIP-SAMv2, a novel framework that integrates the CLIP and SAM models to perform segmentation on clinical scans using text prompts, in both zero-shot and weakly supervised settings. Our approach includes fine-tuning the BiomedCLIP model with a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss, and leveraging the Multi-modal Information Bottleneck (M2IB) to create visual prompts for generating segmentation masks from SAM in the zero-shot setting. We also investigate using zero-shot segmentation labels within a weakly supervised paradigm to enhance segmentation quality further. Extensive testing across four diverse segmentation tasks and medical imaging modalities (breast tumor ultrasound, brain tumor MRI, lung X-ray, and lung CT) demonstrates the high accuracy of our proposed framework. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/MedCLIP-SAMv2">https://github.com/HealthX-Lab/MedCLIP-SAMv2</a>. </p>
<blockquote>
<p>医学图像中的解剖结构和病理区域的分割对于现代临床诊断、疾病研究和治疗计划制定至关重要。虽然基于深度学习的分割技术已经取得了重大进展，但这些方法中的许多在数据效率、通用性和交互性方面仍存在局限性。因此，开发需要较少标注数据集的精确分割方法仍然是医学图像分析中的一个关键挑战。最近，引入具有稳健跨域表示能力的CLIP和Segment-Anything-Model（SAM）等基础模型，为交互式和通用图像分割铺平了道路。然而，针对医学成像中的数据高效分割，这些模型的进一步探索仍然是需要且高度相关的。在本文中，我们介绍了MedCLIP-SAMv2，这是一个结合CLIP和SAM模型的新框架，可以使用文本提示对临床扫描进行零样本和弱监督环境下的分割。我们的方法包括使用新的解耦硬负噪声对比估计（DHN-NCE）损失对BiomedCLIP模型进行微调，并利用多模式信息瓶颈（M2IB）创建视觉提示，以便在零样本环境中从SAM生成分割掩膜。我们还研究在弱监督范式中使用零样本分割标签，以进一步提高分割质量。在四个不同的分割任务和医学成像模式（乳腺肿瘤超声、脑肿瘤MRI、肺部X射线和肺部CT）的广泛测试表明，我们提出的框架具有很高的准确性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/MedCLIP-SAMv2%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HealthX-Lab/MedCLIP-SAMv2找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19483v4">PDF</a> 10 pages, 2 figures, 6 tables</p>
<p><strong>Summary</strong><br>    本文提出MedCLIP-SAMv2框架，结合CLIP和SAM模型，利用文本提示进行医学图像分割，涉及零样本和弱监督设置。通过微调BiomedCLIP模型并应用DHN-NCE损失和M2IB方法，实现在多种医学图像模态和分割任务上的高准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割对于现代临床诊断、疾病研究和治疗计划至关重要。</li>
<li>虽然深度学习在医学图像分割上取得显著进展，但仍存在数据效率、通用性和交互性的挑战。</li>
<li>MedCLIP-SAMv2框架结合了CLIP和SAM模型，用于医学图像分割。</li>
<li>MedCLIP-SAMv2通过文本提示进行分割，适用于零样本和弱监督设置。</li>
<li>框架通过微调BiomedCLIP模型并应用DHN-NCE损失提高分割准确性。</li>
<li>M2IB方法用于创建视觉提示，生成SAM的分割掩膜。</li>
<li>在多种医学图像模态和分割任务上的测试证明了该框架的高准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19483">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d657651226aaccedaa9dd3a03afefce3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd0a68de1f8a6fcfd097c9f41acedbfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b446fbe0406c4b802dc70ba573d20fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16065c2f255030fed0c5141a146a0962.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="PTQ4RIS-Post-Training-Quantization-for-Referring-Image-Segmentation"><a href="#PTQ4RIS-Post-Training-Quantization-for-Referring-Image-Segmentation" class="headerlink" title="PTQ4RIS: Post-Training Quantization for Referring Image Segmentation"></a>PTQ4RIS: Post-Training Quantization for Referring Image Segmentation</h2><p><strong>Authors:Xiaoyan Jiang, Hang Yang, Kaiying Zhu, Xihe Qiu, Shibo Zhao, Sifan Zhou</strong></p>
<p>Referring Image Segmentation (RIS), aims to segment the object referred by a given sentence in an image by understanding both visual and linguistic information. However, existing RIS methods tend to explore top-performance models, disregarding considerations for practical applications on resources-limited edge devices. This oversight poses a significant challenge for on-device RIS inference. To this end, we propose an effective and efficient post-training quantization framework termed PTQ4RIS. Specifically, we first conduct an in-depth analysis of the root causes of performance degradation in RIS model quantization and propose dual-region quantization (DRQ) and reorder-based outlier-retained quantization (RORQ) to address the quantization difficulties in visual and text encoders. Extensive experiments on three benchmarks with different bits settings (from 8 to 4 bits) demonstrates its superior performance. Importantly, we are the first PTQ method specifically designed for the RIS task, highlighting the feasibility of PTQ in RIS applications. Code and video are available at {<a target="_blank" rel="noopener" href="https://github.com/gugu511yy/PTQ4RIS%7D">https://github.com/gugu511yy/PTQ4RIS}</a>. </p>
<blockquote>
<p>参照图像分割（RIS）旨在通过理解视觉和语言信息，对给定句子中提及的对象进行图像分割。然而，现有的RIS方法往往探索性能优越模型，却忽视了在资源有限的边缘设备上实际应用的问题。这一疏忽给设备上的RIS推理带来了重大挑战。为此，我们提出了一种有效且高效的训练后量化框架，称为PTQ4RIS。具体来说，我们首先深入分析了RIS模型量化中性能下降的根本原因，并提出了双区域量化（DRQ）和基于重排的保留异常值量化（RORQ）来解决视觉和文本编码器的量化难题。在三个基准数据集上的实验，采用不同的比特设置（从8位到4位）证明了其卓越的性能。重要的是，我们是第一个专门针对RIS任务设计的PTQ方法，突显了PTQ在RIS应用中的可行性。代码和视频可在[<a target="_blank" rel="noopener" href="https://github.com/gugu511yy/PTQ4RIS]%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/gugu511yy/PTQ4RIS]查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17020v2">PDF</a> Accepted by ICRA 2025.(Update the code link.)</p>
<p><strong>Summary</strong><br>    提出一种针对图像分割的新方法——引用图像分割（RIS），综合考虑视觉和语言信息，但在实际应用于资源有限的边缘设备时面临挑战。为此，研究团队提出了一个高效且实用的后训练量化框架PTQ4RIS，深入分析了量化过程中性能下降的根本原因，并针对性地提出了双区域量化（DRQ）和基于重排序的异常保留量化（RORQ）方法来解决视觉和文本编码器的量化难题。在三个不同比特设置（从8位到4位）下的广泛实验证明其性能优越。此方法旨在为RIS任务进行特定设计的PTQ方法，突显PTQ在RIS应用中的可行性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引用图像分割（RIS）结合了视觉和语言信息，旨在根据句子中的描述在图像中分割目标对象。</li>
<li>现存的RIS方法更注重高性能模型，忽略了在资源受限的边缘设备上的实际应用考虑。</li>
<li>为解决这一问题，提出了后训练量化框架PTQ4RIS，以提高模型在边缘设备的效率和实用性。</li>
<li>PTQ4RIS通过深入分析发现，性能下降的主要原因是量化过程中的问题，特别是视觉和文本编码器的量化难题。</li>
<li>研究团队提出了双区域量化（DRQ）和基于重排序的异常保留量化（RORQ）方法来应对这些挑战。</li>
<li>在三个不同比特设置下的广泛实验证明了PTQ4RIS的优越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17020">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9c25d89eb7a6ed7dee47e044dc0fba6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02e409cbaa629077f77a70ca6c63f421.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-643ffe8871b3219d35c10ef19d2eca9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cafe1f31ee48bf748f04227b9baaac76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6458d9f3968af4b63eb3e5a764f6329.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b47b39abfc0cf001033414f4961f299.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="IRSRMamba-Infrared-Image-Super-Resolution-via-Mamba-based-Wavelet-Transform-Feature-Modulation-Model"><a href="#IRSRMamba-Infrared-Image-Super-Resolution-via-Mamba-based-Wavelet-Transform-Feature-Modulation-Model" class="headerlink" title="IRSRMamba: Infrared Image Super-Resolution via Mamba-based Wavelet   Transform Feature Modulation Model"></a>IRSRMamba: Infrared Image Super-Resolution via Mamba-based Wavelet   Transform Feature Modulation Model</h2><p><strong>Authors:Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Shinichiro Omachi</strong></p>
<p>Infrared image super-resolution demands long-range dependency modeling and multi-scale feature extraction to address challenges such as homogeneous backgrounds, weak edges, and sparse textures. While Mamba-based state-space models (SSMs) excel in global dependency modeling with linear complexity, their block-wise processing disrupts spatial consistency, limiting their effectiveness for IR image reconstruction. We propose IRSRMamba, a novel framework integrating wavelet transform feature modulation for multi-scale adaptation and an SSMs-based semantic consistency loss to restore fragmented contextual information. This design enhances global-local feature fusion, structural coherence, and fine-detail preservation while mitigating block-induced artifacts. Experiments on benchmark datasets demonstrate that IRSRMamba outperforms state-of-the-art methods in PSNR, SSIM, and perceptual quality. This work establishes Mamba-based architectures as a promising direction for high-fidelity IR image enhancement. Code are available at <a target="_blank" rel="noopener" href="https://github.com/yongsongH/IRSRMamba">https://github.com/yongsongH/IRSRMamba</a>. </p>
<blockquote>
<p>红外图像超分辨率处理需要建立长期依赖关系模型和多尺度特征提取，以解决背景均匀、边缘模糊和纹理稀疏等挑战。虽然基于Mamba的状态空间模型（SSMs）在线性复杂度下擅长全局依赖关系建模，但其分块处理会破坏空间一致性，限制了其在红外图像重建中的有效性。我们提出了IRSRMamba这一新型框架，它结合了基于小波变换的特征调制进行多尺度适配，以及基于SSMs的语义一致性损失来恢复分散的上下文信息。这种设计增强了全局和局部特征的融合、结构连贯性和细节保留性，同时减轻了因分块引起的伪影。在基准数据集上的实验表明，IRSRMamba在峰值信噪比（PSNR）、结构相似性（SSIM）和感知质量上均优于现有先进方法。本研究确立了基于Mamba的架构在高保真红外图像增强中的研究前景。代码可通过<a target="_blank" rel="noopener" href="https://github.com/yongsongH/IRSRMamba%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/yongsongH/IRSRMamba获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.09873v2">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong><br>医学红外图像超分辨率需要长程依赖建模和多尺度特征提取，以应对均匀背景、弱边缘和稀疏纹理等挑战。Mamba基状态空间模型虽擅长全局依赖建模，但其块状处理破坏了空间一致性，限制了红外图像重建的效果。本研究提出IRSRMamba框架，集成小波变换特征调制实现多尺度自适应，并引入SSM语义一致性损失恢复断裂的上下文信息。此设计提高了全局局部特征融合、结构连贯性和细节保留能力，同时减少了块状引起的伪影。实验表明，IRSRMamba在PSNR、SSIM和感知质量方面优于先进方法，确立了Mamba基架构在高保真红外图像增强中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学红外图像超分辨率面临的挑战包括均匀背景、弱边缘和稀疏纹理。</li>
<li>Mamba基状态空间模型在全局依赖建模方面具有优势，但块状处理会导致空间一致性问题。</li>
<li>IRSRMamba框架通过集成小波变换特征调制实现多尺度自适应。</li>
<li>SSMs语义一致性损失用于恢复断裂的上下文信息。</li>
<li>IRSRMamba提高了全局局部特征融合、结构连贯性和细节保留能力。</li>
<li>该框架减少了因块状处理引起的伪影。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.09873">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-723890e940e302c7028e2a84f942eb9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cd49631d97573a5cb3cf5290e969f03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fece709d518abca7bfa63cee5bd597e6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MedIAnomaly-A-comparative-study-of-anomaly-detection-in-medical-images"><a href="#MedIAnomaly-A-comparative-study-of-anomaly-detection-in-medical-images" class="headerlink" title="MedIAnomaly: A comparative study of anomaly detection in medical images"></a>MedIAnomaly: A comparative study of anomaly detection in medical images</h2><p><strong>Authors:Yu Cai, Weiwen Zhang, Hao Chen, Kwang-Ting Cheng</strong></p>
<p>Anomaly detection (AD) aims at detecting abnormal samples that deviate from the expected normal patterns. Generally, it can be trained merely on normal data, without a requirement for abnormal samples, and thereby plays an important role in the recognition of rare diseases and health screening in the medical domain. Despite the emergence of numerous methods for medical AD, we observe a lack of a fair and comprehensive evaluation, which causes ambiguous conclusions and hinders the development of this field. To address this problem, this paper builds a benchmark with unified comparison. Seven medical datasets with five image modalities, including chest X-rays, brain MRIs, retinal fundus images, dermatoscopic images, and histopathology whole slide images, are curated for extensive evaluation. Thirty typical AD methods, including reconstruction and self-supervised learning-based methods, are involved in comparison of image-level anomaly classification and pixel-level anomaly segmentation. Furthermore, for the first time, we formally explore the effect of key components in existing methods, clearly revealing unresolved challenges and potential future directions. The datasets and code are available at <a target="_blank" rel="noopener" href="https://github.com/caiyu6666/MedIAnomaly">https://github.com/caiyu6666/MedIAnomaly</a>. </p>
<blockquote>
<p>异常检测（AD）旨在检测与预期正常模式偏离的异常样本。通常，它仅能在正常数据上进行训练，无需异常样本，因此在医学领域的罕见疾病识别和健康筛查中发挥着重要作用。尽管出现了许多医学异常检测方法，但我们发现缺乏公平而全面的评估，这导致结论模糊并阻碍了该领域的发展。针对这一问题，本文建立了一个统一的比较基准。我们精心挑选了七个医学数据集，包含五种图像模态，包括胸部X射线、脑部MRI、眼底视网膜图像、皮肤镜图像和病理全切片图像，进行了广泛评估。本文比较了30种典型的异常检测方法，包括重建和基于自监督学习的方法，涉及图像级异常分类和像素级异常分割。此外，我们首次正式探讨了现有方法中的关键组件的影响，清晰地揭示了未解决的挑战和潜在的未来方向。数据集和代码可在<a target="_blank" rel="noopener" href="https://github.com/caiyu6666/MedIAnomaly%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/caiyu6666/MedIAnomaly上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.04518v3">PDF</a> Accepted to Medical Image Analysis, 2025</p>
<p><strong>Summary</strong><br>医学图像异常检测（AD）旨在发现偏离预期正常模式的异常样本，对于识别罕见疾病和健康筛查具有重要意义。本文建立了一个统一的基准测试平台，用于评估不同的医学图像AD方法。该平台涵盖了七种医学数据集和五种图像模态，包括胸部X射线、脑部MRI、眼底图像、皮肤镜图像和病理全切片图像等。此外，本文首次正式探讨了现有方法中的关键组件的影响，揭示了未解决的挑战和潜在的未来方向。数据集和代码可在GitHub上找到。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>异常检测（AD）在医学图像领域具有重要意义，用于识别罕见疾病和健康筛查。</li>
<li>医学图像AD方法的评估缺乏公平性和综合性，导致结论模糊，阻碍该领域的发展。</li>
<li>为解决这一问题，本文建立了一个统一的基准测试平台，包括七个医学数据集和五种图像模态。</li>
<li>平台涵盖了多种典型的AD方法，包括重建和自监督学习方法，用于比较图像级别的异常分类和像素级别的异常分割。</li>
<li>本文首次探讨了现有方法中的关键组件的影响，包括对异常检测性能的具体作用。</li>
<li>文章揭示了该领域的未解决挑战和潜在未来方向。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.04518">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-15143b7a79e82290d70ad0af6f1a3913.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59974efd00943517ad418cf9283d7874.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a118f40ca2b478b1b0ea6d5822d00f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1c9a2726146103ff9bf7457588f4bb5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-692d1410a958dc1179439a43805fb873.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Joint-enhancement-of-automatic-chest-X-ray-diagnosis-and-radiological-gaze-prediction-with-multi-stage-cooperative-learning"><a href="#Joint-enhancement-of-automatic-chest-X-ray-diagnosis-and-radiological-gaze-prediction-with-multi-stage-cooperative-learning" class="headerlink" title="Joint enhancement of automatic chest X-ray diagnosis and radiological   gaze prediction with multi-stage cooperative learning"></a>Joint enhancement of automatic chest X-ray diagnosis and radiological   gaze prediction with multi-stage cooperative learning</h2><p><strong>Authors:Zirui Qiu, Hassan Rivaz, Yiming Xiao</strong></p>
<p>Purpose: As visual inspection is an inherent process during radiological screening, the associated eye gaze data can provide valuable insights into relevant clinical decisions. As deep learning has become the state-of-the-art for computer-assisted diagnosis, integrating human behavior, such as eye gaze data, into these systems is instrumental to help align machine predictions with clinical diagnostic criteria, thus enhancing the quality of automatic radiological diagnosis. Methods: We propose a novel deep learning framework for joint disease diagnosis and prediction of corresponding clinical visual attention maps for chest X-ray scans. Specifically, we introduce a new dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for visual attention map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks in multi-task learning, we proposed a multi-stage cooperative learning strategy, with contrastive learning for feature encoder pretraining to boost performance. Results: Our proposed method is shown to significantly outperform existing techniques for chest X-ray diagnosis (AUC&#x3D;0.93) and the quality of visual attention map prediction (Correlation coefficient&#x3D;0.58). Conclusion: Benefiting from the proposed multi-task multi-stage cooperative learning, our technique demonstrates the benefit of integrating clinicians’ eye gaze into clinical AI systems to boost performance and potentially explainability. </p>
<blockquote>
<p>目的：视觉检查是放射学筛查过程中的固有过程，相关的眼动数据可以为相关的临床决策提供有价值的见解。随着深度学习成为计算机辅助诊断的最新技术，将人类行为（如眼动数据）整合到这些系统中，有助于使机器预测与临床诊断标准保持一致，从而提高自动放射学诊断的质量。方法：我们提出了一种新型的深度学习框架，用于联合疾病诊断和预测胸部X射线扫描对应的临床视觉注意力图。具体来说，我们引入了一种新的双编码器多任务UNet，它利用DenseNet201主干和基于残差和挤压激励块编码器来预测视觉注意力图特征，以及多尺度特征融合分类器进行疾病分类。为了解决多任务学习中单个任务异步训练日程的问题，我们提出了一种多阶段合作学习策略，使用对比学习对特征编码器进行预训练以提高性能。结果：所提出的方法在胸部X光诊断（AUC&#x3D;0.93）和视觉注意力图预测质量（相关系数&#x3D;0.58）方面显著优于现有技术。结论：得益于所提出的多任务多阶段合作学习，我们的技术证明了将临床医生眼动数据整合到临床人工智能系统中的好处，可以提高性能和潜在的解释性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16970v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型深度学习框架，用于联合诊断关节疾病并预测对应的临床视觉注意力地图。通过引入双编码器多任务UNet网络，结合DenseNet201骨干网络和基于残差与挤压激励块的编码器，以提取多样化的特征用于视觉注意力地图预测和多尺度特征融合分类器进行疾病分类。为解决多任务学习中单个任务训练安排不同步的问题，本文提出了一种多阶段协作学习策略，并利用对比学习对特征编码器进行预训练以提高性能。实验结果表明，该方法在胸部X光诊断上显著优于现有技术，且视觉注意力地图预测质量较高。整合医生视线数据有助于提高临床AI系统的性能和解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉检查是放射学筛查中的固有过程，相关的眼动数据能为临床决策提供宝贵见解。</li>
<li>深度学习已成为计算机辅助诊断的尖端技术，整合人类行为（如眼动数据）有助于使机器预测与临床诊断标准对齐，从而提高自动放射学诊断的质量。</li>
<li>本文提出一种新型深度学习框架，结合双编码器多任务UNet网络进行疾病诊断和视觉注意力地图预测。</li>
<li>该框架利用DenseNet201和基于残差与挤压激励块的编码器提取多样化特征，并采用多尺度特征融合分类器进行疾病分类。</li>
<li>为解决多任务学习中任务训练安排不同步的问题，采用多阶段协作学习策略。</li>
<li>对比学习用于特征编码器的预训练，以提升性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.16970">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-03e531c7197212bfeff092263dcf49ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c101f534c4f95b1b160641952cdef1d9.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="HyperFusion-A-Hypernetwork-Approach-to-Multimodal-Integration-of-Tabular-and-Medical-Imaging-Data-for-Predictive-Modeling"><a href="#HyperFusion-A-Hypernetwork-Approach-to-Multimodal-Integration-of-Tabular-and-Medical-Imaging-Data-for-Predictive-Modeling" class="headerlink" title="HyperFusion: A Hypernetwork Approach to Multimodal Integration of   Tabular and Medical Imaging Data for Predictive Modeling"></a>HyperFusion: A Hypernetwork Approach to Multimodal Integration of   Tabular and Medical Imaging Data for Predictive Modeling</h2><p><strong>Authors:Daniel Duenias, Brennan Nichyporuk, Tal Arbel, Tammy Riklin Raviv</strong></p>
<p>The integration of diverse clinical modalities such as medical imaging and the tabular data extracted from patients’ Electronic Health Records (EHRs) is a crucial aspect of modern healthcare. Integrative analysis of multiple sources can provide a comprehensive understanding of the clinical condition of a patient, improving diagnosis and treatment decision. Deep Neural Networks (DNNs) consistently demonstrate outstanding performance in a wide range of multimodal tasks in the medical domain. However, the complex endeavor of effectively merging medical imaging with clinical, demographic and genetic information represented as numerical tabular data remains a highly active and ongoing research pursuit.   We present a novel framework based on hypernetworks to fuse clinical imaging and tabular data by conditioning the image processing on the EHR’s values and measurements. This approach aims to leverage the complementary information present in these modalities to enhance the accuracy of various medical applications. We demonstrate the strength and generality of our method on two different brain Magnetic Resonance Imaging (MRI) analysis tasks, namely, brain age prediction conditioned by subject’s sex and multi-class Alzheimer’s Disease (AD) classification conditioned by tabular data. We show that our framework outperforms both single-modality models and state-of-the-art MRI tabular data fusion methods. A link to our code can be found at <a target="_blank" rel="noopener" href="https://github.com/daniel4725/HyperFusion">https://github.com/daniel4725/HyperFusion</a> </p>
<blockquote>
<p>多样临床模式如医学成像与从患者电子健康记录（EHRs）中提取的表格数据的融合是现代医疗保健的重要组成部分。对多个来源的综合分析可以提供对患者临床状况的全面了解，从而提高诊断和治疗决策。深度神经网络（DNNs）在医疗领域的多模式任务中始终表现出卓越的性能。然而，有效地将医学成像与临床、人口统计学和遗传信息融合，这些以数值表格数据的形式呈现，仍然是一项活跃而持续的研究追求。我们提出了一种基于超网络的新型框架，通过以电子健康记录的数值为条件来融合临床成像和表格数据。该方法旨在利用这些模式中的互补信息，以提高各种医疗应用的准确性。我们在两个不同的脑部磁共振成像（MRI）分析任务上展示了我们的方法的优势和通用性，即根据受试者性别进行脑龄预测和根据表格数据进行的多类阿尔茨海默病（AD）分类。我们显示，我们的框架优于单模态模型和最先进的MRI表格数据融合方法。我们的代码链接为：<a target="_blank" rel="noopener" href="https://github.com/daniel4725/HyperFusion">https://github.com/daniel4725/HyperFusion</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.13319v2">PDF</a> 20 pages, 11 figures</p>
<p><strong>Summary</strong><br>医学成像与电子健康记录（EHRs）表格数据的融合是现代医疗的重要方向。本文提出一种基于超网络的新框架，旨在通过以EHR的数值数据和测量结果为条件进行图像处理，利用两种模态的互补信息提升医疗应用的准确性。框架在大脑核磁共振成像的两个任务上表现出卓越性能：以性别为条件的脑龄预测和以表格数据为条件的阿尔茨海默病多类别分类。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学成像与电子健康记录（EHRs）表格数据的融合是现代医疗的关键需求。</li>
<li>深神经网络在多模态医学任务中表现优异。</li>
<li>新框架基于超网络，旨在融合医学成像和表格数据。</li>
<li>框架以EHR的数值数据和测量结果为条件进行图像处理。</li>
<li>框架在大脑核磁共振成像的脑龄预测和阿尔茨海默病分类任务上表现优越。</li>
<li>新框架优于单模态模型和先进的MRI表格数据融合方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.13319">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a079bc09178130aa40377f6608457005.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb63a3d0f162aec8027e0bb6095f2d14.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-40f1c3983e8b7242fabb49c350a2cb13.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-02-20  AV-Flow Transforming Text to Audio-Visual Human-like Interactions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-af1c7fd96c47838da7b7147b338a0e4a.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-02-20  Is Noise Conditioning Necessary for Denoising Generative Models?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16065k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
