<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-02-20  Do we still need Human Annotators? Prompting Large Language Models for   Aspect Sentiment Quad Prediction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f39c32afa7bb8bdf0e3667a1ff29a511.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    43 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-20-更新"><a href="#2025-02-20-更新" class="headerlink" title="2025-02-20 更新"></a>2025-02-20 更新</h1><h2 id="Do-we-still-need-Human-Annotators-Prompting-Large-Language-Models-for-Aspect-Sentiment-Quad-Prediction"><a href="#Do-we-still-need-Human-Annotators-Prompting-Large-Language-Models-for-Aspect-Sentiment-Quad-Prediction" class="headerlink" title="Do we still need Human Annotators? Prompting Large Language Models for   Aspect Sentiment Quad Prediction"></a>Do we still need Human Annotators? Prompting Large Language Models for   Aspect Sentiment Quad Prediction</h2><p><strong>Authors:Nils Constantin Hellwig, Jakob Fehle, Udo Kruschwitz, Christian Wolff</strong></p>
<p>Aspect sentiment quadruple prediction (ASQP) facilitates a detailed understanding of opinions expressed in a text by identifying the opinion term, aspect term, aspect category and sentiment polarity for each opinion. However, annotating a full set of training examples to fine-tune models for ASQP is a resource-intensive process. In this study, we explore the capabilities of large language models (LLMs) for zero- and few-shot learning on the ASQP task across five diverse datasets. We report F1 scores slightly below those obtained with state-of-the-art fine-tuned models but exceeding previously reported zero- and few-shot performance. In the 40-shot setting on the Rest16 restaurant domain dataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the best-performing fine-tuned method MVP. Additionally, we report the performance of LLMs in target aspect sentiment detection (TASD), where the F1 scores were also close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot setting, compared to 72.76 with MVP. While human annotators remain essential for achieving optimal performance, LLMs can reduce the need for extensive manual annotation in ASQP tasks. </p>
<blockquote>
<p>面向方面的情感四重预测（ASQP）能够通过识别每个意见的观点词、方面词、方面类别和情感极性，从而促进对文本中所表达意见的深入理解。然而，对全套训练例子进行标注以微调ASQP模型是一个资源密集型的流程。在本研究中，我们探索了大型语言模型（LLM）在五个不同数据集上的零样本和少样本学习在ASQP任务上的能力。我们报告的F1分数略低于使用最新微调模型得到的分数，但超过了之前报告的零样本和少样本性能。在Rest16餐厅域数据集的40个样本设置中，LLM的F1分数为52.46，而表现最佳的MVP微调方法的F1分数为60.39。此外，我们还报告了LLM在目标方面情感检测（TASD）中的表现，其F1分数也与微调模型相近。在Rest16的40个样本设置中，LLM的F1分数为66.03，而MVP的F1分数为72.76。虽然人类标注者对于实现最佳性能仍然至关重要，但LLM可以减少ASQP任务中对大量手动标注的需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13044v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLMs）在面向方面的情感四重预测（ASQP）任务中的零样本和少样本学习能力。研究发现在五个不同的数据集上，LLMs的性能虽然略低于经过微调的最先进模型，但在零样本和少样本情况下的性能却超过了之前的报告。特别是在Rest16餐厅领域数据集的40个样本情况下，LLMs的F1分数达到了52.46%，而最佳微调方法MVP的F1分数为60.39。此外，还报告了目标方面情感检测（TASD）中LLMs的性能，其F1分数接近经过微调模型，但仍有一定差距。虽然人类注释者对于实现最佳性能至关重要，但LLMs可以减少ASQP任务中对大量手动注释的需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Aspect sentiment quadruple prediction (ASQP) 能够深入理解文本中的意见，通过识别观点词、方面词、方面类别和情感极性来详细解析意见内容。</li>
<li>使用大型语言模型（LLMs）进行零样本和少样本学习在ASQP任务上具有可行性。</li>
<li>LLMs在五个不同数据集上的性能表现良好，虽然略低于经过精细调教的先进模型，但在零样本和少样本情境下超越了先前报告的性能。</li>
<li>在Rest16餐厅领域数据集的40个样本情况下，LLMs的F1分数达到52.46%，而最佳模型MVP的F1分数为60.39。</li>
<li>LLMs在目标方面情感检测（TASD）任务中也有良好表现，但距离经过微调的最优模型仍有一定差距。</li>
<li>虽然人类注释者对于获得最佳性能至关重要，但LLMs的应用可以减少对大量手动注释的需求，从而节省资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13044">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4b68e205d9f3b1f04ef1ec67b397935d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ebcf0d1a8b222d8171b948897bc904cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a29bab5da6966b85e087ce3267b2775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e45fe3397ae5f5b1cfb3f4dad94d853.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Scalable-Model-Merging-with-Progressive-Layer-wise-Distillation"><a href="#Scalable-Model-Merging-with-Progressive-Layer-wise-Distillation" class="headerlink" title="Scalable Model Merging with Progressive Layer-wise Distillation"></a>Scalable Model Merging with Progressive Layer-wise Distillation</h2><p><strong>Authors:Jing Xu, Jiazheng Li, Jingzhao Zhang</strong></p>
<p>Model merging offers an effective way to integrate the capabilities of multiple fine-tuned models. However, the performance degradation of the merged model remains a challenge, particularly when none or few data are available. This paper first highlights the necessity of domain-specific data for model merging by proving that data-agnostic algorithms can have arbitrarily bad worst-case performance. Building on this theoretical insight, we explore the relationship between model merging and distillation, introducing a novel few-shot merging algorithm, ProDistill (Progressive Layer-wise Distillation). Unlike common belief that layer wise training hurts performance, we show that layer-wise teacher-student distillation not only enhances the scalability but also improves model merging performance. We conduct extensive experiments to show that compared to existing few-shot merging methods, ProDistill achieves state-of-the-art performance, with up to 6.14% and 6.61% improvements in vision and NLU tasks. Furthermore, we extend the experiments to models with over 10B parameters, showcasing the exceptional scalability of ProDistill. </p>
<blockquote>
<p>模型融合为多个微调模型的集成提供了一种有效途径。然而，融合模型的性能下降仍然是一个挑战，特别是在没有或只有少量数据可用的情况下。本文首先通过证明数据无关算法可能具有任意差的性能来证明特定领域数据对模型融合的必要性。基于这一理论洞察，我们探索了模型融合与蒸馏之间的关系，引入了一种新型的少数样本融合算法——渐进层蒸馏（ProDistill）。与普遍认为的逐层训练会损害性能不同，我们展示了逐层师徒蒸馏不仅提高了可扩展性，还提高了模型融合性能。我们进行了大量实验，结果表明，与现有的少数样本融合方法相比，ProDistill实现了最先进的性能，在视觉和自然语言理解任务上分别提高了高达6.14%和6.61%。此外，我们将实验扩展到了参数超过10B的模型上，展示了ProDistill的卓越可扩展性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12706v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>模型融合是一种有效整合多个微调模型能力的方法，但融合模型的性能下降，特别是在无数据或少数据的情况下，仍然是一个挑战。本文首先通过证明数据无关算法在最坏情况下可能具有任意差的性能来强调领域特定数据对模型融合的必要性。在此基础上，本文探索了模型融合与蒸馏的关系，提出了一种新的少样本融合算法——ProDistill（渐进逐层蒸馏）。与普遍观点相反，本文显示逐层教师-学生蒸馏不仅提高了可扩展性，还提高了模型融合性能。通过大量实验表明，与现有的少样本融合方法相比，ProDistill实现了最先进的性能，在视觉和自然语言理解任务上分别提高了6.14%和6.61%。此外，我们还对超过1 结外拓展到参数超过10B的模型，展示了ProDistill的出色可扩展性。总的来说，该文提出了一种新的少样本融合算法ProDistill算法。通过蒸馏和模型融合的有效结合策略进行演示验证了算法的高效性。。该研究将为解决深度学习的难题如迁移学习和无监督学习提供了新的思路和方法。面向具体任务的特殊训练数据将提高模型的性能，特别是面对大规模数据集时。通过实验结果可以看出，ProDistill算法具有良好的泛化能力和实际应用前景。研究也证明了基于教师-学生模型的蒸馏方法在多模态数据处理中具有广阔的应用前景。它允许同时考虑多源数据的综合作用而不依赖于特定的数据集或任务类型。这为未来的研究开辟了新的方向。未来可以进一步探索如何优化蒸馏策略以适应不同任务的特性以及如何更好地将多种模态的数据进行有效融合以取得更好的性能。此外，该算法在更大规模数据集上的表现值得进一步深入研究。这些研究将有助于推动机器学习领域的发展并推动其在实际应用中的落地。对于实际应用场景如自然语言处理、计算机视觉等领域也可以进行更多的探索和研究工作。具体来说可以研究如何将这些算法应用于实际场景中解决现实世界中存在的问题如图像识别、自然语言翻译等任务中。通过进一步的研究和实践可以不断完善和改进这些算法从而提高其在现实世界中的应用效果。针对这些问题展开深入研究将有助于推动机器学习领域的发展并为相关领域提供有力的支持和技术保障。<strong>关键见解</strong></p>
<ul>
<li>强调了领域特定数据在模型融合中的重要性，并指出数据无关算法可能存在的性能问题。</li>
<li>提出了一种新的少样本融合算法ProDistill，通过渐进逐层蒸馏提高模型融合性能。</li>
<li>实验表明，ProDistill在视觉和自然语言理解任务上较现有方法有明显改进。</li>
<li>ProDistill具有出色的可扩展性，可应用于参数规模较大的模型。</li>
<li>逐层教师-学生蒸馏策略提高了模型融合的效率和性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12706">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9080cd3228379a64049d5df2316ed2d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09f1d8f1c229d50a89849029048be7cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ed86881519d1b1f437ef09c5fc389c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Label-Drop-for-Multi-Aspect-Relation-Modeling-in-Universal-Information-Extraction"><a href="#Label-Drop-for-Multi-Aspect-Relation-Modeling-in-Universal-Information-Extraction" class="headerlink" title="Label Drop for Multi-Aspect Relation Modeling in Universal Information   Extraction"></a>Label Drop for Multi-Aspect Relation Modeling in Universal Information   Extraction</h2><p><strong>Authors:Lu Yang, Jiajia Li, En Ci, Lefei Zhang, Zuchao Li, Ping Wang</strong></p>
<p>Universal Information Extraction (UIE) has garnered significant attention due to its ability to address model explosion problems effectively. Extractive UIE can achieve strong performance using a relatively small model, making it widely adopted. Extractive UIEs generally rely on task instructions for different tasks, including single-target instructions and multiple-target instructions. Single-target instruction UIE enables the extraction of only one type of relation at a time, limiting its ability to model correlations between relations and thus restricting its capability to extract complex relations. While multiple-target instruction UIE allows for the extraction of multiple relations simultaneously, the inclusion of irrelevant relations introduces decision complexity and impacts extraction accuracy. Therefore, for multi-relation extraction, we propose LDNet, which incorporates multi-aspect relation modeling and a label drop mechanism. By assigning different relations to different levels for understanding and decision-making, we reduce decision confusion. Additionally, the label drop mechanism effectively mitigates the impact of irrelevant relations. Experiments show that LDNet outperforms or achieves competitive performance with state-of-the-art systems on 9 tasks, 33 datasets, in both single-modal and multi-modal, few-shot and zero-shot settings.\footnote{<a target="_blank" rel="noopener" href="https://github.com/Lu-Yang666/LDNet%7D">https://github.com/Lu-Yang666/LDNet}</a> </p>
<blockquote>
<p>通用信息抽取（UIE）因其能够有效解决模型爆炸问题而备受关注。使用较小的模型，抽取式UIE就能实现出色的性能，因此得到了广泛的应用。抽取式UIE通常依赖于不同的任务指令来完成不同任务，包括单目标指令和多目标指令。单目标指令UIE一次只能提取一种关系，限制了它建模关系间关联的能力，从而限制了其提取复杂关系的能力。虽然多目标指令UIE可以同时提取多种关系，但包含的不相关关系增加了决策复杂性并影响了提取精度。因此，针对多关系抽取，我们提出了LDNet，它结合了多方面关系建模和标签丢失机制。通过将不同的关系分配给不同的理解和决策层次，我们减少了决策混淆。此外，标签丢失机制有效地减轻了不相关关系的影响。实验表明，LDNet在单模态和多模态、小样本和零样本设置的9个任务、33个数据集上达到了最新系统的性能水平或超越了它们的表现。[^<a target="_blank" rel="noopener" href="https://github.com/Lu-Yang666/LDNet]%E3%80%82">https://github.com/Lu-Yang666/LDNet]。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12614v1">PDF</a> Accepted to NAACL-main 2025</p>
<p><strong>Summary</strong>：信息抽取通用框架（UIE）能够有效解决模型爆炸问题，且具有使用小型模型实现强大性能的能力。研究提出了LDNet模型用于多关系抽取，通过多层面关系建模和标签丢弃机制，提高了决策准确性和提取性能。该模型在单模态和多模态、小样本和零样本场景下表现出卓越性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>UIE框架能缓解模型爆炸问题，并在使用较小的模型时展现出强大的性能。</li>
<li>提取型UIE主要依赖于任务指令来完成不同任务，包括单目标指令和多目标指令。</li>
<li>单目标指令UIE一次只能提取一种关系，限制了其提取复杂关系的能力。</li>
<li>多目标指令UIE虽然能同时提取多种关系，但引入的不相关关系增加了决策复杂性和影响了提取准确性。</li>
<li>LDNet模型被提出用于解决多关系抽取问题，它通过多层面关系建模和标签丢弃机制减少决策混淆，并有效缓解不相关关系的影响。</li>
<li>LDNet模型在多个任务和数据集上的实验表现优于或达到最新技术水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12614">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-94ecb7426fdf348605f1a64f2bf17912.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-692ab86b178528a73362eefbde59cf63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20c1e1b86c0ffb5f6594508e93693f29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd3a0e1e2eff71aae0a44eb655cfe502.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d754b11ebe2feb7858299ffde538bf6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adaptive-Prototype-Model-for-Attribute-based-Multi-label-Few-shot-Action-Recognition"><a href="#Adaptive-Prototype-Model-for-Attribute-based-Multi-label-Few-shot-Action-Recognition" class="headerlink" title="Adaptive Prototype Model for Attribute-based Multi-label Few-shot Action   Recognition"></a>Adaptive Prototype Model for Attribute-based Multi-label Few-shot Action   Recognition</h2><p><strong>Authors:Juefeng Xiao, Tianqi Xiang, Zhigang Tu</strong></p>
<p>In real-world action recognition systems, incorporating more attributes helps achieve a more comprehensive understanding of human behavior. However, using a single model to simultaneously recognize multiple attributes can lead to a decrease in accuracy. In this work, we propose a novel method i.e. Adaptive Attribute Prototype Model (AAPM) for human action recognition, which captures rich action-relevant attribute information and strikes a balance between accuracy and robustness. Firstly, we introduce the Text-Constrain Module (TCM) to incorporate textual information from potential labels, and constrain the construction of different attributes prototype representations. In addition, we explore the Attribute Assignment Method (AAM) to address the issue of training bias and increase robustness during the training process.Furthermore, we construct a new video dataset with attribute-based multi-label called Multi-Kinetics for evaluation, which contains various attribute labels (e.g. action, scene, object, etc.) related to human behavior. Extensive experiments demonstrate that our AAPM achieves the state-of-the-art performance in both attribute-based multi-label few-shot action recognition and single-label few-shot action recognition. The project and dataset are available at an anonymous account <a target="_blank" rel="noopener" href="https://github.com/theAAPM/AAPM">https://github.com/theAAPM/AAPM</a> </p>
<blockquote>
<p>在现实世界的动作识别系统中，融入更多属性有助于更全面地理解人类行为。然而，使用单一模型同时识别多个属性可能会导致精度下降。在这项工作中，我们提出了一种新型方法，即自适应属性原型模型（AAPM）用于人类动作识别，该模型捕捉了丰富的动作相关属性信息，并在精度和稳健性之间达到了平衡。首先，我们引入了文本约束模块（TCM），以融入潜在标签的文本信息，并约束不同属性原型表示的构建。此外，我们探索了属性分配方法（AAM），以解决训练过程中的偏见问题，并提高稳健性。此外，我们构建了一个新的基于属性的多标签视频数据集，名为Multi-Kinetics，用于评估，其中包含与人类行为相关的各种属性标签（例如动作、场景、物体等）。大量实验表明，我们的AAPM在基于属性的多标签小样本动作识别和单标签小样本动作识别方面都达到了最新技术水平。项目和数据集可通过匿名账号<a target="_blank" rel="noopener" href="https://github.com/theAAPM/AAPM%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/theAAPM/AAPM进行访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12582v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了一种新的动作识别方法——自适应属性原型模型（AAPM），该模型能够捕捉丰富的动作相关属性信息，并在准确性和稳健性之间取得平衡。通过引入文本约束模块（TCM）和属性分配方法（AAM），提高了多属性识别系统的性能。同时，构建了一个新的视频数据集Multi-Kinetics，用于评估基于属性的多标签和单标签少样本动作识别的性能。实验表明，AAPM在基于属性的多标签和单标签少样本动作识别方面都达到了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自适应属性原型模型（AAPM）结合了多个属性以提高对人类行为的全面理解。</li>
<li>通过引入文本约束模块（TCM），将文本信息融入潜在标签，约束不同属性原型表示的构建。</li>
<li>提出的属性分配方法（AAM）解决了训练过程中的偏见问题，提高了系统的稳健性。</li>
<li>构建了一个新的视频数据集Multi-Kinetics，用于评估基于属性的多标签动作识别性能。</li>
<li>AAPM在基于属性的多标签少样本动作识别方面达到最先进的性能。</li>
<li>AAPM同样在单标签少样本动作识别方面表现出卓越的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12582">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-58076ae511ebeab6a5b1ce67cc9771bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a3340e5c4631da6ec6caf6a740f6f47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d75ac46816c47614d105c68d195be4b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c9238ee6ec62f1873d0a12fb6ec7fb4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Predicate-Hierarchies-Improve-Few-Shot-State-Classification"><a href="#Predicate-Hierarchies-Improve-Few-Shot-State-Classification" class="headerlink" title="Predicate Hierarchies Improve Few-Shot State Classification"></a>Predicate Hierarchies Improve Few-Shot State Classification</h2><p><strong>Authors:Emily Jin, Joy Hsu, Jiajun Wu</strong></p>
<p>State classification of objects and their relations is core to many long-horizon tasks, particularly in robot planning and manipulation. However, the combinatorial explosion of possible object-predicate combinations, coupled with the need to adapt to novel real-world environments, makes it a desideratum for state classification models to generalize to novel queries with few examples. To this end, we propose PHIER, which leverages predicate hierarchies to generalize effectively in few-shot scenarios. PHIER uses an object-centric scene encoder, self-supervised losses that infer semantic relations between predicates, and a hyperbolic distance metric that captures hierarchical structure; it learns a structured latent space of image-predicate pairs that guides reasoning over state classification queries. We evaluate PHIER in the CALVIN and BEHAVIOR robotic environments and show that PHIER significantly outperforms existing methods in few-shot, out-of-distribution state classification, and demonstrates strong zero- and few-shot generalization from simulated to real-world tasks. Our results demonstrate that leveraging predicate hierarchies improves performance on state classification tasks with limited data. </p>
<blockquote>
<p>对象和它们的关系的状态分类是许多长期任务的核心，特别是在机器人规划和操作方面。然而，可能的对象-谓词组合的组合爆炸，以及需要适应新的现实世界环境，使得状态分类模型需要能够用少量示例推广到新的查询成为一项必要。为此，我们提出了PHIER，它利用谓词层次结构在少量场景中进行有效的推广。PHIER使用以对象为中心的场景编码器、自监督损失来推断谓词之间的语义关系，以及捕捉层次结构的双曲距离度量；它学习图像-谓词对的结构化潜在空间，指导状态分类查询的推理。我们在CALVIN和BEHAVIOR机器人环境中评估了PHIER，结果表明，在少量、分布外的状态分类任务中，PHIER显著优于现有方法，并在从模拟到真实世界任务的零样本和少量样本推广中表现出强大的能力。我们的结果证明，利用谓词层次结构在数据有限的状态分类任务上可以提高性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12481v1">PDF</a> ICLR 2025. First two authors contributed equally. Project page:   <a target="_blank" rel="noopener" href="https://emilyzjin.github.io/projects/phier.html">https://emilyzjin.github.io/projects/phier.html</a></p>
<p><strong>Summary</strong></p>
<p>基于对象和关系分类的状态分类是长期任务的核心，尤其在机器人规划和操作领域。然而，由于可能的对象-谓词组合的爆炸性增长以及对新现实环境的适应需求，状态分类模型需要能够在新查询中快速适应并具有少量样本的泛化能力。为此，我们提出了PHIER方法，它利用谓词层次结构在少量场景中实现有效泛化。PHIER使用以对象为中心的场景编码器、自监督损失来推断谓词之间的语义关系以及双曲距离度量来捕捉层次结构；它学习图像-谓词对的结构化潜在空间，为状态分类查询提供指导。我们在CALVIN和BEHAVIOR机器人环境中评估了PHIER的性能，结果表明，PHIER在少量的未知分布状态分类任务中显著优于现有方法，并展示了从模拟任务到真实任务中零样本和少量样本的出色泛化能力。利用谓词层次结构提高了在有限数据上的状态分类任务性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>状态分类是机器人规划和操作等长期任务的核心。</li>
<li>对象和关系分类对于适应新环境和泛化至关重要。</li>
<li>PHIER方法利用谓词层次结构以实现有效泛化。</li>
<li>PHIER包括以对象为中心的场景编码器、自监督损失和捕捉层次结构的双曲距离度量。</li>
<li>PHIER在机器人环境评估中表现出优异性能。</li>
<li>PHIER在少量未知分布状态分类任务中显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-da9baf64913c6dff7e888ff917eefb6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0b667522e773d83e2492b52ffe9d1f3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UniMatch-Universal-Matching-from-Atom-to-Task-for-Few-Shot-Drug-Discovery"><a href="#UniMatch-Universal-Matching-from-Atom-to-Task-for-Few-Shot-Drug-Discovery" class="headerlink" title="UniMatch: Universal Matching from Atom to Task for Few-Shot Drug   Discovery"></a>UniMatch: Universal Matching from Atom to Task for Few-Shot Drug   Discovery</h2><p><strong>Authors:Ruifeng Li, Mingqian Li, Wei Liu, Yuhua Zhou, Xiangxin Zhou, Yuan Yao, Qiang Zhang, Hongyang Chen</strong></p>
<p>Drug discovery is crucial for identifying candidate drugs for various diseases.However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem. Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures that determine different molecular properties. To address these issues, we introduce Universal Matching Networks (UniMatch), a dual matching framework that integrates explicit hierarchical molecular matching with implicit task-level matching via meta-learning, bridging multi-level molecular representations and task-level generalization. Specifically, our approach explicitly captures structural features across multiple levels, such as atoms, substructures, and molecules, via hierarchical pooling and matching, facilitating precise molecular representation and comparison. Additionally, we employ a meta-learning strategy for implicit task-level matching, allowing the model to capture shared patterns across tasks and quickly adapt to new ones. This unified matching framework ensures effective molecular alignment while leveraging shared meta-knowledge for fast adaptation. Our experimental results demonstrate that UniMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and 6.52% in delta AUPRC. UniMatch also shows excellent generalization ability on the Meta-MolNet benchmark. </p>
<blockquote>
<p>药物发现对于识别各种疾病的候选药物至关重要。然而，其较低的成功率经常导致注释的稀缺，从而构成小样本学习问题。现有方法主要关注单一尺度特征，忽略了决定不同分子特性的分层分子结构。为了解决这些问题，我们引入了Universal Matching Networks（UniMatch），这是一个双匹配框架，它通过元学习将显式的分层分子匹配与隐式的任务级匹配相结合，桥接多层次分子表示和任务级泛化。具体来说，我们的方法通过分层池化和匹配显式捕获多个层次的结构特征，如原子、子结构和分子，促进精确的分子表示和比较。此外，我们采用元学习策略进行隐式任务级匹配，使模型能够捕获任务之间的共享模式并快速适应新任务。这一统一的匹配框架确保了有效的分子对齐，同时利用共享元知识实现快速适应。实验结果表明，UniMatch在MoleculeNet和FS-Mol基准测试中优于最新方法，在AUROC中提高了2.87%，在delta AUPRC中提高了6.52%。UniMatch在Meta-MolNet基准测试中也表现出良好的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12453v1">PDF</a> accepted as ICLR 2025 Spotlight</p>
<p><strong>Summary</strong></p>
<p>药物发现中识别候选药物是一个关键问题，但其低成功率导致标注数据稀缺，构成小样本学习问题。现有方法主要关注单一尺度特征，忽略了决定分子属性的层次结构。本研究引入Universal Matching Networks（UniMatch），结合明确的层次分子匹配和基于元学习的隐任务匹配，构建多层次的分子表示和任务泛化桥梁。通过层次池化和匹配，捕获原子、子结构和分子等多个层次的结构特征。此外，采用元学习策略进行隐任务匹配，使模型能够捕获任务间的共享模式并快速适应新任务。实验结果在MoleculeNet和FS-Mol基准测试中表明，UniMatch优于现有方法，提高了AUROC的2.87%和delta AUPRC的6.52%。同时，UniMatch在Meta-MolNet基准测试中展现出良好的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>药物发现面临低成功率和标注数据稀缺的问题，构成小样本学习挑战。</li>
<li>现有方法主要关注单一尺度特征，忽略了分子的层次结构。</li>
<li>UniMatch是一个双匹配框架，结合明确的层次分子匹配和隐任务匹配。</li>
<li>UniMatch通过层次池化和匹配技术，捕获多个层次的结构特征。</li>
<li>元学习策略用于隐任务匹配，提高模型对任务间共享模式的捕获能力，并加快对新任务的适应。</li>
<li>实验结果表明，UniMatch在多个基准测试中优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12453">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dabcc817a76f2ad19aa9f5afbbe9b7c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66f7098bdbba2205e9cbed26478c6025.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4415c3bbd891c14c91370b5e756384be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f39c32afa7bb8bdf0e3667a1ff29a511.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Gradient-Co-occurrence-Analysis-for-Detecting-Unsafe-Prompts-in-Large-Language-Models"><a href="#Gradient-Co-occurrence-Analysis-for-Detecting-Unsafe-Prompts-in-Large-Language-Models" class="headerlink" title="Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large   Language Models"></a>Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large   Language Models</h2><p><strong>Authors:Jingyuan Yang, Bowen Yan, Rongjun Li, Ziyu Zhou, Xin Chen, Zhiyong Feng, Wei Peng</strong></p>
<p>Unsafe prompts pose significant safety risks to large language models (LLMs). Existing methods for detecting unsafe prompts rely on data-driven fine-tuning to train guardrail models, necessitating significant data and computational resources. In contrast, recent few-shot gradient-based methods emerge, requiring only few safe and unsafe reference prompts. A gradient-based approach identifies unsafe prompts by analyzing consistent patterns of the gradients of safety-critical parameters in LLMs. Although effective, its restriction to directional similarity (cosine similarity) introduces <code>directional bias&#39;&#39;, limiting its capability to identify unsafe prompts. To overcome this limitation, we introduce GradCoo, a novel gradient co-occurrence analysis method that expands the scope of safety-critical parameter identification to include unsigned gradient similarity, thereby reducing the impact of </code>directional bias’’ and enhancing the accuracy of unsafe prompt detection. Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XStest demonstrate that our proposed method can achieve state-of-the-art (SOTA) performance compared to existing methods. Moreover, we confirm the generalizability of GradCoo in detecting unsafe prompts across a range of LLM base models with various sizes and origins. </p>
<blockquote>
<p>不安全的提示对大型语言模型（LLM）构成重大安全风险。现有的检测不安全提示的方法依赖于数据驱动微调来训练护栏模型，这需要大量数据和计算资源。相比之下，最近出现的基于梯度的少样本方法只需要少量安全和不安全的参考提示。基于梯度的方法通过分析和LLM中安全关键参数的一致性梯度模式来识别不安全的提示。尽管这种方法有效，但它对方向相似性（余弦相似性）的限制导致了“方向偏见”，限制了其识别不安全提示的能力。为了克服这一局限性，我们引入了GradCoo，这是一种新颖的梯度共现分析方法，将安全关键参数的识别范围扩大到包括无符号梯度相似性，从而减少“方向偏见”的影响，提高不安全提示检测的准确性。在广泛使用的基准数据集ToxicChat和XStest上的综合实验表明，与现有方法相比，我们提出的方法可以达到最先进的性能。此外，我们通过在不同大小和来源的大型语言模型基础模型上检测不安全提示，证实了GradCoo的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12411v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12411">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-494be2d7bba588a3bd53f3df03f90119.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58b7a2b16754e1ca823a44debd2da16e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97d44ffa80a04143617e31ace8c1b574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cda389c85802a695ba1bf2afdc473035.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cef9b49a80d218fe23f6c7c30311e7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53c3b49b973cbb173781656adc7d5ee7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-Frame-Detection-with-Retrieval-Augmented-Generation"><a href="#Enhancing-Frame-Detection-with-Retrieval-Augmented-Generation" class="headerlink" title="Enhancing Frame Detection with Retrieval Augmented Generation"></a>Enhancing Frame Detection with Retrieval Augmented Generation</h2><p><strong>Authors:Papa Abdou Karim Karou Diallo, Amal Zouaq</strong></p>
<p>Recent advancements in Natural Language Processing have significantly improved the extraction of structured semantic representations from unstructured text, especially through Frame Semantic Role Labeling (FSRL). Despite this progress, the potential of Retrieval-Augmented Generation (RAG) models for frame detection remains under-explored. In this paper, we present the first RAG-based approach for frame detection called RCIF (Retrieve Candidates and Identify Frames). RCIF is also the first approach to operate without the need for explicit target span and comprises three main stages: (1) generation of frame embeddings from various representations ; (2) retrieval of candidate frames given an input text; and (3) identification of the most suitable frames. We conducted extensive experiments across multiple configurations, including zero-shot, few-shot, and fine-tuning settings. Our results show that our retrieval component significantly reduces the complexity of the task by narrowing the search space thus allowing the frame identifier to refine and complete the set of candidates. Our approach achieves state-of-the-art performance on FrameNet 1.5 and 1.7, demonstrating its robustness in scenarios where only raw text is provided. Furthermore, we leverage the structured representation obtained through this method as a proxy to enhance generalization across lexical variations in the task of translating natural language questions into SPARQL queries. </p>
<blockquote>
<p>近期自然语言处理技术的进展，特别是通过框架语义角色标注（FSRL）的技术，已从非结构化文本中显著提高了结构化语义表示的提取。尽管取得了这些进展，但检索增强生成（RAG）模型在框架检测方面的潜力仍被低估。本文中，我们首次提出了基于RAG的框架检测方法，称为RCIF（检索候选对象并识别框架）。RCIF也是第一个无需显式目标跨度即可运行的方法，主要包括三个阶段：（1）从各种表示生成框架嵌入；（2）给定输入文本检索候选框架；（3）确定最合适的框架。我们在多种配置下进行了广泛实验，包括零样本、小样本文本和微调设置。结果表明，我们的检索组件通过缩小搜索空间显著降低了任务的复杂性，从而允许框架标识符对候选集进行细化和补充。我们的方法在FrameNet 1.5和1.7上达到了最新性能水平，证明了其在仅提供原始文本的情况下场景的稳健性。此外，我们借助通过此方法获得的结构化表示作为代理，以提高在将自然语言问题翻译成SPARQL查询的任务中对词汇变化的概括能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12210v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于检索增强生成（RAG）模型的框架检测方法的首次尝试。该方法称为RCIF，无需明确的目标跨度，通过生成框架嵌入、检索候选框架和识别最合适框架三个阶段来实现。实验结果表明，检索组件通过缩小搜索空间显著降低了任务的复杂性，使框架标识符能够完善和补充候选集。该方法在FrameNet 1.5和1.7上实现了最先进的性能，并展示了在只有原始文本提供的场景中增强跨词汇变化推广的能力。此外，通过此方法获得的结构化表示被用作代理，以提高将自然语言问题转换为SPARQL查询任务的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了基于检索增强生成（RAG）模型的框架检测方法RCIF。</li>
<li>RCIF无需明确目标跨度，包含框架嵌入生成、候选框架检索和最适合框架识别三个阶段。</li>
<li>检索组件通过缩小搜索空间降低任务复杂性。</li>
<li>RCIF在FrameNet 1.5和1.7上实现最先进性能，展现鲁棒性，特别是在只有原始文本提供的场景。</li>
<li>利用RCIF获得的结构化表示作为代理，提高了自然语言问题转换为SPARQL查询任务的泛化能力。</li>
<li>RCIF方法结合了生成和检索的优势，为未来框架检测提供了新方向。</li>
<li>实验结果表明，RAG模型在框架检测任务中具有巨大潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12210">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-acfe35284068873d89f75fc82c13359f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10f9af5a4733890d336251da738c76cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78914eb3cc3d57cf319178cf263a6f30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db2f505b089bd6ca7ded19f557d48128.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-Input-Attributions-Interpret-the-Inductive-Reasoning-Process-in-In-Context-Learning"><a href="#Can-Input-Attributions-Interpret-the-Inductive-Reasoning-Process-in-In-Context-Learning" class="headerlink" title="Can Input Attributions Interpret the Inductive Reasoning Process in   In-Context Learning?"></a>Can Input Attributions Interpret the Inductive Reasoning Process in   In-Context Learning?</h2><p><strong>Authors:Mengyu Ye, Tatsuki Kuribayashi, Goro Kobayashi, Jun Suzuki</strong></p>
<p>Interpreting the internal process of neural models has long been a challenge. This challenge remains relevant in the era of large language models (LLMs) and in-context learning (ICL); for example, ICL poses a new issue of interpreting which example in the few-shot examples contributed to identifying&#x2F;solving the task. To this end, in this paper, we design synthetic diagnostic tasks of inductive reasoning, inspired by the generalization tests in linguistics; here, most in-context examples are ambiguous w.r.t. their underlying rule, and one critical example disambiguates the task demonstrated. The question is whether conventional input attribution (IA) methods can track such a reasoning process, i.e., identify the influential example, in ICL. Our experiments provide several practical findings; for example, a certain simple IA method works the best, and the larger the model, the generally harder it is to interpret the ICL with gradient-based IA methods. </p>
<blockquote>
<p>神经网络模型的内部过程解释一直是一个挑战。在大型语言模型（LLM）和上下文学习（ICL）的时代，这一挑战依然具有现实意义。例如，ICL提出了新的解释问题，即在少数例子中，哪个例子对任务的识别&#x2F;解决起到了贡献。为此，本文设计了基于归纳推理的合成诊断任务，该设计灵感来源于语言学的泛化测试。在此场景中，大多数上下文例子对于其隐含的规则是模糊的，只有一个关键例子能够澄清任务。问题是传统的输入归因（IA）方法是否能追踪这样的推理过程，即在ICL中识别有影响的例子。我们的实验提供了几个实际发现：例如，某种简单的IA方法效果最好，模型越大，基于梯度的IA方法通常越难解释ICL。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15628v3">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型（LLMs）和上下文学习（ICL）中的解释模型内部过程的挑战。设计了一种基于语言学推广测试的推理任务诊断工具，其中大多数上下文示例对于其隐含的规则是模糊的，只有一个关键示例能澄清任务。实验发现简单的输入归因方法效果最好，且模型越大，基于梯度的归因方法解释上下文学习的难度越高。文章试图研究如何识别和跟踪这种推理过程的问题仍未得到解决。目前现有的传统输入归因方法难以在较少数据点的情景中捕捉到这类复杂的模式学习信息分布能力低下背后的隐含要素集推理规则。因此，需要开发新的方法来解释大型语言模型在上下文学习中的推理过程。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是本文的主要见解：</p>
<ul>
<li>大型语言模型（LLMs）和上下文学习（ICL）在解释模型内部过程方面存在挑战。</li>
<li>通过设计基于语言学推广测试的推理任务诊断工具，发现大多数上下文示例对于隐含的规则是模糊的。</li>
<li>一个关键示例能够澄清任务，但现有的传统输入归因（IA）方法难以识别和跟踪这种推理过程。</li>
<li>简单输入归因方法在实践中效果最好。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15628">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8df10c25215774ea23e7fb068e39c364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28de4e1dc6a9f27d74eb6d76286ba1cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d756c54cd9a3efe2b1c97aa6049ff5b9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PeerArg-Argumentative-Peer-Review-with-LLMs"><a href="#PeerArg-Argumentative-Peer-Review-with-LLMs" class="headerlink" title="PeerArg: Argumentative Peer Review with LLMs"></a>PeerArg: Argumentative Peer Review with LLMs</h2><p><strong>Authors:Purin Sukpanichnant, Anna Rapberger, Francesca Toni</strong></p>
<p>Peer review is an essential process to determine the quality of papers submitted to scientific conferences or journals. However, it is subjective and prone to biases. Several studies have been conducted to apply techniques from NLP to support peer review, but they are based on black-box techniques and their outputs are difficult to interpret and trust. In this paper, we propose a novel pipeline to support and understand the reviewing and decision-making processes of peer review: the PeerArg system combining LLMs with methods from knowledge representation. PeerArg takes in input a set of reviews for a paper and outputs the paper acceptance prediction. We evaluate the performance of the PeerArg pipeline on three different datasets, in comparison with a novel end-2-end LLM that uses few-shot learning to predict paper acceptance given reviews. The results indicate that the end-2-end LLM is capable of predicting paper acceptance from reviews, but a variant of the PeerArg pipeline outperforms this LLM. </p>
<blockquote>
<p>同行评审是确定提交到科学会议或期刊的论文质量的关键过程。然而，它是主观的，容易存在偏见。已经进行了多项研究，将NLP技术应用于支持同行评审，但它们基于黑箱技术，其输出难以解释和信任。在本文中，我们提出了一种支持并理解同行评审和决策过程的新型管道：PeerArg系统，该系统结合了大型语言模型（LLM）和知识表示方法。PeerArg接受一组对论文的评审意见作为输入，并输出论文的接受预测。我们在三个不同的数据集上评估PeerArg管道的性能，并将其与一种新型端到端LLM进行比较，该LLM使用小样本学习技术根据评审预测论文接受情况。结果表明，端到端LLM能够从评审中预测论文接受情况，但PeerArg管道的一个变体表现优于该LLM。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16813v2">PDF</a> Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的支持并理解同行评审的审稿和决策过程的系统——PeerArg。该系统结合了大型语言模型（LLMs）和知识表示方法，旨在提高同行评审的效率和透明度。通过输入一组论文的评审意见，PeerArg可以预测论文的接受程度。实验结果表明，相较于采用少样本学习的端到端LLM模型，PeerArg系统的变体在预测论文接受程度方面表现更优。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文介绍了PeerArg系统，一个结合了大型语言模型（LLMs）和知识表示方法的同行评审支持系统。</li>
<li>PeerArg旨在提高同行评审的效率和透明度，通过处理评审意见来预测论文的接受程度。</li>
<li>研究对PeerArg管道和采用少样本学习的端到端LLM进行了比较评估。</li>
<li>实验结果表明，PeerArg管道的变体在预测论文接受程度方面表现优于端到端LLM。</li>
<li>虽然LLM可用于预测论文接受程度，但其输出难以解释和信任，而PeerArg提供了更清晰的决策依据。</li>
<li>PeerArg系统可以帮助减少传统同行评审过程中的主观性和偏见。</li>
<li>该研究为改进同行评审过程提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16813">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2a8e71b997f96c44bd5a47165a42ac92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28eaf832aae98e22bb095aed438e7893.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b23f52a43f1d2a72a5e12bbeca4b291.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77401cb8bd507d496f1b8b080d066d50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e28426eae33697d2bb71b7701d883874.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5180ccd13fa598517b5cc37f990d68a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a08c5629f65fbef5309f311a76ac9498.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Acoustic-Prompt-Tuning-Empowering-Large-Language-Models-with-Audition-Capabilities"><a href="#Acoustic-Prompt-Tuning-Empowering-Large-Language-Models-with-Audition-Capabilities" class="headerlink" title="Acoustic Prompt Tuning: Empowering Large Language Models with Audition   Capabilities"></a>Acoustic Prompt Tuning: Empowering Large Language Models with Audition   Capabilities</h2><p><strong>Authors:Jinhua Liang, Xubo Liu, Wenwu Wang, Mark D. Plumbley, Huy Phan, Emmanouil Benetos</strong></p>
<p>The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of language and vision understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capability. In this work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending LLMs and VLMs to the audio domain by injecting audio embeddings to the input of LLMs, namely soft prompting. Specifically, APT applies an instruction-aware audio aligner to generate soft prompts, conditioned on both input text and sounds, as the inputs to the language model. To mitigate data scarcity in the audio domain, a curriculum learning strategy is proposed by formulating diverse audio tasks in a sequential manner. Moreover, we improve the audio language model by using interleaved audio-text embeddings as the input sequence. In this improved model, zero constraints are imposed on the input format, thus it is capable of tackling diverse modelling tasks, such as few-shot audio classification and audio comparison. To further evaluate the advanced ability of the audio networks, we introduce natural language audio reasoning (NLAR), a new task that analyses two audio clips by comparison and summarisation. Experiments show that APT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the expert models (i.e., the networks trained on the target datasets) across various tasks. We finally demonstrate APT’s ability in extending frozen VLMs to the audio domain without fine-tuning, achieving promising results in audio-visual question and answering. Our code and model weights will be released at <a target="_blank" rel="noopener" href="https://github.com/JinhuaLiang/APT">https://github.com/JinhuaLiang/APT</a> </p>
<blockquote>
<p>听觉系统在塑造人类整体感知体验中扮演着重要角色。尽管现有的大型语言模型（LLMs）和视觉语言模型（VLMs）在解决各种语言和视觉理解任务方面表现出巨大的潜力，但其中只有少数能够推广至音频领域而不损害其特定领域的性能。在这项工作中，我们引入了声学提示调整（APT），这是一种新的适配器，通过将音频嵌入注入LLMs的输入来扩展LLMs和VLMs至音频领域，即软提示。具体而言，APT应用指令感知音频对齐器来生成软提示，这些提示基于输入文本和声音，作为语言模型的输入。为了缓解音频领域的数据稀缺问题，我们提出了一种课程学习策略，以顺序方式制定多样化的音频任务。此外，我们通过使用交替的音频文本嵌入作为输入序列来改进音频语言模型。在这个改进后的模型中，对输入格式没有施加任何约束，因此它能够处理各种建模任务，如少样本音频分类和音频比较。为了进一步评估音频网络的先进功能，我们引入了自然语言音频推理（NLAR），这是一个新任务，通过比较和总结两个音频片段进行分析。实验表明，使用APT增强的LLMs（即APT-LLMs）在各种任务上与专家模型（即针对目标数据集训练的网络）相比取得了具有竞争力的结果。最后，我们证明了APT在将冻结的VLMs扩展到音频领域的能力，且在音频视觉问答任务上取得了有前景的结果。我们的代码和模型权重将在<a target="_blank" rel="noopener" href="https://github.com/JinhuaLiang/APT%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/JinhuaLiang/APT上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00249v2">PDF</a> Published at IEEE Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong><br>     本研究介绍了声波提示调整（APT），这是一种将大型语言模型（LLMs）和视觉语言模型（VLMs）扩展到音频领域的新适配器。通过注入音频嵌入到LLMs的输入中，APT实现了软提示生成，并应用指令感知音频对齐器来生成这些提示。APT采用课程学习策略来缓解音频领域的数据稀缺问题，并提出使用交织的音频文本嵌入作为输入序列来改善音频语言模型。此外，还引入了自然语言音频推理（NLAR）新任务来进一步评估音频网络的高级能力。实验表明，APT增强的LLMs在各项任务中取得了与专家模型相当的结果。最后，演示了APT将冻结的VLMs扩展到音频领域的能力，在视听问答任务中取得了有希望的成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>听觉系统在人类整体感知体验中扮演重要角色，目前大型语言模型和视觉语言模型在音频领域的通用性有限。</li>
<li>引入了一种新的适配器——声波提示调整（APT），它能将大型语言模型和视觉语言模型扩展到音频领域。</li>
<li>APT通过注入音频嵌入到语言模型的输入中实现软提示生成，并应用指令感知音频对齐器来处理输入文本和声音。</li>
<li>采用课程学习策略应对音频领域的数据稀缺问题，提出交织的音频文本嵌入来改善音频语言模型。</li>
<li>引入了自然语言音频推理（NLAR）新任务，用于评估音频网络的高级能力。</li>
<li>实验显示，APT增强的LLMs在多种任务中表现良好，与专家模型相当。</li>
<li>APT能将冻结的VLMs扩展到音频领域，在视听问答任务中取得有希望的成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00249">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f53a23a6bbb830ac0fb343625d0dce65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b23d7e308e32857be3cfb056307753e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b68a0ad1e11558c7829f31710e29db0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03b6f1074662e89de283913cd6a2666f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31d6598c7b01f58d71f432dcd8a9931a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-20/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-20/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2bf35c9798792592db76e1d75668c362.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-02-20  3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from   Cortical Surfaces
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-509a10fdf6c6b07c4d0432e6ded7d510.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-02-20  Magma A Foundation Model for Multimodal AI Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
