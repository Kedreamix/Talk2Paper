<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-20  Do we still need Human Annotators? Prompting Large Language Models for   Aspect Sentiment Quad Prediction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f39c32afa7bb8bdf0e3667a1ff29a511.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    43 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-20-æ›´æ–°"><a href="#2025-02-20-æ›´æ–°" class="headerlink" title="2025-02-20 æ›´æ–°"></a>2025-02-20 æ›´æ–°</h1><h2 id="Do-we-still-need-Human-Annotators-Prompting-Large-Language-Models-for-Aspect-Sentiment-Quad-Prediction"><a href="#Do-we-still-need-Human-Annotators-Prompting-Large-Language-Models-for-Aspect-Sentiment-Quad-Prediction" class="headerlink" title="Do we still need Human Annotators? Prompting Large Language Models for   Aspect Sentiment Quad Prediction"></a>Do we still need Human Annotators? Prompting Large Language Models for   Aspect Sentiment Quad Prediction</h2><p><strong>Authors:Nils Constantin Hellwig, Jakob Fehle, Udo Kruschwitz, Christian Wolff</strong></p>
<p>Aspect sentiment quadruple prediction (ASQP) facilitates a detailed understanding of opinions expressed in a text by identifying the opinion term, aspect term, aspect category and sentiment polarity for each opinion. However, annotating a full set of training examples to fine-tune models for ASQP is a resource-intensive process. In this study, we explore the capabilities of large language models (LLMs) for zero- and few-shot learning on the ASQP task across five diverse datasets. We report F1 scores slightly below those obtained with state-of-the-art fine-tuned models but exceeding previously reported zero- and few-shot performance. In the 40-shot setting on the Rest16 restaurant domain dataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the best-performing fine-tuned method MVP. Additionally, we report the performance of LLMs in target aspect sentiment detection (TASD), where the F1 scores were also close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot setting, compared to 72.76 with MVP. While human annotators remain essential for achieving optimal performance, LLMs can reduce the need for extensive manual annotation in ASQP tasks. </p>
<blockquote>
<p>é¢å‘æ–¹é¢çš„æƒ…æ„Ÿå››é‡é¢„æµ‹ï¼ˆASQPï¼‰èƒ½å¤Ÿé€šè¿‡è¯†åˆ«æ¯ä¸ªæ„è§çš„è§‚ç‚¹è¯ã€æ–¹é¢è¯ã€æ–¹é¢ç±»åˆ«å’Œæƒ…æ„Ÿææ€§ï¼Œä»è€Œä¿ƒè¿›å¯¹æ–‡æœ¬ä¸­æ‰€è¡¨è¾¾æ„è§çš„æ·±å…¥ç†è§£ã€‚ç„¶è€Œï¼Œå¯¹å…¨å¥—è®­ç»ƒä¾‹å­è¿›è¡Œæ ‡æ³¨ä»¥å¾®è°ƒASQPæ¨¡å‹æ˜¯ä¸€ä¸ªèµ„æºå¯†é›†å‹çš„æµç¨‹ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äº”ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ åœ¨ASQPä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚æˆ‘ä»¬æŠ¥å‘Šçš„F1åˆ†æ•°ç•¥ä½äºä½¿ç”¨æœ€æ–°å¾®è°ƒæ¨¡å‹å¾—åˆ°çš„åˆ†æ•°ï¼Œä½†è¶…è¿‡äº†ä¹‹å‰æŠ¥å‘Šçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ã€‚åœ¨Rest16é¤å…åŸŸæ•°æ®é›†çš„40ä¸ªæ ·æœ¬è®¾ç½®ä¸­ï¼ŒLLMçš„F1åˆ†æ•°ä¸º52.46ï¼Œè€Œè¡¨ç°æœ€ä½³çš„MVPå¾®è°ƒæ–¹æ³•çš„F1åˆ†æ•°ä¸º60.39ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æŠ¥å‘Šäº†LLMåœ¨ç›®æ ‡æ–¹é¢æƒ…æ„Ÿæ£€æµ‹ï¼ˆTASDï¼‰ä¸­çš„è¡¨ç°ï¼Œå…¶F1åˆ†æ•°ä¹Ÿä¸å¾®è°ƒæ¨¡å‹ç›¸è¿‘ã€‚åœ¨Rest16çš„40ä¸ªæ ·æœ¬è®¾ç½®ä¸­ï¼ŒLLMçš„F1åˆ†æ•°ä¸º66.03ï¼Œè€ŒMVPçš„F1åˆ†æ•°ä¸º72.76ã€‚è™½ç„¶äººç±»æ ‡æ³¨è€…å¯¹äºå®ç°æœ€ä½³æ€§èƒ½ä»ç„¶è‡³å…³é‡è¦ï¼Œä½†LLMå¯ä»¥å‡å°‘ASQPä»»åŠ¡ä¸­å¯¹å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13044v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å‘æ–¹é¢çš„æƒ…æ„Ÿå››é‡é¢„æµ‹ï¼ˆASQPï¼‰ä»»åŠ¡ä¸­çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°åœ¨äº”ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šï¼ŒLLMsçš„æ€§èƒ½è™½ç„¶ç•¥ä½äºç»è¿‡å¾®è°ƒçš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œä½†åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æƒ…å†µä¸‹çš„æ€§èƒ½å´è¶…è¿‡äº†ä¹‹å‰çš„æŠ¥å‘Šã€‚ç‰¹åˆ«æ˜¯åœ¨Rest16é¤å…é¢†åŸŸæ•°æ®é›†çš„40ä¸ªæ ·æœ¬æƒ…å†µä¸‹ï¼ŒLLMsçš„F1åˆ†æ•°è¾¾åˆ°äº†52.46%ï¼Œè€Œæœ€ä½³å¾®è°ƒæ–¹æ³•MVPçš„F1åˆ†æ•°ä¸º60.39ã€‚æ­¤å¤–ï¼Œè¿˜æŠ¥å‘Šäº†ç›®æ ‡æ–¹é¢æƒ…æ„Ÿæ£€æµ‹ï¼ˆTASDï¼‰ä¸­LLMsçš„æ€§èƒ½ï¼Œå…¶F1åˆ†æ•°æ¥è¿‘ç»è¿‡å¾®è°ƒæ¨¡å‹ï¼Œä½†ä»æœ‰ä¸€å®šå·®è·ã€‚è™½ç„¶äººç±»æ³¨é‡Šè€…å¯¹äºå®ç°æœ€ä½³æ€§èƒ½è‡³å…³é‡è¦ï¼Œä½†LLMså¯ä»¥å‡å°‘ASQPä»»åŠ¡ä¸­å¯¹å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Aspect sentiment quadruple prediction (ASQP) èƒ½å¤Ÿæ·±å…¥ç†è§£æ–‡æœ¬ä¸­çš„æ„è§ï¼Œé€šè¿‡è¯†åˆ«è§‚ç‚¹è¯ã€æ–¹é¢è¯ã€æ–¹é¢ç±»åˆ«å’Œæƒ…æ„Ÿææ€§æ¥è¯¦ç»†è§£ææ„è§å†…å®¹ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ åœ¨ASQPä»»åŠ¡ä¸Šå…·æœ‰å¯è¡Œæ€§ã€‚</li>
<li>LLMsåœ¨äº”ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œè™½ç„¶ç•¥ä½äºç»è¿‡ç²¾ç»†è°ƒæ•™çš„å…ˆè¿›æ¨¡å‹ï¼Œä½†åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æƒ…å¢ƒä¸‹è¶…è¶Šäº†å…ˆå‰æŠ¥å‘Šçš„æ€§èƒ½ã€‚</li>
<li>åœ¨Rest16é¤å…é¢†åŸŸæ•°æ®é›†çš„40ä¸ªæ ·æœ¬æƒ…å†µä¸‹ï¼ŒLLMsçš„F1åˆ†æ•°è¾¾åˆ°52.46%ï¼Œè€Œæœ€ä½³æ¨¡å‹MVPçš„F1åˆ†æ•°ä¸º60.39ã€‚</li>
<li>LLMsåœ¨ç›®æ ‡æ–¹é¢æƒ…æ„Ÿæ£€æµ‹ï¼ˆTASDï¼‰ä»»åŠ¡ä¸­ä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ï¼Œä½†è·ç¦»ç»è¿‡å¾®è°ƒçš„æœ€ä¼˜æ¨¡å‹ä»æœ‰ä¸€å®šå·®è·ã€‚</li>
<li>è™½ç„¶äººç±»æ³¨é‡Šè€…å¯¹äºè·å¾—æœ€ä½³æ€§èƒ½è‡³å…³é‡è¦ï¼Œä½†LLMsçš„åº”ç”¨å¯ä»¥å‡å°‘å¯¹å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„éœ€æ±‚ï¼Œä»è€ŒèŠ‚çœèµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4b68e205d9f3b1f04ef1ec67b397935d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ebcf0d1a8b222d8171b948897bc904cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a29bab5da6966b85e087ce3267b2775.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e45fe3397ae5f5b1cfb3f4dad94d853.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Scalable-Model-Merging-with-Progressive-Layer-wise-Distillation"><a href="#Scalable-Model-Merging-with-Progressive-Layer-wise-Distillation" class="headerlink" title="Scalable Model Merging with Progressive Layer-wise Distillation"></a>Scalable Model Merging with Progressive Layer-wise Distillation</h2><p><strong>Authors:Jing Xu, Jiazheng Li, Jingzhao Zhang</strong></p>
<p>Model merging offers an effective way to integrate the capabilities of multiple fine-tuned models. However, the performance degradation of the merged model remains a challenge, particularly when none or few data are available. This paper first highlights the necessity of domain-specific data for model merging by proving that data-agnostic algorithms can have arbitrarily bad worst-case performance. Building on this theoretical insight, we explore the relationship between model merging and distillation, introducing a novel few-shot merging algorithm, ProDistill (Progressive Layer-wise Distillation). Unlike common belief that layer wise training hurts performance, we show that layer-wise teacher-student distillation not only enhances the scalability but also improves model merging performance. We conduct extensive experiments to show that compared to existing few-shot merging methods, ProDistill achieves state-of-the-art performance, with up to 6.14% and 6.61% improvements in vision and NLU tasks. Furthermore, we extend the experiments to models with over 10B parameters, showcasing the exceptional scalability of ProDistill. </p>
<blockquote>
<p>æ¨¡å‹èåˆä¸ºå¤šä¸ªå¾®è°ƒæ¨¡å‹çš„é›†æˆæä¾›äº†ä¸€ç§æœ‰æ•ˆé€”å¾„ã€‚ç„¶è€Œï¼Œèåˆæ¨¡å‹çš„æ€§èƒ½ä¸‹é™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰æˆ–åªæœ‰å°‘é‡æ•°æ®å¯ç”¨çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡é¦–å…ˆé€šè¿‡è¯æ˜æ•°æ®æ— å…³ç®—æ³•å¯èƒ½å…·æœ‰ä»»æ„å·®çš„æ€§èƒ½æ¥è¯æ˜ç‰¹å®šé¢†åŸŸæ•°æ®å¯¹æ¨¡å‹èåˆçš„å¿…è¦æ€§ã€‚åŸºäºè¿™ä¸€ç†è®ºæ´å¯Ÿï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ¨¡å‹èåˆä¸è’¸é¦ä¹‹é—´çš„å…³ç³»ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹çš„å°‘æ•°æ ·æœ¬èåˆç®—æ³•â€”â€”æ¸è¿›å±‚è’¸é¦ï¼ˆProDistillï¼‰ã€‚ä¸æ™®éè®¤ä¸ºçš„é€å±‚è®­ç»ƒä¼šæŸå®³æ€§èƒ½ä¸åŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€å±‚å¸ˆå¾’è’¸é¦ä¸ä»…æé«˜äº†å¯æ‰©å±•æ€§ï¼Œè¿˜æé«˜äº†æ¨¡å‹èåˆæ€§èƒ½ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å°‘æ•°æ ·æœ¬èåˆæ–¹æ³•ç›¸æ¯”ï¼ŒProDistillå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è§†è§‰å’Œè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šåˆ†åˆ«æé«˜äº†é«˜è¾¾6.14%å’Œ6.61%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å®éªŒæ‰©å±•åˆ°äº†å‚æ•°è¶…è¿‡10Bçš„æ¨¡å‹ä¸Šï¼Œå±•ç¤ºäº†ProDistillçš„å“è¶Šå¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12706v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ¨¡å‹èåˆæ˜¯ä¸€ç§æœ‰æ•ˆæ•´åˆå¤šä¸ªå¾®è°ƒæ¨¡å‹èƒ½åŠ›çš„æ–¹æ³•ï¼Œä½†èåˆæ¨¡å‹çš„æ€§èƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— æ•°æ®æˆ–å°‘æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡é¦–å…ˆé€šè¿‡è¯æ˜æ•°æ®æ— å…³ç®—æ³•åœ¨æœ€åæƒ…å†µä¸‹å¯èƒ½å…·æœ‰ä»»æ„å·®çš„æ€§èƒ½æ¥å¼ºè°ƒé¢†åŸŸç‰¹å®šæ•°æ®å¯¹æ¨¡å‹èåˆçš„å¿…è¦æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡æ¢ç´¢äº†æ¨¡å‹èåˆä¸è’¸é¦çš„å…³ç³»ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬èåˆç®—æ³•â€”â€”ProDistillï¼ˆæ¸è¿›é€å±‚è’¸é¦ï¼‰ã€‚ä¸æ™®éè§‚ç‚¹ç›¸åï¼Œæœ¬æ–‡æ˜¾ç¤ºé€å±‚æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦ä¸ä»…æé«˜äº†å¯æ‰©å±•æ€§ï¼Œè¿˜æé«˜äº†æ¨¡å‹èåˆæ€§èƒ½ã€‚é€šè¿‡å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å°‘æ ·æœ¬èåˆæ–¹æ³•ç›¸æ¯”ï¼ŒProDistillå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è§†è§‰å’Œè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šåˆ†åˆ«æé«˜äº†6.14%å’Œ6.61%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹è¶…è¿‡1 ç»“å¤–æ‹“å±•åˆ°å‚æ•°è¶…è¿‡10Bçš„æ¨¡å‹ï¼Œå±•ç¤ºäº†ProDistillçš„å‡ºè‰²å¯æ‰©å±•æ€§ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬èåˆç®—æ³•ProDistillç®—æ³•ã€‚é€šè¿‡è’¸é¦å’Œæ¨¡å‹èåˆçš„æœ‰æ•ˆç»“åˆç­–ç•¥è¿›è¡Œæ¼”ç¤ºéªŒè¯äº†ç®—æ³•çš„é«˜æ•ˆæ€§ã€‚ã€‚è¯¥ç ”ç©¶å°†ä¸ºè§£å†³æ·±åº¦å­¦ä¹ çš„éš¾é¢˜å¦‚è¿ç§»å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚é¢å‘å…·ä½“ä»»åŠ¡çš„ç‰¹æ®Šè®­ç»ƒæ•°æ®å°†æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹å¤§è§„æ¨¡æ•°æ®é›†æ—¶ã€‚é€šè¿‡å®éªŒç»“æœå¯ä»¥çœ‹å‡ºï¼ŒProDistillç®—æ³•å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå®é™…åº”ç”¨å‰æ™¯ã€‚ç ”ç©¶ä¹Ÿè¯æ˜äº†åŸºäºæ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹çš„è’¸é¦æ–¹æ³•åœ¨å¤šæ¨¡æ€æ•°æ®å¤„ç†ä¸­å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚å®ƒå…è®¸åŒæ—¶è€ƒè™‘å¤šæºæ•°æ®çš„ç»¼åˆä½œç”¨è€Œä¸ä¾èµ–äºç‰¹å®šçš„æ•°æ®é›†æˆ–ä»»åŠ¡ç±»å‹ã€‚è¿™ä¸ºæœªæ¥çš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢å¦‚ä½•ä¼˜åŒ–è’¸é¦ç­–ç•¥ä»¥é€‚åº”ä¸åŒä»»åŠ¡çš„ç‰¹æ€§ä»¥åŠå¦‚ä½•æ›´å¥½åœ°å°†å¤šç§æ¨¡æ€çš„æ•°æ®è¿›è¡Œæœ‰æ•ˆèåˆä»¥å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•åœ¨æ›´å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„è¡¨ç°å€¼å¾—è¿›ä¸€æ­¥æ·±å…¥ç ”ç©¶ã€‚è¿™äº›ç ”ç©¶å°†æœ‰åŠ©äºæ¨åŠ¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„å‘å±•å¹¶æ¨åŠ¨å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è½åœ°ã€‚å¯¹äºå®é™…åº”ç”¨åœºæ™¯å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸä¹Ÿå¯ä»¥è¿›è¡Œæ›´å¤šçš„æ¢ç´¢å’Œç ”ç©¶å·¥ä½œã€‚å…·ä½“æ¥è¯´å¯ä»¥ç ”ç©¶å¦‚ä½•å°†è¿™äº›ç®—æ³•åº”ç”¨äºå®é™…åœºæ™¯ä¸­è§£å†³ç°å®ä¸–ç•Œä¸­å­˜åœ¨çš„é—®é¢˜å¦‚å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€ç¿»è¯‘ç­‰ä»»åŠ¡ä¸­ã€‚é€šè¿‡è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå®è·µå¯ä»¥ä¸æ–­å®Œå–„å’Œæ”¹è¿›è¿™äº›ç®—æ³•ä»è€Œæé«˜å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ•ˆæœã€‚é’ˆå¯¹è¿™äº›é—®é¢˜å±•å¼€æ·±å…¥ç ”ç©¶å°†æœ‰åŠ©äºæ¨åŠ¨æœºå™¨å­¦ä¹ é¢†åŸŸçš„å‘å±•å¹¶ä¸ºç›¸å…³é¢†åŸŸæä¾›æœ‰åŠ›çš„æ”¯æŒå’ŒæŠ€æœ¯ä¿éšœã€‚<strong>å…³é”®è§è§£</strong></p>
<ul>
<li>å¼ºè°ƒäº†é¢†åŸŸç‰¹å®šæ•°æ®åœ¨æ¨¡å‹èåˆä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºæ•°æ®æ— å…³ç®—æ³•å¯èƒ½å­˜åœ¨çš„æ€§èƒ½é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬èåˆç®—æ³•ProDistillï¼Œé€šè¿‡æ¸è¿›é€å±‚è’¸é¦æé«˜æ¨¡å‹èåˆæ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒProDistillåœ¨è§†è§‰å’Œè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šè¾ƒç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</li>
<li>ProDistillå…·æœ‰å‡ºè‰²çš„å¯æ‰©å±•æ€§ï¼Œå¯åº”ç”¨äºå‚æ•°è§„æ¨¡è¾ƒå¤§çš„æ¨¡å‹ã€‚</li>
<li>é€å±‚æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦ç­–ç•¥æé«˜äº†æ¨¡å‹èåˆçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9080cd3228379a64049d5df2316ed2d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09f1d8f1c229d50a89849029048be7cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ed86881519d1b1f437ef09c5fc389c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Label-Drop-for-Multi-Aspect-Relation-Modeling-in-Universal-Information-Extraction"><a href="#Label-Drop-for-Multi-Aspect-Relation-Modeling-in-Universal-Information-Extraction" class="headerlink" title="Label Drop for Multi-Aspect Relation Modeling in Universal Information   Extraction"></a>Label Drop for Multi-Aspect Relation Modeling in Universal Information   Extraction</h2><p><strong>Authors:Lu Yang, Jiajia Li, En Ci, Lefei Zhang, Zuchao Li, Ping Wang</strong></p>
<p>Universal Information Extraction (UIE) has garnered significant attention due to its ability to address model explosion problems effectively. Extractive UIE can achieve strong performance using a relatively small model, making it widely adopted. Extractive UIEs generally rely on task instructions for different tasks, including single-target instructions and multiple-target instructions. Single-target instruction UIE enables the extraction of only one type of relation at a time, limiting its ability to model correlations between relations and thus restricting its capability to extract complex relations. While multiple-target instruction UIE allows for the extraction of multiple relations simultaneously, the inclusion of irrelevant relations introduces decision complexity and impacts extraction accuracy. Therefore, for multi-relation extraction, we propose LDNet, which incorporates multi-aspect relation modeling and a label drop mechanism. By assigning different relations to different levels for understanding and decision-making, we reduce decision confusion. Additionally, the label drop mechanism effectively mitigates the impact of irrelevant relations. Experiments show that LDNet outperforms or achieves competitive performance with state-of-the-art systems on 9 tasks, 33 datasets, in both single-modal and multi-modal, few-shot and zero-shot settings.\footnote{<a target="_blank" rel="noopener" href="https://github.com/Lu-Yang666/LDNet%7D">https://github.com/Lu-Yang666/LDNet}</a> </p>
<blockquote>
<p>é€šç”¨ä¿¡æ¯æŠ½å–ï¼ˆUIEï¼‰å› å…¶èƒ½å¤Ÿæœ‰æ•ˆè§£å†³æ¨¡å‹çˆ†ç‚¸é—®é¢˜è€Œå¤‡å—å…³æ³¨ã€‚ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ï¼ŒæŠ½å–å¼UIEå°±èƒ½å®ç°å‡ºè‰²çš„æ€§èƒ½ï¼Œå› æ­¤å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚æŠ½å–å¼UIEé€šå¸¸ä¾èµ–äºä¸åŒçš„ä»»åŠ¡æŒ‡ä»¤æ¥å®Œæˆä¸åŒä»»åŠ¡ï¼ŒåŒ…æ‹¬å•ç›®æ ‡æŒ‡ä»¤å’Œå¤šç›®æ ‡æŒ‡ä»¤ã€‚å•ç›®æ ‡æŒ‡ä»¤UIEä¸€æ¬¡åªèƒ½æå–ä¸€ç§å…³ç³»ï¼Œé™åˆ¶äº†å®ƒå»ºæ¨¡å…³ç³»é—´å…³è”çš„èƒ½åŠ›ï¼Œä»è€Œé™åˆ¶äº†å…¶æå–å¤æ‚å…³ç³»çš„èƒ½åŠ›ã€‚è™½ç„¶å¤šç›®æ ‡æŒ‡ä»¤UIEå¯ä»¥åŒæ—¶æå–å¤šç§å…³ç³»ï¼Œä½†åŒ…å«çš„ä¸ç›¸å…³å…³ç³»å¢åŠ äº†å†³ç­–å¤æ‚æ€§å¹¶å½±å“äº†æå–ç²¾åº¦ã€‚å› æ­¤ï¼Œé’ˆå¯¹å¤šå…³ç³»æŠ½å–ï¼Œæˆ‘ä»¬æå‡ºäº†LDNetï¼Œå®ƒç»“åˆäº†å¤šæ–¹é¢å…³ç³»å»ºæ¨¡å’Œæ ‡ç­¾ä¸¢å¤±æœºåˆ¶ã€‚é€šè¿‡å°†ä¸åŒçš„å…³ç³»åˆ†é…ç»™ä¸åŒçš„ç†è§£å’Œå†³ç­–å±‚æ¬¡ï¼Œæˆ‘ä»¬å‡å°‘äº†å†³ç­–æ··æ·†ã€‚æ­¤å¤–ï¼Œæ ‡ç­¾ä¸¢å¤±æœºåˆ¶æœ‰æ•ˆåœ°å‡è½»äº†ä¸ç›¸å…³å…³ç³»çš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼ŒLDNetåœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€ã€å°æ ·æœ¬å’Œé›¶æ ·æœ¬è®¾ç½®çš„9ä¸ªä»»åŠ¡ã€33ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°ç³»ç»Ÿçš„æ€§èƒ½æ°´å¹³æˆ–è¶…è¶Šäº†å®ƒä»¬çš„è¡¨ç°ã€‚[^<a target="_blank" rel="noopener" href="https://github.com/Lu-Yang666/LDNet]%E3%80%82">https://github.com/Lu-Yang666/LDNet]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12614v1">PDF</a> Accepted to NAACL-main 2025</p>
<p><strong>Summary</strong>ï¼šä¿¡æ¯æŠ½å–é€šç”¨æ¡†æ¶ï¼ˆUIEï¼‰èƒ½å¤Ÿæœ‰æ•ˆè§£å†³æ¨¡å‹çˆ†ç‚¸é—®é¢˜ï¼Œä¸”å…·æœ‰ä½¿ç”¨å°å‹æ¨¡å‹å®ç°å¼ºå¤§æ€§èƒ½çš„èƒ½åŠ›ã€‚ç ”ç©¶æå‡ºäº†LDNetæ¨¡å‹ç”¨äºå¤šå…³ç³»æŠ½å–ï¼Œé€šè¿‡å¤šå±‚é¢å…³ç³»å»ºæ¨¡å’Œæ ‡ç­¾ä¸¢å¼ƒæœºåˆ¶ï¼Œæé«˜äº†å†³ç­–å‡†ç¡®æ€§å’Œæå–æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€ã€å°æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>UIEæ¡†æ¶èƒ½ç¼“è§£æ¨¡å‹çˆ†ç‚¸é—®é¢˜ï¼Œå¹¶åœ¨ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹æ—¶å±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>æå–å‹UIEä¸»è¦ä¾èµ–äºä»»åŠ¡æŒ‡ä»¤æ¥å®Œæˆä¸åŒä»»åŠ¡ï¼ŒåŒ…æ‹¬å•ç›®æ ‡æŒ‡ä»¤å’Œå¤šç›®æ ‡æŒ‡ä»¤ã€‚</li>
<li>å•ç›®æ ‡æŒ‡ä»¤UIEä¸€æ¬¡åªèƒ½æå–ä¸€ç§å…³ç³»ï¼Œé™åˆ¶äº†å…¶æå–å¤æ‚å…³ç³»çš„èƒ½åŠ›ã€‚</li>
<li>å¤šç›®æ ‡æŒ‡ä»¤UIEè™½ç„¶èƒ½åŒæ—¶æå–å¤šç§å…³ç³»ï¼Œä½†å¼•å…¥çš„ä¸ç›¸å…³å…³ç³»å¢åŠ äº†å†³ç­–å¤æ‚æ€§å’Œå½±å“äº†æå–å‡†ç¡®æ€§ã€‚</li>
<li>LDNetæ¨¡å‹è¢«æå‡ºç”¨äºè§£å†³å¤šå…³ç³»æŠ½å–é—®é¢˜ï¼Œå®ƒé€šè¿‡å¤šå±‚é¢å…³ç³»å»ºæ¨¡å’Œæ ‡ç­¾ä¸¢å¼ƒæœºåˆ¶å‡å°‘å†³ç­–æ··æ·†ï¼Œå¹¶æœ‰æ•ˆç¼“è§£ä¸ç›¸å…³å…³ç³»çš„å½±å“ã€‚</li>
<li>LDNetæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡å’Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºæˆ–è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12614">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94ecb7426fdf348605f1a64f2bf17912.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-692ab86b178528a73362eefbde59cf63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20c1e1b86c0ffb5f6594508e93693f29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd3a0e1e2eff71aae0a44eb655cfe502.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d754b11ebe2feb7858299ffde538bf6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adaptive-Prototype-Model-for-Attribute-based-Multi-label-Few-shot-Action-Recognition"><a href="#Adaptive-Prototype-Model-for-Attribute-based-Multi-label-Few-shot-Action-Recognition" class="headerlink" title="Adaptive Prototype Model for Attribute-based Multi-label Few-shot Action   Recognition"></a>Adaptive Prototype Model for Attribute-based Multi-label Few-shot Action   Recognition</h2><p><strong>Authors:Juefeng Xiao, Tianqi Xiang, Zhigang Tu</strong></p>
<p>In real-world action recognition systems, incorporating more attributes helps achieve a more comprehensive understanding of human behavior. However, using a single model to simultaneously recognize multiple attributes can lead to a decrease in accuracy. In this work, we propose a novel method i.e. Adaptive Attribute Prototype Model (AAPM) for human action recognition, which captures rich action-relevant attribute information and strikes a balance between accuracy and robustness. Firstly, we introduce the Text-Constrain Module (TCM) to incorporate textual information from potential labels, and constrain the construction of different attributes prototype representations. In addition, we explore the Attribute Assignment Method (AAM) to address the issue of training bias and increase robustness during the training process.Furthermore, we construct a new video dataset with attribute-based multi-label called Multi-Kinetics for evaluation, which contains various attribute labels (e.g. action, scene, object, etc.) related to human behavior. Extensive experiments demonstrate that our AAPM achieves the state-of-the-art performance in both attribute-based multi-label few-shot action recognition and single-label few-shot action recognition. The project and dataset are available at an anonymous account <a target="_blank" rel="noopener" href="https://github.com/theAAPM/AAPM">https://github.com/theAAPM/AAPM</a> </p>
<blockquote>
<p>åœ¨ç°å®ä¸–ç•Œçš„åŠ¨ä½œè¯†åˆ«ç³»ç»Ÿä¸­ï¼Œèå…¥æ›´å¤šå±æ€§æœ‰åŠ©äºæ›´å…¨é¢åœ°ç†è§£äººç±»è¡Œä¸ºã€‚ç„¶è€Œï¼Œä½¿ç”¨å•ä¸€æ¨¡å‹åŒæ—¶è¯†åˆ«å¤šä¸ªå±æ€§å¯èƒ½ä¼šå¯¼è‡´ç²¾åº¦ä¸‹é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œå³è‡ªé€‚åº”å±æ€§åŸå‹æ¨¡å‹ï¼ˆAAPMï¼‰ç”¨äºäººç±»åŠ¨ä½œè¯†åˆ«ï¼Œè¯¥æ¨¡å‹æ•æ‰äº†ä¸°å¯Œçš„åŠ¨ä½œç›¸å…³å±æ€§ä¿¡æ¯ï¼Œå¹¶åœ¨ç²¾åº¦å’Œç¨³å¥æ€§ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–‡æœ¬çº¦æŸæ¨¡å—ï¼ˆTCMï¼‰ï¼Œä»¥èå…¥æ½œåœ¨æ ‡ç­¾çš„æ–‡æœ¬ä¿¡æ¯ï¼Œå¹¶çº¦æŸä¸åŒå±æ€§åŸå‹è¡¨ç¤ºçš„æ„å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å±æ€§åˆ†é…æ–¹æ³•ï¼ˆAAMï¼‰ï¼Œä»¥è§£å†³è®­ç»ƒè¿‡ç¨‹ä¸­çš„åè§é—®é¢˜ï¼Œå¹¶æé«˜ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„åŸºäºå±æ€§çš„å¤šæ ‡ç­¾è§†é¢‘æ•°æ®é›†ï¼Œåä¸ºMulti-Kineticsï¼Œç”¨äºè¯„ä¼°ï¼Œå…¶ä¸­åŒ…å«ä¸äººç±»è¡Œä¸ºç›¸å…³çš„å„ç§å±æ€§æ ‡ç­¾ï¼ˆä¾‹å¦‚åŠ¨ä½œã€åœºæ™¯ã€ç‰©ä½“ç­‰ï¼‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„AAPMåœ¨åŸºäºå±æ€§çš„å¤šæ ‡ç­¾å°æ ·æœ¬åŠ¨ä½œè¯†åˆ«å’Œå•æ ‡ç­¾å°æ ·æœ¬åŠ¨ä½œè¯†åˆ«æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚é¡¹ç›®å’Œæ•°æ®é›†å¯é€šè¿‡åŒ¿åè´¦å·<a target="_blank" rel="noopener" href="https://github.com/theAAPM/AAPM%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/theAAPM/AAPMè¿›è¡Œè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12582v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§æ–°çš„åŠ¨ä½œè¯†åˆ«æ–¹æ³•â€”â€”è‡ªé€‚åº”å±æ€§åŸå‹æ¨¡å‹ï¼ˆAAPMï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ•æ‰ä¸°å¯Œçš„åŠ¨ä½œç›¸å…³å±æ€§ä¿¡æ¯ï¼Œå¹¶åœ¨å‡†ç¡®æ€§å’Œç¨³å¥æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚é€šè¿‡å¼•å…¥æ–‡æœ¬çº¦æŸæ¨¡å—ï¼ˆTCMï¼‰å’Œå±æ€§åˆ†é…æ–¹æ³•ï¼ˆAAMï¼‰ï¼Œæé«˜äº†å¤šå±æ€§è¯†åˆ«ç³»ç»Ÿçš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œæ„å»ºäº†ä¸€ä¸ªæ–°çš„è§†é¢‘æ•°æ®é›†Multi-Kineticsï¼Œç”¨äºè¯„ä¼°åŸºäºå±æ€§çš„å¤šæ ‡ç­¾å’Œå•æ ‡ç­¾å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒAAPMåœ¨åŸºäºå±æ€§çš„å¤šæ ‡ç­¾å’Œå•æ ‡ç­¾å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªé€‚åº”å±æ€§åŸå‹æ¨¡å‹ï¼ˆAAPMï¼‰ç»“åˆäº†å¤šä¸ªå±æ€§ä»¥æé«˜å¯¹äººç±»è¡Œä¸ºçš„å…¨é¢ç†è§£ã€‚</li>
<li>é€šè¿‡å¼•å…¥æ–‡æœ¬çº¦æŸæ¨¡å—ï¼ˆTCMï¼‰ï¼Œå°†æ–‡æœ¬ä¿¡æ¯èå…¥æ½œåœ¨æ ‡ç­¾ï¼Œçº¦æŸä¸åŒå±æ€§åŸå‹è¡¨ç¤ºçš„æ„å»ºã€‚</li>
<li>æå‡ºçš„å±æ€§åˆ†é…æ–¹æ³•ï¼ˆAAMï¼‰è§£å†³äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„åè§é—®é¢˜ï¼Œæé«˜äº†ç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªæ–°çš„è§†é¢‘æ•°æ®é›†Multi-Kineticsï¼Œç”¨äºè¯„ä¼°åŸºäºå±æ€§çš„å¤šæ ‡ç­¾åŠ¨ä½œè¯†åˆ«æ€§èƒ½ã€‚</li>
<li>AAPMåœ¨åŸºäºå±æ€§çš„å¤šæ ‡ç­¾å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«æ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>AAPMåŒæ ·åœ¨å•æ ‡ç­¾å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-58076ae511ebeab6a5b1ce67cc9771bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a3340e5c4631da6ec6caf6a740f6f47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d75ac46816c47614d105c68d195be4b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c9238ee6ec62f1873d0a12fb6ec7fb4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Predicate-Hierarchies-Improve-Few-Shot-State-Classification"><a href="#Predicate-Hierarchies-Improve-Few-Shot-State-Classification" class="headerlink" title="Predicate Hierarchies Improve Few-Shot State Classification"></a>Predicate Hierarchies Improve Few-Shot State Classification</h2><p><strong>Authors:Emily Jin, Joy Hsu, Jiajun Wu</strong></p>
<p>State classification of objects and their relations is core to many long-horizon tasks, particularly in robot planning and manipulation. However, the combinatorial explosion of possible object-predicate combinations, coupled with the need to adapt to novel real-world environments, makes it a desideratum for state classification models to generalize to novel queries with few examples. To this end, we propose PHIER, which leverages predicate hierarchies to generalize effectively in few-shot scenarios. PHIER uses an object-centric scene encoder, self-supervised losses that infer semantic relations between predicates, and a hyperbolic distance metric that captures hierarchical structure; it learns a structured latent space of image-predicate pairs that guides reasoning over state classification queries. We evaluate PHIER in the CALVIN and BEHAVIOR robotic environments and show that PHIER significantly outperforms existing methods in few-shot, out-of-distribution state classification, and demonstrates strong zero- and few-shot generalization from simulated to real-world tasks. Our results demonstrate that leveraging predicate hierarchies improves performance on state classification tasks with limited data. </p>
<blockquote>
<p>å¯¹è±¡å’Œå®ƒä»¬çš„å…³ç³»çš„çŠ¶æ€åˆ†ç±»æ˜¯è®¸å¤šé•¿æœŸä»»åŠ¡çš„æ ¸å¿ƒï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººè§„åˆ’å’Œæ“ä½œæ–¹é¢ã€‚ç„¶è€Œï¼Œå¯èƒ½çš„å¯¹è±¡-è°“è¯ç»„åˆçš„ç»„åˆçˆ†ç‚¸ï¼Œä»¥åŠéœ€è¦é€‚åº”æ–°çš„ç°å®ä¸–ç•Œç¯å¢ƒï¼Œä½¿å¾—çŠ¶æ€åˆ†ç±»æ¨¡å‹éœ€è¦èƒ½å¤Ÿç”¨å°‘é‡ç¤ºä¾‹æ¨å¹¿åˆ°æ–°çš„æŸ¥è¯¢æˆä¸ºä¸€é¡¹å¿…è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†PHIERï¼Œå®ƒåˆ©ç”¨è°“è¯å±‚æ¬¡ç»“æ„åœ¨å°‘é‡åœºæ™¯ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨å¹¿ã€‚PHIERä½¿ç”¨ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„åœºæ™¯ç¼–ç å™¨ã€è‡ªç›‘ç£æŸå¤±æ¥æ¨æ–­è°“è¯ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä»¥åŠæ•æ‰å±‚æ¬¡ç»“æ„çš„åŒæ›²è·ç¦»åº¦é‡ï¼›å®ƒå­¦ä¹ å›¾åƒ-è°“è¯å¯¹çš„ç»“æ„åŒ–æ½œåœ¨ç©ºé—´ï¼ŒæŒ‡å¯¼çŠ¶æ€åˆ†ç±»æŸ¥è¯¢çš„æ¨ç†ã€‚æˆ‘ä»¬åœ¨CALVINå’ŒBEHAVIORæœºå™¨äººç¯å¢ƒä¸­è¯„ä¼°äº†PHIERï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨å°‘é‡ã€åˆ†å¸ƒå¤–çš„çŠ¶æ€åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒPHIERæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨ä»æ¨¡æ‹Ÿåˆ°çœŸå®ä¸–ç•Œä»»åŠ¡çš„é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬æ¨å¹¿ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜ï¼Œåˆ©ç”¨è°“è¯å±‚æ¬¡ç»“æ„åœ¨æ•°æ®æœ‰é™çš„çŠ¶æ€åˆ†ç±»ä»»åŠ¡ä¸Šå¯ä»¥æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12481v1">PDF</a> ICLR 2025. First two authors contributed equally. Project page:   <a target="_blank" rel="noopener" href="https://emilyzjin.github.io/projects/phier.html">https://emilyzjin.github.io/projects/phier.html</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¯¹è±¡å’Œå…³ç³»åˆ†ç±»çš„çŠ¶æ€åˆ†ç±»æ˜¯é•¿æœŸä»»åŠ¡çš„æ ¸å¿ƒï¼Œå°¤å…¶åœ¨æœºå™¨äººè§„åˆ’å’Œæ“ä½œé¢†åŸŸã€‚ç„¶è€Œï¼Œç”±äºå¯èƒ½çš„å¯¹è±¡-è°“è¯ç»„åˆçš„çˆ†ç‚¸æ€§å¢é•¿ä»¥åŠå¯¹æ–°ç°å®ç¯å¢ƒçš„é€‚åº”éœ€æ±‚ï¼ŒçŠ¶æ€åˆ†ç±»æ¨¡å‹éœ€è¦èƒ½å¤Ÿåœ¨æ–°æŸ¥è¯¢ä¸­å¿«é€Ÿé€‚åº”å¹¶å…·æœ‰å°‘é‡æ ·æœ¬çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†PHIERæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨è°“è¯å±‚æ¬¡ç»“æ„åœ¨å°‘é‡åœºæ™¯ä¸­å®ç°æœ‰æ•ˆæ³›åŒ–ã€‚PHIERä½¿ç”¨ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„åœºæ™¯ç¼–ç å™¨ã€è‡ªç›‘ç£æŸå¤±æ¥æ¨æ–­è°“è¯ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ä»¥åŠåŒæ›²è·ç¦»åº¦é‡æ¥æ•æ‰å±‚æ¬¡ç»“æ„ï¼›å®ƒå­¦ä¹ å›¾åƒ-è°“è¯å¯¹çš„ç»“æ„åŒ–æ½œåœ¨ç©ºé—´ï¼Œä¸ºçŠ¶æ€åˆ†ç±»æŸ¥è¯¢æä¾›æŒ‡å¯¼ã€‚æˆ‘ä»¬åœ¨CALVINå’ŒBEHAVIORæœºå™¨äººç¯å¢ƒä¸­è¯„ä¼°äº†PHIERçš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜ï¼ŒPHIERåœ¨å°‘é‡çš„æœªçŸ¥åˆ†å¸ƒçŠ¶æ€åˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†ä»æ¨¡æ‹Ÿä»»åŠ¡åˆ°çœŸå®ä»»åŠ¡ä¸­é›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬çš„å‡ºè‰²æ³›åŒ–èƒ½åŠ›ã€‚åˆ©ç”¨è°“è¯å±‚æ¬¡ç»“æ„æé«˜äº†åœ¨æœ‰é™æ•°æ®ä¸Šçš„çŠ¶æ€åˆ†ç±»ä»»åŠ¡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŠ¶æ€åˆ†ç±»æ˜¯æœºå™¨äººè§„åˆ’å’Œæ“ä½œç­‰é•¿æœŸä»»åŠ¡çš„æ ¸å¿ƒã€‚</li>
<li>å¯¹è±¡å’Œå…³ç³»åˆ†ç±»å¯¹äºé€‚åº”æ–°ç¯å¢ƒå’Œæ³›åŒ–è‡³å…³é‡è¦ã€‚</li>
<li>PHIERæ–¹æ³•åˆ©ç”¨è°“è¯å±‚æ¬¡ç»“æ„ä»¥å®ç°æœ‰æ•ˆæ³›åŒ–ã€‚</li>
<li>PHIERåŒ…æ‹¬ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„åœºæ™¯ç¼–ç å™¨ã€è‡ªç›‘ç£æŸå¤±å’Œæ•æ‰å±‚æ¬¡ç»“æ„çš„åŒæ›²è·ç¦»åº¦é‡ã€‚</li>
<li>PHIERåœ¨æœºå™¨äººç¯å¢ƒè¯„ä¼°ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>PHIERåœ¨å°‘é‡æœªçŸ¥åˆ†å¸ƒçŠ¶æ€åˆ†ç±»ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da9baf64913c6dff7e888ff917eefb6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0b667522e773d83e2492b52ffe9d1f3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UniMatch-Universal-Matching-from-Atom-to-Task-for-Few-Shot-Drug-Discovery"><a href="#UniMatch-Universal-Matching-from-Atom-to-Task-for-Few-Shot-Drug-Discovery" class="headerlink" title="UniMatch: Universal Matching from Atom to Task for Few-Shot Drug   Discovery"></a>UniMatch: Universal Matching from Atom to Task for Few-Shot Drug   Discovery</h2><p><strong>Authors:Ruifeng Li, Mingqian Li, Wei Liu, Yuhua Zhou, Xiangxin Zhou, Yuan Yao, Qiang Zhang, Hongyang Chen</strong></p>
<p>Drug discovery is crucial for identifying candidate drugs for various diseases.However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem. Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures that determine different molecular properties. To address these issues, we introduce Universal Matching Networks (UniMatch), a dual matching framework that integrates explicit hierarchical molecular matching with implicit task-level matching via meta-learning, bridging multi-level molecular representations and task-level generalization. Specifically, our approach explicitly captures structural features across multiple levels, such as atoms, substructures, and molecules, via hierarchical pooling and matching, facilitating precise molecular representation and comparison. Additionally, we employ a meta-learning strategy for implicit task-level matching, allowing the model to capture shared patterns across tasks and quickly adapt to new ones. This unified matching framework ensures effective molecular alignment while leveraging shared meta-knowledge for fast adaptation. Our experimental results demonstrate that UniMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and 6.52% in delta AUPRC. UniMatch also shows excellent generalization ability on the Meta-MolNet benchmark. </p>
<blockquote>
<p>è¯ç‰©å‘ç°å¯¹äºè¯†åˆ«å„ç§ç–¾ç—…çš„å€™é€‰è¯ç‰©è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå…¶è¾ƒä½çš„æˆåŠŸç‡ç»å¸¸å¯¼è‡´æ³¨é‡Šçš„ç¨€ç¼ºï¼Œä»è€Œæ„æˆå°æ ·æœ¬å­¦ä¹ é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€å°ºåº¦ç‰¹å¾ï¼Œå¿½ç•¥äº†å†³å®šä¸åŒåˆ†å­ç‰¹æ€§çš„åˆ†å±‚åˆ†å­ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Universal Matching Networksï¼ˆUniMatchï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒåŒ¹é…æ¡†æ¶ï¼Œå®ƒé€šè¿‡å…ƒå­¦ä¹ å°†æ˜¾å¼çš„åˆ†å±‚åˆ†å­åŒ¹é…ä¸éšå¼çš„ä»»åŠ¡çº§åŒ¹é…ç›¸ç»“åˆï¼Œæ¡¥æ¥å¤šå±‚æ¬¡åˆ†å­è¡¨ç¤ºå’Œä»»åŠ¡çº§æ³›åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ†å±‚æ± åŒ–å’ŒåŒ¹é…æ˜¾å¼æ•è·å¤šä¸ªå±‚æ¬¡çš„ç»“æ„ç‰¹å¾ï¼Œå¦‚åŸå­ã€å­ç»“æ„å’Œåˆ†å­ï¼Œä¿ƒè¿›ç²¾ç¡®çš„åˆ†å­è¡¨ç¤ºå’Œæ¯”è¾ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å…ƒå­¦ä¹ ç­–ç•¥è¿›è¡Œéšå¼ä»»åŠ¡çº§åŒ¹é…ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•è·ä»»åŠ¡ä¹‹é—´çš„å…±äº«æ¨¡å¼å¹¶å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚è¿™ä¸€ç»Ÿä¸€çš„åŒ¹é…æ¡†æ¶ç¡®ä¿äº†æœ‰æ•ˆçš„åˆ†å­å¯¹é½ï¼ŒåŒæ—¶åˆ©ç”¨å…±äº«å…ƒçŸ¥è¯†å®ç°å¿«é€Ÿé€‚åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniMatchåœ¨MoleculeNetå’ŒFS-MolåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨AUROCä¸­æé«˜äº†2.87%ï¼Œåœ¨delta AUPRCä¸­æé«˜äº†6.52%ã€‚UniMatchåœ¨Meta-MolNetåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12453v1">PDF</a> accepted as ICLR 2025 Spotlight</p>
<p><strong>Summary</strong></p>
<p>è¯ç‰©å‘ç°ä¸­è¯†åˆ«å€™é€‰è¯ç‰©æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œä½†å…¶ä½æˆåŠŸç‡å¯¼è‡´æ ‡æ³¨æ•°æ®ç¨€ç¼ºï¼Œæ„æˆå°æ ·æœ¬å­¦ä¹ é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€å°ºåº¦ç‰¹å¾ï¼Œå¿½ç•¥äº†å†³å®šåˆ†å­å±æ€§çš„å±‚æ¬¡ç»“æ„ã€‚æœ¬ç ”ç©¶å¼•å…¥Universal Matching Networksï¼ˆUniMatchï¼‰ï¼Œç»“åˆæ˜ç¡®çš„å±‚æ¬¡åˆ†å­åŒ¹é…å’ŒåŸºäºå…ƒå­¦ä¹ çš„éšä»»åŠ¡åŒ¹é…ï¼Œæ„å»ºå¤šå±‚æ¬¡çš„åˆ†å­è¡¨ç¤ºå’Œä»»åŠ¡æ³›åŒ–æ¡¥æ¢ã€‚é€šè¿‡å±‚æ¬¡æ± åŒ–å’ŒåŒ¹é…ï¼Œæ•è·åŸå­ã€å­ç»“æ„å’Œåˆ†å­ç­‰å¤šä¸ªå±‚æ¬¡çš„ç»“æ„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å…ƒå­¦ä¹ ç­–ç•¥è¿›è¡Œéšä»»åŠ¡åŒ¹é…ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•è·ä»»åŠ¡é—´çš„å…±äº«æ¨¡å¼å¹¶å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚å®éªŒç»“æœåœ¨MoleculeNetå’ŒFS-MolåŸºå‡†æµ‹è¯•ä¸­è¡¨æ˜ï¼ŒUniMatchä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæé«˜äº†AUROCçš„2.87%å’Œdelta AUPRCçš„6.52%ã€‚åŒæ—¶ï¼ŒUniMatchåœ¨Meta-MolNetåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯ç‰©å‘ç°é¢ä¸´ä½æˆåŠŸç‡å’Œæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæ„æˆå°æ ·æœ¬å­¦ä¹ æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€å°ºåº¦ç‰¹å¾ï¼Œå¿½ç•¥äº†åˆ†å­çš„å±‚æ¬¡ç»“æ„ã€‚</li>
<li>UniMatchæ˜¯ä¸€ä¸ªåŒåŒ¹é…æ¡†æ¶ï¼Œç»“åˆæ˜ç¡®çš„å±‚æ¬¡åˆ†å­åŒ¹é…å’Œéšä»»åŠ¡åŒ¹é…ã€‚</li>
<li>UniMatché€šè¿‡å±‚æ¬¡æ± åŒ–å’ŒåŒ¹é…æŠ€æœ¯ï¼Œæ•è·å¤šä¸ªå±‚æ¬¡çš„ç»“æ„ç‰¹å¾ã€‚</li>
<li>å…ƒå­¦ä¹ ç­–ç•¥ç”¨äºéšä»»åŠ¡åŒ¹é…ï¼Œæé«˜æ¨¡å‹å¯¹ä»»åŠ¡é—´å…±äº«æ¨¡å¼çš„æ•è·èƒ½åŠ›ï¼Œå¹¶åŠ å¿«å¯¹æ–°ä»»åŠ¡çš„é€‚åº”ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒUniMatchåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12453">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dabcc817a76f2ad19aa9f5afbbe9b7c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66f7098bdbba2205e9cbed26478c6025.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4415c3bbd891c14c91370b5e756384be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f39c32afa7bb8bdf0e3667a1ff29a511.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Gradient-Co-occurrence-Analysis-for-Detecting-Unsafe-Prompts-in-Large-Language-Models"><a href="#Gradient-Co-occurrence-Analysis-for-Detecting-Unsafe-Prompts-in-Large-Language-Models" class="headerlink" title="Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large   Language Models"></a>Gradient Co-occurrence Analysis for Detecting Unsafe Prompts in Large   Language Models</h2><p><strong>Authors:Jingyuan Yang, Bowen Yan, Rongjun Li, Ziyu Zhou, Xin Chen, Zhiyong Feng, Wei Peng</strong></p>
<p>Unsafe prompts pose significant safety risks to large language models (LLMs). Existing methods for detecting unsafe prompts rely on data-driven fine-tuning to train guardrail models, necessitating significant data and computational resources. In contrast, recent few-shot gradient-based methods emerge, requiring only few safe and unsafe reference prompts. A gradient-based approach identifies unsafe prompts by analyzing consistent patterns of the gradients of safety-critical parameters in LLMs. Although effective, its restriction to directional similarity (cosine similarity) introduces <code>directional bias&#39;&#39;, limiting its capability to identify unsafe prompts. To overcome this limitation, we introduce GradCoo, a novel gradient co-occurrence analysis method that expands the scope of safety-critical parameter identification to include unsigned gradient similarity, thereby reducing the impact of </code>directional biasâ€™â€™ and enhancing the accuracy of unsafe prompt detection. Comprehensive experiments on the widely-used benchmark datasets ToxicChat and XStest demonstrate that our proposed method can achieve state-of-the-art (SOTA) performance compared to existing methods. Moreover, we confirm the generalizability of GradCoo in detecting unsafe prompts across a range of LLM base models with various sizes and origins. </p>
<blockquote>
<p>ä¸å®‰å…¨çš„æç¤ºå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„æˆé‡å¤§å®‰å…¨é£é™©ã€‚ç°æœ‰çš„æ£€æµ‹ä¸å®‰å…¨æç¤ºçš„æ–¹æ³•ä¾èµ–äºæ•°æ®é©±åŠ¨å¾®è°ƒæ¥è®­ç»ƒæŠ¤æ æ¨¡å‹ï¼Œè¿™éœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ€è¿‘å‡ºç°çš„åŸºäºæ¢¯åº¦çš„å°‘æ ·æœ¬æ–¹æ³•åªéœ€è¦å°‘é‡å®‰å…¨å’Œä¸å®‰å…¨çš„å‚è€ƒæç¤ºã€‚åŸºäºæ¢¯åº¦çš„æ–¹æ³•é€šè¿‡åˆ†æå’ŒLLMä¸­å®‰å…¨å…³é”®å‚æ•°çš„ä¸€è‡´æ€§æ¢¯åº¦æ¨¡å¼æ¥è¯†åˆ«ä¸å®‰å…¨çš„æç¤ºã€‚å°½ç®¡è¿™ç§æ–¹æ³•æœ‰æ•ˆï¼Œä½†å®ƒå¯¹æ–¹å‘ç›¸ä¼¼æ€§ï¼ˆä½™å¼¦ç›¸ä¼¼æ€§ï¼‰çš„é™åˆ¶å¯¼è‡´äº†â€œæ–¹å‘åè§â€ï¼Œé™åˆ¶äº†å…¶è¯†åˆ«ä¸å®‰å…¨æç¤ºçš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†GradCooï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¢¯åº¦å…±ç°åˆ†ææ–¹æ³•ï¼Œå°†å®‰å…¨å…³é”®å‚æ•°çš„è¯†åˆ«èŒƒå›´æ‰©å¤§åˆ°åŒ…æ‹¬æ— ç¬¦å·æ¢¯åº¦ç›¸ä¼¼æ€§ï¼Œä»è€Œå‡å°‘â€œæ–¹å‘åè§â€çš„å½±å“ï¼Œæé«˜ä¸å®‰å…¨æç¤ºæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ToxicChatå’ŒXStestä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•å¯ä»¥è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨ä¸åŒå¤§å°å’Œæ¥æºçš„å¤§å‹è¯­è¨€æ¨¡å‹åŸºç¡€æ¨¡å‹ä¸Šæ£€æµ‹ä¸å®‰å…¨æç¤ºï¼Œè¯å®äº†GradCooçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12411v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-494be2d7bba588a3bd53f3df03f90119.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58b7a2b16754e1ca823a44debd2da16e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97d44ffa80a04143617e31ace8c1b574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cda389c85802a695ba1bf2afdc473035.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2cef9b49a80d218fe23f6c7c30311e7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53c3b49b973cbb173781656adc7d5ee7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-Frame-Detection-with-Retrieval-Augmented-Generation"><a href="#Enhancing-Frame-Detection-with-Retrieval-Augmented-Generation" class="headerlink" title="Enhancing Frame Detection with Retrieval Augmented Generation"></a>Enhancing Frame Detection with Retrieval Augmented Generation</h2><p><strong>Authors:Papa Abdou Karim Karou Diallo, Amal Zouaq</strong></p>
<p>Recent advancements in Natural Language Processing have significantly improved the extraction of structured semantic representations from unstructured text, especially through Frame Semantic Role Labeling (FSRL). Despite this progress, the potential of Retrieval-Augmented Generation (RAG) models for frame detection remains under-explored. In this paper, we present the first RAG-based approach for frame detection called RCIF (Retrieve Candidates and Identify Frames). RCIF is also the first approach to operate without the need for explicit target span and comprises three main stages: (1) generation of frame embeddings from various representations ; (2) retrieval of candidate frames given an input text; and (3) identification of the most suitable frames. We conducted extensive experiments across multiple configurations, including zero-shot, few-shot, and fine-tuning settings. Our results show that our retrieval component significantly reduces the complexity of the task by narrowing the search space thus allowing the frame identifier to refine and complete the set of candidates. Our approach achieves state-of-the-art performance on FrameNet 1.5 and 1.7, demonstrating its robustness in scenarios where only raw text is provided. Furthermore, we leverage the structured representation obtained through this method as a proxy to enhance generalization across lexical variations in the task of translating natural language questions into SPARQL queries. </p>
<blockquote>
<p>è¿‘æœŸè‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ¡†æ¶è¯­ä¹‰è§’è‰²æ ‡æ³¨ï¼ˆFSRLï¼‰çš„æŠ€æœ¯ï¼Œå·²ä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æ˜¾è‘—æé«˜äº†ç»“æ„åŒ–è¯­ä¹‰è¡¨ç¤ºçš„æå–ã€‚å°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œä½†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å‹åœ¨æ¡†æ¶æ£€æµ‹æ–¹é¢çš„æ½œåŠ›ä»è¢«ä½ä¼°ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†åŸºäºRAGçš„æ¡†æ¶æ£€æµ‹æ–¹æ³•ï¼Œç§°ä¸ºRCIFï¼ˆæ£€ç´¢å€™é€‰å¯¹è±¡å¹¶è¯†åˆ«æ¡†æ¶ï¼‰ã€‚RCIFä¹Ÿæ˜¯ç¬¬ä¸€ä¸ªæ— éœ€æ˜¾å¼ç›®æ ‡è·¨åº¦å³å¯è¿è¡Œçš„æ–¹æ³•ï¼Œä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰ä»å„ç§è¡¨ç¤ºç”Ÿæˆæ¡†æ¶åµŒå…¥ï¼›ï¼ˆ2ï¼‰ç»™å®šè¾“å…¥æ–‡æœ¬æ£€ç´¢å€™é€‰æ¡†æ¶ï¼›ï¼ˆ3ï¼‰ç¡®å®šæœ€åˆé€‚çš„æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨å¤šç§é…ç½®ä¸‹è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°æ ·æœ¬æ–‡æœ¬å’Œå¾®è°ƒè®¾ç½®ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ£€ç´¢ç»„ä»¶é€šè¿‡ç¼©å°æœç´¢ç©ºé—´æ˜¾è‘—é™ä½äº†ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œä»è€Œå…è®¸æ¡†æ¶æ ‡è¯†ç¬¦å¯¹å€™é€‰é›†è¿›è¡Œç»†åŒ–å’Œè¡¥å……ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨FrameNet 1.5å’Œ1.7ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨ä»…æä¾›åŸå§‹æ–‡æœ¬çš„æƒ…å†µä¸‹åœºæ™¯çš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å€ŸåŠ©é€šè¿‡æ­¤æ–¹æ³•è·å¾—çš„ç»“æ„åŒ–è¡¨ç¤ºä½œä¸ºä»£ç†ï¼Œä»¥æé«˜åœ¨å°†è‡ªç„¶è¯­è¨€é—®é¢˜ç¿»è¯‘æˆSPARQLæŸ¥è¯¢çš„ä»»åŠ¡ä¸­å¯¹è¯æ±‡å˜åŒ–çš„æ¦‚æ‹¬èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12210v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å‹çš„æ¡†æ¶æ£€æµ‹æ–¹æ³•çš„é¦–æ¬¡å°è¯•ã€‚è¯¥æ–¹æ³•ç§°ä¸ºRCIFï¼Œæ— éœ€æ˜ç¡®çš„ç›®æ ‡è·¨åº¦ï¼Œé€šè¿‡ç”Ÿæˆæ¡†æ¶åµŒå…¥ã€æ£€ç´¢å€™é€‰æ¡†æ¶å’Œè¯†åˆ«æœ€åˆé€‚æ¡†æ¶ä¸‰ä¸ªé˜¶æ®µæ¥å®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ£€ç´¢ç»„ä»¶é€šè¿‡ç¼©å°æœç´¢ç©ºé—´æ˜¾è‘—é™ä½äº†ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œä½¿æ¡†æ¶æ ‡è¯†ç¬¦èƒ½å¤Ÿå®Œå–„å’Œè¡¥å……å€™é€‰é›†ã€‚è¯¥æ–¹æ³•åœ¨FrameNet 1.5å’Œ1.7ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†åœ¨åªæœ‰åŸå§‹æ–‡æœ¬æä¾›çš„åœºæ™¯ä¸­å¢å¼ºè·¨è¯æ±‡å˜åŒ–æ¨å¹¿çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡æ­¤æ–¹æ³•è·å¾—çš„ç»“æ„åŒ–è¡¨ç¤ºè¢«ç”¨ä½œä»£ç†ï¼Œä»¥æé«˜å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºSPARQLæŸ¥è¯¢ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å‹çš„æ¡†æ¶æ£€æµ‹æ–¹æ³•RCIFã€‚</li>
<li>RCIFæ— éœ€æ˜ç¡®ç›®æ ‡è·¨åº¦ï¼ŒåŒ…å«æ¡†æ¶åµŒå…¥ç”Ÿæˆã€å€™é€‰æ¡†æ¶æ£€ç´¢å’Œæœ€é€‚åˆæ¡†æ¶è¯†åˆ«ä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>æ£€ç´¢ç»„ä»¶é€šè¿‡ç¼©å°æœç´¢ç©ºé—´é™ä½ä»»åŠ¡å¤æ‚æ€§ã€‚</li>
<li>RCIFåœ¨FrameNet 1.5å’Œ1.7ä¸Šå®ç°æœ€å…ˆè¿›æ€§èƒ½ï¼Œå±•ç°é²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åªæœ‰åŸå§‹æ–‡æœ¬æä¾›çš„åœºæ™¯ã€‚</li>
<li>åˆ©ç”¨RCIFè·å¾—çš„ç»“æ„åŒ–è¡¨ç¤ºä½œä¸ºä»£ç†ï¼Œæé«˜äº†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºSPARQLæŸ¥è¯¢ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>RCIFæ–¹æ³•ç»“åˆäº†ç”Ÿæˆå’Œæ£€ç´¢çš„ä¼˜åŠ¿ï¼Œä¸ºæœªæ¥æ¡†æ¶æ£€æµ‹æä¾›äº†æ–°æ–¹å‘ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRAGæ¨¡å‹åœ¨æ¡†æ¶æ£€æµ‹ä»»åŠ¡ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-acfe35284068873d89f75fc82c13359f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10f9af5a4733890d336251da738c76cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78914eb3cc3d57cf319178cf263a6f30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db2f505b089bd6ca7ded19f557d48128.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-Input-Attributions-Interpret-the-Inductive-Reasoning-Process-in-In-Context-Learning"><a href="#Can-Input-Attributions-Interpret-the-Inductive-Reasoning-Process-in-In-Context-Learning" class="headerlink" title="Can Input Attributions Interpret the Inductive Reasoning Process in   In-Context Learning?"></a>Can Input Attributions Interpret the Inductive Reasoning Process in   In-Context Learning?</h2><p><strong>Authors:Mengyu Ye, Tatsuki Kuribayashi, Goro Kobayashi, Jun Suzuki</strong></p>
<p>Interpreting the internal process of neural models has long been a challenge. This challenge remains relevant in the era of large language models (LLMs) and in-context learning (ICL); for example, ICL poses a new issue of interpreting which example in the few-shot examples contributed to identifying&#x2F;solving the task. To this end, in this paper, we design synthetic diagnostic tasks of inductive reasoning, inspired by the generalization tests in linguistics; here, most in-context examples are ambiguous w.r.t. their underlying rule, and one critical example disambiguates the task demonstrated. The question is whether conventional input attribution (IA) methods can track such a reasoning process, i.e., identify the influential example, in ICL. Our experiments provide several practical findings; for example, a certain simple IA method works the best, and the larger the model, the generally harder it is to interpret the ICL with gradient-based IA methods. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œæ¨¡å‹çš„å†…éƒ¨è¿‡ç¨‹è§£é‡Šä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„æ—¶ä»£ï¼Œè¿™ä¸€æŒ‘æˆ˜ä¾ç„¶å…·æœ‰ç°å®æ„ä¹‰ã€‚ä¾‹å¦‚ï¼ŒICLæå‡ºäº†æ–°çš„è§£é‡Šé—®é¢˜ï¼Œå³åœ¨å°‘æ•°ä¾‹å­ä¸­ï¼Œå“ªä¸ªä¾‹å­å¯¹ä»»åŠ¡çš„è¯†åˆ«&#x2F;è§£å†³èµ·åˆ°äº†è´¡çŒ®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡è®¾è®¡äº†åŸºäºå½’çº³æ¨ç†çš„åˆæˆè¯Šæ–­ä»»åŠ¡ï¼Œè¯¥è®¾è®¡çµæ„Ÿæ¥æºäºè¯­è¨€å­¦çš„æ³›åŒ–æµ‹è¯•ã€‚åœ¨æ­¤åœºæ™¯ä¸­ï¼Œå¤§å¤šæ•°ä¸Šä¸‹æ–‡ä¾‹å­å¯¹äºå…¶éšå«çš„è§„åˆ™æ˜¯æ¨¡ç³Šçš„ï¼Œåªæœ‰ä¸€ä¸ªå…³é”®ä¾‹å­èƒ½å¤Ÿæ¾„æ¸…ä»»åŠ¡ã€‚é—®é¢˜æ˜¯ä¼ ç»Ÿçš„è¾“å…¥å½’å› ï¼ˆIAï¼‰æ–¹æ³•æ˜¯å¦èƒ½è¿½è¸ªè¿™æ ·çš„æ¨ç†è¿‡ç¨‹ï¼Œå³åœ¨ICLä¸­è¯†åˆ«æœ‰å½±å“çš„ä¾‹å­ã€‚æˆ‘ä»¬çš„å®éªŒæä¾›äº†å‡ ä¸ªå®é™…å‘ç°ï¼šä¾‹å¦‚ï¼ŒæŸç§ç®€å•çš„IAæ–¹æ³•æ•ˆæœæœ€å¥½ï¼Œæ¨¡å‹è¶Šå¤§ï¼ŒåŸºäºæ¢¯åº¦çš„IAæ–¹æ³•é€šå¸¸è¶Šéš¾è§£é‡ŠICLã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15628v3">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä¸­çš„è§£é‡Šæ¨¡å‹å†…éƒ¨è¿‡ç¨‹çš„æŒ‘æˆ˜ã€‚è®¾è®¡äº†ä¸€ç§åŸºäºè¯­è¨€å­¦æ¨å¹¿æµ‹è¯•çš„æ¨ç†ä»»åŠ¡è¯Šæ–­å·¥å…·ï¼Œå…¶ä¸­å¤§å¤šæ•°ä¸Šä¸‹æ–‡ç¤ºä¾‹å¯¹äºå…¶éšå«çš„è§„åˆ™æ˜¯æ¨¡ç³Šçš„ï¼Œåªæœ‰ä¸€ä¸ªå…³é”®ç¤ºä¾‹èƒ½æ¾„æ¸…ä»»åŠ¡ã€‚å®éªŒå‘ç°ç®€å•çš„è¾“å…¥å½’å› æ–¹æ³•æ•ˆæœæœ€å¥½ï¼Œä¸”æ¨¡å‹è¶Šå¤§ï¼ŒåŸºäºæ¢¯åº¦çš„å½’å› æ–¹æ³•è§£é‡Šä¸Šä¸‹æ–‡å­¦ä¹ çš„éš¾åº¦è¶Šé«˜ã€‚æ–‡ç« è¯•å›¾ç ”ç©¶å¦‚ä½•è¯†åˆ«å’Œè·Ÿè¸ªè¿™ç§æ¨ç†è¿‡ç¨‹çš„é—®é¢˜ä»æœªå¾—åˆ°è§£å†³ã€‚ç›®å‰ç°æœ‰çš„ä¼ ç»Ÿè¾“å…¥å½’å› æ–¹æ³•éš¾ä»¥åœ¨è¾ƒå°‘æ•°æ®ç‚¹çš„æƒ…æ™¯ä¸­æ•æ‰åˆ°è¿™ç±»å¤æ‚çš„æ¨¡å¼å­¦ä¹ ä¿¡æ¯åˆ†å¸ƒèƒ½åŠ›ä½ä¸‹èƒŒåçš„éšå«è¦ç´ é›†æ¨ç†è§„åˆ™ã€‚å› æ­¤ï¼Œéœ€è¦å¼€å‘æ–°çš„æ–¹æ³•æ¥è§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ¨ç†è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ–‡çš„ä¸»è¦è§è§£ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åœ¨è§£é‡Šæ¨¡å‹å†…éƒ¨è¿‡ç¨‹æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡è®¾è®¡åŸºäºè¯­è¨€å­¦æ¨å¹¿æµ‹è¯•çš„æ¨ç†ä»»åŠ¡è¯Šæ–­å·¥å…·ï¼Œå‘ç°å¤§å¤šæ•°ä¸Šä¸‹æ–‡ç¤ºä¾‹å¯¹äºéšå«çš„è§„åˆ™æ˜¯æ¨¡ç³Šçš„ã€‚</li>
<li>ä¸€ä¸ªå…³é”®ç¤ºä¾‹èƒ½å¤Ÿæ¾„æ¸…ä»»åŠ¡ï¼Œä½†ç°æœ‰çš„ä¼ ç»Ÿè¾“å…¥å½’å› ï¼ˆIAï¼‰æ–¹æ³•éš¾ä»¥è¯†åˆ«å’Œè·Ÿè¸ªè¿™ç§æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>ç®€å•è¾“å…¥å½’å› æ–¹æ³•åœ¨å®è·µä¸­æ•ˆæœæœ€å¥½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8df10c25215774ea23e7fb068e39c364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28de4e1dc6a9f27d74eb6d76286ba1cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d756c54cd9a3efe2b1c97aa6049ff5b9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PeerArg-Argumentative-Peer-Review-with-LLMs"><a href="#PeerArg-Argumentative-Peer-Review-with-LLMs" class="headerlink" title="PeerArg: Argumentative Peer Review with LLMs"></a>PeerArg: Argumentative Peer Review with LLMs</h2><p><strong>Authors:Purin Sukpanichnant, Anna Rapberger, Francesca Toni</strong></p>
<p>Peer review is an essential process to determine the quality of papers submitted to scientific conferences or journals. However, it is subjective and prone to biases. Several studies have been conducted to apply techniques from NLP to support peer review, but they are based on black-box techniques and their outputs are difficult to interpret and trust. In this paper, we propose a novel pipeline to support and understand the reviewing and decision-making processes of peer review: the PeerArg system combining LLMs with methods from knowledge representation. PeerArg takes in input a set of reviews for a paper and outputs the paper acceptance prediction. We evaluate the performance of the PeerArg pipeline on three different datasets, in comparison with a novel end-2-end LLM that uses few-shot learning to predict paper acceptance given reviews. The results indicate that the end-2-end LLM is capable of predicting paper acceptance from reviews, but a variant of the PeerArg pipeline outperforms this LLM. </p>
<blockquote>
<p>åŒè¡Œè¯„å®¡æ˜¯ç¡®å®šæäº¤åˆ°ç§‘å­¦ä¼šè®®æˆ–æœŸåˆŠçš„è®ºæ–‡è´¨é‡çš„å…³é”®è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå®ƒæ˜¯ä¸»è§‚çš„ï¼Œå®¹æ˜“å­˜åœ¨åè§ã€‚å·²ç»è¿›è¡Œäº†å¤šé¡¹ç ”ç©¶ï¼Œå°†NLPæŠ€æœ¯åº”ç”¨äºæ”¯æŒåŒè¡Œè¯„å®¡ï¼Œä½†å®ƒä»¬åŸºäºé»‘ç®±æŠ€æœ¯ï¼Œå…¶è¾“å‡ºéš¾ä»¥è§£é‡Šå’Œä¿¡ä»»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¯æŒå¹¶ç†è§£åŒè¡Œè¯„å®¡å’Œå†³ç­–è¿‡ç¨‹çš„æ–°å‹ç®¡é“ï¼šPeerArgç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒçŸ¥è¯†è¡¨ç¤ºæ–¹æ³•ã€‚PeerArgæ¥å—ä¸€ç»„å¯¹è®ºæ–‡çš„è¯„å®¡æ„è§ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºè®ºæ–‡çš„æ¥å—é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¯„ä¼°PeerArgç®¡é“çš„æ€§èƒ½ï¼Œå¹¶å°†å…¶ä¸ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯LLMè¿›è¡Œæ¯”è¾ƒï¼Œè¯¥LLMä½¿ç”¨å°æ ·æœ¬å­¦ä¹ æŠ€æœ¯æ ¹æ®è¯„å®¡é¢„æµ‹è®ºæ–‡æ¥å—æƒ…å†µã€‚ç»“æœè¡¨æ˜ï¼Œç«¯åˆ°ç«¯LLMèƒ½å¤Ÿä»è¯„å®¡ä¸­é¢„æµ‹è®ºæ–‡æ¥å—æƒ…å†µï¼Œä½†PeerArgç®¡é“çš„ä¸€ä¸ªå˜ä½“è¡¨ç°ä¼˜äºè¯¥LLMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16813v2">PDF</a> Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ”¯æŒå¹¶ç†è§£åŒè¡Œè¯„å®¡çš„å®¡ç¨¿å’Œå†³ç­–è¿‡ç¨‹çš„ç³»ç»Ÿâ€”â€”PeerArgã€‚è¯¥ç³»ç»Ÿç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒçŸ¥è¯†è¡¨ç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŒè¡Œè¯„å®¡çš„æ•ˆç‡å’Œé€æ˜åº¦ã€‚é€šè¿‡è¾“å…¥ä¸€ç»„è®ºæ–‡çš„è¯„å®¡æ„è§ï¼ŒPeerArgå¯ä»¥é¢„æµ‹è®ºæ–‡çš„æ¥å—ç¨‹åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºé‡‡ç”¨å°‘æ ·æœ¬å­¦ä¹ çš„ç«¯åˆ°ç«¯LLMæ¨¡å‹ï¼ŒPeerArgç³»ç»Ÿçš„å˜ä½“åœ¨é¢„æµ‹è®ºæ–‡æ¥å—ç¨‹åº¦æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ä»‹ç»äº†PeerArgç³»ç»Ÿï¼Œä¸€ä¸ªç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’ŒçŸ¥è¯†è¡¨ç¤ºæ–¹æ³•çš„åŒè¡Œè¯„å®¡æ”¯æŒç³»ç»Ÿã€‚</li>
<li>PeerArgæ—¨åœ¨æé«˜åŒè¡Œè¯„å®¡çš„æ•ˆç‡å’Œé€æ˜åº¦ï¼Œé€šè¿‡å¤„ç†è¯„å®¡æ„è§æ¥é¢„æµ‹è®ºæ–‡çš„æ¥å—ç¨‹åº¦ã€‚</li>
<li>ç ”ç©¶å¯¹PeerArgç®¡é“å’Œé‡‡ç”¨å°‘æ ·æœ¬å­¦ä¹ çš„ç«¯åˆ°ç«¯LLMè¿›è¡Œäº†æ¯”è¾ƒè¯„ä¼°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPeerArgç®¡é“çš„å˜ä½“åœ¨é¢„æµ‹è®ºæ–‡æ¥å—ç¨‹åº¦æ–¹é¢è¡¨ç°ä¼˜äºç«¯åˆ°ç«¯LLMã€‚</li>
<li>è™½ç„¶LLMå¯ç”¨äºé¢„æµ‹è®ºæ–‡æ¥å—ç¨‹åº¦ï¼Œä½†å…¶è¾“å‡ºéš¾ä»¥è§£é‡Šå’Œä¿¡ä»»ï¼Œè€ŒPeerArgæä¾›äº†æ›´æ¸…æ™°çš„å†³ç­–ä¾æ®ã€‚</li>
<li>PeerArgç³»ç»Ÿå¯ä»¥å¸®åŠ©å‡å°‘ä¼ ç»ŸåŒè¡Œè¯„å®¡è¿‡ç¨‹ä¸­çš„ä¸»è§‚æ€§å’Œåè§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ”¹è¿›åŒè¡Œè¯„å®¡è¿‡ç¨‹æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16813">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2a8e71b997f96c44bd5a47165a42ac92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28eaf832aae98e22bb095aed438e7893.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b23f52a43f1d2a72a5e12bbeca4b291.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77401cb8bd507d496f1b8b080d066d50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e28426eae33697d2bb71b7701d883874.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5180ccd13fa598517b5cc37f990d68a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a08c5629f65fbef5309f311a76ac9498.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Acoustic-Prompt-Tuning-Empowering-Large-Language-Models-with-Audition-Capabilities"><a href="#Acoustic-Prompt-Tuning-Empowering-Large-Language-Models-with-Audition-Capabilities" class="headerlink" title="Acoustic Prompt Tuning: Empowering Large Language Models with Audition   Capabilities"></a>Acoustic Prompt Tuning: Empowering Large Language Models with Audition   Capabilities</h2><p><strong>Authors:Jinhua Liang, Xubo Liu, Wenwu Wang, Mark D. Plumbley, Huy Phan, Emmanouil Benetos</strong></p>
<p>The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of language and vision understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capability. In this work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending LLMs and VLMs to the audio domain by injecting audio embeddings to the input of LLMs, namely soft prompting. Specifically, APT applies an instruction-aware audio aligner to generate soft prompts, conditioned on both input text and sounds, as the inputs to the language model. To mitigate data scarcity in the audio domain, a curriculum learning strategy is proposed by formulating diverse audio tasks in a sequential manner. Moreover, we improve the audio language model by using interleaved audio-text embeddings as the input sequence. In this improved model, zero constraints are imposed on the input format, thus it is capable of tackling diverse modelling tasks, such as few-shot audio classification and audio comparison. To further evaluate the advanced ability of the audio networks, we introduce natural language audio reasoning (NLAR), a new task that analyses two audio clips by comparison and summarisation. Experiments show that APT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the expert models (i.e., the networks trained on the target datasets) across various tasks. We finally demonstrate APTâ€™s ability in extending frozen VLMs to the audio domain without fine-tuning, achieving promising results in audio-visual question and answering. Our code and model weights will be released at <a target="_blank" rel="noopener" href="https://github.com/JinhuaLiang/APT">https://github.com/JinhuaLiang/APT</a> </p>
<blockquote>
<p>å¬è§‰ç³»ç»Ÿåœ¨å¡‘é€ äººç±»æ•´ä½“æ„ŸçŸ¥ä½“éªŒä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚å°½ç®¡ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§£å†³å„ç§è¯­è¨€å’Œè§†è§‰ç†è§£ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†å…¶ä¸­åªæœ‰å°‘æ•°èƒ½å¤Ÿæ¨å¹¿è‡³éŸ³é¢‘é¢†åŸŸè€Œä¸æŸå®³å…¶ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å£°å­¦æç¤ºè°ƒæ•´ï¼ˆAPTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é€‚é…å™¨ï¼Œé€šè¿‡å°†éŸ³é¢‘åµŒå…¥æ³¨å…¥LLMsçš„è¾“å…¥æ¥æ‰©å±•LLMså’ŒVLMsè‡³éŸ³é¢‘é¢†åŸŸï¼Œå³è½¯æç¤ºã€‚å…·ä½“è€Œè¨€ï¼ŒAPTåº”ç”¨æŒ‡ä»¤æ„ŸçŸ¥éŸ³é¢‘å¯¹é½å™¨æ¥ç”Ÿæˆè½¯æç¤ºï¼Œè¿™äº›æç¤ºåŸºäºè¾“å…¥æ–‡æœ¬å’Œå£°éŸ³ï¼Œä½œä¸ºè¯­è¨€æ¨¡å‹çš„è¾“å…¥ã€‚ä¸ºäº†ç¼“è§£éŸ³é¢‘é¢†åŸŸçš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œä»¥é¡ºåºæ–¹å¼åˆ¶å®šå¤šæ ·åŒ–çš„éŸ³é¢‘ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨äº¤æ›¿çš„éŸ³é¢‘æ–‡æœ¬åµŒå…¥ä½œä¸ºè¾“å…¥åºåˆ—æ¥æ”¹è¿›éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚åœ¨è¿™ä¸ªæ”¹è¿›åçš„æ¨¡å‹ä¸­ï¼Œå¯¹è¾“å…¥æ ¼å¼æ²¡æœ‰æ–½åŠ ä»»ä½•çº¦æŸï¼Œå› æ­¤å®ƒèƒ½å¤Ÿå¤„ç†å„ç§å»ºæ¨¡ä»»åŠ¡ï¼Œå¦‚å°‘æ ·æœ¬éŸ³é¢‘åˆ†ç±»å’ŒéŸ³é¢‘æ¯”è¾ƒã€‚ä¸ºäº†è¿›ä¸€æ­¥è¯„ä¼°éŸ³é¢‘ç½‘ç»œçš„å…ˆè¿›åŠŸèƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªç„¶è¯­è¨€éŸ³é¢‘æ¨ç†ï¼ˆNLARï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°ä»»åŠ¡ï¼Œé€šè¿‡æ¯”è¾ƒå’Œæ€»ç»“ä¸¤ä¸ªéŸ³é¢‘ç‰‡æ®µè¿›è¡Œåˆ†æã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨APTå¢å¼ºçš„LLMsï¼ˆå³APT-LLMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šä¸ä¸“å®¶æ¨¡å‹ï¼ˆå³é’ˆå¯¹ç›®æ ‡æ•°æ®é›†è®­ç»ƒçš„ç½‘ç»œï¼‰ç›¸æ¯”å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†APTåœ¨å°†å†»ç»“çš„VLMsæ‰©å±•åˆ°éŸ³é¢‘é¢†åŸŸçš„èƒ½åŠ›ï¼Œä¸”åœ¨éŸ³é¢‘è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹æƒé‡å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/JinhuaLiang/APT%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/JinhuaLiang/APTä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.00249v2">PDF</a> Published at IEEE Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ä»‹ç»äº†å£°æ³¢æç¤ºè°ƒæ•´ï¼ˆAPTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ‰©å±•åˆ°éŸ³é¢‘é¢†åŸŸçš„æ–°é€‚é…å™¨ã€‚é€šè¿‡æ³¨å…¥éŸ³é¢‘åµŒå…¥åˆ°LLMsçš„è¾“å…¥ä¸­ï¼ŒAPTå®ç°äº†è½¯æç¤ºç”Ÿæˆï¼Œå¹¶åº”ç”¨æŒ‡ä»¤æ„ŸçŸ¥éŸ³é¢‘å¯¹é½å™¨æ¥ç”Ÿæˆè¿™äº›æç¤ºã€‚APTé‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥æ¥ç¼“è§£éŸ³é¢‘é¢†åŸŸçš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¹¶æå‡ºä½¿ç”¨äº¤ç»‡çš„éŸ³é¢‘æ–‡æœ¬åµŒå…¥ä½œä¸ºè¾“å…¥åºåˆ—æ¥æ”¹å–„éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è‡ªç„¶è¯­è¨€éŸ³é¢‘æ¨ç†ï¼ˆNLARï¼‰æ–°ä»»åŠ¡æ¥è¿›ä¸€æ­¥è¯„ä¼°éŸ³é¢‘ç½‘ç»œçš„é«˜çº§èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒAPTå¢å¼ºçš„LLMsåœ¨å„é¡¹ä»»åŠ¡ä¸­å–å¾—äº†ä¸ä¸“å®¶æ¨¡å‹ç›¸å½“çš„ç»“æœã€‚æœ€åï¼Œæ¼”ç¤ºäº†APTå°†å†»ç»“çš„VLMsæ‰©å±•åˆ°éŸ³é¢‘é¢†åŸŸçš„èƒ½åŠ›ï¼Œåœ¨è§†å¬é—®ç­”ä»»åŠ¡ä¸­å–å¾—äº†æœ‰å¸Œæœ›çš„æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¬è§‰ç³»ç»Ÿåœ¨äººç±»æ•´ä½“æ„ŸçŸ¥ä½“éªŒä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œç›®å‰å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘é¢†åŸŸçš„é€šç”¨æ€§æœ‰é™ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„é€‚é…å™¨â€”â€”å£°æ³¢æç¤ºè°ƒæ•´ï¼ˆAPTï¼‰ï¼Œå®ƒèƒ½å°†å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°éŸ³é¢‘é¢†åŸŸã€‚</li>
<li>APTé€šè¿‡æ³¨å…¥éŸ³é¢‘åµŒå…¥åˆ°è¯­è¨€æ¨¡å‹çš„è¾“å…¥ä¸­å®ç°è½¯æç¤ºç”Ÿæˆï¼Œå¹¶åº”ç”¨æŒ‡ä»¤æ„ŸçŸ¥éŸ³é¢‘å¯¹é½å™¨æ¥å¤„ç†è¾“å…¥æ–‡æœ¬å’Œå£°éŸ³ã€‚</li>
<li>é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥åº”å¯¹éŸ³é¢‘é¢†åŸŸçš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæå‡ºäº¤ç»‡çš„éŸ³é¢‘æ–‡æœ¬åµŒå…¥æ¥æ”¹å–„éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚</li>
<li>å¼•å…¥äº†è‡ªç„¶è¯­è¨€éŸ³é¢‘æ¨ç†ï¼ˆNLARï¼‰æ–°ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°éŸ³é¢‘ç½‘ç»œçš„é«˜çº§èƒ½åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒAPTå¢å¼ºçš„LLMsåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä¸ä¸“å®¶æ¨¡å‹ç›¸å½“ã€‚</li>
<li>APTèƒ½å°†å†»ç»“çš„VLMsæ‰©å±•åˆ°éŸ³é¢‘é¢†åŸŸï¼Œåœ¨è§†å¬é—®ç­”ä»»åŠ¡ä¸­å–å¾—æœ‰å¸Œæœ›çš„æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.00249">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f53a23a6bbb830ac0fb343625d0dce65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b23d7e308e32857be3cfb056307753e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b68a0ad1e11558c7829f31710e29db0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03b6f1074662e89de283913cd6a2666f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31d6598c7b01f58d71f432dcd8a9931a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-20/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-20/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2bf35c9798792592db76e1d75668c362.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-20  3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from   Cortical Surfaces
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-509a10fdf6c6b07c4d0432e6ded7d510.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-20  Magma A Foundation Model for Multimodal AI Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
