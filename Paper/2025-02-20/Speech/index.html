<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-20  A Dual-Stage Time-Context Network for Speech-Based Alzheimer&#39;s Disease   Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a60bf0526ec443753a8e23ebf50b54ec.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    6.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    27 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-20-æ›´æ–°"><a href="#2025-02-20-æ›´æ–°" class="headerlink" title="2025-02-20 æ›´æ–°"></a>2025-02-20 æ›´æ–°</h1><h2 id="A-Dual-Stage-Time-Context-Network-for-Speech-Based-Alzheimerâ€™s-Disease-Detection"><a href="#A-Dual-Stage-Time-Context-Network-for-Speech-Based-Alzheimerâ€™s-Disease-Detection" class="headerlink" title="A Dual-Stage Time-Context Network for Speech-Based Alzheimerâ€™s Disease   Detection"></a>A Dual-Stage Time-Context Network for Speech-Based Alzheimerâ€™s Disease   Detection</h2><p><strong>Authors:Yifan Gao, Long Guo, Hong Liu</strong></p>
<p>Alzheimerâ€™s disease (AD) is a progressive neurodegenerative disorder that leads to irreversible cognitive decline in memory and communication. Early detection of AD through speech analysis is crucial for delaying disease progression. However, existing methods mainly use pre-trained acoustic models for feature extraction but have limited ability to model both local and global patterns in long-duration speech. In this letter, we introduce a Dual-Stage Time-Context Network (DSTC-Net) for speech-based AD detection, integrating local acoustic features with global conversational context in long-duration recordings.We first partition each long-duration recording into fixed-length segments to reduce computational overhead and preserve local temporal details.Next, we feed these segments into an Intra-Segment Temporal Attention (ISTA) module, where a bidirectional Long Short-Term Memory (BiLSTM) network with frame-level attention extracts enhanced local features.Subsequently, a Cross-Segment Context Attention (CSCA) module applies convolution-based context modeling and adaptive attention to unify global patterns across all segments.Extensive experiments on the ADReSSo dataset show that our DSTC-Net outperforms state-of-the-art models, reaching 83.10% accuracy and 83.15% F1. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ˜¯ä¸€ç§è¿›è¡Œæ€§ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œä¼šå¯¼è‡´è®°å¿†å’Œæ²Ÿé€šèƒ½åŠ›çš„ä¸å¯é€†è®¤çŸ¥è¡°é€€ã€‚é€šè¿‡è¯­éŸ³åˆ†æè¿›è¡Œæ—©æœŸADæ£€æµ‹å¯¹äºå»¶ç¼“ç–¾ç—…è¿›å±•è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨é¢„è®­ç»ƒçš„å£°å­¦æ¨¡å‹è¿›è¡Œç‰¹å¾æå–ï¼Œä½†åœ¨é•¿æ—¶è¯­éŸ³ä¸­åŒæ—¶å»ºæ¨¡å±€éƒ¨å’Œå…¨å±€æ¨¡å¼çš„èƒ½åŠ›æœ‰é™ã€‚åœ¨è¿™å°ä¿¡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç”¨äºåŸºäºè¯­éŸ³çš„ADæ£€æµ‹çš„Dual-Stage Time-Contextç½‘ç»œï¼ˆDSTC-Netï¼‰ï¼Œå®ƒå°†å±€éƒ¨å£°å­¦ç‰¹å¾ä¸é•¿æ—¶å½•éŸ³ä¸­çš„å…¨å±€å¯¹è¯ä¸Šä¸‹æ–‡ç›¸ç»“åˆã€‚æˆ‘ä»¬é¦–å…ˆä¼šå°†æ¯ä¸ªé•¿æ—¶å½•éŸ³åˆ†æˆå›ºå®šé•¿åº¦çš„ç‰‡æ®µï¼Œä»¥å‡å°‘è®¡ç®—å¼€é”€å¹¶ä¿ç•™å±€éƒ¨æ—¶é—´ç»†èŠ‚ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™äº›ç‰‡æ®µè¾“å…¥åˆ°Intra-Segment Temporal Attentionï¼ˆISTAï¼‰æ¨¡å—ä¸­ï¼Œå…¶ä¸­ä½¿ç”¨å¸¦æœ‰å¸§çº§æ³¨æ„åŠ›çš„åŒå‘é•¿çŸ­æ—¶è®°å¿†ï¼ˆBiLSTMï¼‰ç½‘ç»œæå–å¢å¼ºçš„å±€éƒ¨ç‰¹å¾ã€‚éšåï¼ŒCross-Segment Context Attentionï¼ˆCSCAï¼‰æ¨¡å—é‡‡ç”¨åŸºäºå·ç§¯çš„ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œè‡ªé€‚åº”æ³¨æ„åŠ›ï¼Œä»¥ç»Ÿä¸€æ‰€æœ‰ç‰‡æ®µä¸­çš„å…¨å±€æ¨¡å¼ã€‚åœ¨ADReSSoæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DSTC-Netä¼˜äºæœ€æ–°æ¨¡å‹ï¼Œè¾¾åˆ°83.10%çš„å‡†ç¡®ç‡å’Œ83.15%çš„F1åˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13064v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¼•å…¥åŒé˜¶æ®µæ—¶é—´ä¸Šä¸‹æ–‡ç½‘ç»œï¼ˆDSTC-Netï¼‰è¿›è¡ŒåŸºäºè¯­éŸ³çš„é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ£€æµ‹ï¼Œç»“åˆé•¿æ—¶å½•éŸ³ä¸­çš„å±€éƒ¨å£°å­¦ç‰¹å¾å’Œå…¨å±€å¯¹è¯ä¸Šä¸‹æ–‡ã€‚é€šè¿‡åˆ†æ®µå½•éŸ³å¹¶åº”ç”¨åŒå‘LSTMç½‘ç»œå’Œå¸§çº§æ³¨æ„åŠ›æå–å¢å¼ºå±€éƒ¨ç‰¹å¾ï¼Œå†é€šè¿‡å·ç§¯ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œè‡ªé€‚åº”æ³¨æ„åŠ›ç»Ÿä¸€å…¨å±€æ¨¡å¼ã€‚åœ¨ADReSSoæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥ç½‘ç»œè¾¾åˆ°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§å’ŒF1åˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ˜¯ä¸€ç§å¯¼è‡´ä¸å¯é€†è®°å¿†å’Œæ²Ÿé€šè®¤çŸ¥è¡°é€€çš„æ¸è¿›æ€§ç¥ç»é€€è¡Œæ€§ç–¾ç—…ã€‚</li>
<li>æ—©æœŸé€šè¿‡è¯­éŸ³åˆ†ææ£€æµ‹ADå¯¹äºå»¶ç¼“ç–¾ç—…è¿›å±•è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä½¿ç”¨é¢„è®­ç»ƒçš„å£°å­¦æ¨¡å‹è¿›è¡Œç‰¹å¾æå–ï¼Œä½†éš¾ä»¥åŒæ—¶å¤„ç†å±€éƒ¨å’Œå…¨å±€æ¨¡å¼çš„é•¿æ—¶è¯­éŸ³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºDual-Stage Time-Context Networkï¼ˆDSTC-Netï¼‰çš„æ¨¡å‹ï¼Œç”¨äºåŸºäºè¯­éŸ³çš„ADæ£€æµ‹ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨åˆ†æ®µæ–¹æ³•å¤„ç†é•¿æ—¶å½•éŸ³ï¼Œæ—¨åœ¨æå–å±€éƒ¨å£°å­¦ç‰¹å¾å’Œå…¨å±€å¯¹è¯ä¸Šä¸‹æ–‡ã€‚</li>
<li>é€šè¿‡åŒå‘LSTMç½‘ç»œå’Œå¸§çº§æ³¨æ„åŠ›å¢å¼ºå±€éƒ¨ç‰¹å¾æå–ï¼Œå¹¶é€šè¿‡å·ç§¯ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œè‡ªé€‚åº”æ³¨æ„åŠ›ç»Ÿä¸€å…¨å±€æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-297e8cfe8c47fdb60ef00db5db975671.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d48911d1248c6f688d40351dfd12d938.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8538f9b15e3ad980388c7ad16065093e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a60bf0526ec443753a8e23ebf50b54ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac21c00789032990b1551a9526573eb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa0cb9beacbcf1f121425dd0790835dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f12879c48e33a54b3f81e89df5cb4a1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Step-Audio-Unified-Understanding-and-Generation-in-Intelligent-Speech-Interaction"><a href="#Step-Audio-Unified-Understanding-and-Generation-in-Intelligent-Speech-Interaction" class="headerlink" title="Step-Audio: Unified Understanding and Generation in Intelligent Speech   Interaction"></a>Step-Audio: Unified Understanding and Generation in Intelligent Speech   Interaction</h2><p><strong>Authors:Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Hongyu Zhou, Jianjian Sun, Brian Li, Chengting Feng, Changyi Wan, Hanpeng Hu, Jianchang Wu, Jiangjie Zhen, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Hongyuan Wang, Kang An, Wei Ji, Wen Li, Xuan Wen, Xiangwen Kong, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Junjing Guo, Jiashuai Liu, Jiahong Liu, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Liang Zhao, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingliang Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Ran Sun, Shuai Shuai, Shaoliang Pang, Shiliang Yang, Shuli Gao, Shanshan Yuan, Siqi Liu, Shihong Deng, Shilei Jiang, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wuxun Xie, Weipeng Ming, Wenqing He, Wen Sun, Xin Han, Xin Huang, Xiaomin Deng, Xiaojia Liu, Xin Wu, Xu Zhao, Yanan Wei, Yanbo Yu, Yang Cao, Yangguang Li, Yangzhen Ma, Yanming Xu, Yaoyu Wang, Yaqiang Shi, Yilei Wang, Yizhuang Zhou, Yinmin Zhong, Yang Zhang, Yaoben Wei, Yu Luo, Yuanwei Lu, Yuhe Yin, Yuchu Luo, Yuanhao Ding, Yuting Yan, Yaqi Dai, Yuxiang Yang, Zhe Xie, Zheng Ge, Zheng Sun, Zhewei Huang, Zhichao Chang, Zhisheng Guan, Zidong Yang, Zili Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Shuchang Zhou, Xiangyu Zhang, Xinhao Zhang, Yibo Zhu</strong></p>
<p>Real-time speech interaction, serving as a fundamental interface for human-machine collaboration, holds immense potential. However, current open-source models face limitations such as high costs in voice data collection, weakness in dynamic control, and limited intelligence. To address these challenges, this paper introduces Step-Audio, the first production-ready open-source solution. Key contributions include: 1) a 130B-parameter unified speech-text multi-modal model that achieves unified understanding and generation, with the Step-Audio-Chat version open-sourced; 2) a generative speech data engine that establishes an affordable voice cloning framework and produces the open-sourced lightweight Step-Audio-TTS-3B model through distillation; 3) an instruction-driven fine control system enabling dynamic adjustments across dialects, emotions, singing, and RAP; 4) an enhanced cognitive architecture augmented with tool calling and role-playing abilities to manage complex tasks effectively. Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following. On open-source benchmarks like LLaMA Question, shows 9.3% average performance improvement, demonstrating our commitment to advancing the development of open-source multi-modal language technologies. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/stepfun-ai/Step-Audio">https://github.com/stepfun-ai/Step-Audio</a>. </p>
<blockquote>
<p>å®æ—¶è¯­éŸ³äº¤äº’ä½œä¸ºäººæœºäº¤äº’çš„åŸºæœ¬æ¥å£ï¼Œå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›®å‰çš„å¼€æºæ¨¡å‹é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚è¯­éŸ³æ•°æ®é‡‡é›†æˆæœ¬é«˜æ˜‚ã€åŠ¨æ€æ§åˆ¶è–„å¼±ä»¥åŠæ™ºèƒ½æœ‰é™ç­‰ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä»‹ç»äº†Step-Audioï¼Œè¿™æ˜¯é¦–ä¸ªç”Ÿäº§å°±ç»ªçš„å¼€æºè§£å†³æ–¹æ¡ˆã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š1ï¼‰ä¸€ä¸ªæ‹¥æœ‰130Bå‚æ•°çš„ç»Ÿä¸€è¯­éŸ³æ–‡æœ¬å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ç°äº†ç»Ÿä¸€çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå…¶ä¸­Step-Audio-Chatç‰ˆæœ¬å·²å¼€æºï¼›2ï¼‰ä¸€ä¸ªç”Ÿæˆå¼è¯­éŸ³æ•°æ®å¼•æ“ï¼Œå»ºç«‹äº†ä¸€ä¸ªç»æµå®æƒ çš„è¯­éŸ³å…‹éš†æ¡†æ¶ï¼Œå¹¶é€šè¿‡è’¸é¦æŠ€æœ¯æ¨å‡ºäº†å¼€æºçš„è½»é‡çº§Step-Audio-TTS-3Bæ¨¡å‹ï¼›3ï¼‰ä¸€ä¸ªæŒ‡ä»¤é©±åŠ¨çš„ç²¾ç»†æ§åˆ¶ç³»ç»Ÿï¼Œèƒ½å¤Ÿå®ç°ä¸åŒæ–¹è¨€ã€æƒ…æ„Ÿã€æ­Œå”±å’ŒRAPçš„åŠ¨æ€è°ƒæ•´ï¼›4ï¼‰ä¸€ä¸ªå¢å¼ºçš„è®¤çŸ¥æ¶æ„ï¼Œé€šè¿‡å·¥å…·è°ƒç”¨å’Œè§’è‰²æ‰®æ¼”èƒ½åŠ›ï¼Œæœ‰æ•ˆç®¡ç†å¤æ‚ä»»åŠ¡ã€‚åŸºäºæˆ‘ä»¬æ–°çš„StepEval-Audio-360è¯„ä¼°åŸºå‡†ï¼ŒStep-Audioåœ¨äººå·¥è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢ã€‚åœ¨LLaMA Questionç­‰å¼€æºåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡æ€§èƒ½æé«˜äº†9.3%ï¼Œè¿™ä½“ç°äº†æˆ‘ä»¬æ¨è¿›å¼€æºå¤šæ¨¡æ€è¯­è¨€æŠ€æœ¯å‘å±•çš„æ‰¿è¯ºã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/stepfun-ai/Step-Audio%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/stepfun-ai/Step-Audioè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.11946v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹åä¸ºStep-Audioçš„åˆ›æ–°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä¸ºå®æ—¶è¯­éŸ³äº¤äº’æä¾›äº†æ–°çš„ç”Ÿäº§å°±ç»ªå¼€æºè§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å¼•å…¥ç»Ÿä¸€è¯­éŸ³æ–‡æœ¬å¤šæ¨¡æ€æ¨¡å‹ã€ç”Ÿæˆå¼è¯­éŸ³æ•°æ®å¼•æ“ã€æŒ‡ä»¤é©±åŠ¨ç²¾ç»†æ§åˆ¶ç³»ç»Ÿä»¥åŠå¢å¼ºè®¤çŸ¥æ¶æ„ï¼ŒStep-Audioè§£å†³äº†ç°æœ‰å¼€æºæ¨¡å‹åœ¨è¯­éŸ³æ•°æ®æ”¶é›†æˆæœ¬ã€åŠ¨æ€æ§åˆ¶èƒ½åŠ›ä»¥åŠæ™ºèƒ½ç¨‹åº¦ä¸Šçš„å±€é™æ€§ã€‚å…¶æ€§èƒ½åœ¨StepEval-Audio-360è¯„ä¼°åŸºå‡†ä¸Šè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨æŒ‡ä»¤éµå¾ªæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œä¸å¼€æºåŸºå‡†ç›¸æ¯”ï¼Œå…¶åœ¨LLaMA Questionä¸Šå¹³å‡æ€§èƒ½æå‡9.3%ï¼Œå±•ç¤ºäº†æ¨è¿›å¼€æºå¤šæ¨¡æ€è¯­è¨€æŠ€æœ¯å‘å±•çš„æ‰¿è¯ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Step-Audioæ˜¯ä¸€é¡¹åˆ›æ–°çš„ç”Ÿäº§å°±ç»ªå¼€æºè§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨æ”¹è¿›å®æ—¶è¯­éŸ³äº¤äº’æŠ€æœ¯ã€‚</li>
<li>Step-Audioå¼•å…¥äº†å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ç°äº†è¯­éŸ³å’Œæ–‡æœ¬çš„è”åˆç†è§£å’Œç”Ÿæˆã€‚</li>
<li>Step-Audioé€šè¿‡ç”Ÿæˆå¼è¯­éŸ³æ•°æ®å¼•æ“é™ä½äº†è¯­éŸ³å…‹éš†çš„æˆæœ¬ï¼Œå¹¶å¼€å‘äº†è½»é‡çº§çš„è¯­éŸ³åˆæˆæ¨¡å‹ã€‚</li>
<li>æŒ‡ä»¤é©±åŠ¨ç²¾ç»†æ§åˆ¶ç³»ç»Ÿä½¿å¾—è¯­éŸ³äº¤äº’æ›´åŠ åŠ¨æ€ï¼Œå¯è°ƒæ•´æ–¹è¨€ã€æƒ…æ„Ÿã€æ­Œå”±å’ŒRAPç­‰å…ƒç´ ã€‚</li>
<li>Step-Audioçš„è®¤çŸ¥æ¶æ„å¾—åˆ°äº†å¢å¼ºï¼Œå…·å¤‡å·¥å…·è°ƒç”¨å’Œè§’è‰²æ‰®æ¼”èƒ½åŠ›ï¼Œä»¥æ›´æœ‰æ•ˆåœ°ç®¡ç†å¤æ‚ä»»åŠ¡ã€‚</li>
<li>Step-Audioçš„æ€§èƒ½åœ¨StepEval-Audio-360è¯„ä¼°åŸºå‡†ä¸Šè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.11946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6ebcf57724b98f534bfa1a29b8752d4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31d6d360dc1ec5145451ae0ac4c21b48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e801713a5fa5f6ee5ecec11004556e3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VarGes-Improving-Variation-in-Co-Speech-3D-Gesture-Generation-via-StyleCLIPS"><a href="#VarGes-Improving-Variation-in-Co-Speech-3D-Gesture-Generation-via-StyleCLIPS" class="headerlink" title="VarGes: Improving Variation in Co-Speech 3D Gesture Generation via   StyleCLIPS"></a>VarGes: Improving Variation in Co-Speech 3D Gesture Generation via   StyleCLIPS</h2><p><strong>Authors:Ming Meng, Ke Mu, Yonggui Zhu, Zhe Zhu, Haoyu Sun, Heyang Yan, Zhaoxin Fan</strong></p>
<p>Generating expressive and diverse human gestures from audio is crucial in fields like human-computer interaction, virtual reality, and animation. Though existing methods have achieved remarkable performance, they often exhibit limitations due to constrained dataset diversity and the restricted amount of information derived from audio inputs. To address these challenges, we present VarGes, a novel variation-driven framework designed to enhance co-speech gesture generation by integrating visual stylistic cues while maintaining naturalness. Our approach begins with the Variation-Enhanced Feature Extraction (VEFE) module, which seamlessly incorporates \textcolor{blue}{style-reference} video data into a 3D human pose estimation network to extract StyleCLIPS, thereby enriching the input with stylistic information. Subsequently, we employ the Variation-Compensation Style Encoder (VCSE), a transformer-style encoder equipped with an additive attention mechanism pooling layer, to robustly encode diverse StyleCLIPS representations and effectively manage stylistic variations. Finally, the Variation-Driven Gesture Predictor (VDGP) module fuses MFCC audio features with StyleCLIPS encodings via cross-attention, injecting this fused data into a cross-conditional autoregressive model to modulate 3D human gesture generation based on audio input and stylistic clues. The efficacy of our approach is validated on benchmark datasets, where it outperforms existing methods in terms of gesture diversity and naturalness. The code and video results will be made publicly available upon acceptance:<a target="_blank" rel="noopener" href="https://github.com/mookerr/VarGES/">https://github.com/mookerr/VarGES/</a> . </p>
<blockquote>
<p>éŸ³é¢‘ç”Ÿæˆçš„è¡¨æƒ…ä¸°å¯Œä¸”å¤šæ ·åŒ–çš„æ‰‹åŠ¿åœ¨äººç±»ä¸è®¡ç®—æœºäº¤äº’ã€è™šæ‹Ÿç°å®å’ŒåŠ¨ç”»ç­‰é¢†åŸŸè‡³å…³é‡è¦ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œä½†ç”±äºæ•°æ®é›†å¤šæ ·æ€§çš„é™åˆ¶ä»¥åŠä»éŸ³é¢‘è¾“å…¥ä¸­è·å¾—çš„ä¿¡æ¯é‡æœ‰é™ï¼Œå®ƒä»¬é€šå¸¸è¡¨ç°å‡ºå±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†VarGesï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å˜åŒ–é©±åŠ¨æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆè§†è§‰é£æ ¼çº¿ç´¢æ¥å¢å¼ºä¸è¯­éŸ³åŒæ­¥çš„æ‰‹åŠ¿ç”Ÿæˆï¼ŒåŒæ—¶ä¿æŒè‡ªç„¶æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å§‹äºå¢å¼ºç‰¹å¾æå–æ¨¡å—ï¼ˆVEFEï¼‰ï¼Œè¯¥æ¨¡å—æ— ç¼åœ°å°†é£æ ¼å‚è€ƒè§†é¢‘æ•°æ®èå…¥3Däººä½“å§¿æ€ä¼°è®¡ç½‘ç»œï¼Œä»è€Œæå–StyleCLIPSï¼Œä½¿è¾“å…¥ä¿¡æ¯ä¸°å¯Œå¹¶å¸¦æœ‰é£æ ¼ä¿¡æ¯ã€‚éšåï¼Œæˆ‘ä»¬é‡‡ç”¨äº†é…å¤‡é™„åŠ æ³¨æ„åŠ›æœºåˆ¶æ± åŒ–å±‚çš„å˜ä½“è¡¥å¿é£æ ¼ç¼–ç å™¨ï¼ˆVCSEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå˜å‹å™¨é£æ ¼çš„ç¼–ç å™¨ï¼Œèƒ½å¤Ÿç¨³å¥åœ°ç¼–ç å„ç§StyleCLIPSè¡¨ç¤ºå¹¶æœ‰æ•ˆåœ°ç®¡ç†é£æ ¼å˜åŒ–ã€‚æœ€åï¼Œå˜åŒ–é©±åŠ¨æ‰‹åŠ¿é¢„æµ‹å™¨ï¼ˆVDGPï¼‰æ¨¡å—é€šè¿‡è·¨æ³¨æ„åŠ›å°†MFCCéŸ³é¢‘ç‰¹å¾ä¸StyleCLIPSç¼–ç èåˆï¼Œå°†æ­¤èåˆæ•°æ®æ³¨å…¥è·¨æ¡ä»¶è‡ªå›å½’æ¨¡å‹ï¼Œæ ¹æ®éŸ³é¢‘è¾“å…¥å’Œé£æ ¼çº¿ç´¢è°ƒåˆ¶3Dæ‰‹åŠ¿ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å·²åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œåœ¨æ‰‹åŠ¿å¤šæ ·æ€§å’Œè‡ªç„¶æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å’Œè§†é¢‘ç»“æœå°†åœ¨æ¥å—åå…¬å¼€ï¼š<a target="_blank" rel="noopener" href="https://github.com/mookerr/VarGES/%E3%80%82">https://github.com/mookerr/VarGES/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10729v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVarGesçš„æ–°é¢–å˜åŒ–é©±åŠ¨æ¡†æ¶ï¼Œç”¨äºé€šè¿‡æ•´åˆè§†è§‰é£æ ¼çº¿ç´¢å¢å¼ºéŸ³é¢‘é©±åŠ¨çš„æ‰‹åŠ¿ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å˜åŒ–å¢å¼ºç‰¹å¾æå–æ¨¡å—ï¼ˆVEFEï¼‰ã€å˜åŒ–è¡¥å¿é£æ ¼ç¼–ç å™¨ï¼ˆVCSEï¼‰å’Œå˜åŒ–é©±åŠ¨æ‰‹åŠ¿é¢„æµ‹å™¨ï¼ˆVDGPï¼‰ç­‰æŠ€æœ¯ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æ•°æ®é›†å¤šæ ·æ€§å’Œä»éŸ³é¢‘è¾“å…¥ä¸­è·å–ä¿¡æ¯å—é™æ–¹é¢çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æ‰‹åŠ¿å¤šæ ·æ€§å’Œè‡ªç„¶åº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VarGesæ˜¯ä¸€ä¸ªå˜åŒ–é©±åŠ¨æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆè§†è§‰é£æ ¼çº¿ç´¢å¢å¼ºéŸ³é¢‘é©±åŠ¨çš„æ‰‹åŠ¿ç”Ÿæˆã€‚</li>
<li>ä½¿ç”¨å˜åŒ–å¢å¼ºç‰¹å¾æå–æ¨¡å—ï¼ˆVEFEï¼‰èå…¥é£æ ¼å‚è€ƒè§†é¢‘æ•°æ®ï¼Œä¸°å¯Œè¾“å…¥çš„é£æ ¼ä¿¡æ¯ã€‚</li>
<li>å˜åŒ–è¡¥å¿é£æ ¼ç¼–ç å™¨ï¼ˆVCSEï¼‰èƒ½æœ‰æ•ˆç¼–ç å¤šæ ·åŒ–çš„StyleCLIPSè¡¨ç¤ºå¹¶ç®¡ç†é£æ ¼å˜åŒ–ã€‚</li>
<li>å˜åŒ–é©±åŠ¨æ‰‹åŠ¿é¢„æµ‹å™¨ï¼ˆVDGPï¼‰ç»“åˆéŸ³é¢‘ç‰¹å¾å’ŒStyleCLIPSç¼–ç ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æ³¨å…¥æ•°æ®ï¼Œæ ¹æ®éŸ³é¢‘è¾“å…¥å’Œé£æ ¼çº¿ç´¢è°ƒèŠ‚3Dæ‰‹åŠ¿ç”Ÿæˆã€‚</li>
<li>VarGesæ¡†æ¶åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„æ‰‹åŠ¿å¤šæ ·æ€§å’Œè‡ªç„¶åº¦ã€‚</li>
<li>å…¬å¼€å¯ç”¨ä»£ç å’Œè§†é¢‘ç»“æœï¼š<a target="_blank" rel="noopener" href="https://github.com/mookerr/VarGES/%E3%80%82">https://github.com/mookerr/VarGES/ã€‚</a></li>
<li>è¯¥æ¡†æ¶åœ¨äººæœºäº¤äº’ã€è™šæ‹Ÿç°å®å’ŒåŠ¨ç”»ç­‰é¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e7c416905094133f1a4ff604ea4d0585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f16ebf26b61f38c7408ef84b5568d926.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09315c1371a85150405d1392b09b6745.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49955f5c218441c72beda1f64c75d3b3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VoxEval-Benchmarking-the-Knowledge-Understanding-Capabilities-of-End-to-End-Spoken-Language-Models"><a href="#VoxEval-Benchmarking-the-Knowledge-Understanding-Capabilities-of-End-to-End-Spoken-Language-Models" class="headerlink" title="VoxEval: Benchmarking the Knowledge Understanding Capabilities of   End-to-End Spoken Language Models"></a>VoxEval: Benchmarking the Knowledge Understanding Capabilities of   End-to-End Spoken Language Models</h2><p><strong>Authors:Wenqian Cui, Xiaoqi Jiao, Ziqiao Meng, Irwin King</strong></p>
<p>With the rising need for speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. While these models require comprehensive world knowledge for meaningful and reliable human interactions, existing question-answering (QA) benchmarks fall short in evaluating SLMsâ€™ knowledge understanding due to their inability to support end-to-end speech evaluation and account for varied input audio conditions. To address these limitations, we present VoxEval, a novel SpeechQA benchmark that assesses SLMsâ€™ knowledge understanding through pure speech interactions. Our benchmark 1) uniquely maintains speech format for both inputs and outputs, 2) evaluates model robustness across diverse input audio conditions, and 3) pioneers the assessment of complex tasks like mathematical reasoning in spoken format. Systematic evaluation demonstrates that VoxEval presents significant challenges to current SLMs, revealing their sensitivity to varying audio conditions and highlighting the need to enhance reasoning capabilities in future development. We hope this benchmark could guide the advancement of more sophisticated and reliable SLMs.\footnote{VoxEval dataset is available at: <a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/VoxEval">https://github.com/dreamtheater123/VoxEval</a> </p>
<blockquote>
<p>éšç€åŸºäºè¯­éŸ³çš„äº¤äº’æ¨¡å‹çš„éœ€ä¸æ–­æ±‚å¢é•¿ï¼Œç«¯åˆ°ç«¯çš„å£è¯­æ¨¡å‹ï¼ˆSLMï¼‰å·²æˆä¸ºä¸€ç§å‰æ™¯å¹¿é˜”è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶è¿™äº›æ¨¡å‹éœ€è¦å…¨é¢çš„ä¸–ç•ŒçŸ¥è¯†æ¥è¿›è¡Œæœ‰æ„ä¹‰å’Œå¯é çš„äººç±»äº¤äº’ï¼Œä½†ç°æœ‰çš„é—®ç­”ï¼ˆQAï¼‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°SLMçš„çŸ¥è¯†ç†è§£æ–¹é¢å´æ˜¾å¾—ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•æ”¯æŒç«¯åˆ°ç«¯çš„è¯­éŸ³è¯„ä¼°ï¼Œä¹Ÿæ— æ³•è€ƒè™‘åˆ°å„ç§è¾“å…¥éŸ³é¢‘æ¡ä»¶ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VoxEvalï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„SpeechQAåŸºå‡†æµ‹è¯•ï¼Œå®ƒé€šè¿‡çº¯è¯­éŸ³äº¤äº’æ¥è¯„ä¼°SLMçš„çŸ¥è¯†ç†è§£ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•1ï¼‰ç‹¬ç‰¹åœ°ä¿æŒè¯­éŸ³è¾“å…¥è¾“å‡ºæ ¼å¼ï¼Œ2ï¼‰è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè¾“å…¥éŸ³é¢‘æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼Œ3ï¼‰ç‡å…ˆè¯„ä¼°å£è¯­å½¢å¼çš„å¤æ‚ä»»åŠ¡ï¼Œå¦‚æ•°å­¦æ¨ç†ã€‚ç³»ç»Ÿè¯„ä¼°è¡¨æ˜ï¼ŒVoxEvalç»™å½“å‰çš„SLMå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œæ­ç¤ºäº†å®ƒä»¬å¯¹å„ç§éŸ³é¢‘æ¡ä»¶çš„æ•æ„Ÿæ€§ï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥å¼€å‘æ—¶éœ€è¦å¢å¼ºæ¨ç†èƒ½åŠ›çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªåŸºå‡†æµ‹è¯•èƒ½å¤Ÿå¼•å¯¼æ›´å…ˆè¿›ã€æ›´å¯é çš„SLMçš„å‘å±•ã€‚VoxEvalæ•°æ®é›†å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/VoxEval%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dreamtheater123/VoxEvalæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04962v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€åŸºäºè¯­éŸ³çš„äº¤äº’æ¨¡å‹éœ€æ±‚çš„å¢åŠ ï¼Œç«¯åˆ°ç«¯çš„å£è¯­æ¨¡å‹ï¼ˆSLMsï¼‰å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç°æœ‰é—®ç­”ï¼ˆQAï¼‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°SLMçš„çŸ¥è¯†ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæ— æ³•æ”¯æŒç«¯åˆ°ç«¯çš„è¯­éŸ³è¯„ä¼°å¹¶è€ƒè™‘å„ç§è¾“å…¥éŸ³é¢‘æ¡ä»¶ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VoxEvalè¿™ä¸€æ–°é¢–çš„SpeechQAåŸºå‡†æµ‹è¯•ï¼Œå®ƒé€šè¿‡çº¯è¯­éŸ³äº¤äº’è¯„ä¼°SLMçš„çŸ¥è¯†ç†è§£ã€‚è¯¥åŸºå‡†æµ‹è¯•å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š1ï¼‰ä¿æŒè¯­éŸ³è¾“å…¥è¾“å‡ºæ ¼å¼çš„ç‹¬ç‰¹æ€§ï¼›2ï¼‰è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè¾“å…¥éŸ³é¢‘æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼›3ï¼‰ç‡å…ˆè¯„ä¼°å£è¯­å½¢å¼çš„å¤æ‚ä»»åŠ¡ï¼Œå¦‚æ•°å­¦æ¨ç†ç­‰ã€‚ç³»ç»Ÿè¯„ä¼°è¡¨æ˜ï¼ŒVoxEvalå¯¹å½“å‰SLMæå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œçªæ˜¾äº†å®ƒä»¬å¯¹å„ç§éŸ³é¢‘æ¡ä»¶çš„æ•æ„Ÿæ€§ï¼Œå¹¶å¼ºè°ƒæœªæ¥å‘å±•ä¸­å¢å¼ºæ¨ç†èƒ½åŠ›çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬æœŸæœ›æ­¤åŸºå‡†æµ‹è¯•èƒ½æ¨åŠ¨æ›´å…ˆè¿›ã€æ›´å¯é çš„SLMçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å£è¯­æ¨¡å‹ï¼ˆSLMsï¼‰åœ¨åŸºäºè¯­éŸ³çš„äº¤äº’æ¨¡å‹ä¸­è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰é—®ç­”åŸºå‡†æµ‹è¯•æ— æ³•å……åˆ†è¯„ä¼°SLMçš„çŸ¥è¯†ç†è§£ï¼Œå°¤å…¶åœ¨ç«¯åˆ°ç«¯è¯­éŸ³è¯„ä¼°å’Œä¸åŒè¾“å…¥éŸ³é¢‘æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚</li>
<li>VoxEvalæ˜¯ä¸€ä¸ªæ–°é¢–çš„SpeechQAåŸºå‡†æµ‹è¯•ï¼Œä»¥çº¯è¯­éŸ³äº¤äº’æ–¹å¼è¯„ä¼°SLMçš„çŸ¥è¯†ç†è§£ã€‚</li>
<li>VoxEvalä¿æŒè¯­éŸ³è¾“å…¥è¾“å‡ºæ ¼å¼çš„ç‹¬ç‰¹æ€§ï¼Œå¹¶è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè¾“å…¥éŸ³é¢‘æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>VoxEvalç‡å…ˆè¯„ä¼°å£è¯­å½¢å¼çš„å¤æ‚ä»»åŠ¡ï¼Œå¦‚æ•°å­¦æ¨ç†ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°æ˜¾ç¤ºï¼ŒVoxEvalå¯¹å½“å‰SLMæå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œçªæ˜¾å…¶éŸ³é¢‘æ¡ä»¶æ•æ„Ÿæ€§å’Œæ¨ç†èƒ½åŠ›çš„ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f95e2458d9fd9423e6baa5cc22c182d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96dda8ab94914b874159e06f6a1a5c66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-176698df07e0774d4b6de7ffa7914b8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b367955e5c3b4c2f226a23879464f3cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c14f7cbb240dab030a64b5bb81320937.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a93be846849c4cdfe8a72b30fae5b70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a80ae975f17b30507b3529d6712c2e64.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ToxiLab-How-Well-Do-Open-Source-LLMs-Generate-Synthetic-Toxicity-Data"><a href="#ToxiLab-How-Well-Do-Open-Source-LLMs-Generate-Synthetic-Toxicity-Data" class="headerlink" title="ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?"></a>ToxiLab: How Well Do Open-Source LLMs Generate Synthetic Toxicity Data?</h2><p><strong>Authors:Zheng Hui, Zhaoxiao Guo, Hang Zhao, Juanyong Duan, Lin Ai, Yinheng Li, Julia Hirschberg, Congrui Huang</strong></p>
<p>Effective toxic content detection relies heavily on high-quality and diverse data, which serve as the foundation for robust content moderation models. Synthetic data has become a common approach for training models across various NLP tasks. However, its effectiveness remains uncertain for highly subjective tasks like hate speech detection, with previous research yielding mixed results. This study explores the potential of open-source LLMs for harmful data synthesis, utilizing controlled prompting and supervised fine-tuning techniques to enhance data quality and diversity. We systematically evaluated 6 open source LLMs on 5 datasets, assessing their ability to generate diverse, high-quality harmful data while minimizing hallucination and duplication. Our results show that Mistral consistently outperforms other open models, and supervised fine-tuning significantly enhances data reliability and diversity. We further analyze the trade-offs between prompt-based vs. fine-tuned toxic data synthesis, discuss real-world deployment challenges, and highlight ethical considerations. Our findings demonstrate that fine-tuned open source LLMs provide scalable and cost-effective solutions to augment toxic content detection datasets, paving the way for more accessible and transparent content moderation tools. </p>
<blockquote>
<p>æœ‰æ•ˆçš„æœ‰æ¯’å†…å®¹æ£€æµ‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æ•°æ®ï¼Œè¿™äº›æ•°æ®ä¸ºæ„å»ºç¨³å¥çš„å†…å®¹ç®¡ç†æ¨¡å‹æä¾›äº†åŸºç¡€ã€‚åˆæˆæ•°æ®å·²æˆä¸ºå„ç§NLPä»»åŠ¡ä¸­è®­ç»ƒæ¨¡å‹çš„å¸¸è§æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¯¹äºåƒä»‡æ¨è¨€è®ºæ£€æµ‹è¿™æ ·çš„é«˜åº¦ä¸»è§‚ä»»åŠ¡ï¼Œå…¶æœ‰æ•ˆæ€§ä»ä¸ç¡®å®šï¼Œä¹‹å‰çš„ç ”ç©¶ç»“æœå–œå¿§å‚åŠã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœ‰å®³æ•°æ®åˆæˆæ–¹é¢çš„æ½œåŠ›ï¼Œåˆ©ç”¨å—æ§æç¤ºå’Œç›‘ç£å¾®è°ƒæŠ€æœ¯æ¥æé«˜æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬åœ¨5ä¸ªæ•°æ®é›†ä¸Šç³»ç»Ÿåœ°è¯„ä¼°äº†6ä¸ªå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯„ä¼°å®ƒä»¬ç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„æœ‰å®³æ•°æ®çš„èƒ½åŠ›ï¼ŒåŒæ—¶å°½é‡å‡å°‘å¹»è§‰å’Œé‡å¤ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒMistralåœ¨å„æ–¹é¢è¡¨ç°å‡ä¼˜äºå…¶ä»–å¼€æºæ¨¡å‹ï¼Œç›‘ç£å¾®è°ƒæ˜¾è‘—æé«˜äº†æ•°æ®çš„å¯é æ€§å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†åŸºäºæç¤ºä¸å¾®è°ƒæœ‰æ¯’æ•°æ®åˆæˆçš„æƒè¡¡ï¼Œè®¨è®ºäº†ç°å®ä¸–ç•Œçš„éƒ¨ç½²æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†ä¼¦ç†è€ƒé‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒåçš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ä¸ºå¢å¼ºæœ‰æ¯’å†…å®¹æ£€æµ‹æ•°æ®é›†æä¾›äº†å¯æ‰©å±•å’Œå…·æœ‰æˆæœ¬æ•ˆç›Šçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæ›´ä¾¿æ·å’Œé€æ˜çš„å†…å®¹ç®¡ç†å·¥å…·çš„æ¨å¹¿å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15175v3">PDF</a> 14 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœ‰å®³æ•°æ®åˆæˆä¸­çš„æ½œåŠ›ï¼Œç ”ç©¶é€šè¿‡å—æ§æç¤ºå’Œç›‘ç£å¾®è°ƒæŠ€æœ¯æé«˜æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ã€‚ç³»ç»Ÿè¯„ä¼°äº†6ä¸ªå¼€æºLLMsåœ¨5ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºMistralè¡¨ç°æœ€ä½³ï¼Œç›‘ç£å¾®è°ƒå¯æ˜¾è‘—æé«˜æ•°æ®å¯é æ€§å’Œå¤šæ ·æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæœ‰æ¯’å†…å®¹æ£€æµ‹æ•°æ®é›†æä¾›äº†å¯æ‰©å±•å’Œç»æµçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ‰æ¯’å†…å®¹æ£€æµ‹ä¾èµ–äºé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æ•°æ®ï¼Œè¿™äº›æ•°æ®æ˜¯å»ºç«‹ç¨³å¥å†…å®¹ç®¡ç†æ¨¡å‹çš„åŸºç¡€ã€‚</li>
<li>åˆæˆæ•°æ®åœ¨å¤šç§NLPä»»åŠ¡ä¸­å¸¸è¢«ç”¨äºæ¨¡å‹è®­ç»ƒï¼Œä½†å¯¹äºé«˜åº¦ä¸»è§‚çš„ä»»åŠ¡å¦‚ä»‡æ¨è¨€è®ºæ£€æµ‹ï¼Œå…¶æ•ˆæœå°šä¸ç¡®å®šã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å…­ç§å¼€æºLLMsåœ¨ç”Ÿæˆå¤šæ ·ä¸”é«˜è´¨é‡çš„æœ‰å®³æ•°æ®æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶å‘ç°Mistralè¡¨ç°æœ€ä½³ã€‚</li>
<li>ç›‘ç£å¾®è°ƒæŠ€æœ¯æ˜¾è‘—æé«˜äº†æ•°æ®å¯é æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>æç¤ºï¼ˆprompt-basedï¼‰ä¸å¾®è°ƒï¼ˆfine-tunedï¼‰ä¹‹é—´çš„æƒè¡¡åœ¨æœ‰æ¯’æ•°æ®åˆæˆä¸­å­˜åœ¨ã€‚</li>
<li>ç ”ç©¶è®¨è®ºäº†å®é™…éƒ¨ç½²æŒ‘æˆ˜å’Œä¼¦ç†è€ƒé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-51f4eaeea1978e4535fb3aadb507bc6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29f3956b38f04e633b8ff6646584aa6b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d54a78ad900e6702b1afa6b8f97dce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5a9013b41d9626c28ec9853f0c9552e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c09b3750e6e9e8f3c889381487706b1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7dc5efa7d9df0d3ca2232a7f0afa7eb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="JOOCI-a-Framework-for-Learning-Comprehensive-Speech-Representations"><a href="#JOOCI-a-Framework-for-Learning-Comprehensive-Speech-Representations" class="headerlink" title="JOOCI: a Framework for Learning Comprehensive Speech Representations"></a>JOOCI: a Framework for Learning Comprehensive Speech Representations</h2><p><strong>Authors:Hemant Yadav, Rajiv Ratn Shah, Sunayana Sitaram</strong></p>
<p>Information in speech can be categorized into two groups: Content (what is being said, such as linguistics) and Other (how it is expressed such as information about speaker and paralinguistic features). Current self-supervised learning (SSL) methods are shown to divide the modelâ€™s representational-depth or layers in two, with earlier layers specializing in Other and later layers in Content related tasks. This layer-wise division is inherently sub-optimal, as neither information type can use all layers to build hierarchical representations. To address this, we propose JOOCI, a novel speech representation learning method that does not compromise on the representational-depth for either information type. JOOCI outperforms WavLM by 26.5%, and other models of similar size (100M parameters), when evaluated on two speaker recognition and two language tasks from the SUPERB benchmark, demonstrating its effectiveness in Jointly Optimizing Other and Content Information (JOOCI). </p>
<blockquote>
<p>è¯­éŸ³ä¸­çš„ä¿¡æ¯å¯ä»¥åˆ†ä¸ºä¸¤å¤§ç±»ï¼šå†…å®¹ï¼ˆæ‰€è¯´çš„å†…å®¹ï¼Œå¦‚è¯­è¨€å­¦ï¼‰å’Œå…¶ä»–ï¼ˆå¦‚ä½•è¡¨è¾¾ï¼Œå¦‚å…³äºè¯´è¯äººå’Œå‰¯è¯­è¨€ç‰¹å¾çš„ä¿¡æ¯ï¼‰ã€‚ç°æœ‰çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•è¢«è¯æ˜ä¼šå°†æ¨¡å‹çš„è¡¨ç¤ºæ·±åº¦æˆ–å±‚æ¬¡åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œè¾ƒæ—©çš„å±‚æ¬¡ä¸“æ³¨äºå…¶ä»–ä»»åŠ¡ï¼Œè€Œè¾ƒåçš„å±‚æ¬¡ä¸“æ³¨äºå†…å®¹ç›¸å…³ä»»åŠ¡ã€‚è¿™ç§é€å±‚åˆ’åˆ†æœ¬è´¨ä¸Šæ˜¯æ¬¡ä¼˜çš„ï¼Œå› ä¸ºä¸¤ç§ä¿¡æ¯ç±»å‹éƒ½æ— æ³•ä½¿ç”¨æ‰€æœ‰å±‚æ¬¡æ¥æ„å»ºå±‚æ¬¡åŒ–è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†JOOCIï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è¯­éŸ³è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œä¸ä¼šæŸå®³ä¸¤ç§ä¿¡æ¯ç±»å‹çš„è¡¨ç¤ºæ·±åº¦ã€‚åœ¨SUPERBåŸºå‡†æµ‹è¯•çš„ä¸¤ä¸ªè¯­éŸ³ä»»åŠ¡å’Œä¸¤ä¸ªè¯­è¨€ä»»åŠ¡ä¸­ï¼ŒJOOCIçš„æ€§èƒ½ä¼˜äºWavLM 26.5%ï¼Œä»¥åŠå…¶ä»–ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ï¼ˆ1äº¿å‚æ•°ï¼‰ï¼Œè¯æ˜äº†å…¶åœ¨è”åˆä¼˜åŒ–å…¶ä»–å’Œå†…å®¹ä¿¡æ¯æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼ˆJOOCIï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11086v3">PDF</a> Submitted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å½“å‰è‡ªç›‘ç£å­¦ä¹ åœ¨è¯­éŸ³è¯†åˆ«ä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯­éŸ³è¡¨ç¤ºå­¦ä¹ æ–¹æ³•JOOCIã€‚è¯¥æ–¹æ³•ä¸å¦¥åäºå†…å®¹å’Œéè¯­è¨€ä¿¡æ¯çš„ä»£è¡¨æ€§æ·±åº¦ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œè¯­éŸ³è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶åœ¨SUPERBåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³ä¿¡æ¯å¯ä»¥åˆ†ä¸ºå†…å®¹ä¿¡æ¯å’Œå…¶ä»–ä¿¡æ¯ä¸¤å¤§ç±»ã€‚</li>
<li>å½“å‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨è¯­éŸ³è¯†åˆ«çš„æ¨¡å‹å±‚æ¬¡æ·±åº¦ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œæ—©æœŸå±‚æ¬¡ä¸“æ³¨äºå…¶ä»–ä¿¡æ¯ï¼ŒåæœŸå±‚æ¬¡ä¸“æ³¨äºå†…å®¹ä¿¡æ¯ã€‚</li>
<li>è¿™ç§å±‚æ¬¡åˆ’åˆ†æ˜¯æ¬¡ä¼˜çš„ï¼Œå› ä¸ºä»»ä½•ä¸€ç§ä¿¡æ¯ç±»å‹éƒ½æ— æ³•åˆ©ç”¨æ‰€æœ‰å±‚æ¬¡æ¥æ„å»ºå±‚æ¬¡åŒ–çš„è¡¨ç¤ºã€‚</li>
<li>JOOCIæ˜¯ä¸€ç§æ–°å‹çš„è¯­éŸ³è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>JOOCIåœ¨SUPERBåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºWavLMå’Œå…¶ä»–ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜åŒ–å†…å®¹å’Œéè¯­è¨€ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>JOOCIçš„ä¼˜åŠ¿åœ¨äºä¸å¦¥åäºä»»ä½•ä¸€ç±»ä¿¡æ¯çš„ä»£è¡¨æ€§æ·±åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11086">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3d46006d746f95722d5b2a793aa9c88c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-273f85224ccc3afaa480fcb1c5c942b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c10fda2db23e2daf5e449a0087f8517.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6634a770b9a4f247610355d032004c63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ae08c8859ab5995b9ae82afbbd65182.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MS-HuBERT-Mitigating-Pre-training-and-Inference-Mismatch-in-Masked-Language-Modelling-methods-for-learning-Speech-Representations"><a href="#MS-HuBERT-Mitigating-Pre-training-and-Inference-Mismatch-in-Masked-Language-Modelling-methods-for-learning-Speech-Representations" class="headerlink" title="MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked   Language Modelling methods for learning Speech Representations"></a>MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked   Language Modelling methods for learning Speech Representations</h2><p><strong>Authors:Hemant Yadav, Sunayana Sitaram, Rajiv Ratn Shah</strong></p>
<p>In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERTâ€™s performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•åœ¨ä»åŸå§‹è¯­éŸ³ä¸­å­¦ä¹ é«˜çº§ä¿¡æ¯æ–¹é¢è·å¾—äº†å¾ˆå¤§çš„å…³æ³¨ã€‚åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼ŒHuBERTåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢è¡¨ç°å‡ºäº†é¡¶å°–çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºé¢„è®­ç»ƒç­–ç•¥çš„å·®å¼‚ï¼ŒHuBERTçš„æ€§èƒ½è½åäºdata2vecã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ï¼ˆiï¼‰ä¸€ç§Swapæ–¹æ³•ï¼Œä»¥è§£å†³HuBERTä¸­è§‚å¯Ÿåˆ°çš„é¢„è®­ç»ƒå’Œæ¨ç†ä¸åŒ¹é…çš„é—®é¢˜ï¼›ï¼ˆiiï¼‰å¹¶èåˆäº†å¤šèšç±»æ©ç é¢„æµ‹æŸå¤±ï¼Œä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ¨¡å‹å®¹é‡ã€‚ç”±æ­¤äº§ç”Ÿçš„æ–¹æ³•æ˜¯MS-HuBERTï¼Œå®ƒæ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºå­¦ä¹ ç¨³å¥çš„è¯­éŸ³è¡¨ç¤ºã€‚åœ¨é’ˆå¯¹ä¸åŒå¾®è°ƒåˆ†å‰²çš„è¯„ä¼°ä¸­ï¼Œå®ƒåœ¨ASR LibrispeechåŸºå‡†æµ‹è¯•ä¸Šå¹³å‡å‡»è´¥äº†åŸç‰ˆHuBERTï¼Œæå‡äº†5%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜ï¼Œåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦åˆ°çš„åµŒå…¥ç¼–ç åŒ…å«é‡è¦ä¿¡æ¯ï¼Œå¯æ”¹è¿›åŸºäºå†…å®¹çš„ä»»åŠ¡ï¼ˆå¦‚ASRï¼‰çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.05661v4">PDF</a> 4 pages, submitted to interspeech2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMS-HuBERTçš„ç«¯åˆ°ç«¯è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºå­¦ä¹ ç¨³å¥çš„è¯­éŸ³è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡è§£å†³HuBERTåœ¨é¢„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­çš„ä¸åŒ¹é…é—®é¢˜ï¼Œå¹¶ç»“åˆå¤šç°‡æ©ç é¢„æµ‹æŸå¤±ï¼Œå®ç°äº†å¯¹HuBERTæ¨¡å‹çš„æ”¹è¿›ã€‚åœ¨ASR LibrispeechåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMS-HuBERTç›¸è¾ƒäºåŸå§‹HuBERTæœ‰5%çš„å¹³å‡æå‡ã€‚åŒæ—¶ï¼Œé¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦åˆ°çš„åµŒå…¥ä¿¡æ¯å¯¹äºæé«˜åŸºäºå†…å®¹çš„ä»»åŠ¡ï¼ˆå¦‚ASRï¼‰çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•åœ¨è¯­éŸ³å­¦ä¹ ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå…¶ä¸­HuBERTåœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>HuBERTåœ¨é¢„è®­ç»ƒå’Œæ¨ç†æ–¹é¢å­˜åœ¨ä¸åŒ¹é…é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>MS-HuBERTæ–¹æ³•é€šè¿‡è§£å†³è¿™ç§ä¸åŒ¹é…é—®é¢˜å¹¶ç»“åˆå¤šç°‡æ©ç é¢„æµ‹æŸå¤±ï¼Œæœ‰æ•ˆæå‡äº†HuBERTçš„æ€§èƒ½ã€‚</li>
<li>MS-HuBERTåœ¨ASR LibrispeechåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºåŸå§‹HuBERTï¼Œå¹³å‡æå‡5%ã€‚</li>
<li>é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦åˆ°çš„åµŒå…¥ä¿¡æ¯å¯¹äºæé«˜åŸºäºå†…å®¹ä»»åŠ¡çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>MS-HuBERTæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼Œé€‚ç”¨äºå­¦ä¹ ç¨³å¥çš„è¯­éŸ³è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.05661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24bce55d0ca58c3f799c96f4eb1c89fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5db8da31f66dee5ea695deeabc6cf752.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27a9a4271a19fbd03aff137644fb8b12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9152fcdf74dc08b5126545f58bf02a9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d066b017394f0b17033363b31f1f6776.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-20/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-20/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b3c08aed10c7cdab53ee2eb15a2d46b3.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-20  Personalized Image Generation with Deep Generative Models A Decade   Survey
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9e6b0dc25000fe5cd5290aaf2ef24b05.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-20  Detection and Geographic Localization of Natural Objects in the Wild A   Case Study on Palms
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
