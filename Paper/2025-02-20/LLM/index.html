<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-20  Re-Align Aligning Vision Language Models via Retrieval-Augmented Direct   Preference Optimization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-774efd57a46de594c5180bbbcdb3454a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-20-æ›´æ–°"><a href="#2025-02-20-æ›´æ–°" class="headerlink" title="2025-02-20 æ›´æ–°"></a>2025-02-20 æ›´æ–°</h1><h2 id="Re-Align-Aligning-Vision-Language-Models-via-Retrieval-Augmented-Direct-Preference-Optimization"><a href="#Re-Align-Aligning-Vision-Language-Models-via-Retrieval-Augmented-Direct-Preference-Optimization" class="headerlink" title="Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct   Preference Optimization"></a>Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct   Preference Optimization</h2><p><strong>Authors:Shuo Xing, Yuping Wang, Peiran Li, Ruizheng Bai, Yueqi Wang, Chengxuan Qian, Huaxiu Yao, Zhengzhong Tu</strong></p>
<p>The emergence of large Vision Language Models (VLMs) has broadened the scope and capabilities of single-modal Large Language Models (LLMs) by integrating visual modalities, thereby unlocking transformative cross-modal applications in a variety of real-world scenarios. Despite their impressive performance, VLMs are prone to significant hallucinations, particularly in the form of cross-modal inconsistencies. Building on the success of Reinforcement Learning from Human Feedback (RLHF) in aligning LLMs, recent advancements have focused on applying direct preference optimization (DPO) on carefully curated datasets to mitigate these issues. Yet, such approaches typically introduce preference signals in a brute-force manner, neglecting the crucial role of visual information in the alignment process. In this paper, we introduce Re-Align, a novel alignment framework that leverages image retrieval to construct a dual-preference dataset, effectively incorporating both textual and visual preference signals. We further introduce rDPO, an extension of the standard direct preference optimization that incorporates an additional visual preference objective during fine-tuning. Our experimental results demonstrate that Re-Align not only mitigates hallucinations more effectively than previous methods but also yields significant performance gains in general visual question-answering (VQA) tasks. Moreover, we show that Re-Align maintains robustness and scalability across a wide range of VLM sizes and architectures. This work represents a significant step forward in aligning multimodal LLMs, paving the way for more reliable and effective cross-modal applications. We release all the code in <a target="_blank" rel="noopener" href="https://github.com/taco-group/Re-Align">https://github.com/taco-group/Re-Align</a>. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‡ºç°ï¼Œé€šè¿‡æ•´åˆè§†è§‰æ¨¡å¼ï¼Œæ‰©å¤§äº†å•æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èŒƒå›´å’Œèƒ½åŠ›ï¼Œä»è€Œè§£é”äº†å„ç§ç°å®åœºæ™¯ä¸­çš„è·¨æ¨¡æ€åº”ç”¨ç¨‹åºçš„å˜é©æ€§åº”ç”¨ã€‚å°½ç®¡å®ƒä»¬è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†VLMså®¹æ˜“å‡ºç°é‡å¤§å¹»è§‰ï¼Œç‰¹åˆ«æ˜¯ä»¥è·¨æ¨¡æ€ä¸ä¸€è‡´çš„å½¢å¼å‡ºç°ã€‚åŸºäºå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åœ¨LLMså¯¹é½ä¸­çš„æˆåŠŸï¼Œæœ€è¿‘çš„ç ”ç©¶è¿›å±•é›†ä¸­åœ¨ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†ä¸Šåº”ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä»¥ç²—æš´çš„æ–¹å¼å¼•å…¥åå¥½ä¿¡å·ï¼Œå¿½è§†äº†è§†è§‰ä¿¡æ¯åœ¨å¯¹é½è¿‡ç¨‹ä¸­çš„å…³é”®ä½œç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Re-Alignï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¯¹é½æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å›¾åƒæ£€ç´¢æ¥æ„å»ºåŒåå¥½æ•°æ®é›†ï¼Œæœ‰æ•ˆåœ°ç»“åˆäº†æ–‡æœ¬å’Œè§†è§‰åå¥½ä¿¡å·ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†rDPOï¼Œè¿™æ˜¯æ ‡å‡†ç›´æ¥åå¥½ä¼˜åŒ–çš„æ‰©å±•ï¼Œå®ƒåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­èå…¥äº†é¢å¤–çš„è§†è§‰åå¥½ç›®æ ‡ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRe-Alignä¸ä»…æ›´æœ‰æ•ˆåœ°å‡è½»äº†å¹»è§‰é—®é¢˜ï¼Œè€Œä¸”åœ¨ä¸€èˆ¬çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸­ä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†Re-Alignåœ¨ä¸åŒè§„æ¨¡å’Œæ¶æ„çš„VLMä¸­å…·æœ‰ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ã€‚è¿™é¡¹å·¥ä½œåœ¨å¯¹é½å¤šæ¨¡æ€LLMsæ–¹é¢è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ï¼Œä¸ºæ›´å¯é å’Œæœ‰æ•ˆçš„è·¨æ¨¡æ€åº”ç”¨ç¨‹åºé“ºå¹³äº†é“è·¯ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/taco-group/Re-Align%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%89%80%E6%9C%89%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/taco-group/Re-Alignä¸Šå‘å¸ƒäº†æ‰€æœ‰ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13146v1">PDF</a> 15 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å…´èµ·é€šè¿‡é›†æˆè§†è§‰æ¨¡å¼æ‰©å±•äº†å•ä¸€æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èŒƒå›´å’Œèƒ½åŠ›ï¼Œä»è€Œå¼€å¯äº†å„ç§ç°å®åœºæ™¯ä¸­çš„è·¨æ¨¡æ€åº”ç”¨ã€‚å°½ç®¡VLMsè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨æ¨¡æ€ä¸ä¸€è‡´çš„å½¢å¼ä¸­ã€‚æœ¬æ–‡å¼•å…¥Re-Alignï¼Œä¸€ç§åˆ©ç”¨å›¾åƒæ£€ç´¢æ„å»ºåŒåå¥½æ•°æ®é›†çš„æ–°å‹å¯¹é½æ¡†æ¶ï¼Œæœ‰æ•ˆç»“åˆæ–‡æœ¬å’Œè§†è§‰åå¥½ä¿¡å·ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒRe-Alignä¸ä»…æ›´æœ‰æ•ˆåœ°å‡è½»äº†å¹»è§‰ï¼Œè€Œä¸”åœ¨ä¸€èˆ¬çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶ä¿æŒäº†åœ¨å„ç§VLMè§„æ¨¡å’Œæ¶æ„ä¸­çš„ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å·¥ä½œä»£è¡¨äº†å¤šæ¨¡æ€LLMå¯¹é½çš„é‡å¤§è¿›æ­¥ï¼Œä¸ºæ›´å¯é å’Œæœ‰æ•ˆçš„è·¨æ¨¡æ€åº”ç”¨é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é›†æˆäº†è§†è§‰å’Œè¯­è¨€æ¨¡å¼ï¼Œå¢å¼ºäº†å•ä¸€æ¨¡æ€è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ¨åŠ¨äº†è·¨æ¨¡æ€åº”ç”¨çš„å‘å±•ã€‚</li>
<li>VLMsé¢ä¸´å¹»è§‰é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨æ¨¡æ€ä¸ä¸€è‡´çš„æƒ…å†µä¸‹ã€‚</li>
<li>Re-Alignæ¡†æ¶åˆ©ç”¨å›¾åƒæ£€ç´¢æ„å»ºåŒåå¥½æ•°æ®é›†ï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰åå¥½ä¿¡å·è¿›è¡Œæ¨¡å‹å¯¹é½ã€‚</li>
<li>Re-Alignæœ‰æ•ˆå‡è½»å¹»è§‰é—®é¢˜ï¼Œæé«˜è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>Re-Alignæ¡†æ¶å…·æœ‰è·¨ä¸åŒVLMè§„æ¨¡å’Œæ¶æ„çš„ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>è¯¥å·¥ä½œä»£è¡¨äº†å¤šæ¨¡æ€LLMå¯¹é½çš„é‡å¤§è¿›æ­¥ï¼Œä¸ºæ›´å¯é å’Œæœ‰æ•ˆçš„è·¨æ¨¡æ€åº”ç”¨æä¾›äº†æ–°æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c381ad434c06198a116f7a5c0937be83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a38753adb2dab15b1d3ff6b211a3684.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3e4babfad475c09c1d993dfba1df4c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba5ea96b681121e63d2b99f18a5f20e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2679786e4bdf3506b07678ca2992c76f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4433ee2d592ab3794318489e07c85a64.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Multimodal-Mamba-Decoder-only-Multimodal-State-Space-Model-via-Quadratic-to-Linear-Distillation"><a href="#Multimodal-Mamba-Decoder-only-Multimodal-State-Space-Model-via-Quadratic-to-Linear-Distillation" class="headerlink" title="Multimodal Mamba: Decoder-only Multimodal State Space Model via   Quadratic to Linear Distillation"></a>Multimodal Mamba: Decoder-only Multimodal State Space Model via   Quadratic to Linear Distillation</h2><p><strong>Authors:Bencheng Liao, Hongyuan Tao, Qian Zhang, Tianheng Cheng, Yingyue Li, Haoran Yin, Wenyu Liu, Xinggang Wang</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLEâ€™s capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6$\times$ speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\times$ speedup and 60.2% memory savings. Code and models are released at <a target="_blank" rel="noopener" href="https://github.com/hustvl/mmMamba">https://github.com/hustvl/mmMamba</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶äºŒæ¬¡è®¡ç®—å¤æ‚æ€§ã€ä¸æ–­å¢é•¿çš„å…³é”®å€¼ç¼“å­˜éœ€æ±‚å’Œä¾èµ–äºå•ç‹¬çš„è§†è§‰ç¼–ç å™¨ï¼Œé¢ä¸´ç€éƒ¨ç½²æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†mmMambaæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ¸è¿›è’¸é¦æ³•ä½¿ç”¨é€‚åº¦çš„å­¦æœ¯è®¡ç®—èµ„æºï¼Œå¼€å‘å…·æœ‰çº¿æ€§å¤æ‚æ€§çš„æœ¬åœ°å¤šæ¨¡æ€çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç›´æ¥è½¬æ¢ç»è¿‡è®­ç»ƒçš„ä»…è§£ç MLLMåˆ°çº¿æ€§å¤æ‚æ€§æ¶æ„æˆä¸ºå¯èƒ½ï¼Œè€Œæ— éœ€ä½¿ç”¨åŸºäºRNNçš„é¢„è®­ç»ƒLLMæˆ–è§†è§‰ç¼–ç å™¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ’­ç§ç­–ç•¥ï¼Œä»å·²è®­ç»ƒçš„Transformerä¸­é›•åˆ»å‡ºMambaï¼Œä»¥åŠä¸€ä¸ªä¸‰é˜¶æ®µçš„è’¸é¦é…æ–¹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†çŸ¥è¯†ä»Transformerè½¬ç§»åˆ°Mambaï¼ŒåŒæ—¶ä¿ç•™å¤šæ¨¡æ€åŠŸèƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜æ”¯æŒçµæ´»çš„æ··åˆæ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†Transformerå’ŒMambaå±‚ï¼Œå¯å®ç°å¯å®šåˆ¶çš„æ•ˆç‡æ€§èƒ½æƒè¡¡ã€‚ä»åŸºäºTransformerçš„ä»…è§£ç HoVLEä¸­æç‚¼å‡ºçš„mmMamba-linearä¸ç°æœ‰çš„çº¿æ€§åŠäºŒæ¬¡å¤æ‚æ€§VLMç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼›è€ŒmmMamba-hybridæ›´è¿›ä¸€æ­¥æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œæ¥è¿‘HoVLEçš„èƒ½åŠ›ã€‚åœ¨å¤„ç†103Kä»¤ç‰Œæ—¶ï¼Œä¸HoVLEç›¸æ¯”ï¼ŒmmMamba-linearå®ç°äº†20.6Ã—çš„åŠ é€Ÿå’Œ75.8%çš„GPUå†…å­˜å‡å°‘ï¼›è€ŒmmMamba-hybridå®ç°äº†13.5Ã—çš„åŠ é€Ÿå’Œ60.2%çš„å†…å­˜èŠ‚çœã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/hustvl/mmMamba%E4%B8%8A%E3%80%82">https://github.com/hustvl/mmMambaä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13145v1">PDF</a> Code and model are available at <a target="_blank" rel="noopener" href="https://github.com/hustvl/mmMamba">https://github.com/hustvl/mmMamba</a></p>
<p><strong>Summary</strong></p>
<p>mmMambaæ¡†æ¶é€šè¿‡æ¸è¿›è’¸é¦æŠ€æœ¯ï¼Œåˆ©ç”¨é€‚åº¦å­¦æœ¯è®¡ç®—èµ„æºï¼Œå®ç°äº†çº¿æ€§å¤æ‚åº¦çš„åŸç”Ÿå¤šæ¨¡æ€çŠ¶æ€ç©ºé—´æ¨¡å‹çš„å¼€å‘ã€‚è¯¥æ–¹æ³•å¯ç›´æ¥å°†è®­ç»ƒå¥½çš„è§£ç å™¨åªæœ‰MLLMsè½¬æ¢ä¸ºçº¿æ€§å¤æ‚åº¦æ¶æ„ï¼Œæ— éœ€é¢„è®­ç»ƒçš„RNNåŸºLLMæˆ–è§†è§‰ç¼–ç å™¨ã€‚æå‡ºä¸€ç§ä»è®­ç»ƒè¿‡çš„Transformerä¸­é›•åˆ»Mambaçš„æ’­ç§ç­–ç•¥ï¼Œä»¥åŠä¸€ä¸ªä¸‰é˜¶æ®µçš„è’¸é¦é…æ–¹ï¼Œå¯æœ‰æ•ˆåœ°å°†çŸ¥è¯†ä»Transformerè½¬ç§»åˆ°Mambaï¼ŒåŒæ—¶ä¿ç•™å¤šæ¨¡æ€èƒ½åŠ›ã€‚è¯¥æ–¹æ³•è¿˜æ”¯æŒçµæ´»çš„æ··åˆæ¶æ„ï¼Œç»“åˆTransformerå’ŒMambaå±‚ï¼Œå®ç°å¯å®šåˆ¶çš„æ•ˆç‡æ€§èƒ½æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>mmMambaæ¡†æ¶å®ç°äº†å¤šæ¨¡æ€çŠ¶æ€ç©ºé—´æ¨¡å‹çš„çº¿æ€§å¤æ‚åº¦å¼€å‘ã€‚</li>
<li>é€šè¿‡æ¸è¿›è’¸é¦æŠ€æœ¯ï¼Œåˆ©ç”¨é€‚åº¦å­¦æœ¯è®¡ç®—èµ„æºï¼Œæ— éœ€é¢„è®­ç»ƒçš„RNNåŸºLLMæˆ–è§†è§‰ç¼–ç å™¨ã€‚</li>
<li>æå‡ºäº†ä»è®­ç»ƒè¿‡çš„Transformerä¸­é›•åˆ»Mambaçš„æ’­ç§ç­–ç•¥ã€‚</li>
<li>ä¸‰é˜¶æ®µçš„è’¸é¦é…æ–¹æœ‰æ•ˆè½¬ç§»çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ç•™å¤šæ¨¡æ€èƒ½åŠ›ã€‚</li>
<li>æ”¯æŒçµæ´»çš„æ··åˆæ¶æ„ï¼Œç»“åˆTransformerå’ŒMambaå±‚ï¼Œå®ç°æ•ˆç‡æ€§èƒ½çš„å¯å®šåˆ¶åŒ–ã€‚</li>
<li>mmMamba-linearç›¸è¾ƒäºç°æœ‰çº¿æ€§åŠäºŒæ¬¡å¤æ‚åº¦VLMså…·æœ‰ç«äº‰åŠ›ï¼Œè€ŒmmMamba-hybridè¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œæ¥è¿‘HoVLEçš„èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤„ç†103Kä»¤ç‰Œæ—¶ï¼ŒmmMamba-linearç›¸æ¯”HoVLEå®ç°äº†20.6å€çš„é€Ÿåº¦æå‡å’Œ75.8%çš„GPUå†…å­˜å‡å°‘ï¼Œè€ŒmmMamba-hybridå®ç°äº†13.5å€çš„é€Ÿåº¦æå‡å’Œ60.2%çš„å†…å­˜èŠ‚çœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13145">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f0a1dd64afc35ac5dbcc7d847c4363b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c300268315c9f424d6eea767e8f5426.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bb026cdf6a0c6fba7723745da4eb516.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AIDE-AI-Driven-Exploration-in-the-Space-of-Code"><a href="#AIDE-AI-Driven-Exploration-in-the-Space-of-Code" class="headerlink" title="AIDE: AI-Driven Exploration in the Space of Code"></a>AIDE: AI-Driven Exploration in the Space of Code</h2><p><strong>Authors:Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, Yuxiang Wu</strong></p>
<p>Machine learning, the foundation of modern artificial intelligence, has driven innovations that have fundamentally transformed the world. Yet, behind advancements lies a complex and often tedious process requiring labor and compute intensive iteration and experimentation. Engineers and scientists developing machine learning models spend much of their time on trial-and-error tasks instead of conceptualizing innovative solutions or research hypotheses. To address this challenge, we introduce AI-Driven Exploration (AIDE), a machine learning engineering agent powered by large language models (LLMs). AIDE frames machine learning engineering as a code optimization problem, and formulates trial-and-error as a tree search in the space of potential solutions. By strategically reusing and refining promising solutions, AIDE effectively trades computational resources for enhanced performance, achieving state-of-the-art results on multiple machine learning engineering benchmarks, including our Kaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ ä½œä¸ºç°ä»£äººå·¥æ™ºèƒ½çš„åŸºç¡€ï¼Œæ¨åŠ¨äº†åˆ›æ–°ï¼Œä»æ ¹æœ¬ä¸Šæ”¹å˜äº†ä¸–ç•Œã€‚ç„¶è€Œï¼Œåœ¨è¿›æ­¥çš„èƒŒåæ˜¯ä¸€ä¸ªå¤æ‚ä¸”å¸¸å¸¸ä¹å‘³çš„è¿‡ç¨‹ï¼Œéœ€è¦åŠ³åŠ¨å’Œè®¡ç®—å¯†é›†å‹çš„è¿­ä»£å’Œå®éªŒã€‚å¼€å‘å’Œç§‘å­¦å®¶åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ŠèŠ±è´¹å¤§é‡æ—¶é—´è¿›è¡Œè¯•é”™ä»»åŠ¡ï¼Œè€Œä¸æ˜¯æ„æƒ³åˆ›æ–°è§£å†³æ–¹æ¡ˆæˆ–ç ”ç©¶å‡è®¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AIé©±åŠ¨çš„æ¢ç´¢ï¼ˆAIDEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æœºå™¨å­¦ä¹ å·¥ç¨‹ä»£ç†ã€‚AIDEå°†æœºå™¨å­¦ä¹ å·¥ç¨‹è§†ä¸ºä»£ç ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å°†è¯•é”™è¡¨è¿°ä¸ºæ½œåœ¨è§£å†³æ–¹æ¡ˆç©ºé—´ä¸­çš„æ ‘æœç´¢ã€‚é€šè¿‡æˆ˜ç•¥æ€§åœ°é‡ç”¨å’Œæ”¹è¿›æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼ŒAIDEæœ‰æ•ˆåœ°ç”¨è®¡ç®—èµ„æºæ¢å–æ€§èƒ½æå‡ï¼Œåœ¨å¤šä¸ªæœºå™¨å­¦ä¹ å·¥ç¨‹åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€æ–°ç»“æœï¼ŒåŒ…æ‹¬æˆ‘ä»¬çš„Kaggleè¯„ä¼°ã€OpenAI MLE-Benchå’ŒMETRs RE-Benchã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13138v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœºå™¨å­¦ä¹ ä½œä¸ºç°ä»£äººå·¥æ™ºèƒ½çš„åŸºç¡€ï¼Œæ¨åŠ¨äº†åˆ›æ–°å¹¶ä»æ ¹æœ¬ä¸Šæ”¹å˜äº†ä¸–ç•Œã€‚ç„¶è€Œï¼Œåœ¨è¿›æ­¥çš„èƒŒåæ˜¯ä¸€ä¸ªå¤æ‚ä¸”ä¹å‘³çš„è¿‡ç¨‹ï¼Œéœ€è¦åŠ³åŠ¨å’Œè®¡ç®—å¯†é›†å‹çš„è¿­ä»£å’Œå®éªŒã€‚å·¥ç¨‹å¸ˆå’Œç§‘å­¦å®¶åœ¨å¼€å‘æœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œå¤§éƒ¨åˆ†æ—¶é—´éƒ½èŠ±åœ¨è¯•é”™ä»»åŠ¡ä¸Šï¼Œè€Œä¸æ˜¯åœ¨æ„æ€åˆ›æ–°è§£å†³æ–¹æ¡ˆæˆ–ç ”ç©¶å‡è®¾ä¸Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AIé©±åŠ¨çš„æ¢ç´¢ï¼ˆAIDEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æœºå™¨å­¦ä¹ å·¥ç¨‹ä»£ç†ã€‚AIDEå°†æœºå™¨å­¦ä¹ å·¥ç¨‹è§†ä¸ºä»£ç ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å°†è¯•é”™è¡¨è¿°ä¸ºæ½œåœ¨è§£å†³æ–¹æ¡ˆç©ºé—´ä¸­çš„æ ‘æœç´¢ã€‚é€šè¿‡æˆ˜ç•¥æ€§åœ°é‡æ–°ä½¿ç”¨å’Œç²¾ç‚¼æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆï¼ŒAIDEæœ‰æ•ˆåœ°ç”¨è®¡ç®—èµ„æºæ¢å–äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªæœºå™¨å­¦ä¹ å·¥ç¨‹åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€æ–°ç»“æœï¼ŒåŒ…æ‹¬æˆ‘ä»¬çš„Kaggleè¯„ä¼°ã€OpenAI MLE-Benchå’ŒMETRs RE-Benchã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœºå™¨å­¦ä¹ æ˜¯ç°ä»£äººå·¥æ™ºèƒ½å‘å±•çš„åŸºçŸ³ï¼Œæ¨åŠ¨äº†ä¸–ç•Œçš„æ ¹æœ¬æ€§å˜é©ã€‚</li>
<li>æœºå™¨å­¦ä¹ è¿›æ­¥èƒŒåæ˜¯ä¸€ä¸ªå¤æ‚ä¸”åŒ…å«å¤§é‡è¯•é”™è¿‡ç¨‹çš„å¼€å‘è¿‡ç¨‹ã€‚</li>
<li>å·¥ç¨‹å¸ˆå’Œç§‘å­¦å®¶åœ¨å¼€å‘æœºå™¨å­¦ä¹ æ¨¡å‹æ—¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯èŠ±è´¹å¤§é‡æ—¶é—´åœ¨è¯•é”™ä»»åŠ¡ä¸Šã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œå¼•å…¥äº†AIé©±åŠ¨çš„æ¢ç´¢ï¼ˆAIDEï¼‰ã€‚</li>
<li>AIDEæ˜¯ä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æœºå™¨å­¦ä¹ å·¥ç¨‹ä»£ç†ï¼Œå°†æœºå™¨å­¦ä¹ å·¥ç¨‹è§†ä¸ºä»£ç ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>AIDEé€šè¿‡æˆ˜ç•¥æ€§åœ°é‡æ–°ä½¿ç”¨å’Œç²¾ç‚¼è§£å†³æ–¹æ¡ˆï¼Œä»¥è®¡ç®—èµ„æºæ¢å–æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13138">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-843bc38c8c2c50eff675f790b7016c6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fe81bb4c4245a2e4e920c88086529c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e46e43cd546ac7b3d4472baadbb6cead.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb1eadab811d2ae354c0fe5c3ab824bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-951be378287ce7f968c1265ef9497cf4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="RuozhiBench-Evaluating-LLMs-with-Logical-Fallacies-and-Misleading-Premises"><a href="#RuozhiBench-Evaluating-LLMs-with-Logical-Fallacies-and-Misleading-Premises" class="headerlink" title="RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading   Premises"></a>RuozhiBench: Evaluating LLMs with Logical Fallacies and Misleading   Premises</h2><p><strong>Authors:Zenan Zhai, Hao Li, Xudong Han, Zhenxuan Zhang, Yixuan Zhang, Timothy Baldwin, Haonan Li</strong></p>
<p>Recent advances in large language models (LLMs) have shown that they can answer questions requiring complex reasoning. However, their ability to identify and respond to text containing logical fallacies or deliberately misleading premises remains less studied. To address this gap, we introduce RuozhiBench, a bilingual dataset comprising 677 carefully curated questions that contain various forms of deceptive reasoning, meticulously crafted through extensive human effort and expert review. In a comprehensive evaluation of 17 LLMs from 5 Series over RuozhiBench using both open-ended and two-choice formats, we conduct extensive analyses on evaluation protocols and result patterns. Despite their high scores on conventional benchmarks, these models showed limited ability to detect and reason correctly about logical fallacies, with even the best-performing model, Claude-3-haiku, achieving only 62% accuracy compared to the human of more than 90%. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥è¡¨æ˜ï¼Œå®ƒä»¬èƒ½å¤Ÿå›ç­”éœ€è¦å¤æ‚æ¨ç†çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è¯†åˆ«å’Œå¤„ç†å«æœ‰é€»è¾‘è°¬è¯¯æˆ–æ•…æ„è¯¯å¯¼æ€§å‰æçš„æ–‡æœ¬æ–¹é¢çš„èƒ½åŠ›ä»ç ”ç©¶è¾ƒå°‘ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RuozhiBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒè¯­æ•°æ®é›†ï¼ŒåŒ…å«677ä¸ªç²¾å¿ƒæŒ‘é€‰çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å«æœ‰å„ç§å½¢å¼çš„æ¬ºéª—æ€§æ¨ç†ï¼Œé€šè¿‡å¤§é‡çš„äººåŠ›æŠ•å…¥å’Œä¸“å®¶è¯„å®¡ç²¾å¿ƒæ„å»ºè€Œæˆã€‚æˆ‘ä»¬ä½¿ç”¨å¼€æ”¾å¼å’Œé€‰æ‹©é¢˜æ ¼å¼ï¼Œåœ¨RuozhiBenchä¸Šå¯¹æ¥è‡ª5ä¸ªç³»åˆ—çš„11ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶å¯¹è¯„ä¼°åè®®å’Œç»“æœæ¨¡å¼è¿›è¡Œäº†å¹¿æ³›åˆ†æã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨ä¼ ç»ŸåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ£€æµ‹é€»è¾‘é”™è¯¯å¹¶è¿›è¡Œæ­£ç¡®æ¨ç†æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹Claude-3-haikuçš„å‡†ç¡®ç‡ä»…ä¸º62%ï¼Œè€Œäººç±»çš„å‡†ç¡®ç‡è¶…è¿‡90%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13125v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†é—®é¢˜ä¸Šçš„è¡¨ç°å¼•äººæ³¨ç›®ï¼Œä½†å®ƒä»¬åœ¨è¯†åˆ«å’Œå¤„ç†å«æœ‰é€»è¾‘è°¬è¯¯æˆ–æ•…æ„è¯¯å¯¼æ€§å‰æçš„æ–‡æœ¬æ–¹é¢çš„èƒ½åŠ›å°šå¾…ç ”ç©¶ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RuozhiBenchåŒè¯­æ•°æ®é›†ï¼ŒåŒ…å«ç²¾å¿ƒæŒ‘é€‰çš„677ä¸ªå«æœ‰å„ç§æ¬ºéª—æ€§æ¨ç†çš„é—®é¢˜ï¼Œé€šè¿‡å¤§é‡äººåŠ›å’Œä¸“å®¶å®¡æŸ¥ç²¾å¿ƒç¼–åˆ¶è€Œæˆã€‚é€šè¿‡å¯¹æ¥è‡ªäº”ä¸ªç³»åˆ—çš„åä¸ƒç§å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å°½ç®¡å®ƒä»¬åœ¨å¸¸è§„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æ£€æµ‹é€»è¾‘é”™è¯¯å’Œæ­£ç¡®æ¨ç†æ–¹é¢çš„èƒ½åŠ›æœ‰é™ã€‚æœ€å¥½çš„æ¨¡å‹Claude-3-haikuçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰62%ï¼Œè€Œäººç±»çš„å‡†ç¡®ç‡è¶…è¿‡90%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†é—®é¢˜ä¸Šçš„è¡¨ç°æœ‰æ‰€çªç ´ã€‚</li>
<li>ç›®å‰å°šå¾…ç ”ç©¶çš„æ˜¯LLMåœ¨è¯†åˆ«å’Œå¤„ç†å«æœ‰é€»è¾‘è°¬è¯¯æˆ–æ•…æ„è¯¯å¯¼æ€§å‰æçš„æ–‡æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>ä¸ºè¯„ä¼°LLMåœ¨è¿™ä¸€é¢†åŸŸçš„æ€§èƒ½ï¼Œæ¨å‡ºäº†RuozhiBenchåŒè¯­æ•°æ®é›†ã€‚</li>
<li>RuozhiBenchåŒ…å«ç²¾å¿ƒæŒ‘é€‰çš„é—®é¢˜ï¼Œæ¶‰åŠå„ç§å½¢å¼çš„æ¬ºéª—æ€§æ¨ç†ã€‚</li>
<li>åœ¨å…¨é¢è¯„ä¼°LLMæ—¶ï¼Œå‘ç°å®ƒä»¬æ£€æµ‹é€»è¾‘é”™è¯¯å’Œæ­£ç¡®æ¨ç†çš„èƒ½åŠ›æœ‰é™ã€‚</li>
<li>åœ¨ä½¿ç”¨RuozhiBenchè¿›è¡Œçš„è¯„ä¼°ä¸­ï¼Œå³ä½¿æ˜¯æœ€å¥½çš„æ¨¡å‹ä¹Ÿä»…è¾¾åˆ°çº¦62%çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-509ab3791bd2f5e31be11b392d5e9263.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df0d77ecf2c93d863d1d2fe88cace7f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcfb83250db686ea7e6a2e9d69375a26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6548d91662ba8de55426be9d112aea98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a18bf090dec3e6d095eefe8392f87b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c7fa3b4352fcb76be76a5eea17408c78.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7fc14d34b4aa2a8b442cfe4f39f9e9e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="STEER-ME-Assessing-the-Microeconomic-Reasoning-of-Large-Language-Models"><a href="#STEER-ME-Assessing-the-Microeconomic-Reasoning-of-Large-Language-Models" class="headerlink" title="STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models"></a>STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models</h2><p><strong>Authors:Narun Raman, Taylor Lundy, Thiago Amin, Jesse Perla, Kevin-Leyton Brown</strong></p>
<p>How should one judge whether a given large language model (LLM) can reliably perform economic reasoning? Most existing LLM benchmarks focus on specific applications and fail to present the model with a rich variety of economic tasks. A notable exception is Raman et al. [2024], who offer an approach for comprehensively benchmarking strategic decision-making; however, this approach fails to address the non-strategic settings prevalent in microeconomics, such as supply-and-demand analysis. We address this gap by taxonomizing microeconomic reasoning into $58$ distinct elements, focusing on the logic of supply and demand, each grounded in up to $10$ distinct domains, $5$ perspectives, and $3$ types. The generation of benchmark data across this combinatorial space is powered by a novel LLM-assisted data generation protocol that we dub auto-STEER, which generates a set of questions by adapting handwritten templates to target new domains and perspectives. Because it offers an automated way of generating fresh questions, auto-STEER mitigates the risk that LLMs will be trained to over-fit evaluation benchmarks; we thus hope that it will serve as a useful tool both for evaluating and fine-tuning models for years to come. We demonstrate the usefulness of our benchmark via a case study on $27$ LLMs, ranging from small open-source models to the current state of the art. We examined each modelâ€™s ability to solve microeconomic problems across our whole taxonomy and present the results across a range of prompting strategies and scoring metrics. </p>
<blockquote>
<p>å¦‚ä½•åˆ¤æ–­ç»™å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦å¯ä»¥è¿›è¡Œå¯é çš„ç»æµæ¨ç†ï¼Ÿå¤§å¤šæ•°ç°æœ‰çš„LLMåŸºå‡†æµ‹è¯•éƒ½ä¸“æ³¨äºç‰¹å®šåº”ç”¨ç¨‹åºï¼Œæœªèƒ½å‘æ¨¡å‹æä¾›ä¸°å¯Œçš„ç»æµä»»åŠ¡ã€‚ä¸€ä¸ªå€¼å¾—æ³¨æ„çš„ä¾‹å¤–æ˜¯æ‹‰æ›¼ç­‰äºº[2024]æå‡ºçš„ä¸€ç§å…¨é¢è¯„ä¼°æˆ˜ç•¥å†³ç­–çš„æ–¹æ³•ï¼Œç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æœªèƒ½è§£å†³åœ¨å¾®è§‚ç»æµå­¦ä¸­æ™®éå­˜åœ¨çš„éæˆ˜ç•¥è®¾ç½®ï¼Œå¦‚ä¾›éœ€åˆ†æã€‚æˆ‘ä»¬é€šè¿‡å°†å¾®è§‚ç»æµæ¨ç†åˆ†ç±»ä¸º58ä¸ªä¸åŒçš„å…ƒç´ æ¥è§£å†³è¿™ä¸€ç©ºç™½ï¼Œè¿™äº›å…ƒç´ é‡ç‚¹å…³æ³¨ä¾›éœ€é€»è¾‘ï¼Œæ¯ä¸ªå…ƒç´ éƒ½åŸºäºæœ€å¤š10ä¸ªä¸åŒçš„é¢†åŸŸã€5ä¸ªè§’åº¦å’Œ3ç§ç±»å‹ã€‚åœ¨æ­¤ç»„åˆç©ºé—´ä¸­ç”ŸæˆåŸºå‡†æµ‹è¯•æ•°æ®æ˜¯ç”±æˆ‘ä»¬ç§°ä¹‹ä¸ºauto-STEERçš„æ–°å‹LLMè¾…åŠ©æ•°æ®ç”Ÿæˆåè®®é©±åŠ¨çš„ï¼Œè¯¥åè®®é€šè¿‡é€‚åº”æ‰‹å†™æ¨¡æ¿æ¥é’ˆå¯¹æ–°é¢†åŸŸå’Œè§’åº¦ç”Ÿæˆä¸€ç³»åˆ—é—®é¢˜ã€‚ç”±äºå®ƒæä¾›äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆæ–°é—®é¢˜çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œå› æ­¤auto-STEERå‡è½»äº†LLMä¼šè¢«è®­ç»ƒè¿‡åº¦ä»¥é€‚åº”è¯„ä¼°åŸºå‡†æµ‹è¯•çš„é£é™©ï¼›å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒåœ¨æœªæ¥å‡ å¹´é‡Œæ—¢å¯ç”¨äºè¯„ä¼°ä¹Ÿå¯ç”¨äºå¾®è°ƒæ¨¡å‹ã€‚æˆ‘ä»¬å¯¹æ¶µç›–å°åˆ°å¼€æºæ¨¡å‹å¤§åˆ°å½“å‰æœ€å…ˆè¿›çš„æ°´å¹³çš„å…±è®¡27ä¸ªLLMè¿›è¡Œäº†æ¡ˆä¾‹ç ”ç©¶ï¼Œè€ƒå¯Ÿæ¯ä¸ªæ¨¡å‹åœ¨æˆ‘ä»¬æ•´ä¸ªåˆ†ç±»ä½“ç³»ä¸­è§£å†³å¾®è§‚ç»æµé—®é¢˜çš„èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—æç¤ºç­–ç•¥å’Œè¯„åˆ†æŒ‡æ ‡ä¸­å‘ˆç°ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13119v1">PDF</a> 18 pages, 11 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ä½•å¯é åœ°è¿›è¡Œç»æµæ¨ç†çš„é—®é¢˜ï¼Œç°æœ‰å¤§å¤šæ•°LLMåŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šåº”ç”¨ä¸Šï¼Œæœªèƒ½å‘ˆç°æ¨¡å‹ä¸°å¯Œçš„ç»æµä»»åŠ¡å¤šæ ·æ€§ã€‚æœ¬æ–‡é€šè¿‡åˆ†ç±»å¾®ç»æµæ¨ç†çš„58ä¸ªä¸åŒå…ƒç´ æ¥è§£å†³è¿™ä¸€ç©ºç™½ï¼Œé‡ç‚¹å…³æ³¨ä¾›éœ€é€»è¾‘ï¼Œæ¯ä¸ªå…ƒç´ åŸºäºå¤šè¾¾10ä¸ªä¸åŒé¢†åŸŸã€5ä¸ªè§†è§’å’Œ3ç§ç±»å‹ã€‚å€ŸåŠ©æ–°å‹LLMè¾…åŠ©æ•°æ®ç”Ÿæˆåè®®auto-STEERï¼Œç”Ÿæˆæ¶µç›–æ­¤ç»„åˆç©ºé—´çš„åŸºå‡†æµ‹è¯•æ•°æ®ã€‚è¯¥åè®®é€šè¿‡é€‚åº”æ‰‹å†™æ¨¡æ¿æ¥å®šä½æ–°é¢†åŸŸå’Œè§†è§’ï¼Œè‡ªåŠ¨ç”Ÿæˆä¸€ç³»åˆ—é—®é¢˜ï¼Œé™ä½äº†LLMè¿‡åº¦é€‚åº”è¯„ä¼°åŸºå‡†æµ‹è¯•çš„é£é™©ã€‚é€šè¿‡ä¸€é¡¹æ¶‰åŠ27ä¸ªLLMçš„æ¡ˆä¾‹ç ”ç©¶ï¼ŒéªŒè¯äº†è¯¥åŸºå‡†æµ‹è¯•çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç°æœ‰LLMåŸºå‡†æµ‹è¯•æœªèƒ½å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨å¤šç§ç»æµä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå­˜åœ¨å¯¹ç»æµæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡åˆ†ç±»å¾®ç»æµæ¨ç†çš„å¤šä¸ªå…ƒç´ æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæ¶µç›–ä¾›éœ€é€»è¾‘ç­‰å¤šä¸ªæ–¹é¢ã€‚</li>
<li>å¼•å…¥äº†æ–°å‹LLMè¾…åŠ©æ•°æ®ç”Ÿæˆåè®®auto-STEERï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ¶µç›–å¹¿æ³›ç»„åˆç©ºé—´çš„é—®é¢˜ï¼Œé™ä½LLMè¿‡åº¦é€‚åº”è¯„ä¼°çš„é£é™©ã€‚</li>
<li>é€šè¿‡ä¸€é¡¹æ¶‰åŠå¤šä¸ªLLMçš„æ¡ˆä¾‹ç ”ç©¶éªŒè¯äº†è¯¥åŸºå‡†æµ‹è¯•çš„å®ç”¨æ€§ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•ä¸ä»…å¯ç”¨äºè¯„ä¼°æ¨¡å‹ï¼Œè¿˜å¯ç”¨äºæ¨¡å‹å¾®è°ƒï¼Œæœ‰æœ›åœ¨æœªæ¥å¤šå¹´å†…æŒç»­å‘æŒ¥ä½œç”¨ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†æ¯ä¸ªLLMåœ¨è§£å†³å¾®ç»æµé—®é¢˜ä¸Šçš„èƒ½åŠ›å·®å¼‚ï¼Œé€šè¿‡ä¸åŒçš„æç¤ºç­–ç•¥å’Œè¯„åˆ†æŒ‡æ ‡å‘ˆç°ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13119">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9fc4393231f3ba405b868fde2d105ced.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-640b5734e29099ff8c9878712321d98c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Text2World-Benchmarking-Large-Language-Models-for-Symbolic-World-Model-Generation"><a href="#Text2World-Benchmarking-Large-Language-Models-for-Symbolic-World-Model-Generation" class="headerlink" title="Text2World: Benchmarking Large Language Models for Symbolic World Model   Generation"></a>Text2World: Benchmarking Large Language Models for Symbolic World Model   Generation</h2><p><strong>Authors:Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Hongyuan Zhang, Wenqi Shao, Ping Luo</strong></p>
<p>Recently, there has been growing interest in leveraging large language models (LLMs) to generate symbolic world models from textual descriptions. Although LLMs have been extensively explored in the context of world modeling, prior studies encountered several challenges, including evaluation randomness, dependence on indirect metrics, and a limited domain scope. To address these limitations, we introduce a novel benchmark, Text2World, based on planning domain definition language (PDDL), featuring hundreds of diverse domains and employing multi-criteria, execution-based metrics for a more robust evaluation. We benchmark current LLMs using Text2World and find that reasoning models trained with large-scale reinforcement learning outperform others. However, even the best-performing model still demonstrates limited capabilities in world modeling. Building on these insights, we examine several promising strategies to enhance the world modeling capabilities of LLMs, including test-time scaling, agent training, and more. We hope that Text2World can serve as a crucial resource, laying the groundwork for future research in leveraging LLMs as world models. The project page is available at <a target="_blank" rel="noopener" href="https://text-to-world.github.io/">https://text-to-world.github.io/</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œäººä»¬è¶Šæ¥è¶Šæœ‰å…´è¶£åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»æ–‡æœ¬æè¿°ç”Ÿæˆç¬¦å·ä¸–ç•Œæ¨¡å‹ã€‚å°½ç®¡LLMåœ¨ä¸–ç•Œå»ºæ¨¡çš„ä¸Šä¸‹æ–‡ä¸­å·²ç»è¢«å¹¿æ³›æ¢ç´¢ï¼Œä½†å…ˆå‰çš„ç ”ç©¶é‡åˆ°äº†å‡ ä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¯„ä¼°çš„éšæœºæ€§ã€å¯¹é—´æ¥æŒ‡æ ‡çš„ä¾èµ–ä»¥åŠæœ‰é™çš„é¢†åŸŸèŒƒå›´ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•å¹³å°Text2Worldï¼Œè¯¥å¹³å°åŸºäºè§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€ï¼ˆPDDLï¼‰ï¼Œæ‹¥æœ‰æ•°ç™¾ä¸ªä¸åŒçš„é¢†åŸŸï¼Œå¹¶ä½¿ç”¨å¤šæ ‡å‡†ã€åŸºäºæ‰§è¡Œçš„æŒ‡æ ‡è¿›è¡Œæ›´ç¨³å¥çš„è¯„ä¼°ã€‚æˆ‘ä»¬ä½¿ç”¨Text2Worldå¯¹å½“å‰LLMè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå‘ç°ä½¿ç”¨å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒæ¨ç†æ¨¡å‹çš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚ç„¶è€Œï¼Œå³ä½¿è¡¨ç°æœ€ä½³çš„æ¨¡å‹åœ¨ä¸–ç•Œå»ºæ¨¡æ–¹é¢ä»ç„¶è¡¨ç°å‡ºæœ‰é™çš„èƒ½åŠ›ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å‡ ç§å¢å¼ºLLMä¸–ç•Œå»ºæ¨¡èƒ½åŠ›çš„æœ‰å‰é€”çš„ç­–ç•¥ï¼ŒåŒ…æ‹¬æµ‹è¯•æ—¶é—´ç¼©æ”¾ã€ä»£ç†è®­ç»ƒç­‰ã€‚æˆ‘ä»¬å¸Œæœ›Text2Worldèƒ½æˆä¸ºä¸€ä¸ªé‡è¦èµ„æºï¼Œä¸ºæœªæ¥åˆ©ç”¨LLMä½œä¸ºä¸–ç•Œæ¨¡å‹çš„ç ”ç©¶å¥ å®šåŸºç¡€ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://text-to-world.github.io/%E8%AE%BF%E9%97%AE%E3%80%82">https://text-to-world.github.io/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13092v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://text-to-world.github.io/">https://text-to-world.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«ç”¨äºä»æ–‡æœ¬æè¿°ç”Ÿæˆç¬¦å·ä¸–ç•Œæ¨¡å‹çš„ç ”ç©¶é€æ¸å¢å¤šã€‚ä¸ºè§£å†³ç°æœ‰ç ”ç©¶çš„æŒ‘æˆ˜ï¼Œå¦‚è¯„ä¼°éšæœºæ€§ã€ä¾èµ–é—´æ¥æŒ‡æ ‡å’Œé¢†åŸŸå±€é™æ€§ï¼Œæå‡ºåŸºäºè§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€ï¼ˆPDDLï¼‰çš„Text2Worldæ–°åŸºå‡†æµ‹è¯•ã€‚å½“å‰LLMçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œé‡‡ç”¨å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ¨ç†æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œä½†æœ€ä½³æ¨¡å‹åœ¨ä¸–ç•Œå»ºæ¨¡æ–¹é¢ä»æœ‰å±€é™ã€‚ä¸ºæå‡LLMçš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›ï¼Œæ¢è®¨äº†å¤šç§ç­–ç•¥ï¼Œå¦‚æµ‹è¯•æ—¶ç¼©æ”¾ã€ä»£ç†è®­ç»ƒå’Œæ›´å¤šç­–ç•¥ã€‚æœŸæœ›Text2Worldèƒ½æˆä¸ºæœªæ¥ç ”ç©¶çš„é‡è¦èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨è¢«ç”¨äºä»æ–‡æœ¬æè¿°ç”Ÿæˆç¬¦å·ä¸–ç•Œæ¨¡å‹ã€‚</li>
<li>Text2WorldåŸºå‡†æµ‹è¯•é‡‡ç”¨è§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€ï¼ˆPDDLï¼‰ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸï¼Œä½¿ç”¨å¤šæ ‡å‡†ã€åŸºäºæ‰§è¡Œçš„åº¦é‡æ–¹æ³•è¿›è¡Œæ›´ç¨³å¥çš„è¯„ä¼°ã€‚</li>
<li>ç°æœ‰LLMåœ¨ä¸–ç•Œå»ºæ¨¡æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå°½ç®¡é‡‡ç”¨å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ¨ç†æ¨¡å‹è¡¨ç°æœ€ä½³ã€‚</li>
<li>Text2Worldé¡¹ç›®æä¾›äº†ä¸€ä¸ªé‡è¦çš„èµ„æºï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸­å­˜åœ¨è¯„ä¼°éšæœºæ€§ã€ä¾èµ–é—´æ¥æŒ‡æ ‡å’Œé¢†åŸŸå±€é™æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡LLMçš„ä¸–ç•Œå»ºæ¨¡èƒ½åŠ›æœ‰å¤šç§ç­–ç•¥ï¼Œå¦‚æµ‹è¯•æ—¶ç¼©æ”¾ã€ä»£ç†è®­ç»ƒå’Œæ›´å¤šç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13092">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b2a051ab40f99dec65839a2567198c24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bde06f91044c85f4fb755a517824b7b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbaaa4fee2f9103ea1570ca037816447.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c355495de58509325825224204ba1dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-120b42d2b24bf503fbfa28a7b6d798ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd2d53b7b56b6e493089bc49131c65dd.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Do-we-still-need-Human-Annotators-Prompting-Large-Language-Models-for-Aspect-Sentiment-Quad-Prediction"><a href="#Do-we-still-need-Human-Annotators-Prompting-Large-Language-Models-for-Aspect-Sentiment-Quad-Prediction" class="headerlink" title="Do we still need Human Annotators? Prompting Large Language Models for   Aspect Sentiment Quad Prediction"></a>Do we still need Human Annotators? Prompting Large Language Models for   Aspect Sentiment Quad Prediction</h2><p><strong>Authors:Nils Constantin Hellwig, Jakob Fehle, Udo Kruschwitz, Christian Wolff</strong></p>
<p>Aspect sentiment quadruple prediction (ASQP) facilitates a detailed understanding of opinions expressed in a text by identifying the opinion term, aspect term, aspect category and sentiment polarity for each opinion. However, annotating a full set of training examples to fine-tune models for ASQP is a resource-intensive process. In this study, we explore the capabilities of large language models (LLMs) for zero- and few-shot learning on the ASQP task across five diverse datasets. We report F1 scores slightly below those obtained with state-of-the-art fine-tuned models but exceeding previously reported zero- and few-shot performance. In the 40-shot setting on the Rest16 restaurant domain dataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the best-performing fine-tuned method MVP. Additionally, we report the performance of LLMs in target aspect sentiment detection (TASD), where the F1 scores were also close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot setting, compared to 72.76 with MVP. While human annotators remain essential for achieving optimal performance, LLMs can reduce the need for extensive manual annotation in ASQP tasks. </p>
<blockquote>
<p>é¢å‘æ–¹é¢çš„æƒ…æ„Ÿå››é‡é¢„æµ‹ï¼ˆASQPï¼‰èƒ½å¤Ÿé€šè¿‡è¯†åˆ«æ¯ä¸ªè§‚ç‚¹çš„æ„è§æœ¯è¯­ã€æ–¹é¢æœ¯è¯­ã€æ–¹é¢ç±»åˆ«å’Œæƒ…æ„Ÿææ€§ï¼Œæ¥ä¿ƒè¿›å¯¹æ–‡æœ¬ä¸­æ‰€è¡¨è¾¾æ„è§çš„æ·±åº¦ç†è§£ã€‚ç„¶è€Œï¼Œä¸ºASQPä»»åŠ¡å…¨é¢æ ‡æ³¨è®­ç»ƒç¤ºä¾‹ä»¥å¾®è°ƒæ¨¡å‹æ˜¯ä¸€ä¸ªèµ„æºå¯†é›†å‹çš„æµç¨‹ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äº”ä¸ªä¸åŒæ•°æ®é›†ä¸Šè¿›è¡Œé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æŠ¥å‘Šçš„F1åˆ†æ•°ç•¥ä½äºä½¿ç”¨æœ€å…ˆè¿›çš„å¾®è°ƒæ¨¡å‹æ‰€è·å¾—çš„åˆ†æ•°ï¼Œä½†è¶…è¿‡äº†ä¹‹å‰æŠ¥å‘Šçš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ã€‚åœ¨Rest16é¤å…é¢†åŸŸæ•°æ®é›†çš„40ä¸ªæ ·æœ¬è®¾ç½®ä¸­ï¼ŒLLMçš„F1åˆ†æ•°è¾¾åˆ°52.46ï¼Œè€Œè¡¨ç°æœ€ä½³çš„MVPå¾®è°ƒæ–¹æ³•è¾¾åˆ°60.39ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æŠ¥å‘Šäº†ç›®æ ‡æ–¹é¢æƒ…æ„Ÿæ£€æµ‹ï¼ˆTASDï¼‰ä¸­LLMçš„è¡¨ç°ï¼Œå…¶F1åˆ†æ•°ä¹Ÿä¸å¾®è°ƒæ¨¡å‹ç›¸è¿‘ï¼Œåœ¨Rest16çš„40ä¸ªæ ·æœ¬è®¾ç½®ä¸­è¾¾åˆ°66.03ï¼Œè€ŒMVPçš„å¾—åˆ†æ˜¯72.76ã€‚è™½ç„¶äººç±»æ³¨é‡Šè€…å¯¹äºå®ç°æœ€ä½³æ€§èƒ½ä»ç„¶æ˜¯è‡³å…³é‡è¦çš„ï¼Œä½†LLMå¯ä»¥å‡å°‘ASQPä»»åŠ¡ä¸­å¯¹å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13044v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ æ–¹é¢ï¼Œå¯¹é¢å‘æƒ…æ„Ÿå››é‡é¢„æµ‹ï¼ˆASQPï¼‰ä»»åŠ¡çš„èƒ½åŠ›è¿›è¡Œäº†ç ”ç©¶ã€‚è¯¥ç ”ç©¶ä½¿ç”¨äº”ä¸ªä¸åŒçš„æ•°æ®é›†å¹¶å¯¹æ¯”å‘ç°ï¼Œè™½ç„¶LLMçš„F1å¾—åˆ†ç•¥ä½äºç»è¿‡ç²¾ç»†è®­ç»ƒçš„æ¨¡å‹ï¼Œä½†åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ æ–¹é¢è¡¨ç°ä¼˜äºå…ˆå‰æŠ¥é“ã€‚åœ¨Rest16é¤å…åŸŸæ•°æ®é›†ä¸Šï¼ŒLLMåœ¨å››åæ¬¡æ ·æœ¬çš„è®¾å®šä¸‹ï¼ŒASQPä»»åŠ¡å®ç°äº†è¾ƒé«˜çš„æ€§èƒ½ï¼Œåˆæ­¥å±•ç°äº†å‡å°‘ASQPä»»åŠ¡ä¸­éœ€è¦å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„éœ€è¦ã€‚LLMè¡¨ç°å‡ºçš„è¿™äº›ç‰¹ç‚¹å¯èƒ½å¯¹è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4b68e205d9f3b1f04ef1ec67b397935d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebcf0d1a8b222d8171b948897bc904cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a29bab5da6966b85e087ce3267b2775.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e45fe3397ae5f5b1cfb3f4dad94d853.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="B-cos-LM-Efficiently-Transforming-Pre-trained-Language-Models-for-Improved-Explainability"><a href="#B-cos-LM-Efficiently-Transforming-Pre-trained-Language-Models-for-Improved-Explainability" class="headerlink" title="B-cos LM: Efficiently Transforming Pre-trained Language Models for   Improved Explainability"></a>B-cos LM: Efficiently Transforming Pre-trained Language Models for   Improved Explainability</h2><p><strong>Authors:Yifan Wang, Sukrut Rao, Ji-Ung Lee, Mayank Jobanputra, Vera Demberg</strong></p>
<p>Post-hoc explanation methods for black-box models often struggle with faithfulness and human interpretability due to the lack of explainability in current neural models. Meanwhile, B-cos networks have been introduced to improve model explainability through architectural and computational adaptations, but their application has so far been limited to computer vision models and their associated training pipelines. In this work, we introduce B-cos LMs, i.e., B-cos networks empowered for NLP tasks. Our approach directly transforms pre-trained language models into B-cos LMs by combining B-cos conversion and task fine-tuning, improving efficiency compared to previous B-cos methods. Our automatic and human evaluation results demonstrate that B-cos LMs produce more faithful and human interpretable explanations than post hoc methods, while maintaining task performance comparable to conventional fine-tuning. Our in-depth analysis explores how B-cos LMs differ from conventionally fine-tuned models in their learning processes and explanation patterns. Finally, we provide practical guidelines for effectively building B-cos LMs based on our findings. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/bcos_lm">https://anonymous.4open.science/r/bcos_lm</a>. </p>
<blockquote>
<p>ç°æœ‰é»‘ç®±æ¨¡å‹çš„åéªŒè§£é‡Šæ–¹æ³•ç”±äºå½“å‰ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ç¼ºä¹è§£é‡Šæ€§ï¼Œå¾€å¾€åœ¨å¿ è¯šæ€§å’Œäººç±»å¯è§£é‡Šæ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚åŒæ—¶ï¼ŒB-cosç½‘ç»œå·²ç»é€šè¿‡æ¶æ„å’Œè®¡ç®—é€‚åº”æ¥æé«˜æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œä½†å…¶åº”ç”¨ä»…é™äºè®¡ç®—æœºè§†è§‰æ¨¡å‹åŠå…¶ç›¸å…³çš„è®­ç»ƒç®¡é“ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†B-cos LMï¼Œå³ç”¨äºNLPä»»åŠ¡çš„B-cosç½‘ç»œã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å°†B-cosè½¬æ¢å’Œä»»åŠ¡å¾®è°ƒç›¸ç»“åˆï¼Œç›´æ¥å°†é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºB-cos LMï¼Œä¸ä¹‹å‰çš„B-cosæ–¹æ³•ç›¸æ¯”ï¼Œæé«˜äº†æ•ˆç‡ã€‚æˆ‘ä»¬çš„è‡ªåŠ¨å’Œäººä¸ºè¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒB-cos LMäº§ç”Ÿçš„è§£é‡Šæ¯”åéªŒæ–¹æ³•æ›´å¿ è¯šå’Œæ˜“äºäººç±»ç†è§£ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡æ€§èƒ½ä¸å¸¸è§„å¾®è°ƒç›¸å½“ã€‚æˆ‘ä»¬çš„æ·±å…¥åˆ†ææ¢è®¨äº†B-cos LMä¸å¸¸è§„å¾®è°ƒæ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹å’Œè§£é‡Šæ¨¡å¼ä¸Šçš„å·®å¼‚ã€‚æœ€åï¼Œæˆ‘ä»¬æ ¹æ®ç ”ç©¶ç»“æœæä¾›äº†æœ‰æ•ˆæ„å»ºB-cos LMçš„å®ç”¨æŒ‡å—ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/bcos_lm%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/bcos_lmæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12992v1">PDF</a> 20 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>B-cosç½‘ç»œé€šè¿‡æ¶æ„å’Œè®¡ç®—é€‚åº”æ€§çš„æ”¹è¿›æé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œä½†å…¶åº”ç”¨ä»…é™äºè®¡ç®—æœºè§†è§‰æ¨¡å‹å’Œç›¸å…³çš„è®­ç»ƒæµç¨‹ã€‚æœ¬ç ”ç©¶å°†B-cosç½‘ç»œæ‰©å±•åˆ°è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œæå‡ºB-cos LMsã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆB-cosè½¬æ¢å’Œä»»åŠ¡å¾®è°ƒï¼Œå°†é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ç›´æ¥è½¬æ¢ä¸ºB-cos LMsï¼Œæé«˜äº†æ•ˆç‡ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒB-cos LMsäº§ç”Ÿçš„è§£é‡Šæ¯”äº‹åæ–¹æ³•æ›´å¿ å®å’Œæ˜“äºäººç±»ç†è§£ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡æ€§èƒ½ä¸å¸¸è§„å¾®è°ƒç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰ç¥ç»ç½‘ç»œæ¨¡å‹ç¼ºä¹è§£é‡Šæ€§ï¼Œå¯¼è‡´é»‘ç®±æ¨¡å‹çš„äº‹åè§£é‡Šæ–¹æ³•é¢ä¸´å¿ å®æ€§å’Œäººç±»è§£é‡Šæ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>B-cosç½‘ç»œé€šè¿‡æ¶æ„å’Œè®¡ç®—é€‚åº”æ€§çš„æ”¹è¿›æé«˜äº†æ¨¡å‹è§£é‡Šæ€§ï¼Œä½†åº”ç”¨å±€é™äºè®¡ç®—æœºè§†è§‰æ¨¡å‹å’Œè®­ç»ƒæµç¨‹ã€‚</li>
<li>æœ¬ç ”ç©¶é¦–æ¬¡å°†B-cosç½‘ç»œæ‰©å±•åˆ°è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œæå‡ºB-cos LMsï¼Œå°†é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹è½¬æ¢ä¸ºæ›´æ˜“äºè§£é‡Šçš„æ¨¡å‹ã€‚</li>
<li>B-cos LMsé€šè¿‡ç»“åˆB-cosè½¬æ¢å’Œä»»åŠ¡å¾®è°ƒæ¥æé«˜æ•ˆç‡ï¼Œäº§ç”Ÿçš„è§£é‡Šæ¯”äº‹åæ–¹æ³•æ›´å¿ å®å’Œæ˜“äºäººç±»ç†è§£ã€‚</li>
<li>B-cos LMsåœ¨ä»»åŠ¡æ€§èƒ½ä¸Šä¸å¸¸è§„å¾®è°ƒç›¸å½“ã€‚</li>
<li>ç ”ç©¶å¯¹B-cos LMsçš„å­¦ä¹ è¿‡ç¨‹å’Œè§£é‡Šæ¨¡å¼è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶å‘ç°å…¶ä¸å¸¸è§„å¾®è°ƒæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68b763249a15cd0e57709250ff0f4e0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96f1e0290c2faca5e5d0e1e25a420f97.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bc5e7cffeffbdc66cc3aa81648a80e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01e07f1377194643b3cf082d0393efc5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-250ec57062671a7a57e4b3b4ac76a226.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Corrupted-but-Not-Broken-Rethinking-the-Impact-of-Corrupted-Data-in-Visual-Instruction-Tuning"><a href="#Corrupted-but-Not-Broken-Rethinking-the-Impact-of-Corrupted-Data-in-Visual-Instruction-Tuning" class="headerlink" title="Corrupted but Not Broken: Rethinking the Impact of Corrupted Data in   Visual Instruction Tuning"></a>Corrupted but Not Broken: Rethinking the Impact of Corrupted Data in   Visual Instruction Tuning</h2><p><strong>Authors:Yunhao Gou, Hansi Yang, Zhili Liu, Kai Chen, Yihan Zeng, Lanqing Hong, Zhenguo Li, Qun Liu, James T. Kwok, Yu Zhang</strong></p>
<p>Visual Instruction Tuning (VIT) enhances Multimodal Large Language Models (MLLMs) but it is hindered by corrupted datasets containing hallucinated content, incorrect responses, and poor OCR quality. While prior works focus on dataset refinement through high-quality data collection or rule-based filtering, they are costly or limited to specific types of corruption. To deeply understand how corrupted data affects MLLMs, in this paper, we systematically investigate this issue and find that while corrupted data degrades the performance of MLLMs, its effects are largely superficial in that the performance of MLLMs can be largely restored by either disabling a small subset of parameters or post-training with a small amount of clean data. Additionally, corrupted MLLMs exhibit improved ability to distinguish clean samples from corrupted ones, enabling the dataset cleaning without external help. Based on those insights, we propose a corruption-robust training paradigm combining self-validation and post-training, which significantly outperforms existing corruption mitigation strategies. </p>
<blockquote>
<p>è§†è§‰æŒ‡ä»¤å¾®è°ƒï¼ˆVITï¼‰è™½ç„¶å¯ä»¥å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ€§èƒ½ï¼Œä½†å—åˆ°åŒ…å«è™šæ„å†…å®¹ã€é”™è¯¯å“åº”å’ŒOCRè´¨é‡å·®çš„è…è´¥æ•°æ®é›†çš„é˜»ç¢ã€‚è™½ç„¶ä»¥å‰çš„å·¥ä½œä¾§é‡äºé€šè¿‡é«˜è´¨é‡çš„æ•°æ®æ”¶é›†æˆ–åŸºäºè§„åˆ™è¿‡æ»¤æ¥æ”¹è¿›æ•°æ®é›†ï¼Œä½†å®ƒä»¬æˆæœ¬é«˜æ˜‚æˆ–ä»…é™äºç‰¹å®šç±»å‹çš„è…è´¥æ•°æ®ã€‚ä¸ºäº†æ·±å…¥äº†è§£è…è´¥æ•°æ®å¦‚ä½•å½±å“MLLMsï¼Œæœ¬æ–‡ç³»ç»Ÿåœ°ç ”ç©¶äº†è¿™ä¸ªé—®é¢˜ï¼Œå‘ç°è™½ç„¶è…è´¥æ•°æ®ä¼šé™ä½MLLMsçš„æ€§èƒ½ï¼Œä½†å…¶å½±å“ä¸»è¦æ˜¯è¡¨é¢çš„ï¼Œé€šè¿‡ç¦ç”¨ä¸€å°éƒ¨åˆ†å‚æ•°æˆ–åœ¨å°‘é‡æ¸…æ´æ•°æ®ä¸Šè¿›è¡Œåè®­ç»ƒï¼Œå¯ä»¥å¤§éƒ¨åˆ†æ¢å¤MLLMsçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¢«æ±¡æŸ“çš„MLLMsè¡¨ç°å‡ºåŒºåˆ†æ¸…æ´æ ·æœ¬å’Œå—æ±¡æŸ“æ ·æœ¬çš„èƒ½åŠ›æœ‰æ‰€æé«˜ï¼Œä»è€Œå¯ä»¥åœ¨æ— éœ€å¤–éƒ¨å¸®åŠ©çš„æƒ…å†µä¸‹è¿›è¡Œæ•°æ®é›†æ¸…ç†ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆè‡ªéªŒè¯å’Œåè®­ç»ƒçš„é”™è¯¯é²æ£’æ€§è®­ç»ƒèŒƒå¼ï¼Œè¯¥èŒƒå¼æ˜¾è‘—ä¼˜äºç°æœ‰çš„é”™è¯¯ç¼“è§£ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆVITï¼‰å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å½±å“ï¼ŒæŒ‡å‡ºå…¶å—åˆ°è…èš€æ•°æ®é›†çš„åˆ¶çº¦ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶ä¾§é‡äºé€šè¿‡é«˜è´¨é‡çš„æ•°æ®æ”¶é›†æˆ–åŸºäºè§„åˆ™çš„æ•°æ®è¿‡æ»¤æ¥æ”¹è¿›æ•°æ®é›†ï¼Œä½†è¿™ç§æ–¹æ³•æˆæœ¬é«˜æ˜‚æˆ–ä»…é™äºç‰¹å®šçš„è…è´¥ç±»å‹ã€‚æœ¬æ–‡é€šè¿‡ç³»ç»Ÿåœ°ç ”ç©¶è…èš€æ•°æ®å¯¹MLLMsçš„å½±å“ï¼Œå‘ç°è…èš€æ•°æ®è™½ç„¶ä¼šé™ä½MLLMsçš„æ€§èƒ½ï¼Œä½†å½±å“ä¸»è¦æ˜¯è¡¨é¢çš„ï¼Œå¯ä»¥é€šè¿‡ç¦ç”¨ä¸€å°éƒ¨åˆ†å‚æ•°æˆ–è¿›è¡Œå°‘é‡æ¸…æ´æ•°æ®çš„åè®­ç»ƒæ¥æ¢å¤å…¶æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè…èš€çš„MLLMsè¡¨ç°å‡ºåŒºåˆ†æ¸…æ´æ ·æœ¬å’Œè…èš€æ ·æœ¬çš„èƒ½åŠ›å¢å¼ºï¼Œå¯å®ç°æ— éœ€å¤–éƒ¨å¸®åŠ©çš„è‡ªåŠ¨æ•°æ®é›†æ¸…æ´—ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæœ¬æ–‡æå‡ºäº†ç»“åˆè‡ªéªŒè¯å’Œåè®­ç»ƒçš„æŠ—è…èš€è®­ç»ƒæ¨¡å¼ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„è…èš€ç¼“è§£ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆVITï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­èµ·åˆ°å¢å¼ºä½œç”¨ï¼Œä½†å—åˆ°è…èš€æ•°æ®é›†çš„åˆ¶çº¦ã€‚</li>
<li>è…èš€æ•°æ®ä¼šé™ä½MLLMsçš„æ€§èƒ½ï¼Œä½†å½±å“æ˜¯è¡¨é¢çš„ï¼Œå¯é€šè¿‡ç¦ç”¨éƒ¨åˆ†å‚æ•°æˆ–åè®­ç»ƒæ¢å¤ã€‚</li>
<li>è…èš€çš„MLLMsèƒ½æ›´å‡†ç¡®åœ°è¯†åˆ«æ¸…æ´æ ·æœ¬å’Œè…èš€æ ·æœ¬ï¼Œå®ç°æ•°æ®é›†çš„è‡ªæ¸…æ´—ã€‚</li>
<li>ç°æœ‰æ•°æ®å‡€åŒ–ç­–ç•¥æˆæœ¬é«˜æ˜‚æˆ–ä»…é™äºç‰¹å®šç±»å‹çš„æ•°æ®è…èš€ã€‚</li>
<li>ç³»ç»Ÿæ€§åœ°ç ”ç©¶è…èš€æ•°æ®å¯¹MLLMsçš„å½±å“æ˜¯ç†è§£å’Œè§£å†³æ•°æ®è…èš€é—®é¢˜çš„å…³é”®ã€‚</li>
<li>ç»“åˆè‡ªéªŒè¯å’Œåè®­ç»ƒçš„æŠ—è…èš€è®­ç»ƒæ¨¡å¼åœ¨ç¼“è§£æ•°æ®è…èš€é—®é¢˜ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90043bac425723c0b9050cb855ab6a55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-040a1dd694cabdd72906dbc6f1112b1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80075faa5d0416d6b56acaacc00bfbf4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74d2a1d20a89f38e01490565482f17a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6714ae1c4e1eb3d3c50bb320c2412c8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DeepResonance-Enhancing-Multimodal-Music-Understanding-via-Music-centric-Multi-way-Instruction-Tuning"><a href="#DeepResonance-Enhancing-Multimodal-Music-Understanding-via-Music-centric-Multi-way-Instruction-Tuning" class="headerlink" title="DeepResonance: Enhancing Multimodal Music Understanding via   Music-centric Multi-way Instruction Tuning"></a>DeepResonance: Enhancing Multimodal Music Understanding via   Music-centric Multi-way Instruction Tuning</h2><p><strong>Authors:Zhuoyuan Mao, Mengjie Zhao, Qiyu Wu, Hiromi Wakaki, Yuki Mitsufuji</strong></p>
<p>Recent advancements in music large language models (LLMs) have significantly improved music understanding tasks, which involve the modelâ€™s ability to analyze and interpret various musical elements. These improvements primarily focused on integrating both music and text inputs. However, the potential of incorporating additional modalities such as images, videos and textual music features to enhance music understanding remains unexplored. To bridge this gap, we propose DeepResonance, a multimodal music understanding LLM fine-tuned via multi-way instruction tuning with multi-way aligned music, text, image, and video data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and Music4way-Any2T, three 4-way training and evaluation datasets designed to enable DeepResonance to integrate both visual and textual music feature content. We also introduce multi-sampled ImageBind embeddings and a pre-alignment Transformer to enhance modality fusion prior to input into text LLMs, tailoring DeepResonance for multi-way instruction tuning. Our model achieves state-of-the-art performances across six music understanding tasks, highlighting the benefits of the auxiliary modalities and the structural superiority of DeepResonance. We plan to open-source the models and the newly constructed datasets. </p>
<blockquote>
<p>è¿‘æœŸéŸ³ä¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æå¤§åœ°æå‡äº†éŸ³ä¹ç†è§£ä»»åŠ¡çš„èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›æ¶‰åŠæ¨¡å‹åˆ†æå’Œè§£é‡Šå„ç§éŸ³ä¹å…ƒç´ çš„èƒ½åŠ›ã€‚è¿™äº›è¿›æ­¥ä¸»è¦èšç„¦äºæ•´åˆéŸ³ä¹å’Œæ–‡æœ¬è¾“å…¥ã€‚ç„¶è€Œï¼Œå°†å›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬éŸ³ä¹ç‰¹å¾ç­‰é¢å¤–æ¨¡å¼çº³å…¥å…¶ä¸­ï¼Œä»¥å¢å¼ºå¯¹éŸ³ä¹çš„ç†è§£ï¼Œè¿™ä¸€æ½œåŠ›å°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†DeepResonanceï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡å¼éŸ³ä¹ç†è§£LLMï¼Œé€šè¿‡å¤šå‘æŒ‡ä»¤è°ƒæ•´ä¸å¤šå‘å¯¹é½çš„éŸ³ä¹ã€æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘æ•°æ®è¿›è¡Œå¾®è°ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†Music4way-MI2Tã€Music4way-MV2Tå’ŒMusic4way-Any2Tä¸‰ä¸ªæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸ºäº†èƒ½å¤Ÿè®©DeepResonanceæ•´åˆè§†è§‰å’Œæ–‡æœ¬éŸ³ä¹ç‰¹å¾å†…å®¹è€Œè®¾è®¡çš„4å‘è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šé‡‡æ ·ImageBindåµŒå…¥å’Œé¢„å¯¹é½Transformerï¼Œä»¥å¢å¼ºæ¨¡æ€èåˆï¼Œç„¶åè¾“å…¥æ–‡æœ¬LLMï¼Œä¸ºDeepResonanceè¿›è¡Œå¤šå‘æŒ‡ä»¤è°ƒæ•´é‡èº«æ‰“é€ ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å…­é¡¹éŸ³ä¹ç†è§£ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œçªå‡ºäº†è¾…åŠ©æ¨¡å¼çš„ç›Šå¤„å’ŒDeepResonanceçš„ç»“æ„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬è®¡åˆ’å…¬å¼€æ¨¡å‹å’Œæ–°å»ºçš„æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12623v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ€è¿‘éŸ³ä¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å·²æ˜¾è‘—æé«˜éŸ³ä¹ç†è§£ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæ¶‰åŠæ¨¡å‹åˆ†æå’Œè§£é‡Šå„ç§éŸ³ä¹å…ƒç´ ã€‚ä¸»è¦æ”¹è¿›åœ¨äºæ•´åˆéŸ³ä¹å’Œæ–‡æœ¬è¾“å…¥ã€‚ç„¶è€Œï¼Œèå…¥å›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬éŸ³ä¹ç‰¹å¾ç­‰é¢å¤–æ¨¡æ€çš„æ½œåŠ›å°šæœªè¢«æ¢ç´¢ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºéŸ³ä¹ç†è§£ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæå‡ºDeepResonanceå¤šæ¨¡æ€éŸ³ä¹ç†è§£LLMï¼Œé€šè¿‡å¤šå‘æŒ‡ä»¤è°ƒæ•´ä¸å¤šå‘å¯¹é½çš„éŸ³ä¹ã€æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘æ•°æ®å¾®è°ƒã€‚ä¸ºæ­¤ï¼Œæ„å»ºäº†Music4way-MI2Tã€Music4way-MV2Tå’ŒMusic4way-Any2Tä¸‰ä¸ª4å‘è®­ç»ƒä¸è¯„ä¼°æ•°æ®é›†ï¼Œä½¿DeepResonanceèƒ½å¤Ÿæ•´åˆè§†è§‰å’Œæ–‡æœ¬éŸ³ä¹ç‰¹å¾å†…å®¹ã€‚è¿˜å¼•å…¥äº†å¤šé‡‡æ ·ImageBindåµŒå…¥å’Œé¢„å¯¹é½Transformerï¼Œä»¥å¢å¼ºæ¨¡æ€èåˆï¼Œç„¶åè¾“å…¥æ–‡æœ¬LLMï¼Œä¸ºDeepResonanceçš„å¤šå‘æŒ‡ä»¤è°ƒæ•´é‡èº«å®šåšã€‚è¯¥æ¨¡å‹åœ¨å…­é¡¹éŸ³ä¹ç†è§£ä»»åŠ¡ä¸Šå–å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œçªæ˜¾è¾…åŠ©æ¨¡æ€çš„ç›Šå¤„å’ŒDeepResonanceçš„ç»“æ„ä¼˜è¶Šæ€§ã€‚è®¡åˆ’å¼€æºæ¨¡å‹å’Œæ–°å»ºæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³ä¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éŸ³ä¹ç†è§£ä»»åŠ¡ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ï¼Œæ¶‰åŠåˆ†æå’Œè§£é‡ŠéŸ³ä¹å…ƒç´ çš„èƒ½åŠ›ã€‚</li>
<li>ä¸»è¦çš„æ”¹è¿›åœ¨äºæ•´åˆéŸ³ä¹å’Œæ–‡æœ¬è¾“å…¥ã€‚</li>
<li>ç›®å‰å°šæœªæ¢ç´¢èå…¥å›¾åƒã€è§†é¢‘ç­‰é¢å¤–æ¨¡æ€çš„æ½œåŠ›ä»¥å¢å¼ºéŸ³ä¹ç†è§£ã€‚</li>
<li>æå‡ºDeepResonanceå¤šæ¨¡æ€éŸ³ä¹ç†è§£LLMï¼Œé€šè¿‡å¤šå‘æŒ‡ä»¤è°ƒæ•´ä¸å¤šæ¨¡æ€æ•°æ®å¾®è°ƒæ¥æé«˜æ€§èƒ½ã€‚</li>
<li>æ„å»ºäº†ä¸‰ä¸ª4å‘è®­ç»ƒä¸è¯„ä¼°æ•°æ®é›†ï¼Œä»¥æ•´åˆè§†è§‰å’Œæ–‡æœ¬éŸ³ä¹ç‰¹å¾å†…å®¹ã€‚</li>
<li>å¼•å…¥å¤šé‡‡æ ·ImageBindåµŒå…¥å’Œé¢„å¯¹é½Transformerï¼Œä»¥å¢å¼ºæ¨¡æ€èåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-321fced2be1b87f8eaef0369281ab384.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b635368c1eae1b0b5d8c43c92859131d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea0b51967c2c111949d921ea37c7bd25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de35bebe2cec5f4fd029994e48236a30.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dffa8adbf2a0daa0a47f3796a10ac276.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PASER-Post-Training-Data-Selection-for-Efficient-Pruned-Large-Language-Model-Recovery"><a href="#PASER-Post-Training-Data-Selection-for-Efficient-Pruned-Large-Language-Model-Recovery" class="headerlink" title="PASER: Post-Training Data Selection for Efficient Pruned Large Language   Model Recovery"></a>PASER: Post-Training Data Selection for Efficient Pruned Large Language   Model Recovery</h2><p><strong>Authors:Bowei He, Lihao Yin, Hui-Ling Zhen, Xiaokun Zhang, Mingxuan Yuan, Chen Ma</strong></p>
<p>Model pruning is an effective approach for compressing large language models. However, this process often leads to significant degradation of model capabilities. While post-training techniques such as instruction tuning are commonly employed to recover model performance, existing methods often overlook the uneven deterioration of model capabilities and incur high computational costs. Moreover, some instruction data irrelevant to model capability recovery may introduce negative effects. To address these challenges, we propose the \textbf{P}ost-training d\textbf{A}ta \textbf{S}election method for \textbf{E}fficient pruned large language model \textbf{R}ecovery (\textbf{PASER}). PASER aims to identify instructions where model capabilities are most severely compromised within a certain recovery data budget. Our approach first applies manifold learning and spectral clustering to group recovery data in the semantic space, revealing capability-specific instruction sets. We then adaptively allocate the data budget to different clusters based on the degrees of model capability degradation. In each cluster, we prioritize data samples where model performance has declined dramatically. To mitigate potential negative transfer, we also detect and filter out conflicting or irrelevant recovery data. Extensive experiments demonstrate that PASER significantly outperforms conventional baselines, effectively recovering the general capabilities of pruned LLMs while utilizing merely 4%-20% of the original post-training data. </p>
<blockquote>
<p>æ¨¡å‹å‰ªææ˜¯å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™ä¸ªè¿‡ç¨‹å¾€å¾€ä¼šå¯¼è‡´æ¨¡å‹èƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚è™½ç„¶è®­ç»ƒåæŠ€æœ¯ï¼ˆå¦‚æŒ‡ä»¤å¾®è°ƒï¼‰é€šå¸¸è¢«ç”¨æ¥æ¢å¤æ¨¡å‹æ€§èƒ½ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†æ¨¡å‹èƒ½åŠ›çš„ä¸å‡åŒ€é€€åŒ–ï¼Œå¹¶äº§ç”Ÿäº†å¾ˆé«˜çš„è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œä¸€äº›ä¸æ¨¡å‹èƒ½åŠ›æ¢å¤æ— å…³çš„æŒ‡ä»¤æ•°æ®å¯èƒ½ä¼šäº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘é«˜æ•ˆå‰ªæå¤§å‹è¯­è¨€æ¨¡å‹æ¢å¤çš„<strong>PASER</strong>ï¼ˆè®­ç»ƒåæ•°æ®é€‰æ‹©æ–¹æ³•ï¼‰ã€‚PASERæ—¨åœ¨è¯†åˆ«åœ¨ç‰¹å®šæ¢å¤æ•°æ®é¢„ç®—ä¸‹æ¨¡å‹èƒ½åŠ›å—æŸæœ€ä¸¥é‡çš„æŒ‡ä»¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåº”ç”¨æµå½¢å­¦ä¹ å’Œè°±èšç±»å°†æ¢å¤æ•°æ®åœ¨è¯­ä¹‰ç©ºé—´ä¸­è¿›è¡Œåˆ†ç»„ï¼Œæ­ç¤ºç‰¹å®šèƒ½åŠ›çš„æŒ‡ä»¤é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®æ¨¡å‹èƒ½åŠ›çš„é€€åŒ–ç¨‹åº¦è‡ªé€‚åº”åœ°åˆ†é…æ•°æ®é¢„ç®—åˆ°ä¸åŒçš„é›†ç¾¤ã€‚åœ¨æ¯ä¸ªé›†ç¾¤ä¸­ï¼Œæˆ‘ä»¬ä¼˜å…ˆå¤„ç†æ¨¡å‹æ€§èƒ½æ€¥å‰§ä¸‹é™çš„æ•°æ®æ ·æœ¬ã€‚ä¸ºäº†å‡å°‘æ½œåœ¨çš„è´Ÿé¢è¿ç§»ï¼Œæˆ‘ä»¬è¿˜æ£€æµ‹å’Œè¿‡æ»¤å‡ºå†²çªæˆ–æ— å…³çš„æ¢å¤æ•°æ®ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPASERæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œåœ¨åˆ©ç”¨ä»…4%~20%çš„åŸå§‹è®­ç»ƒåæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°æ¢å¤äº†å‰ªæå¤§å‹è¯­è¨€æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12594v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹å‰ªææ˜¯å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†ä¼šå¯¼è‡´æ¨¡å‹èƒ½åŠ›æ˜¾è‘—ä¸‹é™ã€‚ä¸ºæ¢å¤æ¨¡å‹æ€§èƒ½ï¼Œé€šå¸¸é‡‡ç”¨è®­ç»ƒåæŠ€æœ¯å¦‚æŒ‡ä»¤å¾®è°ƒï¼Œä½†ç°æœ‰æ–¹æ³•å¿½è§†äº†æ¨¡å‹èƒ½åŠ›çš„ä¸å‡è¡¡ä¸‹é™ï¼Œè®¡ç®—æˆæœ¬é«˜ï¼Œä¸”å¯èƒ½å¼•å…¥ä¸æ¨¡å‹èƒ½åŠ›æ¢å¤ä¸ç›¸å…³çš„æŒ‡ä»¤æ•°æ®å¯¼è‡´è´Ÿé¢å½±å“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹é«˜æ•ˆå‰ªæå¤§å‹è¯­è¨€æ¨¡å‹æ¢å¤çš„åè®­ç»ƒæ•°æ®é€‰æ‹©æ–¹æ³•ï¼ˆPASERï¼‰ã€‚PASERæ—¨åœ¨è¯†åˆ«åœ¨ç‰¹å®šæ¢å¤æ•°æ®é¢„ç®—ä¸­æ¨¡å‹èƒ½åŠ›å—æŸæœ€ä¸¥é‡çš„æŒ‡ä»¤ã€‚å®ƒé¦–å…ˆåº”ç”¨æµå½¢å­¦ä¹ å’Œè°±èšç±»å°†æ¢å¤æ•°æ®åœ¨è¯­ä¹‰ç©ºé—´ä¸­è¿›è¡Œåˆ†ç»„ï¼Œæ­ç¤ºç‰¹å®šèƒ½åŠ›çš„æŒ‡ä»¤é›†ã€‚ç„¶åï¼Œæ ¹æ®æ¨¡å‹èƒ½åŠ›ä¸‹é™çš„ç¨‹åº¦è‡ªé€‚åº”åœ°åˆ†é…æ•°æ®é¢„ç®—åˆ°ä¸åŒçš„é›†ç¾¤ã€‚åœ¨æ¯ä¸ªé›†ç¾¤ä¸­ï¼Œæˆ‘ä»¬ä¼˜å…ˆå¤„ç†æ¨¡å‹æ€§èƒ½æ€¥å‰§ä¸‹é™çš„æ•°æ®æ ·æœ¬ã€‚ä¸ºå‡è½»æ½œåœ¨çš„è´Ÿé¢è¿ç§»ï¼Œæˆ‘ä»¬è¿˜æ£€æµ‹å’Œè¿‡æ»¤å‡ºå†²çªçš„æˆ–æ— å…³çš„æ¢å¤æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒPASERæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œåœ¨åˆ©ç”¨ä»…4%-20%çš„åŸè®­ç»ƒåæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°æ¢å¤äº†å‰ªæLLMçš„ä¸€èˆ¬èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹å‰ªææ˜¯å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†ä¼šå¯¼è‡´æ¨¡å‹èƒ½åŠ›ä¸‹é™ã€‚</li>
<li>ç°æœ‰æ¨¡å‹æ¢å¤æ–¹æ³•å¿½è§†äº†æ¨¡å‹èƒ½åŠ›çš„ä¸å‡è¡¡ä¸‹é™ï¼Œå¹¶å¯èƒ½å¼•å…¥ä¸ç›¸å…³æŒ‡ä»¤æ•°æ®ã€‚</li>
<li>PASERæ–¹æ³•é€šè¿‡è¯†åˆ«æ¨¡å‹èƒ½åŠ›å—æŸæœ€ä¸¥é‡çš„æŒ‡ä»¤æ¥æ¢å¤æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>PASERä½¿ç”¨æµå½¢å­¦ä¹ å’Œè°±èšç±»åœ¨è¯­ä¹‰ç©ºé—´åˆ†ç»„æ¢å¤æ•°æ®ï¼Œä»¥æ­ç¤ºç‰¹å®šèƒ½åŠ›çš„æŒ‡ä»¤é›†ã€‚</li>
<li>PASERæ ¹æ®æ¨¡å‹èƒ½åŠ›ä¸‹é™çš„ç¨‹åº¦è‡ªé€‚åº”åˆ†é…æ•°æ®é¢„ç®—ã€‚</li>
<li>PASERèƒ½ä¼˜å…ˆå¤„ç†æ¨¡å‹æ€§èƒ½æ€¥å‰§ä¸‹é™çš„æ•°æ®æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12594">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-774efd57a46de594c5180bbbcdb3454a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5555ac95c7aaedb75cdf46666d510cc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-286f52e8e57bfc94700a85e341b3d121.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RealSyn-An-Effective-and-Scalable-Multimodal-Interleaved-Document-Transformation-Paradigm"><a href="#RealSyn-An-Effective-and-Scalable-Multimodal-Interleaved-Document-Transformation-Paradigm" class="headerlink" title="RealSyn: An Effective and Scalable Multimodal Interleaved Document   Transformation Paradigm"></a>RealSyn: An Effective and Scalable Multimodal Interleaved Document   Transformation Paradigm</h2><p><strong>Authors:Tiancheng Gu, Kaicheng Yang, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, Jiankang Deng</strong></p>
<p>After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of non-paired data, such as multimodal interleaved documents, remains underutilized for vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. Extensive experiments demonstrate that RealSyn effectively advances vision-language representation learning and exhibits strong scalability. Models pre-trained on RealSyn achieve state-of-the-art performance on multiple downstream tasks. To facilitate future research, the RealSyn dataset and pre-trained model weights are released at <a target="_blank" rel="noopener" href="https://github.com/deepglint/RealSyn">https://github.com/deepglint/RealSyn</a>. </p>
<blockquote>
<p>åœ¨å¤§é‡å›¾æ–‡å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒåï¼Œå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºæœ‰å¸Œæœ›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§é‡çš„éé…å¯¹æ•°æ®ï¼Œå¦‚å¤šæ¨¡å¼äº¤ç»‡çš„æ–‡æ¡£ï¼Œåœ¨è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ä¸­ä»æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™äº›æœªé…å¯¹çš„æ–‡æ¡£ï¼Œæˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªçœŸå®ä¸–ç•Œæ•°æ®æå–ç®¡é“ï¼Œä»¥æå–é«˜è´¨é‡çš„å›¾ç‰‡å’Œæ–‡æœ¬ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ†å±‚æ£€ç´¢æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåœ°å°†æ¯å¼ å›¾åƒä¸å¤šä¸ªè¯­ä¹‰ä¸Šç›¸å…³çš„çœŸå®æ–‡æœ¬å…³è”èµ·æ¥ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºç²¾ç»†ç²’åº¦çš„è§†è§‰ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå›¾åƒè¯­ä¹‰å¢å¼ºç”Ÿæˆæ¨¡å—æ¥è¿›è¡Œåˆæˆæ–‡æœ¬ç”Ÿäº§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è¯­ä¹‰å¹³è¡¡é‡‡æ ·ç­–ç•¥ï¼Œä»¥æé«˜æ•°æ®é›†å¤šæ ·æ€§ï¼Œå®ç°é•¿å°¾æ¦‚å¿µæ›´å¥½çš„å­¦ä¹ ã€‚åŸºäºè¿™äº›åˆ›æ–°ï¼Œæˆ‘ä»¬æ„å»ºäº†RealSynæ•°æ®é›†ï¼Œç»“åˆäº†çœŸå®å’Œåˆæˆæ–‡æœ¬ï¼Œæä¾›ä¸‰ç§è§„æ¨¡ï¼š15Mã€30Må’Œ100Mã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRealSynæœ‰æ•ˆåœ°æ¨åŠ¨äº†è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ çš„å‘å±•ï¼Œå¹¶è¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§ã€‚åœ¨RealSynä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºäº†æ–¹ä¾¿æœªæ¥ç ”ç©¶ï¼ŒRealSynæ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹æƒé‡å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/deepglint/RealSyn%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/deepglint/RealSynå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12513v1">PDF</a> 16 pages, 12 figures, Webpage: <a target="_blank" rel="noopener" href="https://garygutc.github.io/RealSyn">https://garygutc.github.io/RealSyn</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºContrastive Language-Image Pre-trainingï¼ˆCLIPï¼‰æ¨¡å‹çš„é¢„è®­ç»ƒæŠ€æœ¯è™½ç„¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åˆ©ç”¨éé…å¯¹æ•°æ®ï¼ˆå¦‚å¤šåª’ä½“æ··åˆæ–‡æ¡£ï¼‰è¿›è¡Œè§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ æ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†RealSynæ•°æ®é›†ï¼Œé€šè¿‡çœŸå®ä¸–ç•Œæ•°æ®æå–ç®¡é“æå–é«˜è´¨é‡å›¾åƒå’Œæ–‡æœ¬ï¼Œé‡‡ç”¨åˆ†å±‚æ£€ç´¢æ–¹æ³•é«˜æ•ˆåœ°å°†å›¾åƒä¸å¤šä¸ªè¯­ä¹‰ç›¸å…³çš„çœŸå®æ–‡æœ¬ç›¸å…³è”ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†å›¾åƒè¯­ä¹‰å¢å¼ºç”Ÿæˆæ¨¡å—ç”¨äºåˆæˆæ–‡æœ¬ç”Ÿäº§ï¼Œå¹¶é‡‡ç”¨è¯­ä¹‰å¹³è¡¡é‡‡æ ·ç­–ç•¥æé«˜æ•°æ®é›†å¤šæ ·æ€§ï¼Œæ›´å¥½åœ°å­¦ä¹ é•¿å°¾æ¦‚å¿µã€‚RealSynæ•°æ®é›†åŒ…å«çœŸå®å’Œåˆæˆæ–‡æœ¬ï¼Œæä¾›ä¸‰ç§è§„æ¨¡ï¼š15Mã€30Må’Œ100Mã€‚å®éªŒè¡¨æ˜ï¼ŒRealSynæœ‰æ•ˆæ¨è¿›äº†è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§ã€‚åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼ŒåŸºäºRealSyné¢„è®­ç»ƒçš„æ¨¡å‹è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºä¾¿äºæœªæ¥ç ”ç©¶ï¼ŒRealSynæ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹æƒé‡å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆ©ç”¨éé…å¯¹æ•°æ®è¿›è¡Œè§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæ¨å‡ºRealSynæ•°æ®é›†ä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŒ…å«çœŸå®å’Œåˆæˆæ–‡æœ¬ï¼Œæä¾›ä¸åŒè§„æ¨¡é€‰æ‹©ã€‚</li>
<li>RealSyné‡‡ç”¨çœŸå®ä¸–ç•Œæ•°æ®æå–ç®¡é“å’Œåˆ†å±‚æ£€ç´¢æ–¹æ³•ï¼Œé«˜æ•ˆå…³è”å›¾åƒä¸è¯­ä¹‰ç›¸å…³æ–‡æœ¬ã€‚</li>
<li>æå‡ºå›¾åƒè¯­ä¹‰å¢å¼ºç”Ÿæˆæ¨¡å—å’Œè¯­ä¹‰å¹³è¡¡é‡‡æ ·ç­–ç•¥ï¼Œæé«˜æ•°æ®é›†å¤šæ ·æ€§å’Œå­¦ä¹ æ•ˆæœã€‚</li>
<li>RealSynèƒ½æœ‰æ•ˆæ¨è¿›è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>åŸºäºRealSyné¢„è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3e3923aa6e9616b16758a19e3bc3990.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d29ec6cf9e819ffba68f0cae8183f137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5704f48b08ec9f15480494b3937db332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-953baed58bf252e543579ebc34a909b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e47d7ac2a2ebe4874411656d7468ed64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8b35f25b580ce3d2815f5a67cd88a87.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GLoT-A-Novel-Gated-Logarithmic-Transformer-for-Efficient-Sign-Language-Translation"><a href="#GLoT-A-Novel-Gated-Logarithmic-Transformer-for-Efficient-Sign-Language-Translation" class="headerlink" title="GLoT: A Novel Gated-Logarithmic Transformer for Efficient Sign Language   Translation"></a>GLoT: A Novel Gated-Logarithmic Transformer for Efficient Sign Language   Translation</h2><p><strong>Authors:Nada Shahin, Leila Ismail</strong></p>
<p>Machine Translation has played a critical role in reducing language barriers, but its adaptation for Sign Language Machine Translation (SLMT) has been less explored. Existing works on SLMT mostly use the Transformer neural network which exhibits low performance due to the dynamic nature of the sign language. In this paper, we propose a novel Gated-Logarithmic Transformer (GLoT) that captures the long-term temporal dependencies of the sign language as a time-series data. We perform a comprehensive evaluation of GloT with the transformer and transformer-fusion models as a baseline, for Sign-to-Gloss-to-Text translation. Our results demonstrate that GLoT consistently outperforms the other models across all metrics. These findings underscore its potential to address the communication challenges faced by the Deaf and Hard of Hearing community. </p>
<blockquote>
<p>æœºå™¨ç¿»è¯‘åœ¨æ¶ˆé™¤è¯­è¨€éšœç¢æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ï¼Œä½†å…¶åœ¨æ‰‹è¯­æœºå™¨ç¿»è¯‘ï¼ˆSLMTï¼‰æ–¹é¢çš„åº”ç”¨å´é²œæœ‰ç ”ç©¶ã€‚ç°æœ‰çš„SLMTç ”ç©¶å¤§å¤šä½¿ç”¨Transformerç¥ç»ç½‘ç»œï¼Œç”±äºæ‰‹è¯­çš„åŠ¨æ€æ€§ï¼Œå…¶è¡¨ç°è¾ƒå·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„Gated-Logarithmic Transformerï¼ˆGLoTï¼‰ï¼Œå®ƒèƒ½å¤Ÿæ•æ‰æ‰‹è¯­ä½œä¸ºæ—¶é—´åºåˆ—æ•°æ®çš„é•¿æœŸæ—¶é—´ä¾èµ–æ€§ã€‚æˆ‘ä»¬å¯¹GLoTè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œä»¥Transformerå’ŒTransformerèåˆæ¨¡å‹ä½œä¸ºåŸºçº¿ï¼Œè¿›è¡ŒSign-to-Gloss-to-Textç¿»è¯‘ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜ï¼Œåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šï¼ŒGLoTå§‹ç»ˆä¼˜äºå…¶ä»–æ¨¡å‹ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†å…¶åœ¨è§£å†³è‹å“‘å’Œå¬åŠ›å—æŸç¾¤ä½“æ‰€é¢ä¸´çš„æ²Ÿé€šæŒ‘æˆ˜æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12223v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æœºå™¨ç¿»è¯‘åœ¨å‡å°‘è¯­è¨€éšœç¢æ–¹é¢çš„å…³é”®ä½œç”¨ï¼Œä½†å¯¹è‹å“‘äººæ‰‹è¯­ç¿»è¯‘ï¼ˆSLMTï¼‰çš„åº”ç”¨ç ”ç©¶è¾ƒå°‘ã€‚ç°æœ‰çš„SLMTç ”ç©¶å¤§å¤šä½¿ç”¨Transformerç¥ç»ç½‘ç»œï¼Œç”±äºæ‰‹è¯­å…·æœ‰åŠ¨æ€æ€§ï¼Œå…¶æ€§èƒ½è¾ƒä½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é—¨æ§å¯¹æ•°å˜æ¢å™¨ï¼ˆGLoTï¼‰ï¼Œèƒ½å¤Ÿæ•æ‰æ‰‹è¯­çš„é•¿æœŸæ—¶é—´ä¾èµ–æ€§ç‰¹å¾ï¼Œå°†å…¶ä½œä¸ºæ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œåˆ†æã€‚æœ¬æ–‡ä¸å˜æ¢å™¨å’Œå˜æ¢å™¨èåˆæ¨¡å‹è¿›è¡ŒåŸºå‡†æ¯”è¾ƒï¼Œåœ¨Sign-to-Gloss-to-Textç¿»è¯‘ä¸­è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒGLoTåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨è§£å†³è‹å“‘äººå’Œå¬åŠ›å—æŸäººå£«é¢ä¸´çš„æ²Ÿé€šæŒ‘æˆ˜æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹è¯­ç¿»è¯‘åœ¨å‡å°‘è¯­è¨€éšœç¢æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†ç›¸å…³ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚</li>
<li>å½“å‰SLMTç ”ç©¶ä¸»è¦ä½¿ç”¨Transformerç¥ç»ç½‘ç»œï¼Œä½†æ€§èƒ½è¾ƒä½ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é—¨æ§å¯¹æ•°å˜æ¢å™¨ï¼ˆGLoTï¼‰ï¼Œé€‚ç”¨äºæ‰‹è¯­ç¿»è¯‘ã€‚</li>
<li>GLoTèƒ½å¤Ÿæ•æ‰æ‰‹è¯­çš„é•¿æœŸæ—¶é—´ä¾èµ–æ€§ç‰¹å¾ï¼Œå¹¶å°†å…¶è§†ä¸ºæ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œåˆ†æã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒGLoTåœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>GLoTå…·æœ‰è§£å†³è‹å“‘äººå’Œå¬åŠ›å—æŸäººå£«æ²Ÿé€šæŒ‘æˆ˜çš„å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3b72cedbd9c43e0f0e8338db3265835.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37284a7c412f6727914306018d6d7449.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be8ae514cd1b7989eb09f5d57e0dceb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebfacf55ce7e31c5d5237ce02077be9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fae5818cf15f7baa6ef53d2df88e5e5a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AI-and-the-Law-Evaluating-ChatGPTâ€™s-Performance-in-Legal-Classification"><a href="#AI-and-the-Law-Evaluating-ChatGPTâ€™s-Performance-in-Legal-Classification" class="headerlink" title="AI and the Law: Evaluating ChatGPTâ€™s Performance in Legal Classification"></a>AI and the Law: Evaluating ChatGPTâ€™s Performance in Legal Classification</h2><p><strong>Authors:Pawel Weichbroth</strong></p>
<p>The use of ChatGPT to analyze and classify evidence in criminal proceedings has been a topic of ongoing discussion. However, to the best of our knowledge, this issue has not been studied in the context of the Polish language. This study addresses this research gap by evaluating the effectiveness of ChatGPT in classifying legal cases under the Polish Penal Code. The results show excellent binary classification accuracy, with all positive and negative cases correctly categorized. In addition, a qualitative evaluation confirms that the legal basis provided for each case, along with the relevant legal content, was appropriate. The results obtained suggest that ChatGPT can effectively analyze and classify evidence while applying the appropriate legal rules. In conclusion, ChatGPT has the potential to assist interested parties in the analysis of evidence and serve as a valuable legal resource for individuals with less experience or knowledge in this area. </p>
<blockquote>
<p>ä½¿ç”¨ChatGPTæ¥åˆ†æå¹¶åˆ†ç±»åˆ‘äº‹è¯‰è®¼ä¸­çš„è¯æ®ä¸€ç›´æ˜¯æŒç»­è®¨è®ºçš„è¯é¢˜ã€‚ç„¶è€Œï¼Œæ®æˆ‘ä»¬äº†è§£ï¼Œè¿™ä¸ªé—®é¢˜åœ¨æ³¢å…°è¯­ç¯å¢ƒä¸‹å°šæœªè¢«ç ”ç©¶ã€‚æœ¬ç ”ç©¶é€šè¿‡è¯„ä¼°ChatGPTåœ¨æ ¹æ®æ³¢å…°åˆ‘æ³•åˆ†ç±»æ³•å¾‹æ¡ˆä»¶æ–¹é¢çš„æœ‰æ•ˆæ€§æ¥å¡«è¡¥è¿™ä¸€ç ”ç©¶ç©ºç™½ã€‚ç»“æœæ˜¾ç¤ºäºŒå…ƒåˆ†ç±»ç²¾åº¦æé«˜ï¼Œæ‰€æœ‰æ­£é¢å’Œè´Ÿé¢æ¡ˆä¾‹å‡è¢«æ­£ç¡®åˆ†ç±»ã€‚æ­¤å¤–ï¼Œå®šæ€§è¯„ä¼°è¯å®ï¼Œä¸ºæ¯ä¸ªæ¡ˆä¾‹æä¾›çš„æ³•å¾‹ä¾æ®ä»¥åŠç›¸å…³çš„æ³•å¾‹å†…å®¹æ˜¯æ°å½“çš„ã€‚æ‰€è·å¾—çš„ç»“æœè¡¨æ˜ï¼ŒChatGPTå¯ä»¥æœ‰æ•ˆåœ°åˆ†æå’Œåˆ†ç±»è¯æ®ï¼ŒåŒæ—¶é€‚ç”¨é€‚å½“çš„æ³•å¾‹è§„åˆ™ã€‚æ€»ä¹‹ï¼ŒChatGPTæœ‰å¸®åŠ©å„æ–¹åˆ†æè¯æ®çš„æ½œåŠ›ï¼Œå¯¹äºåœ¨æ­¤é¢†åŸŸç¼ºä¹ç»éªŒæˆ–çŸ¥è¯†çš„äººè€Œè¨€ï¼Œå®ƒæ˜¯ä¸€é¡¹æœ‰ä»·å€¼çš„æ³•å¾‹èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12193v1">PDF</a> 15 pages; 1 figure; 2 tables; 32 references</p>
<p><strong>æ‘˜è¦</strong><br>    ChatGPTåœ¨æ³¢å…°è¯­ç¯å¢ƒä¸‹çš„åˆ‘äº‹è¯‰è®¼è¯æ®åˆ†æå’Œåˆ†ç±»æ•ˆæœè¿›è¡Œè¯„ä¼°ï¼Œå±•ç°å‡ºäº†ä¼˜å¼‚çš„äºŒå…ƒåˆ†ç±»å‡†ç¡®ç‡ï¼Œæ­£è´Ÿé¢æ¡ˆä¾‹å‡å‡†ç¡®å½’ç±»ï¼Œå¹¶æä¾›æ°å½“çš„æ³•å¾‹ä¾æ®å’Œç›¸å…³å†…å®¹ã€‚ç»“æœè¡¨æ˜ChatGPTèƒ½æœ‰æ•ˆåˆ†æå¹¶åˆ†ç±»è¯æ®ï¼ŒåŒæ—¶é€‚ç”¨ç›¸å…³æ³•å¾‹è§„åˆ™ã€‚ChatGPTå…·æœ‰ååŠ©å„æ–¹åˆ†æè¯æ®çš„ä»·å€¼ï¼Œå¯¹äºç¼ºä¹ç»éªŒæˆ–çŸ¥è¯†çš„ä¸ªäººè€Œè¨€ï¼Œæ˜¯ä¸€é¡¹å®è´µçš„æ³•å¾‹èµ„æºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶å¡«è¡¥äº†ChatGPTåœ¨æ³¢å…°è¯­ç¯å¢ƒä¸‹åˆ‘äº‹è¯‰è®¼è¯æ®åˆ†æä¸åˆ†ç±»çš„ç ”ç©¶ç©ºç™½ã€‚</li>
<li>ChatGPTå±•ç°å‡ºäº†ä¼˜å¼‚çš„äºŒå…ƒåˆ†ç±»å‡†ç¡®ç‡ï¼Œæ­£è´Ÿé¢æ¡ˆä¾‹å‡å‡†ç¡®å½’ç±»ã€‚</li>
<li>ChatGPTåœ¨åˆ†æè¯æ®å’Œé€‚ç”¨æ³•å¾‹è§„åˆ™æ–¹é¢è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>ChatGPTæä¾›çš„æ³•å¾‹ä¾æ®å’Œç›¸å…³å†…å®¹å…·æœ‰æ°å½“æ€§ã€‚</li>
<li>ChatGPTå¯¹äºååŠ©å„æ–¹åˆ†æè¯æ®å…·æœ‰æ½œåœ¨ä»·å€¼ã€‚</li>
<li>ChatGPTå¯¹äºç¼ºä¹æ³•å¾‹ç»éªŒå’ŒçŸ¥è¯†çš„ä¸ªäººæ¥è¯´æ˜¯ä¸€é¡¹å®è´µçš„èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12193">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc5fb90ffd8330569f6fee92996e289b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-667d1c4d2e1021e8632ae43df4586146.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MUDDFormer-Breaking-Residual-Bottlenecks-in-Transformers-via-Multiway-Dynamic-Dense-Connections"><a href="#MUDDFormer-Breaking-Residual-Bottlenecks-in-Transformers-via-Multiway-Dynamic-Dense-Connections" class="headerlink" title="MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway   Dynamic Dense Connections"></a>MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway   Dynamic Dense Connections</h2><p><strong>Authors:Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan</strong></p>
<p>We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at <a target="_blank" rel="noopener" href="https://github.com/Caiyun-AI/MUDDFormer">https://github.com/Caiyun-AI/MUDDFormer</a> . </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†å¤šå‘åŠ¨æ€å¯†é›†ï¼ˆMUDDï¼‰è¿æ¥ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥è§£å†³å‰©ä½™è¿æ¥çš„é™åˆ¶ï¼Œå¢å¼ºTransformerä¸­çš„è·¨å±‚ä¿¡æ¯æµåŠ¨ã€‚ä¸ç°æœ‰å…·æœ‰é™æ€å’Œå…±äº«è¿æ¥æƒé‡çš„å¯†é›†è¿æ¥æ–¹æ³•ä¸åŒï¼ŒMUDDæ ¹æ®æ¯ä¸ªåºåˆ—ä½ç½®å’Œæ¯ä¸ªTransformerå—çš„è§£è€¦è¾“å…¥æµï¼ˆæŸ¥è¯¢ã€é”®ã€å€¼æˆ–æ®‹å·®ï¼‰çš„éšè—çŠ¶æ€åŠ¨æ€ç”Ÿæˆè¿æ¥æƒé‡ã€‚MUDDè¿æ¥å¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•Transformeræ¶æ„ä¸­ï¼Œä»¥åˆ›å»ºMUDDFormerã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMUDDFormeråœ¨å„ç§æ¨¡å‹æ¶æ„å’Œè§„æ¨¡çš„è¯­è¨€å»ºæ¨¡ä¸­æ˜¾è‘—ä¼˜äºTransformerï¼Œå®ç°äº†ä»¥1.8å€è‡³2.4å€çš„è®¡ç®—é‡è®­ç»ƒTransformerçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMUDDPythia-2.8Båœ¨é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸Pythia-6.9Bç›¸åŒ¹é…ï¼Œç”šè‡³åœ¨äº”æ¬¡æ‹æ‘„ç¯å¢ƒä¸­ä¸Pythia-12Bç›¸æŠ—è¡¡ï¼ŒåŒæ—¶åªå¢åŠ äº†0.23%çš„å‚æ•°å’Œ0.4%çš„è®¡ç®—é‡ã€‚JAXå’ŒPyTorchçš„ä»£ç ä»¥åŠé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Caiyun-AI/MUDDFormer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Caiyun-AI/MUDDFormeræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12170v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æå‡ºMUDDè¿æ¥ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥è§£å†³æ®‹å·®è¿æ¥çš„å±€é™æ€§å¹¶å¢å¼ºTransformerä¸­çš„è·¨å±‚ä¿¡æ¯æµã€‚MUDDèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆè¿æ¥æƒé‡ï¼Œå–å†³äºåºåˆ—ä½ç½®å¤„çš„éšè—çŠ¶æ€å’ŒTransformerå—çš„æ¯ä¸ªç‹¬ç«‹è¾“å…¥æµï¼ˆæŸ¥è¯¢ã€é”®ã€å€¼æˆ–æ®‹å·®ï¼‰ã€‚MUDDè¿æ¥å¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•Transformeræ¶æ„ä¸­ï¼Œåˆ›å»ºMUDDFormerã€‚å®éªŒè¡¨æ˜ï¼ŒMUDDFormeråœ¨å„ç§æ¨¡å‹æ¶æ„å’Œè§„æ¨¡ä¸Šçš„è¯­è¨€å»ºæ¨¡è¡¨ç°å‡ä¼˜äºTransformerï¼Œä»¥1.8X-2.4Xçš„è®¡ç®—é‡è¾¾åˆ°Transformerçš„è®­ç»ƒæ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯MUDDPythia-2.8Båœ¨é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸Pythia-6.9Bç›¸åŒ¹é…ï¼Œå¹¶åœ¨äº”é•œå¤´è®¾ç½®ä¸­ç”šè‡³ä¸Pythia-12Bç›¸ç«äº‰ï¼ŒåŒæ—¶åªå¢åŠ äº†0.23%çš„å‚æ•°å’Œ0.4%çš„è®¡ç®—é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> * MUDDè¿æ¥æ˜¯ä¸€ç§é’ˆå¯¹Transformerä¸­æ®‹å·®è¿æ¥çš„æ”¹è¿›æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºè·¨å±‚ä¿¡æ¯æµã€‚
 * MUDDè¿æ¥å¯ä»¥åŠ¨æ€ç”Ÿæˆè¿æ¥æƒé‡ï¼Œå–å†³äºåºåˆ—ä½ç½®çš„éšè—çŠ¶æ€å’ŒTransformerå—çš„æ¯ä¸ªç‹¬ç«‹è¾“å…¥æµã€‚
 * MUDDè¿æ¥å¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•Transformeræ¶æ„ä¸­ï¼Œå½¢æˆMUDDFormerã€‚
 * MUDDFormeråœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¼˜äºå„ç§Transformeræ¨¡å‹ã€‚
 * MUDDFormerä»¥è¾ƒå°‘çš„è®¡ç®—é‡å’Œå‚æ•°å®ç°äº†é«˜æ€§èƒ½ï¼Œä¾‹å¦‚MUDDPythia-2.8Bçš„æ€§èƒ½ä¸å¤§å‹æ¨¡å‹å¦‚Pythia-6.9Bå’ŒPythia-12Bç›¸å½“ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12170">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-897abc9eca9768872cc0dd6e60ecd29b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a555bd792e9e730b5b2bfd76ee6e795c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25f6447e0c99f7c88379396d567da707.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00c0e7b33698a931f33d3310e8c96029.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-baf8992eb30de7e3c159edb2ab4e1aee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5273c01a1dd8b74b9e2a18d69f7139ee.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="HybriDNA-A-Hybrid-Transformer-Mamba2-Long-Range-DNA-Language-Model"><a href="#HybriDNA-A-Hybrid-Transformer-Mamba2-Long-Range-DNA-Language-Model" class="headerlink" title="HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model"></a>HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model</h2><p><strong>Authors:Mingqian Ma, Guoqing Liu, Chuan Cao, Pan Deng, Tri Dao, Albert Gu, Peiran Jin, Zhao Yang, Yingce Xia, Renqian Luo, Pipi Hu, Zun Wang, Yuan-Jyue Chen, Haiguang Liu, Tao Qin</strong></p>
<p>Advances in natural language processing and large language models have sparked growing interest in modeling DNA, often referred to as the â€œlanguage of lifeâ€. However, DNA modeling poses unique challenges. First, it requires the ability to process ultra-long DNA sequences while preserving single-nucleotide resolution, as individual nucleotides play a critical role in DNA function. Second, success in this domain requires excelling at both generative and understanding tasks: generative tasks hold potential for therapeutic and industrial applications, while understanding tasks provide crucial insights into biological mechanisms and diseases. To address these challenges, we propose HybriDNA, a decoder-only DNA language model that incorporates a hybrid Transformer-Mamba2 architecture, seamlessly integrating the strengths of attention mechanisms with selective state-space models. This hybrid design enables HybriDNA to efficiently process DNA sequences up to 131kb in length with single-nucleotide resolution. HybriDNA achieves state-of-the-art performance across 33 DNA understanding datasets curated from the BEND, GUE, and LRB benchmarks, and demonstrates exceptional capability in generating synthetic cis-regulatory elements (CREs) with desired properties. Furthermore, we show that HybriDNA adheres to expected scaling laws, with performance improving consistently as the model scales from 300M to 3B and 7B parameters. These findings underscore HybriDNAâ€™s versatility and its potential to advance DNA research and applications, paving the way for innovations in understanding and engineering the â€œlanguage of lifeâ€. </p>
<blockquote>
<p>éšç€è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œå¯¹DNAçš„å»ºæ¨¡ï¼Œå¸¸è¢«ç§°ä¸ºâ€œç”Ÿå‘½çš„è¯­è¨€â€ï¼Œå¼•å‘äº†è¶Šæ¥è¶Šå¤šçš„å…´è¶£ã€‚ç„¶è€Œï¼ŒDNAå»ºæ¨¡å­˜åœ¨ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå®ƒéœ€è¦åœ¨ä¿æŒå•æ ¸è‹·é…¸åˆ†è¾¨ç‡çš„åŒæ—¶å¤„ç†è¶…é•¿çš„DNAåºåˆ—ï¼Œå› ä¸ºå•ä¸ªæ ¸è‹·é…¸åœ¨DNAåŠŸèƒ½ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚å…¶æ¬¡ï¼Œåœ¨è¿™ä¸ªé¢†åŸŸå–å¾—æˆåŠŸéœ€è¦åœ¨ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºè‰²ï¼šç”Ÿæˆä»»åŠ¡åœ¨æ²»ç–—å’Œå·¥ä¸šåº”ç”¨æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œè€Œç†è§£ä»»åŠ¡åˆ™æä¾›äº†å¯¹ç”Ÿç‰©æœºåˆ¶å’Œç–¾ç—…çš„æ·±åˆ»è§è§£ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†HybriDNAï¼Œè¿™æ˜¯ä¸€ä¸ªä»…è§£ç çš„DNAè¯­è¨€æ¨¡å‹ï¼Œå®ƒç»“åˆäº†æ··åˆTransformer-Mamba2æ¶æ„ï¼Œæ— ç¼é›†æˆäº†æ³¨æ„åŠ›æœºåˆ¶çš„é€‰æ‹©çŠ¶æ€ç©ºé—´æ¨¡å‹çš„ä¼˜ç‚¹ã€‚è¿™ç§æ··åˆè®¾è®¡ä½¿HybriDNAèƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†é•¿è¾¾131kbçš„DNAåºåˆ—ï¼Œå¹¶ä¿ç•™å•æ ¸è‹·é…¸åˆ†è¾¨ç‡ã€‚HybriDNAåœ¨æ¥è‡ªBENDã€GUEå’ŒLRBåŸºå‡†æµ‹è¯•çš„33ä¸ªDNAç†è§£æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æ˜¾ç¤ºå‡ºç”Ÿæˆå…·æœ‰æ‰€éœ€ç‰¹æ€§çš„åˆæˆé¡ºå¼è°ƒæ§å…ƒä»¶ï¼ˆCREsï¼‰çš„å“è¶Šèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†HybriDNAéµå¾ªé¢„æœŸçš„æ¯”ä¾‹å®šå¾‹ï¼Œéšç€æ¨¡å‹å‚æ•°ä»300Mæ‰©å±•åˆ°3Bå’Œ7Bï¼Œå…¶æ€§èƒ½ä¸€ç›´åœ¨æé«˜ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†HybriDNAçš„é€šç”¨æ€§ä»¥åŠå…¶åœ¨æ¨åŠ¨DNAç ”ç©¶ä¸åº”ç”¨æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºç†è§£å’Œå·¥ç¨‹åŒ–â€œç”Ÿå‘½çš„è¯­è¨€â€å¼€è¾Ÿäº†åˆ›æ–°ä¹‹è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10807v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hybridna-project.github.io/HybriDNA-Project/">https://hybridna-project.github.io/HybriDNA-Project/</a></p>
<p><strong>Summary</strong><br>     è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ¿€å‘äº†äººä»¬å¯¹DNAå»ºæ¨¡çš„å…´è¶£ï¼ŒDNAè¢«ç§°ä¸ºâ€œç”Ÿå‘½çš„è¯­è¨€â€ã€‚ç„¶è€Œï¼ŒDNAå»ºæ¨¡é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå®ƒéœ€è¦åœ¨ä¿æŒå•æ ¸è‹·é…¸åˆ†è¾¨ç‡çš„åŒæ—¶å¤„ç†è¶…é•¿çš„DNAåºåˆ—ã€‚å…¶æ¬¡ï¼ŒæˆåŠŸçš„DNAå»ºæ¨¡éœ€è¦åœ¨ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºè‰²ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†HybriDNAï¼Œä¸€ç§ä»…è§£ç çš„DNAè¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†Transformerå’ŒMamba2çš„æ··åˆæ¶æ„ï¼Œç»“åˆäº†æ³¨æ„åŠ›æœºåˆ¶å’Œé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹çš„ä¼˜ç‚¹ã€‚è¿™ç§æ··åˆè®¾è®¡ä½¿HybriDNAèƒ½å¤Ÿé«˜æ•ˆåœ°å¤„ç†é•¿è¾¾131kbçš„DNAåºåˆ—ï¼Œå¹¶å…·æœ‰å•æ ¸è‹·é…¸åˆ†è¾¨ç‡ã€‚HybriDNAåœ¨æ¥è‡ªBENDã€GUEå’ŒLRBåŸºå‡†æµ‹è¯•çš„33ä¸ªDNAç†è§£æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶èƒ½ç”Ÿæˆå…·æœ‰æ‰€éœ€ç‰¹æ€§çš„åˆæˆé¡ºå¼è°ƒæ§å…ƒä»¶ï¼ˆCREsï¼‰ã€‚æ­¤å¤–ï¼Œéšç€æ¨¡å‹ä»3äº¿åˆ°7äº¿å‚æ•°çš„æ‰©å±•ï¼Œå…¶æ€§èƒ½ä¸æ–­æé«˜ï¼Œè¯æ˜äº†å…¶éµå¾ªé¢„æœŸè§„æ¨¡å®šå¾‹ã€‚è¿™äº›å‘ç°çªæ˜¾äº†HybriDNAçš„é€šç”¨æ€§å’Œåœ¨æ¨åŠ¨DNAç ”ç©¶ä¸åº”ç”¨æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºç†è§£å’Œå·¥ç¨‹åŒ–â€œç”Ÿå‘½è¯­è¨€â€çš„åˆ›æ–°é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DNAå»ºæ¨¡è¢«æè¿°ä¸ºâ€œç”Ÿå‘½è¯­è¨€â€çš„å»ºæ¨¡ï¼Œå…·æœ‰ç‹¬ç‰¹æŒ‘æˆ˜ã€‚</li>
<li>å¤„ç†è¶…é•¿DNAåºåˆ—å¹¶ä¿æŒå•æ ¸è‹·é…¸åˆ†è¾¨ç‡æ˜¯DNAå»ºæ¨¡çš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>ç”Ÿæˆå’Œç†è§£ä»»åŠ¡æ˜¯DNAå»ºæ¨¡æˆåŠŸçš„ä¸¤ä¸ªå…³é”®æ–¹é¢ã€‚</li>
<li>HybriDNAæ˜¯ä¸€ä¸ªæ··åˆæ¶æ„çš„DNAè¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†Transformerå’ŒMamba2çš„ä¼˜ç‚¹ã€‚</li>
<li>HybriDNAèƒ½é«˜æ•ˆå¤„ç†é•¿è¾¾131kbçš„DNAåºåˆ—ï¼Œå…·æœ‰å•æ ¸è‹·é…¸åˆ†è¾¨ç‡ã€‚</li>
<li>åœ¨å¤šä¸ªDNAç†è§£æ•°æ®é›†ä¸Šï¼ŒHybriDNAè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†ç”Ÿæˆåˆæˆé¡ºå¼è°ƒæ§å…ƒä»¶çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdc0e8cb4c016d1bc551820f057fa655.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fd940001ac6c6153642eb392af8693b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Truth-Knows-No-Language-Evaluating-Truthfulness-Beyond-English"><a href="#Truth-Knows-No-Language-Evaluating-Truthfulness-Beyond-English" class="headerlink" title="Truth Knows No Language: Evaluating Truthfulness Beyond English"></a>Truth Knows No Language: Evaluating Truthfulness Beyond English</h2><p><strong>Authors:Blanca Calvo Figueras, Eneko Sagarzazu, Julen Etxaniz, Jeremy Barnes, Pablo Gamallo, Iria De Dios Flores, Rodrigo Agerri</strong></p>
<p>We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been conducted in English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Dataset and code are publicly available under open licenses. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªç»è¿‡ä¸“ä¸šç¿»è¯‘çš„TruthfulQAåŸºå‡†æµ‹è¯•æ‰©å±•ï¼Œè¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°å·´æ–¯å…‹è¯­ã€åŠ æ³°ç½—å°¼äºšè¯­ã€åŠ åˆ©è¥¿äºšè¯­å’Œè¥¿ç­ç‰™è¯­ä¸­çš„çœŸå®æ€§è¯„ä¼°ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çœŸå®æ€§è¯„ä¼°ä¸»è¦ç”¨è‹±è¯­è¿›è¡Œã€‚ç„¶è€Œï¼ŒLLMåœ¨ä¸åŒè¯­è¨€ä¸­ä¿æŒçœŸå®æ€§çš„èƒ½åŠ›ä»æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¯„ä¼°äº†12ç§æœ€å…ˆè¿›çš„å¤§å‹å¼€æºè¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡äººå·¥è¯„ä¼°ã€å¤šé¡¹é€‰æ‹©æŒ‡æ ‡å’ŒLLMä½œä¸ºæ³•å®˜è¯„åˆ†çš„æ–¹å¼ï¼Œå¯¹æ¯”åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶LLMåœ¨è‹±è¯­ä¸­çš„è¡¨ç°æœ€å¥½ï¼Œåœ¨å·´æ–¯å…‹è¯­ï¼ˆèµ„æºæœ€å°‘ï¼‰ä¸­çš„è¡¨ç°æœ€å·®ï¼Œä½†æ€»ä½“è€Œè¨€ï¼Œä¸åŒè¯­è¨€ä¹‹é—´çš„çœŸå®æ€§å·®å¼‚å°äºé¢„æœŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä¸å¤šé¡¹é€‰æ‹©æŒ‡æ ‡ç›¸æ¯”ï¼ŒLLMä½œä¸ºæ³•å®˜çš„è¯„åˆ†ä¸äººç±»åˆ¤æ–­æ›´ä¸ºæ¥è¿‘ï¼Œä¿¡æ¯åœ¨çœŸå®æ€§è¯„ä¼°ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜è¡¨æ˜ï¼Œæœºå™¨ç¿»è¯‘æ˜¯æ‰©å±•çœŸå®æ€§åŸºå‡†æµ‹è¯•åˆ°æ›´å¤šè¯­è¨€çš„ä¸€ç§å¯è¡Œæ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£ä¸“ä¸šç¿»è¯‘çš„æ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œé€šç”¨çŸ¥è¯†é—®é¢˜åœ¨ä¸åŒè¯­è¨€ä¸­çš„å¤„ç†æƒ…å†µæ¯”ä¸Šä¸‹æ–‡å’Œæ—¶é—´ä¾èµ–æ€§é—®é¢˜æ›´å¥½ï¼Œè¿™å¼ºè°ƒäº†éœ€è¦è¿›è¡Œè€ƒè™‘æ–‡åŒ–å’Œæ—¶é—´å˜åŒ–çœŸå®æ€§è¯„ä¼°çš„å¿…è¦æ€§ã€‚æ•°æ®é›†å’Œä»£ç å‡å·²å…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09387v2">PDF</a> 14 pages, 6 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†é’ˆå¯¹å·´æ–¯å…‹è¯­ã€åŠ æ³°ç½—å°¼äºšè¯­ã€åŠ åˆ©è¥¿äºšè¯­å’Œè¥¿ç­ç‰™è¯­çš„çœŸå®æ€§è¯„ä»·åŸºå‡†æµ‹è¯•çš„ä¸“ä¸šç¿»è¯‘æ‰©å±•ç‰ˆæœ¬ã€‚è¯¥ç ”ç©¶è¯„ä¼°äº†12ä¸ªæœ€å…ˆè¿›çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è¯­è¨€ä¸­çš„çœŸå®æ€§ä¿æŒèƒ½åŠ›ï¼Œå¯¹æ¯”äº†åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„äººè¯„ã€å¤šé€‰æŒ‡æ ‡å’ŒLLMä½œä¸ºæ³•å®˜çš„è¯„åˆ†ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMåœ¨è‹±è¯­ä¸­è¡¨ç°æœ€å¥½ï¼Œåœ¨å·´æ–¯å…‹è¯­ï¼ˆèµ„æºæœ€å°‘ï¼‰ä¸­è¡¨ç°æœ€å·®ï¼Œä½†æ€»ä½“ä¸Šçœ‹ï¼Œä¸åŒè¯­è¨€é—´çš„çœŸå®æ€§å·®å¼‚å°äºé¢„æœŸã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¡¨æ˜LLMä½œä¸ºæ³•å®˜çš„è¯„åˆ†ä¸äººç±»åˆ¤æ–­æ›´ä¸ºæ¥è¿‘ï¼Œä¿¡æ¯æ€§åœ¨çœŸå®æ€§è¯„ä¼°ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚æœºå™¨ç¿»è¯‘å¯ä½œä¸ºå°†çœŸå®æ€§åŸºå‡†æµ‹è¯•æ‰©å±•åˆ°å…¶ä»–è¯­è¨€çš„å¯è¡Œæ–¹æ³•ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£ä¸“ä¸šç¿»è¯‘çš„é€”å¾„ã€‚æœ€åï¼Œè§‚å¯Ÿåˆ°æ™®éçŸ¥è¯†çš„é—®é¢˜åœ¨è·¨è¯­è¨€å¤„ç†ä¸Šæ¯”ä¸Šä¸‹æ–‡å’Œæ—¶é—´ä¾èµ–çš„é—®é¢˜æ›´å¥½ï¼Œå¼ºè°ƒçœŸå®æ€§è¯„ä»·éœ€è¦è€ƒè™‘æ–‡åŒ–å’Œæ—¶é—´å˜åŒ–çš„é‡è¦æ€§ã€‚æ•°æ®é›†å’Œä»£ç å‡å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æ‰©å±•äº†TruthfulQAåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å·´æ–¯å…‹è¯­ã€åŠ æ³°ç½—å°¼äºšè¯­ã€åŠ åˆ©è¥¿äºšè¯­å’Œè¥¿ç­ç‰™è¯­ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„çœŸå®æ€§ã€‚</li>
<li>å¯¹æ¯”äº†12ä¸ªå…ˆè¿›çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç§è¯­è¨€ä¸­çš„è¡¨ç°ï¼Œå‘ç°ä¸åŒè¯­è¨€é—´çš„çœŸå®æ€§å·®å¼‚è¾ƒå°ã€‚</li>
<li>LLMä½œä¸ºæ³•å®˜çš„è¯„åˆ†ä¸äººç±»åˆ¤æ–­æ›´ä¸ºæ¥è¿‘ï¼Œä¿¡æ¯æ€§åœ¨çœŸå®æ€§è¯„ä¼°ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>æœºå™¨ç¿»è¯‘å¯ä½œä¸ºå°†çœŸå®æ€§åŸºå‡†æµ‹è¯•æ‰©å±•åˆ°å…¶ä»–è¯­è¨€çš„å¯è¡Œæ–¹æ³•ã€‚</li>
<li>æ™®éçŸ¥è¯†çš„é—®é¢˜åœ¨è·¨è¯­è¨€å¤„ç†ä¸Šè¡¨ç°è¾ƒå¥½ï¼Œå¼ºè°ƒçœŸå®æ€§è¯„ä»·éœ€è€ƒè™‘æ–‡åŒ–å’Œæ—¶é—´å˜åŒ–çš„é‡è¦æ€§ã€‚</li>
<li>å…¬å¼€æ•°æ®é›†å’Œä»£ç å¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-646a8a06160bb21449b906a05954c531.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2574939cec4b6ef838d5b1ba6792994.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cc3d4aa382a1b7632ab9d597e405979.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3423659a2b7cfe4ab0f36d0a5b9847d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-538b13a30e358b4a589f7953740093a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9898a320d6885aa26097938ccef93f1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2017f7c6b6b29744ce16d78a425aaf26.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Beyond-Sample-Level-Feedback-Using-Reference-Level-Feedback-to-Guide-Data-Synthesis"><a href="#Beyond-Sample-Level-Feedback-Using-Reference-Level-Feedback-to-Guide-Data-Synthesis" class="headerlink" title="Beyond Sample-Level Feedback: Using Reference-Level Feedback to Guide   Data Synthesis"></a>Beyond Sample-Level Feedback: Using Reference-Level Feedback to Guide   Data Synthesis</h2><p><strong>Authors:Shuhaib Mehri, Xiusi Chen, Heng Ji, Dilek Hakkani-TÃ¼r</strong></p>
<p>LLMs demonstrate remarkable capabilities in following natural language instructions, largely due to instruction-tuning on high-quality datasets. While synthetic data generation has emerged as a scalable approach for creating such datasets, maintaining consistent quality standards remains challenging. Recent approaches incorporate feedback to improve data quality, but typically operate at the sample level, generating and applying feedback for each response individually. In this work, we propose Reference-Level Feedback, a novel methodology that instead collects feedback based on high-quality reference samples from carefully curated seed data. We use this feedback to capture rich signals of desirable characteristics and propagate it throughout the data synthesis process. We present REFED, a dataset of 10K instruction-response pairs synthesized using such feedback. We demonstrate the effectiveness of our approach by showing that Llama-3.1-8B-Instruct finetuned on REFED achieves state-of-the-art performance among similar-sized SFT-based models on AlpacaEval 2.0 and strong results on Arena-Hard. Through extensive experiments, we show that our approach consistently outperforms traditional sample-level feedback methods with significantly fewer feedback collections and improves performance across different model architectures. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¾ç¤ºå‡ºéµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„æ˜¾è‘—èƒ½åŠ›ï¼Œè¿™ä¸»è¦å½’åŠŸäºåœ¨é«˜è´¨é‡æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ã€‚è™½ç„¶åˆæˆæ•°æ®ç”Ÿæˆå·²ç»æˆä¸ºåˆ›å»ºæ­¤ç±»æ•°æ®é›†çš„å¯æ‰©å±•æ–¹æ³•ï¼Œä½†ä¿æŒä¸€è‡´çš„è´¨é‡æ ‡å‡†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘çš„æ–¹æ³•çº³å…¥åé¦ˆä»¥æé«˜æ•°æ®è´¨é‡ï¼Œä½†é€šå¸¸åœ¨æ ·æœ¬å±‚é¢è¿è¡Œï¼Œä¸ºæ¯ä¸ªå•ç‹¬å“åº”ç”Ÿæˆå¹¶åº”ç”¨åé¦ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé«˜è´¨é‡å‚è€ƒæ ·æœ¬çš„å‚è€ƒçº§åé¦ˆï¼ˆReference-Level Feedbackï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œè¿™äº›å‚è€ƒæ ·æœ¬æ¥æºäºç²¾å¿ƒç­›é€‰çš„ç§å­æ•°æ®ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤åé¦ˆæ•è·ä¸°å¯Œçš„ç†æƒ³ç‰¹å¾ä¿¡å·å¹¶å°†å…¶ä¼ æ’­åˆ°æ•´ä¸ªæ•°æ®åˆæˆè¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬æ¨å‡ºäº†REFEDæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä½¿ç”¨æ­¤ç±»åé¦ˆåˆæˆçš„1ä¸‡æ¡æŒ‡ä»¤å“åº”å¯¹ã€‚é€šè¿‡å±•ç¤ºLLama-3.1-8B-Instructåœ¨REFEDä¸Šå¾®è°ƒåï¼Œåœ¨AlpacaEval 2.0ä¸Šå®ç°ä¸ç±»ä¼¼å¤§å°çš„åŸºäºSFTçš„æ¨¡å‹ç›¸æ¯”çš„æœ€ä½³æ€§èƒ½ä»¥åŠåœ¨Arena-Hardä¸Šçš„è‰¯å¥½ç»“æœï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„æ ·æœ¬çº§åé¦ˆæ–¹æ³•ï¼Œåœ¨æ”¶é›†è¾ƒå°‘çš„åé¦ˆçš„æƒ…å†µä¸‹ï¼Œå¹¶ä¸”æé«˜äº†ä¸åŒæ¨¡å‹æ¶æ„çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04511v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsé€šè¿‡é«˜è´¨é‡æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚å°½ç®¡åˆæˆæ•°æ®ç”Ÿæˆå·²æˆä¸ºåˆ›å»ºæ­¤ç±»æ•°æ®é›†çš„å¯æ‰©å±•æ–¹æ³•ï¼Œä½†ç»´æŒä¸€è‡´çš„è´¨é‡æ ‡å‡†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€æ–°æ–¹æ³•é‡‡ç”¨åé¦ˆæ¥æ”¹å–„æ•°æ®è´¨é‡ï¼Œé€šå¸¸åœ¨æ ·æœ¬å±‚é¢æ“ä½œï¼Œä¸ºæ¯ä¸ªå“åº”å•ç‹¬ç”Ÿæˆå’Œåº”ç”¨åé¦ˆã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç²¾å¿ƒç­›é€‰çš„ç§å­æ•°æ®ä¸­çš„é«˜è´¨é‡å‚è€ƒæ ·æœ¬æ”¶é›†åé¦ˆçš„Reference-Level Feedbackæ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ç§åé¦ˆæ¥æ•æ‰å¯å–çš„ç‰¹æ€§çš„ä¸°å¯Œä¿¡å·ï¼Œå¹¶å°†å…¶ä¼ æ’­åˆ°æ•´ä¸ªæ•°æ®åˆæˆè¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬ä»‹ç»äº†ä½¿ç”¨è¿™ç§åé¦ˆåˆæˆçš„åŒ…å«1ä¸‡æ¡æŒ‡ä»¤å“åº”å¯¹çš„REFEDæ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºREFEDå¾®è°ƒçš„Llama-3.1-8B-Instructæ¨¡å‹åœ¨AlpacaEval 2.0ä¸Šå®ç°äº†åŒç±»å°å‹SFTæ¨¡å‹çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨Arena-Hardä¸Šå–å¾—äº†è‰¯å¥½ç»“æœã€‚ä¸ä¼ ç»Ÿçš„æ ·æœ¬çº§åé¦ˆæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°æ›´ä¼˜è¶Šï¼Œèƒ½æ˜¾è‘—é™ä½åé¦ˆæ”¶é›†æˆæœ¬ï¼Œå¹¶æ”¹å–„ä¸åŒæ¨¡å‹æ¶æ„çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé€šè¿‡é«˜è´¨é‡æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒå±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›æå‡ã€‚</li>
<li>åˆæˆæ•°æ®ç”Ÿæˆæ˜¯åˆ›å»ºé«˜è´¨é‡æ•°æ®é›†çš„å¯æ‰©å±•æ–¹æ³•ï¼Œä½†ç»´æŒè´¨é‡æ ‡å‡†å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æœ€æ–°æ–¹æ³•é‡‡ç”¨æ ·æœ¬çº§åé¦ˆæ¥æ”¹å–„æ•°æ®è´¨é‡ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºReference-Level Feedbackæ–¹æ³•ï¼ŒåŸºäºé«˜è´¨é‡å‚è€ƒæ ·æœ¬æ”¶é›†åé¦ˆã€‚</li>
<li>ä½¿ç”¨æ­¤æ–¹æ³•åˆæˆçš„REFEDæ•°æ®é›†èƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒREFEDå¾®è°ƒçš„Llama-3.1-8B-Instructæ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°æ ‡å‡†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e34acd6609d20f2a262bf53bb300f298.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8055de6d664b9f46b620b803aa677d88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be357cb996cf2a343d7410b4490c132e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DiTAR-Diffusion-Transformer-Autoregressive-Modeling-for-Speech-Generation"><a href="#DiTAR-Diffusion-Transformer-Autoregressive-Modeling-for-Speech-Generation" class="headerlink" title="DiTAR: Diffusion Transformer Autoregressive Modeling for Speech   Generation"></a>DiTAR: Diffusion Transformer Autoregressive Modeling for Speech   Generation</h2><p><strong>Authors:Dongya Jia, Zhuo Chen, Jiawei Chen, Chenpeng Du, Jian Wu, Jian Cong, Xiaobin Zhuang, Chumin Li, Zhen Wei, Yuping Wang, Yuxuan Wang</strong></p>
<p>Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness. </p>
<blockquote>
<p>è¿‘æœŸæœ‰å‡ é¡¹ç ”ç©¶å°è¯•ç»“åˆæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹ï¼Œæ— éœ€ç¦»æ•£è¯­éŸ³æ ‡è®°å³å¯è‡ªå›å½’ç”Ÿæˆè¿ç»­è¯­éŸ³è¡¨ç¤ºã€‚ä½†å®ƒä»¬å¸¸é¢ä¸´è®¡ç®—è´Ÿè½½è¿‡å¤§æˆ–ç»“æœä¸ç†æƒ³ç­‰æŒ‘æˆ˜ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰©æ•£è½¬æ¢å™¨è‡ªå›å½’å»ºæ¨¡ï¼ˆDiTARï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè¯­è¨€æ¨¡å‹å’Œæ‰©æ•£è½¬æ¢å™¨çš„åŸºäºè¡¥ä¸çš„è‡ªå›å½’æ¡†æ¶ã€‚è¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†è‡ªå›å½’æ¨¡å‹å¯¹è¿ç»­æ ‡è®°çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚DiTARé‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„è¡¥ä¸ç”Ÿæˆç­–ç•¥ï¼Œè¯­è¨€æ¨¡å‹å¤„ç†èšåˆçš„è¡¥ä¸åµŒå…¥ï¼Œéšåæ‰©æ•£è½¬æ¢å™¨æ ¹æ®è¯­è¨€æ¨¡å‹çš„è¾“å‡ºç”Ÿæˆä¸‹ä¸€ä¸ªè¡¥ä¸ã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æè®®å°†æ¸©åº¦å®šä¹‰ä¸ºåœ¨åå‘æ‰©æ•£ODEä¸­å¼•å…¥å™ªå£°çš„æ—¶é—´ç‚¹ï¼Œä»¥å¹³è¡¡å¤šæ ·æ€§å’Œç¡®å®šæ€§ã€‚åœ¨å¹¿æ³›çš„è§„æ¨¡åˆ†æä¸­ï¼Œæˆ‘ä»¬è¿˜æ˜¾ç¤ºDiTARå…·æœ‰å‡ºè‰²çš„å¯æ‰©å±•æ€§ã€‚åœ¨é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆä¸­ï¼ŒDiTARåœ¨ç¨³å¥æ€§ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œè‡ªç„¶åº¦æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03930v2">PDF</a> 16 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDiffusion Transformer Autoregressive Modeling (DiTAR)çš„æ–¹æ³•ï¼Œå®ƒå°†è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£å˜å‹å™¨ç›¸ç»“åˆï¼Œé‡‡ç”¨åŸºäºè¡¥ä¸çš„è‡ªå›å½’æ¡†æ¶ï¼Œä»¥è¿ç»­ä»¤ç‰Œè‡ªå›å½’ç”Ÿæˆè¯­éŸ³è¡¨ç¤ºã€‚è¯¥æ–¹æ³•æé«˜äº†è‡ªå›å½’æ¨¡å‹å¯¹è¿ç»­ä»¤ç‰Œçš„æ•ˆç‡ï¼Œå¹¶é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚DiTARé‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥è¿›è¡Œè¡¥ä¸ç”Ÿæˆï¼Œå¹¶åˆ©ç”¨å®šä¹‰çš„å™ªå£°å¼•å…¥æ¸©åº¦æ¥å¹³è¡¡å¤šæ ·æ€§å’Œç¡®å®šæ€§ã€‚åœ¨é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆä¸­ï¼ŒDiTARåœ¨ç¨³å¥æ€§ã€è¯´è¯äººç›¸ä¼¼æ€§å’Œè‡ªç„¶æ€§æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†Diffusion Transformer Autoregressive Modelingï¼ˆDiTARï¼‰æ–¹æ³•ï¼Œç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£å˜å‹å™¨ã€‚</li>
<li>DiTARé‡‡ç”¨åŸºäºè¡¥ä¸çš„è‡ªå›å½’æ¡†æ¶ï¼Œä»¥è¿ç»­ä»¤ç‰Œç”Ÿæˆè¯­éŸ³è¡¨ç¤ºã€‚</li>
<li>åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥ç”¨äºè¡¥ä¸ç”Ÿæˆã€‚</li>
<li>é€šè¿‡å®šä¹‰çš„å™ªå£°å¼•å…¥æ¸©åº¦æ¥å¹³è¡¡æ¨¡å‹çš„å¤šæ ·æ€§å’Œç¡®å®šæ€§ã€‚</li>
<li>DiTARæé«˜äº†è‡ªå›å½’æ¨¡å‹å¯¹è¿ç»­ä»¤ç‰Œçš„æ•ˆç‡ï¼Œå¹¶é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬è¯­éŸ³ç”Ÿæˆä¸­ï¼ŒDiTARåœ¨æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨³å¥æ€§ã€è¯´è¯äººç›¸ä¼¼æ€§ç­‰æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4836e7f99351dd1aa225b0a35ea7264.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfa1a0942a07751f603ae646b5cd3489.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3223eeaa8af78853b6729b093b239b3.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="VLMaterial-Procedural-Material-Generation-with-Large-Vision-Language-Models"><a href="#VLMaterial-Procedural-Material-Generation-with-Large-Vision-Language-Models" class="headerlink" title="VLMaterial: Procedural Material Generation with Large Vision-Language   Models"></a>VLMaterial: Procedural Material Generation with Large Vision-Language   Models</h2><p><strong>Authors:Beichen Li, Rundi Wu, Armando Solar-Lezama, Changxi Zheng, Liang Shi, Bernd Bickel, Wojciech Matusik</strong></p>
<p>Procedural materials, represented as functional node graphs, are ubiquitous in computer graphics for photorealistic material appearance design. They allow users to perform intuitive and precise editing to achieve desired visual appearances. However, creating a procedural material given an input image requires professional knowledge and significant effort. In this work, we leverage the ability to convert procedural materials into standard Python programs and fine-tune a large pre-trained vision-language model (VLM) to generate such programs from input images. To enable effective fine-tuning, we also contribute an open-source procedural material dataset and propose to perform program-level augmentation by prompting another pre-trained large language model (LLM). Through extensive evaluation, we show that our method outperforms previous methods on both synthetic and real-world examples. </p>
<blockquote>
<p>ç¨‹åºåŒ–ææ–™ä»¥åŠŸèƒ½èŠ‚ç‚¹å›¾çš„å½¢å¼å‘ˆç°ï¼Œåœ¨è®¡ç®—æœºå›¾å½¢å­¦çš„çœŸå®æ„Ÿæè´¨å¤–è§‚è®¾è®¡ä¸­éšå¤„å¯è§ã€‚å®ƒä»¬å…è®¸ç”¨æˆ·è¿›è¡Œç›´è§‚å’Œç²¾ç¡®çš„ç¼–è¾‘ï¼Œä»¥è¾¾åˆ°æ‰€éœ€çš„è§†è§‰æ•ˆæœã€‚ç„¶è€Œï¼Œæ ¹æ®è¾“å…¥å›¾åƒåˆ›å»ºç¨‹åºåŒ–ææ–™éœ€è¦ä¸“ä¸šçŸ¥è¯†å¹¶ä»˜å‡ºå¤§é‡åŠªåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å°†ç¨‹åºåŒ–ææ–™è½¬æ¢ä¸ºæ ‡å‡†Pythonç¨‹åºçš„èƒ½åŠ›ï¼Œå¹¶å¾®è°ƒä¸€ä¸ªå¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä»¥ä¾¿ä»è¾“å…¥å›¾åƒç”Ÿæˆæ­¤ç±»ç¨‹åºã€‚ä¸ºäº†å®ç°æœ‰æ•ˆçš„å¾®è°ƒï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå¼€æºçš„ç¨‹åºåŒ–ææ–™æ•°æ®é›†ï¼Œå¹¶æè®®é€šè¿‡æç¤ºå¦ä¸€ä¸ªé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¿›è¡Œç¨‹åºçº§å¢å¼ºã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆå’Œç°å®ä¸–ç•Œç¤ºä¾‹ä¸Šå‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18623v2">PDF</a> ICLR 2025 Spotlight</p>
<p><strong>Summary</strong>ï¼š<br>åˆ©ç”¨åŠŸèƒ½èŠ‚ç‚¹å›¾è¡¨ç¤ºçš„è¿‡ç¨‹ææ–™åœ¨è®¡ç®—æœºå›¾å½¢å­¦ä¸­å¹¿æ³›åº”ç”¨äºé€¼çœŸçš„ææ–™å¤–è§‚è®¾è®¡ä¸­ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡ç›´è§‚å’Œç²¾ç¡®çš„ç¼–è¾‘æ¥å®ç°æœŸæœ›çš„è§†è§‰å¤–è§‚ã€‚ç„¶è€Œï¼Œæ ¹æ®è¾“å…¥å›¾åƒåˆ›å»ºè¿‡ç¨‹ææ–™éœ€è¦ä¸“ä¸šçŸ¥è¯†å¹¶ä»˜å‡ºå¤§é‡åŠªåŠ›ã€‚æœ¬ç ”ç©¶å°†è¿‡ç¨‹ææ–™è½¬æ¢ä¸ºæ ‡å‡†çš„Pythonç¨‹åºï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œå¾®è°ƒï¼Œä»è¾“å…¥å›¾åƒç”Ÿæˆæ­¤ç±»ç¨‹åºã€‚ä¸ºäº†è¿›è¡Œæœ‰æ•ˆçš„å¾®è°ƒï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå¼€æºçš„è¿‡ç¨‹ææ–™æ•°æ®é›†ï¼Œå¹¶æå‡ºé€šè¿‡æç¤ºå¦ä¸€ä¸ªé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¿›è¡Œç¨‹åºçº§å¢å¼ºã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆæˆå’Œç°å®ä¸–ç•Œç¤ºä¾‹ä¸Šå‡ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>è¿‡ç¨‹ææ–™åœ¨è®¡ç®—æœºå›¾å½¢å­¦ä¸­ç”¨äºé€¼çœŸçš„ææ–™å¤–è§‚è®¾è®¡ä¸­ï¼Œä½¿ç”¨åŠŸèƒ½èŠ‚ç‚¹å›¾è¡¨ç¤ºã€‚</li>
<li>ç”¨æˆ·å¯ä»¥é€šè¿‡ç›´è§‚å’Œç²¾ç¡®çš„ç¼–è¾‘å®ç°æœŸæœ›çš„è§†è§‰å¤–è§‚ã€‚</li>
<li>åˆ›å»ºåŸºäºè¾“å…¥å›¾åƒçš„è¿‡ç¨‹ææ–™éœ€è¦ä¸“ä¸šçŸ¥è¯†å’ŒæŠ€èƒ½ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨å°†è¿‡ç¨‹ææ–™è½¬æ¢ä¸ºPythonç¨‹åºå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œå¾®è°ƒçš„æ–¹æ³•ã€‚</li>
<li>æå‡ºä½¿ç”¨å¼€æºçš„è¿‡ç¨‹ææ–™æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åˆ©ç”¨å¦ä¸€ä¸ªé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç¨‹åºçº§å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1a6478dfaf0c93a28123724faeb858a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-562fd8d4b6beaa6e7aec3c457a7748c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-472d1ca1bf56e114f7df153a906c08eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8a9dfe294bcb83e87aafc60cf2bc1bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d75f19779afbb5898756befdc748a7eb.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-20/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-20/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-20/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-509a10fdf6c6b07c4d0432e6ded7d510.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-20  Magma A Foundation Model for Multimodal AI Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-19/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d5aa5c5a3d9b203ea4739956d17383f7.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-19  Talk Structurally, Act Hierarchically A Collaborative Framework for LLM   Multi-Agent Systems
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17982k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
