<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-23  VideoHallu Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-aede71f19068fcc235c663a2692463f6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    41 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-23-æ›´æ–°"><a href="#2025-06-23-æ›´æ–°" class="headerlink" title="2025-06-23 æ›´æ–°"></a>2025-06-23 æ›´æ–°</h1><h2 id="VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding"><a href="#VideoHallu-Evaluating-and-Mitigating-Multi-modal-Hallucinations-on-Synthetic-Video-Understanding" class="headerlink" title="VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding"></a>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on   Synthetic Video Understanding</h2><p><strong>Authors:Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, Jordan Lee Boyd-Graber</strong></p>
<p>Synthetic video generation has gained significant attention for its realism and broad applications, but remains prone to violations of common sense and physical laws. This highlights the need for reliable abnormality detectors that understand such principles and are robust to hallucinations. To address this, we introduce VideoHallu, a benchmark of over 3,000 video QA pairs built from synthetic videos generated by models like Veo2, Sora, and Kling, paired with expert-crafted counterintuitive QA to evaluate the critical thinking abilities of Multi-modal Large Language Models (MLLMs) on abnormalities that are perceptually obvious to humans but often hallucinated due to language priors. VideoHallu evaluates MLLMsâ€™ abnormality detection abilities with examples across alignment, consistency, commonsense, and physics. We benchmark SOTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen2.5-VL, Video-R1, and VideoChat-R1. We observe that these models perform well on many real-world benchmarks like MVBench and MovieChat, but still struggle with basic physics-based and commonsense reasoning in synthetic videos. We further show that post-training with Group Relative Policy Optimization (GRPO), using curriculum learning on datasets combining video QA with counterintuitive commonsense and physics reasoning over real and synthetic videos, improves MLLMsâ€™ abnormality detection and critical thinking, demonstrating the value of targeted training for improving their understanding of commonsense and physical laws. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu.git">https://github.com/zli12321/VideoHallu.git</a>. </p>
<blockquote>
<p>è§†é¢‘ç”ŸæˆæŠ€æœ¯å› å…¶çœŸå®æ„Ÿå’Œå¹¿æ³›åº”ç”¨è€Œå¤‡å—å…³æ³¨ï¼Œä½†ä»ç„¶å­˜åœ¨è¿åå¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„é—®é¢˜ã€‚è¿™å‡¸æ˜¾äº†éœ€è¦å¯é çš„å¼‚å¸¸æ£€æµ‹å™¨çš„å¿…è¦æ€§ï¼Œè¿™äº›æ£€æµ‹å™¨éœ€è¦ç†è§£è¿™äº›åŸåˆ™ï¼Œå¹¶å¯¹å¹»è§‰å…·æœ‰é²æ£’æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoHalluåŸºå‡†æµ‹è¯•ï¼Œå®ƒåŒ…å«è¶…è¿‡3000ä¸ªè§†é¢‘é—®ç­”å¯¹ï¼Œè¿™äº›è§†é¢‘æ˜¯ç”±Veo2ã€Soraå’ŒKlingç­‰æ¨¡å‹ç”Ÿæˆçš„åˆæˆè§†é¢‘ï¼Œä¸ä¸“å®¶è®¾è®¡çš„åç›´è§‰é—®ç­”ç›¸ç»“åˆï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„æ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›ï¼Œè¿™äº›å¼‚å¸¸å¯¹äººç±»æ¥è¯´æ„ŸçŸ¥æ˜æ˜¾ï¼Œä½†ç”±äºè¯­è¨€å…ˆéªŒè€Œå¸¸å¸¸å‡ºç°å¹»è§‰ã€‚VideoHallué€šè¿‡ä¸€è‡´æ€§ã€è¿è´¯æ€§ã€å¸¸è¯†å’Œç‰©ç†ç­‰å¤šä¸ªæ–¹é¢çš„ä¾‹å­æ¥è¯„ä¼°MLLMsçš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹åŒ…æ‹¬GPT-4oã€Gemini-2.5-Proã€Qwen2.5-VLã€Video-R1å’ŒVideoChat-R1åœ¨å†…çš„æœ€æ–°MLLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹åœ¨MVBenchå’ŒMovieChatç­‰ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨åˆæˆè§†é¢‘ä¸­çš„åŸºæœ¬ç‰©ç†å’Œå¸¸è¯†æ¨ç†æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œä½¿ç”¨ç»„åˆè§†é¢‘é—®ç­”ä¸åç›´è§‰å¸¸è¯†å’Œç‰©ç†æ¨ç†çš„æ•°æ®é›†è¿›è¡Œè¯¾ç¨‹å­¦ä¹ çš„åè®­ç»ƒï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå¯ä»¥æ”¹å–„MLLMsçš„å¼‚å¸¸æ£€æµ‹å’Œæ‰¹åˆ¤æ€§æ€ç»´ï¼Œè¿™è¯æ˜äº†æœ‰é’ˆå¯¹æ€§çš„è®­ç»ƒå¯¹äºæé«˜å®ƒä»¬å¯¹å¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„ç†è§£çš„ä»·å€¼ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu.git%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zli12321/VideoHallu.gitä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.01481v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆæˆè§†é¢‘ç”Ÿæˆå› å…¶çœŸå®æ„Ÿå’Œå¹¿æ³›åº”ç”¨è€Œå—åˆ°å…³æ³¨ï¼Œä½†ä»å­˜åœ¨è¿åå¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„é—®é¢˜ã€‚å› æ­¤ï¼Œéœ€è¦å¯é çš„å¼‚å¸¸æ£€æµ‹å™¨æ¥ç†è§£è¿™äº›åŸç†ï¼Œå¹¶å¯¹å¹»è§‰å…·æœ‰é²æ£’æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoHalluåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«è¶…è¿‡3000ä¸ªè§†é¢‘QAå¯¹ï¼Œç”±æ¨¡å‹ç”Ÿæˆçš„åˆæˆè§†é¢‘ä¸ä¸“å®¶è®¾è®¡çš„åç›´è§‰QAç»„æˆï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›ã€‚è¿™äº›å¼‚å¸¸å¯¹äººç±»æ¥è¯´æ˜¯æ˜¾è€Œæ˜“è§çš„ï¼Œä½†ç”±äºè¯­è¨€å…ˆéªŒçŸ¥è¯†å¾€å¾€ä¼šè¢«å¹»è§‰æ‰€å½±å“ã€‚VideoHalluè¯„ä¼°äº†MLLMsåœ¨ä¸€è‡´æ€§ã€å¸¸è¯†å’Œç‰©ç†æ–¹é¢çš„å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹åŒ…æ‹¬GPT-4oã€Gemini-2.5-Proç­‰åœ¨å†…çš„é¡¶å°–MLLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›æ¨¡å‹åœ¨MVBenchå’ŒMovieChatç­‰ç°å®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆæˆè§†é¢‘çš„åŸºäºç‰©ç†å¸¸è¯†çš„æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨æ•°æ®é›†ä¸Šç»“åˆè§†é¢‘QAä¸åç›´è§‰å¸¸è¯†å’Œç‰©ç†æ¨ç†çš„åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œè¯¾ç¨‹å­¦ä¹ ï¼Œå¯ä»¥æ”¹å–„MLLMsçš„å¼‚å¸¸æ£€æµ‹å’Œæ‰¹åˆ¤æ€§æ€ç»´ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zli12321/VideoHallu.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zli12321/VideoHallu.gitä¸Šæ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆæˆè§†é¢‘ç”Ÿæˆå­˜åœ¨è¿åå¸¸è¯†å’Œç‰©ç†å®šå¾‹çš„é—®é¢˜ï¼Œéœ€è¦å¯é çš„å¼‚å¸¸æ£€æµ‹å™¨ã€‚</li>
<li>VideoHalluåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¤„ç†å¹»è§‰çš„èƒ½åŠ›ã€‚</li>
<li>MLLMsåœ¨ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨åˆæˆè§†é¢‘çš„åŸºäºç‰©ç†å¸¸è¯†çš„æ¨ç†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡è¯¾ç¨‹å­¦ä¹ ç»“åˆè§†é¢‘QAä¸åç›´è§‰å¸¸è¯†å’Œç‰©ç†æ¨ç†çš„åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå¯ä»¥æ”¹å–„MLLMsçš„å¼‚å¸¸æ£€æµ‹å’Œæ‰¹åˆ¤æ€§æ€ç»´ã€‚</li>
<li>VideoHalluä¸ºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å¤„ç†åˆæˆè§†é¢‘æ—¶çš„æ€§èƒ½æä¾›äº†ä¸€ä¸ªé‡è¦çš„å·¥å…·ï¼Œå¼ºè°ƒäº†å¸¸è¯†å’Œç‰©ç†ç†è§£çš„é‡è¦æ€§ã€‚</li>
<li>ç›®å‰çš„è¯­è¨€æ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½éš¾ä»¥è¯†åˆ«æ˜æ˜¾çš„å¼‚å¸¸ç°è±¡ï¼Œè¿™çªæ˜¾äº†éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.01481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0733746e310ac120e88dee7a0bb4e33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34c0e565c33c36de724192494cf59dc9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd791c7ebf56cd412d8edaf50d4d8b75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c07288c94070b78a2e6183c618ed95a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d4f30a27d12edd17a705e7e98a27eba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6eba42bf105f2f69700259d4c77c4224.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RefChartQA-Grounding-Visual-Answer-on-Chart-Images-through-Instruction-Tuning"><a href="#RefChartQA-Grounding-Visual-Answer-on-Chart-Images-through-Instruction-Tuning" class="headerlink" title="RefChartQA: Grounding Visual Answer on Chart Images through Instruction   Tuning"></a>RefChartQA: Grounding Visual Answer on Chart Images through Instruction   Tuning</h2><p><strong>Authors:Alexander Vogel, Omar Moured, Yufan Chen, Jiaming Zhang, Rainer Stiefelhagen</strong></p>
<p>Recently, Vision Language Models (VLMs) have increasingly emphasized document visual grounding to achieve better human-computer interaction, accessibility, and detailed understanding. However, its application to visualizations such as charts remains under-explored due to the inherent complexity of interleaved visual-numerical relationships in chart images. Existing chart understanding methods primarily focus on answering questions without explicitly identifying the visual elements that support their predictions. To bridge this gap, we introduce RefChartQA, a novel benchmark that integrates Chart Question Answering (ChartQA) with visual grounding, enabling models to refer elements at multiple granularities within chart images. Furthermore, we conduct a comprehensive evaluation by instruction-tuning 5 state-of-the-art VLMs across different categories. Our experiments demonstrate that incorporating spatial awareness via grounding improves response accuracy by over 15%, reducing hallucinations, and improving model reliability. Additionally, we identify key factors influencing text-spatial alignment, such as architectural improvements in TinyChart, which leverages a token-merging module for enhanced feature fusion. Our dataset is open-sourced for community development and further advancements. All models and code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/moured/RefChartQA">https://github.com/moured/RefChartQA</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¶Šæ¥è¶Šå¼ºè°ƒæ–‡æ¡£çš„è§†è§‰å®šä½ï¼Œä»¥å®ç°æ›´å¥½çš„äººæœºäº¤äº’ã€å¯è®¿é—®æ€§å’Œæ·±å…¥ç†è§£ã€‚ç„¶è€Œï¼Œç”±äºå…¶å¯¹äºå›¾è¡¨å›¾åƒä¸­äº¤ç»‡çš„è§†è§‰æ•°å­—å…³ç³»çš„å†…åœ¨å¤æ‚æ€§ï¼Œå…¶åœ¨å›¾è¡¨ç­‰å¯è§†åŒ–åº”ç”¨æ–¹é¢çš„åº”ç”¨ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„å›¾è¡¨ç†è§£æ–¹æ³•ä¸»è¦å…³æ³¨å›ç­”é—®é¢˜ï¼Œè€Œæ²¡æœ‰æ˜ç¡®è¯†åˆ«æ”¯æŒå…¶é¢„æµ‹çš„è§†è§‰å…ƒç´ ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†RefChartQAï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å›¾è¡¨é—®ç­”ï¼ˆChartQAï¼‰ä¸è§†è§‰å®šä½ç›¸ç»“åˆçš„æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å›¾è¡¨å›¾åƒå†…çš„å¤šä¸ªç²’åº¦çº§åˆ«ä¸Šå‚è€ƒå…ƒç´ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æŒ‡ä»¤è°ƒæ•´ä¸åŒç±»åˆ«çš„5ä¸ªæœ€æ–°VLMsè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œé€šè¿‡å®šä½èå…¥ç©ºé—´æ„è¯†å¯ä»¥æé«˜å›ç­”å‡†ç¡®ç‡è¶…è¿‡15%ï¼Œå‡å°‘å¹»è§‰ï¼Œæé«˜æ¨¡å‹å¯é æ€§ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜ç¡®å®šäº†å½±å“æ–‡æœ¬ç©ºé—´å¯¹é½çš„å…³é”®å› ç´ ï¼Œå¦‚TinyChartçš„æ¶æ„æ”¹è¿›ï¼Œå®ƒåˆ©ç”¨ä»¤ç‰Œåˆå¹¶æ¨¡å—å¢å¼ºç‰¹å¾èåˆã€‚æˆ‘ä»¬çš„æ•°æ®é›†å·²å¼€æºä¾›ç¤¾åŒºå‘å±•å’Œè¿›ä¸€æ­¥è¿›æ­¥ã€‚æ‰€æœ‰æ¨¡å‹å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/moured/RefChartQA%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/moured/RefChartQAä¸Šå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23131v2">PDF</a> Accepted by ICDAR 2025. All models and code will be publicly   available at <a target="_blank" rel="noopener" href="https://github.com/moured/RefChartQA">https://github.com/moured/RefChartQA</a></p>
<p><strong>Summary</strong></p>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿‘å¹´æ¥è¶Šæ¥è¶Šå¼ºè°ƒæ–‡æ¡£çš„è§†è§‰å®šä½ï¼Œä»¥å®ç°æ›´å¥½çš„äººæœºäº¤äº’ã€å¯è®¿é—®æ€§å’Œæ·±å…¥ç†è§£ã€‚ç„¶è€Œï¼Œå…¶åœ¨å›¾è¡¨ç­‰å¯è§†åŒ–æ–¹é¢çš„åº”ç”¨ä»è¢«å¿½è§†ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå›¾è¡¨å›¾åƒä¸­è§†è§‰ä¸æ•°å­—å…³ç³»çš„äº¤ç»‡å¤æ‚æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RefChartQAåŸºå‡†æµ‹è¯•ï¼Œå®ƒå°†å›¾è¡¨é—®ç­”ï¼ˆChartQAï¼‰ä¸è§†è§‰å®šä½ç›¸ç»“åˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å›¾è¡¨å›¾åƒå†…çš„å¤šä¸ªç²’åº¦çº§åˆ«ä¸Šå¼•ç”¨å…ƒç´ ã€‚é€šè¿‡äº”ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ†ç±»æŒ‡ä»¤è°ƒä¼˜è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡èå…¥å®šä½æŠ€æœ¯çš„ç©ºé—´æ„è¯†èƒ½æé«˜å“åº”å‡†ç¡®ç‡è¶…è¿‡15%ï¼Œå‡å°‘äº†å¹»è§‰å¹¶æé«˜äº†æ¨¡å‹çš„å¯é æ€§ã€‚æˆ‘ä»¬è¿˜å…¬å¼€äº†æ•°æ®é›†å¹¶æä¾›äº†ç›¸å…³æ¨¡å‹å’Œä»£ç ï¼Œä»¥ä¾›ç¤¾åŒºå‘å±•å’Œè¿›ä¸€æ­¥è¿›æ­¥ã€‚å…¬å¼€æ•°æ®é›†ç½‘å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/moured/RefChartQA">https://github.com/moured/RefChartQA</a>ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMsæ­£é€æ¸é‡è§†æ–‡æ¡£è§†è§‰å®šä½ï¼Œä»¥æå‡äººæœºäº¤äº’å’Œæ·±å…¥ç†è§£ã€‚</li>
<li>å›¾è¡¨ç­‰å¯è§†åŒ–åœ¨VLMsä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>RefChartQAåŸºå‡†æµ‹è¯•ç»“åˆäº†ChartQAä¸è§†è§‰å®šä½ï¼Œä½¿æ¨¡å‹èƒ½åœ¨å›¾è¡¨å›¾åƒçš„å¤šä¸ªç²’åº¦ä¸Šå¼•ç”¨å…ƒç´ ã€‚</li>
<li>é€šè¿‡èå…¥å®šä½æŠ€æœ¯çš„ç©ºé—´æ„è¯†èƒ½æé«˜æ¨¡å‹çš„å“åº”å‡†ç¡®ç‡ã€å‡å°‘å¹»è§‰å¹¶å¢å¼ºå¯é æ€§ã€‚</li>
<li>RefChartQAæ•°æ®é›†å·²å¼€æºï¼Œä¾›ç¤¾åŒºå‘å±•å’Œè¿›ä¸€æ­¥çš„ç ”ç©¶ä¸è¿›æ­¥ã€‚</li>
<li>äº”ä¸ªé¡¶å°–çš„å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡åˆ†ç±»æŒ‡ä»¤è°ƒä¼˜è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55fb8d62b1f6b6573548b60d92cef0b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8b4289ae7d243fcc6ae5abfd779b27c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8249e1f09e765728bdb54c158a21509.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Transformers-without-Normalization"><a href="#Transformers-without-Normalization" class="headerlink" title="Transformers without Normalization"></a>Transformers without Normalization</h2><p><strong>Authors:Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu</strong></p>
<p>Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) &#x3D; \tanh(\alpha $x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks. </p>
<blockquote>
<p>æ ‡å‡†åŒ–å±‚åœ¨ç°ä»£ç¥ç»ç½‘ç»œä¸­æ— å¤„ä¸åœ¨ï¼Œå¹¶ä¸”é•¿æœŸä»¥æ¥ä¸€ç›´è¢«è®¤ä¸ºæ˜¯å¿…ä¸å¯å°‘çš„ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†ä¸ä½¿ç”¨æ ‡å‡†åŒ–çš„Transformerå¯ä»¥ä½¿ç”¨ä¸€ç§éå¸¸ç®€å•çš„æŠ€æœ¯è¾¾åˆ°ç›¸åŒæˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†Dynamic Tanhï¼ˆDyTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€å…ƒç´ æ“ä½œ$DyT(x) &#x3D; \tanh(\alpha x)$ï¼Œä½œä¸ºTransformerä¸­æ ‡å‡†åŒ–å±‚çš„å³æ’å³ç”¨æ›¿ä»£å“ã€‚DyTçš„çµæ„Ÿæ¥è‡ªäºè§‚å¯ŸTransformerä¸­çš„å±‚æ ‡å‡†åŒ–é€šå¸¸ä¼šäº§ç”Ÿç±»ä¼¼äºåŒæ›²æ­£åˆ‡çš„Så½¢è¾“å…¥è¾“å‡ºæ˜ å°„ã€‚é€šè¿‡èå…¥DyTï¼Œä¸ä½¿ç”¨æ ‡å‡†åŒ–çš„Transformerå¯ä»¥åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹åŒ¹é…æˆ–è¶…è¿‡å…¶æ ‡å‡†åŒ–å¯¹åº”æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œæ— éœ€è°ƒæ•´è¶…å‚æ•°ã€‚æˆ‘ä»¬åœ¨å¤šç§è®¾ç½®ä¸­å¯¹å¸¦æœ‰DyTçš„Transformerçš„æœ‰æ•ˆæ€§è¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬è¯†åˆ«ä¸ç”Ÿæˆã€ç›‘ç£å­¦ä¹ ä¸è‡ªæˆ‘ç›‘ç£å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰å’Œè¯­è¨€æ¨¡å‹ç­‰ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†å¸¸è§„è§‚å¿µï¼Œå³æ ‡å‡†åŒ–å±‚æ˜¯ç°ä»£ç¥ç»ç½‘ç»œä¸å¯æˆ–ç¼ºçš„ç»„æˆéƒ¨åˆ†ï¼Œå¹¶ä¸ºæ·±åº¦å­¦ä¹ ç½‘ç»œä¸­çš„æ ‡å‡†åŒ–å±‚ä½œç”¨æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10622v2">PDF</a> CVPR 2025; Project page: <a target="_blank" rel="noopener" href="https://jiachenzhu.github.io/DyT/">https://jiachenzhu.github.io/DyT/</a></p>
<p><strong>Summary</strong><br>ç°ä»£ç¥ç»ç½‘ç»œä¸­æ™®éå­˜åœ¨çš„å½’ä¸€åŒ–å±‚è¢«è®¤ä¸ºæ˜¯å¿…è¦çš„ï¼Œä½†è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä¸ä½¿ç”¨å½’ä¸€åŒ–çš„Transformerå¯ä»¥é€šè¿‡ä¸€ç§æå…¶ç®€å•çš„æ–¹æ³•å®ç°ç›¸åŒæˆ–æ›´å¥½çš„æ€§èƒ½ã€‚å¼•å…¥äº†ä¸€ç§åŠ¨æ€åŒæ›²æ­£åˆ‡ï¼ˆDyTï¼‰ä½œä¸ºTransformerä¸­å½’ä¸€åŒ–å±‚çš„æ›¿ä»£æ–¹æ¡ˆã€‚DyTå—åˆ°Transformerä¸­å±‚å½’ä¸€åŒ–é€šå¸¸äº§ç”ŸåŒæ›²æ­£åˆ‡å‹è¾“å…¥è¾“å‡ºæ˜ å°„çš„å¯å‘ã€‚é€šè¿‡é‡‡ç”¨DyTï¼Œä¸ä½¿ç”¨å½’ä¸€åŒ–çš„Transformerå¯ä»¥åŒ¹é…æˆ–è¶…è¿‡ä½¿ç”¨å½’ä¸€åŒ–çš„åŒç±»æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¤§éƒ¨åˆ†æƒ…å†µä¸‹æ— éœ€è°ƒæ•´è¶…å‚æ•°ã€‚æˆ‘ä»¬åœ¨å¤šç§è®¾ç½®ä¸‹éªŒè¯äº†ä½¿ç”¨DyTçš„Transformerçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬è¯†åˆ«ã€ç”Ÿæˆã€ç›‘ç£åˆ°è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼Œä»¥åŠè®¡ç®—æœºè§†è§‰å’Œè¯­è¨€æ¨¡å‹ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†å¸¸è§„è®¤çŸ¥ä¸­å½’ä¸€åŒ–å±‚åœ¨ç°ä»£ç¥ç»ç½‘ç»œä¸­çš„ä¸å¯æˆ–ç¼ºæ€§ï¼Œå¹¶ä¸ºæ·±å…¥äº†è§£å…¶åœ¨æ·±åº¦ç½‘ç»œä¸­çš„ä½œç”¨æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformersä¸ä½¿ç”¨å½’ä¸€åŒ–å±‚ä¹Ÿèƒ½å®ç°è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºåŠ¨æ€åŒæ›²æ­£åˆ‡ï¼ˆDyTï¼‰çš„å…ƒç´ çº§æ“ä½œä½œä¸ºTransformerä¸­å½’ä¸€åŒ–å±‚çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>DyTå—å¯å‘äºTransformerä¸­å±‚å½’ä¸€åŒ–äº§ç”Ÿçš„åŒæ›²æ­£åˆ‡å‹è¾“å…¥è¾“å‡ºæ˜ å°„ã€‚</li>
<li>ä½¿ç”¨DyTçš„Transformeråœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¸æˆ–è¶…è¿‡ä½¿ç”¨å½’ä¸€åŒ–çš„æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½¿ç”¨DyTçš„Transformeræ— éœ€è°ƒæ•´è¶…å‚æ•°ã€‚</li>
<li>ç ”ç©¶ç»“æœæŒ‘æˆ˜äº†å¸¸è§„è®¤çŸ¥ä¸­å½’ä¸€åŒ–å±‚åœ¨ç°ä»£ç¥ç»ç½‘ç»œä¸­çš„ä¸å¯æˆ–ç¼ºæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd042278cc2ecf58a06107fcea3d3440.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-135116cdb04453162606bf169bfa00c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e29d4599061eab0983b19972ad7993f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d69ee13085c533c8ea69cde08a0eb5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e6f78d9e059b84ff6fddb19e290c347.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0312f3fcf396feb49879cc0b9f83612b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb40328db8275518338f3b9eddb654e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4f820f9d190e892c9c0a68f1418e6a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4bfe828938c86bcb7885777e8f08d90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-796b4c307f63ef1478b69a7a872d96f0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="EgoBlind-Towards-Egocentric-Visual-Assistance-for-the-Blind"><a href="#EgoBlind-Towards-Egocentric-Visual-Assistance-for-the-Blind" class="headerlink" title="EgoBlind: Towards Egocentric Visual Assistance for the Blind"></a>EgoBlind: Towards Egocentric Visual Assistance for the Blind</h2><p><strong>Authors:Junbin Xiao, Nanxin Huang, Hao Qiu, Zhulin Tao, Xun Yang, Richang Hong, Meng Wang, Angela Yao</strong></p>
<p>We present EgoBlind, the first egocentric VideoQA dataset collected from blind individuals to evaluate the assistive capabilities of contemporary multimodal large language models (MLLMs). EgoBlind comprises 1,392 videos that record the daily lives of real blind users from a first-person perspective. It also features 5,311 questions directly posed or generated and verified by blind individuals to reflect their in-situation needs for visual assistance under various scenarios. We provide each question with an average of 3 reference answers to alleviate subjective evaluation. Using EgoBlind, we comprehensively evaluate 16 advanced MLLMs and find that all models struggle, with the best performers achieving accuracy near 60%, far behind human performance of 87.4%. To guide future advancements, we identify and summarize major limitations of existing MLLMs in egocentric visual assistance for the blind and explore heuristic solutions for improvement. With these efforts, we hope EgoBlind can serve as a valuable foundation for developing more effective AI assistants to enhance the independence of the blind individualsâ€™ lives. Data and evaluation code are available at <a target="_blank" rel="noopener" href="https://github.com/doc-doc/EgoBlind">https://github.com/doc-doc/EgoBlind</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†EgoBlindï¼Œè¿™æ˜¯ä»ç›²äººç¾¤ä½“æ”¶é›†çš„ç¬¬ä¸€ä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¾…åŠ©èƒ½åŠ›ã€‚EgoBlindåŒ…å«1392ä¸ªè§†é¢‘ï¼Œè¿™äº›è§†é¢‘ä»¥ç¬¬ä¸€äººç§°è§†è§’è®°å½•äº†çœŸå®ç›²äººçš„æ—¥å¸¸ç”Ÿæ´»ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†è¿˜åŒ…å«äº†ç”±ç›²äººç›´æ¥æå‡ºæˆ–ç”±ç³»ç»Ÿç”Ÿæˆå¹¶ç»è¿‡ç›²äººéªŒè¯çš„5311ä¸ªé—®é¢˜ï¼Œåæ˜ äº†å„ç§åœºæ™¯ä¸‹ç›²äººåœ¨è§†è§‰è¾…åŠ©æ–¹é¢çš„å®é™…éœ€æ±‚ã€‚é’ˆå¯¹æ¯ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æä¾›äº†å¹³å‡ä¸‰ä¸ªå‚è€ƒç­”æ¡ˆï¼Œä»¥ç¼“è§£ä¸»è§‚è¯„ä»·çš„é—®é¢˜ã€‚ä½¿ç”¨EgoBlindæ•°æ®é›†ï¼Œæˆ‘ä»¬å¯¹16ä¸ªå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°æ‰€æœ‰æ¨¡å‹éƒ½å­˜åœ¨å›°éš¾ï¼Œè¡¨ç°æœ€å¥½çš„æ¨¡å‹å‡†ç¡®ç‡æ¥è¿‘60%ï¼Œè€Œäººç±»çš„å‡†ç¡®ç‡é«˜è¾¾87.4%ã€‚ä¸ºäº†æŒ‡å¯¼æœªæ¥çš„è¿›æ­¥ï¼Œæˆ‘ä»¬æ€»ç»“äº†ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å‘ç›²äººçš„è‡ªæˆ‘ä¸­å¿ƒè§†è§‰è¾…åŠ©æ–¹é¢çš„ä¸»è¦å±€é™æ€§ï¼Œå¹¶æ¢ç´¢äº†æ”¹è¿›å¯å‘å¼è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å¸Œæœ›EgoBlindèƒ½æˆä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„AIåŠ©æ‰‹çš„æœ‰ä»·å€¼çš„åŸºç¡€ï¼Œä»¥æé«˜ç›²äººç”Ÿæ´»çš„ç‹¬ç«‹æ€§ã€‚æ•°æ®å’Œè¯„ä¼°ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/doc-doc/EgoBlind%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/doc-doc/EgoBlindè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08221v2">PDF</a> We extend and resplit the dataset</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬æ¨å‡ºäº†EgoBlindæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªé¢å‘ç›²äººçš„ç¬¬ä¸€äººç§°è§†é¢‘é—®ç­”æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾…åŠ©èƒ½åŠ›ã€‚EgoBlindåŒ…å«æ¥è‡ªçœŸå®ç›²äººæ—¥å¸¸ç”Ÿæ´»çš„1,392ä¸ªç¬¬ä¸€äººç§°è§†è§’çš„è§†é¢‘ï¼Œä»¥åŠç”±ç›²äººç›´æ¥æå‡ºæˆ–ç”Ÿæˆå¹¶éªŒè¯çš„5,311ä¸ªé—®é¢˜ï¼Œåæ˜ äº†ä¸åŒåœºæ™¯ä¸‹ä»–ä»¬å¯¹è§†è§‰è¾…åŠ©çš„éœ€æ±‚ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªé—®é¢˜æä¾›äº†å¹³å‡ä¸‰ä¸ªå‚è€ƒç­”æ¡ˆæ¥ç¼“è§£ä¸»è§‚è¯„ä»·ã€‚ä½¿ç”¨EgoBlindæ•°æ®é›†ï¼Œæˆ‘ä»¬å¯¹åå…­ä¸ªå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°æ‰€æœ‰æ¨¡å‹éƒ½å­˜åœ¨å›°éš¾ï¼Œæœ€ä½³æ¨¡å‹å‡†ç¡®ç‡æ¥è¿‘ç™¾åˆ†ä¹‹å…­åï¼Œè¿œä½äºäººç±»çš„ç™¾åˆ†ä¹‹å…«åä¸ƒç‚¹å››ã€‚æˆ‘ä»¬å¸Œæœ›EgoBlindå¯ä»¥ä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„AIåŠ©æ‰‹æä¾›æœ‰ä»·å€¼çš„åŸºç¡€ï¼Œä»¥æé«˜ç›²äººçš„ç‹¬ç«‹æ€§ã€‚æ•°æ®å’Œè¯„ä¼°ä»£ç å¯åœ¨å…¬å¼€é“¾æ¥æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>EgoBlindæ˜¯é¦–ä¸ªé’ˆå¯¹ç›²äººçš„ç¬¬ä¸€äººç§°è§†é¢‘é—®ç­”æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«æ¥è‡ªçœŸå®ç›²äººæ—¥å¸¸ç”Ÿæ´»çš„è§†é¢‘å’Œæå‡ºçš„é—®é¢˜ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾…åŠ©èƒ½åŠ›ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿™äº›é—®é¢˜æ—¶é¢ä¸´å›°éš¾ï¼Œæœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡è¿œä½äºäººç±»æ°´å¹³ã€‚</li>
<li>æ•°æ®é›†æ—¨åœ¨ä¸ºå¼€å‘æ›´æœ‰æ•ˆçš„AIåŠ©æ‰‹æä¾›åŸºç¡€ï¼Œä»¥å¢å¼ºç›²äººçš„ç‹¬ç«‹æ€§ã€‚</li>
<li>æ•°æ®é›†åŒ…å«äº†ä¸°å¯Œçš„è§†è§‰è¾…åŠ©åœºæ™¯ä¸­çš„é—®é¢˜ï¼Œä¸ºæ¨¡å‹è¯„ä¼°æä¾›äº†å¤šæ ·åŒ–çš„åœºæ™¯å’Œæ•°æ®æ”¯æŒã€‚</li>
<li>å½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¾…åŠ©æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„æ”¹è¿›å’Œåˆ›æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-127b83d8374e1823db2110fa4c37706d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-120b170b15d35ee82d64481a578928ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3ffd9f00ae65b5cb5d9d88ea2598612.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-559152d8d3a57035495bea2bc15a3dc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e10209cea97fcd707a763f09adfee06a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Inst3D-LMM-Instance-Aware-3D-Scene-Understanding-with-Multi-modal-Instruction-Tuning"><a href="#Inst3D-LMM-Instance-Aware-3D-Scene-Understanding-with-Multi-modal-Instruction-Tuning" class="headerlink" title="Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal   Instruction Tuning"></a>Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal   Instruction Tuning</h2><p><strong>Authors:Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, Jianke Zhu</strong></p>
<p>Despite encouraging progress in 3D scene understanding, it remains challenging to develop an effective Large Multi-modal Model (LMM) that is capable of understanding and reasoning in complex 3D environments. Most previous methods typically encode 3D point and 2D image features separately, neglecting interactions between 2D semantics and 3D object properties, as well as the spatial relationships within the 3D environment. This limitation not only hinders comprehensive representations of 3D scene, but also compromises training and inference efficiency. To address these challenges, we propose a unified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with multiple 3D scene understanding tasks simultaneously. To obtain the fine-grained instance-level visual tokens, we first introduce a novel Multi-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D semantics into their corresponding 3D geometric features. For scene-level relation-aware tokens, we further present a 3D Instance Spatial Relation (3D-ISR) module to capture the intricate pairwise spatial relationships among objects. Additionally, we perform end-to-end multi-task instruction tuning simultaneously without the subsequent task-specific fine-tuning. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods across 3D scene understanding, reasoning and grounding tasks. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/hanxunyu/Inst3D-LMM">https://github.com/hanxunyu/Inst3D-LMM</a> </p>
<blockquote>
<p>å°½ç®¡åœ¨3Dåœºæ™¯ç†è§£æ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„è¿›å±•ï¼Œä½†å¼€å‘èƒ½å¤Ÿåœ¨å¤æ‚3Dç¯å¢ƒä¸­è¿›è¡Œç†è§£å’Œæ¨ç†çš„æœ‰æ•ˆå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å¤§å¤šæ•°ä¹‹å‰çš„æ–¹æ³•é€šå¸¸åˆ†åˆ«ç¼–ç 3Dç‚¹å’Œ2Då›¾åƒç‰¹å¾ï¼Œå¿½ç•¥äº†2Dè¯­ä¹‰å’Œ3Då¯¹è±¡å±æ€§ä¹‹é—´çš„äº¤äº’ï¼Œä»¥åŠ3Dç¯å¢ƒå†…çš„ç©ºé—´å…³ç³»ã€‚è¿™ç§å±€é™æ€§ä¸ä»…é˜»ç¢äº†3Dåœºæ™¯çš„ç»¼åˆè¡¨ç¤ºï¼Œè¿˜å½±å“äº†è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00513v2">PDF</a> CVPR2025, Code Link: <a target="_blank" rel="noopener" href="https://github.com/hanxunyu/Inst3D-LMM">https://github.com/hanxunyu/Inst3D-LMM</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>å°½ç®¡åœ¨ä¸‰ç»´åœºæ™¯ç†è§£æ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„è¿›å±•ï¼Œä½†å¼€å‘èƒ½å¤Ÿåœ¨å¤æ‚ä¸‰ç»´ç¯å¢ƒä¸­è¿›è¡Œç†è§£å’Œæ¨ç†çš„æœ‰æ•ˆå¤§å‹å¤šæ¨¡å¼æ¨¡å‹ï¼ˆLMMï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é’ˆå¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å®ä¾‹æ„ŸçŸ¥ä¸‰ç»´å¤§å‹å¤šæ¨¡å¼æ¨¡å‹ï¼ˆInst3D-LMMï¼‰ï¼Œä»¥åŒæ—¶å¤„ç†å¤šä¸ªä¸‰ç»´åœºæ™¯ç†è§£ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥å¤šè§†å›¾è·¨æ¨¡æ€èåˆï¼ˆMCMFï¼‰æ¨¡å—ï¼Œæˆ‘ä»¬å°†å¤šè§†å›¾äºŒç»´è¯­ä¹‰æ³¨å…¥åˆ°å…¶å¯¹åº”çš„ä¸‰ç»´å‡ ä½•ç‰¹å¾ä¸­ï¼Œä»¥è·å¾—ç»†ç²’åº¦çš„å®ä¾‹çº§è§†è§‰æ ‡è®°ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ•è·ç‰©ä½“ä¹‹é—´å¤æ‚çš„ä¸€å¯¹ä¸€ç©ºé—´å…³ç³»ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç»´å®ä¾‹ç©ºé—´å…³ç³»ï¼ˆ3D-ISRï¼‰æ¨¡å—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç«¯åˆ°ç«¯å¤šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´ï¼ŒåŒæ—¶è¿›è¡Œäº†å¤šé¡¹å®éªŒï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ç»´åœºæ™¯ç†è§£ã€æ¨ç†å’Œå®šä½ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼€å‘ä¸€ç§æœ‰æ•ˆçš„å¤§å‹å¤šæ¨¡å¼æ¨¡å‹ï¼ˆLMMï¼‰ä»¥ç†è§£å¤æ‚çš„ä¸‰ç»´ç¯å¢ƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>å¤§å¤šæ•°ä»¥å‰çš„æ–¹æ³•åˆ†åˆ«ç¼–ç ä¸‰ç»´ç‚¹å’ŒäºŒç»´å›¾åƒç‰¹å¾ï¼Œå¿½ç•¥äº†äºŒç»´è¯­ä¹‰å’Œä¸‰ç»´å¯¹è±¡å±æ€§ä¹‹é—´çš„äº¤äº’ä»¥åŠä¸‰ç»´ç¯å¢ƒå†…çš„ç©ºé—´å…³ç³»ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å®ä¾‹æ„ŸçŸ¥ä¸‰ç»´å¤§å‹å¤šæ¨¡å¼æ¨¡å‹ï¼ˆInst3D-LMMï¼‰ã€‚</li>
<li>é€šè¿‡å¤šè§†å›¾è·¨æ¨¡æ€èåˆï¼ˆMCMFï¼‰æ¨¡å—æ³¨å…¥å¤šè§†å›¾äºŒç»´è¯­ä¹‰åˆ°ä¸‰ç»´å‡ ä½•ç‰¹å¾ä¸­ã€‚</li>
<li>æå‡ºäº†ä¸‰ç»´å®ä¾‹ç©ºé—´å…³ç³»ï¼ˆ3D-ISRï¼‰æ¨¡å—æ¥æ•è·ç‰©ä½“ä¹‹é—´å¤æ‚çš„ä¸€å¯¹ä¸€ç©ºé—´å…³ç³»ã€‚</li>
<li>åŒæ—¶è¿›è¡Œç«¯åˆ°ç«¯å¤šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´ï¼Œæ— éœ€åç»­çš„ä»»åŠ¡ç‰¹å®šå¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7bd5c20b4e9d1046bc9c16a464f22896.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-013e502dbdde407d4c199e6edd2e484b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08b1d0d8ad78774e002c26662a40445e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-020ecc80c8721387c2ad327205b752ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f824df1622d27712f8489f755359a28f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d667e04d5c7aa8dcd60d2ff4b4511da9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="The-Sharpness-Disparity-Principle-in-Transformers-for-Accelerating-Language-Model-Pre-Training"><a href="#The-Sharpness-Disparity-Principle-in-Transformers-for-Accelerating-Language-Model-Pre-Training" class="headerlink" title="The Sharpness Disparity Principle in Transformers for Accelerating   Language Model Pre-Training"></a>The Sharpness Disparity Principle in Transformers for Accelerating   Language Model Pre-Training</h2><p><strong>Authors:Jinbo Wang, Mingze Wang, Zhanpeng Zhou, Junchi Yan, Weinan E, Lei Wu</strong></p>
<p>Transformers consist of diverse building blocks, such as embedding layers, normalization layers, self-attention mechanisms, and point-wise feedforward networks. Thus, understanding the differences and interactions among these blocks is important. In this paper, we uncover a clear Sharpness Disparity across these blocks, which emerges early in training and intriguingly persists throughout the training process. Motivated by this finding, we propose Blockwise Learning Rate (LR), a strategy that tailors the LR to each blockâ€™s sharpness, accelerating large language model (LLM) pre-training. By integrating Blockwise LR into AdamW, we consistently achieve lower terminal loss and nearly $2\times$ speedup compared to vanilla AdamW. We demonstrate this acceleration across GPT-2 and LLaMA, with model sizes ranging from 0.12B to 2B and datasets of OpenWebText, MiniPile, and C4. Finally, we incorporate Blockwise LR into Adam-mini (Zhang et al., 2024), a recently proposed memory-efficient variant of Adam, achieving a combined $2\times$ speedup and $2\times$ memory saving. These results underscore the potential of exploiting the sharpness disparity to improve LLM training. </p>
<blockquote>
<p>Transformerç”±å¤šç§æ„å»ºå—ç»„æˆï¼Œå¦‚åµŒå…¥å±‚ã€å½’ä¸€åŒ–å±‚ã€è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œé€ç‚¹å‰é¦ˆç½‘ç»œã€‚å› æ­¤ï¼Œäº†è§£è¿™äº›æ¨¡å—ä¹‹é—´çš„å·®å¼‚å’Œäº¤äº’éå¸¸é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†è¿™äº›æ¨¡å—ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„å°–é”åº¦å·®å¼‚ï¼ˆSharpness Disparityï¼‰ï¼Œè¿™ç§å·®å¼‚åœ¨è®­ç»ƒæ—©æœŸå°±ä¼šå‡ºç°ï¼Œå¹¶ä¸”ä»¤äººå›°æƒ‘çš„æ˜¯ï¼Œå®ƒä¼šä¸€ç›´å­˜åœ¨äºæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ã€‚å—è¿™ä¸€å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å—å­¦ä¹ ç‡ï¼ˆLRï¼‰ç­–ç•¥ï¼Œå³æ ¹æ®æ¯ä¸ªæ¨¡å—çš„å°–é”åº¦é‡èº«å®šåˆ¶å­¦ä¹ ç‡ï¼Œä»¥åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒã€‚é€šè¿‡å°†åˆ†å—LRé›†æˆåˆ°AdamWä¸­ï¼Œæˆ‘ä»¬å§‹ç»ˆå®ç°äº†æ›´ä½çš„ç»ˆç«¯æŸå¤±å’Œä¸æ ‡å‡†AdamWç›¸æ¯”è¿‘2å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬åœ¨GPT-2å’ŒLLaMAä¸Šå±•ç¤ºäº†è¿™ç§åŠ é€Ÿæ•ˆæœï¼Œæ¨¡å‹å¤§å°èŒƒå›´ä»0.12Båˆ°2Bï¼Œæ•°æ®é›†åŒ…æ‹¬OpenWebTextã€MiniPileå’ŒC4ã€‚æœ€åï¼Œæˆ‘ä»¬å°†åˆ†å—LRé›†æˆåˆ°Adam-miniï¼ˆZhangç­‰äººï¼Œ2024å¹´ï¼‰â€”â€”ä¸€ç§æ–°æå‡ºçš„å†…å­˜é«˜æ•ˆçš„Adamå˜ä½“ï¼Œå®ç°äº†2å€çš„é€Ÿåº¦æå‡å’Œ2å€çš„å†…å­˜èŠ‚çœã€‚è¿™äº›ç»“æœçªæ˜¾äº†åˆ©ç”¨å°–é”åº¦å·®å¼‚æ”¹è¿›LLMè®­ç»ƒçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19002v2">PDF</a> 21 pages, accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Transformeræ¨¡å‹å„ç»„ä»¶ï¼ˆå¦‚åµŒå…¥å±‚ã€å½’ä¸€åŒ–å±‚ã€è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œç‚¹å¼å‰é¦ˆç½‘ç»œï¼‰ä¹‹é—´çš„å°–é”åº¦å·®å¼‚ï¼Œå¹¶æå‡ºäº†ä¸€ç§é’ˆå¯¹æ¯ä¸ªç»„ä»¶çš„å°–é”åº¦çš„å—çº§å­¦ä¹ ç‡ï¼ˆLRï¼‰ç­–ç•¥ï¼Œä»¥åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒã€‚é€šè¿‡é›†æˆBlockwise LRåˆ°AdamWä¼˜åŒ–å™¨ï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŸå§‹çš„AdamWç›¸æ¯”ï¼Œæ–°ç­–ç•¥èƒ½å¤Ÿè¾¾åˆ°æ›´ä½çš„ç»ˆç«¯æŸå¤±å¹¶åŠ é€Ÿè¿‘2å€ã€‚æ­¤å¤–ï¼Œè¯¥ç­–ç•¥åœ¨GPT-2å’ŒLLaMAæ¨¡å‹ä¸Šè¡¨ç°å‡ºä¸€è‡´çš„åŠ é€Ÿæ•ˆæœï¼Œå¹¶åœ¨æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é›†ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œç ”ç©¶è¿˜å°†Blockwise LRé›†æˆåˆ°Adam-miniä¼˜åŒ–å™¨ä¸­ï¼Œå®ç°äº†é€Ÿåº¦å’Œå†…å­˜çš„åŒå€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹åŒ…å«å¤šç§æ„å»ºå—ï¼Œç†è§£è¿™äº›å—ä¹‹é—´çš„å·®å¼‚å’Œäº¤äº’éå¸¸é‡è¦ã€‚</li>
<li>ç ”ç©¶å‘ç°Transformeræ¨¡å‹å„ç»„ä»¶ä¹‹é—´å­˜åœ¨å°–é”åº¦å·®å¼‚ï¼Œè¿™ç§å·®å¼‚åœ¨è®­ç»ƒæ—©æœŸå°±ä¼šå‡ºç°ï¼Œå¹¶ä¼šè´¯ç©¿æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å—çº§å­¦ä¹ ç‡ï¼ˆLRï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®æ¯ä¸ªç»„ä»¶çš„å°–é”åº¦æ¥å®šåˆ¶å­¦ä¹ ç‡ï¼Œä»¥åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„è®­ç»ƒã€‚</li>
<li>é›†æˆBlockwise LRåˆ°AdamWä¼˜åŒ–å™¨ï¼Œå®ç°äº†æ¯”åŸå§‹AdamWæ›´ä½çš„ç»ˆç«¯æŸå¤±å’Œè¿‘2å€çš„åŠ é€Ÿã€‚</li>
<li>Blockwise LRç­–ç•¥åœ¨GPT-2å’ŒLLaMAæ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„åŠ é€Ÿæ•ˆæœï¼Œä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Blockwise LRç­–ç•¥åœ¨æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é›†ä¸Šçš„è¡¨ç°å…·æœ‰ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4231e0db7a07fa18d2e0e6b9d307bedc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae0614aa23e49bb814f7975db9c13319.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e00a0d25f6f5096d5c99b455a5f406f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0491cc7d240c7b7b61f7727465ad7e59.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CODESYNC-Synchronizing-Large-Language-Models-with-Dynamic-Code-Evolution-at-Scale"><a href="#CODESYNC-Synchronizing-Large-Language-Models-with-Dynamic-Code-Evolution-at-Scale" class="headerlink" title="CODESYNC: Synchronizing Large Language Models with Dynamic Code   Evolution at Scale"></a>CODESYNC: Synchronizing Large Language Models with Dynamic Code   Evolution at Scale</h2><p><strong>Authors:Chenlong Wang, Zhaoyang Chu, Zhengxiang Cheng, Xuyi Yang, Kaiyue Qiu, Yao Wan, Zhou Zhao, Xuanhua Shi, Dongping Chen</strong></p>
<p>Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMsâ€™ ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Lucky-voyage/Code-Sync">https://github.com/Lucky-voyage/Code-Sync</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨é€‚åº”ä¸æ–­æ¼”å˜çš„ä»£ç çŸ¥è¯†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å…³äºç¬¬ä¸‰æ–¹åº“APIçš„é¢‘ç¹æ›´æ–°ã€‚è¿™ä¸€å±€é™æ€§æºäºé™æ€çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œå¾€å¾€å¯¼è‡´ç”Ÿæˆçš„ä»£ç ä¸å¯æ‰§è¡Œï¼Œæˆ–åœ¨å®‰å…¨æ€§å’Œæ•ˆç‡æ–¹é¢å­˜åœ¨æ¬¡ä¼˜çš„å®ç°ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†CODESYNCï¼Œä¸€ä¸ªç”¨äºè¯†åˆ«è¿‡æ—¶ä»£ç æ¨¡å¼å¹¶ä»Pythonç¬¬ä¸‰æ–¹åº“ä¸­å®æ—¶æ”¶é›†ä»£ç çŸ¥è¯†æ›´æ–°çš„æ•°æ®å¼•æ“ã€‚åŸºäºCODESYNCï¼Œæˆ‘ä»¬å¼€å‘äº†CODESYNCBENCHï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°LLMä¸ä»£ç æ¼”åŒ–åŒæ­¥èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†æ¥è‡ªå…­ä¸ªPythonåº“çš„220ä¸ªAPIçš„å®æ—¶æ›´æ–°ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…æ‹¬ä¸‰é¡¹è¯„ä¼°ä»»åŠ¡çš„3300ä¸ªæµ‹è¯•ç”¨ä¾‹å’Œä¸€ä¸ªåŒ…å«2200ä¸ªè®­ç»ƒæ ·æœ¬çš„æ›´æ–°æ„ŸçŸ¥æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ã€‚å¯¹14ç§æœ€æ–°LLMçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå®ƒä»¬åœ¨åŠ¨æ€ä»£ç æ¼”åŒ–æ–¹é¢é¢ä¸´å›°éš¾ï¼Œå³ä½¿åœ¨å…ˆè¿›çš„çŸ¥è¯†æ›´æ–°æ–¹æ³•ï¼ˆå¦‚DPOã€ORPOå’ŒSimPOï¼‰çš„æ”¯æŒä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å¯ä»¥ä¸ºæœªæ¥å¼€å‘æ›´æœ‰æ•ˆçš„å®æ—¶ä»£ç çŸ¥è¯†æ›´æ–°æ–¹æ³•æä¾›åšå®çš„åŸºç¡€ã€‚å®éªŒä»£ç å’Œæ•°æ®é›†å¯å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Lucky-voyage/Code-Sync%E3%80%82">https://github.com/Lucky-voyage/Code-Syncã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16645v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨é€‚åº”ä¸æ–­æ¼”å˜çš„ä»£ç çŸ¥è¯†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç¬¬ä¸‰æ–¹åº“APIçš„é¢‘ç¹æ›´æ–°ã€‚æœ¬æ–‡æå‡ºCODESYNCæ•°æ®å¼•æ“å’ŒCODESYNCBENCHåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³æ­¤é—®é¢˜å¹¶è¯„ä¼°LLMåŒæ­¥é€‚åº”ä»£ç è¿›åŒ–çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æ”¯æŒå…ˆè¿›çŸ¥è¯†æ›´æ–°æ–¹æ³•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨åŠ¨æ€ä»£ç è¿›åŒ–æ–¹é¢ä¹Ÿé¢ä¸´å›°éš¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨é€‚åº”ä¸æ–­å˜åŒ–çš„ä»£ç çŸ¥è¯†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç¬¬ä¸‰æ–¹åº“APIçš„é¢‘ç¹æ›´æ–°ç»™å¤§å‹è¯­è¨€æ¨¡å‹å¸¦æ¥é€‚åº”éš¾é¢˜ã€‚</li>
<li>CODESYNCæ•°æ®å¼•æ“ç”¨äºè¯†åˆ«è¿‡æ—¶çš„ä»£ç æ¨¡å¼å’Œä»Pythonç¬¬ä¸‰æ–¹åº“æ”¶é›†å®æ—¶ä»£ç çŸ¥è¯†æ›´æ–°ã€‚</li>
<li>CODESYNCBENCHåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åŒæ­¥é€‚åº”ä»£ç è¿›åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†æ¥è‡ªå…­ä¸ªPythonåº“çš„220ä¸ªAPIçš„çœŸå®ä¸–ç•Œæ›´æ–°ï¼Œæä¾›3300ä¸ªæµ‹è¯•ç”¨ä¾‹å’Œæ›´æ–°æ„ŸçŸ¥æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚</li>
<li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯æ”¯æŒå…ˆè¿›çŸ¥è¯†æ›´æ–°æ–¹æ³•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨åŠ¨æ€ä»£ç è¿›åŒ–æ–¹é¢ä»é¢ä¸´å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fcee6d9cfd8d42b34ea0ba230e02c4aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be6605f3e969aa1602d8c4b8fa81bce9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-405c02fb06221a7d7310b9c97359e328.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12bed1bf8df066f687bb07a3b9902929.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c077784ca1d054fc17094b61cb9584f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efc9e34c491e72de41ad7f59a5870804.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Truth-Knows-No-Language-Evaluating-Truthfulness-Beyond-English"><a href="#Truth-Knows-No-Language-Evaluating-Truthfulness-Beyond-English" class="headerlink" title="Truth Knows No Language: Evaluating Truthfulness Beyond English"></a>Truth Knows No Language: Evaluating Truthfulness Beyond English</h2><p><strong>Authors:Blanca Calvo Figueras, Eneko Sagarzazu, Julen Etxaniz, Jeremy Barnes, Pablo Gamallo, Iria De Dios Flores, Rodrigo Agerri</strong></p>
<p>We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been conducted in English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Dataset and code are publicly available under open licenses. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªä¸“ä¸šç¿»è¯‘çš„TruthfulQAåŸºå‡†æµ‹è¯•æ‰©å±•ç‰ˆï¼Œè¯¥ç‰ˆæœ¬æ—¨åœ¨è¯„ä¼°å·´æ–¯å…‹è¯­ã€åŠ æ³°ç½—å°¼äºšè¯­ã€åŠ åˆ©è¥¿äºšè¯­å’Œè¥¿ç­ç‰™è¯­ä¸­çš„çœŸå®æ€§è¯„ä¼°ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çœŸå®æ€§è¯„ä¼°ä¸»è¦éƒ½åœ¨è‹±è¯­ä¸­è¿›è¡Œã€‚ç„¶è€Œï¼ŒLLMåœ¨ä¸åŒè¯­è¨€ä¸­ä¿æŒçœŸå®æ€§çš„èƒ½åŠ›ä»å¾…æ¢ç´¢ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¯„ä¼°äº†12ç§æœ€å…ˆè¿›çš„å¼€æºLLMï¼Œé€šè¿‡äººå·¥è¯„ä¼°ã€å¤šé¡¹é€‰æ‹©æŒ‡æ ‡å’ŒLLM-as-a-Judgeè¯„åˆ†å¯¹æ¯”åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤ä¼˜åŒ–æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶LLMåœ¨è‹±è¯­ä¸­çš„è¡¨ç°æœ€å¥½ï¼Œåœ¨å·´æ–¯å…‹è¯­ï¼ˆèµ„æºæœ€å°‘çš„è¯­è¨€ï¼‰ä¸­çš„è¡¨ç°æœ€å·®ï¼Œä½†æ€»ä½“æ¥è¯´ï¼Œä¸åŒè¯­è¨€ä¹‹é—´çš„çœŸå®æ€§å·®å¼‚æ¯”é¢„æœŸçš„è¦å°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°LLM-as-a-Judgeä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§æ¯”å¤šé¡¹é€‰æ‹©æŒ‡æ ‡æ›´ä¸ºå¯†åˆ‡ï¼Œè€Œä¿¡æ¯æ€§åœ¨çœŸå®æ€§è¯„ä¼°ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜è¡¨æ˜ï¼Œæœºå™¨ç¿»è¯‘æ˜¯æ‰©å±•çœŸå®æ€§åŸºå‡†æµ‹è¯•åˆ°æ›´å¤šè¯­è¨€çš„ä¸€ç§å¯è¡Œæ–¹æ³•ï¼Œä¸ºä¸“ä¸šç¿»è¯‘æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ€åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œä¸ä¸Šä¸‹æ–‡å’Œæ—¶é—´ç›¸å…³çš„é€šç”¨çŸ¥è¯†é—®é¢˜åœ¨ä¸åŒè¯­è¨€ä¸­çš„å¤„ç†æƒ…å†µè¦å¥½äºå…¶ä»–ç±»å‹çš„é—®é¢˜ï¼Œè¿™å¼ºè°ƒäº†åœ¨è¿›è¡ŒçœŸå®æ€§è¯„ä¼°æ—¶éœ€è¦è€ƒè™‘åˆ°æ–‡åŒ–å’Œæ—¶é—´å˜åŒ–çš„å› ç´ ã€‚æ•°æ®é›†å’Œä»£ç å‡å…¬å¼€å¼€æ”¾è®¸å¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09387v3">PDF</a> 14 pages, 6 figures, 8 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ä¸ªä¸“ä¸šç¿»è¯‘çš„TruthfulQAåŸºå‡†æµ‹è¯•æ‰©å±•ç‰ˆæœ¬ï¼Œæ—¨åœ¨è¯„ä¼°å·´æ–¯å…‹è¯­ã€åŠ æ³°ç½—å°¼äºšè¯­ã€åŠ åˆ©è¥¿äºšè¯­å’Œè¥¿ç­ç‰™è¯­çš„çœŸå®æ€§è¯„ä¼°ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‹±æ–‡ä¸­çš„çœŸå®æ€§è¯„ä¼°å·²ç›¸å½“æ™®éï¼Œä½†LLMåœ¨ä¸åŒè¯­è¨€çš„çœŸå®æ€§ä¿æŒèƒ½åŠ›å°šå¾…ç ”ç©¶ã€‚è¯¥ç ”ç©¶è¯„ä¼°äº†12ä¸ªæœ€å…ˆè¿›çš„å¼€æºLLMï¼Œå¯¹æ¯”åŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼Œé‡‡ç”¨äººå·¥è¯„ä¼°ã€å¤šé¡¹é€‰æ‹©æŒ‡æ ‡å’ŒLLM-as-a-Judgeè¯„åˆ†æ³•ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒLLMåœ¨è‹±è¯­ä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨å·´æ–¯å…‹è¯­ï¼ˆèµ„æºæœ€å°‘ï¼‰ä¸­è¡¨ç°æœ€å·®ï¼Œä½†æ•´ä½“çœŸå®æ€§è·¨è¯­è¨€å·®å¼‚å°äºé¢„æœŸã€‚æ­¤å¤–ï¼ŒLLM-as-a-Judgeä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§é«˜äºå¤šé¡¹é€‰æ‹©æŒ‡æ ‡ï¼Œè€Œä¿¡æ¯æ€§åœ¨çœŸå®æ€§è¯„ä¼°ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚ç ”ç©¶ç»“æœè¿˜è¡¨æ˜ï¼Œæœºå™¨ç¿»è¯‘å¯ä½œä¸ºæ‰©å±•çœŸå®æ€§åŸºå‡†æµ‹è¯•è‡³å…¶ä»–è¯­è¨€çš„å¯è¡Œæ–¹æ³•ï¼Œæä¾›ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£ä¸“ä¸šç¿»è¯‘çš„é€”å¾„ã€‚æœ€åè§‚å¯Ÿåˆ°ï¼Œé€šç”¨çŸ¥è¯†é—®é¢˜åœ¨è·¨è¯­è¨€å¤„ç†ä¸Šä¼˜äºä¸Šä¸‹æ–‡å’Œæ—¶é—´ä¾èµ–æ€§é—®é¢˜ï¼Œå¼ºè°ƒåœ¨è¿›è¡ŒçœŸå®æ€§è¯„ä¼°æ—¶éœ€è¦è€ƒè™‘åˆ°æ–‡åŒ–å’Œæ—¶é—´å˜åŒ–ã€‚æ•°æ®é›†å’Œä»£ç å‡å…¬å¼€å¼€æ”¾è®¸å¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶äººå‘˜å¯¹å·´æ–¯å…‹è¯­ã€åŠ æ³°ç½—å°¼äºšè¯­ã€åŠ åˆ©è¥¿äºšè¯­å’Œè¥¿ç­ç‰™è¯­è¿›è¡Œäº†çœŸå®æ€§è¯„ä¼°çš„LLMæ‰©å±•ç ”ç©¶ã€‚</li>
<li>LLMåœ¨è‹±è¯­ä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨å·´æ–¯å…‹è¯­ä¸­æœ€å·®ï¼Œä½†è·¨è¯­è¨€çš„çœŸå®æ€§å·®å¼‚æ€»ä½“è¾ƒå°ã€‚</li>
<li>LLM-as-a-Judgeè¯„åˆ†æ³•ä¸äººç±»åˆ¤æ–­çš„ç›¸å…³æ€§æ›´é«˜ã€‚</li>
<li>ä¿¡æ¯æ€§åœ¨çœŸå®æ€§è¯„ä¼°ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>æœºå™¨ç¿»è¯‘æ˜¯æ‰©å±•çœŸå®æ€§åŸºå‡†æµ‹è¯•è‡³å…¶ä»–è¯­è¨€çš„å¯è¡Œæ–¹æ³•ã€‚</li>
<li>é€šç”¨çŸ¥è¯†é—®é¢˜çš„è·¨è¯­è¨€å¤„ç†èƒ½åŠ›ä¼˜äºä¸Šä¸‹æ–‡å’Œæ—¶é—´ä¾èµ–æ€§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3137019a458b14240432e564df290aa6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09cdb7d97ff0afd86d19a1ea17792c69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bd50d8c8205344070fc9005e9604d1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c07c8b80642faa5cc92f0c557d9c200.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b1a35351fc4bd7e1b87bf5aa72d792e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7597df91e8b1580447761cb223f61c3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15fd07c4a781abb57acf5f9475e78975.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Pap2Pat-Benchmarking-Outline-Guided-Long-Text-Patent-Generation-with-Patent-Paper-Pairs"><a href="#Pap2Pat-Benchmarking-Outline-Guided-Long-Text-Patent-Generation-with-Patent-Paper-Pairs" class="headerlink" title="Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with   Patent-Paper Pairs"></a>Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with   Patent-Paper Pairs</h2><p><strong>Authors:Valentin Knappich, Simon Razniewski, Anna HÃ¤tty, Annemarie Friedrich</strong></p>
<p>Dealing with long and highly complex technical text is a challenge for Large Language Models (LLMs), which still have to unfold their potential in supporting expensive and timeintensive processes like patent drafting. Within patents, the description constitutes more than 90% of the document on average. Yet, its automatic generation remains understudied. When drafting patent applications, patent attorneys typically receive invention reports (IRs), which are usually confidential, hindering research on LLM-supported patent drafting. Often, prepublication research papers serve as IRs. We leverage this duality to build PAP2PAT, an open and realistic benchmark for patent drafting consisting of 1.8k patent-paper pairs describing the same inventions. To address the complex longdocument patent generation task, we propose chunk-based outline-guided generation using the research paper as invention specification. Our extensive evaluation using PAP2PAT and a human case study show that LLMs can effectively leverage information from the paper, but still struggle to provide the necessary level of detail. Fine-tuning leads to more patent-style language, but also to more hallucination. We release our data and code <a target="_blank" rel="noopener" href="https://github.com/boschresearch/Pap2Pat">https://github.com/boschresearch/Pap2Pat</a>. </p>
<blockquote>
<p>å¤„ç†é•¿è€Œå¤æ‚çš„æŠ€æœ¯æ–‡æœ¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨æ”¯æŒå¦‚ä¸“åˆ©æ’°å†™ç­‰æ˜‚è´µä¸”è€—æ—¶çš„æµç¨‹æ—¶ï¼Œä»éœ€å‘æŒ¥å…¶æ½œåŠ›ã€‚åœ¨ä¸“åˆ©æ–‡æ¡£ä¸­ï¼Œæè¿°éƒ¨åˆ†å¹³å‡å æ•´ä¸ªæ–‡æ¡£çš„90%ä»¥ä¸Šã€‚ç„¶è€Œï¼Œå…¶è‡ªåŠ¨ç”Ÿæˆä»ç„¶ç ”ç©¶ä¸è¶³ã€‚åœ¨æ’°å†™ä¸“åˆ©ç”³è¯·æ—¶ï¼Œä¸“åˆ©ä»£ç†äººé€šå¸¸ä¼šæ”¶åˆ°å‘æ˜æŠ¥å‘Šï¼ˆIRsï¼‰ï¼Œè¿™äº›æŠ¥å‘Šé€šå¸¸æ˜¯æœºå¯†çš„ï¼Œé˜»ç¢äº†å…³äºLLMæ”¯æŒçš„ä¸“åˆ©æ’°å†™çš„ç ”ç©¶ã€‚é€šå¸¸ï¼Œé¢„å‘å¸ƒçš„ç§‘ç ”è®ºæ–‡ä¼šä½œä¸ºIRsã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ç§åŒé‡æ€§è´¨æ¥æ„å»ºPAP2PATï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾ä¸”ç°å®çš„ä¸“åˆ©æ’°å†™åŸºå‡†ï¼ŒåŒ…å«1.8kä¸ªæè¿°ç›¸åŒå‘æ˜çš„ä¸“åˆ©-è®ºæ–‡å¯¹ã€‚ä¸ºäº†è§£å†³å¤æ‚çš„é•¿æ–‡æ¡£ä¸“åˆ©ç”Ÿæˆä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåˆ†å—çš„æçº²æŒ‡å¯¼ç”Ÿæˆæ–¹æ³•ï¼Œä»¥è®ºæ–‡ä½œä¸ºå‘æ˜è¯´æ˜ä¹¦ã€‚æˆ‘ä»¬ä½¿ç”¨PAP2PATè¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œå¹¶è¿›è¡Œäººç±»æ¡ˆä¾‹ç ”ç©¶ï¼Œç»“æœæ˜¾ç¤ºLLMå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨è®ºæ–‡ä¸­çš„ä¿¡æ¯ï¼Œä½†åœ¨æä¾›å¿…è¦ç»†èŠ‚æ–¹é¢ä»é¢ä¸´å›°éš¾ã€‚å¾®è°ƒå¯ä»¥äº§ç”Ÿæ›´å¤šç¬¦åˆä¸“åˆ©é£æ ¼çš„è¯­è¨€ï¼Œä½†ä¹Ÿä¼šäº§ç”Ÿæ›´å¤šçš„è™šæ„å†…å®¹ã€‚æˆ‘ä»¬å…¬å¼€äº†æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/boschresearch/Pap2Pat%E3%80%82">https://github.com/boschresearch/Pap2Patã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07009v3">PDF</a> ACL 2025 Findings</p>
<p><strong>æ‘˜è¦</strong></p>
<p>LLMåœ¨å¤„ç†é•¿æœŸä¸”é«˜åº¦å¤æ‚çš„æŠ€æœ¯æ–‡æœ¬æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ”¯æŒå¦‚ä¸“åˆ©èµ·è‰ç­‰æ˜‚è´µä¸”è€—æ—¶çš„æµç¨‹æ—¶ã€‚å°½ç®¡ä¸“åˆ©ä¸­çš„æè¿°éƒ¨åˆ†å¹³å‡å æ–‡æ¡£è¶…è¿‡90%ï¼Œä½†å…¶è‡ªåŠ¨ç”Ÿæˆä»å—åˆ°çš„ç ”ç©¶è¾ƒå°‘ã€‚ä¸“åˆ©å¾‹å¸ˆåœ¨èµ·è‰ä¸“åˆ©ç”³è¯·æ—¶ä¼šæ”¶åˆ°é€šå¸¸ä¿å¯†çš„å‘æ˜æŠ¥å‘Šï¼ˆIRï¼‰ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å¼€å‘äº†PAP2PATæ•°æ®é›†ï¼Œå®ƒåŒ…å«äº†å…³äºç›¸åŒå‘æ˜çš„è¿‘ä¸€ã€å…«ä¸‡ä¸ªä¸“åˆ©æ–‡çŒ®é…å¯¹çš„æ•°æ®é›†ã€‚æå‡ºäº†åŸºäºåˆ†å—çš„æ¦‚è¦å¼•å¯¼ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨ç ”ç©¶è®ºæ–‡ä½œä¸ºå‘æ˜è¯´æ˜ä¹¦æ¥è§£å†³å¤æ‚çš„é•¿æœŸä¸“åˆ©ç”Ÿæˆä»»åŠ¡ã€‚é€šè¿‡å¹¿æ³›çš„åŸºäºPAP2PATçš„æ•°æ®é›†è¯„ä¼°å’Œæ¡ˆä¾‹ç ”ç©¶è¯æ˜ï¼ŒLLMå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨è®ºæ–‡ä¸­çš„ä¿¡æ¯ï¼Œä½†åœ¨æä¾›å¿…è¦çš„ç»†èŠ‚æ–¹é¢ä»æœ‰å›°éš¾ã€‚å¾®è°ƒå¯ä»¥å¢åŠ ä¸“åˆ©é£æ ¼çš„è¯­è¨€ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´è™šæ„æƒ…å†µçš„å‡ºç°ã€‚è¯¥ç ”ç©¶çš„ç›¸å…³æ•°æ®å’Œä»£ç å·²å…¬å¼€å‘å¸ƒäºå¼€æºç½‘ç«™boschresearch&#x2F;Pap2Patã€‚ </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>LLMåœ¨å¤„ç†é•¿æœŸå¤æ‚æŠ€æœ¯æ–‡æœ¬æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¯æŒä¸“åˆ©èµ·è‰ç­‰æµç¨‹æ—¶ã€‚</li>
<li>ä¸“åˆ©ä¸­çš„æè¿°éƒ¨åˆ†å¹³å‡å æ–‡æ¡£è¶…è¿‡90%ï¼Œä½†å…¶è‡ªåŠ¨ç”Ÿæˆçš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚</li>
<li>ç ”ç©¶äººå‘˜åˆ©ç”¨å‘æ˜æŠ¥å‘Šï¼ˆIRï¼‰æ„å»ºäº†å¼€æ”¾ä¸”ç°å®çš„ä¸“åˆ©èµ·è‰åŸºå‡†æ•°æ®é›†PAP2PATã€‚</li>
<li>åŸºäºåˆ†å—çš„æ¦‚è¦å¼•å¯¼ç”Ÿæˆæ–¹æ³•è¢«æå‡ºä»¥è§£å†³å¤æ‚çš„é•¿æœŸä¸“åˆ©ç”Ÿæˆä»»åŠ¡ï¼Œåˆ©ç”¨ç ”ç©¶è®ºæ–‡ä½œä¸ºå‘æ˜è¯´æ˜ä¹¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58094984bd700ac7139132c6556ef4fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cdbdd65d1e1b96aba5fff1770dab158.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2663fc6f54309bc37942c43bfe2d5c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e149b45cb8a81c5be1da153809a59a0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Lean-Workbook-A-large-scale-Lean-problem-set-formalized-from-natural-language-math-problems"><a href="#Lean-Workbook-A-large-scale-Lean-problem-set-formalized-from-natural-language-math-problems" class="headerlink" title="Lean Workbook: A large-scale Lean problem set formalized from natural   language math problems"></a>Lean Workbook: A large-scale Lean problem set formalized from natural   language math problems</h2><p><strong>Authors:Huaiyuan Ying, Zijian Wu, Yihan Geng, Zheng Yuan, Dahua Lin, Kai Chen</strong></p>
<p>Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at <a target="_blank" rel="noopener" href="https://github.com/InternLM/InternLM-Math">https://github.com/InternLM/InternLM-Math</a> and our data at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/InternLM/Lean-Workbook">https://huggingface.co/datasets/InternLM/Lean-Workbook</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨è§£å†³æ•°å­¦é—®é¢˜æ–¹é¢ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸æ“…é•¿ä½¿ç”¨è¯¸å¦‚Leanä¹‹ç±»çš„æ­£å¼è¯­è¨€è¿›è¡Œæ•°å­¦å®šç†è¯æ˜ã€‚è¯¥é¢†åŸŸçš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯è¿™äº›æ­£å¼è¯­è¨€ä¸­å¯ç”¨è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç®¡é“ï¼Œè¯¥ç®¡é“å¯è¿­ä»£ç”Ÿæˆå¹¶è¿‡æ»¤åˆæˆæ•°æ®ï¼Œä»¥å°†è‡ªç„¶è¯­è¨€æ•°å­¦é—®é¢˜è½¬æ¢ä¸ºLean 4è¯­å¥ï¼Œåä¹‹äº¦ç„¶ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåˆæˆæ•°æ®ç®¡é“å¯æä¾›æœ‰ç”¨çš„è®­ç»ƒæ•°æ®ï¼Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¿»è¯‘å’Œç†è§£å¤æ‚æ•°å­¦é—®é¢˜å’Œè¯æ˜æ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æœ€ç»ˆæ•°æ®é›†åŒ…å«å¤§çº¦57Kä¸ªæ­£å¼ä¸éæ­£å¼é—®é¢˜å¯¹ï¼Œä»¥åŠä»æ•°å­¦ç«èµ›è®ºå›ä¸­æœç´¢åˆ°çš„è¯æ˜å’Œ21ä¸ªæ–°çš„å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›é—®é¢˜ã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/InternLM/InternLM-Math%EF%BC%8C%E6%95%B0%E6%8D%AE%E5%B7%B2%E5%85%AC%E5%BC%80%E5%9C%A8https://huggingface.co/datasets/InternLM/Lean-Workbook%E3%80%82">https://github.com/InternLM/InternLM-Mathï¼Œæ•°æ®å·²å…¬å¼€åœ¨https://huggingface.co/datasets/InternLM/Lean-Workbookã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03847v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå°¤å…¶åœ¨è§£å†³æ•°å­¦é—®é¢˜æ–¹é¢ã€‚ç„¶è€Œï¼Œåœ¨åˆ©ç”¨å½¢å¼è¯­è¨€ï¼ˆå¦‚Leanï¼‰è¿›è¡Œæ•°å­¦å®šç†è¯æ˜æ–¹é¢ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚è®­ç»ƒæ•°æ®åœ¨æ­£å¼è¯­è¨€ä¸­çš„åŒ®ä¹æ˜¯è¿™ä¸€é¢†åŸŸçš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç®¡é“ï¼Œè¯¥ç®¡é“å¯è¿­ä»£ç”Ÿæˆå¹¶è¿‡æ»¤åˆæˆæ•°æ®ï¼Œå°†è‡ªç„¶è¯­è¨€æ•°å­¦é—®é¢˜ç¿»è¯‘ä¸ºLean 4è¯­å¥ï¼Œåä¹‹äº¦ç„¶ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œè¯¥åˆæˆæ•°æ®ç®¡é“å¯æä¾›æœ‰ç”¨çš„è®­ç»ƒæ•°æ®ï¼Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å’Œç†è§£å¤æ‚æ•°å­¦é—®é¢˜å’Œè¯æ˜æ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æœ€ç»ˆæ•°æ®é›†åŒ…å«çº¦57Kçš„æ­£å¼ä¸éæ­£å¼é—®é¢˜å¯¹ï¼Œä»¥åŠä»æ•°å­¦ç«èµ›è®ºå›æ£€ç´¢åˆ°çš„è¯æ˜å’Œ21ä¸ªæ–°çš„å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›é—®é¢˜ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/InternLM/InternLM-Math%E5%BC%80%E6%BA%90%E4%BA%86%E4%BB%A3%E7%A0%81%EF%BC%8C%E5%B9%B6%E5%9C%A8https://huggingface.co/datasets/InternLM/Lean-Workbook%E5%88%86%E4%BA%AB%E4%BA%86%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/InternLM/InternLM-Mathå¼€æºäº†ä»£ç ï¼Œå¹¶åœ¨https://huggingface.co/datasets/InternLM/Lean-Workbookåˆ†äº«äº†æ•°æ®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯è§£å†³æ•°å­¦é—®é¢˜æ–¹é¢ã€‚</li>
<li>åœ¨åˆ©ç”¨å½¢å¼è¯­è¨€ï¼ˆå¦‚Leanï¼‰è¿›è¡Œæ•°å­¦å®šç†è¯æ˜æ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>è®­ç»ƒæ•°æ®åœ¨å½¢å¼è¯­è¨€ä¸­çš„ç¼ºä¹æ˜¯è¿™ä¸€æŒ‘æˆ˜çš„ä¸»è¦åŸå› ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹ç®¡é“ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆå¹¶è¿‡æ»¤åˆæˆæ•°æ®ï¼Œä¿ƒè¿›è‡ªç„¶è¯­è¨€ä¸å½¢å¼è¯­è¨€ä¹‹é—´çš„ç¿»è¯‘ã€‚</li>
<li>åˆæˆæ•°æ®ç®¡é“æœ‰åŠ©äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†å’Œç†è§£å¤æ‚æ•°å­¦é—®é¢˜å’Œè¯æ˜çš„èƒ½åŠ›ã€‚</li>
<li>æœ€ç»ˆæ•°æ®é›†åŒ…å«æ­£å¼ä¸éæ­£å¼é—®é¢˜å¯¹ã€ä»æ•°å­¦ç«èµ›è®ºå›æ£€ç´¢åˆ°çš„è¯æ˜ä»¥åŠæ–°çš„å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.03847">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffaa305d169dec5aa54c0fd8b31a4254.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b847108080bddf54a352a59dde000c73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aede71f19068fcc235c663a2692463f6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-23/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-23/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-23/Speech/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ada04062f944b1455d4acf280c88b562.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-23  SMILE Speech Meta In-Context Learning for Low-Resource Language   Automatic Speech Recognition
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-23
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-23/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fa21084d3ba4c0166b1a65ebcbce18cd.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-23  Reward Shaping to Mitigate Reward Hacking in RLHF
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23542.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
