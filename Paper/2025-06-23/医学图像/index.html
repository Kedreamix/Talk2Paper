<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-23  A Hybrid ConvNeXt-EfficientNet AI Solution for Precise Falcon Disease   Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4c1c73982632542b13bac20264fd9e87.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-23
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    43 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-23-æ›´æ–°"><a href="#2025-06-23-æ›´æ–°" class="headerlink" title="2025-06-23 æ›´æ–°"></a>2025-06-23 æ›´æ–°</h1><h2 id="A-Hybrid-ConvNeXt-EfficientNet-AI-Solution-for-Precise-Falcon-Disease-Detection"><a href="#A-Hybrid-ConvNeXt-EfficientNet-AI-Solution-for-Precise-Falcon-Disease-Detection" class="headerlink" title="A Hybrid ConvNeXt-EfficientNet AI Solution for Precise Falcon Disease   Detection"></a>A Hybrid ConvNeXt-EfficientNet AI Solution for Precise Falcon Disease   Detection</h2><p><strong>Authors:Alavikunhu Panthakkan, Zubair Medammal, S M Anzar, Fatma Taher, Hussain Al-Ahmad</strong></p>
<p>Falconry, a revered tradition involving the training and hunting with falcons, requires meticulous health surveillance to ensure the health and safety of these prized birds, particularly in hunting scenarios. This paper presents an innovative method employing a hybrid of ConvNeXt and EfficientNet AI models for the classification of falcon diseases. The study focuses on accurately identifying three conditions: Normal, Liver Disease and â€˜Aspergillosisâ€™. A substantial dataset was utilized for training and validating the model, with an emphasis on key performance metrics such as accuracy, precision, recall, and F1-score. Extensive testing and analysis have shown that our concatenated AI model outperforms traditional diagnostic methods and individual model architectures. The successful implementation of this hybrid AI model marks a significant step forward in precise falcon disease detection and paves the way for future developments in AI-powered avian healthcare solutions. </p>
<blockquote>
<p>é©¯é¹°æœ¯æ˜¯ä¸€é¡¹å—äººå°Šæ•¬çš„ä¼ ç»Ÿï¼Œæ¶‰åŠé©¯åŒ–å’Œç”¨çŒé¹°ç‹©çŒã€‚ä¸ºäº†ä¿è¯è¿™äº›çè´µé¸Ÿç±»ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‹©çŒåœºæ™¯ä¸­çš„å¥åº·å’Œå®‰å…¨é—®é¢˜ï¼Œéœ€è¦å¯¹å…¶è¿›è¡Œç»†è‡´çš„å¥åº·ç›‘æµ‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é‡‡ç”¨ConvNeXtå’ŒEfficientNetäººå·¥æ™ºèƒ½æ¨¡å‹æ··åˆä½“è¿›è¡ŒçŒé¹°ç–¾ç—…åˆ†ç±»çš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥ç ”ç©¶é‡ç‚¹å…³æ³¨å‡†ç¡®è¯†åˆ«ä¸‰ç§çŠ¶å†µï¼šæ­£å¸¸ã€è‚ç—…å’Œâ€œæ›²éœ‰èŒç—…â€ã€‚ç ”ç©¶ä½¿ç”¨å¤§é‡æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ï¼Œé‡ç‚¹å…³æ³¨å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ç­‰å…³é”®æ€§èƒ½æŒ‡æ ‡ã€‚ç»è¿‡å¹¿æ³›æµ‹è¯•å’Œåˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬ç»„åˆçš„äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿè¯Šæ–­æ–¹æ³•å’Œå•ä¸€æ¨¡å‹æ¶æ„ã€‚è¿™ç§æ··åˆäººå·¥æ™ºèƒ½æ¨¡å‹çš„æˆåŠŸå®æ–½æ ‡å¿—ç€ç²¾å‡†çŒé¹°ç–¾ç—…æ£€æµ‹è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œå¹¶ä¸ºæœªæ¥äººå·¥æ™ºèƒ½é©±åŠ¨çš„é¸Ÿç±»åŒ»ç–—ä¿å¥è§£å†³æ–¹æ¡ˆçš„å‘å±•é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14816v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>    æœ¬ç ”ç©¶é‡‡ç”¨ConvNeXtå’ŒEfficientNetäººå·¥æ™ºèƒ½æ¨¡å‹çš„æ··åˆæ–¹æ³•ï¼Œå¯¹çŒé¹°çš„ç–¾ç—…è¿›è¡Œåˆ†ç±»ã€‚ç ”ç©¶é‡ç‚¹æ˜¯å¯¹æ­£å¸¸çŠ¶æ€ã€è‚ç—…å’Œæ›²éœ‰èŒç—…ä¸‰ç§çŠ¶æ€çš„å‡†ç¡®è¯†åˆ«ã€‚é€šè¿‡å¤§é‡æ•°æ®é›†çš„è®­ç»ƒå’ŒéªŒè¯ï¼Œè¯¥æ¨¡å‹çš„å‡†ç¡®æ€§ã€ç²¾ç¡®æ€§ã€å¬å›ç‡å’ŒF1åˆ†æ•°ç­‰å…³é”®æ€§èƒ½æŒ‡æ ‡å‡è¡¨ç°ä¼˜å¼‚ã€‚æµ‹è¯•å’Œåˆ†æè¡¨æ˜ï¼Œè¯¥æ··åˆAIæ¨¡å‹ä¼˜äºä¼ ç»Ÿè¯Šæ–­æ–¹æ³•å’Œå•ç‹¬çš„æ¨¡å‹æ¶æ„ã€‚è¯¥æ··åˆäººå·¥æ™ºèƒ½æ¨¡å‹çš„æˆåŠŸå®æ–½ä¸ºç²¾ç¡®çš„çŒé¹°ç–¾ç—…æ£€æµ‹è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ï¼Œå¹¶ä¸ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„é¸Ÿç±»å¥åº·æŠ¤ç†è§£å†³æ–¹æ¡ˆçš„å‘å±•é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ç ”ç©¶é‡‡ç”¨ConvNeXtå’ŒEfficientNetæ··åˆAIæ¨¡å‹å¯¹çŒé¹°ç–¾ç—…è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>é‡ç‚¹è¯†åˆ«çŒé¹°çš„ä¸‰ç§çŠ¶æ€ï¼šæ­£å¸¸ã€è‚ç—…å’Œæ›²éœ‰èŒç—…ã€‚</li>
<li>ä½¿ç”¨å¤§é‡æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ã€‚</li>
<li>æ¨¡å‹åœ¨å…³é”®æ€§èƒ½æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¦‚å‡†ç¡®æ€§ã€ç²¾ç¡®æ€§ã€å¬å›ç‡å’ŒF1åˆ†æ•°ã€‚</li>
<li>æ··åˆAIæ¨¡å‹åœ¨æµ‹è¯•å’Œåˆ†æä¸­è¡¨ç°å‡ºä¼˜äºä¼ ç»Ÿè¯Šæ–­æ–¹æ³•å’Œå•ç‹¬æ¨¡å‹æ¶æ„çš„æ€§èƒ½ã€‚</li>
<li>æ­¤ç ”ç©¶æ˜¯çŒé¹°ç–¾ç—…æ£€æµ‹çš„ä¸€ä¸ªé‡è¦è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5797dbe4d86de4e2757c26e7facbb5a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8312226c063cf019d07a7a2ff25f1f81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10819f8a1ff23e32a33410ad39dbf31a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d02136aeca5f2d2961f411351b831468.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b20a0b68a0fa9ed4fa8ed49f97bc3fe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a2be20a01a8cefe8e2d4d60f85c6899.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14b21f8b76b81d96da78d81eb23ad4d6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="InceptionMamba-Efficient-Multi-Stage-Feature-Enhancement-with-Selective-State-Space-Model-for-Microscopic-Medical-Image-Segmentation"><a href="#InceptionMamba-Efficient-Multi-Stage-Feature-Enhancement-with-Selective-State-Space-Model-for-Microscopic-Medical-Image-Segmentation" class="headerlink" title="InceptionMamba: Efficient Multi-Stage Feature Enhancement with Selective   State Space Model for Microscopic Medical Image Segmentation"></a>InceptionMamba: Efficient Multi-Stage Feature Enhancement with Selective   State Space Model for Microscopic Medical Image Segmentation</h2><p><strong>Authors:Daniya Najiha Abdul Kareem, Abdul Hannan, Mubashir Noman, Jean Lahoud, Mustansar Fiaz, Hisham Cholakkal</strong></p>
<p>Accurate microscopic medical image segmentation plays a crucial role in diagnosing various cancerous cells and identifying tumors. Driven by advancements in deep learning, convolutional neural networks (CNNs) and transformer-based models have been extensively studied to enhance receptive fields and improve medical image segmentation task. However, they often struggle to capture complex cellular and tissue structures in challenging scenarios such as background clutter and object overlap. Moreover, their reliance on the availability of large datasets for improved performance, along with the high computational cost, limit their practicality. To address these issues, we propose an efficient framework for the segmentation task, named InceptionMamba, which encodes multi-stage rich features and offers both performance and computational efficiency. Specifically, we exploit semantic cues to capture both low-frequency and high-frequency regions to enrich the multi-stage features to handle the blurred region boundaries (e.g., cell boundaries). These enriched features are input to a hybrid model that combines an Inception depth-wise convolution with a Mamba block, to maintain high efficiency and capture inherent variations in the scales and shapes of the regions of interest. These enriched features along with low-resolution features are fused to get the final segmentation mask. Our model achieves state-of-the-art performance on two challenging microscopic segmentation datasets (SegPC21 and GlaS) and two skin lesion segmentation datasets (ISIC2017 and ISIC2018), while reducing computational cost by about 5 times compared to the previous best performing method. </p>
<blockquote>
<p>ç²¾ç¡®çš„å¾®è§‚åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è¯Šæ–­å„ç§ç™Œç»†èƒå’Œè¯†åˆ«è‚¿ç˜¤æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚åœ¨æ·±åº¦å­¦ä¹ å‘å±•çš„æ¨åŠ¨ä¸‹ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒåŸºäºTransformerçš„æ¨¡å‹å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä»¥æé«˜æ„ŸçŸ¥åœºå¹¶æ”¹è¿›åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨èƒŒæ™¯æ‚ä¹±å’Œç‰©ä½“é‡å ç­‰å¤æ‚åœºæ™¯ä¸­ï¼Œå¾€å¾€éš¾ä»¥æ•æ‰å¤æ‚çš„ç»†èƒå’Œç»„ç»‡ç»“æ„ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¾èµ–äºå¤§é‡æ•°æ®é›†æ¥æé«˜æ€§èƒ½ï¼Œä»¥åŠè®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œé™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºåˆ†å‰²ä»»åŠ¡çš„é«˜æ•ˆæ¡†æ¶ï¼Œåä¸ºInceptionMambaã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿç¼–ç å¤šé˜¶æ®µä¸°å¯Œç‰¹å¾ï¼Œå¹¶æä¾›æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨è¯­ä¹‰çº¿ç´¢æ•æ‰ä½é¢‘å’Œé«˜é¢‘åŒºåŸŸï¼Œä»¥ä¸°å¯Œå¤šé˜¶æ®µç‰¹å¾ï¼Œå¤„ç†æ¨¡ç³Šçš„è¾¹ç•ŒåŒºåŸŸï¼ˆå¦‚ç»†èƒè¾¹ç•Œï¼‰ã€‚è¿™äº›ä¸°å¯Œçš„ç‰¹å¾è¢«è¾“å…¥åˆ°ä¸€ä¸ªæ··åˆæ¨¡å‹ä¸­ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†Inceptionæ·±åº¦å·ç§¯å’ŒMambaå—ï¼Œä»¥ä¿æŒé«˜æ•ˆç‡å¹¶æ•æ‰æ„Ÿå…´è¶£åŒºåŸŸçš„å›ºæœ‰å˜åŒ–å’Œå½¢çŠ¶ã€‚è¿™äº›ä¸°å¯Œç‰¹å¾ä¸ä½åˆ†è¾¨ç‡ç‰¹å¾ç›¸èåˆï¼Œä»¥è·å¾—æœ€ç»ˆçš„åˆ†å‰²æ©è†œã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¾®è§‚åˆ†å‰²æ•°æ®é›†ï¼ˆSegPC21å’ŒGlaSï¼‰å’Œä¸¤ä¸ªçš®è‚¤ç—…å˜åˆ†å‰²æ•°æ®é›†ï¼ˆISIC2017å’ŒISIC2018ï¼‰ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ä»¥å‰æ€§èƒ½æœ€ä½³çš„æ–¹æ³•ç›¸æ¯”ï¼Œè®¡ç®—æˆæœ¬é™ä½äº†çº¦5å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12208v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŒ»å­¦å›¾åƒç²¾å‡†åˆ†å‰²å¯¹äºè¯Šæ–­ç™Œç»†èƒå’Œè¯†åˆ«è‚¿ç˜¤è‡³å…³é‡è¦ã€‚æ·±åº¦å­¦ä¹ é©±åŠ¨ä¸‹çš„å·ç§¯ç¥ç»ç½‘ç»œå’ŒåŸºäºtransformerçš„æ¨¡å‹å·²å¹¿æ³›åº”ç”¨äºæå‡æ„Ÿå—é‡å¹¶æ”¹è¿›åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨èƒŒæ™¯æ‚ä¹±å’Œç‰©ä½“é‡å ç­‰å¤æ‚åœºæ™¯ä¸‹ï¼Œéš¾ä»¥æ•æ‰ç»†èƒå’Œç»„ç»‡çš„å¤æ‚ç»“æ„ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¾èµ–å¤§é‡æ•°æ®é›†æ¥æå‡æ€§èƒ½ï¼Œä»¥åŠè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åˆ†å‰²æ¡†æ¶ï¼Œåä¸ºInceptionMambaï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç¼–ç å¤šé˜¶æ®µä¸°å¯Œç‰¹å¾ï¼Œå¹¶æä¾›æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ã€‚é€šè¿‡åˆ©ç”¨è¯­ä¹‰çº¿ç´¢æ•æ‰ä½é¢‘å’Œé«˜é¢‘åŒºåŸŸï¼Œä¸°å¯Œå¤šé˜¶æ®µç‰¹å¾ä»¥å¤„ç†æ¨¡ç³Šçš„è¾¹ç•ŒåŒºåŸŸï¼ˆå¦‚ç»†èƒè¾¹ç•Œï¼‰ã€‚è¿™äº›ä¸°å¯Œçš„ç‰¹å¾è¢«è¾“å…¥åˆ°ä¸€ä¸ªæ··åˆæ¨¡å‹ä¸­ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†Inceptionæ·±åº¦å·ç§¯å’ŒMambaå—ï¼Œä»¥ä¿æŒé«˜æ•ˆç‡å¹¶æ•æ‰æ„Ÿå…´è¶£åŒºåŸŸçš„å›ºæœ‰å˜åŒ–å’Œå½¢çŠ¶ã€‚è¿™äº›ä¸°å¯Œç‰¹å¾ä¸ä½åˆ†è¾¨ç‡ç‰¹å¾ç›¸èåˆï¼Œä»¥è·å¾—æœ€ç»ˆçš„åˆ†å‰²æ©è†œã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ˜¾å¾®é•œåˆ†å‰²æ•°æ®é›†ï¼ˆSegPC21å’ŒGlaSï¼‰ä»¥åŠçš®è‚¤ç—…å˜åˆ†å‰²æ•°æ®é›†ï¼ˆISIC2017å’ŒISIC2018ï¼‰ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸ä»¥å‰æ€§èƒ½æœ€ä½³çš„æ–¹æ³•ç›¸æ¯”ï¼Œè®¡ç®—æˆæœ¬é™ä½äº†çº¦äº”å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è¯Šæ–­ç™Œç»†èƒå’Œè¯†åˆ«è‚¿ç˜¤æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œå’ŒåŸºäºtransformerçš„æ¨¡å‹ï¼Œå·²ç”¨äºæ”¹è¿›åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸‹ï¼ˆå¦‚èƒŒæ™¯æ‚ä¹±å’Œç‰©ä½“é‡å ï¼‰éš¾ä»¥æ•æ‰ç»†èƒå’Œç»„ç»‡çš„å¤æ‚ç»“æ„ã€‚</li>
<li>æå‡ºçš„InceptionMambaæ¡†æ¶èƒ½å¤Ÿç¼–ç å¤šé˜¶æ®µä¸°å¯Œç‰¹å¾ï¼Œæé«˜åˆ†å‰²æ€§èƒ½å’Œå¤„ç†æ¨¡ç³Šè¾¹ç•ŒåŒºåŸŸã€‚</li>
<li>InceptionMambaæ¡†æ¶ç»“åˆäº†Inceptionæ·±åº¦å·ç§¯å’ŒMambaå—ï¼Œä¿æŒé«˜æ•ˆç‡å¹¶æ•æ‰åŒºåŸŸçš„å›ºæœ‰å˜åŒ–å’Œå½¢çŠ¶ã€‚</li>
<li>InceptionMambaæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1adecf59e6d7699dcda9139f0daa53dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc4da3556238feea7b776503fc0c2f07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e51a1b7ecd465dcc5a673887e66d2d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f0e917ec79243a4018c3c9d5a830c96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb2fdf1e39b2236e89bf34a498d4d7b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bcfe928c3c20d5b12b7b1e8774b03d0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="BreastDCEDL-Curating-a-Comprehensive-DCE-MRI-Dataset-and-developing-a-Transformer-Implementation-for-Breast-Cancer-Treatment-Response-Prediction"><a href="#BreastDCEDL-Curating-a-Comprehensive-DCE-MRI-Dataset-and-developing-a-Transformer-Implementation-for-Breast-Cancer-Treatment-Response-Prediction" class="headerlink" title="BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a   Transformer Implementation for Breast Cancer Treatment Response Prediction"></a>BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a   Transformer Implementation for Breast Cancer Treatment Response Prediction</h2><p><strong>Authors:Naomi Fridman, Bubby Solway, Tomer Fridman, Itamar Barnea, Anat Goldshtein</strong></p>
<p>Breast cancer remains a leading cause of cancer-related mortality worldwide, making early detection and accurate treatment response monitoring critical priorities. We present BreastDCEDL, a curated, deep learning-ready dataset comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from 2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts, all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were rigorously converted into standardized 3D NIfTI volumes with preserved signal integrity, accompanied by unified tumor annotations and harmonized clinical metadata including pathologic complete response (pCR), hormone receptor (HR), and HER2 status. Although DCE-MRI provides essential diagnostic information and deep learning offers tremendous potential for analyzing such complex data, progress has been limited by lack of accessible, public, multicenter datasets. BreastDCEDL addresses this gap by enabling development of advanced models, including state-of-the-art transformer architectures that require substantial training data. To demonstrate its capacity for robust modeling, we developed the first transformer-based model for breast DCE-MRI, leveraging Vision Transformer (ViT) architecture trained on RGB-fused images from three contrast phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT model achieved state-of-the-art pCR prediction performance in HR+&#x2F;HER2- patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark splits, offering a framework for reproducible research and enabling clinically meaningful modeling in breast cancer imaging. </p>
<blockquote>
<p>ä¹³è…ºç™Œä»ç„¶æ˜¯å…¨çƒç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå› æ­¤æ—©æœŸæ£€æµ‹å’Œå‡†ç¡®çš„ç–—æ•ˆååº”ç›‘æµ‹æˆä¸ºè‡³å…³é‡è¦çš„ä¼˜å…ˆäº‹é¡¹ã€‚æˆ‘ä»¬æ¨å‡ºäº†BreastDCEDLï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„ã€ä¸ºæ·±åº¦å­¦ä¹ å‡†å¤‡çš„æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªI-SPY1ã€I-SPY2å’ŒDukeé˜Ÿåˆ—çš„2070åä¹³è…ºç™Œæ‚£è€…çš„æœ¯å‰3DåŠ¨æ€å¢å¼ºMRIï¼ˆDCE-MRIï¼‰æ‰«ææ•°æ®ï¼Œæ‰€æœ‰æ•°æ®æºå‡æ¥è‡ªç™Œç—‡æˆåƒæ¡£æ¡ˆã€‚åŸå§‹çš„DICOMæˆåƒæ•°æ®è¢«ä¸¥æ ¼è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„3DNIfTIä½“ç§¯ï¼ŒåŒæ—¶ä¿ç•™äº†ä¿¡å·å®Œæ•´æ€§ï¼Œå¹¶é…æœ‰ç»Ÿä¸€çš„è‚¿ç˜¤æ³¨é‡Šå’Œåè°ƒä¸€è‡´çš„ä¸´åºŠå…ƒæ•°æ®ï¼ŒåŒ…æ‹¬ç—…ç†å®Œå…¨ååº”ï¼ˆpCRï¼‰ã€æ¿€ç´ å—ä½“ï¼ˆHRï¼‰å’ŒHER2çŠ¶æ€ã€‚å°½ç®¡DCE-MRIæä¾›äº†é‡è¦çš„è¯Šæ–­ä¿¡æ¯ï¼Œæ·±åº¦å­¦ä¹ åœ¨åˆ†ææ­¤ç±»å¤æ‚æ•°æ®æ–¹é¢æ‹¥æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ç”±äºç¼ºä¹å¯è®¿é—®çš„å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†ï¼Œè¿›å±•ä¸€ç›´å—åˆ°é™åˆ¶ã€‚BreastDCEDLé€šè¿‡æ”¯æŒå¼€å‘å…ˆè¿›æ¨¡å‹æ¥è§£å†³è¿™ä¸€å·®è·ï¼ŒåŒ…æ‹¬éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„æœ€æ–°å˜å‹å™¨æ¶æ„ã€‚ä¸ºäº†å±•ç¤ºå…¶ç¨³å¥å»ºæ¨¡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†åŸºäºå˜å‹å™¨çš„ç¬¬ä¸€ä¸ªä¹³è…ºç™ŒDCE-MRIæ¨¡å‹ï¼Œåˆ©ç”¨åœ¨ä¸‰ä¸ªå¯¹æ¯”é˜¶æ®µï¼ˆé¢„å¯¹æ¯”ã€æ—©æœŸåå¯¹æ¯”å’Œæ™šæœŸåå¯¹æ¯”ï¼‰çš„RGBèåˆå›¾åƒä¸Šè®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰æ¶æ„ã€‚æˆ‘ä»¬çš„ViTæ¨¡å‹åœ¨HR+&#x2F;HER2-æ‚£è€…ä¸­å®ç°äº†æœ€å…ˆè¿›çš„pCRé¢„æµ‹æ€§èƒ½ï¼ˆAUC 0.94ï¼Œå‡†ç¡®ç‡0.93ï¼‰ã€‚BreastDCEDLåŒ…æ‹¬é¢„å®šä¹‰çš„åŸºå‡†åˆ†å‰²ï¼Œä¸ºå¯é‡å¤ç ”ç©¶æä¾›äº†æ¡†æ¶ï¼Œå¹¶åœ¨ä¹³è…ºç™Œæˆåƒä¸­å®ç°äº†ä¸´åºŠæ„ä¹‰å»ºæ¨¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12190v1">PDF</a> </p>
<p><strong>Summary</strong><br>    ä¹³è…ºç™Œä»æ˜¯å…¨çƒç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œæ—©æœŸå‘ç°å’Œå‡†ç¡®çš„ç–—æ•ˆç›‘æµ‹æ˜¯å…³é”®ã€‚æˆ‘ä»¬æ¨å‡ºäº†BreastDCEDLæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªI-SPY1ã€I-SPY2å’ŒDukeç­‰é˜Ÿåˆ—çš„2,070ä¾‹ä¹³è…ºç™Œæ‚£è€…çš„é¢„æ²»ç–—3DåŠ¨æ€å¢å¼ºMRIï¼ˆDCE-MRIï¼‰æ‰«ææ•°æ®ã€‚æ•°æ®é›†åŒ…æ‹¬æ ‡å‡†åŒ–çš„3DNIfTIä½“ç§¯æ•°æ®ã€ç»Ÿä¸€çš„è‚¿ç˜¤æ³¨é‡Šå’Œåè°ƒçš„ä¸´åºŠå…ƒæ•°æ®ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºVision Transformerçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨HR+&#x2F;HER2-æ‚£è€…ä¸­å®ç°é«˜æ°´å¹³çš„pCRé¢„æµ‹æ€§èƒ½ã€‚BreastDCEDLåŒ…æ‹¬é¢„è®¾çš„åŸºå‡†æµ‹è¯•åˆ†å‰²ï¼Œä¸ºå¯é‡å¤çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå¹¶èƒ½è¿›è¡Œä¹³è…ºç™Œæˆåƒçš„ä¸´åºŠæ„ä¹‰å»ºæ¨¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œä»æ˜¯å…¨çƒä¸»è¦çš„ç™Œç—‡æ­»äº¡åŸå› ä¹‹ä¸€ï¼Œå¼ºè°ƒæ—©æœŸæ£€æµ‹å’Œå‡†ç¡®ç›‘æµ‹æ²»ç–—ååº”çš„é‡è¦æ€§ã€‚</li>
<li>BreastDCEDLæ˜¯ä¸€ä¸ªæ–°çš„æ·±åº¦å­¦ä¹ å’ŒåŒ»å­¦å›¾åƒåˆ†æå‡†å¤‡å°±ç»ªçš„æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªå¤šä¸ªæ¥æºçš„æ ‡å‡†åŒ–3D DCE-MRIæ‰«ææ•°æ®ã€‚</li>
<li>æ•°æ®é›†åŒ…æ‹¬ç»Ÿä¸€çš„è‚¿ç˜¤æ³¨é‡Šå’Œåè°ƒçš„ä¸´åºŠå…ƒæ•°æ®ï¼Œå¦‚ç—…ç†å®Œå…¨ååº”ï¼ˆpCRï¼‰ã€æ¿€ç´ å—ä½“ï¼ˆHRï¼‰å’ŒHER2çŠ¶æ€ã€‚</li>
<li>ç¼ºä¹å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†é™åˆ¶äº†æ·±åº¦å­¦ä¹ åœ¨DCE-MRIæ•°æ®åˆ†ææ–¹é¢çš„è¿›å±•ï¼ŒBreastDCEDLå¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚</li>
<li>æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºVision Transformerçš„æ¨¡å‹ï¼Œç”¨äºåˆ†æDCE-MRIæ•°æ®ï¼Œå¹¶åœ¨ç‰¹å®šæ‚£è€…ç¾¤ä½“ä¸­å®ç°äº†é«˜æ°´å¹³çš„pCRé¢„æµ‹æ€§èƒ½ã€‚</li>
<li>BreastDCEDLåŒ…æ‹¬é¢„è®¾çš„åŸºå‡†æµ‹è¯•åˆ†å‰²ï¼Œä¿ƒè¿›äº†ç ”ç©¶çš„å¯é‡å¤æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2191b7282780ed206572eca848d56daa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e7dad6459b40dbbb6956d89e57e1f6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afe39d0b5fcd50e887b1eddacca0241d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09dc62f96e8a397c731928d624089bbd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FAMSeg-Fetal-Femur-and-Cranial-Ultrasound-Segmentation-Using-Feature-Aware-Attention-and-Mamba-Enhancement"><a href="#FAMSeg-Fetal-Femur-and-Cranial-Ultrasound-Segmentation-Using-Feature-Aware-Attention-and-Mamba-Enhancement" class="headerlink" title="FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using   Feature-Aware Attention and Mamba Enhancement"></a>FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using   Feature-Aware Attention and Mamba Enhancement</h2><p><strong>Authors:Jie He, Minglang Chen, Minying Lu, Bocheng Liang, Junming Wei, Guiyan Peng, Jiaxi Chen, Ying Tan</strong></p>
<p>Accurate ultrasound image segmentation is a prerequisite for precise biometrics and accurate assessment. Relying on manual delineation introduces significant errors and is time-consuming. However, existing segmentation models are designed based on objects in natural scenes, making them difficult to adapt to ultrasound objects with high noise and high similarity. This is particularly evident in small object segmentation, where a pronounced jagged effect occurs. Therefore, this paper proposes a fetal femur and cranial ultrasound image segmentation model based on feature perception and Mamba enhancement to address these challenges. Specifically, a longitudinal and transverse independent viewpoint scanning convolution block and a feature perception module were designed to enhance the ability to capture local detail information and improve the fusion of contextual information. Combined with the Mamba-optimized residual structure, this design suppresses the interference of raw noise and enhances local multi-dimensional scanning. The system builds global information and local feature dependencies, and is trained with a combination of different optimizers to achieve the optimal solution. After extensive experimental validation, the FAMSeg network achieved the fastest loss reduction and the best segmentation performance across images of varying sizes and orientations. </p>
<blockquote>
<p>ç²¾ç¡®çš„è¶…å£°å›¾åƒåˆ†å‰²æ˜¯ç²¾ç¡®ç”Ÿç‰©æµ‹å®šå’Œå‡†ç¡®è¯„ä¼°çš„å‰æã€‚ä¾èµ–æ‰‹åŠ¨æç»˜ä¼šå¯¼è‡´é‡å¤§è¯¯å·®ï¼Œå¹¶ä¸”éå¸¸è€—æ—¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åˆ†å‰²æ¨¡å‹æ˜¯åŸºäºè‡ªç„¶åœºæ™¯ä¸­çš„å¯¹è±¡è®¾è®¡çš„ï¼Œè¿™ä½¿å¾—å®ƒä»¬éš¾ä»¥é€‚åº”é«˜å™ªå£°å’Œé«˜ç›¸ä¼¼æ€§çš„è¶…å£°å¯¹è±¡ã€‚è¿™åœ¨å°å‹å¯¹è±¡åˆ†å‰²ä¸­è¡¨ç°å¾—å°¤ä¸ºæ˜æ˜¾ï¼Œä¼šå‡ºç°æ˜æ˜¾çš„é”¯é½¿çŠ¶æ•ˆæœã€‚å› æ­¤ï¼Œé’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾æ„ŸçŸ¥å’ŒMambaå¢å¼ºçš„èƒå„¿è‚¡éª¨å’Œé¢…éª¨è¶…å£°å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡äº†çºµå‘å’Œæ¨ªå‘ç‹¬ç«‹è§†ç‚¹æ‰«æå·ç§¯å—å’Œç‰¹å¾æ„ŸçŸ¥æ¨¡å—ï¼Œä»¥æé«˜æ•è·å±€éƒ¨ç»†èŠ‚ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶æ”¹å–„ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èåˆã€‚ç»“åˆç»è¿‡Mambaä¼˜åŒ–çš„æ®‹å·®ç»“æ„ï¼Œè¿™ç§è®¾è®¡æŠ‘åˆ¶äº†åŸå§‹å™ªå£°çš„å¹²æ‰°ï¼Œå¢å¼ºäº†å±€éƒ¨å¤šç»´æ‰«æã€‚è¯¥ç³»ç»Ÿå»ºç«‹äº†å…¨å±€ä¿¡æ¯å’Œå±€éƒ¨ç‰¹å¾ä¾èµ–å…³ç³»ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨ç»„åˆè¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼ŒFAMSegç½‘ç»œå®ç°äº†æœ€å¿«çš„æŸå¤±é™ä½å’Œæœ€ä½³çš„åˆ†å‰²æ€§èƒ½ï¼Œé€‚ç”¨äºä¸åŒå¤§å°å’Œæ–¹å‘çš„å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07431v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç‰¹å¾æ„ŸçŸ¥å’ŒMambaå¢å¼ºçš„èƒå„¿è‚¡éª¨å’Œé¢…è„‘è¶…å£°å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹éš¾ä»¥é€‚åº”è¶…å£°å›¾åƒé«˜å™ªå£°å’Œé«˜ç›¸ä¼¼åº¦çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å°ç‰©ä½“åˆ†å‰²ä¸­å‡ºç°çš„æ˜æ˜¾é”¯é½¿æ•ˆåº”ã€‚é€šè¿‡è®¾è®¡çºµå‘å’Œæ¨ªå‘ç‹¬ç«‹è§†è§’æ‰«æå·ç§¯å—å’Œç‰¹å¾æ„ŸçŸ¥æ¨¡å—ï¼Œæé«˜äº†æ•æ‰å±€éƒ¨ç»†èŠ‚ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶ä¼˜åŒ–äº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èåˆã€‚ç»“åˆMambaä¼˜åŒ–æ®‹å·®ç»“æ„ï¼Œè¯¥ç³»ç»ŸæŠ‘åˆ¶äº†åŸå§‹å™ªå£°çš„å¹²æ‰°ï¼Œå¢å¼ºäº†å±€éƒ¨å¤šç»´æ‰«æã€‚ç»è¿‡å®éªŒéªŒè¯ï¼ŒFAMSegç½‘ç»œåœ¨ä¸åŒå¤§å°å’Œæ–¹å‘çš„å›¾åƒä¸Šå®ç°äº†æœ€å¿«çš„æŸå¤±é™ä½å’Œæœ€ä½³çš„åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°å›¾åƒåˆ†å‰²å¯¹äºç²¾ç¡®ç”Ÿç‰©è¯†åˆ«å’Œè¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨åº”å¯¹é«˜å™ªå£°å’Œé«˜ç›¸ä¼¼åº¦çš„è¶…å£°å›¾åƒæ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>é’ˆå¯¹å°ç‰©ä½“åˆ†å‰²ä¸­çš„é”¯é½¿æ•ˆåº”é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºç‰¹å¾æ„ŸçŸ¥çš„è¶…å£°å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>è®¾è®¡äº†çºµå‘å’Œæ¨ªå‘ç‹¬ç«‹è§†è§’æ‰«æå·ç§¯å—ï¼Œæé«˜äº†æ•æ‰å±€éƒ¨ç»†èŠ‚ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ç‰¹å¾æ„ŸçŸ¥æ¨¡å—ä»¥ä¼˜åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èåˆã€‚</li>
<li>ç»“åˆMambaä¼˜åŒ–æ®‹å·®ç»“æ„ï¼ŒæŠ‘åˆ¶äº†åŸå§‹å™ªå£°å¹²æ‰°ï¼Œå¢å¼ºäº†å±€éƒ¨å¤šç»´æ‰«æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-50c0fdf1f22f9e2f1082b93ce026ac83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c1c73982632542b13bac20264fd9e87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ef98628f170c202ef7a91771694060b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Joint-Reconstruction-of-Activity-and-Attenuation-in-PET-by-Diffusion-Posterior-Sampling-in-Wavelet-Coefficient-Space"><a href="#Joint-Reconstruction-of-Activity-and-Attenuation-in-PET-by-Diffusion-Posterior-Sampling-in-Wavelet-Coefficient-Space" class="headerlink" title="Joint Reconstruction of Activity and Attenuation in PET by Diffusion   Posterior Sampling in Wavelet Coefficient Space"></a>Joint Reconstruction of Activity and Attenuation in PET by Diffusion   Posterior Sampling in Wavelet Coefficient Space</h2><p><strong>Authors:ClÃ©mentine Phung-Ngoc, Alexandre Bousse, Antoine De Paepe, Hong-Phuong Dang, Olivier Saut, Dimitris Visvikis</strong></p>
<p>Attenuation correction (AC) is necessary for accurate activity quantification in positron emission tomography (PET). Conventional reconstruction methods typically rely on attenuation maps derived from a co-registered computed tomography (CT) or magnetic resonance imaging scan. However, this additional scan may complicate the imaging workflow, introduce misalignment artifacts and increase radiation exposure. In this paper, we propose a joint reconstruction of activity and attenuation (JRAA) approach that eliminates the need for auxiliary anatomical imaging by relying solely on emission data. This framework combines wavelet diffusion model (WDM) and diffusion posterior sampling (DPS) to reconstruct fully three-dimensional (3-D) data. Experimental results show our method outperforms maximum likelihood activity and attenuation (MLAA) and MLAA with UNet-based post processing, and yields high-quality noise-free reconstructions across various count settings when time-of-flight (TOF) information is available. It is also able to reconstruct non-TOF data, although the reconstruction quality significantly degrades in low-count (LC) conditions, limiting its practical effectiveness in such settings. This approach represents a step towards stand-alone PET imaging by reducing the dependence on anatomical modalities while maintaining quantification accuracy, even in low-count scenarios when TOF information is available. </p>
<blockquote>
<p>è¡°å‡æ ¡æ­£ï¼ˆACï¼‰å¯¹äºæ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰ä¸­çš„æ´»åŠ¨å‡†ç¡®é‡åŒ–æ˜¯å¿…è¦çš„ã€‚ä¼ ç»Ÿé‡å»ºæ–¹æ³•é€šå¸¸ä¾èµ–äºä»å…±åŒæ³¨å†Œçš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆ–ç£å…±æŒ¯æˆåƒæ‰«æä¸­å¾—å‡ºçš„è¡°å‡å›¾ã€‚ç„¶è€Œï¼Œé¢å¤–çš„æ‰«æå¯èƒ½ä¼šä½¿æˆåƒå·¥ä½œæµç¨‹å¤æ‚åŒ–ï¼Œå¼•å…¥é”™ä½ä¼ªå½±å¹¶å¢åŠ è¾å°„æš´éœ²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è”åˆé‡å»ºæ´»åŠ¨å’Œè¡°å‡ï¼ˆJRAAï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…ä¾é å‘å°„æ•°æ®ï¼Œæ¶ˆé™¤äº†å¯¹è¾…åŠ©è§£å‰–æˆåƒçš„éœ€æ±‚ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰æ¥é‡å»ºå…¨ä¸‰ç»´ï¼ˆ3-Dï¼‰æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ‰æ—¶é—´é£è¡Œï¼ˆTOFï¼‰ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤šç§è®¡æ•°è®¾ç½®ä¸‹çš„æ€§èƒ½ä¼˜äºæœ€å¤§å¯èƒ½æ€§æ´»åŠ¨å’Œè¡°å‡ï¼ˆMLAAï¼‰ä»¥åŠåŸºäºUNetçš„åå¤„ç†çš„MLAAï¼Œå¹¶äº§ç”Ÿé«˜è´¨é‡çš„æ— å™ªå£°é‡å»ºã€‚è™½ç„¶è¯¥æ–¹æ³•èƒ½å¤Ÿé‡å»ºéTOFæ•°æ®ï¼Œä½†åœ¨ä½è®¡æ•°ï¼ˆLCï¼‰æ¡ä»¶ä¸‹é‡å»ºè´¨é‡æ˜¾è‘—é™ä½ï¼Œè¿™åœ¨æŸäº›è®¾ç½®ä¸­ä¼šé™åˆ¶å…¶å®è·µæ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡å‡å°‘å¯¹è§£å‰–æ¨¡å¼çš„ä¾èµ–è€Œä¿æŒé‡åŒ–å‡†ç¡®æ€§ï¼Œæœç€ç‹¬ç«‹PETæˆåƒè¿ˆå‡ºäº†ä¸€æ­¥ï¼Œå³ä½¿åœ¨æœ‰æ—¶é—´é£è¡Œä¿¡æ¯çš„æƒ…å†µä¸‹ä½è®¡æ•°åœºæ™¯ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18782v2">PDF</a> 10 pages, 9 figures, 1 table</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‘å°„æ•°æ®çš„æ´»åŠ¨è¡°å‡è”åˆé‡å»ºæ–¹æ³•ï¼ˆJRAAï¼‰ï¼Œæ— éœ€é¢å¤–çš„è§£å‰–æˆåƒã€‚è¯¥æ–¹æ³•ç»“åˆäº†å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰è¿›è¡Œå…¨ä¸‰ç»´æ•°æ®é‡å»ºï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æœ‰æ—¶é—´é£è¡Œä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œåœ¨å„ç§è®¡æ•°è®¾ç½®ä¸‹éƒ½èƒ½äº§ç”Ÿé«˜è´¨é‡çš„æ— å™ªå£°é‡å»ºå›¾åƒï¼Œä¼˜äºæœ€å¤§ä¼¼ç„¶æ´»åŠ¨å’Œè¡°å‡é‡å»ºï¼ˆMLAAï¼‰åŠå…¶åŸºäºUNetçš„åå¤„ç†æ–¹æ³•ã€‚å°½ç®¡åœ¨éæ—¶é—´é£è¡Œæ•°æ®é‡å»ºä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä½è®¡æ•°æ¡ä»¶ä¸‹é‡å»ºè´¨é‡æœ‰æ‰€ä¸‹é™ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»æ˜¾ç¤ºå‡ºå‡å°‘å¯¹è§£å‰–æ¨¡æ€ä¾èµ–çš„åŒæ—¶ç»´æŒé‡åŒ–å‡†ç¡®æ€§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡°å‡æ ¡æ­£ï¼ˆACï¼‰å¯¹äºæ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰ä¸­çš„æ´»åŠ¨å®šé‡è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿé‡å»ºæ–¹æ³•ä¾èµ–äºä»å…±æ³¨å†Œçš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆ–ç£å…±æŒ¯æˆåƒæ‰«æè·å¾—çš„è¡°å‡å›¾ï¼Œè¿™å¯èƒ½ä¼šå¤æ‚åŒ–æˆåƒå·¥ä½œæµç¨‹ï¼Œå¼•å…¥é”™ä½ä¼ªå½±å¹¶å¢åŠ è¾å°„æš´éœ²ã€‚</li>
<li>æå‡ºäº†åŸºäºå‘å°„æ•°æ®çš„æ´»åŠ¨å’Œè¡°å‡è”åˆé‡å»ºï¼ˆJRAAï¼‰æ–¹æ³•ï¼Œæ— éœ€é¢å¤–çš„è§£å‰–æˆåƒã€‚</li>
<li>JRAAæ–¹æ³•ç»“åˆäº†å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰å’Œæ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰è¿›è¡Œå…¨ä¸‰ç»´æ•°æ®é‡å»ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒJRAAæ–¹æ³•åœ¨æœ‰æ—¶é—´é£è¡Œä¿¡æ¯çš„æƒ…å†µä¸‹ä¼˜äºæœ€å¤§ä¼¼ç„¶æ´»åŠ¨å’Œè¡°å‡é‡å»ºï¼ˆMLAAï¼‰åŠå…¶åŸºäºUNetçš„åå¤„ç†æ–¹æ³•ã€‚</li>
<li>JRAAèƒ½å¤Ÿåœ¨ä½è®¡æ•°æ¡ä»¶ä¸‹é‡å»ºéæ—¶é—´é£è¡Œæ•°æ®ï¼Œä½†é‡å»ºè´¨é‡æœ‰æ‰€ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c7d1178258ad1d7a8e607eed8446cbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44ede7683cddb537d59457ccedbd19ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d656d0ab49e0b73aebe3246052e0a482.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VIViT-Variable-Input-Vision-Transformer-Framework-for-3D-MR-Image-Segmentation"><a href="#VIViT-Variable-Input-Vision-Transformer-Framework-for-3D-MR-Image-Segmentation" class="headerlink" title="VIViT: Variable-Input Vision Transformer Framework for 3D MR Image   Segmentation"></a>VIViT: Variable-Input Vision Transformer Framework for 3D MR Image   Segmentation</h2><p><strong>Authors:Badhan Kumar Das, Ajay Singh, Gengyan Zhao, Han Liu, Thomas J. Re, Dorin Comaniciu, Eli Gibson, Andreas Maier</strong></p>
<p>Self-supervised pretrain techniques have been widely used to improve the downstream tasksâ€™ performance. However, real-world magnetic resonance (MR) studies usually consist of different sets of contrasts due to different acquisition protocols, which poses challenges for the current deep learning methods on large-scale pretrain and different downstream tasks with different input requirements, since these methods typically require a fixed set of input modalities or, contrasts. To address this challenge, we propose variable-input ViT (VIViT), a transformer-based framework designed for self-supervised pretraining and segmentation finetuning for variable contrasts in each study. With this ability, our approach can maximize the data availability in pretrain, and can transfer the learned knowledge from pretrain to downstream tasks despite variations in input requirements. We validate our method on brain infarct and brain tumor segmentation, where our method outperforms current CNN and ViT-based models with a mean Dice score of 0.624 and 0.883 respectively. These results highlight the efficacy of our design for better adaptability and performance on tasks with real-world heterogeneous MR data. </p>
<blockquote>
<p>è‡ªç›‘ç£é¢„è®­ç»ƒæŠ€æœ¯åœ¨æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ–¹é¢å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºä¸åŒçš„é‡‡é›†åè®®ï¼Œç°å®ä¸–ç•Œä¸­çš„ç£å…±æŒ¯ï¼ˆMRï¼‰ç ”ç©¶é€šå¸¸ç”±ä¸åŒçš„å¯¹æ¯”ç»„æ„æˆï¼Œè¿™ä¸ºå½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¤§å‹é¢„è®­ç»ƒä»¥åŠå…·æœ‰ä¸åŒè¾“å…¥è¦æ±‚çš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦ä¸€ç»„å›ºå®šçš„è¾“å…¥æ¨¡å¼æˆ–å¯¹æ¯”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯å˜è¾“å…¥ViTï¼ˆVIViTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºtransformerçš„æ¡†æ¶ï¼Œæ—¨åœ¨ç”¨äºæ¯é¡¹ç ”ç©¶çš„å¯å˜å¯¹æ¯”ä¸‹çš„è‡ªç›‘ç£é¢„è®­ç»ƒå’Œåˆ†å‰²å¾®è°ƒã€‚å‡­å€Ÿè¿™ä¸€èƒ½åŠ›ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨é¢„è®­ç»ƒæœŸé—´æœ€å¤§åŒ–æ•°æ®å¯ç”¨æ€§ï¼Œå¹¶ä¸”å°½ç®¡è¾“å…¥è¦æ±‚å­˜åœ¨å·®å¼‚ï¼Œä¹Ÿå¯ä»¥å°†ä»é¢„è®­ç»ƒä¸­å­¦ä¹ çš„çŸ¥è¯†è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è„‘æ¢—æ­»å’Œè„‘è‚¿ç˜¤åˆ†å‰²ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå½“å‰çš„CNNå’ŒViTæ¨¡å‹ï¼Œå¹³å‡Diceå¾—åˆ†åˆ†åˆ«ä¸º0.624å’Œ0.883ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬çš„è®¾è®¡åœ¨å®é™…ä¸–ç•Œä¸­å…·æœ‰æ›´å¥½é€‚åº”æ€§å’Œæ€§èƒ½çš„ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹å¼‚è´¨çš„MRæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08693v2">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè‡ªç›‘ç£é¢„è®­ç»ƒå’Œåˆ†å‰²å¾®è°ƒçš„å¯å˜è¾“å…¥ViTï¼ˆVIViTï¼‰æ¡†æ¶ï¼Œç”¨äºå¤„ç†ä¸åŒç ”ç©¶ä¸­ç”±äºé‡‡é›†åè®®ä¸åŒè€Œäº§ç”Ÿçš„å¤šç§å¯¹æ¯”å›¾åƒã€‚è¯¥æ¡†æ¶å¯æœ€å¤§åŒ–é¢„è®­ç»ƒä¸­çš„æ•°æ®å¯ç”¨æ€§ï¼Œå¹¶èƒ½å¤Ÿåœ¨è¾“å…¥è¦æ±‚ä¸åŒçš„ä»»åŠ¡ä¹‹é—´è½¬ç§»çŸ¥è¯†ã€‚åœ¨è„‘æ¢—æ­»å’Œè„‘è‚¿ç˜¤åˆ†å‰²éªŒè¯ä¸­ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰çš„CNNå’ŒViTæ¨¡å‹ï¼Œå¹³å‡Diceå¾—åˆ†åˆ†åˆ«ä¸º0.624å’Œ0.883ã€‚è¿™æ˜¾ç¤ºäº†è¯¥è®¾è®¡åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰æ›´å¥½çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªç›‘ç£é¢„è®­ç»ƒæŠ€æœ¯å¹¿æ³›åº”ç”¨äºæé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>ç”±äºä¸åŒé‡‡é›†åè®®å¯¼è‡´çš„å¤šç§å¯¹æ¯”å›¾åƒæ˜¯åŒ»å­¦é¢†åŸŸå¸¸è§çš„é—®é¢˜ã€‚</li>
<li>å½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å…·æœ‰ä¸åŒè¾“å…¥è¦æ±‚çš„ä¸‹æ¸¸ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>VIViTæ¡†æ¶è®¾è®¡ç”¨äºå¤„ç†å¯å˜è¾“å…¥çš„åŒ»å­¦å›¾åƒè‡ªç›‘ç£é¢„è®­ç»ƒå’Œåˆ†å‰²å¾®è°ƒã€‚</li>
<li>VIViTæ¡†æ¶å¯æœ€å¤§åŒ–é¢„è®­ç»ƒä¸­çš„æ•°æ®å¯ç”¨æ€§ï¼Œå¹¶é€‚åº”ä¸åŒçš„è¾“å…¥è¦æ±‚ã€‚</li>
<li>VIViTæ¡†æ¶åœ¨è„‘æ¢—æ­»å’Œè„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå¹³å‡Diceå¾—åˆ†é«˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29f9ac28c970149d23cd28cf5db1f1db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bb29e7d3953a8af8dc2b19fffe87aad.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="NSegment-Label-specific-Deformations-for-Remote-Sensing-Image-Segmentation"><a href="#NSegment-Label-specific-Deformations-for-Remote-Sensing-Image-Segmentation" class="headerlink" title="NSegment : Label-specific Deformations for Remote Sensing Image   Segmentation"></a>NSegment : Label-specific Deformations for Remote Sensing Image   Segmentation</h2><p><strong>Authors:Yechan Kim, DongHo Yoon, SooYeon Kim, Moongu Jeon</strong></p>
<p>Labeling errors in remote sensing (RS) image segmentation datasets often remain implicit and subtle due to ambiguous class boundaries, mixed pixels, shadows, complex terrain features, and subjective annotator bias. Furthermore, the scarcity of annotated RS data due to high image acquisition and labeling costs complicates training noise-robust models. While sophisticated mechanisms such as label selection or noise correction might address this issue, they tend to increase training time and add implementation complexity. In this letter, we propose NSegment-a simple yet effective data augmentation solution to mitigate this issue. Unlike traditional methods, it applies elastic transformations only to segmentation labels, varying deformation intensity per sample in each training epoch to address annotation inconsistencies. Experimental results demonstrate that our approach improves the performance of RS image segmentation on various state-of-the-art models. </p>
<blockquote>
<p>é¥æ„Ÿï¼ˆRSï¼‰å›¾åƒåˆ†å‰²æ•°æ®é›†å­˜åœ¨æ ‡ç­¾é”™è¯¯çš„é—®é¢˜å¸¸å¸¸éšæ™¦ä¸”å¾®å¦™ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç±»åˆ«è¾¹ç•Œæ¨¡ç³Šã€åƒç´ æ··åˆã€é˜´å½±ã€åœ°å½¢ç‰¹å¾å¤æ‚ä»¥åŠä¸»è§‚æ ‡æ³¨è€…åè§ç­‰å› ç´ ã€‚æ­¤å¤–ï¼Œç”±äºå›¾åƒé‡‡é›†å’Œæ ‡æ³¨çš„é«˜æˆæœ¬ï¼Œå¯¼è‡´æ ‡æ³¨çš„é¥æ„Ÿæ•°æ®ç¨€ç¼ºï¼Œè¿™å¢åŠ äº†è®­ç»ƒæŠ—å™ªå£°æ¨¡å‹çš„éš¾åº¦ã€‚è™½ç„¶æ ‡ç­¾é€‰æ‹©æˆ–å™ªå£°ä¿®æ­£ç­‰å¤æ‚æœºåˆ¶å¯èƒ½è§£å†³æ­¤é—®é¢˜ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šå¢åŠ è®­ç»ƒæ—¶é—´å¹¶å¢åŠ å®ç°å¤æ‚æ€§ã€‚åœ¨æœ¬ä¿¡ä¸­ï¼Œæˆ‘ä»¬æå‡ºNSegmentâ€”â€”ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆæ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œå®ƒä»…å¯¹åˆ†å‰²æ ‡ç­¾åº”ç”¨å¼¹æ€§è½¬æ¢ï¼Œå¹¶åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸä¸­å¯¹æ¯ä¸ªæ ·æœ¬çš„å˜å½¢å¼ºåº¦è¿›è¡Œå˜åŒ–ï¼Œä»¥è§£å†³æ ‡æ³¨ä¸ä¸€è‡´çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æœ€å…ˆè¿›çš„æ¨¡å‹ä¸Šæé«˜äº†é¥æ„Ÿå›¾åƒåˆ†å‰²çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19634v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºNSegmentçš„æ•°æ®å¢å¼ºè§£å†³æ–¹æ¡ˆï¼Œé’ˆå¯¹é¥æ„Ÿå›¾åƒåˆ†å‰²æ•°æ®é›†å­˜åœ¨çš„æ ‡ç­¾é”™è¯¯é—®é¢˜ï¼Œé€šè¿‡å¼¹æ€§å˜æ¢è°ƒæ•´åˆ†å‰²æ ‡ç­¾ï¼Œæ¯ä¸ªè®­ç»ƒå‘¨æœŸå¯¹æ ·æœ¬çš„å˜å½¢å¼ºåº¦è¿›è¡Œå˜åŒ–ï¼Œä»¥è§£å†³æ ‡æ³¨ä¸ä¸€è‡´çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†é¥æ„Ÿå›¾åƒåˆ†å‰²çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒåˆ†å‰²æ•°æ®é›†çš„æ ‡ç­¾é”™è¯¯é—®é¢˜ç”±äºæ¨¡ç³Šç±»è¾¹ç•Œã€æ··åˆåƒç´ ã€é˜´å½±ã€å¤æ‚åœ°å½¢ç‰¹å¾å’Œä¸»è§‚æ ‡æ³¨è€…åè§è€Œéšæ€§å­˜åœ¨ã€‚</li>
<li>æ ‡æ³¨é”™è¯¯å’Œç¼ºä¹æ ‡æ³¨çš„é¥æ„Ÿæ•°æ®ä½¿å¾—è®­ç»ƒå™ªå£°é²æ£’æ€§æ¨¡å‹å˜å¾—å¤æ‚ã€‚</li>
<li>å½“å‰è§£å†³æœºåˆ¶å¦‚æ ‡ç­¾é€‰æ‹©æˆ–å™ªå£°æ ¡æ­£å¯èƒ½å¢åŠ è®­ç»ƒæ—¶é—´å’Œå®æ–½å¤æ‚æ€§ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºNSegmentçš„æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡å¼¹æ€§å˜æ¢è°ƒæ•´åˆ†å‰²æ ‡ç­¾ï¼Œä»¥ç®€å•æœ‰æ•ˆçš„æ–¹å¼ç¼“è§£æ ‡æ³¨ä¸ä¸€è‡´é—®é¢˜ã€‚</li>
<li>NSegmentæ–¹æ³•é’ˆå¯¹æ¯ä¸ªè®­ç»ƒå‘¨æœŸä¸­çš„æ¯ä¸ªæ ·æœ¬åº”ç”¨ä¸åŒçš„å˜å½¢å¼ºåº¦ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒNSegmentæ–¹æ³•æé«˜äº†é¥æ„Ÿå›¾åƒåˆ†å‰²çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9b24907eb004428f643cf875f110087.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33c03593276d3c86ae89cf8973bb855d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89129005f6a96c419101dbc1481fe831.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec5e4dc6794ac4c51bb073620124620b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d173efb4bf8aca2068de90e2d0fdb6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df0dded65b49c01015169a7707331211.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8804dd7d7b80ca6b48f81ba93190aa99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9aeab011acb4b33988bda713a90f4fd6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Simple-Baseline-with-Single-encoder-for-Referring-Image-Segmentation"><a href="#A-Simple-Baseline-with-Single-encoder-for-Referring-Image-Segmentation" class="headerlink" title="A Simple Baseline with Single-encoder for Referring Image Segmentation"></a>A Simple Baseline with Single-encoder for Referring Image Segmentation</h2><p><strong>Authors:Seonghoon Yu, Ilchae Jung, Byeongju Han, Taeoh Kim, Yunho Kim, Dongyoon Wee, Jeany Son</strong></p>
<p>Referring image segmentation (RIS) requires dense vision-language interactions between visual pixels and textual words to segment objects based on a given description. However, commonly adapted dual-encoders in RIS, e.g., Swin transformer and BERT (uni-modal encoders) or CLIP (a multi-modal dual-encoder), lack dense multi-modal interactions during pre-training, leading to a gap with a pixel-level RIS task. To bridge this gap, existing RIS methods often rely on multi-modal fusion modules that interact two encoders, but this approach leads to high computational costs. In this paper, we present a novel RIS method with a single-encoder, i.e., BEiT-3, maximizing the potential of shared self-attention across all framework components. This enables seamless interactions of two modalities from input to final prediction, producing granularly aligned multi-modal features. Furthermore, we propose lightweight yet effective decoder modules, a Shared FPN and a Shared Mask Decoder, which contribute to the high efficiency of our model. Our simple baseline with a single encoder achieves outstanding performances on the RIS benchmark datasets while maintaining computational efficiency, compared to the most recent SoTA methods based on dual-encoders. </p>
<blockquote>
<p>å›¾åƒå‚ç…§åˆ†å‰²ï¼ˆRISï¼‰éœ€è¦è§†è§‰åƒç´ å’Œæ–‡æœ¬å•è¯ä¹‹é—´çš„å¯†é›†è§†è§‰è¯­è¨€äº¤äº’ï¼Œä»¥ä¾¿æ ¹æ®ç»™å®šæè¿°æ¥åˆ†å‰²å¯¹è±¡ã€‚ç„¶è€Œï¼ŒRISä¸­å¸¸ç”¨çš„åŒç¼–ç å™¨ï¼ˆä¾‹å¦‚Swin transformerå’ŒBERTï¼ˆå•æ¨¡æ€ç¼–ç å™¨ï¼‰æˆ–CLIPï¼ˆå¤šæ¨¡æ€åŒç¼–ç å™¨ï¼‰ï¼‰åœ¨é¢„è®­ç»ƒæœŸé—´ç¼ºä¹å¯†é›†çš„å¤šæ¨¡æ€äº¤äº’ï¼Œè¿™ä¸åƒç´ çº§çš„RISä»»åŠ¡ä¹‹é—´å­˜åœ¨å·®è·ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œç°æœ‰çš„RISæ–¹æ³•é€šå¸¸ä¾èµ–äºå¤šæ¨¡æ€èåˆæ¨¡å—æ¥ä½¿ä¸¤ä¸ªç¼–ç å™¨è¿›è¡Œäº¤äº’ï¼Œä½†è¿™ä¼šå¯¼è‡´è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„RISæ–¹æ³•ï¼Œä½¿ç”¨å•ä¸€ç¼–ç å™¨BEiT-3ï¼Œæœ€å¤§é™åº¦åœ°å‘æŒ¥æ‰€æœ‰æ¡†æ¶ç»„ä»¶å…±äº«çš„è‡ªæ³¨æ„æœºåˆ¶çš„æ½œåŠ›ã€‚è¿™å®ç°äº†ä»è¾“å…¥åˆ°æœ€ç»ˆé¢„æµ‹çš„ä¸¤ä¸ªæ¨¡æ€æ— ç¼äº¤äº’ï¼Œç”Ÿæˆç²¾ç»†å¯¹é½çš„å¤šæ¨¡æ€ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†è½»é‡çº§ä½†æœ‰æ•ˆçš„è§£ç å™¨æ¨¡å—ï¼Œå³å…±äº«FPNå’Œå…±äº«Mask Decoderï¼Œè¿™æœ‰åŠ©äºæå‡æ¨¡å‹çš„é«˜æ•ˆç‡ã€‚ä½¿ç”¨å•ä¸€ç¼–ç å™¨çš„ç®€å•åŸºçº¿æ¨¡å‹åœ¨RISåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼Œä¸åŸºäºåŒç¼–ç å™¨çš„æœ€æ–°æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15521v3">PDF</a> arXiv pre-print</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å‚ç…§å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰éœ€è¦è§†è§‰åƒç´ å’Œæ–‡æœ¬è¯æ±‡ä¹‹é—´çš„å¯†é›†è§†è§‰è¯­è¨€äº¤äº’ã€‚ç„¶è€Œï¼Œå¸¸ç”¨çš„åŒç¼–ç å™¨ï¼ˆå¦‚Swin transformerå’ŒBERTç­‰å•æ¨¡æ€ç¼–ç å™¨æˆ–CLIPç­‰å¤šæ¨¡æ€åŒç¼–ç å™¨ï¼‰åœ¨é¢„è®­ç»ƒæœŸé—´ç¼ºä¹å¯†é›†çš„å¤šæ¨¡æ€äº¤äº’ï¼Œå¯¼è‡´ä¸åƒç´ çº§RISä»»åŠ¡ä¹‹é—´å­˜åœ¨å·®è·ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å•ç¼–ç å™¨å‚ç…§å›¾åƒåˆ†å‰²æ–¹æ³•â€”â€”BEiT-3æ¨¡å‹ï¼Œå……åˆ†åˆ©ç”¨å„ç»„ä»¶ä¹‹é—´çš„å…±äº«è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä¿ƒè¿›ä¸¤ä¸ªæ¨¡æ€åœ¨å…¨è¿‡ç¨‹ä¸­æ— ç¼äº¤äº’ï¼Œå®ç°ç²¾ç»†åŒ–å¤šæ¨¡æ€ç‰¹å¾å¯¹é½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†è½»é‡çº§ä½†é«˜æ•ˆçš„è§£ç å™¨æ¨¡å—ï¼ŒåŒ…æ‹¬å…±äº«FPNå’Œå…±äº«æ©è†œè§£ç å™¨ï¼Œæé«˜äº†æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ã€‚ç›¸è¾ƒäºåŸºäºåŒç¼–ç å™¨çš„æœ€æ–°æ–¹æ³•ï¼Œæœ¬æ¨¡å‹ä»¥å•ç¼–ç å™¨ä¸ºåŸºç¡€ï¼Œåœ¨å‚ç…§å›¾åƒåˆ†å‰²åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‚ç…§å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰éœ€è¦è§†è§‰åƒç´ å’Œæ–‡æœ¬è¯æ±‡ä¹‹é—´çš„å¯†é›†äº¤äº’ã€‚</li>
<li>å¸¸è§åŒç¼–ç å™¨åœ¨é¢„è®­ç»ƒæ—¶ç¼ºä¹å¤šæ¨¡æ€å¯†é›†äº¤äº’ï¼Œé€ æˆä¸åƒç´ çº§RISä»»åŠ¡çš„å·®è·ã€‚</li>
<li>å¼•å…¥æ–°å‹å•ç¼–ç å™¨æ–¹æ³•â€”â€”BEiT-3æ¨¡å‹ï¼Œé€šè¿‡å…±äº«è‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°æ— ç¼çš„å¤šæ¨¡æ€äº¤äº’ã€‚</li>
<li>BEiT-3æ¨¡å‹å…·å¤‡é«˜æ•ˆçš„è§£ç å™¨æ¨¡å—ï¼Œå¦‚å…±äº«FPNå’Œå…±äº«æ©è†œè§£ç å™¨ã€‚</li>
<li>å•ç¼–ç å™¨çš„BEiT-3æ¨¡å‹åœ¨å‚ç…§å›¾åƒåˆ†å‰²åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½å“è¶Šã€‚</li>
<li>è¯¥æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–æœ€æ–°æ–¹æ³•ï¼Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.15521">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5cc865b3387643933a3a6ff55de94f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23aca17115f60c2bf64df9a33068885f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2755cafd9fdcd43015831994488d2106.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14cb7bc065690c00d04ab5ba3df663e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b0e63fc3ddd63d3b28980084715e960.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Advancing-oncology-with-federated-learning-transcending-boundaries-in-breast-lung-and-prostate-cancer-A-systematic-review"><a href="#Advancing-oncology-with-federated-learning-transcending-boundaries-in-breast-lung-and-prostate-cancer-A-systematic-review" class="headerlink" title="Advancing oncology with federated learning: transcending boundaries in   breast, lung, and prostate cancer. A systematic review"></a>Advancing oncology with federated learning: transcending boundaries in   breast, lung, and prostate cancer. A systematic review</h2><p><strong>Authors:Anshu Ankolekar, Sebastian Boie, Maryam Abdollahyan, Emanuela Gadaleta, Seyed Alireza Hasheminasab, Guang Yang, Charles Beauville, Nikolaos Dikaios, George Anthony Kastis, Michael Bussmann, Sara Khalid, Hagen Kruger, Philippe Lambin, Giorgos Papanastasiou</strong></p>
<p>Federated Learning (FL) has emerged as a promising solution to address the limitations of centralised machine learning (ML) in oncology, particularly in overcoming privacy concerns and harnessing the power of diverse, multi-center data. This systematic review synthesises current knowledge on the state-of-the-art FL in oncology, focusing on breast, lung, and prostate cancer. Distinct from previous surveys, our comprehensive review critically evaluates the real-world implementation and impact of FL on cancer care, demonstrating its effectiveness in enhancing ML generalisability, performance and data privacy in clinical settings and data. We evaluated state-of-the-art advances in FL, demonstrating its growing adoption amid tightening data privacy regulations. FL outperformed centralised ML in 15 out of the 25 studies reviewed, spanning diverse ML models and clinical applications, and facilitating integration of multi-modal information for precision medicine. Despite the current challenges identified in reproducibility, standardisation and methodology across studies, the demonstrable benefits of FL in harnessing real-world data and addressing clinical needs highlight its significant potential for advancing cancer research. We propose that future research should focus on addressing these limitations and investigating further advanced FL methods, to fully harness data diversity and realise the transformative power of cutting-edge FL in cancer care. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œæ­£åœ¨è§£å†³é›†ä¸­å¼æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åœ¨è‚¿ç˜¤å­¦ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å…‹æœéšç§æ‹…å¿§å’Œåˆ©ç”¨å¤šæ ·åŒ–ã€å¤šä¸­å¿ƒæ•°æ®çš„èƒ½åŠ›æ–¹é¢ã€‚è¿™ç¯‡ç³»ç»Ÿæ€§ç»¼è¿°ç»¼åˆäº†å½“å‰å…³äºè‚¿ç˜¤å­¦é¢†åŸŸå‰æ²¿è”é‚¦å­¦ä¹ çš„çŸ¥è¯†ï¼Œé‡ç‚¹å…³æ³¨ä¹³è…ºç™Œã€è‚ºç™Œå’Œå‰åˆ—è…ºç™Œã€‚ä¸åŒäºä¹‹å‰çš„è°ƒæŸ¥ï¼Œæˆ‘ä»¬çš„å…¨é¢è¯„è¿°å¯¹è”é‚¦å­¦ä¹ åœ¨ç™Œç—‡æŠ¤ç†ä¸­çš„ç°å®å®æ–½å’Œå½±å“è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜æœºå™¨å­¦ä¹ åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„é€šç”¨æ€§ã€æ€§èƒ½å’Œéšç§ä¿æŠ¤æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¯„ä¼°äº†è”é‚¦å­¦ä¹ å‰æ²¿çš„æœ€æ–°è¿›å±•ï¼Œè¯æ˜äº†å…¶åœ¨æ—¥ç›Šä¸¥æ ¼çš„æ•°æ®éšç§æ³•è§„ä¸‹æ—¥ç›Šæ™®åŠã€‚åœ¨æ‰€å®¡æŸ¥çš„25é¡¹ç ”ç©¶ä¸­ï¼Œè”é‚¦å­¦ä¹ åœ¨15é¡¹ç ”ç©¶ä¸­è¡¨ç°ä¼˜äºé›†ä¸­å¼æœºå™¨å­¦ä¹ ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„æœºå™¨å­¦ä¹ æ¨¡å‹å’Œä¸´åºŠåº”ç”¨ï¼Œä¿ƒè¿›äº†å¤šæ¨¡å¼ä¿¡æ¯çš„æ•´åˆç”¨äºç²¾å‡†åŒ»ç–—ã€‚å°½ç®¡åœ¨ç ”ç©¶ä¸­å­˜åœ¨å¯é‡å¤æ€§ã€æ ‡å‡†åŒ–å’Œæ–¹æ³•å­¦æ–¹é¢çš„å½“å‰æŒ‘æˆ˜ï¼Œä½†è”é‚¦å­¦ä¹ åœ¨åˆ©ç”¨çœŸå®ä¸–ç•Œæ•°æ®å’Œæ»¡è¶³ä¸´åºŠéœ€æ±‚æ–¹é¢çš„æ˜æ˜¾ä¼˜åŠ¿å‡¸æ˜¾äº†å…¶æ¨åŠ¨ç™Œç—‡ç ”ç©¶çš„å·¨å¤§æ½œåŠ›ã€‚æˆ‘ä»¬å»ºè®®åœ¨æœªæ¥çš„ç ”ç©¶ä¸­ï¼Œåº”ä¾§é‡äºè§£å†³è¿™äº›å±€é™æ€§ï¼Œå¹¶è¿›ä¸€æ­¥æ¢è®¨å…ˆè¿›çš„è”é‚¦å­¦ä¹ æ–¹æ³•ï¼Œä»¥å……åˆ†åˆ©ç”¨æ•°æ®çš„å¤šæ ·æ€§å¹¶å®ç°è”é‚¦å­¦ä¹ åœ¨ç™Œç—‡æŠ¤ç†ä¸­çš„å˜é©æ€§åŠ›é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.05249v2">PDF</a> 5 Figures, 3 Tables, 1 Supplementary Table</p>
<p><strong>Summary</strong><br>     è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰åœ¨è‚¿ç˜¤å­¦é¢†åŸŸä¸­å¤®æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰çš„å±€é™æ€§ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å…‹æœéšç§æ‹…å¿§å’Œåˆ©ç”¨å¤šæ ·åŒ–ã€å¤šä¸­å¿ƒæ•°æ®æ–¹é¢ã€‚æœ¬æ–‡ç»¼è¿°äº†å½“å‰å…³äºFLåœ¨è‚¿ç˜¤å­¦é¢†åŸŸçš„æœ€æ–°çŸ¥è¯†ï¼Œé‡ç‚¹å…³æ³¨ä¹³è…ºç™Œã€è‚ºç™Œå’Œå‰åˆ—è…ºç™Œã€‚ä¸ä»¥å¾€è°ƒæŸ¥ä¸åŒï¼Œæˆ‘ä»¬çš„å…¨é¢è¯„ä»·äº†FLåœ¨ç°å®ä¸–ç•Œçš„å®æ–½åŠå…¶å¯¹ç™Œç—‡æŠ¤ç†çš„å½±å“ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜æœºå™¨å­¦ä¹ é€šç”¨æ€§ã€æ€§èƒ½å’Œä¸´åºŠç¯å¢ƒä¸­çš„æ•°æ®éšç§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å°½ç®¡åœ¨é‡å¤æ€§ã€æ ‡å‡†åŒ–å’Œæ–¹æ³•å­¦æ–¹é¢å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼Œä½†FLåœ¨åˆ©ç”¨ç°å®ä¸–ç•Œæ•°æ®å’Œæ»¡è¶³ä¸´åºŠéœ€æ±‚æ–¹é¢çš„æ˜æ˜¾ä¼˜åŠ¿ï¼Œå‡¸æ˜¾å…¶æ¨è¿›ç™Œç—‡ç ”ç©¶çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰åœ¨è‚¿ç˜¤å­¦é¢†åŸŸå±•ç°å‡ºè§£å†³ä¸­å¤®æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å±€é™æ€§çš„æ½œåŠ›ã€‚</li>
<li>FLæœ‰åŠ©äºå…‹æœéšç§æ‹…å¿§å¹¶æœ‰æ•ˆåˆ©ç”¨å¤šæ ·åŒ–ã€å¤šä¸­å¿ƒæ•°æ®ã€‚</li>
<li>ç»¼è¿°èšç„¦äºä¹³è…ºç™Œã€è‚ºç™Œå’Œå‰åˆ—è…ºç™Œçš„FLæœ€æ–°è¿›å±•ã€‚</li>
<li>FLåœ¨ç°å®ä¸–ç•Œçš„å®æ–½åŠå¯¹ç™Œç—‡æŠ¤ç†çš„å½±å“å¾—åˆ°äº†è¯„ä»·ã€‚</li>
<li>FLåœ¨æé«˜æœºå™¨å­¦ä¹ é€šç”¨æ€§ã€æ€§èƒ½å’Œä¸´åºŠç¯å¢ƒæ•°æ®éšç§æ–¹é¢æœ‰æ•ˆã€‚</li>
<li>ç›®å‰é¢ä¸´é‡å¤æ€§ã€æ ‡å‡†åŒ–å’Œæ–¹æ³•å­¦æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>FLåœ¨åˆ©ç”¨ç°å®æ•°æ®ã€æ»¡è¶³ä¸´åºŠéœ€æ±‚æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä»¥åŠå…¶å¯¹ç™Œç—‡ç ”ç©¶çš„æ½œåŠ›å¾—åˆ°äº†å¼ºè°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.05249">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f177c6677918f344dbf26a2a4f922ad1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4d6b155e94dafb67de034881f6521dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74127246f3cc52ebc3dbb985635cb641.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb4b9ca28c8e9d916862a0c218c7704d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07526665f79f9f180d388f45625e4899.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-245e5d82fbf4164f41f019ac8dd1e170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1f6ec39aa1b72f19d10f66274e574c5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Adaptive-Sensitivity-Analysis-for-Robust-Augmentation-against-Natural-Corruptions-in-Image-Segmentation"><a href="#Adaptive-Sensitivity-Analysis-for-Robust-Augmentation-against-Natural-Corruptions-in-Image-Segmentation" class="headerlink" title="Adaptive Sensitivity Analysis for Robust Augmentation against Natural   Corruptions in Image Segmentation"></a>Adaptive Sensitivity Analysis for Robust Augmentation against Natural   Corruptions in Image Segmentation</h2><p><strong>Authors:Laura Zheng, Wenjie Wei, Tony Wu, Jacob Clements, Shreelekha Revankar, Andre Harrison, Yu Shen, Ming C. Lin</strong></p>
<p>Achieving robustness in image segmentation models is challenging due to the fine-grained nature of pixel-level classification. These models, which are crucial for many real-time perception applications, particularly struggle when faced with natural corruptions in the wild for autonomous systems. While sensitivity analysis can help us understand how input variables influence model outputs, its application to natural and uncontrollable corruptions in training data is computationally expensive. In this work, we present an adaptive, sensitivity-guided augmentation method to enhance robustness against natural corruptions. Our sensitivity analysis on average runs 10x faster and requires about 200x less storage than previous sensitivity analysis, enabling practical, on-the-fly estimation during training for a model-free augmentation policy. With minimal fine-tuning, our sensitivity-guided augmentation method achieves improved robustness on both real-world and synthetic datasets compared to state-of-the-art data augmentation techniques in image segmentation. Code implementation for this work can be found at: <a target="_blank" rel="noopener" href="https://github.com/laurayuzheng/SensAug">https://github.com/laurayuzheng/SensAug</a>. </p>
<blockquote>
<p>å®ç°å›¾åƒåˆ†å‰²æ¨¡å‹çš„ç¨³å¥æ€§æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºåƒç´ çº§åˆ†ç±»çš„ç²¾ç»†æ€§è´¨ã€‚è¿™äº›æ¨¡å‹å¯¹äºè®¸å¤šå®æ—¶æ„ŸçŸ¥åº”ç”¨è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹è‡ªä¸»ç³»ç»Ÿé‡å¤–è‡ªç„¶è…èš€æ—¶ç‰¹åˆ«å›°éš¾ã€‚è™½ç„¶æ•æ„Ÿæ€§åˆ†æå¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£è¾“å…¥å˜é‡å¦‚ä½•å½±å“æ¨¡å‹è¾“å‡ºï¼Œä½†å…¶åœ¨è®­ç»ƒæ•°æ®ä¸­è‡ªç„¶å’Œä¸å¯æ§åˆ¶çš„è…èš€çš„åº”ç”¨è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„ã€ä»¥æ•æ„Ÿæ€§ä¸ºæŒ‡å¯¼çš„å¢å¼ºæ–¹æ³•ï¼Œä»¥æé«˜å¯¹è‡ªç„¶è…èš€çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ•æ„Ÿæ€§åˆ†æå¹³å‡è¿è¡Œé€Ÿåº¦æ¯”ä»¥å‰å¿«10å€ï¼Œå­˜å‚¨éœ€æ±‚å¤§çº¦å‡å°‘200å€ï¼Œå®ç°äº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œå®é™…ã€å³æ—¶ä¼°è®¡ï¼Œç”¨äºæ— æ¨¡å‹å¢å¼ºç­–ç•¥ã€‚é€šè¿‡æœ€å°çš„å¾®è°ƒï¼Œä¸å›¾åƒåˆ†å‰²ä¸­æœ€æ–°æ•°æ®å¢å¼ºæŠ€æœ¯ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ä»¥æ•æ„Ÿæ€§ä¸ºæŒ‡å¯¼çš„å¢å¼ºæ–¹æ³•åœ¨ç°å®ä¸–ç•Œå’Œåˆæˆæ•°æ®é›†ä¸Šå®ç°äº†æ›´é«˜çš„ç¨³å¥æ€§ã€‚è¯¥å·¥ä½œçš„ä»£ç å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/laurayuzheng/SensAug">https://github.com/laurayuzheng/SensAug</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01425v5">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„ã€åŸºäºæ•æ„Ÿæ€§çš„å¢å¼ºæ–¹æ³•ï¼Œä»¥æé«˜å›¾åƒåˆ†å‰²æ¨¡å‹å¯¹è‡ªç„¶è…è´¥çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œæ•æ„Ÿæ€§åˆ†æï¼Œå¿«é€Ÿè¯†åˆ«å½±å“æ¨¡å‹è¾“å‡ºçš„å…³é”®è¾“å…¥å˜é‡ï¼Œå¹¶æ®æ­¤è¿›è¡Œå¢å¼ºã€‚ä¸ä¼ ç»Ÿçš„æ•æ„Ÿæ€§åˆ†ææ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•è¿è¡Œé€Ÿåº¦å¿«10å€ï¼Œå­˜å‚¨éœ€æ±‚é™ä½çº¦200å€ï¼Œå¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶ä¼°è®¡æ— æ¨¡å‹å¢å¼ºç­–ç•¥ã€‚é€šè¿‡å¾®è°ƒï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šçš„é²æ£’æ€§å‡ä¼˜äºå½“å‰å…ˆè¿›çš„å›¾åƒåˆ†å‰²æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ†å‰²æ¨¡å‹çš„é²æ£’æ€§é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯é¢å¯¹è‡ªç„¶è…è´¥æ—¶ã€‚</li>
<li>æ•æ„Ÿæ€§åˆ†ææœ‰åŠ©äºç†è§£è¾“å…¥å˜é‡å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ï¼Œä½†åº”ç”¨äºè‡ªç„¶å’Œä¸å¯æ§çš„è…è´¥è®­ç»ƒæ•°æ®è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„ã€åŸºäºæ•æ„Ÿæ€§çš„å¢å¼ºæ–¹æ³•ï¼Œä»¥æé«˜æ¨¡å‹å¯¹è‡ªç„¶è…è´¥çš„é²æ£’æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¿«é€Ÿè¯†åˆ«å…³é”®è¾“å…¥å˜é‡å¹¶è¿›è¡Œå¢å¼ºï¼Œå®ç°äº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å®æ—¶ä¼°è®¡æ— æ¨¡å‹å¢å¼ºç­–ç•¥ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„è¿è¡Œé€Ÿåº¦å¿«10å€ï¼Œå­˜å‚¨éœ€æ±‚é™ä½çº¦200å€ã€‚</li>
<li>é€šè¿‡å¾®è°ƒï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šçš„é²æ£’æ€§å‡ä¼˜äºå½“å‰å…ˆè¿›çš„å›¾åƒåˆ†å‰²æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-608a9e6798f578d46affc81cc42928cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5930adab75637e25ab4ea4b0a7fbfc6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c4799a417948cbcd54162f15f528bc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-885a476d524245fe25ac8183afcef77b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="I2I-Mamba-Multi-modal-medical-image-synthesis-via-selective-state-space-modeling"><a href="#I2I-Mamba-Multi-modal-medical-image-synthesis-via-selective-state-space-modeling" class="headerlink" title="I2I-Mamba: Multi-modal medical image synthesis via selective state space   modeling"></a>I2I-Mamba: Multi-modal medical image synthesis via selective state space   modeling</h2><p><strong>Authors:Omer F. Atli, Bilal Kabas, Fuat Arslan, Arda C. Demirtas, Mahmut Yurt, Onat Dalmaz, Tolga Ã‡ukur</strong></p>
<p>Multi-modal medical image synthesis involves nonlinear transformation of tissue signals between source and target modalities, where tissues exhibit contextual interactions across diverse spatial distances. As such, the utility of a network architecture in synthesis depends on its ability to express these contextual features. Convolutional neural networks (CNNs) offer high local precision at the expense of poor sensitivity to long-range context. While transformers promise to alleviate this issue, they suffer from an unfavorable trade-off between sensitivity to long- versus short-range context due to the intrinsic complexity of attention filters. To effectively capture contextual features while avoiding the complexity-driven trade-offs, here we introduce a novel multi-modal synthesis method, I2I-Mamba, based on the state space modeling (SSM) framework. Focusing on semantic representations across a hybrid residual architecture, I2I-Mamba leverages novel dual-domain Mamba (ddMamba) blocks for complementary contextual modeling in image and Fourier domains, while maintaining spatial precision with convolutional layers. Diverting from conventional raster-scan trajectories, ddMamba leverages novel SSM operators based on a spiral-scan trajectory to learn context with enhanced radial coverage and angular isotropy, and a channel-mixing layer to aggregate context across the channel dimension. Comprehensive demonstrations on multi-contrast MRI and MRI-CT protocols indicate that I2I-Mamba offers superior performance against state-of-the-art CNNs, transformers and SSMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆæ¶‰åŠæºæ¨¡æ€å’Œç›®æ ‡æ¨¡æ€ä¹‹é—´ç»„ç»‡ä¿¡å·çš„éçº¿æ€§è½¬æ¢ï¼Œå…¶ä¸­ç»„ç»‡åœ¨å¤šç§ç©ºé—´è·ç¦»ä¸Šè¡¨ç°å‡ºä¸Šä¸‹æ–‡äº¤äº’ã€‚å› æ­¤ï¼Œåˆæˆä¸­çš„ç½‘ç»œæ¶æ„çš„å®ç”¨æ€§å–å†³äºå…¶è¡¨è¾¾è¿™äº›ä¸Šä¸‹æ–‡ç‰¹å¾çš„èƒ½åŠ›ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è™½ç„¶èƒ½åœ¨æœ¬åœ°æä¾›è¾ƒé«˜çš„ç²¾ç¡®åº¦ï¼Œä½†åœ¨å¯¹è¿œç¨‹ä¸Šä¸‹æ–‡çš„æ•æ„Ÿæ€§æ–¹é¢è¡¨ç°è¾ƒå·®ã€‚è™½ç„¶å˜å‹å™¨æœ‰æœ›ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†ç”±äºæ³¨æ„åŠ›è¿‡æ»¤å™¨çš„å›ºæœ‰å¤æ‚æ€§ï¼Œå®ƒä»¬åœ¨å¤„ç†è¿œç¨‹ä¸è¿‘è·ç¦»ä¸Šä¸‹æ–‡ä¹‹é—´çš„æ•æ„Ÿæ€§æ—¶é¢ä¸´ä¸åˆ©çš„æƒè¡¡ã€‚ä¸ºäº†æœ‰æ•ˆåœ°æ•è·ä¸Šä¸‹æ–‡ç‰¹å¾å¹¶é¿å…å¤æ‚æ€§é©±åŠ¨çš„æƒè¡¡ï¼Œæˆ‘ä»¬åœ¨æ­¤å¼•å…¥äº†ä¸€ç§åŸºäºçŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼ˆSSMï¼‰æ¡†æ¶çš„æ–°å‹å¤šæ¨¡æ€åˆæˆæ–¹æ³•I2I-Mambaã€‚I2I-Mambaä¸“æ³¨äºæ··åˆæ®‹å·®æ¶æ„ä¸­çš„è¯­ä¹‰è¡¨ç¤ºï¼Œåˆ©ç”¨æ–°å‹åŒåŸŸMambaï¼ˆddMambaï¼‰å—åœ¨å›¾åƒå’Œå‚…é‡Œå¶åŸŸä¸­è¿›è¡Œäº’è¡¥çš„ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼ŒåŒæ—¶åˆ©ç”¨å·ç§¯å±‚ä¿æŒç©ºé—´ç²¾åº¦ã€‚ddMambaåç¦»äº†ä¼ ç»Ÿçš„æ‰«æè½¨è¿¹ï¼Œé‡‡ç”¨åŸºäºèºæ—‹æ‰«æè½¨è¿¹çš„æ–°å‹SSMè¿ç®—ç¬¦æ¥å­¦ä¹ ä¸Šä¸‹æ–‡ï¼Œå¢å¼ºäº†å¾„å‘è¦†ç›–å’Œè§’åº¦å‡åŒ€æ€§ï¼Œå¹¶ä½¿ç”¨é€šé“æ··åˆå±‚æ¥èšé›†é€šé“ç»´åº¦ä¸Šçš„ä¸Šä¸‹æ–‡ã€‚åœ¨å¤šå¯¹æ¯”åº¦MRIå’ŒMRI-CTåè®®ä¸Šçš„ç»¼åˆæ¼”ç¤ºè¡¨æ˜ï¼ŒI2I-Mambaåœ¨æ€§èƒ½ä¸Šä¼˜äºæœ€å…ˆè¿›çš„CNNã€å˜å‹å™¨å’ŒSSMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14022v5">PDF</a> 14 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºçŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼ˆSSMï¼‰æ¡†æ¶çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆæ–°æ–¹æ³•â€”â€”I2I-Mambaã€‚è¯¥æ–¹æ³•é€šè¿‡æ··åˆå›¾åƒå’Œå‚…é‡Œå¶åŸŸçš„ddMambaå—è¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡ï¼ŒåŒæ—¶ä¿æŒç©ºé—´ç²¾åº¦ã€‚é‡‡ç”¨èºæ—‹æ‰«æè½¨è¿¹çš„SSMè¿ç®—ç¬¦ï¼Œæé«˜å¾„å‘è¦†ç›–ç‡å’Œè§’å‘åŒæ€§ï¼Œå¹¶é€šè¿‡é€šé“æ··åˆå±‚å®ç°é€šé“ç»´åº¦çš„ä¸Šä¸‹æ–‡èšåˆã€‚åœ¨å¤šç§MRIå’ŒMRI-CTåè®®ä¸Šçš„ç»¼åˆæ¼”ç¤ºè¡¨æ˜ï¼ŒI2I-Mambaç›¸è¾ƒäºå…¶ä»–å…ˆè¿›çš„CNNã€transformerå’ŒSSMå…·æœ‰ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>I2I-Mambaæ˜¯ä¸€ç§åŸºäºçŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼ˆSSMï¼‰çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆæ–°æ–¹æ³•ã€‚</li>
<li>ddMambaå—ç”¨äºåœ¨å›¾åƒå’Œå‚…é‡Œå¶åŸŸè¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œå®ç°äº’è¡¥ã€‚</li>
<li>I2I-Mambaé€šè¿‡æ··åˆä½¿ç”¨å·ç§¯å±‚å’ŒSSMè¿ç®—ç¬¦ï¼Œé¿å…äº†å¤æ‚åº¦çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨èºæ—‹æ‰«æè½¨è¿¹çš„SSMè¿ç®—ç¬¦ï¼Œæé«˜äº†å¾„å‘è¦†ç›–ç‡å’Œè§’å‘åŒæ€§ã€‚</li>
<li>é€šé“æ··åˆå±‚ç”¨äºå®ç°é€šé“ç»´åº¦çš„ä¸Šä¸‹æ–‡èšåˆã€‚</li>
<li>I2I-Mambaåœ¨å¤šç§MRIå’ŒMRI-CTåè®®ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›çš„CNNã€transformerå’ŒSSMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14022">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2cbefa98f24fe6fa4375555a3d7b47a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78ec716b6f7f2ace98f66a04400e70b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c7b5f636af526d9ed4f2361afe9ea74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-362ec27e0eaf3dffc1fac7d7906fef06.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-23/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-24/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-81785035976daa414d4c9fa7bf86cc8c.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-24  Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models An Empirical Evaluation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-23/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bed9e88182809b03f6f81d5a9d0db8c3.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-23  Topology-Aware 3D Gaussian Splatting Leveraging Persistent Homology for   Optimized Structural Integrity
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
