<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-19  A Multi-task Learning Balanced Attention Convolutional Neural Network   Model for Few-shot Underwater Acoustic Target Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-32bb27d88ec1de00ca71c257db318708.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-19-更新"><a href="#2025-04-19-更新" class="headerlink" title="2025-04-19 更新"></a>2025-04-19 更新</h1><h2 id="A-Multi-task-Learning-Balanced-Attention-Convolutional-Neural-Network-Model-for-Few-shot-Underwater-Acoustic-Target-Recognition"><a href="#A-Multi-task-Learning-Balanced-Attention-Convolutional-Neural-Network-Model-for-Few-shot-Underwater-Acoustic-Target-Recognition" class="headerlink" title="A Multi-task Learning Balanced Attention Convolutional Neural Network   Model for Few-shot Underwater Acoustic Target Recognition"></a>A Multi-task Learning Balanced Attention Convolutional Neural Network   Model for Few-shot Underwater Acoustic Target Recognition</h2><p><strong>Authors:Wei Huang, Shumeng Sun, Junpeng Lu, Zhenpeng Xu, Zhengyang Xiu, Hao Zhang</strong></p>
<p>Underwater acoustic target recognition (UATR) is of great significance for the protection of marine diversity and national defense security. The development of deep learning provides new opportunities for UATR, but faces challenges brought by the scarcity of reference samples and complex environmental interference. To address these issues, we proposes a multi-task balanced channel attention convolutional neural network (MT-BCA-CNN). The method integrates a channel attention mechanism with a multi-task learning strategy, constructing a shared feature extractor and multi-task classifiers to jointly optimize target classification and feature reconstruction tasks. The channel attention mechanism dynamically enhances discriminative acoustic features such as harmonic structures while suppressing noise. Experiments on the Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97% classification accuracy and 95% $F1$-score in 27-class few-shot scenarios, significantly outperforming traditional CNN and ACNN models, as well as popular state-of-the-art UATR methods. Ablation studies confirm the synergistic benefits of multi-task learning and attention mechanisms, while a dynamic weighting adjustment strategy effectively balances task contributions. This work provides an efficient solution for few-shot underwater acoustic recognition, advancing research in marine bioacoustics and sonar signal processing. </p>
<blockquote>
<p>水下声学目标识别（UATR）对于保护海洋多样性和国防安全具有重要意义。深度学习的发展为UATR提供了新的机遇，但面临着参考样本稀缺和复杂环境干扰所带来的挑战。为了解决这些问题，我们提出了一种多任务平衡通道注意力卷积神经网络（MT-BCA-CNN）。该方法将通道注意力机制与多任务学习策略相结合，构建共享特征提取器和多任务分类器，联合优化目标分类和特征重建任务。通道注意力机制能够动态增强鉴别性声学特征，如谐波结构，同时抑制噪声。在Watkins海洋生命数据集上的实验表明，MT-BCA-CNN在27类小样本场景下实现了97%的分类准确率和95%的F1分数，显著优于传统的CNN和ACNN模型，以及其他流行的先进UATR方法。消融研究证实了多任务学习和注意力机制的协同效益，而动态权重调整策略有效地平衡了任务贡献。这项工作为小水声识别提供了有效的解决方案，推动了海洋生物声音学和声纳信号处理的研究进展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13102v1">PDF</a> </p>
<p><strong>Summary</strong><br>水下声学目标识别（UATR）对保护海洋多样性和国防安全具有重要意义。深度学习的发展为UATR提供了新的机遇，但面临样本稀缺和复杂环境干扰的挑战。我们提出了一种多任务平衡通道注意力卷积神经网络（MT-BCA-CNN），结合通道注意力机制和多任务学习策略，构建共享特征提取器和多任务分类器，联合优化目标分类和特征重建任务。在瓦特金斯海洋生命数据集上的实验表明，MT-BCA-CNN在27类小样本场景中实现97%的分类准确率和95%的F1分数，显著优于传统CNN和ACNN模型，以及流行的高级UATR方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>水下声学目标识别（UATR）对海洋多样性和国防安全至关重要。</li>
<li>深度学习在UATR中提供新机遇，但面临样本稀缺和环境干扰的挑战。</li>
<li>提出一种多任务平衡通道注意力卷积神经网络（MT-BCA-CNN）。</li>
<li>结合通道注意力机制和多任务学习策略，优化目标分类和特征重建。</li>
<li>MT-BCA-CNN在瓦特金斯海洋生命数据集上实现高分类准确率和F1分数。</li>
<li>相较于传统CNN和ACNN模型以及高级UATR方法，MT-BCA-CNN表现更优秀。</li>
<li>消融研究证实了多任务学习和注意力机制的协同效益，动态加权调整策略有效平衡任务贡献。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13102">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cafabe316f39dd21830c708d70ec6f2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c91f23658d8841229743485284e86623.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7be2d8818386a7aa34b98baaef2a093d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Knowledge-Acquisition-on-Mass-shooting-Events-via-LLMs-for-AI-Driven-Justice"><a href="#Knowledge-Acquisition-on-Mass-shooting-Events-via-LLMs-for-AI-Driven-Justice" class="headerlink" title="Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven   Justice"></a>Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven   Justice</h2><p><strong>Authors:Benign John Ihugba, Afsana Nasrin, Ling Wu, Lin Li, Lijun Qian, Xishuang Dong</strong></p>
<p>Mass-shooting events pose a significant challenge to public safety, generating large volumes of unstructured textual data that hinder effective investigations and the formulation of public policy. Despite the urgency, few prior studies have effectively automated the extraction of key information from these events to support legal and investigative efforts. This paper presented the first dataset designed for knowledge acquisition on mass-shooting events through the application of named entity recognition (NER) techniques. It focuses on identifying key entities such as offenders, victims, locations, and criminal instruments, that are vital for legal and investigative purposes. The NER process is powered by Large Language Models (LLMs) using few-shot prompting, facilitating the efficient extraction and organization of critical information from diverse sources, including news articles, police reports, and social media. Experimental results on real-world mass-shooting corpora demonstrate that GPT-4o is the most effective model for mass-shooting NER, achieving the highest Micro Precision, Micro Recall, and Micro F1-scores. Meanwhile, o1-mini delivers competitive performance, making it a resource-efficient alternative for less complex NER tasks. It is also observed that increasing the shot count enhances the performance of all models, but the gains are more substantial for GPT-4o and o1-mini, highlighting their superior adaptability to few-shot learning scenarios. </p>
<blockquote>
<p>大规模枪击事件对公共安全构成重大挑战，产生大量非结构化文本数据，妨碍了有效的调查和公共政策的制定。尽管形势紧迫，但之前的研究很少有效地实现自动化提取这些事件中的关键信息来支持法律和调查工作。本文通过应用命名实体识别（NER）技术，展示了首个针对大规模枪击事件的知识获取数据集的设计。该数据集侧重于识别对法律和调查目的至关重要的关键实体，如罪犯、受害者、地点和犯罪工具。NER过程由大型语言模型（LLM）通过小样本提示功能驱动，可以高效地提取和组织来自新闻文章、警方报告和社会媒体等不同来源的关键信息。在现实的大规模枪击语料库上的实验结果表明，GPT-4o是枪击事件NER最有效的模型，获得了最高的微观精度、微观召回率和微观F1分数。同时，o1-mini也表现出竞争力，对于较简单的NER任务来说是一个资源高效的替代方案。此外还发现，增加样本数量可以提高所有模型的表现，但GPT-4o和o1-mini的增益更大，凸显了它们在少样本学习场景中的卓越适应性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12545v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文针对大规模枪击事件对公共安全构成的挑战，提出了运用命名实体识别（NER）技术创建首个相关数据集的方法。该数据集能够识别关键实体，如罪犯、受害者、地点和犯罪工具等，对法律和调查工作至关重要。利用大型语言模型（LLMs）进行少样本提示，可从新闻文章、警方报告和社会媒体等来源高效提取和组织关键信息。实验结果表明，GPT-4o在大规模枪击事件NER中最有效，而o1-mini在较简单的NER任务中具有资源效率高的竞争优势。增加射击次数有助于提高所有模型的性能，但GPT-4o和o1-mini的增益更为显著，突显了它们对少样本学习场景的卓越适应性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模枪击事件对公共安全构成重大挑战，亟需自动化提取关键信息以支持法律和调查工作。</li>
<li>命名实体识别（NER）技术在处理大规模枪击事件中具有重要作用，可识别罪犯、受害者、地点和犯罪工具等关键实体。</li>
<li>利用大型语言模型（LLMs）进行少样本提示，可从多种来源高效提取和组织关键信息。</li>
<li>GPT-4o在大规模枪击事件NER任务中表现最佳，具有最高的微精度、微召回率和微F1分数。</li>
<li>o1-mini在资源效率方面表现出竞争优势，适用于较简单的NER任务。</li>
<li>增加射击次数有助于提高所有模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12545">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-34b008a2b7ae2964c09faba6fb8b0ca0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5192b1e185fd3f18015d88abcaebab9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a542dd9034dcc66b4d0f614bc6f37ef1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6218bee2e7c5dc15f2c644249bffcd65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c24c1e5f07a0850a290bb3f0ae5e28be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbba94e4d11ba6b4d2adf16477912bef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87e657391bedbf51ead4037f4388f23b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Sparsity-Outperforms-Low-Rank-Projections-in-Few-Shot-Adaptation"><a href="#Sparsity-Outperforms-Low-Rank-Projections-in-Few-Shot-Adaptation" class="headerlink" title="Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation"></a>Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation</h2><p><strong>Authors:Nairouz Mrabah, Nicolas Richet, Ismail Ben Ayed, Éric Granger</strong></p>
<p>Adapting Vision-Language Models (VLMs) to new domains with few labeled samples remains a significant challenge due to severe overfitting and computational constraints. State-of-the-art solutions, such as low-rank reparameterization, mitigate these issues but often struggle with generalization and require extensive hyperparameter tuning. In this paper, a novel Sparse Optimization (SO) framework is proposed. Unlike low-rank approaches that typically constrain updates to a fixed subspace, our SO method leverages high sparsity to dynamically adjust very few parameters. We introduce two key paradigms. First, we advocate for \textit{local sparsity and global density}, which updates a minimal subset of parameters per iteration while maintaining overall model expressiveness. As a second paradigm, we advocate for \textit{local randomness and global importance}, which sparsifies the gradient using random selection while pruning the first moment based on importance. This combination significantly mitigates overfitting and ensures stable adaptation in low-data regimes. Extensive experiments on 11 diverse datasets show that SO achieves state-of-the-art few-shot adaptation performance while reducing memory overhead. </p>
<blockquote>
<p>将视觉语言模型（VLMs）适应具有少量标记样本的新领域仍然是一个巨大的挑战，这主要是由于严重的过度拟合和计算约束。最先进的解决方案，如低秩重参数化，缓解了这些问题，但在推广方面经常遇到困难，并且需要大量调整超参数。本文提出了一种新的稀疏优化（SO）框架。与通常将更新限制在固定子空间中的低秩方法不同，我们的SO方法利用高稀疏性来动态调整极少的参数。我们介绍了两种关键范式。首先，我们提倡“局部稀疏性和全局密度”，这可以在每次迭代时更新一小部分参数，同时保持模型的整体表达能力。作为第二种范式，我们提倡“局部随机性和全局重要性”，使用随机选择来稀疏梯度，并根据重要性来修剪第一时刻。这种结合显著减轻了过度拟合问题，并确保在低数据情况下实现稳定的适应。在11个不同数据集上的大量实验表明，SO在少量样本适应方面达到了最先进的性能，同时减少了内存开销。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12436v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的Sparse Optimization（SO）框架，用于解决在少量标注样本下将视觉语言模型（VLMs）适应新领域的问题。通过引入局部稀疏性和全局密度以及局部随机性和全局重要性的两个关键范式，SO方法能够在动态调整极少参数的同时避免过度拟合，确保在低数据环境下的稳定适应。实验结果表明，SO在11个不同数据集上实现了最先进的少样本适应性能，并降低了内存开销。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sparse Optimization (SO)框架被提出用于解决在少量标注样本下将视觉语言模型（VLMs）适应新领域的问题。</li>
<li>当前主流方法如低秩重参数化虽然能缓解一些问题，但在泛化和超参数调整方面仍有挑战。</li>
<li>SO方法通过引入局部稀疏性和全局密度以及局部随机性和全局重要性的两个关键范式来动态调整参数。</li>
<li>SO方法能够显著减轻过拟合问题，并确保在低数据环境下的稳定适应。</li>
<li>SO方法在11个不同数据集上实现了最先进的少样本适应性能。</li>
<li>SO方法降低了内存开销，是一种有效的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12436">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1dc426cf3b5ee5f242807be0176ecfe6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72ca8c845e5e0c5b503a9a6cd06d6375.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4545d7555b25ee673a56a9af356a5cf6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DC-SAM-In-Context-Segment-Anything-in-Images-and-Videos-via-Dual-Consistency"><a href="#DC-SAM-In-Context-Segment-Anything-in-Images-and-Videos-via-Dual-Consistency" class="headerlink" title="DC-SAM: In-Context Segment Anything in Images and Videos via Dual   Consistency"></a>DC-SAM: In-Context Segment Anything in Images and Videos via Dual   Consistency</h2><p><strong>Authors:Mengshi Qi, Pengfei Zhu, Xiangtai Li, Xiaoyang Bi, Lu Qi, Huadong Ma, Ming-Hsuan Yang</strong></p>
<p>Given a single labeled example, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation model’s generalization ability and has been applied to various vision tasks, including scene understanding and image&#x2F;video editing. While recent Segment Anything Models have achieved state-of-the-art results in interactive segmentation, these approaches are not directly applicable to in-context segmentation. In this work, we propose the Dual Consistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2 for in-context segmentation of both images and videos. Our key insights are to enhance the features of the SAM’s prompt encoder in segmentation by providing high-quality visual prompts. When generating a mask prior, we fuse the SAM features to better align the prompt encoder. Then, we design a cycle-consistent cross-attention on fused features and initial visual prompts. Next, a dual-branch design is provided by using the discriminative positive and negative prompts in the prompt encoder. Furthermore, we design a simple mask-tube training strategy to adopt our proposed dual consistency method into the mask tube. Although the proposed DC-SAM is primarily designed for images, it can be seamlessly extended to the video domain with the support of SAM2. Given the absence of in-context segmentation in the video domain, we manually curate and construct the first benchmark from existing video segmentation datasets, named In-Context Video Object Segmentation (IC-VOS), to better assess the in-context capability of the model. Extensive experiments demonstrate that our method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on PASCAL-5i, and a J&amp;F score of 71.52 on the proposed IC-VOS benchmark. Our source code and benchmark are available at <a target="_blank" rel="noopener" href="https://github.com/zaplm/DC-SAM">https://github.com/zaplm/DC-SAM</a>. </p>
<blockquote>
<p>给定一个单一标记的示例，上下文内分割旨在分割相应的对象。这种设置被称为小样本学习中的一次分割，旨在探索分割模型的泛化能力，并已应用于各种视觉任务，包括场景理解、图像&#x2F;视频编辑等。尽管最近的Segment Anything Models在交互式分割方面取得了最先进的成果，但这些方法并不直接适用于上下文分割。在这项工作中，我们基于提示微调提出了双一致性SAM（DC-SAM）方法，以适应图像和视频的上下文分割。我们的关键见解是通过提供高质量视觉提示来增强SAM提示编码器在分割中的功能。在生成掩膜先验时，我们融合SAM特征以更好地对齐提示编码器。然后，我们在融合的特征和初始视觉提示上设计了一个循环一致的交叉注意力机制。接下来，通过使用提示编码器中的判别性正向和负向提示，提供了一个双分支设计。此外，我们设计了一个简单的掩膜管训练策略，将所提的双一致性方法应用到掩膜管中。虽然所提出DC-SAM主要为图像设计，但由于SAM2的支持，它可以无缝扩展到视频领域。鉴于视频领域缺乏上下文分割，我们从现有的视频分割数据集中手动筛选和构建第一个基准测试集，名为上下文视频对象分割（IC-VOS），以更好地评估模型的上下文能力。大量实验表明，我们的方法在COCO-20i上实现了55.5（+1.4）的mIoU，在PASCAL-5i上实现了73.0（+1.1）的mIoU，在提出的IC-VOS基准测试集上达到了71.52的J&amp;F分数。我们的源代码和基准测试集可在<a target="_blank" rel="noopener" href="https://github.com/zaplm/DC-SAM%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zaplm/DC-SAM中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12080v2">PDF</a> V1 has been withdrawn due to a template issue, because of the arXiv   policy, we can’t delete it. Please refer to the newest version v2</p>
<p><strong>Summary</strong><br>     本文介绍了一种基于提示调整（prompt-tuning）的Dual Consistency SAM（DC-SAM）方法，用于单次示例下的上下文分割（in-context segmentation）。该方法通过高质量视觉提示增强SAM的提示编码器特征，设计循环一致的跨注意力融合特征和初始视觉提示，并采用双分支设计使用判别性的正负提示。此外，本文还提出了一个简单的mask-tube训练策略，并将DC-SAM扩展到视频领域。为评估模型在视频领域的上下文能力，建立了第一个In-Context Video Object Segmentation（IC-VOS）基准测试集。实验表明，DC-SAM方法在COCO-20i上达到55.5（+1.4）mIoU，在PASCAL-5i上达到73.0（+1.1）mIoU，并在IC-VOS基准测试集上获得71.52的J&amp;F分数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章介绍了一种名为DC-SAM的方法，针对单次示例下的上下文分割问题。</li>
<li>DC-SAM方法基于prompt-tuning技术，旨在通过高质量视觉提示增强特征。</li>
<li>文章设计了循环一致的跨注意力机制来融合特征和初始视觉提示。</li>
<li>采用双分支设计，使用判别性的正负提示在提示编码器中。</li>
<li>提出了一个简单的mask-tube训练策略，将DC-SAM方法应用于视频领域。</li>
<li>为评估模型在视频领域的上下文能力，建立了第一个IC-VOS基准测试集。</li>
<li>实验结果显示DC-SAM方法在多个数据集上取得了显著成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c457651c59c9412b47573ff0840b9a91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c9799504f3fb87b3954c6ff0422be3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bfd5d15905207191d5e913afedf05e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b462c8815030c27c341509f58b8c06c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f366eeff5770f1b4e7b3c6555fc433e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e991b32da864ea5e7c3f4ae845353d1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Adaptive-Decision-Boundary-for-Few-Shot-Class-Incremental-Learning"><a href="#Adaptive-Decision-Boundary-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Adaptive Decision Boundary for Few-Shot Class-Incremental Learning"></a>Adaptive Decision Boundary for Few-Shot Class-Incremental Learning</h2><p><strong>Authors:Linhao Li, Yongzhang Tan, Siyuan Yang, Hao Cheng, Yongfeng Dong, Liang Yang</strong></p>
<p>Few-Shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes from a limited set of training samples without forgetting knowledge of previously learned classes. Conventional FSCIL methods typically build a robust feature extractor during the base training session with abundant training samples and subsequently freeze this extractor, only fine-tuning the classifier in subsequent incremental phases. However, current strategies primarily focus on preventing catastrophic forgetting, considering only the relationship between novel and base classes, without paying attention to the specific decision spaces of each class. To address this challenge, we propose a plug-and-play Adaptive Decision Boundary Strategy (ADBS), which is compatible with most FSCIL methods. Specifically, we assign a specific decision boundary to each class and adaptively adjust these boundaries during training to optimally refine the decision spaces for the classes in each session. Furthermore, to amplify the distinctiveness between classes, we employ a novel inter-class constraint loss that optimizes the decision boundaries and prototypes for each class. Extensive experiments on three benchmarks, namely CIFAR100, miniImageNet, and CUB200, demonstrate that incorporating our ADBS method with existing FSCIL techniques significantly improves performance, achieving overall state-of-the-art results. </p>
<blockquote>
<p>少量类别增量学习（FSCIL）旨在从有限的训练样本中持续学习新类别，同时不遗忘已学习类别的知识。传统的FSCIL方法通常在基础训练阶段利用丰富的训练样本构建强大的特征提取器，随后冻结该提取器，仅在后续的增量阶段微调分类器。然而，当前策略主要侧重于防止灾难性遗忘，只考虑新类别和基础类别之间的关系，而没有关注每个类别的特定决策空间。为了应对这一挑战，我们提出了一种即插即用的自适应决策边界策略（ADBS），可与大多数FSCIL方法兼容。具体来说，我们为每个类别分配一个特定的决策边界，并在训练过程中自适应地调整这些边界，以最优方式细化每个会话中类别的决策空间。此外，为了放大类别之间的差异，我们采用了一种新型类间约束损失，该损失优化了每个类别的决策边界和原型。在CIFAR100、miniImageNet和CUB200三个基准测试上的大量实验表明，将我们的ADBS方法与现有的FSCIL技术相结合，可以显著提高性能，达到最新状态的最佳结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10976v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了Few-Shot类增量学习（FSCIL）的目标和方法。FSCIL旨在从有限的训练样本中持续学习新类，同时不遗忘已学习类的知识。传统FSCIL方法通常在基础训练阶段构建强大的特征提取器，并在后续增量阶段仅微调分类器。然而，当前策略主要关注防止灾难性遗忘，只考虑新类和基类之间的关系，而忽视了每个类的特定决策空间。为此，提出了即插即用的自适应决策边界策略（ADBS），与大多数FSCIL方法兼容。通过为每个类分配特定的决策边界，并在训练过程中自适应地调整这些边界，以优化每个会话中的类的决策空间。此外，采用了一种新型类间约束损失，以优化每个类的决策边界和原型，提高了类之间的区分度。在CIFAR100、miniImageNet和CUB200三个基准测试上的广泛实验表明，将ADBS方法与现有FSCIL技术相结合，可显著提高性能，达到最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Class-Incremental Learning (FSCIL)的目标是持续学习新类，同时保留对旧类的知识。</li>
<li>传统FSCIL方法主要关注基础训练阶段，并在后续增量阶段微调分类器。</li>
<li>当前策略主要关注防止灾难性遗忘，忽视了每个类的特定决策空间。</li>
<li>提出的Adaptive Decision Boundary Strategy (ADBS)策略与大多数FSCIL方法兼容，为每个类分配特定的决策边界并在训练过程中自适应调整。</li>
<li>采用新型类间约束损失来优化决策边界和原型，提高类之间的区分度。</li>
<li>在多个基准测试上的实验表明，结合ADBS与现有FSCIL技术可显著提高性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10976">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b1a42c3d9fe1b779a1ec4d5aad82387b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31ea2229f48d443464a3c69e8764baf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd38f190db35eac8afab0dc6ac8be6f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86068c9a39d4bb320337e2985d14de2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7886339b3032dc74057e2df9865880d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-641b12cd60d19891e58f1df6ad29a79c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rethinking-Few-Shot-Image-Fusion-Granular-Ball-Priors-Enable-General-Purpose-Deep-Fusion"><a href="#Rethinking-Few-Shot-Image-Fusion-Granular-Ball-Priors-Enable-General-Purpose-Deep-Fusion" class="headerlink" title="Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable   General-Purpose Deep Fusion"></a>Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable   General-Purpose Deep Fusion</h2><p><strong>Authors:Minjie Deng, Yan Wei, Hao Zhai, An Wu, Yuncan Ouyang, Qianyao Peng</strong></p>
<p>In image fusion tasks, the absence of real fused images as priors presents a fundamental challenge. Most deep learning-based fusion methods rely on large-scale paired datasets to extract global weighting features from raw images, thereby generating fused outputs that approximate real fused images. In contrast to previous studies, this paper explores few-shot training of neural networks under the condition of having prior knowledge. We propose a novel fusion framework named GBFF, and a Granular Ball Significant Extraction algorithm specifically designed for the few-shot prior setting. All pixel pairs involved in the fusion process are initially modeled as a Coarse-Grained Granular Ball. At the local level, Fine-Grained Granular Balls are used to slide through the brightness space to extract Non-Salient Pixel Pairs, and perform splitting operations to obtain Salient Pixel Pairs. Pixel-wise weights are then computed to generate a pseudo-supervised image. At the global level, pixel pairs with significant contributions to the fusion process are categorized into the Positive Region, while those whose contributions cannot be accurately determined are assigned to the Boundary Region. The Granular Ball performs modality-aware adaptation based on the proportion of the positive region, thereby adjusting the neural network’s loss function and enabling it to complement the information of the boundary region. Extensive experiments demonstrate the effectiveness of both the proposed algorithm and the underlying theory. Compared with state-of-the-art (SOTA) methods, our approach shows strong competitiveness in terms of both fusion time and image expressiveness. Our code is publicly available at: </p>
<blockquote>
<p>在图像融合任务中，缺乏真实融合图像作为先验信息是一个基本挑战。大多数基于深度学习的融合方法依赖于大规模配对数据集，从原始图像中提取全局加权特征，从而生成近似真实融合图像的融合输出。与以前的研究相比，本文探索了在具备先验知识的条件下神经网络的少样本训练。我们提出了一种新的融合框架GBFF（格兰特球法），以及一种针对少样本先验环境的粒状球重要性提取算法。参与融合过程的像素对最初被建模为粗粒度的粒状球。在局部层面上，使用精细粒度的粒状球遍历亮度空间以提取非显著性像素对，并进行分裂操作以获得显著性像素对。然后计算像素级的权重以生成伪监督图像。在全局层面上，将对于融合过程有显著贡献的像素对分类为正区域，而那些贡献无法准确确定的像素对被分配给边界区域。粒状球根据正区域的比例进行模态感知适应，从而调整神经网络的损失函数，使其能够补充边界区域的信息。大量实验证明了所提出算法和底层理论的有效性。与最先进的方法相比，我们的方法在融合时间和图像表现力方面都表现出强大的竞争力。我们的代码可在公开渠道获取：</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08937v2">PDF</a> </p>
<p><strong>Summary</strong>：该论文解决了图像融合任务中缺乏真实融合图像作为先验的问题。提出了一种名为GBFF的新型融合框架，以及专门针对小样本先验环境的Granular Ball Significant Extraction算法。算法通过创建粗粒度和细粒度粒球来处理和提取图像中的非显著像素对和显著像素对，并计算像素级权重生成伪监督图像。同时，粒球还进行模态感知适应，调整神经网络的损失函数以补充边界区域的信息。实验证明该方法和理论的有效性，与现有方法相比，在融合时间和图像表现力方面表现出强竞争力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>论文解决了在图像融合任务中缺乏真实融合图像先验的问题。</li>
<li>提出了一种新型的融合框架GBFF。</li>
<li>引入了Granular Ball Significant Extraction算法，该算法专为小样本先验环境设计。</li>
<li>算法通过创建粗粒度和细粒度粒球来处理像素对，并计算像素级权重生成伪监督图像。</li>
<li>粒球进行模态感知适应，根据正区域的占比调整神经网络的损失函数。</li>
<li>实验证明该方法和理论的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08937">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0c532948e4d681e9a4c8988c25332579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b59ddc8423b000de64207f38b799a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-283928e9489f2d5da3549aef93d34139.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d611a0044f0059d8f8b154677112593d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f22368382d44ac5ddda5e49e8b4d95ad.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MalMixer-Few-Shot-Malware-Classification-with-Retrieval-Augmented-Semi-Supervised-Learning"><a href="#MalMixer-Few-Shot-Malware-Classification-with-Retrieval-Augmented-Semi-Supervised-Learning" class="headerlink" title="MalMixer: Few-Shot Malware Classification with Retrieval-Augmented   Semi-Supervised Learning"></a>MalMixer: Few-Shot Malware Classification with Retrieval-Augmented   Semi-Supervised Learning</h2><p><strong>Authors:Jiliang Li, Yifan Zhang, Yu Huang, Kevin Leach</strong></p>
<p>Recent growth and proliferation of malware have tested practitioners ability to promptly classify new samples according to malware families. In contrast to labor-intensive reverse engineering efforts, machine learning approaches have demonstrated increased speed and accuracy. However, most existing deep-learning malware family classifiers must be calibrated using a large number of samples that are painstakingly manually analyzed before training. Furthermore, as novel malware samples arise that are beyond the scope of the training set, additional reverse engineering effort must be employed to update the training set. The sheer volume of new samples found in the wild creates substantial pressure on practitioners ability to reverse engineer enough malware to adequately train modern classifiers. In this paper, we present MalMixer, a malware family classifier using semi-supervised learning that achieves high accuracy with sparse training data. We present a domain-knowledge-aware data augmentation technique for malware feature representations, enhancing few-shot performance of semi-supervised malware family classification. We show that MalMixer achieves state-of-the-art performance in few-shot malware family classification settings. Our research confirms the feasibility and effectiveness of lightweight, domain-knowledge-aware data augmentation methods for malware features and shows the capabilities of similar semi-supervised classifiers in addressing malware classification issues. </p>
<blockquote>
<p>近期恶意软件的迅速增长和扩散考验了从业者根据恶意软件家族对新样本进行及时分类的能力。与劳动密集型的逆向工程努力相比，机器学习的方法已经显示出更高的速度和准确性。然而，大多数现有的深度学习的恶意软件家族分类器需要使用大量的样本进行校准，这些样本在训练之前需要进行繁琐的手动分析。此外，随着超出训练集范围的新的恶意软件样本的出现，必须使用更多的逆向工程工作来更新训练集。在实际环境中发现的大量新样本给从业者带来了巨大的压力，他们需要重新设计足够的恶意软件来充分训练现代分类器。在本文中，我们介绍了MalMixer，一种使用半监督学习的恶意软件家族分类器，它可以在稀疏的训练数据上实现较高的准确性。我们提出了一种基于领域知识的数据增强技术，用于表示恶意软件的特征，以提高半监督恶意软件家族分类的少数样本性能。我们展示了MalMixer在少数样本的恶意软件家族分类设置中实现了最先进的性能。我们的研究证实了领域知识轻量级的数据增强方法在恶意软件特征中的可行性和有效性，并展示了类似的半监督分类器在处理恶意软件分类问题时的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13213v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于当前病毒样本的不断增长和多样化，机器学习技术在病毒家族分类中的应用日益凸显其速度和准确性优势。然而，现有的深度学习病毒家族分类器需要大量样本进行校准，且在面对新型病毒样本时仍需人工进行逆向工程分析更新训练集。本文提出一种基于半监督学习的病毒家族分类器MalMixer，它可以在稀疏训练数据的情况下实现较高的准确度。本研究设计了一种领域知识增强的数据扩充技术来优化病毒特征表示，提高半监督型病毒家族在少数情况下的分类性能。实验结果证实MalMixer在少量病毒样本的分类场景下具有突破性表现。本文验证了领域知识增强的数据扩充方法对于病毒特征的有效性，以及类似半监督分类器在解决病毒分类问题中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前病毒样本增长迅速，机器学习技术在病毒家族分类中的应用显得尤为重要。</li>
<li>虽然现有深度学习模型表现出较高准确性，但仍面临需要大量训练样本的挑战。新病毒样本的快速涌现导致难以对模型进行持续训练。</li>
<li>MalMixer基于半监督学习技术的提出为解决该问题提供了新的途径，可以在样本稀疏的情况下实现较高的分类准确度。</li>
<li>引入了领域知识增强的数据扩充技术来优化病毒特征表示，提升模型的分类性能。尤其是面对少量样本时的分类效果更为显著。</li>
<li>实验结果显示MalMixer在少数病毒样本分类场景中表现出优异性能。这为同类技术提供了一种可参考的成功案例和进步空间。通过实现数据的自主学习和利用增强了其应用广泛性并减少了人工成本投入，减轻了负担与限制因素提升了效果与价值表现效果与应用潜力得到进一步提升证明了该方法在实际场景下的可行性和可靠性 。此创新研究克服了以往存在的挑战与局限性为未来的研究提供了更多可能性与思路 。 </li>
<li>研究验证了领域知识增强的数据扩充方法对于病毒特征的有效性以及该技术在处理此类问题时的高效能性和便捷性得到了业界广泛认可和发展前景 。这对于计算机安全领域的发展具有重大意义 。同时研究过程中所采用的方法和思路也具有一定的通用性可以应用于其他相关领域的研究中 。 通过对新型技术应用的不断挖掘和创新推动了计算机安全领域的不断进步与发展 。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13213">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7251d8dab1098b2246813f38a1448730.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32bb27d88ec1de00ca71c257db318708.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-910bf5ea638196af9cb760921eea872a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85e209844c377defc431f96d9a81487f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-682db554604070c09fa35e0e49152148.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ExploRLLM-Guiding-Exploration-in-Reinforcement-Learning-with-Large-Language-Models"><a href="#ExploRLLM-Guiding-Exploration-in-Reinforcement-Learning-with-Large-Language-Models" class="headerlink" title="ExploRLLM: Guiding Exploration in Reinforcement Learning with Large   Language Models"></a>ExploRLLM: Guiding Exploration in Reinforcement Learning with Large   Language Models</h2><p><strong>Authors:Runyu Ma, Jelle Luijkx, Zlatan Ajanovic, Jens Kober</strong></p>
<p>In robot manipulation, Reinforcement Learning (RL) often suffers from low sample efficiency and uncertain convergence, especially in large observation and action spaces. Foundation Models (FMs) offer an alternative, demonstrating promise in zero-shot and few-shot settings. However, they can be unreliable due to limited physical and spatial understanding. We introduce ExploRLLM, a method that combines the strengths of both paradigms. In our approach, FMs improve RL convergence by generating policy code and efficient representations, while a residual RL agent compensates for the FMs’ limited physical understanding. We show that ExploRLLM outperforms both policies derived from FMs and RL baselines in table-top manipulation tasks. Additionally, real-world experiments show that the policies exhibit promising zero-shot sim-to-real transfer. Supplementary material is available at <a target="_blank" rel="noopener" href="https://explorllm.github.io/">https://explorllm.github.io</a>. </p>
<blockquote>
<p>在机器人操作领域，强化学习（RL）常常面临样本效率低和收敛不确定的问题，特别是在大型观察和行动空间中。基础模型（FMs）提供了一种替代方案，在零样本和少样本场景中展现出巨大的潜力。然而，它们由于有限的物理和空间理解而可能不可靠。我们引入了ExploRLLM方法，它结合了两种模式的长处。在我们的方法中，基础模型通过生成策略代码和有效表示形式来提高RL的收敛性，而残余的RL代理则弥补了基础模型的有限物理理解。我们证明ExploRLLM在桌面操作任务中的表现优于来自基础模型的策略和RL基准线。此外，真实世界的实验表明，这些策略表现出有希望的从模拟到真实的零样本迁移。更多材料请访问：<a target="_blank" rel="noopener" href="https://explorllm.github.io./">https://explorllm.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.09583v4">PDF</a> 6 pages, 6 figures, IEEE International Conference on Robotics and   Automation (ICRA) 2025</p>
<p><strong>Summary</strong></p>
<p>强化学习在机器人操作中存在样本效率低和收敛不确定的问题，尤其在大型观测和动作空间中尤为明显。而基础模型在大规模观测空间中的零样本和少样本设置中展现出潜力，但受限于物理和空间理解。本研究结合了强化学习和基础模型的优势，提出了ExploRLLM方法。该方法利用基础模型生成策略代码和高效表示，同时使用残余强化学习代理人来补偿基础模型的物理理解局限性。实验结果显示，ExploRLLM在桌面操作任务中优于仅使用基础模型的策略和强化学习基线，并且在实际环境中的实验展示了策略从模拟到现实的零样本转移潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习在机器人操作中存在样本效率和收敛问题。</li>
<li>基础模型在零样本和少样本设置中有潜力，但受限于物理和空间理解。</li>
<li>ExploRLLM结合了强化学习和基础模型的优势。</li>
<li>ExploRLLM利用基础模型生成策略代码和高效表示。</li>
<li>残余强化学习代理人补偿了基础模型的物理理解局限性。</li>
<li>ExploRLLM在桌面操作任务中表现优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.09583">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-48eecde669a3fe9fd064bbc61d630978.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe9297128a5fa28e8a27d5ef3f805459.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9feb18dda2e9da0372a0a5bf7c44c00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e1d2a67149b3e12796c69c72ab05d6e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-43e64679669f89575c9a0fbc6a3ffa71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fd10579fd880e5a359598ff16bd9daf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e80729c5fbc7ce8e255b70cbd57f29a8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-19/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-19/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-19/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0f632dc422e20ea6b7fceb16a7b0e4e4.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-04-19  Multi-Parameter Molecular MRI Quantification using Physics-Informed   Self-Supervised Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-19/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-910ee8930f9a3f2d540062a4db969108.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-19  Exploring Expert Failures Improves LLM Agent Tuning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
