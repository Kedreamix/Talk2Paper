<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-19  Exploring Expert Failures Improves LLM Agent Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d89c4c2885c1355a2b27578c458cce58.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-19-æ›´æ–°"><a href="#2025-04-19-æ›´æ–°" class="headerlink" title="2025-04-19 æ›´æ–°"></a>2025-04-19 æ›´æ–°</h1><h2 id="Exploring-Expert-Failures-Improves-LLM-Agent-Tuning"><a href="#Exploring-Expert-Failures-Improves-LLM-Agent-Tuning" class="headerlink" title="Exploring Expert Failures Improves LLM Agent Tuning"></a>Exploring Expert Failures Improves LLM Agent Tuning</h2><p><strong>Authors:Li-Cheng Lan, Andrew Bai, Minhao Cheng, Ruochen Wang, Cho-Jui Hsieh, Tianyi Zhou</strong></p>
<p>Large Language Models (LLMs) have shown tremendous potential as agents, excelling at tasks that require multiple rounds of reasoning and interactions. Rejection Sampling Fine-Tuning (RFT) has emerged as an effective method for finetuning LLMs as agents: it first imitates expert-generated successful trajectories and further improves agentic skills through iterative fine-tuning on successful, self-generated trajectories. However, since the expert (e.g., GPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler scenarios, many complex subtasks remain unsolved and persistently out-of-distribution (OOD). Upon investigating these challenging subtasks, we discovered that previously failed expert trajectories can often provide valuable guidance, e.g., plans and key actions, that can significantly improve agent exploration efficiency and acquisition of critical skills. Motivated by these observations, we propose Exploring Expert Failures (EEF), which identifies beneficial actions from failed expert trajectories and integrates them into the training dataset. Potentially harmful actions are meticulously excluded to prevent contamination of the model learning process. By leveraging the beneficial actions in expert failures, EEF successfully solves some previously unsolvable subtasks and improves agent tuning performance. Remarkably, our approach achieved a 62% win rate in WebShop, outperforming RFT (53. 6%) and GPT-4 (35. 6%), and to the best of our knowledge, setting a new state-of-the-art as the first method to surpass a score of 0.81 in WebShop and exceed 81 in SciWorld. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ™ºèƒ½ä½“å±•ç°äº†å·¨å¤§çš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤šè½®æ¨ç†å’Œäº¤äº’çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚æ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆRFTï¼‰ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„å¾®è°ƒLLMä½œä¸ºæ™ºèƒ½ä½“çš„æ–¹æ³•å·²ç»å´­éœ²å¤´è§’ï¼šå®ƒé¦–å…ˆæ¨¡ä»¿ä¸“å®¶ç”Ÿæˆçš„æˆåŠŸè½¨è¿¹ï¼Œç„¶åé€šè¿‡è¿­ä»£å¾®è°ƒåœ¨è‡ªæˆ‘ç”Ÿæˆçš„æˆåŠŸè½¨è¿¹ä¸Šè¿›ä¸€æ­¥æ”¹å–„æ™ºèƒ½æŠ€èƒ½ã€‚ç„¶è€Œï¼Œç”±äºä¸“å®¶ï¼ˆä¾‹å¦‚GPT-4ï¼‰ä¸»è¦åœ¨è¾ƒç®€å•çš„å­ä»»åŠ¡ä¸Šå–å¾—æˆåŠŸï¼Œä¸”RFTæœ¬è´¨ä¸Šåå‘äºæ›´ç®€å•åœºæ™¯ï¼Œå› æ­¤è®¸å¤šå¤æ‚çš„å­ä»»åŠ¡ä»ç„¶æ— æ³•è§£å†³å¹¶ä¸”æŒç»­å¤„äºåˆ†å¸ƒå¤–ï¼ˆOODï¼‰ã€‚åœ¨è°ƒæŸ¥è¿™äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„å­ä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬å‘ç°ä¹‹å‰çš„ä¸“å®¶è½¨è¿¹å¤±è´¥å¸¸å¸¸èƒ½æä¾›æœ‰ä»·å€¼çš„æŒ‡å¯¼ï¼Œä¾‹å¦‚è®¡åˆ’å’Œå…³é”®è¡ŒåŠ¨ï¼Œè¿™å¯ä»¥æ˜¾è‘—æé«˜æ™ºèƒ½ä½“çš„æ¢ç´¢æ•ˆç‡å’Œè·å–å…³é”®æŠ€èƒ½çš„èƒ½åŠ›ã€‚åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†æ¢ç´¢ä¸“å®¶å¤±è´¥ï¼ˆEEFï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«å¤±è´¥ä¸“å®¶è½¨è¿¹ä¸­çš„æœ‰ç›Šè¡ŒåŠ¨å¹¶å°†å…¶æ•´åˆåˆ°è®­ç»ƒæ•°æ®é›†ä¸­ã€‚æœ‰å®³çš„è¡ŒåŠ¨è¢«ç²¾å¿ƒæ’é™¤ï¼Œä»¥é˜²æ­¢æ±¡æŸ“æ¨¡å‹å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡åˆ©ç”¨ä¸“å®¶å¤±è´¥ä¸­çš„æœ‰ç›Šè¡ŒåŠ¨ï¼ŒEEFæˆåŠŸè§£å†³äº†ä¸€äº›ä¹‹å‰æ— æ³•è§£å†³çš„å­ä»»åŠ¡å¹¶æé«˜äº†æ™ºèƒ½ä½“è°ƒæ•´æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç½‘ç»œè´­ç‰©ç¯å¢ƒä¸­å®ç°äº†62%çš„èƒœç‡ï¼Œè¶…è¶Šäº†RFTï¼ˆ53.6%ï¼‰å’ŒGPT-4ï¼ˆ35.6%ï¼‰ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨WebShopä¸Šå¾—åˆ†è¶…è¿‡0.81å¹¶åœ¨SciWorldä¸Šå¾—åˆ†è¶…è¿‡81çš„æœ€æ–°æŠ€æœ¯ã€‚ï¼ˆæ³¨ï¼šç”±äºåŸæ–‡ä¸­çš„å…·ä½“æ•°å€¼å¯èƒ½æ¶‰åŠç‰¹å®šçš„å®éªŒæ•°æ®æˆ–ç‰ˆæœ¬æ›´æ–°ï¼Œç¿»è¯‘æ—¶è¯·ç¡®ä¿æ ¸å¯¹åŸæ–‡ä»¥ç¡®ä¿å‡†ç¡®æ€§ã€‚ï¼‰</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13145v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºä»£ç†å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå°¤å…¶åœ¨éœ€è¦å¤šè½®æ¨ç†å’Œäº¤äº’çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚æ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆRFTï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„å¾®è°ƒLLMä½œä¸ºä»£ç†çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ¨¡ä»¿ä¸“å®¶ç”Ÿæˆçš„æˆåŠŸè½¨è¿¹å¹¶è¿›ä¸€æ­¥åœ¨æˆåŠŸçš„è‡ªæˆ‘ç”Ÿæˆè½¨è¿¹ä¸Šè¿›è¡Œè¿­ä»£å¾®è°ƒæ¥æé«˜ä»£ç†æŠ€èƒ½ã€‚ç„¶è€Œï¼Œä¸“å®¶ä¸»è¦åœ¨è¾ƒç®€å•çš„å­ä»»åŠ¡ä¸ŠæˆåŠŸï¼ŒRFTåˆ™å€¾å‘äºæ›´ç®€å•çš„æƒ…å†µï¼Œå¯¼è‡´è®¸å¤šå¤æ‚çš„å­ä»»åŠ¡æ— æ³•è§£å†³å¹¶æŒç»­å¤„äºè¶…å‡ºåˆ†å¸ƒèŒƒå›´ï¼ˆOODï¼‰ã€‚ç ”ç©¶è¿™äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„å­ä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬å‘ç°ä¹‹å‰çš„ä¸“å®¶å¤±è´¥è½¨è¿¹å¸¸èƒ½æä¾›æœ‰ä»·å€¼çš„æŒ‡å¯¼ï¼Œå¦‚è®¡åˆ’å’Œå…³é”®è¡ŒåŠ¨ï¼Œè¿™èƒ½æ˜¾è‘—æé«˜ä»£ç†çš„æ¢ç´¢æ•ˆç‡å’Œå…³é”®æŠ€èƒ½çš„è·å–ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ¢ç´¢ä¸“å®¶å¤±è´¥ï¼ˆEEFï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»å¤±è´¥çš„ä¸“å®¶è½¨è¿¹ä¸­è¯†åˆ«å‡ºæœ‰ç›Šçš„è¡ŒåŠ¨å¹¶å°†å…¶æ•´åˆåˆ°è®­ç»ƒæ•°æ®é›†ä¸­ã€‚æœ‰å®³çš„è¡ŒåŠ¨ä¼šè¢«ç²¾å¿ƒæ’é™¤ï¼Œä»¥é˜²æ­¢æ¨¡å‹å­¦ä¹ è¿‡ç¨‹å—åˆ°æ±¡æŸ“ã€‚é€šè¿‡åˆ©ç”¨ä¸“å®¶å¤±è´¥ä¸­çš„æœ‰ç›Šè¡ŒåŠ¨ï¼ŒEEFæˆåŠŸè§£å†³äº†ä¸€äº›ä¹‹å‰æ— æ³•è§£å†³çš„å­ä»»åŠ¡ï¼Œæé«˜äº†ä»£ç†è°ƒæ•´æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç½‘ç»œè´­ç‰©ä»»åŠ¡ä¸Šçš„èƒœç‡è¾¾åˆ°äº†62%ï¼Œè¶…è¶Šäº†RFTï¼ˆ53.6%ï¼‰å’ŒGPT-4ï¼ˆ35.6%ï¼‰ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç½‘ç»œè´­ç‰©ä»»åŠ¡ä¸Šçš„æœ€æ–°æœ€ä½³è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºä½œä¸ºä»£ç†çš„å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šè½®æ¨ç†å’Œäº¤äº’çš„ä»»åŠ¡ä¸Šã€‚</li>
<li>æ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆRFTï¼‰æ˜¯LLMä»£ç†çš„ä¸€ç§æœ‰æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡æ¨¡ä»¿å’Œå­¦ä¹ æˆåŠŸè½¨è¿¹æ¥æå‡ä»£ç†æŠ€èƒ½ã€‚</li>
<li>ä¸“å®¶ä¸»è¦åœ¨ç®€å•å­ä»»åŠ¡ä¸ŠæˆåŠŸï¼Œå¤æ‚çš„å­ä»»åŠ¡å¸¸å¸¸æ— æ³•è§£å†³å¹¶è¶…å‡ºåˆ†å¸ƒèŒƒå›´ï¼ˆOODï¼‰ã€‚</li>
<li>å¤±è´¥çš„ä¸“å®¶è½¨è¿¹å¯ä»¥æä¾›æœ‰ä»·å€¼çš„æŒ‡å¯¼ï¼Œå¦‚è®¡åˆ’å’Œå…³é”®è¡ŒåŠ¨ï¼Œæé«˜ä»£ç†çš„æ¢ç´¢æ•ˆç‡å’Œå…³é”®æŠ€èƒ½è·å–ã€‚</li>
<li>æ¢ç´¢ä¸“å®¶å¤±è´¥ï¼ˆEEFï¼‰æ–¹æ³•è¯†åˆ«å¹¶æ•´åˆä¸“å®¶å¤±è´¥è½¨è¿¹ä¸­çš„æœ‰ç›Šè¡ŒåŠ¨åˆ°è®­ç»ƒæ•°æ®é›†ä¸­ã€‚</li>
<li>EEFé€šè¿‡åˆ©ç”¨ä¸“å®¶å¤±è´¥ä¸­çš„æœ‰ç›Šè¡ŒåŠ¨ï¼ŒæˆåŠŸè§£å†³äº†ä¸€äº›ä¹‹å‰æ— æ³•è§£å†³çš„å­ä»»åŠ¡ï¼Œæé«˜äº†ä»£ç†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13145">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c299260e55d1190cfd844c937d36366a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3bf46b239256772f8733a40db0b31fd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Energy-Based-Reward-Models-for-Robust-Language-Model-Alignment"><a href="#Energy-Based-Reward-Models-for-Robust-Language-Model-Alignment" class="headerlink" title="Energy-Based Reward Models for Robust Language Model Alignment"></a>Energy-Based Reward Models for Robust Language Model Alignment</h2><p><strong>Authors:Anamika Lochab, Ruqi Zhang</strong></p>
<p>Reward models (RMs) are essential for aligning Large Language Models (LLMs) with human preferences. However, they often struggle with capturing complex human preferences and generalizing to unseen data. To address these challenges, we introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc refinement framework that enhances RM robustness and generalization. EBRM models the reward distribution explicitly, capturing uncertainty in human preferences and mitigating the impact of noisy or misaligned annotations. It achieves this through conflict-aware data filtering, label-noise-aware contrastive training, and hybrid initialization. Notably, EBRM enhances RMs without retraining, making it computationally efficient and adaptable across different models and tasks. Empirical evaluations on RM benchmarks demonstrate significant improvements in both robustness and generalization, achieving up to a 5.97% improvement in safety-critical alignment tasks compared to standard RMs. Furthermore, reinforcement learning experiments confirm that our refined rewards enhance alignment quality, effectively delaying reward hacking. These results demonstrate our approach as a scalable and effective enhancement for existing RMs and alignment pipelines. The code is available at EBRM. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸éš¾ä»¥æ•æ‰å¤æ‚çš„äººç±»åå¥½å¹¶æ¨å¹¿åˆ°æœªè§è¿‡çš„æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºèƒ½é‡çš„å¥–åŠ±æ¨¡å‹ï¼ˆEBRMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„åéªŒç»†åŒ–æ¡†æ¶ï¼Œå¯ä»¥æé«˜RMçš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚EBRMæ˜¾å¼åœ°å»ºæ¨¡å¥–åŠ±åˆ†å¸ƒï¼Œæ•æ‰äººç±»åå¥½çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶å‡è½»å™ªå£°æˆ–é”™ä½æ³¨é‡Šçš„å½±å“ã€‚å®ƒé€šè¿‡å†²çªæ„ŸçŸ¥æ•°æ®è¿‡æ»¤ã€æ ‡ç­¾å™ªå£°æ„ŸçŸ¥å¯¹æ¯”è®­ç»ƒå’Œæ··åˆåˆå§‹åŒ–æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒEBRMåœ¨ä¸éœ€è¦é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å¢å¼ºäº†RMçš„åŠŸèƒ½ï¼Œä½¿å…¶è®¡ç®—æ•ˆç‡é«˜ä¸”é€‚åº”ä¸åŒçš„æ¨¡å‹å’Œä»»åŠ¡ã€‚åœ¨RMåŸºå‡†æµ‹è¯•ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œå®ƒåœ¨ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œåœ¨å®‰å…¨å…³é”®å¯¹é½ä»»åŠ¡ä¸­ï¼Œä¸æ ‡å‡†RMç›¸æ¯”ï¼Œæœ€é«˜å¯æé«˜5.97%ã€‚æ­¤å¤–ï¼Œå¼ºåŒ–å­¦ä¹ å®éªŒè¯å®ï¼Œæˆ‘ä»¬ä¼˜åŒ–çš„å¥–åŠ±æé«˜äº†å¯¹é½è´¨é‡ï¼Œæœ‰æ•ˆåœ°å»¶ç¼“äº†å¥–åŠ±é»‘å®¢æ”»å‡»ã€‚è¿™äº›ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å¯¹ç°æœ‰RMå’Œå¯¹é½ç®¡é“çš„å¯æ‰©å±•å’Œæœ‰æ•ˆçš„å¢å¼ºã€‚ä»£ç å¯åœ¨EBRMç½‘ç«™æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13134v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºèƒ½æºå¥–åŠ±æ¨¡å‹ï¼ˆEBRMï¼‰çš„è½»é‡çº§äº‹åæ”¹è¿›æ¡†æ¶ï¼Œå¯æé«˜å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚EBRMæ˜¾å¼å»ºæ¨¡å¥–åŠ±åˆ†å¸ƒï¼Œæ•æ‰äººç±»åå¥½çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶å‡è½»å™ªå£°æˆ–è¯¯å¯¹é½æ³¨é‡Šçš„å½±å“ã€‚å®ƒé€šè¿‡å†²çªæ„ŸçŸ¥æ•°æ®è¿‡æ»¤ã€æ ‡ç­¾å™ªå£°æ„ŸçŸ¥å¯¹æ¯”è®­ç»ƒå’Œæ··åˆåˆå§‹åŒ–æ¥å®ç°è¿™ä¸€ç‚¹ã€‚EBRMæ— éœ€é‡æ–°è®­ç»ƒå³å¯æé«˜RMçš„æ€§èƒ½ï¼Œå› æ­¤è®¡ç®—æ•ˆç‡é«˜ä¸”é€‚åº”ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ã€‚åœ¨RMåŸºå‡†æµ‹è¯•ä¸Šçš„ç»éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒEBRMåœ¨æé«˜ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢æ˜¾è‘—æé«˜äº†RMçš„æ€§èƒ½ï¼Œåœ¨å…³é”®å¯¹é½ä»»åŠ¡ä¸­æœ€é«˜è¾¾åˆ°äº†æ”¹è¿›ä¸º+ 5.97%ã€‚æ­¤å¤–ï¼Œå¼ºåŒ–å­¦ä¹ å®éªŒè¯å®ï¼Œæˆ‘ä»¬çš„æ”¹è¿›å¥–åŠ±å¢å¼ºäº†å¯¹é½è´¨é‡ï¼Œæœ‰æ•ˆåœ°å»¶è¿Ÿäº†å¥–åŠ±ç ´è§£é—®é¢˜ã€‚è¿™äº›ç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸€ç§å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„å¢å¼ºç°æœ‰RMå’Œå¯¹é½ç®¡é“çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨EBRMæ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>EBRMæ˜¯ä¸€ç§è½»é‡çº§äº‹åæ”¹è¿›æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>EBRMé€šè¿‡æ˜¾å¼å»ºæ¨¡å¥–åŠ±åˆ†å¸ƒæ•æ‰äººç±»åå¥½çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶å‡è½»å™ªå£°æˆ–è¯¯å¯¹é½æ•°æ®çš„å½±å“ã€‚</li>
<li>è¯¥æ–¹æ³•é›†æˆäº†å†²çªæ„ŸçŸ¥æ•°æ®è¿‡æ»¤ã€æ ‡ç­¾å™ªå£°æ„ŸçŸ¥å¯¹æ¯”è®­ç»ƒå’Œæ··åˆåˆå§‹åŒ–æŠ€æœ¯æ¥å®ç°å…¶ç›®æ ‡ã€‚</li>
<li>EBRMèƒ½åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹æé«˜RMæ€§èƒ½ï¼Œä½¿å…¶è®¡ç®—æ•ˆç‡é«˜ä¸”é€‚åº”å¤šç§æ¨¡å‹å’Œä»»åŠ¡ã€‚</li>
<li>é€šè¿‡åŸºå‡†æµ‹è¯•çš„å®éªŒç»“æœè¯å®ï¼ŒEBRMåœ¨å¢å¼ºRMçš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨å…³é”®å¯¹é½ä»»åŠ¡ä¸­ï¼Œä¸æ ‡å‡†RMç›¸æ¯”ï¼ŒEBRMæœ€å¤šå¯å®ç°+ 5.97%çš„æ”¹è¿›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-942f821fbded829733e7c21535ba6496.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04e4db5985b709ad5140b280f1905556.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd0486b5ee7486a587466d5f2e0f7ece.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VistaDPO-Video-Hierarchical-Spatial-Temporal-Direct-Preference-Optimization-for-Large-Video-Models"><a href="#VistaDPO-Video-Hierarchical-Spatial-Temporal-Direct-Preference-Optimization-for-Large-Video-Models" class="headerlink" title="VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference   Optimization for Large Video Models"></a>VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference   Optimization for Large Video Models</h2><p><strong>Authors:Haojian Huang, Haodong Chen, Shengqiong Wu, Meng Luo, Jinlan Fu, Xinya Du, Hanwang Zhang, Hao Fei</strong></p>
<p>Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown promise in video understanding but often suffer from misalignment with human intuition and video hallucination issues. To address these challenges, we introduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal Direct Preference Optimization. VistaDPO enhances text-video preference alignment across three hierarchical levels: i) Instance Level, aligning overall video content with responses; ii) Temporal Level, aligning video temporal semantics with event descriptions; and iii) Perceptive Level, aligning spatial objects with language tokens. Given the lack of datasets for fine-grained video-language preference alignment, we construct VistaDPO-7k, a dataset of 7.2K QA pairs annotated with chosen and rejected responses, along with spatial-temporal grounding information such as timestamps, keyframes, and bounding boxes. Extensive experiments on benchmarks such as Video Hallucination, Video QA, and Captioning performance tasks demonstrate that VistaDPO significantly improves the performance of existing LVMs, effectively mitigating video-language misalignment and hallucination. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/HaroldChen19/VistaDPO">https://github.com/HaroldChen19/VistaDPO</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤§å‹è§†é¢‘æ¨¡å‹ï¼ˆLVMï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å¸¸å¸¸é¢ä¸´ä¸äººç±»ç›´è§‰ä¸ä¸€è‡´å’Œè§†é¢‘å¹»è§‰ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VistaDPOï¼Œä¸€ç§ç”¨äºè§†é¢‘åˆ†å±‚æ—¶ç©ºç›´æ¥åå¥½ä¼˜åŒ–çš„æ–°å‹æ¡†æ¶ã€‚VistaDPOå¢å¼ºæ–‡æœ¬è§†é¢‘åå¥½å¯¹é½ä¸‰ä¸ªå±‚æ¬¡ï¼šiï¼‰å®ä¾‹çº§åˆ«ï¼Œä½¿æ•´ä½“è§†é¢‘å†…å®¹ä¸å“åº”å¯¹é½ï¼›iiï¼‰æ—¶é—´çº§åˆ«ï¼Œä½¿è§†é¢‘æ—¶é—´è¯­ä¹‰ä¸äº‹ä»¶æè¿°å¯¹é½ï¼›ä»¥åŠiiiï¼‰æ„ŸçŸ¥çº§åˆ«ï¼Œä½¿ç©ºé—´å¯¹è±¡ä¸è¯­è¨€ä»¤ç‰Œå¯¹é½ã€‚é‰´äºç¼ºä¹ç²¾ç»†ç²’åº¦çš„è§†é¢‘è¯­è¨€åå¥½å¯¹é½æ•°æ®é›†ï¼Œæˆ‘ä»¬æ„å»ºäº†VistaDPO-7kï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«7.2Ké—®ç­”å¯¹çš„æ•°æ®é›†ï¼Œæ¯ä¸ªé—®ç­”å¯¹éƒ½æ ‡æœ‰é€‰æ‹©å’Œæ‹’ç»å“åº”ï¼Œä»¥åŠæ—¶ç©ºå®šä½ä¿¡æ¯ï¼Œå¦‚æ—¶é—´æˆ³ã€å…³é”®å¸§å’Œè¾¹ç•Œæ¡†ã€‚åœ¨è§†é¢‘å¹»è§‰ã€è§†é¢‘é—®ç­”å’Œå­—å¹•æ€§èƒ½ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVistaDPOæ˜¾è‘—æé«˜äº†ç°æœ‰LVMçš„æ€§èƒ½ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†è§†é¢‘è¯­è¨€çš„ä¸å¯¹é½å’Œå¹»è§‰é—®é¢˜ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HaroldChen19/VistaDPO%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HaroldChen19/VistaDPOè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13122v1">PDF</a> Code and Data: <a target="_blank" rel="noopener" href="https://github.com/HaroldChen19/VistaDPO">https://github.com/HaroldChen19/VistaDPO</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤§å‹è§†é¢‘æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å­˜åœ¨ä¸äººç±»ç›´è§‰ä¸ç¬¦å’Œè§†é¢‘å¹»è§‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºVistaDPOï¼Œä¸€ç§ç”¨äºè§†é¢‘å±‚æ¬¡åŒ–æ—¶ç©ºç›´æ¥åå¥½ä¼˜åŒ–çš„æ–°å‹æ¡†æ¶ã€‚VistaDPOé€šè¿‡ä¸‰ä¸ªå±‚æ¬¡å¢å¼ºæ–‡æœ¬è§†é¢‘åå¥½å¯¹é½ï¼šä¸€ï¼‰å®ä¾‹çº§åˆ«ï¼Œå¯¹é½è§†é¢‘å†…å®¹ä¸æ•´ä½“å“åº”ï¼›äºŒï¼‰æ—¶é—´çº§åˆ«ï¼Œå¯¹é½è§†é¢‘æ—¶é—´è¯­ä¹‰ä¸äº‹ä»¶æè¿°ï¼›ä¸‰ï¼‰æ„ŸçŸ¥çº§åˆ«ï¼Œå¯¹é½ç©ºé—´ç‰©ä½“ä¸è¯­è¨€æ ‡è®°ã€‚ç”±äºç¼ºä¹ç²¾ç»†ç²’åº¦çš„è§†é¢‘è¯­è¨€åå¥½å¯¹é½æ•°æ®é›†ï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«7.2Ké—®ç­”å¯¹åŠé€‰å®šå’Œæ‹’ç»å“åº”çš„VistaDPO-7kæ•°æ®é›†ï¼Œè¿˜åŒ…æ‹¬æ—¶ç©ºå®šä½ä¿¡æ¯å¦‚æ—¶é—´æˆ³ã€å…³é”®å¸§å’Œè¾¹ç•Œæ¡†ã€‚åœ¨è§†é¢‘å¹»è§‰ã€è§†é¢‘é—®ç­”å’Œå­—å¹•æ€§èƒ½ä»»åŠ¡ç­‰æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼ŒVistaDPOå¯æ˜¾è‘—æ”¹å–„ç°æœ‰å¤§å‹è§†é¢‘æ¨¡å‹çš„æ€§èƒ½ï¼Œæœ‰æ•ˆå‡è½»è§†é¢‘è¯­è¨€ä¸å¯¹é½å’Œå¹»è§‰é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVMsåŸºäºLLMsæ„å»ºï¼Œåœ¨è§†é¢‘ç†è§£æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å­˜åœ¨ä¸äººç±»ç›´è§‰ä¸ç¬¦åŠè§†é¢‘å¹»è§‰çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥VistaDPOæ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªå±‚æ¬¡å¢å¼ºæ–‡æœ¬è§†é¢‘åå¥½å¯¹é½ï¼šå®ä¾‹çº§åˆ«ã€æ—¶é—´çº§åˆ«å’Œæ„ŸçŸ¥çº§åˆ«ã€‚</li>
<li>ç¼ºä¹ç²¾ç»†ç²’åº¦çš„è§†é¢‘è¯­è¨€åå¥½å¯¹é½æ•°æ®é›†ï¼Œå› æ­¤æ„å»ºVistaDPO-7kæ•°æ®é›†ã€‚</li>
<li>VistaDPO-7kåŒ…å«7.2Ké—®ç­”å¯¹åŠæ—¶ç©ºå®šä½ä¿¡æ¯ã€‚</li>
<li>å®éªŒè¡¨æ˜VistaDPOå¯æ˜¾è‘—æ”¹å–„å¤§å‹è§†é¢‘æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>VistaDPOèƒ½æœ‰æ•ˆå‡è½»è§†é¢‘è¯­è¨€ä¸å¯¹é½å’Œå¹»è§‰é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18c8e3240afc741c6a2bb8ad503b524d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05e5dbe4e1b5e8e8a9e9b248fdfbdc2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fe5c1f77e9b938071f6b740991c8fc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13d35d327d735396f9392c57492be2d6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Aware-Trajectory-Prediction-via-Rule-Regularized-Heteroscedastic-Deep-Classification"><a href="#Uncertainty-Aware-Trajectory-Prediction-via-Rule-Regularized-Heteroscedastic-Deep-Classification" class="headerlink" title="Uncertainty-Aware Trajectory Prediction via Rule-Regularized   Heteroscedastic Deep Classification"></a>Uncertainty-Aware Trajectory Prediction via Rule-Regularized   Heteroscedastic Deep Classification</h2><p><strong>Authors:Kumar Manas, Christian Schlauch, Adrian Paschke, Christian Wirth, Nadja Klein</strong></p>
<p>Deep learning-based trajectory prediction models have demonstrated promising capabilities in capturing complex interactions. However, their out-of-distribution generalization remains a significant challenge, particularly due to unbalanced data and a lack of enough data and diversity to ensure robustness and calibration. To address this, we propose SHIFT (Spectral Heteroscedastic Informed Forecasting for Trajectories), a novel framework that uniquely combines well-calibrated uncertainty modeling with informative priors derived through automated rule extraction. SHIFT reformulates trajectory prediction as a classification task and employs heteroscedastic spectral-normalized Gaussian processes to effectively disentangle epistemic and aleatoric uncertainties. We learn informative priors from training labels, which are automatically generated from natural language driving rules, such as stop rules and drivability constraints, using a retrieval-augmented generation framework powered by a large language model. Extensive evaluations over the nuScenes dataset, including challenging low-data and cross-location scenarios, demonstrate that SHIFT outperforms state-of-the-art methods, achieving substantial gains in uncertainty calibration and displacement metrics. In particular, our model excels in complex scenarios, such as intersections, where uncertainty is inherently higher. Project page: <a target="_blank" rel="noopener" href="https://kumarmanas.github.io/SHIFT/">https://kumarmanas.github.io/SHIFT/</a>. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„è½¨è¿¹é¢„æµ‹æ¨¡å‹åœ¨æ•æ‰å¤æ‚äº¤äº’æ–¹é¢å±•ç°å‡ºæœ‰å‰æ™¯çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶ç¦»åˆ†å¸ƒæ³›åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å› ä¸ºæ•°æ®ä¸å¹³è¡¡å’Œç¼ºä¹è¶³å¤Ÿçš„æ•°æ®å’Œå¤šæ ·æ€§ä»¥ç¡®ä¿å…¶ç¨³å¥æ€§å’Œæ ¡å‡†æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SHIFTï¼ˆç”¨äºè½¨è¿¹çš„è°±å¼‚æ–¹å·®ä¿¡æ¯é¢„æµ‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç‹¬ç‰¹ç»“åˆè‰¯å¥½æ ¡å‡†çš„ä¸ç¡®å®šæ€§æ¨¡å‹å’Œé€šè¿‡è‡ªåŠ¨åŒ–è§„åˆ™æå–è·å¾—çš„ä¿¡æ¯å…ˆéªŒå€¼çš„æ–°å‹æ¡†æ¶ã€‚SHIFTå°†è½¨è¿¹é¢„æµ‹é‡æ–°è¡¨è¿°ä¸ºåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶é‡‡ç”¨å¼‚æ–¹å·®è°±å½’ä¸€åŒ–é«˜æ–¯è¿‡ç¨‹æœ‰æ•ˆåœ°åˆ†ç¦»å‡ºè®¤çŸ¥ä¸ç¡®å®šæ€§å’Œå¶ç„¶ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬ä»è®­ç»ƒæ ‡ç­¾ä¸­å­¦ä¹ ä¿¡æ¯å…ˆéªŒå€¼ï¼Œè¿™äº›æ ‡ç­¾æ˜¯ç”±è‡ªç„¶è¯­è¨€é©¾é©¶è§„åˆ™ï¼ˆå¦‚åœè½¦è§„åˆ™å’Œé©¾é©¶çº¦æŸï¼‰è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œä½¿ç”¨ç”±å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬å…·æœ‰æŒ‘æˆ˜æ€§çš„ä½æ•°æ®å’Œè·¨ä½ç½®åœºæ™¯ï¼Œè¡¨æ˜SHIFTä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼Œåœ¨ä¸ç¡®å®šæ€§æ ¡å‡†å’Œä½ç§»æŒ‡æ ‡ä¸Šå–å¾—äº†é‡å¤§è¿›å±•ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸ç¡®å®šæ€§å›ºæœ‰çš„å¤æ‚åœºæ™¯ï¼ˆå¦‚äº¤å‰å£ï¼‰ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://kumarmanas.github.io/SHIFT/%E3%80%82">https://kumarmanas.github.io/SHIFT/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13111v1">PDF</a> 17 Pages, 9 figures. Accepted to Robotics: Science and Systems(RSS),   2025</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ è½¨è¿¹é¢„æµ‹æ¨¡å‹åœ¨æ•æ‰å¤æ‚äº¤äº’æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å…¶é¢ä¸´ç€æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ä¸å¹³è¡¡å’Œç¼ºä¹è¶³å¤Ÿå¤šæ ·æ•°æ®çš„æƒ…å†µä¸‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SHIFTæ¡†æ¶ï¼Œç»“åˆæ ¡å‡†ä¸ç¡®å®šæ€§æ¨¡å‹å’Œé€šè¿‡è‡ªåŠ¨åŒ–è§„åˆ™æå–å¾—åˆ°çš„å…ˆéªŒä¿¡æ¯ã€‚SHIFTå°†è½¨è¿¹é¢„æµ‹é‡æ–°å®šä¹‰ä¸ºåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨å¼‚è°±æ ‡å‡†åŒ–é«˜æ–¯è¿‡ç¨‹æœ‰æ•ˆåœ°åŒºåˆ†ä¸ç¡®å®šæ€§çš„å¯ç†è§£å’Œä¸å¯ç†è§£éƒ¨åˆ†ã€‚æˆ‘ä»¬ä»è®­ç»ƒæ ‡ç­¾ä¸­å­¦ä¹ ä¿¡æ¯ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†ï¼Œè¿™äº›æ ‡ç­¾æ˜¯é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ä»è‡ªç„¶è¯­è¨€é©¾é©¶è§„åˆ™ï¼ˆå¦‚åœè½¦è§„åˆ™å’Œé©¾é©¶çº¦æŸï¼‰è‡ªåŠ¨ç”Ÿæˆçš„ã€‚åœ¨nuScenesæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼ŒåŒ…æ‹¬å…·æœ‰æŒ‘æˆ˜æ€§çš„ä½æ•°æ®å’Œè·¨ä½ç½®åœºæ™¯åœ¨å†…ï¼ŒSHIFTåœ¨ä¸ç¡®å®šæ€§å’Œä½ç§»åº¦é‡æ–¹é¢éƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä¸”åœ¨å¤æ‚æ€§é«˜çš„åœºæ™¯ï¼ˆå¦‚äº¤å‰è·¯å£ï¼‰è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ·±åº¦å­¦ä¹ è½¨è¿¹é¢„æµ‹æ¨¡å‹å…·æœ‰æ•æ‰å¤æ‚äº¤äº’çš„å·¨å¤§æ½œåŠ›ï¼Œä½†æ•°æ®ä¸å¹³è¡¡å’Œç¼ºä¹å¤šæ ·æ€§é™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>SHIFTæ¡†æ¶ç»“åˆæ ¡å‡†ä¸ç¡®å®šæ€§æ¨¡å‹å’Œè‡ªåŠ¨åŒ–è§„åˆ™æå–çš„å…ˆéªŒä¿¡æ¯æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>SHIFTå°†è½¨è¿¹é¢„æµ‹å®šä¹‰ä¸ºåˆ†ç±»ä»»åŠ¡å¹¶åˆ©ç”¨å¼‚è°±æ ‡å‡†åŒ–é«˜æ–¯è¿‡ç¨‹æ¥åŒºåˆ†å¯ç†è§£å’Œä¸å¯ç†è§£çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>SHIFTåˆ©ç”¨ä»è®­ç»ƒæ ‡ç­¾ä¸­å­¦ä¹ çš„ä¿¡æ¯ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†ï¼Œè¿™äº›æ ‡ç­¾æ˜¯ä»è‡ªç„¶è¯­è¨€é©¾é©¶è§„åˆ™ä¸­è‡ªåŠ¨ç”Ÿæˆçš„ã€‚</li>
<li>åœ¨nuScenesæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒSHIFTåœ¨ä¸ç¡®å®šæ€§å’Œä½ç§»åº¦é‡æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13111">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df6a7c980e9eff1f47687c97da2ca63e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b304482f2e02ab8d14e0d964535b3fe9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5c38528a8c35dc947da3b992606c79f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d6740406490e374c0c08692650f5aef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d86f7edcb14f5fe568c8fcd9e20325b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01bd4264d5d564ce48dbf1670e3f2ee3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EventVAD-Training-Free-Event-Aware-Video-Anomaly-Detection"><a href="#EventVAD-Training-Free-Event-Aware-Video-Anomaly-Detection" class="headerlink" title="EventVAD: Training-Free Event-Aware Video Anomaly Detection"></a>EventVAD: Training-Free Event-Aware Video Anomaly Detection</h2><p><strong>Authors:Yihua Shao, Haojin He, Sijie Li, Siyu Chen, Xinwei Long, Fanhu Zeng, Yuxuan Fan, Muyang Zhang, Ziyang Yan, Ao Ma, Xiaochen Wang, Hao Tang, Yan Wang, Shuyan Li</strong></p>
<p>Video Anomaly Detection~(VAD) focuses on identifying anomalies within videos. Supervised methods require an amount of in-domain training data and often struggle to generalize to unseen anomalies. In contrast, training-free methods leverage the intrinsic world knowledge of large language models (LLMs) to detect anomalies but face challenges in localizing fine-grained visual transitions and diverse events. Therefore, we propose EventVAD, an event-aware video anomaly detection framework that combines tailored dynamic graph architectures and multimodal LLMs through temporal-event reasoning. Specifically, EventVAD first employs dynamic spatiotemporal graph modeling with time-decay constraints to capture event-aware video features. Then, it performs adaptive noise filtering and uses signal ratio thresholding to detect event boundaries via unsupervised statistical features. The statistical boundary detection module reduces the complexity of processing long videos for MLLMs and improves their temporal reasoning through event consistency. Finally, it utilizes a hierarchical prompting strategy to guide MLLMs in performing reasoning before determining final decisions. We conducted extensive experiments on the UCF-Crime and XD-Violence datasets. The results demonstrate that EventVAD with a 7B MLLM achieves state-of-the-art (SOTA) in training-free settings, outperforming strong baselines that use 7B or larger MLLMs. </p>
<blockquote>
<p>è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰ä¸“æ³¨äºè¯†åˆ«è§†é¢‘ä¸­çš„å¼‚å¸¸æƒ…å†µã€‚æœ‰ç›‘ç£çš„æ–¹æ³•éœ€è¦å¤§é‡é¢†åŸŸå†…çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”å¾€å¾€éš¾ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„å¼‚å¸¸æƒ…å†µã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ— è®­ç»ƒçš„æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…åœ¨ä¸–ç•ŒçŸ¥è¯†æ¥æ£€æµ‹å¼‚å¸¸ï¼Œä½†é¢ä¸´ç€å®šä½ç²¾ç»†è§†è§‰è½¬æ¢å’Œå¤šæ ·åŒ–äº‹ä»¶çš„æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†EventVADï¼Œä¸€ä¸ªç»“åˆå®šåˆ¶çš„åŠ¨æ€å›¾å½¢æ¶æ„å’Œå¤šæ¨¡å¼LLMçš„äº‹ä»¶æ„ŸçŸ¥è§†é¢‘å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡æ—¶é—´äº‹ä»¶æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒEventVADé¦–å…ˆé‡‡ç”¨å…·æœ‰æ—¶é—´è¡°å‡çº¦æŸçš„åŠ¨æ€æ—¶ç©ºå›¾å½¢å»ºæ¨¡ï¼Œä»¥æ•è·äº‹ä»¶æ„ŸçŸ¥çš„è§†é¢‘ç‰¹å¾ã€‚ç„¶åï¼Œå®ƒæ‰§è¡Œè‡ªé€‚åº”å™ªå£°è¿‡æ»¤ï¼Œå¹¶ä½¿ç”¨ä¿¡å·æ¯”ç‡é˜ˆå€¼é€šè¿‡æ— ç›‘ç£ç»Ÿè®¡ç‰¹å¾æ£€æµ‹äº‹ä»¶è¾¹ç•Œã€‚ç»Ÿè®¡è¾¹ç•Œæ£€æµ‹æ¨¡å—é™ä½äº†å¤„ç†é•¿è§†é¢‘å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ€§ï¼Œå¹¶é€šè¿‡äº‹ä»¶ä¸€è‡´æ€§æé«˜äº†å…¶æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚æœ€åï¼Œå®ƒåˆ©ç”¨åˆ†å±‚æç¤ºç­–ç•¥æ¥æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿›è¡Œæ¨ç†ååšå‡ºæœ€ç»ˆå†³å®šã€‚æˆ‘ä»¬åœ¨UCF-Crimeå’ŒXD-Violenceæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨7Bçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„EventVADåœ¨æ— è®­ç»ƒç¯å¢ƒä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œä¼˜äºä½¿ç”¨ç›¸åŒè§„æ¨¡æˆ–æ›´å¤§è§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºå¤§åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13092v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰çš„éš¾é¢˜ï¼ŒåŒ…æ‹¬ç›‘ç£æ–¹æ³•éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ä¸”éš¾ä»¥æ³›åŒ–åˆ°æ–°å¼‚å¸¸ï¼Œè€Œè®­ç»ƒå¤–æ–¹æ³•åˆ™åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å›ºæœ‰ä¸–ç•ŒçŸ¥è¯†è¿›è¡Œæ£€æµ‹ä½†é¢ä¸´æœ¬åœ°åŒ–ç²¾ç»†è§†è§‰è¿‡æ¸¡å’Œå¤šæ ·åŒ–äº‹ä»¶çš„å®šä½æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæå‡ºäº†EventVADï¼Œä¸€ç§ç»“åˆåŠ¨æ€å›¾æ¶æ„å’Œå¤šæ¨¡æ€LLMçš„äº‹ä»¶æ„ŸçŸ¥è§†é¢‘å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œé€šè¿‡æ—¶é—´äº‹ä»¶æ¨ç†è¿›è¡Œæ£€æµ‹ã€‚è¯¥æ¡†æ¶é¦–å…ˆä½¿ç”¨å…·æœ‰æ—¶é—´è¡°å‡çº¦æŸçš„åŠ¨æ€æ—¶ç©ºå›¾æ¨¡å‹æ•è·äº‹ä»¶æ„ŸçŸ¥è§†é¢‘ç‰¹å¾ï¼Œç„¶åé€šè¿‡è‡ªé€‚åº”å™ªå£°è¿‡æ»¤å’Œä¿¡å·æ¯”ç‡é˜ˆå€¼æ£€æµ‹äº‹ä»¶è¾¹ç•Œã€‚æœ€åï¼Œåˆ©ç”¨åˆ†å±‚æç¤ºç­–ç•¥æŒ‡å¯¼MLLMè¿›è¡Œæ¨ç†å¹¶åšå‡ºæœ€ç»ˆå†³å®šã€‚åœ¨UCF-Crimeå’ŒXD-Violenceæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨7B MLLMçš„EventVADåœ¨è®­ç»ƒå¤–è®¾ç½®ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¼˜äºä½¿ç”¨ç›¸åŒè§„æ¨¡æˆ–æ›´å¤§è§„æ¨¡LLMçš„å¼ºå¤§åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰æ—¨åœ¨è¯†åˆ«è§†é¢‘ä¸­çš„å¼‚å¸¸ã€‚</li>
<li>ç›‘ç£æ–¹æ³•éœ€è¦å¤§é‡é¢†åŸŸå†…çš„è®­ç»ƒæ•°æ®ï¼Œä¸”éš¾ä»¥æ³›åŒ–åˆ°æ–°å¼‚å¸¸ã€‚</li>
<li>è®­ç»ƒå¤–æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å›ºæœ‰çŸ¥è¯†æ£€æµ‹å¼‚å¸¸ï¼Œä½†é¢ä¸´å®šä½ç²¾ç»†è§†è§‰è¿‡æ¸¡å’Œå¤šæ ·åŒ–äº‹ä»¶çš„æŒ‘æˆ˜ã€‚</li>
<li>EventVADç»“åˆäº†åŠ¨æ€å›¾æ¶æ„å’Œå¤šæ¨¡æ€LLMï¼Œé€šè¿‡æ—¶é—´äº‹ä»¶æ¨ç†è¿›è¡Œæ£€æµ‹ã€‚</li>
<li>EventVADä½¿ç”¨åŠ¨æ€æ—¶ç©ºå›¾æ¨¡å‹æ•è·äº‹ä»¶æ„ŸçŸ¥è§†é¢‘ç‰¹å¾ã€‚</li>
<li>EventVADé€šè¿‡è‡ªé€‚åº”å™ªå£°è¿‡æ»¤å’Œä¿¡å·æ¯”ç‡é˜ˆå€¼æ£€æµ‹äº‹ä»¶è¾¹ç•Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13092">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-82f5b814fdd9e7b9d8408b1a9bfbe920.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc78fcb41e06345181c609d734098112.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af6f090f645ad320d84c0a676c7e3a6a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7cbd9fdeecef8472cf54afdd2bc6021.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SkyReels-V2-Infinite-length-Film-Generative-Model"><a href="#SkyReels-V2-Infinite-length-Film-Generative-Model" class="headerlink" title="SkyReels-V2: Infinite-length Film Generative Model"></a>SkyReels-V2: Infinite-length Film Generative Model</h2><p><strong>Authors:Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengchen Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, Yahui Zhou</strong></p>
<p>Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMsâ€™ inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation. To address these limitations, we propose SkyReels-V2, an Infinite-length Film Generative Model, that synergizes Multi-modal Large Language Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing Framework. Firstly, we design a comprehensive structural representation of video that combines the general descriptions by the Multi-modal LLM and the detailed shot language by sub-expert models. Aided with human annotation, we then train a unified Video Captioner, named SkyCaptioner-V1, to efficiently label the video data. Secondly, we establish progressive-resolution pretraining for the fundamental video generation, followed by a four-stage post-training enhancement: Initial concept-balanced Supervised Fine-Tuning (SFT) improves baseline quality; Motion-specific Reinforcement Learning (RL) training with human-annotated and synthetic distortion data addresses dynamic artifacts; Our diffusion forcing framework with non-decreasing noise schedules enables long-video synthesis in an efficient search space; Final high-quality SFT refines visual fidelity. All the code and models are available at <a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/SkyReels-V2">https://github.com/SkyworkAI/SkyReels-V2</a>. </p>
<blockquote>
<p>è¿‘æœŸè§†é¢‘ç”ŸæˆæŠ€æœ¯çš„è¿›å±•ä¸»è¦å¾—ç›Šäºæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¡†æ¶çš„é©±åŠ¨ï¼Œä½†ä»å­˜åœ¨åè°ƒæç¤ºéµå¾ªã€è§†è§‰è´¨é‡ã€è¿åŠ¨åŠ¨æ€å’ŒæŒç»­æ—¶é—´ç­‰æ–¹é¢çš„å…³é”®æŒ‘æˆ˜ï¼šä¸ºæå‡ä¸´æ—¶è§†è§‰è´¨é‡è€Œå¦¥åè¿åŠ¨åŠ¨æ€ã€ä¸ºä¼˜å…ˆä¿éšœåˆ†è¾¨ç‡è€Œé™åˆ¶è§†é¢‘æ—¶é•¿ï¼ˆ5-10ç§’ï¼‰ï¼Œä»¥åŠç”±äºé€šç”¨MLLMæ— æ³•è§£é‡Šç”µå½±è¯­æ³•ï¼ˆå¦‚é•œå¤´æ„æˆã€æ¼”å‘˜è¡¨è¾¾å’Œæ‘„å½±æœºè¿åŠ¨ï¼‰è€Œå¯¼è‡´çš„æ‹æ‘„æ„è¯†ç”Ÿæˆä¸è¶³ã€‚è¿™äº›äº¤ç»‡çš„é™åˆ¶é˜»ç¢äº†ç°å®çš„é•¿å½¢å¼åˆæˆå’Œä¸“ä¸šç”µå½±é£æ ¼çš„ç”Ÿæˆã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†SkyReels-V2ï¼Œä¸€ç§æ— é™é•¿åº¦çš„ç”µå½±ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒååŒäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€å¤šé˜¶æ®µé¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£å¼ºåˆ¶æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†è§†é¢‘çš„ç»¼åˆç»“æ„è¡¨ç¤ºï¼Œç»“åˆäº†å¤šæ¨¡æ€LLMçš„ä¸€èˆ¬æè¿°å’Œå­ä¸“å®¶æ¨¡å‹çš„è¯¦ç»†é•œå¤´è¯­è¨€ã€‚å€ŸåŠ©äººå·¥æ³¨é‡Šï¼Œæˆ‘ä»¬éšåè®­ç»ƒäº†ä¸€ä¸ªç»Ÿä¸€çš„è§†é¢‘æ ‡é¢˜ç”Ÿæˆå™¨ï¼Œåä¸ºSkyCaptioner-V1ï¼Œä»¥æœ‰æ•ˆåœ°æ ‡æ³¨è§†é¢‘æ•°æ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä¸ºåŸºæœ¬çš„è§†é¢‘ç”Ÿæˆå»ºç«‹äº†æ¸è¿›å¼åˆ†è¾¨ç‡é¢„è®­ç»ƒï¼Œéšåæ˜¯å››ä¸ªé˜¶æ®µçš„åè®­ç»ƒå¢å¼ºï¼šåˆå§‹æ¦‚å¿µå¹³è¡¡çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æé«˜äº†åŸºçº¿è´¨é‡ï¼›ä½¿ç”¨äººå·¥æ³¨é‡Šå’Œåˆæˆå¤±çœŸæ•°æ®çš„åŠ¨æ€ç‰¹å®šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè§£å†³äº†åŠ¨æ€ä¼ªåƒé—®é¢˜ï¼›æˆ‘ä»¬çš„å…·æœ‰éé€’å‡å™ªå£°å®‰æ’çš„æ‰©æ•£å¼ºåˆ¶æ¡†æ¶èƒ½å¤Ÿåœ¨æœ‰æ•ˆçš„æœç´¢ç©ºé—´ä¸­è¿›è¡Œé•¿è§†é¢‘åˆæˆï¼›æœ€åçš„é«˜è´¨é‡SFTå®Œå–„äº†è§†è§‰ä¿çœŸåº¦ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/SkyReels-V2%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SkyworkAI/SkyReels-V2æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13074v1">PDF</a> 31 pages,10 figures</p>
<p><strong>Summary</strong></p>
<p>å¤©ç©ºå·¥ä½œå›¢é˜Ÿæå‡ºäº†SkyReels-V2æ¨¡å‹ï¼Œä¸€ä¸ªæ— é™é•¿çš„ç”µå½±ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘ç”Ÿæˆä¸­çš„å¤šé‡æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€å¤šé˜¶æ®µé¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£å¼ºè¿«æ¡†æ¶çš„èåˆæ¥å®ç°ã€‚è¯¥æ¨¡å‹è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„è§†é¢‘ç»“æ„è¡¨ç¤ºï¼Œé€šè¿‡ç»Ÿä¸€è§†é¢‘å­—å¹•å™¨SkyCaptioner-V1è¿›è¡Œæ ‡æ³¨ã€‚æ¨¡å‹ç»è¿‡æ¸è¿›å¼åˆ†è¾¨ç‡é¢„è®­ç»ƒï¼Œç„¶åè¿›è¡Œå››é˜¶æ®µçš„åæœŸè®­ç»ƒå¢å¼ºã€‚ä»£ç å’Œæ¨¡å‹å‡å¯åœ¨SkyworkAIçš„GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SkyReels-V2è§£å†³äº†è§†é¢‘ç”Ÿæˆä¸­çš„å¤šé‡æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿åŠ¨åŠ¨æ€ã€è§†è§‰è´¨é‡ã€æŒç»­æ—¶é—´å’Œè°æ³¢ç­‰é—®é¢˜ã€‚</li>
<li>å®ƒæå‡ºäº†ä¸€ä¸ªæ— é™é•¿çš„ç”µå½±ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨ç”Ÿæˆæ›´ä¸ºçœŸå®çš„é•¿æœŸå’Œä¸“ä¸šçš„ç”µå½±é£æ ¼è§†é¢‘ã€‚</li>
<li>é€šè¿‡èåˆå¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€å¤šé˜¶æ®µé¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£å¼ºè¿«æ¡†æ¶ï¼Œå®ç°äº†æ›´å…ˆè¿›çš„è§†é¢‘ç”ŸæˆæŠ€æœ¯ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„è§†é¢‘ç»“æ„è¡¨ç¤ºï¼Œç»“åˆäº†å¤šæ¨¡å¼LLMçš„ä¸€èˆ¬æè¿°å’Œå­ä¸“å®¶æ¨¡å‹çš„è¯¦ç»†é•œå¤´è¯­è¨€ã€‚</li>
<li>é€šè¿‡ç»Ÿä¸€è§†é¢‘å­—å¹•å™¨SkyCaptioner-V1è¿›è¡Œæ ‡æ³¨ï¼Œæé«˜äº†è§†é¢‘æ•°æ®çš„æ•ˆç‡ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æ¸è¿›å¼åˆ†è¾¨ç‡é¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡å››é˜¶æ®µçš„åæœŸè®­ç»ƒå¢å¼ºæ¥æå‡æ€§èƒ½ã€‚æ¯ä¸ªé˜¶æ®µéƒ½æœ‰å…¶ç‰¹å®šçš„è®­ç»ƒç›®æ ‡å’Œç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-10e0d4599b96a127557ff3e201e9ef95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-135bbc853f17d529787e36191b59350b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9fbf10fae977ac87c8ae1741923de30.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5302e464edde26d5c78c6efa43188ed7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32f40e05213ea0ac9ecd71eeecc2b78b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RoboTwin-Dual-Arm-Robot-Benchmark-with-Generative-Digital-Twins"><a href="#RoboTwin-Dual-Arm-Robot-Benchmark-with-Generative-Digital-Twins" class="headerlink" title="RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins"></a>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins</h2><p><strong>Authors:Yao Mu, Tianxing Chen, Zanxin Chen, Shijia Peng, Zhiqian Lan, Zeyu Gao, Zhixuan Liang, Qiaojun Yu, Yude Zou, Mingkun Xu, Lunkai Lin, Zhiqiang Xie, Mingyu Ding, Ping Luo</strong></p>
<p>In the rapidly advancing field of robotics, dual-arm coordination and complex object manipulation are essential capabilities for developing advanced autonomous systems. However, the scarcity of diverse, high-quality demonstration data and real-world-aligned evaluation benchmarks severely limits such development. To address this, we introduce RoboTwin, a generative digital twin framework that uses 3D generative foundation models and large language models to produce diverse expert datasets and provide a real-world-aligned evaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates varied digital twins of objects from single 2D images, generating realistic and interactive scenarios. It also introduces a spatial relation-aware code generation framework that combines object annotations with large language models to break down tasks, determine spatial constraints, and generate precise robotic movement code. Our framework offers a comprehensive benchmark with both simulated and real-world data, enabling standardized evaluation and better alignment between simulated training and real-world performance. We validated our approach using the open-source COBOT Magic Robot platform. Policies pre-trained on RoboTwin-generated data and fine-tuned with limited real-world samples demonstrate significant potential for enhancing dual-arm robotic manipulation systems by improving success rates by over 70% for single-arm tasks and over 40% for dual-arm tasks compared to models trained solely on real-world data. </p>
<blockquote>
<p>åœ¨å¿«é€Ÿå‘å±•çš„æœºå™¨äººæŠ€æœ¯é¢†åŸŸä¸­ï¼ŒåŒè‡‚åè°ƒä¸å¤æ‚ç‰©ä½“æ“æ§æ˜¯å¼€å‘å…ˆè¿›è‡ªä¸»ç³»ç»Ÿä¸å¯æˆ–ç¼ºçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤šæ ·ä¸”é«˜è´¨é‡ç¤ºèŒƒæ•°æ®çš„ç¨€ç¼ºä»¥åŠç¼ºä¹ä¸ç°å®ä¸–ç•Œç›¸åŒ¹é…çš„è¯„ä¼°åŸºå‡†ä¸¥é‡åˆ¶çº¦äº†è¿™ç§å‘å±•ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RoboTwinï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ç”Ÿæˆå¼ä¸‰ç»´æœ¬ä½“æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºç”Ÿæˆå¼æ•°å­—åŒèƒèƒæ¡†æ¶ï¼Œä»¥äº§ç”Ÿå¤šæ ·çš„ä¸“å®¶æ•°æ®é›†å¹¶ä¸ºåŒè‡‚æœºå™¨äººä»»åŠ¡æä¾›ä¸ç°å®ä¸–ç•Œç›¸åŒ¹é…çš„è¯„ä¼°å¹³å°ã€‚å…·ä½“æ¥è¯´ï¼ŒRoboTwiné€šè¿‡å•ä¸ªäºŒç»´å›¾åƒåˆ›å»ºå¤šæ ·åŒ–çš„æ•°å­—åŒèƒèƒå¯¹è±¡ï¼Œç”Ÿæˆç°å®ä¸”äº’åŠ¨çš„åœºæ™¯ã€‚å®ƒè¿˜å¼•å…¥äº†ä¸€ä¸ªç©ºé—´å…³ç³»æ„ŸçŸ¥çš„ä»£ç ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆå¯¹è±¡æ³¨é‡Šå’Œå¤§å‹è¯­è¨€æ¨¡å‹æ¥åˆ†è§£ä»»åŠ¡ã€ç¡®å®šç©ºé—´çº¦æŸå¹¶ç”Ÿæˆç²¾ç¡®çš„æœºå™¨äººè¿åŠ¨ä»£ç ã€‚æˆ‘ä»¬çš„æ¡†æ¶æä¾›äº†åŒ…å«æ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®ä¸–ç•Œæ•°æ®çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œèƒ½å¤Ÿå®ç°æ ‡å‡†åŒ–è¯„ä¼°ï¼Œå¹¶æ”¹å–„æ¨¡æ‹Ÿè®­ç»ƒå’Œç°å®è¡¨ç°ä¹‹é—´çš„åŒ¹é…åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨å¼€æºçš„COBOT Magic Robotå¹³å°éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚åœ¨RoboTwinç”Ÿæˆçš„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨æœ‰é™çš„çœŸå®ä¸–ç•Œæ ·æœ¬ä¸Šè¿›è¡Œå¾®è°ƒçš„æ”¿ç­–ï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä¸ä»…ä½¿ç”¨çœŸå®ä¸–ç•Œæ•°æ®è®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼ŒåŒè‡‚ä»»åŠ¡çš„æˆåŠŸç‡æé«˜äº†è¶…è¿‡70%ï¼Œå•è‡‚ä»»åŠ¡çš„æˆåŠŸç‡æé«˜äº†è¶…è¿‡40%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13059v1">PDF</a> CVPR 2025 Highlight. 22 pages. Project page:   <a target="_blank" rel="noopener" href="https://robotwin-benchmark.github.io/">https://robotwin-benchmark.github.io/</a></p>
<p><strong>Summary</strong><br>åœ¨æœºå™¨äººæŠ€æœ¯çš„è¿…é€Ÿå‘å±•ä¸­ï¼ŒåŒè‡‚åè°ƒå’Œå¤æ‚ç‰©ä½“æ“ä½œå¯¹äºå¼€å‘å…ˆè¿›è‡ªä¸»ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç¼ºä¹å¤šæ ·åŒ–å’Œé«˜è´¨é‡ç¤ºèŒƒæ•°æ®ä»¥åŠç°å®ä¸–ç•Œè¯„ä¼°åŸºå‡†ä¸¥é‡é™åˆ¶äº†æ­¤ç±»å‘å±•ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºRoboTwinç”Ÿæˆå¼æ•°å­—åŒèƒèƒæ¡†æ¶ï¼Œåˆ©ç”¨ä¸‰ç»´ç”ŸæˆåŸºç¡€æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹æ¥æ„å»ºå¤šæ ·åŒ–çš„ä¸“å®¶æ•°æ®é›†ï¼Œå¹¶ä¸ºåŒè‡‚æœºå™¨äººä»»åŠ¡æä¾›ä¸ç°å®ç”Ÿæ´»ç›¸å¥‘åˆçš„è¯„ä¼°å¹³å°ã€‚RoboTwinæ¡†æ¶ä»å•ä¸€äºŒç»´å›¾åƒåˆ›å»ºå¯¹è±¡å¤šæ ·åŒ–çš„æ•°å­—åŒèƒèƒï¼Œç”ŸæˆçœŸå®ä¸”äº’åŠ¨çš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œå®ƒå¼•å…¥ç©ºé—´å…³ç³»æ„ŸçŸ¥çš„ä»£ç ç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆå¯¹è±¡æ³¨é‡Šå’Œå¤§å‹è¯­è¨€æ¨¡å‹æ¥æ‹†è§£ä»»åŠ¡ã€ç¡®å®šç©ºé—´çº¦æŸå’Œç”Ÿæˆç²¾ç¡®çš„æœºå™¨äººç§»åŠ¨ä»£ç ã€‚è¯¥æ¡†æ¶æä¾›æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå®ç°æ ‡å‡†åŒ–è¯„ä¼°ä»¥åŠæ¨¡æ‹Ÿè®­ç»ƒå’Œç°å®è¡¨ç°ä¹‹é—´çš„æ›´å¥½å¥‘åˆã€‚æˆ‘ä»¬åœ¨å¼€æºCOBOT Magic Robotå¹³å°ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»è¿‡RoboTwinç”Ÿæˆæ•°æ®é¢„è®­ç»ƒå¹¶åœ¨æœ‰é™çœŸå®æ ·æœ¬ä¸Šè¿›è¡Œå¾®è°ƒçš„æ”¿ç­–æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡æé«˜å•è‡‚ä»»åŠ¡å’ŒåŒè‡‚ä»»åŠ¡çš„æˆåŠŸç‡è¶…è¿‡70%å’Œ40%ï¼Œç›¸è¾ƒäºä»…åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººæŠ€æœ¯ä¸­åŒè‡‚åè°ƒå’Œå¤æ‚ç‰©ä½“æ“ä½œæ˜¯å¼€å‘å…ˆè¿›è‡ªä¸»ç³»ç»Ÿçš„æ ¸å¿ƒèƒ½åŠ›ã€‚</li>
<li>å½“å‰ç¼ºä¹å¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„ç¤ºèŒƒæ•°æ®ä»¥åŠç°å®ä¸–ç•Œè¯„ä¼°åŸºå‡†é™åˆ¶äº†å‘å±•ã€‚</li>
<li>RoboTwinæ¡†æ¶ä½¿ç”¨ä¸‰ç»´ç”ŸæˆåŸºç¡€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>RoboTwinåˆ›å»ºå¤šæ ·åŒ–çš„æ•°å­—åŒèƒèƒå’Œç”ŸæˆçœŸå®äº’åŠ¨åœºæ™¯ã€‚</li>
<li>å¼•å…¥ç©ºé—´å…³ç³»æ„ŸçŸ¥çš„ä»£ç ç”Ÿæˆæ¡†æ¶æ¥æ‹†è§£ä»»åŠ¡ã€ç¡®å®šç©ºé—´çº¦æŸå’Œç”Ÿæˆç²¾ç¡®æœºå™¨äººç§»åŠ¨ä»£ç ã€‚</li>
<li>æä¾›æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå®ç°æ ‡å‡†åŒ–è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8ac6f3803d961e237f2811ea88f10b59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72d143de5431d6b75c03d34d39432c07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49196c8ed0afeb77a7f7553049311fd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5d8bae53f4f34e88f5badc110d610ba.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="InstructRAG-Leveraging-Retrieval-Augmented-Generation-on-Instruction-Graphs-for-LLM-Based-Task-Planning"><a href="#InstructRAG-Leveraging-Retrieval-Augmented-Generation-on-Instruction-Graphs-for-LLM-Based-Task-Planning" class="headerlink" title="InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction   Graphs for LLM-Based Task Planning"></a>InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction   Graphs for LLM-Based Task Planning</h2><p><strong>Authors:Zheng Wang, Shu Xian Teo, Jun Jie Chew, Wei Shi</strong></p>
<p>Recent advancements in large language models (LLMs) have enabled their use as agents for planning complex tasks. Existing methods typically rely on a thought-action-observation (TAO) process to enhance LLM performance, but these approaches are often constrained by the LLMsâ€™ limited knowledge of complex tasks. Retrieval-augmented generation (RAG) offers new opportunities by leveraging external databases to ground generation in retrieved information. In this paper, we identify two key challenges (enlargability and transferability) in applying RAG to task planning. We propose InstructRAG, a novel solution within a multi-agent meta-reinforcement learning framework, to address these challenges. InstructRAG includes a graph to organize past instruction paths (sequences of correct actions), an RL-Agent with Reinforcement Learning to expand graph coverage for enlargability, and an ML-Agent with Meta-Learning to improve task generalization for transferability. The two agents are trained end-to-end to optimize overall planning performance. Our experiments on four widely used task planning datasets demonstrate that InstructRAG significantly enhances performance and adapts efficiently to new tasks, achieving up to a 19.2% improvement over the best existing approach. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å…¶èƒ½å¤Ÿä½œä¸ºæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„ä»£ç†ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ€ç»´-è¡ŒåŠ¨-è§‚å¯Ÿï¼ˆTAOï¼‰è¿‡ç¨‹æ¥æé«˜LLMçš„æ€§èƒ½ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å—åˆ°LLMå¯¹å¤æ‚ä»»åŠ¡äº†è§£æœ‰é™çš„åˆ¶çº¦ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡åˆ©ç”¨å¤–éƒ¨æ•°æ®åº“æ¥æ”¯æŒç”ŸæˆåŸºäºæ£€ç´¢çš„ä¿¡æ¯ï¼Œæä¾›äº†æ–°çš„æœºä¼šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æŒ‡å‡ºäº†å°†RAGåº”ç”¨äºä»»åŠ¡è§„åˆ’æ‰€é¢ä¸´çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå³å¯æ‰©å±•æ€§å’Œå¯è¿ç§»æ€§ã€‚æˆ‘ä»¬æå‡ºäº†InstructRAGï¼Œè¿™æ˜¯ä¸€ç§åœ¨å¤šæ™ºèƒ½ä½“å…ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶å†…çš„æ–°å‹è§£å†³æ–¹æ¡ˆï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚InstructRAGåŒ…æ‹¬ä¸€ä¸ªç”¨äºç»„ç»‡è¿‡å»æŒ‡ä»¤è·¯å¾„ï¼ˆæ­£ç¡®è¡ŒåŠ¨åºåˆ—ï¼‰çš„å›¾ã€ä¸€ä¸ªç”¨äºæ‰©å¤§å›¾è¦†ç›–ä»¥æé«˜å¯æ‰©å±•æ€§çš„å¼ºåŒ–å­¦ä¹ RLä»£ç†ï¼Œä»¥åŠä¸€ä¸ªç”¨äºæ”¹è¿›ä»»åŠ¡æ³›åŒ–ä»¥æé«˜å¯è¿ç§»æ€§çš„å…ƒå­¦ä¹ MLä»£ç†ã€‚è¿™ä¸¤ä¸ªä»£ç†è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒä»¥ä¼˜åŒ–æ•´ä½“è§„åˆ’æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„ä»»åŠ¡è§„åˆ’æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒInstructRAGæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå¹¶èƒ½é«˜æ•ˆé€‚åº”æ–°ä»»åŠ¡ï¼Œç›¸è¾ƒäºæœ€ä½³ç°æœ‰æ–¹æ³•å®ç°äº†é«˜è¾¾19.2%çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13032v1">PDF</a> This paper has been accepted by SIGIR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä½¿å…¶èƒ½å¤Ÿä½œä¸ºæ‰§è¡Œå¤æ‚ä»»åŠ¡çš„ä»£ç†ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿï¼ˆTAOï¼‰è¿‡ç¨‹æ¥æé«˜LLMçš„æ€§èƒ½ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å—åˆ°LLMå¯¹å¤æ‚ä»»åŠ¡äº†è§£æœ‰é™çš„åˆ¶çº¦ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡åˆ©ç”¨å¤–éƒ¨æ•°æ®åº“ä½¿ç”ŸæˆåŸºäºæ£€ç´¢çš„ä¿¡æ¯æä¾›äº†æ–°çš„æœºä¼šã€‚æœ¬æ–‡è¯†åˆ«äº†å°†RAGåº”ç”¨äºä»»åŠ¡è§„åˆ’çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå³å¯æ‰©å±•æ€§å’Œå¯è½¬ç§»æ€§ã€‚æˆ‘ä»¬æå‡ºäº†InstructRAGï¼Œè¿™æ˜¯ä¸€ç§å¤šä»£ç†å…ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶å†…çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚InstructRAGåŒ…æ‹¬ä¸€ä¸ªç»„ç»‡è¿‡å»æŒ‡ä»¤è·¯å¾„çš„å›¾ï¼ˆæ­£ç¡®è¡ŒåŠ¨åºåˆ—ï¼‰ã€ä¸€ä¸ªç”¨äºæ‰©å¤§å›¾çš„è¦†ç›–èŒƒå›´çš„å¼ºåŒ–å­¦ä¹ å¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œä»¥åŠä¸€ä¸ªç”¨äºæ”¹è¿›ä»»åŠ¡æ³›åŒ–çš„å…ƒå­¦ä¹ æœºå™¨å­¦ä¹ ä»£ç†ã€‚è¿™ä¸¤ä¸ªä»£ç†ç«¯å¯¹ç«¯è®­ç»ƒä»¥ä¼˜åŒ–æ•´ä½“è§„åˆ’æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„ä»»åŠ¡è§„åˆ’æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒInstructRAGæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå¹¶èƒ½é«˜æ•ˆé€‚åº”æ–°ä»»åŠ¡ï¼Œç›¸è¾ƒäºæœ€ä½³ç°æœ‰æ–¹æ³•å®ç°äº†æœ€é«˜è¾¾19.2%çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²ç”¨ä½œå¤æ‚ä»»åŠ¡çš„ä»£ç†ï¼Œä½†å—é™äºå¯¹ä»»åŠ¡äº†è§£æœ‰é™ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åˆ©ç”¨å¤–éƒ¨æ•°æ®åº“ï¼Œä½¿ç”ŸæˆåŸºäºæ£€ç´¢çš„ä¿¡æ¯ã€‚</li>
<li>åº”ç”¨RAGäºä»»åŠ¡è§„åˆ’é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå¯æ‰©å±•æ€§å’Œå¯è½¬ç§»æ€§ã€‚</li>
<li>InstructRAGæ˜¯ä¸€ä¸ªå¤šä»£ç†è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ç”¨äºç»„ç»‡è¿‡å»æŒ‡ä»¤è·¯å¾„çš„å›¾ã€‚</li>
<li>InstructRAGä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä»£ç†æ‰©å¤§å›¾çš„è¦†ç›–èŒƒå›´ï¼Œä½¿ç”¨å…ƒå­¦ä¹ ä»£ç†æé«˜ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç«¯å¯¹ç«¯è®­ç»ƒä¸¤ä¸ªä»£ç†ä»¥ä¼˜åŒ–æ•´ä½“è§„åˆ’æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-615ae1c58a765cf44929e8698227f424.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bdc1250aaa447ec3bc232248b541c1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f42ab4ee582abdd1c52b51b508e7599f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Paging-Dr-GPT-Extracting-Information-from-Clinical-Notes-to-Enhance-Patient-Predictions"><a href="#Paging-Dr-GPT-Extracting-Information-from-Clinical-Notes-to-Enhance-Patient-Predictions" class="headerlink" title="Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance   Patient Predictions"></a>Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance   Patient Predictions</h2><p><strong>Authors:David Anderson, Michaela Anderson, Margret Bjarnadottir, Stephen Mahar, Shriyan Reyya</strong></p>
<p>There is a long history of building predictive models in healthcare using tabular data from electronic medical records. However, these models fail to extract the information found in unstructured clinical notes, which document diagnosis, treatment, progress, medications, and care plans. In this study, we investigate how answers generated by GPT-4o-mini (ChatGPT) to simple clinical questions about patients, when given access to the patientâ€™s discharge summary, can support patient-level mortality prediction. Using data from 14,011 first-time admissions to the Coronary Care or Cardiovascular Intensive Care Units in the MIMIC-IV Note dataset, we implement a transparent framework that uses GPT responses as input features in logistic regression models. Our findings demonstrate that GPT-based models alone can outperform models trained on standard tabular data, and that combining both sources of information yields even greater predictive power, increasing AUC by an average of 5.1 percentage points and increasing positive predictive value by 29.9 percent for the highest-risk decile. These results highlight the value of integrating large language models (LLMs) into clinical prediction tasks and underscore the broader potential for using LLMs in any domain where unstructured text data remains an underutilized resource. </p>
<blockquote>
<p>åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œä½¿ç”¨ç”µå­åŒ»ç–—è®°å½•çš„è¡¨æ ¼æ•°æ®æ„å»ºé¢„æµ‹æ¨¡å‹æœ‰ç€æ‚ ä¹…çš„å†å²ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æ— æ³•æå–éç»“æ„åŒ–ä¸´åºŠç¬”è®°ä¸­çš„ä¿¡æ¯ï¼Œè¿™äº›ç¬”è®°è®°å½•äº†è¯Šæ–­ã€æ²»ç–—ã€è¿›å±•ã€è¯ç‰©å’ŒæŠ¤ç†è®¡åˆ’ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†GPT-4o-miniï¼ˆChatGPTï¼‰åœ¨è·å¾—æ‚£è€…å‡ºé™¢æ€»ç»“åï¼Œå¯¹æ‚£è€…ç®€å•ä¸´åºŠé—®é¢˜çš„å›ç­”å¦‚ä½•æ”¯æŒæ‚£è€…çº§åˆ«çš„æ­»äº¡é¢„æµ‹ã€‚æˆ‘ä»¬ä½¿ç”¨MIMIC-IV Noteæ•°æ®é›†ä¸­14,011åé¦–æ¬¡å…¥ä½å† çŠ¶åŠ¨è„‰ç›‘æŠ¤å®¤æˆ–å¿ƒè¡€ç®¡é‡ç—‡ç›‘æŠ¤å®¤æ‚£è€…çš„æ•°æ®ï¼Œå®æ–½äº†ä¸€ä¸ªé€æ˜çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨GPTçš„å“åº”ä½œä¸ºé€»è¾‘å›å½’æ¨¡å‹çš„è¾“å…¥ç‰¹å¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒåŸºäºGPTçš„æ¨¡å‹æœ¬èº«å°±èƒ½è¡¨ç°å‡ºè¶…è¶Šä»…ä½¿ç”¨è¡¨æ ¼æ•°æ®è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ï¼Œè€Œä¸”ç»“åˆè¿™ä¸¤ç§ä¿¡æ¯æ¥æºç”šè‡³èƒ½äº§ç”Ÿæ›´å¤§çš„é¢„æµ‹èƒ½åŠ›ï¼Œæœ€é«˜é£é™©ååˆ†ä½çš„AUCå¹³å‡æé«˜äº†5.1ä¸ªç™¾åˆ†ç‚¹ï¼Œé˜³æ€§é¢„æµ‹å€¼æé«˜äº†29.9ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åœ¨ä¸´åºŠé¢„æµ‹ä»»åŠ¡ä¸­æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»·å€¼ï¼Œå¹¶å¼ºè°ƒäº†åœ¨å¤§è§„æ¨¡ä½¿ç”¨éç»“æ„åŒ–æ–‡æœ¬æ•°æ®çš„é¢†åŸŸä¸­åº”ç”¨LLMçš„æ›´å¹¿æ³›æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12338v1">PDF</a> Paper and Online Supplement combined into one PDF. 26 pages. 2   figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨GPT-4o-miniï¼ˆChatGPTï¼‰å¤„ç†æ‚£è€…å‡ºé™¢æ€»ç»“ä¸­çš„éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ï¼Œä»¥æ”¯æŒæ‚£è€…å±‚é¢æ­»äº¡ç‡é¢„æµ‹çš„ç ”ç©¶ã€‚ç ”ç©¶ä½¿ç”¨æ¥è‡ªMIMIC-IV Noteæ•°æ®é›†çš„å¿ƒè¡€ç®¡é‡ç—‡ç›‘æŠ¤å•å…ƒé¦–æ¬¡å…¥é™¢æ‚£è€…çš„æ•°æ®ï¼Œå‘ç°åŸºäºGPTçš„æ¨¡å‹åœ¨å•ç‹¬ä½¿ç”¨æ—¶è¡¨ç°ä¼˜äºåŸºäºæ ‡å‡†è¡¨æ ¼æ•°æ®çš„æ¨¡å‹ï¼Œå¹¶ä¸”ä¸¤è€…ç»“åˆèƒ½æé«˜é¢„æµ‹èƒ½åŠ›ï¼Œæœ€é«˜é£é™©ç»„çš„AUCå¹³å‡æé«˜5.1ä¸ªç™¾åˆ†ç‚¹ï¼Œé˜³æ€§é¢„æµ‹å€¼æé«˜29.9%ã€‚è¿™æ˜¾ç¤ºäº†å°†å¤§å‹è¯­è¨€æ¨¡å‹æ•´åˆåˆ°ä¸´åºŠé¢„æµ‹ä»»åŠ¡ä¸­çš„ä»·å€¼ï¼Œå¹¶å¼ºè°ƒäº†åœ¨ä½¿ç”¨éç»“æ„åŒ–æ–‡æœ¬æ•°æ®çš„é¢†åŸŸä¸­åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨GPT-4o-miniå¤„ç†éç»“æ„åŒ–ä¸´åºŠç¬”è®°æ•°æ®ï¼Œæ”¯æŒæ‚£è€…æ­»äº¡ç‡é¢„æµ‹ã€‚</li>
<li>GPTæ¨¡å‹åœ¨å•ç‹¬ä½¿ç”¨æ—¶è¡¨ç°ä¼˜äºåŸºäºæ ‡å‡†è¡¨æ ¼æ•°æ®çš„æ¨¡å‹ã€‚</li>
<li>ç»“åˆGPTæ¨¡å‹å’Œè¡¨æ ¼æ•°æ®èƒ½æé«˜é¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>åœ¨æœ€é«˜é£é™©ç»„ä¸­ï¼ŒAUCå’Œé˜³æ€§é¢„æµ‹å€¼æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹åˆ°ä¸´åºŠé¢„æµ‹ä»»åŠ¡ä¸­å…·æœ‰é‡è¦çš„ä»·å€¼ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éç»“æ„åŒ–æ–‡æœ¬æ•°æ®æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12338">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71af46994045e72b0daff5734650589d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc1340158629819b0d8e1fa929410ef2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0969b1da76f37b8152b98205873be5c6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Span-level-Emotion-Cause-Category-Triplet-Extraction-with-Instruction-Tuning-LLMs-and-Data-Augmentation"><a href="#Span-level-Emotion-Cause-Category-Triplet-Extraction-with-Instruction-Tuning-LLMs-and-Data-Augmentation" class="headerlink" title="Span-level Emotion-Cause-Category Triplet Extraction with Instruction   Tuning LLMs and Data Augmentation"></a>Span-level Emotion-Cause-Category Triplet Extraction with Instruction   Tuning LLMs and Data Augmentation</h2><p><strong>Authors:Xiangju Li, Dong Yang, Xiaogang Zhu, Faliang Huang, Peng Zhang, Zhongying Zhao</strong></p>
<p>Span-level emotion-cause-category triplet extraction represents a novel and complex challenge within emotion cause analysis. This task involves identifying emotion spans, cause spans, and their associated emotion categories within the text to form structured triplets. While prior research has predominantly concentrated on clause-level emotion-cause pair extraction and span-level emotion-cause detection, these methods often confront challenges originating from redundant information retrieval and difficulty in accurately determining emotion categories, particularly when emotions are expressed implicitly or ambiguously. To overcome these challenges, this study explores a fine-grained approach to span-level emotion-cause-category triplet extraction and introduces an innovative framework that leverages instruction tuning and data augmentation techniques based on large language models. The proposed method employs task-specific triplet extraction instructions and utilizes low-rank adaptation to fine-tune large language models, eliminating the necessity for intricate task-specific architectures. Furthermore, a prompt-based data augmentation strategy is developed to address data scarcity by guiding large language models in generating high-quality synthetic training data. Extensive experimental evaluations demonstrate that the proposed approach significantly outperforms existing baseline methods, achieving at least a 12.8% improvement in span-level emotion-cause-category triplet extraction metrics. The results demonstrate the methodâ€™s effectiveness and robustness, offering a promising avenue for advancing research in emotion cause analysis. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/zxgnlp/InstruDa-LLM">https://github.com/zxgnlp/InstruDa-LLM</a>. </p>
<blockquote>
<p>è·¨çº§æƒ…ç»ª-åŸå› -ç±»åˆ«ä¸‰å…ƒç»„æå–ä»£è¡¨äº†æƒ…ç»ªåŸå› åˆ†æä¸­çš„ä¸€ä¸ªæ–°é¢–ä¸”å¤æ‚çš„æŒ‘æˆ˜ã€‚æ­¤ä»»åŠ¡æ¶‰åŠåœ¨æ–‡æœ¬ä¸­è¯†åˆ«æƒ…ç»ªè·¨åº¦ã€åŸå› è·¨åº¦åŠå…¶ç›¸å…³çš„æƒ…ç»ªç±»åˆ«ï¼Œä»¥å½¢æˆç»“æ„åŒ–ä¸‰å…ƒç»„ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¥å­çº§æƒ…ç»ª-åŸå› å¯¹æå–å’Œè·¨çº§æƒ…ç»ª-åŸå› æ£€æµ‹ä¸Šï¼Œä½†è¿™äº›æ–¹æ³•ç»å¸¸é¢ä¸´æ¥è‡ªå†—ä½™ä¿¡æ¯æ£€ç´¢çš„æŒ‘æˆ˜ï¼Œä»¥åŠåœ¨å‡†ç¡®ç¡®å®šæƒ…ç»ªç±»åˆ«æ–¹é¢çš„å›°éš¾ï¼Œç‰¹åˆ«æ˜¯å½“æƒ…ç»ªè¢«éšå«æˆ–æ¨¡ç³Šè¡¨è¾¾æ—¶ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æ¢ç´¢äº†ä¸€ç§ç²¾ç»†çš„è·¨çº§æƒ…ç»ª-åŸå› -ç±»åˆ«ä¸‰å…ƒç»„æå–æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æŒ‡ä»¤è°ƒæ•´å’Œæ•°æ®å¢å¼ºæŠ€æœ¯ã€‚æ‰€æå‡ºçš„æ–¹æ³•é‡‡ç”¨ç‰¹å®šçš„ä¸‰å…ƒç»„æå–æŒ‡ä»¤ï¼Œå¹¶åˆ©ç”¨ä½ç§©é€‚åº”æ¥å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ— éœ€ä½¿ç”¨å¤æ‚çš„ç‰¹å®šä»»åŠ¡æ¶æ„ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºæç¤ºçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡åˆæˆè®­ç»ƒæ•°æ®æ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œåœ¨è·¨çº§æƒ…ç»ª-åŸå› -ç±»åˆ«ä¸‰å…ƒç»„æå–æŒ‡æ ‡ä¸Šè‡³å°‘æé«˜äº†12.8%ã€‚ç»“æœè¡¨æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Œä¸ºæƒ…ç»ªåŸå› åˆ†æçš„ç ”ç©¶æä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zxgnlp/InstruDa-LLM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zxgnlp/InstruDa-LLMä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12331v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç²¾ç»†åŒ–æƒ…æ„Ÿèµ·å› åˆ†ç±»ä¸‰å…ƒç»„æå–æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æŒ‡ä»¤è°ƒä¼˜å’ŒåŸºäºæŒ‡ä»¤çš„æ•°æ®æ‰©å……æŠ€æœ¯ï¼Œå…‹æœäº†å†—ä½™ä¿¡æ¯æ£€ç´¢å’Œç¡®å®šæƒ…æ„Ÿç±»åˆ«çš„å›°éš¾ã€‚é€šè¿‡ä»»åŠ¡ç‰¹å®šçš„ä¸‰å…ƒç»„æå–æŒ‡ä»¤å’Œä½ç§©é€‚åº”æŠ€æœ¯ï¼Œå®ç°äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒï¼Œæ— éœ€å¤æ‚çš„ä»»åŠ¡ç‰¹å®šæ¶æ„ã€‚åŒæ—¶ï¼Œå¼€å‘äº†ä¸€ç§åŸºäºæç¤ºçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œé€šè¿‡å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡åˆæˆè®­ç»ƒæ•°æ®æ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æƒ…æ„Ÿèµ·å› åˆ†ç±»ä¸‰å…ƒç»„æå–æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œè‡³å°‘æé«˜äº†12.8%ã€‚æºä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æƒ…æ„Ÿèµ·å› åˆ†ç±»ä¸‰å…ƒç»„æå–æ˜¯ä¸€ä¸ªæ–°å…´ä¸”å¤æ‚çš„æŒ‘æˆ˜ï¼Œæ¶‰åŠè¯†åˆ«æ–‡æœ¬ä¸­çš„æƒ…æ„Ÿè·¨åº¦ã€åŸå› è·¨åº¦åŠå…¶ç›¸å…³æƒ…æ„Ÿç±»åˆ«ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´å†—ä½™ä¿¡æ¯æ£€ç´¢å’Œç¡®å®šæƒ…æ„Ÿç±»åˆ«ï¼ˆå°¤å…¶æ˜¯éšæ€§æˆ–æ¨¡ç³Šè¡¨è¾¾çš„æƒ…æ„Ÿï¼‰çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨ç²¾ç»†åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿèµ·å› åˆ†ç±»ä¸‰å…ƒç»„æå–ã€‚</li>
<li>åˆ›æ–°æ€§åœ°ä½¿ç”¨æŒ‡ä»¤è°ƒä¼˜å’Œæ•°æ®æ‰©å……æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>é‡‡ç”¨ä»»åŠ¡ç‰¹å®šçš„æŒ‡ä»¤å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ— éœ€å¤æ‚çš„ä»»åŠ¡ç‰¹å®šæ¶æ„ã€‚</li>
<li>é€šè¿‡åŸºäºæç¤ºçš„æ•°æ®å¢å¼ºç­–ç•¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œç”Ÿæˆé«˜è´¨é‡åˆæˆè®­ç»ƒæ•°æ®ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d89c4c2885c1355a2b27578c458cce58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75b7c746af2098bd2785c12235739a81.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8fc4118231e35aa3e7efd41d9b270f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-716f1b1d34999309af403cfa20803726.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ReTool-Reinforcement-Learning-for-Strategic-Tool-Use-in-LLMs"><a href="#ReTool-Reinforcement-Learning-for-Strategic-Tool-Use-in-LLMs" class="headerlink" title="ReTool: Reinforcement Learning for Strategic Tool Use in LLMs"></a>ReTool: Reinforcement Learning for Strategic Tool Use in LLMs</h2><p><strong>Authors:Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, Wanjun Zhong</strong></p>
<p>While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the modelâ€™s tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReToolâ€™s superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAIâ€™s o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an â€˜â€™aha momentâ€™â€™ in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems. </p>
<blockquote>
<p>è™½ç„¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„æ¨ç†æ¨¡å‹ï¼ˆä¾‹å¦‚DeepSeek R1ï¼‰åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦ç»“æ„åŒ–é—®é¢˜è§£å†³ï¼ˆå¦‚å‡ ä½•æ¨ç†ã€ç®€æ´è®¡ç®—æˆ–å¤æ‚æ–¹ç¨‹æ±‚è§£ï¼‰çš„åœºæ™¯ä¸­å´è¡¨ç°ä¸ä½³ï¼Œè¿™äº›é¢†åŸŸæ­£æ˜¯ä»£ç è§£é‡Šå™¨ï¼ˆCIï¼‰ç­‰è®¡ç®—å·¥å…·å±•ç°æ˜æ˜¾ä¼˜åŠ¿çš„åœ°æ–¹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ReToolï¼Œå®ƒé€šè¿‡å·¥å…·é›†æˆå­¦ä¹ å¢å¼ºé•¿å½¢å¼æ¨ç†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®åŠŸèƒ½ï¼šï¼ˆ1ï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€äº¤æ›¿å®æ—¶ä»£ç æ‰§è¡Œï¼›ï¼ˆ2ï¼‰ä¸€ç§è‡ªåŠ¨åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¨¡å¼ï¼Œå…è®¸é€šè¿‡å¤šè½®å®æ—¶ä»£ç æ‰§è¡Œè¿›è¡Œç­–ç•¥æ¨æ¼”ï¼Œå¹¶æ ¹æ®ç»“æœåé¦ˆæ¥æ•™æˆæ¨¡å‹ä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨å·¥å…·ã€‚ReToolé‡‡ç”¨ç³»ç»Ÿçš„è®­ç»ƒæ¡†æ¶ï¼Œä»ç”Ÿæˆåˆæˆå†·å¯åŠ¨æ•°æ®å¼€å§‹ï¼Œä»¥äº§ç”Ÿç”¨äºå¾®è°ƒåŸºç¡€æ¨¡å‹çš„é•¿å½¢å¼ä»£ç è¾…åŠ©æ¨ç†è½¨è¿¹ã€‚éšåçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåˆ©ç”¨ä»»åŠ¡ç»“æœä½œä¸ºå¥–åŠ±æ¥è¿­ä»£ä¼˜åŒ–æ¨¡å‹å¯¹å·¥å…·ä½¿ç”¨ç­–ç•¥çš„è°ƒæ•´ï¼Œä»è€Œèƒ½å¤Ÿè‡ªä¸»å‘ç°æœ€ä½³çš„å·¥å…·è°ƒç”¨æ¨¡å¼è€Œæ— éœ€äººä¸ºå…ˆéªŒçŸ¥è¯†ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„MATH OlympiadåŸºå‡†æµ‹è¯•AIMEä¸Šçš„å®éªŒè¯æ˜äº†ReToolçš„ä¼˜è¶Šæ€§ï¼šæˆ‘ä»¬çš„32Bæ¨¡å‹åœ¨400æ­¥è®­ç»ƒåè¾¾åˆ°äº†67%çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºåŸºäºæ–‡æœ¬çš„å¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼ˆå‡†ç¡®ç‡ä¸º40%ï¼Œè®­ç»ƒæ­¥éª¤ä¸º1080æ­¥ï¼‰ï¼Œåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šæ›´èƒœä¸€ç­¹ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒReTool-32Båœ¨æ‰©å±•è®¾ç½®ä¸­è¾¾åˆ°äº†72.5%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†OpenAIçš„o1-previewåŸºå‡†çš„27.9%ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ˜¾ç¤ºå‡ºç°ä»£ç è‡ªæˆ‘ä¿®æ­£ç­‰çªå‘è¡Œä¸ºï¼Œæ ‡å¿—ç€æ¨¡å‹è‡ªä¸»æŒæ¡è‡ªé€‚åº”å·¥å…·ä½¿ç”¨çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†ç»“æœé©±åŠ¨çš„å·¥å…·é›†æˆåœ¨æ¨åŠ¨å¤æ‚æ•°å­¦æ¨ç†æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæ··åˆç¥ç»ç¬¦å·ç³»ç»Ÿæä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11536v2">PDF</a> fix typos</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ¨ç†æ¨¡å‹åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦ç»“æ„åŒ–é—®é¢˜è§£å†³ï¼ˆå¦‚å‡ ä½•æ¨ç†ã€ç®€æ´è®¡ç®—å’Œå¤æ‚æ–¹ç¨‹æ±‚è§£ï¼‰çš„åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ReToolå·¥å…·é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å®æ—¶ä»£ç æ‰§è¡Œä¸è‡ªç„¶è¯­è¨€æ¨ç†è¿‡ç¨‹çš„åŠ¨æ€äº¤ç»‡ï¼Œä»¥åŠåŸºäºç»“æœåé¦ˆçš„è‡ªåŠ¨åŒ–å¼ºåŒ–å­¦ä¹ æ¨¡å¼ï¼Œæé«˜äº†é•¿å½¢å¼æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReToolåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„MATH OlympiadåŸºå‡†æµ‹è¯•AIMEä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºä»…åŸºäºæ–‡æœ¬çš„RLåŸºçº¿ã€‚ReToolæ¨¡å‹è‡ªä¸»å­¦ä¹ é€‚åº”å·¥å…·ä½¿ç”¨çš„è¡Œä¸ºï¼Œæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ¨ç†æ¨¡å‹åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç»“æ„åŒ–é—®é¢˜è§£å†³æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ReToolå·¥å…·é›†æˆå­¦ä¹ æ–¹æ³•é€šè¿‡ç»“åˆå®æ—¶ä»£ç æ‰§è¡Œä¸è‡ªç„¶è¯­è¨€æ¨ç†ï¼Œå¢å¼ºäº†é•¿å½¢å¼æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ReToolé‡‡ç”¨è‡ªåŠ¨åŒ–å¼ºåŒ–å­¦ä¹ æ¨¡å¼ï¼Œæ ¹æ®ç»“æœåé¦ˆè°ƒæ•´å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚</li>
<li>ReToolåœ¨MATH OlympiadåŸºå‡†æµ‹è¯•AIMEä¸Šè¡¨ç°ä¼˜è¶Šï¼Œæ•ˆç‡ä¸æ€§èƒ½å‡ä¼˜äºåŸºäºæ–‡æœ¬çš„RLåŸºçº¿ã€‚</li>
<li>ReToolæ¨¡å‹å±•ç°å‡ºè‡ªä¸»å‘ç°æœ€ä¼˜å·¥å…·è°ƒç”¨æ¨¡å¼çš„æ½œåŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨æ‰©å±•è®¾ç½®ä¸‹è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ï¼Œè¶…è¿‡OpenAIçš„o1-previewã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-52b232050a403eed6d9921e11f769967.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-953adfbbde6bfedaa9b3b98c71097382.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e844ceb7678a43a71bac624c0c8b6bc3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SIFT-50M-A-Large-Scale-Multilingual-Dataset-for-Speech-Instruction-Fine-Tuning"><a href="#SIFT-50M-A-Large-Scale-Multilingual-Dataset-for-Speech-Instruction-Fine-Tuning" class="headerlink" title="SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction   Fine-Tuning"></a>SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction   Fine-Tuning</h2><p><strong>Authors:Prabhat Pandey, Rupak Vignesh Swaminathan, K V Vijay Girish, Arunasish Sen, Jian Xie, Grant P. Strimel, Andreas Schwarz</strong></p>
<p>We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing a diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, a benchmark dataset specifically designed to evaluate the instruction-following capabilities of speech-text LLMs. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SIFTï¼ˆè¯­éŸ³æŒ‡ä»¤å¾®è°ƒï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ª50ç™¾ä¸‡ç¤ºä¾‹æ•°æ®é›†ï¼Œä¸“ä¸ºè¯­éŸ³æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‡ä»¤å¾®è°ƒå’Œé¢„è®­ç»ƒè€Œè®¾è®¡ã€‚SIFT-50Mæ˜¯ä»å…¬å¼€å¯ç”¨çš„è¯­éŸ³è¯­æ–™åº“ä¸­æ„å»ºè€Œæˆçš„ï¼Œè¿™äº›è¯­æ–™åº“æ€»å…±åŒ…å«1ä¸‡å››åƒå°æ—¶çš„è¯­éŸ³ï¼Œå¹¶åˆ©ç”¨LLMå’Œç°æˆçš„ä¸“å®¶æ¨¡å‹ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº”ç§è¯­è¨€ï¼ŒåŒ…æ‹¬å¤šæ ·åŒ–çš„è¯­éŸ³ç†è§£å’Œå¯æ§è¯­éŸ³ç”ŸæˆæŒ‡ä»¤ã€‚ä½¿ç”¨SIFT-50Mï¼Œæˆ‘ä»¬è®­ç»ƒäº†SIFT-LLMï¼Œåœ¨æŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„è¯­éŸ³æ–‡æœ¬LLMï¼ŒåŒæ—¶åœ¨åŸºæœ¬çš„è¯­éŸ³ä»»åŠ¡ä¸­å–å¾—äº†æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚ä¸ºäº†æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†EvalSIFTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è¯­éŸ³æ–‡æœ¬LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„åŸºå‡†æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09081v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬ä»‹ç»äº†SIFTï¼ˆè¯­éŸ³æŒ‡ä»¤å¾®è°ƒï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«5000ä¸‡ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œæ—¨åœ¨ç”¨äºè¯­éŸ³æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‡ä»¤å¾®è°ƒå’Œé¢„è®­ç»ƒã€‚SIFT-50Mæ˜¯ä»å…¬å¼€å¯ç”¨çš„è¯­éŸ³è¯­æ–™åº“ä¸­æ„å»ºè€Œæˆï¼Œå…¶ä¸­åŒ…å«14000å°æ—¶çš„è¯­éŸ³ï¼Œå¹¶åˆ©ç”¨LLMå’Œç°æˆçš„ä¸“å®¶æ¨¡å‹ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº”ç§è¯­è¨€ï¼ŒåŒ…æ‹¬å¤šæ ·åŒ–çš„è¯­éŸ³ç†è§£ä»¥åŠå¯æ§çš„è¯­éŸ³ç”ŸæˆæŒ‡ä»¤ã€‚ä½¿ç”¨SIFT-50Mè®­ç»ƒçš„SIFT-LLMåœ¨æŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„è¯­éŸ³æ–‡æœ¬LLMï¼ŒåŒæ—¶åœ¨åŸºç¡€è¯­éŸ³ä»»åŠ¡ä¸Šå®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ä¸ºäº†æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†EvalSIFTåŸºå‡†æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è¯­éŸ³æ–‡æœ¬LLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SIFTæ˜¯ä¸€ä¸ªåŒ…å«50Må®ä¾‹çš„æ•°æ®é›†ï¼Œç”¨äºè¯­éŸ³æŒ‡ä»¤å¾®è°ƒå’Œå¤§æ¨¡å‹é¢„è®­ç»ƒã€‚</li>
<li>æ•°æ®é›†ç”±å…¬å¼€å¯ç”¨çš„è¯­éŸ³è¯­æ–™åº“æ„å»ºï¼ŒåŒ…å«14Kå°æ—¶çš„è¯­éŸ³æ•°æ®ã€‚</li>
<li>SIFTåˆ©ç”¨LLMå’Œç°æˆçš„ä¸“å®¶æ¨¡å‹ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–äº”ç§è¯­è¨€ï¼ŒåŒ…å«å¤šç§è¯­éŸ³ç†è§£å’Œå¯æ§è¯­éŸ³ç”ŸæˆæŒ‡ä»¤ã€‚</li>
<li>SIFT-LLMåœ¨æŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–è¯­éŸ³æ–‡æœ¬LLMã€‚</li>
<li>SIFT-LLMåœ¨åŸºç¡€è¯­éŸ³ä»»åŠ¡ä¸Šå®ç°å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09081">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98771a0eb9bffa9892b36082ac97a597.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ead3f990201607a20b0443ed55f48c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d34478728f05525643d1c6663bc4df4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ffa9cdddb2dc6c3b39a6bf09819067e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning"><a href="#GPG-A-Simple-and-Strong-Reinforcement-Learning-Baseline-for-Model-Reasoning" class="headerlink" title="GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning"></a>GPG: A Simple and Strong Reinforcement Learning Baseline for Model   Reasoning</h2><p><strong>Authors:Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang</strong></p>
<p>Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. By eliminating the critic and reference models, avoiding KL divergence constraints, and addressing the advantage and gradient estimation bias, our approach significantly simplifies the training process compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. As illustrated in Figure 1, extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG">https://github.com/AMAP-ML/GPG</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥ç›´æ¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿‡å¤šä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°ç ”ç©¶äº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æç®€çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒGPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„å¼ºåŒ–å­¦ä¹ ç›®æ ‡ï¼Œä»è€Œæ— éœ€æ›¿ä»£æŸå¤±å‡½æ•°ã€‚é€šè¿‡æ¶ˆé™¤æ‰¹è¯„è€…å’Œå‚è€ƒæ¨¡å‹ï¼Œé¿å…KLæ•£åº¦çº¦æŸï¼Œå¹¶è§£å†³ä¼˜åŠ¿å’Œæ¢¯åº¦ä¼°è®¡åå·®çš„é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§ç®€åŒ–äº†ä¸ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸æ¯”çš„è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸ä¾èµ–è¾…åŠ©æŠ€æœ¯æˆ–è°ƒæ•´çš„æƒ…å†µä¸‹å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å¦‚å›¾1æ‰€ç¤ºï¼Œå¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”åœ¨å„ç§å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºGRPOã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/GPG%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AMAP-ML/GPGä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02546v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ç›´æ¥æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€è¿‡å¤šä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æœ¬ç ”ç©¶é‡æ–°å®¡è§†äº†ä¼ ç»Ÿçš„ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æç®€çš„RLæ–¹æ³•â€”â€”ç¾¤ç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ã€‚GPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„RLç›®æ ‡ï¼Œä»è€Œæ— éœ€æ›¿ä»£æŸå¤±å‡½æ•°ã€‚é€šè¿‡æ¶ˆé™¤è¯„è®ºå®¶æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ï¼Œé¿å…KLæ•£åº¦çº¦æŸï¼Œå¹¶è§£å†³ä¼˜åŠ¿å’Œæ¢¯åº¦ä¼°è®¡åå·®é—®é¢˜ï¼ŒGPGç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒGPGä¸ä»…é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œè€Œä¸”åœ¨å„ç§å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸Šå‡ä¼˜äºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯ä»¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸”ä¸éœ€è¦å¤§é‡ç›‘ç£å¾®è°ƒã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”ç¾¤ç»„ç­–ç•¥æ¢¯åº¦ï¼ˆGPGï¼‰ã€‚</li>
<li>GPGç›´æ¥ä¼˜åŒ–åŸå§‹çš„å¼ºåŒ–å­¦ä¹ ç›®æ ‡ï¼Œæ— éœ€ä½¿ç”¨æ›¿ä»£æŸå¤±å‡½æ•°ã€‚</li>
<li>GPGç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œé€šè¿‡æ¶ˆé™¤è¯„è®ºå®¶æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹ï¼Œå¹¶è§£å†³äº†KLæ•£åº¦çº¦æŸã€ä¼˜åŠ¿ä¼°è®¡å’Œæ¢¯åº¦ä¼°è®¡åå·®ç­‰é—®é¢˜ã€‚</li>
<li>GPGåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚</li>
<li>GPGæ–¹æ³•é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-96d443e6632ee50810fe008b9f05c935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b54a2d1843508cb20fbb024aeea7c04b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd74a39c41367cbdd9c94aee2c56beae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b303edd1c6c88e2bbd9d342c7c4fd646.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7dc6170186382993f0f806dfaa2836eb.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RealSyn-An-Effective-and-Scalable-Multimodal-Interleaved-Document-Transformation-Paradigm"><a href="#RealSyn-An-Effective-and-Scalable-Multimodal-Interleaved-Document-Transformation-Paradigm" class="headerlink" title="RealSyn: An Effective and Scalable Multimodal Interleaved Document   Transformation Paradigm"></a>RealSyn: An Effective and Scalable Multimodal Interleaved Document   Transformation Paradigm</h2><p><strong>Authors:Tiancheng Gu, Kaicheng Yang, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, Jiankang Deng</strong></p>
<p>After pre-training on extensive image-text pairs, Contrastive Language-Image Pre-training (CLIP) demonstrates promising performance on a wide variety of benchmarks. However, a substantial volume of multimodal interleaved documents remains underutilized for contrastive vision-language representation learning. To fully leverage these unpaired documents, we initially establish a Real-World Data Extraction pipeline to extract high-quality images and texts. Then we design a hierarchical retrieval method to efficiently associate each image with multiple semantically relevant realistic texts. To further enhance fine-grained visual information, we propose an image semantic augmented generation module for synthetic text production. Furthermore, we employ a semantic balance sampling strategy to improve dataset diversity, enabling better learning of long-tail concepts. Based on these innovations, we construct RealSyn, a dataset combining realistic and synthetic texts, available in three scales: 15M, 30M, and 100M. We compare our dataset with other widely used datasets of equivalent scale for CLIP training. Models pre-trained on RealSyn consistently achieve state-of-the-art performance across various downstream tasks, including linear probe, zero-shot transfer, zero-shot robustness, and zero-shot retrieval. Furthermore, extensive experiments confirm that RealSyn significantly enhances contrastive vision-language representation learning and demonstrates robust scalability. To facilitate future research, the RealSyn dataset and pretrained model weights are released at <a target="_blank" rel="noopener" href="https://github.com/deepglint/RealSyn">https://github.com/deepglint/RealSyn</a>. </p>
<blockquote>
<p>åœ¨å¤§é‡å›¾æ–‡å¯¹ä¸Šè¿›è¡Œé¢„è®­ç»ƒåï¼Œå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºæœ‰å¸Œæœ›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤§é‡çš„å¤šæ¨¡æ€äº¤é”™æ–‡æ¡£åœ¨å¯¹æ¯”è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ä¸­ä»æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™äº›æœªé…å¯¹çš„æ–‡æ¡£ï¼Œæˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªçœŸå®ä¸–ç•Œæ•°æ®æå–ç®¡é“ï¼Œä»¥æå–é«˜è´¨é‡çš„å›¾ç‰‡å’Œæ–‡å­—ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ†å±‚æ£€ç´¢æ–¹æ³•ï¼Œä»¥æœ‰æ•ˆåœ°å°†æ¯å¼ å›¾åƒä¸å¤šä¸ªè¯­ä¹‰ä¸Šç›¸å…³çš„çœŸå®æ–‡æœ¬ç›¸å…³è”ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¸°å¯Œç²¾ç»†çš„è§†è§‰ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå›¾åƒè¯­ä¹‰å¢å¼ºç”Ÿæˆæ¨¡å—æ¥è¿›è¡Œåˆæˆæ–‡æœ¬çš„ç”Ÿäº§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è¯­ä¹‰å¹³è¡¡é‡‡æ ·ç­–ç•¥ï¼Œä»¥æé«˜æ•°æ®é›†å¤šæ ·æ€§ï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ é•¿å°¾æ¦‚å¿µã€‚åŸºäºè¿™äº›åˆ›æ–°ï¼Œæˆ‘ä»¬æ„å»ºäº†RealSynæ•°æ®é›†ï¼Œèåˆäº†çœŸå®å’Œåˆæˆæ–‡æœ¬ï¼Œæä¾›ä¸‰ç§è§„æ¨¡ï¼š1.5äº¿ã€3äº¿å’Œ10äº¿ã€‚æˆ‘ä»¬å°†æ•°æ®é›†ä¸å…¶ä»–å¹¿æ³›ç”¨äºCLIPè®­ç»ƒçš„åŒç­‰è§„æ¨¡æ•°æ®é›†è¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒåŸºäºRealSyné¢„è®­ç»ƒçš„æ¨¡å‹å§‹ç»ˆå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬çº¿æ€§æ¢æµ‹ã€é›¶æ ·æœ¬è¿ç§»ã€é›¶æ ·æœ¬é²æ£’æ€§å’Œé›¶æ ·æœ¬æ£€ç´¢ã€‚æ­¤å¤–ï¼Œå¤§é‡å®éªŒè¯å®ï¼ŒRealSynæ˜¾è‘—å¢å¼ºäº†å¯¹æ¯”è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶è¡¨ç°å‡ºç¨³å¥çš„å¯æ‰©å±•æ€§ã€‚ä¸ºäº†æ–¹ä¾¿æœªæ¥çš„ç ”ç©¶ï¼ŒRealSynæ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹æƒé‡å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/deepglint/RealSyn">https://github.com/deepglint/RealSyn</a>ä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12513v2">PDF</a> 15 pages, 12 figures, Webpage: <a target="_blank" rel="noopener" href="https://garygutc.github.io/RealSyn">https://garygutc.github.io/RealSyn</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒçš„æ”¹è¿›æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºRealSynæ•°æ®é›†æ¥æå‡å¯¹æ¯”è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ã€‚RealSynæ•°æ®é›†ç»“åˆäº†çœŸå®å’Œåˆæˆæ–‡æœ¬ï¼Œå…·æœ‰å¤šå±‚æ¬¡è§„æ¨¡ã€‚åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒåŸºäºRealSynæ•°æ®é›†çš„é¢„è®­ç»ƒæ¨¡å‹å–å¾—äº†æœ€æ–°æ€§èƒ½è¡¨ç°ã€‚è¯¥æ•°æ®é›†åŠé¢„è®­ç»ƒæ¨¡å‹æƒé‡å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œä½†åœ¨å¯¹æ¯”è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ ä¸Šå­˜åœ¨å¤§é‡æœªå……åˆ†åˆ©ç”¨çš„å¤šæ¨¡æ€äº¤é”™æ–‡æ¡£ã€‚</li>
<li>æå‡ºå»ºç«‹Real-World Data Extractionç®¡é“æ¥æå–é«˜è´¨é‡å›¾åƒå’Œæ–‡æœ¬ã€‚</li>
<li>è®¾è®¡äº†å±‚æ¬¡åŒ–çš„æ£€ç´¢æ–¹æ³•æ¥é«˜æ•ˆåœ°å°†æ¯ä¸ªå›¾åƒä¸å¤šä¸ªè¯­ä¹‰ç›¸å…³çš„ç°å®æ–‡æœ¬ç›¸å…³è”ã€‚</li>
<li>å¼•å…¥äº†å›¾åƒè¯­ä¹‰å¢å¼ºç”Ÿæˆæ¨¡å—æ¥ç”Ÿæˆåˆæˆæ–‡æœ¬ï¼Œä»¥æé«˜ç²¾ç»†çš„è§†è§‰ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨è¯­ä¹‰å¹³è¡¡é‡‡æ ·ç­–ç•¥æé«˜æ•°æ®é›†å¤šæ ·æ€§ï¼Œæ›´å¥½åœ°å­¦ä¹ é•¿å°¾æ¦‚å¿µã€‚</li>
<li>åˆ›å»ºäº†RealSynæ•°æ®é›†ï¼Œç»“åˆçœŸå®å’Œåˆæˆæ–‡æœ¬ï¼Œæä¾›ä¸‰ç§è§„æ¨¡ä¾›é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-681cac9664c6fbede45afc4383bbc0f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69b7e3d74bef32ca7a8cd55c9481278d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-391631cf0c5982e96005f08d60ef4997.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71ab7801274088fe7c0b5a856861f101.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7de3a57e554c786e1474e1221d2c1591.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aed75b8d29afc5644022b2ebe7b4b44.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="UniForm-A-Unified-Multi-Task-Diffusion-Transformer-for-Audio-Video-Generation"><a href="#UniForm-A-Unified-Multi-Task-Diffusion-Transformer-for-Audio-Video-Generation" class="headerlink" title="UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video   Generation"></a>UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video   Generation</h2><p><strong>Authors:Lei Zhao, Linfeng Feng, Dongxu Ge, Rujin Chen, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, Xuelong Li</strong></p>
<p>With the rise of diffusion models, audio-video generation has been revolutionized. However, most existing methods rely on separate modules for each modality, with limited exploration of unified generative architectures. In addition, many are confined to a single task and small-scale datasets. To address these limitations, we first propose UniForm, a unified multi-task diffusion transformer that jointly generates audio and visual modalities in a shared latent space. A single diffusion process models both audio and video, capturing the inherent correlations between sound and vision. Second, we introduce task-specific noise schemes and task tokens, enabling a single model to support multiple tasks, including text-to-audio-video, audio-to-video, and video-to-audio generation. Furthermore, by leveraging large language models and a large-scale text-audio-video combined dataset, UniForm achieves greater generative diversity than prior approaches. Extensive experiments show that UniForm achieves the state-of-the-art performance across audio-video generation tasks, producing content that is both well-aligned and close to real-world data distributions. Our demos are available at <a target="_blank" rel="noopener" href="https://uniform-t2av.github.io/">https://uniform-t2av.github.io/</a>. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹çš„å…´èµ·ï¼ŒéŸ³è§†é¢‘ç”Ÿæˆé¢†åŸŸå‘ç”Ÿäº†é©å‘½æ€§çš„å˜åŒ–ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºé’ˆå¯¹æ¯ç§æ¨¡æ€çš„ç‹¬ç«‹æ¨¡å—ï¼Œå¯¹ç»Ÿä¸€ç”Ÿæˆæ¶æ„çš„æ¢ç´¢æœ‰é™ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ–¹æ³•ä»…é™äºå•ä¸€ä»»åŠ¡å’Œå°è§„æ¨¡æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºUniFormï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡æ‰©æ•£å˜å‹å™¨ï¼Œå®ƒåœ¨ä¸€ä¸ªå…±äº«æ½œåœ¨ç©ºé—´ä¸­è”åˆç”ŸæˆéŸ³é¢‘å’Œè§†è§‰æ¨¡æ€ã€‚ä¸€ä¸ªå•ä¸€çš„æ‰©æ•£è¿‡ç¨‹åŒæ—¶å»ºæ¨¡éŸ³é¢‘å’Œè§†é¢‘ï¼Œæ•æ‰å£°éŸ³å’Œè§†è§‰ä¹‹é—´çš„å†…åœ¨å…³è”ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç‰¹å®šä»»åŠ¡çš„å™ªå£°æ–¹æ¡ˆå’Œä»»åŠ¡ä»¤ç‰Œï¼Œä½¿å•ä¸€æ¨¡å‹èƒ½å¤Ÿæ”¯æŒå¤šä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°éŸ³é¢‘è§†é¢‘ã€éŸ³é¢‘åˆ°è§†é¢‘å’Œè§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§è§„æ¨¡æ–‡æœ¬-éŸ³é¢‘-è§†é¢‘ç»„åˆæ•°æ®é›†ï¼ŒUniFormå®ç°äº†æ¯”å…ˆå‰æ–¹æ³•æ›´å¤§çš„ç”Ÿæˆå¤šæ ·æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniFormåœ¨éŸ³é¢‘è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”Ÿæˆçš„å†…å®¹æ—¢ç¬¦åˆå®é™…æ•°æ®åˆ†å¸ƒåˆæ¥è¿‘çœŸå®ä¸–ç•Œã€‚æˆ‘ä»¬çš„æ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://uniform-t2av.github.io/">https://uniform-t2av.github.io/</a>ä¸ŠæŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03897v4">PDF</a> Our demos are available at <a target="_blank" rel="noopener" href="https://uniform-t2av.github.io/">https://uniform-t2av.github.io/</a></p>
<p><strong>Summary</strong><br>     éšç€æ‰©æ•£æ¨¡å‹çš„å‘å±•ï¼ŒéŸ³è§†é¢‘ç”Ÿæˆé¢†åŸŸç»å†äº†é©æ–°ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–å•ç‹¬æ¨¡å—å¤„ç†ä¸åŒæ¨¡æ€ï¼Œå¯¹ç»Ÿä¸€ç”Ÿæˆæ¶æ„çš„æ¢ç´¢æœ‰é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºUniFormï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡æ‰©æ•£å˜å‹å™¨ï¼Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è”åˆç”ŸæˆéŸ³é¢‘å’Œè§†é¢‘æ¨¡æ€ã€‚å•ä¸€æ‰©æ•£è¿‡ç¨‹åŒæ—¶å»ºæ¨¡éŸ³é¢‘å’Œè§†é¢‘ï¼Œæ•æ‰å£°éŸ³ä¸è§†è§‰ä¹‹é—´çš„å†…åœ¨å…³è”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥ä»»åŠ¡ç‰¹å®šå™ªå£°æ–¹æ¡ˆå’Œä»»åŠ¡ä»¤ç‰Œï¼Œä½¿å•ä¸€æ¨¡å‹æ”¯æŒå¤šä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°éŸ³è§†é¢‘ã€éŸ³é¢‘åˆ°è§†é¢‘å’Œè§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆã€‚é€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§è§„æ¨¡æ–‡æœ¬-éŸ³é¢‘-è§†é¢‘ç»„åˆæ•°æ®é›†ï¼ŒUniFormè¾ƒå…ˆå‰æ–¹æ³•å®ç°æ›´å¤§çš„ç”Ÿæˆå¤šæ ·æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒUniFormåœ¨éŸ³é¢‘è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œç”Ÿæˆçš„å†…å®¹ä¸ç°å®ä¸–ç•Œæ•°æ®åˆ†å¸ƒç›¸ç¬¦ã€‚æ¼”ç¤ºç½‘ç«™ä¸ºï¼š<a target="_blank" rel="noopener" href="https://uniform-t2av.github.io/">https://uniform-t2av.github.io/ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¸¦åŠ¨äº†éŸ³è§†é¢‘ç”Ÿæˆçš„é©æ–°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šé‡‡ç”¨ç‹¬ç«‹æ¨¡å—å¤„ç†ä¸åŒæ¨¡æ€ï¼Œç¼ºä¹ç»Ÿä¸€ç”Ÿæˆæ¶æ„ã€‚</li>
<li>UniFormæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šä»»åŠ¡æ‰©æ•£å˜å‹å™¨ï¼Œèƒ½åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è”åˆç”ŸæˆéŸ³é¢‘å’Œè§†é¢‘ã€‚</li>
<li>UniFormé€šè¿‡å•ä¸€æ‰©æ•£è¿‡ç¨‹åŒæ—¶å»ºæ¨¡éŸ³é¢‘å’Œè§†é¢‘ï¼Œæ•æ‰ä¸¤è€…é—´çš„å†…åœ¨å…³è”ã€‚</li>
<li>å¼•å…¥ä»»åŠ¡ç‰¹å®šå™ªå£°æ–¹æ¡ˆå’Œä»»åŠ¡ä»¤ç‰Œï¼Œæ”¯æŒå¤šç§ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒUniFormå®ç°æ›´é«˜çš„ç”Ÿæˆå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-84e1f5ce04965763b98f5f1b56c07913.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-becaa2ae47ce0be076cee13aa7869a87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8a80c8c0cedce3aaf980ff29458e033.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00ddf9d4335d19dbd77a8a96b70e6854.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed9bdfc8e238838bcccba5b040050b3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4868a37d33b7ccc69f5aac4701315cd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Multimodal-LLMs-Can-Reason-about-Aesthetics-in-Zero-Shot"><a href="#Multimodal-LLMs-Can-Reason-about-Aesthetics-in-Zero-Shot" class="headerlink" title="Multimodal LLMs Can Reason about Aesthetics in Zero-Shot"></a>Multimodal LLMs Can Reason about Aesthetics in Zero-Shot</h2><p><strong>Authors:Ruixiang Jiang, Changwen Chen</strong></p>
<p>The rapid progress of generative art has democratized the creation of visually pleasing imagery. However, achieving genuine artistic impact - the kind that resonates with viewers on a deeper, more meaningful level - requires a sophisticated aesthetic sensibility. This sensibility involves a multi-faceted reasoning process extending beyond mere visual appeal, which is often overlooked by current computational models. This paper pioneers an approach to capture this complex process by investigating how the reasoning capabilities of Multimodal LLMs (MLLMs) can be effectively elicited for aesthetic judgment. Our analysis reveals a critical challenge: MLLMs exhibit a tendency towards hallucinations during aesthetic reasoning, characterized by subjective opinions and unsubstantiated artistic interpretations. We further demonstrate that these limitations can be overcome by employing an evidence-based, objective reasoning process, as substantiated by our proposed baseline, ArtCoT. MLLMs prompted by this principle produce multi-faceted and in-depth aesthetic reasoning that aligns significantly better with human judgment. These findings have direct applications in areas such as AI art tutoring and as reward models for generative art. Ultimately, our work paves the way for AI systems that can truly understand, appreciate, and generate artworks that align with the sensible human aesthetic standard. </p>
<blockquote>
<p>ç”Ÿæˆè‰ºæœ¯çš„å¿«é€Ÿå‘å±•ä½¿å¾—åˆ›å»ºè§†è§‰æ„‰æ‚¦çš„å›¾åƒå˜å¾—æ°‘ä¸»åŒ–ã€‚ç„¶è€Œï¼Œè¦å®ç°çœŸæ­£çš„è‰ºæœ¯å½±å“åŠ›â€”â€”é‚£ç§ä¸è§‚ä¼—åœ¨æ›´æ·±å±‚æ¬¡ã€æ›´æœ‰æ„ä¹‰å±‚é¢ä¸Šäº§ç”Ÿå…±é¸£çš„å½±å“åŠ›ï¼Œéœ€è¦æ•é”çš„å®¡ç¾æ„ŸçŸ¥ã€‚è¿™ç§æ„ŸçŸ¥æ¶‰åŠä¸€ä¸ªè¶…è¶Šå•çº¯è§†è§‰å¸å¼•åŠ›çš„å¤šå…ƒæ¨ç†è¿‡ç¨‹ï¼Œè¿™é€šå¸¸è¢«å½“å‰çš„è®¡ç®—æ¨¡å‹æ‰€å¿½è§†ã€‚æœ¬æ–‡å¼€åˆ›äº†ä¸€ç§æ–¹æ³•æ¥æ•æ‰è¿™ä¸€å¤æ‚è¿‡ç¨‹ï¼Œé€šè¿‡è°ƒæŸ¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›å¦‚ä½•æœ‰æ•ˆåœ°ç”¨äºç¾å­¦åˆ¤æ–­ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šMLLMsåœ¨ç¾å­¦æ¨ç†æ—¶å€¾å‘äºå‡ºç°å¹»è§‰ï¼Œè¡¨ç°ä¸ºä¸»è§‚æ„è§å’Œæ— æ ¹æ®çš„è‰ºæœ¯è§£è¯»ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œé€šè¿‡é‡‡ç”¨åŸºäºè¯æ®çš„å®¢è§‚æ¨ç†è¿‡ç¨‹ï¼Œå¯ä»¥å…‹æœè¿™äº›å±€é™æ€§ï¼Œæ­£å¦‚æˆ‘ä»¬æå‡ºçš„åŸºçº¿æ¨¡å‹ArtCoTæ‰€è¯å®çš„é‚£æ ·ã€‚éµå¾ªè¿™ä¸€åŸåˆ™çš„MLLMsäº§ç”Ÿäº†å¤šå…ƒä¸”æ·±å…¥çš„å®¡ç¾æ¨ç†ï¼Œä¸äººç±»åˆ¤æ–­æ›´åŠ å»åˆã€‚è¿™äº›å‘ç°å¯ç›´æ¥åº”ç”¨äºäººå·¥æ™ºèƒ½è‰ºæœ¯è¾…å¯¼å’Œç”Ÿæˆè‰ºæœ¯çš„å¥–åŠ±æ¨¡å‹ç­‰é¢†åŸŸã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºäººæœºç³»ç»ŸçœŸæ­£èƒ½å¤Ÿç†è§£ã€æ¬£èµå’Œç”Ÿæˆç¬¦åˆäººç±»å®¡ç¾æ ‡å‡†çš„è‰ºæœ¯ä½œå“é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.09012v2">PDF</a> WIP, Homepage <a target="_blank" rel="noopener" href="https://github.com/songrise/MLLM4Art">https://github.com/songrise/MLLM4Art</a></p>
<p><strong>Summary</strong><br>     ç”Ÿæˆè‰ºæœ¯çš„å¿«é€Ÿå‘å±•ä½¿å¾—åˆ›é€ è§†è§‰æ„‰æ‚¦çš„å›¾åƒå¾—ä»¥æ™®åŠã€‚ç„¶è€Œï¼Œå®ç°çœŸæ­£çš„è‰ºæœ¯å½±å“åŠ›ï¼Œå³é‚£ç§ä¸è§‚ä¼—æ›´æ·±å±‚æ¬¡ã€æ›´æœ‰æ„ä¹‰åœ°äº§ç”Ÿå…±é¸£çš„å½±å“åŠ›ï¼Œéœ€è¦ä¸€ç§å¤æ‚çš„ç¾å­¦æ„ŸçŸ¥ã€‚è¿™ç§æ„ŸçŸ¥æ¶‰åŠå¤šæ–¹é¢çš„æ¨ç†è¿‡ç¨‹ï¼Œè¶…è¶Šå•çº¯çš„è§†è§‰å¸å¼•åŠ›ï¼Œè€Œè¿™ä¸€ç‚¹å¾€å¾€è¢«å½“å‰çš„è®¡ç®—æ¨¡å‹æ‰€å¿½è§†ã€‚æœ¬æ–‡ç‡å…ˆæ¢ç´¢äº†ä¸€ç§æ–¹æ³•æ¥æ•æ‰è¿™ä¸€å¤æ‚è¿‡ç¨‹ï¼Œé€šè¿‡è°ƒæŸ¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›å¦‚ä½•æœ‰æ•ˆåœ°ç”¨äºç¾å­¦åˆ¤æ–­ã€‚åˆ†ææ˜¾ç¤ºäº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šMLLMsåœ¨ç¾å­¦æ¨ç†æ—¶å€¾å‘äºå‡ºç°å¹»è§‰ï¼Œè¡¨ç°ä¸ºä¸»è§‚æ„è§å’Œæ— æ ¹æ®çš„è‰ºæœ¯è§£è¯»ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œé€šè¿‡é‡‡ç”¨åŸºäºè¯æ®çš„å®¢è§‚æ¨ç†è¿‡ç¨‹å¯ä»¥å…‹æœè¿™äº›é™åˆ¶ï¼Œå¦‚æˆ‘ä»¬æ‰€æå‡ºçš„åŸºçº¿ArtCoTæ‰€ç¤ºã€‚éµå¾ªè¿™ä¸€åŸåˆ™çš„MLLMsäº§ç”Ÿäº†å¤šå±‚æ¬¡ã€æ·±å…¥çš„ç¾å­¦æ¨ç†ï¼Œä¸äººç±»åˆ¤æ–­æ›´åŠ å¥‘åˆã€‚è¿™äº›å‘ç°å¯ç›´æ¥åº”ç”¨äºäººå·¥æ™ºèƒ½è‰ºæœ¯è¾…å¯¼å’Œå¥–åŠ±æ¨¡å‹ç­‰é¢†åŸŸã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºèƒ½å¤ŸçœŸæ­£ç†è§£ã€æ¬£èµå’Œç”Ÿæˆç¬¦åˆäººç±»å®¡ç¾æ ‡å‡†çš„è‰ºæœ¯ä½œå“çš„AIç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆè‰ºæœ¯çš„è¿›æ­¥ä½¿å¾—åˆ›é€ è§†è§‰æ„‰æ‚¦å›¾åƒå˜å¾—å®¹æ˜“ï¼Œä½†å®ç°çœŸæ­£çš„è‰ºæœ¯å½±å“åŠ›éœ€è¦å¤æ‚çš„ç¾å­¦æ„ŸçŸ¥ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç¾å­¦åˆ¤æ–­ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†å­˜åœ¨å€¾å‘äºå¹»è§‰çš„é—®é¢˜ã€‚</li>
<li>MLLMsåœ¨ç¾å­¦æ¨ç†æ—¶è¡¨ç°ä¸ºä¸»è§‚æ„è§å’Œæ— æ ¹æ®çš„è‰ºæœ¯è§£è¯»ï¼Œéœ€è¦å…‹æœã€‚</li>
<li>é‡‡ç”¨åŸºäºè¯æ®çš„å®¢è§‚æ¨ç†è¿‡ç¨‹å¯ä»¥æ”¹è¿›MLLMsçš„ç¾å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>éµå¾ªè¿™ä¸€åŸåˆ™çš„MLLMsäº§ç”Ÿçš„ç¾å­¦æ¨ç†ä¸äººç±»åˆ¤æ–­æ›´åŠ å¥‘åˆã€‚</li>
<li>è¯¥ç ”ç©¶çš„åº”ç”¨é¢†åŸŸåŒ…æ‹¬äººå·¥æ™ºèƒ½è‰ºæœ¯è¾…å¯¼å’Œå¥–åŠ±æ¨¡å‹ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.09012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-efe26e0f6037342dd0da7e42db87b35f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7479ee8ca56672de6fca439f00f17799.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c9c51b88a4e27f50a2eeea89283d20b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36466d6623908a54b07c52fef63db9e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a0459e8a6b7257143c430b52d4a78f8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e93a31c13d60971a8578ec9d445a62ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-799288b3b5e7d9e856b02ccc1098c021.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c46bbc14136cf893fc68ddf07ddf97b2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Online-Video-Understanding-OVBench-and-VideoChat-Online"><a href="#Online-Video-Understanding-OVBench-and-VideoChat-Online" class="headerlink" title="Online Video Understanding: OVBench and VideoChat-Online"></a>Online Video Understanding: OVBench and VideoChat-Online</h2><p><strong>Authors:Zhenpeng Huang, Xinhao Li, Jiaqi Li, Jing Wang, Xiangyu Zeng, Cheng Liang, Tao Wu, Xi Chen, Liang Li, Limin Wang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have significantly progressed in offline video understanding. However, applying these models to real-world scenarios, such as autonomous driving and human-computer interaction, presents unique challenges due to the need for real-time processing of continuous online video streams. To this end, this paper presents systematic efforts from three perspectives: evaluation benchmark, model architecture, and training strategy. First, we introduce OVBench, a comprehensive question-answering benchmark designed to evaluate modelsâ€™ ability to perceive, memorize, and reason within online video contexts. It features 6 core task types across three temporal contexts-past, current, and future-forming 16 subtasks from diverse datasets. Second, we propose a new Pyramid Memory Bank (PMB) that effectively retains key spatiotemporal information in video streams. Third, we proposed an offline-to-online learning paradigm, designing an interleaved dialogue format for online video data and constructing an instruction-tuning dataset tailored for online video training. This framework led to the development of VideoChat-Online, a robust and efficient model for online video understanding. Despite the lower computational cost and higher efficiency, VideoChat-Online outperforms existing state-of-the-art offline and online models across popular offline video benchmarks and OVBench, demonstrating the effectiveness of our model architecture and training strategy. % Our approach surpasses existing state-of-the-art offline models Qwen2-VL 7B and online models Flash-VStream, by 4.19% and 23.7% on OVBench, respectively. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç¦»çº¿è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹åº”ç”¨äºè‡ªåŠ¨é©¾é©¶å’Œäººæœºäº¤äº’ç­‰ç°å®åœºæ™¯ï¼Œç”±äºå¯¹è¿ç»­åœ¨çº¿è§†é¢‘æµçš„å®æ—¶å¤„ç†éœ€æ±‚ï¼Œå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ä»è¯„ä¼°åŸºå‡†ã€æ¨¡å‹æ¶æ„å’ŒåŸ¹è®­ç­–ç•¥ä¸‰ä¸ªæ–¹é¢è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†OVBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨çº¿è§†é¢‘è¯­å¢ƒä¸­çš„æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†èƒ½åŠ›çš„ç»¼åˆé—®ç­”åŸºå‡†æµ‹è¯•ã€‚å®ƒæ¶µç›–äº†è¿‡å»ã€ç°åœ¨å’Œæœªæ¥çš„ä¸‰ä¸ªæ—¶é—´èƒŒæ™¯ï¼ŒåŒ…å«16ä¸ªå­ä»»åŠ¡ï¼Œæ¶‰åŠå…­ä¸ªæ ¸å¿ƒä»»åŠ¡ç±»å‹ï¼Œè·¨è¶Šä¸åŒçš„æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„Pyramid Memory Bankï¼ˆPMBï¼‰ï¼Œå®ƒæœ‰æ•ˆåœ°ä¿ç•™äº†è§†é¢‘æµä¸­çš„å…³é”®æ—¶ç©ºä¿¡æ¯ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä»ç¦»çº¿åˆ°åœ¨çº¿çš„å­¦ä¹ èŒƒå¼ï¼Œä¸ºåœ¨çº¿è§†é¢‘æ•°æ®è®¾è®¡äº†ä¸€ç§äº¤æ›¿å¯¹è¯æ ¼å¼ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªé€‚ç”¨äºåœ¨çº¿è§†é¢‘è®­ç»ƒçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚è¿™ä¸€æ¡†æ¶å‚¬ç”Ÿäº†VideoChat-Onlineæ¨¡å‹ï¼Œä¸€ä¸ªç”¨äºåœ¨çº¿è§†é¢‘ç†è§£çš„ç¨³å¥é«˜æ•ˆæ¨¡å‹ã€‚VideoChat-Onlineåœ¨æµè¡Œçš„ç¦»çº¿è§†é¢‘åŸºå‡†æµ‹è¯•å’ŒOVBenchä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„æœ€æ–°ç¦»çº¿æ¨¡å‹å’Œåœ¨çº¿æ¨¡å‹ï¼Œè™½ç„¶å…¶è®¡ç®—æˆæœ¬æ›´ä½ã€æ•ˆç‡æ›´é«˜ï¼Œä½†å±•ç¤ºäº†æˆ‘ä»¬æ¨¡å‹æ¶æ„å’ŒåŸ¹è®­ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„æœ€æ–°ç¦»çº¿æ¨¡å‹Qwen2-VL 7Bå’Œåœ¨çº¿æ¨¡å‹Flash-VStreamï¼Œåœ¨OVBenchä¸Šçš„æ€§èƒ½åˆ†åˆ«æé«˜äº†4.19%å’Œ23.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00584v2">PDF</a> CVPR 2025 Camera Ready Version. Project Page:   <a target="_blank" rel="noopener" href="https://videochat-online.github.io/">https://videochat-online.github.io</a></p>
<p><strong>Summary</strong></p>
<p>åœ¨çº¿è§†é¢‘ç†è§£é¢†åŸŸï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—ã€‚ç„¶è€Œï¼Œå°†å…¶åº”ç”¨äºè‡ªåŠ¨é©¾é©¶å’Œäººæœºäº¤äº’ç­‰ç°å®åœºæ™¯æ—¶ï¼Œç”±äºéœ€è¦å®æ—¶å¤„ç†è¿ç»­çš„åœ¨çº¿è§†é¢‘æµï¼Œé¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°ä»‹ç»äº†ä¸‰ä¸ªæ–¹é¢çš„åŠªåŠ›ï¼šè¯„ä¼°åŸºå‡†ã€æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥ã€‚é¦–å…ˆæ¨å‡ºOVBenchè¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨çº¿è§†é¢‘è¯­å¢ƒä¸­çš„æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæå‡ºæ–°çš„Pyramid Memory Bankï¼ˆPMBï¼‰ï¼Œæœ‰æ•ˆä¿ç•™è§†é¢‘æµä¸­çš„å…³é”®æ—¶ç©ºä¿¡æ¯ã€‚æœ€åï¼Œå»ºç«‹äº†ä¸€ç§ç¦»çº¿åˆ°åœ¨çº¿çš„å­¦ä¹ èŒƒå¼ï¼Œä¸ºåœ¨çº¿è§†é¢‘æ•°æ®è®¾è®¡äº†ä¸€ç§äº¤æ›¿å¯¹è¯æ ¼å¼ï¼Œå¹¶æ„å»ºäº†é’ˆå¯¹åœ¨çº¿è§†é¢‘è®­ç»ƒçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚è¿™äº›æˆæœä¿ƒä½¿VideoChat-Onlineæ¨¡å‹çš„å‘å±•ï¼Œè¯¥æ¨¡å‹åœ¨åœ¨çº¿è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºç¨³å¥é«˜æ•ˆçš„ç‰¹ç‚¹ï¼Œä¸”åœ¨æµè¡Œçš„ç¦»çº¿è§†é¢‘åŸºå‡†æµ‹è¯•å’ŒOVBenchä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„ç¦»çº¿æ¨¡å‹å’Œåœ¨çº¿æ¨¡å‹ï¼Œè¯æ˜äº†æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨åœ¨çº¿è§†é¢‘ç†è§£é¢†åŸŸå–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†åº”ç”¨äºç°å®åœºæ™¯å¦‚è‡ªä¸»é©¾é©¶å’Œäººæœºäº¤äº’æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥OVBenchè¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨çº¿è§†é¢‘è¯­å¢ƒä¸­çš„æ„ŸçŸ¥ã€è®°å¿†å’Œæ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«16ä¸ªå­ä»»åŠ¡ã€‚</li>
<li>æå‡ºPyramid Memory Bankï¼ˆPMBï¼‰ï¼Œæœ‰æ•ˆä¿ç•™è§†é¢‘æµä¸­çš„å…³é”®æ—¶ç©ºä¿¡æ¯ã€‚</li>
<li>å»ºç«‹äº†ä¸€ç§ç¦»çº¿åˆ°åœ¨çº¿çš„å­¦ä¹ èŒƒå¼ï¼Œä¸ºåœ¨çº¿è§†é¢‘æ•°æ®è®¾è®¡äº¤æ›¿å¯¹è¯æ ¼å¼ï¼Œå¹¶æ„å»ºæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚</li>
<li>VideoChat-Onlineæ¨¡å‹åœ¨åœ¨çº¿è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºç¨³å¥é«˜æ•ˆçš„ç‰¹ç‚¹ã€‚</li>
<li>VideoChat-Onlineåœ¨æµè¡Œçš„ç¦»çº¿è§†é¢‘åŸºå‡†æµ‹è¯•å’ŒOVBenchä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„ç¦»çº¿æ¨¡å‹å’Œåœ¨çº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ef32fd18c0ebe2910e639f28233ecefb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cacd6415abededa1b0c4a6547b751e40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab18516ff73e9af95bdfe3507871fb7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea4907373b99df97816b9ee4c8cd71d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0d12de939c51784137957c54087888f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Tokenphormer-Structure-aware-Multi-token-Graph-Transformer-for-Node-Classification"><a href="#Tokenphormer-Structure-aware-Multi-token-Graph-Transformer-for-Node-Classification" class="headerlink" title="Tokenphormer: Structure-aware Multi-token Graph Transformer for Node   Classification"></a>Tokenphormer: Structure-aware Multi-token Graph Transformer for Node   Classification</h2><p><strong>Authors:Zijie Zhou, Zhaoqi Lu, Xuekai Wei, Rongqin Chen, Shenghui Zhang, Pak Lon Ip, Leong Hou U</strong></p>
<p>Graph Neural Networks (GNNs) are widely used in graph data mining tasks. Traditional GNNs follow a message passing scheme that can effectively utilize local and structural information. However, the phenomena of over-smoothing and over-squashing limit the receptive field in message passing processes. Graph Transformers were introduced to address these issues, achieving a global receptive field but suffering from the noise of irrelevant nodes and loss of structural information. Therefore, drawing inspiration from fine-grained token-based representation learning in Natural Language Processing (NLP), we propose the Structure-aware Multi-token Graph Transformer (Tokenphormer), which generates multiple tokens to effectively capture local and structural information and explore global information at different levels of granularity. Specifically, we first introduce the walk-token generated by mixed walks consisting of four walk types to explore the graph and capture structure and contextual information flexibly. To ensure local and global information coverage, we also introduce the SGPM-token (obtained through the Self-supervised Graph Pre-train Model, SGPM) and the hop-token, extending the length and density limit of the walk-token, respectively. Finally, these expressive tokens are fed into the Transformer model to learn node representations collaboratively. Experimental results demonstrate that the capability of the proposed Tokenphormer can achieve state-of-the-art performance on node classification tasks. </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰è¢«å¹¿æ³›åº”ç”¨äºå›¾æ•°æ®æŒ–æ˜ä»»åŠ¡ä¸­ã€‚ä¼ ç»Ÿçš„GNNséµå¾ªæ¶ˆæ¯ä¼ é€’æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å±€éƒ¨å’Œç»“æ„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦æŒ¤å‹çš„ç°è±¡é™åˆ¶äº†æ¶ˆæ¯ä¼ é€’è¿‡ç¨‹ä¸­çš„æ„Ÿå—é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†å›¾è½¬æ¢å™¨ï¼ˆGraph Transformersï¼‰ï¼Œå®ç°å…¨å±€æ„Ÿå—é‡ï¼Œä½†ä¼šé¢ä¸´æ— å…³èŠ‚ç‚¹çš„å™ªå£°å’Œç»“æ„ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ç²¾ç»†ç²’åº¦åŸºäºä»¤ç‰Œè¡¨ç¤ºå­¦ä¹ ä¸­è·å¾—çµæ„Ÿï¼Œæå‡ºäº†ç»“æ„æ„ŸçŸ¥å¤šä»¤ç‰Œå›¾è½¬æ¢å™¨ï¼ˆTokenphormerï¼‰ã€‚å®ƒé€šè¿‡ç”Ÿæˆå¤šä¸ªä»¤ç‰Œæ¥æœ‰æ•ˆæ•è·å±€éƒ¨å’Œç»“æ„ä¿¡æ¯ï¼Œå¹¶åœ¨ä¸åŒç²’åº¦çº§åˆ«ä¸Šæ¢ç´¢å…¨å±€ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥ç”±å››ç§æ­¥è¡Œç±»å‹ç»„æˆçš„æ··åˆæ­¥è¡Œç”Ÿæˆçš„walk-tokenæ¥æ¢ç´¢å›¾å½¢å¹¶çµæ´»åœ°æ•è·ç»“æ„å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºäº†ç¡®ä¿æœ¬åœ°å’Œå…¨å±€ä¿¡æ¯è¦†ç›–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†é€šè¿‡è‡ªç›‘ç£å›¾é¢„è®­ç»ƒæ¨¡å‹ï¼ˆSGPMï¼‰è·å¾—çš„SGPM-tokenå’Œæ‰©å±•walk-tokené•¿åº¦å’Œå¯†åº¦çš„hop-tokenã€‚æœ€åï¼Œè¿™äº›è¡¨è¾¾æ€§ä»¤ç‰Œè¢«è¾“å…¥åˆ°Transformeræ¨¡å‹ä¸­ï¼Œä»¥ååŒå­¦ä¹ èŠ‚ç‚¹è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„Tokenphormerçš„èƒ½åŠ›åœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15302v2">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å¹¿æ³›åº”ç”¨äºå›¾æ•°æ®æŒ–æ˜ä»»åŠ¡ï¼Œä½†å­˜åœ¨è¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦å‹ç¼©ç°è±¡ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†å›¾Transformerï¼Œä½†å¯èƒ½å—åˆ°æ— å…³èŠ‚ç‚¹å™ªå£°å’Œç»“æ„ä¿¡æ¯æŸå¤±çš„å½±å“ã€‚å—è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ç²¾ç»†ç²’åº¦ä»¤ç‰Œè¡¨ç¤ºå­¦ä¹ çš„å¯å‘ï¼Œæå‡ºç»“æ„æ„ŸçŸ¥å¤šä»¤ç‰Œå›¾Transformerï¼ˆTokenphormerï¼‰ï¼Œç”Ÿæˆå¤šä¸ªä»¤ç‰Œä»¥æœ‰æ•ˆæ•è·å±€éƒ¨å’Œç»“æ„ä¿¡æ¯ï¼Œå¹¶åœ¨ä¸åŒç²’åº¦çº§åˆ«ä¸Šæ¢ç´¢å…¨å±€ä¿¡æ¯ã€‚é€šè¿‡æ··åˆæ­¥è¡Œç”Ÿæˆçš„walk-tokenã€é€šè¿‡è‡ªç›‘ç£å›¾é¢„è®­ç»ƒæ¨¡å‹ï¼ˆSGPMï¼‰è·å¾—çš„SGPM-tokenä»¥åŠæ‰©å±•walk-tokené•¿åº¦å’Œå¯†åº¦çš„hop-tokenï¼Œå°†è¿™äº›è¡¨è¾¾æ€§ä»¤ç‰Œé¦ˆå…¥Transformeræ¨¡å‹ï¼Œä»¥ååŒå­¦ä¹ èŠ‚ç‚¹è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTokenphormeråœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GNNsåœ¨å›¾æ•°æ®æŒ–æ˜ä»»åŠ¡ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨è¿‡åº¦å¹³æ»‘å’Œè¿‡åº¦å‹ç¼©é—®é¢˜ï¼Œå½±å“æ¶ˆæ¯ä¼ é€’è¿‡ç¨‹çš„æ¥æ”¶èŒƒå›´ã€‚</li>
<li>å›¾Transformerè¢«å¼•å…¥ä»¥è§£å†³GNNsçš„é—®é¢˜ï¼Œå®ç°å…¨å±€æ¥æ”¶èŒƒå›´ï¼Œä½†å¯èƒ½å—åˆ°æ— å…³èŠ‚ç‚¹å™ªå£°å’Œç»“æ„ä¿¡æ¯æŸå¤±çš„å½±å“ã€‚</li>
<li>Tokenphormerå—NLPä¸­ç²¾ç»†ç²’åº¦ä»¤ç‰Œè¡¨ç¤ºå­¦ä¹ çš„å¯å‘ï¼Œç”Ÿæˆå¤šä¸ªä»¤ç‰Œä»¥æ•è·å±€éƒ¨å’Œç»“æ„åŒ–ä¿¡æ¯ï¼Œå¹¶æ¢ç´¢ä¸åŒç²’åº¦çº§åˆ«çš„å…¨å±€ä¿¡æ¯ã€‚</li>
<li>Tokenphormeré€šè¿‡æ··åˆæ­¥è¡Œã€SGPMï¼ˆè‡ªç›‘ç£å›¾é¢„è®­ç»ƒæ¨¡å‹ï¼‰å’Œæ‰©å±•ä»¤ç‰Œé•¿åº¦ä¸å¯†åº¦çš„ç­–ç•¥ç”Ÿæˆä»¤ç‰Œã€‚</li>
<li>Tokenphormerå°†è¡¨è¾¾æ€§ä»¤ç‰Œè¾“å…¥åˆ°Transformeræ¨¡å‹ä¸­ï¼Œä»¥ååŒå­¦ä¹ èŠ‚ç‚¹è¡¨ç¤ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒTokenphormeråœ¨èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9b7e6667f3e7f6043230a4af16086e06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f262bfc241545c20e6d379eeff71ada.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a1e483bfb49846d9bbc4466cdff3cec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-814b4c688f16ca16d0676cdffda7ce64.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Efficient-Federated-Finetuning-of-Tiny-Transformers-with-Resource-Constrained-Devices"><a href="#Efficient-Federated-Finetuning-of-Tiny-Transformers-with-Resource-Constrained-Devices" class="headerlink" title="Efficient Federated Finetuning of Tiny Transformers with   Resource-Constrained Devices"></a>Efficient Federated Finetuning of Tiny Transformers with   Resource-Constrained Devices</h2><p><strong>Authors:Kilian Pfeiffer, Mohamed Aboelenien Ahmed, Ramin Khalili, JÃ¶rg Henkel</strong></p>
<p>In recent years, Large Language Models (LLMs) through Transformer structures have dominated many machine learning tasks, especially text processing. However, these models require massive amounts of data for training and induce high resource requirements, particularly in terms of the large number of Floating Point Operations (FLOPs) and the high amounts of memory needed. To fine-tune such a model in a parameter-efficient way, techniques like Adapter or LoRA have been developed. However, we observe that the application of LoRA, when used in federated learning (FL), while still being parameter-efficient, is memory and FLOP inefficient. Based on that observation, we develop a novel layer finetuning scheme that allows devices in cross-device FL to make use of pretrained neural networks (NNs) while adhering to given resource constraints. We show that our presented scheme outperforms the current state of the art when dealing with homogeneous or heterogeneous computation and memory constraints and is on par with LoRA regarding limited communication, thereby achieving significantly higher accuracies in FL training. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œé€šè¿‡Transformerç»“æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬å¤„ç†æ–¹é¢ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶äº§ç”Ÿäº†å·¨å¤§çš„èµ„æºéœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰æ•°é‡å’Œæ‰€éœ€å†…å­˜æ–¹é¢ã€‚ä¸ºäº†ä»¥å‚æ•°é«˜æ•ˆçš„æ–¹å¼å¾®è°ƒæ­¤ç±»æ¨¡å‹ï¼Œå¼€å‘äº†Adapteræˆ–LoRAç­‰æŠ€æœ¯ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å½“LoRAç”¨äºè”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ—¶ï¼Œè™½ç„¶å®ƒä»ç„¶æ˜¯å‚æ•°é«˜æ•ˆçš„ï¼Œä½†åœ¨å†…å­˜å’ŒFLOPæ–¹é¢å¹¶ä¸é«˜æ•ˆã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹å±‚å¾®è°ƒæ–¹æ¡ˆï¼Œå…è®¸è·¨è®¾å¤‡è”é‚¦å­¦ä¹ ä¸­çš„è®¾å¤‡åˆ©ç”¨é¢„è®­ç»ƒçš„ç¥ç»ç½‘ç»œï¼ˆNNsï¼‰ï¼ŒåŒæ—¶æ»¡è¶³ç»™å®šçš„èµ„æºé™åˆ¶ã€‚æˆ‘ä»¬æ˜¾ç¤ºï¼Œåœ¨å¤„ç†åŒç±»æˆ–å¼‚ç±»è®¡ç®—å’Œå†…å­˜çº¦æŸæ—¶ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ¡ˆä¼˜äºå½“å‰çš„æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”åœ¨æœ‰é™çš„é€šä¿¡æ–¹é¢ä¸LoRAç›¸å½“ï¼Œä»è€Œåœ¨è”é‚¦å­¦ä¹ ä¸­å®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07826v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡Transformerç»“æ„åœ¨æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬å¤„ç†æ–¹é¢ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”å…·æœ‰é«˜çš„èµ„æºè¦æ±‚ï¼Œå¦‚æµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰å’Œå†…å­˜ã€‚ä¸ºäº†ä»¥å‚æ•°æœ‰æ•ˆçš„æ–¹å¼å¾®è°ƒè¿™äº›æ¨¡å‹ï¼Œå¼€å‘äº†Adapteræˆ–LoRAç­‰æŠ€æœ¯ã€‚ç„¶è€Œï¼Œåœ¨è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸­åº”ç”¨LoRAæ—¶ï¼Œè™½ç„¶ä»ç„¶æ˜¯å‚æ•°æœ‰æ•ˆçš„ï¼Œä½†åœ¨å†…å­˜å’ŒFLOPæ–¹é¢å¹¶ä¸é«˜æ•ˆã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹å±‚å¾®è°ƒæ–¹æ¡ˆï¼Œå…è®¸è·¨è®¾å¤‡FLä¸­çš„è®¾å¤‡åœ¨éµå®ˆç»™å®šèµ„æºçº¦æŸçš„åŒæ—¶åˆ©ç”¨é¢„è®­ç»ƒç¥ç»ç½‘ç»œï¼ˆNNsï¼‰ã€‚æˆ‘ä»¬å±•ç¤ºï¼Œä¸å½“å‰æŠ€æœ¯ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨å¤„ç†åŒè´¨æˆ–å¼‚è´¨è®¡ç®—å’Œå†…å­˜çº¦æŸæ–¹é¢è¡¨ç°æ›´å¥½ï¼Œå¹¶åœ¨æœ‰é™çš„é€šä¿¡æ–¹é¢ä¸LoRAä¸ç›¸ä¸Šä¸‹ï¼Œä»è€Œåœ¨è”é‚¦å­¦ä¹ è®­ç»ƒä¸­å®ç°æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²ä¸»å¯¼è®¸å¤šæœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œå°¤å…¶åœ¨æ–‡æœ¬å¤„ç†æ–¹é¢ã€‚</li>
<li>LLMè®­ç»ƒéœ€è¦å¤§é‡çš„æ•°æ®å’Œèµ„æºï¼Œå¦‚FLOPså’Œå†…å­˜ã€‚</li>
<li>LoRAæŠ€æœ¯åœ¨è”é‚¦å­¦ä¹ ä¸­è™½å‚æ•°æœ‰æ•ˆä½†å†…å­˜å’ŒFLOPæ•ˆç‡ä½ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°å‹å±‚å¾®è°ƒæ–¹æ¡ˆï¼Œå…è®¸è·¨è®¾å¤‡FLä¸­çš„è®¾å¤‡åˆ©ç”¨é¢„è®­ç»ƒç¥ç»ç½‘ç»œå¹¶éµå®ˆèµ„æºçº¦æŸã€‚</li>
<li>è¯¥æ–¹æ¡ˆåœ¨å¤„ç†åŒè´¨æˆ–å¼‚è´¨è®¡ç®—å’Œå†…å­˜çº¦æŸæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ¡ˆåœ¨æœ‰é™çš„é€šä¿¡æ–¹é¢ä¸LoRAè¡¨ç°ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3aa6ca05194364c02380f0f0e640e27b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f3e488d9ffc38da4b02c1f80e463598.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a935c38628809510786073a3aab8d0d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90f6c266358bc20553878b0ea19323fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b6303faccd767eb268af524c9f0175e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86e7132e8e60318d2a85e6e966b35552.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ViTOC-Vision-Transformer-and-Object-aware-Captioner"><a href="#ViTOC-Vision-Transformer-and-Object-aware-Captioner" class="headerlink" title="ViTOC: Vision Transformer and Object-aware Captioner"></a>ViTOC: Vision Transformer and Object-aware Captioner</h2><p><strong>Authors:Feiyang Huang</strong></p>
<p>This paper presents ViTOC (Vision Transformer and Object-aware Captioner), a novel vision-language model for image captioning that addresses the challenges of accuracy and diversity in generated descriptions. Unlike conventional approaches, ViTOC employs a dual-path architecture based on Vision Transformer and object detector, effectively fusing global visual features and local object information through learnable vectors. The model introduces an innovative object-aware prompting strategy that significantly enhances its capability in handling long-tail data. Experiments on the standard COCO dataset demonstrate that ViTOC outperforms baseline models across all evaluation metrics. Additionally, we propose a reference-free evaluation method based on CLIP to further validate the modelâ€™s effectiveness. By utilizing pretrained visual model parameters, ViTOC achieves efficient end-to-end training. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ViTOCï¼ˆè§†è§‰è½¬æ¢å™¨ä¸å¯¹è±¡æ„ŸçŸ¥æè¿°ç”Ÿæˆå™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå›¾åƒæè¿°çš„æ–°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè§£å†³äº†ç”Ÿæˆæè¿°ä¸­çš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§æŒ‘æˆ˜ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒViTOCé‡‡ç”¨åŸºäºè§†è§‰è½¬æ¢å™¨å’Œå¯¹è±¡æ£€æµ‹å™¨çš„åŒè·¯å¾„æ¶æ„ï¼Œé€šè¿‡å¯å­¦ä¹ å‘é‡æœ‰æ•ˆåœ°èåˆå…¨å±€è§†è§‰ç‰¹å¾å’Œå±€éƒ¨å¯¹è±¡ä¿¡æ¯ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„åŸºäºå¯¹è±¡çš„æç¤ºç­–ç•¥ï¼Œæå¤§åœ°æé«˜äº†å…¶å¤„ç†é•¿å°¾æ•°æ®çš„èƒ½åŠ›ã€‚åœ¨æ ‡å‡†COCOæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒViTOCåœ¨æ‰€æœ‰è¯„ä»·æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºCLIPçš„æ— å‚è€ƒè¯„ä»·æ–¹æ³•ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹å‚æ•°ï¼ŒViTOCå®ç°äº†é«˜æ•ˆç«¯åˆ°ç«¯è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07265v4">PDF</a> The core idea is too close to what has been published in other   journals</p>
<p><strong>Summary</strong></p>
<p>ViTOCæ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå›¾åƒæè¿°ï¼Œè§£å†³äº†ç”Ÿæˆæè¿°ä¸­çš„å‡†ç¡®æ€§å’Œå¤šæ ·æ€§æŒ‘æˆ˜ã€‚å®ƒé‡‡ç”¨åŸºäºVision Transformerå’Œå¯¹è±¡æ£€æµ‹å™¨çš„åŒè·¯å¾„æ¶æ„ï¼Œé€šè¿‡å¯å­¦ä¹ å‘é‡æœ‰æ•ˆèåˆå…¨å±€è§†è§‰ç‰¹å¾å’Œå±€éƒ¨å¯¹è±¡ä¿¡æ¯ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„å¯¹è±¡æ„ŸçŸ¥æç¤ºç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†å¤„ç†é•¿å°¾æ•°æ®çš„èƒ½åŠ›ã€‚åœ¨COCOæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒViTOCåœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§åŸºäºCLIPçš„æ— å‚è€ƒè¯„ä»·æ–¹æ³•ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ViTOCåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹å‚æ•°ï¼Œå®ç°äº†é«˜æ•ˆçš„ç«¯åˆ°ç«¯è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViTOCæ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºå›¾åƒæè¿°ä»»åŠ¡ã€‚</li>
<li>ViTOCé‡‡ç”¨åŒè·¯å¾„æ¶æ„ï¼Œç»“åˆVision Transformerå’Œå¯¹è±¡æ£€æµ‹å™¨ã€‚</li>
<li>ViTOCèåˆäº†å…¨å±€è§†è§‰ç‰¹å¾å’Œå±€éƒ¨å¯¹è±¡ä¿¡æ¯ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>ViTOCå¼•å…¥äº†å¯¹è±¡æ„ŸçŸ¥æç¤ºç­–ç•¥ï¼Œæœ‰æ•ˆå¤„ç†é•¿å°¾æ•°æ®ã€‚</li>
<li>åœ¨COCOæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒViTOCä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚</li>
<li>æå‡ºäº†åŸºäºCLIPçš„æ— å‚è€ƒè¯„ä»·æ–¹æ³•ï¼Œä»¥è¿›ä¸€æ­¥éªŒè¯æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ViTOCåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰æ¨¡å‹å‚æ•°ï¼Œå®ç°äº†é«˜æ•ˆçš„ç«¯åˆ°ç«¯è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.07265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ef1b054854e41687787802113c1cded.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8ea6ba099f8fa324a7f8a161915e82d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6f83bb462127039f345b16dfa68fa6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b7efa2ec823e914f2925871ba418d64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91bc1780b9655667b817eb0c97270f85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a38332e1420c35778edbe42673a45c81.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-19/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-19/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-19/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-910ee8930f9a3f2d540062a4db969108.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-19  Exploring Expert Failures Improves LLM Agent Tuning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-19/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-a5ff509c118576b1dfc2567ebf0bef75.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-19  SkyReels-V2 Infinite-length Film Generative Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
