<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-19  SkyReels-V2 Infinite-length Film Generative Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a5ff509c118576b1dfc2567ebf0bef75.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-19-æ›´æ–°"><a href="#2025-04-19-æ›´æ–°" class="headerlink" title="2025-04-19 æ›´æ–°"></a>2025-04-19 æ›´æ–°</h1><h2 id="SkyReels-V2-Infinite-length-Film-Generative-Model"><a href="#SkyReels-V2-Infinite-length-Film-Generative-Model" class="headerlink" title="SkyReels-V2: Infinite-length Film Generative Model"></a>SkyReels-V2: Infinite-length Film Generative Model</h2><p><strong>Authors:Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengchen Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, Yahui Zhou</strong></p>
<p>Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMsâ€™ inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation. To address these limitations, we propose SkyReels-V2, an Infinite-length Film Generative Model, that synergizes Multi-modal Large Language Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing Framework. Firstly, we design a comprehensive structural representation of video that combines the general descriptions by the Multi-modal LLM and the detailed shot language by sub-expert models. Aided with human annotation, we then train a unified Video Captioner, named SkyCaptioner-V1, to efficiently label the video data. Secondly, we establish progressive-resolution pretraining for the fundamental video generation, followed by a four-stage post-training enhancement: Initial concept-balanced Supervised Fine-Tuning (SFT) improves baseline quality; Motion-specific Reinforcement Learning (RL) training with human-annotated and synthetic distortion data addresses dynamic artifacts; Our diffusion forcing framework with non-decreasing noise schedules enables long-video synthesis in an efficient search space; Final high-quality SFT refines visual fidelity. All the code and models are available at <a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/SkyReels-V2">https://github.com/SkyworkAI/SkyReels-V2</a>. </p>
<blockquote>
<p>æœ€è¿‘è§†é¢‘ç”Ÿæˆçš„è¿›å±•å¾—ç›Šäºæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¡†æ¶çš„æ¨åŠ¨ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€äº›å…³é”®æŒ‘æˆ˜ï¼Œå¦‚å¦‚ä½•åœ¨æç¤ºéµå¾ªã€è§†è§‰è´¨é‡ã€è¿åŠ¨åŠ¨æ€å’ŒæŒç»­æ—¶é—´ä¹‹é—´å–å¾—å¹³è¡¡ï¼šä¸ºäº†åœ¨å¢å¼ºæ—¶é—´è§†è§‰è´¨é‡æ–¹é¢åšå‡ºå¦¥åè€Œç‰ºç‰²è¿åŠ¨åŠ¨æ€ï¼Œä¸ºä¼˜å…ˆè€ƒè™‘åˆ†è¾¨ç‡è€Œé™åˆ¶è§†é¢‘æ—¶é•¿ï¼ˆ5-10ç§’ï¼‰ï¼Œä»¥åŠç”±äºé€šç”¨MLLMæ— æ³•è§£é‡Šç”µå½±è¯­æ³•ï¼ˆå¦‚é•œå¤´æ„å›¾ã€æ¼”å‘˜è¡¨æƒ…å’Œç›¸æœºè¿åŠ¨ï¼‰å¯¼è‡´çš„é•œå¤´æ„ŸçŸ¥ç”Ÿæˆä¸è¶³ã€‚è¿™äº›äº¤ç»‡åœ¨ä¸€èµ·çš„å±€é™æ€§é˜»ç¢äº†çœŸå®çš„é•¿å½¢å¼åˆæˆå’Œä¸“ä¸šç”µå½±é£æ ¼çš„ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SkyReels-V2ï¼Œä¸€ç§æ— é™é•¿ç”µå½±ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒååŒäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€å¤šé˜¶æ®µé¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£å¼ºåˆ¶æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å…¨é¢çš„è§†é¢‘ç»“æ„è¡¨ç¤ºï¼Œå®ƒç»“åˆäº†å¤šæ¨¡æ€LLMçš„ä¸€èˆ¬æè¿°å’Œå­ä¸“å®¶æ¨¡å‹çš„è¯¦ç»†é•œå¤´è¯­è¨€ã€‚å€ŸåŠ©äººå·¥æ ‡æ³¨ï¼Œæˆ‘ä»¬éšåè®­ç»ƒäº†ä¸€ä¸ªç»Ÿä¸€çš„è§†é¢‘å­—å¹•å™¨SkyCaptioner-V1ï¼Œä»¥æœ‰æ•ˆåœ°æ ‡æ³¨è§†é¢‘æ•°æ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä¸ºåŸºæœ¬çš„è§†é¢‘ç”Ÿæˆå»ºç«‹äº†æ¸è¿›å¼åˆ†è¾¨ç‡é¢„è®­ç»ƒï¼Œéšåæ˜¯å››ä¸ªé˜¶æ®µçš„åè®­ç»ƒå¢å¼ºï¼šåˆå§‹æ¦‚å¿µå¹³è¡¡çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æé«˜äº†åŸºçº¿è´¨é‡ï¼›ä½¿ç”¨äººå·¥æ³¨é‡Šå’Œåˆæˆå¤±çœŸæ•°æ®çš„ç‰¹å®šè¿åŠ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè§£å†³äº†åŠ¨æ€ä¼ªåƒé—®é¢˜ï¼›æˆ‘ä»¬çš„æ‰©æ•£å¼ºåˆ¶æ¡†æ¶é…åˆéé€’å‡å™ªå£°æ—¶é—´è¡¨ï¼Œèƒ½å¤Ÿåœ¨æœ‰æ•ˆçš„æœç´¢ç©ºé—´ä¸­è¿›è¡Œé•¿è§†é¢‘åˆæˆï¼›æœ€åçš„é«˜è´¨é‡SFTæ”¹è¿›äº†è§†è§‰ä¿çœŸåº¦ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/SkyReels-V2%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SkyworkAI/SkyReels-V2æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13074v1">PDF</a> 31 pages,10 figures</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å½“å‰è§†é¢‘ç”Ÿæˆä¸­é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œå¦‚æç¤ºéµå®ˆã€è§†è§‰è´¨é‡ã€è¿åŠ¨åŠ¨åŠ›å­¦å’ŒæŒç»­æ—¶é—´ç­‰ï¼Œæå‡ºäº†SkyReels-V2æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€å¤šé˜¶æ®µé¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£å¼ºåˆ¶æ¡†æ¶ç­‰æŠ€æœ¯ï¼Œæ—¨åœ¨å®ç°æ— é™é•¿ç”µå½±ç”Ÿæˆã€‚è®¾è®¡å…¨é¢çš„è§†é¢‘ç»“æ„è¡¨ç¤ºï¼Œç»“åˆé€šç”¨æè¿°å’Œè¯¦ç»†é•œå¤´è¯­è¨€ï¼Œè®­ç»ƒç»Ÿä¸€è§†é¢‘æ ‡æ³¨å™¨SkyCaptioner-V1ã€‚å»ºç«‹æ¸è¿›å¼åˆ†è¾¨ç‡é¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡å››ä¸ªé˜¶æ®µçš„åæœŸè®­ç»ƒå¢å¼ºï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆè´¨é‡ã€è§£å†³åŠ¨æ€ä¼ªåƒé—®é¢˜ã€å®ç°é•¿è§†é¢‘åˆæˆå¹¶åœ¨é«˜æ•ˆæœç´¢ç©ºé—´ä¸­ä¼˜åŒ–è§†è§‰ä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†é¢‘ç”Ÿæˆé¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æç¤ºéµå®ˆã€è§†è§‰è´¨é‡ã€è¿åŠ¨åŠ¨åŠ›å­¦å’ŒæŒç»­æ—¶é—´çš„å¹³è¡¡ã€‚</li>
<li>SkyReels-V2æ¨¡å‹æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå®ç°æ— é™é•¿ç”µå½±ç”Ÿæˆã€‚</li>
<li>SkyReels-V2ç»“åˆäº†MLLMã€å¤šé˜¶æ®µé¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£å¼ºåˆ¶æ¡†æ¶ç­‰æŠ€æœ¯ã€‚</li>
<li>è®¾è®¡äº†å…¨é¢çš„è§†é¢‘ç»“æ„è¡¨ç¤ºï¼Œç»“åˆé€šç”¨æè¿°å’Œè¯¦ç»†é•œå¤´è¯­è¨€ã€‚</li>
<li>è®­ç»ƒäº†ç»Ÿä¸€è§†é¢‘æ ‡æ³¨å™¨SkyCaptioner-V1ï¼Œç”¨äºé«˜æ•ˆæ ‡æ³¨è§†é¢‘æ•°æ®ã€‚</li>
<li>å»ºç«‹äº†æ¸è¿›å¼åˆ†è¾¨ç‡é¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡å››ä¸ªé˜¶æ®µçš„åæœŸè®­ç»ƒå¢å¼ºæ¥æé«˜è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚</li>
<li>SkyReels-V2æ¨¡å‹å¯ä»¥è§£å†³åŠ¨æ€ä¼ªåƒé—®é¢˜ï¼Œå®ç°é•¿è§†é¢‘åˆæˆå¹¶åœ¨é«˜æ•ˆæœç´¢ç©ºé—´ä¸­ä¼˜åŒ–è§†è§‰ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-10e0d4599b96a127557ff3e201e9ef95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-135bbc853f17d529787e36191b59350b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9fbf10fae977ac87c8ae1741923de30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5302e464edde26d5c78c6efa43188ed7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32f40e05213ea0ac9ecd71eeecc2b78b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="NoisyRollout-Reinforcing-Visual-Reasoning-with-Data-Augmentation"><a href="#NoisyRollout-Reinforcing-Visual-Reasoning-with-Data-Augmentation" class="headerlink" title="NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation"></a>NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation</h2><p><strong>Authors:Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, Michael Qizhe Shieh</strong></p>
<p>Recent advances in reinforcement learning (RL) have strengthened the reasoning capabilities of vision-language models (VLMs). However, enhancing policy exploration to more effectively scale test-time compute remains underexplored in VLMs. In addition, VLMs continue to struggle with imperfect visual perception, which in turn affects the subsequent reasoning process. To this end, we propose NoisyRollout, a simple yet effective RL approach that mixes trajectories from both clean and moderately distorted images to introduce targeted diversity in visual perception and the resulting reasoning patterns. Without additional training cost, NoisyRollout enhances the exploration capabilities of VLMs by incorporating a vision-oriented inductive bias. Furthermore, NoisyRollout employs a noise annealing schedule that gradually reduces distortion strength over training, ensuring benefit from noisy signals early while maintaining training stability and scalability in later stages. With just 2.1K training samples, NoisyRollout achieves state-of-the-art performance among open-source RL-tuned models on 5 out-of-domain benchmarks spanning both reasoning and perception tasks, while preserving comparable or even better in-domain performance. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æœ€æ–°è¿›å±•æé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨VLMä¸­ï¼Œå…³äºå¦‚ä½•æ›´æœ‰æ•ˆåœ°æ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—çš„ç­–ç•¥æ¢ç´¢ä»ç„¶ç¼ºä¹æ¢ç´¢ã€‚æ­¤å¤–ï¼ŒVLMåœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢ä»å­˜åœ¨ç¼ºé™·ï¼Œè¿›è€Œå½±å“åç»­çš„æ¨ç†è¿‡ç¨‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†NoisyRolloutï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„RLæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ··åˆå¹²å‡€å›¾åƒå’Œé€‚åº¦å¤±çœŸå›¾åƒçš„è½¨è¿¹æ¥å¼•å…¥æœ‰é’ˆå¯¹æ€§çš„è§†è§‰æ„ŸçŸ¥å’Œç»“æœæ¨ç†æ¨¡å¼çš„å¤šæ ·æ€§ã€‚NoisyRollouté€šè¿‡å¼•å…¥é¢å‘è§†è§‰çš„å½’çº³åç½®ï¼Œåœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†VLMçš„æ¢ç´¢èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒNoisyRollouté‡‡ç”¨å™ªå£°é€€ç«æ—¶é—´è¡¨ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸é™ä½å¤±çœŸå¼ºåº¦ï¼Œç¡®ä¿åœ¨æ—©æœŸä»å™ªå£°ä¿¡å·ä¸­å—ç›Šï¼ŒåŒæ—¶åœ¨åæœŸä¿æŒè®­ç»ƒç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ã€‚ä»…ä½¿ç”¨2.1Kè®­ç»ƒæ ·æœ¬ï¼ŒNoisyRolloutåœ¨æ¶µç›–æ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡çš„äº”ä¸ªè·¨åŸŸåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¼€æºRLè°ƒä¼˜æ¨¡å‹ä¸­çš„æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“ç”šè‡³æ›´å¥½çš„åŸŸå†…æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13055v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æœ€æ–°è¿›å±•å¢å¼ºäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæé«˜ç­–ç•¥æ¢ç´¢ä»¥æ›´æœ‰æ•ˆåœ°æ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—ä»åœ¨VLMä¸­æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æ­¤å¤–ï¼ŒVLMä»ç„¶é¢ä¸´ä¸å®Œç¾çš„è§†è§‰æ„ŸçŸ¥é—®é¢˜ï¼Œè¿›è€Œå½±å“åç»­çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†NoisyRolloutï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„RLæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ··åˆæ¥è‡ªå¹²å‡€å’Œé€‚åº¦å¤±çœŸå›¾åƒçš„è½¨è¿¹æ¥å¼•å…¥æœ‰é’ˆå¯¹æ€§çš„è§†è§‰æ„ŸçŸ¥å’Œç»“æœæ¨ç†æ¨¡å¼çš„å¤šæ ·æ€§ã€‚NoisyRolloutåœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡èå…¥é¢å‘è§†è§‰çš„å½’çº³åç½®ï¼Œå¢å¼ºäº†VLMçš„æ¢ç´¢èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒNoisyRollouté‡‡ç”¨å™ªå£°é€€ç«è®¡åˆ’ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œé€æ¸é™ä½å¤±çœŸå¼ºåº¦ï¼Œç¡®ä¿ä»å™ªå£°ä¿¡å·ä¸­å—ç›Šçš„åŒæ—¶ä¿æŒè®­ç»ƒçš„ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§ã€‚ä»…ä½¿ç”¨2.1Kè®­ç»ƒæ ·æœ¬ï¼ŒNoisyRolloutåœ¨è·¨è¶Šæ¨ç†å’Œæ„ŸçŸ¥ä»»åŠ¡çš„äº”ä¸ªåŸŸå¤–åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¼€æºRLè°ƒä¼˜æ¨¡å‹ä¸­çš„æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†åŸŸå†…æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>VLMåœ¨ç­–ç•¥æ¢ç´¢å’Œè§†è§‰æ„ŸçŸ¥æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>NoisyRolloutæ˜¯ä¸€ç§è§£å†³è¿™äº›æŒ‘æˆ˜çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ··åˆå¹²å‡€å’Œå¤±çœŸå›¾åƒçš„è½¨è¿¹æ¥å¼•å…¥è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†æ¨¡å¼çš„å¤šæ ·æ€§ã€‚</li>
<li>NoisyRolloutåœ¨ä¸å¢åŠ é¢å¤–è®­ç»ƒæˆæœ¬çš„æƒ…å†µä¸‹å¢å¼ºäº†VLMçš„æ¢ç´¢èƒ½åŠ›ã€‚</li>
<li>NoisyRollouté‡‡ç”¨å™ªå£°é€€ç«è®¡åˆ’ä»¥ç¡®ä¿ä»å™ªå£°ä¿¡å·ä¸­å—ç›Šå¹¶ç»´æŒè®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>ä»…ä½¿ç”¨æœ‰é™çš„è®­ç»ƒæ ·æœ¬ï¼ŒNoisyRolloutåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9776e70786296ee202fb6856a5c2197.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4b9f8aff159733e95a1e713e673416c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c3a925c34790b7031ee47cc7698243e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2e3a6130cbb5e6779af7910dbb658e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bf383710802ec4aab0301f5e1e0cc58.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InstructRAG-Leveraging-Retrieval-Augmented-Generation-on-Instruction-Graphs-for-LLM-Based-Task-Planning"><a href="#InstructRAG-Leveraging-Retrieval-Augmented-Generation-on-Instruction-Graphs-for-LLM-Based-Task-Planning" class="headerlink" title="InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction   Graphs for LLM-Based Task Planning"></a>InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction   Graphs for LLM-Based Task Planning</h2><p><strong>Authors:Zheng Wang, Shu Xian Teo, Jun Jie Chew, Wei Shi</strong></p>
<p>Recent advancements in large language models (LLMs) have enabled their use as agents for planning complex tasks. Existing methods typically rely on a thought-action-observation (TAO) process to enhance LLM performance, but these approaches are often constrained by the LLMsâ€™ limited knowledge of complex tasks. Retrieval-augmented generation (RAG) offers new opportunities by leveraging external databases to ground generation in retrieved information. In this paper, we identify two key challenges (enlargability and transferability) in applying RAG to task planning. We propose InstructRAG, a novel solution within a multi-agent meta-reinforcement learning framework, to address these challenges. InstructRAG includes a graph to organize past instruction paths (sequences of correct actions), an RL-Agent with Reinforcement Learning to expand graph coverage for enlargability, and an ML-Agent with Meta-Learning to improve task generalization for transferability. The two agents are trained end-to-end to optimize overall planning performance. Our experiments on four widely used task planning datasets demonstrate that InstructRAG significantly enhances performance and adapts efficiently to new tasks, achieving up to a 19.2% improvement over the best existing approach. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å…¶èƒ½å¤Ÿä½œä¸ºå®Œæˆå¤æ‚ä»»åŠ¡çš„ä»£ç†ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ€ç»´-è¡ŒåŠ¨-è§‚å¯Ÿï¼ˆTAOï¼‰è¿‡ç¨‹æ¥æé«˜LLMçš„æ€§èƒ½ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å—åˆ°LLMå¯¹å¤æ‚ä»»åŠ¡çŸ¥è¯†æœ‰é™çš„åˆ¶çº¦ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡åˆ©ç”¨å¤–éƒ¨æ•°æ®åº“ä½¿ç”ŸæˆåŸºäºæ£€ç´¢çš„ä¿¡æ¯æä¾›äº†æ–°çš„æœºä¼šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯†åˆ«äº†å°†RAGåº”ç”¨äºä»»åŠ¡è§„åˆ’çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå³å¯æ‰©å±•æ€§å’Œå¯è½¬ç§»æ€§ã€‚æˆ‘ä»¬æå‡ºInstructRAGï¼Œè¿™æ˜¯ä¸€ç§å¤šæ™ºèƒ½ä½“å…ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶å†…çš„æ–°å‹è§£å†³æ–¹æ¡ˆï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚InstructRAGåŒ…æ‹¬ä¸€ä¸ªå›¾ï¼Œç”¨äºç»„ç»‡è¿‡å»çš„æŒ‡ä»¤è·¯å¾„ï¼ˆæ­£ç¡®çš„è¡ŒåŠ¨åºåˆ—ï¼‰ï¼Œä¸€ä¸ªRLæ™ºèƒ½ä½“ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥æ‰©å¤§å›¾çš„è¦†ç›–ä»¥å¢åŠ å¯æ‰©å±•æ€§ï¼Œä»¥åŠä¸€ä¸ªä½¿ç”¨å…ƒå­¦ä¹ çš„MLæ™ºèƒ½ä½“ä»¥æé«˜ä»»åŠ¡æ³›åŒ–æ€§ä»¥å¢å¼ºå¯è¿ç§»æ€§ã€‚è¿™ä¸¤ä¸ªæ™ºèƒ½ä½“é€šè¿‡ç«¯åˆ°ç«¯çš„è®­ç»ƒä»¥ä¼˜åŒ–æ•´ä½“è§„åˆ’æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„ä»»åŠ¡è§„åˆ’æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒInstructRAGæ˜¾è‘—æé«˜äº†æ€§èƒ½å¹¶èƒ½æœ‰æ•ˆåœ°é€‚åº”æ–°ä»»åŠ¡ï¼Œä¸æœ€ä½³ç°æœ‰æ–¹æ³•ç›¸æ¯”å®ç°äº†é«˜è¾¾19.2%çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13032v1">PDF</a> This paper has been accepted by SIGIR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿‘æœŸè¿›å±•ä½¿å¾—å®ƒä»¬å¯ä»¥ä½œä¸ºå¤æ‚ä»»åŠ¡çš„ä»£ç†ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿï¼ˆTAOï¼‰è¿‡ç¨‹æ¥æé«˜LLMæ€§èƒ½ï¼Œä½†å—é™äºLLMå¯¹å¤æ‚ä»»åŠ¡çš„æœ‰é™çŸ¥è¯†ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡åˆ©ç”¨å¤–éƒ¨æ•°æ®åº“ä½¿ç”ŸæˆåŸºäºæ£€ç´¢çš„ä¿¡æ¯æˆä¸ºå¯èƒ½ã€‚æœ¬æ–‡æŒ‡å‡ºå°†RAGåº”ç”¨äºä»»åŠ¡è§„åˆ’çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šå¯æ‰©å¤§æ€§å’Œå¯è½¬ç§»æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºInstructRAGï¼Œè¿™æ˜¯ä¸€ç§å¤šä»£ç†å…ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶å†…çš„è§£å†³æ–¹æ¡ˆã€‚InstructRAGåŒ…æ‹¬ä¸€ä¸ªç»„ç»‡è¿‡å»æŒ‡ä»¤è·¯å¾„çš„å›¾ã€ä¸€ä¸ªç”¨äºæ‰©å¤§æ€§çš„å¼ºåŒ–å­¦ä¹ ä»£ç†æ¥æ‰©å¤§å›¾çš„è¦†ç›–é¢ï¼Œä»¥åŠä¸€ä¸ªç”¨äºè½¬ç§»æ€§çš„å…ƒå­¦ä¹ ä»£ç†æ¥æé«˜ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚ä¸¤ä¸ªä»£ç†ç«¯åˆ°ç«¯è®­ç»ƒä»¥ä¼˜åŒ–æ•´ä½“è§„åˆ’æ€§èƒ½ã€‚åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„ä»»åŠ¡è§„åˆ’æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒInstructRAGæ˜¾è‘—æé«˜äº†æ€§èƒ½å¹¶é«˜æ•ˆé€‚åº”æ–°ä»»åŠ¡ï¼Œç›¸æ¯”æœ€ä½³ç°æœ‰æ–¹æ³•æœ€å¤šæé«˜äº†19.2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä½œä¸ºå¤æ‚ä»»åŠ¡çš„ä»£ç†ã€‚</li>
<li>ç°æœ‰LLMæ€§èƒ½æå‡æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚æœ‰é™çš„ä»»åŠ¡çŸ¥è¯†ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•é€šè¿‡åˆ©ç”¨å¤–éƒ¨æ•°æ®åº“æ”¹è¿›äº†LLMçš„æ€§èƒ½ã€‚</li>
<li>å°†RAGåº”ç”¨äºä»»åŠ¡è§„åˆ’é¢ä¸´å¯æ‰©å¤§æ€§å’Œå¯è½¬ç§»æ€§ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>InstructRAGæ˜¯ä¸€ä¸ªè§£å†³è¿™äº›æŒ‘æˆ˜çš„æ–°æ–¹æ³•ï¼Œä½¿ç”¨å¤šä»£ç†å…ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>InstructRAGåŒ…æ‹¬ç”¨äºæ‰©å¤§æ€§çš„å¼ºåŒ–å­¦ä¹ ä»£ç†å’Œç”¨äºè½¬ç§»æ€§çš„å…ƒå­¦ä¹ ä»£ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-615ae1c58a765cf44929e8698227f424.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bdc1250aaa447ec3bc232248b541c1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f42ab4ee582abdd1c52b51b508e7599f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="QLLM-Do-We-Really-Need-a-Mixing-Network-for-Credit-Assignment-in-Multi-Agent-Reinforcement-Learning"><a href="#QLLM-Do-We-Really-Need-a-Mixing-Network-for-Credit-Assignment-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="QLLM: Do We Really Need a Mixing Network for Credit Assignment in   Multi-Agent Reinforcement Learning?"></a>QLLM: Do We Really Need a Mixing Network for Credit Assignment in   Multi-Agent Reinforcement Learning?</h2><p><strong>Authors:Zhouyang Jiang, Bin Zhang, Airong Wei, Zhiwei Xu</strong></p>
<p>Credit assignment has remained a fundamental challenge in multi-agent reinforcement learning (MARL). Previous studies have primarily addressed this issue through value decomposition methods under the centralized training with decentralized execution paradigm, where neural networks are utilized to approximate the nonlinear relationship between individual Q-values and the global Q-value. Although these approaches have achieved considerable success in various benchmark tasks, they still suffer from several limitations, including imprecise attribution of contributions, limited interpretability, and poor scalability in high-dimensional state spaces. To address these challenges, we propose a novel algorithm, \textbf{QLLM}, which facilitates the automatic construction of credit assignment functions using large language models (LLMs). Specifically, the concept of \textbf{TFCAF} is introduced, wherein the credit allocation process is represented as a direct and expressive nonlinear functional formulation. A custom-designed \textit{coder-evaluator} framework is further employed to guide the generation, verification, and refinement of executable code by LLMs, significantly mitigating issues such as hallucination and shallow reasoning during inference. Extensive experiments conducted on several standard MARL benchmarks demonstrate that the proposed method consistently outperforms existing state-of-the-art baselines. Moreover, QLLM exhibits strong generalization capability and maintains compatibility with a wide range of MARL algorithms that utilize mixing networks, positioning it as a promising and versatile solution for complex multi-agent scenarios. </p>
<blockquote>
<p>åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­ï¼Œä¿¡ç”¨åˆ†é…ä¸€ç›´æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚å…ˆå‰çš„ç ”ç©¶ä¸»è¦é€šè¿‡é›†ä¸­è®­ç»ƒä¸åˆ†æ•£æ‰§è¡ŒèŒƒå¼ä¸‹çš„ä»·å€¼åˆ†è§£æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼ä¸ªä½“Qå€¼çš„å…¨å±€Qå€¼ä¹‹é—´çš„éçº¿æ€§å…³ç³»ã€‚å°½ç®¡è¿™äº›æ–¹æ³•åœ¨å„ç§åŸºå‡†ä»»åŠ¡ä¸Šå–å¾—äº†ç›¸å½“å¤§çš„æˆåŠŸï¼Œä½†å®ƒä»¬ä»å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼ŒåŒ…æ‹¬è´¡çŒ®åˆ†é…ä¸ç²¾ç¡®ã€è§£é‡Šæ€§æœ‰é™ä»¥åŠåœ¨é«˜ç»´çŠ¶æ€ç©ºé—´ä¸­å¯æ‰©å±•æ€§å·®ç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç®—æ³•QLLMï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨æ„å»ºä¿¡ç”¨åˆ†é…å‡½æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œå¼•å…¥äº†TFCAFçš„æ¦‚å¿µï¼Œå…¶ä¸­ä¿¡ç”¨åˆ†é…è¿‡ç¨‹è¢«è¡¨ç¤ºä¸ºç›´æ¥è€Œå¯Œæœ‰è¡¨ç°åŠ›çš„éçº¿æ€§å‡½æ•°å…¬å¼ã€‚è¿›ä¸€æ­¥é‡‡ç”¨å®šåˆ¶è®¾è®¡çš„ç¼–ç å™¨-è¯„ä¼°å™¨æ¡†æ¶ï¼Œä»¥æŒ‡å¯¼LLMç”Ÿæˆã€éªŒè¯å’Œç»†åŒ–å¯æ‰§è¡Œä»£ç ï¼Œä»è€Œæ˜¾è‘—ç¼“è§£æ¨ç†è¿‡ç¨‹ä¸­çš„å¹»è§‰å’Œæµ…è–„æ¨ç†é—®é¢˜ã€‚åœ¨å¤šä¸ªæ ‡å‡†MARLåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„åŸºçº¿ã€‚æ­¤å¤–ï¼ŒQLLMå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸ä½¿ç”¨æ··åˆç½‘ç»œçš„ä¸€ç³»åˆ—MARLç®—æ³•å…¼å®¹ï¼Œè¿™ä½¿å…¶æˆä¸ºå¤„ç†å¤æ‚å¤šæ™ºèƒ½ä½“åœºæ™¯çš„æœ‰å‰é€”å’Œå¤šåŠŸèƒ½è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12961v1">PDF</a> 9 pages, 7 figures</p>
<p><strong>Summary</strong>ï¼šåœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­ï¼Œä¿¡ç”¨åˆ†é…ä¸€ç›´æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é€šè¿‡ä»·å€¼åˆ†è§£æ–¹æ³•åœ¨é›†ä¸­è®­ç»ƒä¸åˆ†æ•£æ‰§è¡Œçš„æ¨¡å¼ä¸‹è§£å†³æ­¤é—®é¢˜ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼ä¸ªä½“Qå€¼çš„å…¨å±€Qå€¼ä¹‹é—´çš„éçº¿æ€§å…³ç³»ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨å½’å› ä¸å‡†ç¡®ã€è§£é‡Šæ€§æœ‰é™ä»¥åŠé«˜ç»´çŠ¶æ€ç©ºé—´ä¸­æ‰©å±•æ€§å·®ç­‰å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•QLLMï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨æ„å»ºä¿¡ç”¨åˆ†é…å‡½æ•°ã€‚é€šè¿‡å¼•å…¥TFCAFæ¦‚å¿µï¼Œå°†ä¿¡ç”¨åˆ†é…è¿‡ç¨‹è¡¨ç¤ºä¸ºç›´æ¥å’Œéçº¿æ€§çš„åŠŸèƒ½å…¬å¼ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å®šåˆ¶è®¾è®¡çš„ç¼–ç -è¯„ä¼°æ¡†æ¶æ¥æŒ‡å¯¼LLMç”Ÿæˆã€éªŒè¯å’Œç»†åŒ–å¯æ‰§è¡Œä»£ç ï¼Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å¹»è§‰å’Œæµ…å±‚æ¨ç†é—®é¢˜ã€‚åœ¨å¤šä¸ªæ ‡å‡†MARLåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æŒç»­ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿ã€‚åŒæ—¶ï¼ŒQLLMè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸åˆ©ç”¨æ··åˆç½‘ç»œçš„å„ç§MARLç®—æ³•å…¼å®¹ï¼Œæˆä¸ºå¤æ‚å¤šæ™ºèƒ½ä½“åœºæ™¯ä¸­æœ‰å‰é€”å’Œé€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¿¡ç”¨åˆ†é…ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡ä»·å€¼åˆ†è§£æ¥è§£å†³æ­¤é—®é¢˜ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•QLLMï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨æ„å»ºä¿¡ç”¨åˆ†é…å‡½æ•°ã€‚</li>
<li>å¼•å…¥TFCAFæ¦‚å¿µï¼Œå°†ä¿¡ç”¨åˆ†é…è¿‡ç¨‹ç›´æ¥å’Œéçº¿æ€§åœ°è¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨ç¼–ç -è¯„ä¼°æ¡†æ¶æ¥æŒ‡å¯¼LLMç”Ÿæˆå’ŒéªŒè¯ä»£ç ï¼Œå‡å°‘æ¨ç†ä¸­çš„è¯¯å·®ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒQLLMè¡¨ç°ä¼˜è¶Šï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a41b1b05b4f07b128df84fdc68a963da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86e16e5ed71e7ae43a7206f2b35c0ddd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d212015c0a1155e684047f71895f1017.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-the-Geometric-Problem-Solving-Ability-of-Multimodal-LLMs-via-Symbolic-Neural-Integration"><a href="#Enhancing-the-Geometric-Problem-Solving-Ability-of-Multimodal-LLMs-via-Symbolic-Neural-Integration" class="headerlink" title="Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via   Symbolic-Neural Integration"></a>Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via   Symbolic-Neural Integration</h2><p><strong>Authors:Yicheng Pan, Zhenrong Zhang, Pengfei Hu, Jiefeng Ma, Jun Du, Jianshu Zhang, Quan Liu, Jianqing Gao, Feng Ma</strong></p>
<p>Recent advances in Multimodal Large Language Models (MLLMs) have achieved remarkable progress in general domains and demonstrated promise in multimodal mathematical reasoning. However, applying MLLMs to geometry problem solving (GPS) remains challenging due to lack of accurate step-by-step solution data and severe hallucinations during reasoning. In this paper, we propose GeoGen, a pipeline that can automatically generates step-wise reasoning paths for geometry diagrams. By leveraging the precise symbolic reasoning, \textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To further enhance the logical reasoning ability of MLLMs, we train \textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated by GeoGen. Serving as a bridge between natural language and symbolic systems, GeoLogic enables symbolic tools to help verifying MLLM outputs, making the reasoning process more rigorous and alleviating hallucinations. Experimental results show that our approach consistently improves the performance of MLLMs, achieving remarkable results on benchmarks for geometric reasoning tasks. This improvement stems from our integration of the strengths of LLMs and symbolic systems, which enables a more reliable and interpretable approach for the GPS task. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/ycpNotFound/GeoGen">https://github.com/ycpNotFound/GeoGen</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é€šç”¨é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œå¹¶åœ¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†MLLMsåº”ç”¨äºå‡ ä½•é—®é¢˜æ±‚è§£ï¼ˆGPSï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç¼ºä¹å‡†ç¡®çš„é€æ­¥è§£å†³æ–¹æ¡ˆæ•°æ®ï¼Œä»¥åŠåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šå‡ºç°ä¸¥é‡çš„å¹»è§‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GeoGenï¼Œè¿™æ˜¯ä¸€ä¸ªå¯ä»¥è‡ªåŠ¨ä¸ºå‡ ä½•å›¾è¡¨ç”Ÿæˆé€æ­¥æ¨ç†è·¯å¾„çš„ç®¡é“ã€‚é€šè¿‡åˆ©ç”¨ç²¾ç¡®çš„ç¬¦å·æ¨ç†ï¼ŒGeoGenç”Ÿæˆå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºMLLMsçš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬ä½¿ç”¨GeoGenç”Ÿæˆåˆæˆæ•°æ®æ¥è®­ç»ƒGeoLogicï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚GeoLogicä½œä¸ºè‡ªç„¶è¯­è¨€ä¸ç¬¦å·ç³»ç»Ÿä¹‹é—´çš„æ¡¥æ¢ï¼Œèƒ½å¤Ÿåˆ©ç”¨ç¬¦å·å·¥å…·å¸®åŠ©éªŒè¯MLLMçš„è¾“å‡ºï¼Œä½¿æ¨ç†è¿‡ç¨‹æ›´åŠ ä¸¥è°¨ï¼Œå¹¶å‡è½»å¹»è§‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸€è‡´åœ°æé«˜äº†MLLMçš„æ€§èƒ½ï¼Œåœ¨å‡ ä½•æ¨ç†ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚è¿™ç§æ”¹è¿›æºäºæˆ‘ä»¬æ•´åˆäº†LLMså’Œç¬¦å·ç³»ç»Ÿçš„ä¼˜åŠ¿ï¼Œä¸ºGPSä»»åŠ¡æä¾›äº†ä¸€ç§æ›´å¯é ã€å¯è§£é‡Šçš„æ–¹æ³•ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ycpNotFound/GeoGen%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/ycpNotFound/GeoGenä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12773v1">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong><br>å‡ ä½•é—®é¢˜æ±‚è§£ï¼ˆGPSï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„åº”ç”¨ä¸­ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹å‡†ç¡®çš„é€æ­¥è§£ç­”æ•°æ®å’Œæ¨ç†è¿‡ç¨‹ä¸­çš„ä¸¥é‡å¹»è§‰ã€‚æœ¬æ–‡æå‡ºGeoGenç®¡é“ï¼Œå¯è‡ªåŠ¨ç”Ÿæˆå‡ ä½•å›¾è¡¨çš„é€æ­¥æ¨ç†è·¯å¾„ï¼Œå¹¶ç»“åˆç²¾ç¡®ç¬¦å·æ¨ç†ï¼Œç”Ÿæˆå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚ä¸ºè¿›ä¸€æ­¥å¢å¼ºMLLMsçš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬åˆ©ç”¨GeoGenç”Ÿæˆçš„æ•°æ®è®­ç»ƒäº†GeoLogicå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚GeoLogicä½œä¸ºè‡ªç„¶è¯­è¨€ä¸ç¬¦å·ç³»ç»Ÿä¹‹é—´çš„æ¡¥æ¢ï¼Œå¯ä½¿ç¬¦å·å·¥å…·å¸®åŠ©éªŒè¯MLLMè¾“å‡ºï¼Œä½¿æ¨ç†è¿‡ç¨‹æ›´åŠ ä¸¥è°¨ï¼Œå‡è½»å¹»è§‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æŒç»­æé«˜äº†MLLMsçš„æ€§èƒ½ï¼Œåœ¨å‡ ä½•æ¨ç†ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚è¿™ä¸€æ”¹è¿›æºäºLLMså’Œç¬¦å·ç³»ç»Ÿçš„ç»“åˆï¼Œä¸ºGPSä»»åŠ¡æä¾›äº†æ›´å¯é ã€å¯è§£é‡Šçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å‡ ä½•é—®é¢˜æ±‚è§£ï¼ˆGPSï¼‰ä¸­çš„åº”ç”¨å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦å› ä¸ºç¼ºä¹å‡†ç¡®çš„é€æ­¥è§£ç­”æ•°æ®å’Œæ¨ç†è¿‡ç¨‹ä¸­çš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>GeoGenç®¡é“å¯ä»¥è‡ªåŠ¨ç”Ÿæˆå‡ ä½•å›¾è¡¨çš„é€æ­¥æ¨ç†è·¯å¾„ï¼Œç”Ÿæˆé«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚</li>
<li>GeoGenç»“åˆç²¾ç¡®ç¬¦å·æ¨ç†ï¼Œæé«˜äº†å¤§æ•°æ®ç”Ÿæˆçš„è´¨é‡ã€‚</li>
<li>GeoLogicå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ©ç”¨GeoGenç”Ÿæˆçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¢å¼ºäº†MLLMsçš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚</li>
<li>GeoLogicä½œä¸ºè‡ªç„¶è¯­è¨€ä¸ç¬¦å·ç³»ç»Ÿä¹‹é—´çš„æ¡¥æ¢ï¼Œç¬¦å·å·¥å…·å¯å¸®åŠ©éªŒè¯MLLMè¾“å‡ºï¼Œä½¿æ¨ç†æ›´åŠ ä¸¥è°¨ã€‚</li>
<li>ç»“åˆLLMså’Œç¬¦å·ç³»ç»Ÿçš„ä¼˜åŠ¿ï¼ŒGPSä»»åŠ¡æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œå®éªŒç»“æœæ˜¾ç¤ºåœ¨å‡ ä½•æ¨ç†ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-61498abc081d7981cffeb5b17578a27a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a181370a4f0f804cba85a1e67342958d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fddc575c576cf42a4ef7eb6ae347e8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-890f5c3ba41d746cdddff661730e80af.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GraphOmni-A-Comprehensive-and-Extendable-Benchmark-Framework-for-Large-Language-Models-on-Graph-theoretic-Tasks"><a href="#GraphOmni-A-Comprehensive-and-Extendable-Benchmark-Framework-for-Large-Language-Models-on-Graph-theoretic-Tasks" class="headerlink" title="GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large   Language Models on Graph-theoretic Tasks"></a>GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large   Language Models on Graph-theoretic Tasks</h2><p><strong>Authors:Hao Xu, Xiangru Jian, Xinjian Zhao, Wei Pang, Chao Zhang, Suyuchen Wang, Qixin Zhang, Joao Monteiro, Qiuzhuang Sun, Tianshu Yu</strong></p>
<p>In this paper, we presented GraphOmni, a comprehensive benchmark framework for systematically evaluating the graph reasoning capabilities of LLMs. By analyzing critical dimensions, including graph types, serialization formats, and prompt schemes, we provided extensive insights into the strengths and limitations of current LLMs. Our empirical findings emphasize that no single serialization or prompting strategy consistently outperforms others. Motivated by these insights, we propose a reinforcement learning-based approach that dynamically selects the best serialization-prompt pairings, resulting in significant accuracy improvements. GraphOmniâ€™s modular and extensible design establishes a robust foundation for future research, facilitating advancements toward general-purpose graph reasoning models. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GraphOmniï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºç³»ç»Ÿåœ°è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„å›¾æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åˆ†æå…³é”®ç»´åº¦ï¼ŒåŒ…æ‹¬å›¾ç±»å‹ã€åºåˆ—åŒ–æ ¼å¼å’Œæç¤ºæ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯¹å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿å’Œå±€é™æ€§æä¾›äº†æ·±åˆ»çš„è§è§£ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶å‘ç°ï¼Œæ²¡æœ‰ä¸€ç§åºåˆ—åŒ–æˆ–æç¤ºç­–ç•¥å§‹ç»ˆä¼˜äºå…¶ä»–ç­–ç•¥ã€‚å—è¿™äº›è§è§£çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥åŠ¨æ€é€‰æ‹©æœ€ä½³çš„åºåˆ—åŒ–æç¤ºé…å¯¹ï¼Œä»è€Œæ˜¾è‘—æé«˜å‡†ç¡®æ€§ã€‚GraphOmniçš„æ¨¡å—åŒ–å¯æ‰©å±•è®¾è®¡ä¸ºæœªæ¥ç ”ç©¶å»ºç«‹äº†åšå®çš„åŸºç¡€ï¼Œä¿ƒè¿›äº†é€šç”¨å›¾æ¨ç†æ¨¡å‹çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12764v1">PDF</a> 82 pages</p>
<p><strong>Summary</strong><br>è¿™æ˜¯ä¸€ç¯‡å…³äºGraphOmniçš„è®ºæ–‡ï¼Œå®ƒæ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å›¾æ¨ç†èƒ½åŠ›çš„åŸºå‡†æ¡†æ¶ã€‚è®ºæ–‡é€šè¿‡åˆ†æäº†åŒ…æ‹¬å›¾ç±»å‹ã€åºåˆ—åŒ–æ ¼å¼å’Œæç¤ºæ–¹æ¡ˆç­‰å…³é”®ç»´åº¦ï¼Œå¯¹å½“å‰LLMsçš„ä¼˜åŠ¿å’Œå±€é™æ€§è¿›è¡Œäº†æ·±åˆ»æ´å¯Ÿã€‚åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ä½³çš„åºåˆ—åŒ–æç¤ºé…å¯¹ï¼Œä»è€Œæ˜¾è‘—æé«˜å‡†ç¡®æ€§ã€‚GraphOmniçš„æ¨¡å—åŒ–å¯æ‰©å±•è®¾è®¡ä¸ºå…¶æœªæ¥çš„ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ï¼Œä¿ƒè¿›äº†é€šç”¨å›¾æ¨ç†æ¨¡å‹çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GraphOmniæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å›¾æ¨ç†èƒ½åŠ›çš„åŸºå‡†æ¡†æ¶ã€‚</li>
<li>è®ºæ–‡åˆ†æäº†åŒ…æ‹¬å›¾ç±»å‹ã€åºåˆ—åŒ–æ ¼å¼å’Œæç¤ºæ–¹æ¡ˆåœ¨å†…çš„å…³é”®ç»´åº¦ã€‚</li>
<li>å½“å‰LLMsçš„ä¼˜åŠ¿å’Œå±€é™æ€§å¾—åˆ°äº†æ·±åˆ»æ´å¯Ÿã€‚</li>
<li>æ²¡æœ‰ä¸€ç§åºåˆ—åŒ–æˆ–æç¤ºç­–ç•¥å§‹ç»ˆä¼˜äºå…¶ä»–ç­–ç•¥ã€‚</li>
<li>åŸºäºè¿™äº›å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ä½³çš„åºåˆ—åŒ–æç¤ºé…å¯¹ï¼Œä»¥æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>GraphOmniçš„æ¨¡å—åŒ–å¯æ‰©å±•è®¾è®¡ä¸ºå…¶æœªæ¥çš„ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12764">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-003c221ebe32ff3038d12fe963b160bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-898108b18fd21c86bffbfb5bcc3d17d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-101f36b6e97fac30682766a89749db75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-786a6a40d0c5804c74a6e5230896fefd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4351c1d247e720c3a98bbf888b38f75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbd9e9dd08f2153870028195ac842693.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Pandora-A-Code-Driven-Large-Language-Model-Agent-for-Unified-Reasoning-Across-Diverse-Structured-Knowledge"><a href="#Pandora-A-Code-Driven-Large-Language-Model-Agent-for-Unified-Reasoning-Across-Diverse-Structured-Knowledge" class="headerlink" title="Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning   Across Diverse Structured Knowledge"></a>Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning   Across Diverse Structured Knowledge</h2><p><strong>Authors:Yongrui Chen, Junhao He, Linbo Fu, Shenyu Zhang, Rihui Jin, Xinbang Dai, Jiaqi Li, Dehai Min, Nan Hu, Yuxin Zhang, Guilin Qi, Yi Huang, Tongtong Wu</strong></p>
<p>Unified Structured Knowledge Reasoning (USKR) aims to answer natural language questions (NLQs) by using structured sources such as tables, databases, and knowledge graphs in a unified way. Existing USKR methods either rely on employing task-specific strategies or custom-defined representations, which struggle to leverage the knowledge transfer between different SKR tasks or align with the prior of LLMs, thereby limiting their performance. This paper proposes a novel USKR framework named \textsc{Pandora}, which takes advantage of \textsc{Python}â€™s \textsc{Pandas} API to construct a unified knowledge representation for alignment with LLM pre-training. It employs an LLM to generate textual reasoning steps and executable Python code for each question. Demonstrations are drawn from a memory of training examples that cover various SKR tasks, facilitating knowledge transfer. Extensive experiments on four benchmarks involving three SKR tasks demonstrate that \textsc{Pandora} outperforms existing unified frameworks and competes effectively with task-specific methods. </p>
<blockquote>
<p>ç»Ÿä¸€ç»“æ„åŒ–çŸ¥è¯†æ¨ç†ï¼ˆUSKRï¼‰æ—¨åœ¨é€šè¿‡ç»“æ„åŒ–æ¥æºï¼ˆå¦‚è¡¨æ ¼ã€æ•°æ®åº“å’ŒçŸ¥è¯†å›¾è°±ï¼‰ä»¥ç»Ÿä¸€çš„æ–¹å¼å›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜ï¼ˆNLQsï¼‰ã€‚ç°æœ‰çš„USKRæ–¹æ³•è¦ä¹ˆä¾èµ–äºé‡‡ç”¨ç‰¹å®šä»»åŠ¡çš„ç­–ç•¥ï¼Œè¦ä¹ˆä¾èµ–äºè‡ªå®šä¹‰è¡¨ç¤ºï¼Œè¿™å¾ˆéš¾åˆ©ç”¨ä¸åŒSKRä»»åŠ¡ä¹‹é—´çš„çŸ¥è¯†è½¬ç§»æˆ–ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…ˆéªŒçŸ¥è¯†å¯¹é½ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„USKRæ¡†æ¶ï¼Œåä¸ºPandoraï¼Œå®ƒåˆ©ç”¨Pythonçš„Pandas APIæ„å»ºç»Ÿä¸€çš„çŸ¥è¯†è¡¨ç¤ºï¼Œä»¥ä¸LLMé¢„è®­ç»ƒå¯¹é½ã€‚å®ƒåˆ©ç”¨LLMç”Ÿæˆæ–‡æœ¬æ¨ç†æ­¥éª¤å’Œé’ˆå¯¹æ¯ä¸ªé—®é¢˜çš„å¯æ‰§è¡Œçš„Pythonä»£ç ã€‚æ¼”ç¤ºå†…å®¹æ¥è‡ªæ¶µç›–å„ç§SKRä»»åŠ¡çš„è®­ç»ƒç¤ºä¾‹å†…å­˜ï¼Œä¿ƒè¿›äº†çŸ¥è¯†è¿ç§»ã€‚åœ¨æ¶‰åŠä¸‰ä¸ªSKRä»»åŠ¡çš„å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPandoraåœ¨ç°æœ‰ç»Ÿä¸€æ¡†æ¶ä¸­å…·æœ‰å‡ºè‰²çš„è¡¨ç°ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°ä¸ç‰¹å®šä»»åŠ¡çš„æ–¹æ³•ç«äº‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12734v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æè¿°äº†ä¸€ç§åä¸ºPandoraçš„ç»Ÿä¸€ç»“æ„åŒ–çŸ¥è¯†æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Pythonçš„Pandas APIæ„å»ºç»Ÿä¸€çŸ¥è¯†è¡¨ç¤ºï¼Œä¸å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå¯¹é½ï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–æ•°æ®æºå›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚å®ƒä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ¨ç†æ­¥éª¤å’Œé’ˆå¯¹æ¯ä¸ªé—®é¢˜çš„å¯æ‰§è¡ŒPythonä»£ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPandoraåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰ç»Ÿä¸€æ¡†æ¶ï¼Œå¹¶ä¸ç‰¹å®šä»»åŠ¡æ–¹æ³•æœ‰æ•ˆç«äº‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Pandoraæ˜¯ä¸€ä¸ªåŸºäºç»Ÿä¸€ç»“æ„åŒ–çŸ¥è¯†æ¨ç†ï¼ˆUSKRï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»“æ„åŒ–æ•°æ®æºå›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚</li>
<li>Pandoraåˆ©ç”¨Pythonçš„Pandas APIæ„å»ºç»Ÿä¸€çŸ¥è¯†è¡¨ç¤ºï¼Œä»¥ä¾¿ä¸å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå¯¹é½ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ¨ç†æ­¥éª¤å’Œé’ˆå¯¹æ¯ä¸ªé—®é¢˜çš„å¯æ‰§è¡ŒPythonä»£ç ã€‚</li>
<li>Pandoraæ”¯æŒçŸ¥è¯†è½¬ç§»ï¼Œå…¶è®­ç»ƒç¤ºä¾‹æ¶µç›–å„ç§SKRä»»åŠ¡ã€‚</li>
<li>è¿›è¡Œäº†å››é¡¹åŸºå‡†æµ‹è¯•ï¼Œæ¶‰åŠä¸‰ç§SKRä»»åŠ¡ï¼Œç»“æœè¡¨æ˜Pandoraåœ¨å¤šä¸ªç»Ÿä¸€æ¡†æ¶ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>Pandoraèƒ½ä¸ä»»åŠ¡ç‰¹å®šçš„æ–¹æ³•æœ‰æ•ˆç«äº‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b429e954d9dcebabe41078e6f00761f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-acaa9f3dd6a42ceccec3ad2b18a082aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a09c8faed87a5e4874d817c29e24076.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-910ee8930f9a3f2d540062a4db969108.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d37eaf21eeec2aa393979a9bd641bda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-601faaf6fa1fbee490a297c464fffc83.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Embodied-R-Collaborative-Framework-for-Activating-Embodied-Spatial-Reasoning-in-Foundation-Models-via-Reinforcement-Learning"><a href="#Embodied-R-Collaborative-Framework-for-Activating-Embodied-Spatial-Reasoning-in-Foundation-Models-via-Reinforcement-Learning" class="headerlink" title="Embodied-R: Collaborative Framework for Activating Embodied Spatial   Reasoning in Foundation Models via Reinforcement Learning"></a>Embodied-R: Collaborative Framework for Activating Embodied Spatial   Reasoning in Foundation Models via Reinforcement Learning</h2><p><strong>Authors:Baining Zhao, Ziyou Wang, Jianjie Fang, Chen Gao, Fanhang Man, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li, Wenwu Zhu</strong></p>
<p>Humans can perceive and reason about spatial relationships from sequential visual observations, such as egocentric video streams. However, how pretrained models acquire such abilities, especially high-level reasoning, remains unclear. This paper introduces Embodied-R, a collaborative framework combining large-scale Vision-Language Models (VLMs) for perception and small-scale Language Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a novel reward system considering think-answer logical consistency, the model achieves slow-thinking capabilities with limited computational resources. After training on only 5k embodied video samples, Embodied-R with a 3B LM matches state-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on both in-distribution and out-of-distribution embodied spatial reasoning tasks. Embodied-R also exhibits emergent thinking patterns such as systematic analysis and contextual integration. We further explore research questions including response length, training on VLM, strategies for reward design, and differences in model generalization after SFT (Supervised Fine-Tuning) and RL training. </p>
<blockquote>
<p>äººç±»èƒ½å¤Ÿä»è¿ç»­çš„è§†è§‰è§‚å¯Ÿï¼ˆå¦‚ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘æµï¼‰ä¸­æ„ŸçŸ¥å’Œæ¨ç†ç©ºé—´å…³ç³»ã€‚ç„¶è€Œï¼Œå…³äºé¢„è®­ç»ƒæ¨¡å‹å¦‚ä½•è·å¾—è¿™ç§èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œä»ç„¶ä¸æ¸…æ¥šã€‚æœ¬æ–‡ä»‹ç»äº†Embodied-Rï¼Œè¿™æ˜¯ä¸€ä¸ªåä½œæ¡†æ¶ï¼Œç»“åˆäº†å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œæ„ŸçŸ¥å’Œå°è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰è¿›è¡Œæ¨ç†ã€‚ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œè€ƒè™‘æ€è€ƒ-å›ç­”é€»è¾‘ä¸€è‡´æ€§çš„æ–°å‹å¥–åŠ±ç³»ç»Ÿï¼Œè¯¥æ¨¡å‹åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å®ç°äº†ç¼“æ…¢æ€è€ƒçš„èƒ½åŠ›ã€‚ä»…åœ¨5000ä¸ªå®ä½“è§†é¢‘æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒåï¼ŒEmbodied-Rä¸åŒ…å«3Bçš„LMåŒ¹é…äº†æœ€å…ˆè¿›çš„æ¨¡æ€æ¨ç†æ¨¡å‹ï¼ˆOpenAI-o1ã€Gemini-2.5-proï¼‰åœ¨å®ä½“ç©ºé—´å†…çš„æ¨ç†ä»»åŠ¡ï¼ˆåŒ…æ‹¬åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–ä»»åŠ¡ï¼‰ã€‚Embodied-Rè¿˜æ˜¾ç¤ºå‡ºç³»ç»Ÿçš„åˆ†æå’Œä¸Šä¸‹æ–‡é›†æˆç­‰æ–°å…´çš„æ€è€ƒæ¨¡å¼ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢è®¨äº†åŒ…æ‹¬ç­”æ¡ˆé•¿åº¦ã€åœ¨VLMä¸Šè¿›è¡Œè®­ç»ƒã€å¥–åŠ±è®¾è®¡ç­–ç•¥ä»¥åŠç»è¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒRLè®­ç»ƒåæ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å·®å¼‚ç­‰ç ”ç©¶é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12680v1">PDF</a> 12 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºEmbodied-Rçš„åä½œæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œæ„ŸçŸ¥å’Œå°è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è¿›è¡Œæ¨ç†ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ–°å‹å¥–åŠ±ç³»ç»Ÿï¼Œè¯¥æ¨¡å‹åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å®ç°äº†æ…¢æ€è€ƒèƒ½åŠ›ã€‚è®­ç»ƒä»…ä½¿ç”¨5kä¸ªåµŒå…¥å¼è§†é¢‘æ ·æœ¬ï¼ŒEmbodied-Rä¸æœ€å…ˆè¿›çš„å¤šåª’ä½“æ¨ç†æ¨¡å‹åœ¨å†…éƒ¨å’Œè·¨åˆ†å¸ƒçš„ç©ºé—´æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ç›¸å½“ï¼Œå¹¶å±•ç°å‡ºç³»ç»Ÿåˆ†æå’Œä¸Šä¸‹æ–‡é›†æˆç­‰çªå‘æ€è€ƒæ¨¡å¼ã€‚åŒæ—¶ï¼Œæ–‡ç« æ¢è®¨äº†å“åº”é•¿åº¦ã€VLMè®­ç»ƒã€å¥–åŠ±è®¾è®¡ç­–ç•¥ä»¥åŠç»è¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒRLè®­ç»ƒåçš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®å¼‚ç­‰é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Embodied-Ræ¡†æ¶ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œè¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ï¼Œç”¨äºå¤„ç†ä»è¿ç»­è§†è§‰è§‚å¯Ÿä¸­çš„ç©ºé—´å…³ç³»ç†è§£å’Œæ¨ç†ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œæ–°å‹å¥–åŠ±ç³»ç»Ÿï¼ŒEmbodied-Rå®ç°äº†æ…¢æ€è€ƒèƒ½åŠ›ï¼Œåœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹è¡¨ç°å‡ºè‰²ã€‚</li>
<li>Embodied-Råœ¨ä»…ä½¿ç”¨5kä¸ªåµŒå…¥å¼è§†é¢‘æ ·æœ¬è¿›è¡Œè®­ç»ƒåï¼Œè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›çš„å¤šåª’ä½“æ¨ç†æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>Embodied-Rå±•ç°å‡ºç³»ç»Ÿåˆ†æå’Œä¸Šä¸‹æ–‡é›†æˆç­‰çªå‘æ€è€ƒæ¨¡å¼ã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†å“åº”é•¿åº¦å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä»¥åŠVLMè®­ç»ƒçš„é‡è¦æ€§ã€‚</li>
<li>å¥–åŠ±è®¾è®¡ç­–ç•¥åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce253a166b12aa68e4f0edcc0c741000.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87529659dc3460c3e4fbfc525475b77f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f4e8bce95390240e0da37c42c69ac78.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VLMGuard-R1-Proactive-Safety-Alignment-for-VLMs-via-Reasoning-Driven-Prompt-Optimization"><a href="#VLMGuard-R1-Proactive-Safety-Alignment-for-VLMs-via-Reasoning-Driven-Prompt-Optimization" class="headerlink" title="VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven   Prompt Optimization"></a>VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven   Prompt Optimization</h2><p><strong>Authors:Menglan Chen, Xianghe Pang, Jingjing Dong, WenHao Wang, Yaxin Du, Siheng Chen</strong></p>
<p>Aligning Vision-Language Models (VLMs) with safety standards is essential to mitigate risks arising from their multimodal complexity, where integrating vision and language unveils subtle threats beyond the reach of conventional safeguards. Inspired by the insight that reasoning across modalities is key to preempting intricate vulnerabilities, we propose a novel direction for VLM safety: multimodal reasoning-driven prompt rewriting. To this end, we introduce VLMGuard-R1, a proactive framework that refines user inputs through a reasoning-guided rewriter, dynamically interpreting text-image interactions to deliver refined prompts that bolster safety across diverse VLM architectures without altering their core parameters. To achieve this, we devise a three-stage reasoning pipeline to synthesize a dataset that trains the rewriter to infer subtle threats, enabling tailored, actionable responses over generic refusals. Extensive experiments across three benchmarks with five VLMs reveal that VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1 achieves a remarkable 43.59% increase in average safety across five models on the SIUO benchmark. </p>
<blockquote>
<p>å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸å®‰å…¨æ ‡å‡†å¯¹é½å¯¹äºå‡è½»ç”±äºå®ƒä»¬çš„å¤šæ¨¡æ€å¤æ‚æ€§äº§ç”Ÿçš„é£é™©è‡³å…³é‡è¦ã€‚åœ¨è¿™ç§å¤æ‚æ€§ä¸­ï¼Œèåˆè§†è§‰å’Œè¯­è¨€ä¼šæ­ç¤ºè¶…å‡ºä¼ ç»Ÿå®‰å…¨ä¿æŠ¤èŒƒå›´çš„å¾®å¦™å¨èƒã€‚é€šè¿‡è·¨æ¨¡æ€æ¨ç†æ˜¯é¢„é˜²å¤æ‚æ¼æ´çš„å…³é”®è¿™ä¸€è§è§£çš„å¯å‘ï¼Œæˆ‘ä»¬ä¸ºVLMå®‰å…¨æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ–¹å‘ï¼šä»¥å¤šæ¨¡æ€æ¨ç†é©±åŠ¨æç¤ºé‡å†™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†VLMGuard-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸»åŠ¨æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ¨ç†å¼•å¯¼çš„é‡å†™å™¨æ¥ç²¾ç‚¼ç”¨æˆ·è¾“å…¥ï¼ŒåŠ¨æ€è§£é‡Šæ–‡æœ¬å›¾åƒäº¤äº’ï¼Œä»¥æä¾›ç²¾ç‚¼çš„æç¤ºï¼ŒåŠ å¼ºä¸åŒVLMæ¶æ„çš„å®‰å…¨æ€§è€Œä¸æ”¹å˜å…¶æ ¸å¿ƒå‚æ•°ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸‰é˜¶æ®µæ¨ç†ç®¡é“æ¥åˆæˆä¸€ä¸ªæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†è®­ç»ƒé‡å†™å™¨ä»¥æ¨æ–­å¾®å¦™çš„å¨èƒï¼Œä»è€Œåœ¨é€šç”¨æ‹’ç»ä¹‹å¤–å®ç°é‡èº«å®šåˆ¶ã€å¯æ“ä½œçš„å“åº”ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†ä¸Šå¯¹äº”ä¸ªVLMè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVLMGuard-R1ä¼˜äºå››ä¸ªåŸºå‡†ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œåœ¨SIUOåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVLMGuard-R1åœ¨äº”ä¸ªæ¨¡å‹ä¸Šçš„å¹³å‡å®‰å…¨æ€§æé«˜äº†æƒŠäººçš„43.59%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12661v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸å®‰å…¨æ ‡å‡†å¯¹é½çš„é‡è¦æ€§ï¼Œä»¥ç¼“è§£å¤šæ¨¡æ€å¤æ‚æ€§å¸¦æ¥çš„é£é™©ã€‚ä¸ºåº”å¯¹å¤æ‚çš„æ¼æ´å’Œå¾®å¦™çš„å¨èƒï¼Œæå‡ºä¸€ç§åŸºäºå¤šæ¨¡æ€æ¨ç†çš„æç¤ºé‡å†™æ–¹å‘ï¼Œå¹¶ä»‹ç»VLMGuard-R1æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨ç†å¼•å¯¼çš„é‡å†™å™¨ä¼˜åŒ–ç”¨æˆ·è¾“å…¥ï¼Œé€šè¿‡è§£é‡Šæ–‡æœ¬å›¾åƒäº¤äº’ï¼Œæä¾›ç²¾ç‚¼çš„æç¤ºï¼Œå¢å¼ºä¸åŒVLMæ¶æ„çš„å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¸æ”¹å˜å…¶æ ¸å¿ƒå‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒVLMGuard-R1åœ¨ä¸‰ä¸ªåŸºå‡†ä¸Šè¡¨ç°å‡ºå‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶åœ¨SIUOåŸºå‡†ä¸Šå®ç°äº†å¹³å‡å®‰å…¨æ€§æé«˜43.59%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åº”ä¸å®‰å…¨æ ‡å‡†å¯¹é½ä»¥åº”å¯¹å¤šæ¨¡æ€é£é™©ã€‚</li>
<li>å¤šæ¨¡æ€æ¨ç†æ˜¯é¢„é˜²å¤æ‚å’Œå¾®å¦™å¨èƒçš„å…³é”®ã€‚</li>
<li>VLMGuard-R1æ¡†æ¶é€šè¿‡æ¨ç†å¼•å¯¼çš„é‡å†™å™¨ä¼˜åŒ–ç”¨æˆ·è¾“å…¥ã€‚</li>
<li>VLMGuard-R1èƒ½å¤Ÿè§£é‡Šæ–‡æœ¬å›¾åƒäº¤äº’ï¼Œæä¾›ç²¾ç‚¼çš„æç¤ºã€‚</li>
<li>VLMGuard-R1åœ¨ä¸æ”¹å˜VLMæ ¸å¿ƒå‚æ•°çš„æƒ…å†µä¸‹å¢å¼ºå…¶å®‰å…¨æ€§ã€‚</li>
<li>VLMGuard-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-09db06783d090fed854711cde6f3a621.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f5d69e20826c08da412f433d4330b04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41762f5c615a8fbc9d4c7770a5ddb2a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3202f62c2b00fc9362cfafbf8af2d724.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GeoSense-Evaluating-Identification-and-Application-of-Geometric-Principles-in-Multimodal-Reasoning"><a href="#GeoSense-Evaluating-Identification-and-Application-of-Geometric-Principles-in-Multimodal-Reasoning" class="headerlink" title="GeoSense: Evaluating Identification and Application of Geometric   Principles in Multimodal Reasoning"></a>GeoSense: Evaluating Identification and Application of Geometric   Principles in Multimodal Reasoning</h2><p><strong>Authors:Liangyu Xu, Yingxiu Zhao, Jingyun Wang, Yingyao Wang, Bu Pi, Chen Wang, Mingliang Zhang, Jihao Gu, Xiang Li, Xiaoyong Zhu, Jun Song, Bo Zheng</strong></p>
<p>Geometry problem-solving (GPS), a challenging task requiring both visual comprehension and symbolic reasoning, effectively measures the reasoning capabilities of multimodal large language models (MLLMs). Humans exhibit strong reasoning ability in this task through accurate identification and adaptive application of geometric principles within visual contexts. However, existing benchmarks fail to jointly assess both dimensions of the human-like geometric reasoning mechanism in MLLMs, remaining a critical gap in assessing their ability to tackle GPS. To this end, we introduce GeoSense, the first comprehensive bilingual benchmark designed to systematically evaluate the geometric reasoning abilities of MLLMs through the lens of geometric principles. GeoSense features a five-level hierarchical framework of geometric principles spanning plane and solid geometry, an intricately annotated dataset of 1,789 problems, and an innovative evaluation strategy. Through extensive experiments on GeoSense with various open-source and closed-source MLLMs, we observe that Gemini-2.0-pro-flash performs best, achieving an overall score of $65.3$. Our in-depth analysis reveals that the identification and application of geometric principles remain a bottleneck for leading MLLMs, jointly hindering their reasoning abilities. These findings underscore GeoSenseâ€™s potential to guide future advancements in MLLMsâ€™ geometric reasoning capabilities, paving the way for more robust and human-like reasoning in artificial intelligence. </p>
<blockquote>
<p>å‡ ä½•é—®é¢˜è§£å†³ï¼ˆGPSï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¦æ±‚è§†è§‰ç†è§£å’Œç¬¦å·æ¨ç†èƒ½åŠ›ï¼Œå®ƒæœ‰æ•ˆåœ°è¡¡é‡äº†å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚äººç±»åœ¨æ­¤ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨è§†è§‰ç¯å¢ƒä¸­å‡†ç¡®è¯†åˆ«å¹¶çµæ´»åº”ç”¨å‡ ä½•åŸç†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•æœªèƒ½è”åˆè¯„ä¼°äººç±»å¼å‡ ä½•æ¨ç†æœºåˆ¶çš„ä¸¤ä¸ªæ–¹é¢åœ¨MLLMsä¸­çš„è¡¨ç°ï¼Œè¿™åœ¨è¯„ä¼°MLLMså¤„ç†GPSçš„èƒ½åŠ›æ–¹é¢å­˜åœ¨å…³é”®å·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†GeoSenseï¼Œè¿™æ˜¯é¦–ä¸ªå…¨é¢çš„åŒè¯­åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡å‡ ä½•åŸç†çš„é€é•œç³»ç»Ÿåœ°è¯„ä¼°MLLMsçš„å‡ ä½•æ¨ç†èƒ½åŠ›ã€‚GeoSenseçš„ç‰¹ç‚¹æ˜¯ä¸€ä¸ªæ¶µç›–å¹³é¢å’Œç«‹ä½“å‡ ä½•çš„äº”çº§å±‚æ¬¡åŒ–å‡ ä½•åŸç†æ¡†æ¶ã€ä¸€ä¸ªç²¾å¿ƒæ ‡æ³¨çš„åŒ…å«1789ä¸ªé—®é¢˜çš„æ•°æ®é›†ä»¥åŠåˆ›æ–°çš„è¯„ä¼°ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨GeoSenseä¸Šä¸å„ç§å¼€æºå’Œé—­æºçš„MLLMsè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå‘ç°Gemini-2.0-pro-flashè¡¨ç°æœ€ä½³ï¼Œæ€»ä½“å¾—åˆ†ä¸º65.3åˆ†ã€‚æˆ‘ä»¬çš„æ·±å…¥åˆ†æè¡¨æ˜ï¼Œå‡ ä½•åŸç†çš„è¯†åˆ«å’Œåº”ç”¨ä»ç„¶æ˜¯é¢†å…ˆMLLMsçš„ç“¶é¢ˆï¼Œå…±åŒåˆ¶çº¦äº†å®ƒä»¬çš„æ¨ç†èƒ½åŠ›ã€‚è¿™äº›å‘ç°çªæ˜¾äº†GeoSenseåœ¨æŒ‡å¯¼æœªæ¥MLLMså‡ ä½•æ¨ç†èƒ½åŠ›è¿›æ­¥æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºäººå·¥æ™ºèƒ½ä¸­æ›´ç¨³å¥å’Œäººç±»å¼çš„æ¨ç†é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12597v1">PDF</a> 10 pages, 8 figures</p>
<p><strong>Summary</strong><br>å‡ ä½•é—®é¢˜è§£å†³ï¼ˆGPSï¼‰æ˜¯è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¨ç†èƒ½åŠ›çš„é‡è¦ä»»åŠ¡ã€‚äººç±»æ“…é•¿é€šè¿‡è§†è§‰ä¸Šä¸‹æ–‡å‡†ç¡®è¯†åˆ«å¹¶çµæ´»åº”ç”¨å‡ ä½•åŸç†æ¥è§£å†³æ­¤ç±»é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½å…¨é¢è¯„ä¼°MLLMsçš„å‡ ä½•æ¨ç†æœºåˆ¶ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºGeoSenseï¼Œé¦–ä¸ªå…¨é¢è¯„ä¼°MLLMså‡ ä½•æ¨ç†èƒ½åŠ›çš„åŒè¯­åŸºå‡†æµ‹è¯•ã€‚GeoSenseåŒ…å«äº”ä¸ªå±‚æ¬¡çš„å‡ ä½•åŸç†æ¡†æ¶ã€ç²¾å¿ƒæ ‡æ³¨çš„1789ä¸ªé—®é¢˜é›†å’Œæ–°é¢–çš„è¯„ä»·ç­–ç•¥ã€‚é€šè¿‡å®éªŒå‘ç°ï¼ŒGemini-2.0-pro-flashè¡¨ç°æœ€ä½³ï¼Œæ€»ä½“å¾—åˆ†65.3ã€‚åˆ†ææ˜¾ç¤ºï¼Œè¯†åˆ«å’Œåº”ç”¨å‡ ä½•åŸç†ä»æ˜¯MLLMsçš„ç“¶é¢ˆã€‚GeoSenseæœ‰æœ›ä¸ºæœªæ¥MLLMsçš„å‡ ä½•æ¨ç†èƒ½åŠ›å‘å±•æä¾›æŒ‡å¯¼ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½å®ç°æ›´ç¨³å¥ã€äººæ€§åŒ–æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡ ä½•é—®é¢˜è§£å†³ï¼ˆGPSï¼‰æ˜¯è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>äººç±»é€šè¿‡è§†è§‰ä¸Šä¸‹æ–‡å‡†ç¡®è¯†åˆ«å¹¶çµæ´»åº”ç”¨å‡ ä½•åŸç†è§£å†³GPSé—®é¢˜ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½å…¨é¢è¯„ä¼°MLLMsçš„å‡ ä½•æ¨ç†èƒ½åŠ›ï¼Œå­˜åœ¨å…³é”®å·®è·ã€‚</li>
<li>GeoSenseæ˜¯é¦–ä¸ªå…¨é¢è¯„ä¼°MLLMså‡ ä½•æ¨ç†èƒ½åŠ›çš„åŒè¯­åŸºå‡†æµ‹è¯•ã€‚</li>
<li>GeoSenseåŒ…å«äº”ä¸ªå±‚æ¬¡çš„å‡ ä½•åŸç†æ¡†æ¶ã€1789ä¸ªé—®é¢˜é›†å’Œæ–°é¢–çš„è¯„ä»·ç­–ç•¥ã€‚</li>
<li>åœ¨GeoSenseæµ‹è¯•ä¸­ï¼ŒGemini-2.0-pro-flashè¡¨ç°æœ€ä½³ï¼Œæ€»ä½“å¾—åˆ†65.3ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c31331ddc0bbb22d8c068fec5a1170b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-084f6b9eb3ec35d991535c3e11b8b43f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31de1853f9282f0dd18f23fc3857f866.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-664ac3d9488f88252f52eaaf2eba4540.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99fd8b59dafce93a69114f519c19770b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eb40742a126b234b426bcb997cc2fe8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4633cd1412f0c7e125f97a3757d8b7ec.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="ELAB-Extensive-LLM-Alignment-Benchmark-in-Persian-Language"><a href="#ELAB-Extensive-LLM-Alignment-Benchmark-in-Persian-Language" class="headerlink" title="ELAB: Extensive LLM Alignment Benchmark in Persian Language"></a>ELAB: Extensive LLM Alignment Benchmark in Persian Language</h2><p><strong>Authors:Zahra Pourbahman, Fatemeh Rajabi, Mohammadhossein Sadeghi, Omid Ghahroodi, Somaye Bakhshaei, Arash Amini, Reza Kazemi, Mahdieh Soleymani Baghshah</strong></p>
<p>This paper presents a comprehensive evaluation framework for aligning Persian Large Language Models (LLMs) with critical ethical dimensions, including safety, fairness, and social norms. It addresses the gaps in existing LLM evaluation frameworks by adapting them to Persian linguistic and cultural contexts. This benchmark creates three types of Persian-language benchmarks: (i) translated data, (ii) new data generated synthetically, and (iii) new naturally collected data. We translate Anthropic Red Teaming data, AdvBench, HarmBench, and DecodingTrust into Persian. Furthermore, we create ProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets to address harmful and prohibited content in indigenous culture. Moreover, we collect extensive dataset as GuardBench-fa to consider Persian cultural norms. By combining these datasets, our work establishes a unified framework for evaluating Persian LLMs, offering a new approach to culturally grounded alignment evaluation. A systematic evaluation of Persian LLMs is performed across the three alignment aspects: safety (avoiding harmful content), fairness (mitigating biases), and social norms (adhering to culturally accepted behaviors). We present a publicly available leaderboard that benchmarks Persian LLMs with respect to safety, fairness, and social norms at: <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation">https://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå°†æ³¢æ–¯è¯­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å…³é”®ä¼¦ç†ç»´åº¦ï¼ˆåŒ…æ‹¬å®‰å…¨ã€å…¬å¹³å’Œç¤¾ä¼šè§„èŒƒï¼‰å¯¹é½ã€‚å®ƒé€šè¿‡é€‚åº”æ³¢æ–¯è¯­çš„è¯­è¨€å’Œæ–‡åŒ–èƒŒæ™¯æ¥å¼¥è¡¥ç°æœ‰LLMè¯„ä¼°æ¡†æ¶çš„ç©ºç™½ã€‚æ­¤åŸºå‡†æµ‹è¯•åˆ›å»ºäº†ä¸‰ç§æ³¢æ–¯è¯­åŸºå‡†æµ‹è¯•ï¼šä¸€æ˜¯ç¿»è¯‘æ•°æ®ï¼ŒäºŒæ˜¯åˆæˆç”Ÿæˆçš„æ–°æ•°æ®ï¼Œä¸‰æ˜¯æ–°æ”¶é›†çš„è‡ªç„¶æ•°æ®ã€‚æˆ‘ä»¬å°†Anthropic Red Teamingæ•°æ®ã€AdvBenchã€HarmBenchå’ŒDecodingTrustç¿»è¯‘æˆæ³¢æ–¯è¯­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ›å»ºäº†ProhibiBench-faã€SafeBench-faã€FairBench-faå’ŒSocialBench-faç­‰æ–°æ•°æ®é›†ï¼Œä»¥è§£å†³æœ¬åœŸæ–‡åŒ–ä¸­çš„æœ‰å®³å’Œç¦æ­¢å†…å®¹é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ”¶é›†äº†å¹¿æ³›çš„GuardBench-faæ•°æ®é›†ï¼Œä»¥è€ƒè™‘æ³¢æ–¯æ–‡åŒ–è§„èŒƒã€‚é€šè¿‡ç»“åˆè¿™äº›æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„å·¥ä½œå»ºç«‹äº†è¯„ä¼°æ³¢æ–¯è¯­LLMçš„ç»Ÿä¸€æ¡†æ¶ï¼Œä¸ºåŸºäºæ–‡åŒ–çš„å¯¹é½è¯„ä¼°æä¾›äº†æ–°çš„æ–¹æ³•ã€‚æˆ‘ä»¬å¯¹æ³¢æ–¯LLMè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œæ¶µç›–äº†ä¸‰ä¸ªå¯¹é½æ–¹é¢ï¼šå®‰å…¨ï¼ˆé¿å…æœ‰å®³å†…å®¹ï¼‰ã€å…¬å¹³ï¼ˆå‡è½»åè§ï¼‰å’Œç¤¾ä¼šè§„èŒƒï¼ˆéµå®ˆæ–‡åŒ–å¯æ¥å—çš„è¡Œä¸ºï¼‰ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation">https://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation</a>ä¸Šå±•ç¤ºäº†ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„æ’è¡Œæ¦œï¼Œè¯¥æ’è¡Œæ¦œå¯¹æ³¢æ–¯LLMåœ¨å®‰å…¨ã€å…¬å¹³å’Œç¤¾ä¼šè§„èŒƒæ–¹é¢è¿›è¡Œäº†è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12553v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ­¤è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è®©æ³¢æ–¯è¯­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å…³é”®çš„ä¼¦ç†ç»´åº¦ï¼ˆåŒ…æ‹¬å®‰å…¨ã€å…¬å¹³å’Œç¤¾ä¼šè§„èŒƒï¼‰ä¿æŒä¸€è‡´ã€‚é€šè¿‡ç¿»è¯‘å’Œåˆ›å»ºå¤šç§æ³¢æ–¯è¯­æ•°æ®é›†ï¼Œè¯¥è®ºæ–‡å¡«è¡¥äº†ç°æœ‰LLMè¯„ä¼°æ¡†æ¶çš„ç©ºç™½ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥è¯„ä¼°æ³¢æ–¯è¯­LLMï¼Œä¸ºæ–‡åŒ–æ ¹æ¤çš„å¯¹é½è¯„ä¼°æä¾›äº†æ–°çš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶è¿˜å¯¹æ³¢æ–¯è¯­LLMè¿›è¡Œäº†ç³»ç»Ÿæ€§çš„å®‰å…¨ã€å…¬å¹³å’Œç¤¾ä¼šè§„èŒƒæ–¹é¢çš„è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è®©æ³¢æ–¯è¯­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼¦ç†ç»´åº¦ï¼ˆåŒ…æ‹¬å®‰å…¨ã€å…¬å¹³å’Œç¤¾ä¼šè§„èŒƒï¼‰ä¸Šå¯¹é½ã€‚</li>
<li>é€šè¿‡é€‚åº”æ³¢æ–¯è¯­çš„è¯­è¨€æ–‡åŒ–èƒŒæ™¯ï¼Œå¡«è¡¥äº†ç°æœ‰LLMè¯„ä¼°æ¡†æ¶çš„ç©ºç™½ã€‚</li>
<li>åˆ›å»ºäº†å¤šç§æ³¢æ–¯è¯­æ•°æ®é›†ï¼ŒåŒ…æ‹¬ç¿»è¯‘æ•°æ®ã€æ–°åˆæˆæ•°æ®å’Œæ–°è‡ªç„¶æ”¶é›†æ•°æ®ã€‚</li>
<li>é¦–æ¬¡åˆ›å»ºäº†é’ˆå¯¹æœ‰å®³å’Œç¦æ­¢å†…å®¹çš„æ³¢æ–¯è¯­æ•°æ®é›†ï¼Œä»¥ç¬¦åˆå½“åœ°æ–‡åŒ–è§„èŒƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆè¿™äº›æ•°æ®é›†æ¥è¯„ä¼°æ³¢æ–¯è¯­LLMã€‚</li>
<li>å¯¹æ³¢æ–¯è¯­LLMè¿›è¡Œäº†å®‰å…¨ã€å…¬å¹³å’Œç¤¾ä¼šè§„èŒƒæ–¹é¢çš„ç³»ç»Ÿæ€§è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-262b772d0129dd74940d941966562722.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67a673494038aeec0084c61f57998784.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fe5e10ea72c5af0e6ef85f040c17791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd779cc680369ffa379f36067e07e3f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d10419aad2a2803115b56f55f99b3172.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-604e1ecf97d15b5aabf7a1e06177b3a9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Diversity-and-Quality-of-LLM-Generated-Content"><a href="#Evaluating-the-Diversity-and-Quality-of-LLM-Generated-Content" class="headerlink" title="Evaluating the Diversity and Quality of LLM Generated Content"></a>Evaluating the Diversity and Quality of LLM Generated Content</h2><p><strong>Authors:Alexander Shypula, Shuo Li, Botong Zhang, Vishakh Padmakumar, Kayo Yin, Osbert Bastani</strong></p>
<p>Recent work suggests that preference-tuning techniquesâ€“including Reinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO, as well as alternatives like DPOâ€“reduce diversity, creating a dilemma given that such models are widely deployed in applications requiring diverse outputs. To address this, we introduce a framework for measuring effective semantic diversityâ€“diversity among outputs that meet quality thresholdsâ€“which better reflects the practical utility of large language models (LLMs). Using open-ended tasks that require no human intervention, we find counterintuitive results: although preference-tuned modelsâ€“especially those trained via RLâ€“exhibit reduced lexical and syntactic diversity, they produce greater effective semantic diversity than SFT or base models, not from increasing diversity among high-quality outputs, but from generating more high-quality outputs overall. We discover that preference tuning reduces syntactic diversity while preserving semantic diversityâ€“revealing a distinction between diversity in form and diversity in content that traditional metrics often overlook. Our analysis further shows that smaller models are consistently more parameter-efficient at generating unique content within a fixed sampling budget, offering insights into the relationship between model scaling and diversity. These findings have important implications for applications that require diverse yet high-quality outputs, from creative assistance to synthetic data generation. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåå¥½è°ƒæ•´æŠ€æœ¯â€”â€”åŒ…æ‹¬åŸºäºäººç±»åå¥½åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ–¹æ³•ï¼Œå¦‚PPOå’ŒGRPOï¼Œä»¥åŠDPOç­‰æ›¿ä»£æ–¹æ³•â€”â€”ä¼šå‡å°‘å¤šæ ·æ€§ï¼Œè¿™æ„æˆäº†ä¸€ä¸ªå›°å¢ƒï¼Œå› ä¸ºè¿™äº›æ¨¡å‹å¹¿æ³›åº”ç”¨äºéœ€è¦å¤šæ ·åŒ–è¾“å‡ºçš„åº”ç”¨ç¨‹åºä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¡¡é‡æœ‰æ•ˆè¯­ä¹‰å¤šæ ·æ€§çš„æ¡†æ¶â€”â€”å³æ»¡è¶³è´¨é‡é˜ˆå€¼çš„è¾“å‡ºä¹‹é—´çš„å¤šæ ·æ€§â€”â€”è¿™ä¸ªæ¡†æ¶èƒ½æ›´å¥½åœ°åæ˜ å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®é™…æ•ˆç”¨ã€‚é€šè¿‡ä½¿ç”¨ä¸éœ€è¦äººå·¥å¹²é¢„çš„å¼€æ”¾å¼ä»»åŠ¡ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€äº›æ„æƒ³ä¸åˆ°çš„ç»“æœï¼šå°½ç®¡åå¥½è°ƒæ•´æ¨¡å‹â€”â€”ç‰¹åˆ«æ˜¯é‚£äº›ç»è¿‡RLè®­ç»ƒçš„æ¨¡å‹â€”â€”è¡¨ç°å‡ºè¯æ±‡å’Œè¯­æ³•å¤šæ ·æ€§çš„å‡å°‘ï¼Œä½†å®ƒä»¬äº§ç”Ÿçš„æœ‰æ•ˆè¯­ä¹‰å¤šæ ·æ€§å´å¤§äºSFTæˆ–åŸºç¡€æ¨¡å‹ã€‚è¿™å¹¶éæ¥è‡ªé«˜è´¨é‡è¾“å‡ºä¹‹é—´çš„å¤šæ ·æ€§å¢åŠ ï¼Œè€Œæ˜¯æ¥è‡ªæ€»ä½“ä¸Šç”Ÿæˆäº†æ›´å¤šçš„é«˜è´¨é‡è¾“å‡ºã€‚æˆ‘ä»¬å‘ç°åå¥½è°ƒæ•´æŠ€æœ¯è™½ç„¶å‡å°‘äº†è¯­æ³•å¤šæ ·æ€§ï¼Œä½†ä¿ç•™äº†è¯­ä¹‰å¤šæ ·æ€§â€”â€”æ­ç¤ºäº†å½¢å¼ä¸Šçš„å¤šæ ·æ€§å’Œå†…å®¹ä¸Šçš„å¤šæ ·æ€§ä¹‹é—´çš„åŒºåˆ«ï¼Œè¿™æ˜¯ä¼ ç»Ÿåº¦é‡æ ‡å‡†ç»å¸¸å¿½ç•¥çš„ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜è¡¨æ˜ï¼Œåœ¨å›ºå®šçš„é‡‡æ ·é¢„ç®—å†…ï¼Œè¾ƒå°çš„æ¨¡å‹åœ¨ç”Ÿæˆç‹¬ç‰¹å†…å®¹æ–¹é¢å§‹ç»ˆå…·æœ‰æ›´é«˜çš„å‚æ•°æ•ˆç‡ï¼Œè¿™ä¸ºæˆ‘ä»¬æä¾›äº†æ¨¡å‹æ‰©å±•ä¸å¤šæ ·æ€§ä¹‹é—´çš„å…³ç³»æ´å¯Ÿã€‚è¿™äº›å‘ç°å¯¹äºéœ€è¦å¤šæ ·ä¸”é«˜è´¨é‡è¾“å‡ºçš„åº”ç”¨ç¨‹åºï¼Œå¦‚åˆ›æ„è¾…åŠ©å’Œåˆæˆæ•°æ®ç”Ÿæˆç­‰ï¼Œå…·æœ‰é‡è¦çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12522v1">PDF</a> ICLR 2025 Third Workshop on Deep Learning for Code</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æ¢è®¨äº†ä¸€ç§æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¡¡é‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰æ•ˆè¯­ä¹‰å¤šæ ·æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡åå¥½è°ƒæ•´æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ¨¡å‹ï¼‰åœ¨è¯æ±‡å’Œå¥æ³•å¤šæ ·æ€§æ–¹é¢è¡¨ç°å‡å°‘ï¼Œä½†å®ƒä»¬äº§ç”Ÿæ›´å¤§çš„æœ‰æ•ˆè¯­ä¹‰å¤šæ ·æ€§ã€‚è¿™ä¸»è¦æºäºç”Ÿæˆæ›´å¤šé«˜è´¨é‡è¾“å‡ºçš„æ€»ä½“æ•°é‡ï¼Œè€Œéå•ä¸€é«˜è´¨é‡è¾“å‡ºçš„å¤šæ ·æ€§ã€‚ç ”ç©¶è¿˜æŒ‡å‡ºæ¨¡å‹è§„æ¨¡ä¸å¤šæ ·æ€§ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å¼ºè°ƒä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡å¸¸å¸¸å¿½è§†å½¢å¼ä¸å†…å®¹çš„åŒºåˆ«ã€‚è¿™äº›å‘ç°å¯¹äºéœ€è¦å¤šæ ·ä¸”é«˜è´¨é‡è¾“å‡ºçš„åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åå¥½è°ƒæ•´æŠ€æœ¯ï¼ˆå¦‚RLHFã€PPOå’ŒGRPOç­‰ï¼‰è™½ç„¶å‡å°‘æ¨¡å‹çš„è¾“å‡ºå¤šæ ·æ€§ï¼Œä½†é€šè¿‡ç”Ÿæˆæ›´å¤šé«˜è´¨é‡è¾“å‡ºæé«˜äº†æœ‰æ•ˆè¯­ä¹‰å¤šæ ·æ€§ã€‚</li>
<li>æœ‰æ•ˆè¯­ä¹‰å¤šæ ·æ€§è¯„ä¼°æ¡†æ¶èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åæ˜ å¤§å‹è¯­è¨€æ¨¡å‹çš„å®ç”¨æ€§ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ¨¡å‹åœ¨å¥æ³•å¤šæ ·æ€§ä¸Šè¡¨ç°å‡å°‘ï¼Œä½†è¯­ä¹‰å¤šæ ·æ€§å¾—ä»¥ä¿æŒã€‚</li>
<li>ä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡å¾€å¾€å¿½è§†å½¢å¼ä¸å†…å®¹çš„åŒºåˆ«ã€‚</li>
<li>è¾ƒå°æ¨¡å‹åœ¨å›ºå®šé‡‡æ ·é¢„ç®—å†…æ›´å…·å‚æ•°æ•ˆç‡ï¼Œèƒ½ç”Ÿæˆç‹¬ç‰¹çš„å†…å®¹ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡ä¸ç”Ÿæˆå¤šæ ·æ€§ä¹‹é—´å­˜åœ¨å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61552dd23af92d75c5c97804283b29f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb7a7ac66dca101714c55d8bcd086078.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e999f2461b80b21be76d2d413e953212.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Socrates-or-Smartypants-Testing-Logic-Reasoning-Capabilities-of-Large-Language-Models-with-Logic-Programming-based-Test-Oracles"><a href="#Socrates-or-Smartypants-Testing-Logic-Reasoning-Capabilities-of-Large-Language-Models-with-Logic-Programming-based-Test-Oracles" class="headerlink" title="Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large   Language Models with Logic Programming-based Test Oracles"></a>Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large   Language Models with Logic Programming-based Test Oracles</h2><p><strong>Authors:Zihao Xu, Junchen Ding, Yiling Lou, Kun Zhang, Dong Gong, Yuekang Li</strong></p>
<p>Large Language Models (LLMs) have achieved significant progress in language understanding and reasoning. Evaluating and analyzing their logical reasoning abilities has therefore become essential. However, existing datasets and benchmarks are often limited to overly simplistic, unnatural, or contextually constrained examples. In response to the growing demand, we introduce SmartyPat-Bench, a challenging, naturally expressed, and systematically labeled benchmark derived from real-world high-quality Reddit posts containing subtle logical fallacies. Unlike existing datasets and benchmarks, it provides more detailed annotations of logical fallacies and features more diverse data. To further scale up the study and address the limitations of manual data collection and labeling - such as fallacy-type imbalance and labor-intensive annotation - we introduce SmartyPat, an automated framework powered by logic programming-based oracles. SmartyPat utilizes Prolog rules to systematically generate logically fallacious statements, which are then refined into fluent natural-language sentences by LLMs, ensuring precise fallacy representation. Extensive evaluation demonstrates that SmartyPat produces fallacies comparable in subtlety and quality to human-generated content and significantly outperforms baseline methods. Finally, experiments reveal nuanced insights into LLM capabilities, highlighting that while excessive reasoning steps hinder fallacy detection accuracy, structured reasoning enhances fallacy categorization performance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å’Œæ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚å› æ­¤ï¼Œè¯„ä¼°å’Œåˆ†æå®ƒä»¬çš„é€»è¾‘æ¨ç†èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•é€šå¸¸ä»…é™äºè¿‡äºç®€å•ã€ä¸è‡ªç„¶æˆ–ä¸Šä¸‹æ–‡å—é™çš„ç¤ºä¾‹ã€‚ä¸ºæ»¡è¶³æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SmartyPat-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ã€è‡ªç„¶è¡¨è¾¾ã€ç³»ç»Ÿæ ‡æ³¨çš„åŸºå‡†æµ‹è¯•ï¼Œæ¥æºäºç°å®ä¸–ç•Œä¸­é«˜è´¨é‡Redditå¸–å­çš„å¾®å¦™é€»è¾‘è°¬è¯¯ã€‚ä¸ç°æœ‰æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´è¯¦ç»†çš„é€»è¾‘è°¬è¯¯æ³¨é‡Šï¼Œå¹¶ä¸”å…·æœ‰æ›´å¤šæ ·åŒ–çš„æ•°æ®ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ‰©å±•ç ”ç©¶å¹¶è§£å†³æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ ‡æ³¨çš„å±€é™æ€§ï¼ˆä¾‹å¦‚è°¬è¯¯ç±»å‹ä¸å¹³è¡¡å’ŒåŠ³åŠ¨å¯†é›†å‹çš„æ³¨é‡Šï¼‰ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SmartyPatï¼Œè¿™æ˜¯ä¸€ä¸ªç”±é€»è¾‘ç¼–ç¨‹é©±åŠ¨çš„è‡ªåŠ¨åŒ–æ¡†æ¶ã€‚SmartyPatåˆ©ç”¨Prologè§„åˆ™ç³»ç»Ÿåœ°ç”Ÿæˆé€»è¾‘ä¸Šæœ‰é”™è¯¯çš„é™ˆè¿°ï¼Œç„¶åé€šè¿‡LLMå°†å…¶ç²¾ç‚¼æˆæµç•…çš„è‡ªç„¶è¯­è¨€å¥å­ï¼Œç¡®ä¿ç²¾ç¡®è¡¨ç¤ºè°¬è¯¯ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSmartyPatäº§ç”Ÿçš„è°¬è¯¯åœ¨ç»†å¾®æ€§å’Œè´¨é‡æ–¹é¢ä¸äººç±»ç”Ÿæˆçš„å†…å®¹ç›¸å½“ï¼Œå¹¶ä¸”æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æœ€åï¼Œå®éªŒæ­ç¤ºäº†LLMèƒ½åŠ›çš„å¾®å¦™è§è§£ï¼Œé‡ç‚¹è¡¨æ˜è™½ç„¶è¿‡å¤šçš„æ¨ç†æ­¥éª¤ä¼šé˜»ç¢é”™è¯¯æ£€æµ‹ç²¾åº¦ï¼Œä½†ç»“æ„åŒ–æ¨ç†ä¼šæé«˜é”™è¯¯åˆ†ç±»æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12312v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å’Œæ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå› æ­¤å¯¹å®ƒä»¬çš„é€»è¾‘æ¨ç†èƒ½åŠ›è¿›è¡Œè¯„ä¼°å’Œåˆ†æè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•é€šå¸¸å±€é™äºè¿‡äºç®€å•ã€ä¸è‡ªç„¶æˆ–ä¸Šä¸‹æ–‡å—é™çš„ç¤ºä¾‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SmartyPat-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ã€è‡ªç„¶è¡¨è¾¾çš„ã€ç³»ç»Ÿæ ‡æ³¨çš„åŸºå‡†æµ‹è¯•ï¼Œæ¥æºäºç°å®ä¸–ç•Œä¸­é«˜è´¨é‡Redditå¸–å­çš„å¾®å¦™é€»è¾‘è°¬è¯¯ã€‚ä¸ç°æœ‰æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´è¯¦ç»†çš„é€»è¾‘è°¬è¯¯æ³¨é‡Šï¼Œå¹¶ä¸”æ•°æ®æ›´åŠ å¤šæ ·åŒ–ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ‰©å¤§ç ”ç©¶è§„æ¨¡å¹¶è§£å†³æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ ‡æ³¨çš„å±€é™æ€§ï¼ˆå¦‚é€»è¾‘é”™è¯¯ç±»å‹ä¸å¹³è¡¡å’ŒåŠ³åŠ¨å¯†é›†å‹çš„æ ‡æ³¨ï¼‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†SmartyPatï¼Œè¿™æ˜¯ä¸€ä¸ªç”±é€»è¾‘ç¼–ç¨‹æ”¯æŒçš„è‡ªåŠ¨æ¡†æ¶ã€‚SmartyPatåˆ©ç”¨Prologè§„åˆ™ç³»ç»Ÿåœ°ç”Ÿæˆé€»è¾‘ä¸Šæœ‰è¯¯çš„é™ˆè¿°ï¼Œç„¶åé€šè¿‡LLMå°†å…¶è½¬åŒ–ä¸ºæµç•…çš„è‡ªç„¶è¯­è¨€å¥å­ï¼Œç¡®ä¿ç²¾ç¡®è¡¨ç¤ºé€»è¾‘è°¬è¯¯ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒSmartyPatäº§ç”Ÿçš„é€»è¾‘é”™è¯¯ä¸äººç±»ç”Ÿæˆçš„å†…å®¹åœ¨ç»†å¾®æ€§å’Œè´¨é‡ä¸Šç›¸å½“ï¼Œå¹¶æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚æœ€åï¼Œå®éªŒæ­ç¤ºäº†å…³äºLLMèƒ½åŠ›çš„å¾®å¦™è§è§£ï¼Œå¼ºè°ƒè¿‡å¤šçš„æ¨ç†æ­¥éª¤ä¼šé˜»ç¢é€»è¾‘é”™è¯¯çš„æ£€æµ‹å‡†ç¡®æ€§ï¼Œè€Œç»“æ„åŒ–æ¨ç†åˆ™èƒ½æé«˜é€»è¾‘é”™è¯¯çš„åˆ†ç±»æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­è¨€å’Œæ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œéœ€è¦å¯¹å…¶é€»è¾‘æ¨ç†èƒ½åŠ›è¿›è¡Œè¯„ä¼°å’Œåˆ†æã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ï¼Œå¸¸å¸¸è¿‡äºç®€å•æˆ–ä¸Šä¸‹æ–‡å—é™ï¼Œéš¾ä»¥æ»¡è¶³è¯„ä¼°éœ€æ±‚ã€‚</li>
<li>å¼•å…¥SmartyPat-BenchåŸºå‡†æµ‹è¯•ï¼Œæºè‡ªçœŸå®ä¸–ç•Œçš„Redditå¸–å­ï¼ŒåŒ…å«å¾®å¦™çš„é€»è¾‘é”™è¯¯ï¼Œæä¾›æ›´è¯¦ç»†å’Œå¤šæ ·åŒ–çš„æ•°æ®ã€‚</li>
<li>æå‡ºSmartyPatè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨é€»è¾‘ç¼–ç¨‹ç”Ÿæˆé€»è¾‘é”™è¯¯ï¼Œå¹¶ç”±LLMè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€å¥å­ï¼Œç¡®ä¿ç²¾ç¡®è¡¨ç¤ºé€»è¾‘é”™è¯¯ã€‚</li>
<li>SmartyPatè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç”Ÿæˆçš„é€»è¾‘é”™è¯¯ä¸äººç±»å†…å®¹ç›¸å½“ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>å®éªŒè¡¨æ˜è¿‡å¤šçš„æ¨ç†æ­¥éª¤ä¼šå½±å“é€»è¾‘é”™è¯¯çš„æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d8784316bee63f5f6050a512a16888c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b79c1f32f7cf937aaf6082571a0c1dd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67b44bd27eb0e14a5995928326535100.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ReTool-Reinforcement-Learning-for-Strategic-Tool-Use-in-LLMs"><a href="#ReTool-Reinforcement-Learning-for-Strategic-Tool-Use-in-LLMs" class="headerlink" title="ReTool: Reinforcement Learning for Strategic Tool Use in LLMs"></a>ReTool: Reinforcement Learning for Strategic Tool Use in LLMs</h2><p><strong>Authors:Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, Wanjun Zhong</strong></p>
<p>While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the modelâ€™s tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReToolâ€™s superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAIâ€™s o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an â€˜â€™aha momentâ€™â€™ in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems. </p>
<blockquote>
<p>è™½ç„¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„æ¨ç†æ¨¡å‹ï¼ˆä¾‹å¦‚DeepSeek R1ï¼‰åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦ç»“æ„åŒ–é—®é¢˜è§£å†³ï¼ˆå¦‚å‡ ä½•æ¨ç†ã€ç®€æ´è®¡ç®—æˆ–å¤æ‚æ–¹ç¨‹æ±‚è§£ï¼‰çš„åœºæ™¯ä¸­ï¼Œå®ƒä»¬é¢ä¸´æŒ‘æˆ˜ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œä»£ç è§£é‡Šå™¨ï¼ˆCIï¼‰ç­‰è®¡ç®—å·¥å…·æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ReToolã€‚ReToolé€šè¿‡å·¥å…·é›†æˆå­¦ä¹ å¢å¼ºé•¿å½¢å¼æ¨ç†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®åŠŸèƒ½ï¼šï¼ˆ1ï¼‰åœ¨è‡ªç„¶è¯­è¨€æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åœ°å®æ—¶æ‰§è¡Œä»£ç ï¼›ï¼ˆ2ï¼‰ä¸€ç§è‡ªåŠ¨åŒ–çš„RLèŒƒå¼ï¼Œå…è®¸æ ¹æ®ç»“æœåé¦ˆè¿›è¡Œç­–ç•¥æ»šåŠ¨å’Œå¤šå›åˆå®æ—¶ä»£ç æ‰§è¡Œï¼Œå¹¶æ•™å¯¼æ¨¡å‹ä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨å·¥å…·ã€‚ReToolé‡‡ç”¨ç³»ç»Ÿçš„è®­ç»ƒæ¡†æ¶ï¼Œä»ç”Ÿæˆåˆæˆå†·å¯åŠ¨æ•°æ®å¼€å§‹ï¼Œä»¥äº§ç”Ÿç”¨äºå¾®è°ƒåŸºç¡€æ¨¡å‹çš„ä»£ç å¢å¼ºé•¿å½¢å¼æ¨ç†è½¨è¿¹ã€‚éšåçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåˆ©ç”¨ä»»åŠ¡ç»“æœä½œä¸ºå¥–åŠ±æ¥è¿­ä»£åœ°ä¼˜åŒ–æ¨¡å‹çš„å·¥å…·ä½¿ç”¨ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»å‘ç°æœ€ä½³çš„å·¥å…·è°ƒç”¨æ¨¡å¼è€Œæ— éœ€äººç±»å…ˆéªŒçŸ¥è¯†ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„MATH OlympiadåŸºå‡†AIMEä¸Šçš„å®éªŒè¯æ˜äº†ReToolçš„ä¼˜è¶Šæ€§ï¼šæˆ‘ä»¬çš„32Bæ¨¡å‹åœ¨400ä¸ªè®­ç»ƒæ­¥éª¤å†…è¾¾åˆ°äº†67%çš„å‡†ç¡®ç‡ï¼Œä¼˜äºåŸºäºæ–‡æœ¬çš„RLåŸºçº¿ï¼ˆå‡†ç¡®ç‡ä»…ä¸º40%ï¼Œéœ€è¦1080ä¸ªè®­ç»ƒæ­¥éª¤ï¼‰ï¼Œå¹¶ä¸”åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¸Šéƒ½æœ‰æ˜¾è‘—æé«˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒReTool-32Båœ¨æ‰©å±•è®¾ç½®ä¸­çš„å‡†ç¡®ç‡è¾¾åˆ°äº†72.5%ï¼Œæ¯”OpenAIçš„o1-previewé«˜å‡º27.9%ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ˜¾ç¤ºå‡ºç°äº†ä»£ç è‡ªæˆ‘ä¿®æ­£ç­‰çªå‘è¡Œä¸ºï¼Œè¿™è¡¨æ˜æ¨¡å‹è‡ªä¸»åœ°æŒæ¡äº†è‡ªé€‚åº”å·¥å…·çš„ä½¿ç”¨ï¼Œè¾¾åˆ°äº†ä¸€ä¸ªâ€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ä»¥ç»“æœä¸ºå¯¼å‘çš„å·¥å…·é›†æˆåœ¨æ¨åŠ¨å¤æ‚æ•°å­¦æ¨ç†æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæ··åˆç¥ç»ç¬¦å·ç³»ç»Ÿæä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11536v2">PDF</a> fix typos</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ¨ç†æ¨¡å‹åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä½†åœ¨éœ€è¦ç»“æ„åŒ–é—®é¢˜è§£å†³ï¼ˆå¦‚å‡ ä½•æ¨ç†ã€ç®€æ´è®¡ç®—å’Œå¤æ‚æ–¹ç¨‹æ±‚è§£ï¼‰çš„é¢†åŸŸå­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ReToolå·¥å…·ï¼Œå®ƒé€šè¿‡æ•´åˆå·¥å…·å­¦ä¹ å’Œå®æ—¶ä»£ç æ‰§è¡Œï¼Œå¢å¼ºäº†é•¿å½¢å¼æ¨ç†èƒ½åŠ›ã€‚ReToolé‡‡ç”¨è‡ªåŠ¨åŒ–RLèŒƒå¼ï¼Œå…è®¸ç­–ç•¥æ»šåŠ¨è¿›è¡Œå¤šè½®å®æ—¶ä»£ç æ‰§è¡Œï¼Œå¹¶æ ¹æ®ç»“æœåé¦ˆæ•™æˆæ¨¡å‹ä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨å·¥å…·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReToolåœ¨MATH OlympiadåŸºå‡†æµ‹è¯•AIMEä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…¶32Bæ¨¡å‹åœ¨ä»…400æ­¥è®­ç»ƒåè¾¾åˆ°67%çš„å‡†ç¡®ç‡ï¼Œä¼˜äºçº¯æ–‡æœ¬RLåŸºçº¿ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ˜¾ç¤ºï¼ŒReToolæ¨¡å‹å…·æœ‰è‡ªä¸»æŒæ¡è‡ªé€‚åº”å·¥å…·ä½¿ç”¨çš„èƒ½åŠ›ï¼Œè¿™æ ‡å¿—ç€ç¥ç»ç¬¦å·æ··åˆç³»ç»Ÿçš„æ½œåœ¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ¨¡å‹åœ¨æ–‡æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç»“æ„åŒ–é—®é¢˜è§£å†³æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ReToolå·¥å…·é€šè¿‡æ•´åˆå·¥å…·å­¦ä¹ å’Œå®æ—¶ä»£ç æ‰§è¡Œå¢å¼ºäº†é•¿å½¢å¼æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ReToolé‡‡ç”¨è‡ªåŠ¨åŒ–RLèŒƒå¼ï¼Œèƒ½å¤Ÿæ ¹æ®ç»“æœåé¦ˆæ•™æˆæ¨¡å‹ä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨å·¥å…·ã€‚</li>
<li>ReToolåœ¨MATH OlympiadåŸºå‡†æµ‹è¯•AIMEä¸Šå…·æœ‰å“è¶Šæ€§èƒ½ï¼Œå‡†ç¡®ç‡é«˜ä¸”è®­ç»ƒæ­¥éª¤å°‘ã€‚</li>
<li>ReToolæ¨¡å‹å…·æœ‰è‡ªä¸»æŒæ¡è‡ªé€‚åº”å·¥å…·ä½¿ç”¨çš„èƒ½åŠ›ã€‚</li>
<li>ReToolçš„å®éªŒç»“æœå±•ç¤ºäº†ç¥ç»ç¬¦å·æ··åˆç³»ç»Ÿçš„æ½œåœ¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52b232050a403eed6d9921e11f769967.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-953adfbbde6bfedaa9b3b98c71097382.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e844ceb7678a43a71bac624c0c8b6bc3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="CameraBench-Benchmarking-Visual-Reasoning-in-MLLMs-via-Photography"><a href="#CameraBench-Benchmarking-Visual-Reasoning-in-MLLMs-via-Photography" class="headerlink" title="CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography"></a>CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography</h2><p><strong>Authors:I-Sheng Fang, Jun-Cheng Chen</strong></p>
<p>Large language models (LLMs) and multimodal large language models (MLLMs) have significantly advanced artificial intelligence. However, visual reasoning, reasoning involving both visual and textual inputs, remains underexplored. Recent advancements, including the reasoning models like OpenAI o1 and Gemini 2.0 Flash Thinking, which incorporate image inputs, have opened this capability. In this ongoing work, we focus specifically on photography-related tasks because a photo is a visual snapshot of the physical world where the underlying physics (i.e., illumination, blur extent, etc.) interplay with the camera parameters. Successfully reasoning from the visual information of a photo to identify these numerical camera settings requires the MLLMs to have a deeper understanding of the underlying physics for precise visual comprehension, representing a challenging and intelligent capability essential for practical applications like photography assistant agents. We aim to evaluate MLLMs on their ability to distinguish visual differences related to numerical camera settings, extending a methodology previously proposed for vision-language models (VLMs). Our preliminary results demonstrate the importance of visual reasoning in photography-related tasks. Moreover, these results show that no single MLLM consistently dominates across all evaluation tasks, demonstrating ongoing challenges and opportunities in developing MLLMs with better visual reasoning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œæ¶‰åŠè§†è§‰å’Œæ–‡æœ¬è¾“å…¥çš„è§†è§‰æ¨ç†ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ€è¿‘çš„è¿›å±•ï¼ŒåŒ…æ‹¬èå…¥å›¾åƒè¾“å…¥çš„æ¨ç†æ¨¡å‹ï¼Œå¦‚OpenAI o1å’ŒGemini 2.0 Flash Thinkingï¼Œå·²ç»å¼€å¯äº†è¿™ä¸€åŠŸèƒ½ã€‚åœ¨è¿™é¡¹æŒç»­çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç‰¹åˆ«å…³æ³¨æ‘„å½±ç›¸å…³ä»»åŠ¡ï¼Œå› ä¸ºç…§ç‰‡æ˜¯ç‰©ç†ä¸–ç•Œçš„è§†è§‰å¿«ç…§ï¼Œå…¶ä¸­åŸºç¡€ç‰©ç†å­¦ï¼ˆå³ç…§æ˜ã€æ¨¡ç³Šç¨‹åº¦ç­‰ï¼‰ä¸ç›¸æœºå‚æ•°ç›¸äº’ä½œç”¨ã€‚ä»ç…§ç‰‡çš„è§†è§‰ä¿¡æ¯ä¸­æˆåŠŸæ¨ç†å‡ºè¿™äº›æ•°å€¼ç›¸æœºè®¾ç½®ï¼Œéœ€è¦MLLMå¯¹åŸºç¡€ç‰©ç†å­¦æœ‰æ›´æ·±å…¥çš„ç†è§£ï¼Œä»¥å®ç°ç²¾ç¡®çš„è§†è§‰ç†è§£ï¼Œè¿™æ˜¯æ‘„å½±åŠ©ç†ä»£ç†ç­‰å®é™…åº”ç”¨ä¸­ä¸å¯æˆ–ç¼ºçš„æŒ‘æˆ˜æ€§å’Œæ™ºèƒ½èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¯¹MLLMè¿›è¡Œè¯„ä¼°ï¼Œè¯„ä¼°å…¶åœ¨åŒºåˆ†ä¸æ•°å€¼ç›¸æœºè®¾ç½®ç›¸å…³çš„è§†è§‰å·®å¼‚æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶æ‰©å±•å…ˆå‰ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æå‡ºçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„åˆæ­¥ç»“æœè¯æ˜äº†è§†è§‰æ¨ç†åœ¨æ‘„å½±ç›¸å…³ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œè¿™äº›ç»“æœè¡¨æ˜ï¼Œæ²¡æœ‰å•ä¸€MLLMåœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­éƒ½å§‹ç»ˆå æ®ä¸»å¯¼åœ°ä½ï¼Œè¿™æ˜¾ç¤ºäº†å¼€å‘å…·æœ‰æ›´å¥½è§†è§‰æ¨ç†èƒ½åŠ›çš„MLLMæ‰€é¢ä¸´çš„æŒç»­æŒ‘æˆ˜å’Œæœºé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10090v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è§†è§‰æ¨ç†ï¼Œå³æ¶‰åŠè§†è§‰å’Œæ–‡æœ¬è¾“å…¥çš„æ¨ç†ï¼Œä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚æœ€è¿‘ï¼Œä¸€äº›ç»“åˆå›¾åƒè¾“å…¥çš„æ¨ç†æ¨¡å‹ï¼Œå¦‚OpenAI o1å’ŒGemini 2.0 Flash Thinkingï¼Œå¼€å§‹å…·å¤‡è¿™ç§èƒ½åŠ›ã€‚å½“å‰çš„ç ”ç©¶é‡ç‚¹é›†ä¸­åœ¨æ‘„å½±ç›¸å…³ä»»åŠ¡ä¸Šï¼Œå› ä¸ºç…§ç‰‡æ˜¯ç‰©ç†ä¸–ç•Œçš„è§†è§‰å¿«ç…§ï¼Œå…¶ä¸­åŸºç¡€ç‰©ç†å­¦ï¼ˆå¦‚ç…§æ˜ã€æ¨¡ç³Šç¨‹åº¦ç­‰ï¼‰ä¸ç›¸æœºå‚æ•°ç›¸äº’ä½œç”¨ã€‚è¯„ä¼°MLLMsä»ç…§ç‰‡çš„è§†è§‰ä¿¡æ¯ä¸­è¯†åˆ«è¿™äº›æ•°å€¼ç›¸æœºè®¾ç½®çš„èƒ½åŠ›ï¼Œéœ€è¦å®ƒä»¬å¯¹åŸºç¡€ç‰©ç†å­¦æœ‰æ›´æ·±çš„ç†è§£ï¼Œä»¥å®ç°ç²¾ç¡®è§†è§‰ç†è§£ï¼Œè¿™æ˜¯æ‘„å½±åŠ©ç†ä»£ç†ç­‰å®é™…åº”ç”¨ä¸­å¿…ä¸å¯å°‘çš„ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§å’Œæ™ºèƒ½èƒ½åŠ›çš„æŠ€èƒ½ã€‚åˆæ­¥ç»“æœè¡¨æ˜è§†è§‰æ¨ç†åœ¨æ‘„å½±ç›¸å…³ä»»åŠ¡ä¸­çš„é‡è¦æ€§ï¼Œä¸”æ²¡æœ‰å•ä¸€MLLMåœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­å§‹ç»ˆå æ®ä¸»å¯¼åœ°ä½ï¼Œè¿™è¡¨æ˜åœ¨å¼€å‘å…·æœ‰æ›´å¥½è§†è§‰æ¨ç†èƒ½åŠ›çš„MLLMsæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜å’Œæœºé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äººå·¥æ™ºèƒ½ä¸­çš„åº”ç”¨å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>è§†è§‰æ¨ç†ä»ç„¶æ˜¯ä¸€ä¸ªè¢«è¾ƒå°‘æ¢ç´¢çš„é¢†åŸŸï¼Œä½†å·²å¼•èµ·å…³æ³¨å¹¶æœ‰æ‰€çªç ´ã€‚</li>
<li>æ‘„å½±ç›¸å…³ä»»åŠ¡æ˜¯å½“å‰ç ”ç©¶çš„é‡ç‚¹ï¼Œå› ä¸ºå®ƒç»“åˆäº†è§†è§‰ç†è§£å’Œæ•°å€¼ç›¸æœºè®¾ç½®åˆ†æã€‚</li>
<li>ç†è§£åŸºç¡€ç‰©ç†å­¦å’Œç›¸æœºå‚æ•°å¯¹å®ç°ç²¾ç¡®çš„è§†è§‰ç†è§£è‡³å…³é‡è¦ã€‚</li>
<li>MLLMsåœ¨æ‘„å½±ç›¸å…³ä»»åŠ¡ä¸­çš„åˆæ­¥è¯„ä¼°æ˜¾ç¤ºäº†è§†è§‰æ¨ç†çš„é‡è¦æ€§ã€‚</li>
<li>ç›®å‰æ²¡æœ‰å•ä¸€MLLMåœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œè¯´æ˜å¼€å‘å…·æœ‰è§†è§‰æ¨ç†èƒ½åŠ›çš„MLLMsä»é¢ä¸´æŒ‘æˆ˜å’Œæœºé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1b3bce0527cf42abd153678d13a3fffd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32128662ed1277818e37f8dfeaf5d3bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fad692f21531b4f1110ba53492ebc54a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e476209eb5d2b60ef511f28df2cea99.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93a0d9f79e98563924d4dff17a2a5999.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Why-We-Feel-Breaking-Boundaries-in-Emotional-Reasoning-with-Multimodal-Large-Language-Models"><a href="#Why-We-Feel-Breaking-Boundaries-in-Emotional-Reasoning-with-Multimodal-Large-Language-Models" class="headerlink" title="Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal   Large Language Models"></a>Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal   Large Language Models</h2><p><strong>Authors:Yuxiang Lin, Jingdong Sun, Zhi-Qi Cheng, Jue Wang, Haomin Liang, Zebang Cheng, Yifei Dong, Jun-Yan He, Xiaojiang Peng, Xian-Sheng Hua</strong></p>
<p>Most existing emotion analysis emphasizes which emotion arises (e.g., happy, sad, angry) but neglects the deeper why. We propose Emotion Interpretation (EI), focusing on causal factors-whether explicit (e.g., observable objects, interpersonal interactions) or implicit (e.g., cultural context, off-screen events)-that drive emotional responses. Unlike traditional emotion recognition, EI tasks require reasoning about triggers instead of mere labeling. To facilitate EI research, we present EIBench, a large-scale benchmark encompassing 1,615 basic EI samples and 50 complex EI samples featuring multifaceted emotions. Each instance demands rationale-based explanations rather than straightforward categorization. We further propose a Coarse-to-Fine Self-Ask (CFSA) annotation pipeline, which guides Vision-Language Models (VLLMs) through iterative question-answer rounds to yield high-quality labels at scale. Extensive evaluations on open-source and proprietary large language models under four experimental settings reveal consistent performance gaps-especially for more intricate scenarios-underscoring EIâ€™s potential to enrich empathetic, context-aware AI applications. Our benchmark and methods are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Lum1104/EIBench">https://github.com/Lum1104/EIBench</a>, offering a foundation for advanced multimodal causal analysis and next-generation affective computing. </p>
<blockquote>
<p>ç°æœ‰å¤§å¤šæ•°æƒ…æ„Ÿåˆ†æä¾§é‡äºå“ªç§æƒ…ç»ªå‡ºç°ï¼ˆä¾‹å¦‚å¿«ä¹ã€æ‚²ä¼¤ã€æ„¤æ€’ï¼‰ï¼Œå´å¿½è§†äº†æ›´æ·±å±‚çš„åŸå› ã€‚æˆ‘ä»¬æå‡ºæƒ…æ„Ÿè§£è¯»ï¼ˆEIï¼‰ï¼Œä¸“æ³¨äºé©±åŠ¨æƒ…æ„Ÿååº”çš„å› æœå› ç´ ï¼Œæ— è®ºæ˜¯æ˜¾æ€§çš„ï¼ˆä¾‹å¦‚å¯è§‚å¯Ÿçš„å¯¹è±¡ã€äººé™…äº’åŠ¨ï¼‰è¿˜æ˜¯éšæ€§çš„ï¼ˆä¾‹å¦‚æ–‡åŒ–èƒŒæ™¯ã€å±å¹•å¤–çš„äº‹ä»¶ï¼‰ã€‚ä¸ä¼ ç»Ÿçš„æƒ…æ„Ÿè¯†åˆ«ä¸åŒï¼ŒEIä»»åŠ¡éœ€è¦è¿›è¡Œè§¦å‘å› ç´ æ¨ç†ï¼Œè€Œä¸ä»…ä»…æ˜¯æ ‡ç­¾åˆ†ç±»ã€‚ä¸ºäº†ä¿ƒè¿›EIç ”ç©¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EIBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«1615ä¸ªåŸºæœ¬çš„EIæ ·æœ¬å’Œ50ä¸ªå¤æ‚çš„EIæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬å…·æœ‰å¤šå…ƒçš„æƒ…æ„Ÿç‰¹ç‚¹ã€‚æ¯ä¸ªå®ä¾‹éƒ½éœ€è¦åŸºäºç†æ€§çš„è§£é‡Šï¼Œè€Œä¸æ˜¯ç›´æ¥çš„åˆ†ç±»ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä»ç²—ç³™åˆ°ç²¾ç»†çš„è‡ªæˆ‘æé—®ï¼ˆCFSAï¼‰æ³¨é‡Šç®¡é“ï¼Œå®ƒé€šè¿‡å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLLMï¼‰è¿›è¡Œè¿­ä»£é—®ç­”å›åˆï¼Œä»è€Œå¤§è§„æ¨¡ç”Ÿæˆé«˜è´¨é‡æ ‡ç­¾ã€‚åœ¨å››ç§å®éªŒè®¾ç½®ä¸‹ï¼Œå¯¹å¼€æºå’Œä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºäº†ä¸€è‡´çš„æ€§èƒ½å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨æ›´å¤æ‚åœºæ™¯ä¸­ï¼Œè¿™çªæ˜¾äº†EIåœ¨ä¸°å¯Œå…·æœ‰åŒæƒ…å¿ƒã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„AIåº”ç”¨æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œæ–¹æ³•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Lum1104/EIBench%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%EF%BC%8C%E4%B8%BA%E5%85%88%E8%BF%9B%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%BC%8F%E5%9B%A0%E6%9E%9C%E5%88%86%E6%9E%90%E5%92%8C%E4%B8%8B%E4%B8%80%E4%BB%A3%E6%83%85%E6%84%9F%E8%AE%A1%E7%AE%97%E6%8F%90%E4%BE%9B%E4%BA%86%E5%9F%BA%E7%A1%80%E3%80%82">https://github.com/Lum1104/EIBenchå…¬å¼€è®¿é—®ï¼Œä¸ºå…ˆè¿›çš„å¤šæ¨¡å¼å› æœåˆ†æå’Œä¸‹ä¸€ä»£æƒ…æ„Ÿè®¡ç®—æä¾›äº†åŸºç¡€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07521v2">PDF</a> Accepted at CVPR Workshop NEXD 2025. 21 pages, Project:   <a target="_blank" rel="noopener" href="https://github.com/Lum1104/EIBench">https://github.com/Lum1104/EIBench</a></p>
<p><strong>Summary</strong><br>æƒ…æ„Ÿåˆ†æç°æœ‰ç ”ç©¶å¤šå…³æ³¨æƒ…ç»ªç±»å‹çš„å‡ºç°ï¼Œå¿½è§†äº†æ·±å±‚æ¬¡çš„åŸå› ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºæƒ…æ„Ÿè§£è¯»ï¼ˆEIï¼‰ï¼Œå…³æ³¨å¯¼è‡´æƒ…ç»ªååº”çš„å†…éšå¤–æ˜¾åŸå› ã€‚ä¸ºå®ç°EIä»»åŠ¡ï¼Œæˆ‘ä»¬åˆ¶å®šäº†å¤§è§„æ¨¡çš„åŸºå‡†æµ‹è¯•EIBenchï¼ŒåŒ…æ‹¬ä¸€åƒå¤šä¸ªåŸºç¡€æ ·æœ¬å’Œäº”åä¸ªå¤æ‚æ ·æœ¬ï¼Œè¦æ±‚åŸºäºç†ç”±çš„è§£é‡Šè€Œéç®€å•åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä»ç²—ç³™åˆ°ç²¾ç»†çš„è‡ªæˆ‘æé—®ï¼ˆCFSAï¼‰æ ‡æ³¨æµç¨‹ï¼ŒæŒ‡å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰è¿›è¡Œé«˜è´¨é‡æ ‡æ³¨ã€‚å¯¹å¼€æºå’Œä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œåœ¨å¤æ‚çš„åœºæ™¯ä¸­ï¼Œæ€§èƒ½å·®è·å°¤ä¸ºæ˜¾è‘—ï¼Œå‡¸æ˜¾äº†EIåœ¨ä¸°å¯Œå…·æœ‰åŒç†å¿ƒå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„AIåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œæ–¹æ³•å·²åœ¨å…¬å¼€å¹³å°ä¸Šå‘å¸ƒï¼Œä¸ºå…ˆè¿›çš„å¤šæ¨¡æ€å› æœåˆ†æå’Œä¸‹ä¸€ä»£æƒ…æ„Ÿè®¡ç®—æä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æƒ…æ„Ÿåˆ†æå…³æ³¨æƒ…ç»ªç±»å‹ï¼Œå¿½è§†æƒ…æ„Ÿäº§ç”Ÿçš„æ·±å±‚åŸå› ã€‚</li>
<li>æå‡ºæƒ…æ„Ÿè§£è¯»ï¼ˆEIï¼‰ï¼Œé‡ç‚¹ç ”ç©¶æƒ…æ„Ÿäº§ç”Ÿçš„å› æœå› ç´ ã€‚</li>
<li>ä»‹ç»äº†å¤§è§„æ¨¡çš„EIåŸºå‡†æµ‹è¯•EIBenchï¼ŒåŒ…å«åŸºç¡€æ ·æœ¬å’Œå¤æ‚æ ·æœ¬ã€‚</li>
<li>æå‡ºäº†CFSAæ ‡æ³¨æµç¨‹ï¼Œç”¨äºæŒ‡å¯¼è¯­è¨€æ¨¡å‹è¿›è¡Œé«˜è´¨é‡æ ‡æ³¨ã€‚</li>
<li>å¯¹ä¸åŒå®éªŒè®¾ç½®ä¸‹çš„è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå‘ç°å¤æ‚åœºæ™¯ä¸‹æ€§èƒ½å·®è·æ˜¾è‘—ã€‚</li>
<li>EIç ”ç©¶æœ‰åŠ©äºä¸°å¯Œå…·æœ‰åŒç†å¿ƒå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„AIåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07521">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f99e989419f0f5c3a2ee3ec4a275a6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc6e33b57fb7e3e51a09f652af82fdb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6231af177a1fce424098921faaefb94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d535e7099d3f06c7d98bd32ee2245864.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f5fc7a8aff5237e953bcd53ef359d6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-798c403c4105bfea7dc240b46b56618b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="RAISE-Reinforenced-Adaptive-Instruction-Selection-For-Large-Language-Models"><a href="#RAISE-Reinforenced-Adaptive-Instruction-Selection-For-Large-Language-Models" class="headerlink" title="RAISE: Reinforenced Adaptive Instruction Selection For Large Language   Models"></a>RAISE: Reinforenced Adaptive Instruction Selection For Large Language   Models</h2><p><strong>Authors:Lv Qingsong, Yangning Li, Zihua Lan, Zishan Xu, Jiwei Tang, Yinghui Li, Wenhao Jiang, Hai-Tao Zheng, Philip S. Yu</strong></p>
<p>In the instruction fine-tuning of large language models (LLMs), it has become a consensus that a few high-quality instructions are superior to a large number of low-quality instructions. At present, many instruction selection methods have been proposed, but most of these methods select instruction based on heuristic quality metrics, and only consider data selection before training. These designs lead to insufficient optimization of instruction fine-tuning, and fixed heuristic indicators are often difficult to optimize for specific tasks. So we designed a dynamic, task-objective-driven instruction selection framework RAISE(Reinforenced Adaptive Instruction SElection), which incorporates the entire instruction fine-tuning process into optimization, selecting instruction at each step based on the expected impact of instruction on model performance improvement. Our approach is well interpretable and has strong task-specific optimization capabilities. By modeling dynamic instruction selection as a sequential decision-making process, we use RL to train our selection strategy. Extensive experiments and result analysis prove the superiority of our method compared with other instruction selection methods. Notably, RAISE achieves superior performance by updating only 1% of the training steps compared to full-data training, demonstrating its efficiency and effectiveness. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‡ä»¤å¾®è°ƒä¸­ï¼Œå·²ç»è¾¾æˆä¸€ç§å…±è¯†ï¼Œå³å°‘æ•°é«˜è´¨é‡æŒ‡ä»¤ä¼˜äºå¤§é‡ä½è´¨é‡æŒ‡ä»¤ã€‚ç›®å‰ï¼Œå·²ç»æå‡ºäº†è®¸å¤šæŒ‡ä»¤é€‰æ‹©æ–¹æ³•ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•éƒ½æ˜¯åŸºäºå¯å‘å¼è´¨é‡æŒ‡æ ‡æ¥é€‰æ‹©æŒ‡ä»¤ï¼Œåªåœ¨è®­ç»ƒå‰è€ƒè™‘æ•°æ®é€‰æ‹©ã€‚è¿™äº›è®¾è®¡å¯¼è‡´æŒ‡ä»¤å¾®è°ƒä¼˜åŒ–ä¸è¶³ï¼Œå›ºå®šçš„å¯å‘å¼æŒ‡æ ‡å¾€å¾€éš¾ä»¥é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŠ¨æ€ã€ä»»åŠ¡ç›®æ ‡é©±åŠ¨çš„æŒ‡ä»¤é€‰æ‹©æ¡†æ¶RAISEï¼ˆå¼ºåŒ–è‡ªé€‚åº”æŒ‡ä»¤é€‰æ‹©ï¼‰ï¼Œå®ƒå°†æ•´ä¸ªæŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹çº³å…¥ä¼˜åŒ–ï¼Œæ ¹æ®æŒ‡ä»¤å¯¹æ¨¡å‹æ€§èƒ½æ”¹è¿›çš„é¢„æœŸå½±å“ï¼Œåœ¨æ¯ä¸€æ­¥é€‰æ‹©æŒ‡ä»¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„ä»»åŠ¡ç‰¹å®šä¼˜åŒ–èƒ½åŠ›ã€‚é€šè¿‡å°†åŠ¨æ€æŒ‡ä»¤é€‰æ‹©å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒæˆ‘ä»¬çš„é€‰æ‹©ç­–ç•¥ã€‚å¤§é‡çš„å®éªŒå’Œç»“æœåˆ†æè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸å…¶ä»–æŒ‡ä»¤é€‰æ‹©æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRAISEä»…åœ¨1%çš„è®­ç»ƒæ­¥éª¤ä¸­è¿›è¡Œæ›´æ–°å°±å®ç°äº†ä¼˜äºå…¨æ•°æ®è®­ç»ƒçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07282v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒä¸­ï¼Œé«˜è´¨é‡æŒ‡ä»¤ä¼˜äºå¤§é‡ä½è´¨é‡æŒ‡ä»¤ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºå¯å‘å¼è´¨é‡æŒ‡æ ‡è¿›è¡ŒæŒ‡ä»¤é€‰æ‹©ï¼Œå¹¶åœ¨è®­ç»ƒå‰è¿›è¡Œæ•°æ®é€‰æ‹©ï¼Œå¯¼è‡´æŒ‡ä»¤å¾®è°ƒä¼˜åŒ–ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåŠ¨æ€ã€ä»»åŠ¡ç›®æ ‡é©±åŠ¨çš„æŒ‡ä»¤é€‰æ‹©æ¡†æ¶RAISEï¼Œå°†æ•´ä¸ªæŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹çº³å…¥ä¼˜åŒ–ï¼Œæ ¹æ®æŒ‡ä»¤å¯¹æ¨¡å‹æ€§èƒ½æå‡çš„é¢„æœŸå½±å“è¿›è¡ŒåŠ¨æ€é€‰æ‹©ã€‚è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼Œå…·æœ‰å¼ºå¤§çš„ä»»åŠ¡ç‰¹å®šä¼˜åŒ–èƒ½åŠ›ï¼Œå°†åŠ¨æ€æŒ‡ä»¤é€‰æ‹©å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚å®éªŒè¯æ˜ï¼Œä¸å…¶ä»–æŒ‡ä»¤é€‰æ‹©æ–¹æ³•ç›¸æ¯”ï¼ŒRAISEæ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œå¹¶ä¸”åœ¨ä»…æ›´æ–°1%çš„è®­ç»ƒæ­¥éª¤ä¸‹å®ç°ä¼˜è¶Šæ€§èƒ½ï¼Œå±•ç°å‡ºå…¶é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒä¸­ï¼Œå°‘é‡é«˜è´¨é‡æŒ‡ä»¤ä¼˜äºå¤§é‡ä½è´¨é‡æŒ‡ä»¤ã€‚</li>
<li>ç°æœ‰æŒ‡ä»¤é€‰æ‹©æ–¹æ³•ä¸»è¦åŸºäºå¯å‘å¼è´¨é‡æŒ‡æ ‡ï¼Œå­˜åœ¨ä¼˜åŒ–ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>RAISEæ¡†æ¶æ˜¯ä¸€ç§åŠ¨æ€ã€ä»»åŠ¡ç›®æ ‡é©±åŠ¨çš„æŒ‡ä»¤é€‰æ‹©æ–¹æ³•ï¼Œå°†æŒ‡ä»¤å¾®è°ƒæ•´ä¸ªè¿‡ç¨‹çº³å…¥ä¼˜åŒ–ã€‚</li>
<li>RAISEæ–¹æ³•æ ¹æ®æŒ‡ä»¤å¯¹æ¨¡å‹æ€§èƒ½æå‡çš„é¢„æœŸå½±å“è¿›è¡ŒåŠ¨æ€é€‰æ‹©ã€‚</li>
<li>RAISEæ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„ä»»åŠ¡ç‰¹å®šä¼˜åŒ–èƒ½åŠ›ã€‚</li>
<li>RAISEå°†åŠ¨æ€æŒ‡ä»¤é€‰æ‹©å»ºæ¨¡ä¸ºåºåˆ—å†³ç­–è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-224700e83335fc4f59b702396f9381c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a5ff509c118576b1dfc2567ebf0bef75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fcfd1635d841c76a611597fe1b79deb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fc14aad3145cc3708a71e28b695a436.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7b250aa317632ec0403c95915f2f73e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="VideoChat-R1-Enhancing-Spatio-Temporal-Perception-via-Reinforcement-Fine-Tuning"><a href="#VideoChat-R1-Enhancing-Spatio-Temporal-Perception-via-Reinforcement-Fine-Tuning" class="headerlink" title="VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement   Fine-Tuning"></a>VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement   Fine-Tuning</h2><p><strong>Authors:Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang</strong></p>
<p>Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs. </p>
<blockquote>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ åœ¨å¤šåª’ä½“å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¿›å±•éå¸¸æ˜¾è‘—ã€‚å°½ç®¡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’ŒåŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶åœ¨æ–‡æœ¬å’Œå›¾åƒé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°æ¢è®¨äº†ä½¿ç”¨GRPOçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰åœ¨è§†é¢‘MLLMsä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æé«˜æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›çš„åŒæ—¶ä¿æŒé€šç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRFTå¯¹äºç‰¹å®šä»»åŠ¡çš„æ”¹è¿›éå¸¸æ•°æ®é«˜æ•ˆã€‚é€šè¿‡æœ‰é™æ ·æœ¬çš„æ—¶ç©ºæ„ŸçŸ¥ç›®æ ‡ä¸Šçš„å¤šä»»åŠ¡RFTï¼Œæˆ‘ä»¬å¼€å‘å‡ºäº†VideoChat-R1ï¼Œè¿™æ˜¯ä¸€æ¬¾å¼ºå¤§çš„è§†é¢‘MLLMï¼Œå®ƒåœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å¯¹è¯èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºæ–°å…´çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚ä¸Qwen2.5-VL-7Bç›¸æ¯”ï¼ŒVideoChat-R1åœ¨è¯¸å¦‚æ—¶é—´å®šä½ï¼ˆ+31.8ï¼‰å’Œå¯¹è±¡è·Ÿè¸ªï¼ˆ+31.2ï¼‰çš„ä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡äº†æ•°å€ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨é€šç”¨é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMEã€MVBenchå’ŒPerception Testï¼‰ä¸Šçš„è¡¨ç°ä¹Ÿæ˜¾è‘—æé«˜ï¼ˆ+0.9ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†RFTåœ¨è§†é¢‘MLLMç‰¹å®šä»»åŠ¡å¢å¼ºæ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½ä¸ºæœªæ¥è§†é¢‘MLLMçš„å¼ºåŒ–å­¦ä¹ ç ”ç©¶æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06958v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†é‡è¦è¿›å±•ã€‚æœ¬è®ºæ–‡æ¢ç´¢äº†ä½¿ç”¨å¼ºåŒ–ç²¾ç»†è°ƒæ•´ï¼ˆRFTï¼‰ä¸é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„è§†é¢‘MLLMsæŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›åŒæ—¶ä¿æŒé€šç”¨èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒRFTå¯¹äºç‰¹å®šä»»åŠ¡æ”¹è¿›éå¸¸é«˜æ•ˆï¼Œé€šè¿‡å¤šä»»åŠ¡RFTåœ¨æ—¶ç©ºæ„ŸçŸ¥ç›®æ ‡ä¸Šè¿›è¡Œæœ‰é™æ ·æœ¬è®­ç»ƒï¼Œå¼€å‘å‡ºVideoChat-R1æ¨¡å‹ï¼Œåœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™èŠå¤©èƒ½åŠ›ï¼Œå±•ç°å‡ºæ–°å…´çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>Group Relative Policy Optimization (GRPO) åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„åº”ç”¨å—é™ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–ç²¾ç»†è°ƒæ•´ï¼ˆRFTï¼‰å’ŒGRPOçš„ç»“åˆï¼Œæé«˜äº†è§†é¢‘MLLMsçš„æ—¶ç©ºæ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>RFTå…·æœ‰é«˜åº¦æ•°æ®æ•ˆç‡ï¼Œèƒ½é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œæœ‰æ•ˆæ”¹è¿›ã€‚</li>
<li>å¼€å‘å‡ºçš„VideoChat-R1æ¨¡å‹åœ¨æ—¶ç©ºæ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬é—®ç­”åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>VideoChat-R1ç›¸è¾ƒäºQwen2.5-VL-7Bæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ€§èƒ½æœ‰æ‰€æå‡ï¼Œå¦‚æ—¶é—´å®šä½å’Œç›®æ ‡è·Ÿè¸ªç­‰ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8c510b59052f476fc77849daf8998957.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89b3adc13557b37016357b42472fc020.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b04301c3b58011a6301355232fd14e08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54a060be55db18ac081a0a4b1a302330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-247a56547855773e359a20635b8f470e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c392c0aeea22b73fb78f63e50e43e246.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DeepResearcher-Scaling-Deep-Research-via-Reinforcement-Learning-in-Real-world-Environments"><a href="#DeepResearcher-Scaling-Deep-Research-via-Reinforcement-Learning-in-Real-world-Environments" class="headerlink" title="DeepResearcher: Scaling Deep Research via Reinforcement Learning in   Real-world Environments"></a>DeepResearcher: Scaling Deep Research via Reinforcement Learning in   Real-world Environments</h2><p><strong>Authors:Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, Pengfei Liu</strong></p>
<p>Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at <a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/DeepResearcher">https://github.com/GAIR-NLP/DeepResearcher</a>. </p>
<blockquote>
<p>é…å¤‡ç½‘é¡µæœç´¢åŠŸèƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸­å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡çš„æç¤ºï¼ˆåŸºäºæç¤ºå·¥ç¨‹çš„æ–¹æ³•ï¼‰ï¼Œå…¶æ€§èƒ½è„†å¼±ï¼Œæˆ–è€…åœ¨å—æ§çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç¯å¢ƒä¸­ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆåŸºäºRAGçš„æ–¹æ³•ï¼‰ï¼Œæ— æ³•æ•æ‰ç°å®ä¸–ç•Œä¸­äº’åŠ¨çš„å¤æ‚æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DeepResearcherï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒLLMæ·±åº¦ç ”ç©¶ä»£ç†çš„å…¨é¢æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡çœŸå®çš„ç½‘é¡µæœç´¢äº’åŠ¨æ¥å®ç°ã€‚ä¸å‡è®¾æ‰€æœ‰å¿…è¦ä¿¡æ¯éƒ½å­˜åœ¨äºå›ºå®šè¯­æ–™åº“ä¸­çš„RAGæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒä»£ç†ä»¥åº”å¯¹å¼€æ”¾ç½‘ç»œçš„å˜ˆæ‚ã€éç»“æ„åŒ–å’ŒåŠ¨æ€æ€§è´¨ã€‚æˆ‘ä»¬å®ç°äº†ä¸€ç§ä¸“ç”¨å¤šä»£ç†æ¶æ„ï¼Œæµè§ˆä»£ç†å¯ä»¥ä»å„ç§ç½‘é¡µç»“æ„ä¸­æå–ç›¸å…³ä¿¡æ¯ï¼Œå¹¶å…‹æœé‡å¤§æŠ€æœ¯æŒ‘æˆ˜ã€‚åœ¨å¼€æ”¾åŸŸç ”ç©¶ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDeepResearcherè¾ƒåŸºäºæç¤ºå·¥ç¨‹çš„åŸºçº¿å®ç°äº†é«˜è¾¾28.9ç‚¹çš„å®è´¨æ€§æ”¹è¿›ï¼Œè¾ƒåŸºäºRAGçš„RLä»£ç†å®ç°äº†é«˜è¾¾7.2ç‚¹çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„å®šæ€§åˆ†ææ­ç¤ºäº†æ¥è‡ªç«¯åˆ°ç«¯RLè®­ç»ƒçš„çªå‘è®¤çŸ¥è¡Œä¸ºï¼ŒåŒ…æ‹¬åˆ¶å®šè®¡åˆ’çš„èƒ½åŠ›ã€ä»å¤šä¸ªæ¥æºè¿›è¡Œäº¤å‰éªŒè¯ä¿¡æ¯çš„èƒ½åŠ›ã€å‚ä¸è‡ªæˆ‘åæ€ä»¥é‡æ–°å®šå‘ç ”ç©¶çš„èƒ½åŠ›ï¼Œä»¥åŠåœ¨æ— æ³•æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆæ—¶ä¿æŒè¯šå®ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒï¼Œåœ¨çœŸå®ä¸–ç•Œç½‘ç»œç¯å¢ƒä¸­è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒä¸ä»…æ˜¯å®ç°ç»†èŠ‚ï¼Œè€Œä¸”æ˜¯å¼€å‘ä¸ç°å®ä¸–ç•Œåº”ç”¨ç›¸åŒ¹é…çš„ç¨³å¥ç ”ç©¶èƒ½åŠ›çš„æ ¹æœ¬è¦æ±‚ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/DeepResearcher%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86DeepResearcher%E3%80%82">https://github.com/GAIR-NLP/DeepResearcherä¸Šå‘å¸ƒäº†DeepResearcherã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.03160v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆç½‘ç»œæœç´¢èƒ½åŠ›åœ¨æ·±ç ”ç©¶ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡çš„æç¤ºæˆ–å¼ºåŒ–å­¦ä¹ åœ¨å—æ§çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç¯å¢ƒä¸­ï¼Œè¿™ä¸¤è€…éƒ½æ— æ³•å®Œå…¨æ•æ‰çœŸå®ä¸–ç•Œäº¤äº’çš„å¤æ‚æ€§ã€‚æœ¬æ–‡æå‡ºDeepResearcherï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¸ºåŸºç¡€çš„ç ”ç©¶ä»£ç†äººçš„å…¨é¢æ¡†æ¶ã€‚ä¸åŸºäºRAGçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒä»£ç†äººåœ¨å¼€æ”¾ç½‘ç»œä¸Šæµè§ˆï¼Œåº”å¯¹å™ªéŸ³ã€éç»“æ„åŒ–å’ŒåŠ¨æ€çš„ç¯å¢ƒã€‚é€šè¿‡å®æ–½ç‰¹æ®Šçš„å¤šä»£ç†æ¶æ„ï¼Œæå–æ¥è‡ªå„ç§ç½‘é¡µç»“æ„çš„ç›¸å…³ä¿¡æ¯å¹¶å…‹æœé‡å¤§æŠ€æœ¯æŒ‘æˆ˜ã€‚åœ¨å¼€æ”¾åŸŸç ”ç©¶ä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDeepResearcherç›¸å¯¹äºåŸºäºæç¤ºçš„å·¥ç¨‹æ–¹æ³•å’ŒåŸºäºRAGçš„RLä»£ç†åˆ†åˆ«å®ç°äº†é«˜è¾¾28.9ç‚¹å’Œ7.2ç‚¹çš„å®è´¨æ€§æ”¹è¿›ã€‚æˆ‘ä»¬çš„å®šæ€§åˆ†ææ­ç¤ºäº†ä»ç«¯åˆ°ç«¯çš„RLè®­ç»ƒä¸­æ¶Œç°å‡ºçš„è®¤çŸ¥è¡Œä¸ºï¼ŒåŒ…æ‹¬åˆ¶å®šè®¡åˆ’ã€ä»å¤šä¸ªæ¥æºäº¤å‰éªŒè¯ä¿¡æ¯ã€è¿›è¡Œè‡ªæˆ‘åæ€ä»¥è°ƒæ•´ç ”ç©¶æ–¹å‘ä»¥åŠåœ¨æ— æ³•æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆæ—¶ä¿æŒè¯šå®ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨çœŸå®ä¸–ç•Œç½‘ç»œç¯å¢ƒä¸­è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒä¸ä»…æ˜¯å®ç°ç»†èŠ‚ï¼Œè€Œä¸”æ˜¯å¼€å‘ä¸ç°å®ä¸–ç•Œåº”ç”¨ç›¸åŒ¹é…çš„ç¨³å¥ç ”ç©¶èƒ½åŠ›çš„æ ¹æœ¬è¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆç½‘ç»œæœç´¢èƒ½åŠ›åœ¨æ·±ç ”ç©¶ä»»åŠ¡ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–æ‰‹åŠ¨è®¾è®¡çš„æç¤ºæˆ–å¼ºåŒ–å­¦ä¹ åœ¨å—æ§ç¯å¢ƒï¼Œéš¾ä»¥æ•æ‰çœŸå®ä¸–ç•Œäº¤äº’çš„å¤æ‚æ€§ã€‚</li>
<li>DeepResearcheræ˜¯é¦–ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒè¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒLLMçš„å…¨é¢æ¡†æ¶ã€‚</li>
<li>DeepResearcherèƒ½åœ¨å¼€æ”¾ç½‘ç»œä¸Šè¿›è¡Œæµè§ˆå¹¶åº”å¯¹å™ªéŸ³ã€éç»“æ„åŒ–å’ŒåŠ¨æ€çš„ç¯å¢ƒã€‚</li>
<li>å¤šä»£ç†æ¶æ„è¢«ç”¨äºæå–ç½‘é¡µä¿¡æ¯å¹¶å…‹æœæŠ€æœ¯æŒ‘æˆ˜ã€‚</li>
<li>DeepResearcherç›¸å¯¹äºåŸºäºæç¤ºå·¥ç¨‹å’ŒåŸºäºRAGçš„RLæ–¹æ³•æœ‰æ˜æ˜¾æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.03160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e43a5167be47b646f10110caa663433.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fca4243f7e11b0d587e275a3743f5a51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-125621e5736861a2281682fd03b6fbaa.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-19/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-19/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-19/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d89c4c2885c1355a2b27578c458cce58.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-19  Exploring Expert Failures Improves LLM Agent Tuning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fa41f28c87761083ffb984a8d919b3d8.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-18  Comparative Evaluation of Radiomics and Deep Learning Models for Disease   Detection in Chest Radiography
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
