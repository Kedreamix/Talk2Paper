<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-8187121cc6a85f340153eb9f2d1aecc3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    50 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-05-æ›´æ–°"><a href="#2025-06-05-æ›´æ–°" class="headerlink" title="2025-06-05 æ›´æ–°"></a>2025-06-05 æ›´æ–°</h1><h2 id="Revisiting-Continuity-of-Image-Tokens-for-Cross-domain-Few-shot-Learning"><a href="#Revisiting-Continuity-of-Image-Tokens-for-Cross-domain-Few-shot-Learning" class="headerlink" title="Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning"></a>Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning</h2><p><strong>Authors:Shuai Yi, Yixiong Zou, Yuhua Li, Ruixuan Li</strong></p>
<p>Vision Transformer (ViT) has achieved remarkable success due to its large-scale pretraining on general domains, but it still faces challenges when applying it to downstream distant domains that have only scarce training data, which gives rise to the Cross-Domain Few-Shot Learning (CDFSL) task. Inspired by Self-Attentionâ€™s insensitivity to token orders, we find an interesting phenomenon neglected in current works: disrupting the continuity of image tokens (i.e., making pixels not smoothly transited across patches) in ViT leads to a noticeable performance decline in the general (source) domain but only a marginal decrease in downstream target domains. This questions the role of image tokensâ€™ continuity in ViTâ€™s generalization under large domain gaps. In this paper, we delve into this phenomenon for an interpretation. We find continuity aids ViT in learning larger spatial patterns, which are harder to transfer than smaller ones, enlarging domain distances. Meanwhile, it implies that only smaller patterns within each patch could be transferred under extreme domain gaps. Based on this interpretation, we further propose a simple yet effective method for CDFSL that better disrupts the continuity of image tokens, encouraging the model to rely less on large patterns and more on smaller ones. Extensive experiments show the effectiveness of our method in reducing domain gaps and outperforming state-of-the-art works. Codes and models are available at <a target="_blank" rel="noopener" href="https://github.com/shuaiyi308/ReCIT">https://github.com/shuaiyi308/ReCIT</a>. </p>
<blockquote>
<p>Vision Transformerï¼ˆViTï¼‰ç”±äºåœ¨é€šç”¨é¢†åŸŸçš„å¤§è§„æ¨¡é¢„è®­ç»ƒè€Œå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨åº”ç”¨äºåªæœ‰å°‘é‡è®­ç»ƒæ•°æ®çš„ä¸‹æ¸¸é¢†åŸŸæ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™å¼•å‘äº†è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰çš„ä»»åŠ¡ã€‚å—è‡ªæ³¨æ„åŠ›å¯¹ä»¤ç‰Œé¡ºåºçš„ä¸æ•æ„Ÿæ€§çš„å¯å‘ï¼Œæˆ‘ä»¬å‘ç°å½“å‰å·¥ä½œä¸­å¿½ç•¥äº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šç ´åå›¾åƒä»¤ç‰Œè¿ç»­æ€§ï¼ˆå³ï¼Œä½¿åƒç´ åœ¨è¡¥ä¸ä¹‹é—´ä¸å¹³ç¨³è¿‡æ¸¡ï¼‰åœ¨ViTä¸­ä¼šå¯¼è‡´åœ¨ä¸€èˆ¬ï¼ˆæºï¼‰é¢†åŸŸçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä½†åœ¨ä¸‹æ¸¸ç›®æ ‡é¢†åŸŸåªä¼šå¯¼è‡´è½»å¾®çš„æ€§èƒ½ä¸‹é™ã€‚è¿™è´¨ç–‘äº†å›¾åƒä»¤ç‰Œè¿ç»­æ€§åœ¨ViTè·¨å¤§é¢†åŸŸå·®è·æ—¶çš„é€šç”¨åŒ–ä½œç”¨ã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†è¿™ä¸€ç°è±¡ä»¥è¿›è¡Œè§£é‡Šã€‚æˆ‘ä»¬å‘ç°è¿ç»­æ€§æœ‰åŠ©äºViTå­¦ä¹ æ›´å¤§çš„ç©ºé—´æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼æ¯”å°çš„æ¨¡å¼æ›´éš¾è¿ç§»ï¼Œä»è€Œæ‰©å¤§äº†é¢†åŸŸé—´çš„è·ç¦»ã€‚åŒæ—¶ï¼Œå®ƒæš—ç¤ºåœ¨æç«¯çš„é¢†åŸŸå·®è·ä¸‹ï¼Œæ¯ä¸ªè¡¥ä¸å†…çš„å°æ¨¡å¼å¯èƒ½ä¼šå‘ç”Ÿè½¬ç§»ã€‚åŸºäºè¿™ä¸€è§£é‡Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„CDFSLæ–¹æ³•ï¼Œæ›´å¥½åœ°ç ´åäº†å›¾åƒä»¤ç‰Œçš„è¿ç»­æ€§ï¼Œé¼“åŠ±æ¨¡å‹æ›´å°‘åœ°ä¾èµ–å¤§æ¨¡å¼ï¼Œæ›´å¤šåœ°ä¾èµ–å°æ¨¡å¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡å°‘é¢†åŸŸå·®è·å’Œæ€§èƒ½ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shuaiyi308/ReCIT%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/shuaiyi308/ReCITä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03110v1">PDF</a> Accepted by ICML 2025(spotlight)</p>
<p><strong>Summary</strong></p>
<p>ViTåœ¨å¤§è§„æ¨¡é€šç”¨åŸŸé¢„è®­ç»ƒä¸Šå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨åº”ç”¨äºä¸‹æ¸¸çš„è¿œè·ç¦»é¢†åŸŸæ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡ç ”ç©¶äº†ViTä¸­å›¾åƒæ ‡è®°çš„è¿ç»­æ€§å¯¹è·¨åŸŸå°‘æ ·æœ¬å­¦ä¹ çš„å½±å“ï¼Œå‘ç°ç ´åå›¾åƒæ ‡è®°çš„è¿ç»­æ€§æœ‰åŠ©äºæé«˜æ¨¡å‹åœ¨ç›®æ ‡é¢†åŸŸçš„æ€§èƒ½ã€‚é€šè¿‡æ·±å…¥ç ”ç©¶è¿™ä¸€ç°è±¡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡æ›´å¥½åœ°ç ´åå›¾åƒæ ‡è®°çš„è¿ç»­æ€§æ¥å‡å°‘é¢†åŸŸå·®è·ï¼Œå¹¶è¶…è¶Šç°æœ‰çš„å‰æ²¿å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer (ViT) åœ¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨åº”ç”¨äºä¸‹æ¸¸çš„è¿œè·ç¦»é¢†åŸŸæ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç ´åå›¾åƒæ ‡è®°çš„è¿ç»­æ€§ï¼ˆå³åƒç´ åœ¨è¡¥ä¸ä¹‹é—´ä¸å¹³ç¨³è¿‡æ¸¡ï¼‰åœ¨ä¸€èˆ¬ï¼ˆæºï¼‰åŸŸä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œä½†åœ¨ä¸‹æ¸¸ç›®æ ‡åŸŸä»…å¯¼è‡´è½»å¾®ä¸‹é™ã€‚</li>
<li>å›¾åƒæ ‡è®°çš„è¿ç»­æ€§æœ‰åŠ©äºViTå­¦ä¹ æ›´å¤§çš„ç©ºé—´æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼åœ¨æç«¯é¢†åŸŸå·®å¼‚ä¸‹éš¾ä»¥è½¬ç§»ã€‚è¿™æ„å‘³ç€åœ¨æç«¯é¢†åŸŸå·®å¼‚ä¸‹ï¼Œæ¯ä¸ªè¡¥ä¸å†…çš„å°æ¨¡å¼å¯èƒ½è¢«è½¬ç§»ã€‚</li>
<li>åŸºäºè¿™ä¸€å‘ç°ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡æ›´å¥½åœ°ç ´åå›¾åƒæ ‡è®°çš„è¿ç»­æ€§æ¥é¼“åŠ±æ¨¡å‹æ›´å°‘åœ°ä¾èµ–å¤§æ¨¡å¼ï¼Œæ›´å¤šåœ°ä¾èµ–å°æ¨¡å¼ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å‡å°‘é¢†åŸŸå·®è·æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„å‰æ²¿å·¥ä½œã€‚</li>
<li>è®ºæ–‡æä¾›äº†ç›¸å…³çš„ä»£ç å’Œæ¨¡å‹é“¾æ¥ä¾›ç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-387e7aebdffa0599834f1db44b8c9b2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46daf2da3af6abc7c96e18fa530cb24f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14a9186dc06be06f6a3d62b966ef6520.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bc3fb734959cb6804002733ef4794e4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Random-Registers-for-Cross-Domain-Few-Shot-Learning"><a href="#Random-Registers-for-Cross-Domain-Few-Shot-Learning" class="headerlink" title="Random Registers for Cross-Domain Few-Shot Learning"></a>Random Registers for Cross-Domain Few-Shot Learning</h2><p><strong>Authors:Shuai Yi, Yixiong Zou, Yuhua Li, Ruixuan Li</strong></p>
<p>Cross-domain few-shot learning (CDFSL) aims to transfer knowledge from a data-sufficient source domain to data-scarce target domains. Although Vision Transformer (ViT) has shown superior capability in many vision tasks, its transferability against huge domain gaps in CDFSL is still under-explored. In this paper, we find an intriguing phenomenon: during the source-domain training, prompt tuning, as a common way to train ViT, could be harmful for the generalization of ViT in target domains, but setting them to random noises (i.e., random registers) could consistently improve target-domain performance. We then delve into this phenomenon for an interpretation. We find that learnable prompts capture domain information during the training on the source dataset, which views irrelevant visual patterns as vital cues for recognition. This can be viewed as a kind of overfitting and increases the sharpness of the loss landscapes. In contrast, random registers are essentially a novel way of perturbing attention for the sharpness-aware minimization, which helps the model find a flattened minimum in loss landscapes, increasing the transferability. Based on this phenomenon and interpretation, we further propose a simple but effective approach for CDFSL to enhance the perturbation on attention maps by adding random registers on the semantic regions of image tokens, improving the effectiveness and efficiency of random registers. Extensive experiments on four benchmarks validate our rationale and state-of-the-art performance. Codes and models are available at <a target="_blank" rel="noopener" href="https://github.com/shuaiyi308/REAP">https://github.com/shuaiyi308/REAP</a>. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰æ—¨åœ¨å°†ä»æ•°æ®å……è¶³æ¥æºåŸŸçš„çŸ¥è¯†è½¬ç§»åˆ°æ•°æ®ç¨€ç¼ºçš„ç›®æ ‡åŸŸã€‚å°½ç®¡Vision Transformerï¼ˆViTï¼‰åœ¨è®¸å¤šè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨CDFSLä¸­é’ˆå¯¹å·¨å¤§çš„åŸŸå·®å¼‚çš„å¯è½¬ç§»æ€§ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šåœ¨æºåŸŸè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½œä¸ºè®­ç»ƒViTçš„ä¸€ç§å¸¸è§æ–¹æ³•ï¼Œæç¤ºè°ƒæ•´å¯èƒ½ä¼šæŸå®³ViTåœ¨ç›®æ ‡åŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å°†å…¶è®¾ç½®ä¸ºéšæœºå™ªå£°ï¼ˆå³éšæœºå¯„å­˜å™¨ï¼‰å¯ä»¥æŒç»­æé«˜ç›®æ ‡åŸŸçš„æ€§èƒ½ã€‚ç„¶åï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†è¿™ä¸€ç°è±¡å¹¶è¿›è¡Œäº†è§£é‡Šã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨æºæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œå¯å­¦ä¹ çš„æç¤ºä¼šæ•è·åŸŸä¿¡æ¯ï¼Œå°†ä¸ç›¸å…³çš„è§†è§‰æ¨¡å¼è§†ä¸ºè¯†åˆ«çš„å…³é”®çº¿ç´¢ã€‚è¿™å¯ä»¥è¢«è§†ä¸ºä¸€ç§è¿‡æ‹Ÿåˆï¼Œå¹¶å¢åŠ äº†æŸå¤±æ™¯è§‚çš„å°–é”åº¦ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œéšæœºå¯„å­˜å™¨æœ¬è´¨ä¸Šæ˜¯æ‰°åŠ¨æ³¨æ„åŠ›çš„ä¸€ç§æ–°å‹æ–¹å¼ï¼Œç”¨äºå°–é”åº¦æ„ŸçŸ¥æœ€å°åŒ–ï¼Œæœ‰åŠ©äºæ¨¡å‹åœ¨æŸå¤±æ™¯è§‚ä¸­æ‰¾åˆ°å¹³å¦çš„æœ€å°å€¼ï¼Œä»è€Œæé«˜å¯è½¬ç§»æ€§ã€‚åŸºäºè¿™ç§ç°è±¡å’Œè§£é‡Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†é’ˆå¯¹CDFSLçš„ç®€å•æœ‰æ•ˆæ–¹æ³•ï¼Œé€šè¿‡åœ¨å›¾åƒä»¤ç‰Œçš„è¯­ä¹‰åŒºåŸŸä¸Šæ·»åŠ éšæœºå¯„å­˜å™¨æ¥åŠ å¼ºå¯¹æ³¨æ„åŠ›å›¾çš„æ‰°åŠ¨ï¼Œæé«˜äº†éšæœºå¯„å­˜å™¨çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬ç†è®ºçš„åˆç†æ€§å¹¶è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/shuaiyi308/REAP%E3%80%82">https://github.com/shuaiyi308/REAPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02843v1">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰ä¸­ï¼ŒVision Transformerï¼ˆViTï¼‰åœ¨æºåŸŸè®­ç»ƒæ—¶çš„prompt tuningå¯¹ç›®æ ‡åŸŸæ³›åŒ–æ€§èƒ½çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œç›¸è¾ƒäºä½¿ç”¨prompt tuningï¼Œå°†æ¨¡å‹è®¾ç½®ä¸ºéšæœºå™ªå£°ï¼ˆå³éšæœºå¯„å­˜å™¨ï¼‰å¯ä»¥æé«˜ç›®æ ‡åŸŸçš„æ€§èƒ½ã€‚æœ¬æ–‡è¿›ä¸€æ­¥æ¢ç©¶å…¶åŸå› ï¼Œå‘ç°å­¦ä¹ åˆ°çš„æç¤ºåœ¨æºæ•°æ®é›†è®­ç»ƒæ—¶ä¼šæ•è·åŸŸä¿¡æ¯ï¼Œè¿™å¯èƒ½ä½¿æ¨¡å‹å¯¹ç‰¹å®šæ¨¡å¼çš„è§†è§‰å›¾æ¡ˆè¿‡äºæ•æ„Ÿè€Œé™ä½æ³›åŒ–æ€§èƒ½ã€‚ä¸æ­¤ç›¸åï¼Œéšæœºå¯„å­˜å™¨æœ¬è´¨ä¸Šæ˜¯ä¸€ç§å¯¹æ³¨æ„åŠ›è¿›è¡Œæ‰°åŠ¨çš„æ–¹æ³•ï¼Œæœ‰åŠ©äºæ¨¡å‹åœ¨æŸå¤±æ™¯è§‚ä¸­æ‰¾åˆ°å¹³å¦çš„æœ€å°å€¼ï¼Œä»è€Œæé«˜è¿ç§»èƒ½åŠ›ã€‚åŸºäºæ­¤ç°è±¡å’Œè§£é‡Šï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹CDFSLçš„ç®€å•æœ‰æ•ˆçš„éšæœºå¯„å­˜å™¨å¼ºåŒ–æ³¨æ„åŠ›å›¾çš„æ–¹æ¡ˆï¼Œå®ç°è¾ƒé«˜çš„æ•ˆæœã€‚åŒæ—¶æœ‰ä¼—å¤šå®éªŒéªŒè¯äº†è¯¥æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformeråœ¨è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ä¸­çš„è¿ç§»èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>æç¤ºè®­ç»ƒå¯èƒ½ä¼šæ•è·æºåŸŸçš„ç‰¹å®šä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ç›®æ ‡åŸŸçš„æ³›åŒ–æ€§èƒ½ä¸‹é™ã€‚</li>
<li>éšæœºå™ªå£°ï¼ˆéšæœºå¯„å­˜å™¨ï¼‰çš„è®¾å®šæœ‰åŠ©äºæ¨¡å‹æé«˜ç›®æ ‡åŸŸçš„æ€§èƒ½è¡¨ç°ã€‚è¿™ç§å™ªå£°å¯ä»¥ä½¿æ¨¡å‹æ³¨æ„åŠ›å¾—ä»¥æ‰°åŠ¨å¹¶é™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚</li>
<li>é€šè¿‡éšæœºå¯„å­˜å™¨å¼ºåŒ–æ³¨æ„åŠ›å›¾çš„æ–¹æ³•å¯ä»¥è¿›ä¸€æ­¥æå‡æ¨¡å‹åœ¨CDFSLä¸­çš„è¡¨ç°ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02843">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7aec4b4cb3886684cfb8c3ee27a47343.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba799daefa60168611660a07c87de0f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8876359eaf4c9ddfe33f6de1b6bb1528.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb465ff09555a14b2e7c8c641d0818f8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Self-Disentanglement-and-Re-Composition-for-Cross-Domain-Few-Shot-Segmentation"><a href="#Self-Disentanglement-and-Re-Composition-for-Cross-Domain-Few-Shot-Segmentation" class="headerlink" title="Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot   Segmentation"></a>Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot   Segmentation</h2><p><strong>Authors:Jintao Tong, Yixiong Zou, Guangyao Chen, Yuhua Li, Ruixuan Li</strong></p>
<p>Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a source-domain dataset to unseen target-domain datasets with limited annotations. Current methods typically compare the distance between training and testing samples for mask prediction. However, we find an entanglement problem exists in this widely adopted method, which tends to bind sourcedomain patterns together and make each of them hard to transfer. In this paper, we aim to address this problem for the CD-FSS task. We first find a natural decomposition of the ViT structure, based on which we delve into the entanglement problem for an interpretation. We find the decomposed ViT components are crossly compared between images in distance calculation, where the rational comparisons are entangled with those meaningless ones by their equal importance, leading to the entanglement problem. Based on this interpretation, we further propose to address the entanglement problem by learning to weigh for all comparisons of ViT components, which learn disentangled features and re-compose them for the CD-FSS task, benefiting both the generalization and finetuning. Experiments show that our model outperforms the state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under 1-shot and 5-shot settings, respectively. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰æ—¨åœ¨å°†æºåŸŸæ•°æ®é›†çš„çŸ¥è¯†è½¬ç§»åˆ°æœªè§è¿‡çš„ç›®æ ‡åŸŸæ•°æ®é›†ä¸Šï¼Œä¸”ç›®æ ‡åŸŸæ•°æ®é›†æ ‡æ³¨æœ‰é™ã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸é€šè¿‡æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬ä¹‹é—´çš„è·ç¦»æ¥è¿›è¡Œæ©è†œé¢„æµ‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å¹¿æ³›é‡‡ç”¨çš„è¿™ç§æ–¹æ³•ä¸­å­˜åœ¨çº ç¼ é—®é¢˜ï¼Œå®ƒå€¾å‘äºå°†æºåŸŸæ¨¡å¼ç»‘å®šåœ¨ä¸€èµ·ï¼Œä½¿å¾—æ¯ä¸ªæ¨¡å¼éƒ½éš¾ä»¥è½¬ç§»ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³CD-FSSä»»åŠ¡ä¸­çš„è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†ViTç»“æ„çš„è‡ªç„¶åˆ†è§£ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ·±å…¥ç ”ç©¶äº†çº ç¼ é—®é¢˜ä»¥è¿›è¡Œè§£é‡Šã€‚æˆ‘ä»¬å‘ç°åˆ†è§£åçš„ViTç»„ä»¶åœ¨è·ç¦»è®¡ç®—æ—¶ä¼šè¿›è¡Œå›¾åƒé—´çš„äº¤å‰æ¯”è¾ƒï¼Œå…¶ä¸­åˆç†çš„æ¯”è¾ƒä¸æ— æ„ä¹‰çš„æ¯”è¾ƒçº ç¼ åœ¨ä¸€èµ·ï¼Œå…·æœ‰åŒç­‰é‡è¦æ€§ï¼Œå¯¼è‡´äº†çº ç¼ é—®é¢˜ã€‚åŸºäºè¿™ä¸€è§£é‡Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºé€šè¿‡ä¸ºæ‰€æœ‰ViTç»„ä»¶çš„æ¯”è¾ƒå­¦ä¹ æƒé‡æ¥è§£å†³çº ç¼ é—®é¢˜ï¼Œè¿™äº›ç»„ä»¶å­¦ä¹ è§£è€¦çš„ç‰¹å¾å¹¶é‡æ–°ç»„åˆå®ƒä»¬ä»¥ç”¨äºCD-FSSä»»åŠ¡ï¼Œæœ‰ç›Šäºæ³›åŒ–å’Œå¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šè¶…è¿‡äº†æœ€æ–°çš„CD-FSSæ–¹æ³•ï¼Œåœ¨1æ¬¡å’Œ5æ¬¡æ‹æ‘„è®¾ç½®ä¸‹åˆ†åˆ«æé«˜äº†1.92%å’Œ1.88%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02677v1">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰ä»»åŠ¡ä¸­çš„çº ç¼ é—®é¢˜ï¼Œå¹¶å¯¹æ­¤æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚æ–‡ç« é¦–å…ˆåŸºäºViTç»“æ„è¿›è¡Œäº†è‡ªç„¶åˆ†è§£ï¼Œå‘ç°äº†çº ç¼ é—®é¢˜çš„å­˜åœ¨ï¼Œç„¶åæå‡ºäº†é€šè¿‡å­¦ä¹ ä¸ºViTç»„ä»¶çš„å¯¹æ¯”è¿›è¡ŒåŠ æƒæ¥è§£å†³çº ç¼ é—®é¢˜çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå­¦ä¹ è§£çº ç¼ ç‰¹å¾å¹¶é‡æ–°ç»„åˆï¼Œæœ‰åŠ©äºæé«˜CD-FSSä»»åŠ¡çš„æ³›åŒ–å’Œå¾®è°ƒèƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šè¶…è¿‡äº†ç°æœ‰CD-FSSæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSSä»»åŠ¡æ—¨åœ¨ä»æºåŸŸæ•°æ®é›†å‘æœªè§è¿‡çš„ç›®æ ‡åŸŸæ•°æ®é›†è¿›è¡ŒçŸ¥è¯†è¿ç§»ï¼Œä¸”ç›®æ ‡æ•°æ®é›†æ ‡æ³¨æœ‰é™ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦é€šè¿‡æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬ä¹‹é—´çš„è·ç¦»æ¥è¿›è¡Œæ©è†œé¢„æµ‹ï¼Œä½†å­˜åœ¨çº ç¼ é—®é¢˜ï¼Œå³æºåŸŸæ¨¡å¼ç›¸äº’ç»‘å®šï¼Œéš¾ä»¥è¿›è¡Œè¿ç§»ã€‚</li>
<li>æ–‡ç« åŸºäºViTç»“æ„è¿›è¡Œäº†è‡ªç„¶åˆ†è§£ï¼Œå‘ç°äº†çº ç¼ é—®é¢˜çš„å­˜åœ¨ï¼Œå¹¶å¯¹æ­¤è¿›è¡Œäº†è¯¦ç»†è§£é‡Šã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºï¼ŒViTç»„ä»¶åœ¨è·ç¦»è®¡ç®—ä¸­è¿›è¡Œäº¤å‰æ¯”è¾ƒæ—¶ï¼Œæœ‰æ„ä¹‰çš„æ¯”è¾ƒå’Œæ— æ„ä¹‰çš„æ¯”è¾ƒçº ç¼ åœ¨ä¸€èµ·ï¼Œå¯¼è‡´çº ç¼ é—®é¢˜ã€‚</li>
<li>ä¸ºäº†è§£å†³çº ç¼ é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†é€šè¿‡å­¦ä¹ ä¸ºViTç»„ä»¶çš„å¯¹æ¯”è¿›è¡ŒåŠ æƒçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå­¦ä¹ è§£çº ç¼ ç‰¹å¾å¹¶é‡æ–°ç»„åˆã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºæé«˜CD-FSSä»»åŠ¡çš„æ³›åŒ–å’Œå¾®è°ƒèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b06d2620bc517f6db45a09e4e5109f94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-402ca82f88135de987c6d21d1ee47e30.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb9f38507982ac949105175dc8681ca5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9e4a098e266a732b3d12d528ca5c134.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b27ec9b41c4c61c1c92c943fecd8f6fe.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multi-modal-brain-MRI-synthesis-based-on-SwinUNETR"><a href="#Multi-modal-brain-MRI-synthesis-based-on-SwinUNETR" class="headerlink" title="Multi-modal brain MRI synthesis based on SwinUNETR"></a>Multi-modal brain MRI synthesis based on SwinUNETR</h2><p><strong>Authors:Haowen Pang, Weiyan Guo, Chuyang Ye</strong></p>
<p>Multi-modal brain magnetic resonance imaging (MRI) plays a crucial role in clinical diagnostics by providing complementary information across different imaging modalities. However, a common challenge in clinical practice is missing MRI modalities. In this paper, we apply SwinUNETR to the synthesize of missing modalities in brain MRI. SwinUNETR is a novel neural network architecture designed for medical image analysis, integrating the strengths of Swin Transformer and convolutional neural networks (CNNs). The Swin Transformer, a variant of the Vision Transformer (ViT), incorporates hierarchical feature extraction and window-based self-attention mechanisms, enabling it to capture both local and global contextual information effectively. By combining the Swin Transformer with CNNs, SwinUNETR merges global context awareness with detailed spatial resolution. This hybrid approach addresses the challenges posed by the varying modality characteristics and complex brain structures, facilitating the generation of accurate and realistic synthetic images. We evaluate the performance of SwinUNETR on brain MRI datasets and demonstrate its superior capability in generating clinically valuable images. Our results show significant improvements in image quality, anatomical consistency, and diagnostic value. </p>
<blockquote>
<p>å¤šæ¨¡æ€è„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ä¸´åºŠè¯Šæ–­ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå› ä¸ºå®ƒèƒ½æä¾›ä¸åŒæˆåƒæ¨¡å¼ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯ã€‚ç„¶è€Œï¼Œåœ¨ä¸´åºŠå®è·µä¸­ï¼Œä¸€ä¸ªå¸¸è§çš„æŒ‘æˆ˜æ˜¯ç¼ºå¤±MRIæ¨¡å¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†SwinUNETRåº”ç”¨äºè„‘MRIä¸­ç¼ºå¤±æ¨¡å¼çš„åˆæˆã€‚SwinUNETRæ˜¯ä¸€ç§ä¸ºåŒ»å­¦å›¾åƒåˆ†æè®¾è®¡çš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œèåˆäº†Swin Transformerå’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ä¼˜åŠ¿ã€‚Swin Transformeræ˜¯Vision Transformerï¼ˆViTï¼‰çš„ä¸€ç§å˜ä½“ï¼Œç»“åˆäº†åˆ†å±‚ç‰¹å¾æå–å’ŒåŸºäºçª—å£çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é€šè¿‡å°†Swin Transformerä¸CNNç›¸ç»“åˆï¼ŒSwinUNETRèåˆäº†å…¨å±€ä¸Šä¸‹æ–‡æ„è¯†å’Œè¯¦ç»†çš„ç©ºé—´åˆ†è¾¨ç‡ã€‚è¿™ç§æ··åˆæ–¹æ³•è§£å†³äº†ä¸åŒæ¨¡æ€ç‰¹å¾å’Œå¤æ‚è„‘ç»“æ„æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæœ‰åˆ©äºç”Ÿæˆå‡†ç¡®å’Œé€¼çœŸçš„åˆæˆå›¾åƒã€‚æˆ‘ä»¬åœ¨è„‘MRIæ•°æ®é›†ä¸Šè¯„ä¼°äº†SwinUNETRçš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ç”Ÿæˆå…·æœ‰ä¸´åºŠä»·å€¼å›¾åƒæ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœåœ¨å›¾åƒè´¨é‡ã€è§£å‰–ä¸€è‡´æ€§å’Œè¯Šæ–­ä»·å€¼æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02467v1">PDF</a> 9 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€è„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ä¸´åºŠè¯Šæ–­ä¸­çš„é‡è¦ä½œç”¨ï¼Œæœ¬æ–‡åˆ©ç”¨SwinUNETRåˆæˆç¼ºå¤±æ¨¡æ€çš„è„‘MRIã€‚SwinUNETRæ˜¯ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†æçš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç»“åˆäº†Swin Transformerå’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ä¼˜åŠ¿ã€‚è¯¥æ¶æ„èåˆäº†å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œç²¾ç»†çš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œèƒ½æœ‰æ•ˆåº”å¯¹ä¸åŒæ¨¡æ€ç‰¹å¾å’Œå¤æ‚è„‘ç»“æ„å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œç”Ÿæˆå‡†ç¡®é€¼çœŸçš„åˆæˆå›¾åƒã€‚åœ¨è„‘MRIæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒSwinUNETRåœ¨å›¾åƒè´¨é‡ã€è§£å‰–ä¸€è‡´æ€§å’Œè¯Šæ–­ä»·å€¼æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€MRIåœ¨ä¸´åºŠè¯Šæ–­ä¸­æä¾›ä¸åŒæˆåƒæ¨¡æ€çš„äº’è¡¥ä¿¡æ¯ï¼Œå¯¹ç¼ºå¤±æ¨¡æ€çš„åˆæˆå…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>SwinUNETRæ˜¯ä¸€ç§ç»“åˆäº†Swin Transformerå’ŒCNNçš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†æã€‚</li>
<li>Swin Transformeræ˜¯Vision Transformerï¼ˆViTï¼‰çš„ä¸€ç§å˜ä½“ï¼Œå…·æœ‰å±‚æ¬¡åŒ–ç‰¹å¾æå–å’ŒåŸºäºçª—å£çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>SwinUNETRå°†å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥ä¸ç²¾ç»†çš„ç©ºé—´åˆ†è¾¨ç‡ç›¸ç»“åˆï¼Œåº”å¯¹ä¸åŒæ¨¡æ€ç‰¹å¾å’Œå¤æ‚è„‘ç»“æ„çš„æŒ‘æˆ˜ã€‚</li>
<li>SwinUNETRåœ¨è„‘MRIæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¯„ä¼°è¡¨æ˜å…¶åœ¨å›¾åƒè´¨é‡ã€è§£å‰–ä¸€è‡´æ€§å’Œè¯Šæ–­ä»·å€¼æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚</li>
<li>SwinUNETRç”Ÿæˆçš„åˆæˆå›¾åƒå…·æœ‰å‡†ç¡®æ€§å’Œé€¼çœŸæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸´åºŠåº”ç”¨ä¸­æœ‰æ½œåŠ›æé«˜è¯Šæ–­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-101d4e8c0e012b217349f8c3a7cb6d6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2960ebb0017918f9368b90f15a99fe5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b3f299fa53d6a0a187b0f6c228aa01f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11edaf5738dc5a5d9be2ca99c1e933e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc66021e19ac7c1de23c57cf8e2c5b0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fedc952046aad915fa0cb77cd5e4e41.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Video-Level-Language-Driven-Video-Based-Visible-Infrared-Person-Re-Identification"><a href="#Video-Level-Language-Driven-Video-Based-Visible-Infrared-Person-Re-Identification" class="headerlink" title="Video-Level Language-Driven Video-Based Visible-Infrared Person   Re-Identification"></a>Video-Level Language-Driven Video-Based Visible-Infrared Person   Re-Identification</h2><p><strong>Authors:Shuang Li, Jiaxu Leng, Changjiang Kuang, Mingpi Tan, Xinbo Gao</strong></p>
<p>Video-based Visible-Infrared Person Re-Identification (VVI-ReID) aims to match pedestrian sequences across modalities by extracting modality-invariant sequence-level features. As a high-level semantic representation, language provides a consistent description of pedestrian characteristics in both infrared and visible modalities. Leveraging the Contrastive Language-Image Pre-training (CLIP) model to generate video-level language prompts and guide the learning of modality-invariant sequence-level features is theoretically feasible. However, the challenge of generating and utilizing modality-shared video-level language prompts to address modality gaps remains a critical problem. To address this problem, we propose a simple yet powerful framework, video-level language-driven VVI-ReID (VLD), which consists of two core modules: invariant-modality language prompting (IMLP) and spatial-temporal prompting (STP). IMLP employs a joint fine-tuning strategy for the visual encoder and the prompt learner to effectively generate modality-shared text prompts and align them with visual features from different modalities in CLIPâ€™s multimodal space, thereby mitigating modality differences. Additionally, STP models spatiotemporal information through two submodules, the spatial-temporal hub (STH) and spatial-temporal aggregation (STA), which further enhance IMLP by incorporating spatiotemporal information into text prompts. The STH aggregates and diffuses spatiotemporal information into the [CLS] token of each frame across the vision transformer (ViT) layers, whereas STA introduces dedicated identity-level loss and specialized multihead attention to ensure that the STH focuses on identity-relevant spatiotemporal feature aggregation. The VLD framework achieves state-of-the-art results on two VVI-ReID benchmarks. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/Visuang/VLD">https://github.com/Visuang/VLD</a>. </p>
<blockquote>
<p>åŸºäºè§†é¢‘çš„å¯è§å…‰-çº¢å¤–è¡Œäººå†è¯†åˆ«ï¼ˆVVI-ReIDï¼‰æ—¨åœ¨é€šè¿‡æå–è·¨æ¨¡æ€çš„åºåˆ—çº§ç‰¹å¾æ¥åŒ¹é…è·¨æ¨¡æ€çš„è¡Œäººåºåˆ—ã€‚ä½œä¸ºä¸€ç§é«˜çº§è¯­ä¹‰è¡¨ç¤ºï¼Œè¯­è¨€ä¸ºçº¢å¤–å’Œå¯è§å…‰ä¸¤ç§æ¨¡æ€ä¸‹çš„è¡Œäººç‰¹å¾æä¾›äº†è¿è´¯çš„æè¿°ã€‚åˆ©ç”¨å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ¨¡å‹ç”Ÿæˆè§†é¢‘çº§è¯­è¨€æç¤ºå¹¶å¼•å¯¼å­¦ä¹ è·¨æ¨¡æ€çš„åºåˆ—çº§ç‰¹å¾åœ¨ç†è®ºä¸Šå¯è¡Œã€‚ç„¶è€Œï¼Œç”Ÿæˆå’Œåˆ©ç”¨è·¨æ¨¡æ€å…±äº«çš„è§†é¢‘çº§è¯­è¨€æç¤ºæ¥è§£å†³æ¨¡æ€å·®å¼‚çš„æŒ‘æˆ˜ä»ç„¶æ˜¯å…³é”®æ€§é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•è€Œå¼ºå¤§çš„æ¡†æ¶ï¼Œå³è§†é¢‘çº§è¯­è¨€é©±åŠ¨VVI-ReIDï¼ˆVLDï¼‰ï¼Œå®ƒåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šä¸å˜æ¨¡æ€è¯­è¨€æç¤ºï¼ˆIMLPï¼‰å’Œç©ºé—´æ—¶é—´æç¤ºï¼ˆSTPï¼‰ã€‚IMLPé‡‡ç”¨è”åˆå¾®è°ƒç­–ç•¥ï¼Œå¯¹è§†è§‰ç¼–ç å™¨å’Œæç¤ºå­¦ä¹ è€…è¿›è¡Œæœ‰æ•ˆè®­ç»ƒï¼Œä»¥ç”Ÿæˆè·¨æ¨¡æ€å…±äº«æ–‡æœ¬æç¤ºï¼Œå¹¶å°†å®ƒä»¬ä¸CLIPå¤šæ¨¡æ€ç©ºé—´ä¸­çš„ä¸åŒæ¨¡æ€çš„è§†è§‰ç‰¹å¾å¯¹é½ï¼Œä»è€Œå‡è½»æ¨¡æ€å·®å¼‚ã€‚æ­¤å¤–ï¼ŒSTPé€šè¿‡ä¸¤ä¸ªå­æ¨¡å—ï¼Œå³ç©ºé—´æ—¶é—´ä¸­å¿ƒï¼ˆSTHï¼‰å’Œç©ºé—´æ—¶é—´èšåˆï¼ˆSTAï¼‰ï¼Œå¯¹æ—¶ç©ºä¿¡æ¯è¿›è¡Œå»ºæ¨¡ï¼Œé€šè¿‡å°†æ—¶ç©ºä¿¡æ¯èå…¥æ–‡æœ¬æç¤ºæ¥è¿›ä¸€æ­¥å¢å¼ºIMLPã€‚STHå°†æ¯å¸§çš„æ—¶ç©ºä¿¡æ¯èšåˆå¹¶æ‰©æ•£åˆ°è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å„å±‚çš„[CLS]æ ‡è®°ä¸­ï¼Œè€ŒSTAå¼•å…¥ä¸“ç”¨çš„èº«ä»½çº§æŸå¤±å’Œç‰¹æ®Šçš„å¤šå¤´æ³¨æ„åŠ›ï¼Œä»¥ç¡®ä¿STHä¸“æ³¨äºèº«ä»½ç›¸å…³çš„æ—¶ç©ºç‰¹å¾èšåˆã€‚VLDæ¡†æ¶åœ¨ä¸¤ä¸ªVVI-ReIDåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Visuang/VLD%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Visuang/VLDä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02439v1">PDF</a> Accepted by IEEE TIFS</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºè§†é¢‘çš„å¤šæ¨¡æ€è¡Œäººå†è¯†åˆ«ï¼ˆVVI-ReIDï¼‰æ—¨åœ¨é€šè¿‡æå–è·¨æ¨¡æ€çš„åºåˆ—çº§ç‰¹å¾æ¥åŒ¹é…ä¸åŒæ¨¡æ€ä¸‹çš„è¡Œäººåºåˆ—ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§ç®€æ´è€Œå¼ºå¤§çš„æ¡†æ¶ï¼Œå³è§†é¢‘çº§è¯­è¨€é©±åŠ¨çš„VVI-ReIDï¼ˆVLDï¼‰ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šä¸å˜æ¨¡æ€è¯­è¨€æç¤ºï¼ˆIMLPï¼‰å’Œæ—¶ç©ºæç¤ºï¼ˆSTPï¼‰ã€‚IMLPé‡‡ç”¨è”åˆå¾®è°ƒç­–ç•¥ï¼Œå¯¹è§†è§‰ç¼–ç å™¨å’Œæç¤ºå­¦ä¹ è€…è¿›è¡Œæœ‰æ•ˆè®­ç»ƒï¼Œç”Ÿæˆè·¨æ¨¡æ€å…±äº«æ–‡æœ¬æç¤ºï¼Œå¹¶å°†å®ƒä»¬ä¸ä¸åŒæ¨¡æ€çš„è§†è§‰ç‰¹å¾å¯¹é½åˆ°CLIPçš„å¤šæ¨¡æ€ç©ºé—´ä¸­ï¼Œä»è€Œå‡è½»æ¨¡æ€å·®å¼‚ã€‚STPæ¨¡å—é€šè¿‡ä¸¤ä¸ªå­æ¨¡å—ï¼ˆæ—¶ç©ºä¸­å¿ƒå’Œæ—¶ç©ºèšåˆï¼‰å°†æ—¶ç©ºä¿¡æ¯èå…¥æ–‡æœ¬æç¤ºä¸­ï¼Œè¿›ä¸€æ­¥å¢å¼ºIMLPçš„æ•ˆæœã€‚è¯¥æ¡†æ¶åœ¨ä¸¤é¡¹VVI-ReIDåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Visuang/VLD%E3%80%82">https://github.com/Visuang/VLDã€‚</a></p>
<p><strong>è¦ç‚¹æç‚¼</strong></p>
<ol>
<li>VVI-ReIDçš„ç›®æ ‡æ˜¯åŒ¹é…è·¨æ¨¡æ€çš„è¡Œäººåºåˆ—ï¼Œé€šè¿‡æå–åºåˆ—çº§ç‰¹å¾æ¥è¿›è¡Œè¯†åˆ«ã€‚</li>
<li>åˆ©ç”¨Contrastive Language-Image Pre-training (CLIP)æ¨¡å‹ç”Ÿæˆè§†é¢‘çº§è¯­è¨€æç¤ºï¼Œä¸ºè§£å†³æ¨¡æ€å·®å¼‚é—®é¢˜æä¾›äº†ç†è®ºå¯è¡Œæ€§ã€‚</li>
<li>æå‡ºçš„è§†é¢‘çº§è¯­è¨€é©±åŠ¨æ¡†æ¶ï¼ˆVLDï¼‰åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šä¸å˜æ¨¡æ€è¯­è¨€æç¤ºï¼ˆIMLPï¼‰å’Œæ—¶ç©ºæç¤ºï¼ˆSTPï¼‰ã€‚</li>
<li>IMLPæ¨¡å—é€šè¿‡è”åˆå¾®è°ƒç­–ç•¥ç”Ÿæˆè·¨æ¨¡æ€å…±äº«æ–‡æœ¬æç¤ºï¼Œå¹¶å‡è½»æ¨¡æ€å·®å¼‚ã€‚</li>
<li>STPæ¨¡å—é€šè¿‡æ—¶ç©ºä¸­å¿ƒå’Œæ—¶ç©ºèšåˆä¸¤ä¸ªå­æ¨¡å—èå…¥æ—¶ç©ºä¿¡æ¯ï¼Œè¿›ä¸€æ­¥å¢å¼ºè¯†åˆ«æ•ˆæœã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨VVI-ReIDçš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c03d68cf92104a7d640221b626161d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed87dd759899125534f98259ebfe344c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-460028b1f2849d37712b4dc11d415b60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1acee7f96b73e0ed08ef726c4a069290.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Dual-Process-Image-Generation"><a href="#Dual-Process-Image-Generation" class="headerlink" title="Dual-Process Image Generation"></a>Dual-Process Image Generation</h2><p><strong>Authors:Grace Luo, Jonathan Granskog, Aleksander Holynski, Trevor Darrell</strong></p>
<p>Prior methods for controlling image generation are limited in their ability to be taught new tasks. In contrast, vision-language models, or VLMs, can learn tasks in-context and produce the correct outputs for a given input. We propose a dual-process distillation scheme that allows feed-forward image generators to learn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the generated images and backpropagates this gradient to update the weights of the image generator. Our general framework enables a wide variety of new control tasks through the same text-and-image based interface. We showcase a handful of applications of this technique for different types of control signals, such as commonsense inferences and visual prompts. With our method, users can implement multimodal controls for properties such as color palette, line weight, horizon position, and relative depth within a matter of minutes. Project page: <a target="_blank" rel="noopener" href="https://dual-process.github.io/">https://dual-process.github.io</a>. </p>
<blockquote>
<p>å…ˆå‰æ§åˆ¶å›¾åƒç”Ÿæˆçš„æ–¹æ³•åœ¨æ•™æˆæ–°ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›æœ‰é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¯ä»¥åœ¨ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ ä»»åŠ¡å¹¶ä¸ºç»™å®šè¾“å…¥äº§ç”Ÿæ­£ç¡®çš„è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒè¿‡ç¨‹è’¸é¦æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå…è®¸å‰é¦ˆå›¾åƒç”Ÿæˆå™¨ä»æ·±æ€ç†Ÿè™‘çš„VLMä¸­å­¦ä¹ æ–°ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆä½¿ç”¨VLMå¯¹ç”Ÿæˆçš„å›¾åƒè¿›è¡Œè¯„åˆ†ï¼Œå¹¶å°†æ­¤æ¢¯åº¦åå‘ä¼ æ’­ä»¥æ›´æ–°å›¾åƒç”Ÿæˆå™¨çš„æƒé‡ã€‚æˆ‘ä»¬çš„é€šç”¨æ¡†æ¶é€šè¿‡ç›¸åŒçš„æ–‡æœ¬å’Œå›¾åƒæ¥å£æ”¯æŒå¤šç§æ–°æ§åˆ¶ä»»åŠ¡ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¯¥æŠ€æœ¯åœ¨ä¸åŒç±»å‹çš„æ§åˆ¶ä¿¡å·æ–¹é¢çš„å‡ ä¸ªåº”ç”¨ï¼Œå¦‚å¸¸è¯†æ¨ç†å’Œè§†è§‰æç¤ºã€‚ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç”¨æˆ·å¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å®ç°é¢œè‰²è°ƒã€çº¿æ¡ç²—ç»†ã€åœ°å¹³ä½ç½®ä»¥åŠç›¸å¯¹æ·±åº¦ç­‰å¤šæ¨¡å¼æ§åˆ¶ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://dual-process.github.io./">https://dual-process.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01955v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŒè¿‡ç¨‹è’¸é¦æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå…è®¸å‰é¦ˆå›¾åƒç”Ÿæˆå™¨ä»æ·±æ€ç†Ÿè™‘çš„è§†ç•Œè¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å­¦ä¹ æ–°ä»»åŠ¡ã€‚é€šè¿‡åˆ©ç”¨VLMå¯¹ç”Ÿæˆçš„å›¾åƒè¿›è¡Œè¯„åˆ†å¹¶å°†æ¢¯åº¦åå‘ä¼ æ’­ä»¥æ›´æ–°å›¾åƒç”Ÿæˆå™¨çš„æƒé‡ï¼Œè¯¥æ–¹æ¡ˆå®ç°äº†æ–‡æœ¬å’Œå›¾åƒåŸºäºæ¥å£çš„å¤šæ ·åŒ–æ–°æ§åˆ¶ä»»åŠ¡ã€‚è¯¥æŠ€æœ¯å¯å¿«é€Ÿå®ç°å¤šç§æ§åˆ¶ä¿¡å·çš„åº”ç”¨ï¼Œå¦‚å¸¸è¯†æ¨ç†å’Œè§†è§‰æç¤ºï¼Œç”¨æˆ·å¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å®ç°é¢œè‰²è°ƒè‰²æ¿ã€çº¿æ¡ç²—ç»†ã€åœ°å¹³çº¿ä½ç½®å’Œç›¸å¯¹æ·±åº¦çš„å¤šæ¨¡å¼æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å›¾åƒç”Ÿæˆæ§åˆ¶æ–¹æ³•å­˜åœ¨æ— æ³•å­¦ä¹ æ–°ä»»åŠ¡çš„å±€é™æ€§ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯ä»¥åœ¨ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ æ–°ä»»åŠ¡å¹¶ä¸ºç»™å®šè¾“å…¥äº§ç”Ÿæ­£ç¡®è¾“å‡ºã€‚</li>
<li>æå‡ºçš„åŒè¿‡ç¨‹è’¸é¦æ–¹æ¡ˆå…è®¸å‰é¦ˆå›¾åƒç”Ÿæˆå™¨ä»æ·±æ€ç†Ÿè™‘çš„VLMså­¦ä¹ æ–°ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ¡ˆä½¿ç”¨VLMå¯¹ç”Ÿæˆçš„å›¾åƒè¿›è¡Œè¯„åˆ†ï¼Œå¹¶é€šè¿‡åå‘ä¼ æ’­æ›´æ–°å›¾åƒç”Ÿæˆå™¨çš„æƒé‡ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ–‡æœ¬å’Œå›¾åƒæ¥å£æ”¯æŒå¤šç§æ–°æ§åˆ¶ä»»åŠ¡ã€‚</li>
<li>è¯¥æŠ€æœ¯å¯ä»¥å¿«é€Ÿå®ç°å¤šç§æ§åˆ¶ä¿¡å·çš„åº”ç”¨ï¼Œå¦‚å¸¸è¯†æ¨ç†å’Œè§†è§‰æç¤ºã€‚</li>
<li>ç”¨æˆ·å¯ä»¥è¿…é€Ÿå®ç°é¢œè‰²è°ƒè‰²æ¿ã€çº¿æ¡ç²—ç»†ã€åœ°å¹³çº¿ä½ç½®å’Œç›¸å¯¹æ·±åº¦çš„å¤šæ¨¡å¼æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-87755e022352dadfba93506f95cc73fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-034aa7dcb97c5bb0150911e509c2c6b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-326767d00ef7c16d907c33738e452dd3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5382c3f8c112bb5dd2c9141f61b170a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb413109e3a1d076af57ee969343695f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Speed-up-of-Vision-Transformer-Models-by-Attention-aware-Token-Filtering"><a href="#Speed-up-of-Vision-Transformer-Models-by-Attention-aware-Token-Filtering" class="headerlink" title="Speed-up of Vision Transformer Models by Attention-aware Token Filtering"></a>Speed-up of Vision Transformer Models by Attention-aware Token Filtering</h2><p><strong>Authors:Takahiro Naruko, Hiroaki Akutsu</strong></p>
<p>Vision Transformer (ViT) models have made breakthroughs in image embedding extraction, which provide state-of-the-art performance in tasks such as zero-shot image classification. However, the models suffer from a high computational burden. In this paper, we propose a novel speed-up method for ViT models called Attention-aware Token Filtering (ATF). ATF consists of two main ideas: a novel token filtering module and a filtering strategy. The token filtering module is introduced between a tokenizer and a transformer encoder of the ViT model, without modifying or fine-tuning of the transformer encoder. The module filters out tokens inputted to the encoder so that it keeps tokens in regions of specific object types dynamically and keeps tokens in regions that statically receive high attention in the transformer encoder. This filtering strategy maintains task accuracy while filtering out tokens inputted to the transformer encoder. Evaluation results on retrieval tasks show that ATF provides $2.8\times$ speed-up to a ViT model, SigLIP, while maintaining the retrieval recall rate. </p>
<blockquote>
<p>Vision Transformerï¼ˆViTï¼‰æ¨¡å‹åœ¨å›¾åƒåµŒå…¥æå–æ–¹é¢å–å¾—äº†çªç ´ï¼Œå…¶åœ¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é¢ä¸´ç€è®¡ç®—è´Ÿæ‹…è¾ƒå¤§çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºåŠ é€ŸViTæ¨¡å‹çš„æ–°å‹æ–¹æ³•ï¼Œç§°ä¸ºæ³¨æ„åŠ›æ„ŸçŸ¥ä»¤ç‰Œè¿‡æ»¤ï¼ˆATFï¼‰ã€‚ATFä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ€æƒ³ï¼šä¸€ä¸ªæ–°é¢–çš„ä»¤ç‰Œè¿‡æ»¤æ¨¡å—å’Œè¿‡æ»¤ç­–ç•¥ã€‚ä»¤ç‰Œè¿‡æ»¤æ¨¡å—è¢«å¼•å…¥åˆ°ViTæ¨¡å‹çš„ä»¤ç‰Œå™¨å’Œå˜å‹å™¨ç¼–ç å™¨ä¹‹é—´ï¼Œæ— éœ€ä¿®æ”¹æˆ–å¾®è°ƒå˜å‹å™¨ç¼–ç å™¨ã€‚è¯¥æ¨¡å—è¿‡æ»¤æ‰è¾“å…¥åˆ°ç¼–ç å™¨çš„ä»¤ç‰Œï¼Œä»¥ä¾¿åŠ¨æ€åœ°ä¿ç•™ç‰¹å®šå¯¹è±¡ç±»å‹åŒºåŸŸçš„ä»¤ç‰Œï¼Œå¹¶ä¿ç•™åœ¨å˜å‹å™¨ç¼–ç å™¨ä¸­é™æ€æ¥æ”¶é«˜æ³¨æ„åŠ›çš„åŒºåŸŸçš„ä»¤ç‰Œã€‚è¿™ç§è¿‡æ»¤ç­–ç•¥åœ¨è¿‡æ»¤æ‰è¾“å…¥åˆ°å˜å‹å™¨ç¼–ç å™¨çš„ä»¤ç‰Œçš„åŒæ—¶ä¿æŒäº†ä»»åŠ¡å‡†ç¡®æ€§ã€‚åœ¨æ£€ç´¢ä»»åŠ¡ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒATFåœ¨ä¿æŒæ£€ç´¢å¬å›ç‡çš„åŒæ—¶ï¼Œä¸ºViTæ¨¡å‹SigLIPæä¾›äº†2.8å€çš„é€Ÿåº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01519v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹ä¸­ï¼Œé’ˆå¯¹å›¾åƒåµŒå…¥æå–å–å¾—äº†çªç ´æ€§è¿›å±•ï¼Œå…¶åœ¨é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹è®¡ç®—è´Ÿæ‹…è¾ƒé«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ³¨æ„åŠ›æ„ŸçŸ¥ä»¤ç‰Œè¿‡æ»¤ï¼ˆATFï¼‰çš„ViTæ¨¡å‹åŠ é€Ÿæ–¹æ³•ã€‚ATFä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç†å¿µï¼šæ–°é¢–ä»¤ç‰Œè¿‡æ»¤æ¨¡å—å’Œè¿‡æ»¤ç­–ç•¥ã€‚ä»¤ç‰Œè¿‡æ»¤æ¨¡å—è¢«å¼•å…¥ViTæ¨¡å‹çš„ä»¤ç‰Œå™¨å’Œè½¬æ¢å™¨ç¼–ç å™¨ä¹‹é—´ï¼Œæ— éœ€ä¿®æ”¹æˆ–å¾®è°ƒè½¬æ¢å™¨ç¼–ç å™¨ã€‚è¯¥æ¨¡å—åŠ¨æ€ä¿ç•™ç‰¹å®šå¯¹è±¡åŒºåŸŸçš„ä»¤ç‰Œå¹¶ä¿ç•™é™æ€æ¥æ”¶è½¬æ¢å™¨ç¼–ç å™¨ä¸­é«˜å…³æ³¨åº¦çš„åŒºåŸŸçš„ä»¤ç‰Œã€‚æ­¤è¿‡æ»¤ç­–ç•¥åœ¨è¿‡æ»¤è¾“å…¥åˆ°è½¬æ¢å™¨ç¼–ç å™¨çš„ä»¤ç‰Œçš„åŒæ—¶ä¿æŒä»»åŠ¡å‡†ç¡®æ€§ã€‚åœ¨æ£€ç´¢ä»»åŠ¡ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒATFèƒ½åœ¨ä¿æŒæ£€ç´¢å¬å›ç‡çš„åŒæ—¶ï¼Œå°†ViTæ¨¡å‹SigLIPçš„é€Ÿåº¦æé«˜2.8å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer (ViT) åœ¨å›¾åƒåµŒå…¥æå–æ–¹é¢å–å¾—çªç ´ï¼Œé€‚ç”¨äºé›¶æ ·æœ¬å›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ã€‚</li>
<li>ViTæ¨¡å‹é¢ä¸´é«˜è®¡ç®—è´Ÿæ‹…é—®é¢˜ã€‚</li>
<li>æå‡ºäº†åä¸ºATFçš„ViTæ¨¡å‹åŠ é€Ÿæ–¹æ³•ï¼ŒåŒ…æ‹¬æ–°é¢–çš„ä»¤ç‰Œè¿‡æ»¤æ¨¡å—å’Œè¿‡æ»¤ç­–ç•¥ã€‚</li>
<li>ä»¤ç‰Œè¿‡æ»¤æ¨¡å—ä½äºViTæ¨¡å‹çš„ä»¤ç‰Œå™¨å’Œè½¬æ¢å™¨ç¼–ç å™¨ä¹‹é—´ï¼Œæ— éœ€ä¿®æ”¹æˆ–å¾®è°ƒè½¬æ¢å™¨ç¼–ç å™¨ã€‚</li>
<li>ATFé€šè¿‡åŠ¨æ€ä¿ç•™ç‰¹å®šå¯¹è±¡åŒºåŸŸçš„ä»¤ç‰Œå’Œé™æ€é«˜å…³æ³¨åº¦åŒºåŸŸçš„ä»¤ç‰Œè¿›è¡Œè¿‡æ»¤ã€‚<br>6.ATFèƒ½åœ¨ä¿æŒä»»åŠ¡å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå‡å°‘è¾“å…¥åˆ°è½¬æ¢å™¨ç¼–ç å™¨çš„ä»¤ç‰Œæ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e191e117382e93f563de2c97c5b0944e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6b216933f506ad403fe69e17136bb6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5bf21c854a97ab5642ba043428a50cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b431f58501c73cb902bd8523ba4dd6e8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SAM-I2V-Upgrading-SAM-to-Support-Promptable-Video-Segmentation-with-Less-than-0-2-Training-Cost"><a href="#SAM-I2V-Upgrading-SAM-to-Support-Promptable-Video-Segmentation-with-Less-than-0-2-Training-Cost" class="headerlink" title="SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with   Less than 0.2% Training Cost"></a>SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with   Less than 0.2% Training Cost</h2><p><strong>Authors:Haiyang Mei, Pengyu Zhang, Mike Zheng Shou</strong></p>
<p>Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V, an effective image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAMâ€™s static image encoder to enable spatiotemporal video perception, (ii) a memory filtering strategy that selects the most relevant past frames for more effective utilization of historical information, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves over 90% of SAM 2â€™s performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Code and model are available at: <a target="_blank" rel="noopener" href="https://github.com/showlab/SAM-I2V">https://github.com/showlab/SAM-I2V</a>. </p>
<blockquote>
<p>åƒSegment Anything Modelï¼ˆSAMï¼‰è¿™æ ·çš„åŸºç¡€æ¨¡å‹å·²ç»åœ¨è®¡ç®—æœºè§†è§‰ä¸­æ¨åŠ¨äº†å¯æç¤ºçš„å›¾åƒåˆ†å‰²çš„æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œå°†è¿™äº›èƒ½åŠ›æ‰©å±•åˆ°è§†é¢‘å´é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€åœºæ™¯ä¸­ç¡®ä¿ç²¾ç¡®å’Œæ—¶ç©ºä¸€è‡´çš„é®ç½©ä¼ æ’­æ–¹é¢ã€‚SAM 2 é€šè¿‡åœ¨å¤§é‡å›¾åƒå’Œè§†é¢‘æ•°æ®ä¸Šä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ä»¥å­¦ä¹ å¤æ‚çš„æ—¶ç©ºå…³è”æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™å¯¼è‡´äº†å·¨å¤§çš„è®­ç»ƒæˆæœ¬ï¼Œé˜»ç¢äº†ç ”ç©¶å’Œå®é™…åº”ç”¨éƒ¨ç½²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SAM-I2Vï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆçš„ä»å›¾åƒåˆ°è§†é¢‘çš„å‡çº§æ–¹æ³•ï¼Œç”¨äºåŸ¹å…»å¯æç¤ºçš„è§†é¢‘åˆ†å‰²ï¼ˆPVSï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å‡çº§é¢„è®­ç»ƒçš„SAMæ¥æ”¯æŒPVSï¼Œå¤§å¤§é™ä½äº†è®­ç»ƒå¤æ‚åº¦å’Œèµ„æºéœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆiï¼‰ä¸€ç§åŸºäºSAMé™æ€å›¾åƒç¼–ç å™¨çš„å›¾åƒåˆ°è§†é¢‘ç‰¹å¾æå–å‡çº§å™¨ï¼Œä»¥å®ç°æ—¶ç©ºè§†é¢‘æ„ŸçŸ¥ï¼›ï¼ˆiiï¼‰ä¸€ç§è®°å¿†è¿‡æ»¤ç­–ç•¥ï¼Œç”¨äºé€‰æ‹©æœ€ç›¸å…³çš„è¿‡å»å¸§ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å†å²ä¿¡æ¯ï¼›ï¼ˆiiiï¼‰ä¸€ç§åˆ©ç”¨å¯¹è±¡è®°å¿†çš„è®°å¿†å³æç¤ºæœºåˆ¶ï¼Œä»¥ç¡®ä¿åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶ç©ºä¸€è‡´é®ç½©ä¼ æ’­ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†SAM 2æ€§èƒ½çš„90%ä»¥ä¸Šï¼ŒåŒæ—¶ä»…ä½¿ç”¨å…¶0.2%çš„è®­ç»ƒæˆæœ¬ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºPVSæä¾›äº†ä¸€æ¡èµ„æºé«˜æ•ˆçš„é€”å¾„ï¼Œé™ä½äº†PVSæ¨¡å‹è®¾è®¡çš„è¿›ä¸€æ­¥ç ”ç©¶éšœç¢ï¼Œå¹¶å¯ç”¨äº†è¯¥é¢†åŸŸçš„æ›´å¹¿æ³›åº”ç”¨å’Œè¿›å±•ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/showlab/SAM-I2V">https://github.com/showlab/SAM-I2V</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01304v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong>ï¼šåŸºäºSAMæ¨¡å‹çš„å¯æç¤ºè§†é¢‘åˆ†å‰²æŠ€æœ¯ï¼ˆSAM-I2Vï¼‰ç ”ç©¶ä»‹ç»äº†ä¸€ç§æœ‰æ•ˆçš„å›¾åƒåˆ°è§†é¢‘çš„å‡çº§æ–¹æ³•ï¼Œç”¨äºæ„å»ºå¯æç¤ºè§†é¢‘åˆ†å‰²ï¼ˆPVSï¼‰æ¨¡å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡å‡çº§é¢„è®­ç»ƒçš„SAMæ¨¡å‹æ¥æ”¯æŒPVSï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒå¤æ‚æ€§å’Œèµ„æºéœ€æ±‚ã€‚å¼•å…¥ä¸‰é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼ŒåŒ…æ‹¬åŸºäºSAMé™æ€å›¾åƒç¼–ç å™¨çš„å›¾åƒåˆ°è§†é¢‘ç‰¹å¾æå–å‡çº§å™¨ã€è®°å¿†è¿‡æ»¤ç­–ç•¥ä»¥åŠåˆ©ç”¨å¯¹è±¡è®°å¿†çš„æç¤ºæœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†è¶…è¿‡90%çš„SAM 2æ€§èƒ½ï¼ŒåŒæ—¶ä»…ä½¿ç”¨å…¶0.2%çš„è®­ç»ƒæˆæœ¬ã€‚ä¸ºPVSè®¾è®¡æä¾›äº†èµ„æºé«˜æ•ˆçš„é€”å¾„ï¼Œé™ä½äº†è¿›ä¸€æ­¥ç ”ç©¶å’Œå®è·µåº”ç”¨çš„é—¨æ§›ã€‚ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SAMæ¨¡å‹åœ¨æç¤ºå›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å°†å…¶æ‰©å±•åˆ°è§†é¢‘é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç¡®ä¿åŠ¨æ€åœºæ™¯ä¸­çš„ç²¾ç¡®å’Œæ—¶åºä¸€è‡´çš„æ©è†œä¼ æ’­ã€‚</li>
<li>SAM 2æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡å›¾åƒå’Œè§†é¢‘æ•°æ®ä»å¤´å¼€å§‹è®­ç»ƒæ¥å­¦ä¹ å¤æ‚çš„æ—¶ç©ºå…³è”ï¼Œä½†å¸¦æ¥äº†å·¨å¤§çš„è®­ç»ƒæˆæœ¬ï¼Œé˜»ç¢äº†ç ”ç©¶å’Œå®é™…åº”ç”¨éƒ¨ç½²ã€‚</li>
<li>SAM-I2Væ–¹æ³•ä»‹ç»äº†ä¸€ç§æœ‰æ•ˆçš„å›¾åƒåˆ°è§†é¢‘çš„å‡çº§ç­–ç•¥ï¼Œä»¥æ”¯æŒæç¤ºè§†é¢‘åˆ†å‰²ï¼ˆPVSï¼‰ã€‚è¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†è®­ç»ƒå¤æ‚æ€§å’Œèµ„æºéœ€æ±‚ã€‚</li>
<li>SAM-I2Vå¼•å…¥äº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šå›¾åƒåˆ°è§†é¢‘ç‰¹å¾æå–å‡çº§å™¨ã€è®°å¿†è¿‡æ»¤ç­–ç•¥å’Œè®°å¿†æç¤ºæœºåˆ¶ï¼Œç¡®ä¿åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶ç©ºä¸€è‡´æ©è†œä¼ æ’­ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSAM-I2Væ–¹æ³•å®ç°äº†ä¸SAM 2ç›¸å½“çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚</li>
<li>å…¬å¼€çš„ä»£ç å’Œæ¨¡å‹ä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹çš„ç ”ç©¶æä¾›äº†ä¾¿åˆ©ï¼Œä¿ƒè¿›äº†è§†é¢‘åˆ†å‰²é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01304">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0c6b8d8ddcf2279995e9c5eb8c0cb8e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-307d789d3f53844008e298dc60575b8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3f8a61f35826bea6db2504c57921fa8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Understanding-Model-Reprogramming-for-CLIP-via-Decoupling-Visual-Prompts"><a href="#Understanding-Model-Reprogramming-for-CLIP-via-Decoupling-Visual-Prompts" class="headerlink" title="Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts"></a>Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts</h2><p><strong>Authors:Chengyi Cai, Zesheng Ye, Lei Feng, Jianzhong Qi, Feng Liu</strong></p>
<p>Model reprogramming adapts pretrained models to downstream tasks by modifying only the input and output spaces. Visual reprogramming (VR) is one instance for vision tasks that adds a trainable noise pattern (i.e., a visual prompt) to input images to facilitate downstream classification. The existing VR approaches for CLIP train a single visual prompt using all descriptions of different downstream classes. However, the limited learning capacity may result in (1) a failure to capture diverse aspects of the descriptions (e.g., shape, color, and texture), and (2) a possible bias toward less informative attributes that do not help distinguish between classes. In this paper, we introduce a decoupling-and-reweighting framework. Our decoupled visual prompts (DVP) are optimized using descriptions grouped by explicit causes (DVP-cse) or unsupervised clusters (DVP-cls). Then, we integrate the outputs of these visual prompts with a probabilistic reweighting matrix (PRM) that measures their contributions to each downstream class. Theoretically, DVP lowers the empirical risk bound. Experimentally, DVP outperforms baselines on average across 11 downstream datasets. Notably, the DVP-PRM integration enables insights into how individual visual prompts influence classification decisions, providing a probabilistic framework for understanding reprogramming. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tmlr-group/DecoupledVP">https://github.com/tmlr-group/DecoupledVP</a>. </p>
<blockquote>
<p>æ¨¡å‹é‡æ„é€šè¿‡ä»…ä¿®æ”¹è¾“å…¥å’Œè¾“å‡ºç©ºé—´æ¥é€‚åº”é¢„è®­ç»ƒæ¨¡å‹ä»¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚è§†è§‰é‡æ„ï¼ˆVRï¼‰æ˜¯è§†è§‰ä»»åŠ¡çš„ä¸€ä¸ªå®ä¾‹ï¼Œå®ƒå‘è¾“å…¥å›¾åƒæ·»åŠ å¯è®­ç»ƒçš„å™ªå£°æ¨¡å¼ï¼ˆå³è§†è§‰æç¤ºï¼‰ä»¥ä¿ƒè¿›ä¸‹æ¸¸åˆ†ç±»ã€‚ç°æœ‰çš„CLIPçš„VRæ–¹æ³•ä½¿ç”¨ä¸åŒä¸‹æ¸¸ç±»åˆ«çš„æ‰€æœ‰æè¿°æ¥è®­ç»ƒä¸€ä¸ªå•ä¸€è§†è§‰æç¤ºã€‚ç„¶è€Œï¼Œæœ‰é™çš„å­¦ä¹ èƒ½åŠ›å¯èƒ½å¯¼è‡´ï¼ˆ1ï¼‰æ— æ³•æ•æ‰æè¿°çš„å„ä¸ªæ–¹é¢ï¼ˆä¾‹å¦‚å½¢çŠ¶ã€é¢œè‰²å’Œçº¹ç†ï¼‰ï¼›ï¼ˆ2ï¼‰åå‘äºå¯èƒ½ä¸å…·æœ‰åŒºåˆ†ä¸åŒç±»åˆ«èƒ½åŠ›çš„ä¿¡æ¯è¾ƒå°‘çš„å±æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè§£è€¦å’ŒåŠ æƒæ¡†æ¶ã€‚æˆ‘ä»¬çš„è§£è€¦è§†è§‰æç¤ºï¼ˆDVPï¼‰ä½¿ç”¨æŒ‰æ˜ç¡®åŸå› åˆ†ç»„ï¼ˆDVP-cseï¼‰çš„æè¿°æˆ–æ— ç›‘ç£èšç±»ï¼ˆDVP-clsï¼‰è¿›è¡Œä¼˜åŒ–ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ¦‚ç‡åŠ æƒçŸ©é˜µï¼ˆPRMï¼‰æ•´åˆè¿™äº›è§†è§‰æç¤ºçš„è¾“å‡ºï¼Œè¯¥çŸ©é˜µè¡¡é‡å®ƒä»¬å¯¹ä¸‹æ¸¸æ¯ä¸ªç±»åˆ«çš„è´¡çŒ®ã€‚ç†è®ºä¸Šï¼ŒDVPé™ä½äº†ç»éªŒé£é™©ç•Œé™ã€‚å®éªŒä¸Šï¼ŒDVPåœ¨å¹³å‡æ„ä¹‰ä¸Šä¼˜äºåœ¨11ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„åŸºå‡†æµ‹è¯•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDVP-PRMé›†æˆæä¾›äº†å…³äºå•ä¸ªè§†è§‰æç¤ºå¦‚ä½•å½±å“åˆ†ç±»å†³ç­–çš„ä¿¡æ¯ï¼Œæä¾›äº†ä¸€ä¸ªç†è§£é‡æ„çš„æ¦‚ç‡æ¡†æ¶ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tmlr-group/DecoupledVP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tmlr-group/DecoupledVPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01000v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰é‡ç¼–ç¨‹ï¼ˆVRï¼‰çš„ä¸€ç§æ–°æ–¹æ³•â€”â€”è§£è€¦å’Œé‡æƒæ¡†æ¶ä¸‹çš„è§†è§‰æç¤ºï¼ˆDVPï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹ä¸åŒä¸‹æ¸¸ç±»åˆ«çš„æè¿°è¿›è¡Œåˆ†ç»„ä¼˜åŒ–è§†è§‰æç¤ºï¼Œé™ä½ç»éªŒé£é™©ç•Œé™ã€‚åœ¨å¤šä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDVPä¼˜äºåŸºå‡†æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼ŒDVP-PRMé›†æˆæä¾›äº†å¯¹å•ä¸ªè§†è§‰æç¤ºå¦‚ä½•å½±å“åˆ†ç±»å†³ç­–çš„ç†è§£ï¼Œä¸ºé‡æ–°ç¼–ç¨‹æä¾›äº†ä¸€ä¸ªæ¦‚ç‡æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰é‡ç¼–ç¨‹ï¼ˆVRï¼‰æ˜¯ä¸€ç§ç”¨äºé€‚åº”é¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œåªä¿®æ”¹è¾“å…¥å’Œè¾“å‡ºç©ºé—´ã€‚é€šè¿‡å‘è¾“å…¥å›¾åƒæ·»åŠ å¯è®­ç»ƒçš„å™ªå£°æ¨¡å¼ï¼ˆå³è§†è§‰æç¤ºï¼‰æ¥é€‚åº”ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ã€‚ç°æœ‰çš„VRæ–¹æ³•å¯èƒ½æ— æ³•æ•æ‰æè¿°çš„å„ä¸ªæ–¹é¢ï¼Œå¹¶ä¸”å¯èƒ½åå‘äºæ²¡æœ‰å¸®åŠ©åŒºåˆ†ç±»çš„ä¿¡æ¯è¾ƒå°‘çš„å±æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªè§£è€¦å’Œé‡æƒæ¡†æ¶ä¸‹çš„è§†è§‰æç¤ºï¼ˆDVPï¼‰ã€‚é€šè¿‡æ­¤æ¡†æ¶ä¼˜åŒ–çš„è§†è§‰æç¤ºè¢«ç§°ä¸ºè§£è€¦çš„è§†è§‰æç¤ºï¼ˆDVPï¼‰ã€‚</li>
<li>DVPé€šè¿‡ä¸¤ç§æ–¹å¼ä¼˜åŒ–è§†è§‰æç¤ºï¼šä¸€ç§æ˜¯åŸºäºæ˜ç¡®çš„å› æœåˆ†ç»„è¿›è¡Œä¼˜åŒ–ï¼ˆDVP-cseï¼‰ï¼Œå¦ä¸€ç§æ˜¯åŸºäºæ— ç›‘ç£èšç±»è¿›è¡Œä¼˜åŒ–ï¼ˆDVP-clsï¼‰ã€‚é€šè¿‡è¿™ä¸¤ç§æ–¹å¼ä¼˜åŒ–çš„è§†è§‰æç¤ºå¯ä»¥æ›´æœ‰æ•ˆåœ°æ•è·æè¿°ä¸­çš„ä¿¡æ¯å¹¶é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚ç†è®ºè€Œè¨€ï¼Œè¯¥æ¡†æ¶æœ‰åŠ©äºé™ä½ç»éªŒé£é™©ç•Œé™ã€‚å› æ­¤ç›¸å¯¹äºåŸºçº¿æ–¹æ³•æœ‰æ›´ä¼˜çš„è¡¨ç°ã€‚åœ¨å¤šä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¿™ä¸€ç‚¹ã€‚</li>
<li>DVPä¸æ¦‚ç‡é‡æƒçŸ©é˜µï¼ˆPRMï¼‰çš„ç»“åˆä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿç†è§£å•ä¸ªè§†è§‰æç¤ºå¦‚ä½•å½±å“åˆ†ç±»å†³ç­–ï¼Œå¹¶ä¸ºæ¨¡å‹é‡æ–°ç¼–ç¨‹æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’å’Œç†è§£æ¡†æ¶ã€‚è¿™å¯¹äºè§£é‡Šå’Œæ”¹è¿›æ¨¡å‹çš„æ€§èƒ½éå¸¸é‡è¦ã€‚å¯¹äºç†è§£å’Œæ”¹è¿›æ¨¡å‹æ€§èƒ½éå¸¸é‡è¦ã€‚æ­¤ä»£ç çš„å¼€æºå®ç°ä¸ºç”¨æˆ·æä¾›äº†ä¸€ä¸ªäº†è§£å’Œè¿ç”¨è¯¥æ–¹æ³•çš„æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01000">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e5d89ed57a510ec190795faddc1f565.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4120af3424f23c2ce1607e92e00ec71c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-250a6315282d0c14a19200aa57042195.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8187121cc6a85f340153eb9f2d1aecc3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PerFormer-A-Permutation-Based-Vision-Transformer-for-Remaining-Useful-Life-Prediction"><a href="#PerFormer-A-Permutation-Based-Vision-Transformer-for-Remaining-Useful-Life-Prediction" class="headerlink" title="PerFormer: A Permutation Based Vision Transformer for Remaining Useful   Life Prediction"></a>PerFormer: A Permutation Based Vision Transformer for Remaining Useful   Life Prediction</h2><p><strong>Authors:Zhengyang Fan, Wanru Li, Kuo-chu Chang, Ting Yuan</strong></p>
<p>Accurately estimating the remaining useful life (RUL) for degradation systems is crucial in modern prognostic and health management (PHM). Convolutional Neural Networks (CNNs), initially developed for tasks like image and video recognition, have proven highly effectively in RUL prediction, demonstrating remarkable performance. However, with the emergence of the Vision Transformer (ViT), a Transformer model tailored for computer vision tasks such as image classification, and its demonstrated superiority over CNNs, there is a natural inclination to explore its potential in enhancing RUL prediction accuracy. Nonetheless, applying ViT directly to multivariate sensor data for RUL prediction poses challenges, primarily due to the ambiguous nature of spatial information in time series data. To address this issue, we introduce the PerFormer, a permutation-based vision transformer approach designed to permute multivariate time series data, mimicking spatial characteristics akin to image data, thereby making it suitable for ViT. To generate the desired permutation matrix, we introduce a novel permutation loss function aimed at guiding the convergence of any matrix towards a permutation matrix. Our experiments on NASAâ€™s C-MAPSS dataset demonstrate the PerFormerâ€™s superior performance in RUL prediction compared to state-of-the-art methods employing CNNs, Recurrent Neural Networks (RNNs), and various Transformer models. This underscores its effectiveness and potential in PHM applications. </p>
<blockquote>
<p>å¯¹é€€åŒ–ç³»ç»Ÿçš„å‰©ä½™ä½¿ç”¨å¯¿å‘½ï¼ˆRULï¼‰è¿›è¡Œå‡†ç¡®ä¼°è®¡æ˜¯ç°ä»£é¢„æµ‹ä¸å¥åº·ç®¡ç†ï¼ˆPHMï¼‰ä¸­çš„å…³é”®ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æœ€åˆè¢«å¼€å‘ç”¨äºå›¾åƒå’Œè§†é¢‘è¯†åˆ«ç­‰ä»»åŠ¡ï¼Œå·²ç»è¯æ˜åœ¨RULé¢„æµ‹ä¸­çš„é«˜åº¦æœ‰æ•ˆæ€§ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œéšç€ç”¨äºå›¾åƒåˆ†ç±»ç­‰è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„å®šåˆ¶Transformeræ¨¡å‹â€”â€”è§†è§‰Transformerï¼ˆViTï¼‰çš„å‡ºç°ï¼Œä»¥åŠå…¶ç›¸è¾ƒäºCNNçš„ä¼˜è¶Šæ€§è¯æ˜ï¼Œäººä»¬è‡ªç„¶å€¾å‘äºæ¢ç´¢å…¶åœ¨æé«˜RULé¢„æµ‹ç²¾åº¦æ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥å°†ViTåº”ç”¨äºå¤šå…ƒä¼ æ„Ÿå™¨æ•°æ®çš„RULé¢„æµ‹å´å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ—¶é—´åºåˆ—æ•°æ®ä¸­ç©ºé—´ä¿¡æ¯çš„æ¨¡ç³Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†PerFormerï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ’åˆ—çš„æ„¿æ™¯è½¬æ¢å™¨æ–¹æ³•ï¼Œæ—¨åœ¨æ’åˆ—å¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®ï¼Œæ¨¡ä»¿ç±»ä¼¼äºå›¾åƒæ•°æ®çš„ç©ºé—´ç‰¹å¾ï¼Œä»è€Œä½¿å…¶é€‚åˆViTã€‚ä¸ºäº†ç”Ÿæˆæ‰€éœ€çš„æ’åˆ—çŸ©é˜µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ’åˆ—æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨å¼•å¯¼ä»»ä½•çŸ©é˜µå‘æ’åˆ—çŸ©é˜µæ”¶æ•›ã€‚æˆ‘ä»¬åœ¨NASAçš„C-MAPSSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPerFormeråœ¨RULé¢„æµ‹æ–¹é¢çš„æ€§èƒ½ä¼˜äºé‡‡ç”¨CNNã€å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œå„ç§Transformeræ¨¡å‹çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚è¿™çªæ˜¾äº†å…¶åœ¨PHMåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00259v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨å‰©ä½™ä½¿ç”¨å¯¿å‘½ï¼ˆRULï¼‰é¢„æµ‹ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†éšç€é’ˆå¯¹è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„å®šåˆ¶ç‰ˆVision Transformerï¼ˆViTï¼‰çš„å‡ºç°ï¼Œå…¶åœ¨RULé¢„æµ‹ä¸­çš„æ½œåŠ›å¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨ViTäºå¤šå…ƒä¼ æ„Ÿå™¨æ•°æ®è¿›è¡ŒRULé¢„æµ‹å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†PerFormerï¼Œä¸€ç§åŸºäºæ’åˆ—çš„Vision Transformeræ–¹æ³•ï¼Œå¯å¯¹å¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œæ’åˆ—ï¼Œæ¨¡æ‹Ÿå›¾åƒæ•°æ®çš„ç©ºé—´ç‰¹å¾ã€‚é€šè¿‡å¼•å…¥æ–°å‹æ’åˆ—æŸå¤±å‡½æ•°ï¼ŒæŒ‡å¯¼æ’åˆ—çŸ©é˜µçš„æ”¶æ•›ã€‚åœ¨NASAçš„C-MAPSSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPerFormeråœ¨RULé¢„æµ‹æ–¹é¢çš„æ€§èƒ½ä¼˜äºé‡‡ç”¨CNNã€å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œå„ç§Transformeræ¨¡å‹çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œçªæ˜¾å…¶åœ¨é¢„åä¸å¥åº·ç®¡ç†ï¼ˆPHMï¼‰åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§åŠæ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨å‰©ä½™ä½¿ç”¨å¯¿å‘½ï¼ˆRULï¼‰é¢„æµ‹ä¸­å·²æœ‰å“è¶Šè¡¨ç°ã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰çš„å‡ºç°ä¸ºRULé¢„æµ‹å¸¦æ¥äº†æ–°çš„æ½œåŠ›ã€‚</li>
<li>ç›´æ¥åº”ç”¨ViTäºå¤šå…ƒä¼ æ„Ÿå™¨æ•°æ®å­˜åœ¨æŒ‘æˆ˜ï¼Œå› æ—¶é—´åºåˆ—æ•°æ®çš„ç©ºé—´ä¿¡æ¯ä¸æ˜ç¡®ã€‚</li>
<li>å¼•å…¥PerFormerï¼Œä¸€ç§åŸºäºæ’åˆ—çš„Vision Transformeræ–¹æ³•ï¼Œèƒ½æ’åˆ—å¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®ï¼Œæ¨¡æ‹Ÿå›¾åƒæ•°æ®çš„ç©ºé—´ç‰¹å¾ã€‚</li>
<li>æ–°å‹æ’åˆ—æŸå¤±å‡½æ•°ç”¨äºç”Ÿæˆæ’åˆ—çŸ©é˜µï¼Œå¹¶å¼•å¯¼å…¶æ”¶æ•›ã€‚</li>
<li>åœ¨NASAçš„C-MAPSSæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒPerFormeråœ¨RULé¢„æµ‹æ–¹é¢æ€§èƒ½å“è¶Šï¼Œä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e19218d3784cb84ea0372c84aff13cf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07987e73c12645a9a675a02d2ca34332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf163ed0a641d652e910aa5257af767a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36d7e68fce904fa3cee18e5ac80262b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b222b0f4028bfb38e3c6700149875244.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1dc4ba1cceef39874058b23060cc9934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18f5e1ae723051de8e251f53be469cc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efbadda203516f295385634ce64ffe3d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Bayesian-Prompt-Flow-Learning-for-Zero-Shot-Anomaly-Detection"><a href="#Bayesian-Prompt-Flow-Learning-for-Zero-Shot-Anomaly-Detection" class="headerlink" title="Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection"></a>Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection</h2><p><strong>Authors:Zhen Qu, Xian Tao, Xinyi Gong, Shichen Qu, Qiyu Chen, Zhengtao Zhang, Xingang Wang, Guiguang Ding</strong></p>
<p>Recently, vision-language models (e.g. CLIP) have demonstrated remarkable performance in zero-shot anomaly detection (ZSAD). By leveraging auxiliary data during training, these models can directly perform cross-category anomaly detection on target datasets, such as detecting defects on industrial product surfaces or identifying tumors in organ tissues. Existing approaches typically construct text prompts through either manual design or the optimization of learnable prompt vectors. However, these methods face several challenges: 1) handcrafted prompts require extensive expert knowledge and trial-and-error; 2) single-form learnable prompts struggle to capture complex anomaly semantics; and 3) an unconstrained prompt space limits generalization to unseen categories. To address these issues, we propose Bayesian Prompt Flow Learning (Bayes-PFL), which models the prompt space as a learnable probability distribution from a Bayesian perspective. Specifically, a prompt flow module is designed to learn both image-specific and image-agnostic distributions, which are jointly utilized to regularize the text prompt space and improve the modelâ€™s generalization on unseen categories. These learned distributions are then sampled to generate diverse text prompts, effectively covering the prompt space. Additionally, a residual cross-model attention (RCA) module is introduced to better align dynamic text embeddings with fine-grained image features. Extensive experiments on 15 industrial and medical datasets demonstrate our methodâ€™s superior performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xiaozhen228/Bayes-PFL">https://github.com/xiaozhen228/Bayes-PFL</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰åœ¨é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚è¿™äº›æ¨¡å‹é€šè¿‡åˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¾…åŠ©æ•°æ®ï¼Œå¯ä»¥ç›´æ¥å¯¹ç›®æ ‡æ•°æ®é›†è¿›è¡Œè·¨ç±»åˆ«å¼‚å¸¸æ£€æµ‹ï¼Œå¦‚æ£€æµ‹å·¥ä¸šäº§å“è¡¨é¢çš„ç¼ºé™·æˆ–è¯†åˆ«ç»„ç»‡ä¸­çš„è‚¿ç˜¤ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡æ‰‹åŠ¨è®¾è®¡æˆ–ä¼˜åŒ–å¯å­¦ä¹ çš„æç¤ºå‘é‡æ¥æ„å»ºæ–‡æœ¬æç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é¢ä¸´å‡ ä¸ªæŒ‘æˆ˜ï¼š1ï¼‰æ‰‹å·¥åˆ¶ä½œçš„æç¤ºéœ€è¦å¹¿æ³›çš„ä¸“ä¸šçŸ¥è¯†å’Œè¯•é”™ï¼›2ï¼‰å•ä¸€å½¢å¼çš„å¯å­¦ä¹ æç¤ºéš¾ä»¥æ•æ‰å¤æ‚çš„å¼‚å¸¸è¯­ä¹‰ï¼›3ï¼‰æ— çº¦æŸçš„æç¤ºç©ºé—´é™åˆ¶äº†æœªè§ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è´å¶æ–¯æç¤ºæµå­¦ä¹ ï¼ˆBayes-PFLï¼‰ï¼Œå®ƒä»è´å¶æ–¯è§’åº¦å°†æç¤ºç©ºé—´å»ºæ¨¡ä¸ºå¯å­¦ä¹ çš„æ¦‚ç‡åˆ†å¸ƒã€‚å…·ä½“æ¥è¯´ï¼Œè®¾è®¡äº†ä¸€ä¸ªæç¤ºæµæ¨¡å—æ¥å­¦ä¹ å›¾åƒç‰¹å®šå’Œå›¾åƒé€šç”¨çš„åˆ†å¸ƒï¼Œè¿™äº›åˆ†å¸ƒè¢«å…±åŒç”¨æ¥è§„èŒƒæ–‡æœ¬æç¤ºç©ºé—´ï¼Œæé«˜æ¨¡å‹åœ¨æœªè§ç±»åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å­¦ä¹ åˆ°çš„åˆ†å¸ƒç„¶åè¢«é‡‡æ ·ä»¥ç”Ÿæˆå¤šæ ·åŒ–çš„æ–‡æœ¬æç¤ºï¼Œæœ‰æ•ˆåœ°è¦†ç›–äº†æç¤ºç©ºé—´ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ®‹å·®è·¨æ¨¡å‹æ³¨æ„åŠ›ï¼ˆRCAï¼‰æ¨¡å—ï¼Œä»¥æ›´å¥½åœ°å°†åŠ¨æ€æ–‡æœ¬åµŒå…¥ä¸ç²¾ç»†å›¾åƒç‰¹å¾å¯¹é½ã€‚åœ¨15ä¸ªå·¥ä¸šå’ŒåŒ»ç–—æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiaozhen228/Bayes-PFL%E3%80%82">https://github.com/xiaozhen228/Bayes-PFLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10080v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè´å¶æ–¯æç¤ºæµå­¦ä¹ ï¼ˆBayes-PFLï¼‰çš„æ–¹æ³•ï¼Œç”¨äºè§£å†³è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»è´å¶æ–¯è§†è§’å¯¹æç¤ºç©ºé—´è¿›è¡Œå»ºæ¨¡ï¼Œè§£å†³äº†æ‰‹å·¥æç¤ºéœ€è¦å¤§é‡ä¸“ä¸šçŸ¥è¯†å’Œåå¤è¯•éªŒçš„é—®é¢˜ï¼Œä»¥åŠå•ä¸€å½¢å¼çš„å­¦ä¹ æç¤ºéš¾ä»¥æ•æ‰å¤æ‚çš„å¼‚å¸¸è¯­ä¹‰å’Œä¸å—çº¦æŸçš„æç¤ºç©ºé—´é™åˆ¶äº†æœªè§ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡æ‰‹åŠ¨è®¾è®¡æˆ–ä¼˜åŒ–å­¦ä¹ æç¤ºå‘é‡æ¥æ„å»ºæ–‡æœ¬æç¤ºï¼Œä½†å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>è´å¶æ–¯æç¤ºæµå­¦ä¹ ï¼ˆBayes-PFLï¼‰æ–¹æ³•æå‡ºï¼Œå°†æç¤ºç©ºé—´å»ºæ¨¡ä¸ºå¯ä»è´å¶æ–¯è§†è§’å­¦ä¹ çš„æ¦‚ç‡åˆ†å¸ƒã€‚</li>
<li>Bayes-PFLè®¾è®¡äº†ä¸€ä¸ªæç¤ºæµæ¨¡å—æ¥å­¦ä¹ å›¾åƒç‰¹å®šå’Œå›¾åƒé€šç”¨çš„åˆ†å¸ƒï¼Œä»¥è§„èŒƒæ–‡æœ¬æç¤ºç©ºé—´å¹¶æé«˜æ¨¡å‹å¯¹æœªè§ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡é‡‡æ ·å­¦ä¹ åˆ°çš„åˆ†å¸ƒæ¥ç”Ÿæˆå¤šæ ·åŒ–çš„æ–‡æœ¬æç¤ºï¼Œæœ‰æ•ˆè¦†ç›–æç¤ºç©ºé—´ã€‚</li>
<li>å¼•å…¥æ®‹å·®è·¨æ¨¡å‹æ³¨æ„åŠ›ï¼ˆRCAï¼‰æ¨¡å—ï¼Œä»¥æ›´å¥½åœ°å¯¹é½åŠ¨æ€æ–‡æœ¬åµŒå…¥å’Œç»†ç²’åº¦å›¾åƒç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b524108baea065a77395afd254108739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c94fdcc90b6b6383389dc62ddbed2cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adb006e8cc299475ed5d64fc6ce62fdd.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ViFOR-A-Fourier-Enhanced-Vision-Transformer-for-Multi-Image-Super-Resolution-in-Earth-System"><a href="#ViFOR-A-Fourier-Enhanced-Vision-Transformer-for-Multi-Image-Super-Resolution-in-Earth-System" class="headerlink" title="ViFOR: A Fourier-Enhanced Vision Transformer for Multi-Image   Super-Resolution in Earth System"></a>ViFOR: A Fourier-Enhanced Vision Transformer for Multi-Image   Super-Resolution in Earth System</h2><p><strong>Authors:Ehsan Zeraatkar, Salah A Faroughi, Jelena TeÅ¡iÄ‡</strong></p>
<p>Super-resolution (SR) is crucial for enhancing the spatial resolution of Earth System Model (ESM) data, thereby enabling more precise analysis of environmental processes. This paper introduces ViFOR, a novel SR algorithm integrating Vision Transformers (ViTs) with Fourier-based Implicit Neural Representation Networks (INRs). ViFOR effectively captures global context and high-frequency details essential for accurate SR reconstruction by embedding Fourier-based activation functions within the transformer architecture. Extensive experiments demonstrate that ViFOR consistently outperforms state-of-the-art methods, including ViT, SIREN, and SRGANs, in terms of Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE) for both global and local imagery. ViFOR achieves PSNR improvements of up to 4.18 dB, 1.56 dB, and 1.73 dB over ViT on full-image Source Temperature, Shortwave, and Longwave Flux datasets. These results highlight ViFORâ€™s effectiveness and potential for advancing high-resolution climate data analysis. </p>
<blockquote>
<p>è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰å¯¹äºæé«˜åœ°çƒç³»ç»Ÿæ¨¡å‹ï¼ˆESMï¼‰æ•°æ®çš„ç©ºé—´åˆ†è¾¨ç‡è‡³å…³é‡è¦ï¼Œä»è€Œèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°åˆ†æç¯å¢ƒè¿‡ç¨‹ã€‚æœ¬æ–‡ä»‹ç»äº†ViFORï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹SRç®—æ³•ï¼Œå®ƒå°†è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ä¸åŸºäºå‚…é‡Œå¶éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼ˆINRsï¼‰ç›¸ç»“åˆã€‚ViFORé€šè¿‡å°†åŸºäºå‚…é‡Œå¶çš„æ¿€æ´»å‡½æ•°åµŒå…¥åˆ°è½¬æ¢å™¨æ¶æ„ä¸­ï¼Œæœ‰æ•ˆåœ°æ•è·å…¨å±€ä¸Šä¸‹æ–‡å’Œå¯¹äºå‡†ç¡®SRé‡å»ºè‡³å…³é‡è¦çš„é«˜é¢‘ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨å…¨å±€è¿˜æ˜¯å±€éƒ¨å›¾åƒæ–¹é¢ï¼ŒViFORåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æ–¹é¢å‡ä¼˜äºåŒ…æ‹¬ViTã€SIRENå’ŒSRGANsç­‰æœ€æ–°æ–¹æ³•ã€‚ViFORåœ¨å…¨å›¾åƒæºæ¸©åº¦ã€çŸ­æ³¢å’Œé•¿æ³¢é€šé‡æ•°æ®é›†ä¸Šçš„PSNRåˆ†åˆ«æ¯”ViTæé«˜äº†4.18 dBã€1.56 dBå’Œ1.73 dBã€‚è¿™äº›ç»“æœçªå‡ºäº†ViFORçš„æœ‰æ•ˆæ€§ä»¥åŠå…¶åœ¨æ¨è¿›é«˜åˆ†è¾¨ç‡æ°”å€™æ•°æ®åˆ†ææ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12427v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ViFORï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è¶…åˆ†è¾¨ç‡ç®—æ³•ï¼Œç»“åˆäº†åŸºäºVision Transformerçš„æ¨¡å‹å’ŒåŸºäºå‚…ç«‹å¶å˜æ¢çš„éšç¥ç»è¡¨ç¤ºç½‘ç»œã€‚ViFORé€šè¿‡å°†å‚…ç«‹å¶æ¿€æ´»å‡½æ•°åµŒå…¥åˆ°Transformeræ¶æ„ä¸­ï¼Œæœ‰æ•ˆåœ°æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡å’Œé«˜é¢‘ç»†èŠ‚ï¼Œå¯¹äºå‡†ç¡®çš„è¶…åˆ†è¾¨ç‡é‡å»ºè‡³å…³é‡è¦ã€‚å®éªŒè¡¨æ˜ï¼ŒViFORåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æ–¹é¢ï¼Œå…¨é¢è¶…è¶Šäº†åŒ…æ‹¬ViTã€SIRENå’ŒSRGANç­‰åœ¨å†…çš„æœ€æ–°æ–¹æ³•ï¼Œæ— è®ºæ˜¯åœ¨å…¨å±€è¿˜æ˜¯å±€éƒ¨å›¾åƒä¸Šå‡è¡¨ç°ä¼˜å¼‚ã€‚ç‰¹åˆ«æ˜¯å¯¹äºå…¨å›¾åƒæºæ¸©åº¦ã€çŸ­æ³¢å’Œé•¿æ³¢æµé‡æ•°æ®é›†ï¼ŒViFORç›¸æ¯”ViTçš„PSNRæå‡åˆ†åˆ«è¾¾åˆ°äº†4.18åˆ†è´ã€1.56åˆ†è´å’Œ1.73åˆ†è´ã€‚è¿™æ˜¾ç¤ºäº†ViFORåœ¨æ¨è¿›é«˜åˆ†è¾¨ç‡æ°”å€™æ•°æ®åˆ†ææ–¹é¢çš„æ½œåŠ›å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViFORæ˜¯ä¸€ç§ç»“åˆäº†Vision Transformerå’ŒåŸºäºå‚…ç«‹å¶å˜æ¢çš„éšç¥ç»è¡¨ç¤ºç½‘ç»œçš„æ–°å‹è¶…åˆ†è¾¨ç‡ç®—æ³•ã€‚</li>
<li>ViFORé€šè¿‡å°†å‚…ç«‹å¶æ¿€æ´»å‡½æ•°åµŒå…¥åˆ°Transformeræ¶æ„ä¸­ï¼Œä»¥æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡å’Œé«˜é¢‘ç»†èŠ‚ï¼Œè¿™å¯¹äºå‡†ç¡®è¶…åˆ†è¾¨ç‡é‡å»ºè‡³å…³é‡è¦ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒViFORåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æ–¹é¢è¶…è¶Šäº†å…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</li>
<li>ViFORåœ¨å…¨å›¾åƒæºæ¸©åº¦ã€çŸ­æ³¢å’Œé•¿æ³¢æµé‡æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºViTï¼ŒPSNRæå‡æ˜¾è‘—ã€‚</li>
<li>ViFORç®—æ³•åœ¨æ¨è¿›é«˜åˆ†è¾¨ç‡æ°”å€™æ•°æ®åˆ†ææ–¹é¢å…·æœ‰æ½œåŠ›å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>ViFORç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆç»“åˆå…¨çƒè§†é‡å’Œç»†èŠ‚ä¿¡æ¯ï¼Œè¿™å¯¹äºç¯å¢ƒè¿‡ç¨‹çš„ç²¾ç¡®åˆ†æè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-daefd5915166b5bcc37598aaa2d92baf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af0150ed65f474417cc023973aa52402.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6437149d065176fa7aca7013baadfed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6592be2ac7852a4a6fb687445344f9b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d57f9bea2051f578e472059d9887a7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-86a497d6dab6790501ab71120aa120b0.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Efficient Test-time Adaptive Object Detection via Sensitivity-Guided   Pruning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-dce9c22fa66fd9ddad5d9ddfa9f79e77.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  EgoVLM Policy Optimization for Egocentric Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
