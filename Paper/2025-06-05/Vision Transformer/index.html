<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-06-05  Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-8187121cc6a85f340153eb9f2d1aecc3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    50 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-05-更新"><a href="#2025-06-05-更新" class="headerlink" title="2025-06-05 更新"></a>2025-06-05 更新</h1><h2 id="Revisiting-Continuity-of-Image-Tokens-for-Cross-domain-Few-shot-Learning"><a href="#Revisiting-Continuity-of-Image-Tokens-for-Cross-domain-Few-shot-Learning" class="headerlink" title="Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning"></a>Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning</h2><p><strong>Authors:Shuai Yi, Yixiong Zou, Yuhua Li, Ruixuan Li</strong></p>
<p>Vision Transformer (ViT) has achieved remarkable success due to its large-scale pretraining on general domains, but it still faces challenges when applying it to downstream distant domains that have only scarce training data, which gives rise to the Cross-Domain Few-Shot Learning (CDFSL) task. Inspired by Self-Attention’s insensitivity to token orders, we find an interesting phenomenon neglected in current works: disrupting the continuity of image tokens (i.e., making pixels not smoothly transited across patches) in ViT leads to a noticeable performance decline in the general (source) domain but only a marginal decrease in downstream target domains. This questions the role of image tokens’ continuity in ViT’s generalization under large domain gaps. In this paper, we delve into this phenomenon for an interpretation. We find continuity aids ViT in learning larger spatial patterns, which are harder to transfer than smaller ones, enlarging domain distances. Meanwhile, it implies that only smaller patterns within each patch could be transferred under extreme domain gaps. Based on this interpretation, we further propose a simple yet effective method for CDFSL that better disrupts the continuity of image tokens, encouraging the model to rely less on large patterns and more on smaller ones. Extensive experiments show the effectiveness of our method in reducing domain gaps and outperforming state-of-the-art works. Codes and models are available at <a target="_blank" rel="noopener" href="https://github.com/shuaiyi308/ReCIT">https://github.com/shuaiyi308/ReCIT</a>. </p>
<blockquote>
<p>Vision Transformer（ViT）由于在通用领域的大规模预训练而取得了显著的成功，但在应用于只有少量训练数据的下游领域时仍面临挑战，这引发了跨域小样本学习（CDFSL）的任务。受自注意力对令牌顺序的不敏感性的启发，我们发现当前工作中忽略了一个有趣的现象：破坏图像令牌连续性（即，使像素在补丁之间不平稳过渡）在ViT中会导致在一般（源）领域的性能显著下降，但在下游目标领域只会导致轻微的性能下降。这质疑了图像令牌连续性在ViT跨大领域差距时的通用化作用。本文深入探讨了这一现象以进行解释。我们发现连续性有助于ViT学习更大的空间模式，这些模式比小的模式更难迁移，从而扩大了领域间的距离。同时，它暗示在极端的领域差距下，每个补丁内的小模式可能会发生转移。基于这一解释，我们进一步提出了一种简单有效的CDFSL方法，更好地破坏了图像令牌的连续性，鼓励模型更少地依赖大模式，更多地依赖小模式。大量实验表明，我们的方法在减少领域差距和性能上优于最新技术。相关代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/shuaiyi308/ReCIT%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/shuaiyi308/ReCIT上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03110v1">PDF</a> Accepted by ICML 2025(spotlight)</p>
<p><strong>Summary</strong></p>
<p>ViT在大规模通用域预训练上取得了显著成功，但在应用于下游的远距离领域时仍面临挑战，特别是在训练数据稀缺的情况下。本文研究了ViT中图像标记的连续性对跨域少样本学习的影响，发现破坏图像标记的连续性有助于提高模型在目标领域的性能。通过深入研究这一现象，本文提出了一种简单而有效的方法，通过更好地破坏图像标记的连续性来减少领域差距，并超越现有的前沿工作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer (ViT) 在大规模预训练方面取得了成功，但在应用于下游的远距离领域时仍面临挑战。</li>
<li>破坏图像标记的连续性（即像素在补丁之间不平稳过渡）在一般（源）域会导致性能显著下降，但在下游目标域仅导致轻微下降。</li>
<li>图像标记的连续性有助于ViT学习更大的空间模式，这些模式在极端领域差异下难以转移。这意味着在极端领域差异下，每个补丁内的小模式可能被转移。</li>
<li>基于这一发现，本文提出了一种简单而有效的方法，通过更好地破坏图像标记的连续性来鼓励模型更少地依赖大模式，更多地依赖小模式。</li>
<li>该方法在减少领域差距方面表现出色，并超越了现有的前沿工作。</li>
<li>论文提供了相关的代码和模型链接供研究者和开发者使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03110">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-387e7aebdffa0599834f1db44b8c9b2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46daf2da3af6abc7c96e18fa530cb24f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14a9186dc06be06f6a3d62b966ef6520.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bc3fb734959cb6804002733ef4794e4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Random-Registers-for-Cross-Domain-Few-Shot-Learning"><a href="#Random-Registers-for-Cross-Domain-Few-Shot-Learning" class="headerlink" title="Random Registers for Cross-Domain Few-Shot Learning"></a>Random Registers for Cross-Domain Few-Shot Learning</h2><p><strong>Authors:Shuai Yi, Yixiong Zou, Yuhua Li, Ruixuan Li</strong></p>
<p>Cross-domain few-shot learning (CDFSL) aims to transfer knowledge from a data-sufficient source domain to data-scarce target domains. Although Vision Transformer (ViT) has shown superior capability in many vision tasks, its transferability against huge domain gaps in CDFSL is still under-explored. In this paper, we find an intriguing phenomenon: during the source-domain training, prompt tuning, as a common way to train ViT, could be harmful for the generalization of ViT in target domains, but setting them to random noises (i.e., random registers) could consistently improve target-domain performance. We then delve into this phenomenon for an interpretation. We find that learnable prompts capture domain information during the training on the source dataset, which views irrelevant visual patterns as vital cues for recognition. This can be viewed as a kind of overfitting and increases the sharpness of the loss landscapes. In contrast, random registers are essentially a novel way of perturbing attention for the sharpness-aware minimization, which helps the model find a flattened minimum in loss landscapes, increasing the transferability. Based on this phenomenon and interpretation, we further propose a simple but effective approach for CDFSL to enhance the perturbation on attention maps by adding random registers on the semantic regions of image tokens, improving the effectiveness and efficiency of random registers. Extensive experiments on four benchmarks validate our rationale and state-of-the-art performance. Codes and models are available at <a target="_blank" rel="noopener" href="https://github.com/shuaiyi308/REAP">https://github.com/shuaiyi308/REAP</a>. </p>
<blockquote>
<p>跨域小样本学习（CDFSL）旨在将从数据充足来源域的知识转移到数据稀缺的目标域。尽管Vision Transformer（ViT）在许多视觉任务中表现出了卓越的能力，但在CDFSL中针对巨大的域差异的可转移性仍然未被充分探索。在本文中，我们发现了一个有趣的现象：在源域训练过程中，作为训练ViT的一种常见方法，提示调整可能会损害ViT在目标域中的泛化能力，但将其设置为随机噪声（即随机寄存器）可以持续提高目标域的性能。然后，我们深入研究了这一现象并进行了解释。我们发现，在源数据集上进行训练时，可学习的提示会捕获域信息，将不相关的视觉模式视为识别的关键线索。这可以被视为一种过拟合，并增加了损失景观的尖锐度。相比之下，随机寄存器本质上是扰动注意力的一种新型方式，用于尖锐度感知最小化，有助于模型在损失景观中找到平坦的最小值，从而提高可转移性。基于这种现象和解释，我们进一步提出了针对CDFSL的简单有效方法，通过在图像令牌的语义区域上添加随机寄存器来加强对注意力图的扰动，提高了随机寄存器的有效性和效率。在四个基准测试上的大量实验验证了我们理论的合理性并达到了最新技术水平。相关代码和模型可访问<a target="_blank" rel="noopener" href="https://github.com/shuaiyi308/REAP%E3%80%82">https://github.com/shuaiyi308/REAP。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02843v1">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了跨域小样本学习（CDFSL）中，Vision Transformer（ViT）在源域训练时的prompt tuning对目标域泛化性能的影响。研究发现，相较于使用prompt tuning，将模型设置为随机噪声（即随机寄存器）可以提高目标域的性能。本文进一步探究其原因，发现学习到的提示在源数据集训练时会捕获域信息，这可能使模型对特定模式的视觉图案过于敏感而降低泛化性能。与此相反，随机寄存器本质上是一种对注意力进行扰动的方法，有助于模型在损失景观中找到平坦的最小值，从而提高迁移能力。基于此现象和解释，本文提出了一种针对CDFSL的简单有效的随机寄存器强化注意力图的方案，实现较高的效果。同时有众多实验验证了该方案的有效性。相关代码和模型已在GitHub上公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer在跨域小样本学习中的迁移能力尚未得到充分研究。</li>
<li>提示训练可能会捕获源域的特定信息，导致模型在目标域的泛化性能下降。</li>
<li>随机噪声（随机寄存器）的设定有助于模型提高目标域的性能表现。这种噪声可以使模型注意力得以扰动并降低过拟合风险。</li>
<li>通过随机寄存器强化注意力图的方法可以进一步提升模型在CDFSL中的表现。</li>
<li>该方法在不同基准测试上的表现均优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02843">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7aec4b4cb3886684cfb8c3ee27a47343.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba799daefa60168611660a07c87de0f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8876359eaf4c9ddfe33f6de1b6bb1528.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb465ff09555a14b2e7c8c641d0818f8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Self-Disentanglement-and-Re-Composition-for-Cross-Domain-Few-Shot-Segmentation"><a href="#Self-Disentanglement-and-Re-Composition-for-Cross-Domain-Few-Shot-Segmentation" class="headerlink" title="Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot   Segmentation"></a>Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot   Segmentation</h2><p><strong>Authors:Jintao Tong, Yixiong Zou, Guangyao Chen, Yuhua Li, Ruixuan Li</strong></p>
<p>Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a source-domain dataset to unseen target-domain datasets with limited annotations. Current methods typically compare the distance between training and testing samples for mask prediction. However, we find an entanglement problem exists in this widely adopted method, which tends to bind sourcedomain patterns together and make each of them hard to transfer. In this paper, we aim to address this problem for the CD-FSS task. We first find a natural decomposition of the ViT structure, based on which we delve into the entanglement problem for an interpretation. We find the decomposed ViT components are crossly compared between images in distance calculation, where the rational comparisons are entangled with those meaningless ones by their equal importance, leading to the entanglement problem. Based on this interpretation, we further propose to address the entanglement problem by learning to weigh for all comparisons of ViT components, which learn disentangled features and re-compose them for the CD-FSS task, benefiting both the generalization and finetuning. Experiments show that our model outperforms the state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under 1-shot and 5-shot settings, respectively. </p>
<blockquote>
<p>跨域小样本分割（CD-FSS）旨在将源域数据集的知识转移到未见过的目标域数据集上，且目标域数据集标注有限。当前的方法通常通过比较训练和测试样本之间的距离来进行掩膜预测。然而，我们发现广泛采用的这种方法中存在纠缠问题，它倾向于将源域模式绑定在一起，使得每个模式都难以转移。本文旨在解决CD-FSS任务中的这个问题。首先，我们找到了ViT结构的自然分解，并在此基础上深入研究了纠缠问题以进行解释。我们发现分解后的ViT组件在距离计算时会进行图像间的交叉比较，其中合理的比较与无意义的比较纠缠在一起，具有同等重要性，导致了纠缠问题。基于这一解释，我们进一步提出通过为所有ViT组件的比较学习权重来解决纠缠问题，这些组件学习解耦的特征并重新组合它们以用于CD-FSS任务，有益于泛化和微调。实验表明，我们的模型在平均准确率上超过了最新的CD-FSS方法，在1次和5次拍摄设置下分别提高了1.92%和1.88%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02677v1">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了跨域小样本分割（CD-FSS）任务中的纠缠问题，并对此提出了解决方案。文章首先基于ViT结构进行了自然分解，发现了纠缠问题的存在，然后提出了通过学习为ViT组件的对比进行加权来解决纠缠问题的方法。该方法能够学习解纠缠特征并重新组合，有助于提高CD-FSS任务的泛化和微调能力。实验表明，该方法在平均准确率上超过了现有CD-FSS方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSS任务旨在从源域数据集向未见过的目标域数据集进行知识迁移，且目标数据集标注有限。</li>
<li>当前方法主要通过比较训练和测试样本之间的距离来进行掩膜预测，但存在纠缠问题，即源域模式相互绑定，难以进行迁移。</li>
<li>文章基于ViT结构进行了自然分解，发现了纠缠问题的存在，并对此进行了详细解释。</li>
<li>文章指出，ViT组件在距离计算中进行交叉比较时，有意义的比较和无意义的比较纠缠在一起，导致纠缠问题。</li>
<li>为了解决纠缠问题，文章提出了通过学习为ViT组件的对比进行加权的方法，该方法能够学习解纠缠特征并重新组合。</li>
<li>该方法有助于提高CD-FSS任务的泛化和微调能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02677">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b06d2620bc517f6db45a09e4e5109f94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-402ca82f88135de987c6d21d1ee47e30.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb9f38507982ac949105175dc8681ca5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9e4a098e266a732b3d12d528ca5c134.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b27ec9b41c4c61c1c92c943fecd8f6fe.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multi-modal-brain-MRI-synthesis-based-on-SwinUNETR"><a href="#Multi-modal-brain-MRI-synthesis-based-on-SwinUNETR" class="headerlink" title="Multi-modal brain MRI synthesis based on SwinUNETR"></a>Multi-modal brain MRI synthesis based on SwinUNETR</h2><p><strong>Authors:Haowen Pang, Weiyan Guo, Chuyang Ye</strong></p>
<p>Multi-modal brain magnetic resonance imaging (MRI) plays a crucial role in clinical diagnostics by providing complementary information across different imaging modalities. However, a common challenge in clinical practice is missing MRI modalities. In this paper, we apply SwinUNETR to the synthesize of missing modalities in brain MRI. SwinUNETR is a novel neural network architecture designed for medical image analysis, integrating the strengths of Swin Transformer and convolutional neural networks (CNNs). The Swin Transformer, a variant of the Vision Transformer (ViT), incorporates hierarchical feature extraction and window-based self-attention mechanisms, enabling it to capture both local and global contextual information effectively. By combining the Swin Transformer with CNNs, SwinUNETR merges global context awareness with detailed spatial resolution. This hybrid approach addresses the challenges posed by the varying modality characteristics and complex brain structures, facilitating the generation of accurate and realistic synthetic images. We evaluate the performance of SwinUNETR on brain MRI datasets and demonstrate its superior capability in generating clinically valuable images. Our results show significant improvements in image quality, anatomical consistency, and diagnostic value. </p>
<blockquote>
<p>多模态脑磁共振成像（MRI）在临床诊断中扮演着至关重要的角色，因为它能提供不同成像模式之间的互补信息。然而，在临床实践中，一个常见的挑战是缺失MRI模式。在本文中，我们将SwinUNETR应用于脑MRI中缺失模式的合成。SwinUNETR是一种为医学图像分析设计的新型神经网络架构，融合了Swin Transformer和卷积神经网络（CNN）的优势。Swin Transformer是Vision Transformer（ViT）的一种变体，结合了分层特征提取和基于窗口的自注意力机制，能够有效地捕捉局部和全局上下文信息。通过将Swin Transformer与CNN相结合，SwinUNETR融合了全局上下文意识和详细的空间分辨率。这种混合方法解决了不同模态特征和复杂脑结构所带来的挑战，有利于生成准确和逼真的合成图像。我们在脑MRI数据集上评估了SwinUNETR的性能，并展示了其在生成具有临床价值图像方面的卓越能力。我们的结果在图像质量、解剖一致性和诊断价值方面都有显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02467v1">PDF</a> 9 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>基于多模态脑磁共振成像（MRI）在临床诊断中的重要作用，本文利用SwinUNETR合成缺失模态的脑MRI。SwinUNETR是一种用于医学图像分析的新型神经网络架构，结合了Swin Transformer和卷积神经网络（CNN）的优势。该架构融合了全局上下文感知和精细的空间分辨率，能有效应对不同模态特征和复杂脑结构带来的挑战，生成准确逼真的合成图像。在脑MRI数据集上的评估结果表明，SwinUNETR在图像质量、解剖一致性和诊断价值方面表现出卓越的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态MRI在临床诊断中提供不同成像模态的互补信息，对缺失模态的合成具有挑战。</li>
<li>SwinUNETR是一种结合了Swin Transformer和CNN的新型神经网络架构，用于医学图像分析。</li>
<li>Swin Transformer是Vision Transformer（ViT）的一种变体，具有层次化特征提取和基于窗口的自注意力机制，能捕捉局部和全局上下文信息。</li>
<li>SwinUNETR将全局上下文感知与精细的空间分辨率相结合，应对不同模态特征和复杂脑结构的挑战。</li>
<li>SwinUNETR在脑MRI数据集上的性能评估表明其在图像质量、解剖一致性和诊断价值方面的卓越能力。</li>
<li>SwinUNETR生成的合成图像具有准确性和逼真性。</li>
<li>该方法在临床应用中有潜力提高诊断的准确性和可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02467">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-101d4e8c0e012b217349f8c3a7cb6d6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2960ebb0017918f9368b90f15a99fe5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b3f299fa53d6a0a187b0f6c228aa01f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11edaf5738dc5a5d9be2ca99c1e933e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc66021e19ac7c1de23c57cf8e2c5b0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fedc952046aad915fa0cb77cd5e4e41.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Video-Level-Language-Driven-Video-Based-Visible-Infrared-Person-Re-Identification"><a href="#Video-Level-Language-Driven-Video-Based-Visible-Infrared-Person-Re-Identification" class="headerlink" title="Video-Level Language-Driven Video-Based Visible-Infrared Person   Re-Identification"></a>Video-Level Language-Driven Video-Based Visible-Infrared Person   Re-Identification</h2><p><strong>Authors:Shuang Li, Jiaxu Leng, Changjiang Kuang, Mingpi Tan, Xinbo Gao</strong></p>
<p>Video-based Visible-Infrared Person Re-Identification (VVI-ReID) aims to match pedestrian sequences across modalities by extracting modality-invariant sequence-level features. As a high-level semantic representation, language provides a consistent description of pedestrian characteristics in both infrared and visible modalities. Leveraging the Contrastive Language-Image Pre-training (CLIP) model to generate video-level language prompts and guide the learning of modality-invariant sequence-level features is theoretically feasible. However, the challenge of generating and utilizing modality-shared video-level language prompts to address modality gaps remains a critical problem. To address this problem, we propose a simple yet powerful framework, video-level language-driven VVI-ReID (VLD), which consists of two core modules: invariant-modality language prompting (IMLP) and spatial-temporal prompting (STP). IMLP employs a joint fine-tuning strategy for the visual encoder and the prompt learner to effectively generate modality-shared text prompts and align them with visual features from different modalities in CLIP’s multimodal space, thereby mitigating modality differences. Additionally, STP models spatiotemporal information through two submodules, the spatial-temporal hub (STH) and spatial-temporal aggregation (STA), which further enhance IMLP by incorporating spatiotemporal information into text prompts. The STH aggregates and diffuses spatiotemporal information into the [CLS] token of each frame across the vision transformer (ViT) layers, whereas STA introduces dedicated identity-level loss and specialized multihead attention to ensure that the STH focuses on identity-relevant spatiotemporal feature aggregation. The VLD framework achieves state-of-the-art results on two VVI-ReID benchmarks. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/Visuang/VLD">https://github.com/Visuang/VLD</a>. </p>
<blockquote>
<p>基于视频的可见光-红外行人再识别（VVI-ReID）旨在通过提取跨模态的序列级特征来匹配跨模态的行人序列。作为一种高级语义表示，语言为红外和可见光两种模态下的行人特征提供了连贯的描述。利用对比语言图像预训练（CLIP）模型生成视频级语言提示并引导学习跨模态的序列级特征在理论上可行。然而，生成和利用跨模态共享的视频级语言提示来解决模态差异的挑战仍然是关键性问题。针对这一问题，我们提出了一个简单而强大的框架，即视频级语言驱动VVI-ReID（VLD），它包含两个核心模块：不变模态语言提示（IMLP）和空间时间提示（STP）。IMLP采用联合微调策略，对视觉编码器和提示学习者进行有效训练，以生成跨模态共享文本提示，并将它们与CLIP多模态空间中的不同模态的视觉特征对齐，从而减轻模态差异。此外，STP通过两个子模块，即空间时间中心（STH）和空间时间聚合（STA），对时空信息进行建模，通过将时空信息融入文本提示来进一步增强IMLP。STH将每帧的时空信息聚合并扩散到视觉转换器（ViT）各层的[CLS]标记中，而STA引入专用的身份级损失和特殊的多头注意力，以确保STH专注于身份相关的时空特征聚合。VLD框架在两个VVI-ReID基准测试中达到了最新水平的结果。代码将在<a target="_blank" rel="noopener" href="https://github.com/Visuang/VLD%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Visuang/VLD上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02439v1">PDF</a> Accepted by IEEE TIFS</p>
<p><strong>摘要</strong><br>基于视频的多模态行人再识别（VVI-ReID）旨在通过提取跨模态的序列级特征来匹配不同模态下的行人序列。本研究提出一种简洁而强大的框架，即视频级语言驱动的VVI-ReID（VLD），该框架包括两个核心模块：不变模态语言提示（IMLP）和时空提示（STP）。IMLP采用联合微调策略，对视觉编码器和提示学习者进行有效训练，生成跨模态共享文本提示，并将它们与不同模态的视觉特征对齐到CLIP的多模态空间中，从而减轻模态差异。STP模块通过两个子模块（时空中心和时空聚合）将时空信息融入文本提示中，进一步增强IMLP的效果。该框架在两项VVI-ReID基准测试中取得了最新成果。代码将发布在<a target="_blank" rel="noopener" href="https://github.com/Visuang/VLD%E3%80%82">https://github.com/Visuang/VLD。</a></p>
<p><strong>要点提炼</strong></p>
<ol>
<li>VVI-ReID的目标是匹配跨模态的行人序列，通过提取序列级特征来进行识别。</li>
<li>利用Contrastive Language-Image Pre-training (CLIP)模型生成视频级语言提示，为解决模态差异问题提供了理论可行性。</li>
<li>提出的视频级语言驱动框架（VLD）包含两个核心模块：不变模态语言提示（IMLP）和时空提示（STP）。</li>
<li>IMLP模块通过联合微调策略生成跨模态共享文本提示，并减轻模态差异。</li>
<li>STP模块通过时空中心和时空聚合两个子模块融入时空信息，进一步增强识别效果。</li>
<li>该框架在VVI-ReID的基准测试中取得了最新成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02439">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7c03d68cf92104a7d640221b626161d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed87dd759899125534f98259ebfe344c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-460028b1f2849d37712b4dc11d415b60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1acee7f96b73e0ed08ef726c4a069290.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Dual-Process-Image-Generation"><a href="#Dual-Process-Image-Generation" class="headerlink" title="Dual-Process Image Generation"></a>Dual-Process Image Generation</h2><p><strong>Authors:Grace Luo, Jonathan Granskog, Aleksander Holynski, Trevor Darrell</strong></p>
<p>Prior methods for controlling image generation are limited in their ability to be taught new tasks. In contrast, vision-language models, or VLMs, can learn tasks in-context and produce the correct outputs for a given input. We propose a dual-process distillation scheme that allows feed-forward image generators to learn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the generated images and backpropagates this gradient to update the weights of the image generator. Our general framework enables a wide variety of new control tasks through the same text-and-image based interface. We showcase a handful of applications of this technique for different types of control signals, such as commonsense inferences and visual prompts. With our method, users can implement multimodal controls for properties such as color palette, line weight, horizon position, and relative depth within a matter of minutes. Project page: <a target="_blank" rel="noopener" href="https://dual-process.github.io/">https://dual-process.github.io</a>. </p>
<blockquote>
<p>先前控制图像生成的方法在教授新任务方面的能力有限。相比之下，视觉语言模型（VLM）可以在上下文中学习任务并为给定输入产生正确的输出。我们提出了一种双过程蒸馏方案，该方案允许前馈图像生成器从深思熟虑的VLM中学习新任务。我们的方案使用VLM对生成的图像进行评分，并将此梯度反向传播以更新图像生成器的权重。我们的通用框架通过相同的文本和图像接口支持多种新控制任务。我们展示了该技术在不同类型的控制信号方面的几个应用，如常识推理和视觉提示。使用我们的方法，用户可以在几分钟内实现颜色调、线条粗细、地平位置以及相对深度等多模式控制。项目页面：<a target="_blank" rel="noopener" href="https://dual-process.github.io./">https://dual-process.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01955v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种双过程蒸馏方案，该方案允许前馈图像生成器从深思熟虑的视界语言模型（VLMs）学习新任务。通过利用VLM对生成的图像进行评分并将梯度反向传播以更新图像生成器的权重，该方案实现了文本和图像基于接口的多样化新控制任务。该技术可快速实现多种控制信号的应用，如常识推理和视觉提示，用户可以在几分钟内实现颜色调色板、线条粗细、地平线位置和相对深度的多模式控制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有图像生成控制方法存在无法学习新任务的局限性。</li>
<li>视觉语言模型（VLMs）可以在上下文中学习新任务并为给定输入产生正确输出。</li>
<li>提出的双过程蒸馏方案允许前馈图像生成器从深思熟虑的VLMs学习新任务。</li>
<li>该方案使用VLM对生成的图像进行评分，并通过反向传播更新图像生成器的权重。</li>
<li>该框架通过文本和图像接口支持多种新控制任务。</li>
<li>该技术可以快速实现多种控制信号的应用，如常识推理和视觉提示。</li>
<li>用户可以迅速实现颜色调色板、线条粗细、地平线位置和相对深度的多模式控制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01955">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-87755e022352dadfba93506f95cc73fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-034aa7dcb97c5bb0150911e509c2c6b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-326767d00ef7c16d907c33738e452dd3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5382c3f8c112bb5dd2c9141f61b170a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb413109e3a1d076af57ee969343695f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Speed-up-of-Vision-Transformer-Models-by-Attention-aware-Token-Filtering"><a href="#Speed-up-of-Vision-Transformer-Models-by-Attention-aware-Token-Filtering" class="headerlink" title="Speed-up of Vision Transformer Models by Attention-aware Token Filtering"></a>Speed-up of Vision Transformer Models by Attention-aware Token Filtering</h2><p><strong>Authors:Takahiro Naruko, Hiroaki Akutsu</strong></p>
<p>Vision Transformer (ViT) models have made breakthroughs in image embedding extraction, which provide state-of-the-art performance in tasks such as zero-shot image classification. However, the models suffer from a high computational burden. In this paper, we propose a novel speed-up method for ViT models called Attention-aware Token Filtering (ATF). ATF consists of two main ideas: a novel token filtering module and a filtering strategy. The token filtering module is introduced between a tokenizer and a transformer encoder of the ViT model, without modifying or fine-tuning of the transformer encoder. The module filters out tokens inputted to the encoder so that it keeps tokens in regions of specific object types dynamically and keeps tokens in regions that statically receive high attention in the transformer encoder. This filtering strategy maintains task accuracy while filtering out tokens inputted to the transformer encoder. Evaluation results on retrieval tasks show that ATF provides $2.8\times$ speed-up to a ViT model, SigLIP, while maintaining the retrieval recall rate. </p>
<blockquote>
<p>Vision Transformer（ViT）模型在图像嵌入提取方面取得了突破，其在零样本图像分类等任务中达到了最先进的性能。然而，这些模型面临着计算负担较大的问题。在本文中，我们提出了一种用于加速ViT模型的新型方法，称为注意力感知令牌过滤（ATF）。ATF主要包括两个主要思想：一个新颖的令牌过滤模块和过滤策略。令牌过滤模块被引入到ViT模型的令牌器和变压器编码器之间，无需修改或微调变压器编码器。该模块过滤掉输入到编码器的令牌，以便动态地保留特定对象类型区域的令牌，并保留在变压器编码器中静态接收高注意力的区域的令牌。这种过滤策略在过滤掉输入到变压器编码器的令牌的同时保持了任务准确性。在检索任务上的评估结果表明，ATF在保持检索召回率的同时，为ViT模型SigLIP提供了2.8倍的速度提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01519v1">PDF</a> </p>
<p><strong>Summary</strong><br>在视觉转换器（ViT）模型中，针对图像嵌入提取取得了突破性进展，其在零样本图像分类等任务中表现出卓越性能。然而，这些模型计算负担较高。本文提出了一种名为注意力感知令牌过滤（ATF）的ViT模型加速方法。ATF主要包括两个核心理念：新颖令牌过滤模块和过滤策略。令牌过滤模块被引入ViT模型的令牌器和转换器编码器之间，无需修改或微调转换器编码器。该模块动态保留特定对象区域的令牌并保留静态接收转换器编码器中高关注度的区域的令牌。此过滤策略在过滤输入到转换器编码器的令牌的同时保持任务准确性。在检索任务上的评估结果表明，ATF能在保持检索召回率的同时，将ViT模型SigLIP的速度提高2.8倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer (ViT) 在图像嵌入提取方面取得突破，适用于零样本图像分类等任务。</li>
<li>ViT模型面临高计算负担问题。</li>
<li>提出了名为ATF的ViT模型加速方法，包括新颖的令牌过滤模块和过滤策略。</li>
<li>令牌过滤模块位于ViT模型的令牌器和转换器编码器之间，无需修改或微调转换器编码器。</li>
<li>ATF通过动态保留特定对象区域的令牌和静态高关注度区域的令牌进行过滤。<br>6.ATF能在保持任务准确性的同时，减少输入到转换器编码器的令牌数量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01519">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e191e117382e93f563de2c97c5b0944e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6b216933f506ad403fe69e17136bb6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5bf21c854a97ab5642ba043428a50cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b431f58501c73cb902bd8523ba4dd6e8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SAM-I2V-Upgrading-SAM-to-Support-Promptable-Video-Segmentation-with-Less-than-0-2-Training-Cost"><a href="#SAM-I2V-Upgrading-SAM-to-Support-Promptable-Video-Segmentation-with-Less-than-0-2-Training-Cost" class="headerlink" title="SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with   Less than 0.2% Training Cost"></a>SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with   Less than 0.2% Training Cost</h2><p><strong>Authors:Haiyang Mei, Pengyu Zhang, Mike Zheng Shou</strong></p>
<p>Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V, an effective image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAM’s static image encoder to enable spatiotemporal video perception, (ii) a memory filtering strategy that selects the most relevant past frames for more effective utilization of historical information, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves over 90% of SAM 2’s performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Code and model are available at: <a target="_blank" rel="noopener" href="https://github.com/showlab/SAM-I2V">https://github.com/showlab/SAM-I2V</a>. </p>
<blockquote>
<p>像Segment Anything Model（SAM）这样的基础模型已经在计算机视觉中推动了可提示的图像分割的显著进步。然而，将这些能力扩展到视频却面临着巨大的挑战，特别是在动态场景中确保精确和时空一致的遮罩传播方面。SAM 2 通过在大量图像和视频数据上从头开始训练模型以学习复杂的时空关联来解决这个问题，这导致了巨大的训练成本，阻碍了研究和实际应用部署。在本文中，我们介绍了SAM-I2V，这是一种有效的从图像到视频的升级方法，用于培养可提示的视频分割（PVS）模型。我们的方法通过升级预训练的SAM来支持PVS，大大降低了训练复杂度和资源需求。为此，我们引入了三个关键创新点：（i）一种基于SAM静态图像编码器的图像到视频特征提取升级器，以实现时空视频感知；（ii）一种记忆过滤策略，用于选择最相关的过去帧以更有效地利用历史信息；（iii）一种利用对象记忆的记忆即提示机制，以确保动态场景中的时空一致遮罩传播。综合实验表明，我们的方法达到了SAM 2性能的90%以上，同时仅使用其0.2%的训练成本。我们的工作为PVS提供了一条资源高效的途径，降低了PVS模型设计的进一步研究障碍，并启用了该领域的更广泛应用和进展。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/showlab/SAM-I2V">https://github.com/showlab/SAM-I2V</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01304v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong>：基于SAM模型的可提示视频分割技术（SAM-I2V）研究介绍了一种有效的图像到视频的升级方法，用于构建可提示视频分割（PVS）模型。该方法通过升级预训练的SAM模型来支持PVS，显著降低了训练复杂性和资源需求。引入三项关键技术创新，包括基于SAM静态图像编码器的图像到视频特征提取升级器、记忆过滤策略以及利用对象记忆的提示机制。实验表明，该方法实现了超过90%的SAM 2性能，同时仅使用其0.2%的训练成本。为PVS设计提供了资源高效的途径，降低了进一步研究和实践应用的门槛。代码和模型已公开。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>SAM模型在提示图像分割方面表现出卓越性能，但将其扩展到视频面临挑战，特别是确保动态场景中的精确和时序一致的掩膜传播。</li>
<li>SAM 2模型通过大规模图像和视频数据从头开始训练来学习复杂的时空关联，但带来了巨大的训练成本，阻碍了研究和实际应用部署。</li>
<li>SAM-I2V方法介绍了一种有效的图像到视频的升级策略，以支持提示视频分割（PVS）。该方法显著降低了训练复杂性和资源需求。</li>
<li>SAM-I2V引入了三项关键创新：图像到视频特征提取升级器、记忆过滤策略和记忆提示机制，确保动态场景中的时空一致掩膜传播。</li>
<li>实验表明，SAM-I2V方法实现了与SAM 2相当的性能表现，同时大幅降低了训练成本。</li>
<li>公开的代码和模型为资源受限环境下的研究提供了便利，促进了视频分割领域的进一步研究和应用发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01304">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0c6b8d8ddcf2279995e9c5eb8c0cb8e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-307d789d3f53844008e298dc60575b8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3f8a61f35826bea6db2504c57921fa8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Understanding-Model-Reprogramming-for-CLIP-via-Decoupling-Visual-Prompts"><a href="#Understanding-Model-Reprogramming-for-CLIP-via-Decoupling-Visual-Prompts" class="headerlink" title="Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts"></a>Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts</h2><p><strong>Authors:Chengyi Cai, Zesheng Ye, Lei Feng, Jianzhong Qi, Feng Liu</strong></p>
<p>Model reprogramming adapts pretrained models to downstream tasks by modifying only the input and output spaces. Visual reprogramming (VR) is one instance for vision tasks that adds a trainable noise pattern (i.e., a visual prompt) to input images to facilitate downstream classification. The existing VR approaches for CLIP train a single visual prompt using all descriptions of different downstream classes. However, the limited learning capacity may result in (1) a failure to capture diverse aspects of the descriptions (e.g., shape, color, and texture), and (2) a possible bias toward less informative attributes that do not help distinguish between classes. In this paper, we introduce a decoupling-and-reweighting framework. Our decoupled visual prompts (DVP) are optimized using descriptions grouped by explicit causes (DVP-cse) or unsupervised clusters (DVP-cls). Then, we integrate the outputs of these visual prompts with a probabilistic reweighting matrix (PRM) that measures their contributions to each downstream class. Theoretically, DVP lowers the empirical risk bound. Experimentally, DVP outperforms baselines on average across 11 downstream datasets. Notably, the DVP-PRM integration enables insights into how individual visual prompts influence classification decisions, providing a probabilistic framework for understanding reprogramming. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tmlr-group/DecoupledVP">https://github.com/tmlr-group/DecoupledVP</a>. </p>
<blockquote>
<p>模型重构通过仅修改输入和输出空间来适应预训练模型以适应下游任务。视觉重构（VR）是视觉任务的一个实例，它向输入图像添加可训练的噪声模式（即视觉提示）以促进下游分类。现有的CLIP的VR方法使用不同下游类别的所有描述来训练一个单一视觉提示。然而，有限的学习能力可能导致（1）无法捕捉描述的各个方面（例如形状、颜色和纹理）；（2）偏向于可能不具有区分不同类别能力的信息较少的属性。在本文中，我们引入了一个解耦和加权框架。我们的解耦视觉提示（DVP）使用按明确原因分组（DVP-cse）的描述或无监督聚类（DVP-cls）进行优化。然后，我们使用概率加权矩阵（PRM）整合这些视觉提示的输出，该矩阵衡量它们对下游每个类别的贡献。理论上，DVP降低了经验风险界限。实验上，DVP在平均意义上优于在11个下游数据集上的基准测试。值得注意的是，DVP-PRM集成提供了关于单个视觉提示如何影响分类决策的信息，提供了一个理解重构的概率框架。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/tmlr-group/DecoupledVP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tmlr-group/DecoupledVP找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01000v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了视觉重编程（VR）的一种新方法——解耦和重权框架下的视觉提示（DVP）。该框架通过对不同下游类别的描述进行分组优化视觉提示，降低经验风险界限。在多个下游数据集上的实验表明，DVP优于基准方法。特别是，DVP-PRM集成提供了对单个视觉提示如何影响分类决策的理解，为重新编程提供了一个概率框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉重编程（VR）是一种用于适应预训练模型的方法，只修改输入和输出空间。通过向输入图像添加可训练的噪声模式（即视觉提示）来适应下游分类任务。现有的VR方法可能无法捕捉描述的各个方面，并且可能偏向于没有帮助区分类的信息较少的属性。为了解决这些问题，提出了一个解耦和重权框架下的视觉提示（DVP）。通过此框架优化的视觉提示被称为解耦的视觉提示（DVP）。</li>
<li>DVP通过两种方式优化视觉提示：一种是基于明确的因果分组进行优化（DVP-cse），另一种是基于无监督聚类进行优化（DVP-cls）。通过这两种方式优化的视觉提示可以更有效地捕获描述中的信息并适应下游任务。理论而言，该框架有助于降低经验风险界限。因此相对于基线方法有更优的表现。在多个下游数据集上的实验验证了这一点。</li>
<li>DVP与概率重权矩阵（PRM）的结合使得我们能够理解单个视觉提示如何影响分类决策，并为模型重新编程提供了一个新的视角和理解框架。这对于解释和改进模型的性能非常重要。对于理解和改进模型性能非常重要。此代码的开源实现为用户提供了一个了解和运用该方法的机会。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01000">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1e5d89ed57a510ec190795faddc1f565.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4120af3424f23c2ce1607e92e00ec71c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-250a6315282d0c14a19200aa57042195.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8187121cc6a85f340153eb9f2d1aecc3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PerFormer-A-Permutation-Based-Vision-Transformer-for-Remaining-Useful-Life-Prediction"><a href="#PerFormer-A-Permutation-Based-Vision-Transformer-for-Remaining-Useful-Life-Prediction" class="headerlink" title="PerFormer: A Permutation Based Vision Transformer for Remaining Useful   Life Prediction"></a>PerFormer: A Permutation Based Vision Transformer for Remaining Useful   Life Prediction</h2><p><strong>Authors:Zhengyang Fan, Wanru Li, Kuo-chu Chang, Ting Yuan</strong></p>
<p>Accurately estimating the remaining useful life (RUL) for degradation systems is crucial in modern prognostic and health management (PHM). Convolutional Neural Networks (CNNs), initially developed for tasks like image and video recognition, have proven highly effectively in RUL prediction, demonstrating remarkable performance. However, with the emergence of the Vision Transformer (ViT), a Transformer model tailored for computer vision tasks such as image classification, and its demonstrated superiority over CNNs, there is a natural inclination to explore its potential in enhancing RUL prediction accuracy. Nonetheless, applying ViT directly to multivariate sensor data for RUL prediction poses challenges, primarily due to the ambiguous nature of spatial information in time series data. To address this issue, we introduce the PerFormer, a permutation-based vision transformer approach designed to permute multivariate time series data, mimicking spatial characteristics akin to image data, thereby making it suitable for ViT. To generate the desired permutation matrix, we introduce a novel permutation loss function aimed at guiding the convergence of any matrix towards a permutation matrix. Our experiments on NASA’s C-MAPSS dataset demonstrate the PerFormer’s superior performance in RUL prediction compared to state-of-the-art methods employing CNNs, Recurrent Neural Networks (RNNs), and various Transformer models. This underscores its effectiveness and potential in PHM applications. </p>
<blockquote>
<p>对退化系统的剩余使用寿命（RUL）进行准确估计是现代预测与健康管理（PHM）中的关键。卷积神经网络（CNN）最初被开发用于图像和视频识别等任务，已经证明在RUL预测中的高度有效性，表现出卓越的性能。然而，随着用于图像分类等计算机视觉任务的定制Transformer模型——视觉Transformer（ViT）的出现，以及其相较于CNN的优越性证明，人们自然倾向于探索其在提高RUL预测精度方面的潜力。然而，直接将ViT应用于多元传感器数据的RUL预测却存在挑战，这主要是因为时间序列数据中空间信息的模糊性。为了解决这一问题，我们引入了PerFormer，这是一种基于排列的愿景转换器方法，旨在排列多元时间序列数据，模仿类似于图像数据的空间特征，从而使其适合ViT。为了生成所需的排列矩阵，我们引入了一种新型排列损失函数，旨在引导任何矩阵向排列矩阵收敛。我们在NASA的C-MAPSS数据集上的实验表明，PerFormer在RUL预测方面的性能优于采用CNN、循环神经网络（RNN）和各种Transformer模型的最先进方法。这突显了其在PHM应用中的有效性和潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00259v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>卷积神经网络（CNN）在剩余使用寿命（RUL）预测中表现出卓越的性能，但随着针对计算机视觉任务的定制版Vision Transformer（ViT）的出现，其在RUL预测中的潜力备受关注。然而，直接应用ViT于多元传感器数据进行RUL预测存在挑战。为此，我们引入了PerFormer，一种基于排列的Vision Transformer方法，可对多元时间序列数据进行排列，模拟图像数据的空间特征。通过引入新型排列损失函数，指导排列矩阵的收敛。在NASA的C-MAPSS数据集上的实验表明，PerFormer在RUL预测方面的性能优于采用CNN、循环神经网络（RNN）和各种Transformer模型的最先进方法，突显其在预后与健康管理（PHM）应用中的有效性及潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>卷积神经网络（CNN）在剩余使用寿命（RUL）预测中已有卓越表现。</li>
<li>Vision Transformer（ViT）的出现为RUL预测带来了新的潜力。</li>
<li>直接应用ViT于多元传感器数据存在挑战，因时间序列数据的空间信息不明确。</li>
<li>引入PerFormer，一种基于排列的Vision Transformer方法，能排列多元时间序列数据，模拟图像数据的空间特征。</li>
<li>新型排列损失函数用于生成排列矩阵，并引导其收敛。</li>
<li>在NASA的C-MAPSS数据集上的实验显示，PerFormer在RUL预测方面性能卓越，优于其他模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00259">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e19218d3784cb84ea0372c84aff13cf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07987e73c12645a9a675a02d2ca34332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf163ed0a641d652e910aa5257af767a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36d7e68fce904fa3cee18e5ac80262b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b222b0f4028bfb38e3c6700149875244.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1dc4ba1cceef39874058b23060cc9934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18f5e1ae723051de8e251f53be469cc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efbadda203516f295385634ce64ffe3d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Bayesian-Prompt-Flow-Learning-for-Zero-Shot-Anomaly-Detection"><a href="#Bayesian-Prompt-Flow-Learning-for-Zero-Shot-Anomaly-Detection" class="headerlink" title="Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection"></a>Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection</h2><p><strong>Authors:Zhen Qu, Xian Tao, Xinyi Gong, Shichen Qu, Qiyu Chen, Zhengtao Zhang, Xingang Wang, Guiguang Ding</strong></p>
<p>Recently, vision-language models (e.g. CLIP) have demonstrated remarkable performance in zero-shot anomaly detection (ZSAD). By leveraging auxiliary data during training, these models can directly perform cross-category anomaly detection on target datasets, such as detecting defects on industrial product surfaces or identifying tumors in organ tissues. Existing approaches typically construct text prompts through either manual design or the optimization of learnable prompt vectors. However, these methods face several challenges: 1) handcrafted prompts require extensive expert knowledge and trial-and-error; 2) single-form learnable prompts struggle to capture complex anomaly semantics; and 3) an unconstrained prompt space limits generalization to unseen categories. To address these issues, we propose Bayesian Prompt Flow Learning (Bayes-PFL), which models the prompt space as a learnable probability distribution from a Bayesian perspective. Specifically, a prompt flow module is designed to learn both image-specific and image-agnostic distributions, which are jointly utilized to regularize the text prompt space and improve the model’s generalization on unseen categories. These learned distributions are then sampled to generate diverse text prompts, effectively covering the prompt space. Additionally, a residual cross-model attention (RCA) module is introduced to better align dynamic text embeddings with fine-grained image features. Extensive experiments on 15 industrial and medical datasets demonstrate our method’s superior performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xiaozhen228/Bayes-PFL">https://github.com/xiaozhen228/Bayes-PFL</a>. </p>
<blockquote>
<p>最近，视觉语言模型（例如CLIP）在零样本异常检测（ZSAD）中表现出了卓越的性能。这些模型通过利用训练过程中的辅助数据，可以直接对目标数据集进行跨类别异常检测，如检测工业产品表面的缺陷或识别组织中的肿瘤。现有方法通常通过手动设计或优化可学习的提示向量来构建文本提示。然而，这些方法面临几个挑战：1）手工制作的提示需要广泛的专业知识和试错；2）单一形式的可学习提示难以捕捉复杂的异常语义；3）无约束的提示空间限制了未见类别的泛化能力。为了解决这些问题，我们提出了贝叶斯提示流学习（Bayes-PFL），它从贝叶斯角度将提示空间建模为可学习的概率分布。具体来说，设计了一个提示流模块来学习图像特定和图像通用的分布，这些分布被共同用来规范文本提示空间，提高模型在未见类别上的泛化能力。这些学习到的分布然后被采样以生成多样化的文本提示，有效地覆盖了提示空间。此外，还引入了一个残差跨模型注意力（RCA）模块，以更好地将动态文本嵌入与精细图像特征对齐。在15个工业和医疗数据集上的大量实验证明了我们方法的优越性。代码可用在<a target="_blank" rel="noopener" href="https://github.com/xiaozhen228/Bayes-PFL%E3%80%82">https://github.com/xiaozhen228/Bayes-PFL。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10080v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为贝叶斯提示流学习（Bayes-PFL）的方法，用于解决视觉语言模型在零样本异常检测（ZSAD）中的挑战。该方法通过从贝叶斯视角对提示空间进行建模，解决了手工提示需要大量专业知识和反复试验的问题，以及单一形式的学习提示难以捕捉复杂的异常语义和不受约束的提示空间限制了未见类别的泛化能力的问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（如CLIP）在零样本异常检测（ZSAD）中表现出卓越性能。</li>
<li>现有方法通过手动设计或优化学习提示向量来构建文本提示，但存在挑战。</li>
<li>贝叶斯提示流学习（Bayes-PFL）方法提出，将提示空间建模为可从贝叶斯视角学习的概率分布。</li>
<li>Bayes-PFL设计了一个提示流模块来学习图像特定和图像通用的分布，以规范文本提示空间并提高模型对未见类别的泛化能力。</li>
<li>通过采样学习到的分布来生成多样化的文本提示，有效覆盖提示空间。</li>
<li>引入残差跨模型注意力（RCA）模块，以更好地对齐动态文本嵌入和细粒度图像特征。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b524108baea065a77395afd254108739.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c94fdcc90b6b6383389dc62ddbed2cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adb006e8cc299475ed5d64fc6ce62fdd.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ViFOR-A-Fourier-Enhanced-Vision-Transformer-for-Multi-Image-Super-Resolution-in-Earth-System"><a href="#ViFOR-A-Fourier-Enhanced-Vision-Transformer-for-Multi-Image-Super-Resolution-in-Earth-System" class="headerlink" title="ViFOR: A Fourier-Enhanced Vision Transformer for Multi-Image   Super-Resolution in Earth System"></a>ViFOR: A Fourier-Enhanced Vision Transformer for Multi-Image   Super-Resolution in Earth System</h2><p><strong>Authors:Ehsan Zeraatkar, Salah A Faroughi, Jelena Tešić</strong></p>
<p>Super-resolution (SR) is crucial for enhancing the spatial resolution of Earth System Model (ESM) data, thereby enabling more precise analysis of environmental processes. This paper introduces ViFOR, a novel SR algorithm integrating Vision Transformers (ViTs) with Fourier-based Implicit Neural Representation Networks (INRs). ViFOR effectively captures global context and high-frequency details essential for accurate SR reconstruction by embedding Fourier-based activation functions within the transformer architecture. Extensive experiments demonstrate that ViFOR consistently outperforms state-of-the-art methods, including ViT, SIREN, and SRGANs, in terms of Peak Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE) for both global and local imagery. ViFOR achieves PSNR improvements of up to 4.18 dB, 1.56 dB, and 1.73 dB over ViT on full-image Source Temperature, Shortwave, and Longwave Flux datasets. These results highlight ViFOR’s effectiveness and potential for advancing high-resolution climate data analysis. </p>
<blockquote>
<p>超分辨率（SR）对于提高地球系统模型（ESM）数据的空间分辨率至关重要，从而能够更精确地分析环境过程。本文介绍了ViFOR，这是一种新型SR算法，它将视觉转换器（ViTs）与基于傅里叶隐式神经网络表示（INRs）相结合。ViFOR通过将基于傅里叶的激活函数嵌入到转换器架构中，有效地捕获全局上下文和对于准确SR重建至关重要的高频细节。大量实验表明，无论是在全局还是局部图像方面，ViFOR在峰值信噪比（PSNR）和均方误差（MSE）方面均优于包括ViT、SIREN和SRGANs等最新方法。ViFOR在全图像源温度、短波和长波通量数据集上的PSNR分别比ViT提高了4.18 dB、1.56 dB和1.73 dB。这些结果突出了ViFOR的有效性以及其在推进高分辨率气候数据分析方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12427v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了ViFOR，这是一种新型的超分辨率算法，结合了基于Vision Transformer的模型和基于傅立叶变换的隐神经表示网络。ViFOR通过将傅立叶激活函数嵌入到Transformer架构中，有效地捕捉全局上下文和高频细节，对于准确的超分辨率重建至关重要。实验表明，ViFOR在峰值信噪比（PSNR）和均方误差（MSE）方面，全面超越了包括ViT、SIREN和SRGAN等在内的最新方法，无论是在全局还是局部图像上均表现优异。特别是对于全图像源温度、短波和长波流量数据集，ViFOR相比ViT的PSNR提升分别达到了4.18分贝、1.56分贝和1.73分贝。这显示了ViFOR在推进高分辨率气候数据分析方面的潜力和有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViFOR是一种结合了Vision Transformer和基于傅立叶变换的隐神经表示网络的新型超分辨率算法。</li>
<li>ViFOR通过将傅立叶激活函数嵌入到Transformer架构中，以捕捉全局上下文和高频细节，这对于准确超分辨率重建至关重要。</li>
<li>实验表明，ViFOR在峰值信噪比（PSNR）和均方误差（MSE）方面超越了其他最新方法。</li>
<li>ViFOR在全图像源温度、短波和长波流量数据集上的表现优于ViT，PSNR提升显著。</li>
<li>ViFOR算法在推进高分辨率气候数据分析方面具有潜力和有效性。</li>
<li>ViFOR算法能够有效结合全球视野和细节信息，这对于环境过程的精确分析至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12427">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-daefd5915166b5bcc37598aaa2d92baf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af0150ed65f474417cc023973aa52402.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6437149d065176fa7aca7013baadfed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6592be2ac7852a4a6fb687445344f9b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d57f9bea2051f578e472059d9887a7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-86a497d6dab6790501ab71120aa120b0.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-06-05  Efficient Test-time Adaptive Object Detection via Sensitivity-Guided   Pruning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-dce9c22fa66fd9ddad5d9ddfa9f79e77.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-06-05  EgoVLM Policy Optimization for Egocentric Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
