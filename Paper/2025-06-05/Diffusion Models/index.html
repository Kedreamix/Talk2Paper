<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  EDITOR Effective and Interpretable Prompt Inversion for Text-to-Image   Diffusion Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-4ff2737bf419a0bcca28f1acc438e1a6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    63 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-05-æ›´æ–°"><a href="#2025-06-05-æ›´æ–°" class="headerlink" title="2025-06-05 æ›´æ–°"></a>2025-06-05 æ›´æ–°</h1><h2 id="EDITOR-Effective-and-Interpretable-Prompt-Inversion-for-Text-to-Image-Diffusion-Models"><a href="#EDITOR-Effective-and-Interpretable-Prompt-Inversion-for-Text-to-Image-Diffusion-Models" class="headerlink" title="EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image   Diffusion Models"></a>EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image   Diffusion Models</h2><p><strong>Authors:Mingzhe Li, Gehao Zhang, Zhenting Wang, Shiqing Ma, Siqi Pan, Richard Cartwright, Juan Zhai</strong></p>
<p>Text-to-image generation models~(e.g., Stable Diffusion) have achieved significant advancements, enabling the creation of high-quality and realistic images based on textual descriptions. Prompt inversion, the task of identifying the textual prompt used to generate a specific artifact, holds significant potential for applications including data attribution, model provenance, and watermarking validation. Recent studies introduced a delayed projection scheme to optimize for prompts representative of the vocabulary space, though challenges in semantic fluency and efficiency remain. Advanced image captioning models or visual large language models can generate highly interpretable prompts, but they often lack in image similarity. In this paper, we propose a prompt inversion technique called \sys for text-to-image diffusion models, which includes initializing embeddings using a pre-trained image captioning model, refining them through reverse-engineering in the latent space, and converting them to texts using an embedding-to-text model. Our experiments on the widely-used datasets, such as MS COCO, LAION, and Flickr, show that our method outperforms existing methods in terms of image similarity, textual alignment, prompt interpretability and generalizability. We further illustrate the application of our generated prompts in tasks such as cross-concept image synthesis, concept manipulation, evolutionary multi-concept generation and unsupervised segmentation. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆä¾‹å¦‚Stable Diffusionï¼‰å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°åˆ›å»ºé«˜è´¨é‡å’Œé€¼çœŸçš„å›¾åƒã€‚æç¤ºåè½¬ï¼ˆprompt inversionï¼‰æ˜¯è¯†åˆ«ç”¨äºç”Ÿæˆç‰¹å®šè‰ºæœ¯å“çš„æ–‡æœ¬æç¤ºçš„ä»»åŠ¡ï¼Œå¯¹äºæ•°æ®å½’å±ã€æ¨¡å‹æ¥æºå’Œæ°´å°éªŒè¯ç­‰åº”ç”¨å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚æœ€è¿‘çš„ç ”ç©¶å¼•å…¥äº†ä¸€ç§å»¶è¿ŸæŠ•å½±æ–¹æ¡ˆï¼Œä»¥ä¼˜åŒ–ä»£è¡¨è¯æ±‡ç©ºé—´çš„æç¤ºï¼Œå°½ç®¡åœ¨è¯­ä¹‰æµç•…æ€§å’Œæ•ˆç‡æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚é«˜çº§å›¾åƒæè¿°æ¨¡å‹æˆ–è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜åº¦å¯è§£é‡Šçš„æç¤ºï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹å›¾åƒç›¸ä¼¼æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æç¤ºåè½¬æŠ€æœ¯ï¼Œç§°ä¸º\sysï¼Œå®ƒä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒæè¿°æ¨¡å‹è¿›è¡ŒåµŒå…¥åˆå§‹åŒ–ï¼Œé€šè¿‡åå‘å·¥ç¨‹åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨åµŒå…¥åˆ°æ–‡æœ¬æ¨¡å‹å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬ã€‚æˆ‘ä»¬åœ¨MS COCOã€LAIONå’ŒFlickrç­‰å¸¸ç”¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒç›¸ä¼¼æ€§ã€æ–‡æœ¬å¯¹é½ã€æç¤ºå¯è§£é‡Šæ€§å’Œé€šç”¨æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯´æ˜äº†æ‰€ç”Ÿæˆçš„æç¤ºåœ¨è·¨æ¦‚å¿µå›¾åƒåˆæˆã€æ¦‚å¿µæ“ä½œã€è¿›åŒ–å¤šæ¦‚å¿µç”Ÿæˆå’Œæ— ç›‘ç£åˆ†å‰²ç­‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03067v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸçš„å›¾åƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹çš„æç¤ºåè½¬æŠ€æœ¯ï¼Œé€šè¿‡é¢„è®­ç»ƒå›¾åƒæè¿°æ¨¡å‹è¿›è¡ŒåµŒå…¥åˆå§‹åŒ–ï¼Œåœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œåå‘å·¥ç¨‹ä¼˜åŒ–ï¼Œå¹¶ä½¿ç”¨åµŒå…¥åˆ°æ–‡æœ¬çš„æ¨¡å‹è¿›è¡Œè½¬æ¢ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒç›¸ä¼¼æ€§ã€æ–‡æœ¬å¯¹é½ã€æç¤ºå¯è§£é‡Šæ€§å’Œé€šç”¨æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†åœ¨è·¨æ¦‚å¿µå›¾åƒåˆæˆã€æ¦‚å¿µæ“ä½œã€è¿›åŒ–å¤šæ¦‚å¿µç”Ÿæˆå’Œæ— ç›‘ç£åˆ†å‰²ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹å¦‚Stable Diffusionå·²èƒ½ç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸçš„å›¾åƒã€‚</li>
<li>æç¤ºåè½¬æŠ€æœ¯å¯ç”¨äºç¡®å®šç”Ÿæˆç‰¹å®šå·¥ä»¶æ‰€ä½¿ç”¨çš„æ–‡æœ¬æç¤ºã€‚</li>
<li>ç°æœ‰æŒ‘æˆ˜åŒ…æ‹¬è¯­ä¹‰æµç•…æ€§å’Œæ•ˆç‡é—®é¢˜ã€‚</li>
<li>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºåè½¬æŠ€æœ¯\sysï¼Œç”¨äºæ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>\sysæŠ€æœ¯åŒ…æ‹¬ä½¿ç”¨é¢„è®­ç»ƒå›¾åƒæè¿°æ¨¡å‹è¿›è¡ŒåµŒå…¥åˆå§‹åŒ–ï¼Œæ½œåœ¨ç©ºé—´åå‘å·¥ç¨‹ä¼˜åŒ–ï¼Œä»¥åŠä½¿ç”¨åµŒå…¥åˆ°æ–‡æœ¬çš„æ¨¡å‹è¿›è¡Œè½¬æ¢ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒç›¸ä¼¼æ€§ã€æ–‡æœ¬å¯¹é½ã€æç¤ºå¯è§£é‡Šæ€§å’Œé€šç”¨æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43dc2320dc52eb70ed687f21b5cb1450.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b59f01fb34c917f32a5c52ebd2692263.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93bb3495cff649f1f893d5094e248419.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56b0fbec6332768e99af37f3f9242fb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f77bf345060414c540acea76b26ec5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c9f84927f75a818d489330c415f2f06.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Astrophotography-turbulence-mitigation-via-generative-models"><a href="#Astrophotography-turbulence-mitigation-via-generative-models" class="headerlink" title="Astrophotography turbulence mitigation via generative models"></a>Astrophotography turbulence mitigation via generative models</h2><p><strong>Authors:Joonyeoup Kim, Yu Yuan, Xingguang Zhang, Xijun Wang, Stanley Chan</strong></p>
<p>Photography is the cornerstone of modern astronomical and space research. However, most astronomical images captured by ground-based telescopes suffer from atmospheric turbulence, resulting in degraded imaging quality. While multi-frame strategies like lucky imaging can mitigate some effects, they involve intensive data acquisition and complex manual processing. In this paper, we propose AstroDiff, a generative restoration method that leverages both the high-quality generative priors and restoration capabilities of diffusion models to mitigate atmospheric turbulence. Extensive experiments demonstrate that AstroDiff outperforms existing state-of-the-art learning-based methods in astronomical image turbulence mitigation, providing higher perceptual quality and better structural fidelity under severe turbulence conditions. Our code and additional results are available at <a target="_blank" rel="noopener" href="https://web-six-kappa-66.vercel.app/">https://web-six-kappa-66.vercel.app/</a> </p>
<blockquote>
<p>æ‘„å½±æ˜¯ç°ä»£å¤©æ–‡å­¦å’Œç©ºé—´ç ”ç©¶çš„æ ¸å¿ƒã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç”±åœ°é¢æœ›è¿œé•œæ‹æ‘„çš„å¤©æ–‡å›¾åƒå—åˆ°å¤§æ°”æ¹æµçš„å½±å“ï¼Œå¯¼è‡´æˆåƒè´¨é‡ä¸‹é™ã€‚è™½ç„¶åƒå¹¸è¿æˆåƒè¿™æ ·çš„å¤šå¸§ç­–ç•¥å¯ä»¥å‡è½»ä¸€äº›å½±å“ï¼Œä½†å®ƒä»¬æ¶‰åŠå¤§é‡çš„æ•°æ®è·å–å’Œå¤æ‚çš„æ‰‹åŠ¨å¤„ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AstroDiffï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é«˜è´¨ç”Ÿæˆä¼˜å…ˆæƒå’Œæ¢å¤èƒ½åŠ›æ¥å‡è½»å¤§æ°”æ¹æµå½±å“çš„ç”Ÿæˆæ¢å¤æ–¹æ³•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAstroDiffåœ¨å¤©æ–‡å›¾åƒæ¹æµæŠ‘åˆ¶æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„å­¦ä¹ å‹æ–¹æ³•ï¼Œåœ¨ä¸¥é‡çš„æ¹æµæ¡ä»¶ä¸‹æä¾›äº†æ›´é«˜çš„æ„ŸçŸ¥è´¨é‡å’Œæ›´å¥½çš„ç»“æ„ä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„ä»£ç å’Œé¢å¤–ç»“æœå¯åœ¨[<a target="_blank" rel="noopener" href="https://web-six-kappa-6/">https://web-six-kappa-6</a> 6.vercel.app&#x2F;]æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02981v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤©æ–‡å­¦ç ”ç©¶ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ°é¢æœ›è¿œé•œæ‹æ‘„çš„å¤©æ–‡å›¾åƒå—åˆ°å¤§æ°”æ‰°åŠ¨çš„å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ¢å¤æ–¹æ³•â€”â€”AstroDiffã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é«˜è´¨é‡å’Œæ¢å¤èƒ½åŠ›ï¼Œæœ‰æ•ˆå‡è½»å¤§æ°”æ‰°åŠ¨çš„å½±å“ã€‚å®éªŒè¯æ˜ï¼ŒAstroDiffåœ¨å¤©æ–‡å›¾åƒå»å™ªé¢†åŸŸä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸¥é‡æ‰°åŠ¨æ¡ä»¶ä¸‹ï¼Œå…¶æ„ŸçŸ¥è´¨é‡æ›´é«˜ï¼Œç»“æ„ä¿çœŸåº¦æ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤©æ–‡å­¦ç ”ç©¶ä¸­ï¼Œåœ°é¢æœ›è¿œé•œæ‹æ‘„çš„å¤©æ–‡å›¾åƒå—åˆ°å¤§æ°”æ‰°åŠ¨çš„å½±å“ã€‚</li>
<li>å¤šå¸§æˆåƒç­–ç•¥å¦‚å¹¸è¿æˆåƒå¯ä»¥éƒ¨åˆ†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†éœ€è¦å¤§é‡æ•°æ®è·å–å’Œå¤æ‚çš„åæœŸå¤„ç†ã€‚</li>
<li>AstroDiffæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ¢å¤æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§æ°”æ‰°åŠ¨é—®é¢˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å…·æœ‰é«˜è´¨é‡å’Œæ¢å¤èƒ½åŠ›ï¼Œæœ‰åŠ©äºæ”¹å–„å¤©æ–‡å›¾åƒçš„è´¨é‡ã€‚</li>
<li>AstroDiffåœ¨å¤©æ–‡å›¾åƒå»å™ªé¢†åŸŸå®éªŒè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰æ›´é«˜çš„æ„ŸçŸ¥è´¨é‡å’Œç»“æ„ä¿çœŸåº¦ã€‚</li>
<li>è¯¥æ–¹æ³•çš„ä»£ç å’Œæ›´å¤šå®éªŒç»“æœå¯ä»¥åœ¨ç›¸å…³ç½‘ç«™æ‰¾åˆ°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-359dfcdba556cf5f7c616f3f2956fcec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f84fc12a489cfd6b6050d851445315ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8da8aaffde8f2ef36565ed406dac96f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11e7ff578cf7386e8e4347a76288baa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bc6ece091c224f149427cbc4761a35f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-844507cd6f4e797cae8b52ce77787708.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3634d5c22593914529c42dfde928653c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c56b1ff8feaaa6f9e9d484977242286b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d76be0785a5f13f6ff78ede90d2d2ba.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Smoothed-Preference-Optimization-via-ReNoise-Inversion-for-Aligning-Diffusion-Models-with-Varied-Human-Preferences"><a href="#Smoothed-Preference-Optimization-via-ReNoise-Inversion-for-Aligning-Diffusion-Models-with-Varied-Human-Preferences" class="headerlink" title="Smoothed Preference Optimization via ReNoise Inversion for Aligning   Diffusion Models with Varied Human Preferences"></a>Smoothed Preference Optimization via ReNoise Inversion for Aligning   Diffusion Models with Varied Human Preferences</h2><p><strong>Authors:Yunhong Lu, Qichao Wang, Hengyuan Cao, Xiaoyin Xu, Min Zhang</strong></p>
<p>Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation models with human preferences using pairwise preference data. Although substantial resources are expended in collecting and labeling datasets, a critical aspect is often neglected: \textit{preferences vary across individuals and should be represented with more granularity.} To address this, we propose SmPO-Diffusion, a novel method for modeling preference distributions to improve the DPO objective, along with a numerical upper bound estimation for the diffusion optimization objective. First, we introduce a smoothed preference distribution to replace the original binary distribution. We employ a reward model to simulate human preferences and apply preference likelihood averaging to improve the DPO loss, such that the loss function approaches zero when preferences are similar. Furthermore, we utilize an inversion technique to simulate the trajectory preference distribution of the diffusion model, enabling more accurate alignment with the optimization objective. Our approach effectively mitigates issues of excessive optimization and objective misalignment present in existing methods through straightforward modifications. Our SmPO-Diffusion achieves state-of-the-art performance in preference evaluation, outperforming baselines across metrics with lower training costs. The project page is <a target="_blank" rel="noopener" href="https://jaydenlyh.github.io/SmPO-project-page/">https://jaydenlyh.github.io/SmPO-project-page/</a>. </p>
<blockquote>
<p>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½¿ç”¨æˆå¯¹åå¥½æ•°æ®å°†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ã€‚è™½ç„¶æ”¶é›†å’Œæ ‡æ³¨æ•°æ®é›†éœ€è¦è€—è´¹å¤§é‡èµ„æºï¼Œä½†ä¸€ä¸ªå…³é”®æ–¹é¢å¾€å¾€è¢«å¿½è§†ï¼š*ä¸åŒä¸ªä½“çš„åå¥½å„ä¸ç›¸åŒï¼Œåº”ä»¥æ›´ç²¾ç»†çš„æ–¹å¼è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SmPO-Diffusionï¼Œè¿™æ˜¯ä¸€ç§å»ºæ¨¡åå¥½åˆ†å¸ƒä»¥æ”¹è¿›DPOç›®æ ‡çš„æ–°æ–¹æ³•ï¼Œä»¥åŠä¸€ç§å¯¹æ‰©æ•£ä¼˜åŒ–ç›®æ ‡çš„æ•°å€¼ä¸Šé™ä¼°è®¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥å¹³æ»‘çš„åå¥½åˆ†å¸ƒæ¥æ›¿æ¢åŸå§‹çš„äºŒè¿›åˆ†å¸ƒã€‚æˆ‘ä»¬ä½¿ç”¨å¥–åŠ±æ¨¡å‹æ¥æ¨¡æ‹Ÿäººç±»åå¥½ï¼Œå¹¶åº”ç”¨åå¥½å¯èƒ½æ€§å¹³å‡æ³•æ¥æ”¹è¿›DPOæŸå¤±ï¼Œä»è€Œä½¿æŸå¤±å‡½æ•°åœ¨åå¥½ç›¸ä¼¼æ—¶æ¥è¿‘é›¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿ç”¨åæ¼”æŠ€æœ¯æ¨¡æ‹Ÿæ‰©æ•£æ¨¡å‹çš„è½¨è¿¹åå¥½åˆ†å¸ƒï¼Œä½¿ä¸ä¼˜åŒ–ç›®æ ‡çš„å¯¹é½æ›´åŠ å‡†ç¡®ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç®€å•çš„ä¿®æ”¹æœ‰æ•ˆåœ°ç¼“è§£äº†ç°æœ‰æ–¹æ³•ä¸­è¿‡åº¦ä¼˜åŒ–å’Œç›®æ ‡ä¸å¯¹é½çš„é—®é¢˜ã€‚SmPO-Diffusionåœ¨åå¥½è¯„ä¼°æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æŒ‡æ ‡ä¸Šä¼˜äºåŸºçº¿ï¼Œå¹¶ä¸”é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://jaydenlyh.github.io/SmPO-project-page/%E3%80%82">https://jaydenlyh.github.io/SmPO-project-page/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02698v1">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Direct Preference Optimizationï¼ˆDPOï¼‰åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•ä¸­çš„ä¸è¶³ã€‚ä¸ºäº†æ”¹è¿›DPOï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•SmPO-Diffusionï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»åå¥½åˆ†å¸ƒæ¥ä¼˜åŒ–DPOç›®æ ‡ï¼Œå¹¶å¼•å…¥äº†å¹³æ»‘åå¥½åˆ†å¸ƒå’Œå¥–åŠ±æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜åˆ©ç”¨åæ¼”æŠ€æœ¯æ¨¡æ‹Ÿæ‰©æ•£æ¨¡å‹çš„è½¨è¿¹åå¥½åˆ†å¸ƒï¼Œæé«˜äº†ä¼˜åŒ–ç›®æ ‡çš„å‡†ç¡®æ€§ã€‚SmPO-Diffusionåœ¨åå¥½è¯„ä¼°æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»¥æ›´ä½çš„è®­ç»ƒæˆæœ¬è¶…è¶Šäº†åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Direct Preference Optimization (DPO) ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­ï¼Œåˆ©ç”¨æˆå¯¹åå¥½æ•°æ®å¯¹é½äººç±»åå¥½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½è§†äº†ä¸ªäººåå¥½çš„å¤šæ ·æ€§ï¼Œåº”è¯¥æ›´ç»†è‡´åœ°è¡¨ç¤ºè¿™äº›åå¥½ã€‚</li>
<li>SmPO-Diffusionæ–¹æ³•å¼•å…¥å¹³æ»‘åå¥½åˆ†å¸ƒæ¥æ”¹è¿›DPOç›®æ ‡ï¼Œå¹¶æ¨¡æ‹Ÿäººç±»åå¥½ã€‚</li>
<li>åˆ©ç”¨å¥–åŠ±æ¨¡å‹å’Œæ”¹è¿›çš„DPOæŸå¤±æ¥æé«˜æ€§èƒ½ï¼Œå½“åå¥½ç›¸ä¼¼æ—¶æŸå¤±å‡½æ•°æ¥è¿‘é›¶ã€‚</li>
<li>ä½¿ç”¨åæ¼”æŠ€æœ¯æ¨¡æ‹Ÿæ‰©æ•£æ¨¡å‹çš„è½¨è¿¹åå¥½åˆ†å¸ƒï¼Œæé«˜ä¸ä¼˜åŒ–ç›®æ ‡çš„å¯¹é½ç²¾åº¦ã€‚</li>
<li>SmPO-Diffusionæœ‰æ•ˆç¼“è§£äº†ç°æœ‰æ–¹æ³•ä¸­çš„è¿‡åº¦ä¼˜åŒ–å’Œç›®æ ‡ä¸å¯¹é½é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-014b9acaad8b30e6a2643f7ad18eb389.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b03e0f1affb96ca12192b1373cd69733.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fa2116ae724c8ef509c8648351861e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdef7621a4b0601a72c49b8fe875c298.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DCI-Dual-Conditional-Inversion-for-Boosting-Diffusion-Based-Image-Editing"><a href="#DCI-Dual-Conditional-Inversion-for-Boosting-Diffusion-Based-Image-Editing" class="headerlink" title="DCI: Dual-Conditional Inversion for Boosting Diffusion-Based Image   Editing"></a>DCI: Dual-Conditional Inversion for Boosting Diffusion-Based Image   Editing</h2><p><strong>Authors:Zixiang Li, Haoyu Wang, Wei Wang, Chuangchuang Tan, Yunchao Wei, Yao Zhao</strong></p>
<p>Diffusion models have achieved remarkable success in image generation and editing tasks. Inversion within these models aims to recover the latent noise representation for a real or generated image, enabling reconstruction, editing, and other downstream tasks. However, to date, most inversion approaches suffer from an intrinsic trade-off between reconstruction accuracy and editing flexibility. This limitation arises from the difficulty of maintaining both semantic alignment and structural consistency during the inversion process. In this work, we introduce Dual-Conditional Inversion (DCI), a novel framework that jointly conditions on the source prompt and reference image to guide the inversion process. Specifically, DCI formulates the inversion process as a dual-condition fixed-point optimization problem, minimizing both the latent noise gap and the reconstruction error under the joint guidance. This design anchors the inversion trajectory in both semantic and visual space, leading to more accurate and editable latent representations. Our novel setup brings new understanding to the inversion process. Extensive experiments demonstrate that DCI achieves state-of-the-art performance across multiple editing tasks, significantly improving both reconstruction quality and editing precision. Furthermore, we also demonstrate that our method achieves strong results in reconstruction tasks, implying a degree of robustness and generalizability approaching the ultimate goal of the inversion process. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚è¿™äº›æ¨¡å‹ä¸­çš„åæ¼”æ—¨åœ¨æ¢å¤çœŸå®æˆ–ç”Ÿæˆå›¾åƒçš„æ½œåœ¨å™ªå£°è¡¨ç¤ºï¼Œä»è€Œå®ç°é‡å»ºã€ç¼–è¾‘å’Œå…¶ä»–ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿„ä»Šä¸ºæ­¢ï¼Œå¤§å¤šæ•°åæ¼”æ–¹æ³•é¢ä¸´ç€é‡å»ºç²¾åº¦å’Œç¼–è¾‘çµæ´»æ€§ä¹‹é—´çš„å†…åœ¨æƒè¡¡ã€‚è¿™ä¸€å±€é™æ€§æºäºåœ¨åæ¼”è¿‡ç¨‹ä¸­ä¿æŒè¯­ä¹‰å¯¹é½å’Œç»“æ„æ€§ä¸€è‡´æ€§çš„å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Dual-Conditional Inversionï¼ˆDCIï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒè”åˆåŸºäºæºæç¤ºå’Œå‚è€ƒå›¾åƒè¿›è¡Œæ¡ä»¶è®¾å®šï¼Œä»¥æŒ‡å¯¼åæ¼”è¿‡ç¨‹ã€‚å…·ä½“è€Œè¨€ï¼ŒDCIå°†åæ¼”è¿‡ç¨‹åˆ¶å®šä¸ºä¸€ä¸ªåŒæ¡ä»¶å›ºå®šç‚¹ä¼˜åŒ–é—®é¢˜ï¼Œåœ¨è”åˆæŒ‡å¯¼ä¸‹æœ€å°åŒ–æ½œåœ¨å™ªå£°å·®è·å’Œé‡å»ºè¯¯å·®ã€‚è¿™ç§è®¾è®¡åœ¨è¯­ä¹‰å’Œè§†è§‰ç©ºé—´ä¸Šé”šå®šäº†åæ¼”è½¨è¿¹ï¼Œä»è€Œå¾—åˆ°æ›´å‡†ç¡®å’Œå¯ç¼–è¾‘çš„æ½œåœ¨è¡¨ç¤ºã€‚æˆ‘ä»¬æ–°é¢–çš„è®¾ç½®å¸¦æ¥äº†å¯¹åæ¼”è¿‡ç¨‹çš„æ–°ç†è§£ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDCIåœ¨å¤šä¸ªç¼–è¾‘ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†é‡å»ºè´¨é‡å’Œç¼–è¾‘ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡å»ºä»»åŠ¡ä¸­å–å¾—äº†å¼ºå¤§çš„ç»“æœï¼Œè¿™æš—ç¤ºäº†åæ¼”è¿‡ç¨‹çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§æ¥è¿‘æœ€ç»ˆç›®æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02560v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”åŒé‡æ¡ä»¶åè½¬ï¼ˆDCIï¼‰ï¼Œé€šè¿‡åŒæ—¶è€ƒè™‘æºæç¤ºå’Œå‚è€ƒå›¾åƒæ¥æŒ‡å¯¼åè½¬è¿‡ç¨‹ï¼Œä»è€Œè§£å†³äº†ä»¥å¾€åè½¬æ–¹æ³•å­˜åœ¨çš„é‡å»ºç²¾åº¦ä¸ç¼–è¾‘çµæ´»æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚DCIå°†åè½¬è¿‡ç¨‹åˆ¶å®šä¸ºåŒé‡æ¡ä»¶ä¸‹çš„å›ºå®šç‚¹ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡æœ€å°åŒ–æ½œåœ¨å™ªå£°å·®è·å’Œé‡å»ºè¯¯å·®ï¼Œä½¿åè½¬è½¨è¿¹åœ¨è¯­ä¹‰å’Œè§†è§‰ç©ºé—´ä¸­å¾—ä»¥å›ºå®šï¼Œä»è€Œè·å¾—æ›´å‡†ç¡®ä¸”å¯ç¼–è¾‘çš„æ½œåœ¨è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒDCIåœ¨å¤šä¸ªç¼–è¾‘ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†é‡å»ºè´¨é‡å’Œç¼–è¾‘ç²¾åº¦ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨é‡å»ºä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„ç»“æœï¼Œå±•ç°å‡ºå…¶ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>ç°æœ‰åè½¬æ–¹æ³•é¢ä¸´é‡å»ºç²¾åº¦ä¸ç¼–è¾‘çµæ´»æ€§ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>DCIï¼ˆåŒé‡æ¡ä»¶åè½¬ï¼‰æ–¹æ³•æå‡ºï¼Œé€šè¿‡åŒæ—¶è€ƒè™‘æºæç¤ºå’Œå‚è€ƒå›¾åƒæ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>DCIå°†åè½¬è¿‡ç¨‹è§†ä¸ºåŒé‡æ¡ä»¶ä¸‹çš„å›ºå®šç‚¹ä¼˜åŒ–é—®é¢˜ï¼Œæœ€å°åŒ–æ½œåœ¨å™ªå£°å·®è·å’Œé‡å»ºè¯¯å·®ã€‚</li>
<li>DCIä½¿åè½¬è½¨è¿¹åœ¨è¯­ä¹‰å’Œè§†è§‰ç©ºé—´ä¸­å›ºå®šï¼Œäº§ç”Ÿæ›´å‡†ç¡®ä¸”å¯ç¼–è¾‘çš„æ½œåœ¨è¡¨ç¤ºã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDCIåœ¨å¤šä¸ªç¼–è¾‘ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜é‡å»ºè´¨é‡å’Œç¼–è¾‘ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-046580ea9a35fbd3db79d0cd2694381c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20a7241306873663e9f6eb2b335115ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53fe6cf551103db3787cd849ffd319a8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Guiding-Registration-with-Emergent-Similarity-from-Pre-Trained-Diffusion-Models"><a href="#Guiding-Registration-with-Emergent-Similarity-from-Pre-Trained-Diffusion-Models" class="headerlink" title="Guiding Registration with Emergent Similarity from Pre-Trained Diffusion   Models"></a>Guiding Registration with Emergent Similarity from Pre-Trained Diffusion   Models</h2><p><strong>Authors:Nurislam Tursynbek, Hastings Greer, Basar Demir, Marc Niethammer</strong></p>
<p>Diffusion models, while trained for image generation, have emerged as powerful foundational feature extractors for downstream tasks. We find that off-the-shelf diffusion models, trained exclusively to generate natural RGB images, can identify semantically meaningful correspondences in medical images. Building on this observation, we propose to leverage diffusion model features as a similarity measure to guide deformable image registration networks. We show that common intensity-based similarity losses often fail in challenging scenarios, such as when certain anatomies are visible in one image but absent in another, leading to anatomically inaccurate alignments. In contrast, our method identifies true semantic correspondences, aligning meaningful structures while disregarding those not present across images. We demonstrate superior performance of our approach on two tasks: multimodal 2D registration (DXA to X-Ray) and monomodal 3D registration (brain-extracted to non-brain-extracted MRI). Code: <a target="_blank" rel="noopener" href="https://github.com/uncbiag/dgir">https://github.com/uncbiag/dgir</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹è™½ç„¶è¢«è®­ç»ƒç”¨äºå›¾åƒç”Ÿæˆï¼Œä½†å·²ä½œä¸ºä¸‹æ¸¸ä»»åŠ¡çš„å¼ºå¤§åŸºç¡€ç‰¹å¾æå–å™¨è€Œå‡ºç°ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸“ä¸ºç”Ÿæˆè‡ªç„¶RGBå›¾åƒè€Œè®­ç»ƒçš„å³ç”¨å‹æ‰©æ•£æ¨¡å‹å¯ä»¥è¯†åˆ«åŒ»å­¦å›¾åƒä¸­çš„è¯­ä¹‰å¯¹åº”ç‰©ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç‰¹å¾ä½œä¸ºç›¸ä¼¼æ€§åº¦é‡æ¥æŒ‡å¯¼å¯å˜å½¢å›¾åƒé…å‡†ç½‘ç»œã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒåŸºäºå¼ºåº¦çš„å¸¸è§ç›¸ä¼¼æ€§æŸå¤±åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å¸¸å¸¸å¤±è´¥ï¼Œä¾‹å¦‚åœ¨ä¸€å¼ å›¾åƒä¸­å¯è§æŸäº›è§£å‰–ç»“æ„è€Œåœ¨å¦ä¸€å¼ å›¾åƒä¸­ç¼ºå¤±æ—¶ï¼Œä¼šå¯¼è‡´è§£å‰–ç»“æ„ä¸å‡†ç¡®çš„å¯¹é½ã€‚ä¸ä¹‹ç›¸åï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«çœŸæ­£çš„è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œå¯¹é½æœ‰æ„ä¹‰çš„ç»“æ„ï¼ŒåŒæ—¶å¿½ç•¥é‚£äº›åœ¨ä¸åŒå›¾åƒä¸­ä¸å­˜åœ¨çš„éƒ¨åˆ†ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼šå¤šæ¨¡æ€äºŒç»´é…å‡†ï¼ˆDXAåˆ°Xå°„çº¿ï¼‰å’Œå•æ¨¡æ€ä¸‰ç»´é…å‡†ï¼ˆæå–å¤§è„‘ä¸éæå–å¤§è„‘çš„MRIï¼‰ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/uncbiag/dgir">https://github.com/uncbiag/dgir</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02419v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹æœ€åˆæ˜¯ä¸ºå›¾åƒç”Ÿæˆè€Œè®¾è®¡çš„ï¼Œä½†ç°åœ¨å·²æˆä¸ºä¸‹æ¸¸ä»»åŠ¡çš„åŸºç¡€ç‰¹å¾æå–å™¨ã€‚ç ”ç©¶å‘ç°ï¼Œç°æˆçš„æ‰©æ•£æ¨¡å‹ï¼Œåœ¨ä»…ç”¨äºç”Ÿæˆè‡ªç„¶RGBå›¾åƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿåœ¨åŒ»å­¦å›¾åƒä¸­è¯†åˆ«å‡ºè¯­ä¹‰ä¸Šå¯¹åº”çš„å†…å®¹ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬æè®®åˆ©ç”¨æ‰©æ•£æ¨¡å‹ç‰¹å¾ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡ï¼Œä»¥æŒ‡å¯¼å¯å˜å½¢å›¾åƒé…å‡†ç½‘ç»œã€‚å¸¸è§çš„åŸºäºå¼ºåº¦çš„ç›¸ä¼¼åº¦æŸå¤±åœ¨ç‰¹å®šåœºæ™¯ä¸‹ä¼šå¤±æ•ˆï¼Œå¦‚æŸäº›è§£å‰–ç»“æ„åœ¨ä¸€å¹…å›¾åƒä¸­å¯è§è€Œåœ¨å¦ä¸€å¹…ä¸­ç¼ºå¤±æ—¶ï¼Œä¼šå¯¼è‡´è§£å‰–ç»“æ„å¯¹åº”ä¸å‡†ç¡®ã€‚ä¸ä¹‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«çœŸæ­£çš„è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œå¯¹é½æœ‰æ„ä¹‰çš„ç»“æ„ï¼ŒåŒæ—¶å¿½ç•¥é‚£äº›åœ¨ä¸åŒå›¾åƒä¸­ä¸å­˜åœ¨çš„éƒ¨åˆ†ã€‚æˆ‘ä»¬åœ¨ä¸¤ç§ä»»åŠ¡ä¸Šå±•ç¤ºäº†å…¶å“è¶Šæ€§èƒ½ï¼šå¤šæ¨¡æ€çš„äºŒç»´é…å‡†ï¼ˆDXAåˆ°Xå…‰ï¼‰å’Œå•æ¨¡æ€çš„ä¸‰ç»´é…å‡†ï¼ˆæå–å¤§è„‘ä¸æœªæå–å¤§è„‘çš„MRIï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²è¢«å‘ç°å¯ä»¥ä½œä¸ºå¼ºå¤§çš„ç‰¹å¾æå–å™¨ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œä¸ä»…åœ¨å›¾åƒç”Ÿæˆä¸Šæœ‰ä¼˜åŠ¿ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿåœ¨åŒ»å­¦å›¾åƒä¸­è¯†åˆ«è¯­ä¹‰ä¸Šçš„å¯¹åº”å…³ç³»ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç‰¹å¾è¢«æè®®ç”¨ä½œç›¸ä¼¼åº¦åº¦é‡ï¼Œä»¥æ”¹è¿›å¯å˜å½¢å›¾åƒé…å‡†ç½‘ç»œã€‚</li>
<li>åœ¨ç‰¹å®šåœºæ™¯ä¸‹ï¼Œå¸¸è§çš„åŸºäºå¼ºåº¦çš„ç›¸ä¼¼åº¦æŸå¤±ä¼šå¤±æ•ˆï¼Œå¯¼è‡´è§£å‰–ç»“æ„å¯¹åº”ä¸å‡†ç¡®ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•èƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«å¹¶å¯¹é½æœ‰æ„ä¹‰çš„ç»“æ„ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸¤ç§ä¸åŒç±»å‹çš„å›¾åƒé…å‡†ä»»åŠ¡ä¸Šå±•ç¤ºäº†å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c506878063bf7765fd73d4785bff5520.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8ce72588d2db8d36046920d9f1061af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c011f0d2c878cb79bfdf55991d82156.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec558a6b36e6147460ad72aaba33adbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0593e30b441a059c31c1ff1065ada802.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="IMAGHarmony-Controllable-Image-Editing-with-Consistent-Object-Quantity-and-Layout"><a href="#IMAGHarmony-Controllable-Image-Editing-with-Consistent-Object-Quantity-and-Layout" class="headerlink" title="IMAGHarmony: Controllable Image Editing with Consistent Object Quantity   and Layout"></a>IMAGHarmony: Controllable Image Editing with Consistent Object Quantity   and Layout</h2><p><strong>Authors:Fei Shen, Xiaoyu Du, Yutong Gao, Jian Yu, Yushe Cao, Xing Lei, Jinhui Tang</strong></p>
<p>Recent diffusion models have advanced image editing by enhancing visual quality and control, supporting broad applications across creative and personalized domains. However, current image editing largely overlooks multi-object scenarios, where precise control over object categories, counts, and spatial layouts remains a significant challenge. To address this, we introduce a new task, quantity-and-layout consistent image editing (QL-Edit), which aims to enable fine-grained control of object quantity and spatial structure in complex scenes. We further propose IMAGHarmony, a structure-aware framework that incorporates harmony-aware attention (HA) to integrate multimodal semantics, explicitly modeling object counts and layouts to enhance editing accuracy and structural consistency. In addition, we observe that diffusion models are susceptible to initial noise and exhibit strong preferences for specific noise patterns. Motivated by this, we present a preference-guided noise selection (PNS) strategy that chooses semantically aligned initial noise samples based on vision-language matching, thereby improving generation stability and layout consistency in multi-object editing. To support evaluation, we construct HarmonyBench, a comprehensive benchmark covering diverse quantity and layout control scenarios. Extensive experiments demonstrate that IMAGHarmony consistently outperforms state-of-the-art methods in structural alignment and semantic accuracy. The code and model are available at <a target="_blank" rel="noopener" href="https://github.com/muzishen/IMAGHarmony">https://github.com/muzishen/IMAGHarmony</a>. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹é€šè¿‡æé«˜è§†è§‰è´¨é‡å’Œæ§åˆ¶åŠ›ï¼Œæ¨åŠ¨äº†å›¾åƒç¼–è¾‘çš„è¿›å±•ï¼Œå¹¶å¹¿æ³›æ”¯æŒåˆ›æ„å’Œä¸ªæ€§åŒ–é¢†åŸŸçš„åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰å›¾åƒç¼–è¾‘å¤§å¤šå¿½è§†äº†å¤šç›®æ ‡åœºæ™¯ï¼Œå¯¹ç›®æ ‡ç±»åˆ«ã€æ•°é‡å’Œç©ºé—´å¸ƒå±€çš„ç²¾ç»†æ§åˆ¶ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹æ–°ä»»åŠ¡â€”â€”æ•°é‡ä¸å¸ƒå±€ä¸€è‡´å›¾åƒç¼–è¾‘ï¼ˆQL-Editï¼‰ï¼Œæ—¨åœ¨å®ç°å¯¹å¤æ‚åœºæ™¯ä¸­ç›®æ ‡æ•°é‡å’Œç©ºé—´ç»“æ„çš„ç²¾ç»†æ§åˆ¶ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†IMAGHarmonyï¼Œè¿™æ˜¯ä¸€ä¸ªç»“æ„æ„ŸçŸ¥çš„æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å’Œè°æ„ŸçŸ¥æ³¨æ„åŠ›ï¼ˆHAï¼‰æ¥æ•´åˆå¤šæ¨¡å¼è¯­ä¹‰ï¼Œæ˜¾å¼åœ°å»ºæ¨¡ç›®æ ‡æ•°é‡å’Œå¸ƒå±€ï¼Œä»¥æé«˜ç¼–è¾‘ç²¾åº¦å’Œç»“æ„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ‰©æ•£æ¨¡å‹å¯¹åˆå§‹å™ªå£°æ•æ„Ÿï¼Œå¹¶è¡¨ç°å‡ºå¯¹ç‰¹å®šå™ªå£°æ¨¡å¼çš„å¼ºçƒˆåå¥½ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åå¥½å¼•å¯¼å™ªå£°é€‰æ‹©ï¼ˆPNSï¼‰ç­–ç•¥ï¼Œå®ƒåŸºäºè§†è§‰è¯­è¨€åŒ¹é…é€‰æ‹©è¯­ä¹‰å¯¹é½çš„åˆå§‹å™ªå£°æ ·æœ¬ï¼Œä»è€Œæé«˜å¤šç›®æ ‡ç¼–è¾‘ä¸­çš„ç”Ÿæˆç¨³å®šæ€§å’Œå¸ƒå±€ä¸€è‡´æ€§ã€‚ä¸ºäº†æ”¯æŒè¯„ä¼°ï¼Œæˆ‘ä»¬æ„å»ºäº†HarmonyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å¤šç§æ•°é‡å’Œå¸ƒå±€æ§åˆ¶åœºæ™¯çš„å…¨é¢åŸºå‡†æµ‹è¯•é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒIMAGHarmonyåœ¨ç»“æ„å¯¹é½å’Œè¯­ä¹‰å‡†ç¡®æ€§æ–¹é¢å§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/muzishen/IMAGHarmony%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/muzishen/IMAGHarmonyæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01949v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘é¢†åŸŸå–å¾—è¿›å±•ï¼Œæ”¯æŒåˆ›æ„å’Œä¸ªäººåŒ–åº”ç”¨çš„å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œå¤šç›®æ ‡åœºæ™¯ä¸‹çš„å›¾åƒç¼–è¾‘ä»é¢ä¸´ç²¾ç¡®æ§åˆ¶ç›®æ ‡ç±»åˆ«ã€æ•°é‡å’Œç©ºé—´å¸ƒå±€çš„éš¾é¢˜ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥QL-Editä»»åŠ¡ï¼Œæ—¨åœ¨å®ç°å¤æ‚åœºæ™¯ä¸­ç›®æ ‡æ•°é‡å’Œç©ºé—´ç»“æ„çš„ç²¾ç»†æ§åˆ¶ã€‚æå‡ºIMAGHarmonyç»“æ„æ„ŸçŸ¥æ¡†æ¶ï¼Œç»“åˆå’Œè°æ„ŸçŸ¥æ³¨æ„åŠ›ï¼ˆHAï¼‰æ•´åˆå¤šæ¨¡å¼è¯­ä¹‰ï¼Œæ˜¾å¼å»ºæ¨¡ç›®æ ‡æ•°é‡å’Œå¸ƒå±€æé«˜ç¼–è¾‘ç²¾åº¦å’Œç»“æ„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œå‘ç°æ‰©æ•£æ¨¡å‹æ˜“å—åˆå§‹å™ªå£°å½±å“å¹¶åå¥½ç‰¹å®šå™ªå£°æ¨¡å¼ï¼Œå› æ­¤æå‡ºåŸºäºåå¥½å¼•å¯¼çš„å™ªå£°é€‰æ‹©ï¼ˆPNSï¼‰ç­–ç•¥ï¼Œé€šè¿‡è§†è§‰è¯­è¨€åŒ¹é…é€‰æ‹©è¯­ä¹‰å¯¹é½çš„åˆå§‹å™ªå£°æ ·æœ¬ï¼Œæé«˜å¤šç›®æ ‡ç¼–è¾‘ä¸­çš„ç”Ÿæˆç¨³å®šæ€§å’Œå¸ƒå±€ä¸€è‡´æ€§ã€‚å»ºç«‹HarmonyBenchç»¼åˆåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¶µç›–å¤šç§æ•°é‡å’Œå¸ƒå±€æ§åˆ¶åœºæ™¯ã€‚å®éªŒè¡¨æ˜ï¼ŒIMAGHarmonyåœ¨ç»“æ„å¯¹é½å’Œè¯­ä¹‰å‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä¸­å–å¾—è¿›å±•ï¼Œæ”¯æŒåˆ›æ„å’Œä¸ªäººåŒ–åº”ç”¨ã€‚</li>
<li>å½“å‰å›¾åƒç¼–è¾‘åœ¨å¤šç›®æ ‡åœºæ™¯çš„æ§åˆ¶ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥QL-Editä»»åŠ¡ä»¥å®ç°ç›®æ ‡æ•°é‡å’Œç©ºé—´ç»“æ„çš„ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>æå‡ºIMAGHarmonyæ¡†æ¶ï¼Œç»“åˆå’Œè°æ„ŸçŸ¥æ³¨æ„åŠ›æ•´åˆå¤šæ¨¡å¼è¯­ä¹‰ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ˜“å—åˆå§‹å™ªå£°å½±å“å¹¶å…·æœ‰å™ªå£°æ¨¡å¼åå¥½ã€‚</li>
<li>æå‡ºPNSç­–ç•¥ï¼Œé€šè¿‡è§†è§‰è¯­è¨€åŒ¹é…é€‰æ‹©è¯­ä¹‰å¯¹é½çš„åˆå§‹å™ªå£°æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b77d25c11283c8240203b72a7da7ffbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b95097c6d234d19e7172921057a7184.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6220214695fa89e8288ba479e888983f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-805032cb3b170acb19c6bacfbb9b9aad.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Interpreting-Large-Text-to-Image-Diffusion-Models-with-Dictionary-Learning"><a href="#Interpreting-Large-Text-to-Image-Diffusion-Models-with-Dictionary-Learning" class="headerlink" title="Interpreting Large Text-to-Image Diffusion Models with Dictionary   Learning"></a>Interpreting Large Text-to-Image Diffusion Models with Dictionary   Learning</h2><p><strong>Authors:Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy</strong></p>
<p>Sparse autoencoders are a promising new approach for decomposing language model activations for interpretation and control. They have been applied successfully to vision transformer image encoders and to small-scale diffusion models. Inference-Time Decomposition of Activations (ITDA) is a recently proposed variant of dictionary learning that takes the dictionary to be a set of data points from the activation distribution and reconstructs them with gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large text-to-image diffusion model, Flux 1, and consider the interpretability of embeddings of both by introducing a visual automated interpretation pipeline. We find that SAEs accurately reconstruct residual stream embeddings and beat MLP neurons on interpretability. We are able to use SAE features to steer image generation through activation addition. We find that ITDA has comparable interpretability to SAEs. </p>
<blockquote>
<p>ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–°æ–¹æ³•ï¼Œç”¨äºåˆ†è§£è¯­è¨€æ¨¡å‹çš„æ¿€æ´»ä»¥è¿›è¡Œè§£é‡Šå’Œæ§åˆ¶ã€‚å®ƒä»¬å·²æˆåŠŸåº”ç”¨äºè§†è§‰è½¬æ¢å™¨å›¾åƒç¼–ç å™¨å’Œå°å‹æ‰©æ•£æ¨¡å‹ã€‚æ¿€æ´»çš„æ¨ç†æ—¶é—´åˆ†è§£ï¼ˆITDAï¼‰æ˜¯å­—å…¸å­¦ä¹ çš„ä¸€ä¸ªæ–°è¿‘æå‡ºçš„å˜ä½“ï¼Œå®ƒå°†å­—å…¸è§†ä¸ºæ¿€æ´»åˆ†å¸ƒä¸­çš„æ•°æ®ç‚¹é›†ï¼Œå¹¶é€šè¿‡æ¢¯åº¦è¿½æ±‚è¿›è¡Œé‡å»ºã€‚æˆ‘ä»¬å°†ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰å’ŒITDAåº”ç”¨äºå¤§å‹æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹Flux 1ï¼Œå¹¶é€šè¿‡å¼•å…¥è§†è§‰è‡ªåŠ¨åŒ–è§£é‡Šç®¡é“è€ƒè™‘ä¸¤è€…çš„åµŒå…¥è§£é‡Šæ€§ã€‚æˆ‘ä»¬å‘ç°SAEèƒ½å¤Ÿå‡†ç¡®é‡å»ºæ®‹å·®æµåµŒå…¥ï¼Œå¹¶åœ¨è§£é‡Šæ€§æ–¹é¢ä¼˜äºMLPç¥ç»å…ƒã€‚æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨SAEç‰¹æ€§é€šè¿‡æ¿€æ´»æ·»åŠ æ¥å¼•å¯¼å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬å‘ç°ITDAçš„è§£é‡Šæ€§ä¸SAEç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24360v2">PDF</a> 10 pages, 10 figures, Mechanistic Interpretability for Vision at CVPR   2025</p>
<p><strong>Summary</strong></p>
<p>ç¨€ç–è‡ªç¼–ç å™¨æ˜¯åˆ†è§£è¯­è¨€æ¨¡å‹æ¿€æ´»ä»¥è¿›è¡Œè§£é‡Šå’Œæ§åˆ¶çš„ä¸€ç§æ–°å…´ã€æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚å·²æˆåŠŸåº”ç”¨äºè§†è§‰è½¬æ¢å™¨å›¾åƒç¼–ç å™¨å’Œå°å‹æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬åº”ç”¨ç¨€ç–è‡ªç¼–ç å™¨å’Œæ¨ç†æ—¶é—´æ¿€æ´»åˆ†è§£ï¼ˆITDAï¼‰åˆ°å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹Flux 1ï¼Œå¹¶é€šè¿‡å¼•å…¥è§†è§‰è‡ªåŠ¨åŒ–è§£é‡Šç®¡é“è€ƒè™‘ä¸¤è€…çš„åµŒå…¥è§£é‡Šæ€§ã€‚ç ”ç©¶å‘ç°ï¼ŒSAEèƒ½å¤Ÿå‡†ç¡®é‡å»ºæ®‹å·®æµåµŒå…¥ï¼Œå¹¶åœ¨è§£é‡Šæ€§æ–¹é¢ä¼˜äºMLPç¥ç»å…ƒã€‚æˆ‘ä»¬è¿˜èƒ½å¤Ÿä½¿ç”¨SAEç‰¹æ€§é€šè¿‡æ¿€æ´»æ·»åŠ æ¥æ“æ§å›¾åƒç”Ÿæˆã€‚ITDAçš„è§£é‡Šæ€§ä¸SAEç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨€ç–è‡ªç¼–ç å™¨æ˜¯åˆ†è§£è¯­è¨€æ¨¡å‹æ¿€æ´»çš„æ–°å…´æ–¹æ³•ï¼Œç”¨äºè§£é‡Šå’Œæ§åˆ¶ã€‚</li>
<li>ç¨€ç–è‡ªç¼–ç å™¨å·²æˆåŠŸåº”ç”¨äºè§†è§‰è½¬æ¢å™¨å›¾åƒç¼–ç å™¨å’Œå°å‹æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æ¨ç†æ—¶é—´åˆ†è§£æ¿€æ´»ï¼ˆITDAï¼‰æ˜¯å­—å…¸å­¦ä¹ çš„ä¸€ç§å˜ä½“ï¼Œèƒ½å¤Ÿé‡å»ºæ¿€æ´»åˆ†å¸ƒçš„æ•°æ®ç‚¹ã€‚</li>
<li>åœ¨å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹Flux 1ä¸Šåº”ç”¨äº†SAEå’ŒITDAã€‚</li>
<li>SAEèƒ½å¤Ÿå‡†ç¡®é‡å»ºæ®‹å·®æµåµŒå…¥ï¼Œä¸”åœ¨è§£é‡Šæ€§æ–¹é¢ä¼˜äºMLPç¥ç»å…ƒã€‚</li>
<li>SAEç‰¹æ€§å¯ç”¨äºæ“æ§å›¾åƒç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-603051626b2dbf03d37254dda6c6859e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5af5085d406ff9dd99f7a78d6879004.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ff2737bf419a0bcca28f1acc438e1a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-464854384adbd7779b5f8aa7619f4926.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e45deecaa551bc541ad2a68085ab6fe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46be4dcec21f8eb73a7ad696931c864f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Video-Motion-Graphs"><a href="#Video-Motion-Graphs" class="headerlink" title="Video Motion Graphs"></a>Video Motion Graphs</h2><p><strong>Authors:Haiyang Liu, Zhan Xu, Fa-Ting Hong, Hsin-Ping Huang, Yi Zhou, Yang Zhou</strong></p>
<p>We present Video Motion Graphs, a system designed to generate realistic human motion videos. Using a reference video and conditional signals such as music or motion tags, the system synthesizes new videos by first retrieving video clips with gestures matching the conditions and then generating interpolation frames to seamlessly connect clip boundaries. The core of our approach is HMInterp, a robust Video Frame Interpolation (VFI) model that enables seamless interpolation of discontinuous frames, even for complex motion scenarios like dancing. HMInterp i) employs a dual-branch interpolation approach, combining a Motion Diffusion Model for human skeleton motion interpolation with a diffusion-based video frame interpolation model for final frame generation. ii) adopts condition progressive training to effectively leverage identity strong and weak conditions, such as images and pose. These designs ensure both high video texture quality and accurate motion trajectory. Results show that our Video Motion Graphs outperforms existing generative- and retrieval-based methods for multi-modal conditioned human motion video generation. Project page can be found at <a target="_blank" rel="noopener" href="https://h-liu1997.github.io/Video-Motion-Graphs/">https://h-liu1997.github.io/Video-Motion-Graphs/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†è§†é¢‘è¿åŠ¨å›¾ï¼ˆVideo Motion Graphsï¼‰ç³»ç»Ÿï¼Œæ—¨åœ¨ç”Ÿæˆé€¼çœŸçš„äººç±»è¿åŠ¨è§†é¢‘ã€‚è¯¥ç³»ç»Ÿä½¿ç”¨å‚è€ƒè§†é¢‘å’Œæ¡ä»¶ä¿¡å·ï¼ˆå¦‚éŸ³ä¹æˆ–è¿åŠ¨æ ‡ç­¾ï¼‰æ¥åˆæˆæ–°è§†é¢‘ã€‚å®ƒé¦–å…ˆæ£€ç´¢ä¸æ¡ä»¶åŒ¹é…çš„è§†é¢‘ç‰‡æ®µï¼Œç„¶åç”Ÿæˆæ’å¸§ä»¥æ— ç¼è¿æ¥ç‰‡æ®µè¾¹ç•Œã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯HMInterpï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„è§†é¢‘å¸§æ’å€¼ï¼ˆVFIï¼‰æ¨¡å‹ï¼Œå³ä½¿å¯¹äºè·³èˆç­‰å¤æ‚è¿åŠ¨åœºæ™¯ï¼Œä¹Ÿèƒ½å®ç°æ— ç¼æ’å€¼ã€‚HMInterpé‡‡ç”¨åŒåˆ†æ”¯æ’å€¼æ–¹æ³•ï¼Œç»“åˆç”¨äºäººä½“éª¨æ¶è¿åŠ¨æ’å€¼çš„è¿åŠ¨æ‰©æ•£æ¨¡å‹ä¸åŸºäºæ‰©æ•£çš„è§†é¢‘å¸§æ’å€¼æ¨¡å‹è¿›è¡Œæœ€ç»ˆå¸§ç”Ÿæˆã€‚å®ƒé‡‡ç”¨æ¡ä»¶æ¸è¿›è®­ç»ƒï¼Œæœ‰æ•ˆåˆ©ç”¨å¼ºæ¡ä»¶å’Œå¼±æ¡ä»¶ï¼ˆå¦‚å›¾åƒå’Œå§¿æ€ï¼‰ã€‚è¿™äº›è®¾è®¡ç¡®ä¿äº†è§†é¢‘çº¹ç†è´¨é‡é«˜ä¸”è¿åŠ¨è½¨è¿¹å‡†ç¡®ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è§†é¢‘è¿åŠ¨å›¾ç³»ç»Ÿåœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„äººç±»è¿åŠ¨è§†é¢‘ç”Ÿæˆä¸­ä¼˜äºç°æœ‰çš„ç”Ÿæˆå‹å’Œæ£€ç´¢å‹æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢å¯è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://h-liu1997.github.io/Video-Motion-Graphs/]">https://h-liu1997.github.io/Video-Motion-Graphs/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20218v2">PDF</a> 14 pages,10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Video Motion Graphsç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®å‚è€ƒè§†é¢‘ã€éŸ³ä¹æˆ–åŠ¨ä½œæ ‡ç­¾ç­‰æ¡ä»¶ä¿¡å·ç”Ÿæˆé€¼çœŸçš„äººç±»è¿åŠ¨è§†é¢‘ã€‚å®ƒé€šè¿‡æ£€ç´¢åŒ¹é…æ¡ä»¶çš„è§†é¢‘ç‰‡æ®µï¼Œå¹¶åˆ©ç”¨HMInterpè¿›è¡Œæ’å¸§ï¼Œç”Ÿæˆæµç•…çš„æ–°è§†é¢‘ã€‚HMInterpæ˜¯ä¸€ä¸ªå¼ºå¤§çš„è§†é¢‘å¸§æ’å€¼æ¨¡å‹ï¼Œèƒ½åœ¨å¤æ‚åŠ¨ä½œåœºæ™¯å¦‚èˆè¹ˆä¸­æ— ç¼æ’å€¼æ–­å¸§ã€‚é‡‡ç”¨åŒåˆ†æ”¯æ’å€¼æ–¹æ³•ï¼Œç»“åˆäººä½“éª¨æ¶è¿åŠ¨æ’å€¼çš„Motion Diffusion Modelå’Œæœ€ç»ˆå¸§ç”Ÿæˆçš„æ‰©æ•£å¼è§†é¢‘å¸§æ’å€¼æ¨¡å‹ã€‚å¹¶é‡‡ç”¨æ¡ä»¶æ¸è¿›è®­ç»ƒï¼Œæœ‰æ•ˆåˆ©ç”¨èº«ä»½å¼ºå¼±æ¡ä»¶å¦‚å›¾åƒå’Œå§¿æ€ã€‚ç»“æœè¯æ˜Video Motion Graphsåœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹çš„äººç±»è¿åŠ¨è§†é¢‘ç”Ÿæˆä¸­è¡¨ç°ä¼˜äºç°æœ‰ç”Ÿæˆå’Œæ£€ç´¢æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video Motion Graphsç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®æ¡ä»¶ä¿¡å·ç”Ÿæˆé€¼çœŸçš„äººç±»è¿åŠ¨è§†é¢‘ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡æ£€ç´¢åŒ¹é…æ¡ä»¶çš„è§†é¢‘ç‰‡æ®µå¹¶æ’å¸§æ¥ç”Ÿæˆæ–°è§†é¢‘ã€‚</li>
<li>HMInterpæ˜¯ä¸€ä¸ªå¼ºå¤§çš„è§†é¢‘å¸§æ’å€¼æ¨¡å‹ï¼Œæ”¯æŒå¤æ‚è¿åŠ¨åœºæ™¯çš„æ— ç¼æ’å€¼ã€‚</li>
<li>HMInterpé‡‡ç”¨åŒåˆ†æ”¯æ’å€¼æ–¹æ³•ï¼Œç»“åˆMotion Diffusion Modelå’Œæ‰©æ•£å¼è§†é¢‘å¸§æ’å€¼æ¨¡å‹ã€‚</li>
<li>æ¡ä»¶æ¸è¿›è®­ç»ƒæŠ€æœ¯è¢«ç”¨æ¥æœ‰æ•ˆåˆ©ç”¨ä¸åŒçš„æ¡ä»¶ä¿¡æ¯ã€‚</li>
<li>Video Motion Graphsåœ¨å¤šé¡¹æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>é¡¹ç›®é¡µé¢æä¾›äº†æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20218">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9ecdc20352cd85a2faacb945000f72c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92f740231ffa4cb6db83ed8b8ad7d0f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d581a96b8df655f231a6b9da8828238.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7077c1a68795b3eb41e9631b990455f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf409feb4ddc735e4f8f428ca04cef9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be66ce06c93e246accd51af75a8cc9d8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Concept-Corrector-Erase-concepts-on-the-fly-for-text-to-image-diffusion-models"><a href="#Concept-Corrector-Erase-concepts-on-the-fly-for-text-to-image-diffusion-models" class="headerlink" title="Concept Corrector: Erase concepts on the fly for text-to-image diffusion   models"></a>Concept Corrector: Erase concepts on the fly for text-to-image diffusion   models</h2><p><strong>Authors:Zheling Meng, Bo Peng, Xiaochuan Jin, Yueming Lyu, Wei Wang, Jing Dong, Tieniu Tan</strong></p>
<p>Text-to-image diffusion models have demonstrated the underlying risk of generating various unwanted content, such as sexual elements. To address this issue, the task of concept erasure has been introduced, aiming to erase any undesired concepts that the models can generate. Previous methods, whether training-based or training-free, have primarily focused on the input side, i.e., texts. However, they often suffer from incomplete erasure due to limitations in the generalization from limited prompts to diverse image content. In this paper, motivated by the notion that concept erasure on the output side, i.e., generated images, may be more direct and effective, we propose Concept Corrector. It checks target concepts based on visual features provided by final generated images predicted at certain time steps. Further, it incorporates Concept Removal Attention to erase generated concept features. It overcomes the limitations of existing methods, which are either unable to remove the concept features that have been generated in images or rely on the assumption that the related concept words are contained in input prompts. In the whole pipeline, our method changes no model parameters and only requires a given target concept as well as the corresponding replacement content, which is easy to implement. To the best of our knowledge, this is the first erasure method based on intermediate-generated images, achieving the ability to erase concepts on the fly. The experiments on various concepts demonstrate its impressive erasure performance. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºç”Ÿæˆå„ç§ä¸æƒ³è¦å†…å®¹ï¼ˆå¦‚æ€§å…ƒç´ ï¼‰çš„æ½œåœ¨é£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥äº†æ¦‚å¿µæ¶ˆé™¤çš„ä»»åŠ¡ï¼Œæ—¨åœ¨æ¶ˆé™¤æ¨¡å‹å¯èƒ½ç”Ÿæˆçš„ä¸éœ€è¦çš„æ¦‚å¿µã€‚ä»¥å‰çš„æ–¹æ³•ï¼Œæ— è®ºæ˜¯åŸºäºè®­ç»ƒçš„ï¼Œè¿˜æ˜¯éè®­ç»ƒçš„ï¼Œä¸»è¦å…³æ³¨è¾“å…¥æ–¹é¢ï¼Œå³æ–‡æœ¬ã€‚ç„¶è€Œï¼Œç”±äºä»æœ‰é™çš„æç¤ºæ¨å¹¿åˆ°å¤šæ ·åŒ–çš„å›¾åƒå†…å®¹çš„å±€é™æ€§ï¼Œå®ƒä»¬ç»å¸¸é­å—ä¸å®Œå…¨æ¶ˆé™¤çš„é—®é¢˜ã€‚æœ¬æ–‡å—åˆ°è¾“å‡ºä¾§ï¼ˆå³ç”Ÿæˆçš„å›¾åƒï¼‰æ¦‚å¿µæ¶ˆé™¤å¯èƒ½æ›´ç›´æ¥å’Œæœ‰æ•ˆçš„æ¦‚å¿µçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†æ¦‚å¿µä¿®æ­£å™¨ã€‚å®ƒåŸºäºæœ€ç»ˆç”Ÿæˆçš„å›¾åƒåœ¨ç‰¹å®šæ—¶é—´æ­¥é•¿é¢„æµ‹çš„è§†è§‰ç‰¹å¾æ¥æ£€æŸ¥ç›®æ ‡æ¦‚å¿µã€‚æ­¤å¤–ï¼Œå®ƒç»“åˆäº†æ¦‚å¿µå»é™¤æ³¨æ„åŠ›æ¥æ¶ˆé™¤ç”Ÿæˆçš„æ¦‚å¿µç‰¹å¾ã€‚å®ƒå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆæ— æ³•æ¶ˆé™¤å·²ç”Ÿæˆå›¾åƒä¸­çš„æ¦‚å¿µç‰¹å¾ï¼Œè¦ä¹ˆä¾èµ–äºç›¸å…³æ¦‚å¿µè¯åŒ…å«åœ¨è¾“å…¥æç¤ºä¸­çš„å‡è®¾ã€‚åœ¨æ•´ä¸ªæµç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä¼šæ›´æ”¹ä»»ä½•æ¨¡å‹å‚æ•°ï¼Œåªéœ€ç»™å®šç›®æ ‡æ¦‚å¿µä»¥åŠç›¸åº”çš„æ›¿æ¢å†…å®¹å³å¯ï¼Œæ˜“äºå®ç°ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸­é—´ç”Ÿæˆçš„å›¾åƒçš„é¦–ä¸ªæ¶ˆé™¤æ–¹æ³•ï¼Œå®ç°äº†å³æ—¶æ¶ˆé™¤æ¦‚å¿µçš„èƒ½åŠ›ã€‚å¯¹å„ç§æ¦‚å¿µçš„å®éªŒè¯æ˜äº†å…¶ä»¤äººå°è±¡æ·±åˆ»çš„æ¶ˆé™¤æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16368v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç”Ÿæˆå›¾åƒçš„æ¦‚å¿µä¿®æ­£å™¨ï¼ˆConcept Correctorï¼‰ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸æƒ³è¦å†…å®¹ï¼ˆå¦‚æ€§å…ƒç´ ç­‰ï¼‰çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æ£€æŸ¥ç‰¹å®šæ—¶é—´æ­¥é•¿ç”Ÿæˆçš„å›¾åƒçš„è§†è§‰ç‰¹å¾æ¥éªŒè¯ç›®æ ‡æ¦‚å¿µï¼Œå¹¶ç»“åˆæ¦‚å¿µç§»é™¤æ³¨æ„åŠ›æœºåˆ¶æ¥æ¶ˆé™¤ç”Ÿæˆçš„æ¦‚å¿µç‰¹å¾ã€‚æ­¤æ–¹æ³•å…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä»…éœ€è¦ç›®æ ‡æ¦‚å¿µåŠå…¶æ›¿æ¢å†…å®¹ï¼Œæ˜“äºå®ç°ï¼Œä¸”åœ¨å„ç§æ¦‚å¿µä¸Šçš„å®éªŒè¯æ˜äº†å…¶å‡ºè‰²çš„æ¶ˆé™¤æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å­˜åœ¨ç”Ÿæˆä¸æƒ³è¦å†…å®¹çš„æ½œåœ¨é£é™©ï¼Œå¦‚æ€§å…ƒç´ ç­‰ã€‚</li>
<li>æ¦‚å¿µæ¶ˆé™¤ä»»åŠ¡æ—¨åœ¨æ¶ˆé™¤æ¨¡å‹ä¸­ç”Ÿæˆçš„ä¸æƒ³è¦çš„æ¦‚å¿µã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è¾“å…¥ä¾§çš„æ¦‚å¿µæ¶ˆé™¤ï¼Œä½†å­˜åœ¨ä»æœ‰é™æç¤ºæ¨å¹¿åˆ°å¤šæ ·å›¾åƒå†…å®¹çš„å±€é™æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„Concept CorrectoråŸºäºç”Ÿæˆå›¾åƒçš„æ¦‚å¿µä¿®æ­£ï¼Œé€šè¿‡æ£€æŸ¥ç”Ÿæˆçš„å›¾åƒçš„è§†è§‰ç‰¹å¾æ¥éªŒè¯å’Œæ¶ˆé™¤ç›®æ ‡æ¦‚å¿µã€‚</li>
<li>Concept Correctorç»“åˆäº†æ¦‚å¿µç§»é™¤æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œä»…éœ€è¦ç›®æ ‡æ¦‚å¿µåŠå…¶æ›¿æ¢å†…å®¹ï¼Œæ˜“äºå®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ee959307c2e481f28e0e6562feac2a0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b4da7a9be505a1fc2deae1b93cc6d9bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20a799b6ed5cda8e0bd6c22833e7540e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior"><a href="#Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior" class="headerlink" title="Rethinking Diffusion Posterior Sampling: From Conditional Score   Estimator to Maximizing a Posterior"></a>Rethinking Diffusion Posterior Sampling: From Conditional Score   Estimator to Maximizing a Posterior</h2><p><strong>Authors:Tongda Xu, Xiyan Cai, Xinjie Zhang, Xingtong Ge, Dailan He, Ming Sun, Jingjing Liu, Ya-Qin Zhang, Jian Li, Yan Wang</strong></p>
<p>Recent advancements in diffusion models have been leveraged to address inverse problems without additional training, and Diffusion Posterior Sampling (DPS) (Chung et al., 2022a) is among the most popular approaches. Previous analyses suggest that DPS accomplishes posterior sampling by approximating the conditional score. While in this paper, we demonstrate that the conditional score approximation employed by DPS is not as effective as previously assumed, but rather aligns more closely with the principle of maximizing a posterior (MAP). This assertion is substantiated through an examination of DPS on 512x512 ImageNet images, revealing that: 1) DPSâ€™s conditional score estimation significantly diverges from the score of a well-trained conditional diffusion model and is even inferior to the unconditional score; 2) The mean of DPSâ€™s conditional score estimation deviates significantly from zero, rendering it an invalid score estimation; 3) DPS generates high-quality samples with significantly lower diversity. In light of the above findings, we posit that DPS more closely resembles MAP than a conditional score estimator, and accordingly propose the following enhancements to DPS: 1) we explicitly maximize the posterior through multi-step gradient ascent and projection; 2) we utilize a light-weighted conditional score estimator trained with only 100 images and 8 GPU hours. Extensive experimental results indicate that these proposed improvements significantly enhance DPSâ€™s performance. The source code for these improvements is provided in <a target="_blank" rel="noopener" href="https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior">https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior</a>. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•å·²è¢«ç”¨äºè§£å†³æ— éœ€é¢å¤–è®­ç»ƒçš„åé—®é¢˜ï¼Œå…¶ä¸­æ‰©æ•£åé‡‡æ ·ï¼ˆDPSï¼‰ï¼ˆChungç­‰äººï¼Œ2022aï¼‰æ˜¯æœ€å—æ¬¢è¿çš„æ–¹æ³•ä¹‹ä¸€ã€‚ä¹‹å‰çš„åˆ†æè¡¨æ˜ï¼ŒDPSé€šè¿‡åéªŒé‡‡æ ·å®ç°æ¡ä»¶è¯„åˆ†çš„è¿‘ä¼¼ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æˆ‘ä»¬è¯æ˜DPSæ‰€é‡‡ç”¨çš„æ¡ä»¶è¯„åˆ†è¿‘ä¼¼å¹¶ä¸åƒä¹‹å‰å‡è®¾çš„é‚£æ ·æœ‰æ•ˆï¼Œè€Œæ›´æ¥è¿‘äºæœ€å¤§åéªŒï¼ˆMAPï¼‰çš„åŸåˆ™ã€‚é€šè¿‡å¯¹DPSåœ¨512x512 ImageNetå›¾åƒä¸Šçš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ï¼š1ï¼‰DPSçš„æ¡ä»¶è¯„åˆ†ä¼°è®¡ä¸è®­ç»ƒè‰¯å¥½çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„è¯„åˆ†å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç”šè‡³ä¸å¦‚æ— æ¡ä»¶è¯„åˆ†ï¼›2ï¼‰DPSçš„æ¡ä»¶è¯„åˆ†ä¼°è®¡å‡å€¼åç¦»é›¶ï¼Œä½¿å…¶æˆä¸ºæ— æ•ˆçš„è¯„åˆ†ä¼°è®¡ï¼›3ï¼‰è™½ç„¶DPSå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ ·æœ¬ï¼Œä½†æ ·æœ¬å¤šæ ·æ€§æ˜¾è‘—é™ä½ã€‚é‰´äºä¸Šè¿°å‘ç°ï¼Œæˆ‘ä»¬è®¤ä¸ºDPSæ›´æ¥è¿‘MAPè€Œéæ¡ä»¶è¯„åˆ†ä¼°è®¡å™¨ï¼Œå¹¶æ®æ­¤å¯¹DPSæå‡ºä»¥ä¸‹æ”¹è¿›ï¼š1ï¼‰æˆ‘ä»¬é€šè¿‡å¤šæ­¥æ¢¯åº¦ä¸Šå‡å’ŒæŠ•å½±æ¥æ˜¾å¼åœ°æœ€å¤§åŒ–åéªŒï¼›2ï¼‰æˆ‘ä»¬ä½¿ç”¨è½»é‡çº§çš„æ¡ä»¶è¯„åˆ†ä¼°è®¡å™¨ï¼Œä»…ä½¿ç”¨100å¼ å›¾åƒå’Œ8ä¸ªGPUå°æ—¶è¿›è¡Œè®­ç»ƒã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ”¹è¿›æ˜¾è‘—æé«˜äº†DPSçš„æ€§èƒ½ã€‚ç›¸å…³æºä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior%E3%80%82">https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posteriorã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18913v2">PDF</a> ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•è¢«ç”¨æ¥è§£å†³æ— éœ€é¢å¤–è®­ç»ƒçš„åé—®é¢˜ï¼Œå…¶ä¸­Diffusion Posterior Samplingï¼ˆDPSï¼‰æ˜¯æœ€å—æ¬¢è¿çš„æ–¹æ³•ä¹‹ä¸€ã€‚æœ¬æ–‡é€šè¿‡å¯¹DPSçš„æ·±å…¥åˆ†æï¼ŒæŒ‡å‡ºå…¶é‡‡ç”¨çš„æ¡ä»¶åˆ†æ•°è¿‘ä¼¼å¹¶ä¸åƒå…ˆå‰è®¤ä¸ºçš„é‚£æ ·æœ‰æ•ˆï¼Œè€Œæ˜¯æ›´ç¬¦åˆæœ€å¤§åéªŒæ¦‚ç‡ï¼ˆMAPï¼‰çš„åŸç†ã€‚é€šè¿‡å¯¹512x512 ImageNetå›¾åƒä¸Šçš„DPSç ”ç©¶ï¼Œå‘ç°å…¶å­˜åœ¨çš„å‡ ä¸ªé—®é¢˜ï¼šå…¶ä¸€ï¼ŒDPSçš„æ¡ä»¶åˆ†æ•°ä¼°è®¡ä¸è®­ç»ƒè‰¯å¥½çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åˆ†æ•°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç”šè‡³ä¸å¦‚æ— æ¡ä»¶åˆ†æ•°ï¼›å…¶äºŒï¼ŒDPSçš„æ¡ä»¶åˆ†æ•°ä¼°è®¡å‡å€¼åç¦»é›¶ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ— æ•ˆçš„åˆ†æ•°ä¼°è®¡ï¼›å…¶ä¸‰ï¼ŒDPSç”Ÿæˆçš„æ ·æœ¬è´¨é‡é«˜ä½†å¤šæ ·æ€§æ˜¾è‘—é™ä½ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºå¯¹DPSçš„æ”¹è¿›æ–¹æ¡ˆï¼šä¸€æ˜¯é€šè¿‡å¤šæ­¥æ¢¯åº¦ä¸Šå‡å’ŒæŠ•å½±æ˜¾å¼æœ€å¤§åŒ–åéªŒæ¦‚ç‡ï¼›äºŒæ˜¯ä½¿ç”¨è½»é‡çº§çš„æ¡ä»¶åˆ†æ•°ä¼°è®¡å™¨ï¼Œä»…ä½¿ç”¨100å¼ å›¾åƒå’Œ8 GPUå°æ—¶è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ”¹è¿›æ˜¾è‘—æå‡äº†DPSçš„æ€§èƒ½ã€‚æ”¹è¿›ä»£ç çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterioræ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Diffusion Posterior Sampling (DPS) è¢«ç”¨æ¥è§£å†³æ— éœ€é¢å¤–è®­ç»ƒçš„åé—®é¢˜ã€‚</li>
<li>æœ¬æ–‡å‘ç°DPSçš„æ¡ä»¶åˆ†æ•°ä¼°è®¡å­˜åœ¨é—®é¢˜ï¼Œä¸è®­ç»ƒè‰¯å¥½çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„åˆ†æ•°å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>DPSçš„æ¡ä»¶åˆ†æ•°ä¼°è®¡å‡å€¼åç¦»é›¶ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ— æ•ˆçš„åˆ†æ•°ä¼°è®¡ã€‚</li>
<li>DPSç”Ÿæˆçš„æ ·æœ¬è™½ç„¶è´¨é‡é«˜ä½†å¤šæ ·æ€§ä¸è¶³ã€‚</li>
<li>æœ¬æ–‡æå‡ºé€šè¿‡å¤šæ­¥æ¢¯åº¦ä¸Šå‡å’ŒæŠ•å½±æ¥æ˜¾å¼æœ€å¤§åŒ–åéªŒæ¦‚ç‡çš„æ–¹æ³•æ”¹è¿›DPSã€‚</li>
<li>ä½¿ç”¨è½»é‡çº§çš„æ¡ä»¶åˆ†æ•°ä¼°è®¡å™¨ï¼Œè®­ç»ƒæˆæœ¬é™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2d28402af25821f3506a45795608c001.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-770d0a4a5fb0dbeb82ded82fd11e6603.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12f9fa664bea9c8224036374d9c412e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76042aa7d25da026f15e8744112dcde0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Constant-Rate-Scheduling-Constant-Rate-Distributional-Change-for-Efficient-Training-and-Sampling-in-Diffusion-Models"><a href="#Constant-Rate-Scheduling-Constant-Rate-Distributional-Change-for-Efficient-Training-and-Sampling-in-Diffusion-Models" class="headerlink" title="Constant Rate Scheduling: Constant-Rate Distributional Change for   Efficient Training and Sampling in Diffusion Models"></a>Constant Rate Scheduling: Constant-Rate Distributional Change for   Efficient Training and Sampling in Diffusion Models</h2><p><strong>Authors:Shuntaro Okada, Kenji Doi, Ryota Yoshihashi, Hirokatsu Kataoka, Tomohiro Tanaka</strong></p>
<p>We propose a general approach to optimize noise schedules for training and sampling in diffusion models. Our approach optimizes the noise schedules to ensure a constant rate of change in the probability distribution of diffused data throughout the diffusion process. Any distance metric for measuring the probability-distributional change is applicable to our approach, and we introduce three distance metrics. We evaluated the effectiveness of our approach on unconditional and class-conditional image-generation tasks using the LSUN (Horse, Bedroom, Church), ImageNet, FFHQ, and CIFAR10 datasets. Through extensive experiments, we confirmed that our approach broadly improves the performance of pixel-space and latent-space diffusion models regardless of the dataset, sampler, and number of function evaluations ranging from 5 to 250. Notably, by using our approach for optimizing both training and sampling schedules, we achieved a state-of-the-art FID score of 2.03 without sacrificing mode coverage on LSUN Horse 256 $\times$ 256. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹æ‰©æ•£æ¨¡å‹è®­ç»ƒå’Œé‡‡æ ·çš„å™ªå£°è°ƒåº¦ä¼˜åŒ–çš„ä¸€èˆ¬æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¼˜åŒ–å™ªå£°è°ƒåº¦ï¼Œä»¥ç¡®ä¿æ‰©æ•£æ•°æ®æ¦‚ç‡åˆ†å¸ƒåœ¨æ•´ä¸ªæ‰©æ•£è¿‡ç¨‹ä¸­çš„å˜åŒ–é€Ÿç‡æ’å®šã€‚ä»»ä½•ç”¨äºè¡¡é‡æ¦‚ç‡åˆ†å¸ƒå˜åŒ–çš„è·ç¦»åº¦é‡éƒ½é€‚ç”¨äºæˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸‰ç§è·ç¦»åº¦é‡ã€‚æˆ‘ä»¬åœ¨LSUNï¼ˆé©¬ã€å§å®¤ã€æ•™å ‚ï¼‰ã€ImageNetã€FFHQå’ŒCIFAR10æ•°æ®é›†ä¸Šï¼Œå¯¹æ— æ¡ä»¶å’Œç±»åˆ«æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯å®äº†æˆ‘ä»¬æ–¹æ³•å¹¿æ³›æé«˜äº†åƒç´ ç©ºé—´å’Œæ½œåœ¨ç©ºé—´æ‰©æ•£æ¨¡å‹æ€§èƒ½ï¼Œæ— è®ºæ•°æ®é›†ã€é‡‡æ ·å™¨å’ŒåŠŸèƒ½è¯„ä¼°æ¬¡æ•°ï¼ˆä»5åˆ°250æ¬¡ï¼‰å¦‚ä½•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡é‡‡ç”¨æˆ‘ä»¬çš„æ–¹æ³•æ¥ä¼˜åŒ–è®­ç»ƒå’Œé‡‡æ ·è°ƒåº¦ï¼Œæˆ‘ä»¬åœ¨LSUN Horse 256 x 256ä¸Šå®ç°äº†å‰æ‰€æœªæœ‰çš„FIDåˆ†æ•°2.03ï¼ŒåŒæ—¶æ²¡æœ‰ç‰ºç‰²æ¨¡å¼è¦†ç›–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12188v3">PDF</a> 44 pages, 20 figures, 25 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¼˜åŒ–æ‰©æ•£æ¨¡å‹è®­ç»ƒå’Œé‡‡æ ·è¿‡ç¨‹ä¸­å™ªå£°è°ƒåº¦çš„ä¸€èˆ¬æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–å™ªå£°è°ƒåº¦æ¥ç¡®ä¿æ‰©æ•£æ•°æ®æ¦‚ç‡åˆ†å¸ƒåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„å˜åŒ–é€Ÿç‡æ’å®šã€‚æ–‡ä¸­ä»‹ç»äº†ä¸‰ç§é€‚ç”¨äºè¯¥æ–¹æ³•çš„è·ç¦»åº¦é‡æŒ‡æ ‡ï¼Œå¹¶åœ¨LSUNï¼ˆé©¬ã€å§å®¤ã€æ•™å ‚ï¼‰ã€ImageNetã€FFHQå’ŒCIFAR10ç­‰æ•°æ®é›†çš„å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯å¹¿æ³›æé«˜åƒç´ ç©ºé—´å’Œæ½œåœ¨ç©ºé—´æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œé€‚ç”¨äºä¸åŒçš„æ•°æ®é›†ã€é‡‡æ ·å™¨å’ŒåŠŸèƒ½è¯„ä¼°æ¬¡æ•°ã€‚ç‰¹åˆ«æ˜¯åœ¨LSUN Horse 256Ã—256æ•°æ®é›†ä¸Šï¼Œé€šè¿‡ä¼˜åŒ–è®­ç»ƒå’Œé‡‡æ ·è°ƒåº¦ï¼Œå®ç°äº†æ— æ¨¡å¼ä¸¢å¤±çš„FIDåˆ†æ•°ä¸º2.03ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„å™ªå£°è°ƒåº¦æ–¹æ³•ï¼Œç¡®ä¿åœ¨æ•´ä¸ªæ‰©æ•£è¿‡ç¨‹ä¸­æ¦‚ç‡åˆ†å¸ƒçš„å˜åŒ–é€Ÿç‡æ’å®šã€‚</li>
<li>ä»‹ç»äº†ä¸‰ç§é€‚ç”¨äºè¯¥æ–¹æ³•çš„è·ç¦»åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºæµ‹é‡æ¦‚ç‡åˆ†å¸ƒçš„å˜åŒ–ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ï¼ˆå¦‚LSUNã€ImageNetã€FFHQå’ŒCIFAR10ï¼‰ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¯å¹¿æ³›åº”ç”¨äºåƒç´ ç©ºé—´å’Œæ½œåœ¨ç©ºé—´çš„æ‰©æ•£æ¨¡å‹ï¼Œé€‚ç”¨äºä¸åŒçš„æ•°æ®é›†ã€é‡‡æ ·å™¨å’ŒåŠŸèƒ½è¯„ä¼°æ¬¡æ•°ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–è®­ç»ƒå’Œé‡‡æ ·è°ƒåº¦ï¼Œå®ç°äº†åœ¨LSUN Horse 256Ã—256æ•°æ®é›†ä¸Šçš„å…ˆè¿›FIDåˆ†æ•°ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿæé«˜å›¾åƒç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶åœ¨ä¸ç‰ºç‰²æ¨¡å¼è¦†ç›–çš„æƒ…å†µä¸‹è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.12188">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc080101785c46a474dbdc64f5f4ca97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-038a54d33df35c8315cda910ca87f570.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DeepSPV-A-Deep-Learning-Pipeline-for-3D-Spleen-Volume-Estimation-from-2D-Ultrasound-Images"><a href="#DeepSPV-A-Deep-Learning-Pipeline-for-3D-Spleen-Volume-Estimation-from-2D-Ultrasound-Images" class="headerlink" title="DeepSPV: A Deep Learning Pipeline for 3D Spleen Volume Estimation from   2D Ultrasound Images"></a>DeepSPV: A Deep Learning Pipeline for 3D Spleen Volume Estimation from   2D Ultrasound Images</h2><p><strong>Authors:Zhen Yuan, David Stojanovski, Lei Li, Alberto Gomez, Haran Jogeesvaran, Esther Puyol-AntÃ³n, Baba Inusa, Andrew P. King</strong></p>
<p>Splenomegaly, the enlargement of the spleen, is an important clinical indicator for various associated medical conditions, such as sickle cell disease (SCD). Spleen length measured from 2D ultrasound is the most widely used metric for characterising spleen size. However, it is still considered a surrogate measure, and spleen volume remains the gold standard for assessing spleen size. Accurate spleen volume measurement typically requires 3D imaging modalities, such as computed tomography or magnetic resonance imaging, but these are not widely available, especially in the Global South which has a high prevalence of SCD. In this work, we introduce a deep learning pipeline, DeepSPV, for precise spleen volume estimation from single or dual 2D ultrasound images. The pipeline involves a segmentation network and a variational autoencoder for learning low-dimensional representations from the estimated segmentations. We investigate three approaches for spleen volume estimation and our best model achieves 86.62%&#x2F;92.5% mean relative volume accuracy (MRVA) under single-view&#x2F;dual-view settings, surpassing the performance of human experts. In addition, the pipeline can provide confidence intervals for the volume estimates as well as offering benefits in terms of interpretability, which further support clinicians in decision-making when identifying splenomegaly. We evaluate the full pipeline using a highly realistic synthetic dataset generated by a diffusion model, achieving an overall MRVA of 83.0% from a single 2D ultrasound image. Our proposed DeepSPV is the first work to use deep learning to estimate 3D spleen volume from 2D ultrasound images and can be seamlessly integrated into the current clinical workflow for spleen assessment. </p>
<blockquote>
<p>è„¾è‚¿å¤§å³è„¾è„å¢å¤§ï¼Œæ˜¯å¤šç§ç›¸å…³ç–¾ç—…ï¼ˆå¦‚é•°çŠ¶ç»†èƒç—…ï¼‰çš„é‡è¦ä¸´åºŠæŒ‡æ ‡ã€‚è„¾è„é•¿åº¦é€šè¿‡äºŒç»´è¶…å£°æµ‹é‡æ˜¯è¡¨å¾è„¾è„å¤§å°æœ€å¸¸ç”¨çš„æŒ‡æ ‡ã€‚ç„¶è€Œï¼Œè¿™ä»ç„¶è¢«è§†ä¸ºä¸€ç§æ›¿ä»£æµ‹é‡æ‰‹æ®µï¼Œè„¾è„ä½“ç§¯ä»ç„¶æ˜¯è¯„ä¼°è„¾è„å¤§å°çš„é‡‘æ ‡å‡†ã€‚å‡†ç¡®çš„è„¾è„ä½“ç§¯æµ‹é‡é€šå¸¸éœ€è¦ä¸‰ç»´æˆåƒæ¨¡å¼ï¼Œå¦‚è®¡ç®—æœºæ–­å±‚æ‰«ææˆ–ç£å…±æŒ¯æˆåƒï¼Œä½†è¿™äº›å¹¶ä¸æ™®éå¯ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¨çƒå—éƒ¨åœ°åŒºï¼Œé•°çŠ¶ç»†èƒç—…å‘ç—…ç‡è¾ƒé«˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ·±åº¦å­¦ä¹ ç®¡é“DeepSPVï¼Œç”¨äºä»å•ä¸ªæˆ–ä¸¤ä¸ªäºŒç»´è¶…å£°å›¾åƒä¸­ç²¾ç¡®ä¼°è®¡è„¾è„ä½“ç§¯ã€‚è¯¥ç®¡é“åŒ…æ‹¬ä¸€ä¸ªåˆ†å‰²ç½‘ç»œå’Œä¸€ä¸ªå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼Œç”¨äºä»ä¼°è®¡çš„åˆ†å‰²ä¸­å­¦ä¹ ä½ç»´è¡¨ç¤ºã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸‰ç§ä¼°è®¡è„¾è„ä½“ç§¯çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹åœ¨å•è§†å›¾&#x2F;åŒè§†å›¾è®¾ç½®ä¸‹è¾¾åˆ°86.62%&#x2F;92.5%çš„å¹³å‡ç›¸å¯¹ä½“ç§¯ç²¾åº¦ï¼ˆMRVAï¼‰ï¼Œè¶…è¿‡äº†äººç±»ä¸“å®¶çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯¥ç®¡é“å¯ä»¥ä¸ºä½“ç§¯ä¼°è®¡æä¾›ç½®ä¿¡åŒºé—´ï¼Œå¹¶åœ¨å¯è§£é‡Šæ€§æ–¹é¢å¸¦æ¥å¥½å¤„ï¼Œè¿™æœ‰åŠ©äºä¸´åºŠåŒ»ç”Ÿåœ¨è¯Šæ–­è„¾è‚¿å¤§æ—¶åšå‡ºå†³ç­–ã€‚æˆ‘ä»¬ä½¿ç”¨é€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é«˜åº¦é€¼çœŸçš„åˆæˆæ•°æ®é›†å¯¹å…¨æµç¨‹è¿›è¡Œäº†è¯„ä¼°ï¼Œä»å•ä¸ªäºŒç»´è¶…å£°å›¾åƒè¾¾åˆ°çš„æ€»ä½“å¹³å‡ç›¸å¯¹ä½“ç§¯ç²¾åº¦ä¸º83.0%ã€‚æˆ‘ä»¬æå‡ºçš„DeepSPVæ˜¯ç¬¬ä¸€ä¸ªä½¿ç”¨æ·±åº¦å­¦ä¹ ä»äºŒç»´è¶…å£°å›¾åƒä¼°è®¡ä¸‰ç»´è„¾è„ä½“ç§¯çš„å·¥ä½œï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°å½“å‰çš„è„¾è„è¯„ä¼°ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11190v2">PDF</a> arXiv admin note: substantial text overlap with arXiv:2308.08038</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œä»å•æˆ–åŒ2Dè¶…å£°å›¾åƒä¸­ç²¾ç¡®ä¼°è®¡è„¾è„ä½“ç§¯ã€‚æå‡ºDeepSPVç®¡é“ï¼ŒåŒ…å«åˆ†å‰²ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼Œç”¨äºå­¦ä¹ ä¼°è®¡åˆ†å‰²çš„ä½ç»´è¡¨ç¤ºã€‚ç ”ç©¶ä¸‰ç§è„¾è„ä½“ç§¯ä¼°è®¡æ–¹æ³•ï¼Œæœ€ä½³æ¨¡å‹åœ¨å•è§†å›¾&#x2F;åŒè§†å›¾è®¾ç½®ä¸‹è¾¾åˆ°86.62%&#x2F;92.5%çš„å¹³å‡ç›¸å¯¹ä½“ç§¯ç²¾åº¦ï¼ˆMRVAï¼‰ï¼Œè¶…è¶Šäººç±»ä¸“å®¶çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç®¡é“å¯ä¸ºä½“ç§¯ä¼°è®¡æä¾›ç½®ä¿¡åŒºé—´ï¼Œæé«˜å¯è§£é‡Šæ€§ï¼Œæ”¯æŒåŒ»ç”Ÿåœ¨è¯†åˆ«è„¾è‚¿å¤§æ—¶çš„å†³ç­–ã€‚ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é«˜åº¦ç°å®åˆæˆæ•°æ®é›†è¯„ä¼°æ•´ä½“ç®¡é“ï¼Œä»å•ä¸ª2Dè¶…å£°å›¾åƒè·å¾—æ€»ä½“MRVAä¸º83.0%ã€‚DeepSPVæ˜¯é¦–ä¸ªåˆ©ç”¨æ·±åº¦å­¦ä¹ ä»2Dè¶…å£°å›¾åƒä¼°è®¡3Dè„¾è„ä½“ç§¯çš„å·¥ä½œï¼Œå¯æ— ç¼é›†æˆåˆ°å½“å‰ä¸´åºŠå·¥ä½œæµç¨‹ä¸­è¿›è¡Œè„¾è„è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„¾è‚¿å¤§æ˜¯é‡è¦çš„ä¸´åºŠæŒ‡æ ‡ï¼Œæ¶‰åŠå¤šç§åŒ»å­¦çŠ¶å†µå¦‚é•°çŠ¶ç»†èƒç—…ï¼ˆSCDï¼‰ã€‚</li>
<li>è„¾è„é•¿åº¦é€šå¸¸é€šè¿‡äºŒç»´è¶…å£°æµ‹é‡ï¼Œä½†è„¾è„ä½“ç§¯æ˜¯é‡‘æ ‡å‡†ã€‚</li>
<li>ä¸‰ç»´æˆåƒæŠ€æœ¯å¦‚è®¡ç®—æœºæ–­å±‚æ‰«ææˆ–ç£å…±æŒ¯æˆåƒç”¨äºç²¾ç¡®æµ‹é‡è„¾è„ä½“ç§¯ï¼Œä½†åœ¨å…¨çƒå—éƒ¨åœ°åŒºæ™®åŠç¨‹åº¦è¾ƒä½ã€‚</li>
<li>å¼•å…¥æ·±åº¦å­¦ä¹ ç®¡é“DeepSPVï¼Œå¯ä»å•ä¸ªæˆ–åŒä¸ªäºŒç»´è¶…å£°å›¾åƒç²¾ç¡®ä¼°è®¡è„¾è„ä½“ç§¯ã€‚</li>
<li>DeepSPVåŒ…æ‹¬åˆ†å‰²ç½‘ç»œå’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼Œå¹¶è°ƒæŸ¥äº†ä¸‰ç§è„¾è„ä½“ç§¯ä¼°è®¡æ–¹æ³•ã€‚</li>
<li>æœ€ä½³æ¨¡å‹è¡¨ç°è¶…è¿‡äººç±»ä¸“å®¶ï¼Œä¸ºä½“ç§¯ä¼°è®¡æä¾›ç½®ä¿¡åŒºé—´ï¼Œå¹¶æé«˜å¯è§£é‡Šæ€§ä»¥è¾…åŠ©åŒ»ç”Ÿå†³ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d79903506f1384800c7f0c7719b170d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78f9d261f77bcce3a0d5cc6e1a10072e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d7a40f9b1b7afece38c0d99f1d76a84.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Shallow-Diffuse-Robust-and-Invisible-Watermarking-through-Low-Dimensional-Subspaces-in-Diffusion-Models"><a href="#Shallow-Diffuse-Robust-and-Invisible-Watermarking-through-Low-Dimensional-Subspaces-in-Diffusion-Models" class="headerlink" title="Shallow Diffuse: Robust and Invisible Watermarking through   Low-Dimensional Subspaces in Diffusion Models"></a>Shallow Diffuse: Robust and Invisible Watermarking through   Low-Dimensional Subspaces in Diffusion Models</h2><p><strong>Authors:Wenda Li, Huijie Zhang, Qing Qu</strong></p>
<p>The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce Shallow Diffuse, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, Shallow Diffuse decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that our Shallow Diffuse outperforms existing watermarking methods in terms of robustness and consistency. The codes will be released at <a target="_blank" rel="noopener" href="https://github.com/liwd190019/Shallow-Diffuse">https://github.com/liwd190019/Shallow-Diffuse</a>. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„AIå†…å®¹å¹¿æ³›åº”ç”¨ï¼Œå…³äºè¯¯å¯¼ä¿¡æ¯å’Œç‰ˆæƒä¾µçŠ¯çš„æ‹…å¿§æ—¥ç›ŠåŠ å‰§ã€‚æ°´å°æŠ€æœ¯æ˜¯è¯†åˆ«è¿™äº›AIç”Ÿæˆå›¾åƒå¹¶é˜²æ­¢å…¶è¢«æ»¥ç”¨çš„ä¸€ç§å…³é”®æŠ€æœ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Shallow Diffuseï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ°´å°æŠ€æœ¯ï¼Œèƒ½å°†ç¨³å¥ä¸”ä¸å¯è§çš„æ°´å°åµŒå…¥åˆ°æ‰©æ•£æ¨¡å‹çš„è¾“å‡ºä¸­ã€‚ä¸åœ¨æ•´ä¸ªæ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­æ•´åˆæ°´å°çš„ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒShallow Diffuseé€šè¿‡åˆ©ç”¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­å­˜åœ¨çš„ä½ç»´å­ç©ºé—´æ¥è§£è€¦è¿™äº›æ­¥éª¤ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿å¤§éƒ¨åˆ†æ°´å°ä½äºè¯¥å­ç©ºé—´çš„é›¶ç©ºé—´ä¸­ï¼Œä»è€Œæœ‰æ•ˆåœ°å°†å…¶ä¸å›¾åƒç”Ÿæˆè¿‡ç¨‹åˆ†ç¦»ã€‚æˆ‘ä»¬çš„ç†è®ºå’Œå®è¯åˆ†æè¡¨æ˜ï¼Œè¿™ç§è§£è€¦ç­–ç•¥å¤§å¤§æé«˜äº†æ•°æ®ç”Ÿæˆçš„ä¸€è‡´æ€§å’Œæ°´å°çš„å¯æ£€æµ‹æ€§ã€‚è¿›ä¸€æ­¥çš„å¹¿æ³›å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„Shallow Diffuseåœ¨ç¨³å¥æ€§å’Œä¸€è‡´æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„æ°´å°æ–¹æ³•ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/liwd190019/Shallow-Diffuse%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/liwd190019/Shallow-Diffuseå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21088v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºShallow Diffuseçš„æ–°æ°´å°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èƒ½å¤Ÿå°†ç¨³å¥ä¸”ä¸å¯è§çš„æ°´å°åµŒå…¥åˆ°æ‰©æ•£æ¨¡å‹è¾“å‡ºä¸­ã€‚ä¸å…¶ä»–åœ¨æ•´ä¸ªæ‰©æ•£é‡‡æ ·è¿‡ç¨‹ä¸­é›†æˆæ°´å°çš„æ–¹æ³•ä¸åŒï¼ŒShallow Diffuseåˆ©ç”¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä½ç»´å­ç©ºé—´ï¼Œå°†æ°´å°ä¸å›¾åƒç”Ÿæˆè¿‡ç¨‹åˆ†ç¦»ã€‚è¯¥æ–¹æ³•æ—¢å¢å¼ºäº†æ•°æ®ç”Ÿæˆçš„ä¸€è‡´æ€§ï¼Œåˆæé«˜äº†æ°´å°çš„å¯æ£€æµ‹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆå†…å®¹çš„æ™®åŠå¼•å‘äº†å…³äºè¯¯å¯¼ä¿¡æ¯å’Œç‰ˆæƒä¾µçŠ¯çš„æ‹…å¿§ï¼Œå› æ­¤æ°´å°æŠ€æœ¯å¯¹äºè¯†åˆ«AIç”Ÿæˆçš„å›¾åƒå’Œé˜²æ­¢å…¶æ»¥ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>Shallow Diffuseæ˜¯ä¸€ç§æ–°çš„æ°´å°æŠ€æœ¯ï¼Œå¯ä»¥åµŒå…¥åˆ°æ‰©æ•£æ¨¡å‹çš„è¾“å‡ºä¸­ï¼Œä¸”åµŒå…¥çš„æ°´å°æ—¢ç¨³å¥åˆä¸å¯è§ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒShallow Diffuseåˆ©ç”¨å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä½ç»´å­ç©ºé—´ï¼Œå°†æ°´å°åµŒå…¥æ­¥éª¤ä¸å›¾åƒç”Ÿæˆè¿‡ç¨‹åˆ†ç¦»ã€‚</li>
<li>è¿™ç§åˆ†ç¦»ç­–ç•¥æé«˜äº†æ•°æ®ç”Ÿæˆçš„ä¸€è‡´æ€§å’Œæ°´å°çš„å¯æ£€æµ‹æ€§ã€‚</li>
<li>Shallow Diffuseåœ¨ç¨³å¥æ€§å’Œä¸€è‡´æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„æ°´å°æ–¹æ³•ã€‚</li>
<li>è¯¥æŠ€æœ¯çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/liwd190019/Shallow-Diffuse%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/liwd190019/Shallow-Diffuseä¸Šå‘å¸ƒã€‚</a></li>
<li>è¯¥æŠ€æœ¯å¯¹äºä¿æŠ¤ç‰ˆæƒå’Œç»´æŠ¤ä¿¡æ¯çš„çœŸå®æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-157982e9b50335e134693e30a2ada37c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48b28dcaa97a5be412d19f73b755ef3f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Eliminating-Oversaturation-and-Artifacts-of-High-Guidance-Scales-in-Diffusion-Models"><a href="#Eliminating-Oversaturation-and-Artifacts-of-High-Guidance-Scales-in-Diffusion-Models" class="headerlink" title="Eliminating Oversaturation and Artifacts of High Guidance Scales in   Diffusion Models"></a>Eliminating Oversaturation and Artifacts of High Guidance Scales in   Diffusion Models</h2><p><strong>Authors:Seyedmorteza Sadat, Otmar Hilliges, Romann M. Weber</strong></p>
<p>Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this paper, we revisit the CFG update rule and introduce modifications to address this issue. We first decompose the update term in CFG into parallel and orthogonal components with respect to the conditional model prediction and observe that the parallel component primarily causes oversaturation, while the orthogonal component enhances image quality. Accordingly, we propose down-weighting the parallel component to achieve high-quality generations without oversaturation. Additionally, we draw a connection between CFG and gradient ascent and introduce a new rescaling and momentum method for the CFG update rule based on this insight. Our approach, termed adaptive projected guidance (APG), retains the quality-boosting advantages of CFG while enabling the use of higher guidance scales without oversaturation. APG is easy to implement and introduces practically no additional computational overhead to the sampling process. Through extensive experiments, we demonstrate that APG is compatible with various conditional diffusion models and samplers, leading to improved FID, recall, and saturation scores while maintaining precision comparable to CFG, making our method a superior plug-and-play alternative to standard classifier-free guidance. </p>
<blockquote>
<p>æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰å¯¹äºæé«˜æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡å’Œè¾“å…¥æ¡ä»¶ä¸æœ€ç»ˆè¾“å‡ºä¹‹é—´çš„å¯¹é½æ€§è‡³å…³é‡è¦ã€‚è™½ç„¶é€šå¸¸éœ€è¦è¾ƒé«˜çš„å¼•å¯¼å°ºåº¦æ¥å¢å¼ºè¿™äº›æ–¹é¢ï¼Œä½†å®ƒä¹Ÿä¼šå¯¼è‡´è¿‡é¥±å’Œå’Œä¸åˆ‡å®é™…çš„ä¼ªå½±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†CFGçš„æ›´æ–°è§„åˆ™ï¼Œå¹¶è¿›è¡Œäº†ä¿®æ”¹ä»¥è§£å†³æ­¤é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆå°†CFGçš„æ›´æ–°æœ¯è¯­åˆ†è§£ä¸ºç›¸å¯¹äºæ¡ä»¶æ¨¡å‹é¢„æµ‹å¹¶è¡Œå’Œæ­£äº¤çš„åˆ†é‡ï¼Œå¹¶è§‚å¯Ÿåˆ°å¹¶è¡Œåˆ†é‡ä¸»è¦å¯¼è‡´è¿‡é¥±å’Œï¼Œè€Œæ­£äº¤åˆ†é‡æé«˜äº†å›¾åƒè´¨é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡é™ä½å¹¶è¡Œåˆ†é‡çš„æƒé‡ï¼Œå®ç°äº†é«˜è´¨é‡ç”Ÿæˆè€Œä¸å‡ºç°è¿‡é¥±å’Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†CFGä¸æ¢¯åº¦ä¸Šå‡è”ç³»èµ·æ¥ï¼Œå¹¶åŸºäºè¿™ä¸€è§è§£ä¸ºCFGæ›´æ–°è§„åˆ™å¼•å…¥äº†ä¸€ç§æ–°çš„é‡æ–°ç¼©æ”¾å’ŒåŠ¨é‡æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºè‡ªé€‚åº”æŠ•å½±å¼•å¯¼ï¼ˆAPGï¼‰ï¼Œä¿ç•™äº†CFGæé«˜è´¨é‡çš„å¥½å¤„ï¼ŒåŒæ—¶èƒ½å¤Ÿåœ¨ä¸ä½¿ç”¨æ›´é«˜å¼•å¯¼å°ºåº¦çš„æƒ…å†µä¸‹å®ç°è¿‡é¥±å’Œã€‚APGæ˜“äºå®ç°ï¼Œå‡ ä¹ä¸ä¼šç»™é‡‡æ ·è¿‡ç¨‹å¢åŠ é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†APGä¸å„ç§æ¡ä»¶æ‰©æ•£æ¨¡å‹å’Œé‡‡æ ·å™¨å…¼å®¹ï¼Œåœ¨æé«˜FIDã€å¬å›ç‡å’Œé¥±å’Œåº¦å¾—åˆ†çš„åŒæ—¶ï¼Œä¿æŒä¸CFGç›¸å½“çš„ç²¾åº¦ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬çš„æ–¹æ³•æˆä¸ºæ ‡å‡†æ— åˆ†ç±»å™¨å¼•å¯¼çš„ä¸€ç§ä¼˜è¶Šçš„å¯æ’æ‹”æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02416v2">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºä¸€ç§æ”¹è¿›æ–¹æ¡ˆâ€”â€”è‡ªé€‚åº”æŠ•å½±æŒ‡å¯¼ï¼ˆAPGï¼‰ï¼Œæ—¨åœ¨è§£å†³åˆ†ç±»å™¨æŒ‡å¯¼åœ¨æ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆé«˜è´¨é‡è¾“å‡ºå’Œè¾“å…¥å¯¹é½æ—¶çš„ä¸è¶³ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œé€šè¿‡åˆ†è§£æŒ‡å¯¼æ›´æ–°è§„åˆ™ä¸­çš„å¹¶è¡Œå’Œæ­£äº¤æˆåˆ†ï¼Œå¯æœ‰æ•ˆè°ƒæ•´ç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œé¥±å’Œåº¦ï¼Œå®ç°æ›´é«˜è´¨é‡çš„ç”Ÿæˆä¸”ä¸ä¼šå‡ºç°è¿‡åº¦é¥±å’Œç°è±¡ã€‚æ–°æ–¹æ³•ç»“åˆæ¢¯åº¦ä¸Šå‡æ€è·¯ï¼Œå®æ–½ç®€ä¾¿ä¸”å‡ ä¹ä¸å¢åŠ è®¡ç®—å¼€é”€ã€‚å®éªŒè¯æ˜ï¼ŒAPGåœ¨ä¸åŒæ¡ä»¶æ‰©æ•£æ¨¡å‹å’Œé‡‡æ ·å™¨ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜FIDã€å¬å›ç‡å’Œé¥±å’Œåº¦å¾—åˆ†ï¼ŒåŒæ—¶ä¿æŒä¸CFGç›¸å½“çš„ç²¾åº¦ï¼Œæˆä¸ºæ ‡å‡†åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼çš„å“è¶Šæ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ï¼ˆCFGï¼‰å¯¹æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡å’Œè¾“å…¥ä¸è¾“å‡ºå¯¹é½è‡³å…³é‡è¦ã€‚</li>
<li>é«˜æŒ‡å¯¼å°ºåº¦è™½èƒ½æå‡ç”Ÿæˆè´¨é‡å’Œå¯¹é½åº¦ï¼Œä½†ä¼šå¯¼è‡´è¿‡åº¦é¥±å’Œå’Œä¸ç°å®çš„è‰ºæœ¯å“ã€‚</li>
<li>é€šè¿‡åˆ†è§£CFGçš„æ›´æ–°è§„åˆ™ï¼Œå‘ç°å¹¶è¡Œæˆåˆ†ä¸»è¦å¯¼è‡´è¿‡åº¦é¥±å’Œï¼Œè€Œæ­£äº¤æˆåˆ†èƒ½æé«˜å›¾åƒè´¨é‡ã€‚</li>
<li>æå‡ºè‡ªé€‚åº”æŠ•å½±æŒ‡å¯¼ï¼ˆAPGï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä¸‹è°ƒå¹¶è¡Œæˆåˆ†æƒé‡å®ç°é«˜è´¨é‡ç”Ÿæˆä¸”é¿å…è¿‡åº¦é¥±å’Œã€‚</li>
<li>APGä¸æ¢¯åº¦ä¸Šå‡ç›¸ç»“åˆï¼Œä¸ºCFGæ›´æ–°è§„åˆ™å¼•å…¥æ–°çš„é‡æ–°ç¼©æ”¾å’ŒåŠ¨é‡æ–¹æ³•ã€‚</li>
<li>APGæ–¹æ³•æ˜“äºå®æ–½ï¼Œå‡ ä¹ä¸å¢åŠ é‡‡æ ·è¿‡ç¨‹çš„è®¡ç®—å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-454fa1c7399fb082186b3313c9c245af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2526d70fa597f1af9aa79af0141cb6c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63fdb8105f5bf11bb07bc91192865f30.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Ensemble-Kalman-Diffusion-Guidance-A-Derivative-free-Method-for-Inverse-Problems"><a href="#Ensemble-Kalman-Diffusion-Guidance-A-Derivative-free-Method-for-Inverse-Problems" class="headerlink" title="Ensemble Kalman Diffusion Guidance: A Derivative-free Method for Inverse   Problems"></a>Ensemble Kalman Diffusion Guidance: A Derivative-free Method for Inverse   Problems</h2><p><strong>Authors:Hongkai Zheng, Wenda Chu, Austin Wang, Nikola Kovachki, Ricardo Baptista, Yisong Yue</strong></p>
<p>When solving inverse problems, one increasingly popular approach is to use pre-trained diffusion models as plug-and-play priors. This framework can accommodate different forward models without re-training while preserving the generative capability of diffusion models. Despite their success in many imaging inverse problems, most existing methods rely on privileged information such as derivative, pseudo-inverse, or full knowledge about the forward model. This reliance poses a substantial limitation that restricts their use in a wide range of problems where such information is unavailable, such as in many scientific applications. We propose Ensemble Kalman Diffusion Guidance (EnKG), a derivative-free approach that can solve inverse problems by only accessing forward model evaluations and a pre-trained diffusion model prior. We study the empirical effectiveness of EnKG across various inverse problems, including scientific settings such as inferring fluid flows and astronomical objects, which are highly non-linear inverse problems that often only permit black-box access to the forward model. We open-source our code at <a target="_blank" rel="noopener" href="https://github.com/devzhk/enkg-pytorch">https://github.com/devzhk/enkg-pytorch</a>. </p>
<blockquote>
<p>åœ¨è§£å†³åé—®é¢˜æ—¶ï¼Œè¶Šæ¥è¶Šæµè¡Œçš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä½œä¸ºå³æ’å³ç”¨çš„å…ˆéªŒã€‚è¯¥æ¡†æ¶å¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹é€‚åº”ä¸åŒçš„å‰å‘æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚å°½ç®¡å®ƒä»¬åœ¨è®¸å¤šæˆåƒåé—®é¢˜ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºç‰¹æƒä¿¡æ¯ï¼Œå¦‚å¯¼æ•°ã€ä¼ªé€†æˆ–å…³äºå‰å‘æ¨¡å‹çš„å…¨çŸ¥ã€‚è¿™ç§ä¾èµ–æ„æˆäº†å·¨å¤§çš„å±€é™æ€§ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å¹¿æ³›çš„é—®é¢˜ä¸­çš„ä½¿ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨æ²¡æœ‰æ­¤ç±»ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œå¦‚åœ¨è®¸å¤šç§‘å­¦åº”ç”¨ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†æ— å¯¼æ•°çš„é›†åˆå¡å°”æ›¼æ‰©æ•£å¼•å¯¼ï¼ˆEnKGï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åªéœ€è®¿é—®å‰å‘æ¨¡å‹è¯„ä¼°å’Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å…ˆéªŒæ¥è§£å†³åé—®é¢˜ã€‚æˆ‘ä»¬ç ”ç©¶äº†EnKGåœ¨å„ç§åé—®é¢˜ä¸Šçš„ç»éªŒæœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ç§‘å­¦ç¯å¢ƒä¸‹çš„æµä½“æµåŠ¨å’Œå¤©æ–‡å¯¹è±¡æ¨æ–­ç­‰é«˜åº¦éçº¿æ€§åé—®é¢˜ï¼Œè¿™äº›é—®é¢˜é€šå¸¸åªå…è®¸å¯¹å‰å‘æ¨¡å‹çš„é»‘ç®±è®¿é—®ã€‚æˆ‘ä»¬çš„ä»£ç å¼€æºåœ¨<a target="_blank" rel="noopener" href="https://github.com/devzhk/enkg-pytorch%E3%80%82">https://github.com/devzhk/enkg-pytorchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20175v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä½œä¸ºå³æ’å³ç”¨å…ˆéªŒåœ¨è§£å†³åé—®é¢˜ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå…¼å®¹ä¸åŒçš„å‰å‘æ¨¡å‹è€Œæ— éœ€é‡æ–°è®­ç»ƒï¼ŒåŒæ—¶ä¿ç•™æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚é’ˆå¯¹å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºæ´¾ç”Ÿä¿¡æ¯ã€ä¼ªé€†æˆ–å‰å‘æ¨¡å‹çš„å®Œå…¨çŸ¥è¯†ç­‰å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºæ— å¯¼æ•°çš„é›†åˆå¡å°”æ›¼æ‰©æ•£å¼•å¯¼ï¼ˆEnKGï¼‰æ–¹æ³•ï¼Œä»…é€šè¿‡è®¿é—®å‰å‘æ¨¡å‹è¯„ä¼°å’Œé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å…ˆéªŒæ¥è§£å†³åé—®é¢˜ã€‚æˆ‘ä»¬åœ¨å„ç§åé—®é¢˜ä¸Šç ”ç©¶äº†EnKGçš„å®è¯æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ç§‘å­¦è®¾ç½®å¦‚æ¨æ–­æµä½“æµåŠ¨å’Œå¤©æ–‡å¯¹è±¡ç­‰é«˜åº¦éçº¿æ€§åé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹è¢«è¶Šæ¥è¶Šå¤šåœ°ç”¨äºè§£å†³åé—®é¢˜ï¼Œä½œä¸ºå³æ’å³ç”¨å…ˆéªŒä½¿ç”¨ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹æ¡†æ¶å¯ä»¥é€‚åº”ä¸åŒçš„å‰å‘æ¨¡å‹ï¼Œæ— éœ€é‡æ–°è®­ç»ƒï¼ŒåŒæ—¶ä¿ç•™ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºæ´¾ç”Ÿä¿¡æ¯æˆ–å‰å‘æ¨¡å‹çš„å®Œå…¨çŸ¥è¯†ï¼Œè¿™åœ¨è®¸å¤šç§‘å­¦åº”ç”¨ä¸­ä¸å¯ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— å¯¼æ•°çš„é›†åˆå¡å°”æ›¼æ‰©æ•£å¼•å¯¼ï¼ˆEnKGï¼‰æ–¹æ³•æ¥è§£å†³åé—®é¢˜ã€‚</li>
<li>EnKGä»…é€šè¿‡è®¿é—®å‰å‘æ¨¡å‹è¯„ä»·å’Œæ‰©æ•£æ¨¡å‹å…ˆéªŒå·¥ä½œã€‚</li>
<li>EnKGåœ¨åŒ…æ‹¬æµä½“æµåŠ¨å’Œå¤©æ–‡å¯¹è±¡æ¨æ–­åœ¨å†…çš„é«˜åº¦éçº¿æ€§åé—®é¢˜ä¸Šå…·æœ‰å®è¯æœ‰æ•ˆæ€§ã€‚</li>
<li>å…¬å¼€äº†EnKGçš„å¼€æºä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.20175">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-50d0296dbcb1e6c9346d51093a72af10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01d4ea78e7faf7939740a8cee3e5b7cd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-for-Tabular-Data-Imputation-and-Synthetic-Data-Generation"><a href="#Diffusion-Models-for-Tabular-Data-Imputation-and-Synthetic-Data-Generation" class="headerlink" title="Diffusion Models for Tabular Data Imputation and Synthetic Data   Generation"></a>Diffusion Models for Tabular Data Imputation and Synthetic Data   Generation</h2><p><strong>Authors:Mario VillaizÃ¡n-Vallelado, Matteo Salvatori, Carlos Segura, Ioannis Arapakis</strong></p>
<p>Data imputation and data generation have important applications for many domains, like healthcare and finance, where incomplete or missing data can hinder accurate analysis and decision-making. Diffusion models have emerged as powerful generative models capable of capturing complex data distributions across various data modalities such as image, audio, and time series data. Recently, they have been also adapted to generate tabular data. In this paper, we propose a diffusion model for tabular data that introduces three key enhancements: (1) a conditioning attention mechanism, (2) an encoder-decoder transformer as the denoising network, and (3) dynamic masking. The conditioning attention mechanism is designed to improve the modelâ€™s ability to capture the relationship between the condition and synthetic data. The transformer layers help model interactions within the condition (encoder) or synthetic data (decoder), while dynamic masking enables our model to efficiently handle both missing data imputation and synthetic data generation tasks within a unified framework. We conduct a comprehensive evaluation by comparing the performance of diffusion models with transformer conditioning against state-of-the-art techniques, such as Variational Autoencoders, Generative Adversarial Networks and Diffusion Models, on benchmark datasets. Our evaluation focuses on the assessment of the generated samples with respect to three important criteria, namely: (1) Machine Learning efficiency, (2) statistical similarity, and (3) privacy risk mitigation. For the task of data imputation, we consider the efficiency of the generated samples across different levels of missing features. </p>
<blockquote>
<p>æ•°æ®è¡¥å…¨å’Œæ•°æ®ç”Ÿæˆåœ¨è®¸å¤šé¢†åŸŸéƒ½æœ‰é‡è¦åº”ç”¨ï¼Œå¦‚åŒ»ç–—å’Œé‡‘èï¼Œå…¶ä¸­ä¸å®Œæ•´æˆ–ç¼ºå¤±çš„æ•°æ®å¯èƒ½ä¼šé˜»ç¢å‡†ç¡®çš„åˆ†æå’Œå†³ç­–ã€‚æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿæ•æ‰å›¾åƒã€éŸ³é¢‘å’Œæ—¶é—´åºåˆ—æ•°æ®ç­‰ä¸åŒæ•°æ®æ¨¡æ€çš„å¤æ‚æ•°æ®åˆ†å¸ƒã€‚æœ€è¿‘ï¼Œå®ƒä»¬ä¹Ÿè¢«æ”¹ç¼–ä¸ºç”Ÿæˆè¡¨æ ¼æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè¡¨æ ¼æ•°æ®çš„æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†ä¸‰ä¸ªå…³é”®æ”¹è¿›ï¼šï¼ˆ1ï¼‰æ¡ä»¶æ³¨æ„åŠ›æœºåˆ¶ï¼Œï¼ˆ2ï¼‰ç¼–ç å™¨-è§£ç å™¨è½¬æ¢å™¨ä½œä¸ºå»å™ªç½‘ç»œï¼Œä»¥åŠï¼ˆ3ï¼‰åŠ¨æ€æ©ç ã€‚æ¡ä»¶æ³¨æ„åŠ›æœºåˆ¶æ—¨åœ¨æé«˜æ¨¡å‹æ•æ‰æ¡ä»¶ä¸åˆæˆæ•°æ®ä¹‹é—´å…³ç³»çš„èƒ½åŠ›ã€‚è½¬æ¢å™¨å±‚æœ‰åŠ©äºå¯¹æ¡ä»¶ï¼ˆç¼–ç å™¨ï¼‰æˆ–åˆæˆæ•°æ®ï¼ˆè§£ç å™¨ï¼‰å†…çš„äº¤äº’è¿›è¡Œå»ºæ¨¡ï¼Œè€ŒåŠ¨æ€æ©ç ä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ç»Ÿä¸€æ¡†æ¶å†…é«˜æ•ˆåœ°å¤„ç†ç¼ºå¤±æ•°æ®è¡¥å…¨å’Œåˆæˆæ•°æ®ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡å°†å…·æœ‰è½¬æ¢å™¨æ¡ä»¶çš„æ‰©æ•£æ¨¡å‹æ€§èƒ½ä¸æœ€æ–°æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒï¼Œè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨åŸºå‡†æ•°æ®é›†ä¸Šä¸å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ç­‰æœ€æ–°æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„è¯„ä¼°é‡ç‚¹æ˜¯æ ¹æ®ä¸‰ä¸ªé‡è¦æ ‡å‡†å¯¹ç”Ÿæˆçš„æ ·æœ¬è¿›è¡Œè¯„ä¼°ï¼Œå³ï¼šï¼ˆ1ï¼‰æœºå™¨å­¦ä¹ æ•ˆç‡ï¼Œï¼ˆ2ï¼‰ç»Ÿè®¡ç›¸ä¼¼æ€§ï¼Œï¼ˆ3ï¼‰éšç§é£é™©ç¼“è§£ã€‚å¯¹äºæ•°æ®è¡¥å…¨ä»»åŠ¡ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸åŒç¼ºå¤±ç‰¹å¾çº§åˆ«ä¸‹ç”Ÿæˆæ ·æœ¬çš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02549v2">PDF</a> 25 pages, 7 figures, 6 tables</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå¤„ç†è¡¨æ ¼æ•°æ®çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶å¼•å…¥ä¸‰å¤§å¢å¼ºæœºåˆ¶ï¼šæ¡ä»¶æ³¨æ„æœºåˆ¶ã€ç¼–ç è§£ç å™¨è½¬æ¢å™¨å’ŒåŠ¨æ€æ©ç ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ•æ‰æ¡ä»¶ä¸åˆæˆæ•°æ®ä¹‹é—´çš„å…³ç³»ï¼Œé€šè¿‡è½¬æ¢å™¨å±‚åœ¨æ¡ä»¶æˆ–åˆæˆæ•°æ®å†…éƒ¨è¿›è¡Œäº¤äº’å»ºæ¨¡ï¼Œå¹¶èƒ½åœ¨ç»Ÿä¸€æ¡†æ¶å†…å¤„ç†ç¼ºå¤±æ•°æ®å¡«å……å’Œåˆæˆæ•°æ®ç”Ÿæˆä»»åŠ¡ã€‚æ¨¡å‹ç»è¿‡å…¨é¢çš„è¯„ä¼°ï¼Œå¹¶ä¸å½“å‰æŠ€æœ¯ä¸»æµçš„æŠ€æœ¯å¦‚å˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚è¯„ä¼°çš„é‡ç‚¹æ˜¯ç”Ÿæˆçš„æ ·æœ¬åœ¨æœºå™¨å­¦ä¹ æ•ˆç‡ã€ç»Ÿè®¡ç›¸ä¼¼æ€§å’Œéšç§é£é™©ç¼“è§£æ–¹é¢çš„è¡¨ç°ã€‚å¯¹äºæ•°æ®å¡«å……ä»»åŠ¡ï¼Œæ¨¡å‹åœ¨ä¸åŒç¨‹åº¦çš„ç‰¹å¾ç¼ºå¤±ä¸‹çš„æ•ˆç‡ä¹Ÿè¢«è€ƒè™‘åœ¨å†…ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²ç”¨äºç”Ÿæˆå›¾åƒã€éŸ³é¢‘å’Œæ—¶é—´åºåˆ—æ•°æ®ç­‰å¤æ‚æ•°æ®åˆ†å¸ƒï¼Œç°åœ¨ä¹Ÿè¢«åº”ç”¨äºè¡¨æ ¼æ•°æ®çš„ç”Ÿæˆå’Œå¤„ç†ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„æ‰©æ•£æ¨¡å‹å¼•å…¥ä¸‰å¤§å¢å¼ºæœºåˆ¶ï¼šæ¡ä»¶æ³¨æ„æœºåˆ¶ã€ç¼–ç è§£ç å™¨è½¬æ¢å™¨å’ŒåŠ¨æ€æ©ç ï¼Œä»¥æ”¹å–„æ•°æ®ç”Ÿæˆå’Œå¤„ç†æ•ˆæœã€‚</li>
<li>æ¡ä»¶æ³¨æ„æœºåˆ¶æœ‰åŠ©äºæé«˜æ¨¡å‹æ•æ‰æ¡ä»¶ä¸åˆæˆæ•°æ®ä¹‹é—´å…³ç³»çš„èƒ½åŠ›ã€‚</li>
<li>ç¼–ç è§£ç å™¨è½¬æ¢å™¨æœ‰åŠ©äºåœ¨æ¡ä»¶ï¼ˆç¼–ç å™¨ï¼‰æˆ–åˆæˆæ•°æ®ï¼ˆè§£ç å™¨ï¼‰å†…éƒ¨è¿›è¡Œäº¤äº’å»ºæ¨¡ã€‚</li>
<li>åŠ¨æ€æ©ç ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å†…å¤„ç†ç¼ºå¤±æ•°æ®å¡«å……å’Œåˆæˆæ•°æ®ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>æ¨¡å‹ç»è¿‡å…¨é¢çš„è¯„ä¼°ï¼Œä¸å˜åˆ†è‡ªç¼–ç å™¨ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ‰©æ•£æ¨¡å‹ç­‰ä¸»æµæŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯„ä¼°é›†ä¸­åœ¨ç”Ÿæˆçš„æ ·æœ¬çš„æœºå™¨å­¦ä¹ æ•ˆç‡ã€ç»Ÿè®¡ç›¸ä¼¼æ€§å’Œéšç§é£é™©ç¼“è§£æ–¹é¢ï¼ŒåŒæ—¶è€ƒè™‘äº†ä¸åŒæ°´å¹³çš„ç‰¹å¾ç¼ºå¤±ä¸‹çš„æ•°æ®å¡«å……æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.02549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e5aa1e470ca782fbdf2ed7a9c3f13ca.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-971ee3a8f96ee3b15eaa632cec6e76c5.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2dfa2294bd52ebc10f97ceb0dc633c8d.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Efficiency without Compromise CLIP-aided Text-to-Image GANs with   Increased Diversity
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
