<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-06-05  LEG-SLAM Real-Time Language-Enhanced Gaussian Splatting for SLAM">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b8d64ab7bc1103f0840b78ea9e49a398.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    47 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-05-更新"><a href="#2025-06-05-更新" class="headerlink" title="2025-06-05 更新"></a>2025-06-05 更新</h1><h2 id="LEG-SLAM-Real-Time-Language-Enhanced-Gaussian-Splatting-for-SLAM"><a href="#LEG-SLAM-Real-Time-Language-Enhanced-Gaussian-Splatting-for-SLAM" class="headerlink" title="LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM"></a>LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM</h2><p><strong>Authors:Roman Titkov, Egor Zubkov, Dmitry Yudin, Jaafar Mahmoud, Malik Mohrat, Gennady Sidorov</strong></p>
<p>Modern Gaussian Splatting methods have proven highly effective for real-time photorealistic rendering of 3D scenes. However, integrating semantic information into this representation remains a significant challenge, especially in maintaining real-time performance for SLAM (Simultaneous Localization and Mapping) applications. In this work, we introduce LEG-SLAM – a novel approach that fuses an optimized Gaussian Splatting implementation with visual-language feature extraction using DINOv2 followed by a learnable feature compressor based on Principal Component Analysis, while enabling an online dense SLAM. Our method simultaneously generates high-quality photorealistic images and semantically labeled scene maps, achieving real-time scene reconstruction with more than 10 fps on the Replica dataset and 18 fps on ScanNet. Experimental results show that our approach significantly outperforms state-of-the-art methods in reconstruction speed while achieving competitive rendering quality. The proposed system eliminates the need for prior data preparation such as camera’s ego motion or pre-computed static semantic maps. With its potential applications in autonomous robotics, augmented reality, and other interactive domains, LEG-SLAM represents a significant step forward in real-time semantic 3D Gaussian-based SLAM. Project page: <a target="_blank" rel="noopener" href="https://titrom025.github.io/LEG-SLAM/">https://titrom025.github.io/LEG-SLAM/</a> </p>
<blockquote>
<p>现代的高斯绘制法已被证明对于三维场景的实时逼真渲染高度有效。然而，将语义信息集成到此表示中仍然是一个重大挑战，尤其是在保持SLAM（同时定位和地图构建）应用的实时性能方面。在这项工作中，我们介绍了LEG-SLAM——一种新颖的方法，它将优化的高斯绘制实现与基于DINOv2的视觉语言特征提取相结合，然后基于主成分分析的可学习特征压缩器，同时实现在线密集SLAM。我们的方法可以同时生成高质量逼真图像和语义标记的场景地图，在Replica数据集上以超过10帧&#x2F;秒的速度实现实时场景重建，在ScanNet上达到18帧&#x2F;秒。实验结果表明，我们的方法在重建速度上显著优于最先进的方法，同时达到有竞争力的渲染质量。所提出系统消除了对先验数据准备的需求，例如相机的自我运动或预先计算的静态语义地图。其在自主机器人、增强现实和其他交互领域具有潜在应用，LEG-SLAM代表了基于实时语义三维高斯SLAM的重大进步。项目页面：<a target="_blank" rel="noopener" href="https://titrom025.github.io/LEG-SLAM/">https://titrom025.github.io/LEG-SLAM/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03073v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了一种新的方法LEG-SLAM，它将优化的高斯Splatting实现与基于DINOv2的视觉语言特征提取相结合，再通过主成分分析的可学习特征压缩机，实现了在线密集SLAM。该方法可实时生成高质量的照片级图像和语义标记的场景地图，在Replica数据集上达到超过10帧的重建速度，在ScanNet上达到18帧。相较于现有技术，该方法在重建速度上表现出显著优势，同时保持竞争力强的渲染质量。无需预先准备数据，如相机运动或预先计算静态语义地图，LEG-SLAM在自主机器人、增强现实和其他交互领域具有广泛的应用前景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LEG-SLAM结合了优化的高斯Splatting方法和基于DINOv2的视觉语言特征提取。</li>
<li>引入可学习特征压缩机，基于主成分分析（PCA）。</li>
<li>实现了在线密集SLAM，可同时生成高质量照片级图像和语义标记的场景地图。</li>
<li>在Replica数据集和ScanNet上的实验结果表明，LEG-SLAM在重建速度上显著优于现有技术。</li>
<li>LEG-SLAM达到了实时的渲染速度，并具有竞争性的渲染质量。</li>
<li>该方法无需预先准备数据，如相机运动或预计算的静态语义地图。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03073">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3517c2739e30c94189a1fae78fcbf51b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0416ce68a9da31b0f610103765e74f14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a84e99a63ebed02f41a829538c00477b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21456ca89b95ef4fab539a6bfc7612ce.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RobustSplat-Decoupling-Densification-and-Dynamics-for-Transient-Free-3DGS"><a href="#RobustSplat-Decoupling-Densification-and-Dynamics-for-Transient-Free-3DGS" class="headerlink" title="RobustSplat: Decoupling Densification and Dynamics for Transient-Free   3DGS"></a>RobustSplat: Decoupling Densification and Dynamics for Transient-Free   3DGS</h2><p><strong>Authors:Chuanyu Fu, Yuqi Zhang, Kunbin Yao, Guanying Chen, Yuan Xiong, Chuan Huang, Shuguang Cui, Xiaochun Cao</strong></p>
<p>3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting&#x2F;cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is <a target="_blank" rel="noopener" href="https://fcyycf.github.io/RobustSplat/">https://fcyycf.github.io/RobustSplat/</a>. </p>
<blockquote>
<p>3D Gaussian Splatting（3DGS）因其在新视角合成和3D建模中的实时、逼真渲染而受到广泛关注。然而，现有方法在模拟受瞬态物体影响的场景时存在困难，导致渲染图像出现伪影。我们发现，高斯增密过程在增强场景细节捕捉的同时，无意中添加了对瞬态扰动的建模高斯，从而产生了这些伪影。为了解决这个问题，我们提出了RobustSplat，这是一种基于两种关键设计的稳健解决方案。首先，我们引入了一种延迟高斯增长策略，该策略优先优化静态场景结构，然后允许高斯分裂&#x2F;克隆，从而减轻在早期优化中对瞬态物体的过度拟合。其次，我们设计了一种级联遮罩引导方法，首先利用低分辨率特征相似性监督进行可靠的初始瞬态遮罩估计，利用其更强的语义一致性和对噪声的鲁棒性，然后过渡到高分辨率监督以实现更精确的遮罩预测。在多个具有挑战性的数据集上的广泛实验表明，我们的方法优于现有方法，清楚地证明了我们的方法的稳健性和有效性。我们的项目页面是<a target="_blank" rel="noopener" href="https://fcyycf.github.io/RobustSplat/%E3%80%82">https://fcyycf.github.io/RobustSplat/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02751v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://fcyycf.github.io/RobustSplat/">https://fcyycf.github.io/RobustSplat/</a></p>
<p><strong>Summary</strong></p>
<p>3DGS技术通过实时、逼真的渲染实现新颖的视点合成和3D建模，得到广泛关注。但现有方法在模拟受到瞬态对象影响的环境时存在挑战，导致渲染图像出现伪影。针对此问题，我们提出了RobustSplat方案，包含两大设计创新：一是引入延迟高斯增长策略，优先优化静态场景结构后才开始高斯分裂或克隆，减少早期优化对瞬态对象的过度拟合；二是设计规模级联遮罩提升策略，首先利用低分辨率特征相似性监督进行可靠的初始瞬态遮罩估计，再过渡到高分辨率监督以实现更精确的遮罩预测。实验证明，该方法在多个具有挑战的数据集上优于现有技术。更多信息可参见项目网站：<a target="_blank" rel="noopener" href="https://fcyycf.github.io/RobustSplat/">https://fcyycf.github.io/RobustSplat/</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3DGS技术广泛应用于实时、逼真的渲染，用于新颖的视点合成和3D建模。</li>
<li>现有方法在模拟受瞬态对象影响的环境时存在挑战，导致渲染图像出现伪影。</li>
<li>RobustSplat方案通过引入延迟高斯增长策略，减少早期优化对瞬态对象的过度拟合。</li>
<li>采用规模级联遮罩提升策略，利用低分辨率特征相似性监督进行可靠的初始瞬态遮罩估计，再过渡到高分辨率监督以实现更精确的遮罩预测。</li>
<li>该方法在多数据集上的表现优于现有技术。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02751">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d55d86dae3f648e30e6779059bade1a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df546257e62284077018d9ac8fe30a43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ca79ff60c49da814d65b2a67857ceb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8d64ab7bc1103f0840b78ea9e49a398.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62ee31099c9bc22520ad0ee7f216e6c5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VTGaussian-SLAM-RGBD-SLAM-for-Large-Scale-Scenes-with-Splatting-View-Tied-3D-Gaussians"><a href="#VTGaussian-SLAM-RGBD-SLAM-for-Large-Scale-Scenes-with-Splatting-View-Tied-3D-Gaussians" class="headerlink" title="VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting   View-Tied 3D Gaussians"></a>VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting   View-Tied 3D Gaussians</h2><p><strong>Authors:Pengchong Hu, Zhizhong Han</strong></p>
<p>Jointly estimating camera poses and mapping scenes from RGBD images is a fundamental task in simultaneous localization and mapping (SLAM). State-of-the-art methods employ 3D Gaussians to represent a scene, and render these Gaussians through splatting for higher efficiency and better rendering. However, these methods cannot scale up to extremely large scenes, due to the inefficient tracking and mapping strategies that need to optimize all 3D Gaussians in the limited GPU memories throughout the training to maintain the geometry and color consistency to previous RGBD observations. To resolve this issue, we propose novel tracking and mapping strategies to work with a novel 3D representation, dubbed view-tied 3D Gaussians, for RGBD SLAM systems. View-tied 3D Gaussians is a kind of simplified Gaussians, which is tied to depth pixels, without needing to learn locations, rotations, and multi-dimensional variances. Tying Gaussians to views not only significantly saves storage but also allows us to employ many more Gaussians to represent local details in the limited GPU memory. Moreover, our strategies remove the need of maintaining all Gaussians learnable throughout the training, while improving rendering quality, and tracking accuracy. We justify the effectiveness of these designs, and report better performance over the latest methods on the widely used benchmarks in terms of rendering and tracking accuracy and scalability. Please see our project page for code and videos at <a target="_blank" rel="noopener" href="https://machineperceptionlab.github.io/VTGaussian-SLAM-Project">https://machineperceptionlab.github.io/VTGaussian-SLAM-Project</a> . </p>
<blockquote>
<p>从RGBD图像中联合估计相机姿态并绘制场景是同时定位与地图构建（SLAM）中的一项基本任务。最新方法采用3D高斯来代表场景，并通过溅射（splatting）来提高效率和渲染效果。然而，这些方法无法扩展到非常大的场景，因为有限的GPU内存需要在训练过程中优化所有3D高斯来维持几何和颜色一致性到先前的RGBD观察结果，这使得跟踪和映射策略效率低下。为了解决这个问题，我们提出了新颖的跟踪和映射策略，与一种名为“视图绑定3D高斯”的新型3D表示相结合，用于RGBD SLAM系统。视图绑定3D高斯是一种简化型高斯，它与深度像素绑定，无需学习位置、旋转和多维方差。将高斯与视图绑定不仅大大节省了存储空间，而且允许我们使用更多高斯在有限的GPU内存中表示局部细节。此外，我们的策略无需在训练过程中保持所有高斯可学习，同时提高了渲染质量和跟踪精度。我们验证了这些设计的有效性，并在广泛使用的基准测试上报告了比最新方法更好的性能，涉及渲染和跟踪精度和可扩展性。有关代码和视频，请访问我们的项目页面：<a target="_blank" rel="noopener" href="https://machineperceptionlab.github.io/VTGaussian-SLAM-Project%E3%80%82">https://machineperceptionlab.github.io/VTGaussian-SLAM-Project。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02741v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对RGBD SLAM系统的视绑3D高斯（View-tied 3D Gaussians）新技术。该技术解决了大规模场景下的跟踪和映射效率问题，通过绑定深度像素的简化高斯表示方法，有效节省存储空间并提高渲染质量和跟踪精度。新技术实现了更好的性能表现，可在广泛使用的基准测试上超越最新方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了一种名为视绑3D高斯（View-tied 3D Gaussians）的新型3D表示方法，用于RGBD SLAM系统。</li>
<li>视绑3D高斯通过绑定深度像素的简化高斯表示，无需学习位置、旋转和多维方差，显著节省了存储空间。</li>
<li>新技术提高了渲染质量和跟踪精度，解决了在大规模场景下的跟踪和映射效率问题。</li>
<li>新技术在广泛使用的基准测试上实现了更好的性能表现，超越了最新方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02741">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-01e0e924e195b1d7fded50549cc28fcf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d16f5268feb569636c7abee146c1f4a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9c9b3651013c6ee4ffe7913f64aeb7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2bd686b2420158f1554198d42f51ca67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-910bbef16267eb85f0f5bd1bb4a719ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36053b9f5df9fa7fdd204afd80cd79a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf8311f118b7cbe35781bf761574bccd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="EyeNavGS-A-6-DoF-Navigation-Dataset-and-Record-n-Replay-Software-for-Real-World-3DGS-Scenes-in-VR"><a href="#EyeNavGS-A-6-DoF-Navigation-Dataset-and-Record-n-Replay-Software-for-Real-World-3DGS-Scenes-in-VR" class="headerlink" title="EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for   Real-World 3DGS Scenes in VR"></a>EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for   Real-World 3DGS Scenes in VR</h2><p><strong>Authors:Zihao Ding, Cheng-Tse Lee, Mufeng Zhu, Tao Guan, Yuan-Chun Sun, Cheng-Hsin Hsu, Yao Liu</strong></p>
<p>3D Gaussian Splatting (3DGS) is an emerging media representation that reconstructs real-world 3D scenes in high fidelity, enabling 6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However, developing and evaluating 3DGS-enabled applications and optimizing their rendering performance, require realistic user navigation data. Such data is currently unavailable for photorealistic 3DGS reconstructions of real-world scenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available 6-DoF navigation dataset featuring traces from 46 participants exploring twelve diverse, real-world 3DGS scenes. The dataset was collected at two sites, using the Meta Quest Pro headsets, recording the head pose and eye gaze data for each rendered frame during free world standing 6-DoF navigation. For each of the twelve scenes, we performed careful scene initialization to correct for scene tilt and scale, ensuring a perceptually-comfortable VR experience. We also release our open-source SIBR viewer software fork with record-and-replay functionalities and a suite of utility tools for data processing, conversion, and visualization. The EyeNavGS dataset and its accompanying software tools provide valuable resources for advancing research in 6-DoF viewport prediction, adaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The EyeNavGS dataset is available at: <a target="_blank" rel="noopener" href="https://symmru.github.io/EyeNavGS/">https://symmru.github.io/EyeNavGS/</a>. </p>
<blockquote>
<p>3D高斯拼贴（3DGS）是一种新兴媒体表示形式，能够以高保真方式重建真实世界的3D场景，从而在虚拟现实（VR）中实现六自由度（6-DoF）导航。然而，开发和评估3DGS应用程序并优化其渲染性能，需要真实的用户导航数据。目前尚无可用于真实世界场景的光现实3DGS重建的数据集。本文介绍了EyeNavGS（EyeNavGS），这是第一个公开的六自由度导航数据集，包含来自46名参与者探索的十二个不同真实世界3DGS场景的轨迹。该数据集是在两个站点使用Meta Quest Pro头戴显示器收集的，记录了在自由世界站立六自由度导航期间每个渲染帧的头部姿势和眼睛注视数据。对于十二个场景中的每一个场景，我们都进行了仔细的初始化操作来纠正场景的倾斜和比例问题，确保感知舒适的虚拟现实体验。我们还发布了开源的SIBR查看器软件分支，具有记录和回放功能以及用于数据处理、转换和可视化的实用工具套件。EyeNavGS数据集及其配套的软件工具为推进六自由度视口预测、自适应流、三维显著性以及三维GS场景的焦点渲染研究提供了宝贵的资源。EyeNavGS数据集可在以下网址获取：<a target="_blank" rel="noopener" href="https://symmru.github.io/EyeNavGS/">https://symmru.github.io/EyeNavGS/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02380v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了名为EyeNavGS的公开数据集，该数据集包含利用Meta Quest Pro头盔收集的来自真实场景的三维场景模拟中的6自由度导航数据。数据集中的场景已经经过细致的初始化处理，可用于支持多种研究和工具的开发，为推进基于3DGS场景的6自由度视图预测、自适应流传输、三维显著性检测以及焦点渲染等技术提供了宝贵的资源。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>EyeNavGS是首个公开的基于三维高斯拼贴技术的真实世界场景再现的六自由度导航数据集。</li>
<li>数据集收集了来自不同参与者在不同场景中的自由行走轨迹，并记录了头部姿态和眼睛注视数据。</li>
<li>数据集收集过程中使用了Meta Quest Pro头盔进行数据采集。</li>
<li>每个场景都经过了细致的初始化处理，以确保VR体验的视觉舒适性。</li>
<li>该数据集包含记录回放功能的开源SIBR查看器软件及其数据处理和可视化工具。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02380">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-129a7b1643288b156b9e59ef1d523072.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6248723ede6164ec65abac3c56f9222.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a6206cdc1853b61ac431bd7c1fbad0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bcb67b8ce64eb4d0dfde3ca82b8e810.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de5c5841a1f64dad2c19b10886e96a79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b87ec28b9157bdd8a647bfac9247e03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e20878cfab0c07ccdbad73e9ce328d68.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GSCodec-Studio-A-Modular-Framework-for-Gaussian-Splat-Compression"><a href="#GSCodec-Studio-A-Modular-Framework-for-Gaussian-Splat-Compression" class="headerlink" title="GSCodec Studio: A Modular Framework for Gaussian Splat Compression"></a>GSCodec Studio: A Modular Framework for Gaussian Splat Compression</h2><p><strong>Authors:Sicheng Li, Chengzhen Wu, Hao Li, Xiang Gao, Yiyi Liao, Lu Yu</strong></p>
<p>3D Gaussian Splatting and its extension to 4D dynamic scenes enable photorealistic, real-time rendering from real-world captures, positioning Gaussian Splats (GS) as a promising format for next-generation immersive media. However, their high storage requirements pose significant challenges for practical use in sharing, transmission, and storage. Despite various studies exploring GS compression from different perspectives, these efforts remain scattered across separate repositories, complicating benchmarking and the integration of best practices. To address this gap, we present GSCodec Studio, a unified and modular framework for GS reconstruction, compression, and rendering. The framework incorporates a diverse set of 3D&#x2F;4D GS reconstruction methods and GS compression techniques as modular components, facilitating flexible combinations and comprehensive comparisons. By integrating best practices from community research and our own explorations, GSCodec Studio supports the development of compact representation and compression solutions for static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec, achieving competitive rate-distortion performance in static and dynamic GS compression. The code for our framework is publicly available at <a target="_blank" rel="noopener" href="https://github.com/JasonLSC/GSCodec_Studio">https://github.com/JasonLSC/GSCodec_Studio</a> , to advance the research on Gaussian Splats compression. </p>
<blockquote>
<p>3D高斯贴片技术及其扩展到4D动态场景的能力，使得从真实世界捕捉的光照现实实时渲染成为可能，将高斯贴片（GS）定位为下一代沉浸式媒体的有前途的格式。然而，它们的高存储要求给共享、传输和存储的实际应用带来了重大挑战。尽管有许多研究从不同角度探索了GS压缩，但这些努力仍然分散在不同的存储库中，使得基准测试和最佳实践的整合变得复杂。为了解决这一空白，我们推出了GSCodec Studio，这是一个用于GS重建、压缩和渲染的统一模块化框架。该框架结合了多种3D&#x2F;4D GS重建方法和GS压缩技术作为模块化组件，促进了灵活的组合和全面的比较。通过整合社区研究和我们自己的探索中的最佳实践，GSCodec Studio支持静态和动态高斯贴图的紧凑表示和压缩解决方案的开发，即我们的静态和动态GSCodec，实现在静态和动态GS压缩中具有竞争力的速率失真性能。我们的框架代码公开在<a target="_blank" rel="noopener" href="https://github.com/JasonLSC/GSCodec_Studio">https://github.com/JasonLSC/GSCodec_Studio</a>，以推动高斯贴片压缩的研究进展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01822v1">PDF</a> Repository of the project: <a target="_blank" rel="noopener" href="https://github.com/JasonLSC/GSCodec_Studio">https://github.com/JasonLSC/GSCodec_Studio</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了三维高斯贴图（Gaussian Splats，GS）在渲染真实场景中的重要作用，并指出其在下一代沉浸式媒体中的潜力。然而，其高存储需求限制了实际应用中的共享、传输和存储。为解决此问题，本文提出了一个统一、模块化的框架——GSCodec Studio，用于GS的重建、压缩和渲染。该框架集成了多种GS重建方法和压缩技术，支持灵活组合和全面比较，实现了静态和动态高斯贴图的紧凑表示和压缩解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D Gaussian Splatting及扩展至4D动态场景为真实场景的实时渲染提供了可能，成为下一代沉浸式媒体的有前途的格式。</li>
<li>高存储需求是Gaussian Splats实际应用中的主要挑战，需要解决共享、传输和存储的问题。</li>
<li>GSCodec Studio是一个统一、模块化的框架，用于Gaussian Splats的重建、压缩和渲染。</li>
<li>GSCodec Studio集成了多种GS重建方法和压缩技术，促进了最佳实践的集成和灵活组合。</li>
<li>框架支持静态和动态高斯贴图的紧凑表示和压缩解决方案，分别实现了Static和Dynamic GSCodec。</li>
<li>GSCodec Studio的代码已公开可用，以推动Gaussian Splats压缩的研究进展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01822">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d2721667f132123c0e5d77440d471f07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20c6e446203cd2fcc254c9a6f3a4285.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59355fedc75ffdc5bd87a6dd3c241782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7aa0af0805428cae125a4202da03f72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1eefb0cf048c62b4c3b1827f3e70b59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23b86fccf9bf936ada2b3ddd50f81054.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="WoMAP-World-Models-For-Embodied-Open-Vocabulary-Object-Localization"><a href="#WoMAP-World-Models-For-Embodied-Open-Vocabulary-Object-Localization" class="headerlink" title="WoMAP: World Models For Embodied Open-Vocabulary Object Localization"></a>WoMAP: World Models For Embodied Open-Vocabulary Object Localization</h2><p><strong>Authors:Tenny Yin, Zhiting Mei, Tao Sun, Lihan Zha, Emily Zhou, Jeremy Bao, Miyu Yamane, Ola Shorinwa, Anirudha Majumdar</strong></p>
<p>Language-instructed active object localization is a critical challenge for robots, requiring efficient exploration of partially observable environments. However, state-of-the-art approaches either struggle to generalize beyond demonstration datasets (e.g., imitation learning methods) or fail to generate physically grounded actions (e.g., VLMs). To address these limitations, we introduce WoMAP (World Models for Active Perception): a recipe for training open-vocabulary object localization policies that: (i) uses a Gaussian Splatting-based real-to-sim-to-real pipeline for scalable data generation without the need for expert demonstrations, (ii) distills dense rewards signals from open-vocabulary object detectors, and (iii) leverages a latent world model for dynamics and rewards prediction to ground high-level action proposals at inference time. Rigorous simulation and hardware experiments demonstrate WoMAP’s superior performance in a broad range of zero-shot object localization tasks, with more than 9x and 2x higher success rates compared to VLM and diffusion policy baselines, respectively. Further, we show that WoMAP achieves strong generalization and sim-to-real transfer on a TidyBot. </p>
<blockquote>
<p>语言指导下的主动目标定位对于机器人来说是一个关键挑战，需要有效地探索部分可观察的环境。然而，最新方法要么难以在演示数据集之外进行推广（例如模仿学习方法），要么无法生成物理基础动作（例如视觉语言模型）。为了解决这些局限性，我们引入了WoMAP（用于主动感知的世界模型）：一个训练开放词汇目标定位策略的方法，该方法（i）使用基于高斯涂覆的真实到模拟再到真实的管道进行可扩展的数据生成，无需专家演示；（ii）从开放词汇目标检测器中提炼出密集奖励信号；（iii）利用潜在世界模型进行动力学和奖励预测，在推理时间为高级行动提议提供基础。严格的模拟和硬件实验表明，WoMAP在广泛的零样本目标定位任务中表现出卓越性能，与视觉语言模型和扩散政策基准相比，成功率分别提高了9倍和2倍。此外，我们还展示了WoMAP在TidyBot上实现了强大的泛化和模拟到现实的迁移能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01600v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这篇文本介绍了针对机器人语言指导主动物体定位的挑战，提出一种名为WoMAP（世界模型主动感知）的方法。该方法通过真实到模拟再到真实的管道进行可扩展的数据生成，无需专家演示；从开放词汇物体检测器中提炼出密集奖励信号，并利用潜在世界模型预测动力学和奖励，以在推理时实现高级动作提议的接地。在广泛的零镜头物体定位任务中，WoMAP的性能远超其他方法，如在VLM和扩散政策基准上的成功率分别提高了9倍和两倍。此外，WoMAP在TidyBot上实现了强大的泛化和模拟到现实的迁移。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WoMAP是一种用于训练开放词汇物体定位策略的方法，适用于机器人语言指导主动物体定位的挑战。</li>
<li>WoMAP通过真实到模拟再到真实的管道进行数据生成，这一方法无需专家演示，具有可扩展性。</li>
<li>WoMAP能从开放词汇物体检测器中提炼密集奖励信号。</li>
<li>WoMAP利用潜在世界模型预测动力学和奖励，实现高级动作提议的接地。</li>
<li>在零镜头物体定位任务中，WoMAP的性能显著超过其他方法，如VLM和扩散政策基准。</li>
<li>WoMAP在TidyBot上实现了强大的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01600">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a9b21985a8c9d936aa614078e9a3e48a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03be1093ba596ff9e3ab0b51ba180377.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59a02af21c324356da95be574ddec37a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26fff2fe9ff932339ab9949be4cc8bb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a687f07d3d5d633fd9fbd412214a63c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RadarSplat-Radar-Gaussian-Splatting-for-High-Fidelity-Data-Synthesis-and-3D-Reconstruction-of-Autonomous-Driving-Scenes"><a href="#RadarSplat-Radar-Gaussian-Splatting-for-High-Fidelity-Data-Synthesis-and-3D-Reconstruction-of-Autonomous-Driving-Scenes" class="headerlink" title="RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis   and 3D Reconstruction of Autonomous Driving Scenes"></a>RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis   and 3D Reconstruction of Autonomous Driving Scenes</h2><p><strong>Authors:Pou-Chun Kung, Skanda Harisha, Ram Vasudevan, Aline Eid, Katherine A. Skinner</strong></p>
<p>High-Fidelity 3D scene reconstruction plays a crucial role in autonomous driving by enabling novel data generation from existing datasets. This allows simulating safety-critical scenarios and augmenting training datasets without incurring further data collection costs. While recent advances in radiance fields have demonstrated promising results in 3D reconstruction and sensor data synthesis using cameras and LiDAR, their potential for radar remains largely unexplored. Radar is crucial for autonomous driving due to its robustness in adverse weather conditions like rain, fog, and snow, where optical sensors often struggle. Although the state-of-the-art radar-based neural representation shows promise for 3D driving scene reconstruction, it performs poorly in scenarios with significant radar noise, including receiver saturation and multipath reflection. Moreover, it is limited to synthesizing preprocessed, noise-excluded radar images, failing to address realistic radar data synthesis. To address these limitations, this paper proposes RadarSplat, which integrates Gaussian Splatting with novel radar noise modeling to enable realistic radar data synthesis and enhanced 3D reconstruction. Compared to the state-of-the-art, RadarSplat achieves superior radar image synthesis (+3.4 PSNR &#x2F; 2.6x SSIM) and improved geometric reconstruction (-40% RMSE &#x2F; 1.5x Accuracy), demonstrating its effectiveness in generating high-fidelity radar data and scene reconstruction. A project page is available at <a target="_blank" rel="noopener" href="https://umautobots.github.io/radarsplat">https://umautobots.github.io/radarsplat</a>. </p>
<blockquote>
<p>高保真三维场景重建在自动驾驶中起着至关重要的作用，它能够从现有数据集中生成新型数据。这使得能够模拟安全关键场景并扩充训练数据集，而无需承担进一步的数据收集成本。虽然最近辐射场在利用相机和激光雷达进行三维重建和传感器数据合成方面取得了有前景的结果，但它们在雷达方面的潜力仍未得到充分探索。由于雷达在雨、雾和雪等恶劣天气条件下的稳健性，使其成为自动驾驶的关键技术，而光学传感器在这些条件下往往表现挣扎。尽管基于最新雷达的神经表示在三维驾驶场景重建方面显示出希望，但在雷达噪声较大的场景中表现不佳，包括接收器饱和和多路径反射。此外，它仅限于合成预处理的、排除噪声的雷达图像，未能解决真实的雷达数据合成问题。为了解决这些局限性，本文提出了RadarSplat，它结合了高斯绘图和新颖的雷达噪声建模，以实现真实的雷达数据合成和增强的三维重建。与现有技术相比，RadarSplat实现了卓越的雷达图像合成（+ 3.4 PSNR &#x2F; 2.6x SSIM）和改进了几何重建（- 40% RMSE &#x2F; 1.5x精度），证明了其在生成高保真雷达数据和场景重建方面的有效性。项目页面可在[<a target="_blank" rel="noopener" href="https://umautobots.github.io/radarsplat]%E6%9F%A5%E7%9C%8B%E3%80%82">https://umautobots.github.io/radarsplat]查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01379v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了高保真3D场景重建对于自动驾驶的重要性，它通过现有数据集生成新型数据，模拟关键安全场景并扩充训练数据集。虽然辐射场在3D重建和传感器数据合成方面取得了进展，但其在雷达方面的潜力尚未被充分探索。文章指出雷达在恶劣天气条件下的自主性驾驶中的稳健性，如雨雪雾等。当前最先进的雷达神经网络表示在雷达噪声较大的场景中表现不佳。为解决此问题，本文提出了RadarSplat，结合高斯Splatting和新型雷达噪声建模，实现真实雷达数据合成和增强的3D重建。相较于其他方法，RadarSplat在雷达图像合成和几何重建方面表现出更优越的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高保真3D场景重建在自动驾驶中起到关键作用，能通过现有数据集生成新型数据。</li>
<li>辐射场在3D重建和传感器数据合成方面取得进展，但雷达潜力尚未被充分探索。</li>
<li>雷达在恶劣天气条件下的自主性驾驶中具有稳健性。</li>
<li>当前最先进的雷达神经网络表示在噪声较大的场景中表现不佳。</li>
<li>RadarSplat通过结合高斯Splatting和新型雷达噪声建模，实现真实雷达数据合成和增强的3D重建。</li>
<li>RadarSplat在雷达图像合成和几何重建方面表现出卓越效果，相比现有方法有所提高。</li>
<li>项目页面：<a target="_blank" rel="noopener" href="https://umautobots.github.io/radarsplat">https://umautobots.github.io/radarsplat</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01379">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-49c35cee54d1c9d97a7379a9d0b800b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0783d6926053827e9d68a27135662191.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23b8c8a6a97f47de7f2f63001db2aab4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-183a6c3e835a0260a315f6c5211f948e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a0aedf7a0e4710e2e3021290110b477.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31c3bcdbdf4eb7a56736a517524346c4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="3D-Gaussian-Splat-Vulnerabilities"><a href="#3D-Gaussian-Splat-Vulnerabilities" class="headerlink" title="3D Gaussian Splat Vulnerabilities"></a>3D Gaussian Splat Vulnerabilities</h2><p><strong>Authors:Matthew Hull, Haoyang Yang, Pratham Mehta, Mansi Phute, Aeree Cho, Haoran Wang, Matthew Lau, Wenke Lee, Willian T. Lunardi, Martin Andreoni, Polo Chau</strong></p>
<p>With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical applications, how can an adversary manipulate the scene to cause harm? We introduce CLOAK, the first attack that leverages view-dependent Gaussian appearances - colors and textures that change with viewing angle - to embed adversarial content visible only from specific viewpoints. We further demonstrate DAGGER, a targeted adversarial attack directly perturbing 3D Gaussians without access to underlying training data, deceiving multi-stage object detectors e.g., Faster R-CNN, through established methods such as projected gradient descent. These attacks highlight underexplored vulnerabilities in 3DGS, introducing a new potential threat to robotic learning for autonomous navigation and other safety-critical 3DGS applications. </p>
<blockquote>
<p>随着3D高斯喷射技术（3DGS）在关键安全应用中的广泛应用，对手如何操作场景以造成伤害？我们引入了CLOAK，这是第一种利用视角相关的高斯外观的攻击方法——颜色和纹理会随着观察角度的变化而变化，以嵌入仅在特定视角可见的对立内容。我们进一步展示了DAGGER，这是一种有针对性的对抗性攻击，可以直接干扰3D高斯，无需访问底层训练数据，通过投影梯度下降法等既定方法欺骗多阶段目标检测器（例如Faster R-CNN）。这些攻击凸显了之前未开发的针对机器人学习的安全隐患和3DGS的安全风险应用。这带来了一种新型潜在威胁，即对自主导航和安全至关重要的机器人学习领域以及其他领域的安全威胁。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00280v1">PDF</a> 4 pages, 4 figures, CVPR ‘25 Workshop on Neural Fields Beyond   Conventional Cameras</p>
<p><strong>摘要</strong><br>基于文中的核心要点进行简明阐述，具体内容如下：本文揭示了通过三维高斯地图渲染（3DGS）构建的三维场景中潜在的安全隐患问题。文中介绍了如何通过利用视角相关的高斯外观（即颜色和纹理随视角变化而变化）来嵌入对抗内容，并从特定视角展现对抗内容。此外，文章还提出了一种名为DAGGER的针对性攻击方法，直接干扰三维高斯数据，无需接触底层训练数据，就能欺骗多阶段目标检测器，如Faster R-CNN等。这些攻击凸显了三维高斯地图渲染的未开发脆弱性，对于依赖这一技术的自主导航和安全要求严格的三维场景应用造成了潜在的威胁。需要注意的是，由于安全性问题逐渐凸显，目前学界对如何利用3DGS进行攻击的研究尚处于起步阶段。这些攻击可能带来一系列风险问题，因此在未来对场景安全性进行分析和建模时需要考虑相关潜在威胁。总体而言，该文在保障安全领域提出了一种重要的安全挑战与风险点，并对如何利用和防范基于视角的高斯外观嵌入对抗性内容给出了方向性指导。随着三维技术的普及和深化应用，对场景安全性的研究将更加重要和紧迫。因此，在设计和应用相关技术时，必须高度重视安全问题并对其进行充分的考量。在实际应用场景中采取相应的防范措施以降低风险隐患是当前的首要任务。然而应对该威胁的方式也需要深入探讨和研究以形成更为完善的解决方案和措施体系以应对可能出现的各种安全隐患和挑战。总体来看本文的研究成果具有重要的研究价值和社会价值为该领域的技术研究与发展提供了新的研究思路和技术创新点对未来可能的发展产生了深远影响为该领域的科研人员提供了重要启示和指导方向有助于促进相关技术的发展与完善更好地满足社会发展需求服务于社会经济发展和人类生活水平的提高。总的来说该研究成果具有重要的理论和实践意义为相关领域的发展提供了重要的启示和指导方向具有重要的社会价值和应用前景。值得注意的是虽然该技术在军事等领域有一定的应用价值但其应用场景范围仍有待拓展和创新研发未来对该技术的研究应更加深入更加多元化以应对各种潜在的应用场景和需求。未来对于基于三维技术的对抗性内容研究将更加复杂多样其安全性和可靠性问题也将成为研究的重点之一需要更多的关注和投入以确保技术的安全和稳定发展。综上所述该研究具有重要的现实意义和潜在应用价值为相关领域的发展提供了重要的启示和指导方向并有望在未来发挥更大的作用推动相关领域的技术进步和创新发展。同时该研究也提醒我们在实际应用中应加强对相关技术的安全性和可靠性评估以确保技术的安全和稳定发展并避免潜在的安全风险和挑战。同时该研究也为我们提供了一个新的视角来审视和理解三维技术的安全性和可靠性问题为我们提供了一个重要的研究方向和思路以更好地应对未来的技术挑战和发展需求实现更加智能化安全化的技术世界推动社会的进步和发展提供更坚实的技术支撑和发展动力。”, “<strong>关键见解</strong>： </p>
<ul>
<li>介绍了利用三维高斯地图渲染（3DGS）技术的潜在安全隐患问题。 </li>
<li>利用视角相关的特性嵌入对抗内容，并从特定视角展现对抗内容的方法介绍。 </li>
<li>提出一种名为DAGGER的攻击方法，直接干扰三维高斯数据并欺骗多阶段目标检测器。 </li>
<li>高亮出三维高斯地图渲染技术脆弱性方面还未得到充分研究和开发的领域及其对安全的影响的重要性进行阐述与分析对于该技术在安全方面的实际应用进行了探讨与展望以及未来发展的潜在风险挑战进行梳理和总结进一步提醒相关领域研究者对技术应用的全面考虑和对安全问题的重视提供了对未来研究和发展方向的重要启示和思考”。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00280">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3a02f5b0ffac458e2033e923a95e2489.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b43b559dfb37676e27e8a07c1be51748.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a110f3f3e3ad132c23f1ab3b2749f8c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Adaptive-Voxelization-for-Transform-coding-of-3D-Gaussian-splatting-data"><a href="#Adaptive-Voxelization-for-Transform-coding-of-3D-Gaussian-splatting-data" class="headerlink" title="Adaptive Voxelization for Transform coding of 3D Gaussian splatting data"></a>Adaptive Voxelization for Transform coding of 3D Gaussian splatting data</h2><p><strong>Authors:Chenjunjie Wang, Shashank N. Sridhara, Eduardo Pavez, Antonio Ortega, Cheng Chang</strong></p>
<p>We present a novel compression framework for 3D Gaussian splatting (3DGS) data that leverages transform coding tools originally developed for point clouds. Contrary to existing 3DGS compression methods, our approach can produce compressed 3DGS models at multiple bitrates in a computationally efficient way. Point cloud voxelization is a discretization technique that point cloud codecs use to improve coding efficiency while enabling the use of fast transform coding algorithms. We propose an adaptive voxelization algorithm tailored to 3DGS data, to avoid the inefficiencies introduced by uniform voxelization used in point cloud codecs. We ensure the positions of larger volume Gaussians are represented at high resolution, as these significantly impact rendering quality. Meanwhile, a low-resolution representation is used for dense regions with smaller Gaussians, which have a relatively lower impact on rendering quality. This adaptive voxelization approach significantly reduces the number of Gaussians and the bitrate required to encode the 3DGS data. After voxelization, many Gaussians are moved or eliminated. Thus, we propose to fine-tune&#x2F;recolor the remaining 3DGS attributes with an initialization that can reduce the amount of retraining required. Experimental results on pre-trained datasets show that our proposed compression framework outperforms existing methods. </p>
<blockquote>
<p>我们提出了一种针对3D高斯拼接（3DGS）数据的新型压缩框架，该框架利用了点云原始开发的转换编码工具。与现有的3DGS压缩方法不同，我们的方法能够以计算高效的方式生成多个比特率的压缩3DGS模型。点云体素化是一种离散化技术，点云编解码器使用它来改进编码效率，同时启用快速转换编码算法的使用。我们针对3DGS数据提出了一种自适应体素化算法，以避免点云编解码器中使用的统一体素化所带来的低效性。我们确保大体积高斯的位置以高分辨率表示，因为这些对渲染质量有重大影响。同时，对于具有较小高斯值的密集区域，我们使用低分辨率表示，这些高斯对渲染质量的影响相对较小。这种自适应体素化方法显著减少了高斯数量和编码3DGS数据所需的比特率。体素化后，许多高斯被移动或消除。因此，我们建议使用初始化对剩余的3DGS属性进行微调&#x2F;重新着色，这样可以减少所需的重训量。在预训练数据集上的实验结果表明，我们提出的压缩框架优于现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00271v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本研究提出了一种新颖的压缩框架，用于处理三维高斯球描数据（简称“CG框架”）。通过采用专门针对点云开发的转换编码工具，实现能够在多种比特率下生成压缩的三维高斯球描模型，且计算效率高。本研究提出了针对点云编码的高效离散化技术——点云体素化。在现有点云编码器的体素化基础上，我们提出了一种针对三维高斯球描数据的自适应体素化算法，旨在避免统一体素化带来的低效问题。对于体积较大的高斯模型的位置进行高分辨率表达以提升渲染质量，而在密度高的区域采用较低分辨率的小高斯表达，这得益于它们在渲染质量上相对较低的影响。最终证明这种自适应体素化算法能够大幅度减少高斯数量以及编码三维高斯球描数据所需的比特率。对剩余三维高斯球描属性进行精细化调整，使它们在重新训练阶段更具灵活性，从而在实验测试中实现了优异的性能表现。总体而言，该研究不仅突破了传统三维高斯球描数据压缩方法的局限，而且为提高大规模数据集压缩效率和计算效率提供了新的方向。通过对比实验验证，该压缩框架性能优于现有方法。</p>
<p><strong>关键要点</strong></p>
<p>一、引入新的压缩框架处理三维高斯球描数据，首次采用针对点云的转换编码工具进行开发，生成了可支持多比特率的压缩模型。<br>二、引入点云体素化技术并优化改进其效率问题以适应三维高斯球描数据压缩需求。此自适应体素化算法允许在保留高质量渲染效果的同时降低编码复杂度。<br>三、采用精细化调整策略处理剩余三维高斯球描属性，减少重新训练的需求和复杂性。这一策略显著提升了压缩框架的性能表现。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00271">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d4047376fc6fb7e8ad299df7c222d70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15e9e733023736532faff18fd79e8f33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1d2422704b97cba2445d3a4320a3734.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be60fdd3600c8e165c12c7dbb39be630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58e72342d20bd34af676c6fea2c3acf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18f8311ca84e3bc1cb79279891686e08.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Survey-of-3D-Reconstruction-with-Event-Cameras"><a href="#A-Survey-of-3D-Reconstruction-with-Event-Cameras" class="headerlink" title="A Survey of 3D Reconstruction with Event Cameras"></a>A Survey of 3D Reconstruction with Event Cameras</h2><p><strong>Authors:Chuanzhi Xu, Haoxian Zhou, Langyi Chen, Haodong Chen, Ying Zhou, Vera Chung, Qiang Qu, Weidong Cai</strong></p>
<p>Event cameras are rapidly emerging as powerful vision sensors for 3D reconstruction, uniquely capable of asynchronously capturing per-pixel brightness changes. Compared to traditional frame-based cameras, event cameras produce sparse yet temporally dense data streams, enabling robust and accurate 3D reconstruction even under challenging conditions such as high-speed motion, low illumination, and extreme dynamic range scenarios. These capabilities offer substantial promise for transformative applications across various fields, including autonomous driving, robotics, aerial navigation, and immersive virtual reality. In this survey, we present the first comprehensive review exclusively dedicated to event-based 3D reconstruction. Existing approaches are systematically categorised based on input modality into stereo, monocular, and multimodal systems, and further classified according to reconstruction methodologies, including geometry-based techniques, deep learning approaches, and neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Within each category, methods are chronologically organised to highlight the evolution of key concepts and advancements. Furthermore, we provide a detailed summary of publicly available datasets specifically suited to event-based reconstruction tasks. Finally, we discuss significant open challenges in dataset availability, standardised evaluation, effective representation, and dynamic scene reconstruction, outlining insightful directions for future research. This survey aims to serve as an essential reference and provides a clear and motivating roadmap toward advancing the state of the art in event-driven 3D reconstruction. </p>
<blockquote>
<p>事件相机正迅速崛起，成为强大的三维重建视觉传感器，其独特之处在于能够异步捕获每个像素的亮度变化。与传统基于帧的相机相比，事件相机产生稀疏但时间密集的数据流，即使在高速运动、低光照和极端动态范围等挑战条件下，也能实现稳健和准确的三维重建。这些功能为自主驾驶、机器人技术、空中导航和沉浸式虚拟现实等各个领域的应用提供了巨大的潜力。在本文中，我们对基于事件的三维重建进行了首次全面综述。现有方法被系统地根据输入模式分为立体、单目和多模态系统，并进一步根据重建方法分类，包括基于几何的技术、深度学习方法以及神经渲染技术，如神经辐射场（NeRF）和三维高斯拼贴（3DGS）。在每个类别中，方法按时间顺序组织，以突出关键概念和进展的演变。此外，我们还详细总结了适用于基于事件重建任务的公开数据集。最后，我们讨论了数据集可用性、标准化评估、有效表示和动态场景重建等重大开放挑战，并指出了未来研究的深刻方向。本综述旨在作为重要的参考资料，为推进事件驱动三维重建的现有技术提供清晰且有激励作用的路线图。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08438v2">PDF</a> 24 pages, 16 figures, 11 tables</p>
<p><strong>Summary</strong></p>
<p>事件相机作为强大的视觉传感器，正在迅速崛起并应用于三维重建领域。与传统基于帧的相机相比，事件相机能够异步捕获像素级别的亮度变化，从而产生稀疏但时间密集的数据流。这使得事件相机能够在高速运动、低光照和极端动态范围等挑战条件下实现稳健而准确的三维重建。本文首次进行全面的事件驱动三维重建综述，对现有的方法进行了系统分类，并提供了详细的方法论概述、公开数据集汇总以及未来研究方向的挑战性讨论。旨在为推进事件驱动三维重建领域的研究提供必要的参考和激励。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>事件相机能够异步捕获像素级别的亮度变化，适用于三维重建。</li>
<li>事件相机在挑战条件下（如高速运动、低光照和极端动态范围）实现稳健准确的三维重建。</li>
<li>现有事件驱动的三维重建方法被系统分类为立体、单目和多模态系统，并基于重建方法论进行分类，包括基于几何的技术、深度学习和神经渲染技术（如神经辐射场和3D高斯喷涂）。</li>
<li>文章提供了对适用于事件驱动重建任务的公开数据集的详细汇总。</li>
<li>数据集可用性、标准化评估、有效表示和动态场景重建是当前的重大挑战。</li>
<li>文章为事件驱动的三维重建领域提供了必要的参考和激励。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08438">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c5f1922a414743e710990d217a48a161.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45a4e499c63576c2773dc896d054c193.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dfa2294bd52ebc10f97ceb0dc633c8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8222df8fb0fd815cb7db91446c9da768.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14fb24b7f525a756d3f607a817096d8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d144fa0a10fc9a6ae445aacf40c18653.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd984d2a58a75745c301a2d71f06c3d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc54dddb96a123a2bbb06fdfb7e01822.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GASP-Gaussian-Splatting-for-Physic-Based-Simulations"><a href="#GASP-Gaussian-Splatting-for-Physic-Based-Simulations" class="headerlink" title="GASP: Gaussian Splatting for Physic-Based Simulations"></a>GASP: Gaussian Splatting for Physic-Based Simulations</h2><p><strong>Authors:Piotr Borycki, Weronika Smolak, Joanna Waczyńska, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek</strong></p>
<p>Physics simulation is paramount for modeling and utilization of 3D scenes in various real-world applications. However, its integration with state-of-the-art 3D scene rendering techniques such as Gaussian Splatting (GS) remains challenging. Existing models use additional meshing mechanisms, including triangle or tetrahedron meshing, marching cubes, or cage meshes. As an alternative, we can modify the physics grounded Newtonian dynamics to align with 3D Gaussian components. Current models take the first-order approximation of a deformation map, which locally approximates the dynamics by linear transformations. In contrast, our Gaussian Splatting for Physics-Based Simulations (GASP) model uses such a map (without any modifications) and flat Gaussian distributions, which are parameterized by three points (mesh faces). Subsequently, each 3D point (mesh face node) is treated as a discrete entity within a 3D space. Consequently, the problem of modeling Gaussian components is reduced to working with 3D points. Additionally, the information on mesh faces can be used to incorporate further properties into the physics model, facilitating the use of triangles. Resulting solution can be integrated into any physics engine that can be treated as a black box. As demonstrated in our studies, the proposed model exhibits superior performance on a diverse range of benchmark datasets designed for 3D object rendering. </p>
<blockquote>
<p>物理模拟在多种现实世界应用中对3D场景的建模和利用至关重要。然而，将其与最先进的3D场景渲染技术（如高斯贴图技术）相结合仍然具有挑战性。现有模型采用额外的网格化机制，包括三角形或四面体网格化、行进立方体或笼形网格。作为替代方案，我们可以修改基于物理的牛顿动力学，使其与3D高斯组件保持一致。当前模型采用一阶变形映射近似值，通过线性变换局部近似动力学。相比之下，我们的基于物理模拟的高斯贴图模型（GASP）使用这样的映射（无需任何修改）和平坦的高斯分布，由三个点（网格面）进行参数化。随后，每个3D点（网格面节点）被视为一个三维空间内的离散实体。因此，对高斯组件进行建模的问题就简化为处理三维点的问题。此外，网格面的信息可用于将更多属性纳入物理模型，便于使用三角形。所得解决方案可以集成到任何可以视为黑箱的物理引擎中。根据我们的研究，该模型在针对3D对象渲染设计的各种基准数据集上表现出卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.05819v2">PDF</a> </p>
<p><strong>Summary</strong><br>     物理模拟在模拟和实际应用三维场景方面至关重要，但将其与最新的三维场景渲染技术（如高斯贴图）相结合仍具有挑战性。现有模型采用附加的网格化机制，如三角或四面体网格化等。本文提出的基于物理的模型通过修改牛顿动力学来与三维高斯组件对齐，不使用任何附加的网格化技术。此外，通过利用三角形网格的信息来完善物理模型性能，改进方案能够被任何物理引擎集成并展现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>物理模拟在模拟和实际应用三维场景中的重要性。</li>
<li>将物理模拟与最新的三维场景渲染技术结合具有挑战性。</li>
<li>现有模型采用附加的网格化机制（三角或四面体网格化等）。</li>
<li>本研究提出了一种基于物理模型的改进方案，该方案修改牛顿动力学以与三维高斯组件对齐，无需附加网格化技术。</li>
<li>通过利用三角形网格信息来提高物理模型性能。</li>
<li>改进方案能够集成到任何物理引擎中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.05819">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9f02ffc64b3b1739cdce0744cda4e3b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31eb1f8ab3c9f817a9221f96150b2090.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239de194f817945076cb080d0f0d891a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-595c3cbd816379332b5e21125ea04539.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17afd4ad99c26790b256530782144d41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c225355113cccea3a562a588233a444e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37c60fb61f8e27afcc21f5b582e399ca.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2dfa2294bd52ebc10f97ceb0dc633c8d.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-06-05  Efficiency without Compromise CLIP-aided Text-to-Image GANs with   Increased Diversity
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d918ae4c9b85a585569aed5c64a3cc36.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-06-05  UMA Ultra-detailed Human Avatars via Multi-level Surface Alignment
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23667.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
