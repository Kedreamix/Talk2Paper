<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-63a1bb49e374efbe2544cce7ab15d54e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-05-æ›´æ–°"><a href="#2025-06-05-æ›´æ–°" class="headerlink" title="2025-06-05 æ›´æ–°"></a>2025-06-05 æ›´æ–°</h1><h2 id="Revisiting-Continuity-of-Image-Tokens-for-Cross-domain-Few-shot-Learning"><a href="#Revisiting-Continuity-of-Image-Tokens-for-Cross-domain-Few-shot-Learning" class="headerlink" title="Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning"></a>Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning</h2><p><strong>Authors:Shuai Yi, Yixiong Zou, Yuhua Li, Ruixuan Li</strong></p>
<p>Vision Transformer (ViT) has achieved remarkable success due to its large-scale pretraining on general domains, but it still faces challenges when applying it to downstream distant domains that have only scarce training data, which gives rise to the Cross-Domain Few-Shot Learning (CDFSL) task. Inspired by Self-Attentionâ€™s insensitivity to token orders, we find an interesting phenomenon neglected in current works: disrupting the continuity of image tokens (i.e., making pixels not smoothly transited across patches) in ViT leads to a noticeable performance decline in the general (source) domain but only a marginal decrease in downstream target domains. This questions the role of image tokensâ€™ continuity in ViTâ€™s generalization under large domain gaps. In this paper, we delve into this phenomenon for an interpretation. We find continuity aids ViT in learning larger spatial patterns, which are harder to transfer than smaller ones, enlarging domain distances. Meanwhile, it implies that only smaller patterns within each patch could be transferred under extreme domain gaps. Based on this interpretation, we further propose a simple yet effective method for CDFSL that better disrupts the continuity of image tokens, encouraging the model to rely less on large patterns and more on smaller ones. Extensive experiments show the effectiveness of our method in reducing domain gaps and outperforming state-of-the-art works. Codes and models are available at <a target="_blank" rel="noopener" href="https://github.com/shuaiyi308/ReCIT">https://github.com/shuaiyi308/ReCIT</a>. </p>
<blockquote>
<p>Vision Transformerï¼ˆViTï¼‰ç”±äºåœ¨é€šç”¨é¢†åŸŸçš„å¤§è§„æ¨¡é¢„è®­ç»ƒè€Œå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨åº”ç”¨äºåªæœ‰å°‘é‡è®­ç»ƒæ•°æ®çš„ä¸‹æ¸¸è¿œè·ç¦»é¢†åŸŸæ—¶ï¼Œä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™å¼•å‘äº†è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰ä»»åŠ¡çš„å‡ºç°ã€‚å—è‡ªæ³¨æ„åŠ›å¯¹ä»¤ç‰Œé¡ºåºçš„ä¸æ•æ„Ÿæ€§çš„å¯å‘ï¼Œæˆ‘ä»¬å‘ç°å½“å‰å·¥ä½œä¸­å¿½ç•¥äº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šç ´åå›¾åƒä»¤ç‰Œçš„è¿ç»­æ€§ï¼ˆå³åœ¨ViTä¸­ä½¿åƒç´ åœ¨è¡¥ä¸ä¹‹é—´ä¸å¹³ç¨³è¿‡æ¸¡ï¼‰ä¼šå¯¼è‡´åœ¨é€šç”¨ï¼ˆæºï¼‰é¢†åŸŸæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè€Œåœ¨ä¸‹æ¸¸ç›®æ ‡é¢†åŸŸä»…å‡ºç°è½»å¾®ä¸‹é™ã€‚è¿™ä¸€ç°è±¡å¯¹å›¾åƒä»¤ç‰Œè¿ç»­æ€§åœ¨ViTè·¨è¶Šå¤§é¢†åŸŸå·®è·æ—¶çš„æ¨å¹¿ä½œç”¨æå‡ºäº†è´¨ç–‘ã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†è¿™ä¸€ç°è±¡ï¼Œä»¥å¯¹å…¶è¿›è¡Œè§£é‡Šã€‚æˆ‘ä»¬å‘ç°è¿ç»­æ€§æœ‰åŠ©äºViTå­¦ä¹ æ›´å¤§çš„ç©ºé—´æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼æ¯”å°çš„æ¨¡å¼æ›´éš¾è¿ç§»ï¼Œä»è€Œæ‰©å¤§äº†é¢†åŸŸè·ç¦»ã€‚åŒæ—¶ï¼Œå®ƒæš—ç¤ºåœ¨æç«¯é¢†åŸŸå·®å¼‚ä¸‹ï¼Œæ¯ä¸ªè¡¥ä¸å†…çš„å°æ¨¡å¼éƒ½å¯ä»¥è¿›è¡Œè¿ç§»ã€‚åŸºäºè¿™ä¸€è§£é‡Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†é’ˆå¯¹CDFSLçš„ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œæ›´å¥½åœ°ç ´åäº†å›¾åƒä»¤ç‰Œçš„è¿ç»­æ€§ï¼Œé¼“åŠ±æ¨¡å‹æ›´å°‘åœ°ä¾èµ–å¤§æ¨¡å¼ï¼Œæ›´å¤šåœ°ä¾èµ–å°æ¨¡å¼ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡å°‘é¢†åŸŸå·®è·å’Œè¶…è¶Šæœ€æ–°ä½œå“æ–¹é¢éå¸¸æœ‰æ•ˆã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shuaiyi308/ReCIT%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/shuaiyi308/ReCITä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03110v1">PDF</a> Accepted by ICML 2025(spotlight)</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†Vision Transformerï¼ˆViTï¼‰åœ¨å¤„ç†è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ï¼ˆCDFSLï¼‰æ—¶çš„å±€é™æ€§ï¼Œå‘ç°æ‰°ä¹±å›¾åƒæ ‡è®°çš„è¿ç»­æ€§åœ¨æºåŸŸæ€§èƒ½ä¸‹é™è¾ƒå°ç›®æ ‡åŸŸæ—¶ä¼šå‡ºç°æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚ç ”ç©¶æŒ‡å‡ºè¿ç»­æ€§æœ‰åŠ©äºViTå­¦ä¹ æ›´å¤§çš„ç©ºé—´æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼åœ¨è·¨åŸŸè½¬ç§»æ—¶è¾ƒéš¾è½¬ç§»ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•æ¥æ›´å¥½åœ°ç ´åå›¾åƒæ ‡è®°çš„è¿ç»­æ€§ï¼Œé¼“åŠ±æ¨¡å‹æ›´å¤šåœ°ä¾èµ–å°æ¨¡å¼è€Œéå¤§æ¨¡å¼ï¼Œä»è€Œå‡å°‘åŸŸå·®è·å¹¶è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer (ViT) åœ¨å¤„ç†è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ï¼ˆCDFSLï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æ‰°ä¹±å›¾åƒæ ‡è®°çš„è¿ç»­æ€§å¯¹ViTåœ¨æºåŸŸçš„æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œä½†å¯¹ç›®æ ‡åŸŸçš„å½±å“è¾ƒå°ã€‚</li>
<li>è¿ç»­æ€§æœ‰åŠ©äºViTå­¦ä¹ æ›´å¤§çš„ç©ºé—´æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼åœ¨è·¨åŸŸè½¬ç§»æ—¶è¾ƒéš¾è½¬ç§»ã€‚</li>
<li>æ›´å°çš„æ¨¡å¼å¯èƒ½åœ¨æç«¯è·¨åŸŸæƒ…å†µä¸‹æ›´å®¹æ˜“è¢«è½¬ç§»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•æ¥ç ´åå›¾åƒæ ‡è®°çš„è¿ç»­æ€§ï¼Œä»¥å‡å°‘å¯¹å¤§å‹æ¨¡å¼çš„ä¾èµ–å¹¶å¢åŠ å¯¹å°æ¨¡å¼çš„ä¾èµ–ã€‚</li>
<li>æ­¤æ–¹æ³•åœ¨å‡å°‘åŸŸå·®è·æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-387e7aebdffa0599834f1db44b8c9b2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46daf2da3af6abc7c96e18fa530cb24f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14a9186dc06be06f6a3d62b966ef6520.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bc3fb734959cb6804002733ef4794e4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Leveraging-Information-Retrieval-to-Enhance-Spoken-Language-Understanding-Prompts-in-Few-Shot-Learning"><a href="#Leveraging-Information-Retrieval-to-Enhance-Spoken-Language-Understanding-Prompts-in-Few-Shot-Learning" class="headerlink" title="Leveraging Information Retrieval to Enhance Spoken Language   Understanding Prompts in Few-Shot Learning"></a>Leveraging Information Retrieval to Enhance Spoken Language   Understanding Prompts in Few-Shot Learning</h2><p><strong>Authors:Pierre Lepagnol, Sahar Ghannay, Thomas Gerald, Christophe Servan, Sophie Rosset</strong></p>
<p>Understanding user queries is fundamental in many applications, such as home assistants, booking systems, or recommendations. Accordingly, it is crucial to develop accurate Spoken Language Understanding (SLU) approaches to ensure the reliability of the considered system. Current State-of-the-Art SLU techniques rely on large amounts of training data; however, only limited annotated examples are available for specific tasks or languages.   In the meantime, instruction-tuned large language models (LLMs) have shown exceptional performance on unseen tasks in a few-shot setting when provided with adequate prompts. In this work, we propose to explore example selection by leveraging Information retrieval (IR) approaches to build an enhanced prompt that is applied to an SLU task. We evaluate the effectiveness of the proposed method on several SLU benchmarks. Experimental results show that lexical IR methods significantly enhance performance without increasing prompt length. </p>
<blockquote>
<p>åœ¨è¯¸å¤šåº”ç”¨ï¼ˆå¦‚å®¶ç”¨åŠ©æ‰‹ã€é¢„è®¢ç³»ç»Ÿæˆ–æ¨èç³»ç»Ÿï¼‰ä¸­ï¼Œç†è§£ç”¨æˆ·æŸ¥è¯¢æ˜¯æ ¹æœ¬æ‰€åœ¨ã€‚å› æ­¤ï¼Œä¸ºäº†ç¡®ä¿ç›¸å…³ç³»ç»Ÿçš„å¯é æ€§ï¼Œå¼€å‘å‡†ç¡®çš„å£è¯­ç†è§£ï¼ˆSLUï¼‰æ–¹æ³•è‡³å…³é‡è¦ã€‚å½“å‰æœ€å…ˆè¿›çš„SLUæŠ€æœ¯ä¾èµ–äºå¤§é‡çš„è®­ç»ƒæ•°æ®ï¼›ç„¶è€Œï¼Œå¯¹äºç‰¹å®šä»»åŠ¡æˆ–è¯­è¨€æ¥è¯´ï¼Œå¯ç”¨çš„å¸¦æ ‡æ³¨çš„ä¾‹å­å´ç›¸å¯¹è¾ƒå°‘ã€‚åŒæ—¶ï¼Œé€šè¿‡æä¾›é€‚å½“æç¤ºï¼Œç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„ å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœªè§ä»»åŠ¡çš„å°‘é‡æ ·æœ¬åœºæ™¯ä¸­è¡¨ç°å‡ºäº†å“è¶Šæ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡åˆ©ç”¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æ–¹æ³•è¿›è¡Œç¤ºä¾‹é€‰æ‹©ï¼Œä»¥æ„å»ºç”¨äºSLUä»»åŠ¡çš„å¢å¼ºæç¤ºã€‚æˆ‘ä»¬åœ¨å¤šä¸ªSLUåŸºå‡†æµ‹è¯•ä¸Šå¯¹æ‰€ç”¨æ–¹æ³•çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯æ³•IRæ–¹æ³•å¯æ˜¾è‘—å¢å¼ºæ€§èƒ½ï¼Œè€Œæ— éœ€å¢åŠ æç¤ºé•¿åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03035v1">PDF</a> Conference paper accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºä¿¡æ¯æ£€ç´¢æ–¹æ³•çš„é€‰æ‹©æ€§ç¤ºä¾‹åœ¨å¢å¼ºè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚æ–‡ç« æå‡ºåˆ©ç”¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æ–¹æ³•æ„å»ºå¢å¼ºæç¤ºï¼Œå¹¶å°†å…¶åº”ç”¨äºè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸­çš„å£è¯­ç†è§£ï¼ˆSLUï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯æ±‡ä¿¡æ¯æ£€ç´¢æ–¹æ³•åœ¨ä¸å¢åŠ æç¤ºé•¿åº¦çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”¨æˆ·æŸ¥è¯¢ç†è§£åœ¨è®¸å¤šåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œå¦‚å®¶åº­åŠ©æ‰‹ã€é¢„è®¢ç³»ç»Ÿå’Œæ¨èç³»ç»Ÿç­‰ã€‚</li>
<li>å‡†ç¡®çš„è¯­è¨€ç†è§£æŠ€æœ¯å¯¹äºç¡®ä¿ç³»ç»Ÿçš„å¯é æ€§è‡³å…³é‡è¦ã€‚å½“å‰å…ˆè¿›çš„è‡ªç„¶è¯­è¨€ç†è§£æŠ€æœ¯ä¾èµ–äºå¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>åœ¨æœ‰é™æ ‡æ³¨ç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼ŒæŒ‡ä»¤ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°‘æ•°åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨ä¿¡æ¯æ£€ç´¢æ–¹æ³•ï¼Œæœ¬æ–‡æå‡ºäº†ç¤ºä¾‹é€‰æ‹©ä»¥å¢å¼ºè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯æ±‡ä¿¡æ¯æ£€ç´¢æ–¹æ³•åœ¨ä¸å¢åŠ æç¤ºé•¿åº¦çš„æƒ…å†µä¸‹æé«˜äº†å£è¯­ç†è§£çš„æ€§èƒ½ã€‚</li>
<li>æ­¤æ–¹æ³•åœ¨å„ç§è‡ªç„¶è¯­è¨€ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03035">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-73a00fdd24165e6819ef8215714c5283.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd36244eb9d0da7db22cfaf0be5f10d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31a25bf9e1c9bdff0bf34a0f3e57116d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-430a0e9622689bd138e2216081fcff75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c2a30f0ec3e3c8e0e8f331f7e1336b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-180c47f5bf3e9358b004cbf8e97e4280.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Auto-Annotation-from-Annotation-Guidelines-A-Benchmark-through-3D-LiDAR-Detection"><a href="#Towards-Auto-Annotation-from-Annotation-Guidelines-A-Benchmark-through-3D-LiDAR-Detection" class="headerlink" title="Towards Auto-Annotation from Annotation Guidelines: A Benchmark through   3D LiDAR Detection"></a>Towards Auto-Annotation from Annotation Guidelines: A Benchmark through   3D LiDAR Detection</h2><p><strong>Authors:Yechi Ma, Wei Hua, Shu Kong</strong></p>
<p>A crucial yet under-appreciated prerequisite in machine learning solutions for real-applications is data annotation: human annotators are hired to manually label data according to detailed, expert-crafted guidelines. This is often a laborious, tedious, and costly process. To study methods for facilitating data annotation, we introduce a new benchmark AnnoGuide: Auto-Annotation from Annotation Guidelines. It aims to evaluate automated methods for data annotation directly from expert-defined annotation guidelines, eliminating the need for manual labeling. As a case study, we repurpose the well-established nuScenes dataset, commonly used in autonomous driving research, which provides comprehensive annotation guidelines for labeling LiDAR point clouds with 3D cuboids across 18 object classes. These guidelines include a few visual examples and textual descriptions, but no labeled 3D cuboids in LiDAR data, making this a novel task of multi-modal few-shot 3D detection without 3D annotations. The advances of powerful foundation models (FMs) make AnnoGuide especially timely, as FMs offer promising tools to tackle its challenges. We employ a conceptually straightforward pipeline that (1) utilizes open-source FMs for object detection and segmentation in RGB images, (2) projects 2D detections into 3D using known camera poses, and (3) clusters LiDAR points within the frustum of each 2D detection to generate a 3D cuboid. Starting with a non-learned solution that leverages off-the-shelf FMs, we progressively refine key components and achieve significant performance improvements, boosting 3D detection mAP from 12.1 to 21.9! Nevertheless, our results highlight that AnnoGuide remains an open and challenging problem, underscoring the urgent need for developing LiDAR-based FMs. We release our code and models at GitHub: <a target="_blank" rel="noopener" href="https://annoguide.github.io/annoguide3Dbenchmark">https://annoguide.github.io/annoguide3Dbenchmark</a> </p>
<blockquote>
<p>åœ¨çœŸå®åº”ç”¨ä¸­çš„æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆä¸­ä¸€ä¸ªå…³é”®ä½†å¸¸è¢«ä½ä¼°çš„å‰ææ˜¯æ•°æ®æ ‡æ³¨ã€‚éœ€è¦é›‡ä½£äººç±»æ ‡æ³¨è€…æ ¹æ®è¯¦ç»†çš„ä¸“ä¸šåˆ¶å®šçš„æŒ‡å¯¼æ–¹é’ˆæ¥æ‰‹åŠ¨æ ‡æ³¨æ•°æ®ã€‚è¿™é€šå¸¸æ˜¯ä¸€ä¸ªç¹çã€ä¹å‘³ä¸”æˆæœ¬é«˜æ˜‚çš„è¿‡ç¨‹ã€‚ä¸ºäº†ç ”ç©¶ä¿ƒè¿›æ•°æ®æ ‡æ³¨çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•AnnoGuideï¼šç›´æ¥ä»æ ‡æ³¨æŒ‡å—è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ã€‚å®ƒçš„ç›®æ ‡æ˜¯è¯„ä¼°ç›´æ¥ä»ä¸“å®¶å®šä¹‰çš„æ ‡æ³¨æŒ‡å—è¿›è¡Œæ•°æ®æ ‡æ³¨çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œä»è€Œæ¶ˆé™¤å¯¹æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ã€‚ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å¯¹è‡ªä¸»é©¾é©¶ç ”ç©¶ä¸­å¸¸ç”¨çš„æˆç†Ÿæ•°æ®é›†nuScenesè¿›è¡Œé‡æ–°å®šä½ï¼Œè¯¥æ•°æ®é›†æä¾›äº†å…³äºæ¿€å…‰é›·è¾¾ç‚¹äº‘ä¸è·¨è¶Š18ä¸ªå¯¹è±¡ç±»åˆ«çš„ä¸‰ç»´è¾¹ç•Œæ¡†çš„è¯¦ç»†æ ‡æ³¨æŒ‡å—ã€‚è¿™äº›æŒ‡å—åŒ…æ‹¬å°‘æ•°è§†è§‰ç¤ºä¾‹å’Œæ–‡æœ¬æè¿°ï¼Œä½†æ²¡æœ‰æ¿€å…‰é›·è¾¾æ•°æ®ä¸­çš„æ ‡è®°ä¸‰ç»´è¾¹ç•Œæ¡†ï¼Œè¿™ä½¿å¾—æˆä¸ºä¸€é¡¹æ–°é¢–çš„å¤šæ¨¡æ€å°‘æ ·æœ¬ä¸‰ç»´æ£€æµ‹ä»»åŠ¡ä½†ä¸æ¶‰åŠä¸‰ç»´æ³¨é‡Šã€‚å¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„å‘å±•ä½¿AnnoGuideå°¤ä¸ºåŠæ—¶ï¼Œå› ä¸ºFMsæä¾›äº†è§£å†³å…¶æŒ‘æˆ˜çš„æœ‰å‰é€”çš„å·¥å…·ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ªæ¦‚å¿µä¸Šç®€å•çš„ç®¡é“ï¼Œè¯¥ç®¡é“ï¼ˆ1ï¼‰åˆ©ç”¨ç”¨äºå¯¹è±¡æ£€æµ‹å’Œåˆ†å‰²çš„å¼€æºFMsåœ¨RGBå›¾åƒä¸­ï¼Œï¼ˆ2ï¼‰ä½¿ç”¨å·²çŸ¥çš„ç›¸æœºå§¿æ€å°†äºŒç»´æ£€æµ‹æŠ•å½±åˆ°ä¸‰ç»´ç©ºé—´ï¼Œï¼ˆ3ï¼‰åœ¨æ¯ä¸ªäºŒç»´æ£€æµ‹çš„è§†é”¥å†…å¯¹æ¿€å…‰é›·è¾¾ç‚¹è¿›è¡Œèšç±»ä»¥ç”Ÿæˆä¸‰ç»´è¾¹ç•Œæ¡†ã€‚ä»ä¸€ä¸ªåˆ©ç”¨ç°æˆçš„FMsçš„éå­¦ä¹ è§£å†³æ–¹æ¡ˆå¼€å§‹ï¼Œæˆ‘ä»¬é€æ­¥æ”¹è¿›å…³é”®ç»„ä»¶å¹¶å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°†ä¸‰ç»´æ£€æµ‹mAPä»12.1æé«˜åˆ°21.9ï¼å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬çš„ç»“æœçªæ˜¾å‡ºAnnoGuideä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå¼ºè°ƒè¿«åˆ‡éœ€è¦åœ¨æ¿€å…‰é›·è¾¾åŸºç¡€ä¸Šå¼€å‘FMsã€‚æˆ‘ä»¬åœ¨GitHubä¸Šå‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹ï¼š<a target="_blank" rel="noopener" href="https://annoguide.github.io/annoguide3Dbenchmark">https://annoguide.github.io/annoguide3Dbenchmark</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02914v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆåœ¨å®é™…åº”ç”¨ä¸­çš„ä¸€ä¸ªé‡è¦ä½†è¢«å¿½è§†çš„å‰ææ˜¯æ•°æ®æ ‡æ³¨ï¼Œéœ€è¦äººå·¥æ ¹æ®è¯¦ç»†çš„ä¸“å®¶åˆ¶å®šçš„æŒ‡å—è¿›è¡Œæ ‡æ³¨ï¼Œè¿™æ˜¯ä¸€ä¸ªç¹çä¸”æˆæœ¬é«˜æ˜‚çš„è¿‡ç¨‹ã€‚ä¸ºç®€åŒ–æ•°æ®æ ‡æ³¨è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„åŸºå‡†æµ‹è¯•AnnoGuideï¼šç›´æ¥ä»æ ‡æ³¨æŒ‡å—è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ã€‚ä»¥è‡ªåŠ¨é©¾é©¶ç ”ç©¶ä¸­å¸¸ç”¨çš„nuScenesæ•°æ®é›†ä¸ºä¾‹ï¼Œå®ƒåœ¨LiDARç‚¹äº‘ä¸Šä½¿ç”¨18ç±»å¯¹è±¡çš„3Dæ¡†æä¾›å…¨é¢çš„æ ‡æ³¨æŒ‡å—ï¼Œä½†æ²¡æœ‰æä¾›LiDARæ•°æ®çš„3Dæ ‡æ³¨ã€‚åˆ©ç”¨å¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç®€å•çš„ç®¡é“å°†RGBå›¾åƒä¸­çš„å¯¹è±¡æ£€æµ‹å’Œåˆ†å‰²ç”¨äºLiDARç‚¹äº‘ä¸­çš„3Dæ£€æµ‹ï¼Œé€æ­¥æ”¹è¿›å…³é”®ç»„ä»¶å¹¶å–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°†3Dæ£€æµ‹çš„mAPä»12.1æå‡è‡³21.9ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†AnnoGuideä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œè¿«åˆ‡éœ€è¦å¼€å‘åŸºäºLiDARçš„åŸºç¡€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®æ ‡æ³¨æ˜¯æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆå®é™…åº”ç”¨ä¸­çš„å…³é”®å‰æï¼Œä½†å¸¸è¢«å¿½è§†ã€‚</li>
<li>æ•°æ®æ ‡æ³¨æ˜¯ä¸€ä¸ªç¹çã€è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚çš„è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥æ–°çš„åŸºå‡†æµ‹è¯•AnnoGuideï¼Œæ—¨åœ¨ä»æ ‡æ³¨æŒ‡å—ç›´æ¥è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ï¼Œç®€åŒ–æµç¨‹ã€‚</li>
<li>ä»¥nuScenesæ•°æ®é›†ä¸ºä¾‹ï¼Œå±•ç¤ºåœ¨LiDARç‚¹äº‘ä¸Šçš„å¤šæ¨¡æ€å°‘æ ·æœ¬3Dæ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>åˆ©ç”¨åŸºç¡€æ¨¡å‹é‡‡ç”¨ç®€å•ç®¡é“å®ç°RGBå›¾åƒä¸­çš„å¯¹è±¡æ£€æµ‹å’Œåˆ†å‰²ï¼Œå¹¶å°†å…¶åº”ç”¨äºLiDARç‚¹äº‘ä¸­çš„3Dæ£€æµ‹ã€‚</li>
<li>é€šè¿‡é€æ­¥æ”¹è¿›å…³é”®ç»„ä»¶ï¼Œæ˜¾è‘—æå‡æ€§èƒ½ï¼Œå°†3Dæ£€æµ‹çš„mAPä»12.1æå‡è‡³21.9ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-beb749f5496ca634b5621e8858d7ad1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fbd764fa2eabe219ee3a96f33a7ebd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6496905386b6746d94b7b43c617625c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23b1b770d2391d33bf5866bfced8e5ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e65c08266d414725bf7d2fd219469438.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Token-and-Span-Classification-for-Entity-Recognition-in-French-Historical-Encyclopedias"><a href="#Token-and-Span-Classification-for-Entity-Recognition-in-French-Historical-Encyclopedias" class="headerlink" title="Token and Span Classification for Entity Recognition in French   Historical Encyclopedias"></a>Token and Span Classification for Entity Recognition in French   Historical Encyclopedias</h2><p><strong>Authors:Ludovic Moncla, HÃ©di Zeghidi</strong></p>
<p>Named Entity Recognition (NER) in historical texts presents unique challenges due to non-standardized language, archaic orthography, and nested or overlapping entities. This study benchmarks a diverse set of NER approaches, ranging from classical Conditional Random Fields (CRFs) and spaCy-based models to transformer-based architectures such as CamemBERT and sequence-labeling models like Flair. Experiments are conducted on the GeoEDdA dataset, a richly annotated corpus derived from 18th-century French encyclopedias. We propose framing NER as both token-level and span-level classification to accommodate complex nested entity structures typical of historical documents. Additionally, we evaluate the emerging potential of few-shot prompting with generative language models for low-resource scenarios. Our results demonstrate that while transformer-based models achieve state-of-the-art performance, especially on nested entities, generative models offer promising alternatives when labeled data are scarce. The study highlights ongoing challenges in historical NER and suggests avenues for hybrid approaches combining symbolic and neural methods to better capture the intricacies of early modern French text. </p>
<blockquote>
<p>åœ¨å†å²æ–‡æœ¬ä¸­çš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰é¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºè¯­è¨€éæ ‡å‡†åŒ–ã€å¤è€çš„æ­£å­—æ³•ä»¥åŠåµŒå¥—æˆ–é‡å çš„å®ä½“ã€‚æœ¬ç ”ç©¶å¯¹ä¸€ç³»åˆ—NERæ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›æ–¹æ³•åŒ…æ‹¬ç»å…¸çš„åŸºäºæ¡ä»¶éšæœºåœºï¼ˆCRFï¼‰å’ŒspaCyæ¨¡å‹çš„æ–¹æ³•ï¼Œä»¥åŠåŸºäºè½¬æ¢å™¨çš„æ¶æ„ï¼ˆå¦‚CamemBERTï¼‰å’Œåºåˆ—æ ‡ç­¾æ¨¡å‹ï¼ˆå¦‚Flairï¼‰ã€‚å®éªŒæ˜¯åœ¨GeoEDdAæ•°æ®é›†ä¸Šè¿›è¡Œçš„ï¼Œè¯¥æ•°æ®é›†æ˜¯ä»18ä¸–çºªæ³•å›½ç™¾ç§‘å…¨ä¹¦æ´¾ç”Ÿçš„ä¸°å¯Œæ³¨é‡Šè¯­æ–™åº“ã€‚æˆ‘ä»¬æå‡ºå°†NERè§†ä¸ºè¯çº§å’Œè·¨åº¦çº§çš„åˆ†ç±»ï¼Œä»¥é€‚åº”å†å²æ–‡æ¡£ä¸­å…¸å‹çš„å¤æ‚åµŒå¥—å®ä½“ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†åœ¨ä½èµ„æºæƒ…å†µä¸‹ç”Ÿæˆè¯­è¨€æ¨¡å‹çš„å°‘é‡æç¤ºçš„æ½œåœ¨æ–°å…´èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè™½ç„¶åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹åœ¨åµŒå¥—å®ä½“æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†åœ¨æ ‡è®°æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆæ¨¡å‹æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶çªå‡ºäº†å†å²NERä¸­çš„æŒç»­æŒ‘æˆ˜ï¼Œå¹¶å»ºè®®ç»“åˆç¬¦å·æ–¹æ³•å’Œç¥ç»æ–¹æ³•çš„æ··åˆæ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°æ•æ‰æ—©æœŸç°ä»£æ³•è¯­æ–‡æœ¬çš„å¤æ‚æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02872v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å†å²æ–‡æœ¬ä¸­è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éæ ‡å‡†è¯­è¨€ã€å¤è€çš„æ­£å­—æ³•ä»¥åŠåµŒå¥—æˆ–é‡å çš„å®ä½“ç­‰é—®é¢˜ã€‚æ–‡ç« è¯„ä¼°äº†ä¸€ç³»åˆ—NERæ–¹æ³•ï¼Œä»ä¼ ç»Ÿçš„æ¡ä»¶éšæœºåœºï¼ˆCRFï¼‰å’ŒspaCyæ¨¡å‹åˆ°åŸºäºå˜å‹å™¨çš„æ¶æ„ï¼ˆå¦‚CamemBERTï¼‰å’Œåºåˆ—æ ‡ç­¾æ¨¡å‹ï¼ˆå¦‚Flairï¼‰ã€‚åœ¨GeoEDdAæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºå˜å‹å™¨çš„æ¨¡å‹åœ¨è¯†åˆ«åµŒå¥—å®ä½“æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè€Œç”Ÿæˆæ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç ”ç©¶å¼ºè°ƒäº†å†å²æ–‡æœ¬NERçš„å½“å‰æŒ‘æˆ˜ï¼Œå¹¶å»ºè®®ç»“åˆç¬¦å·æ–¹æ³•å’Œç¥ç»ç½‘ç»œæ–¹æ³•çš„æ··åˆæ–¹æ³•ï¼Œä»¥æ›´å¥½åœ°æ•æ‰æ—©æœŸç°ä»£æ³•è¯­æ–‡æœ¬çš„å¤æ‚æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†å²æ–‡æœ¬NERé¢ä¸´éæ ‡å‡†è¯­è¨€ã€å¤è€æ­£å­—æ³•å’ŒåµŒå¥—å®ä½“çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯„ä¼°äº†å¤šç§NERæ–¹æ³•ï¼ŒåŒ…æ‹¬æ¡ä»¶éšæœºåœºã€spaCyæ¨¡å‹ã€åŸºäºå˜å‹å™¨çš„æ¶æ„å’Œåºåˆ—æ ‡ç­¾æ¨¡å‹ã€‚</li>
<li>åŸºäºå˜å‹å™¨çš„æ¨¡å‹åœ¨è¯†åˆ«åµŒå¥—å®ä½“æ–¹é¢è¡¨ç°å‡ºæœ€æ–°æ°´å¹³çš„æŠ€æœ¯ã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>å†å²æ–‡æœ¬NERä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ··åˆæ–¹æ³•ç»“åˆç¬¦å·æ–¹æ³•å’Œç¥ç»ç½‘ç»œæ–¹æ³•å¯èƒ½æ›´å¥½åœ°æ•æ‰æ—©æœŸç°ä»£æ³•è¯­æ–‡æœ¬çš„å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9e4ece6c905358b015c915f55d2308b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-005597afdf578d433391a2f9102cfab1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bb82690dec318d7175123b35da609e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-107a3fa1870a94700a16a2a9706cbfbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a835b815620bbda535d5b2b85c7a6eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6c27f22bfeda4de47020de914b9d499.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d934f767bc23c81a30461554a31d1eb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Random-Registers-for-Cross-Domain-Few-Shot-Learning"><a href="#Random-Registers-for-Cross-Domain-Few-Shot-Learning" class="headerlink" title="Random Registers for Cross-Domain Few-Shot Learning"></a>Random Registers for Cross-Domain Few-Shot Learning</h2><p><strong>Authors:Shuai Yi, Yixiong Zou, Yuhua Li, Ruixuan Li</strong></p>
<p>Cross-domain few-shot learning (CDFSL) aims to transfer knowledge from a data-sufficient source domain to data-scarce target domains. Although Vision Transformer (ViT) has shown superior capability in many vision tasks, its transferability against huge domain gaps in CDFSL is still under-explored. In this paper, we find an intriguing phenomenon: during the source-domain training, prompt tuning, as a common way to train ViT, could be harmful for the generalization of ViT in target domains, but setting them to random noises (i.e., random registers) could consistently improve target-domain performance. We then delve into this phenomenon for an interpretation. We find that learnable prompts capture domain information during the training on the source dataset, which views irrelevant visual patterns as vital cues for recognition. This can be viewed as a kind of overfitting and increases the sharpness of the loss landscapes. In contrast, random registers are essentially a novel way of perturbing attention for the sharpness-aware minimization, which helps the model find a flattened minimum in loss landscapes, increasing the transferability. Based on this phenomenon and interpretation, we further propose a simple but effective approach for CDFSL to enhance the perturbation on attention maps by adding random registers on the semantic regions of image tokens, improving the effectiveness and efficiency of random registers. Extensive experiments on four benchmarks validate our rationale and state-of-the-art performance. Codes and models are available at <a target="_blank" rel="noopener" href="https://github.com/shuaiyi308/REAP">https://github.com/shuaiyi308/REAP</a>. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰æ—¨åœ¨å°†çŸ¥è¯†ä»æ•°æ®å……è¶³æ¥æºåŸŸè½¬ç§»åˆ°æ•°æ®ç¨€ç¼ºç›®æ ‡åŸŸã€‚å°½ç®¡è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰åœ¨è®¸å¤šè§†è§‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶åœ¨CDFSLä¸­é¢å¯¹å·¨å¤§åŸŸå·®è·çš„å¯è½¬ç§»æ€§ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šåœ¨æºåŸŸè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½œä¸ºè®­ç»ƒViTçš„ä¸€ç§å¸¸è§æ–¹æ³•ï¼Œæç¤ºè°ƒæ•´å¯èƒ½ä¼šæŸå®³ViTåœ¨ç›®æ ‡åŸŸä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å°†å…¶è®¾ç½®ä¸ºéšæœºå™ªå£°ï¼ˆå³éšæœºå¯„å­˜å™¨ï¼‰å¯ä»¥æŒç»­æé«˜ç›®æ ‡åŸŸçš„æ€§èƒ½ã€‚æˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†è¿™ä¸€ç°è±¡å¹¶è¿›è¡Œäº†è§£é‡Šã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨æºæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œå¯å­¦ä¹ çš„æç¤ºä¼šæ•è·åŸŸä¿¡æ¯ï¼Œå°†ä¸ç›¸å…³çš„è§†è§‰æ¨¡å¼è§†ä¸ºè¯†åˆ«çš„å…³é”®çº¿ç´¢ã€‚è¿™å¯ä»¥è¢«è§†ä¸ºä¸€ç§è¿‡åº¦æ‹Ÿåˆï¼Œå¹¶å¢åŠ äº†æŸå¤±æ™¯è§‚çš„å°–é”åº¦ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œéšæœºå¯„å­˜å™¨æœ¬è´¨ä¸Šæ˜¯é€šè¿‡å¯¹æ³¨æ„åŠ›è¿›è¡Œæ‰°åŠ¨æ¥å®ç°é”åº¦æ„ŸçŸ¥æœ€å°åŒ–çš„ä¸€ç§æ–°æ–¹æ³•ï¼Œè¿™æœ‰åŠ©äºæ¨¡å‹åœ¨æŸå¤±æ™¯è§‚ä¸­æ‰¾åˆ°ä¸€ä¸ªå¹³å¦çš„æœ€å°å€¼ï¼Œä»è€Œæé«˜å¯è½¬ç§»æ€§ã€‚åŸºäºè¿™ç§ç°è±¡å’Œè§£é‡Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä¸ºCDFSLæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨å›¾åƒæ ‡è®°çš„è¯­ä¹‰åŒºåŸŸä¸Šæ·»åŠ éšæœºå¯„å­˜å™¨æ¥å¢åŠ æ³¨æ„åŠ›å›¾çš„æ‰°åŠ¨ï¼Œä»è€Œæé«˜éšæœºå¯„å­˜å™¨çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„åˆç†æ€§å¹¶è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shuaiyi308/REAP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/shuaiyi308/REAPä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02843v1">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰ä¸­ï¼Œä½¿ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ—¶é‡åˆ°çš„ä¸€ä¸ªæœ‰è¶£ç°è±¡ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æºåŸŸè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¸¸è§çš„æç¤ºè°ƒæ•´æ–¹å¼å¯èƒ½å¯¹ç›®æ ‡åŸŸçš„æ³›åŒ–èƒ½åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œè€Œå°†è®¾ç½®è½¬ä¸ºéšæœºå™ªå£°ï¼ˆå³éšæœºå¯„å­˜å™¨ï¼‰èƒ½å¤ŸæŒç»­æé«˜ç›®æ ‡åŸŸæ€§èƒ½ã€‚å¯¹æ­¤ç°è±¡è¿›è¡Œæ·±å…¥äº†è§£åå‘ç°ï¼Œå¯å­¦ä¹ çš„æç¤ºåœ¨æºæ•°æ®é›†è®­ç»ƒæ—¶ä¼šæ•è·åŸŸä¿¡æ¯ï¼Œå°†ä¸ç›¸å…³çš„è§†è§‰æ¨¡å¼è§†ä¸ºé‡è¦è¯†åˆ«çº¿ç´¢ï¼Œé€ æˆè¿‡åº¦æ‹Ÿåˆå’ŒæŸå¤±æ™¯è§‚çš„å°–é”åŒ–ã€‚ç›¸åï¼Œéšæœºå¯„å­˜å™¨æœ¬è´¨ä¸Šæ˜¯ä¸€ç§æ‰°åŠ¨æ³¨æ„åŠ›æœºåˆ¶çš„æ–°æ–¹æ³•ï¼Œæœ‰åŠ©äºæ¨¡å‹åœ¨æŸå¤±æ™¯è§‚ä¸­æ‰¾åˆ°å¹³å¦æœ€å°å€¼ï¼Œæé«˜è¿ç§»èƒ½åŠ›ã€‚åŸºäºæ­¤ç°è±¡å’Œè§£é‡Šï¼Œæ–‡ç« è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•æ¥å¢å¼ºCDFSLä¸­çš„æ³¨æ„åŠ›æ˜ å°„æ‰°åŠ¨ï¼Œé€šè¿‡åœ¨å›¾åƒæ ‡è®°çš„è¯­ä¹‰åŒºåŸŸä¸Šæ·»åŠ éšæœºå¯„å­˜å™¨æ¥æé«˜å…¶æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ç»è¿‡å››ä¸ªåŸºå‡†æµ‹è¯•çš„å®éªŒéªŒè¯ï¼Œè¯¥ç ”ç©¶è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰æ—¨åœ¨ä»æ•°æ®å……è¶³çš„æºåŸŸè¿ç§»çŸ¥è¯†åˆ°æ•°æ®ç¨€ç¼ºçš„ç›®æ ‡åŸŸã€‚</li>
<li>è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰åœ¨CDFSLä¸­çš„è¿ç§»èƒ½åŠ›å°šæœªå®Œå…¨æ¢ç´¢ã€‚</li>
<li>æç¤ºè°ƒæ•´åœ¨æºåŸŸè®­ç»ƒä¸­å¯èƒ½å¯¹ViTåœ¨ç›®æ ‡åŸŸçš„æ³›åŒ–èƒ½åŠ›äº§ç”Ÿè´Ÿé¢å½±å“ã€‚</li>
<li>éšæœºå™ªå£°ï¼ˆéšæœºå¯„å­˜å™¨ï¼‰èƒ½å¤Ÿæé«˜ç›®æ ‡åŸŸæ€§èƒ½ã€‚</li>
<li>å¯å­¦ä¹ çš„æç¤ºä¼šæ•è·æºæ•°æ®é›†ä¸­çš„åŸŸä¿¡æ¯ï¼Œå¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆå’ŒæŸå¤±æ™¯è§‚çš„å°–é”åŒ–ã€‚</li>
<li>éšæœºå¯„å­˜å™¨é€šè¿‡æ‰°åŠ¨æ³¨æ„åŠ›æœºåˆ¶å¸®åŠ©æ¨¡å‹æ‰¾åˆ°æŸå¤±æ™¯è§‚ä¸­çš„å¹³å¦æœ€å°å€¼ï¼Œæé«˜è¿ç§»èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02843">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7aec4b4cb3886684cfb8c3ee27a47343.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba799daefa60168611660a07c87de0f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8876359eaf4c9ddfe33f6de1b6bb1528.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb465ff09555a14b2e7c8c641d0818f8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Learned-Cost-Model-based-Cross-engine-Optimizer-for-SQL-Workloads"><a href="#A-Learned-Cost-Model-based-Cross-engine-Optimizer-for-SQL-Workloads" class="headerlink" title="A Learned Cost Model-based Cross-engine Optimizer for SQL Workloads"></a>A Learned Cost Model-based Cross-engine Optimizer for SQL Workloads</h2><p><strong>Authors:AndrÃ¡s Strausz, Niels Pardon, Ioana Giurgiu</strong></p>
<p>Lakehouse systems enable the same data to be queried with multiple execution engines. However, selecting the engine best suited to run a SQL query still requires a priori knowledge of the query computational requirements and an engine capability, a complex and manual task that only becomes more difficult with the emergence of new engines and workloads. In this paper, we address this limitation by proposing a cross-engine optimizer that can automate engine selection for diverse SQL queries through a learned cost model. Optimized with hints, a query plan is used for query cost prediction and routing. Cost prediction is formulated as a multi-task learning problem, and multiple predictor heads, corresponding to different engines and provisionings, are used in the model architecture. This eliminates the need to train engine-specific models and allows the flexible addition of new engines at a minimal fine-tuning cost. Results on various databases and engines show that using a query optimized logical plan for cost estimation decreases the average Q-error by even 12.6% over using unoptimized plans as input. Moreover, the proposed cross-engine optimizer reduces the total workload runtime by up to 25.2% in a zero-shot setting and 30.4% in a few-shot setting when compared to random routing. </p>
<blockquote>
<p>æ¹–ä»“ç³»ç»Ÿå…è®¸ä½¿ç”¨å¤šä¸ªæ‰§è¡Œå¼•æ“æŸ¥è¯¢ç›¸åŒçš„æ•°æ®ã€‚ç„¶è€Œï¼Œé€‰æ‹©æœ€é€‚åˆè¿è¡ŒSQLæŸ¥è¯¢çš„å¼•æ“ä»ç„¶éœ€è¦é¢„å…ˆäº†è§£æŸ¥è¯¢è®¡ç®—è¦æ±‚ä»¥åŠå¼•æ“èƒ½åŠ›ï¼Œè¿™æ˜¯ä¸€é¡¹éšç€æ–°å¼•æ“å’Œå·¥ä½œè´Ÿè½½çš„å‡ºç°è€Œå˜å¾—è¶Šæ¥è¶Šå¤æ‚çš„æ‰‹åŠ¨ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§è·¨å¼•æ“ä¼˜åŒ–å™¨æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œè¯¥ä¼˜åŒ–å™¨å¯ä»¥é€šè¿‡å­¦ä¹ æˆæœ¬æ¨¡å‹æ¥è‡ªåŠ¨ä¸ºå„ç§SQLæŸ¥è¯¢é€‰æ‹©å¼•æ“ã€‚è¯¥ä¼˜åŒ–å™¨é€šè¿‡æç¤ºè¿›è¡Œä¼˜åŒ–ï¼Œä½¿ç”¨æŸ¥è¯¢è®¡åˆ’è¿›è¡ŒæŸ¥è¯¢æˆæœ¬é¢„æµ‹å’Œè·¯ç”±ã€‚æˆæœ¬é¢„æµ‹è¢«åˆ¶å®šä¸ºå¤šä»»åŠ¡å­¦ä¹ é—®é¢˜ï¼Œæ¨¡å‹æ¶æ„ä¸­ä½¿ç”¨äº†ä¸ä¸åŒå¼•æ“å’Œé…ç½®ç›¸å¯¹åº”çš„å¤šä¸ªé¢„æµ‹å™¨å¤´ã€‚è¿™æ¶ˆé™¤äº†éœ€è¦è®­ç»ƒç‰¹å®šäºå¼•æ“çš„æ¨¡å‹çš„éœ€æ±‚ï¼Œå¹¶å…è®¸çµæ´»åœ°æ·»åŠ æ–°å¼•æ“ï¼Œè€Œåªéœ€æœ€å°çš„å¾®è°ƒæˆæœ¬ã€‚åœ¨å„ç§æ•°æ®åº“å’Œå¼•æ“ä¸Šçš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç»è¿‡ä¼˜åŒ–çš„æŸ¥è¯¢é€»è¾‘è®¡åˆ’è¿›è¡Œæˆæœ¬ä¼°è®¡ï¼Œå¯ä»¥å°†å¹³å‡Qè¯¯å·®é™ä½12.6%ï¼Œè¶…è¿‡ä½¿ç”¨æœªä¼˜åŒ–çš„è®¡åˆ’ä½œä¸ºè¾“å…¥ã€‚æ­¤å¤–ï¼Œä¸éšæœºè·¯ç”±ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„è·¨å¼•æ“ä¼˜åŒ–å™¨åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å‡å°‘äº†é«˜è¾¾25.2%çš„æ€»å·¥ä½œé‡è¿è¡Œæ—¶é—´ï¼Œåœ¨å°‘æ•°æ ·æœ¬è®¾ç½®ä¸‹å‡å°‘äº†30.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02802v1">PDF</a> 6 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨å¼•æ“ä¼˜åŒ–å™¨ï¼Œé€šè¿‡é‡‡ç”¨å­¦ä¹ æˆæœ¬æ¨¡å‹è‡ªåŠ¨é€‰æ‹©é€‚åˆè¿è¡Œä¸åŒSQLæŸ¥è¯¢çš„å¼•æ“ã€‚è¯¥ä¼˜åŒ–å™¨åˆ©ç”¨æç¤ºå’Œä¼˜åŒ–åçš„æŸ¥è¯¢è®¡åˆ’è¿›è¡Œæˆæœ¬é¢„æµ‹å’Œè·¯ç”±é€‰æ‹©ã€‚æˆæœ¬é¢„æµ‹è¢«è½¬åŒ–ä¸ºå¤šä»»åŠ¡å­¦ä¹ é—®é¢˜ï¼Œæ¨¡å‹æ¶æ„ä¸­é‡‡ç”¨å¤šä¸ªé¢„æµ‹å™¨å¤´ï¼Œå¯¹åº”ä¸åŒçš„å¼•æ“å’Œé…ç½®ã€‚æ­¤æ–¹æ³•æ— éœ€è®­ç»ƒç‰¹å®šå¼•æ“çš„æ¨¡å‹ï¼Œå¯çµæ´»æ·»åŠ æ–°å¼•æ“å¹¶é™ä½å¾®è°ƒæˆæœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ä¼˜åŒ–åçš„æŸ¥è¯¢é€»è¾‘è®¡åˆ’è¿›è¡Œæˆæœ¬ä¼°è®¡å¯é™ä½å¹³å‡Qè¯¯å·®ï¼Œä¸”è·¨å¼•æ“ä¼˜åŒ–å™¨å¯å‡å°‘æ€»å·¥ä½œé‡è¿è¡Œæ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lakehouse systems enable querying the same data with multiple execution engines.</li>
<li>è‡ªåŠ¨åŒ–å¼•æ“é€‰æ‹©å¯¹äºè¿è¡Œä¸åŒSQLæŸ¥è¯¢è‡³å…³é‡è¦ã€‚</li>
<li>è·¨å¼•æ“ä¼˜åŒ–å™¨é€šè¿‡é‡‡ç”¨å­¦ä¹ æˆæœ¬æ¨¡å‹æ¥è§£å†³è‡ªåŠ¨åŒ–å¼•æ“é€‰æ‹©é—®é¢˜ã€‚</li>
<li>æŸ¥è¯¢è®¡åˆ’ç”¨äºé¢„æµ‹æŸ¥è¯¢æˆæœ¬å’Œè·¯ç”±é€‰æ‹©ã€‚</li>
<li>æˆæœ¬é¢„æµ‹è¢«è½¬åŒ–ä¸ºå¤šä»»åŠ¡å­¦ä¹ é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨ä¼˜åŒ–åçš„æŸ¥è¯¢é€»è¾‘è®¡åˆ’å¯é™ä½å¹³å‡Qè¯¯å·®ï¼Œå¹¶å‡å°‘æ€»å·¥ä½œé‡è¿è¡Œæ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1f543b96a55efc33d631740a7517cfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f7ac3521d0b546a2056fb0585209894.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c02d75792725c3d2822aa476bd89640e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12c8ca74c0fb807730b894aec95f1a15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e23857334cbc9243aed31481f1ebaef4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63a1bb49e374efbe2544cce7ab15d54e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8cad4f489a9bdadd30326f769a3ff82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b866ef3d7e193737904054ea92dae618.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Self-Disentanglement-and-Re-Composition-for-Cross-Domain-Few-Shot-Segmentation"><a href="#Self-Disentanglement-and-Re-Composition-for-Cross-Domain-Few-Shot-Segmentation" class="headerlink" title="Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot   Segmentation"></a>Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot   Segmentation</h2><p><strong>Authors:Jintao Tong, Yixiong Zou, Guangyao Chen, Yuhua Li, Ruixuan Li</strong></p>
<p>Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a source-domain dataset to unseen target-domain datasets with limited annotations. Current methods typically compare the distance between training and testing samples for mask prediction. However, we find an entanglement problem exists in this widely adopted method, which tends to bind sourcedomain patterns together and make each of them hard to transfer. In this paper, we aim to address this problem for the CD-FSS task. We first find a natural decomposition of the ViT structure, based on which we delve into the entanglement problem for an interpretation. We find the decomposed ViT components are crossly compared between images in distance calculation, where the rational comparisons are entangled with those meaningless ones by their equal importance, leading to the entanglement problem. Based on this interpretation, we further propose to address the entanglement problem by learning to weigh for all comparisons of ViT components, which learn disentangled features and re-compose them for the CD-FSS task, benefiting both the generalization and finetuning. Experiments show that our model outperforms the state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under 1-shot and 5-shot settings, respectively. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰æ—¨åœ¨å°†æºåŸŸæ•°æ®é›†çš„çŸ¥è¯†è½¬ç§»åˆ°æœªè§è¿‡çš„ç›®æ ‡åŸŸæ•°æ®é›†ä¸Šï¼Œä¸”ç›®æ ‡åŸŸæ•°æ®é›†å¸¦æœ‰æœ‰é™çš„æ³¨é‡Šã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸é€šè¿‡æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬ä¹‹é—´çš„è·ç¦»æ¥è¿›è¡Œæ©è†œé¢„æµ‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å¹¿æ³›é‡‡ç”¨çš„æ–¹æ³•ä¸­å­˜åœ¨çº ç¼ é—®é¢˜ï¼Œè¿™ä¸€é—®é¢˜å€¾å‘äºå°†æºåŸŸæ¨¡å¼ç»‘å®šåœ¨ä¸€èµ·ï¼Œä½¿å¾—æ¯ä¸ªæ¨¡å¼éƒ½éš¾ä»¥è½¬ç§»ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³CD-FSSä»»åŠ¡çš„è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†ViTç»“æ„çš„è‡ªç„¶åˆ†è§£ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ·±å…¥ç ”ç©¶çº ç¼ é—®é¢˜ä»¥è¿›è¡Œè§£é‡Šã€‚æˆ‘ä»¬å‘ç°åˆ†è§£çš„ViTç»„ä»¶åœ¨è·ç¦»è®¡ç®—ä¸­ä¼šäº¤å‰æ¯”è¾ƒå›¾åƒï¼Œå…¶ä¸­åˆç†æ¯”è¾ƒä¸æ— æ„ä¹‰æ¯”è¾ƒçº ç¼ åœ¨ä¸€èµ·ï¼Œå®ƒä»¬çš„é‡è¦æ€§ç›¸ç­‰ï¼Œå¯¼è‡´äº†çº ç¼ é—®é¢˜ã€‚åŸºäºæ­¤è§£é‡Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºé€šè¿‡ä¸ºæ‰€æœ‰ViTç»„ä»¶çš„æ¯”è¾ƒå­¦ä¹ æƒé‡æ¥è§£å†³çº ç¼ é—®é¢˜ï¼Œè¿™äº›ç»„ä»¶å­¦ä¹ è§£è€¦çš„ç‰¹å¾å¹¶é‡æ–°ç»„åˆå®ƒä»¬ä»¥ç”¨äºCD-FSSä»»åŠ¡ï¼Œæœ‰åˆ©äºæ³›åŒ–å’Œå¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šåˆ†åˆ«è¶…å‡ºæœ€æ–°CD-FSSæ–¹æ³•1.92%å’Œ1.88%ï¼Œåœ¨å•æ ·æœ¬å’Œäº”æ ·æœ¬è®¾ç½®ä¸‹å‡å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02677v1">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰çš„ç›®æ ‡æ˜¯åœ¨å…·æœ‰æœ‰é™æ ‡æ³¨çš„æœªè§ç›®æ ‡åŸŸæ•°æ®é›†ä¸Šè½¬ç§»æºåŸŸæ•°æ®é›†çš„çŸ¥è¯†ã€‚å½“å‰æ–¹æ³•ä¸»è¦é€šè¿‡æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬ä¹‹é—´çš„è·ç¦»è¿›è¡Œæ©è†œé¢„æµ‹ï¼Œä½†å­˜åœ¨çº ç¼ é—®é¢˜ï¼Œå€¾å‘äºå°†æºåŸŸæ¨¡å¼ç»‘å®šåœ¨ä¸€èµ·ï¼Œä½¿å¾—æ¯ä¸ªæ¨¡å¼çš„è½¬ç§»éƒ½å˜å¾—æ›´åŠ å›°éš¾ã€‚æœ¬æ–‡æ—¨åœ¨è§£å†³CD-FSSä»»åŠ¡ä¸­çš„è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆæ‰¾åˆ°ViTç»“æ„çš„è‡ªç„¶åˆ†è§£ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ·±å…¥ç ”ç©¶çº ç¼ é—®é¢˜ä»¥è¿›è¡Œè§£é‡Šã€‚æˆ‘ä»¬å‘ç°åˆ†è§£çš„ViTç»„ä»¶åœ¨è·ç¦»è®¡ç®—æ—¶ç›¸äº’æ¯”è¾ƒå›¾åƒï¼Œå…¶ä¸­åˆç†çš„æ¯”è¾ƒä¸æ— æ„ä¹‰çš„æ¯”è¾ƒçº ç¼ åœ¨ä¸€èµ·ï¼Œå¯¼è‡´çº ç¼ é—®é¢˜ã€‚åŸºäºæ­¤è§£é‡Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºé€šè¿‡å­¦ä¹ ä¸ºæ‰€æœ‰ViTç»„ä»¶çš„æ¯”è¾ƒè¿›è¡ŒåŠ æƒæ¥è§£å†³çº ç¼ é—®é¢˜ï¼Œå­¦ä¹ è§£çº ç¼ ç‰¹å¾å¹¶é‡æ–°ç»„åˆå®ƒä»¬ä»¥ç”¨äºCD-FSSä»»åŠ¡ï¼Œè¿™æœ‰åˆ©äºæ¨¡å‹çš„æ³›åŒ–å’Œå¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSSæ—¨åœ¨ä»æºåŸŸæ•°æ®é›†è½¬ç§»çŸ¥è¯†åˆ°ç›®æ ‡åŸŸæ•°æ®é›†ï¼Œé¢å¯¹æœ‰é™æ ‡æ³¨çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•é€šè¿‡æ¯”è¾ƒè®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬ä¹‹é—´çš„è·ç¦»è¿›è¡Œæ©è†œé¢„æµ‹ï¼Œä½†å­˜åœ¨çº ç¼ é—®é¢˜ã€‚</li>
<li>çº ç¼ é—®é¢˜æºäºViTç»„ä»¶åœ¨è·ç¦»è®¡ç®—æ—¶çš„æ¯”è¾ƒæ–¹å¼ï¼Œåˆç†çš„æ¯”è¾ƒä¸æ— æ„ä¹‰çš„æ¯”è¾ƒçº ç¼ åœ¨ä¸€èµ·ã€‚</li>
<li>æå‡ºé€šè¿‡å­¦ä¹ ä¸ºViTç»„ä»¶çš„æ¯”è¾ƒè¿›è¡ŒåŠ æƒæ¥è§£å†³çº ç¼ é—®é¢˜ã€‚</li>
<li>è§£çº ç¼ ç‰¹å¾çš„å­¦ä¹ æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ³›åŒ–å’Œå¾®è°ƒèƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰CD-FSSæ–¹æ³•ç›¸æ¯”ï¼Œæ–°æ¨¡å‹åœ¨1-shotå’Œ5-shotè®¾ç½®ä¸‹çš„å¹³å‡å‡†ç¡®åº¦åˆ†åˆ«æé«˜äº†1.92%å’Œ1.88%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02677">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b06d2620bc517f6db45a09e4e5109f94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-402ca82f88135de987c6d21d1ee47e30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb9f38507982ac949105175dc8681ca5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9e4a098e266a732b3d12d528ca5c134.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b27ec9b41c4c61c1c92c943fecd8f6fe.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ORPP-Self-Optimizing-Role-playing-Prompts-to-Enhance-Language-Model-Capabilities"><a href="#ORPP-Self-Optimizing-Role-playing-Prompts-to-Enhance-Language-Model-Capabilities" class="headerlink" title="ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model   Capabilities"></a>ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model   Capabilities</h2><p><strong>Authors:Yifan Duan, Yihong Tang, Kehai Chen, Liqiang Nie, Min Zhang</strong></p>
<p>High-quality prompts are crucial for eliciting outstanding performance from large language models (LLMs) on complex tasks. Existing research has explored model-driven strategies for prompt optimization. However, these methods often suffer from high computational overhead or require strong optimization capabilities from the model itself, which limits their broad applicability.To address these challenges, we propose ORPP (Optimized Role-Playing Prompt),a framework that enhances model performance by optimizing and generating role-playing prompts. The core idea of ORPP is to confine the prompt search space to role-playing scenarios, thereby fully activating the modelâ€™s intrinsic capabilities through carefully crafted, high-quality role-playing prompts. Specifically, ORPP first performs iterative optimization on a small subset of training samples to generate high-quality role-playing prompts. Then, leveraging the modelâ€™s few-shot learning capability, it transfers the optimization experience to efficiently generate suitable prompts for the remaining samples.Our experimental results show that ORPP not only matches but in most cases surpasses existing mainstream prompt optimization methods in terms of performance. Notably, ORPP demonstrates superior â€œplug-and-playâ€ capability. In most cases, it can be integrated with various other prompt methods and further enhance their effectiveness. </p>
<blockquote>
<p>é«˜è´¨é‡æç¤ºå¯¹äºæ¿€å‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„å“è¶Šæ€§èƒ½è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶å·²ç»æ¢ç´¢äº†åŸºäºæ¨¡å‹çš„æç¤ºä¼˜åŒ–ç­–ç•¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸é¢ä¸´é«˜è®¡ç®—å¼€é”€æˆ–éœ€è¦æ¨¡å‹æœ¬èº«çš„å¼ºå¤§ä¼˜åŒ–èƒ½åŠ›ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å¹¿æ³›åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ORPPï¼ˆä¼˜åŒ–è§’è‰²æ‰®æ¼”æç¤ºï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¼˜åŒ–å’Œç”Ÿæˆè§’è‰²æ‰®æ¼”æç¤ºæ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ORPPçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†æç¤ºæœç´¢ç©ºé—´é™åˆ¶åœ¨è§’è‰²æ‰®æ¼”åœºæ™¯ä¸Šï¼Œé€šè¿‡ç²¾å¿ƒæ„å»ºçš„é«˜è´¨é‡è§’è‰²æ‰®æ¼”æç¤ºï¼Œå……åˆ†æ¿€æ´»æ¨¡å‹çš„å†…åœ¨èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒORPPé¦–å…ˆå¯¹è®­ç»ƒæ ·æœ¬çš„ä¸€ä¸ªå°å­é›†è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è§’è‰²æ‰®æ¼”æç¤ºã€‚ç„¶åï¼Œåˆ©ç”¨æ¨¡å‹çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œå°†ä¼˜åŒ–ç»éªŒè½¬ç§»åˆ°ä¸ºå…¶ä½™æ ·æœ¬ç”Ÿæˆåˆé€‚çš„æç¤ºã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒORPPä¸ä»…åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†ç°æœ‰ä¸»æµæç¤ºä¼˜åŒ–æ–¹æ³•çš„æ°´å‡†ï¼Œè€Œä¸”åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¶…è¿‡äº†å®ƒä»¬ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒORPPå±•ç¤ºäº†å‡ºè‰²çš„â€œå³æ’å³ç”¨â€èƒ½åŠ›ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå®ƒå¯ä»¥ä¸å…¶ä»–æç¤ºæ–¹æ³•ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02480v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜è´¨é‡æç¤ºçš„å¼•å¯¼ä¸‹ï¼Œèƒ½åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç°æœ‰ç ”ç©¶å·²æ¢ç´¢äº†åŸºäºæ¨¡å‹çš„æç¤ºä¼˜åŒ–ç­–ç•¥ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€å­˜åœ¨è®¡ç®—å¼€é”€å¤§æˆ–éœ€è¦æ¨¡å‹è‡ªèº«å¼ºå¤§çš„ä¼˜åŒ–èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºORPPï¼ˆä¼˜åŒ–è§’è‰²æ‰®æ¼”æç¤ºï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–å’Œç”Ÿæˆè§’è‰²æ‰®æ¼”æç¤ºæ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ORPPçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†æç¤ºæœç´¢ç©ºé—´é™åˆ¶åœ¨è§’è‰²æ‰®æ¼”åœºæ™¯ä¸Šï¼Œé€šè¿‡ç²¾å¿ƒæ„å»ºçš„é«˜è´¨é‡è§’è‰²æ‰®æ¼”æç¤ºï¼Œå……åˆ†æ¿€æ´»æ¨¡å‹çš„å†…åœ¨èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒORPPä¸ä»…åœ¨æ€§èƒ½ä¸Šè¾¾åˆ°ç”šè‡³è¶…è¶Šäº†ä¸»æµæç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œè€Œä¸”å…·æœ‰å‡ºè‰²çš„â€œå³æ’å³ç”¨â€èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸å…¶ä»–æç¤ºæ–¹æ³•ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡æç¤ºå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æç¤ºä¼˜åŒ–æ–¹æ³•å­˜åœ¨è®¡ç®—å¼€é”€å¤§æˆ–éœ€è¦æ¨¡å‹è‡ªèº«å¼ºå¤§ä¼˜åŒ–èƒ½åŠ›çš„å±€é™æ€§ã€‚</li>
<li>ORPPæ¡†æ¶é€šè¿‡ä¼˜åŒ–å’Œç”Ÿæˆè§’è‰²æ‰®æ¼”æç¤ºæ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ORPPå°†æç¤ºæœç´¢ç©ºé—´é™åˆ¶åœ¨è§’è‰²æ‰®æ¼”åœºæ™¯ï¼Œå……åˆ†æ¿€æ´»æ¨¡å‹çš„å†…åœ¨èƒ½åŠ›ã€‚</li>
<li>ORPPé€šè¿‡è¿­ä»£ä¼˜åŒ–ä¸€å°éƒ¨åˆ†è®­ç»ƒæ ·æœ¬æ¥ç”Ÿæˆé«˜è´¨é‡çš„è§’è‰²æ‰®æ¼”æç¤ºã€‚</li>
<li>åˆ©ç”¨æ¨¡å‹çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼ŒORPPå°†ä¼˜åŒ–ç»éªŒè½¬ç§»åˆ°å…¶ä»–æ ·æœ¬ä¸Šï¼Œç”Ÿæˆåˆé€‚çš„æç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02480">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd3c0a4713924649de8f6f13d471a0ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92f145ecdeabcc8afe93cbd5c376a11b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d33406ef75148facc64e7e593462847f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13edce24e66e5dfbb3161dcd876e4781.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-AI-Agent-Architectures-for-Entity-Relationship-Classification"><a href="#Comparative-Analysis-of-AI-Agent-Architectures-for-Entity-Relationship-Classification" class="headerlink" title="Comparative Analysis of AI Agent Architectures for Entity Relationship   Classification"></a>Comparative Analysis of AI Agent Architectures for Entity Relationship   Classification</h2><p><strong>Authors:Maryam Berijanian, Kuldeep Singh, Amin Sehati</strong></p>
<p>Entity relationship classification remains a challenging task in information extraction, especially in scenarios with limited labeled data and complex relational structures. In this study, we conduct a comparative analysis of three distinct AI agent architectures designed to perform relation classification using large language models (LLMs). The agentic architectures explored include (1) reflective self-evaluation, (2) hierarchical task decomposition, and (3) a novel multi-agent dynamic example generation mechanism, each leveraging different modes of reasoning and prompt adaptation. In particular, our dynamic example generation approach introduces real-time cooperative and adversarial prompting. We systematically compare their performance across multiple domains and model backends. Our experiments demonstrate that multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models. These findings offer practical guidance for the design of modular, generalizable LLM-based systems for structured relation extraction. The source codes and dataset are available at \href{<a target="_blank" rel="noopener" href="https://github.com/maryambrj/ALIEN.git%7D%7Bhttps://github.com/maryambrj/ALIEN.git%7D">https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}</a>. </p>
<blockquote>
<p>å®ä½“å…³ç³»åˆ†ç±»åœ¨ä¿¡æ¯æå–ä¸­ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾æ•°æ®æœ‰é™å’Œå…³ç³»ç»“æ„å¤æ‚çš„æƒ…å†µä¸‹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸‰ç§æ—¨åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå…³ç³»åˆ†ç±»çš„ç‹¬ç‰¹AIä»£ç†æ¶æ„è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚æ‰€æ¢è®¨çš„ä»£ç†æ¶æ„åŒ…æ‹¬ï¼ˆ1ï¼‰åæ€è‡ªæˆ‘è¯„ä»·ã€ï¼ˆ2ï¼‰å±‚æ¬¡ä»»åŠ¡åˆ†è§£å’Œï¼ˆ3ï¼‰ä¸€ç§æ–°å‹çš„å¤šä»£ç†åŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæœºåˆ¶ï¼Œæ¯ç§æœºåˆ¶éƒ½åˆ©ç”¨ä¸åŒçš„æ¨ç†æ¨¡å¼å’Œæç¤ºé€‚åº”æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„åŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæ–¹æ³•å¼•å…¥äº†å®æ—¶åˆä½œå’Œå¯¹æŠ—æ€§æç¤ºã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†å®ƒä»¬åœ¨å¤šä¸ªé¢†åŸŸå’Œæ¨¡å‹åç«¯çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå¤šä»£ç†åè°ƒå§‹ç»ˆä¼˜äºæ ‡å‡†çš„å°æ ·æœ¬æç¤ºï¼Œå¹¶æ¥è¿‘å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°ä¸ºè®¾è®¡æ¨¡å—åŒ–ã€å¯æ¨å¹¿çš„åŸºäºLLMçš„ç»“æ„åŒ–å…³ç³»æå–ç³»ç»Ÿæä¾›äº†å®é™…æŒ‡å¯¼ã€‚æºä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/maryambrj/ALIEN.git%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/maryambrj/ALIEN.gitè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02426v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å®ä½“å…³ç³»åˆ†ç±»åœ¨ä¿¡æ¯æå–ä¸­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾æ•°æ®æœ‰é™å’Œå…³ç³»ç»“æ„å¤æ‚çš„æƒ…å†µä¸‹ã€‚æ–‡ç« å¯¹æ¯”åˆ†æäº†ä¸‰ç§ç”¨äºå…³ç³»åˆ†ç±»çš„AIä»£ç†æ¶æ„ï¼ŒåŒ…æ‹¬åæ€è‡ªæˆ‘è¯„ä»·ã€å±‚æ¬¡ä»»åŠ¡åˆ†è§£å’Œæ–°å‹å¤šä»£ç†åŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæœºåˆ¶ã€‚å…¶ä¸­ï¼ŒåŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæ–¹æ³•å¼•å…¥äº†å®æ—¶åˆä½œå’Œå¯¹æŠ—æ€§æç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œå¤šä»£ç†ååŒä¸€è‡´åœ°è¶…è¶Šäº†æ ‡å‡†å°‘æ ·æœ¬æç¤ºï¼Œå¹¶æ¥è¿‘å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ä¸ºè®¾è®¡æ¨¡å—åŒ–çš„ã€é€šç”¨çš„å¤§è¯­è¨€æ¨¡å‹åŸºç¡€çš„ç³»ç»Ÿè¿›è¡Œç»“æ„åŒ–å…³ç³»æå–æä¾›äº†å®é™…æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®ä½“å…³ç³»åˆ†ç±»æ˜¯ä¿¡æ¯æå–ä¸­çš„ä¸€é¡¹æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨æ•°æ®æœ‰é™å’Œå…³ç³»ç»“æ„å¤æ‚çš„æƒ…å†µä¸‹ã€‚</li>
<li>åˆ†æäº†ä¸‰ç§AIä»£ç†æ¶æ„ï¼šåæ€è‡ªæˆ‘è¯„ä»·ã€å±‚æ¬¡ä»»åŠ¡åˆ†è§£å’Œæ–°å‹å¤šä»£ç†åŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæœºåˆ¶ã€‚</li>
<li>å¤šä»£ç†åŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæ–¹æ³•ç»“åˆäº†å®æ—¶åˆä½œå’Œå¯¹æŠ—æ€§æç¤ºã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šä»£ç†ååŒåœ¨å…³ç³»åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œè¶…è¶Šäº†æ ‡å‡†å°‘æ ·æœ¬æç¤ºæ–¹æ³•ã€‚</li>
<li>å¤šä»£ç†ååŒæ€§èƒ½æ¥è¿‘å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè®¾è®¡æ¨¡å—åŒ–çš„ã€é€šç”¨çš„å¤§è¯­è¨€æ¨¡å‹åŸºç¡€çš„ç³»ç»Ÿè¿›è¡Œç»“æ„åŒ–å…³ç³»æå–æä¾›äº†å®è·µæŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a5e0a575e20c16c04842404f9b38425.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dea697f76100335fdf89746282a33143.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87c560b9afb5054fbd70c7ab48032fb7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7218792e58acd738a798fe0b1b511b8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79971973db5de7b925deef4f1ab0716c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-505b5150ebb1833e5037a144bdcfa5ad.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ViTNF-Leveraging-Neural-Fields-to-Boost-Vision-Transformers-in-Generalized-Category-Discovery"><a href="#ViTNF-Leveraging-Neural-Fields-to-Boost-Vision-Transformers-in-Generalized-Category-Discovery" class="headerlink" title="ViTNF: Leveraging Neural Fields to Boost Vision Transformers in   Generalized Category Discovery"></a>ViTNF: Leveraging Neural Fields to Boost Vision Transformers in   Generalized Category Discovery</h2><p><strong>Authors:Jiayi Su, Dequan Jin</strong></p>
<p>Generalized category discovery (GCD) is a highly popular task in open-world recognition, aiming to identify unknown class samples using known class data. By leveraging pre-training, meta-training, and fine-tuning, ViT achieves excellent few-shot learning capabilities. Its MLP head is a feedforward network, trained synchronously with the entire network in the same process, increasing the training cost and difficulty without fully leveraging the power of the feature extractor. This paper proposes a new architecture by replacing the MLP head with a neural field-based one. We first present a new static neural field function to describe the activity distribution of the neural field and then use two static neural field functions to build an efficient few-shot classifier. This neural field-based (NF) classifier consists of two coupled static neural fields. It stores the feature information of support samples by its elementary field, the known categories by its high-level field, and the category information of support samples by its cross-field connections. We replace the MLP head with the proposed NF classifier, resulting in a novel architecture ViTNF, and simplify the three-stage training mode by pre-training the feature extractor on source tasks and training the NF classifier with support samples in meta-testing separately, significantly reducing ViTâ€™s demand for training samples and the difficulty of model training. To enhance the modelâ€™s capability in identifying new categories, we provide an effective algorithm to determine the lateral interaction scale of the elementary field. Experimental results demonstrate that our model surpasses existing state-of-the-art methods on CIFAR-100, ImageNet-100, CUB-200, and Standard Cars, achieving dramatic accuracy improvements of 19% and 16% in new and all classes, respectively, indicating a notable advantage in GCD. </p>
<blockquote>
<p>å¹¿ä¹‰ç±»åˆ«å‘ç°ï¼ˆGCDï¼‰æ˜¯å¼€æ”¾ä¸–ç•Œè¯†åˆ«ä¸­éå¸¸å—æ¬¢è¿çš„ä»»åŠ¡ï¼Œæ—¨åœ¨åˆ©ç”¨å·²çŸ¥ç±»åˆ«çš„æ•°æ®æ¥è¯†åˆ«æœªçŸ¥ç±»åˆ«çš„æ ·æœ¬ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒã€å…ƒè®­ç»ƒå’Œå¾®è°ƒï¼ŒViTå®ç°äº†å‡ºè‰²çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚å…¶MLPå¤´æ˜¯å‰é¦ˆç½‘ç»œï¼Œä¸æ•´ä¸ªç½‘ç»œåœ¨åŒä¸€è¿›ç¨‹ä¸­åŒæ­¥è®­ç»ƒï¼Œå¢åŠ äº†è®­ç»ƒæˆæœ¬å’Œéš¾åº¦ï¼Œè€Œæ²¡æœ‰å……åˆ†åˆ©ç”¨ç‰¹å¾æå–å™¨çš„åŠ›é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„ï¼Œç”¨åŸºäºç¥ç»åœºçš„å¤´æ¥æ›¿æ¢MLPå¤´ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§æ–°çš„é™æ€ç¥ç»åœºå‡½æ•°æ¥æè¿°ç¥ç»åœºçš„æ´»åŠ¨åˆ†å¸ƒï¼Œç„¶åä½¿ç”¨ä¸¤ä¸ªé™æ€ç¥ç»åœºå‡½æ•°æ¥æ„å»ºæœ‰æ•ˆçš„å°‘æ ·æœ¬åˆ†ç±»å™¨ã€‚è¿™ä¸ªåŸºäºç¥ç»åœºï¼ˆNFï¼‰çš„åˆ†ç±»å™¨ç”±ä¸¤ä¸ªè€¦åˆçš„é™æ€ç¥ç»åœºç»„æˆã€‚å®ƒé€šè¿‡åŸºæœ¬åœºå­˜å‚¨æ”¯æŒæ ·æœ¬çš„ç‰¹å¾ä¿¡æ¯ï¼Œé€šè¿‡é«˜çº§åœºå­˜å‚¨å·²çŸ¥ç±»åˆ«çš„ä¿¡æ¯ï¼Œå¹¶é€šè¿‡è·¨åœºè¿æ¥å­˜å‚¨æ”¯æŒæ ·æœ¬çš„ç±»åˆ«ä¿¡æ¯ã€‚æˆ‘ä»¬ç”¨æå‡ºçš„NFåˆ†ç±»å™¨æ›¿æ¢MLPå¤´ï¼Œå¾—åˆ°äº†æ–°å‹æ¶æ„ViTNFï¼Œé€šè¿‡é¢„è®­ç»ƒç‰¹å¾æå–å™¨åœ¨æºä»»åŠ¡ä¸Šï¼Œä»¥åŠåœ¨å…ƒæµ‹è¯•ä¸­ç”¨æ”¯æŒæ ·æœ¬åˆ†åˆ«è®­ç»ƒNFåˆ†ç±»å™¨ï¼Œç®€åŒ–äº†ä¸‰é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œå¤§å¤§é™ä½äº†ViTå¯¹è®­ç»ƒæ ·æœ¬çš„éœ€æ±‚å’Œæ¨¡å‹è®­ç»ƒçš„éš¾åº¦ã€‚ä¸ºäº†æé«˜æ¨¡å‹è¯†åˆ«æ–°ç±»åˆ«çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§ç¡®å®šåŸºæœ¬åœºæ¨ªå‘äº¤äº’å°ºåº¦çš„æœ‰æ•ˆç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨CIFAR-100ã€ImageNet-100ã€CUB-200å’ŒStandard Carsç­‰æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ–°ç±»åˆ«å’Œæ‰€æœ‰ç±»åˆ«çš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†19%å’Œ16%ï¼Œåœ¨GCDä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02367v1">PDF</a> 22 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¶æ„ViTNFï¼Œå®ƒé€šè¿‡åˆ©ç”¨ç¥ç»ç½‘ç»œåœºæ›¿æ¢ViTä¸­çš„MLPå¤´æ¥å®ç°æ›´é«˜æ•ˆçš„å°‘æ ·æœ¬åˆ†ç±»ã€‚æ–°æ¶æ„é€šè¿‡ä¸¤ä¸ªè€¦åˆçš„é™æ€ç¥ç»ç½‘ç»œåœºæ¥å­˜å‚¨æ”¯æŒæ ·æœ¬çš„ç‰¹å¾ä¿¡æ¯ã€å·²çŸ¥ç±»åˆ«çš„ä¿¡æ¯ä»¥åŠæ”¯æŒæ ·æœ¬çš„ç±»åˆ«ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªç¡®å®šåŸºæœ¬åœºæ¨ªå‘äº¤äº’å°ºåº¦çš„æœ‰æ•ˆç®—æ³•ï¼Œç”¨äºæé«˜æ¨¡å‹è¯†åˆ«æ–°ç±»åˆ«çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨CIFAR-100ã€ImageNet-100ã€CUB-200å’ŒStandard Carsç­‰æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œåœ¨æ–°ç±»å’Œæ‰€æœ‰ç±»çš„å‡†ç¡®ç‡åˆ†åˆ«æé«˜äº†19%å’Œ16%ï¼Œåœ¨å¹¿ä¹‰ç±»åˆ«å‘ç°ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­ä»‹ç»äº†å¹¿ä¹‰ç±»åˆ«å‘ç°ï¼ˆGCDï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨åˆ©ç”¨å·²çŸ¥ç±»åˆ«æ•°æ®è¯†åˆ«æœªçŸ¥ç±»åˆ«æ ·æœ¬ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„ViTNFï¼Œé€šè¿‡æ›¿æ¢ViTä¸­çš„MLPå¤´ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œåœºå®ç°å°‘æ ·æœ¬å­¦ä¹ ã€‚</li>
<li>ViTNFæ¶æ„é€šè¿‡ä¸¤ä¸ªè€¦åˆçš„é™æ€ç¥ç»ç½‘ç»œåœºå­˜å‚¨æ”¯æŒæ ·æœ¬å’Œå·²çŸ¥ç±»åˆ«çš„ç‰¹å¾ä¿¡æ¯ã€‚</li>
<li>æä¾›äº†ç¡®å®šåŸºæœ¬åœºæ¨ªå‘äº¤äº’å°ºåº¦çš„æœ‰æ•ˆç®—æ³•ï¼Œä»¥æé«˜æ¨¡å‹è¯†åˆ«æ–°ç±»åˆ«çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒViTNFåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå‡†ç¡®ç‡çš„æé«˜è¯æ˜äº†å…¶åœ¨GCDä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ã€‚</li>
<li>ViTNFç®€åŒ–äº†è®­ç»ƒæ¨¡å¼ï¼Œé€šè¿‡é¢„è®­ç»ƒç‰¹å¾æå–å™¨å’Œåœ¨å…ƒæµ‹è¯•é˜¶æ®µåˆ†åˆ«è®­ç»ƒNFåˆ†ç±»å™¨ä¸æ”¯æŒæ ·æœ¬ï¼Œé™ä½äº†ViTå¯¹è®­ç»ƒæ ·æœ¬çš„éœ€æ±‚å’Œæ¨¡å‹è®­ç»ƒçš„éš¾åº¦ã€‚</li>
<li>ViTNFæ¶æ„çš„å¼•å…¥æœ‰åŠ©äºæ¨åŠ¨å°‘æ ·æœ¬å­¦ä¹ é¢†åŸŸçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-952d9315c63f4cffc3b5ae2e136930f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f6b9f1d25eb7d917f2d1bd2be9f6552.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef072c884e426cc4632a2ce57028ebb0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Something-Just-Like-TRuST-Toxicity-Recognition-of-Span-and-Target"><a href="#Something-Just-Like-TRuST-Toxicity-Recognition-of-Span-and-Target" class="headerlink" title="Something Just Like TRuST : Toxicity Recognition of Span and Target"></a>Something Just Like TRuST : Toxicity Recognition of Span and Target</h2><p><strong>Authors:Berk Atil, Namrata Sureddy, Rebecca J. Passonneau</strong></p>
<p>Toxicity in online content, including content generated by language models, has become a critical concern due to its potential for negative psychological and social impact. This paper introduces TRuST, a comprehensive dataset designed to improve toxicity detection that merges existing datasets, and has labels for toxicity, target social group, and toxic spans. It includes a diverse range of target groups such as ethnicity, gender, religion, disability, and politics, with both human&#x2F;machine-annotated and human machine-generated data. We benchmark state-of-the-art large language models (LLMs) on toxicity detection, target group identification, and toxic span extraction. We find that fine-tuned models consistently outperform zero-shot and few-shot prompting, though performance remains low for certain social groups. Further, reasoning capabilities do not significantly improve performance, indicating that LLMs have weak social reasoning skills. </p>
<blockquote>
<p>ç½‘ç»œå†…å®¹ä¸­çš„æ¯’æ€§ï¼ŒåŒ…æ‹¬ç”±è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ï¼Œç”±äºå…¶å¯èƒ½äº§ç”Ÿçš„è´Ÿé¢å¿ƒç†å’Œç¤¾ä¼šå½±å“ï¼Œå·²æˆä¸ºäººä»¬å…³æ³¨çš„é‡ç‚¹ã€‚æœ¬æ–‡ä»‹ç»äº†TRuSTæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºäº†æ”¹å–„æ¯’æ€§æ£€æµ‹è€Œè®¾è®¡çš„ç»¼åˆæ•°æ®é›†ï¼Œå®ƒåˆå¹¶äº†ç°æœ‰æ•°æ®é›†ï¼Œå¹¶ä¸ºæ¯’æ€§ã€ç›®æ ‡ç¤¾ä¼šç¾¤ä½“å’Œæœ‰æ¯’è·¨åº¦æä¾›äº†æ ‡ç­¾ã€‚å®ƒæ¶µç›–äº†å¹¿æ³›çš„ç›®æ ‡ç¾¤ä½“ï¼Œå¦‚ç§æ—ã€æ€§åˆ«ã€å®—æ•™ã€æ®‹ç–¾å’Œæ”¿æ²»ç­‰ï¼ŒåŒ…æ‹¬äººæœºæ ‡æ³¨å’Œäººæœºç”Ÿæˆçš„æ•°æ®ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¯’æ€§æ£€æµ‹ã€ç›®æ ‡ç¾¤ä½“è¯†åˆ«å’Œæœ‰æ¯’è·¨åº¦æå–æ–¹é¢è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å‘ç°å¾®è°ƒæ¨¡å‹å§‹ç»ˆä¼˜äºé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºï¼Œå°½ç®¡æŸäº›ç¤¾ä¼šç¾¤ä½“çš„æ€§èƒ½ä»ç„¶è¾ƒä½ã€‚æ­¤å¤–ï¼Œæ¨ç†èƒ½åŠ›å¹¶æ²¡æœ‰æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œè¿™è¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¤¾ä¼šæ¨ç†èƒ½åŠ›è¾ƒå¼±ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02326v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨åœ¨çº¿å†…å®¹ä¸­çš„æ¯’æ€§é—®é¢˜ï¼ŒåŒ…æ‹¬ç”±è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ã€‚æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†TRuSTï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨æé«˜æ¯’æ€§æ£€æµ‹èƒ½åŠ›ï¼Œå®ƒé€šè¿‡åˆå¹¶ç°æœ‰æ•°æ®é›†ï¼Œå¹¶ä¸ºæ¯’æ€§ã€ç›®æ ‡ç¤¾ä¼šç¾¤ä½“å’Œæ¯’æ€§è·¨åº¦æä¾›æ ‡ç­¾ã€‚æ–‡ç« è¿˜è¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¯’æ€§æ£€æµ‹ã€ç›®æ ‡ç¾¤ä½“è¯†åˆ«å’Œæ¯’æ€§è·¨åº¦æå–æ–¹é¢çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå¾®è°ƒæ¨¡å‹å§‹ç»ˆä¼˜äºé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºï¼Œä½†å¯¹æŸäº›ç¤¾ä¼šç¾¤ä½“çš„è¡¨ç°ä»ç„¶è¾ƒä½ã€‚æ­¤å¤–ï¼Œæ¨ç†èƒ½åŠ›å¹¶æ²¡æœ‰æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œè¡¨æ˜LLMsåœ¨ç¤¾äº¤æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨çº¿å†…å®¹çš„æ¯’æ€§é—®é¢˜æ—¥ç›Šä¸¥é‡ï¼Œéœ€è¦å…³æ³¨ã€‚</li>
<li>TRuSTæ•°æ®é›†æ—¨åœ¨æé«˜æ¯’æ€§æ£€æµ‹èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šç§ç›®æ ‡ç¾¤ä½“ï¼Œå¦‚ç§æ—ã€æ€§åˆ«ã€å®—æ•™ã€æ®‹ç–¾å’Œæ”¿æ²»ç­‰ã€‚</li>
<li>æ•°æ®é›†åŒ…å«äººç±»&#x2F;æœºå™¨æ³¨é‡Šå’Œç”±æœºå™¨ç”Ÿæˆçš„æ•°æ®ã€‚</li>
<li>è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸‰ä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼šæ¯’æ€§æ£€æµ‹ã€ç›®æ ‡ç¾¤ä½“è¯†åˆ«å’Œæ¯’æ€§è·¨åº¦æå–ã€‚</li>
<li>å¾®è°ƒæ¨¡å‹åœ¨ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºã€‚</li>
<li>å¯¹äºæŸäº›ç¤¾ä¼šç¾¤ä½“çš„æ€§èƒ½ä»ç„¶è¾ƒä½ï¼Œè¡¨æ˜è¯­è¨€æ¨¡å‹åœ¨æŸäº›æ–¹é¢çš„è¡¨ç°æœ‰å¾…æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1c9e8cb131521401693be03db730f8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-965ecd7b238de5dbcaefc812d52d6d28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b170c8f47fe5124f9caa4a832bc8325c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bdf382da307b50cba2d7911268926c4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Statement-Tuning-Enables-Efficient-Cross-lingual-Generalization-in-Encoder-only-Models"><a href="#Statement-Tuning-Enables-Efficient-Cross-lingual-Generalization-in-Encoder-only-Models" class="headerlink" title="Statement-Tuning Enables Efficient Cross-lingual Generalization in   Encoder-only Models"></a>Statement-Tuning Enables Efficient Cross-lingual Generalization in   Encoder-only Models</h2><p><strong>Authors:Ahmed Elshabrawy, Thanh-Nhi Nguyen, Yeeun Kang, Lihan Feng, Annant Jain, Faadil Abdullah Shaikh, Jonibek Mansurov, Mohamed Fazli Mohamed Imam, Jesus-German Ortiz-Barajas, Rendi Chevi, Alham Fikri Aji</strong></p>
<p>Large Language Models (LLMs) excel in zero-shot and few-shot tasks, but achieving similar performance with encoder-only models like BERT and RoBERTa has been challenging due to their architecture. However, encoders offer advantages such as lower computational and memory costs. Recent work adapts them for zero-shot generalization using Statement Tuning, which reformulates tasks into finite templates. We extend this approach to multilingual NLP, exploring whether encoders can achieve zero-shot cross-lingual generalization and serve as efficient alternatives to memory-intensive LLMs for low-resource languages. Our results show that state-of-the-art encoder models generalize well across languages, rivaling multilingual LLMs while being more efficient. We also analyze multilingual Statement Tuning dataset design, efficiency gains, and language-specific generalization, contributing to more inclusive and resource-efficient NLP models. We release our code and models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºå…¶æ¶æ„ç‰¹ç‚¹ï¼Œä½¿ç”¨ç¼–ç å™¨æ¨¡å‹ï¼ˆå¦‚BERTå’ŒRoBERTaï¼‰å®ç°ç±»ä¼¼æ€§èƒ½ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç¼–ç å™¨å…·æœ‰è®¡ç®—å’Œå†…å­˜æˆæœ¬ä½çš„ä¼˜åŠ¿ã€‚æœ€è¿‘çš„å·¥ä½œé€šè¿‡Statement Tuningå°†å…¶é€‚åº”äºé›¶æ ·æœ¬æ³›åŒ–ï¼Œå°†ä»»åŠ¡é‡æ–°æ ¼å¼åŒ–ä¸ºæœ‰é™çš„æ¨¡æ¿ã€‚æˆ‘ä»¬å°†æ­¤æ–¹æ³•æ‰©å±•åˆ°å¤šè¯­ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œæ¢ç´¢ç¼–ç å™¨æ˜¯å¦å¯ä»¥å®ç°é›¶æ ·æœ¬è·¨è¯­è¨€æ³›åŒ–ï¼Œå¹¶ä½œä¸ºå†…å­˜å¯†é›†å‹å¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œç”¨äºä½èµ„æºè¯­è¨€ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„ç¼–ç å™¨æ¨¡å‹åœ¨è·¨è¯­è¨€æ–¹é¢çš„æ³›åŒ–æ•ˆæœå¾ˆå¥½ï¼Œä¸å¤šè¯­ç§çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”å…·æœ‰æ›´é«˜çš„æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†å¤šè¯­ç§Statement Tuningæ•°æ®é›†çš„è®¾è®¡ã€æ•ˆç‡æå‡å’Œè¯­è¨€ç‰¹å®šæ³›åŒ–ï¼Œä¸ºæ„å»ºæ›´å…·åŒ…å®¹æ€§å’Œèµ„æºæ•ˆç‡çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹åšå‡ºè´¡çŒ®ã€‚æˆ‘ä»¬å…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01592v1">PDF</a> Accepted to ACL 2025 (Findings)</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ç”±äºæ¶æ„é™åˆ¶ï¼Œä½¿ç”¨ç¼–ç å™¨æ¨¡å‹ï¼ˆå¦‚BERTå’ŒRoBERTaï¼‰å®ç°ç±»ä¼¼æ€§èƒ½ä¸€ç›´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç„¶è€Œï¼Œç¼–ç å™¨å…·æœ‰ä½è®¡ç®—å’Œå†…å­˜æˆæœ¬çš„ä¼˜åŠ¿ã€‚æœ€è¿‘çš„å·¥ä½œä½¿ç”¨è¯­å¥è°ƒæ•´æ³•å°†å…¶é€‚åº”äºé›¶æ ·æœ¬æ³›åŒ–ï¼Œå°†ä»»åŠ¡é‡æ–°åˆ¶å®šä¸ºæœ‰é™æ¨¡æ¿ã€‚æˆ‘ä»¬å°†å…¶æ‰©å±•åˆ°å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ï¼Œæ¢è®¨ç¼–ç å™¨æ˜¯å¦å¯ä»¥å®ç°é›¶æ ·æœ¬è·¨è¯­è¨€æ³›åŒ–ï¼Œå¹¶ä½œä¸ºå†…å­˜å¯†é›†å‹LLMsçš„æ›¿ä»£æ–¹æ¡ˆï¼Œåœ¨ä½èµ„æºè¯­è¨€ä¸­å®ç°é«˜æ•ˆæœåŠ¡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„ç¼–ç å™¨æ¨¡å‹è·¨è¯­è¨€æ³›åŒ–æ•ˆæœè‰¯å¥½ï¼Œå¯ä¸å¤šè¯­è¨€LLMsç›¸æŠ—è¡¡ï¼ŒåŒæ—¶æ•ˆç‡æ›´é«˜ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†å¤šè¯­è¨€è¯­å¥è°ƒæ•´æ•°æ®é›†çš„è®¾è®¡ã€æ•ˆç‡å¢ç›Šå’Œè¯­è¨€ç‰¹å®šæ³›åŒ–ï¼Œä¸ºæ›´å…·åŒ…å®¹æ€§å’Œèµ„æºæ•ˆç‡çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹åšå‡ºè´¡çŒ®ã€‚æˆ‘ä»¬å…¬å¼€äº†ä»£ç å’Œæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ç¼–ç å™¨æ¨¡å‹å®ç°ç±»ä¼¼æ€§èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç¼–ç å™¨æ¨¡å‹å…·æœ‰ä½è®¡ç®—å’Œå†…å­˜æˆæœ¬çš„ä¼˜åŠ¿ã€‚</li>
<li>è¯­å¥è°ƒæ•´æ³•è¢«ç”¨äºé€‚åº”ç¼–ç å™¨æ¨¡å‹çš„é›¶æ ·æœ¬æ³›åŒ–ï¼Œå°†ä»»åŠ¡è½¬åŒ–ä¸ºæœ‰é™æ¨¡æ¿ã€‚</li>
<li>å°†ç¼–ç å™¨æ¨¡å‹æ‰©å±•åˆ°å¤šè¯­è¨€NLPï¼Œå®ç°é›¶æ ·æœ¬è·¨è¯­è¨€æ³›åŒ–ã€‚</li>
<li>æœ€å…ˆè¿›çš„ç¼–ç å™¨æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œæ•ˆç‡é«˜äºå¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>åˆ†æè¡¨æ˜ï¼Œå¤šè¯­è¨€è¯­å¥è°ƒæ•´æ•°æ®é›†è®¾è®¡ã€æ•ˆç‡å¢ç›Šå’Œè¯­è¨€ç‰¹å®šæ³›åŒ–æ˜¯æ”¹è¿›NLPæ¨¡å‹çš„é‡è¦æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a007b7d115a8a268630babd2c6cd9176.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59672e86eb15f8e00dae6256584f13fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c21508b1887a2eefe007c696d1ce795c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7973df5639fe378d0825719df939df7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a43e6d53de48b1eae60edb869ad42d4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08e85c00643c04e35e22203653558c94.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-Few-shot-Graph-Neural-Architecture-Search-via-Partitioning-Gradient-Contribution"><a href="#Towards-Efficient-Few-shot-Graph-Neural-Architecture-Search-via-Partitioning-Gradient-Contribution" class="headerlink" title="Towards Efficient Few-shot Graph Neural Architecture Search via   Partitioning Gradient Contribution"></a>Towards Efficient Few-shot Graph Neural Architecture Search via   Partitioning Gradient Contribution</h2><p><strong>Authors:Wenhao Song, Xuan Wu, Bo Yang, You Zhou, Yubin Xiao, Yanchun Liang, Hongwei Ge, Heow Pueh Lee, Chunguo Wu</strong></p>
<p>To address the weight coupling problem, certain studies introduced few-shot Neural Architecture Search (NAS) methods, which partition the supernet into multiple sub-supernets. However, these methods often suffer from computational inefficiency and tend to provide suboptimal partitioning schemes. To address this problem more effectively, we analyze the weight coupling problem from a novel perspective, which primarily stems from distinct modules in succeeding layers imposing conflicting gradient directions on the preceding layer modules. Based on this perspective, we propose the Gradient Contribution (GC) method that efficiently computes the cosine similarity of gradient directions among modules by decomposing the Vector-Jacobian Product during supernet backpropagation. Subsequently, the modules with conflicting gradient directions are allocated to distinct sub-supernets while similar ones are grouped together. To assess the advantages of GC and address the limitations of existing Graph Neural Architecture Search methods, which are limited to searching a single type of Graph Neural Networks (Message Passing Neural Networks (MPNNs) or Graph Transformers (GTs)), we propose the Unified Graph Neural Architecture Search (UGAS) framework, which explores optimal combinations of MPNNs and GTs. The experimental results demonstrate that GC achieves state-of-the-art (SOTA) performance in supernet partitioning quality and time efficiency. In addition, the architectures searched by UGAS+GC outperform both the manually designed GNNs and those obtained by existing NAS methods. Finally, ablation studies further demonstrate the effectiveness of all proposed methods. </p>
<blockquote>
<p>ä¸ºäº†è§£å†³æƒé‡è€¦åˆé—®é¢˜ï¼Œä¸€äº›ç ”ç©¶å¼•å…¥äº†å°æ ·æœ¬ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æ–¹æ³•ï¼Œå°†è¶…ç½‘åˆ†å‰²æˆå¤šä¸ªå­è¶…ç½‘ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å­˜åœ¨è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå¹¶ä¸”å¾€å¾€æä¾›æ¬¡ä¼˜çš„åˆ†å‰²æ–¹æ¡ˆã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªæ–°çš„è§’åº¦åˆ†æäº†æƒé‡è€¦åˆé—®é¢˜ï¼Œå…¶ä¸»è¦æºäºåç»­å±‚ä¸­çš„ä¸åŒæ¨¡å—å¯¹å‰ä¸€å±‚æ¨¡å—æ–½åŠ å†²çªçš„æ¢¯åº¦æ–¹å‘ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ¢¯åº¦è´¡çŒ®ï¼ˆGCï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ†è§£è¶…ç½‘åå‘ä¼ æ’­ä¸­çš„å‘é‡-é›…å¯æ¯”ä¹˜ç§¯ï¼Œæœ‰æ•ˆåœ°è®¡ç®—äº†æ¨¡å—ä¹‹é—´æ¢¯åº¦æ–¹å‘çš„ä½™å¼¦ç›¸ä¼¼æ€§ã€‚éšåï¼Œå…·æœ‰å†²çªæ¢¯åº¦æ–¹å‘çš„æ¨¡å—è¢«åˆ†é…åˆ°ä¸åŒçš„å­è¶…ç½‘ä¸­ï¼Œè€Œç›¸ä¼¼çš„æ¨¡å—åˆ™ç»„åˆåœ¨ä¸€èµ·ã€‚ä¸ºäº†è¯„ä¼°GCçš„ä¼˜åŠ¿ï¼Œå¹¶è§£å†³ç°æœ‰å›¾ç¥ç»ç½‘ç»œæ¶æ„æœç´¢æ–¹æ³•çš„å±€é™æ€§ï¼ˆè¿™äº›æ–¹æ³•ä»…é™äºæœç´¢å•ä¸€ç±»å‹çš„å›¾ç¥ç»ç½‘ç»œï¼Œå¦‚æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰æˆ–å›¾è½¬æ¢å™¨ï¼ˆGTsï¼‰ï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€å›¾ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆUGASï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ¢ç´¢äº†MPNNså’ŒGTsçš„æœ€ä½³ç»„åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGCåœ¨è¶…ç½‘åˆ†å‰²è´¨é‡å’Œæ—¶é—´æ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚æ­¤å¤–ï¼Œé€šè¿‡UGAS+GCæœç´¢çš„æ¶æ„åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†æ‰‹åŠ¨è®¾è®¡çš„GNNså’Œç°æœ‰NASæ–¹æ³•å¾—åˆ°çš„æ¶æ„ã€‚æœ€åï¼Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†æ‰€æœ‰æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01231v1">PDF</a> Accepted by SIGKDD 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŸºäºç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰çš„æƒé‡è€¦åˆé—®é¢˜ï¼Œå¹¶æå‡ºäº†å…¨æ–°çš„è§†è§’å’Œæ–¹æ³•æ¥è§£å†³è¯¥é—®é¢˜ã€‚é€šè¿‡åˆ†è§£å‘é‡é›…å¯æ¯”ä¹˜ç§¯ï¼Œæå‡ºäº†æ¢¯åº¦è´¡çŒ®ï¼ˆGCï¼‰æ–¹æ³•æ¥è®¡ç®—æ¨¡å—é—´çš„æ¢¯åº¦æ–¹å‘ç›¸ä¼¼æ€§ï¼Œä¼˜åŒ–ç½‘ç»œç»“æ„ä¸­çš„å­è¶…ç½‘åˆ’åˆ†ï¼Œä»è€Œæå‡äº†ç½‘ç»œçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³ç°æœ‰å›¾å½¢ç¥ç»ç½‘ç»œæ¶æ„æœç´¢æ–¹æ³•åªé’ˆå¯¹å•ä¸€ç±»å‹çš„å›¾å½¢ç¥ç»ç½‘ç»œçš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ç»Ÿä¸€å›¾å½¢ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆUGASï¼‰æ¡†æ¶ï¼Œç»“åˆæ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œå’Œå›¾å½¢è½¬æ¢å™¨ï¼Œä»¥å¯»æ‰¾æœ€ä½³ç»„åˆæ–¹å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGCæ–¹æ³•åœ¨è¶…ç½‘åˆ’åˆ†è´¨é‡å’Œæ—¶é—´æ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€ä¼˜æ°´å¹³ï¼Œè€ŒUGAS+GCæœç´¢åˆ°çš„æ¶æ„åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†æ‰‹åŠ¨è®¾è®¡çš„GNNå’Œç°æœ‰NASæ–¹æ³•ã€‚æœ€åçš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ‰€æœ‰æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶é’ˆå¯¹ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ä¸­çš„æƒé‡è€¦åˆé—®é¢˜ï¼Œæå‡ºäº†å…¨æ–°çš„è§†è§’å’Œæ–¹æ³•æ¥è§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ¢¯åº¦è´¡çŒ®ï¼ˆGCï¼‰æ–¹æ³•è®¡ç®—æ¨¡å—é—´çš„æ¢¯åº¦æ–¹å‘ç›¸ä¼¼æ€§ï¼Œä¼˜åŒ–å­è¶…ç½‘åˆ’åˆ†ã€‚</li>
<li>GCæ–¹æ³•æé«˜äº†è¶…ç½‘åˆ’åˆ†è´¨é‡å’Œæ—¶é—´æ•ˆç‡ï¼Œè¾¾åˆ°æœ€ä¼˜æ°´å¹³ã€‚</li>
<li>UGASæ¡†æ¶ç»“åˆäº†æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œå’Œå›¾å½¢è½¬æ¢å™¨ï¼Œä»¥å¯»æ‰¾æœ€ä½³ç»„åˆæ–¹å¼ï¼Œè§£å†³äº†ç°æœ‰å›¾å½¢ç¥ç»ç½‘ç»œæ¶æ„æœç´¢æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>UGAS+GCæœç´¢åˆ°çš„æ¶æ„æ€§èƒ½è¶…è¿‡äº†æ‰‹åŠ¨è®¾è®¡çš„GNNå’Œç°æœ‰NASæ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01231">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c69af6f04992d0f3803e1c5bf657aa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c67e342f880ff0083b07ddfd6e84028.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e568338becf7495ce7fa7721d20f295e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b908473d21ffd51b42e6e4e99eda109.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-660f0034c0115d5736355c76d9530254.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Structured-Gradient-Guidance-for-Few-Shot-Adaptation-in-Large-Language-Models"><a href="#Structured-Gradient-Guidance-for-Few-Shot-Adaptation-in-Large-Language-Models" class="headerlink" title="Structured Gradient Guidance for Few-Shot Adaptation in Large Language   Models"></a>Structured Gradient Guidance for Few-Shot Adaptation in Large Language   Models</h2><p><strong>Authors:Hongye Zheng, Yichen Wang, Ray Pan, Guiran Liu, Binrong Zhu, Hanlu Zhang</strong></p>
<p>This paper presents a gradient-informed fine-tuning method for large language models under few-shot conditions. The goal is to enhance task adaptability and training stability when data is limited. The method builds on a base loss function and introduces two gradient-related regularization terms. The first enforces gradient direction consistency to guide parameter updates along task-relevant directions and prevent drift. The second controls gradient magnitude to avoid abnormal updates. Together, these components support a more efficient and stable optimization path. To further improve cross-task generalization, the method incorporates a gradient alignment mechanism. This mechanism measures the consistency between optimization directions of the source and target tasks. It enhances fine-tuning performance in multi-task and cross-domain scenarios. Across various natural language understanding tasks, the method outperforms existing fine-tuning strategies in average accuracy, gradient stability, and directional alignment. Empirical evaluations under different sample sizes and domain-specific tasks confirm the methodâ€™s robustness and broad applicability in low-resource environments. In particular, the method shows clear advantages in controlling parameter update paths. The results demonstrate that a gradient-based fine-tuning framework can effectively leverage the representational power of large language models. It ensures training stability while reducing dependence on large volumes of labeled data. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¢¯åº¦çš„å¾®è°ƒæ–¹æ³•ï¼Œç”¨äºåœ¨å°‘é‡æ ·æœ¬æ¡ä»¶ä¸‹å¯¹å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ç›®æ ‡æ˜¯æé«˜ä»»åŠ¡é€‚åº”æ€§å’Œè®­ç»ƒç¨³å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚è¯¥æ–¹æ³•åŸºäºåŸºç¡€æŸå¤±å‡½æ•°ï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªä¸æ¢¯åº¦ç›¸å…³çš„æ­£åˆ™åŒ–é¡¹ã€‚ç¬¬ä¸€é¡¹å¼ºåˆ¶æ‰§è¡Œæ¢¯åº¦æ–¹å‘ä¸€è‡´æ€§ï¼Œä»¥å¼•å¯¼å‚æ•°æ›´æ–°æ²¿ç€ä»»åŠ¡ç›¸å…³æ–¹å‘è¿›è¡Œï¼Œé˜²æ­¢æ¼‚ç§»ã€‚ç¬¬äºŒé¡¹æ§åˆ¶æ¢¯åº¦å¹…åº¦ï¼Œä»¥é¿å…å¼‚å¸¸æ›´æ–°ã€‚è¿™ä¸¤é¡¹å…±åŒæ”¯æŒæ›´é«˜æ•ˆç¨³å®šçš„ä¼˜åŒ–è·¯å¾„ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¯¥æ–¹æ³•è¿˜èå…¥äº†ä¸€ç§æ¢¯åº¦å¯¹é½æœºåˆ¶ã€‚è¯¥æœºåˆ¶è¡¡é‡æºä»»åŠ¡å’Œç›®æ ‡ä»»åŠ¡ä¼˜åŒ–æ–¹å‘çš„ä¸€è‡´æ€§ï¼Œåœ¨å¤šä»»åŠ¡å’Œè·¨åŸŸåœºæ™¯ä¸­æé«˜äº†å¾®è°ƒæ€§èƒ½ã€‚åœ¨å„ç§è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡å‡†ç¡®ç‡ã€æ¢¯åº¦ç¨³å®šæ€§å’Œæ–¹å‘å¯¹é½æ–¹é¢å‡ä¼˜äºç°æœ‰çš„å¾®è°ƒç­–ç•¥ã€‚åœ¨ä¸åŒæ ·æœ¬é‡å’Œç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ä¸Šçš„å®è¯è¯„ä¼°è¯æ˜äº†è¯¥æ–¹æ³•åœ¨ä½èµ„æºç¯å¢ƒä¸‹çš„ç¨³å¥æ€§å’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚å°¤å…¶æ˜¯ï¼Œè¯¥æ–¹æ³•åœ¨æ§åˆ¶å‚æ•°æ›´æ–°è·¯å¾„æ–¹é¢æ˜¾ç¤ºå‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ¢¯åº¦çš„å¾®è°ƒæ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œç¡®ä¿è®­ç»ƒç¨³å®šæ€§ï¼ŒåŒæ—¶å‡å°‘å¯¹å¤§é‡æ ‡è®°æ•°æ®çš„ä¾èµ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00726v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¢¯åº¦çš„å¾®è°ƒæ–¹æ³•ï¼Œç”¨äºåœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¯¥æ–¹æ³•æ—¨åœ¨æé«˜ä»»åŠ¡é€‚åº”æ€§å’Œè®­ç»ƒç¨³å®šæ€§ï¼Œé€šè¿‡åŸºç¡€æŸå¤±å‡½æ•°å’Œä¸¤ä¸ªä¸æ¢¯åº¦ç›¸å…³çš„æ­£åˆ™åŒ–é¡¹æ¥å®ç°ã€‚ç¬¬ä¸€ä¸ªé¡¹é€šè¿‡å¼ºåˆ¶æ¢¯åº¦æ–¹å‘ä¸€è‡´æ€§æ¥å¼•å¯¼å‚æ•°æ›´æ–°æ²¿ç€ä»»åŠ¡ç›¸å…³æ–¹å‘è¿›è¡Œï¼Œé˜²æ­¢æ¼‚ç§»ï¼›ç¬¬äºŒä¸ªé¡¹æ§åˆ¶æ¢¯åº¦å¹…åº¦ï¼Œé¿å…å¼‚å¸¸æ›´æ–°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¼•å…¥äº†ä¸€ç§æ¢¯åº¦å¯¹é½æœºåˆ¶ï¼Œä»¥æé«˜è·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡å‡†ç¡®ç‡ã€æ¢¯åº¦ç¨³å®šæ€§å’Œæ–¹å‘å¯¹é½æ–¹é¢ä¼˜äºç°æœ‰çš„å¾®è°ƒç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒæ ·æœ¬é‡å’Œç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºç¨³å¥æ€§å’Œå¹¿æ³›çš„åº”ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ§åˆ¶å‚æ•°æ›´æ–°è·¯å¾„æ–¹é¢æ˜¾ç¤ºå‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ¢¯åº¦çš„å¾®è°ƒæ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œç¡®ä¿è®­ç»ƒç¨³å®šæ€§ï¼ŒåŒæ—¶å‡å°‘å¯¹å¤§é‡æ ‡è®°æ•°æ®çš„ä¾èµ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¢¯åº¦çš„å¾®è°ƒæ–¹æ³•ï¼Œé€‚ç”¨äºæ•°æ®æœ‰é™æƒ…å†µä¸‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ–¹æ³•å»ºç«‹åœ¨åŸºç¡€æŸå¤±å‡½æ•°ä¸Šï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªä¸æ¢¯åº¦ç›¸å…³çš„æ­£åˆ™åŒ–é¡¹ï¼Œä»¥æé«˜ä»»åŠ¡é€‚åº”æ€§å’Œè®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>æ¢¯åº¦æ–¹å‘ä¸€è‡´æ€§çš„å¼ºåˆ¶å®æ–½å¼•å¯¼å‚æ•°æ›´æ–°æ²¿ä»»åŠ¡ç›¸å…³æ–¹å‘è¿›è¡Œï¼Œé˜²æ­¢å‚æ•°æ¼‚ç§»ã€‚</li>
<li>æ¢¯åº¦å¹…åº¦çš„æ§åˆ¶é¿å…äº†å¼‚å¸¸æ›´æ–°ï¼Œç¡®ä¿äº†æ›´ç¨³å®šå’Œé«˜æ•ˆçš„ä¼˜åŒ–è·¯å¾„ã€‚</li>
<li>å¼•å…¥çš„æ¢¯åº¦å¯¹é½æœºåˆ¶æé«˜äº†è·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œå°¤å…¶åœ¨å¤šä»»åŠ¡å’Œè·¨åŸŸåœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡å‡†ç¡®ç‡ã€æ¢¯åº¦ç¨³å®šæ€§å’Œæ–¹å‘å¯¹é½æ–¹é¢è¶…è¶Šç°æœ‰å¾®è°ƒç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0363106ed4d73d67a1bf40975cb71ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5645324b47dcbdb7affe504eb2b99fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18e5652c05c8b1cf52f3b494af66d1f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50241e509ee98fd0d63e5f5c34bd803e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Performance-Analysis-of-Few-Shot-Learning-Approaches-for-Bangla-Handwritten-Character-and-Digit-Recognition"><a href="#Performance-Analysis-of-Few-Shot-Learning-Approaches-for-Bangla-Handwritten-Character-and-Digit-Recognition" class="headerlink" title="Performance Analysis of Few-Shot Learning Approaches for Bangla   Handwritten Character and Digit Recognition"></a>Performance Analysis of Few-Shot Learning Approaches for Bangla   Handwritten Character and Digit Recognition</h2><p><strong>Authors:Mehedi Ahamed, Radib Bin Kabir, Tawsif Tashwar Dipto, Mueeze Al Mushabbir, Sabbir Ahmed, Md. Hasanul Kabir</strong></p>
<p>This study investigates the performance of few-shot learning (FSL) approaches in recognizing Bangla handwritten characters and numerals using limited labeled data. It demonstrates the applicability of these methods to scripts with intricate and complex structures, where dataset scarcity is a common challenge. Given the complexity of Bangla script, we hypothesize that models performing well on these characters can generalize effectively to languages of similar or lower structural complexity. To this end, we introduce SynergiProtoNet, a hybrid network designed to improve the recognition accuracy of handwritten characters and digits. The model integrates advanced clustering techniques with a robust embedding framework to capture fine-grained details and contextual nuances. It leverages multi-level (both high- and low-level) feature extraction within a prototypical learning framework. We rigorously benchmark SynergiProtoNet against several state-of-the-art few-shot learning models: BD-CSPN, Prototypical Network, Relation Network, Matching Network, and SimpleShot, across diverse evaluation settings including Monolingual Intra-Dataset Evaluation, Monolingual Inter-Dataset Evaluation, Cross-Lingual Transfer, and Split Digit Testing. Experimental results show that SynergiProtoNet consistently outperforms existing methods, establishing a new benchmark in few-shot learning for handwritten character and digit recognition. The code is available on GitHub: <a target="_blank" rel="noopener" href="https://github.com/MehediAhamed/SynergiProtoNet">https://github.com/MehediAhamed/SynergiProtoNet</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å°æ ·å­¦ä¹ ï¼ˆFSLï¼‰æ–¹æ³•åœ¨åˆ©ç”¨æœ‰é™æ ‡è®°æ•°æ®è¯†åˆ«å­ŸåŠ æ‹‰è¯­æ‰‹å†™å­—ç¬¦å’Œæ•°å­—æ—¶çš„æ€§èƒ½ã€‚å®ƒè¯æ˜äº†è¿™äº›æ–¹æ³•å¯¹äºå…·æœ‰å¤æ‚ç»“æ„çš„è„šæœ¬çš„é€‚ç”¨æ€§ï¼Œå…¶ä¸­æ•°æ®é›†ç¨€ç¼ºæ˜¯ä¸€ä¸ªå¸¸è§çš„æŒ‘æˆ˜ã€‚è€ƒè™‘åˆ°å­ŸåŠ æ‹‰è„šæœ¬çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬å‡è®¾åœ¨è¿™äº›å­—ç¬¦ä¸Šè¡¨ç°è‰¯å¥½çš„æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æ¨å¹¿åˆ°ç»“æ„ç›¸ä¼¼æˆ–è¾ƒä½çš„è¯­è¨€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SynergiProtoNetï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆç½‘ç»œï¼Œæ—¨åœ¨æé«˜æ‰‹å†™å­—ç¬¦å’Œæ•°å­—çš„è¯†åˆ«å‡†ç¡®ç‡ã€‚è¯¥æ¨¡å‹å°†å…ˆè¿›çš„èšç±»æŠ€æœ¯ä¸ç¨³å¥çš„åµŒå…¥æ¡†æ¶ç›¸ç»“åˆï¼Œä»¥æ•æ‰ç»†å¾®çš„ç»†èŠ‚å’Œä¸Šä¸‹æ–‡ç»†å¾®å·®åˆ«ã€‚å®ƒåœ¨åŸå‹å­¦ä¹ æ¡†æ¶å†…åˆ©ç”¨å¤šå±‚æ¬¡ï¼ˆé«˜çº§å’Œä½çº§ï¼‰ç‰¹å¾æå–ã€‚æˆ‘ä»¬å¯¹SynergiProtoNetä¸å‡ ç§æœ€å…ˆè¿›çš„å°æ ·å­¦ä¹ æ¨¡å‹è¿›è¡Œäº†ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬BD-CSPNã€åŸå‹ç½‘ç»œã€å…³ç³»ç½‘ç»œã€åŒ¹é…ç½‘ç»œå’ŒSimpleShotï¼Œåœ¨ä¸åŒçš„è¯„ä¼°ç¯å¢ƒè®¾ç½®ä¸‹ï¼ŒåŒ…æ‹¬å•è¯­å†…æ•°æ®é›†è¯„ä¼°ã€å•è¯­é—´æ•°æ®é›†è¯„ä¼°ã€è·¨è¯­è¨€è¿ç§»å’Œåˆ†å‰²æ•°å­—æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynergiProtoNetå§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æ‰‹å†™å­—ç¬¦å’Œæ•°å­—çš„å°æ ·å­¦ä¹ è¯†åˆ«æ–¹é¢å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚ä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/MehediAhamed/SynergiProtoNet%E3%80%82">https://github.com/MehediAhamed/SynergiProtoNetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00447v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†å°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰æ–¹æ³•åœ¨è¯†åˆ«å­ŸåŠ æ‹‰è¯­æ‰‹å†™å­—ç¬¦å’Œæ•°å­—æ–¹é¢çš„æ€§èƒ½ï¼Œå¹¶åˆ©ç”¨æœ‰é™æ ‡è®°æ•°æ®è¿›è¡Œäº†å®éªŒã€‚ç ”ç©¶å±•ç¤ºäº†è¿™äº›æ–¹æ³•åœ¨å…·æœ‰å¤æ‚ç»“æ„çš„è„šæœ¬ä¸­çš„åº”ç”¨ï¼Œå…¶ä¸­æ•°æ®é›†ç¨€ç¼ºæ˜¯ä¸€ä¸ªå¸¸è§æŒ‘æˆ˜ã€‚é’ˆå¯¹å­ŸåŠ æ‹‰è¯­è„šæœ¬çš„å¤æ‚æ€§ï¼Œç ”ç©¶å‡è®¾åœ¨å­—ç¬¦ä¸Šè¡¨ç°è‰¯å¥½çš„æ¨¡å‹å¯ä»¥æ¨å¹¿åˆ°ç»“æ„ç›¸ä¼¼æˆ–è¾ƒä½çš„è¯­è¨€ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºSynergiProtoNetçš„æ··åˆç½‘ç»œï¼Œæ—¨åœ¨æé«˜æ‰‹å†™å­—ç¬¦å’Œæ•°å­—çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å…ˆè¿›çš„èšç±»æŠ€æœ¯å’Œç¨³å¥çš„åµŒå…¥æ¡†æ¶ï¼Œä»¥æ•æ‰ç»†å¾®çš„ç»†èŠ‚å’Œä¸Šä¸‹æ–‡ç»†å¾®å·®åˆ«ã€‚å®ƒåœ¨åŸå‹å­¦ä¹ æ¡†æ¶å†…åˆ©ç”¨å¤šå±‚æ¬¡ï¼ˆé«˜ä½å±‚æ¬¡ï¼‰çš„ç‰¹å¾æå–ã€‚æœ¬ç ”ç©¶å¯¹SynergiProtoNetä¸å‡ ç§å…ˆè¿›çš„å°æ ·æœ¬å­¦ä¹ æ¨¡å‹è¿›è¡Œäº†ä¸¥æ ¼çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬BD-CSPNã€åŸå‹ç½‘ç»œã€å…³ç³»ç½‘ç»œã€åŒ¹é…ç½‘ç»œå’ŒSimpleShotç­‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSynergiProtoNetåœ¨å¤šç§è¯„ä¼°è®¾ç½®ä¸‹å‡è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºæ‰‹å†™å­—ç¬¦å’Œæ•°å­—è¯†åˆ«çš„å°æ ·æœ¬å­¦ä¹ æ ‘ç«‹äº†æ–°åŸºå‡†ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ¢è®¨äº†å°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰åœ¨è¯†åˆ«å­ŸåŠ æ‹‰è¯­æ‰‹å†™å­—ç¬¦å’Œæ•°å­—æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºSynergiProtoNetçš„æ··åˆç½‘ç»œæ¨¡å‹ï¼Œç”¨äºæé«˜æ‰‹å†™å­—ç¬¦å’Œæ•°å­—çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚</li>
<li>SynergiProtoNetç»“åˆäº†å…ˆè¿›çš„èšç±»æŠ€æœ¯å’Œç¨³å¥çš„åµŒå…¥æ¡†æ¶ï¼Œæ•æ‰ç»†å¾®çš„ç»†èŠ‚å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨åŸå‹å­¦ä¹ æ¡†æ¶å†…åˆ©ç”¨å¤šå±‚æ¬¡ç‰¹å¾æå–ã€‚</li>
<li>ç ”ç©¶å¯¹SynergiProtoNetä¸å¤šç§å…ˆè¿›çš„å°æ ·æœ¬å­¦ä¹ æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSynergiProtoNetåœ¨æ‰‹å†™å­—ç¬¦å’Œæ•°å­—è¯†åˆ«çš„æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-75744f96276b56230e6f60420dffa6ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2736f5d16ef7023e2d861ea77ef38969.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bdc698ff98c2b44fd9ee79ae451947c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Material-Recognition-from-Local-Appearance"><a href="#Hierarchical-Material-Recognition-from-Local-Appearance" class="headerlink" title="Hierarchical Material Recognition from Local Appearance"></a>Hierarchical Material Recognition from Local Appearance</h2><p><strong>Authors:Matthew Beveridge, Shree K. Nayar</strong></p>
<p>We introduce a taxonomy of materials for hierarchical recognition from local appearance. Our taxonomy is motivated by vision applications and is arranged according to the physical traits of materials. We contribute a diverse, in-the-wild dataset with images and depth maps of the taxonomy classes. Utilizing the taxonomy and dataset, we present a method for hierarchical material recognition based on graph attention networks. Our model leverages the taxonomic proximity between classes and achieves state-of-the-art performance. We demonstrate the modelâ€™s potential to generalize to adverse, real-world imaging conditions, and that novel views rendered using the depth maps can enhance this capability. Finally, we show the modelâ€™s capacity to rapidly learn new materials in a few-shot learning setting. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºå±€éƒ¨å¤–è§‚è¿›è¡Œå±‚æ¬¡è¯†åˆ«çš„ææ–™åˆ†ç±»æ³•ã€‚æˆ‘ä»¬çš„åˆ†ç±»æ³•å—åˆ°è§†è§‰åº”ç”¨çš„å¯å‘ï¼Œå¹¶æ ¹æ®ææ–™çš„ç‰©ç†ç‰¹å¾è¿›è¡Œæ’åˆ—ã€‚æˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªåŒ…å«åˆ†ç±»å›¾åƒå’Œæ·±åº¦å›¾çš„å¤šæ ·åŒ–ã€é‡å¤–æ•°æ®é›†ã€‚åˆ©ç”¨åˆ†ç±»æ³•å’Œæ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œçš„å±‚æ¬¡ææ–™è¯†åˆ«æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨ç±»ä¹‹é—´çš„åˆ†ç±»æ¥è¿‘æ€§ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¯æ˜äº†è¯¥æ¨¡å‹åœ¨æ¶åŠ£çš„ã€çœŸå®ä¸–ç•Œçš„æˆåƒæ¡ä»¶ä¸‹å…·æœ‰æ³›åŒ–æ½œåŠ›ï¼Œå¹¶ä¸”ä½¿ç”¨æ·±åº¦å›¾å‘ˆç°çš„æ–°è§†è§’å¯ä»¥å¢å¼ºè¿™ç§èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨å°‘é‡å­¦ä¹ æ¡ä»¶ä¸‹å¿«é€Ÿå­¦ä¹ æ–°ææ–™çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22911v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå±€éƒ¨å¤–è§‚çš„ç”¨äºå±‚æ¬¡è¯†åˆ«çš„ææ–™åˆ†ç±»ä½“ç³»ã€‚è¯¥åˆ†ç±»ä½“ç³»å—è§†è§‰åº”ç”¨çš„å¯å‘ï¼Œæ ¹æ®ææ–™çš„ç‰©ç†ç‰¹å¾è¿›è¡Œæ’åˆ—ç»„åˆã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è´¡çŒ®äº†ä¸€ä¸ªåŒ…å«åˆ†ç±»å›¾åƒå’Œæ·±åº¦å›¾çš„é‡ç”Ÿæ•°æ®é›†ã€‚åŸºäºè¯¥åˆ†ç±»ä½“ç³»å’Œæ•°æ®é›†ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œçš„å±‚æ¬¡ææ–™è¯†åˆ«æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç±»ä¹‹é—´çš„åˆ†ç±»é‚»è¿‘æ€§ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨æ¶åŠ£çš„ã€çœŸå®ä¸–ç•Œçš„æˆåƒæ¡ä»¶ä¸‹æ³›åŒ–çš„æ½œåŠ›ï¼Œå¹¶ä½¿ç”¨æ·±åº¦å›¾æ¸²æŸ“çš„æ–°è§†è§’å¯ä»¥å¢å¼ºè¿™ä¸€èƒ½åŠ›ã€‚æœ€åï¼Œå±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ç¯å¢ƒä¸‹å¿«é€Ÿå­¦ä¹ æ–°ææ–™çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§åŸºäºç‰©ç†ç‰¹æ€§çš„ææ–™åˆ†ç±»ä½“ç³»ï¼Œé€‚ç”¨äºè§†è§‰åº”ç”¨ã€‚</li>
<li>æä¾›äº†ä¸€ä¸ªåŒ…å«å›¾åƒå’Œæ·±åº¦å›¾çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œç”¨äºææ–™çš„å±‚æ¬¡è¯†åˆ«ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œçš„å±‚æ¬¡ææ–™è¯†åˆ«æ–¹æ³•ã€‚</li>
<li>æ¨¡å‹åˆ©ç”¨ç±»ä¹‹é—´çš„åˆ†ç±»é‚»è¿‘æ€§ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨æ¶åŠ£çš„ã€çœŸå®ä¸–ç•Œçš„æˆåƒæ¡ä»¶ä¸‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨æ·±åº¦å›¾æ¸²æŸ“çš„æ–°è§†è§’å¯ä»¥å¢å¼ºæ¨¡å‹çš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f459dc740155f935ef0d5c93157501c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e54ed452c901b8dabf804d9cf39e9a12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75af70afb0c744287fdb407193bfcd29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e6dac87a7c92374ba073039029d16d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c6fb40fb9641761d614d821c391e191.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Similarity-Paradigm-Through-Textual-Regularization-Without-Forgetting"><a href="#A-Similarity-Paradigm-Through-Textual-Regularization-Without-Forgetting" class="headerlink" title="A Similarity Paradigm Through Textual Regularization Without Forgetting"></a>A Similarity Paradigm Through Textual Regularization Without Forgetting</h2><p><strong>Authors:Fangming Cui, Jan Fong, Rongfei Zeng, Xinmei Tian, Jun Yu</strong></p>
<p>Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks. While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions. It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts. In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting. SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework. 1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features. 2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization. Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts. Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods. </p>
<blockquote>
<p>æç¤ºå­¦ä¹ å·²æˆä¸ºå°†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€‚åº”å¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚è™½ç„¶ä¼˜åŒ–ä¸Šä¸‹æ–‡å¯¹äºæé«˜ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½å¯èƒ½æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å®ƒå¾€å¾€ä¼šå¯¼è‡´å¯¹æ¥è‡ªä¸åŒåˆ†å¸ƒçš„æœªè§ç±»åˆ«æˆ–æ•°æ®é›†çš„æ³›åŒ–æ€§èƒ½ä¸ä½³ã€‚è¿™å¯èƒ½æ˜¯ç”±äºæ–‡æœ¬æç¤ºå€¾å‘äºè¿‡åº¦æ‹Ÿåˆä¸‹æ¸¸æ•°æ®åˆ†å¸ƒï¼Œå¯¼è‡´å¿˜è®°æ‰‹å·¥åˆ¶ä½œçš„æç¤ºä¸­å¾—å‡ºçš„é€šç”¨çŸ¥è¯†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºåŸºäºæ–‡æœ¬æ­£åˆ™åŒ–çš„ç›¸ä¼¼æ€§èŒƒå¼ï¼ˆSPTRï¼‰çš„æ–°å‹æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œä»¥å®ç°æ— é—å¿˜çš„æç¤ºå­¦ä¹ ã€‚SPTRæ˜¯ä¸€ä¸ªåŸºäºæ‰‹å·¥åˆ¶ä½œçš„æç¤ºçš„ä¸¤é¢è®¾è®¡ï¼Œæ˜¯ä¸€ä¸ªä¸å¯åˆ†å‰²çš„æ¡†æ¶ã€‚é¦–å…ˆï¼Œä¸ºäº†é¿å…å¿˜è®°ä¸€èˆ¬çš„æ–‡æœ¬çŸ¥è¯†ï¼Œæˆ‘ä»¬å¼•å…¥äº†æœ€ä¼˜ä¼ è¾“ä½œä¸ºæ–‡æœ¬æ­£åˆ™åŒ–æ¥ç²¾ç»†åœ°ç¡®ä¿ä¸æ‰‹å·¥åˆ¶ä½œçš„åŠŸèƒ½çš„è¿‘ä¼¼å’Œè°ƒæ•´æ–‡æœ¬ç‰¹å¾ã€‚å…¶æ¬¡ï¼Œä¸ºäº†ä¸æ–­é‡Šæ”¾å¤šä¸ªæ‰‹å·¥åˆ¶ä½œçš„æç¤ºçš„é€šç”¨èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªç„¶å¯¹é½å¾—åˆ†å’Œå¯¹æŠ—æ€§å¯¹é½å¾—åˆ†çš„ç›¸ä¼¼æ€§èŒƒå¼ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ä¸¤ä¸ªæ¨¡å—çš„å…±åŒç›®æ ‡æ˜¯è§£å†³æ³›åŒ–é—®é¢˜ï¼Œæ—¨åœ¨ä»å¤šä¸ªæ‰‹å·¥åˆ¶ä½œçš„æç¤ºä¸­æœ€å¤§åŒ–æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è·¨è¶Š11ä¸ªæ•°æ®é›†çš„å››ä¸ªä»£è¡¨æ€§ä»»åŠ¡ï¼ˆå³éæ³›åŒ–çš„å°æ ·æœ¬å­¦ä¹ ã€åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–ã€è·¨æ•°æ®é›†æ³›åŒ–å’ŒåŸŸæ³›åŒ–ï¼‰ä¸Šè¯æ˜äº†SPTRä¼˜äºç°æœ‰çš„æç¤ºå­¦ä¹ æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14376v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åœ¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­ä½¿ç”¨æç¤ºå­¦ä¹ ï¼ˆPrompt Learningï¼‰æ–¹æ³•æ—¶é‡åˆ°çš„æŒ‘æˆ˜ã€‚å°½ç®¡ä¼˜åŒ–ä¸Šä¸‹æ–‡å¯¹äºæé«˜ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½å¯èƒ½æœ‰æ•ˆï¼Œä½†å®ƒå¯èƒ½å¯¼è‡´åœ¨æœªè§çš„ç±»åˆ«æˆ–æ•°æ®é›†ä¸Šçš„æ³›åŒ–æ€§èƒ½ä¸‹é™ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºç›¸ä¼¼æ€§èŒƒå¼ä¸‹çš„æ–‡æœ¬æ­£åˆ™åŒ–ï¼ˆSPTRï¼‰çš„æ–°å‹æ–¹æ³•æ¥è§£å†³æç¤ºå­¦ä¹ ä¸­çš„é—å¿˜é—®é¢˜ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼Œé‡‡ç”¨æ‰‹å†™æç¤ºå½¢å¼çš„ä¸åˆ†ç¦»æ¡†æ¶ï¼šé¦–å…ˆå¼•å…¥æœ€ä½³ä¼ è¾“ä½œä¸ºæ–‡æœ¬æ­£åˆ™åŒ–ï¼Œç¡®ä¿æ‰‹å†™ç‰¹å¾å’Œè°ƒæ•´æ–‡æœ¬ç‰¹å¾çš„è¿‘ä¼¼åŒ¹é…ï¼›å…¶æ¬¡æå‡ºç›¸ä¼¼æ€§èŒƒå¼æ¥æ”¹è¿›æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•çš„ç›®çš„æ˜¯ä»å¤šä¸ªæ‰‹å†™æç¤ºä¸­æœ€å¤§é™åº¦åœ°æé«˜æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è·¨è¶Šå¤šä¸ªæ•°æ®é›†çš„å››é¡¹ä»£è¡¨æ€§ä»»åŠ¡ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒSPTRä¼˜äºç°æœ‰çš„æç¤ºå­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬ä¸­ä¸ƒä¸ªé‡è¦çš„è§‚ç‚¹æˆ–å‘ç°ï¼š</p>
<ol>
<li>æç¤ºå­¦ä¹ åœ¨è§†è§‰è¯­è¨€æ¨¡å‹é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ä¼˜åŒ–ä¸Šä¸‹æ–‡è™½ç„¶èƒ½æé«˜ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼Œä½†å¯èƒ½å¯¼è‡´æ³›åŒ–æ€§èƒ½ä¸‹é™ã€‚</li>
<li>SPTRæ–¹æ³•æ—¨åœ¨è§£å†³æç¤ºå­¦ä¹ ä¸­çš„é—å¿˜é—®é¢˜ã€‚</li>
<li>SPTRåŒ…å«ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ï¼šæ–‡æœ¬æ­£åˆ™åŒ–å’Œç›¸ä¼¼æ€§èŒƒå¼ã€‚</li>
<li>æ–‡æœ¬æ­£åˆ™åŒ–ç¡®ä¿æ‰‹å†™ç‰¹å¾å’Œè°ƒæ•´æ–‡æœ¬ç‰¹å¾çš„è¿‘ä¼¼åŒ¹é…ã€‚</li>
<li>ç›¸ä¼¼æ€§èŒƒå¼æ—¨åœ¨æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6af21f4788380094690e1cc458b25340.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ca0670cbf65a815276bae63f283e896.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b668efe2973ac72089818093d14a2947.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79c4d3f7f00da30844efae3481bd785f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26942adab36734fbc0c96371ae565df0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-131d0a7b7cd8e42c27e14d985dc229fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d438bb3c665833358ce8da713d9500a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Are-Transformers-Able-to-Reason-by-Connecting-Separated-Knowledge-in-Training-Data"><a href="#Are-Transformers-Able-to-Reason-by-Connecting-Separated-Knowledge-in-Training-Data" class="headerlink" title="Are Transformers Able to Reason by Connecting Separated Knowledge in   Training Data?"></a>Are Transformers Able to Reason by Connecting Separated Knowledge in   Training Data?</h2><p><strong>Authors:Yutong Yin, Zhaoran Wang</strong></p>
<p>Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B &#x3D; f(A) ) from one source and ( C &#x3D; g(B) ) from another, they can deduce ( C&#x3D;g(B)&#x3D;g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, â€œFTCTâ€ (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing. </p>
<blockquote>
<p>äººç±»èƒ½å¤Ÿé€šè¿‡æ•´åˆæ¥è‡ªä¸åŒæ¥æºçš„çŸ¥è¯†å±•ç°å‡ºæƒŠäººçš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ‰äººä»æŸä¸€æ¥æºå­¦ä¹ åˆ°ï¼ˆB&#x3D;fï¼ˆAï¼‰ï¼‰å¹¶ä»å¦ä¸€æ¥æºå­¦ä¹ åˆ°ï¼ˆC&#x3D;gï¼ˆBï¼‰ï¼‰ï¼Œä»–ä»¬å³ä½¿åœ¨æ²¡æœ‰åŒæ—¶é‡åˆ°ï¼ˆABCï¼‰çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½æ¨å¯¼å‡ºï¼ˆC&#x3D;gï¼ˆBï¼‰&#x3D;gï¼ˆfï¼ˆAï¼‰ï¼‰ï¼‰ï¼Œè¿™å±•ç°äº†äººç±»æ™ºåŠ›çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹åˆæˆå­¦ä¹ ä»»åŠ¡â€œFTCTâ€ï¼ˆè®­ç»ƒæ—¶ç‰‡æ®µåŒ–ï¼Œæµ‹è¯•æ—¶é“¾æ¥ï¼‰ï¼Œä»¥éªŒè¯Transformerå¤åˆ¶è¿™é¡¹æŠ€èƒ½çš„æ½œåŠ›å¹¶è§£é‡Šå…¶å†…åœ¨æœºåˆ¶ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæ•°æ®ç”±æ¥è‡ªæ•´ä½“å› æœå›¾çš„åˆ†ç¦»çŸ¥è¯†ç‰‡æ®µç»„æˆã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼ŒTransformerå¿…é¡»é€šè¿‡æ•´åˆè¿™äº›ç‰‡æ®µæ¥æ¨æ–­å®Œæ•´çš„å› æœå›¾è½¨è¿¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®ä¸­æœªå‡ºç°æ­£ç¡®çš„ç‰‡æ®µç»„åˆï¼Œé€šè¿‡å°‘é‡æ€ç»´é“¾æç¤ºçš„Transformerä¹Ÿèƒ½åœ¨FTCTä¸Šè¡¨ç°å‡ºç»„åˆæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç»„åˆæ¨ç†èƒ½åŠ›çš„å‡ºç°ä¸æ¨¡å‹å¤æ‚åº¦å’Œè®­ç»ƒ-æµ‹è¯•æ•°æ®ç›¸ä¼¼æ€§ä¹‹é—´å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ã€‚æˆ‘ä»¬ä»ç†è®ºå’Œå®è·µä¸¤æ–¹é¢æå‡ºï¼ŒTransformerä»è®­ç»ƒä¸­å­¦ä¹ äº†ä¸€ä¸ªé€šç”¨çš„åŸºç¡€ç¨‹åºï¼Œä»è€Œåœ¨æµ‹è¯•è¿‡ç¨‹ä¸­å®ç°äº†æœ‰æ•ˆçš„ç»„åˆæ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15857v7">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†äººç±»å…·å¤‡å°†ä¸åŒæ¥æºçš„çŸ¥è¯†è¿›è¡Œæ•´åˆè¿›è¡Œæ¨ç†çš„å‡ºè‰²èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ä¸€ç§åä¸ºâ€œFTCTâ€ï¼ˆè®­ç»ƒæ—¶ç¢ç‰‡åŒ–ï¼Œæµ‹è¯•æ—¶é“¾æ¥ï¼‰çš„åˆæˆå­¦ä¹ ä»»åŠ¡ï¼ŒéªŒè¯äº†Transformeræ¨¡å‹åœ¨å¤åˆ¶æ­¤æŠ€èƒ½æ–¹é¢çš„æ½œåŠ›å¹¶è§£é‡Šäº†å…¶å†…åœ¨æœºåˆ¶ã€‚è®­ç»ƒé˜¶æ®µæ•°æ®æ¥è‡ªæ•´ä½“çš„å› æœå›¾ä¸­çš„çŸ¥è¯†ç‰‡æ®µï¼Œè€Œåœ¨æµ‹è¯•é˜¶æ®µï¼ŒTransformerå¿…é¡»é€šè¿‡æ•´åˆè¿™äº›ç‰‡æ®µæ¥æ¨æ–­å®Œæ•´çš„å› æœå›¾è½¨è¿¹ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯åªä½¿ç”¨å°‘æ•°Chain-of-Thoughtæç¤ºä¹Ÿèƒ½è®©Transformeråœ¨FTCTä¸Šæ‰§è¡Œç»„åˆæ¨ç†ï¼Œæ­ç¤ºæ­£ç¡®çš„ç‰‡æ®µç»„åˆï¼Œå³ä¾¿è¿™äº›ç»„åˆåœ¨è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å¤æ‚æ€§å’Œè®­ç»ƒæµ‹è¯•æ•°æ®çš„ç›¸ä¼¼æ€§å¯¹ç»„åˆæ¨ç†èƒ½åŠ›çš„å‡ºç°æœ‰å¼ºçƒˆå½±å“ã€‚ä½œè€…è®¤ä¸ºTransformerèƒ½ä»è®­ç»ƒä¸­å­¦ä¹ åˆ°ä¸€ä¸ªå¯æ¦‚æ‹¬çš„åº•å±‚ç¨‹åºï¼Œå¹¶åœ¨æµ‹è¯•é˜¶æ®µè¿›è¡Œæœ‰æ•ˆçš„ç»„åˆæ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»å…·å¤‡å¼ºå¤§çš„ç»„åˆæ¨ç†èƒ½åŠ›ï¼Œèƒ½ä»ä¸åŒæ¥æºçš„çŸ¥è¯†ä¸­æ•´åˆä¿¡æ¯å¹¶è¿›è¡Œæ¨ç†ã€‚</li>
<li>â€œFTCTâ€ä»»åŠ¡è¢«è®¾è®¡ç”¨äºéªŒè¯Transformeræ¨¡å‹æ˜¯å¦å…·å¤‡è¿™ç§ç»„åˆæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨FTCTä»»åŠ¡ä¸­ï¼ŒTransformeråœ¨æµ‹è¯•é˜¶æ®µèƒ½é€šè¿‡æ•´åˆè®­ç»ƒé˜¶æ®µè·å¾—çš„çŸ¥è¯†ç‰‡æ®µæ¥æ¨æ–­å®Œæ•´çš„å› æœå›¾è½¨è¿¹ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å°‘é‡Chain-of-Thoughtæç¤ºï¼ŒTransformerèƒ½æ‰§è¡Œç»„åˆæ¨ç†å¹¶æ­ç¤ºæ­£ç¡®çš„çŸ¥è¯†ç‰‡æ®µç»„åˆã€‚</li>
<li>ç»„åˆæ¨ç†èƒ½åŠ›çš„å‡ºç°ä¸æ¨¡å‹çš„å¤æ‚æ€§å’Œè®­ç»ƒæµ‹è¯•æ•°æ®çš„ç›¸ä¼¼æ€§å¯†åˆ‡ç›¸å…³ã€‚</li>
<li>Transformeråœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½å­¦ä¹ åˆ°ä¸€ä¸ªå¯æ¦‚æ‹¬çš„åº•å±‚ç¨‹åºï¼Œè¯¥ç¨‹åºæœ‰åŠ©äºåœ¨æµ‹è¯•é˜¶æ®µè¿›è¡Œæœ‰æ•ˆçš„ç»„åˆæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9dd5545de3690fe5a7146f2f4eec2086.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1c1b5dbd935aa5291362d8ec4ddd356.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e4d424947576c136c0ec5353921e72a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Can-Input-Attributions-Explain-Inductive-Reasoning-in-In-Context-Learning"><a href="#Can-Input-Attributions-Explain-Inductive-Reasoning-in-In-Context-Learning" class="headerlink" title="Can Input Attributions Explain Inductive Reasoning in In-Context   Learning?"></a>Can Input Attributions Explain Inductive Reasoning in In-Context   Learning?</h2><p><strong>Authors:Mengyu Ye, Tatsuki Kuribayashi, Goro Kobayashi, Jun Suzuki</strong></p>
<p>Interpreting the internal process of neural models has long been a challenge. This challenge remains relevant in the era of large language models (LLMs) and in-context learning (ICL); for example, ICL poses a new issue of interpreting which example in the few-shot examples contributed to identifying&#x2F;solving the task. To this end, in this paper, we design synthetic diagnostic tasks of inductive reasoning, inspired by the generalization tests typically adopted in psycholinguistics. Here, most in-context examples are ambiguous w.r.t. their underlying rule, and one critical example disambiguates it. The question is whether conventional input attribution (IA) methods can track such a reasoning process, i.e., identify the influential example, in ICL. Our experiments provide several practical findings; for example, a certain simple IA method works the best, and the larger the model, the generally harder it is to interpret the ICL with gradient-based IA methods. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œæ¨¡å‹çš„å†…éƒ¨è¿‡ç¨‹è§£é‡Šä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™ä¸ªæŒ‘æˆ˜åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„æ—¶ä»£ä»ç„¶å…·æœ‰ç°å®æ„ä¹‰ã€‚ä¾‹å¦‚ï¼ŒICLæå‡ºäº†ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼Œå³è§£é‡Šå°‘æ•°ä¾‹å­ä¸­çš„å“ªä¸€ä¸ªä¾‹å­å¯¹è¯†åˆ«&#x2F;è§£å†³ä»»åŠ¡æœ‰æ‰€è´¡çŒ®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡è®¾è®¡äº†åŸºäºå¿ƒç†è¯­è¨€å­¦ä¸­é€šå¸¸é‡‡ç”¨çš„æ¦‚æ‹¬æµ‹è¯•çš„å½’çº³æ¨ç†çš„åˆæˆè¯Šæ–­ä»»åŠ¡ã€‚åœ¨è¿™é‡Œï¼Œå¤§å¤šæ•°ä¸Šä¸‹æ–‡ä¾‹å­åœ¨åŸºç¡€è§„åˆ™æ–¹é¢éƒ½æ˜¯æ¨¡ç³Šçš„ï¼Œåªæœ‰ä¸€ä¸ªå…³é”®ä¾‹å­å¯ä»¥æ¶ˆé™¤æ­§ä¹‰ã€‚é—®é¢˜æ˜¯ä¼ ç»Ÿçš„è¾“å…¥å½’å› ï¼ˆIAï¼‰æ–¹æ³•æ˜¯å¦èƒ½å¤Ÿè¿½è¸ªè¿™æ ·çš„æ¨ç†è¿‡ç¨‹ï¼Œå³è¯†åˆ«å‡ºä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„å…³é”®ä¾‹å­ã€‚æˆ‘ä»¬çš„å®éªŒå¾—å‡ºäº†å‡ ä¸ªå®é™…å‘ç°ï¼šä¾‹å¦‚ï¼ŒæŸç§ç®€å•çš„IAæ–¹æ³•æ•ˆæœæœ€ä½³ï¼Œæ¨¡å‹è¶Šå¤§ï¼ŒåŸºäºæ¢¯åº¦çš„IAæ–¹æ³•æ¥è§£é‡Šå…¶è¯­å¢ƒå«ä¹‰å¾€å¾€æ›´åŠ å›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15628v4">PDF</a> Findings of ACL 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è®¾è®¡äº†ä¸€ç³»åˆ—åˆæˆè¯Šæ–­ä»»åŠ¡ï¼Œä»¥æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œå­¦ä¹ æ—¶çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚ç ”ç©¶å‘ç°ï¼Œä¼ ç»Ÿçš„è¾“å…¥å½’å› æ–¹æ³•å¯èƒ½æ— æ³•æœ‰æ•ˆè¿½è¸ªè¿™äº›æ¨ç†è¿‡ç¨‹ï¼Œä¸€ä¸ªç®€å•çš„è¾“å…¥å½’å› æ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œå¹¶ä¸”æ¨¡å‹è§„æ¨¡è¶Šå¤§ï¼Œä½¿ç”¨åŸºäºæ¢¯åº¦çš„è¾“å…¥å½’å› æ–¹æ³•æ¥è§£é‡Šå…¶åœ¨ä¸Šä¸‹æ–‡ä¸­çš„å­¦ä¹ éš¾åº¦é€šå¸¸è¶Šé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¾è®¡åˆæˆè¯Šæ–­ä»»åŠ¡ä»¥æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å¤§å¤šæ•°ä¸Šä¸‹æ–‡ç¤ºä¾‹å¯¹äºå…¶éšå«çš„è§„åˆ™æ˜¯æ¨¡ç³Šçš„ï¼Œä¸€ä¸ªå…³é”®ç¤ºä¾‹èµ·åˆ°äº†æ¾„æ¸…ä½œç”¨ã€‚</li>
<li>ä¼ ç»Ÿçš„è¾“å…¥å½’å› æ–¹æ³•å¯èƒ½æ— æ³•è¿½è¸ªæ¨ç†è¿‡ç¨‹ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„æ–¹æ³•æ¥è¯†åˆ«å½±å“æ€§çš„ç¤ºä¾‹ã€‚</li>
<li>ä¸€ä¸ªç®€å•çš„è¾“å…¥å½’å› æ–¹æ³•è¡¨ç°æœ€ä½³ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡è¶Šå¤§ï¼Œä½¿ç”¨åŸºäºæ¢¯åº¦çš„è¾“å…¥å½’å› æ–¹æ³•æ¥è§£é‡Šå…¶åœ¨ä¸Šä¸‹æ–‡ä¸­çš„å­¦ä¹ éš¾åº¦è¶Šé«˜ã€‚</li>
<li>ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„è§£é‡Šæ€§æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ï¼Œéœ€è¦æ›´å¤šç ”ç©¶æ¥æ¢ç´¢æœ‰æ•ˆçš„è§£é‡Šæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9f7b494f5e12087069c1b415a9e13edb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43f3662c7cda0c8dd5fd9ce073068d8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b70ad7af86f752f3e1108c541d7e7e7.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Hyperband-based-Bayesian-Optimization-for-Black-box-Prompt-Selection"><a href="#Hyperband-based-Bayesian-Optimization-for-Black-box-Prompt-Selection" class="headerlink" title="Hyperband-based Bayesian Optimization for Black-box Prompt Selection"></a>Hyperband-based Bayesian Optimization for Black-box Prompt Selection</h2><p><strong>Authors:Lennart Schneider, Martin Wistuba, Aaron Klein, Jacek Golebiowski, Giovanni Zappella, Felice Antonio Merra</strong></p>
<p>Optimal prompt selection is crucial for maximizing large language model (LLM) performance on downstream tasks, especially in black-box settings where models are only accessible via APIs. Black-box prompt selection is challenging due to potentially large, combinatorial search spaces, absence of gradient information, and high evaluation cost of prompts on a validation set. We propose HbBoPs, a novel method that combines a structural-aware deep kernel Gaussian Process with Hyperband as a multi-fidelity scheduler to efficiently select prompts. HbBoPs uses embeddings of instructions and few-shot exemplars, treating them as modular components within prompts. This enhances the surrogate modelâ€™s ability to predict which prompt to evaluate next in a sample-efficient manner. Hyperband improves query-efficiency by adaptively allocating resources across different fidelity levels, reducing the number of validation instances required for evaluating prompts. Extensive experiments across ten diverse benchmarks and three LLMs demonstrate that HbBoPs outperforms state-of-the-art methods in both performance and efficiency. </p>
<blockquote>
<p>æœ€ä¼˜æç¤ºé€‰æ‹©å¯¹äºæœ€å¤§åŒ–ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åªèƒ½é€šè¿‡APIè®¿é—®æ¨¡å‹çš„é»‘ç®±ç¯å¢ƒä¸­æ›´æ˜¯å¦‚æ­¤ã€‚é»‘ç®±æç¤ºé€‰æ‹©å…·æœ‰æŒ‘æˆ˜æ€§ï¼ŒåŸå› åœ¨äºæ½œåœ¨çš„åºå¤§ç»„åˆæœç´¢ç©ºé—´ã€ç¼ºå°‘æ¢¯åº¦ä¿¡æ¯ä»¥åŠæç¤ºåœ¨éªŒè¯é›†ä¸Šçš„é«˜è¯„ä¼°æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†HbBoPsï¼Œè¿™æ˜¯ä¸€ç§å°†ç»“æ„æ„ŸçŸ¥æ·±åº¦å†…æ ¸é«˜æ–¯è¿‡ç¨‹ä¸Hyperbandå¤šä¿çœŸè°ƒåº¦å™¨ç›¸ç»“åˆçš„æ–°å‹æ–¹æ³•ï¼Œä»¥é«˜æ•ˆé€‰æ‹©æç¤ºã€‚HbBoPsä½¿ç”¨æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹çš„åµŒå…¥ï¼Œå°†å®ƒä»¬è§†ä¸ºæç¤ºä¸­çš„æ¨¡å—åŒ–ç»„ä»¶ã€‚è¿™å¢å¼ºäº†æ›¿ä»£æ¨¡å‹ä»¥æ ·æœ¬é«˜æ•ˆçš„æ–¹å¼é¢„æµ‹åº”è¯„ä¼°å“ªä¸ªæç¤ºçš„èƒ½åŠ›ã€‚Hyperbandé€šè¿‡è‡ªé€‚åº”åœ°åœ¨ä¸åŒä¿çœŸåº¦çº§åˆ«ä¸Šåˆ†é…èµ„æºæ¥æé«˜æŸ¥è¯¢æ•ˆç‡ï¼Œå‡å°‘äº†è¯„ä¼°æç¤ºæ‰€éœ€çš„éªŒè¯å®ä¾‹æ•°é‡ã€‚åœ¨åä¸ªä¸åŒåŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªLLMä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHbBoPsåœ¨æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07820v2">PDF</a> Accepted at ICML 2025. 26 pages, 11 tables, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œæœ€ä¼˜æç¤ºé€‰æ‹©çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åªèƒ½é€šè¿‡APIè®¿é—®æ¨¡å‹çš„é»‘ç®±è®¾ç½®ä¸­ã€‚é’ˆå¯¹é»‘ç®±æç¤ºé€‰æ‹©é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æœç´¢ç©ºé—´å¯èƒ½å¾ˆå¤§ã€ç¼ºå°‘æ¢¯åº¦ä¿¡æ¯å’Œæç¤ºéªŒè¯æˆæœ¬é«˜ç­‰é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æç¤ºé€‰æ‹©æ–¹æ³•HbBoPsã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç»“æ„æ„ŸçŸ¥æ·±åº¦æ ¸é«˜æ–¯è¿‡ç¨‹ä¸Hyperbandå¤šä¿çœŸè°ƒåº¦å™¨ï¼Œä»¥é«˜æ•ˆé€‰æ‹©æç¤ºã€‚HbBoPsä½¿ç”¨æŒ‡ä»¤å’Œå°‘é‡èŒƒä¾‹çš„åµŒå…¥ï¼Œå°†å…¶ä½œä¸ºæç¤ºä¸­çš„æ¨¡å—åŒ–ç»„ä»¶ã€‚è¿™å¢å¼ºäº†æ›¿ä»£æ¨¡å‹ä»¥æ ·æœ¬é«˜æ•ˆçš„æ–¹å¼é¢„æµ‹åº”è¯„ä¼°å“ªä¸ªæç¤ºçš„èƒ½åŠ›ã€‚Hyperbandé€šè¿‡è‡ªé€‚åº”åˆ†é…ä¸åŒä¿çœŸåº¦çº§åˆ«çš„èµ„æºæ¥æé«˜æŸ¥è¯¢æ•ˆç‡ï¼Œå‡å°‘äº†è¯„ä¼°æç¤ºæ‰€éœ€çš„éªŒè¯å®ä¾‹æ•°é‡ã€‚åœ¨åä¸ªä¸åŒåŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒHbBoPsåœ¨æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å¼ºè°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œæœ€ä¼˜æç¤ºé€‰æ‹©çš„é‡è¦æ€§ã€‚</li>
<li>é»‘ç®±ç¯å¢ƒä¸­è¿›è¡Œæç¤ºé€‰æ‹©é¢ä¸´æœç´¢ç©ºé—´å·¨å¤§ã€ç¼ºä¹æ¢¯åº¦ä¿¡æ¯å’ŒéªŒè¯æˆæœ¬é«˜æ˜‚ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æç¤ºé€‰æ‹©æ–¹æ³•HbBoPsï¼Œç»“åˆäº†ç»“æ„æ„ŸçŸ¥æ·±åº¦æ ¸é«˜æ–¯è¿‡ç¨‹å’ŒHyperbandå¤šä¿çœŸè°ƒåº¦å™¨ã€‚</li>
<li>HbBoPsä½¿ç”¨æŒ‡ä»¤å’Œå°‘é‡èŒƒä¾‹çš„åµŒå…¥ä½œä¸ºæç¤ºä¸­çš„æ¨¡å—åŒ–ç»„ä»¶ï¼Œæé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>Hyperbandæé«˜äº†æŸ¥è¯¢æ•ˆç‡ï¼Œé€šè¿‡è‡ªé€‚åº”åˆ†é…èµ„æºå‡å°‘äº†è¯„ä¼°æç¤ºæ‰€éœ€çš„éªŒè¯å®ä¾‹æ•°é‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒHbBoPsåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šåœ¨æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢éƒ½è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07820">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf079f5aaa47bb286770a032f3e70b8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8363c9cc01f3fe047f5a4d4b5d0c27d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff693cd7e45d5880d6b9629e397bb770.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7d0d25c2b17cbab3b36d57b1061793c1.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  A Dynamic Transformer Network for Vehicle Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-91a59868f89dd3d2ca16d14dde92a0b0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  GUI-Actor Coordinate-Free Visual Grounding for GUI Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
