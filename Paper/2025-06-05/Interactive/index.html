<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive 方向最新论文已更新，请持续关注 Update in 2025-06-05  Towards a Japanese Full-duplex Spoken Dialogue System">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-226f2d3a37045a091dcbce8e5c282893.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    37 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-05-更新"><a href="#2025-06-05-更新" class="headerlink" title="2025-06-05 更新"></a>2025-06-05 更新</h1><h2 id="Towards-a-Japanese-Full-duplex-Spoken-Dialogue-System"><a href="#Towards-a-Japanese-Full-duplex-Spoken-Dialogue-System" class="headerlink" title="Towards a Japanese Full-duplex Spoken Dialogue System"></a>Towards a Japanese Full-duplex Spoken Dialogue System</h2><p><strong>Authors:Atsumoto Ohashi, Shinya Iizuka, Jingjing Jiang, Ryuichiro Higashinaka</strong></p>
<p>Full-duplex spoken dialogue systems, which can model simultaneous bidirectional features of human conversations such as speech overlaps and backchannels, have attracted significant attention recently. However, the study of full-duplex spoken dialogue systems for the Japanese language has been limited, and the research on their development in Japanese remains scarce. In this paper, we present the first publicly available full-duplex spoken dialogue model in Japanese, which is built upon Moshi, a full-duplex dialogue model in English. Our model is trained through a two-stage process: pre-training on a large-scale spoken dialogue data in Japanese, followed by fine-tuning on high-quality stereo spoken dialogue data. We further enhance the model’s performance by incorporating synthetic dialogue data generated by a multi-stream text-to-speech system. Evaluation experiments demonstrate that the trained model outperforms Japanese baseline models in both naturalness and meaningfulness. </p>
<blockquote>
<p>近期，能够模拟人类对话的双向同时性特征，如言语重叠和反馈通道的双向全工对话系统引起了人们的广泛关注。然而，关于日语双向全工对话系统的研究十分有限，其开发研究仍然很少。在本文中，我们推出了首个公开的日语双向全工对话模型。该模型建立在英语双向对话模型Moshi的基础上。我们的模型通过两个阶段进行训练：首先在大规模的日语口语对话数据上进行预训练，然后在高质量立体声口语对话数据上进行微调。我们通过引入由多流文本到语音系统生成的人工对话数据，进一步提高了模型的性能。评估实验表明，训练后的模型在自然性和有意义性方面都优于日语基线模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02979v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>近期，全双工对话系统备受关注，可对人类对话的双向特性进行建模，如语音重叠和反馈通道。尽管对日语全双工对话系统的研究有限，但本文首次公开推出日语全双工对话模型。该模型基于英语全双工对话模型Moshi构建，通过两阶段训练过程：首先在大量日语对话数据上进行预训练，然后在高质量立体声对话数据上进行微调。通过结合多流文本到语音系统生成的综合对话数据，进一步提高模型性能。评估实验表明，训练后的模型在自然度和意义性方面都优于日语基准模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>全双工对话系统能够建模人类对话的双向特性，如语音重叠和反馈通道。</li>
<li>日语全双工对话系统的研究相对有限。</li>
<li>本文首次公开推出日语全双工对话模型。</li>
<li>模型基于英语全双工对话模型Moshi构建。</li>
<li>模型通过两阶段训练过程：预训练和微调。</li>
<li>合成对话数据通过多流文本到语音系统生成，增强了模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b7400a8cc0d30f4bfd02ffe1a4326777.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76749f6f0d5726186e99f2cc572edd14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b56fe64df06e5e9b544a1ae35ffc2db7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-847c4c71e81a6d97a225e83bfc956bb6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CoDial-Interpretable-Task-Oriented-Dialogue-Systems-Through-Dialogue-Flow-Alignment"><a href="#CoDial-Interpretable-Task-Oriented-Dialogue-Systems-Through-Dialogue-Flow-Alignment" class="headerlink" title="CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue   Flow Alignment"></a>CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue   Flow Alignment</h2><p><strong>Authors:Radin Shayanfar, Chu Fei Luo, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu</strong></p>
<p>It is often challenging to teach specialized, unseen tasks to dialogue systems due to the high cost of expert knowledge, training data, and high technical difficulty. To support domain-specific applications - such as law, medicine, or finance - it is essential to build frameworks that enable non-technical experts to define, test, and refine system behaviour with minimal effort. Achieving this requires cross-disciplinary collaboration between developers and domain specialists. In this work, we introduce a novel framework, CoDial (Code for Dialogue), that converts expert knowledge, represented as a novel structured heterogeneous graph, into executable conversation logic. CoDial can be easily implemented in existing guardrailing languages, such as Colang, to enable interpretable, modifiable, and true zero-shot specification of task-oriented dialogue systems. Empirically, CoDial achieves state-of-the-art performance on the STAR dataset for inference-based models and is competitive with similar baselines on the well-known MultiWOZ dataset. We also demonstrate CoDial’s iterative improvement via manual and LLM-aided feedback, making it a practical tool for expert-guided alignment of LLMs in high-stakes domains. </p>
<blockquote>
<p>向对话系统教授特殊的、未见过的任务通常具有挑战性，这主要是由于专业知识、训练数据和技术难度都很高。为了支持特定领域的应用程序，如法律、医学或金融，建立框架是至关重要的，这些框架使得非技术专家能够轻松地定义、测试和微调系统行为。实现这一点需要开发者和领域专家之间的跨学科合作。在这项工作中，我们引入了一种新型框架 CoDial（用于对话的代码），它将专业知识转化为可执行对话逻辑的新型结构化异构图。CoDial 可以轻松实现在现有的防护语言（如 Colang）中，以实现可解释、可修改和任务导向型对话系统的真正零镜头规范。实证表明，CoDial 在基于推理的 STAR 数据集上实现了最先进的性能表现，并在著名的 MultiWOZ 数据集上与类似基线相比具有竞争力。我们还通过手动和大型语言模型辅助的反馈展示了 CoDial 的迭代改进，使其成为高风险领域专家引导的大型语言模型对齐的实用工具。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02264v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于高成本的专业知识和训练数据挑战，针对如法律、医学和金融等领域的应用程序开发低成本的通用型交互框架十分关键。此研究提出了名为CoDial的跨平台新型框架，能精准有效运用该专业知识设计通用的任务型对话系统行为模式，开发者将理论理解转换成图形语言并转化逻辑行为模型，可直接应用于现有守护墙语言（如Colang）。经验显示，CoDial在STAR数据集上推理模型性能领先业界其他系统；同时在知名MultiWOZ数据集上也有卓越表现。其能够人工和语言模型反馈机制中不断完善进步，在实际场景有着较高实用度。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>针对特定领域任务对话系统的开发面临专业知识成本高昂的挑战。</li>
<li>为应对各领域专家的开发难题，构建一个桥梁显得尤为重要。本次引入的新型框架名为CoDial框架为相关行业的语言直接开发交流体系开辟了可能，这为设计者以有效手段展现了对知识的理解和掌握方式并做出相对应的处理反馈行为，大大降低各行业的参与者构建实施复杂度及搭建工作周期的时间投入，显著提高非专业领域专业人士也能够做到方便有效地对系统开发结果进行自我评估和检查效果验证、保证相应的智能化构建环节迭代顺畅及零缺陷关键场景的执行力展现打下基础保障支撑力度。。采用抽象技术门槛低的方式让领域专家通过直观易懂的方式介入语言模型设计过程。实现语言模型决策行为贴近专家意图的初衷。让领域专家以用户的角度最大程度在现有的机器学习基础成果的基础上从面向开发任务的形式转化成面实际应用操作服务的大众角色并应用满足用户对提升专业内容效率表达层面的提升、消除各种差异领域的“需求方话语权在业内水土不服的情况的发生，并让从社会群众的“消费语境方面与专业进行联动决策的思路发展技术给专业人员减少后期校准的负担而付诸行动，“逐渐向着我们进行开展预期的一种简单粗暴通过技术和各行各业的智能构建业务的捷径操作的发展领域模式进而优化着训练好各类型机器自我完善的不同细分领域语言的特性价值的发展大方向而努力的方向迈出坚定步伐，。助力优化模型实际应用效率达到一个新高度进而拓宽新市场空间。”&gt; 开发门槛较高的通用任务型对话系统的学习面临因有较高的应用场模拟构造高昂耗损困境的一种有效解决的新策略的出现提供重要支持。CoDial框架为行业专家提供了直观介入语言模型设计过程的途径，使得领域专家能够以用户的角度参与到语言模型的构建过程中，促使语言模型的决策行为更符合专家意图和用户需求，提升了模型的实际应用效率和市场价值。采用反馈机制使得该框架可以不断完善进步，助力优化模型的实际应用效率达到一个新高度。同时，CoDial框架的出现也拓宽了新的市场空间。通过跨平台集成利用和不同方面的跨学科技术的通力协作之下，。并在有效地节省科研人员进行逐步操作的误差几时的机率调整造成的的时间和压力过程当中成就了广泛的应用落脚点最终实现让人机交互越来越自然的现状改进和提升并且赋予多行业具有领域背景的从业者更高的能力展现他们宝贵的实践技能并通过这样的研发推动业务服务方式的创新发展更加突出战略化服务层级的深度发展以及提高协同创新能力从而引领行业创新发展的方向。。最终助力推动人机交互技术不断朝着更自然、更智能的方向发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02264">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-978587cc6b14ba477ecd08d2d9619d71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-133b1c4fa141caf5067ef0c15425ba0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d85a06b0be6e754ef3fdde31235f8b07.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="KokoroChat-A-Japanese-Psychological-Counseling-Dialogue-Dataset-Collected-via-Role-Playing-by-Trained-Counselors"><a href="#KokoroChat-A-Japanese-Psychological-Counseling-Dialogue-Dataset-Collected-via-Role-Playing-by-Trained-Counselors" class="headerlink" title="KokoroChat: A Japanese Psychological Counseling Dialogue Dataset   Collected via Role-Playing by Trained Counselors"></a>KokoroChat: A Japanese Psychological Counseling Dialogue Dataset   Collected via Role-Playing by Trained Counselors</h2><p><strong>Authors:Zhiyang Qi, Takumasa Kaneko, Keiko Takamizo, Mariko Ukiyo, Michimasa Inaba</strong></p>
<p>Generating psychological counseling responses with language models relies heavily on high-quality datasets. Crowdsourced data collection methods require strict worker training, and data from real-world counseling environments may raise privacy and ethical concerns. While recent studies have explored using large language models (LLMs) to augment psychological counseling dialogue datasets, the resulting data often suffers from limited diversity and authenticity. To address these limitations, this study adopts a role-playing approach where trained counselors simulate counselor-client interactions, ensuring high-quality dialogues while mitigating privacy risks. Using this method, we construct KokoroChat, a Japanese psychological counseling dialogue dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive client feedback. Experimental results demonstrate that fine-tuning open-source LLMs with KokoroChat improves both the quality of generated counseling responses and the automatic evaluation of counseling dialogues. The KokoroChat dataset is available at <a target="_blank" rel="noopener" href="https://github.com/UEC-InabaLab/KokoroChat">https://github.com/UEC-InabaLab/KokoroChat</a>. </p>
<blockquote>
<p>利用语言模型生成心理咨询响应严重依赖于高质量的数据集。众包数据收集方法需要严格的工人培训，而来自现实世界咨询环境的数据可能会引发隐私和伦理担忧。虽然最近的研究已经探索了使用大型语言模型（LLM）来增加心理咨询对话数据集，但所得数据往往存在多样性和真实性的局限性。为了解决这些局限性，本研究采用角色扮演的方法，训练有素的咨询师模拟咨询师-客户互动，确保高质量的对话，同时降低隐私风险。使用这种方法，我们构建了KokoroChat，这是一个日语心理咨询对话数据集，包含6589个长对话，每个对话都有全面的客户反馈。实验结果表明，用KokoroChat微调开源LLM，可以提高生成的咨询响应的质量和咨询对话的自动评估效果。KokoroChat数据集可通过<a target="_blank" rel="noopener" href="https://github.com/UEC-InabaLab/KokoroChat%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/UEC-InabaLab/KokoroChat获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01357v1">PDF</a> Accepted to ACL 2025 Main Conference</p>
<p><strong>Summary</strong><br>心理咨询服务响应的语言模型生成严重依赖于高质量数据集。本研究采用角色扮演的方法，训练咨询师模拟咨询师-客户互动，构建了一个日本心理咨询对话数据集KokoroChat，包括6589个长对话和相应的客户反馈。实验结果表明，使用KokoroChat微调开源LLM可以提高生成的咨询响应的质量和自动评估的咨询对话的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高质量数据集对于生成心理咨询服务响应的语言模型至关重要。</li>
<li>众筹数据收集方法需要严格的工人培训。</li>
<li>真实心理咨询环境的数据可能引发隐私和道德问题。</li>
<li>现有研究中，使用大型语言模型（LLM）增强心理咨询对话数据集的数据往往存在多样性和真实性的局限性。</li>
<li>本研究采用角色扮演方法模拟咨询师-客户互动，确保高质量对话并降低隐私风险。</li>
<li>构建了一个日本心理咨询对话数据集KokoroChat，包括长对话和相应的客户反馈。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01357">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5701dfdd65f314f121a68537b44a81f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9bb037ec4fd2123d65a710dda27e6ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca1a771f5e8666c5575fc5a66025cc80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab638476a131c303571c25eb1d12c8c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-226f2d3a37045a091dcbce8e5c282893.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Speaking-Beyond-Language-A-Large-Scale-Multimodal-Dataset-for-Learning-Nonverbal-Cues-from-Video-Grounded-Dialogues"><a href="#Speaking-Beyond-Language-A-Large-Scale-Multimodal-Dataset-for-Learning-Nonverbal-Cues-from-Video-Grounded-Dialogues" class="headerlink" title="Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning   Nonverbal Cues from Video-Grounded Dialogues"></a>Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning   Nonverbal Cues from Video-Grounded Dialogues</h2><p><strong>Authors:Youngmin Kim, Jiwan Chung, Jisoo Kim, Sunghyun Lee, Sangkyu Lee, Junhyeok Kim, Cheoljong Yang, Youngjae Yu</strong></p>
<p>Nonverbal communication is integral to human interaction, with gestures, facial expressions, and body language conveying critical aspects of intent and emotion. However, existing large language models (LLMs) fail to effectively incorporate these nonverbal elements, limiting their capacity to create fully immersive conversational experiences. We introduce MARS, a multimodal language model designed to understand and generate nonverbal cues alongside text, bridging this gap in conversational AI. Our key innovation is VENUS, a large-scale dataset comprising annotated videos with time-aligned text, facial expressions, and body language. Leveraging VENUS, we train MARS with a next-token prediction objective, combining text with vector-quantized nonverbal representations to achieve multimodal understanding and generation within a unified framework. Based on various analyses of the VENUS datasets, we validate its substantial scale and high effectiveness. Our quantitative and qualitative results demonstrate that MARS successfully generates text and nonverbal languages, corresponding to conversational input. </p>
<blockquote>
<p>非言语交流是人类交流的重要组成部分，手势、面部表情和身体语言传达了意图和情绪的关键方面。然而，现有的大型语言模型（LLM）无法有效地融入这些非言语元素，限制了其创造沉浸式对话体验的能力。我们引入了MARS，这是一个多模态语言模型，旨在理解和生成与文本并行的非言语线索，以弥补会话人工智能中的这一空白。我们的关键创新之处在于VENUS，这是一个大规模数据集，包含带有时间对齐文本、面部表情和身体语言的注释视频。利用VENUS，我们以下一个标记预测目标训练MARS，将文本与向量量化的非言语表示相结合，在一个统一框架内实现多模态理解和生成。通过对VENUS数据集的各种分析，我们验证了其大规模和高效率。我们的定量和定性结果表明，MARS成功生成与对话输入相对应的文字和非言语语言。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00958v1">PDF</a> Accepted to ACL 2025 (Main), Our code and dataset:   <a target="_blank" rel="noopener" href="https://github.com/winston1214/nonverbal-conversation">https://github.com/winston1214/nonverbal-conversation</a></p>
<p><strong>Summary</strong></p>
<p>非言语沟通在人类互动中占据重要地位，包括手势、面部表情和身体语言传递了意图和情感的关键方面。然而，现有的大型语言模型（LLMs）无法有效地融入这些非言语元素，限制了它们在创建沉浸式对话体验方面的能力。为此，我们引入了MARS，这是一个多模态语言模型，旨在理解与生成非言语线索，同时处理文本，以弥补对话式人工智能中的这一鸿沟。我们的关键创新在于VENUS数据集，这是一个包含标注视频、时间对齐文本、面部表情和身体语言的大规模数据集。通过使用VENUS数据集训练MARS模型，结合文本和向量量化的非言语表示形式，在统一框架内实现多模态理解和生成。通过对VENUS数据集的各种分析，验证了其大规模和高效率。我们的定量和定性结果表明，MARS能够成功生成与对话输入相对应的文字和非言语语言。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>非言语沟通在人类互动中非常重要，包括手势、面部表情和身体语言等。</li>
<li>当前大型语言模型（LLMs）无法有效融入非言语元素，限制了对话体验的真实性。</li>
<li>MARS是一个多模态语言模型，能够理解和生成非言语线索，同时处理文本。</li>
<li>VENUS是一个大型数据集，包含标注视频、时间对齐文本、面部表情和身体语言等信息。</li>
<li>MARS使用VENUS数据集进行训练，实现了多模态理解和生成。</li>
<li>通过定量和定性分析验证，MARS能够成功生成与对话输入相对应的文字和非言语语言。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b0d76d027c46a0de5e2521ec6835a120.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7935fb0ce8a14ed47defee5de69e6df1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-798e6d433b908a827fd3523fbd3710c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbdded2589148016f81c37c7074005f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-936b61943baf2b80982b9a9564e02ad6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CoVoMix2-Advancing-Zero-Shot-Dialogue-Generation-with-Fully-Non-Autoregressive-Flow-Matching"><a href="#CoVoMix2-Advancing-Zero-Shot-Dialogue-Generation-with-Fully-Non-Autoregressive-Flow-Matching" class="headerlink" title="CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully   Non-Autoregressive Flow Matching"></a>CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully   Non-Autoregressive Flow Matching</h2><p><strong>Authors:Leying Zhang, Yao Qian, Xiaofei Wang, Manthan Thakker, Dongmei Wang, Jianwei Yu, Haibin Wu, Yuxuan Hu, Jinyu Li, Yanmin Qian, Sheng Zhao</strong></p>
<p>Generating natural-sounding, multi-speaker dialogue is crucial for applications such as podcast creation, virtual agents, and multimedia content generation. However, existing systems struggle to maintain speaker consistency, model overlapping speech, and synthesize coherent conversations efficiently. In this paper, we introduce CoVoMix2, a fully non-autoregressive framework for zero-shot multi-talker dialogue generation. CoVoMix2 directly predicts mel-spectrograms from multi-stream transcriptions using a flow-matching-based generative model, eliminating the reliance on intermediate token representations. To better capture realistic conversational dynamics, we propose transcription-level speaker disentanglement, sentence-level alignment, and prompt-level random masking strategies. Our approach achieves state-of-the-art performance, outperforming strong baselines like MoonCast and Sesame in speech quality, speaker consistency, and inference speed. Notably, CoVoMix2 operates without requiring transcriptions for the prompt and supports controllable dialogue generation, including overlapping speech and precise timing control, demonstrating strong generalizability to real-world speech generation scenarios. </p>
<blockquote>
<p>生成自然音调的、多说话者的对话对于播客创作、虚拟代理和多媒体内容生成等应用至关重要。然而，现有系统在维持说话者一致性、模拟重叠语音和高效合成连贯对话方面存在困难。在本文中，我们介绍了CoVoMix2，这是一个用于零样本多说话者对话生成的全非自回归框架。CoVoMix2直接使用基于流匹配的生成模型，从多流转录中预测梅尔频谱图，从而消除了对中间令牌表示的依赖。为了更好地捕捉现实对话的动态，我们提出了转录级说话者分离、句子级对齐和提示级随机掩码策略。我们的方法达到了最先进的性能，在语音质量、说话者一致性和推理速度方面超越了MoonCast和Sesame等强基线。值得注意的是，CoVoMix2在提示时不需要转录，并支持可控对话生成，包括重叠语音和精确的时间控制，显示出对真实世界语音生成场景的强泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00885v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了CoVoMix2，一个用于零样本多说话者对话生成的全非自回归框架。它通过直接预测多流转录的梅尔频谱图，消除了对中间令牌表示的依赖。提出的方法在语音质量、说话者一致性和推理速度方面优于MoonCast和Sesame等强基线，并适用于现实世界的语音生成场景，支持可控对话生成，包括重叠语音和精确的时间控制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoVoMix2是一个用于零样本多说话者对话生成的全非自回归框架。</li>
<li>该方法直接预测梅尔频谱图，消除了对中间令牌表示的依赖。</li>
<li>通过转录级说话者分离、句子级对齐和提示级随机遮蔽策略，更好地捕捉现实对话的动态。</li>
<li>CoVoMix2在语音质量、说话者一致性和推理速度方面达到最新技术水平。</li>
<li>该方法无需提示的转录，并且支持可控对话生成。</li>
<li>CoVoMix2可以生成重叠语音，并具有精确的时间控制能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00885">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-066905fde80d45053381076779cfe317.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dd8b28ff613cee8e89e3d193f6e5e50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c60a46240a3ca7c519f5844c1ed022e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9df9e0e1be8643e2a06c5933e199bbbd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Training-for-Open-E2E-Spoken-Dialogue-Systems"><a href="#Chain-of-Thought-Training-for-Open-E2E-Spoken-Dialogue-Systems" class="headerlink" title="Chain-of-Thought Training for Open E2E Spoken Dialogue Systems"></a>Chain-of-Thought Training for Open E2E Spoken Dialogue Systems</h2><p><strong>Authors:Siddhant Arora, Jinchuan Tian, Hayato Futami, Jee-weon Jung, Jiatong Shi, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe</strong></p>
<p>Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue systems preserve full differentiability and capture non-phonemic information, making them well-suited for modeling spoken interactions. However, existing E2E approaches often require large-scale training data and generates responses lacking semantic coherence. We propose a simple yet effective strategy leveraging a chain-of-thought (CoT) formulation, ensuring that training on conversational data remains closely aligned with the multimodal language model (LM)’s pre-training on speech recognition~(ASR), text-to-speech synthesis (TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over the baseline, successfully training spoken dialogue systems on publicly available human-human conversation datasets, while being compute-efficient enough to train on just 300 hours of public human-human conversation data, such as the Switchboard. We will publicly release our models and training code. </p>
<blockquote>
<p>不同于传统的级联管道，端到端（E2E）口语对话系统保持完整的可微性并捕捉非语音信息，使其非常适合对口语交互进行建模。然而，现有的E2E方法通常需要大规模的训练数据，并且生成的响应缺乏语义连贯性。我们提出了一种简单有效的策略，利用思维链（CoT）公式，确保在对话数据上的训练与多模态语言模型（LM）在语音识别（ASR）、文本到语音合成（TTS）和文本LM任务上的预训练紧密对齐。我们的方法在基准测试上实现了超过1.5分的ROUGE-1改进，成功地在公开可用的人类对话数据集上训练了口语对话系统，同时计算效率足够高，只需在300小时公开人类对话数据（如Switchboard）上进行训练。我们将公开发布我们的模型和训练代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00722v1">PDF</a> Accepted at INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了端对端（E2E）对话系统的优势，包括全可微性和对非语音信息的捕捉能力，使其适合模拟口语交互。然而，现有E2E方法需要大量训练数据，生成的响应缺乏语义连贯性。本文提出了一种利用思维链（CoT）的方法，确保对话数据的训练与多模态语言模型（LM）的预训练紧密对齐，包括语音识别（ASR）、文本合成语音（TTS）和文本LM任务。该方法在公开的人类对话数据集上成功训练了对话系统，并在Rouge-1指标上实现了超过基线模型1.5的提升，同时在仅使用300小时公开人类对话数据的情况下具有很高的计算效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>端对端（E2E）对话系统可以保留全可微性并捕捉非语音信息，使其适合模拟口语交互。</li>
<li>现有E2E方法存在需要大规模训练数据和生成的响应缺乏语义连贯性的问题。</li>
<li>利用思维链（CoT）的方法确保对话数据训练与多模态语言模型的预训练紧密对齐。</li>
<li>该方法成功在公开的人类对话数据集上训练了对话系统。</li>
<li>与基线模型相比，该方法在Rouge-1指标上实现了超过1.5的提升。</li>
<li>该方法具有高度的计算效率，可以在仅使用300小时公开人类对话数据的情况下进行训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00722">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-43c53528406a787aa022ed7a37ac9772.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3aabb0724524d837b8ba4fb695aa26b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b8f420b2610b8f8f1210ee1bd5a106d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43cf3653181841430e5eec7649b2fb40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e9210fc4aac39dd76f46dbd50237eb1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SafeTy-Reasoning-Elicitation-Alignment-for-Multi-Turn-Dialogues"><a href="#SafeTy-Reasoning-Elicitation-Alignment-for-Multi-Turn-Dialogues" class="headerlink" title="SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues"></a>SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues</h2><p><strong>Authors:Martin Kuo, Jianyi Zhang, Aolin Ding, Louis DiValentin, Amin Hass, Benjamin F Morris, Isaac Jacobson, Randolph Linderman, James Kiessling, Nicolas Ramos, Bhavna Gopal, Maziyar Baran Pouyan, Changwei Liu, Hai Li, Yiran Chen</strong></p>
<p>Malicious attackers can exploit large language models (LLMs) by engaging them in multi-turn dialogues to achieve harmful objectives, posing significant safety risks to society. To address this challenge, we propose a novel defense mechanism: SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues (STREAM). STREAM defends LLMs against multi-turn attacks while preserving their functional capabilities. Our approach involves constructing a human-annotated dataset, the Safety Reasoning Multi-turn Dialogues dataset, which is used to fine-tune a plug-and-play safety reasoning moderator. This model is designed to identify malicious intent hidden within multi-turn conversations and alert the target LLM of potential risks. We evaluate STREAM across multiple LLMs against prevalent multi-turn attack strategies. Experimental results demonstrate that our method significantly outperforms existing defense techniques, reducing the Attack Success Rate (ASR) by 51.2%, all while maintaining comparable LLM capability. </p>
<blockquote>
<p>恶意攻击者可以通过与大型语言模型（LLM）进行多轮对话来利用它们，以实现有害目标，给社会带来重大安全风险。为了解决这一挑战，我们提出了一种新的防御机制：多轮对话安全推理引导对齐（STREAM）。STREAM在保护大型语言模型免受多轮攻击的同时，保持了它们的功能性。我们的方法包括构建一个人工标注的数据集——安全推理多轮对话数据集，用于微调即插即用的安全推理调解器。该模型旨在识别多轮对话中隐藏的恶意意图，并提醒目标大型语言模型存在的潜在风险。我们在多个大型语言模型上评估了STREAM对常见多轮攻击策略的防御能力。实验结果表明，我们的方法显著优于现有防御技术，攻击成功率（ASR）降低了51.2%，同时保持了大型语言模型的能力相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00668v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）易受恶意攻击者利用，通过多轮对话实现有害目标，对社会安全构成重大风险。为应对这一挑战，提出一种新型防御机制：STREAM（针对多轮对话的安全推理引导对齐）。STREAM能够在保护LLM功能的同时，防御多轮攻击。其方法是通过构建一个人工标注的数据集——安全推理多轮对话数据集，对即插即用安全推理调解器进行微调。该模型旨在识别多轮对话中隐藏的恶意意图，并提醒目标LLM潜在风险。在多个LLM上针对流行的多轮攻击策略对STREAM进行评估，实验结果表明，该方法显著优于现有防御技术，攻击成功率（ASR）降低51.2%，同时保持LLM的能力相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>恶意攻击者可以利用大型语言模型（LLMs）的多轮对话功能实现有害目标，对社会安全构成风险。</li>
<li>提出的防御机制STREAM旨在保护LLMs免受多轮攻击，同时保持其功能。</li>
<li>STREAM通过构建安全推理多轮对话数据集，对安全推理调解器进行微调。</li>
<li>该模型能够识别多轮对话中的恶意意图。</li>
<li>STREAM提醒目标LLM在对话中的潜在风险。</li>
<li>实验结果表明，STREAM在防御多轮攻击方面显著优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00668">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ea7a7740d1653927df1b5413fc4d964d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57dcb49d39e30baa97fa060debfe38db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-348e723001cdb47265044f55e6b1483d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c43c88a4080f1f63af4d99cf2a5ca9cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d86290c2c50fd60ee1611c55fec3fbf6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Improving-Dialogue-State-Tracking-through-Combinatorial-Search-for-In-Context-Examples"><a href="#Improving-Dialogue-State-Tracking-through-Combinatorial-Search-for-In-Context-Examples" class="headerlink" title="Improving Dialogue State Tracking through Combinatorial Search for   In-Context Examples"></a>Improving Dialogue State Tracking through Combinatorial Search for   In-Context Examples</h2><p><strong>Authors:Haesung Pyun, Yoonah Park, Yohan Jo</strong></p>
<p>In dialogue state tracking (DST), in-context learning comprises a retriever that selects labeled dialogues as in-context examples and a DST model that uses these examples to infer the dialogue state of the query dialogue. Existing methods for constructing training data for retrievers suffer from three key limitations: (1) the synergistic effect of examples is not considered, (2) the linguistic characteristics of the query are not sufficiently factored in, and (3) scoring is not directly optimized for DST performance. Consequently, the retriever can fail to retrieve examples that would substantially improve DST performance. To address these issues, we present CombiSearch, a method that scores effective in-context examples based on their combinatorial impact on DST performance. Our evaluation on MultiWOZ shows that retrievers trained with CombiSearch surpass state-of-the-art models, achieving a 20x gain in data efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch attains a 12% absolute improvement in the upper bound DST performance over traditional approaches when no retrieval errors are assumed. This significantly increases the headroom for practical DST performance while demonstrating that existing methods rely on suboptimal data for retriever training. </p>
<blockquote>
<p>在对话状态跟踪（DST）中，上下文学习包括一个检索器，该检索器选择标记对话作为上下文示例，以及一个使用这些示例来推断查询对话状态的DST模型。现有的构建检索器训练数据的方法存在三个主要局限性：（1）没有考虑示例的协同作用，（2）没有充分考虑查询的语言特征，（3）评分没有直接针对DST性能进行优化。因此，检索器可能会无法检索到会大大提高DST性能的示例。为了解决这些问题，我们提出了CombiSearch方法，该方法基于其对DST性能的组合影响对有效的上下文示例进行评分。我们在MultiWOZ上的评估显示，使用CombiSearch训练的检索器超越了最新模型，实现了20倍的数据效率，并在SGD数据集上具有良好的泛化能力。此外，当没有假设检索错误时，CombiSearch在传统方法的基础上实现了上限DST性能12%的绝对提升。这显著增加了实际DST性能的潜力，同时表明现有方法依赖于次优数据进行检索器训练。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00622v2">PDF</a> This paper has been accepted for publication at ACL 2025</p>
<p><strong>Summary</strong></p>
<p>在对话状态追踪（DST）中，上下文学习包括选择标记对话作为上下文示例的检索器，以及使用这些示例来推断查询对话状态的DST模型。现有构建检索器训练数据的方法存在三个主要局限性：一是没有考虑示例的协同作用，二是没有充分考虑查询的语言特征，三是评分不是直接针对DST性能进行优化。因此，检索器可能无法检索到能显著改善DST性能的例子。为解决这些问题，我们提出了CombiSearch方法，该方法基于示例对DST性能的组合影响进行评分。在MultiWOZ上的评估显示，使用CombiSearch训练的检索器超越了最新模型，实现了20倍的数据效率提升，并能很好地推广到SGD数据集。此外，假设没有检索错误的情况下，CombiSearch相较于传统方法在上限DST性能上取得了12%的绝对提升。这显著增加了实际DST性能的潜力，并表明现有方法依赖于次优数据进行检索器训练。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对话状态追踪（DST）中的上下文学习涉及检索器和DST模型，其中检索器选择标记对话作为上下文示例。</li>
<li>现有构建检索器训练数据的方法存在三个主要局限性：未考虑示例协同作用、未充分考虑查询语言特征，以及评分未针对DST性能优化。</li>
<li>CombiSearch方法基于示例对DST性能的组合影响进行评分，能有效解决上述问题。</li>
<li>在MultiWOZ数据集上，使用CombiSearch的检索器训练取得了显著效果，包括提高数据效率和DST性能。</li>
<li>CombiSearch方法在假设无检索错误的情况下，相较于传统方法实现了12%的绝对性能提升。</li>
<li>这表明现有方法使用的数据对于训练检索器可能是次优的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00622">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-73189feae486cb4d58e217ee06743ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3823e6f3b6caa81cbf278a7e4a9b62ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dd4f48dad18e464a7310eb3f46664bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c78006386941695ade328ba2e1c295f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-933ebe5487fde3f515605ae20a8e1003.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="3MDBench-Medical-Multimodal-Multi-agent-Dialogue-Benchmark"><a href="#3MDBench-Medical-Multimodal-Multi-agent-Dialogue-Benchmark" class="headerlink" title="3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark"></a>3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark</h2><p><strong>Authors:Ivan Sviridov, Amina Miftakhova, Artemiy Tereshchenko, Galina Zubkova, Pavel Blinov, Andrey Savchenko</strong></p>
<p>Though Large Vision-Language Models (LVLMs) are being actively explored in medicine, their ability to conduct telemedicine consultations combining accurate diagnosis with professional dialogue remains underexplored. In this paper, we present 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark), an open-source framework for simulating and evaluating LVLM-driven telemedical consultations. 3MDBench simulates patient variability through four temperament-based Patient Agents and an Assessor Agent that jointly evaluate diagnostic accuracy and dialogue quality. It includes 3013 cases across 34 diagnoses drawn from real-world telemedicine interactions, combining textual and image-based data. The experimental study compares diagnostic strategies for popular LVLMs, including GPT-4o-mini, LLaVA-3.2-11B-Vision-Instruct, and Qwen2-VL-7B-Instruct. We demonstrate that multimodal dialogue with internal reasoning improves F1 score by 6.5% over non-dialogue settings, highlighting the importance of context-aware, information-seeking questioning. Moreover, injecting predictions from a diagnostic convolutional network into the LVLM’s context boosts F1 by up to 20%. Source code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/3mdbench_acl-0511">https://anonymous.4open.science/r/3mdbench_acl-0511</a>. </p>
<blockquote>
<p>虽然大型视觉语言模型（LVLMs）在医学领域正受到积极研究，但它们结合准确诊断和专业对话进行远程医疗咨询的能力仍研究不足。在本文中，我们介绍了3MDBench（医疗多模态多智能体对话基准测试），这是一个用于模拟和评估LVLM驱动的远程医疗咨询的开源框架。3MDBench通过四种基于气质的患者智能体和评估智能体来模拟患者的变异性，共同评估诊断准确性和对话质量。它包括来自现实世界远程医疗互动的34种诊断中的3013个案例，涵盖文本和基于图像的数据。实验研究了流行的LVLMs的诊断策略，包括GPT-4o-mini、LLaVA-3.2-11B-Vision-Instruct和Qwen2-VL-7B-Instruct。我们证明，与无对话设置相比，具有内部推理的多模态对话可以提高F1分数6.5%，这突出了上下文感知和信息搜索问题的重要性。此外，将诊断卷积网络的预测注入LVLM的语境中，F1可提高高达20%。源代码可在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/3mdbench_acl-0511%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/3mdbench_acl-0511找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13861v2">PDF</a> 35 pages, 13 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Large Vision-Language Models（LVLMs）在医学领域的应用，并着重探讨了它们在远程医疗咨询中的表现。为了模拟和评估LVLM驱动的远程医疗咨询，提出了一种名为3MDBench的开放源代码框架。该框架通过模拟不同性格的患者代理和评估代理来评价诊断准确性和对话质量。实验研究表明，多模式对话与内部推理可以提高F1分数，通过将诊断卷积网络的预测结果注入LVLM的上下文，可以提高F1分数达20%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Large Vision-Language Models (LVLMs) 在医学领域的应用正在积极探索，但在远程医疗咨询中的综合准确诊断和专业对话能力尚未得到充分研究。</li>
<li>3MDBench是一个开放源代码框架，用于模拟和评估LVLM驱动的远程医疗咨询。</li>
<li>3MDBench包括基于四种性格的患者代理和评估代理，以评价诊断准确性和对话质量。</li>
<li>实验研究比较了流行的LVLMs在医疗领域的诊断策略。</li>
<li>多模式对话与内部推理可以提高诊断的准确性。</li>
<li>将诊断卷积网络的预测结果注入LVLM的上下文可以进一步提高诊断的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13861">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-40938c90254bf0113e6b09589f24d7d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53b86ae1f32ded68f38060a56de04b70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc0cd8b9b85db9ae21c774d2247eea08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cb7d1d58ccd2c237b2e0b54803c9c78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75948df5b5df1198a82726fad55e8d85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-275e27b1894e94542bdbca2691cbc4a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6dabc8147eaa423a322265313a94492.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Akan-Cinematic-Emotions-ACE-A-Multimodal-Multi-party-Dataset-for-Emotion-Recognition-in-Movie-Dialogues"><a href="#Akan-Cinematic-Emotions-ACE-A-Multimodal-Multi-party-Dataset-for-Emotion-Recognition-in-Movie-Dialogues" class="headerlink" title="Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for   Emotion Recognition in Movie Dialogues"></a>Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for   Emotion Recognition in Movie Dialogues</h2><p><strong>Authors:David Sasu, Zehui Wu, Ziwei Gong, Run Chen, Pengyuan Shi, Lin Ai, Julia Hirschberg, Natalie Schluter</strong></p>
<p>In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the significant lack of resources for low-resource languages in emotion recognition research. ACE, developed for the Akan language, contains 385 emotion-labeled dialogues and 6,162 utterances across audio, visual, and textual modalities, along with word-level prosodic prominence annotations. The presence of prosodic labels in this dataset also makes it the first prosodically annotated African language dataset. We demonstrate the quality and utility of ACE through experiments using state-of-the-art emotion recognition methods, establishing solid baselines for future research. We hope ACE inspires further work on inclusive, linguistically and culturally diverse NLP resources. </p>
<blockquote>
<p>在这篇论文中，我们介绍了Akan对话情感（ACE）数据集，这是针对非洲语言的首个多模态情感对话数据集，解决了情感识别研究中低资源语言资源严重匮乏的问题。ACE是为Akan语言开发的，包含385个情感标签对话和6162个音频、视觉和文本模态的言论，以及词级韵律重点注释。该数据集中还包含韵律标签，使其成为首个经韵律注释的非洲语言数据集。我们通过使用最先进的情感识别方法进行的实验展示了ACE的质量和实用性，为未来的研究奠定了坚实的基准。我们希望ACE能激发对包容性、语言和文化多样的自然语言处理资源的进一步研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10973v3">PDF</a> Accepted to Findings at ACL 2025</p>
<p><strong>Summary</strong></p>
<p>ACE数据集是首个针对非洲语言的多模态情感对话数据集，弥补了低资源语言情感识别研究的资源匮乏问题。该数据集为Akan语言开发，包含385个情感标签对话和6,162个跨音频、视觉和文本模态的发音，以及单词级别的韵律重点注释。ACE数据集通过采用最前沿的情感识别方法进行实验，证明了其质量和实用性，为未来研究奠定了坚实的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ACE数据集是首个针对非洲语言的多模态情感对话数据集。</li>
<li>ACE数据集旨在弥补低资源语言在情感识别研究中的资源匮乏问题。</li>
<li>数据集包含385个情感标签对话和6,162个跨音频、视觉和文本模态的发音。</li>
<li>ACE数据集是首个包含韵律注释的非洲语言数据集。</li>
<li>通过采用最前沿的情感识别方法进行实验，证明了ACE数据集的质量和实用性。</li>
<li>ACE数据集为未来研究奠定了坚实的基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10973">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f6c4898de1d3580590474e1dd6b1edb1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7961c7732a0dcdcf78dc07dc363e6abb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-08ff66db25e95cb57721dce54568c22c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f0b45f7b4b31ed668f302ca39d796dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2f2a6b240ff4367e78d380e9a269ccf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ace98215964e1e900c1a7e66c6fa7c32.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2e35b10a36d4c149c69be27a29f7472c.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-06-05  TalkingMachines Real-Time Audio-Driven FaceTime-Style Video via   Autoregressive Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-be64077dbb482c82f476bdd21fb1ad4e.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-06-05  Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich   Representation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26551.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
