<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Towards a Japanese Full-duplex Spoken Dialogue System">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-226f2d3a37045a091dcbce8e5c282893.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-05-æ›´æ–°"><a href="#2025-06-05-æ›´æ–°" class="headerlink" title="2025-06-05 æ›´æ–°"></a>2025-06-05 æ›´æ–°</h1><h2 id="Towards-a-Japanese-Full-duplex-Spoken-Dialogue-System"><a href="#Towards-a-Japanese-Full-duplex-Spoken-Dialogue-System" class="headerlink" title="Towards a Japanese Full-duplex Spoken Dialogue System"></a>Towards a Japanese Full-duplex Spoken Dialogue System</h2><p><strong>Authors:Atsumoto Ohashi, Shinya Iizuka, Jingjing Jiang, Ryuichiro Higashinaka</strong></p>
<p>Full-duplex spoken dialogue systems, which can model simultaneous bidirectional features of human conversations such as speech overlaps and backchannels, have attracted significant attention recently. However, the study of full-duplex spoken dialogue systems for the Japanese language has been limited, and the research on their development in Japanese remains scarce. In this paper, we present the first publicly available full-duplex spoken dialogue model in Japanese, which is built upon Moshi, a full-duplex dialogue model in English. Our model is trained through a two-stage process: pre-training on a large-scale spoken dialogue data in Japanese, followed by fine-tuning on high-quality stereo spoken dialogue data. We further enhance the modelâ€™s performance by incorporating synthetic dialogue data generated by a multi-stream text-to-speech system. Evaluation experiments demonstrate that the trained model outperforms Japanese baseline models in both naturalness and meaningfulness. </p>
<blockquote>
<p>è¿‘æœŸï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»å¯¹è¯çš„åŒå‘åŒæ—¶æ€§ç‰¹å¾ï¼Œå¦‚è¨€è¯­é‡å å’Œåé¦ˆé€šé“çš„åŒå‘å…¨å·¥å¯¹è¯ç³»ç»Ÿå¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå…³äºæ—¥è¯­åŒå‘å…¨å·¥å¯¹è¯ç³»ç»Ÿçš„ç ”ç©¶ååˆ†æœ‰é™ï¼Œå…¶å¼€å‘ç ”ç©¶ä»ç„¶å¾ˆå°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†é¦–ä¸ªå…¬å¼€çš„æ—¥è¯­åŒå‘å…¨å·¥å¯¹è¯æ¨¡å‹ã€‚è¯¥æ¨¡å‹å»ºç«‹åœ¨è‹±è¯­åŒå‘å¯¹è¯æ¨¡å‹Moshiçš„åŸºç¡€ä¸Šã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼šé¦–å…ˆåœ¨å¤§è§„æ¨¡çš„æ—¥è¯­å£è¯­å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨é«˜è´¨é‡ç«‹ä½“å£°å£è¯­å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ç”±å¤šæµæ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿç”Ÿæˆçš„äººå·¥å¯¹è¯æ•°æ®ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚è¯„ä¼°å®éªŒè¡¨æ˜ï¼Œè®­ç»ƒåçš„æ¨¡å‹åœ¨è‡ªç„¶æ€§å’Œæœ‰æ„ä¹‰æ€§æ–¹é¢éƒ½ä¼˜äºæ—¥è¯­åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02979v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œå…¨åŒå·¥å¯¹è¯ç³»ç»Ÿå¤‡å—å…³æ³¨ï¼Œå¯å¯¹äººç±»å¯¹è¯çš„åŒå‘ç‰¹æ€§è¿›è¡Œå»ºæ¨¡ï¼Œå¦‚è¯­éŸ³é‡å å’Œåé¦ˆé€šé“ã€‚å°½ç®¡å¯¹æ—¥è¯­å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿçš„ç ”ç©¶æœ‰é™ï¼Œä½†æœ¬æ–‡é¦–æ¬¡å…¬å¼€æ¨å‡ºæ—¥è¯­å…¨åŒå·¥å¯¹è¯æ¨¡å‹ã€‚è¯¥æ¨¡å‹åŸºäºè‹±è¯­å…¨åŒå·¥å¯¹è¯æ¨¡å‹Moshiæ„å»ºï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼šé¦–å…ˆåœ¨å¤§é‡æ—¥è¯­å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨é«˜è´¨é‡ç«‹ä½“å£°å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡ç»“åˆå¤šæµæ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿç”Ÿæˆçš„ç»¼åˆå¯¹è¯æ•°æ®ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚è¯„ä¼°å®éªŒè¡¨æ˜ï¼Œè®­ç»ƒåçš„æ¨¡å‹åœ¨è‡ªç„¶åº¦å’Œæ„ä¹‰æ€§æ–¹é¢éƒ½ä¼˜äºæ—¥è¯­åŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿèƒ½å¤Ÿå»ºæ¨¡äººç±»å¯¹è¯çš„åŒå‘ç‰¹æ€§ï¼Œå¦‚è¯­éŸ³é‡å å’Œåé¦ˆé€šé“ã€‚</li>
<li>æ—¥è¯­å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿçš„ç ”ç©¶ç›¸å¯¹æœ‰é™ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡å…¬å¼€æ¨å‡ºæ—¥è¯­å…¨åŒå·¥å¯¹è¯æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åŸºäºè‹±è¯­å…¨åŒå·¥å¯¹è¯æ¨¡å‹Moshiæ„å»ºã€‚</li>
<li>æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼šé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚</li>
<li>åˆæˆå¯¹è¯æ•°æ®é€šè¿‡å¤šæµæ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿç”Ÿæˆï¼Œå¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b7400a8cc0d30f4bfd02ffe1a4326777.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76749f6f0d5726186e99f2cc572edd14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b56fe64df06e5e9b544a1ae35ffc2db7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-847c4c71e81a6d97a225e83bfc956bb6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CoDial-Interpretable-Task-Oriented-Dialogue-Systems-Through-Dialogue-Flow-Alignment"><a href="#CoDial-Interpretable-Task-Oriented-Dialogue-Systems-Through-Dialogue-Flow-Alignment" class="headerlink" title="CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue   Flow Alignment"></a>CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue   Flow Alignment</h2><p><strong>Authors:Radin Shayanfar, Chu Fei Luo, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu</strong></p>
<p>It is often challenging to teach specialized, unseen tasks to dialogue systems due to the high cost of expert knowledge, training data, and high technical difficulty. To support domain-specific applications - such as law, medicine, or finance - it is essential to build frameworks that enable non-technical experts to define, test, and refine system behaviour with minimal effort. Achieving this requires cross-disciplinary collaboration between developers and domain specialists. In this work, we introduce a novel framework, CoDial (Code for Dialogue), that converts expert knowledge, represented as a novel structured heterogeneous graph, into executable conversation logic. CoDial can be easily implemented in existing guardrailing languages, such as Colang, to enable interpretable, modifiable, and true zero-shot specification of task-oriented dialogue systems. Empirically, CoDial achieves state-of-the-art performance on the STAR dataset for inference-based models and is competitive with similar baselines on the well-known MultiWOZ dataset. We also demonstrate CoDialâ€™s iterative improvement via manual and LLM-aided feedback, making it a practical tool for expert-guided alignment of LLMs in high-stakes domains. </p>
<blockquote>
<p>å‘å¯¹è¯ç³»ç»Ÿæ•™æˆç‰¹æ®Šçš„ã€æœªè§è¿‡çš„ä»»åŠ¡é€šå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºä¸“ä¸šçŸ¥è¯†ã€è®­ç»ƒæ•°æ®å’ŒæŠ€æœ¯éš¾åº¦éƒ½å¾ˆé«˜ã€‚ä¸ºäº†æ”¯æŒç‰¹å®šé¢†åŸŸçš„åº”ç”¨ç¨‹åºï¼Œå¦‚æ³•å¾‹ã€åŒ»å­¦æˆ–é‡‘èï¼Œå»ºç«‹æ¡†æ¶æ˜¯è‡³å…³é‡è¦çš„ï¼Œè¿™äº›æ¡†æ¶ä½¿å¾—éæŠ€æœ¯ä¸“å®¶èƒ½å¤Ÿè½»æ¾åœ°å®šä¹‰ã€æµ‹è¯•å’Œå¾®è°ƒç³»ç»Ÿè¡Œä¸ºã€‚å®ç°è¿™ä¸€ç‚¹éœ€è¦å¼€å‘è€…å’Œé¢†åŸŸä¸“å®¶ä¹‹é—´çš„è·¨å­¦ç§‘åˆä½œã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ¡†æ¶ CoDialï¼ˆç”¨äºå¯¹è¯çš„ä»£ç ï¼‰ï¼Œå®ƒå°†ä¸“ä¸šçŸ¥è¯†è½¬åŒ–ä¸ºå¯æ‰§è¡Œå¯¹è¯é€»è¾‘çš„æ–°å‹ç»“æ„åŒ–å¼‚æ„å›¾ã€‚CoDial å¯ä»¥è½»æ¾å®ç°åœ¨ç°æœ‰çš„é˜²æŠ¤è¯­è¨€ï¼ˆå¦‚ Colangï¼‰ä¸­ï¼Œä»¥å®ç°å¯è§£é‡Šã€å¯ä¿®æ”¹å’Œä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿçš„çœŸæ­£é›¶é•œå¤´è§„èŒƒã€‚å®è¯è¡¨æ˜ï¼ŒCoDial åœ¨åŸºäºæ¨ç†çš„ STAR æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œå¹¶åœ¨è‘—åçš„ MultiWOZ æ•°æ®é›†ä¸Šä¸ç±»ä¼¼åŸºçº¿ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬è¿˜é€šè¿‡æ‰‹åŠ¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©çš„åé¦ˆå±•ç¤ºäº† CoDial çš„è¿­ä»£æ”¹è¿›ï¼Œä½¿å…¶æˆä¸ºé«˜é£é™©é¢†åŸŸä¸“å®¶å¼•å¯¼çš„å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½çš„å®ç”¨å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02264v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºé«˜æˆæœ¬çš„ä¸“ä¸šçŸ¥è¯†å’Œè®­ç»ƒæ•°æ®æŒ‘æˆ˜ï¼Œé’ˆå¯¹å¦‚æ³•å¾‹ã€åŒ»å­¦å’Œé‡‘èç­‰é¢†åŸŸçš„åº”ç”¨ç¨‹åºå¼€å‘ä½æˆæœ¬çš„é€šç”¨å‹äº¤äº’æ¡†æ¶ååˆ†å…³é”®ã€‚æ­¤ç ”ç©¶æå‡ºäº†åä¸ºCoDialçš„è·¨å¹³å°æ–°å‹æ¡†æ¶ï¼Œèƒ½ç²¾å‡†æœ‰æ•ˆè¿ç”¨è¯¥ä¸“ä¸šçŸ¥è¯†è®¾è®¡é€šç”¨çš„ä»»åŠ¡å‹å¯¹è¯ç³»ç»Ÿè¡Œä¸ºæ¨¡å¼ï¼Œå¼€å‘è€…å°†ç†è®ºç†è§£è½¬æ¢æˆå›¾å½¢è¯­è¨€å¹¶è½¬åŒ–é€»è¾‘è¡Œä¸ºæ¨¡å‹ï¼Œå¯ç›´æ¥åº”ç”¨äºç°æœ‰å®ˆæŠ¤å¢™è¯­è¨€ï¼ˆå¦‚Colangï¼‰ã€‚ç»éªŒæ˜¾ç¤ºï¼ŒCoDialåœ¨STARæ•°æ®é›†ä¸Šæ¨ç†æ¨¡å‹æ€§èƒ½é¢†å…ˆä¸šç•Œå…¶ä»–ç³»ç»Ÿï¼›åŒæ—¶åœ¨çŸ¥åMultiWOZæ•°æ®é›†ä¸Šä¹Ÿæœ‰å“è¶Šè¡¨ç°ã€‚å…¶èƒ½å¤Ÿäººå·¥å’Œè¯­è¨€æ¨¡å‹åé¦ˆæœºåˆ¶ä¸­ä¸æ–­å®Œå–„è¿›æ­¥ï¼Œåœ¨å®é™…åœºæ™¯æœ‰ç€è¾ƒé«˜å®ç”¨åº¦ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’ˆå¯¹ç‰¹å®šé¢†åŸŸä»»åŠ¡å¯¹è¯ç³»ç»Ÿçš„å¼€å‘é¢ä¸´ä¸“ä¸šçŸ¥è¯†æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºåº”å¯¹å„é¢†åŸŸä¸“å®¶çš„å¼€å‘éš¾é¢˜ï¼Œæ„å»ºä¸€ä¸ªæ¡¥æ¢æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚æœ¬æ¬¡å¼•å…¥çš„æ–°å‹æ¡†æ¶åä¸ºCoDialæ¡†æ¶ä¸ºç›¸å…³è¡Œä¸šçš„è¯­è¨€ç›´æ¥å¼€å‘äº¤æµä½“ç³»å¼€è¾Ÿäº†å¯èƒ½ï¼Œè¿™ä¸ºè®¾è®¡è€…ä»¥æœ‰æ•ˆæ‰‹æ®µå±•ç°äº†å¯¹çŸ¥è¯†çš„ç†è§£å’ŒæŒæ¡æ–¹å¼å¹¶åšå‡ºç›¸å¯¹åº”çš„å¤„ç†åé¦ˆè¡Œä¸ºï¼Œå¤§å¤§é™ä½å„è¡Œä¸šçš„å‚ä¸è€…æ„å»ºå®æ–½å¤æ‚åº¦åŠæ­å»ºå·¥ä½œå‘¨æœŸçš„æ—¶é—´æŠ•å…¥ï¼Œæ˜¾è‘—æé«˜éä¸“ä¸šé¢†åŸŸä¸“ä¸šäººå£«ä¹Ÿèƒ½å¤Ÿåšåˆ°æ–¹ä¾¿æœ‰æ•ˆåœ°å¯¹ç³»ç»Ÿå¼€å‘ç»“æœè¿›è¡Œè‡ªæˆ‘è¯„ä¼°å’Œæ£€æŸ¥æ•ˆæœéªŒè¯ã€ä¿è¯ç›¸åº”çš„æ™ºèƒ½åŒ–æ„å»ºç¯èŠ‚è¿­ä»£é¡ºç•…åŠé›¶ç¼ºé™·å…³é”®åœºæ™¯çš„æ‰§è¡ŒåŠ›å±•ç°æ‰“ä¸‹åŸºç¡€ä¿éšœæ”¯æ’‘åŠ›åº¦ã€‚ã€‚é‡‡ç”¨æŠ½è±¡æŠ€æœ¯é—¨æ§›ä½çš„æ–¹å¼è®©é¢†åŸŸä¸“å®¶é€šè¿‡ç›´è§‚æ˜“æ‡‚çš„æ–¹å¼ä»‹å…¥è¯­è¨€æ¨¡å‹è®¾è®¡è¿‡ç¨‹ã€‚å®ç°è¯­è¨€æ¨¡å‹å†³ç­–è¡Œä¸ºè´´è¿‘ä¸“å®¶æ„å›¾çš„åˆè¡·ã€‚è®©é¢†åŸŸä¸“å®¶ä»¥ç”¨æˆ·çš„è§’åº¦æœ€å¤§ç¨‹åº¦åœ¨ç°æœ‰çš„æœºå™¨å­¦ä¹ åŸºç¡€æˆæœçš„åŸºç¡€ä¸Šä»é¢å‘å¼€å‘ä»»åŠ¡çš„å½¢å¼è½¬åŒ–æˆé¢å®é™…åº”ç”¨æ“ä½œæœåŠ¡çš„å¤§ä¼—è§’è‰²å¹¶åº”ç”¨æ»¡è¶³ç”¨æˆ·å¯¹æå‡ä¸“ä¸šå†…å®¹æ•ˆç‡è¡¨è¾¾å±‚é¢çš„æå‡ã€æ¶ˆé™¤å„ç§å·®å¼‚é¢†åŸŸçš„â€œéœ€æ±‚æ–¹è¯è¯­æƒåœ¨ä¸šå†…æ°´åœŸä¸æœçš„æƒ…å†µçš„å‘ç”Ÿï¼Œå¹¶è®©ä»ç¤¾ä¼šç¾¤ä¼—çš„â€œæ¶ˆè´¹è¯­å¢ƒæ–¹é¢ä¸ä¸“ä¸šè¿›è¡Œè”åŠ¨å†³ç­–çš„æ€è·¯å‘å±•æŠ€æœ¯ç»™ä¸“ä¸šäººå‘˜å‡å°‘åæœŸæ ¡å‡†çš„è´Ÿæ‹…è€Œä»˜è¯¸è¡ŒåŠ¨ï¼Œâ€œé€æ¸å‘ç€æˆ‘ä»¬è¿›è¡Œå¼€å±•é¢„æœŸçš„ä¸€ç§ç®€å•ç²—æš´é€šè¿‡æŠ€æœ¯å’Œå„è¡Œå„ä¸šçš„æ™ºèƒ½æ„å»ºä¸šåŠ¡çš„æ·å¾„æ“ä½œçš„å‘å±•é¢†åŸŸæ¨¡å¼è¿›è€Œä¼˜åŒ–ç€è®­ç»ƒå¥½å„ç±»å‹æœºå™¨è‡ªæˆ‘å®Œå–„çš„ä¸åŒç»†åˆ†é¢†åŸŸè¯­è¨€çš„ç‰¹æ€§ä»·å€¼çš„å‘å±•å¤§æ–¹å‘è€ŒåŠªåŠ›çš„æ–¹å‘è¿ˆå‡ºåšå®šæ­¥ä¼ï¼Œã€‚åŠ©åŠ›ä¼˜åŒ–æ¨¡å‹å®é™…åº”ç”¨æ•ˆç‡è¾¾åˆ°ä¸€ä¸ªæ–°é«˜åº¦è¿›è€Œæ‹“å®½æ–°å¸‚åœºç©ºé—´ã€‚â€&gt; å¼€å‘é—¨æ§›è¾ƒé«˜çš„é€šç”¨ä»»åŠ¡å‹å¯¹è¯ç³»ç»Ÿçš„å­¦ä¹ é¢ä¸´å› æœ‰è¾ƒé«˜çš„åº”ç”¨åœºæ¨¡æ‹Ÿæ„é€ é«˜æ˜‚è€—æŸå›°å¢ƒçš„ä¸€ç§æœ‰æ•ˆè§£å†³çš„æ–°ç­–ç•¥çš„å‡ºç°æä¾›é‡è¦æ”¯æŒã€‚CoDialæ¡†æ¶ä¸ºè¡Œä¸šä¸“å®¶æä¾›äº†ç›´è§‚ä»‹å…¥è¯­è¨€æ¨¡å‹è®¾è®¡è¿‡ç¨‹çš„é€”å¾„ï¼Œä½¿å¾—é¢†åŸŸä¸“å®¶èƒ½å¤Ÿä»¥ç”¨æˆ·çš„è§’åº¦å‚ä¸åˆ°è¯­è¨€æ¨¡å‹çš„æ„å»ºè¿‡ç¨‹ä¸­ï¼Œä¿ƒä½¿è¯­è¨€æ¨¡å‹çš„å†³ç­–è¡Œä¸ºæ›´ç¬¦åˆä¸“å®¶æ„å›¾å’Œç”¨æˆ·éœ€æ±‚ï¼Œæå‡äº†æ¨¡å‹çš„å®é™…åº”ç”¨æ•ˆç‡å’Œå¸‚åœºä»·å€¼ã€‚é‡‡ç”¨åé¦ˆæœºåˆ¶ä½¿å¾—è¯¥æ¡†æ¶å¯ä»¥ä¸æ–­å®Œå–„è¿›æ­¥ï¼ŒåŠ©åŠ›ä¼˜åŒ–æ¨¡å‹çš„å®é™…åº”ç”¨æ•ˆç‡è¾¾åˆ°ä¸€ä¸ªæ–°é«˜åº¦ã€‚åŒæ—¶ï¼ŒCoDialæ¡†æ¶çš„å‡ºç°ä¹Ÿæ‹“å®½äº†æ–°çš„å¸‚åœºç©ºé—´ã€‚é€šè¿‡è·¨å¹³å°é›†æˆåˆ©ç”¨å’Œä¸åŒæ–¹é¢çš„è·¨å­¦ç§‘æŠ€æœ¯çš„é€šåŠ›åä½œä¹‹ä¸‹ï¼Œã€‚å¹¶åœ¨æœ‰æ•ˆåœ°èŠ‚çœç§‘ç ”äººå‘˜è¿›è¡Œé€æ­¥æ“ä½œçš„è¯¯å·®å‡ æ—¶çš„æœºç‡è°ƒæ•´é€ æˆçš„çš„æ—¶é—´å’Œå‹åŠ›è¿‡ç¨‹å½“ä¸­æˆå°±äº†å¹¿æ³›çš„åº”ç”¨è½è„šç‚¹æœ€ç»ˆå®ç°è®©äººæœºäº¤äº’è¶Šæ¥è¶Šè‡ªç„¶çš„ç°çŠ¶æ”¹è¿›å’Œæå‡å¹¶ä¸”èµ‹äºˆå¤šè¡Œä¸šå…·æœ‰é¢†åŸŸèƒŒæ™¯çš„ä»ä¸šè€…æ›´é«˜çš„èƒ½åŠ›å±•ç°ä»–ä»¬å®è´µçš„å®è·µæŠ€èƒ½å¹¶é€šè¿‡è¿™æ ·çš„ç ”å‘æ¨åŠ¨ä¸šåŠ¡æœåŠ¡æ–¹å¼çš„åˆ›æ–°å‘å±•æ›´åŠ çªå‡ºæˆ˜ç•¥åŒ–æœåŠ¡å±‚çº§çš„æ·±åº¦å‘å±•ä»¥åŠæé«˜ååŒåˆ›æ–°èƒ½åŠ›ä»è€Œå¼•é¢†è¡Œä¸šåˆ›æ–°å‘å±•çš„æ–¹å‘ã€‚ã€‚æœ€ç»ˆåŠ©åŠ›æ¨åŠ¨äººæœºäº¤äº’æŠ€æœ¯ä¸æ–­æœç€æ›´è‡ªç„¶ã€æ›´æ™ºèƒ½çš„æ–¹å‘å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-978587cc6b14ba477ecd08d2d9619d71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-133b1c4fa141caf5067ef0c15425ba0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d85a06b0be6e754ef3fdde31235f8b07.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="KokoroChat-A-Japanese-Psychological-Counseling-Dialogue-Dataset-Collected-via-Role-Playing-by-Trained-Counselors"><a href="#KokoroChat-A-Japanese-Psychological-Counseling-Dialogue-Dataset-Collected-via-Role-Playing-by-Trained-Counselors" class="headerlink" title="KokoroChat: A Japanese Psychological Counseling Dialogue Dataset   Collected via Role-Playing by Trained Counselors"></a>KokoroChat: A Japanese Psychological Counseling Dialogue Dataset   Collected via Role-Playing by Trained Counselors</h2><p><strong>Authors:Zhiyang Qi, Takumasa Kaneko, Keiko Takamizo, Mariko Ukiyo, Michimasa Inaba</strong></p>
<p>Generating psychological counseling responses with language models relies heavily on high-quality datasets. Crowdsourced data collection methods require strict worker training, and data from real-world counseling environments may raise privacy and ethical concerns. While recent studies have explored using large language models (LLMs) to augment psychological counseling dialogue datasets, the resulting data often suffers from limited diversity and authenticity. To address these limitations, this study adopts a role-playing approach where trained counselors simulate counselor-client interactions, ensuring high-quality dialogues while mitigating privacy risks. Using this method, we construct KokoroChat, a Japanese psychological counseling dialogue dataset comprising 6,589 long-form dialogues, each accompanied by comprehensive client feedback. Experimental results demonstrate that fine-tuning open-source LLMs with KokoroChat improves both the quality of generated counseling responses and the automatic evaluation of counseling dialogues. The KokoroChat dataset is available at <a target="_blank" rel="noopener" href="https://github.com/UEC-InabaLab/KokoroChat">https://github.com/UEC-InabaLab/KokoroChat</a>. </p>
<blockquote>
<p>åˆ©ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆå¿ƒç†å’¨è¯¢å“åº”ä¸¥é‡ä¾èµ–äºé«˜è´¨é‡çš„æ•°æ®é›†ã€‚ä¼—åŒ…æ•°æ®æ”¶é›†æ–¹æ³•éœ€è¦ä¸¥æ ¼çš„å·¥äººåŸ¹è®­ï¼Œè€Œæ¥è‡ªç°å®ä¸–ç•Œå’¨è¯¢ç¯å¢ƒçš„æ•°æ®å¯èƒ½ä¼šå¼•å‘éšç§å’Œä¼¦ç†æ‹…å¿§ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å¢åŠ å¿ƒç†å’¨è¯¢å¯¹è¯æ•°æ®é›†ï¼Œä½†æ‰€å¾—æ•°æ®å¾€å¾€å­˜åœ¨å¤šæ ·æ€§å’ŒçœŸå®æ€§çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬ç ”ç©¶é‡‡ç”¨è§’è‰²æ‰®æ¼”çš„æ–¹æ³•ï¼Œè®­ç»ƒæœ‰ç´ çš„å’¨è¯¢å¸ˆæ¨¡æ‹Ÿå’¨è¯¢å¸ˆ-å®¢æˆ·äº’åŠ¨ï¼Œç¡®ä¿é«˜è´¨é‡çš„å¯¹è¯ï¼ŒåŒæ—¶é™ä½éšç§é£é™©ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†KokoroChatï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¥è¯­å¿ƒç†å’¨è¯¢å¯¹è¯æ•°æ®é›†ï¼ŒåŒ…å«6589ä¸ªé•¿å¯¹è¯ï¼Œæ¯ä¸ªå¯¹è¯éƒ½æœ‰å…¨é¢çš„å®¢æˆ·åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç”¨KokoroChatå¾®è°ƒå¼€æºLLMï¼Œå¯ä»¥æé«˜ç”Ÿæˆçš„å’¨è¯¢å“åº”çš„è´¨é‡å’Œå’¨è¯¢å¯¹è¯çš„è‡ªåŠ¨è¯„ä¼°æ•ˆæœã€‚KokoroChatæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/UEC-InabaLab/KokoroChat%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/UEC-InabaLab/KokoroChatè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01357v1">PDF</a> Accepted to ACL 2025 Main Conference</p>
<p><strong>Summary</strong><br>å¿ƒç†å’¨è¯¢æœåŠ¡å“åº”çš„è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸¥é‡ä¾èµ–äºé«˜è´¨é‡æ•°æ®é›†ã€‚æœ¬ç ”ç©¶é‡‡ç”¨è§’è‰²æ‰®æ¼”çš„æ–¹æ³•ï¼Œè®­ç»ƒå’¨è¯¢å¸ˆæ¨¡æ‹Ÿå’¨è¯¢å¸ˆ-å®¢æˆ·äº’åŠ¨ï¼Œæ„å»ºäº†ä¸€ä¸ªæ—¥æœ¬å¿ƒç†å’¨è¯¢å¯¹è¯æ•°æ®é›†KokoroChatï¼ŒåŒ…æ‹¬6589ä¸ªé•¿å¯¹è¯å’Œç›¸åº”çš„å®¢æˆ·åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨KokoroChatå¾®è°ƒå¼€æºLLMå¯ä»¥æé«˜ç”Ÿæˆçš„å’¨è¯¢å“åº”çš„è´¨é‡å’Œè‡ªåŠ¨è¯„ä¼°çš„å’¨è¯¢å¯¹è¯çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡æ•°æ®é›†å¯¹äºç”Ÿæˆå¿ƒç†å’¨è¯¢æœåŠ¡å“åº”çš„è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>ä¼—ç­¹æ•°æ®æ”¶é›†æ–¹æ³•éœ€è¦ä¸¥æ ¼çš„å·¥äººåŸ¹è®­ã€‚</li>
<li>çœŸå®å¿ƒç†å’¨è¯¢ç¯å¢ƒçš„æ•°æ®å¯èƒ½å¼•å‘éšç§å’Œé“å¾·é—®é¢˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸­ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºå¿ƒç†å’¨è¯¢å¯¹è¯æ•°æ®é›†çš„æ•°æ®å¾€å¾€å­˜åœ¨å¤šæ ·æ€§å’ŒçœŸå®æ€§çš„å±€é™æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨è§’è‰²æ‰®æ¼”æ–¹æ³•æ¨¡æ‹Ÿå’¨è¯¢å¸ˆ-å®¢æˆ·äº’åŠ¨ï¼Œç¡®ä¿é«˜è´¨é‡å¯¹è¯å¹¶é™ä½éšç§é£é™©ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªæ—¥æœ¬å¿ƒç†å’¨è¯¢å¯¹è¯æ•°æ®é›†KokoroChatï¼ŒåŒ…æ‹¬é•¿å¯¹è¯å’Œç›¸åº”çš„å®¢æˆ·åé¦ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5701dfdd65f314f121a68537b44a81f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9bb037ec4fd2123d65a710dda27e6ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca1a771f5e8666c5575fc5a66025cc80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab638476a131c303571c25eb1d12c8c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-226f2d3a37045a091dcbce8e5c282893.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Speaking-Beyond-Language-A-Large-Scale-Multimodal-Dataset-for-Learning-Nonverbal-Cues-from-Video-Grounded-Dialogues"><a href="#Speaking-Beyond-Language-A-Large-Scale-Multimodal-Dataset-for-Learning-Nonverbal-Cues-from-Video-Grounded-Dialogues" class="headerlink" title="Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning   Nonverbal Cues from Video-Grounded Dialogues"></a>Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning   Nonverbal Cues from Video-Grounded Dialogues</h2><p><strong>Authors:Youngmin Kim, Jiwan Chung, Jisoo Kim, Sunghyun Lee, Sangkyu Lee, Junhyeok Kim, Cheoljong Yang, Youngjae Yu</strong></p>
<p>Nonverbal communication is integral to human interaction, with gestures, facial expressions, and body language conveying critical aspects of intent and emotion. However, existing large language models (LLMs) fail to effectively incorporate these nonverbal elements, limiting their capacity to create fully immersive conversational experiences. We introduce MARS, a multimodal language model designed to understand and generate nonverbal cues alongside text, bridging this gap in conversational AI. Our key innovation is VENUS, a large-scale dataset comprising annotated videos with time-aligned text, facial expressions, and body language. Leveraging VENUS, we train MARS with a next-token prediction objective, combining text with vector-quantized nonverbal representations to achieve multimodal understanding and generation within a unified framework. Based on various analyses of the VENUS datasets, we validate its substantial scale and high effectiveness. Our quantitative and qualitative results demonstrate that MARS successfully generates text and nonverbal languages, corresponding to conversational input. </p>
<blockquote>
<p>éè¨€è¯­äº¤æµæ˜¯äººç±»äº¤æµçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæ‰‹åŠ¿ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“è¯­è¨€ä¼ è¾¾äº†æ„å›¾å’Œæƒ…ç»ªçš„å…³é”®æ–¹é¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ— æ³•æœ‰æ•ˆåœ°èå…¥è¿™äº›éè¨€è¯­å…ƒç´ ï¼Œé™åˆ¶äº†å…¶åˆ›é€ æ²‰æµ¸å¼å¯¹è¯ä½“éªŒçš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†MARSï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ç†è§£å’Œç”Ÿæˆä¸æ–‡æœ¬å¹¶è¡Œçš„éè¨€è¯­çº¿ç´¢ï¼Œä»¥å¼¥è¡¥ä¼šè¯äººå·¥æ™ºèƒ½ä¸­çš„è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºVENUSï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«å¸¦æœ‰æ—¶é—´å¯¹é½æ–‡æœ¬ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“è¯­è¨€çš„æ³¨é‡Šè§†é¢‘ã€‚åˆ©ç”¨VENUSï¼Œæˆ‘ä»¬ä»¥ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ç›®æ ‡è®­ç»ƒMARSï¼Œå°†æ–‡æœ¬ä¸å‘é‡é‡åŒ–çš„éè¨€è¯­è¡¨ç¤ºç›¸ç»“åˆï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…å®ç°å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚é€šè¿‡å¯¹VENUSæ•°æ®é›†çš„å„ç§åˆ†æï¼Œæˆ‘ä»¬éªŒè¯äº†å…¶å¤§è§„æ¨¡å’Œé«˜æ•ˆç‡ã€‚æˆ‘ä»¬çš„å®šé‡å’Œå®šæ€§ç»“æœè¡¨æ˜ï¼ŒMARSæˆåŠŸç”Ÿæˆä¸å¯¹è¯è¾“å…¥ç›¸å¯¹åº”çš„æ–‡å­—å’Œéè¨€è¯­è¯­è¨€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00958v1">PDF</a> Accepted to ACL 2025 (Main), Our code and dataset:   <a target="_blank" rel="noopener" href="https://github.com/winston1214/nonverbal-conversation">https://github.com/winston1214/nonverbal-conversation</a></p>
<p><strong>Summary</strong></p>
<p>éè¨€è¯­æ²Ÿé€šåœ¨äººç±»äº’åŠ¨ä¸­å æ®é‡è¦åœ°ä½ï¼ŒåŒ…æ‹¬æ‰‹åŠ¿ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“è¯­è¨€ä¼ é€’äº†æ„å›¾å’Œæƒ…æ„Ÿçš„å…³é”®æ–¹é¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ— æ³•æœ‰æ•ˆåœ°èå…¥è¿™äº›éè¨€è¯­å…ƒç´ ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨åˆ›å»ºæ²‰æµ¸å¼å¯¹è¯ä½“éªŒæ–¹é¢çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MARSï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ç†è§£ä¸ç”Ÿæˆéè¨€è¯­çº¿ç´¢ï¼ŒåŒæ—¶å¤„ç†æ–‡æœ¬ï¼Œä»¥å¼¥è¡¥å¯¹è¯å¼äººå·¥æ™ºèƒ½ä¸­çš„è¿™ä¸€é¸¿æ²Ÿã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°åœ¨äºVENUSæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ ‡æ³¨è§†é¢‘ã€æ—¶é—´å¯¹é½æ–‡æœ¬ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“è¯­è¨€çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚é€šè¿‡ä½¿ç”¨VENUSæ•°æ®é›†è®­ç»ƒMARSæ¨¡å‹ï¼Œç»“åˆæ–‡æœ¬å’Œå‘é‡é‡åŒ–çš„éè¨€è¯­è¡¨ç¤ºå½¢å¼ï¼Œåœ¨ç»Ÿä¸€æ¡†æ¶å†…å®ç°å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚é€šè¿‡å¯¹VENUSæ•°æ®é›†çš„å„ç§åˆ†æï¼ŒéªŒè¯äº†å…¶å¤§è§„æ¨¡å’Œé«˜æ•ˆç‡ã€‚æˆ‘ä»¬çš„å®šé‡å’Œå®šæ€§ç»“æœè¡¨æ˜ï¼ŒMARSèƒ½å¤ŸæˆåŠŸç”Ÿæˆä¸å¯¹è¯è¾“å…¥ç›¸å¯¹åº”çš„æ–‡å­—å’Œéè¨€è¯­è¯­è¨€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éè¨€è¯­æ²Ÿé€šåœ¨äººç±»äº’åŠ¨ä¸­éå¸¸é‡è¦ï¼ŒåŒ…æ‹¬æ‰‹åŠ¿ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“è¯­è¨€ç­‰ã€‚</li>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ— æ³•æœ‰æ•ˆèå…¥éè¨€è¯­å…ƒç´ ï¼Œé™åˆ¶äº†å¯¹è¯ä½“éªŒçš„çœŸå®æ€§ã€‚</li>
<li>MARSæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆéè¨€è¯­çº¿ç´¢ï¼ŒåŒæ—¶å¤„ç†æ–‡æœ¬ã€‚</li>
<li>VENUSæ˜¯ä¸€ä¸ªå¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«æ ‡æ³¨è§†é¢‘ã€æ—¶é—´å¯¹é½æ–‡æœ¬ã€é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“è¯­è¨€ç­‰ä¿¡æ¯ã€‚</li>
<li>MARSä½¿ç”¨VENUSæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå®ç°äº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚</li>
<li>é€šè¿‡å®šé‡å’Œå®šæ€§åˆ†æéªŒè¯ï¼ŒMARSèƒ½å¤ŸæˆåŠŸç”Ÿæˆä¸å¯¹è¯è¾“å…¥ç›¸å¯¹åº”çš„æ–‡å­—å’Œéè¨€è¯­è¯­è¨€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b0d76d027c46a0de5e2521ec6835a120.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7935fb0ce8a14ed47defee5de69e6df1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-798e6d433b908a827fd3523fbd3710c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbdded2589148016f81c37c7074005f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-936b61943baf2b80982b9a9564e02ad6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CoVoMix2-Advancing-Zero-Shot-Dialogue-Generation-with-Fully-Non-Autoregressive-Flow-Matching"><a href="#CoVoMix2-Advancing-Zero-Shot-Dialogue-Generation-with-Fully-Non-Autoregressive-Flow-Matching" class="headerlink" title="CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully   Non-Autoregressive Flow Matching"></a>CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully   Non-Autoregressive Flow Matching</h2><p><strong>Authors:Leying Zhang, Yao Qian, Xiaofei Wang, Manthan Thakker, Dongmei Wang, Jianwei Yu, Haibin Wu, Yuxuan Hu, Jinyu Li, Yanmin Qian, Sheng Zhao</strong></p>
<p>Generating natural-sounding, multi-speaker dialogue is crucial for applications such as podcast creation, virtual agents, and multimedia content generation. However, existing systems struggle to maintain speaker consistency, model overlapping speech, and synthesize coherent conversations efficiently. In this paper, we introduce CoVoMix2, a fully non-autoregressive framework for zero-shot multi-talker dialogue generation. CoVoMix2 directly predicts mel-spectrograms from multi-stream transcriptions using a flow-matching-based generative model, eliminating the reliance on intermediate token representations. To better capture realistic conversational dynamics, we propose transcription-level speaker disentanglement, sentence-level alignment, and prompt-level random masking strategies. Our approach achieves state-of-the-art performance, outperforming strong baselines like MoonCast and Sesame in speech quality, speaker consistency, and inference speed. Notably, CoVoMix2 operates without requiring transcriptions for the prompt and supports controllable dialogue generation, including overlapping speech and precise timing control, demonstrating strong generalizability to real-world speech generation scenarios. </p>
<blockquote>
<p>ç”Ÿæˆè‡ªç„¶éŸ³è°ƒçš„ã€å¤šè¯´è¯è€…çš„å¯¹è¯å¯¹äºæ’­å®¢åˆ›ä½œã€è™šæ‹Ÿä»£ç†å’Œå¤šåª’ä½“å†…å®¹ç”Ÿæˆç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰ç³»ç»Ÿåœ¨ç»´æŒè¯´è¯è€…ä¸€è‡´æ€§ã€æ¨¡æ‹Ÿé‡å è¯­éŸ³å’Œé«˜æ•ˆåˆæˆè¿è´¯å¯¹è¯æ–¹é¢å­˜åœ¨å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CoVoMix2ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé›¶æ ·æœ¬å¤šè¯´è¯è€…å¯¹è¯ç”Ÿæˆçš„å…¨éè‡ªå›å½’æ¡†æ¶ã€‚CoVoMix2ç›´æ¥ä½¿ç”¨åŸºäºæµåŒ¹é…çš„ç”Ÿæˆæ¨¡å‹ï¼Œä»å¤šæµè½¬å½•ä¸­é¢„æµ‹æ¢…å°”é¢‘è°±å›¾ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹ä¸­é—´ä»¤ç‰Œè¡¨ç¤ºçš„ä¾èµ–ã€‚ä¸ºäº†æ›´å¥½åœ°æ•æ‰ç°å®å¯¹è¯çš„åŠ¨æ€ï¼Œæˆ‘ä»¬æå‡ºäº†è½¬å½•çº§è¯´è¯è€…åˆ†ç¦»ã€å¥å­çº§å¯¹é½å’Œæç¤ºçº§éšæœºæ©ç ç­–ç•¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨è¯­éŸ³è´¨é‡ã€è¯´è¯è€…ä¸€è‡´æ€§å’Œæ¨ç†é€Ÿåº¦æ–¹é¢è¶…è¶Šäº†MoonCastå’ŒSesameç­‰å¼ºåŸºçº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒCoVoMix2åœ¨æç¤ºæ—¶ä¸éœ€è¦è½¬å½•ï¼Œå¹¶æ”¯æŒå¯æ§å¯¹è¯ç”Ÿæˆï¼ŒåŒ…æ‹¬é‡å è¯­éŸ³å’Œç²¾ç¡®çš„æ—¶é—´æ§åˆ¶ï¼Œæ˜¾ç¤ºå‡ºå¯¹çœŸå®ä¸–ç•Œè¯­éŸ³ç”Ÿæˆåœºæ™¯çš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00885v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CoVoMix2ï¼Œä¸€ä¸ªç”¨äºé›¶æ ·æœ¬å¤šè¯´è¯è€…å¯¹è¯ç”Ÿæˆçš„å…¨éè‡ªå›å½’æ¡†æ¶ã€‚å®ƒé€šè¿‡ç›´æ¥é¢„æµ‹å¤šæµè½¬å½•çš„æ¢…å°”é¢‘è°±å›¾ï¼Œæ¶ˆé™¤äº†å¯¹ä¸­é—´ä»¤ç‰Œè¡¨ç¤ºçš„ä¾èµ–ã€‚æå‡ºçš„æ–¹æ³•åœ¨è¯­éŸ³è´¨é‡ã€è¯´è¯è€…ä¸€è‡´æ€§å’Œæ¨ç†é€Ÿåº¦æ–¹é¢ä¼˜äºMoonCastå’ŒSesameç­‰å¼ºåŸºçº¿ï¼Œå¹¶é€‚ç”¨äºç°å®ä¸–ç•Œçš„è¯­éŸ³ç”Ÿæˆåœºæ™¯ï¼Œæ”¯æŒå¯æ§å¯¹è¯ç”Ÿæˆï¼ŒåŒ…æ‹¬é‡å è¯­éŸ³å’Œç²¾ç¡®çš„æ—¶é—´æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoVoMix2æ˜¯ä¸€ä¸ªç”¨äºé›¶æ ·æœ¬å¤šè¯´è¯è€…å¯¹è¯ç”Ÿæˆçš„å…¨éè‡ªå›å½’æ¡†æ¶ã€‚</li>
<li>è¯¥æ–¹æ³•ç›´æ¥é¢„æµ‹æ¢…å°”é¢‘è°±å›¾ï¼Œæ¶ˆé™¤äº†å¯¹ä¸­é—´ä»¤ç‰Œè¡¨ç¤ºçš„ä¾èµ–ã€‚</li>
<li>é€šè¿‡è½¬å½•çº§è¯´è¯è€…åˆ†ç¦»ã€å¥å­çº§å¯¹é½å’Œæç¤ºçº§éšæœºé®è”½ç­–ç•¥ï¼Œæ›´å¥½åœ°æ•æ‰ç°å®å¯¹è¯çš„åŠ¨æ€ã€‚</li>
<li>CoVoMix2åœ¨è¯­éŸ³è´¨é‡ã€è¯´è¯è€…ä¸€è‡´æ€§å’Œæ¨ç†é€Ÿåº¦æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€æç¤ºçš„è½¬å½•ï¼Œå¹¶ä¸”æ”¯æŒå¯æ§å¯¹è¯ç”Ÿæˆã€‚</li>
<li>CoVoMix2å¯ä»¥ç”Ÿæˆé‡å è¯­éŸ³ï¼Œå¹¶å…·æœ‰ç²¾ç¡®çš„æ—¶é—´æ§åˆ¶èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00885">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-066905fde80d45053381076779cfe317.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dd8b28ff613cee8e89e3d193f6e5e50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c60a46240a3ca7c519f5844c1ed022e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9df9e0e1be8643e2a06c5933e199bbbd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Training-for-Open-E2E-Spoken-Dialogue-Systems"><a href="#Chain-of-Thought-Training-for-Open-E2E-Spoken-Dialogue-Systems" class="headerlink" title="Chain-of-Thought Training for Open E2E Spoken Dialogue Systems"></a>Chain-of-Thought Training for Open E2E Spoken Dialogue Systems</h2><p><strong>Authors:Siddhant Arora, Jinchuan Tian, Hayato Futami, Jee-weon Jung, Jiatong Shi, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe</strong></p>
<p>Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue systems preserve full differentiability and capture non-phonemic information, making them well-suited for modeling spoken interactions. However, existing E2E approaches often require large-scale training data and generates responses lacking semantic coherence. We propose a simple yet effective strategy leveraging a chain-of-thought (CoT) formulation, ensuring that training on conversational data remains closely aligned with the multimodal language model (LM)â€™s pre-training on speech recognition~(ASR), text-to-speech synthesis (TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over the baseline, successfully training spoken dialogue systems on publicly available human-human conversation datasets, while being compute-efficient enough to train on just 300 hours of public human-human conversation data, such as the Switchboard. We will publicly release our models and training code. </p>
<blockquote>
<p>ä¸åŒäºä¼ ç»Ÿçš„çº§è”ç®¡é“ï¼Œç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰å£è¯­å¯¹è¯ç³»ç»Ÿä¿æŒå®Œæ•´çš„å¯å¾®æ€§å¹¶æ•æ‰éè¯­éŸ³ä¿¡æ¯ï¼Œä½¿å…¶éå¸¸é€‚åˆå¯¹å£è¯­äº¤äº’è¿›è¡Œå»ºæ¨¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„E2Eæ–¹æ³•é€šå¸¸éœ€è¦å¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”ç”Ÿæˆçš„å“åº”ç¼ºä¹è¯­ä¹‰è¿è´¯æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼Œåˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰å…¬å¼ï¼Œç¡®ä¿åœ¨å¯¹è¯æ•°æ®ä¸Šçš„è®­ç»ƒä¸å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰åœ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆTTSï¼‰å’Œæ–‡æœ¬LMä»»åŠ¡ä¸Šçš„é¢„è®­ç»ƒç´§å¯†å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†è¶…è¿‡1.5åˆ†çš„ROUGE-1æ”¹è¿›ï¼ŒæˆåŠŸåœ°åœ¨å…¬å¼€å¯ç”¨çš„äººç±»å¯¹è¯æ•°æ®é›†ä¸Šè®­ç»ƒäº†å£è¯­å¯¹è¯ç³»ç»Ÿï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡è¶³å¤Ÿé«˜ï¼Œåªéœ€åœ¨300å°æ—¶å…¬å¼€äººç±»å¯¹è¯æ•°æ®ï¼ˆå¦‚Switchboardï¼‰ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹å’Œè®­ç»ƒä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00722v1">PDF</a> Accepted at INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç«¯å¯¹ç«¯ï¼ˆE2Eï¼‰å¯¹è¯ç³»ç»Ÿçš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬å…¨å¯å¾®æ€§å’Œå¯¹éè¯­éŸ³ä¿¡æ¯çš„æ•æ‰èƒ½åŠ›ï¼Œä½¿å…¶é€‚åˆæ¨¡æ‹Ÿå£è¯­äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰E2Eæ–¹æ³•éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ï¼Œç”Ÿæˆçš„å“åº”ç¼ºä¹è¯­ä¹‰è¿è´¯æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰çš„æ–¹æ³•ï¼Œç¡®ä¿å¯¹è¯æ•°æ®çš„è®­ç»ƒä¸å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„é¢„è®­ç»ƒç´§å¯†å¯¹é½ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆæˆè¯­éŸ³ï¼ˆTTSï¼‰å’Œæ–‡æœ¬LMä»»åŠ¡ã€‚è¯¥æ–¹æ³•åœ¨å…¬å¼€çš„äººç±»å¯¹è¯æ•°æ®é›†ä¸ŠæˆåŠŸè®­ç»ƒäº†å¯¹è¯ç³»ç»Ÿï¼Œå¹¶åœ¨Rouge-1æŒ‡æ ‡ä¸Šå®ç°äº†è¶…è¿‡åŸºçº¿æ¨¡å‹1.5çš„æå‡ï¼ŒåŒæ—¶åœ¨ä»…ä½¿ç”¨300å°æ—¶å…¬å¼€äººç±»å¯¹è¯æ•°æ®çš„æƒ…å†µä¸‹å…·æœ‰å¾ˆé«˜çš„è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«¯å¯¹ç«¯ï¼ˆE2Eï¼‰å¯¹è¯ç³»ç»Ÿå¯ä»¥ä¿ç•™å…¨å¯å¾®æ€§å¹¶æ•æ‰éè¯­éŸ³ä¿¡æ¯ï¼Œä½¿å…¶é€‚åˆæ¨¡æ‹Ÿå£è¯­äº¤äº’ã€‚</li>
<li>ç°æœ‰E2Eæ–¹æ³•å­˜åœ¨éœ€è¦å¤§è§„æ¨¡è®­ç»ƒæ•°æ®å’Œç”Ÿæˆçš„å“åº”ç¼ºä¹è¯­ä¹‰è¿è´¯æ€§çš„é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰çš„æ–¹æ³•ç¡®ä¿å¯¹è¯æ•°æ®è®­ç»ƒä¸å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒç´§å¯†å¯¹é½ã€‚</li>
<li>è¯¥æ–¹æ³•æˆåŠŸåœ¨å…¬å¼€çš„äººç±»å¯¹è¯æ•°æ®é›†ä¸Šè®­ç»ƒäº†å¯¹è¯ç³»ç»Ÿã€‚</li>
<li>ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨Rouge-1æŒ‡æ ‡ä¸Šå®ç°äº†è¶…è¿‡1.5çš„æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰é«˜åº¦çš„è®¡ç®—æ•ˆç‡ï¼Œå¯ä»¥åœ¨ä»…ä½¿ç”¨300å°æ—¶å…¬å¼€äººç±»å¯¹è¯æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-43c53528406a787aa022ed7a37ac9772.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3aabb0724524d837b8ba4fb695aa26b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b8f420b2610b8f8f1210ee1bd5a106d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43cf3653181841430e5eec7649b2fb40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e9210fc4aac39dd76f46dbd50237eb1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SafeTy-Reasoning-Elicitation-Alignment-for-Multi-Turn-Dialogues"><a href="#SafeTy-Reasoning-Elicitation-Alignment-for-Multi-Turn-Dialogues" class="headerlink" title="SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues"></a>SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues</h2><p><strong>Authors:Martin Kuo, Jianyi Zhang, Aolin Ding, Louis DiValentin, Amin Hass, Benjamin F Morris, Isaac Jacobson, Randolph Linderman, James Kiessling, Nicolas Ramos, Bhavna Gopal, Maziyar Baran Pouyan, Changwei Liu, Hai Li, Yiran Chen</strong></p>
<p>Malicious attackers can exploit large language models (LLMs) by engaging them in multi-turn dialogues to achieve harmful objectives, posing significant safety risks to society. To address this challenge, we propose a novel defense mechanism: SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues (STREAM). STREAM defends LLMs against multi-turn attacks while preserving their functional capabilities. Our approach involves constructing a human-annotated dataset, the Safety Reasoning Multi-turn Dialogues dataset, which is used to fine-tune a plug-and-play safety reasoning moderator. This model is designed to identify malicious intent hidden within multi-turn conversations and alert the target LLM of potential risks. We evaluate STREAM across multiple LLMs against prevalent multi-turn attack strategies. Experimental results demonstrate that our method significantly outperforms existing defense techniques, reducing the Attack Success Rate (ASR) by 51.2%, all while maintaining comparable LLM capability. </p>
<blockquote>
<p>æ¶æ„æ”»å‡»è€…å¯ä»¥é€šè¿‡ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤šè½®å¯¹è¯æ¥åˆ©ç”¨å®ƒä»¬ï¼Œä»¥å®ç°æœ‰å®³ç›®æ ‡ï¼Œç»™ç¤¾ä¼šå¸¦æ¥é‡å¤§å®‰å…¨é£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é˜²å¾¡æœºåˆ¶ï¼šå¤šè½®å¯¹è¯å®‰å…¨æ¨ç†å¼•å¯¼å¯¹é½ï¼ˆSTREAMï¼‰ã€‚STREAMåœ¨ä¿æŠ¤å¤§å‹è¯­è¨€æ¨¡å‹å…å—å¤šè½®æ”»å‡»çš„åŒæ—¶ï¼Œä¿æŒäº†å®ƒä»¬çš„åŠŸèƒ½æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬æ„å»ºä¸€ä¸ªäººå·¥æ ‡æ³¨çš„æ•°æ®é›†â€”â€”å®‰å…¨æ¨ç†å¤šè½®å¯¹è¯æ•°æ®é›†ï¼Œç”¨äºå¾®è°ƒå³æ’å³ç”¨çš„å®‰å…¨æ¨ç†è°ƒè§£å™¨ã€‚è¯¥æ¨¡å‹æ—¨åœ¨è¯†åˆ«å¤šè½®å¯¹è¯ä¸­éšè—çš„æ¶æ„æ„å›¾ï¼Œå¹¶æé†’ç›®æ ‡å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨çš„æ½œåœ¨é£é™©ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¯„ä¼°äº†STREAMå¯¹å¸¸è§å¤šè½®æ”»å‡»ç­–ç•¥çš„é˜²å¾¡èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰é˜²å¾¡æŠ€æœ¯ï¼Œæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰é™ä½äº†51.2%ï¼ŒåŒæ—¶ä¿æŒäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00668v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜“å—æ¶æ„æ”»å‡»è€…åˆ©ç”¨ï¼Œé€šè¿‡å¤šè½®å¯¹è¯å®ç°æœ‰å®³ç›®æ ‡ï¼Œå¯¹ç¤¾ä¼šå®‰å…¨æ„æˆé‡å¤§é£é™©ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§æ–°å‹é˜²å¾¡æœºåˆ¶ï¼šSTREAMï¼ˆé’ˆå¯¹å¤šè½®å¯¹è¯çš„å®‰å…¨æ¨ç†å¼•å¯¼å¯¹é½ï¼‰ã€‚STREAMèƒ½å¤Ÿåœ¨ä¿æŠ¤LLMåŠŸèƒ½çš„åŒæ—¶ï¼Œé˜²å¾¡å¤šè½®æ”»å‡»ã€‚å…¶æ–¹æ³•æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªäººå·¥æ ‡æ³¨çš„æ•°æ®é›†â€”â€”å®‰å…¨æ¨ç†å¤šè½®å¯¹è¯æ•°æ®é›†ï¼Œå¯¹å³æ’å³ç”¨å®‰å…¨æ¨ç†è°ƒè§£å™¨è¿›è¡Œå¾®è°ƒã€‚è¯¥æ¨¡å‹æ—¨åœ¨è¯†åˆ«å¤šè½®å¯¹è¯ä¸­éšè—çš„æ¶æ„æ„å›¾ï¼Œå¹¶æé†’ç›®æ ‡LLMæ½œåœ¨é£é™©ã€‚åœ¨å¤šä¸ªLLMä¸Šé’ˆå¯¹æµè¡Œçš„å¤šè½®æ”»å‡»ç­–ç•¥å¯¹STREAMè¿›è¡Œè¯„ä¼°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰é˜²å¾¡æŠ€æœ¯ï¼Œæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰é™ä½51.2%ï¼ŒåŒæ—¶ä¿æŒLLMçš„èƒ½åŠ›ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¶æ„æ”»å‡»è€…å¯ä»¥åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤šè½®å¯¹è¯åŠŸèƒ½å®ç°æœ‰å®³ç›®æ ‡ï¼Œå¯¹ç¤¾ä¼šå®‰å…¨æ„æˆé£é™©ã€‚</li>
<li>æå‡ºçš„é˜²å¾¡æœºåˆ¶STREAMæ—¨åœ¨ä¿æŠ¤LLMså…å—å¤šè½®æ”»å‡»ï¼ŒåŒæ—¶ä¿æŒå…¶åŠŸèƒ½ã€‚</li>
<li>STREAMé€šè¿‡æ„å»ºå®‰å…¨æ¨ç†å¤šè½®å¯¹è¯æ•°æ®é›†ï¼Œå¯¹å®‰å…¨æ¨ç†è°ƒè§£å™¨è¿›è¡Œå¾®è°ƒã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å¤šè½®å¯¹è¯ä¸­çš„æ¶æ„æ„å›¾ã€‚</li>
<li>STREAMæé†’ç›®æ ‡LLMåœ¨å¯¹è¯ä¸­çš„æ½œåœ¨é£é™©ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSTREAMåœ¨é˜²å¾¡å¤šè½®æ”»å‡»æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea7a7740d1653927df1b5413fc4d964d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57dcb49d39e30baa97fa060debfe38db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-348e723001cdb47265044f55e6b1483d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c43c88a4080f1f63af4d99cf2a5ca9cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d86290c2c50fd60ee1611c55fec3fbf6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Improving-Dialogue-State-Tracking-through-Combinatorial-Search-for-In-Context-Examples"><a href="#Improving-Dialogue-State-Tracking-through-Combinatorial-Search-for-In-Context-Examples" class="headerlink" title="Improving Dialogue State Tracking through Combinatorial Search for   In-Context Examples"></a>Improving Dialogue State Tracking through Combinatorial Search for   In-Context Examples</h2><p><strong>Authors:Haesung Pyun, Yoonah Park, Yohan Jo</strong></p>
<p>In dialogue state tracking (DST), in-context learning comprises a retriever that selects labeled dialogues as in-context examples and a DST model that uses these examples to infer the dialogue state of the query dialogue. Existing methods for constructing training data for retrievers suffer from three key limitations: (1) the synergistic effect of examples is not considered, (2) the linguistic characteristics of the query are not sufficiently factored in, and (3) scoring is not directly optimized for DST performance. Consequently, the retriever can fail to retrieve examples that would substantially improve DST performance. To address these issues, we present CombiSearch, a method that scores effective in-context examples based on their combinatorial impact on DST performance. Our evaluation on MultiWOZ shows that retrievers trained with CombiSearch surpass state-of-the-art models, achieving a 20x gain in data efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch attains a 12% absolute improvement in the upper bound DST performance over traditional approaches when no retrieval errors are assumed. This significantly increases the headroom for practical DST performance while demonstrating that existing methods rely on suboptimal data for retriever training. </p>
<blockquote>
<p>åœ¨å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰ä¸­ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ åŒ…æ‹¬ä¸€ä¸ªæ£€ç´¢å™¨ï¼Œè¯¥æ£€ç´¢å™¨é€‰æ‹©æ ‡è®°å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼Œä»¥åŠä¸€ä¸ªä½¿ç”¨è¿™äº›ç¤ºä¾‹æ¥æ¨æ–­æŸ¥è¯¢å¯¹è¯çŠ¶æ€çš„DSTæ¨¡å‹ã€‚ç°æœ‰çš„æ„å»ºæ£€ç´¢å™¨è®­ç»ƒæ•°æ®çš„æ–¹æ³•å­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™æ€§ï¼šï¼ˆ1ï¼‰æ²¡æœ‰è€ƒè™‘ç¤ºä¾‹çš„ååŒä½œç”¨ï¼Œï¼ˆ2ï¼‰æ²¡æœ‰å……åˆ†è€ƒè™‘æŸ¥è¯¢çš„è¯­è¨€ç‰¹å¾ï¼Œï¼ˆ3ï¼‰è¯„åˆ†æ²¡æœ‰ç›´æ¥é’ˆå¯¹DSTæ€§èƒ½è¿›è¡Œä¼˜åŒ–ã€‚å› æ­¤ï¼Œæ£€ç´¢å™¨å¯èƒ½ä¼šæ— æ³•æ£€ç´¢åˆ°ä¼šå¤§å¤§æé«˜DSTæ€§èƒ½çš„ç¤ºä¾‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CombiSearchæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºå…¶å¯¹DSTæ€§èƒ½çš„ç»„åˆå½±å“å¯¹æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡ç¤ºä¾‹è¿›è¡Œè¯„åˆ†ã€‚æˆ‘ä»¬åœ¨MultiWOZä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨CombiSearchè®­ç»ƒçš„æ£€ç´¢å™¨è¶…è¶Šäº†æœ€æ–°æ¨¡å‹ï¼Œå®ç°äº†20å€çš„æ•°æ®æ•ˆç‡ï¼Œå¹¶åœ¨SGDæ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå½“æ²¡æœ‰å‡è®¾æ£€ç´¢é”™è¯¯æ—¶ï¼ŒCombiSearchåœ¨ä¼ ç»Ÿæ–¹æ³•çš„åŸºç¡€ä¸Šå®ç°äº†ä¸Šé™DSTæ€§èƒ½12%çš„ç»å¯¹æå‡ã€‚è¿™æ˜¾è‘—å¢åŠ äº†å®é™…DSTæ€§èƒ½çš„æ½œåŠ›ï¼ŒåŒæ—¶è¡¨æ˜ç°æœ‰æ–¹æ³•ä¾èµ–äºæ¬¡ä¼˜æ•°æ®è¿›è¡Œæ£€ç´¢å™¨è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00622v2">PDF</a> This paper has been accepted for publication at ACL 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¯¹è¯çŠ¶æ€è¿½è¸ªï¼ˆDSTï¼‰ä¸­ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ åŒ…æ‹¬é€‰æ‹©æ ‡è®°å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡ç¤ºä¾‹çš„æ£€ç´¢å™¨ï¼Œä»¥åŠä½¿ç”¨è¿™äº›ç¤ºä¾‹æ¥æ¨æ–­æŸ¥è¯¢å¯¹è¯çŠ¶æ€çš„DSTæ¨¡å‹ã€‚ç°æœ‰æ„å»ºæ£€ç´¢å™¨è®­ç»ƒæ•°æ®çš„æ–¹æ³•å­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™æ€§ï¼šä¸€æ˜¯æ²¡æœ‰è€ƒè™‘ç¤ºä¾‹çš„ååŒä½œç”¨ï¼ŒäºŒæ˜¯æ²¡æœ‰å……åˆ†è€ƒè™‘æŸ¥è¯¢çš„è¯­è¨€ç‰¹å¾ï¼Œä¸‰æ˜¯è¯„åˆ†ä¸æ˜¯ç›´æ¥é’ˆå¯¹DSTæ€§èƒ½è¿›è¡Œä¼˜åŒ–ã€‚å› æ­¤ï¼Œæ£€ç´¢å™¨å¯èƒ½æ— æ³•æ£€ç´¢åˆ°èƒ½æ˜¾è‘—æ”¹å–„DSTæ€§èƒ½çš„ä¾‹å­ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CombiSearchæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºç¤ºä¾‹å¯¹DSTæ€§èƒ½çš„ç»„åˆå½±å“è¿›è¡Œè¯„åˆ†ã€‚åœ¨MultiWOZä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨CombiSearchè®­ç»ƒçš„æ£€ç´¢å™¨è¶…è¶Šäº†æœ€æ–°æ¨¡å‹ï¼Œå®ç°äº†20å€çš„æ•°æ®æ•ˆç‡æå‡ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°SGDæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œå‡è®¾æ²¡æœ‰æ£€ç´¢é”™è¯¯çš„æƒ…å†µä¸‹ï¼ŒCombiSearchç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•åœ¨ä¸Šé™DSTæ€§èƒ½ä¸Šå–å¾—äº†12%çš„ç»å¯¹æå‡ã€‚è¿™æ˜¾è‘—å¢åŠ äº†å®é™…DSTæ€§èƒ½çš„æ½œåŠ›ï¼Œå¹¶è¡¨æ˜ç°æœ‰æ–¹æ³•ä¾èµ–äºæ¬¡ä¼˜æ•°æ®è¿›è¡Œæ£€ç´¢å™¨è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹è¯çŠ¶æ€è¿½è¸ªï¼ˆDSTï¼‰ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ¶‰åŠæ£€ç´¢å™¨å’ŒDSTæ¨¡å‹ï¼Œå…¶ä¸­æ£€ç´¢å™¨é€‰æ‹©æ ‡è®°å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡ç¤ºä¾‹ã€‚</li>
<li>ç°æœ‰æ„å»ºæ£€ç´¢å™¨è®­ç»ƒæ•°æ®çš„æ–¹æ³•å­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™æ€§ï¼šæœªè€ƒè™‘ç¤ºä¾‹ååŒä½œç”¨ã€æœªå……åˆ†è€ƒè™‘æŸ¥è¯¢è¯­è¨€ç‰¹å¾ï¼Œä»¥åŠè¯„åˆ†æœªé’ˆå¯¹DSTæ€§èƒ½ä¼˜åŒ–ã€‚</li>
<li>CombiSearchæ–¹æ³•åŸºäºç¤ºä¾‹å¯¹DSTæ€§èƒ½çš„ç»„åˆå½±å“è¿›è¡Œè¯„åˆ†ï¼Œèƒ½æœ‰æ•ˆè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>åœ¨MultiWOZæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨CombiSearchçš„æ£€ç´¢å™¨è®­ç»ƒå–å¾—äº†æ˜¾è‘—æ•ˆæœï¼ŒåŒ…æ‹¬æé«˜æ•°æ®æ•ˆç‡å’ŒDSTæ€§èƒ½ã€‚</li>
<li>CombiSearchæ–¹æ³•åœ¨å‡è®¾æ— æ£€ç´¢é”™è¯¯çš„æƒ…å†µä¸‹ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•å®ç°äº†12%çš„ç»å¯¹æ€§èƒ½æå‡ã€‚</li>
<li>è¿™è¡¨æ˜ç°æœ‰æ–¹æ³•ä½¿ç”¨çš„æ•°æ®å¯¹äºè®­ç»ƒæ£€ç´¢å™¨å¯èƒ½æ˜¯æ¬¡ä¼˜çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73189feae486cb4d58e217ee06743ba2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3823e6f3b6caa81cbf278a7e4a9b62ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dd4f48dad18e464a7310eb3f46664bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c78006386941695ade328ba2e1c295f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-933ebe5487fde3f515605ae20a8e1003.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="3MDBench-Medical-Multimodal-Multi-agent-Dialogue-Benchmark"><a href="#3MDBench-Medical-Multimodal-Multi-agent-Dialogue-Benchmark" class="headerlink" title="3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark"></a>3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark</h2><p><strong>Authors:Ivan Sviridov, Amina Miftakhova, Artemiy Tereshchenko, Galina Zubkova, Pavel Blinov, Andrey Savchenko</strong></p>
<p>Though Large Vision-Language Models (LVLMs) are being actively explored in medicine, their ability to conduct telemedicine consultations combining accurate diagnosis with professional dialogue remains underexplored. In this paper, we present 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark), an open-source framework for simulating and evaluating LVLM-driven telemedical consultations. 3MDBench simulates patient variability through four temperament-based Patient Agents and an Assessor Agent that jointly evaluate diagnostic accuracy and dialogue quality. It includes 3013 cases across 34 diagnoses drawn from real-world telemedicine interactions, combining textual and image-based data. The experimental study compares diagnostic strategies for popular LVLMs, including GPT-4o-mini, LLaVA-3.2-11B-Vision-Instruct, and Qwen2-VL-7B-Instruct. We demonstrate that multimodal dialogue with internal reasoning improves F1 score by 6.5% over non-dialogue settings, highlighting the importance of context-aware, information-seeking questioning. Moreover, injecting predictions from a diagnostic convolutional network into the LVLMâ€™s context boosts F1 by up to 20%. Source code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/3mdbench_acl-0511">https://anonymous.4open.science/r/3mdbench_acl-0511</a>. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸæ­£å—åˆ°ç§¯æç ”ç©¶ï¼Œä½†å®ƒä»¬ç»“åˆå‡†ç¡®è¯Šæ–­å’Œä¸“ä¸šå¯¹è¯è¿›è¡Œè¿œç¨‹åŒ»ç–—å’¨è¯¢çš„èƒ½åŠ›ä»ç ”ç©¶ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†3MDBenchï¼ˆåŒ»ç–—å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“å¯¹è¯åŸºå‡†æµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ¨¡æ‹Ÿå’Œè¯„ä¼°LVLMé©±åŠ¨çš„è¿œç¨‹åŒ»ç–—å’¨è¯¢çš„å¼€æºæ¡†æ¶ã€‚3MDBenché€šè¿‡å››ç§åŸºäºæ°”è´¨çš„æ‚£è€…æ™ºèƒ½ä½“å’Œè¯„ä¼°æ™ºèƒ½ä½“æ¥æ¨¡æ‹Ÿæ‚£è€…çš„å˜å¼‚æ€§ï¼Œå…±åŒè¯„ä¼°è¯Šæ–­å‡†ç¡®æ€§å’Œå¯¹è¯è´¨é‡ã€‚å®ƒåŒ…æ‹¬æ¥è‡ªç°å®ä¸–ç•Œè¿œç¨‹åŒ»ç–—äº’åŠ¨çš„34ç§è¯Šæ–­ä¸­çš„3013ä¸ªæ¡ˆä¾‹ï¼Œæ¶µç›–æ–‡æœ¬å’ŒåŸºäºå›¾åƒçš„æ•°æ®ã€‚å®éªŒç ”ç©¶äº†æµè¡Œçš„LVLMsçš„è¯Šæ–­ç­–ç•¥ï¼ŒåŒ…æ‹¬GPT-4o-miniã€LLaVA-3.2-11B-Vision-Instructå’ŒQwen2-VL-7B-Instructã€‚æˆ‘ä»¬è¯æ˜ï¼Œä¸æ— å¯¹è¯è®¾ç½®ç›¸æ¯”ï¼Œå…·æœ‰å†…éƒ¨æ¨ç†çš„å¤šæ¨¡æ€å¯¹è¯å¯ä»¥æé«˜F1åˆ†æ•°6.5%ï¼Œè¿™çªå‡ºäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œä¿¡æ¯æœç´¢é—®é¢˜çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œå°†è¯Šæ–­å·ç§¯ç½‘ç»œçš„é¢„æµ‹æ³¨å…¥LVLMçš„è¯­å¢ƒä¸­ï¼ŒF1å¯æé«˜é«˜è¾¾20%ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/3mdbench_acl-0511%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/3mdbench_acl-0511æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13861v2">PDF</a> 35 pages, 13 figures, 7 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Large Vision-Language Modelsï¼ˆLVLMsï¼‰åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶ç€é‡æ¢è®¨äº†å®ƒä»¬åœ¨è¿œç¨‹åŒ»ç–—å’¨è¯¢ä¸­çš„è¡¨ç°ã€‚ä¸ºäº†æ¨¡æ‹Ÿå’Œè¯„ä¼°LVLMé©±åŠ¨çš„è¿œç¨‹åŒ»ç–—å’¨è¯¢ï¼Œæå‡ºäº†ä¸€ç§åä¸º3MDBenchçš„å¼€æ”¾æºä»£ç æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿä¸åŒæ€§æ ¼çš„æ‚£è€…ä»£ç†å’Œè¯„ä¼°ä»£ç†æ¥è¯„ä»·è¯Šæ–­å‡†ç¡®æ€§å’Œå¯¹è¯è´¨é‡ã€‚å®éªŒç ”ç©¶è¡¨æ˜ï¼Œå¤šæ¨¡å¼å¯¹è¯ä¸å†…éƒ¨æ¨ç†å¯ä»¥æé«˜F1åˆ†æ•°ï¼Œé€šè¿‡å°†è¯Šæ–­å·ç§¯ç½‘ç»œçš„é¢„æµ‹ç»“æœæ³¨å…¥LVLMçš„ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥æé«˜F1åˆ†æ•°è¾¾20%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Large Vision-Language Models (LVLMs) åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨æ­£åœ¨ç§¯ææ¢ç´¢ï¼Œä½†åœ¨è¿œç¨‹åŒ»ç–—å’¨è¯¢ä¸­çš„ç»¼åˆå‡†ç¡®è¯Šæ–­å’Œä¸“ä¸šå¯¹è¯èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>3MDBenchæ˜¯ä¸€ä¸ªå¼€æ”¾æºä»£ç æ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿå’Œè¯„ä¼°LVLMé©±åŠ¨çš„è¿œç¨‹åŒ»ç–—å’¨è¯¢ã€‚</li>
<li>3MDBenchåŒ…æ‹¬åŸºäºå››ç§æ€§æ ¼çš„æ‚£è€…ä»£ç†å’Œè¯„ä¼°ä»£ç†ï¼Œä»¥è¯„ä»·è¯Šæ–­å‡†ç¡®æ€§å’Œå¯¹è¯è´¨é‡ã€‚</li>
<li>å®éªŒç ”ç©¶æ¯”è¾ƒäº†æµè¡Œçš„LVLMsåœ¨åŒ»ç–—é¢†åŸŸçš„è¯Šæ–­ç­–ç•¥ã€‚</li>
<li>å¤šæ¨¡å¼å¯¹è¯ä¸å†…éƒ¨æ¨ç†å¯ä»¥æé«˜è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</li>
<li>å°†è¯Šæ–­å·ç§¯ç½‘ç»œçš„é¢„æµ‹ç»“æœæ³¨å…¥LVLMçš„ä¸Šä¸‹æ–‡å¯ä»¥è¿›ä¸€æ­¥æé«˜è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13861">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40938c90254bf0113e6b09589f24d7d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53b86ae1f32ded68f38060a56de04b70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc0cd8b9b85db9ae21c774d2247eea08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cb7d1d58ccd2c237b2e0b54803c9c78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75948df5b5df1198a82726fad55e8d85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-275e27b1894e94542bdbca2691cbc4a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6dabc8147eaa423a322265313a94492.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Akan-Cinematic-Emotions-ACE-A-Multimodal-Multi-party-Dataset-for-Emotion-Recognition-in-Movie-Dialogues"><a href="#Akan-Cinematic-Emotions-ACE-A-Multimodal-Multi-party-Dataset-for-Emotion-Recognition-in-Movie-Dialogues" class="headerlink" title="Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for   Emotion Recognition in Movie Dialogues"></a>Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for   Emotion Recognition in Movie Dialogues</h2><p><strong>Authors:David Sasu, Zehui Wu, Ziwei Gong, Run Chen, Pengyuan Shi, Lin Ai, Julia Hirschberg, Natalie Schluter</strong></p>
<p>In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the first multimodal emotion dialogue dataset for an African language, addressing the significant lack of resources for low-resource languages in emotion recognition research. ACE, developed for the Akan language, contains 385 emotion-labeled dialogues and 6,162 utterances across audio, visual, and textual modalities, along with word-level prosodic prominence annotations. The presence of prosodic labels in this dataset also makes it the first prosodically annotated African language dataset. We demonstrate the quality and utility of ACE through experiments using state-of-the-art emotion recognition methods, establishing solid baselines for future research. We hope ACE inspires further work on inclusive, linguistically and culturally diverse NLP resources. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Akanå¯¹è¯æƒ…æ„Ÿï¼ˆACEï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯é’ˆå¯¹éæ´²è¯­è¨€çš„é¦–ä¸ªå¤šæ¨¡æ€æƒ…æ„Ÿå¯¹è¯æ•°æ®é›†ï¼Œè§£å†³äº†æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶ä¸­ä½èµ„æºè¯­è¨€èµ„æºä¸¥é‡åŒ®ä¹çš„é—®é¢˜ã€‚ACEæ˜¯ä¸ºAkanè¯­è¨€å¼€å‘çš„ï¼ŒåŒ…å«385ä¸ªæƒ…æ„Ÿæ ‡ç­¾å¯¹è¯å’Œ6162ä¸ªéŸ³é¢‘ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„è¨€è®ºï¼Œä»¥åŠè¯çº§éŸµå¾‹é‡ç‚¹æ³¨é‡Šã€‚è¯¥æ•°æ®é›†ä¸­è¿˜åŒ…å«éŸµå¾‹æ ‡ç­¾ï¼Œä½¿å…¶æˆä¸ºé¦–ä¸ªç»éŸµå¾‹æ³¨é‡Šçš„éæ´²è¯­è¨€æ•°æ®é›†ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨æœ€å…ˆè¿›çš„æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•è¿›è¡Œçš„å®éªŒå±•ç¤ºäº†ACEçš„è´¨é‡å’Œå®ç”¨æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºå‡†ã€‚æˆ‘ä»¬å¸Œæœ›ACEèƒ½æ¿€å‘å¯¹åŒ…å®¹æ€§ã€è¯­è¨€å’Œæ–‡åŒ–å¤šæ ·çš„è‡ªç„¶è¯­è¨€å¤„ç†èµ„æºçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10973v3">PDF</a> Accepted to Findings at ACL 2025</p>
<p><strong>Summary</strong></p>
<p>ACEæ•°æ®é›†æ˜¯é¦–ä¸ªé’ˆå¯¹éæ´²è¯­è¨€çš„å¤šæ¨¡æ€æƒ…æ„Ÿå¯¹è¯æ•°æ®é›†ï¼Œå¼¥è¡¥äº†ä½èµ„æºè¯­è¨€æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶çš„èµ„æºåŒ®ä¹é—®é¢˜ã€‚è¯¥æ•°æ®é›†ä¸ºAkanè¯­è¨€å¼€å‘ï¼ŒåŒ…å«385ä¸ªæƒ…æ„Ÿæ ‡ç­¾å¯¹è¯å’Œ6,162ä¸ªè·¨éŸ³é¢‘ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„å‘éŸ³ï¼Œä»¥åŠå•è¯çº§åˆ«çš„éŸµå¾‹é‡ç‚¹æ³¨é‡Šã€‚ACEæ•°æ®é›†é€šè¿‡é‡‡ç”¨æœ€å‰æ²¿çš„æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•è¿›è¡Œå®éªŒï¼Œè¯æ˜äº†å…¶è´¨é‡å’Œå®ç”¨æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ACEæ•°æ®é›†æ˜¯é¦–ä¸ªé’ˆå¯¹éæ´²è¯­è¨€çš„å¤šæ¨¡æ€æƒ…æ„Ÿå¯¹è¯æ•°æ®é›†ã€‚</li>
<li>ACEæ•°æ®é›†æ—¨åœ¨å¼¥è¡¥ä½èµ„æºè¯­è¨€åœ¨æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶ä¸­çš„èµ„æºåŒ®ä¹é—®é¢˜ã€‚</li>
<li>æ•°æ®é›†åŒ…å«385ä¸ªæƒ…æ„Ÿæ ‡ç­¾å¯¹è¯å’Œ6,162ä¸ªè·¨éŸ³é¢‘ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„å‘éŸ³ã€‚</li>
<li>ACEæ•°æ®é›†æ˜¯é¦–ä¸ªåŒ…å«éŸµå¾‹æ³¨é‡Šçš„éæ´²è¯­è¨€æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨æœ€å‰æ²¿çš„æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•è¿›è¡Œå®éªŒï¼Œè¯æ˜äº†ACEæ•°æ®é›†çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚</li>
<li>ACEæ•°æ®é›†ä¸ºæœªæ¥ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6c4898de1d3580590474e1dd6b1edb1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7961c7732a0dcdcf78dc07dc363e6abb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-08ff66db25e95cb57721dce54568c22c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f0b45f7b4b31ed668f302ca39d796dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2f2a6b240ff4367e78d380e9a269ccf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ace98215964e1e900c1a7e66c6fa7c32.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2e35b10a36d4c149c69be27a29f7472c.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  TalkingMachines Real-Time Audio-Driven FaceTime-Style Video via   Autoregressive Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-be64077dbb482c82f476bdd21fb1ad4e.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich   Representation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32251.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
