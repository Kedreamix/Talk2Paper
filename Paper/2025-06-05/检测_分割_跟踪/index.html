<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Efficient Test-time Adaptive Object Detection via Sensitivity-Guided   Pruning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-86a497d6dab6790501ab71120aa120b0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    36 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-05-æ›´æ–°"><a href="#2025-06-05-æ›´æ–°" class="headerlink" title="2025-06-05 æ›´æ–°"></a>2025-06-05 æ›´æ–°</h1><h2 id="Efficient-Test-time-Adaptive-Object-Detection-via-Sensitivity-Guided-Pruning"><a href="#Efficient-Test-time-Adaptive-Object-Detection-via-Sensitivity-Guided-Pruning" class="headerlink" title="Efficient Test-time Adaptive Object Detection via Sensitivity-Guided   Pruning"></a>Efficient Test-time Adaptive Object Detection via Sensitivity-Guided   Pruning</h2><p><strong>Authors:Kunyu Wang, Xueyang Fu, Xin Lu, Chengjie Ge, Chengzhi Cao, Wei Zhai, Zheng-Jun Zha</strong></p>
<p>Continual test-time adaptive object detection (CTTA-OD) aims to online adapt a source pre-trained detector to ever-changing environments during inference under continuous domain shifts. Most existing CTTA-OD methods prioritize effectiveness while overlooking computational efficiency, which is crucial for resource-constrained scenarios. In this paper, we propose an efficient CTTA-OD method via pruning. Our motivation stems from the observation that not all learned source features are beneficial; certain domain-sensitive feature channels can adversely affect target domain performance. Inspired by this, we introduce a sensitivity-guided channel pruning strategy that quantifies each channel based on its sensitivity to domain discrepancies at both image and instance levels. We apply weighted sparsity regularization to selectively suppress and prune these sensitive channels, focusing adaptation efforts on invariant ones. Additionally, we introduce a stochastic channel reactivation mechanism to restore pruned channels, enabling recovery of potentially useful features and mitigating the risks of early pruning. Extensive experiments on three benchmarks show that our method achieves superior adaptation performance while reducing computational overhead by 12% in FLOPs compared to the recent SOTA method. </p>
<blockquote>
<p>æŒç»­æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ç›®æ ‡æ£€æµ‹ï¼ˆCTTA-ODï¼‰æ—¨åœ¨åœ¨çº¿é€‚åº”æºé¢„è®­ç»ƒæ£€æµ‹å™¨ï¼Œä»¥åº”å¯¹ä¸æ–­å˜åŒ–çš„åŸŸè½¬ç§»è¿‡ç¨‹ä¸­çš„æ¨ç†ç¯å¢ƒã€‚ç°æœ‰çš„å¤§å¤šæ•°CTTA-ODæ–¹æ³•è™½ç„¶é‡è§†æœ‰æ•ˆæ€§ï¼Œå´å¿½ç•¥äº†è®¡ç®—æ•ˆç‡ï¼Œè¿™åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸­è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ä¿®å‰ªæ¥æé«˜CTTA-ODæ•ˆç‡çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„åŠ¨æœºæºäºè¿™æ ·ä¸€ä¸ªè§‚å¯Ÿï¼šå¹¶éæ‰€æœ‰å­¦åˆ°çš„æºç‰¹å¾éƒ½æ˜¯æœ‰ç›Šçš„ï¼›æŸäº›åŸŸæ•æ„Ÿç‰¹å¾é€šé“å¯èƒ½ä¼šå¯¹ç›®æ ‡åŸŸæ€§èƒ½äº§ç”Ÿä¸åˆ©å½±å“ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ•æ„Ÿåº¦çš„é€šé“ä¿®å‰ªç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®å›¾åƒå’Œå®ä¾‹çº§åˆ«çš„åŸŸå·®å¼‚æ•æ„Ÿåº¦æ¥é‡åŒ–æ¯ä¸ªé€šé“ã€‚æˆ‘ä»¬åº”ç”¨åŠ æƒç¨€ç–æ­£åˆ™åŒ–æ¥æœ‰é€‰æ‹©åœ°æŠ‘åˆ¶å’Œä¿®å‰ªè¿™äº›æ•æ„Ÿé€šé“ï¼Œå°†è‡ªé€‚åº”åŠªåŠ›é›†ä¸­åœ¨ä¸å˜é€šé“ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§éšæœºé€šé“å†æ¿€æ´»æœºåˆ¶ï¼Œä»¥æ¢å¤ä¿®å‰ªçš„é€šé“ï¼Œä»è€Œå®ç°æ½œåœ¨æœ‰ç”¨ç‰¹å¾çš„æ¢å¤ï¼Œå¹¶é™ä½æ—©æœŸä¿®å‰ªçš„é£é™©ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¾¾åˆ°ä¼˜è¶Šçš„é€‚åº”æ€§èƒ½çš„åŒæ—¶ï¼Œä¸æœ€è¿‘çš„æœ€ä¼˜æ–¹æ³•ç›¸æ¯”ï¼Œæµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPsï¼‰å‡å°‘äº†12%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02462v1">PDF</a> Accepted as CVPR 2025 oral paper</p>
<p><strong>Summary</strong></p>
<p>åœ¨çº¿è‡ªé€‚åº”å¯¹è±¡æ£€æµ‹ï¼ˆCTTA-ODï¼‰æ—¨åœ¨åœ¨æ–°ç¯å¢ƒæŒç»­å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œå¯¹æºé¢„è®­ç»ƒæ£€æµ‹å™¨è¿›è¡Œåœ¨çº¿é€‚åº”ã€‚ç°æœ‰CTTA-ODæ–¹æ³•è¿‡äºæ³¨é‡æ•ˆæœè€Œå¿½è§†è®¡ç®—æ•ˆç‡ï¼Œè¿™å¯¹äºèµ„æºå—é™çš„åœºæ™¯è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå‰ªæçš„é«˜æ•ˆCTTA-ODæ–¹æ³•ã€‚æˆ‘ä»¬çš„åŠ¨æœºæºäºè§‚å¯Ÿåˆ°çš„å¹¶éæ‰€æœ‰å­¦ä¹ åˆ°çš„æºç‰¹å¾éƒ½æœ‰ç›Šï¼ŒæŸäº›å¯¹é¢†åŸŸæ•æ„Ÿçš„ç‰¹å¾é€šé“å¯èƒ½ä¼šå¯¹ç›®æ ‡åŸŸæ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ•æ„Ÿæ€§å¼•å¯¼çš„æ¸ é“å‰ªæç­–ç•¥ï¼Œæ ¹æ®å›¾åƒå’Œå®ä¾‹çº§åˆ«çš„é¢†åŸŸå·®å¼‚æ•æ„Ÿæ€§æ¥é‡åŒ–æ¯ä¸ªé€šé“ã€‚æˆ‘ä»¬åº”ç”¨åŠ æƒç¨€ç–æ­£åˆ™åŒ–æ¥æœ‰é€‰æ‹©åœ°æŠ‘åˆ¶å’Œå‰ªé™¤è¿™äº›æ•æ„Ÿé€šé“ï¼Œå°†é€‚åº”åŠªåŠ›é›†ä¸­åœ¨ä¸å˜é€šé“ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§éšæœºé€šé“é‡æ–°æ¿€æ´»æœºåˆ¶ï¼Œä»¥æ¢å¤è¢«å‰ªæçš„é€šé“ï¼Œä»è€Œæ¢å¤æ½œåœ¨çš„æœ‰ç”¨ç‰¹å¾å¹¶é™ä½æ—©æœŸå‰ªæçš„é£é™©ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šéƒ½å®ç°äº†å‡ºè‰²çš„é€‚åº”æ€§èƒ½ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œè®¡ç®—é‡å‡å°‘äº†12%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CTTA-ODæ—¨åœ¨åœ¨çº¿é€‚åº”æºé¢„è®­ç»ƒæ£€æµ‹å™¨ä»¥åº”å¯¹ä¸æ–­å˜åŒ–çš„ç¯å¢ƒã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¿‡äºæ³¨é‡æ•ˆæœè€Œå¿½è§†è®¡ç®—æ•ˆç‡ï¼Œè¿™åœ¨èµ„æºå—é™åœºæ™¯ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ•æ„Ÿæ€§å¼•å¯¼çš„æ¸ é“å‰ªæç­–ç•¥ï¼Œé€šè¿‡é‡åŒ–æ¯ä¸ªé€šé“çš„æ•æ„Ÿæ€§æ¥é€‰æ‹©æ€§åœ°å‰ªé™¤å¯¹é¢†åŸŸæ•æ„Ÿçš„ç‰¹å¾é€šé“ã€‚</li>
<li>åº”ç”¨åŠ æƒç¨€ç–æ­£åˆ™åŒ–ä»¥æŠ‘åˆ¶å¹¶å‰ªé™¤æ•æ„Ÿé€šé“ï¼ŒåŒæ—¶é›†ä¸­é€‚åº”åŠªåŠ›åœ¨ä¸å˜é€šé“ä¸Šã€‚</li>
<li>å¼•å…¥éšæœºé€šé“é‡æ–°æ¿€æ´»æœºåˆ¶æ¥æ¢å¤æ½œåœ¨çš„æœ‰ç”¨ç‰¹å¾å¹¶é™ä½æ—©æœŸå‰ªæçš„é£é™©ã€‚</li>
<li>åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å‡ºè‰²çš„é€‚åº”æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a7e548888d0eb6845528ccf7870c71b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e2671dcc84ed52be6282c339e24782d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe6c8b35e521491bf44c3ea0df6edffb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d6192ec5f435bae64e69f5dc15b7ee81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca826078a3a28061054e20f5ac6fc4b7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="OD3-Optimization-free-Dataset-Distillation-for-Object-Detection"><a href="#OD3-Optimization-free-Dataset-Distillation-for-Object-Detection" class="headerlink" title="OD3: Optimization-free Dataset Distillation for Object Detection"></a>OD3: Optimization-free Dataset Distillation for Object Detection</h2><p><strong>Authors:Salwa K. Al Khatib, Ahmed ElHagry, Shitong Shao, Zhiqiang Shen</strong></p>
<p>Training large neural networks on large-scale datasets requires substantial computational resources, particularly for dense prediction tasks such as object detection. Although dataset distillation (DD) has been proposed to alleviate these demands by synthesizing compact datasets from larger ones, most existing work focuses solely on image classification, leaving the more complex detection setting largely unexplored. In this paper, we introduce OD3, a novel optimization-free data distillation framework specifically designed for object detection. Our approach involves two stages: first, a candidate selection process in which object instances are iteratively placed in synthesized images based on their suitable locations, and second, a candidate screening process using a pre-trained observer model to remove low-confidence objects. We perform our data synthesis framework on MS COCO and PASCAL VOC, two popular detection datasets, with compression ratios ranging from 0.25% to 5%. Compared to the prior solely existing dataset distillation method on detection and conventional core set selection methods, OD3 delivers superior accuracy, establishes new state-of-the-art results, surpassing prior best method by more than 14% on COCO mAP50 at a compression ratio of 1.0%. Code and condensed datasets are available at: <a target="_blank" rel="noopener" href="https://github.com/VILA-Lab/OD3">https://github.com/VILA-Lab/OD3</a>. </p>
<blockquote>
<p>è®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è±¡æ£€æµ‹ç­‰å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­ã€‚è™½ç„¶æ•°æ®é›†è’¸é¦ï¼ˆDDï¼‰å·²è¢«æå‡ºé€šè¿‡ä»è¾ƒå¤§çš„æ•°æ®é›†ä¸­åˆæˆç´§å‡‘æ•°æ®é›†æ¥ç¼“è§£è¿™äº›éœ€æ±‚ï¼Œä½†ç°æœ‰çš„å¤§éƒ¨åˆ†å·¥ä½œä»…ä¸“æ³¨äºå›¾åƒåˆ†ç±»ï¼Œè€Œæ›´å¤æ‚çš„ç›®æ ‡æ£€æµ‹è®¾ç½®åˆ™å¾ˆå°‘è¢«æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OD3ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€ä¼˜åŒ–çš„æ•°æ®è’¸é¦æ¡†æ¶ï¼Œä¸“ä¸ºå¯¹è±¡æ£€æµ‹è®¾è®¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å€™é€‰é€‰æ‹©è¿‡ç¨‹ï¼Œå…¶ä¸­å¯¹è±¡å®ä¾‹ä¼šåŸºäºåˆé€‚çš„ä½ç½®è¿­ä»£åœ°æ”¾ç½®åœ¨åˆæˆå›¾åƒä¸­ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„è§‚å¯Ÿè€…æ¨¡å‹è¿›è¡Œå€™é€‰ç­›é€‰è¿‡ç¨‹ï¼Œä»¥å»é™¤ä½ç½®ä¿¡åº¦çš„å¯¹è±¡ã€‚æˆ‘ä»¬åœ¨MS COCOå’ŒPASCAL VOCè¿™ä¸¤ä¸ªæµè¡Œçš„æ£€æµ‹æ•°æ®é›†ä¸Šæ‰§è¡Œäº†æ•°æ®åˆæˆæ¡†æ¶ï¼Œå‹ç¼©ç‡èŒƒå›´ä¸º0.25%~5%ã€‚ä¸ç°æœ‰çš„å…¶ä»–æ£€æµ‹æ–¹æ³•ä»¥åŠä¼ ç»Ÿçš„æ ¸å¿ƒé›†é€‰æ‹©æ–¹æ³•ç›¸æ¯”ï¼ŒOD3å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ£€æµ‹ç»“æœï¼Œåœ¨å‹ç¼©ç‡ä¸º1.0%çš„COCO mAP50ä¸Šè¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•è¶…è¿‡14%ã€‚ä»£ç å’Œæµ“ç¼©æ•°æ®é›†å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/VILA-Lab/OD3%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/VILA-Lab/OD3æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01942v1">PDF</a> Equal Contribution of the first three authors</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ç›®æ ‡æ£€æµ‹ä»»åŠ¡çš„æ— ä¼˜åŒ–æ•°æ®è’¸é¦æ¡†æ¶OD3ï¼Œæ—¨åœ¨ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­åˆæˆç´§å‡‘æ•°æ®é›†ï¼Œé™ä½è®¡ç®—èµ„æºéœ€æ±‚ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šå€™é€‰é€‰æ‹©é˜¶æ®µå’Œå€™é€‰ç­›é€‰é˜¶æ®µã€‚é€šè¿‡è¿­ä»£æ”¾ç½®ç›®æ ‡å®ä¾‹å’ŒåŸºäºé¢„è®­ç»ƒè§‚å¯Ÿè€…æ¨¡å‹å»é™¤ä½ç½®ä¿¡åº¦ç›®æ ‡ï¼Œå®ç°åœ¨MS COCOå’ŒPASCAL VOCæ•°æ®é›†ä¸Šçš„æ•°æ®åˆæˆã€‚OD3åœ¨å‹ç¼©æ¯”ä¸º0.25%è‡³5%çš„æƒ…å†µä¸‹ï¼Œç›¸è¾ƒäºç°æœ‰çš„æ£€æµ‹æ•°æ®é›†è’¸é¦æ–¹æ³•å’Œå¸¸è§„æ ¸å¿ƒé›†é€‰æ‹©æ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œè¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨COCO mAP50çš„å‹ç¼©æ¯”ä¸º1.0%æ—¶ï¼Œè¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•è¶…è¿‡14%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OD3æ˜¯ä¸€ç§é’ˆå¯¹ç›®æ ‡æ£€æµ‹ä»»åŠ¡çš„æ•°æ®è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­åˆæˆç´§å‡‘æ•°æ®é›†ä»¥é™ä½è®¡ç®—èµ„æºéœ€æ±‚ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šé€šè¿‡è¿­ä»£æ”¾ç½®ç›®æ ‡å®ä¾‹è¿›è¡Œå€™é€‰é€‰æ‹©ï¼Œä»¥åŠåŸºäºé¢„è®­ç»ƒè§‚å¯Ÿè€…æ¨¡å‹å»é™¤ä½ç½®ä¿¡åº¦ç›®æ ‡è¿›è¡Œå€™é€‰ç­›é€‰ã€‚</li>
<li>OD3åœ¨MS COCOå’ŒPASCAL VOCæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå®ç°äº†æ•°æ®åˆæˆã€‚</li>
<li>åœ¨å‹ç¼©æ¯”ä¸º1.0%æ—¶ï¼ŒOD3åœ¨COCO mAP50ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•è¶…è¿‡14%ï¼Œå±•ç°äº†å…¶ä¼˜è¶Šæ€§ã€‚</li>
<li>OD3ç›¸è¾ƒäºç°æœ‰çš„æ£€æµ‹æ•°æ®é›†è’¸é¦æ–¹æ³•å’Œå¸¸è§„æ ¸å¿ƒé›†é€‰æ‹©æ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>OD3çš„ä»£ç å’Œåˆæˆæ•°æ®é›†å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/VILA-Lab/OD3%E3%80%82">https://github.com/VILA-Lab/OD3ã€‚</a></li>
<li>OD3æ¡†æ¶ä¸ºé™ä½å¤§è§„æ¨¡ç¥ç»ç½‘ç»œåœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸Šçš„è®¡ç®—èµ„æºéœ€æ±‚æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f2847bfef2dc5b0e33be32b0e554856f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dea7dbb8f4a60c87f036f36cd6f8230f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-624f267626584fda3e9444cc36382b4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa0c77e799084472c9f0ccc6dc8f4858.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Seg2Any-Open-set-Segmentation-Mask-to-Image-Generation-with-Precise-Shape-and-Semantic-Control"><a href="#Seg2Any-Open-set-Segmentation-Mask-to-Image-Generation-with-Precise-Shape-and-Semantic-Control" class="headerlink" title="Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise   Shape and Semantic Control"></a>Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise   Shape and Semantic Control</h2><p><strong>Authors:Danfeng li, Hui Zhang, Sheng Wang, Jiacheng Li, Zuxuan Wu</strong></p>
<p>Despite recent advances in diffusion models, top-tier text-to-image (T2I) models still struggle to achieve precise spatial layout control, i.e. accurately generating entities with specified attributes and locations. Segmentation-mask-to-image (S2I) generation has emerged as a promising solution by incorporating pixel-level spatial guidance and regional text prompts. However, existing S2I methods fail to simultaneously ensure semantic consistency and shape consistency. To address these challenges, we propose Seg2Any, a novel S2I framework built upon advanced multimodal diffusion transformers (e.g. FLUX). First, to achieve both semantic and shape consistency, we decouple segmentation mask conditions into regional semantic and high-frequency shape components. The regional semantic condition is introduced by a Semantic Alignment Attention Mask, ensuring that generated entities adhere to their assigned text prompts. The high-frequency shape condition, representing entity boundaries, is encoded as an Entity Contour Map and then introduced as an additional modality via multi-modal attention to guide image spatial structure. Second, to prevent attribute leakage across entities in multi-entity scenarios, we introduce an Attribute Isolation Attention Mask mechanism, which constrains each entityâ€™s image tokens to attend exclusively to themselves during image self-attention. To support open-set S2I generation, we construct SACap-1M, a large-scale dataset containing 1 million images with 5.9 million segmented entities and detailed regional captions, along with a SACap-Eval benchmark for comprehensive S2I evaluation. Extensive experiments demonstrate that Seg2Any achieves state-of-the-art performance on both open-set and closed-set S2I benchmarks, particularly in fine-grained spatial and attribute control of entities. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œé¡¶çº§æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ä»ç„¶éš¾ä»¥å®ç°ç²¾ç¡®çš„ç©ºé—´å¸ƒå±€æ§åˆ¶ï¼Œå³å‡†ç¡®ç”Ÿæˆå…·æœ‰æŒ‡å®šå±æ€§å’Œä½ç½®çš„å®ä½“ã€‚åˆ†å‰²æ©è†œåˆ°å›¾åƒï¼ˆS2Iï¼‰ç”Ÿæˆé€šè¿‡ç»“åˆåƒç´ çº§ç©ºé—´æŒ‡å¯¼å’ŒåŒºåŸŸæ–‡æœ¬æç¤ºï¼Œå·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„S2Iæ–¹æ³•æ— æ³•åŒæ—¶ç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§å’Œå½¢çŠ¶ä¸€è‡´æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Seg2Anyï¼Œä¸€ä¸ªåŸºäºå…ˆè¿›çš„å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆä¾‹å¦‚FLUXï¼‰çš„æ–°å‹S2Iæ¡†æ¶ã€‚é¦–å…ˆï¼Œä¸ºäº†å®ç°è¯­ä¹‰å’Œå½¢çŠ¶çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å°†åˆ†å‰²æ©è†œæ¡ä»¶åˆ†è§£ä¸ºåŒºåŸŸè¯­ä¹‰å’Œé«˜é¢‘å½¢çŠ¶ç»„ä»¶ã€‚é€šè¿‡è¯­ä¹‰å¯¹é½æ³¨æ„åŠ›æ©è†œå¼•å…¥åŒºåŸŸè¯­ä¹‰æ¡ä»¶ï¼Œç¡®ä¿ç”Ÿæˆçš„å®ä½“ç¬¦åˆå…¶åˆ†é…åˆ°çš„æ–‡æœ¬æç¤ºã€‚é«˜é¢‘å½¢çŠ¶æ¡ä»¶ä»£è¡¨å®ä½“è¾¹ç•Œï¼Œè¢«ç¼–ç ä¸ºå®ä½“è½®å»“å›¾ï¼Œç„¶åä½œä¸ºå¤šæ¨¡æ€æ³¨æ„åŠ›çš„ä¸€ç§é¢å¤–æ¨¡æ€å¼•å…¥ï¼Œä»¥å¼•å¯¼å›¾åƒçš„ç©ºé—´ç»“æ„ã€‚å…¶æ¬¡ï¼Œä¸ºäº†é˜²æ­¢å¤šå®ä½“åœºæ™¯ä¸­å±æ€§åœ¨å®ä½“ä¹‹é—´çš„æ³„éœ²ï¼Œæˆ‘ä»¬å¼•å…¥äº†å±æ€§éš”ç¦»æ³¨æ„åŠ›æ©è†œæœºåˆ¶ï¼Œè¯¥æœºåˆ¶çº¦æŸæ¯ä¸ªå®ä½“çš„å›¾åƒä»¤ç‰Œåœ¨å›¾åƒè‡ªæ³¨æ„åŠ›æœŸé—´åªä¸“æ³¨äºè‡ªèº«ã€‚ä¸ºäº†æ”¯æŒå¼€æ”¾é›†S2Iç”Ÿæˆï¼Œæˆ‘ä»¬æ„å»ºäº†SACap-1Mï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«100ä¸‡å¼ å›¾åƒã€590ä¸‡ä¸ªåˆ†å‰²å®ä½“å’Œè¯¦ç»†åŒºåŸŸæè¿°çš„å¤§å‹æ•°æ®é›†ï¼Œä»¥åŠç”¨äºå…¨é¢S2Iè¯„ä»·çš„SACap-EvalåŸºå‡†æµ‹è¯•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSeg2Anyåœ¨å¼€æ”¾é›†å’Œå°é—­é›†S2IåŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ä½“çš„ç²¾ç»†ç©ºé—´å’Œæ§åˆ¶å±æ€§æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00596v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å‰²æ©è†œåˆ°å›¾åƒï¼ˆS2Iï¼‰æ¡†æ¶Seg2Anyï¼Œç”¨äºè§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ç²¾ç¡®ç©ºé—´å¸ƒå±€æ§åˆ¶çš„é—®é¢˜ã€‚Seg2Anyåˆ©ç”¨å…ˆè¿›çš„è·¨æ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆå¦‚FLUXï¼‰ï¼Œé€šè¿‡åˆ†ç¦»è¯­ä¹‰å’Œå½¢çŠ¶ä¸€è‡´æ€§æ¡ä»¶ï¼Œå®ç°æ›´ç²¾ç»†çš„å®ä½“è¾¹ç•Œå’Œæ–‡æœ¬æç¤ºå¯¹é½ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¼•å…¥äº†ä¸€ç§å±æ€§éš”ç¦»æ³¨æ„åŠ›æ©è†œæœºåˆ¶ï¼Œä»¥é˜²æ­¢å¤šå®ä½“åœºæ™¯ä¸­å±æ€§æ³„éœ²ã€‚å®éªŒè¡¨æ˜ï¼ŒSeg2Anyåœ¨å¼€æ”¾å’Œå°é—­é›†åˆçš„S2IåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Seg2Anyæ˜¯ä¸€ä¸ªæ–°çš„åˆ†å‰²æ©è†œåˆ°å›¾åƒï¼ˆS2Iï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ç²¾ç¡®ç©ºé—´å¸ƒå±€æ§åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>Seg2Anyåˆ©ç”¨å…ˆè¿›çš„è·¨æ¨¡æ€æ‰©æ•£è½¬æ¢å™¨ï¼ˆå¦‚FLUXï¼‰ï¼Œé€šè¿‡åŒºåŸŸè¯­ä¹‰æ¡ä»¶å’Œé«˜é¢‘å½¢çŠ¶æ¡ä»¶çš„åˆ†ç¦»ï¼Œå®ç°äº†è¯­ä¹‰å’Œå½¢çŠ¶çš„ä¸€è‡´æ€§ã€‚</li>
<li>æ¡†æ¶ä¸­å¼•å…¥äº†è¯­ä¹‰å¯¹é½æ³¨æ„åŠ›æ©è†œå’Œå®ä½“è½®å»“å›¾ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å®ä½“ç¬¦åˆæ–‡æœ¬æç¤ºå¹¶ä¿æŒå½¢çŠ¶ä¸€è‡´æ€§ã€‚</li>
<li>ä¸ºäº†é˜²æ­¢å¤šå®ä½“åœºæ™¯ä¸­å±æ€§æ³„éœ²ï¼ŒSeg2Anyå¼•å…¥äº†å±æ€§éš”ç¦»æ³¨æ„åŠ›æ©è†œæœºåˆ¶ã€‚</li>
<li>Seg2Anyæ”¯æŒå¼€æ”¾é›†S2Iç”Ÿæˆï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†SACap-1Mï¼ŒåŒ…å«100ä¸‡å¼ å›¾åƒå’Œè¯¦ç»†çš„åŒºåŸŸå­—å¹•ã€‚</li>
<li>SACap-EvalåŸºå‡†æµ‹è¯•ç”¨äºå…¨é¢è¯„ä¼°S2Iæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a9c4b5600510f5971870852b61b111c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7717ba103a924244aecb250565904867.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecf2619380b76c2f311c3b2be4604b15.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="iDPA-Instance-Decoupled-Prompt-Attention-for-Incremental-Medical-Object-Detection"><a href="#iDPA-Instance-Decoupled-Prompt-Attention-for-Incremental-Medical-Object-Detection" class="headerlink" title="iDPA: Instance Decoupled Prompt Attention for Incremental Medical Object   Detection"></a>iDPA: Instance Decoupled Prompt Attention for Incremental Medical Object   Detection</h2><p><strong>Authors:Huahui Yi, Wei Xu, Ziyuan Qin, Xi Chen, Xiaohu Wu, Kang Li, Qicheng Lao</strong></p>
<p>Existing prompt-based approaches have demonstrated impressive performance in continual learning, leveraging pre-trained large-scale models for classification tasks; however, the tight coupling between foreground-background information and the coupled attention between prompts and image-text tokens present significant challenges in incremental medical object detection tasks, due to the conceptual gap between medical and natural domains. To overcome these challenges, we introduce the \method<del>framework, which comprises two main components: 1) Instance-level Prompt Generation (\ipg), which decouples fine-grained instance-level knowledge from images and generates prompts that focus on dense predictions, and 2) Decoupled Prompt Attention (\dpa), which decouples the original prompt attention, enabling a more direct and efficient transfer of prompt information while reducing memory usage and mitigating catastrophic forgetting. We collect 13 clinical, cross-modal, multi-organ, and multi-category datasets, referred to as \dataset, and experiments demonstrate that \method</del>outperforms existing SOTA methods, with FAP improvements of 5.44%, 4.83%, 12.88%, and 4.59% in full data, 1-shot, 10-shot, and 50-shot settings, respectively. </p>
<blockquote>
<p>ç°æœ‰åŸºäºæç¤ºçš„æ–¹æ³•åœ¨æŒç»­å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§è§„æ¨¡æ¨¡å‹è¿›è¡Œåˆ†ç±»ä»»åŠ¡ï¼›ç„¶è€Œï¼Œåœ¨å¢é‡åŒ»å­¦ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œå‰æ™¯ä¸èƒŒæ™¯ä¿¡æ¯ä¹‹é—´çš„ç´§å¯†è€¦åˆä»¥åŠæç¤ºä¸å›¾åƒæ–‡æœ¬æ ‡è®°ä¹‹é—´çš„è€¦åˆæ³¨æ„åŠ›ï¼Œç”±äºåŒ»å­¦å’Œè‡ªç„¶é¢†åŸŸä¹‹é—´çš„æ¦‚å¿µå·®è·ï¼Œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†\method<del>æ¡†æ¶ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼š1ï¼‰å®ä¾‹çº§æç¤ºç”Ÿæˆï¼ˆ\ipgï¼‰ï¼Œå®ƒä»å›¾åƒä¸­è§£è€¦å‡ºç²¾ç»†çš„å®ä¾‹çº§çŸ¥è¯†ï¼Œå¹¶ç”Ÿæˆä¸“æ³¨äºå¯†é›†é¢„æµ‹çš„æç¤ºï¼›2ï¼‰è§£è€¦æç¤ºæ³¨æ„åŠ›ï¼ˆ\dpaï¼‰ï¼Œå®ƒè§£è€¦äº†åŸå§‹çš„æç¤ºæ³¨æ„åŠ›ï¼Œä½¿æç¤ºä¿¡æ¯çš„ä¼ é€’æ›´åŠ ç›´æ¥å’Œé«˜æ•ˆï¼ŒåŒæ—¶å‡å°‘äº†å†…å­˜ä½¿ç”¨å¹¶å‡è½»äº†ç¾éš¾æ€§é—å¿˜ã€‚æˆ‘ä»¬æ”¶é›†äº†13ä¸ªä¸´åºŠã€è·¨æ¨¡æ€ã€å¤šå™¨å®˜å’Œå¤šç±»åˆ«çš„æ•°æ®é›†ï¼Œç§°ä¸º\datasetï¼Œå®éªŒè¡¨æ˜\method</del>åœ¨å…¨é¢æ•°æ®ã€å•æ¬¡å­¦ä¹ ã€åæ¬¡å­¦ä¹ å’Œäº”åæ¬¡å­¦ä¹ çš„è®¾å®šä¸‹ï¼Œè¾ƒç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æœ‰äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒFAPåˆ†åˆ«æé«˜äº†5.44%ã€4.83%ã€12.88%å’Œ4.59%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00406v1">PDF</a> accepted to ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è¿ç»­å­¦ä¹ ä¸­çš„åŒ»å­¦å¯¹è±¡æ£€æµ‹ä»»åŠ¡é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†æ–°çš„æ¡†æ¶\method~ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼šInstance-level Prompt Generationï¼ˆ\ipgï¼‰å’ŒDecoupled Prompt Attentionï¼ˆ\dpaï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæé«˜äº†FAPæŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é¢„è®­ç»ƒçš„å¤§è§„æ¨¡æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„è¿ç»­å­¦ä¹ æ•ˆæœä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†åœ¨åŒ»å­¦å¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>\method~æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„ä»¶ï¼š\ipgå’Œ\dpaï¼Œåˆ†åˆ«ç”¨äºç”Ÿæˆå®ä¾‹çº§æç¤ºå’Œè§£è€¦æç¤ºæ³¨æ„åŠ›ã€‚</li>
<li>\method~æ¡†æ¶é€šè¿‡å‡å°‘å†…å­˜ä½¿ç”¨å’Œç¼“è§£ç¾éš¾æ€§é—å¿˜ï¼Œä½¿æç¤ºä¿¡æ¯æ›´åŠ ç›´æ¥å’Œé«˜æ•ˆã€‚</li>
<li>æ”¶é›†çš„\datasetåŒ…å«å¤šç§ä¸´åºŠã€è·¨æ¨¡æ€ã€å¤šå™¨å®˜å’Œå¤šç±»åˆ«çš„æ•°æ®é›†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œ\method~æ¡†æ¶åœ¨å…¨æ•°æ®ã€1-shotã€10-shotå’Œ50-shotè®¾ç½®ä¸‹ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cb0b6bc9e50b5b948ee244857998b4e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47efb4804bec33875de05b5995402f7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-072a4cef9c543cfc5eec9c6fbead83ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d6d2aee4fe567b9c82408b258f34240.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Test-time-Vocabulary-Adaptation-for-Language-driven-Object-Detection"><a href="#Test-time-Vocabulary-Adaptation-for-Language-driven-Object-Detection" class="headerlink" title="Test-time Vocabulary Adaptation for Language-driven Object Detection"></a>Test-time Vocabulary Adaptation for Language-driven Object Detection</h2><p><strong>Authors:Mingxuan Liu, Tyler L. Hayes, Massimiliano Mancini, Elisa Ricci, Riccardo Volpi, Gabriela Csurka</strong></p>
<p>Open-vocabulary object detection models allow users to freely specify a class vocabulary in natural language at test time, guiding the detection of desired objects. However, vocabularies can be overly broad or even mis-specified, hampering the overall performance of the detector. In this work, we propose a plug-and-play Vocabulary Adapter (VocAda) to refine the user-defined vocabulary, automatically tailoring it to categories that are relevant for a given image. VocAda does not require any training, it operates at inference time in three steps: i) it uses an image captionner to describe visible objects, ii) it parses nouns from those captions, and iii) it selects relevant classes from the user-defined vocabulary, discarding irrelevant ones. Experiments on COCO and Objects365 with three state-of-the-art detectors show that VocAda consistently improves performance, proving its versatility. The code is open source. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹æ¨¡å‹å…è®¸ç”¨æˆ·åœ¨æµ‹è¯•æ—¶è‡ªç”±æŒ‡å®šè‡ªç„¶è¯­è¨€ä¸­çš„ç±»åˆ«è¯æ±‡è¡¨ï¼Œä»è€Œå¼•å¯¼æ‰€éœ€å¯¹è±¡çš„æ£€æµ‹ã€‚ç„¶è€Œï¼Œè¯æ±‡è¡¨å¯èƒ½è¿‡äºå®½æ³›ç”šè‡³æŒ‡å®šé”™è¯¯ï¼Œä»è€Œå½±å“æ£€æµ‹å™¨çš„æ•´ä½“æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„è¯æ±‡è¡¨é€‚é…å™¨ï¼ˆVocAdaï¼‰æ¥ä¼˜åŒ–ç”¨æˆ·å®šä¹‰çš„è¯æ±‡è¡¨ï¼Œè‡ªåŠ¨å°†å…¶è°ƒæ•´ä¸ºä¸ç»™å®šå›¾åƒç›¸å…³çš„ç±»åˆ«ã€‚VocAdaæ— éœ€ä»»ä½•è®­ç»ƒï¼Œå®ƒå¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆ†ä¸‰æ­¥æ“ä½œï¼šiï¼‰ä½¿ç”¨å›¾åƒå­—å¹•æè¿°å¯è§å¯¹è±¡ï¼Œiiï¼‰è§£æè¿™äº›å­—å¹•ä¸­çš„åè¯ï¼Œiiiï¼‰ä»ç”¨æˆ·å®šä¹‰çš„è¯æ±‡è¡¨ä¸­é€‰æ‹©ç›¸å…³ç±»åˆ«ï¼Œä¸¢å¼ƒä¸ç›¸å…³çš„ç±»åˆ«ã€‚åœ¨COCOå’ŒObjects365ä¸Šä¸ä¸‰ç§æœ€å…ˆè¿›æ£€æµ‹å™¨çš„å®éªŒè¡¨æ˜ï¼ŒVocAdaçš„æ€§èƒ½å§‹ç»ˆå¾—åˆ°æå‡ï¼Œè¯æ˜äº†å…¶é€šç”¨æ€§ã€‚ä»£ç å·²å¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00333v1">PDF</a> Accepted as a conference paper at ICIP 2025</p>
<p><strong>Summary</strong><br>     æœ¬å·¥ä½œæå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„è¯æ±‡é€‚é…å™¨ï¼ˆVocAdaï¼‰ï¼Œç”¨äºç²¾ç‚¼ç”¨æˆ·å®šä¹‰çš„è¯æ±‡ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸ç»™å®šå›¾åƒç›¸å…³çš„ç±»åˆ«ã€‚VocAdaæ— éœ€ä»»ä½•è®­ç»ƒï¼Œå¯åœ¨æ¨ç†é˜¶æ®µè¿è¡Œï¼Œåˆ†ä¸ºä¸‰æ­¥ï¼šä¸€ã€ä½¿ç”¨å›¾åƒæè¿°å™¨æè¿°å¯è§ç‰©ä½“ï¼›äºŒã€è§£æè¿™äº›æè¿°ä¸­çš„åè¯ï¼›ä¸‰ã€ä»ç”¨æˆ·å®šä¹‰çš„è¯æ±‡ä¸­é€‰æ‹©ç›¸å…³ç±»åˆ«ï¼Œèˆå¼ƒä¸ç›¸å…³çš„ç±»åˆ«ã€‚åœ¨COCOå’ŒObjects365ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVocAdaå¯æŒç»­æå‡æ€§èƒ½ï¼Œè¯æ˜å…¶é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹æ¨¡å‹å…è®¸åœ¨æµ‹è¯•æ—¶ä»¥è‡ªç„¶è¯­è¨€è‡ªç”±æŒ‡å®šç±»åˆ«è¯æ±‡ï¼ŒæŒ‡å¯¼æ‰€éœ€å¯¹è±¡çš„æ£€æµ‹ã€‚</li>
<li>è¯æ±‡å¯èƒ½è¿‡äºå¹¿æ³›æˆ–è¯¯æŒ‡å®šï¼Œå½±å“æ£€æµ‹å™¨çš„æ•´ä½“æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„Vocabulary Adapterï¼ˆVocAdaï¼‰æ—¨åœ¨ç²¾ç‚¼ç”¨æˆ·å®šä¹‰çš„è¯æ±‡ã€‚</li>
<li>VocAdaåœ¨æ¨ç†é˜¶æ®µè¿è¡Œï¼Œæ— éœ€ä»»ä½•è®­ç»ƒã€‚</li>
<li>VocAdaé€šè¿‡ä¸‰ä¸ªæ­¥éª¤è‡ªåŠ¨è°ƒæ•´ä¸ç»™å®šå›¾åƒç›¸å…³çš„ç±»åˆ«ï¼šä½¿ç”¨å›¾åƒæè¿°å™¨æè¿°ç‰©ä½“ï¼Œè§£ææè¿°ä¸­çš„åè¯ï¼Œå¹¶ä»ç”¨æˆ·å®šä¹‰çš„è¯æ±‡ä¸­é€‰æ‹©ç›¸å…³ç±»åˆ«ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒVocAdaå¯æå‡æ£€æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64c37a32ba7d6e38ebe06a8388d3c170.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9a2d633f393b11f67a9dcdb7cbd9843.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2740ebc789e2146c85ef0b01d3865d11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3ce79cee639d42063d2ba6a5efa2585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2b37a6fa0f508927b89f2e2495d3210.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FastCAR-Fast-Classification-And-Regression-for-Task-Consolidation-in-Multi-Task-Learning-to-Model-a-Continuous-Property-Variable-of-Detected-Object-Class"><a href="#FastCAR-Fast-Classification-And-Regression-for-Task-Consolidation-in-Multi-Task-Learning-to-Model-a-Continuous-Property-Variable-of-Detected-Object-Class" class="headerlink" title="FastCAR: Fast Classification And Regression for Task Consolidation in   Multi-Task Learning to Model a Continuous Property Variable of Detected   Object Class"></a>FastCAR: Fast Classification And Regression for Task Consolidation in   Multi-Task Learning to Model a Continuous Property Variable of Detected   Object Class</h2><p><strong>Authors:Anoop Kini, Andreas Jansche, Timo Bernthaler, Gerhard Schneider</strong></p>
<p>FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL) for a classification and a regression task, despite the non-triviality of task heterogeneity with only a subtle correlation. The approach addresses the classification of a detected object (occupying the entire image frame) and regression for modeling a continuous property variable (for instances of an object class), a crucial use case in science and engineering. FastCAR involves a label transformation approach that is amenable for use with only a single-task regression network architecture. FastCAR outperforms traditional MTL model families, parametrized in the landscape of architecture and loss weighting schemes, when learning both tasks are collectively considered (classification accuracy of 99.54%, regression mean absolute percentage error of 2.4%). The experiments performed used â€œAdvanced Steel Property Datasetâ€ contributed by us <a target="_blank" rel="noopener" href="https://github.com/fastcandr/AdvancedSteel-Property-Dataset">https://github.com/fastcandr/AdvancedSteel-Property-Dataset</a>. The dataset comprises 4536 images of 224x224 pixels, annotated with discrete object classes and its hardness property that can take continuous values. Our proposed FastCAR approach for task consolidation achieves training time efficiency (2.52x quicker) and reduced inference latency (55% faster) than benchmark MTL networks. </p>
<blockquote>
<p>FastCARæ˜¯ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰ä¸­çš„æ–°å‹ä»»åŠ¡æ•´åˆæ–¹æ³•ï¼Œé€‚ç”¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚å°½ç®¡ä»»åŠ¡ä¹‹é—´å­˜åœ¨éå¹³å‡¡æ€§ä¸”ä»…å­˜åœ¨ç»†å¾®ç›¸å…³æ€§ï¼Œä½†è¯¥æ–¹æ³•è§£å†³äº†æ£€æµ‹åˆ°çš„å¯¹è±¡ï¼ˆå æ®æ•´ä¸ªå›¾åƒå¸§ï¼‰çš„åˆ†ç±»é—®é¢˜ä»¥åŠè¿ç»­å±æ€§å˜é‡ï¼ˆå¯¹äºå¯¹è±¡ç±»çš„å®ä¾‹ï¼‰çš„å›å½’å»ºæ¨¡é—®é¢˜ï¼Œè¿™åœ¨ç§‘å­¦å’Œå·¥ç¨‹ä¸­æ˜¯ä¸€ä¸ªå…³é”®ç”¨ä¾‹ã€‚FastCARæ¶‰åŠæ ‡ç­¾è½¬æ¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€‚ç”¨äºä»…ä½¿ç”¨å•ä»»åŠ¡å›å½’ç½‘ç»œæ¶æ„ã€‚å½“åŒæ—¶è€ƒè™‘è¿™ä¸¤ä¸ªä»»åŠ¡çš„å­¦ä¹ æ—¶ï¼ŒFastCARåœ¨æ¶æ„å’ŒæŸå¤±åŠ æƒæ–¹æ¡ˆçš„æ™¯è§‚ä¸­è¶…è¶Šäº†ä¼ ç»Ÿçš„MTLæ¨¡å‹å®¶æ—ï¼ˆåˆ†ç±»å‡†ç¡®åº¦ä¸º99.54%ï¼Œå›å½’å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ä¸º2.4%ï¼‰ã€‚æ‰€è¿›è¡Œçš„å®éªŒä½¿ç”¨äº†æˆ‘ä»¬æä¾›çš„â€œé«˜çº§é’¢å±æ€§æ•°æ®é›†â€ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/fastcandr/AdvancedSteel-Property-Dataset%EF%BC%89%E3%80%82%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8C%85%E5%90%AB%E7%BB%8F%E8%BF%87%E6%A0%87%E8%AE%B0%E7%9A%84%E5%8C%85%E5%90%AB%E7%A6%BB%E6%95%A3%E5%AF%B9%E8%B1%A1%E7%B1%BB%E4%BB%A5%E5%8F%8A%E5%85%B6%E5%8F%AF%E8%8E%B7%E5%8F%96%E8%BF%9E%E7%BB%AD%E5%80%BC%E7%9A%84%E7%A1%AC%E5%BA%A6%E5%B1%9E%E6%80%A7%E7%9A%84%E5%9B%BE%E5%83%8F%E5%85%B1%E8%AE%A14536%E5%BC%A0%EF%BC%8C%E6%AF%8F%E5%BC%A0%E5%83%8F%E7%B4%A0%E5%A4%A7%E5%B0%8F%E4%B8%BA224x224%E3%80%82%E6%88%91%E4%BB%AC%E6%8F%90%E5%87%BA%E7%9A%84%E7%94%A8%E4%BA%8E%E4%BB%BB%E5%8A%A1%E6%95%B4%E5%90%88%E7%9A%84FastCAR%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0%E4%BA%86%E8%AE%AD%E7%BB%83%E6%97%B6%E9%97%B4%E6%95%88%E7%8E%87%EF%BC%88%E6%8F%90%E9%AB%98%E4%BA%862.52%E5%80%8D%EF%BC%89%E5%92%8C%E6%8E%A8%E7%90%86%E5%BB%B6%E8%BF%9F%EF%BC%88%E5%87%8F%E5%B0%91%E4%BA%8655%%EF%BC%89%E7%9B%B8%E6%AF%94%E5%9F%BA%E5%87%86MTL%E7%BD%91%E7%BB%9C%E6%9B%B4%E5%BF%AB%E9%80%9F%E9%AB%98%E6%95%88%E3%80%82">https://github.com/fastcandr/AdvancedSteel-Property-Datasetï¼‰ã€‚è¯¥æ•°æ®é›†åŒ…å«ç»è¿‡æ ‡è®°çš„åŒ…å«ç¦»æ•£å¯¹è±¡ç±»ä»¥åŠå…¶å¯è·å–è¿ç»­å€¼çš„ç¡¬åº¦å±æ€§çš„å›¾åƒå…±è®¡4536å¼ ï¼Œæ¯å¼ åƒç´ å¤§å°ä¸º224x224ã€‚æˆ‘ä»¬æå‡ºçš„ç”¨äºä»»åŠ¡æ•´åˆçš„FastCARæ–¹æ³•å®ç°äº†è®­ç»ƒæ—¶é—´æ•ˆç‡ï¼ˆæé«˜äº†2.52å€ï¼‰å’Œæ¨ç†å»¶è¿Ÿï¼ˆå‡å°‘äº†55%ï¼‰ç›¸æ¯”åŸºå‡†MTLç½‘ç»œæ›´å¿«é€Ÿé«˜æ•ˆã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00208v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>FastCARæ˜¯ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰ä¸­çš„æ–°å‹ä»»åŠ¡æ•´åˆæ–¹æ³•ï¼Œé€‚ç”¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚å®ƒè§£å†³äº†æ£€æµ‹åˆ°çš„å¯¹è±¡åˆ†ç±»ï¼ˆå æ®æ•´ä¸ªå›¾åƒå¸§ï¼‰å’Œè¿ç»­å±æ€§å˜é‡å›å½’çš„é—®é¢˜ã€‚FastCARé€šè¿‡æ ‡ç­¾è½¬æ¢æ–¹æ³•å®ç°ï¼Œè¯¥æ–¹æ³•é€‚ç”¨äºä»…ä½¿ç”¨å•ä»»åŠ¡å›å½’ç½‘ç»œæ¶æ„ã€‚å½“åŒæ—¶è€ƒè™‘ä¸¤ä¸ªä»»åŠ¡æ—¶ï¼ŒFastCARåœ¨æ¶æ„å’ŒæŸå¤±åŠ æƒæ–¹æ¡ˆçš„å‚æ•°åŒ–ä¸­ä¼˜äºä¼ ç»ŸMTLæ¨¡å‹å®¶æ—ï¼Œåˆ†ç±»å‡†ç¡®åº¦è¾¾åˆ°99.54%ï¼Œå›å½’å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ä¸º2.4%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„è®­ç»ƒæ•ˆç‡é«˜ï¼ˆæ¯”åŸºå‡†MTLç½‘ç»œå¿«2.52å€ï¼‰ï¼Œæ¨ç†å»¶è¿Ÿé™ä½ï¼ˆå‡å°‘55%ï¼‰ã€‚ä½¿ç”¨æˆ‘ä»¬æä¾›çš„â€œé«˜çº§é’¢æå±æ€§æ•°æ®é›†â€è¿›è¡Œå®éªŒï¼Œè¯¥æ•°æ®é›†åŒ…å«æ ‡æ³¨æœ‰ç¦»æ•£å¯¹è±¡ç±»åˆ«å’Œå…¶ç¡¬åº¦å±æ€§çš„å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FastCARæ˜¯ä¸€ç§å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æ–°å‹ä»»åŠ¡æ•´åˆæ–¹æ³•ï¼Œç”¨äºè§£å†³åˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚</li>
<li>å®ƒè§£å†³äº†æ£€æµ‹åˆ°çš„å¯¹è±¡åˆ†ç±»é—®é¢˜å’Œè¿ç»­å±æ€§å˜é‡çš„å›å½’é—®é¢˜ã€‚</li>
<li>FastCARé€šè¿‡æ ‡ç­¾è½¬æ¢æ–¹æ³•å®ç°ï¼Œé€‚ç”¨äºå•ä»»åŠ¡å›å½’ç½‘ç»œæ¶æ„ã€‚</li>
<li>åœ¨åŒæ—¶è€ƒè™‘ä¸¤ä¸ªä»»åŠ¡æ—¶ï¼ŒFastCARåœ¨åˆ†ç±»å’Œå›å½’æ–¹é¢éƒ½è¡¨ç°å‡ºä¼˜äºä¼ ç»ŸMTLæ¨¡å‹çš„ä¼˜åŠ¿ã€‚</li>
<li>FastCARçš„åˆ†ç±»å‡†ç¡®åº¦é«˜è¾¾99.54%ï¼Œå›å½’å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ä¸º2.4%ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰é«˜æ•ˆçš„è®­ç»ƒæ€§èƒ½ï¼Œæ¯”åŸºå‡†MTLç½‘ç»œå¿«2.52å€ï¼Œå¹¶é™ä½äº†55%çš„æ¨ç†å»¶è¿Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21f9ac443e5a3866a1a89e33ce10178e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86a497d6dab6790501ab71120aa120b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ee81e806dba0d539e6d0db2de6604b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af5c3d8235537625e715830c03cad5d7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MTevent-A-Multi-Task-Event-Camera-Dataset-for-6D-Pose-Estimation-and-Moving-Object-Detection"><a href="#MTevent-A-Multi-Task-Event-Camera-Dataset-for-6D-Pose-Estimation-and-Moving-Object-Detection" class="headerlink" title="MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and   Moving Object Detection"></a>MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and   Moving Object Detection</h2><p><strong>Authors:Shrutarv Awasthi, Anas Gouda, Sven Franke, JÃ©rÃ´me Rutinowski, Frank Hoffmann, Moritz Roidl</strong></p>
<p>Mobile robots are reaching unprecedented speeds, with platforms like Unitree B2, and Fraunhofer O3dyn achieving maximum speeds between 5 and 10 m&#x2F;s. However, effectively utilizing such speeds remains a challenge due to the limitations of RGB cameras, which suffer from motion blur and fail to provide real-time responsiveness. Event cameras, with their asynchronous operation, and low-latency sensing, offer a promising alternative for high-speed robotic perception. In this work, we introduce MTevent, a dataset designed for 6D pose estimation and moving object detection in highly dynamic environments with large detection distances. Our setup consists of a stereo-event camera and an RGB camera, capturing 75 scenes, each on average 16 seconds, and featuring 16 unique objects under challenging conditions such as extreme viewing angles, varying lighting, and occlusions. MTevent is the first dataset to combine high-speed motion, long-range perception, and real-world object interactions, making it a valuable resource for advancing event-based vision in robotics. To establish a baseline, we evaluate the task of 6D pose estimation using NVIDIAâ€™s FoundationPose on RGB images, achieving an Average Recall of 0.22 with ground-truth masks, highlighting the limitations of RGB-based approaches in such dynamic settings. With MTevent, we provide a novel resource to improve perception models and foster further research in high-speed robotic vision. The dataset is available for download <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/anas-gouda/MTevent">https://huggingface.co/datasets/anas-gouda/MTevent</a> </p>
<blockquote>
<p>ç§»åŠ¨æœºå™¨äººçš„é€Ÿåº¦å·²ç»è¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„æ°´å¹³ï¼Œå¦‚Unitree B2å’ŒFraunhofer O3dynç­‰å¹³å°ï¼Œæœ€é«˜é€Ÿåº¦å¯è¾¾5è‡³10ç±³&#x2F;ç§’ã€‚ç„¶è€Œï¼Œç”±äºRGBç›¸æœºçš„å±€é™æ€§ï¼Œæœ‰æ•ˆåˆ©ç”¨è¿™æ ·çš„é€Ÿåº¦ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼ŒRGBç›¸æœºå—åˆ°è¿åŠ¨æ¨¡ç³Šçš„å½±å“ï¼Œæ— æ³•æä¾›å®æ—¶å“åº”ã€‚äº‹ä»¶ç›¸æœºå…·æœ‰å¼‚æ­¥æ“ä½œå’Œä½å»¶è¿Ÿæ„ŸçŸ¥çš„ç‰¹ç‚¹ï¼Œä¸ºé«˜é€Ÿæœºå™¨äººæ„ŸçŸ¥æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MTeventæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨ç”¨äºé«˜åº¦åŠ¨æ€ç¯å¢ƒä¸­å¤§èŒƒå›´æ£€æµ‹ä¸‹çš„6Då§¿æ€ä¼°è®¡å’Œè¿åŠ¨ç›®æ ‡æ£€æµ‹ã€‚æˆ‘ä»¬çš„è®¾å¤‡é…ç½®åŒ…æ‹¬ç«‹ä½“äº‹ä»¶ç›¸æœºå’ŒRGBç›¸æœºï¼Œæ‹æ‘„äº†75ä¸ªåœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯å¹³å‡16ç§’ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹å±•ç¤ºäº†16ä¸ªç‹¬ç‰¹ç‰©ä½“ï¼Œä¾‹å¦‚æç«¯è§†è§’ã€å…‰çº¿å˜åŒ–å’Œé®æŒ¡ã€‚MTeventæ˜¯ç¬¬ä¸€ä¸ªç»“åˆé«˜é€Ÿè¿åŠ¨ã€é•¿è·ç¦»æ„ŸçŸ¥å’Œç°å®ä¸–ç•Œç‰©ä½“äº¤äº’çš„æ•°æ®é›†ï¼Œæ˜¯æ¨åŠ¨æœºå™¨äººåŸºäºäº‹ä»¶è§†è§‰å‘å±•çš„å®è´µèµ„æºã€‚ä¸ºäº†å»ºç«‹åŸºå‡†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä½¿ç”¨NVIDIAçš„FoundationPoseåœ¨RGBå›¾åƒä¸Šè¿›è¡Œ6Då§¿æ€ä¼°è®¡çš„ä»»åŠ¡ï¼Œä½¿ç”¨çœŸå®æ©è†œçš„å¹³å‡å¬å›ç‡ä¸º0.22ï¼Œè¿™çªæ˜¾äº†åœ¨åŠ¨æ€ç¯å¢ƒä¸­åŸºäºRGBçš„æ–¹æ³•çš„å±€é™æ€§ã€‚é€šè¿‡MTeventæ•°æ®é›†ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ”¹è¿›æ„ŸçŸ¥æ¨¡å‹å¹¶ä¿ƒè¿›é«˜é€Ÿæœºå™¨äººè§†è§‰ç ”ç©¶çš„å®è´µèµ„æºã€‚è¯¥æ•°æ®é›†å¯ä¸‹è½½ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/anas-gouda/MTevent%E3%80%82">https://huggingface.co/datasets/anas-gouda/MTeventã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11282v2">PDF</a> Accepted at 2025 IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition Workshops (CVPRW); Fifth International Workshop on Event-Based   Vision</p>
<p><strong>Summary</strong>ï¼šéšç€ç§»åŠ¨æœºå™¨äººæŠ€æœ¯çš„é«˜é€Ÿå‘å±•ï¼Œä»¥Unitree B2å’ŒFraunhofer O3dynç­‰å¹³å°ä¸ºä»£è¡¨çš„æœºå™¨äººé€Ÿåº¦ä¸æ–­çªç ´ï¼Œä½†ä¼ ç»ŸRGBç›¸æœºåœ¨é«˜é€Ÿè¿åŠ¨ä¸‹ä¼šå‡ºç°è¿åŠ¨æ¨¡ç³Šå’Œå®æ—¶å“åº”ä¸è¶³çš„é—®é¢˜ã€‚äº‹ä»¶ç›¸æœºå› å…¶å¼‚æ­¥æ“ä½œå’Œä½å»¶è¿Ÿæ„ŸçŸ¥ç‰¹æ€§ï¼Œä¸ºé«˜é€Ÿæœºå™¨äººæ„ŸçŸ¥æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶å¼•å…¥äº†MTeventæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é€‚ç”¨äºé«˜é€ŸåŠ¨æ€ç¯å¢ƒä¸‹çš„å¤§è·ç¦»æ£€æµ‹ä¸­çš„6Då§¿æ€ä¼°è®¡å’Œç§»åŠ¨ç‰©ä½“æ£€æµ‹ã€‚æ•°æ®é›†ç”±ç«‹ä½“äº‹ä»¶ç›¸æœºå’ŒRGBç›¸æœºç»„æˆï¼Œå…±æ•æ‰äº†75ä¸ªåœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯å¹³å‡æŒç»­16ç§’ï¼ŒåŒ…å«16ä¸ªç‹¬ç‰¹ç‰©ä½“åœ¨æç«¯è§†è§’ã€ä¸åŒå…‰ç…§å’Œé®æŒ¡ç­‰æŒ‘æˆ˜æ¡ä»¶ä¸‹çš„å›¾åƒã€‚MTeventæ˜¯é¦–ä¸ªç»“åˆé«˜é€Ÿè¿åŠ¨ã€é•¿è·ç¦»æ„ŸçŸ¥å’Œç°å®ä¸–ç•Œç‰©ä½“äº¤äº’çš„æ•°æ®é›†ï¼Œå¯¹äºæ¨åŠ¨åŸºäºäº‹ä»¶è§†è§‰çš„æœºå™¨äººæŠ€æœ¯è¿›æ­¥å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç§»åŠ¨æœºå™¨äººé€Ÿåº¦ä¸æ–­çªç ´ï¼Œä½†RGBç›¸æœºåœ¨é«˜é€Ÿç¯å¢ƒä¸‹çš„æ„ŸçŸ¥å­˜åœ¨å±€é™ã€‚</li>
<li>äº‹ä»¶ç›¸æœºä¸ºé«˜é€Ÿæœºå™¨äººæ„ŸçŸ¥æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>MTeventæ•°æ®é›†æ˜¯é¦–ä¸ªç»“åˆé«˜é€Ÿè¿åŠ¨ã€é•¿è·ç¦»æ„ŸçŸ¥å’Œç°å®ä¸–ç•Œç‰©ä½“äº¤äº’çš„æ•°æ®é›†ã€‚</li>
<li>MTeventåŒ…å«75ä¸ªåœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯å¹³å‡æŒç»­16ç§’ï¼ŒåŒ…å«å¤šç§æŒ‘æˆ˜æ¡ä»¶ä¸‹çš„å›¾åƒã€‚</li>
<li>NVIDIAçš„FoundationPoseåœ¨RGBå›¾åƒä¸Šçš„6Då§¿æ€ä¼°è®¡ä»»åŠ¡è¯„ä»·è¡¨æ˜ï¼Œåœ¨åŠ¨æ€è®¾ç½®ä¸­çš„å¹³å‡å¬å›ç‡ä¸º0.22ï¼Œå‡¸æ˜¾RGBç›¸æœºåœ¨æ­¤ç±»ç¯å¢ƒä¸‹çš„å±€é™æ€§ã€‚</li>
<li>MTeventæ•°æ®é›†ä¸ºæ”¹è¿›æ„ŸçŸ¥æ¨¡å‹å’Œæ¨åŠ¨é«˜é€Ÿæœºå™¨äººè§†è§‰ç ”ç©¶æä¾›äº†å®è´µèµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-478f21ee5a962bf3de3836f4159056cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-834cc3c00b8fa0aa06c067a606ed100e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a59ec0df294fdb6e661de9111400be8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b09b88853d0e95282b0225b30ef5dbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a536310910713e0ea477d04f41f20ac0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aa050908ddd6867f1b574e2673813560.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LEGNet-Lightweight-Edge-Gaussian-Driven-Network-for-Low-Quality-Remote-Sensing-Image-Object-Detection"><a href="#LEGNet-Lightweight-Edge-Gaussian-Driven-Network-for-Low-Quality-Remote-Sensing-Image-Object-Detection" class="headerlink" title="LEGNet: Lightweight Edge-Gaussian Driven Network for Low-Quality Remote   Sensing Image Object Detection"></a>LEGNet: Lightweight Edge-Gaussian Driven Network for Low-Quality Remote   Sensing Image Object Detection</h2><p><strong>Authors:Wei Lu, Si-Bao Chen, Hui-Dong Li, Qing-Ling Shu, Chris H. Q. Ding, Jin Tang, Bin Luo</strong></p>
<p>Remote sensing object detection (RSOD) often suffers from degradations such as low spatial resolution, sensor noise, motion blur, and adverse illumination. These factors diminish feature distinctiveness, leading to ambiguous object representations and inadequate foreground-background separation. Existing RSOD methods exhibit limitations in robust detection of low-quality objects. To address these pressing challenges, we introduce LEGNet, a lightweight backbone network featuring a novel Edge-Gaussian Aggregation (EGA) module specifically engineered to enhance feature representation derived from low-quality remote sensing images. EGA module integrates: (a) orientation-aware Scharr filters to sharpen crucial edge details often lost in low-contrast or blurred objects, and (b) Gaussian-prior-based feature refinement to suppress noise and regularize ambiguous feature responses, enhancing foreground saliency under challenging conditions. EGA module alleviates prevalent problems in reduced contrast, structural discontinuities, and ambiguous feature responses prevalent in degraded images, effectively improving model robustness while maintaining computational efficiency. Comprehensive evaluations across five benchmarks (DOTA-v1.0, v1.5, DIOR-R, FAIR1M-v1.0, and VisDrone2019) demonstrate that LEGNet achieves state-of-the-art performance, particularly in detecting low-quality objects. The code is available at <a target="_blank" rel="noopener" href="https://github.com/lwCVer/LEGNet">https://github.com/lwCVer/LEGNet</a>. </p>
<blockquote>
<p>é¥æ„Ÿç›®æ ‡æ£€æµ‹ï¼ˆRSODï¼‰ç»å¸¸å—åˆ°ç©ºé—´åˆ†è¾¨ç‡ä½ã€ä¼ æ„Ÿå™¨å™ªå£°ã€è¿åŠ¨æ¨¡ç³Šå’Œä¸è‰¯ç…§æ˜ç­‰é—®é¢˜çš„å›°æ‰°ã€‚è¿™äº›å› ç´ é™ä½äº†ç‰¹å¾çš„è¾¨è¯†åº¦ï¼Œå¯¼è‡´ç›®æ ‡è¡¨ç¤ºä¸æ˜ç¡®ï¼Œå‰æ™¯ä¸èƒŒæ™¯åˆ†ç¦»ä¸è¶³ã€‚ç°æœ‰çš„é¥æ„Ÿç›®æ ‡æ£€æµ‹æ–¹æ³•åœ¨ä½è´¨é‡ç›®æ ‡æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›ç´§è¿«æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LEGNetç½‘ç»œï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æ–°å‹è¾¹ç¼˜é«˜æ–¯èšåˆï¼ˆEGAï¼‰æ¨¡å—çš„è½»é‡çº§ä¸»å¹²ç½‘ç»œï¼Œä¸“é—¨è®¾è®¡ç”¨äºå¢å¼ºä»ä½è´¨é‡é¥æ„Ÿå›¾åƒä¸­æ´¾ç”Ÿçš„ç‰¹å¾è¡¨ç¤ºã€‚EGAæ¨¡å—é›†æˆäº†ï¼šï¼ˆaï¼‰å®šå‘æ„ŸçŸ¥çš„Scharræ»¤æ³¢å™¨ï¼Œç”¨äºé”åŒ–ä½å¯¹æ¯”åº¦æˆ–æ¨¡ç³Šå¯¹è±¡é€šå¸¸ä¸¢å¤±çš„å…³é”®è¾¹ç¼˜ç»†èŠ‚ï¼›ï¼ˆbï¼‰åŸºäºé«˜æ–¯å…ˆéªŒçš„ç‰¹å¾ç»†åŒ–ï¼Œç”¨äºæŠ‘åˆ¶å™ªå£°å¹¶è§„èŒƒåŒ–æ¨¡ç³Šçš„ç‰¹å¾å“åº”ï¼Œä»è€Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹å¢å¼ºå‰æ™¯æ˜¾è‘—æ€§ã€‚EGAæ¨¡å—ç¼“è§£äº†åœ¨å¯¹æ¯”åº¦é™ä½ã€ç»“æ„ä¸è¿ç»­å’Œæ¨¡ç³Šç‰¹å¾å“åº”ä¸­æ™®éå­˜åœ¨çš„é—®é¢˜ï¼Œåœ¨é€€åŒ–å›¾åƒä¸­æœ‰æ•ˆæé«˜äº†æ¨¡å‹çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆDOTA-v1.0ã€v1.5ã€DIOR-Rã€FAIR1M-v1.0å’ŒVisDrone2019ï¼‰ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒLEGNetè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹ä½è´¨é‡ç›®æ ‡æ–¹é¢ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/lwCVer/LEGNet%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lwCVer/LEGNetè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.14012v2">PDF</a> 17 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>è¿œç¨‹é¥æ„Ÿç‰©ä½“æ£€æµ‹ï¼ˆRSODï¼‰é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚ä½ç©ºé—´åˆ†è¾¨ç‡ã€ä¼ æ„Ÿå™¨å™ªå£°ã€è¿åŠ¨æ¨¡ç³Šå’Œä¸è‰¯ç…§æ˜ç­‰ã€‚è¿™äº›é—®é¢˜å¯¼è‡´ç‰¹å¾åŒºåˆ†åº¦é™ä½ï¼Œå‡ºç°å¯¹è±¡è¡¨ç¤ºä¸æ˜ç¡®å’Œå‰æ™¯èƒŒæ™¯åˆ†ç¦»ä¸è¶³çš„æƒ…å†µã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†LEGNetç½‘ç»œï¼Œè¯¥ç½‘ç»œé‡‡ç”¨äº†ä¸€ç§æ–°å‹çš„è¾¹ç¼˜é«˜æ–¯èšåˆï¼ˆEGAï¼‰æ¨¡å—ï¼Œæ—¨åœ¨å¢å¼ºä»ä½è´¨é‡é¥æ„Ÿå›¾åƒä¸­æ´¾ç”Ÿçš„ç‰¹å¾è¡¨ç¤ºã€‚EGAæ¨¡å—é€šè¿‡ç»“åˆæ–¹å‘æ„ŸçŸ¥çš„Scharræ»¤æ³¢å™¨å’ŒåŸºäºé«˜æ–¯å…ˆéªŒçš„ç‰¹å¾ç»†åŒ–ï¼Œæé«˜äº†è¾¹ç¼˜ç»†èŠ‚çš„æ¸…æ™°åº¦ï¼Œå¹¶æŠ‘åˆ¶äº†å™ªå£°ã€‚è¿™æé«˜äº†æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLEGNetåœ¨ä½è´¨é‡å¯¹è±¡æ£€æµ‹æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿œç¨‹é¥æ„Ÿç‰©ä½“æ£€æµ‹ï¼ˆRSODï¼‰é¢ä¸´ä½ç©ºé—´åˆ†è¾¨ç‡ã€ä¼ æ„Ÿå™¨å™ªå£°ã€è¿åŠ¨æ¨¡ç³Šå’Œä¸è‰¯ç…§æ˜ç­‰æŒ‘æˆ˜ã€‚</li>
<li>ç‰¹å¾åŒºåˆ†åº¦é™ä½å¯¼è‡´å¯¹è±¡è¡¨ç¤ºä¸æ˜ç¡®å’Œå‰æ™¯èƒŒæ™¯åˆ†ç¦»ä¸è¶³ã€‚</li>
<li>LEGNetç½‘ç»œé€šè¿‡é‡‡ç”¨æ–°å‹çš„è¾¹ç¼˜é«˜æ–¯èšåˆï¼ˆEGAï¼‰æ¨¡å—æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>EGAæ¨¡å—æé«˜äº†è¾¹ç¼˜ç»†èŠ‚çš„æ¸…æ™°åº¦å¹¶æŠ‘åˆ¶äº†å™ªå£°ã€‚</li>
<li>EGAæ¨¡å—åŒ…æ‹¬æ–¹å‘æ„ŸçŸ¥çš„Scharræ»¤æ³¢å™¨å’ŒåŸºäºé«˜æ–¯å…ˆéªŒçš„ç‰¹å¾ç»†åŒ–ã€‚</li>
<li>LEGNetåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½è´¨é‡å¯¹è±¡æ£€æµ‹æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.14012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-631dd53038de5ee714b7cde8bfc5cba6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-46bb96fdef96ae481b190643240c5653.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8adbd44fd7a985197c0098ee4867c0dd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Low-Resolution-Self-Attention-for-Semantic-Segmentation"><a href="#Low-Resolution-Self-Attention-for-Semantic-Segmentation" class="headerlink" title="Low-Resolution Self-Attention for Semantic Segmentation"></a>Low-Resolution Self-Attention for Semantic Segmentation</h2><p><strong>Authors:Yu-Huan Wu, Shi-Chen Zhang, Yun Liu, Le Zhang, Xin Zhan, Daquan Zhou, Jiashi Feng, Ming-Ming Cheng, Liangli Zhen</strong></p>
<p>Semantic segmentation tasks naturally require high-resolution information for pixel-wise segmentation and global context information for class prediction. While existing vision transformers demonstrate promising performance, they often utilize high-resolution context modeling, resulting in a computational bottleneck. In this work, we challenge conventional wisdom and introduce the Low-Resolution Self-Attention (LRSA) mechanism to capture global context at a significantly reduced computational cost, i.e., FLOPs. Our approach involves computing self-attention in a fixed low-resolution space regardless of the input imageâ€™s resolution, with additional 3x3 depth-wise convolutions to capture fine details in the high-resolution space. We demonstrate the effectiveness of our LRSA approach by building the LRFormer, a vision transformer with an encoder-decoder structure. Extensive experiments on the ADE20K, COCO-Stuff, and Cityscapes datasets demonstrate that LRFormer outperforms state-of-the-art models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/yuhuan-wu/LRFormer">https://github.com/yuhuan-wu/LRFormer</a>. </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²ä»»åŠ¡è‡ªç„¶éœ€è¦é«˜åˆ†è¾¨ç‡ä¿¡æ¯è¿›è¡Œåƒç´ çº§åˆ†å‰²å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œç±»åˆ«é¢„æµ‹ã€‚å°½ç®¡ç°æœ‰çš„è§†è§‰å˜å‹å™¨è¡¨ç°å‡ºäº†æœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸åˆ©ç”¨é«˜åˆ†è¾¨ç‡ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œå¯¼è‡´è®¡ç®—ç“¶é¢ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æŒ‘æˆ˜ä¼ ç»Ÿæ™ºæ…§ï¼Œå¼•å…¥äº†ä½åˆ†è¾¨ç‡è‡ªæ³¨æ„ï¼ˆLRSAï¼‰æœºåˆ¶ï¼Œä»¥æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼ˆå³FLOPsï¼‰æ•è·å…¨å±€ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠåœ¨å›ºå®šçš„ä½åˆ†è¾¨ç‡ç©ºé—´ä¸­è¿›è¡Œè‡ªæ³¨æ„è®¡ç®—ï¼Œè€Œä¸ç®¡è¾“å…¥å›¾åƒçš„åˆ†è¾¨ç‡å¦‚ä½•ï¼Œå¹¶ä½¿ç”¨é¢å¤–çš„3x3æ·±åº¦å¯åˆ†ç¦»å·ç§¯æ¥æ•è·é«˜åˆ†è¾¨ç‡ç©ºé—´ä¸­çš„ç»†èŠ‚ã€‚æˆ‘ä»¬é€šè¿‡æ„å»ºLRFormerâ€”â€”ä¸€ç§å…·æœ‰ç¼–ç å™¨-è§£ç å™¨ç»“æ„çš„è§†è§‰å˜å‹å™¨æ¥è¯æ˜æˆ‘ä»¬çš„LRSAæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åœ¨ADE20Kã€COCO-Stuffå’ŒCityscapesæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLRFormerä¼˜äºæœ€æ–°æ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yuhuan-wu/LRFormer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yuhuan-wu/LRFormeræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05026v3">PDF</a> Accepted by IEEE TPAMI; 14 pages, 6 figures, 14 tables</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­è®¡ç®—é‡å¤§å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä½åˆ†è¾¨ç‡è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„LRSAæ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ä½åˆ†è¾¨ç‡ç©ºé—´è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—ï¼Œå¹¶é€šè¿‡æ·±åº¦å¯åˆ†ç¦»å·ç§¯æ•æ‰é«˜åˆ†è¾¨ç‡ç©ºé—´çš„ç»†èŠ‚ä¿¡æ¯ï¼Œä»è€Œåœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºLRSAæ–¹æ³•çš„LRFormeråœ¨ADE20Kã€COCO-Stuffå’ŒCityscapesæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­ä¹‰åˆ†å‰²ä»»åŠ¡éœ€è¦é«˜åˆ†è¾¨ä¿¡æ¯å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>ä¼ ç»Ÿè§†é‡å˜æ¢å™¨åœ¨è®¡ç®—é‡å¤§å’Œè®¡ç®—æˆæœ¬é«˜çš„æƒ…å†µä¸‹ï¼Œåœ¨é«˜åˆ†è¾¨ç‡ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>LRSAæœºåˆ¶åœ¨ä½åˆ†è¾¨ç‡ç©ºé—´è¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
<li>LRSAé€šè¿‡æ·±åº¦å¯åˆ†ç¦»å·ç§¯æ•æ‰é«˜åˆ†è¾¨ç‡ç©ºé—´çš„ç»†èŠ‚ä¿¡æ¯ã€‚</li>
<li>LRSAæ–¹æ³•é€šè¿‡æ„å»ºLRFormeræ¨¡å‹å®ç°ï¼Œè¯¥æ¨¡å‹å…·æœ‰ç¼–ç å™¨-è§£ç å™¨ç»“æ„ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLRFormeråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.05026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01d5bf7db17e4df56be42e510b7a7ed6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b4d9a9cc88400a1cc84ba1ebfe5efea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09fbb4bc325e44195dc9c75d4adc6580.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73ec99bdab683b6bc2b7af1724f52936.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cce431af12b7cccd52692e0c943621ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56bd03bd93b4cbfaa39b0a8d521ff553.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed8af1fe2d041f5183d6ecda9ed4a8a8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5519fd18407786cd7e76489e87e9e414.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  High-Contrast Coronagraphy
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-8187121cc6a85f340153eb9f2d1aecc3.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24474.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
