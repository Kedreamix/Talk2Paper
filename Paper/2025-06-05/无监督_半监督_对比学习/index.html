<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="无监督/半监督/对比学习">
    <meta name="description" content="无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-06-05  High-Contrast Coronagraphy">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>无监督/半监督/对比学习 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5519fd18407786cd7e76489e87e9e414.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">无监督/半监督/对比学习</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">无监督/半监督/对比学习</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                无监督/半监督/对比学习
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-05-更新"><a href="#2025-06-05-更新" class="headerlink" title="2025-06-05 更新"></a>2025-06-05 更新</h1><h2 id="High-Contrast-Coronagraphy"><a href="#High-Contrast-Coronagraphy" class="headerlink" title="High-Contrast Coronagraphy"></a>High-Contrast Coronagraphy</h2><p><strong>Authors:Matthew A. Kenworthy, Sebastiaan Y. Haffert</strong></p>
<p>Imaging terrestrial exoplanets around nearby stars is a formidable technical challenge, requiring the development of coronagraphs to suppress the stellar halo of diffracted light at the location of the planet. In this review, we derive the science requirement for high-contrast imaging, present an overview of diffraction theory and the Lyot coronagraph, and define the parameters used in our optimization. We detail the working principles of coronagraphs both in the laboratory and on-sky with current high-contrast instruments, and we describe the required algorithms and processes necessary for terrestrial planet imaging with the extremely large telescopes and proposed space telescope missions:   * Imaging terrestrial planets around nearby stars is possible with a combination of coronagraphs and active wavefront control using feedback from wavefront sensors.   * Ground based 8-40m class telescopes can target the habitable zone around nearby M dwarf stars with contrasts of $10^{-7}$ and space telescopes can search around solar-type stars with contrasts of $10^{-10}$.   * Focal plane wavefront sensing, hybrid coronagraph designs and multiple closed loops providing active correction are required to reach the highest sensitivities.   * Polarization effects need to be mitigated for reaching $10^{-10}$ contrasts whilst keeping exoplanet yields as high as possible.   * Recent technological developments, including photonics and microwave kinetic inductance detectors, will be folded into high-contrast instruments. </p>
<blockquote>
<p>成像邻近恒星周围的地外行星是一项艰巨的技术挑战，需要开发消光仪来抑制行星位置处衍射光形成的恒星光晕。在本综述中，我们推导了高对比度成像的科学要求，概述了衍射理论和消光仪的原理，并定义了优化中使用的参数。我们详细介绍了实验室和当前高对比度仪器在天空中的消光仪工作原理，并描述了使用极大望远镜和拟议的空间望远镜任务进行地球行星成像所需的算法和流程：<em>结合消光仪和活动波前控制，使用波前传感器的反馈，可以实现邻近恒星周围地球行星的成像。</em>地面8-40米级望远镜可以在邻近的矮星周围的可居住区域实现对比度为达到$ 10^{-7}$的成像，而太空望远镜可以在太阳型恒星周围实现对比度为$ 10^{-10}$的搜索。<em>为了达到最高的灵敏度，需要采用焦平面波前检测、混合消光仪设计和提供主动校正的多个闭环。</em>为了达到$ 10^{-10}$的对比度并保持尽可能高的地外行星产量，需要缓解偏振效应。*包括光子学和微波动电感检测器在内的最新技术将被纳入高对比度仪器中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02907v1">PDF</a> Invited review article. 41 pages, 12 figures, 1 table. The paper is   in a reproducible workflow repository at   <a target="_blank" rel="noopener" href="https://github.com/mkenworthy/ARAA_HCC">https://github.com/mkenworthy/ARAA_HCC</a></p>
<p><strong>Summary</strong></p>
<p>该文探讨了使用高对比度成像技术来观测邻近恒星周围的地外行星的挑战。文中详细介绍了高对比度成像的科学要求、衍射理论及Lyot掩星仪的原理，并概述了优化参数。同时，文章还介绍了实验室和现有高对比度仪器中掩星仪的工作原理，以及使用超大望远镜和未来太空望远镜任务进行地球类行星成像所需的算法和流程。文章指出，结合掩星仪和活动波前控制，使用波前传感器的反馈，可以在地面基础8-40米级的望远镜和太空望远镜上实现邻近恒星周围地球类行星的成像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>掩星仪和活动波前控制技术的结合是实现邻近恒星周围地球类行星成像的关键。</li>
<li>地面基础的8-40米级望远镜可以针对M矮星的宜居区域进行高对比度成像，而太空望远镜可以针对太阳型星的更高对比度区域进行搜索。</li>
<li>为了达到最高的灵敏度，需要焦点平面波前检测、混合掩星仪设计和提供主动校正的多个闭环。</li>
<li>极化效应需要在达到10^-10对比度时予以缓解，同时保持尽可能高的类地行星产量。</li>
<li>最近的技术发展，如光子学和微波动电感检测器，将被整合到高对比度仪器中。</li>
<li>文中介绍了衍射理论的基础知识，这是理解高对比度成像技术的基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02907">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-74e48cba7ca8c46d7dd93d10d3626b62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23bfea25d913d6fee4a273e0036dcf54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c449788b9d495e453dedb0fa9654689.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-568c6336ad7e2ae0d0258f9a9447a863.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a50410603a6e75a9b63580edc9b3acc4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VLCD-Vision-Language-Contrastive-Distillation-for-Accurate-and-Efficient-Automatic-Placenta-Analysis"><a href="#VLCD-Vision-Language-Contrastive-Distillation-for-Accurate-and-Efficient-Automatic-Placenta-Analysis" class="headerlink" title="VLCD: Vision-Language Contrastive Distillation for Accurate and   Efficient Automatic Placenta Analysis"></a>VLCD: Vision-Language Contrastive Distillation for Accurate and   Efficient Automatic Placenta Analysis</h2><p><strong>Authors:Manas Mehta, Yimu Pan, Kelly Gallagher, Alison D. Gernand, Jeffery A. Goldstein, Delia Mwinyelle, Leena Mithal, James Z. Wang</strong></p>
<p>Pathological examination of the placenta is an effective method for detecting and mitigating health risks associated with childbirth. Recent advancements in AI have enabled the use of photographs of the placenta and pathology reports for detecting and classifying signs of childbirth-related pathologies. However, existing automated methods are computationally extensive, which limits their deployability. We propose two modifications to vision-language contrastive learning (VLC) frameworks to enhance their accuracy and efficiency: (1) text-anchored vision-language contrastive knowledge distillation (VLCD)-a new knowledge distillation strategy for medical VLC pretraining, and (2) unsupervised predistillation using a large natural images dataset for improved initialization. Our approach distills efficient neural networks that match or surpass the teacher model in performance while achieving model compression and acceleration. Our results showcase the value of unsupervised predistillation in improving the performance and robustness of our approach, specifically for lower-quality images. VLCD serves as an effective way to improve the efficiency and deployability of medical VLC approaches, making AI-based healthcare solutions more accessible, especially in resource-constrained environments. </p>
<blockquote>
<p>胎盘病理检查是检测和缓解与分娩相关的健康风险的有效方法。最近人工智能的进步使得能够使用胎盘照片和病理报告来检测和分类与分娩相关的病理迹象。然而，现有的自动化计算方法计算量大，限制了其部署能力。我们针对视觉语言对比学习（VLC）框架提出了两项改进，以提高其准确性和效率：（1）文本锚定的视觉语言对比知识蒸馏（VLCD）——一种用于医学VLC预训练的新知识蒸馏策略；（2）使用大型自然图像数据集进行无监督预蒸馏，以改进初始化。我们的方法提炼了高效的神经网络，在性能上达到或超越了教师模型的性能，同时实现了模型压缩和加速。我们的结果展示了无监督预蒸馏在提高我们方法的性能和稳健性方面的价值，尤其是针对低质量图像。VLCD是提高医学VLC方法效率和部署能力的有效途径，使基于人工智能的卫生解决方案更加容易获得，特别是在资源受限的环境中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02229v1">PDF</a> Proceedings of the 9th International Workshop on Health Intelligence,   in conjunction with the Annual AAAI Conference on Artificial Intelligence,   Philadelphia, Pennsylvania, March 2025</p>
<p><strong>摘要</strong><br>     病理胎盘检查是检测和缓解分娩健康风险的有效方法。最近人工智能的进步使得可以使用胎盘照片和病理报告来检测和分类与分娩相关的病理迹象。然而，现有的自动化方法在计算上非常密集，限制了其部署能力。本文提出对视觉语言对比学习（VLC）框架的两种改进方法，以提高其准确性和效率：（1）文本锚定的视觉语言对比知识蒸馏（VLCD）-一种新的用于医学VLC预训练的知识蒸馏策略；（2）使用大量自然图像数据集进行无监督预蒸馏，以改善初始化。本文的方法蒸馏出高效神经网络，在性能上匹配或超越教师模型，同时实现模型压缩和加速。结果展示了无监督预蒸馏在提高性能和稳健性方面的价值，尤其对于低质量图像。VLCD是提高医学VLC方法效率和部署能力的有效途径，使得基于人工智能的卫生解决方案更加易于获取，特别是在资源受限的环境中。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>病理胎盘检查是检测分娩健康风险的有效方法。</li>
<li>人工智能在胎盘病理学中的应用已经取得了进展，但现有方法在计算上较为密集。</li>
<li>本文提出了两种改进视觉语言对比学习（VLC）的方法以增强其效率和准确性。</li>
<li>文本锚定的视觉语言对比知识蒸馏（VLCD）是一种新的医学VLC预训练知识蒸馏策略。</li>
<li>通过使用大量自然图像数据集进行无监督预蒸馏，改善了模型的初始化并提高了性能。</li>
<li>方法成功实现了模型压缩和加速，提高了教师模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02229">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f85e68b504d45d35f6fa8b02eb78067.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a5b8925234eed1ec44f0c0a6cbcf4fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c555b432025b65c78064e1a10476d5ce.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Ridgeformer-Mutli-Stage-Contrastive-Training-For-Fine-grained-Cross-Domain-Fingerprint-Recognition"><a href="#Ridgeformer-Mutli-Stage-Contrastive-Training-For-Fine-grained-Cross-Domain-Fingerprint-Recognition" class="headerlink" title="Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained   Cross-Domain Fingerprint Recognition"></a>Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained   Cross-Domain Fingerprint Recognition</h2><p><strong>Authors:Shubham Pandey, Bhavin Jawade, Srirangaraj Setlur</strong></p>
<p>The increasing demand for hygienic and portable biometric systems has underscored the critical need for advancements in contactless fingerprint recognition. Despite its potential, this technology faces notable challenges, including out-of-focus image acquisition, reduced contrast between fingerprint ridges and valleys, variations in finger positioning, and perspective distortion. These factors significantly hinder the accuracy and reliability of contactless fingerprint matching. To address these issues, we propose a novel multi-stage transformer-based contactless fingerprint matching approach that first captures global spatial features and subsequently refines localized feature alignment across fingerprint samples. By employing a hierarchical feature extraction and matching pipeline, our method ensures fine-grained, cross-sample alignment while maintaining the robustness of global feature representation. We perform extensive evaluations on publicly available datasets such as HKPolyU and RidgeBase under different evaluation protocols, such as contactless-to-contact matching and contactless-to-contactless matching and demonstrate that our proposed approach outperforms existing methods, including COTS solutions. </p>
<blockquote>
<p>随着对卫生和便携生物识别系统需求的不断增加，无接触式指纹识别技术的进展至关重要。尽管该技术具有潜力，但它面临着一些显著挑战，包括图像获取失焦、指纹脊和谷之间对比度降低、手指定位变化以及透视畸变。这些因素严重阻碍了无接触指纹匹配的准确性和可靠性。为了解决这些问题，我们提出了一种基于多阶段变压器架构的无接触指纹匹配新方法。该方法首先捕获全局空间特征，随后对指纹样本中的局部特征进行精细对齐。通过采用分层特征提取和匹配管道，我们的方法确保了跨样本的精细粒度对齐，同时保持了全局特征表示的稳健性。我们在HKPolyU和RidgeBase等公开数据集上进行了广泛评估，根据不同的评估协议（如非接触式与接触式匹配和非接触式与非接触式匹配）进行了实验，结果表明我们提出的方法优于现有方法，包括现成的商业技术解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01806v1">PDF</a> Accepted to IEEE International Conference on Image Processing 2025</p>
<p><strong>Summary</strong><br>     随着对卫生和便携生物识别系统需求的增加，无接触指纹识别的技术挑战日益凸显。为解决图像采集失焦、指纹脊谷对比度降低、手指定位变化和透视畸变等问题，我们提出了一种基于多阶段Transformer的无接触指纹匹配新方法。该方法采用分层特征提取和匹配管道，确保精细的跨样本对齐，同时保持全局特征表示的稳健性。在HKPolyU和RidgeBase等公开数据集上进行的广泛评估表明，该方法在接触式和无接触匹配协议上均优于现有方法，包括商业解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>无接触指纹识别技术面临图像采集失焦等挑战。</li>
<li>指纹脊谷对比度降低影响识别准确性。</li>
<li>手指定位变化和透视畸变对接触式指纹识别带来困扰。</li>
<li>提出了一种基于多阶段Transformer的新方法，用于无接触指纹匹配。</li>
<li>采用分层特征提取和匹配管道，确保精细跨样本对齐。</li>
<li>方法在公开数据集上的性能优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01806">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c749b47c7e5224e45a188c0c080da63c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd28defcfbfccb96c5558fe10a2b4550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36857da777cd4c6d2642613c9f7d575e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31819f220c404b9e3069454ae0c9c66b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Contra4-Evaluating-Contrastive-Cross-Modal-Reasoning-in-Audio-Video-Image-and-3D"><a href="#Contra4-Evaluating-Contrastive-Cross-Modal-Reasoning-in-Audio-Video-Image-and-3D" class="headerlink" title="Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video,   Image, and 3D"></a>Contra4: Evaluating Contrastive Cross-Modal Reasoning in Audio, Video,   Image, and 3D</h2><p><strong>Authors:Artemis Panagopoulou, Le Xue, Honglu Zhou, silvio savarese, Ran Xu, Caiming Xiong, Chris Callison-Burch, Mark Yatskar, Juan Carlos Niebles</strong></p>
<p>Real-world decision-making often begins with identifying which modality contains the most relevant information for a given query. While recent multimodal models have made impressive progress in processing diverse inputs, it remains unclear whether they can reason contrastively across multiple modalities to select the one that best satisfies a natural language prompt. We argue this capability is foundational, especially in retrieval-augmented and decision-time contexts, where systems must evaluate multiple signals and identify which one conveys the relevant information. To evaluate this skill, we introduce Contra4, a dataset for contrastive cross-modal reasoning across four modalities: image, audio, video, and 3D. Each example presents a natural language question alongside multiple candidate modality instances, and the model must select the one that semantically aligns with the prompt. Contra4 combines human-annotated captions with a mixture-of-models round-trip-consistency filter to ensure high-quality supervision, resulting in 174k training examples and a manually verified test set of 2.3k samples. While task-specific fine-tuning improves performance by 56% relative to baseline, state-of-the-art models still achieve only 56% accuracy overall and 42% in four-modality settings, underscoring a significant limitation in current multimodal models. </p>
<blockquote>
<p>现实世界中的决策通常始于确定哪种模态对于给定查询包含最相关的信息。虽然最近的多媒体模型在处理各种输入方面取得了令人印象深刻的进展，但尚不清楚它们是否能够在多个模态之间进行对比推理，以选择最能满足自然语言提示的那一个。我们认为这种能力是基础性的，特别是在增强检索和决策时间上下文中，系统必须评估多种信号并确定哪一种传达了相关信息。为了评估这种能力，我们推出了Contra4，这是一个用于四模态对比跨模态推理的数据集：图像、音频、视频和3D。每个示例都提供了一个自然语言问题以及多个候选模态实例，模型必须选择语义上与提示对齐的那一个。Contra4结合了人类注释的标题和混合模型的往返一致性过滤器，以确保高质量的监督，从而产生17.4万份训练样本和经过人工验证的2300个样本测试集。与基线相比，特定任务的微调将性能提高了56%，而最先进的模型总体准确率仅为56%，在四模态设置下准确率为42%，这突显了当前多媒体模型的重大局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01275v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了多模态决策制定的重要性，特别是在检索增强和决策时间上下文中。文章指出，尽管近期多模态模型在处理多样化输入方面取得了显著进展，但在对比跨多个模态进行推理以选择最符合自然语言提示的模态方面仍存在不足。为评估这一技能，文章引入了Contra4数据集，包含图像、音频、视频和3D四种模态的对比跨模态推理。该数据集通过人类注释的标题和混合模型的往返一致性过滤器确保高质量监督。尽管特定任务微调提高了性能，但现有模型的总体准确率仍然较低，突显了当前多模态模型的一个重大局限。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章中强调了多模态决策制定的重要性，特别是在需要根据不同模态的信息进行选择的情境下。</li>
<li>现有的多模态模型在对比跨模态推理方面存在不足，无法选择最符合自然语言提示的模态。</li>
<li>引入了一个名为Contra4的新数据集，用于评估跨图像、音频、视频和3D四种模态的对比推理能力。</li>
<li>Contra4数据集通过结合人类注释和混合模型的往返一致性过滤器来确保高质量监督。</li>
<li>数据集包含17.4万份训练样本和经过人工验证的2300份测试样本。</li>
<li>任务特定微调可以改善性能，但现有模型的总体准确率仍然较低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01275">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-922c958dc0285996be56c91b7240e7a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5519fd18407786cd7e76489e87e9e414.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f13e1a3ac321cbde82c97ed86a3dc099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f9b123f89b0353f7d4c7e900816483d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ECP-Mamba-An-Efficient-Multi-scale-Self-supervised-Contrastive-Learning-Method-with-State-Space-Model-for-PolSAR-Image-Classification"><a href="#ECP-Mamba-An-Efficient-Multi-scale-Self-supervised-Contrastive-Learning-Method-with-State-Space-Model-for-PolSAR-Image-Classification" class="headerlink" title="ECP-Mamba: An Efficient Multi-scale Self-supervised Contrastive Learning   Method with State Space Model for PolSAR Image Classification"></a>ECP-Mamba: An Efficient Multi-scale Self-supervised Contrastive Learning   Method with State Space Model for PolSAR Image Classification</h2><p><strong>Authors:Zuzheng Kuang, Haixia Bi, Chen Xu, Jian Sun</strong></p>
<p>Recently, polarimetric synthetic aperture radar (PolSAR) image classification has been greatly promoted by deep neural networks. However,current deep learning-based PolSAR classification methods encounter difficulties due to its dependence on extensive labeled data and the computational inefficiency of architectures like Transformers. This paper presents ECP-Mamba, an efficient framework integrating multi-scale self-supervised contrastive learning with a state space model (SSM) backbone. Specifically, ECP-Mamba addresses annotation scarcity through a multi-scale predictive pretext task based on local-to-global feature correspondences, which uses a simplified self-distillation paradigm without negative sample pairs. To enhance computational efficiency,the Mamba architecture (a selective SSM) is first tailored for pixel-wise PolSAR classification task by designing a spiral scan strategy. This strategy prioritizes causally relevant features near the central pixel, leveraging the localized nature of pixel-wise classification tasks. Additionally, the lightweight Cross Mamba module is proposed to facilitates complementary multi-scale feature interaction with minimal overhead. Extensive experiments across four benchmark datasets demonstrate ECP-Mamba’s effectiveness in balancing high accuracy with resource efficiency. On the Flevoland 1989 dataset, ECP-Mamba achieves state-of-the-art performance with an overall accuracy of 99.70%, average accuracy of 99.64% and Kappa coefficient of 99.62e-2. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/HaixiaBi1982/ECP_Mamba">https://github.com/HaixiaBi1982/ECP_Mamba</a>. </p>
<blockquote>
<p>近期，极化合成孔径雷达（PolSAR）图像分类在深度神经网络的大力推动下取得了很大进展。然而，目前基于深度学习的PolSAR分类方法面临着依赖大量标记数据和Transformer架构计算效率不高的困难。本文提出了ECP-Mamba，一个集成了多尺度自监督对比学习与状态空间模型（SSM）主干的高效框架。具体来说，ECP-Mamba通过基于局部到全局特征对应的多尺度预测伪装任务来解决标注稀缺问题，该任务使用简化的自蒸馏范式而不使用负样本对。为提高计算效率，Mamba架构（一种选择性SSM）首先针对像素级的PolSAR分类任务进行定制，通过设计螺旋扫描策略来实现像素级的分类任务。该策略优先利用中央像素附近因果相关的特征，利用像素级分类任务的局部性。此外，还提出了轻量级的Cross Mamba模块，以促进多尺度特征的互补交互，并尽量减少开销。在四个基准数据集上的广泛实验表明，ECP-Mamba在平衡高准确性与资源效率方面非常有效。在Flevoland 1989数据集上，ECP-Mamba取得了最新技术成果，总体精度达到99.70%，平均精度为99.64%，Kappa系数为99.62e-2。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/HaixiaBi1982/ECP_Mamba%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/HaixiaBi1982/ECP_Mamba上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01040v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于极坐标合成孔径雷达（PolSAR）图像分类问题，当前深度学习的方法依赖于大量标注数据且计算效率低下。本文提出ECP-Mamba框架，通过多尺度自监督对比学习与状态空间模型（SSM）的结合来解决这些问题。通过基于局部到全局特征对应的多尺度预测预训练任务来解决标注稀缺问题，采用简化的自蒸馏范式，无需负样本对。为提高计算效率，Mamba架构首次为像素级的PolSAR分类任务量身定制，通过螺旋扫描策略优先处理与中心像素相关的特征。此外，还提出了轻量级的Cross Mamba模块，以促进多尺度特征的互补交互。在四个基准数据集上的实验表明，ECP-Mamba在高精度和资源效率之间取得了平衡，特别是在Flevoland 1989数据集上取得了卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ECP-Mamba结合了多尺度自监督对比学习与状态空间模型（SSM），为PolSAR图像分类提供了一种高效框架。</li>
<li>通过基于局部到全局特征对应的多尺度预测预训练任务解决标注稀缺问题。</li>
<li>采用简化的自蒸馏范式，无需负样本对，提高了模型的实用性。</li>
<li>Mamba架构为像素级的PolSAR分类任务量身定制，通过螺旋扫描策略处理相关特征，提高了计算效率。</li>
<li>提出了轻量级的Cross Mamba模块，促进多尺度特征的互补交互。</li>
<li>ECP-Mamba在四个基准数据集上实现了卓越的性能平衡，特别是在Flevoland 1989数据集上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01040">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d14f45f11193783ed0dab14f3f9e7feb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aee241447328064935d59ccbd91bdcee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d480fa71c23f11d510b58189ced7fbe4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Symmetrical-Visual-Contrastive-Optimization-Aligning-Vision-Language-Models-with-Minimal-Contrastive-Images"><a href="#Symmetrical-Visual-Contrastive-Optimization-Aligning-Vision-Language-Models-with-Minimal-Contrastive-Images" class="headerlink" title="Symmetrical Visual Contrastive Optimization: Aligning Vision-Language   Models with Minimal Contrastive Images"></a>Symmetrical Visual Contrastive Optimization: Aligning Vision-Language   Models with Minimal Contrastive Images</h2><p><strong>Authors:Shengguang Wu, Fan-Yun Sun, Kaiyue Wen, Nick Haber</strong></p>
<p>Recent studies have shown that Large Vision-Language Models (VLMs) tend to neglect image content and over-rely on language-model priors, resulting in errors in visually grounded tasks and hallucinations. We hypothesize that this issue arises because existing VLMs are not explicitly trained to generate texts that are accurately grounded in fine-grained image details. To enhance visual feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive Optimization), a novel finetuning objective that steers the model toward capturing important visual details and aligning them with corresponding text tokens. To further facilitate this detailed alignment, we introduce MVC, a paired image-text dataset built by automatically filtering and augmenting visual counterfactual data to challenge the model with hard contrastive cases involving Minimal Visual Contrasts. Experiments show that our method consistently improves VLM performance across diverse benchmarks covering various abilities and domains, achieving up to a 22% reduction in hallucinations, and significant gains in vision-centric and general tasks. Notably, these improvements become increasingly pronounced in benchmarks with higher visual dependency. In short, S-VCO offers a significant enhancement of VLM’s visually-dependent task performance while retaining or even improving the model’s general abilities. We opensource our code at <a target="_blank" rel="noopener" href="https://s-vco.github.io/">https://s-vco.github.io/</a> </p>
<blockquote>
<p>最近的研究表明，大型视觉语言模型（VLMs）往往忽视图像内容，过度依赖语言模型的先验知识，导致视觉定位任务出错和出现幻觉。我们假设这个问题出现的原因是现有的VLMs没有被明确地训练去生成准确基于精细图像内容的文本。为了增强VLM训练过程中的视觉反馈，我们提出了S-VCO（对称视觉对比优化），这是一种新的微调目标，引导模型捕捉重要的视觉细节，并将它们与相应的文本标记对齐。为了进一步促进这种详细的对齐，我们引入了MVC，这是一个通过自动过滤和增强视觉反事实数据构建的配对图像文本数据集，以具有最小视觉对比的困难对比案例来挑战模型。实验表明，我们的方法在不同的基准测试中始终提高了VLM的性能，涵盖了各种能力和领域，幻觉减少了高达22%，在视觉为中心的任务和一般任务上都取得了显著的进步。值得注意的是，在视觉依赖性较高的基准测试中，这些改进变得更加突出。简而言之，S-VCO在保留甚至提高模型一般能力的同时，显著提高了VLM对视觉依赖的任务性能。我们在<a target="_blank" rel="noopener" href="https://s-vco.github.io/%E5%BC%80%E6%BA%90%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://s-vco.github.io/开源了我们的代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13928v2">PDF</a> Accepted to ACL 2025 Main. Project Website: <a target="_blank" rel="noopener" href="https://s-vco.github.io/">https://s-vco.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文指出大型视觉语言模型（VLMs）在处理视觉任务时存在忽视图像内容、过度依赖语言模型先验的问题，导致错误和幻觉。为解决这一问题，提出S-VCO（对称视觉对比优化）方法，通过构建新的训练目标，增强模型捕捉图像细节的能力，并与文本进行对齐。同时引入MVC数据集，通过自动筛选和增强视觉对比数据，挑战模型在最小视觉对比情况下的表现。实验表明，该方法在不同基准测试中表现优越，减少了幻觉现象，显著提高了视觉任务表现。总的来说，S-VCO增强了VLM的视觉依赖性任务性能，同时保持或提高了模型的通用能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型视觉语言模型（VLMs）在处理视觉任务时存在忽视图像内容的问题。</li>
<li>VLMs过度依赖语言模型先验，导致错误和幻觉。</li>
<li>S-VCO方法通过构建新的训练目标，增强模型捕捉图像细节的能力。</li>
<li>S-VCO引入MVC数据集，通过自动筛选和增强视觉对比数据，提高模型表现。</li>
<li>实验表明S-VCO方法在不同基准测试中表现优越，减少了幻觉现象。</li>
<li>S-VCO增强了VLM的视觉依赖性任务性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13928">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dc883c2fc7bdb5635f0e2660875494e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01a83fee2bbc289b9a439ba9fc4487f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d1c09a1eac7c39d47e001e20a5d73aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a738c9950eb5275742eb7f2b7a4f49af.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Concept-Based-Explanations-and-Class-Contrasting"><a href="#Concept-Based-Explanations-and-Class-Contrasting" class="headerlink" title="Concept Based Explanations and Class Contrasting"></a>Concept Based Explanations and Class Contrasting</h2><p><strong>Authors:Rudolf Herdt, Daniel Otero Baguer</strong></p>
<p>Explaining deep neural networks is challenging, due to their large size and non-linearity. In this paper, we introduce a concept-based explanation method, in order to explain the prediction for an individual class, as well as contrasting any two classes, i.e. explain why the model predicts one class over the other. We test it on several openly available classification models trained on ImageNet1K. We perform both qualitative and quantitative tests. For example, for a ResNet50 model from pytorch model zoo, we can use the explanation for why the model predicts a class ‘A’ to automatically select four dataset crops where the model does not predict class ‘A’. The model then predicts class ‘A’ again for the newly combined image in 91.1% of the cases (works for 911 out of the 1000 classes). The code including an .ipynb example is available on github: <a target="_blank" rel="noopener" href="https://github.com/rherdt185/concept-based-explanations-and-class-contrasting">https://github.com/rherdt185/concept-based-explanations-and-class-contrasting</a> </p>
<blockquote>
<p>解释深度神经网络是一项具有挑战性的任务，因为其规模庞大且非线性。在本文中，我们介绍了一种基于概念的解释方法，旨在解释针对单个类别的预测，并对比任何两个类别，即解释模型为何会预测某一类别而非其他类别。我们在公开可用的多个在ImageNet1K上训练的分类模型上进行了测试。我们既进行定性测试又进行定量测试。例如，对于来自pytorch模型库的ResNet50模型，我们可以使用模型预测类别“A”的解释来自动选择四个数据集裁剪图像，其中模型不预测类别“A”。然后，对于新组合的图像，模型在91.1%的情况下再次预测类别“A”（在1000个类别中适用于911个类别）。包括一个.ipynb示例的代码可在github上找到：<a target="_blank" rel="noopener" href="https://github.com/rherdt185/concept-based-explanations-and-class-contrasting">https://github.com/rherdt185/concept-based-explanations-and-class-contrasting</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03422v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于概念的解释方法，用于解释深度神经网络对单个类别的预测，以及对比任何两个类别的预测。通过对多个公开可用的ImageNet1K分类模型进行测试，该方法既能进行定性测试，也能进行定量测试。例如，对于PyTorch模型库中的ResNet50模型，该方法可以解释模型预测某一类别（如类别A）的原因，并自动选择四个数据集裁剪图像，在这些图像上模型不预测类别A。然后，对于新组合的图像，模型在91.1%的情况下再次预测类别A（适用于1000个类别中的911个类别）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种基于概念的解释方法，用于解释深度神经网络对单个类别的预测以及对比任意两个类别的预测。</li>
<li>方法在多个公开可用的ImageNet1K分类模型上进行了测试。</li>
<li>提供了定性和定量测试。</li>
<li>对于ResNet50模型，可以通过解释模型预测某一类别（如类别A）的原因，自动选择数据集裁剪图像。</li>
<li>在新组合的图像上，模型在大部分情况下能够继续预测同一类别。</li>
<li>该方法包括一个可用的代码示例，可以在GitHub上找到：<a target="_blank" rel="noopener" href="https://github.com/rherdt185/concept-based-explanations-and-class-contrasting%E3%80%82">https://github.com/rherdt185/concept-based-explanations-and-class-contrasting。</a></li>
<li>此方法提供了一个深入了解模型决策过程的工具，可能有助于增强模型的可靠性和透明度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03422">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe24f4741ff249da12705e0b837c9981.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bc757bb50b837a9f834a37b37f456dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be994d0f91ae3397b01e3303b8c6e3a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c26bac2e7ce75ed22e991dd3ce84ef6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-786ea5e19d9131c4c2c3809c94514f13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d5437eda09b725204ea87274b97239d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c8b8e5d67d3f2226ac0a472d19fe2eb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">无监督/半监督/对比学习</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Speech/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-462ee0d85174a23ec13fc95abddc28ab.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech 方向最新论文已更新，请持续关注 Update in 2025-06-05  Towards a Japanese Full-duplex Spoken Dialogue System
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-86a497d6dab6790501ab71120aa120b0.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-06-05  Efficient Test-time Adaptive Object Detection via Sensitivity-Guided   Pruning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25370.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
