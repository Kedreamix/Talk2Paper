<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-971ee3a8f96ee3b15eaa632cec6e76c5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-05-æ›´æ–°"><a href="#2025-06-05-æ›´æ–°" class="headerlink" title="2025-06-05 æ›´æ–°"></a>2025-06-05 æ›´æ–°</h1><h2 id="Zero-Shot-Tree-Detection-and-Segmentation-from-Aerial-Forest-Imagery"><a href="#Zero-Shot-Tree-Detection-and-Segmentation-from-Aerial-Forest-Imagery" class="headerlink" title="Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery"></a>Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery</h2><p><strong>Authors:Michelle Chen, David Russell, Amritha Pallavoor, Derek Young, Jane Wu</strong></p>
<p>Large-scale delineation of individual trees from remote sensing imagery is crucial to the advancement of ecological research, particularly as climate change and other environmental factors rapidly transform forest landscapes across the world. Current RGB tree segmentation methods rely on training specialized machine learning models with labeled tree datasets. While these learning-based approaches can outperform manual data collection when accurate, the existing models still depend on training data thatâ€™s hard to scale. In this paper, we investigate the efficacy of using a state-of-the-art image segmentation model, Segment Anything Model 2 (SAM2), in a zero-shot manner for individual tree detection and segmentation. We evaluate a pretrained SAM2 model on two tasks in this domain: (1) zero-shot segmentation and (2) zero-shot transfer by using predictions from an existing tree detection model as prompts. Our results suggest that SAM2 not only has impressive generalization capabilities, but also can form a natural synergy with specialized methods trained on in-domain labeled data. We find that applying large pretrained models to problems in remote sensing is a promising avenue for future progress. We make our code available at: <a target="_blank" rel="noopener" href="https://github.com/open-forest-observatory/tree-detection-framework">https://github.com/open-forest-observatory/tree-detection-framework</a>. </p>
<blockquote>
<p>ä»é¥æ„Ÿå½±åƒå¤§è§„æ¨¡å‹¾ç”»å•æ£µæ ‘å¯¹äºç”Ÿæ€ç ”ç©¶çš„è¿›æ­¥è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯éšç€æ°”å€™å˜åŒ–å’Œå…¶ä»–ç¯å¢ƒå› ç´ è¿…é€Ÿæ”¹å˜å…¨çƒæ£®æ—æ™¯è§‚ã€‚å½“å‰çš„RGBæ ‘åˆ†å‰²æ–¹æ³•ä¾èµ–äºä½¿ç”¨å¸¦æœ‰æ ‡ç­¾çš„æ ‘æ•°æ®é›†è®­ç»ƒä¸“é—¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚è™½ç„¶è¿™äº›åŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨å‡†ç¡®æ—¶å¯ä»¥è¡¨ç°å‡ºè¶…è¶Šæ‰‹åŠ¨æ•°æ®æ”¶é›†çš„æ•ˆèƒ½ï¼Œä½†ç°æœ‰æ¨¡å‹ä»ç„¶ä¾èµ–äºéš¾ä»¥æ‰©å±•çš„è®­ç»ƒæ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†ä½¿ç”¨æœ€å…ˆè¿›çš„å›¾åƒåˆ†å‰²æ¨¡å‹â€”â€”Segment Anything Model 2ï¼ˆSAM2ï¼‰è¿›è¡Œé›¶æ ·æœ¬ä¸ªäººæ ‘æœ¨æ£€æµ‹å’Œåˆ†å‰²çš„æ•ˆç”¨ã€‚æˆ‘ä»¬è¯„ä¼°äº†é¢„è®­ç»ƒçš„SAM2æ¨¡å‹åœ¨è¯¥é¢†åŸŸçš„ä¸¤ä¸ªä»»åŠ¡ï¼šï¼ˆ1ï¼‰é›¶æ ·æœ¬åˆ†å‰²å’Œï¼ˆ2ï¼‰ä½¿ç”¨ç°æœ‰æ ‘æœ¨æ£€æµ‹æ¨¡å‹çš„é¢„æµ‹ç»“æœä½œä¸ºæç¤ºè¿›è¡Œé›¶æ ·æœ¬è¿ç§»ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒSAM2ä¸ä»…å…·æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸”å¯ä»¥ä¸åœ¨é¢†åŸŸæ ‡ç­¾æ•°æ®ä¸Šè®­ç»ƒçš„ä¸“ç”¨æ–¹æ³•å½¢æˆè‡ªç„¶çš„ååŒä½œç”¨ã€‚æˆ‘ä»¬å‘ç°å°†å¤§å‹é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨äºé¥æ„Ÿé—®é¢˜æ˜¯ä¸€ä¸ªå¾ˆæœ‰å‰é€”çš„æœªæ¥å‘å±•æ–¹å‘ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/open-forest-observatory/tree-detection-framework%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/open-forest-observatory/tree-detection-frameworkæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03114v1">PDF</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/open-forest-observatory/tree-detection-framework">https://github.com/open-forest-observatory/tree-detection-framework</a></p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§è§„æ¨¡é¥æ„Ÿå½±åƒä¸­ç²¾å‡†å‹¾ç”»å‡ºæ ‘æœ¨ä¸ªä½“å¯¹äºç”Ÿæ€ç ”ç©¶è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯éšç€æ°”å€™å˜åŒ–å’Œç¯å¢ƒå› ç´ å¿«é€Ÿæ”¹å˜å…¨çƒæ£®æ—æ™¯è§‚ã€‚å½“å‰RGBæ ‘åˆ†å‰²æ–¹æ³•ä¾èµ–äºä½¿ç”¨æ ‡è®°æ ‘æ•°æ®é›†è®­ç»ƒä¸“é—¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚è™½ç„¶è¿™äº›åŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§æ–¹é¢å¯ä»¥è¶…è¶Šæ‰‹åŠ¨æ•°æ®æ”¶é›†ï¼Œä½†ç°æœ‰æ¨¡å‹ä»ç„¶ä¾èµ–äºéš¾ä»¥æ‰©å±•çš„è®­ç»ƒæ•°æ®ã€‚æœ¬æ–‡è°ƒæŸ¥äº†ä½¿ç”¨æœ€æ–°å›¾åƒåˆ†å‰²æ¨¡å‹Segment Anything Model 2ï¼ˆSAM2ï¼‰è¿›è¡Œé›¶æ ·æœ¬ä¸ªäººæ ‘æœ¨æ£€æµ‹å’Œåˆ†å‰²çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šè¯„ä¼°äº†é¢„è®­ç»ƒçš„SAM2æ¨¡å‹ï¼šï¼ˆ1ï¼‰é›¶æ ·æœ¬åˆ†å‰²å’Œï¼ˆ2ï¼‰ä½¿ç”¨ç°æœ‰æ ‘æœ¨æ£€æµ‹æ¨¡å‹çš„é¢„æµ‹ç»“æœä½œä¸ºæç¤ºè¿›è¡Œé›¶æ ·æœ¬è¿ç§»ã€‚ç»“æœè¡¨æ˜ï¼ŒSAM2ä¸ä»…å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸”å¯ä»¥ä¸åœ¨åŸŸå†…æ ‡è®°æ•°æ®ä¸Šè®­ç»ƒçš„ä¸“é—¨æ–¹æ³•å½¢æˆè‡ªç„¶ååŒã€‚æˆ‘ä»¬å‘ç°å°†å¤§å‹é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨äºé¥æ„Ÿé—®é¢˜æ˜¯ä¸€ä¸ªå€¼å¾—æœªæ¥å…³æ³¨çš„é€”å¾„ã€‚æˆ‘ä»¬æä¾›çš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/open-forest-observatory/tree-detection-framework%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/open-forest-observatory/tree-detection-frameworkæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡ä»é¥æ„Ÿå½±åƒä¸­å‹¾ç”»æ ‘æœ¨å¯¹ç”Ÿæ€ç ”ç©¶è‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨æ°”å€™å˜åŒ–å½±å“æ£®æ—æ™¯è§‚çš„èƒŒæ™¯ä¸‹ã€‚</li>
<li>å½“å‰RGBæ ‘åˆ†å‰²æ–¹æ³•ä¾èµ–äºè®­ç»ƒæœ‰æ ‡ç­¾çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½†è¿™ç§æ–¹æ³•éš¾ä»¥æ‰©å±•ã€‚</li>
<li>Segment Anything Model 2 (SAM2)æ¨¡å‹åœ¨é›¶æ ·æœ¬ä¸ªäººæ ‘æœ¨æ£€æµ‹å’Œåˆ†å‰²ä¸Šè¡¨ç°å‡ºè‰¯å¥½æ•ˆæœã€‚</li>
<li>SAM2æ¨¡å‹å…·æœ‰æ˜¾è‘—æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶èƒ½ä¸é’ˆå¯¹ç‰¹å®šæ•°æ®è®­ç»ƒçš„ä¸“é—¨æ–¹æ³•ååŒå·¥ä½œã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„çš„å¤§å‹æ¨¡å‹å¤„ç†é¥æ„Ÿé—®é¢˜æ˜¯æœªæ¥ç ”ç©¶çš„ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚</li>
<li>æœ¬æ–‡è¯„ä¼°äº†ä¸¤ç§ä»»åŠ¡ï¼šé›¶æ ·æœ¬åˆ†å‰²å’Œé›¶æ ·æœ¬è¿ç§»ï¼Œåè€…é€šè¿‡ä½¿ç”¨ç°æœ‰æ ‘æœ¨æ£€æµ‹æ¨¡å‹çš„é¢„æµ‹ç»“æœä½œä¸ºæç¤ºæ¥å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d946d8057ea6e312be171578e559003.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c6bef55ee6ef5a27b4c5c99ebbf62de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbff572a3e5dca57ccc9fd6549dbe6ec.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Self-Prompting-SAM-A-Prompt-Free-Medical-Image-Segmentation-Framework"><a href="#Hierarchical-Self-Prompting-SAM-A-Prompt-Free-Medical-Image-Segmentation-Framework" class="headerlink" title="Hierarchical Self-Prompting SAM: A Prompt-Free Medical Image   Segmentation Framework"></a>Hierarchical Self-Prompting SAM: A Prompt-Free Medical Image   Segmentation Framework</h2><p><strong>Authors:Mengmeng Zhang, Xingyuan Dai, Yicheng Sun, Jing Wang, Yueyang Yao, Xiaoyan Gong, Fuze Cong, Feiyue Wang, Yisheng Lv</strong></p>
<p>Although the Segment Anything Model (SAM) is highly effective in natural image segmentation, it requires dependencies on prompts, which limits its applicability to medical imaging where manual prompts are often unavailable. Existing efforts to fine-tune SAM for medical segmentation typically struggle to remove this dependency. We propose Hierarchical Self-Prompting SAM (HSP-SAM), a novel self-prompting framework that enables SAM to achieve strong performance in prompt-free medical image segmentation. Unlike previous self-prompting methods that remain limited to positional prompts similar to vanilla SAM, we are the first to introduce learning abstract prompts during the self-prompting process. This simple and intuitive self-prompting framework achieves superior performance on classic segmentation tasks such as polyp and skin lesion segmentation, while maintaining robustness across diverse medical imaging modalities. Furthermore, it exhibits strong generalization to unseen datasets, achieving improvements of up to 14.04% over previous state-of-the-art methods on some challenging benchmarks. These results suggest that abstract prompts encapsulate richer and higher-dimensional semantic information compared to positional prompts, thereby enhancing the modelâ€™s robustness and generalization performance. All models and codes will be released upon acceptance. </p>
<blockquote>
<p>å°½ç®¡Segment Anything Modelï¼ˆSAMï¼‰åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²ä¸­æ•ˆæœæ˜¾è‘—ï¼Œä½†å®ƒä¾èµ–äºæç¤ºï¼Œè¿™åœ¨æ‰‹åŠ¨æç¤ºé€šå¸¸ä¸å¯ç”¨çš„åŒ»å­¦æˆåƒä¸­é™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚ç°æœ‰çš„é’ˆå¯¹åŒ»å­¦åˆ†å‰²çš„SAMå¾®è°ƒåŠªåŠ›é€šå¸¸éš¾ä»¥æ¶ˆé™¤è¿™ç§ä¾èµ–ã€‚æˆ‘ä»¬æå‡ºäº†åˆ†å±‚è‡ªæç¤ºSAMï¼ˆHSP-SAMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è‡ªæç¤ºæ¡†æ¶ï¼Œä½¿SAMåœ¨æ— æç¤ºåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚ä¸ä»¥å¾€ä»…é™äºä½ç½®æç¤ºçš„è‡ªæç¤ºæ–¹æ³•ï¼ˆç±»ä¼¼äºæ™®é€šSAMï¼‰ä¸åŒï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªåœ¨è‡ªæç¤ºè¿‡ç¨‹ä¸­å¼•å…¥æŠ½è±¡æç¤ºçš„ã€‚è¿™ç§ç®€å•ç›´è§‚çš„è‡ªæç¤ºæ¡†æ¶åœ¨ç»å…¸åˆ†å‰²ä»»åŠ¡ï¼ˆå¦‚æ¯è‚‰å’Œçš®è‚¤ç—…å˜åˆ†å‰²ï¼‰ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸­ä¿æŒäº†ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æœªè§æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä¸€äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¾ƒä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•æé«˜äº†é«˜è¾¾14.04%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œä¸ä½ç½®æç¤ºç›¸æ¯”ï¼ŒæŠ½è±¡æç¤ºå°è£…äº†æ›´ä¸°å¯Œã€æ›´é«˜ç»´åº¦çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºäº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œæ³›åŒ–æ€§èƒ½ã€‚æ‰€æœ‰æ¨¡å‹å’Œä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02854v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºSegment Anything Modelï¼ˆSAMï¼‰åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²ä¸­çš„å‡ºè‰²è¡¨ç°ï¼Œä½†å…¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²åº”ç”¨ä¸­å—é™äºéœ€è¦æç¤ºçš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚è‡ªæç¤ºæ¡†æ¶HSP-SAMï¼Œä½¿SAMåœ¨æ— æç¤ºåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ã€‚HSP-SAMå¼•å…¥æŠ½è±¡æç¤ºå­¦ä¹ ï¼Œå®ç°è¶…è¶Šç»å…¸åˆ†å‰²ä»»åŠ¡å¦‚æ¯è‚‰å’Œçš®è‚¤ç—…å˜åˆ†å‰²çš„ä¼˜å¼‚è¡¨ç°ï¼ŒåŒæ—¶é€‚ç”¨äºå¤šç§åŒ»å­¦å½±åƒæ¨¡æ€ã€‚å…¶å±•ç°å‡ºå¯¹æœªè§æ•°æ®é›†çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä¸€äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­è¾ƒå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•æå‡äº†æœ€å¤šè¾¾14.04%ã€‚è¿™äº›ç»“æœæš—ç¤ºæŠ½è±¡æç¤ºåŒ…å«æ›´ä¸°å¯Œå’Œé«˜ç»´åº¦çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œæ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HSP-SAMæ˜¯ä¸€ä¸ªæ–°é¢–çš„åˆ†å±‚è‡ªæç¤ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Segment Anything Modelåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¯¹æç¤ºçš„ä¾èµ–é—®é¢˜ã€‚</li>
<li>ä¸ç°æœ‰çš„è‡ªæç¤ºæ–¹æ³•ä¸åŒï¼ŒHSP-SAMé¦–æ¬¡å¼•å…¥äº†æŠ½è±¡æç¤ºå­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>HSP-SAMæ¡†æ¶è¶…è¶Šäº†ç»å…¸åˆ†å‰²ä»»åŠ¡å¦‚æ¯è‚‰å’Œçš®è‚¤ç—…å˜åˆ†å‰²çš„ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>HSP-SAMåœ¨å„ç§åŒ»å­¦å½±åƒæ¨¡æ€ä¸Šå±•ç°å‡ºç¨³å¥æ€§ã€‚</li>
<li>HSP-SAMå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯¹æœªè§æ•°æ®é›†è¡¨ç°è‰¯å¥½ã€‚</li>
<li>ä¸å…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒHSP-SAMåœ¨æŸäº›æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­æé«˜äº†æœ€å¤šè¾¾14.04%çš„æ€§èƒ½ã€‚</li>
<li>ç»“æœè¡¨æ˜ï¼ŒæŠ½è±¡æç¤ºè¾ƒä¼ ç»Ÿä½ç½®æç¤ºåŒ…å«äº†æ›´ä¸°å¯Œã€æ›´é«˜ç»´åº¦çš„è¯­ä¹‰ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ae849344b9fe849025b4d8b35284ea1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-022718f52859359e23ec2bfe88c5ea11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6997e920d446b16a978bab80d56d054d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9fa179abb1b6be53ca970f52a26b63e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multi-wavelength-analysis-of-FSRQ-B2-1348-30B-Constraints-on-the-jet-power"><a href="#Multi-wavelength-analysis-of-FSRQ-B2-1348-30B-Constraints-on-the-jet-power" class="headerlink" title="Multi-wavelength analysis of FSRQ B2 1348+30B: Constraints on the jet   power"></a>Multi-wavelength analysis of FSRQ B2 1348+30B: Constraints on the jet   power</h2><p><strong>Authors:Sajad Ahanger, Sunder Sahayanathan, Sitha K. Jagan, Shah Zahir, Naseer Iqbal</strong></p>
<p>We present 14.5-year multi-wavelength analysis of flat-spectrum radio quasar B2 1348+30B using Swift-UVOT, Swift-XRT, and Fermi-LAT observations. In the gamma-ray band, the 3 day bin lightcurve reveals two major flaring events on 2010-09-19 (55458 MJD) and 2022-05-26 (59725 MJD) detected at flux levels $(2.5\pm 0.5) \times 10^{-7},\rm{ph,cm^{-2},s^{-1}}$ and $(5.2\pm 0.6) \times 10^{-7},\rm{ph,cm^{-2},s^{-1}}$. The Bayesian block analysis of the flares suggested the variability timescale to be $\leq$ 3,day. To study the dynamic nature of the source, multi-wavelength spectrum was obtained for three flux states which includes the two flaring state and a relative low state. The $\gamma$-ray spectra of the source in all the states are well fitted by a power-law model with maximum photon energy &lt; 20 GeV. In X-ray, a power-law model can explain the flaring state spectra while a broken power-law with extremely hard high energy component was required to model the low flux state. This indicates the presence of the low energy cutoff in the Compton spectral component. A simple one-zone leptonic model involving synchrotron, synchrotron self Compton and external Compton mechanism can successfully reproduce the broad-band spectral energy distribution of all the flux states. The model parameters suggest significant increase in the jet Lorentz factor during the high flux states. Further, the best fit parameters are used to constrain the minimum energy of the emitting electron distribution from the hard high energy spectrum of the low flux state. This analysis was extended to draw limits on the kinetic power of the blazar jet and was compared with the Eddington luminosity of the central black hole. </p>
<blockquote>
<p>æˆ‘ä»¬åˆ©ç”¨Swift-UVOTã€Swift-XRTå’ŒFermi-LATè§‚æµ‹æ•°æ®ï¼Œå¯¹å¹³è°±å°„ç”µç±»æ˜Ÿä½“B2 1348+30Bè¿›è¡Œäº†é•¿è¾¾14.5å¹´çš„å¤šæ³¢é•¿åˆ†æã€‚åœ¨ä¼½é©¬å°„çº¿æ³¢æ®µï¼Œä¸ºæœŸ3å¤©çš„å…‰å˜æ›²çº¿æ˜¾ç¤ºï¼Œåœ¨å†å…ƒæ—¶åˆ»55458 MJDï¼ˆå³2010å¹´9æœˆ19æ—¥ï¼‰å’Œå†å…ƒæ—¶åˆ»59725 MJDï¼ˆå³2022å¹´5æœˆ26æ—¥ï¼‰å‘ç”Ÿäº†ä¸¤æ¬¡ä¸»è¦è€€å‘äº‹ä»¶ï¼Œæµé‡æ°´å¹³åˆ†åˆ«ä¸ºï¼ˆ2.5Â±0.5ï¼‰Ã— 10^-7 ph cm^-2 s^-1å’Œï¼ˆ5.2Â±0.6ï¼‰Ã— 10^-7 ph cm^-2 s^-1ã€‚è€€å‘äº‹ä»¶çš„è´å¶æ–¯å—åˆ†æè¡¨æ˜å…¶å˜åŒ–æ—¶é—´å°ºåº¦ä¸è¶…è¿‡3å¤©ã€‚ä¸ºäº†ç ”ç©¶æºåŠ¨æ€ç‰¹æ€§ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸‰ç§æµé‡çŠ¶æ€ä¸‹çš„å¤šæ³¢é•¿å…‰è°±ï¼ŒåŒ…æ‹¬ä¸¤æ¬¡è€€å‘çŠ¶æ€å’Œä¸€æ¬¡ç›¸å¯¹ä½çŠ¶æ€ã€‚æºåœ¨æ‰€æœ‰çŠ¶æ€ä¸‹çš„ä¼½é©¬å°„çº¿å…‰è°±å‡å¯ç”¨å¹‚å¾‹æ¨¡å‹å¾ˆå¥½åœ°æ‹Ÿåˆï¼Œæœ€å¤§å…‰å­èƒ½é‡å°äº20 GeVã€‚åœ¨Xå°„çº¿æ³¢æ®µï¼Œå¹‚å¾‹æ¨¡å‹å¯ä»¥è§£é‡Šè€€å‘çŠ¶æ€çš„å…‰è°±ï¼Œè€Œå¯¹ä½æµé‡çŠ¶æ€åˆ™éœ€è¦ä¸€ä¸ªå…·æœ‰æç¡¬é«˜èƒ½åˆ†é‡çš„æˆªæ–­å¹‚å¾‹æ¨¡å‹æ¥æè¿°ã€‚è¿™è¡¨æ˜åº·æ™®é¡¿å…‰è°±åˆ†é‡ä¸­å­˜åœ¨ä½èƒ½æˆªæ–­ã€‚ä¸€ä¸ªæ¶‰åŠåŒæ­¥è¾å°„ã€åŒæ­¥è‡ªåº·æ™®é¡¿å’Œå¤–éƒ¨åº·æ™®é¡¿æœºåˆ¶çš„ç®€å•å•åŒºè½»å­æ¨¡å‹èƒ½å¤ŸæˆåŠŸåœ°å†ç°æ‰€æœ‰æµé‡çŠ¶æ€çš„å®½é¢‘è°±èƒ½é‡åˆ†å¸ƒã€‚æ¨¡å‹å‚æ•°è¡¨æ˜åœ¨é«˜æµé‡çŠ¶æ€ä¸‹æ´›ä¼¦å…¹å› å­æ˜¾è‘—å¢åŠ ã€‚æ­¤å¤–ï¼Œæœ€ä½³æ‹Ÿåˆå‚æ•°è¢«ç”¨æ¥é™åˆ¶ä½æµé‡çŠ¶æ€ç¡¬é«˜èƒ½è°±å‘å°„ç”µå­åˆ†å¸ƒçš„æœ€å°èƒ½é‡ã€‚è¿™ä¸€åˆ†æè¢«ç”¨æ¥å¯¹å–·æµçš„åŠ¨åŠ›å­¦åŠŸç‡è®¾ç½®é™åˆ¶ï¼Œå¹¶ä¸ä¸­å¿ƒé»‘æ´çš„è‰¾ä¸é¡¿å…‰åº¦è¿›è¡Œæ¯”è¾ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02821v1">PDF</a> 14 pages, 6 figures, 5 tables, Accepted for publication in JHEAP</p>
<p><strong>Summary</strong></p>
<p>åœ¨é•¿è¾¾14.5å¹´çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹å…·æœ‰å¹³å¦å…‰è°±çš„å°„ç”µç±»æ˜Ÿä½“B2 1348+30Bè¿›è¡Œäº†å¤šæ³¢é•¿åˆ†æï¼Œä½¿ç”¨äº†Swift-UVOTã€Swift-XRTå’ŒFermi-LATè§‚æµ‹æ•°æ®ã€‚åœ¨ä¼½é©¬å°„çº¿æ³¢æ®µï¼Œå‘ç°äº†ä¸¤ä¸ªä¸»è¦è€€æ–‘äº‹ä»¶ï¼Œå…¶å…‰å˜æ›²çº¿æ­ç¤ºäº†è€€æ–‘æ´»åŠ¨çš„åŠ¨æ€ç‰¹å¾ã€‚å¤šæ³¢é•¿å…‰è°±åˆ†æè¡¨æ˜ï¼Œæºåœ¨ä¸åŒæµé‡çŠ¶æ€ä¸‹çš„å…‰è°±ç‰¹å¾æœ‰æ‰€ä¸åŒï¼Œè¿™æ”¯æŒäº†ä½¿ç”¨ç®€å•çš„ä¸€åŒºè±æ™®é¡¿æ¨¡å‹æ¥è§£é‡Šå…¶è°±èƒ½é‡åˆ†å¸ƒçš„è§‚ç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥åˆ†æè¿˜æä¾›äº†å¯¹å–·æµæ´›ä¼¦å…¹å› å­å’Œç”µå­åˆ†å¸ƒæœ€å°èƒ½é‡çš„äº†è§£ï¼Œå¹¶è¿›ä¸€æ­¥å¯¹ç±»æ˜Ÿä½“å–·æµçš„åŠ¨åŠ›è¿›è¡Œäº†é™åˆ¶å’Œæ¯”è¾ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨Swift-UVOTã€Swift-XRTå’ŒFermi-LATè§‚æµ‹æ•°æ®å¯¹å¹³è°±å°„ç”µç±»æ˜Ÿä½“B2 1348+30Bè¿›è¡Œäº†é•¿è¾¾14.5å¹´çš„å¤šæ³¢é•¿åˆ†æã€‚</li>
<li>åœ¨ä¼½é©¬å°„çº¿æ³¢æ®µå‘ç°ä¸¤ä¸ªä¸»è¦è€€æ–‘äº‹ä»¶ï¼Œæ­ç¤ºäº†è€€æ–‘æ´»åŠ¨çš„åŠ¨æ€ç‰¹å¾ã€‚</li>
<li>é€šè¿‡å¤šæ³¢é•¿å…‰è°±åˆ†æå‘ç°ä¸åŒæµé‡çŠ¶æ€ä¸‹çš„æºå…·æœ‰ä¸åŒçš„å…‰è°±ç‰¹æ€§ã€‚</li>
<li>ç®€å•çš„ä¸€åŒºè±æ™®é¡¿æ¨¡å‹æˆåŠŸåœ°å†ç°äº†æ‰€æœ‰æµé‡çŠ¶æ€çš„è°±èƒ½é‡åˆ†å¸ƒã€‚</li>
<li>æºåœ¨ä½æµé‡çŠ¶æ€ä¸‹çš„é«˜èƒ½è°±éœ€è¦ç¡¬çš„é«˜èƒ½æˆåˆ†æ¥è§£é‡Šï¼Œæš—ç¤ºäº†Comptonè°±æˆåˆ†çš„ä½èƒ½æˆªæ–­çš„å­˜åœ¨ã€‚</li>
<li>æºçš„å–·æµæ´›ä¼¦å…¹å› å­åœ¨é«˜æµé‡çŠ¶æ€ä¸‹æ˜¾è‘—å¢åŠ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af60d2267558211faae5388094fc6118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca028bfeac51c119c1a820fe01cf0eef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f48ee11a73a2b1247c7508ca1328c68.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25f298ff1d558e30a075296d26325a72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b60961eb680596b8828bc2e24de779f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4aea9b9747bb526c961e5765bf41d801.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c8c6f1898efa95c62fb33cb438440e2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="On-the-influence-of-language-similarity-in-non-target-speaker-verification-trials"><a href="#On-the-influence-of-language-similarity-in-non-target-speaker-verification-trials" class="headerlink" title="On the influence of language similarity in non-target speaker   verification trials"></a>On the influence of language similarity in non-target speaker   verification trials</h2><p><strong>Authors:Paul M. Reuter, Michael Jessen</strong></p>
<p>In this paper, we investigate the influence of language similarity in cross-lingual non-target speaker verification trials using a state-of-the-art speaker verification system, ECAPA-TDNN, trained on multilingual and monolingual variants of the VoxCeleb dataset. Our analysis of the score distribution patterns on multilingual Globalphone and LDC CTS reveals a clustering effect in speaker comparisons involving a training language, whereby the choice of comparison language only minimally impacts scores. Conversely, we observe a language similarity effect in trials involving languages not included in the training set of the speaker verification system, with scores correlating with language similarity measured by a language classification system, especially when using multilingual training data. </p>
<blockquote>
<p>æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶è¯­è¨€ç›¸ä¼¼æ€§å¯¹ä½¿ç”¨æœ€å‰æ²¿è¯´è¯äººéªŒè¯ç³»ç»ŸECAPA-TDNNè¿›è¡Œçš„è·¨è¯­è¨€éç›®æ ‡è¯´è¯äººéªŒè¯è¯•éªŒçš„å½±å“ã€‚è¯¥ç³»ç»Ÿæ˜¯åœ¨VoxCelebæ•°æ®é›†çš„å¤šè¯­ç§å’Œå•è¯­ç§å˜ä½“ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚æˆ‘ä»¬å¯¹å¤šè¯­ç§Globalphoneå’ŒLDC CTSä¸Šçš„å¾—åˆ†åˆ†å¸ƒæ¨¡å¼è¿›è¡Œåˆ†æï¼Œå‘ç°åœ¨æ¶‰åŠè®­ç»ƒè¯­è¨€çš„è¯´è¯äººæ¯”è¾ƒä¸­å‡ºç°äº†èšç±»æ•ˆåº”ï¼Œæ‰€é€‰çš„æ¯”è¾ƒè¯­è¨€å¯¹å¾—åˆ†çš„å½±å“å¾®ä¹å…¶å¾®ã€‚ç›¸åï¼Œåœ¨æ¶‰åŠä¸åœ¨è¯´è¯äººéªŒè¯ç³»ç»Ÿè®­ç»ƒé›†ä¸­çš„è¯­è¨€çš„è¯•éªŒä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¯­è¨€ç›¸ä¼¼æ€§æ•ˆåº”ï¼Œå¾—åˆ†ä¸ç”±è¯­è¨€åˆ†ç±»ç³»ç»Ÿæµ‹å¾—çš„è¯­è¨€ç›¸ä¼¼æ€§ç›¸å…³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å¤šè¯­ç§è®­ç»ƒæ•°æ®æ—¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02777v1">PDF</a> accepted to Interspeech 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†è¯­è¨€ç›¸ä¼¼æ€§å¯¹è·¨è¯­ç§éç›®æ ‡è¯´è¯äººéªŒè¯è¯•éªŒçš„å½±å“ã€‚ç ”ç©¶ä½¿ç”¨äº†åŸºäºæœ€å‰æ²¿çš„è¯´è¯äººéªŒè¯ç³»ç»ŸECAPA-TDNNï¼Œè¯¥ç³»ç»Ÿåœ¨VoxCelebæ•°æ®é›†çš„å¤šè¯­ç§å’Œå•è¯­ç§å˜ä½“ä¸Šè¿›è¡Œè®­ç»ƒã€‚åˆ†æGlobalphoneå’ŒLDC CTSä¸Šçš„å¾—åˆ†åˆ†å¸ƒæ¨¡å¼æ˜¾ç¤ºï¼Œåœ¨æ¶‰åŠè®­ç»ƒè¯­è¨€çš„è¯´è¯äººæ¯”è¾ƒä¸­ï¼Œå¯¹æ¯”è¯­è¨€çš„é€‰æ‹©å¯¹å¾—åˆ†å½±å“ç”šå¾®ï¼Œä½†åœ¨æ¶‰åŠä¸åœ¨éªŒè¯ç³»ç»Ÿè®­ç»ƒé›†ä¸­çš„è¯­è¨€çš„è¯•éªŒä¸­ï¼Œè§‚å¯Ÿåˆ°è¯­è¨€ç›¸ä¼¼æ€§å¯¹å¾—åˆ†æœ‰æ˜¾è‘—å½±å“ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨å¤šè¯­ç§è®­ç»ƒæ•°æ®æ—¶ï¼Œå¾—åˆ†ä¸è¯­è¨€åˆ†ç±»ç³»ç»Ÿçš„è¯­è¨€ç›¸ä¼¼æ€§åº¦é‡ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äº†è·¨è¯­ç§éç›®æ ‡è¯´è¯äººéªŒè¯ä¸­è¯­è¨€ç›¸ä¼¼æ€§çš„å½±å“ã€‚</li>
<li>é‡‡ç”¨ECAPA-TDNNè¿™ä¸€å…ˆè¿›çš„è¯´è¯äººéªŒè¯ç³»ç»Ÿè¿›è¡Œåˆ†æã€‚</li>
<li>åœ¨æ¶‰åŠè®­ç»ƒè¯­è¨€çš„è¯´è¯äººæ¯”è¾ƒä¸­ï¼Œå¯¹æ¯”è¯­è¨€çš„é€‰æ‹©å¯¹å¾—åˆ†å½±å“è¾ƒå°ã€‚</li>
<li>åœ¨æ¶‰åŠéè®­ç»ƒè¯­è¨€æ—¶ï¼Œè§‚å¯Ÿåˆ°è¯­è¨€ç›¸ä¼¼æ€§å¯¹å¾—åˆ†æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>å¤šè¯­ç§è®­ç»ƒæ•°æ®åœ¨è¡¡é‡è¯­è¨€ç›¸ä¼¼æ€§æ—¶å°¤ä¸ºé‡è¦ã€‚</li>
<li>å¾—åˆ†åˆ†å¸ƒæ¨¡å¼åœ¨è·¨è¯­ç§éªŒè¯ä¸­å‘ˆç°å‡ºè¯­è¨€èšç±»æ•ˆåº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd1a35d36f9749436ffed39905ae1654.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d0ccb495624acc33b1fbc03580a9808.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e03fd61acd7d564052037beb3fb292ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45447514b8c763c1b5c747bcca7d47d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c4d46ef17729fd085fb81f3190a5c94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b9f17089fe935cc9a945020741f1c00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2bf8e22ad247b3134b073c5f872b648.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Simple-Good-Fast-Self-Supervised-World-Models-Free-of-Baggage"><a href="#Simple-Good-Fast-Self-Supervised-World-Models-Free-of-Baggage" class="headerlink" title="Simple, Good, Fast: Self-Supervised World Models Free of Baggage"></a>Simple, Good, Fast: Self-Supervised World Models Free of Baggage</h2><p><strong>Authors:Jan Robine, Marc HÃ¶ftmann, Stefan Harmeling</strong></p>
<p>What are the essential components of world models? How far do we get with world models that are not employing RNNs, transformers, discrete representations, and image reconstructions? This paper introduces SGF, a Simple, Good, and Fast world model that uses self-supervised representation learning, captures short-time dependencies through frame and action stacking, and enhances robustness against model errors through data augmentation. We extensively discuss SGFâ€™s connections to established world models, evaluate the building blocks in ablation studies, and demonstrate good performance through quantitative comparisons on the Atari 100k benchmark. </p>
<blockquote>
<p>ä¸–ç•Œæ¨¡å‹çš„å¿…è¦ç»„æˆéƒ¨åˆ†æ˜¯ä»€ä¹ˆï¼Ÿåœ¨ä¸ä½¿ç”¨RNNã€transformerã€ç¦»æ•£è¡¨ç¤ºå’Œå›¾åƒé‡å»ºçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬èƒ½èµ°å¤šè¿œï¼Ÿæœ¬æ–‡ä»‹ç»äº†SGFï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ã€è‰¯å¥½å’Œå¿«é€Ÿçš„ä¸–ç•Œæ¨¡å‹ï¼Œå®ƒé‡‡ç”¨è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œé€šè¿‡å¸§å’ŒåŠ¨ä½œå †å æ•æ‰çŸ­æœŸä¾èµ–æ€§ï¼Œå¹¶é€šè¿‡æ•°æ®å¢å¼ºæé«˜æ¨¡å‹è¯¯å·®çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬æ·±å…¥è®¨è®ºäº†SGFä¸ç°æœ‰ä¸–ç•Œæ¨¡å‹ä¹‹é—´çš„è”ç³»ï¼Œåœ¨æ¶ˆèç ”ç©¶ä¸­è¯„ä¼°äº†å„ä¸ªç»„æˆéƒ¨åˆ†ï¼Œå¹¶åœ¨Atari 100kåŸºå‡†æµ‹è¯•ä¸Šé€šè¿‡å®šé‡æ¯”è¾ƒå±•ç¤ºäº†è‰¯å¥½çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02612v1">PDF</a> Published as a conference paper at ICLR 2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/jrobine/sgf">https://github.com/jrobine/sgf</a></p>
<p><strong>Summary</strong><br>ä¸–ç•Œæ¨¡å‹çš„å¿…è¦ç»„æˆéƒ¨åˆ†æ˜¯ä»€ä¹ˆï¼Ÿä¸ä½¿ç”¨RNNsã€transformersã€ç¦»æ•£è¡¨ç¤ºå’Œå›¾åƒé‡å»ºçš„ä¸–ç•Œæ¨¡å‹èƒ½èµ°å¤šè¿œï¼Ÿæœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç®€å•ã€è‰¯å¥½å’Œå¿«é€Ÿçš„SGFä¸–ç•Œæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œé€šè¿‡å¸§å’ŒåŠ¨ä½œå †å æ•æ‰çŸ­æœŸä¾èµ–å…³ç³»ï¼Œå¹¶é€šè¿‡æ•°æ®å¢å¼ºæé«˜æ¨¡å‹è¯¯å·®çš„ç¨³å¥æ€§ã€‚æœ¬æ–‡å¹¿æ³›è®¨è®ºäº†SGFä¸ç°æœ‰ä¸–ç•Œæ¨¡å‹çš„å…³è”ï¼Œé€šè¿‡å±€éƒ¨æ¶ˆèç ”ç©¶è¯„ä¼°äº†æ„å»ºå—ï¼Œå¹¶åœ¨Atari 100kåŸºå‡†æµ‹è¯•ä¸­é€šè¿‡å®šé‡æ¯”è¾ƒå±•ç¤ºäº†è‰¯å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SGFæ˜¯ä¸€ç§ç®€å•ã€è‰¯å¥½å’Œå¿«é€Ÿçš„ä¸–ç•Œæ¨¡å‹ã€‚</li>
<li>SGFé‡‡ç”¨è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>SGFé€šè¿‡å¸§å’ŒåŠ¨ä½œå †å æ•æ‰çŸ­æœŸä¾èµ–å…³ç³»ã€‚</li>
<li>SGFé€šè¿‡æ•°æ®å¢å¼ºæé«˜æ¨¡å‹è¯¯å·®çš„ç¨³å¥æ€§ã€‚</li>
<li>SGFä¸ç°æœ‰ä¸–ç•Œæ¨¡å‹çš„å…³è”è¢«å¹¿æ³›è®¨è®ºã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯„ä¼°äº†SGFçš„æ„å»ºå—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1b2714fec78a342dd3bb8bb8d741af26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac5f5ab225dfe95ef8d95726393137bd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Insight-into-the-origin-of-multiwavelength-emissions-of-PKS-1510-089-through-modeling-12-SEDs-from-2008-to-2015"><a href="#Insight-into-the-origin-of-multiwavelength-emissions-of-PKS-1510-089-through-modeling-12-SEDs-from-2008-to-2015" class="headerlink" title="Insight into the origin of multiwavelength emissions of PKS 1510-089   through modeling 12 SEDs from 2008 to 2015"></a>Insight into the origin of multiwavelength emissions of PKS 1510-089   through modeling 12 SEDs from 2008 to 2015</h2><p><strong>Authors:Maichang Lei, Yuan Zheng, Jianfu Zhang, Yuhai Yuan, Jiancheng Wang</strong></p>
<p>PKS,1510$-$089 is one of the most peculiar sources among the FSRQs, exhibiting a notable big blue bump (BBB). This provides an unique opportunity to explore the coupling between the activity of the central engine and the relativistic jet, offering further insight into the origin of the multiwavelength emissions. To this end, we collected multiwavelength data spanning four periods from 2008 to 2015 and performed the spectral energy distribution (SED) modeling using a one-zone homogeneous leptonic model. In the model, a multichromatic accretion disk (AD) is used to fit the optical&#x2F;UV data sets, while the external radiation fields from the broad-line region (BLR) and dusty torus (DT) are properly considered to produce the high-energy $\gamma$-ray emissions. Our best fit to 12 SEDs yields the following results: (i) The innermost stable orbit ($R_{\rm ISO}$) of the AD is not stable but varies between $3,R_{\rm S}$ and $18,R_{\rm S}$ during these observations. (ii) The high-energy hump of the SED is well dominated by Compton scattering of the BLR photons, while the X-ray flux may be comprised of multiple radiation components. (iii) The $\gamma$-ray emitting regions are generally matter-dominated, with low magnetization, and are located beyond the BLR but within the DT. At such distance, the multiwavelength emissions are likely to originate from shock accelerations; (iv) For the energization of the relativistic jet, our study supports the Blandford$-$Znajek (BZ) mechanism, instead of the Blandford$-$Payne (BP) mechanism, as the latter fails to power the jet. </p>
<blockquote>
<p>PKS 1510-089æ˜¯FSRQsä¸­æœ€ç‰¹æ®Šçš„æºä¹‹ä¸€ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„å¤§è“åŒ…ï¼ˆBBBï¼‰ã€‚è¿™ä¸ºæ¢ç´¢ä¸­å¿ƒå‘åŠ¨æœºæ´»åŠ¨ä¸ç›¸å¯¹è®ºå–·æµä¹‹é—´çš„è€¦åˆæä¾›äº†ç‹¬ç‰¹çš„æœºä¼šï¼Œä¸ºè¿›ä¸€æ­¥æ·±å…¥äº†è§£å¤šæ³¢é•¿å‘å°„çš„èµ·æºæä¾›äº†å¯ç¤ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ”¶é›†äº†2008å¹´è‡³2015å¹´çš„å››æœŸå¤šæ³¢é•¿æ•°æ®ï¼Œå¹¶ä½¿ç”¨å•ä¸€åŒºåŸŸå‡åŒ€è½»å­æ¨¡å‹è¿›è¡Œäº†è°±èƒ½é‡åˆ†å¸ƒï¼ˆSEDï¼‰å»ºæ¨¡ã€‚åœ¨æ¨¡å‹ä¸­ï¼Œä½¿ç”¨å¤šè‰²å¸ç§¯ç›˜ï¼ˆADï¼‰æ¥æ‹Ÿåˆå…‰å­¦&#x2F;ç´«å¤–æ•°æ®é›†ï¼ŒåŒæ—¶é€‚å½“è€ƒè™‘äº†æ¥è‡ªå®½çº¿åŒºï¼ˆBLRï¼‰å’Œå°˜åŸƒç¯ï¼ˆDTï¼‰çš„å¤–éƒ¨è¾å°„åœºï¼Œä»¥äº§ç”Ÿé«˜èƒ½ä¼½é©¬å°„çº¿å‘å°„ã€‚æˆ‘ä»¬å¯¹12ä¸ªSEDçš„æœ€ä½³æ‹Ÿåˆå¾—å‡ºä»¥ä¸‹ç»“æœï¼šï¼ˆiï¼‰ADçš„æœ€å†…ç¨³å®šè½¨é“ï¼ˆRISOï¼‰ä¸ç¨³å®šï¼Œåœ¨è¿™äº›è§‚æµ‹æœŸé—´åœ¨3Rså’Œ18Rsä¹‹é—´å˜åŒ–ã€‚ï¼ˆiiï¼‰SEDçš„é«˜èƒ½å³°ä¸»è¦ç”±BLRå…‰å­çš„åº·æ™®é¡¿æ•£å°„å æ®ä¸»å¯¼åœ°ä½ï¼Œè€ŒXå°„çº¿æµé‡å¯èƒ½ç”±å¤šä¸ªè¾å°„æˆåˆ†ç»„æˆã€‚ï¼ˆiiiï¼‰ä¼½é©¬å°„çº¿å‘å°„åŒºåŸŸé€šå¸¸æ˜¯ç‰©è´¨ä¸»å¯¼çš„ï¼Œå…·æœ‰ä½ç£åŒ–å¼ºåº¦ï¼Œä½äºBLRä¹‹å¤–ä½†åœ¨DTä¹‹å†…ã€‚åœ¨è¿™æ ·çš„è·ç¦»ä¸‹ï¼Œå¤šæ³¢é•¿å‘å°„å¾ˆå¯èƒ½æºäºå†²å‡»åŠ é€Ÿï¼›ï¼ˆivï¼‰å…³äºç›¸å¯¹è®ºå–·æµçš„èƒ½é‡åŒ–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ”¯æŒBlandford-Znajekï¼ˆBZï¼‰æœºåˆ¶ï¼Œè€Œä¸æ˜¯Blandford-Payneï¼ˆBPï¼‰æœºåˆ¶ï¼Œå› ä¸ºåè€…æ— æ³•ä¸ºå–·æµæä¾›åŠ¨åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02500v1">PDF</a> 33 pages, 13 figure, 5 Tables. Accepted for publication in ApJ</p>
<p><strong>Summary</strong><br>    PKS 1510$-$089æ˜¯FSRQsä¸­æœ€å¥‡ç‰¹çš„æºä¹‹ä¸€ï¼Œå…·æœ‰æ˜¾è‘—çš„å¤§è“åŒ…ï¼ˆBBBï¼‰ã€‚é€šè¿‡å¯¹å…¶çš„ç ”ç©¶ï¼Œå¯ä»¥æ›´å¥½åœ°äº†è§£ä¸­å¿ƒå¼•æ“æ´»åŠ¨ä¸ç›¸å¯¹è®ºå–·æµçš„è€¦åˆå…³ç³»ï¼Œè¿›è€Œæ·±å…¥ç ”ç©¶å¤šæ³¢é•¿å‘å°„çš„èµ·æºã€‚ç ”ç©¶ä½¿ç”¨å•ä¸€åŒºåŸŸå‡åŒ€è±æ™®é¡¿æ¨¡å‹è¿›è¡Œè°±èƒ½é‡åˆ†å¸ƒï¼ˆSEDï¼‰å»ºæ¨¡ï¼Œå‘ç°ADçš„æœ€å†…ç¨³å®šè½¨é“ä¸ç¨³å®šï¼Œé«˜èƒ½é‡å‡¸èµ·ä¸»è¦ç”±BLRå…‰å­çš„åº·æ™®é¡¿æ•£å°„ä¸»å¯¼ï¼Œä¼½é©¬å°„çº¿å‘å°„åŒºåŸŸé€šå¸¸ç‰©è´¨å ä¸»å¯¼ï¼Œä¸”ä½äºBLRä¹‹å¤–ä½†DTä¹‹å†…ã€‚ç ”ç©¶æ”¯æŒBZæœºåˆ¶ä¸ºç›¸å¯¹è®ºå–·æµæä¾›èƒ½é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PKS 1510$-$089æ˜¯ä¸€ä¸ªå…·æœ‰æ˜¾è‘—å¤§è“åŒ…ï¼ˆBBBï¼‰çš„FSRQsç‰¹æ®Šæºï¼Œä¸ºç ”ç©¶ä¸­å¿ƒå¼•æ“æ´»åŠ¨ä¸ç›¸å¯¹è®ºå–·æµè€¦åˆå…³ç³»æä¾›äº†ç‹¬ç‰¹æœºä¼šã€‚</li>
<li>é€šè¿‡SEDå»ºæ¨¡å‘ç°ADæœ€å†…ç¨³å®šè½¨é“ä¸ç¨³å®šï¼Œä¸”åœ¨ä¸åŒè§‚å¯Ÿæ—¶æœŸæœ‰æ‰€å˜åŒ–ã€‚</li>
<li>é«˜èƒ½å‡¸èµ·ä¸»è¦ç”±BLRå…‰å­çš„åº·æ™®é¡¿æ•£å°„ä¸»å¯¼ã€‚</li>
<li>ä¼½é©¬å°„çº¿å‘å°„åŒºåŸŸé€šå¸¸ç‰©è´¨å ä¸»å¯¼ï¼Œä¸”ä½äºBLRä¹‹å¤–ä½†DTä¹‹å†…ï¼Œå¤šæ³¢é•¿å‘å°„å¯èƒ½æºäºå†²å‡»åŠ é€Ÿã€‚</li>
<li>ç ”ç©¶ç»“æœæ”¯æŒBZæœºåˆ¶ä¸ºç›¸å¯¹è®ºå–·æµæä¾›èƒ½é‡ï¼Œè€ŒBPæœºåˆ¶å¯èƒ½æ— æ³•æä¾›è¶³å¤Ÿçš„èƒ½é‡ã€‚</li>
<li>Xå°„çº¿æµé‡å¯èƒ½ç”±å¤šç§è¾å°„æˆåˆ†ç»„æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86eea578236c2d384fd28ce92a101a06.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af0083cd6be229f38058d0fda6a92bce.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-modal-brain-MRI-synthesis-based-on-SwinUNETR"><a href="#Multi-modal-brain-MRI-synthesis-based-on-SwinUNETR" class="headerlink" title="Multi-modal brain MRI synthesis based on SwinUNETR"></a>Multi-modal brain MRI synthesis based on SwinUNETR</h2><p><strong>Authors:Haowen Pang, Weiyan Guo, Chuyang Ye</strong></p>
<p>Multi-modal brain magnetic resonance imaging (MRI) plays a crucial role in clinical diagnostics by providing complementary information across different imaging modalities. However, a common challenge in clinical practice is missing MRI modalities. In this paper, we apply SwinUNETR to the synthesize of missing modalities in brain MRI. SwinUNETR is a novel neural network architecture designed for medical image analysis, integrating the strengths of Swin Transformer and convolutional neural networks (CNNs). The Swin Transformer, a variant of the Vision Transformer (ViT), incorporates hierarchical feature extraction and window-based self-attention mechanisms, enabling it to capture both local and global contextual information effectively. By combining the Swin Transformer with CNNs, SwinUNETR merges global context awareness with detailed spatial resolution. This hybrid approach addresses the challenges posed by the varying modality characteristics and complex brain structures, facilitating the generation of accurate and realistic synthetic images. We evaluate the performance of SwinUNETR on brain MRI datasets and demonstrate its superior capability in generating clinically valuable images. Our results show significant improvements in image quality, anatomical consistency, and diagnostic value. </p>
<blockquote>
<p>å¤šæ¨¡æ€è„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ä¸´åºŠè¯Šæ–­ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿæä¾›ä¸åŒæˆåƒæ¨¡å¼ä¹‹é—´çš„è¡¥å……ä¿¡æ¯ã€‚ç„¶è€Œï¼Œä¸´åºŠå®è·µä¸­çš„ä¸€ä¸ªå¸¸è§æŒ‘æˆ˜æ˜¯MRIæ¨¡å¼ç¼ºå¤±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†SwinUNETRåº”ç”¨äºè„‘MRIä¸­ç¼ºå¤±æ¨¡å¼çš„åˆæˆã€‚SwinUNETRæ˜¯ä¸€ç§æ–°å‹çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä¸“ä¸ºåŒ»å­¦å›¾åƒåˆ†æè€Œè®¾è®¡ï¼Œèåˆäº†Swin Transformerå’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ä¼˜åŠ¿ã€‚Swin Transformeræ˜¯Vision Transformerï¼ˆViTï¼‰çš„ä¸€ç§å˜ä½“ï¼Œç»“åˆäº†åˆ†å±‚ç‰¹å¾æå–å’ŒåŸºäºçª—å£çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚é€šè¿‡å°†Swin Transformerä¸CNNç›¸ç»“åˆï¼ŒSwinUNETRèåˆäº†å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œè¯¦ç»†çš„ç©ºé—´åˆ†è¾¨ç‡ã€‚è¿™ç§æ··åˆæ–¹æ³•è§£å†³äº†ç”±ä¸åŒçš„æ¨¡æ€ç‰¹å¾å’Œå¤æ‚çš„è„‘ç»“æ„æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä¿ƒè¿›äº†å‡†ç¡®ä¸”é€¼çœŸçš„åˆæˆå›¾åƒçš„äº§ç”Ÿã€‚æˆ‘ä»¬åœ¨è„‘MRIæ•°æ®é›†ä¸Šè¯„ä¼°äº†SwinUNETRçš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ç”Ÿæˆå…·æœ‰ä¸´åºŠä»·å€¼å›¾åƒæ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœåœ¨å›¾åƒè´¨é‡ã€è§£å‰–ä¸€è‡´æ€§å’Œè¯Šæ–­ä»·å€¼æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹å–„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02467v1">PDF</a> 9 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€è„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ä¸´åºŠè¯Šæ–­ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œèƒ½æä¾›ä¸åŒæˆåƒæ¨¡å¼çš„ä¿¡æ¯äº’è¡¥ã€‚é’ˆå¯¹ä¸´åºŠå®è·µä¸­ç¼ºå¤±MRIæ¨¡æ€çš„é—®é¢˜ï¼Œæœ¬æ–‡åº”ç”¨SwinUNETRè¿›è¡Œè„‘MRIç¼ºå¤±æ¨¡æ€çš„åˆæˆã€‚SwinUNETRæ˜¯ä¸€ç§ç»“åˆäº†Swin Transformerå’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¼˜åŠ¿çš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†æã€‚å®ƒé€šè¿‡ç»“åˆSwin Transformerçš„åˆ†å±‚ç‰¹å¾æå–å’ŒåŸºäºçª—å£çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆæ•æ‰å±€éƒ¨å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å°†Swin Transformerä¸CNNç›¸ç»“åˆï¼ŒSwinUNETRèåˆäº†å…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œè¯¦ç»†çš„ç©ºé—´åˆ†è¾¨ç‡ã€‚è¿™ç§æ··åˆæ–¹æ³•è§£å†³äº†ä¸åŒæ¨¡æ€ç‰¹å¾å’Œå¤æ‚è„‘ç»“æ„å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®ä¸”é€¼çœŸçš„åˆæˆå›¾åƒã€‚åœ¨è„‘MRIæ•°æ®é›†ä¸Šè¯„ä¼°SwinUNETRçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨ç”Ÿæˆå…·æœ‰ä¸´åºŠä»·å€¼å›¾åƒæ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€è„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å¯¹äºä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ï¼Œèƒ½èåˆä¸åŒæˆåƒæ¨¡å¼çš„ä¿¡æ¯ã€‚</li>
<li>ç¼ºå¤±MRIæ¨¡æ€æ˜¯ä¸´åºŠå®è·µä¸­ä¸€ä¸ªå¸¸è§é—®é¢˜ã€‚</li>
<li>SwinUNETRæ˜¯ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†æçš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç»“åˆäº†Swin Transformerå’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚</li>
<li>SwinUNETRèƒ½å¤Ÿç”Ÿæˆå‡†ç¡®ä¸”é€¼çœŸçš„åˆæˆå›¾åƒï¼Œè§£å†³ä¸åŒæ¨¡æ€ç‰¹å¾å’Œå¤æ‚è„‘ç»“æ„å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>SwinUNETRåœ¨è„‘MRIæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½ç”Ÿæˆå…·æœ‰ä¸´åºŠä»·å€¼çš„å›¾åƒã€‚</li>
<li>SwinUNETRé€šè¿‡ç»“åˆå…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œè¯¦ç»†çš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œæé«˜äº†å›¾åƒè´¨é‡å’Œè¯Šæ–­ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-101d4e8c0e012b217349f8c3a7cb6d6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2960ebb0017918f9368b90f15a99fe5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b3f299fa53d6a0a187b0f6c228aa01f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11edaf5738dc5a5d9be2ca99c1e933e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc66021e19ac7c1de23c57cf8e2c5b0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7fedc952046aad915fa0cb77cd5e4e41.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Guiding-Registration-with-Emergent-Similarity-from-Pre-Trained-Diffusion-Models"><a href="#Guiding-Registration-with-Emergent-Similarity-from-Pre-Trained-Diffusion-Models" class="headerlink" title="Guiding Registration with Emergent Similarity from Pre-Trained Diffusion   Models"></a>Guiding Registration with Emergent Similarity from Pre-Trained Diffusion   Models</h2><p><strong>Authors:Nurislam Tursynbek, Hastings Greer, Basar Demir, Marc Niethammer</strong></p>
<p>Diffusion models, while trained for image generation, have emerged as powerful foundational feature extractors for downstream tasks. We find that off-the-shelf diffusion models, trained exclusively to generate natural RGB images, can identify semantically meaningful correspondences in medical images. Building on this observation, we propose to leverage diffusion model features as a similarity measure to guide deformable image registration networks. We show that common intensity-based similarity losses often fail in challenging scenarios, such as when certain anatomies are visible in one image but absent in another, leading to anatomically inaccurate alignments. In contrast, our method identifies true semantic correspondences, aligning meaningful structures while disregarding those not present across images. We demonstrate superior performance of our approach on two tasks: multimodal 2D registration (DXA to X-Ray) and monomodal 3D registration (brain-extracted to non-brain-extracted MRI). Code: <a target="_blank" rel="noopener" href="https://github.com/uncbiag/dgir">https://github.com/uncbiag/dgir</a> </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹è™½ç„¶è¢«è®­ç»ƒç”¨äºå›¾åƒç”Ÿæˆï¼Œä½†å·²é€æ¸æˆä¸ºä¸‹æ¸¸ä»»åŠ¡çš„å¼ºå¤§åŸºç¡€ç‰¹å¾æå–å™¨ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸“ä¸ºç”Ÿæˆè‡ªç„¶RGBå›¾åƒè€Œè®­ç»ƒçš„å³ç”¨å‹æ‰©æ•£æ¨¡å‹å¯ä»¥è¯†åˆ«åŒ»å­¦å›¾åƒä¸­çš„è¯­ä¹‰å¯¹åº”ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç‰¹å¾ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡æ¥æŒ‡å¯¼å¯å˜å½¢å›¾åƒé…å‡†ç½‘ç»œã€‚æˆ‘ä»¬æ˜¾ç¤ºï¼Œå¸¸ç”¨çš„åŸºäºå¼ºåº¦çš„ç›¸ä¼¼åº¦æŸå¤±åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ç»å¸¸å¤±è´¥ï¼Œä¾‹å¦‚åœ¨ä¸€ç§å›¾åƒä¸­å¯è§æŸäº›è§£å‰–ç»“æ„è€Œåœ¨å¦ä¸€ç§å›¾åƒä¸­ç¼ºå¤±æ—¶ï¼Œè¿™ä¼šå¯¼è‡´è§£å‰–ä¸Šä¸å‡†ç¡®çš„å¯¹é½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè¯†åˆ«çœŸæ­£çš„è¯­ä¹‰å¯¹åº”ï¼Œå¯¹é½æœ‰æ„ä¹‰çš„ç»“æ„ï¼ŒåŒæ—¶å¿½ç•¥é‚£äº›åœ¨ä¸åŒå›¾åƒä¸­ä¸å­˜åœ¨çš„ç»“æ„ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä»»åŠ¡ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼šå¤šæ¨¡æ€äºŒç»´é…å‡†ï¼ˆDXAåˆ°Xå°„çº¿ï¼‰å’Œå•æ¨¡æ€ä¸‰ç»´é…å‡†ï¼ˆæå–å¤§è„‘ä¸éæå–å¤§è„‘çš„MRIï¼‰ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/uncbiag/dgir">https://github.com/uncbiag/dgir</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02419v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong><br>    æ‰©æ•£æ¨¡å‹è™½è¢«è®­ç»ƒç”¨äºå›¾åƒç”Ÿæˆï¼Œä½†å·²æˆä¸ºå¼ºå¤§çš„ä¸‹æ¸¸ä»»åŠ¡åŸºç¡€ç‰¹å¾æå–å™¨ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œç°æˆçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«åŒ»å­¦å›¾åƒä¸­çš„è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æå‡ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç‰¹å¾ä½œä¸ºç›¸ä¼¼åº¦åº¦é‡ï¼Œä»¥æŒ‡å¯¼å¯å˜å½¢å›¾åƒé…å‡†ç½‘ç»œã€‚åœ¨ç‰¹å®šè§£å‰–ç»“æ„åœ¨ä¸€å›¾ä¸­å¯è§è€Œåœ¨å¦ä¸€å›¾ä¸­ç¼ºå¤±ç­‰æŒ‘æˆ˜åœºæ™¯ä¸­ï¼Œå¸¸è§çš„åŸºäºå¼ºåº¦çš„ç›¸ä¼¼åº¦æŸå¤±å¸¸å¤±æ•ˆï¼Œè€Œæœ¬ç ”ç©¶æ–¹æ³•èƒ½è¯†åˆ«çœŸæ­£çš„è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œå¯¹é½æœ‰æ„ä¹‰ç»“æ„ï¼ŒåŒæ—¶å¿½ç•¥ä¸åŒå›¾åƒä¸­ä¸å­˜åœ¨çš„å†…å®¹ã€‚åœ¨ä¸¤ç§ä»»åŠ¡ä¸Šï¼Œæœ¬ç ”ç©¶æ–¹æ³•è¡¨ç°ä¼˜è¶Šï¼šå¤šæ¨¡æ€äºŒç»´é…å‡†ï¼ˆDXAåˆ°Xå…‰ï¼‰å’Œå•æ¨¡æ€ä¸‰ç»´é…å‡†ï¼ˆæå–å¤§è„‘ä¸æœªæå–å¤§è„‘çš„MRIï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¯ä½œä¸ºå¼ºå¤§çš„ä¸‹æ¸¸ä»»åŠ¡ç‰¹å¾æå–å™¨ï¼Œä¸ä»…ç”¨äºå›¾åƒç”Ÿæˆã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«åŒ»å­¦å›¾åƒä¸­çš„è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚</li>
<li>åœ¨æŒ‘æˆ˜åœºæ™¯ä¸­ï¼Œå¸¸è§çš„åŸºäºå¼ºåº¦çš„ç›¸ä¼¼åº¦æŸå¤±å¯èƒ½ä¼šå¤±æ•ˆã€‚</li>
<li>ç ”ç©¶æå‡ºåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç‰¹å¾æ¥æŒ‡å¯¼å¯å˜å½¢å›¾åƒé…å‡†ç½‘ç»œã€‚</li>
<li>æœ¬ç ”ç©¶æ–¹æ³•èƒ½è¯†åˆ«çœŸæ­£çš„è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œå¯¹é½æœ‰æ„ä¹‰ç»“æ„ã€‚</li>
<li>æœ¬ç ”ç©¶åœ¨ä¸¤ç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼šå¤šæ¨¡æ€äºŒç»´é…å‡†å’Œå•æ¨¡æ€ä¸‰ç»´é…å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c506878063bf7765fd73d4785bff5520.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8ce72588d2db8d36046920d9f1061af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c011f0d2c878cb79bfdf55991d82156.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec558a6b36e6147460ad72aaba33adbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0593e30b441a059c31c1ff1065ada802.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Medical-World-Model-Generative-Simulation-of-Tumor-Evolution-for-Treatment-Planning"><a href="#Medical-World-Model-Generative-Simulation-of-Tumor-Evolution-for-Treatment-Planning" class="headerlink" title="Medical World Model: Generative Simulation of Tumor Evolution for   Treatment Planning"></a>Medical World Model: Generative Simulation of Tumor Evolution for   Treatment Planning</h2><p><strong>Authors:Yijun Yang, Zhao-Yang Wang, Qiuping Liu, Shuwen Sun, Kang Wang, Rama Chellappa, Zongwei Zhou, Alan Yuille, Lei Zhu, Yu-Dong Zhang, Jieneng Chen</strong></p>
<p>Providing effective treatment and making informed clinical decisions are essential goals of modern medicine and clinical care. We are interested in simulating disease dynamics for clinical decision-making, leveraging recent advances in large generative models. To this end, we introduce the Medical World Model (MeWM), the first world model in medicine that visually predicts future disease states based on clinical decisions. MeWM comprises (i) vision-language models to serve as policy models, and (ii) tumor generative models as dynamics models. The policy model generates action plans, such as clinical treatments, while the dynamics model simulates tumor progression or regression under given treatment conditions. Building on this, we propose the inverse dynamics model that applies survival analysis to the simulated post-treatment tumor, enabling the evaluation of treatment efficacy and the selection of the optimal clinical action plan. As a result, the proposed MeWM simulates disease dynamics by synthesizing post-treatment tumors, with state-of-the-art specificity in Turing tests evaluated by radiologists. Simultaneously, its inverse dynamics model outperforms medical-specialized GPTs in optimizing individualized treatment protocols across all metrics. Notably, MeWM improves clinical decision-making for interventional physicians, boosting F1-score in selecting the optimal TACE protocol by 13%, paving the way for future integration of medical world models as the second readers. </p>
<blockquote>
<p>åœ¨ç°ä»£åŒ»å­¦å’Œä¸´åºŠæŠ¤ç†ä¸­ï¼Œæä¾›æœ‰æ•ˆçš„æ²»ç–—æ–¹æ³•å’Œåšå‡ºæ˜æ™ºçš„ä¸´åºŠå†³ç­–æ˜¯ä¸å¯æˆ–ç¼ºçš„ç›®æ ‡ã€‚æˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯åˆ©ç”¨å¤§å‹ç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•æ¥æ¨¡æ‹Ÿç–¾ç—…åŠ¨æ€ä»¥è¾…åŠ©ä¸´åºŠå†³ç­–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒ»å­¦ä¸–ç•Œæ¨¡å‹ï¼ˆMeWMï¼‰ï¼Œè¿™æ˜¯åŒ»å­¦é¢†åŸŸä¸­çš„é¦–ä¸ªä¸–ç•Œæ¨¡å‹ï¼Œèƒ½å¤ŸåŸºäºä¸´åºŠå†³ç­–è§†è§‰é¢„æµ‹æœªæ¥çš„ç–¾ç—…çŠ¶æ€ã€‚MeWMåŒ…æ‹¬ï¼ˆiï¼‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨ä½œç­–ç•¥æ¨¡å‹ï¼Œï¼ˆiiï¼‰è‚¿ç˜¤ç”Ÿæˆæ¨¡å‹ï¼Œç”¨ä½œåŠ¨æ€æ¨¡å‹ã€‚ç­–ç•¥æ¨¡å‹ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’ï¼Œå¦‚ä¸´åºŠæ²»ç–—ï¼Œè€ŒåŠ¨æ€æ¨¡å‹æ¨¡æ‹Ÿåœ¨ç»™å®šçš„æ²»ç–—æ¡ä»¶ä¸‹çš„è‚¿ç˜¤è¿›å±•æˆ–æ¶ˆé€€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†é€†å‘åŠ¨æ€æ¨¡å‹ï¼Œå¯¹æ¨¡æ‹Ÿçš„è‚¿ç˜¤è¿›è¡Œç”Ÿå­˜åˆ†æä»¥è¯„ä¼°æ²»ç–—æ•ˆæœå’Œé€‰æ‹©æœ€ä½³çš„ä¸´åºŠè¡ŒåŠ¨è®¡åˆ’ã€‚å› æ­¤ï¼Œæ‰€æå‡ºçš„MeWMé€šè¿‡åˆæˆæ²»ç–—åçš„è‚¿ç˜¤æ¥æ¨¡æ‹Ÿç–¾ç—…åŠ¨æ€ï¼Œå…¶åœ¨å›¾çµæµ‹è¯•ä¸­è¡¨ç°å‡ºæœ€æ–°çš„ç‰¹å¼‚æ€§æ°´å¹³å¹¶å¾—åˆ°æ”¾å°„ç§‘çš„è¯„ä¼°éªŒè¯ã€‚åŒæ—¶ï¼Œå…¶é€†å‘åŠ¨æ€æ¨¡å‹åœ¨ä¼˜åŒ–ä¸ªä½“åŒ–æ²»ç–—æ–¹æ¡ˆæ–¹é¢ä¼˜äºåŒ»å­¦ä¸“ç”¨GPTæ¨¡å‹çš„æ‰€æœ‰æŒ‡æ ‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMeWMæé«˜äº†ä»‹å…¥åŒ»å¸ˆçš„ä¸´åºŠå†³ç­–èƒ½åŠ›ï¼Œåœ¨é€‰æ‹©æœ€ä½³TACEåè®®æ—¶æé«˜äº†F1åˆ†æ•°è¾¾13%ï¼Œä¸ºæœªæ¥çš„åŒ»å­¦ä¸–ç•Œæ¨¡å‹ä½œä¸ºç¬¬äºŒè¯Šæ–­åŒ»å¸ˆçš„æ•´åˆé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02327v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŒ»å­¦ä¸–ç•Œæ¨¡å‹ï¼ˆMeWMï¼‰åˆ©ç”¨å¤§å‹ç”Ÿæˆæ¨¡å‹ï¼Œæ¨¡æ‹Ÿç–¾ç—…åŠ¨æ€ä»¥è¾…åŠ©ä¸´åºŠå†³ç­–ã€‚MeWMåŒ…æ‹¬ç­–ç•¥æ¨¡å‹å’ŒåŠ¨æ€æ¨¡å‹ä¸¤éƒ¨åˆ†ï¼Œåˆ†åˆ«ç”Ÿæˆæ²»ç–—è®¡åˆ’å’Œæ¨¡æ‹Ÿè‚¿ç˜¤å‘å±•ã€‚å…¶é€†åŠ¨æ€æ¨¡å‹åº”ç”¨ç”Ÿå­˜åˆ†æè¯„ä¼°æ²»ç–—æ•ˆæœï¼Œä¼˜åŒ–æ²»ç–—æ–¹æ¡ˆé€‰æ‹©ã€‚MeWMåœ¨è‚¿ç˜¤æ¨¡æ‹Ÿä¸Šå…·æœ‰å…ˆè¿›ç‰¹å¼‚æ€§ï¼Œåœ¨åŒ»ç”Ÿè¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜åŒ–ä¸ªä½“åŒ–æ²»ç–—åè®®ï¼Œæé«˜åŒ»ç”Ÿå†³ç­–å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MeWMæ˜¯é¦–ä¸ªç”¨äºåŒ»å­¦é¢†åŸŸçš„ä¸–ç•Œæ¨¡å‹ï¼Œå¯é€šè¿‡è§†è§‰é¢„æµ‹æœªæ¥ç–¾ç—…çŠ¶æ€ï¼Œè¾…åŠ©ä¸´åºŠå†³ç­–ã€‚</li>
<li>MeWMåŒ…æ‹¬ç­–ç•¥æ¨¡å‹å’ŒåŠ¨æ€æ¨¡å‹ï¼Œåˆ†åˆ«è´Ÿè´£ç”Ÿæˆæ²»ç–—è®¡åˆ’å’Œæ¨¡æ‹Ÿè‚¿ç˜¤å‘å±•ã€‚</li>
<li>MeWMçš„é€†åŠ¨æ€æ¨¡å‹åº”ç”¨ç”Ÿå­˜åˆ†æè¯„ä¼°æ²»ç–—æ•ˆæœï¼Œä¼˜åŒ–æ²»ç–—æ–¹æ¡ˆé€‰æ‹©ã€‚</li>
<li>MeWMåœ¨æ¨¡æ‹Ÿç–¾ç—…åŠ¨æ€æ–¹é¢å…·æœ‰å…ˆè¿›ç‰¹å¼‚æ€§ï¼Œåœ¨åŒ»ç”Ÿè¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸åŒ»ç–—ä¸“ä¸šGPTç›¸æ¯”ï¼ŒMeWMåœ¨ä¼˜åŒ–ä¸ªä½“åŒ–æ²»ç–—åè®®æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>MeWMæé«˜äº†åŒ»ç”Ÿåœ¨ä¸´åºŠå†³ç­–ä¸­çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é€‰æ‹©æœ€ä½³TACEåè®®æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cd9dc95ab93657ed72d0dcb398fdab07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae84ed978d63e5f07abff5f75bcd19a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8358b355c0730651a7f31c8b7398250e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3a604ee30f82d1ffb77c3ff9b0a85b7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Implicit-Deformable-Medical-Image-Registration-with-Learnable-Kernels"><a href="#Implicit-Deformable-Medical-Image-Registration-with-Learnable-Kernels" class="headerlink" title="Implicit Deformable Medical Image Registration with Learnable Kernels"></a>Implicit Deformable Medical Image Registration with Learnable Kernels</h2><p><strong>Authors:Stefano Fogarollo, Gregor Laimer, Reto Bale, Matthias Harders</strong></p>
<p>Deformable medical image registration is an essential task in computer-assisted interventions. This problem is particularly relevant to oncological treatments, where precise image alignment is necessary for tracking tumor growth, assessing treatment response, and ensuring accurate delivery of therapies. Recent AI methods can outperform traditional techniques in accuracy and speed, yet they often produce unreliable deformations that limit their clinical adoption. In this work, we address this challenge and introduce a novel implicit registration framework that can predict accurate and reliable deformations. Our insight is to reformulate image registration as a signal reconstruction problem: we learn a kernel function that can recover the dense displacement field from sparse keypoint correspondences. We integrate our method in a novel hierarchical architecture, and estimate the displacement field in a coarse-to-fine manner. Our formulation also allows for efficient refinement at test time, permitting clinicians to easily adjust registrations when needed. We validate our method on challenging intra-patient thoracic and abdominal zero-shot registration tasks, using public and internal datasets from the local University Hospital. Our method not only shows competitive accuracy to state-of-the-art approaches, but also bridges the generalization gap between implicit and explicit registration techniques. In particular, our method generates deformations that better preserve anatomical relationships and matches the performance of specialized commercial systems, underscoring its potential for clinical adoption. </p>
<blockquote>
<p>å¯å˜å½¢åŒ»å­¦å›¾åƒé…å‡†æ˜¯è®¡ç®—æœºè¾…åŠ©å¹²é¢„ä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ã€‚è¿™ä¸ªé—®é¢˜åœ¨è‚¿ç˜¤æ²»ç–—ä¸­å°¤å…¶é‡è¦ï¼Œå› ä¸ºéœ€è¦ç²¾ç¡®çš„å›¾åƒå¯¹é½ä»¥è¿½è¸ªè‚¿ç˜¤ç”Ÿé•¿æƒ…å†µã€è¯„ä¼°æ²»ç–—æ•ˆæœï¼Œå¹¶ç¡®ä¿å‡†ç¡®çš„æ²»ç–—æ–¹æ¡ˆå®æ–½ã€‚è™½ç„¶æœ€è¿‘çš„AIæ–¹æ³•åœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦æ–¹é¢å¯ä»¥è¶…è¶Šä¼ ç»ŸæŠ€æœ¯ï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šäº§ç”Ÿä¸å¯é çš„å˜å½¢ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸Šçš„é‡‡ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°å‹éšå¼é…å‡†æ¡†æ¶ï¼Œå¯ä»¥é¢„æµ‹å‡†ç¡®ä¸”å¯é çš„å˜å½¢ã€‚æˆ‘ä»¬çš„è§è§£æ˜¯å°†å›¾åƒé…å‡†é‡æ–°è¡¨è¿°ä¸ºä¿¡å·é‡å»ºé—®é¢˜ï¼šæˆ‘ä»¬å­¦ä¹ ä¸€ä¸ªå†…æ ¸å‡½æ•°ï¼Œå¯ä»¥ä»ç¨€ç–å…³é”®ç‚¹å¯¹åº”å…³ç³»æ¢å¤å¯†é›†ä½ç§»åœºã€‚æˆ‘ä»¬å°†è¯¥æ–¹æ³•é›†æˆåˆ°ä¸€ç§æ–°å‹åˆ†å±‚æ¶æ„ä¸­ï¼Œä»¥ä»ç²—åˆ°ç»†çš„æ–¹å¼ä¼°è®¡ä½ç§»åœºã€‚æˆ‘ä»¬çš„å…¬å¼è¿˜å…è®¸åœ¨æµ‹è¯•æ—¶è¿›è¡Œé«˜æ•ˆçš„ä¼˜åŒ–è°ƒæ•´ï¼Œä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿæ ¹æ®éœ€è¦è½»æ¾è°ƒæ•´é…å‡†ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ‚£è€…å†…éƒ¨èƒ¸è…”å’Œè…¹éƒ¨é›¶å°„å‡»é…å‡†ä»»åŠ¡ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä½¿ç”¨äº†å…¬å…±å’Œæœ¬åœ°å¤§å­¦åŒ»é™¢çš„å†…éƒ¨æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å±•ç¤ºäº†ä¸æœ€æ–°æŠ€æœ¯ç›¸ç«äº‰å‡†ç¡®æ€§ï¼Œè€Œä¸”å¼¥åˆäº†éšå¼å’Œæ˜¾å¼é…å‡†æŠ€æœ¯ä¹‹é—´çš„æ³›åŒ–å·®è·ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„å˜å½¢æ›´å¥½åœ°ä¿ç•™äº†è§£å‰–å…³ç³»ï¼Œå¹¶åŒ¹é…äº†ä¸“ä¸šå•†ä¸šç³»ç»Ÿçš„æ€§èƒ½ï¼Œçªæ˜¾äº†å…¶ä¸´åºŠé‡‡ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02150v1">PDF</a> MICCAI 2025 Provisional Accept</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„éšå¼æ³¨å†Œæ¡†æ¶ï¼Œç”¨äºè§£å†³åŒ»å­¦å›¾åƒå˜å½¢æ³¨å†Œçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†å›¾åƒæ³¨å†Œé‡æ–°æ„å»ºä¸ºä¿¡å·é‡å»ºé—®é¢˜ï¼Œé€šè¿‡å­¦ä¹ ä»ç¨€ç–å…³é”®ç‚¹å¯¹åº”å…³ç³»ä¸­æ¢å¤å¯†é›†ä½ç§»åœºã€‚è¯¥æ¡†æ¶ç»“åˆäº†åˆ†å±‚æ¶æ„ï¼Œå¹¶ä»¥ä»ç²—åˆ°ç»†çš„æ–¹å¼ä¼°è®¡ä½ç§»åœºã€‚æ­¤å¤–ï¼Œå®ƒå…è®¸åœ¨æµ‹è¯•æ—¶è¿›è¡Œé«˜æ•ˆçš„è°ƒæ•´ï¼Œä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿæ ¹æ®éœ€è¦è½»æ¾è°ƒæ•´æ³¨å†Œã€‚æœ¬æ–‡çš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ‚£è€…å†…éƒ¨èƒ¸è…”å’Œè…¹éƒ¨é›¶å°„å‡»æ³¨å†Œä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶è¡¨ç°å‡ºä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨éšå¼å’Œæ˜¾å¼æ³¨å†ŒæŠ€æœ¯ä¹‹é—´å»ºç«‹äº†æ¡¥æ¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†éšå¼æ³¨å†Œæ¡†æ¶æ¥è§£å†³åŒ»å­¦å›¾åƒå˜å½¢æ³¨å†Œé—®é¢˜ã€‚</li>
<li>å°†å›¾åƒæ³¨å†Œé‡æ–°æ„å»ºä¸ºä¿¡å·é‡å»ºé—®é¢˜ï¼Œå­¦ä¹ ä»ç¨€ç–å…³é”®ç‚¹å¯¹åº”å…³ç³»ä¸­æ¢å¤å¯†é›†ä½ç§»åœºã€‚</li>
<li>ç»“åˆåˆ†å±‚æ¶æ„ï¼Œä»¥ä»ç²—åˆ°ç»†çš„æ–¹å¼ä¼°è®¡ä½ç§»åœºã€‚</li>
<li>æ–¹æ³•å…è®¸åœ¨æµ‹è¯•æ—¶è¿›è¡Œé«˜æ•ˆçš„è°ƒæ•´ï¼Œæ–¹ä¾¿ä¸´åºŠåŒ»ç”ŸæŒ‰éœ€è°ƒæ•´æ³¨å†Œã€‚</li>
<li>æ–¹æ³•åœ¨æŒ‘æˆ˜æ€§çš„æ‚£è€…å†…éƒ¨èƒ¸è…”å’Œè…¹éƒ¨é›¶å°„å‡»æ³¨å†Œä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</li>
<li>ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºç›¸å½“çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-92291ebe7eee590dd5bda5ade0fb1d7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8315baf30ada87fd5ce1c69477ae9536.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ce0a0392bd155cb1cd37d8f56c6088d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Tomographic-Foundation-Model-â€“-FORCE-Flow-Oriented-Reconstruction-Conditioning-Engine"><a href="#Tomographic-Foundation-Model-â€“-FORCE-Flow-Oriented-Reconstruction-Conditioning-Engine" class="headerlink" title="Tomographic Foundation Model â€“ FORCE: Flow-Oriented Reconstruction   Conditioning Engine"></a>Tomographic Foundation Model â€“ FORCE: Flow-Oriented Reconstruction   Conditioning Engine</h2><p><strong>Authors:Wenjun Xia, Chuang Niu, Ge Wang</strong></p>
<p>Computed tomography (CT) is a major medical imaging modality. Clinical CT scenarios, such as low-dose screening, sparse-view scanning, and metal implants, often lead to severe noise and artifacts in reconstructed images, requiring improved reconstruction techniques. The introduction of deep learning has significantly advanced CT image reconstruction. However, obtaining paired training data remains rather challenging due to patient motion and other constraints. Although deep learning methods can still perform well with approximately paired data, they inherently carry the risk of hallucination due to data inconsistencies and model instability. In this paper, we integrate the data fidelity with the state-of-the-art generative AI model, referred to as the Poisson flow generative model (PFGM) with a generalized version PFGM++, and propose a novel CT framework: Flow-Oriented Reconstruction Conditioning Engine (FORCE). In our experiments, the proposed method shows superior performance in various CT imaging tasks, outperforming existing unsupervised reconstruction approaches. </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ˜¯ä¸€ç§ä¸»è¦çš„åŒ»å­¦æˆåƒæ¨¡å¼ã€‚åœ¨ä¸´åºŠCTåœºæ™¯ä¸­ï¼Œå¦‚ä½å‰‚é‡ç­›æŸ¥ã€ç¨€ç–è§†å›¾æ‰«æå’Œé‡‘å±æ¤å…¥ç‰©ï¼Œé‡å»ºçš„å›¾åƒå¾€å¾€ä¼šäº§ç”Ÿä¸¥é‡çš„å™ªå£°å’Œä¼ªå½±ï¼Œè¿™è¦æ±‚æ”¹è¿›é‡å»ºæŠ€æœ¯ã€‚æ·±åº¦å­¦ä¹ å¼•å…¥åˆ°CTå›¾åƒé‡å»ºä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºæ‚£è€…ç§»åŠ¨å’Œå…¶ä»–çº¦æŸæ¡ä»¶ï¼Œè·å–é…å¯¹è®­ç»ƒæ•°æ®ä»ç„¶ç›¸å½“å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ æ–¹æ³•ä»ç„¶å¯ä»¥åœ¨è¿‘ä¼¼é…å¯¹æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†ç”±äºæ•°æ®ä¸ä¸€è‡´å’Œæ¨¡å‹ä¸ç¨³å®šï¼Œå®ƒä»¬å¤©ç”Ÿå­˜åœ¨å‡ºç°å¹»è§‰çš„é£é™©ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ•°æ®ä¿çœŸåº¦ä¸æœ€æ–°ç”Ÿæˆçš„äººå·¥æ™ºèƒ½æ¨¡å‹ç›¸ç»“åˆï¼Œç§°ä¸ºPoissonæµç”Ÿæˆæ¨¡å‹ï¼ˆPFGMï¼‰åŠå…¶é€šç”¨ç‰ˆæœ¬PFGM++ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹çš„CTæ¡†æ¶ï¼šé¢å‘æµåŠ¨çš„é‡å»ºè°ƒèŠ‚å¼•æ“ï¼ˆFORCEï¼‰ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§CTæˆåƒä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ— ç›‘ç£é‡å»ºæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02149v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ·±åº¦å­¦ä¹ å’Œç”Ÿæˆå¼AIæ¨¡å‹çš„åŒ»å­¦CTå›¾åƒé‡å»ºæŠ€æœ¯ã€‚ç”±äºä¸´åºŠCTåœºæ™¯ä¸­å¸¸å¸¸å­˜åœ¨å™ªå£°å’Œä¼ªå½±é—®é¢˜ï¼Œéœ€è¦æ”¹è¿›é‡å»ºæŠ€æœ¯ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºFORCEçš„æ–°å‹CTæ¡†æ¶ï¼Œç»“åˆäº†æ•°æ®ä¿çœŸåº¦å’Œæœ€æ–°çš„ç”Ÿæˆå¼AIæ¨¡å‹â€”â€”Poissonæµç”Ÿæˆæ¨¡å‹ï¼ˆPFGMï¼‰åŠå…¶å¹¿ä¹‰ç‰ˆæœ¬PFGM++ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§CTæˆåƒä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰çš„æ— ç›‘ç£é‡å»ºæ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CTå›¾åƒé‡å»ºæŠ€æœ¯åœ¨ä¸´åºŠåº”ç”¨ä¸­é¢ä¸´å™ªå£°å’Œä¼ªå½±é—®é¢˜ï¼Œéœ€è¦æ”¹è¿›ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨CTå›¾åƒé‡å»ºä¸­çš„åº”ç”¨å—åˆ°é…å¯¹è®­ç»ƒæ•°æ®è·å–çš„æŒ‘æˆ˜ã€‚</li>
<li>æ‚£è€…è¿åŠ¨å’Œå…¶ä»–çº¦æŸæ˜¯è·å–é…å¯¹è®­ç»ƒæ•°æ®çš„ä¸»è¦éš¾é¢˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¤§çº¦é…å¯¹æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å­˜åœ¨æ•°æ®ä¸ä¸€è‡´å’Œæ¨¡å‹ä¸ç¨³å®šçš„é£é™©ã€‚</li>
<li>ä½œè€…æå‡ºäº†ç»“åˆæ•°æ®ä¿çœŸåº¦å’Œç”Ÿæˆå¼AIæ¨¡å‹çš„FORCEæ¡†æ¶ã€‚</li>
<li>FORCEæ¡†æ¶ä½¿ç”¨äº†Poissonæµç”Ÿæˆæ¨¡å‹ï¼ˆPFGMï¼‰åŠå…¶å¹¿ä¹‰ç‰ˆæœ¬PFGM++ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFORCEæ¡†æ¶åœ¨å¤šç§CTæˆåƒä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„æ— ç›‘ç£é‡å»ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24f99b47e3adaf5a64620cf0fc885e34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c9427665c1b16cbb9defe205cb41dfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24255e0b9a0d36ce0f2f012efb9aa57d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beda54c34a4d905c3c2f525281ebdae1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6c66125d65b082f327bb4c5233630d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b292183a90dedbf931648e36d44b3daa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdb4b52f97dd6da638d1f67a60883fba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ced067724dae8996caba9b898681b2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a903c16ece9172727e1c07927d9019e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RAW-Image-Reconstruction-from-RGB-on-Smartphones-NTIRE-2025-Challenge-Report"><a href="#RAW-Image-Reconstruction-from-RGB-on-Smartphones-NTIRE-2025-Challenge-Report" class="headerlink" title="RAW Image Reconstruction from RGB on Smartphones. NTIRE 2025 Challenge   Report"></a>RAW Image Reconstruction from RGB on Smartphones. NTIRE 2025 Challenge   Report</h2><p><strong>Authors:Marcos V. Conde, Radu Timofte, Radu Berdan, Beril Besbinar, Daisuke Iso, Pengzhou Ji, Xiong Dun, Zeying Fan, Chen Wu, Zhansheng Wang, Pengbo Zhang, Jiazi Huang, Qinglin Liu, Wei Yu, Shengping Zhang, Xiangyang Ji, Kyungsik Kim, Minkyung Kim, Hwalmin Lee, Hekun Ma, Huan Zheng, Yanyan Wei, Zhao Zhang, Jing Fang, Meilin Gao, Xiang Yu, Shangbin Xie, Mengyuan Sun, Huanjing Yue, Jingyu Yang Huize Cheng, Shaomeng Zhang, Zhaoyang Zhang, Haoxiang Liang</strong></p>
<p>Numerous low-level vision tasks operate in the RAW domain due to its linear properties, bit depth, and sensor designs. Despite this, RAW image datasets are scarce and more expensive to collect than the already large and public sRGB datasets. For this reason, many approaches try to generate realistic RAW images using sensor information and sRGB images. This paper covers the second challenge on RAW Reconstruction from sRGB (Reverse ISP). We aim to recover RAW sensor images from smartphones given the corresponding sRGB images without metadata and, by doing this, &#96;&#96;reverseâ€ the ISP transformation. Over 150 participants joined this NTIRE 2025 challenge and submitted efficient models. The proposed methods and benchmark establish the state-of-the-art for generating realistic RAW data. </p>
<blockquote>
<p>ç”±äºRAWåŸŸçš„çº¿æ€§å±æ€§ã€ä½æ·±åº¦å’Œä¼ æ„Ÿå™¨è®¾è®¡ï¼Œè®¸å¤šä½çº§è§†è§‰ä»»åŠ¡éƒ½åœ¨RAWåŸŸä¸­è¿›è¡Œã€‚å°½ç®¡å¦‚æ­¤ï¼ŒRAWå›¾åƒæ•°æ®é›†éå¸¸ç¨€ç¼ºï¼Œä¸”æ”¶é›†æˆæœ¬é«˜äºå·²ç»å¤§é‡å­˜åœ¨çš„å…¬å¼€sRGBæ•°æ®é›†ã€‚å› æ­¤ï¼Œè®¸å¤šæ–¹æ³•è¯•å›¾åˆ©ç”¨ä¼ æ„Ÿå™¨ä¿¡æ¯å’ŒsRGBå›¾åƒç”Ÿæˆé€¼çœŸçš„RAWå›¾åƒã€‚æœ¬æ–‡æ¶‰åŠä»sRGBè¿›è¡ŒRAWé‡å»ºï¼ˆåå‘ISPï¼‰çš„ç¬¬äºŒä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨æ²¡æœ‰å…ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ ¹æ®ç›¸åº”çš„sRGBå›¾åƒæ¢å¤æ¥è‡ªæ™ºèƒ½æ‰‹æœºçš„RAWä¼ æ„Ÿå™¨å›¾åƒï¼Œå¹¶å› æ­¤â€œåå‘â€ISPè½¬æ¢ã€‚è¶…è¿‡150åå‚ä¸è€…åŠ å…¥äº†è¿™ä¸€NTIRE 2025æŒ‘æˆ˜èµ›ï¼Œå¹¶æäº¤äº†é«˜æ•ˆæ¨¡å‹ã€‚æ‰€æå‡ºçš„æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•å»ºç«‹äº†ç”Ÿæˆé€¼çœŸRAWæ•°æ®çš„æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01947v1">PDF</a> CVPR 2025 - New Trends in Image Restoration and Enhancement (NTIRE)</p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†åœ¨RAWåŸŸè¿›è¡Œä½å±‚æ¬¡è§†è§‰ä»»åŠ¡çš„é‡è¦æ€§ï¼Œä½†ç”±äºRAWå›¾åƒæ•°æ®é›†ç¨€ç¼ºä¸”æ”¶é›†æˆæœ¬è¾ƒé«˜ï¼Œè®¸å¤šæ–¹æ³•å°è¯•åˆ©ç”¨ä¼ æ„Ÿå™¨ä¿¡æ¯å’ŒsRGBå›¾åƒç”Ÿæˆé€¼çœŸçš„RAWå›¾åƒã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»äº†ä»sRGBå›¾åƒé‡å»ºRAWä¼ æ„Ÿå™¨å›¾åƒçš„ç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼ˆåå‘ISPï¼‰ã€‚ç›®æ ‡æ˜¯åˆ©ç”¨å¯¹åº”çš„sRGBå›¾åƒæ¢å¤RAWä¼ æ„Ÿå™¨å›¾åƒï¼Œæ— éœ€å…ƒæ•°æ®å‚ä¸ã€‚è¶…è¿‡150åå‚ä¸è€…å‚åŠ äº†NTIRE 2025æŒ‘æˆ˜èµ›ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„æ¨¡å‹ï¼Œä¸ºç”Ÿæˆé€¼çœŸçš„RAWæ•°æ®å»ºç«‹äº†æœ€æ–°æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RAWåŸŸå› å…¶çº¿æ€§ç‰¹æ€§ã€ä½æ·±åº¦å’Œä¼ æ„Ÿå™¨è®¾è®¡è€Œè¢«ç”¨äºä¼—å¤šä½å±‚æ¬¡è§†è§‰ä»»åŠ¡ã€‚</li>
<li>RAWå›¾åƒæ•°æ®é›†ç¨€ç¼ºä¸”æ”¶é›†æˆæœ¬è¾ƒé«˜ï¼Œå› æ­¤æœ‰ç ”ç©¶å°è¯•ç”¨sRGBå›¾åƒå’Œä¼ æ„Ÿå™¨ä¿¡æ¯ç”Ÿæˆé€¼çœŸçš„RAWå›¾åƒã€‚</li>
<li>æœ¬æ–‡å…³æ³¨ä»sRGBå›¾åƒé‡å»ºRAWä¼ æ„Ÿå™¨å›¾åƒçš„ç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼Œå³åå‘ISPã€‚</li>
<li>ç›®æ ‡æ˜¯ä»…ä½¿ç”¨å¯¹åº”çš„sRGBå›¾åƒæ¢å¤RAWä¼ æ„Ÿå™¨å›¾åƒï¼Œæ— éœ€å…ƒæ•°æ®å‚ä¸ã€‚</li>
<li>è¶…è¿‡150åå‚ä¸è€…å‚ä¸äº†NTIRE 2025æŒ‘æˆ˜èµ›ï¼Œæå‡ºäº†å¤šç§é«˜æ•ˆæ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cd80bec8ae68b69f484df2af6ae5a1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bccfed1c0fb5c41720dfba9872ddad22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03ab8a796f3918708c88ce089ffbf3c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7de39a68b6c7233d937807fca3b0626.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f4ffe7ebc01a6ce1f59a8484435481f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="unMORE-Unsupervised-Multi-Object-Segmentation-via-Center-Boundary-Reasoning"><a href="#unMORE-Unsupervised-Multi-Object-Segmentation-via-Center-Boundary-Reasoning" class="headerlink" title="unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary   Reasoning"></a>unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary   Reasoning</h2><p><strong>Authors:Yafei Yang, Zihui Zhang, Bo Yang</strong></p>
<p>We study the challenging problem of unsupervised multi-object segmentation on single images. Existing methods, which rely on image reconstruction objectives to learn objectness or leverage pretrained image features to group similar pixels, often succeed only in segmenting simple synthetic objects or discovering a limited number of real-world objects. In this paper, we introduce unMORE, a novel two-stage pipeline designed to identify many complex objects in real-world images. The key to our approach involves explicitly learning three levels of carefully defined object-centric representations in the first stage. Subsequently, our multi-object reasoning module utilizes these learned object priors to discover multiple objects in the second stage. Notably, this reasoning module is entirely network-free and does not require human labels. Extensive experiments demonstrate that unMORE significantly outperforms all existing unsupervised methods across 6 real-world benchmark datasets, including the challenging COCO dataset, achieving state-of-the-art object segmentation results. Remarkably, our method excels in crowded images where all baselines collapse. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†å•å›¾åƒçš„æ— ç›‘ç£å¤šç›®æ ‡åˆ†å‰²è¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå›¾åƒé‡å»ºç›®æ ‡æ¥å­¦ä¹ å¯¹è±¡æ€§ï¼Œæˆ–è€…åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒç‰¹å¾æ¥å°†ç›¸ä¼¼çš„åƒç´ ç»„åˆåœ¨ä¸€èµ·ï¼Œä½†å®ƒä»¬é€šå¸¸ä»…åœ¨åˆ†å‰²ç®€å•åˆæˆå¯¹è±¡æˆ–å‘ç°æœ‰é™æ•°é‡çš„çœŸå®ä¸–ç•Œå¯¹è±¡æ—¶æˆåŠŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†unMOREï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæµç¨‹ï¼Œæ—¨åœ¨è¯†åˆ«çœŸå®ä¸–ç•Œå›¾åƒä¸­çš„å¤šä¸ªå¤æ‚å¯¹è±¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®åœ¨äºåœ¨ç¬¬ä¸€é˜¶æ®µæ˜¾å¼åœ°å­¦ä¹ ä¸‰ä¸ªå±‚æ¬¡ç²¾å¿ƒå®šä¹‰çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è¡¨ç¤ºã€‚éšåï¼Œæˆ‘ä»¬çš„å¤šç›®æ ‡æ¨ç†æ¨¡å—åˆ©ç”¨è¿™äº›å­¦ä¹ åˆ°çš„å¯¹è±¡å…ˆéªŒçŸ¥è¯†åœ¨ç¬¬äºŒé˜¶æ®µå‘ç°å¤šä¸ªå¯¹è±¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨ç†æ¨¡å—å®Œå…¨æ— éœ€ç½‘ç»œä¸”ä¸éœ€è¦äººå·¥æ ‡ç­¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨åŒ…æ‹¬å…·æœ‰æŒ‘æˆ˜æ€§çš„COCOæ•°æ®é›†åœ¨å†…çš„å…­ä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†ä¸Šï¼ŒunMOREæ˜¾è‘—ä¼˜äºæ‰€æœ‰ç°æœ‰æ— ç›‘ç£æ–¹æ³•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å¯¹è±¡åˆ†å‰²ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‹¥æŒ¤çš„å›¾åƒä¸­çš„è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œè€Œæ‰€æœ‰åŸºçº¿æ–¹æ³•å‡åœ¨æ­¤å¤„å¤±æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01778v1">PDF</a> ICML 2025. Code and data are available at:   <a target="_blank" rel="noopener" href="https://github.com/vLAR-group/unMORE">https://github.com/vLAR-group/unMORE</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å•å›¾åƒçš„æ— ç›‘ç£å¤šç›®æ ‡åˆ†å‰²é—®é¢˜ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ï¼ˆå¦‚ä¾èµ–å›¾åƒé‡å»ºç›®æ ‡æ¥å­¦ä¹ å¯¹è±¡æ€§ï¼Œæˆ–åˆ©ç”¨é¢„è®­ç»ƒå›¾åƒç‰¹å¾æ¥åˆ†ç»„ç›¸ä¼¼åƒç´ ï¼‰åœ¨åˆ†å‰²å¤æ‚ç°å®ä¸–ç•Œå¯¹è±¡æ—¶çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µç®¡é“unMOREã€‚è¯¥æ–¹æ³•ç¬¬ä¸€é˜¶æ®µé€šè¿‡æ˜¾å¼å­¦ä¹ ä¸‰ä¸ªå±‚æ¬¡çš„å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºï¼Œç¬¬äºŒé˜¶æ®µåˆ©ç”¨å¤šç›®æ ‡æ¨ç†æ¨¡å—å‘ç°å¤šä¸ªå¯¹è±¡ï¼Œæ— éœ€ç½‘ç»œä¸”æ— éœ€äººä¸ºæ ‡ç­¾ã€‚åœ¨å…­ä¸ªç°å®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒunMOREåœ¨åŒ…æ‹¬å…·æœ‰æŒ‘æˆ˜æ€§çš„COCOæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç›®æ ‡åˆ†å‰²ç»“æœã€‚ç‰¹åˆ«åœ°ï¼Œè¯¥æ–¹æ³•åœ¨æ‹¥æŒ¤å›¾åƒä¸­çš„è¡¨ç°è¶…è¶Šäº†æ‰€æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ— ç›‘ç£å¤šç›®æ ‡åˆ†å‰²åœ¨å•å›¾åƒçš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨åˆ†å‰²å¤æ‚ç°å®ä¸–ç•Œå¯¹è±¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µç®¡é“unMOREæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡æ˜¾å¼å­¦ä¹ ä¸‰ä¸ªå±‚æ¬¡çš„å¯¹è±¡ä¸­å¿ƒè¡¨ç¤ºã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µåˆ©ç”¨å¤šç›®æ ‡æ¨ç†æ¨¡å—å‘ç°å¤šä¸ªå¯¹è±¡ï¼Œæ— éœ€ç½‘ç»œå’Œäººä¸ºæ ‡ç­¾ã€‚</li>
<li>åœ¨å¤šä¸ªç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒunMOREæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51b513032f259b96ce3bc0306609e16e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca05402db53504c3570049d0700c5f53.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-424dd130d3b9e2fabea326c17cf5dd11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a12ba3340061d97baf1fa1ef0310b03.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SAM-I2V-Upgrading-SAM-to-Support-Promptable-Video-Segmentation-with-Less-than-0-2-Training-Cost"><a href="#SAM-I2V-Upgrading-SAM-to-Support-Promptable-Video-Segmentation-with-Less-than-0-2-Training-Cost" class="headerlink" title="SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with   Less than 0.2% Training Cost"></a>SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with   Less than 0.2% Training Cost</h2><p><strong>Authors:Haiyang Mei, Pengyu Zhang, Mike Zheng Shou</strong></p>
<p>Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V, an effective image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAMâ€™s static image encoder to enable spatiotemporal video perception, (ii) a memory filtering strategy that selects the most relevant past frames for more effective utilization of historical information, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves over 90% of SAM 2â€™s performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Code and model are available at: <a target="_blank" rel="noopener" href="https://github.com/showlab/SAM-I2V">https://github.com/showlab/SAM-I2V</a>. </p>
<blockquote>
<p>ç±»ä¼¼Segment Anything Modelï¼ˆSAMï¼‰è¿™æ ·çš„åŸºç¡€æ¨¡å‹å·²ç»åœ¨è®¡ç®—æœºè§†è§‰çš„å¯æç¤ºå›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå°†è¿™äº›èƒ½åŠ›æ‰©å±•åˆ°è§†é¢‘å´é¢ä¸´ç€å·¨å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¡®ä¿åŠ¨æ€åœºæ™¯ä¸­çš„ç²¾ç¡®å’Œæ—¶é—´ä¸Šä¸€è‡´çš„æ©è†œä¼ æ’­æ–¹é¢ã€‚SAM 2 é€šè¿‡åœ¨å¤§é‡å›¾åƒå’Œè§†é¢‘æ•°æ®ä¸Šè¿›è¡Œæ¨¡å‹è®­ç»ƒæ¥å­¦ä¹ å¤æ‚çš„æ—¶ç©ºå…³è”ï¼Œä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†è¿™ä¹Ÿå¯¼è‡´äº†å·¨å¤§çš„è®­ç»ƒæˆæœ¬ï¼Œé˜»ç¢äº†ç ”ç©¶å’Œå®é™…åº”ç”¨éƒ¨ç½²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æœ‰æ•ˆçš„å›¾åƒåˆ°è§†é¢‘çš„å‡çº§æ–¹æ³•SAM-I2Vï¼Œç”¨äºåŸ¹å…»å¯æç¤ºè§†é¢‘åˆ†å‰²ï¼ˆPVSï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å‡çº§é¢„è®­ç»ƒçš„SAMæ¥æ”¯æŒPVSï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒå¤æ‚æ€§å’Œèµ„æºéœ€æ±‚ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼šï¼ˆiï¼‰åŸºäºSAMçš„é™æ€å›¾åƒç¼–ç å™¨æ„å»ºå›¾åƒåˆ°è§†é¢‘ç‰¹å¾æå–å‡çº§å™¨ï¼Œä»¥å®ç°æ—¶ç©ºè§†é¢‘æ„ŸçŸ¥ï¼›ï¼ˆiiï¼‰ä¸€ç§è®°å¿†è¿‡æ»¤ç­–ç•¥ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„è¿‡å»å¸§ï¼Œä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å†å²ä¿¡æ¯ï¼›ï¼ˆiiiï¼‰ä¸€ç§è®°å¿†æç¤ºæœºåˆ¶ï¼Œåˆ©ç”¨å¯¹è±¡å†…å­˜ç¡®ä¿åŠ¨æ€åœºæ™¯ä¸­çš„æ—¶é—´ä¸Šä¸€è‡´çš„æ©è†œä¼ æ’­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†SAM 2æ€§èƒ½çš„90%ä»¥ä¸Šï¼ŒåŒæ—¶ä»…ä½¿ç”¨å…¶0.2%çš„è®­ç»ƒæˆæœ¬ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºPVSæä¾›äº†ä¸€æ¡èµ„æºé«˜æ•ˆçš„è·¯å¾„ï¼Œé™ä½äº†PVSæ¨¡å‹è®¾è®¡çš„è¿›ä¸€æ­¥ç ”ç©¶éšœç¢ï¼Œå¹¶å¯ç”¨äº†è¯¥é¢†åŸŸçš„æ›´å¹¿æ³›åº”ç”¨å’Œè¿›å±•ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/showlab/SAM-I2V%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/showlab/SAM-I2Væ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01304v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>SAM-I2Væ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„SAMå›¾åƒæ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥ä¸‰é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼Œå®ç°äº†è§†é¢‘åˆ†å‰²çš„å‡çº§ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒå¤æ‚åº¦å’Œèµ„æºéœ€æ±‚ã€‚è¯¥æŠ€æœ¯åŒ…æ‹¬ï¼šåŸºäºSAMé™æ€å›¾åƒç¼–ç å™¨çš„å›¾åƒåˆ°è§†é¢‘ç‰¹å¾æå–å‡çº§å™¨ã€è®°å¿†è¿‡æ»¤ç­–ç•¥ä»¥åŠåˆ©ç”¨å¯¹è±¡è®°å¿†çš„å†…å­˜æç¤ºæœºåˆ¶ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†è¶…è¿‡90%çš„SAM 2æ€§èƒ½ï¼ŒåŒæ—¶ä»…ä½¿ç”¨å…¶0.2%çš„è®­ç»ƒæˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAM-I2Væ˜¯Segment Anything Modelï¼ˆSAMï¼‰çš„è§†é¢‘åˆ†å‰²å‡çº§æ–¹æ³•ã€‚</li>
<li>SAM-I2Vè§£å†³äº†å°†å›¾åƒåˆ†å‰²æ¨¡å‹åº”ç”¨äºè§†é¢‘æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ç²¾ç¡®ä¸”æ—¶é—´ä¸€è‡´çš„æ©è†œä¼ æ’­ã€‚</li>
<li>é€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°å®ç°è§†é¢‘åˆ†å‰²çš„å‡çº§ï¼šå›¾åƒåˆ°è§†é¢‘ç‰¹å¾æå–å‡çº§å™¨ã€è®°å¿†è¿‡æ»¤ç­–ç•¥å’Œå†…å­˜æç¤ºæœºåˆ¶ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSAM-I2Vå®ç°äº†è¶…è¿‡90%çš„SAM 2æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†è®­ç»ƒæˆæœ¬å’Œèµ„æºéœ€æ±‚ã€‚</li>
<li>SAM-I2Vå·¥ä½œä¸ºå¯æç¤ºè§†é¢‘åˆ†å‰²ï¼ˆPVSï¼‰æä¾›äº†èµ„æºé«˜æ•ˆçš„é€”å¾„ï¼Œé™ä½äº†è¿›ä¸€æ­¥ç ”ç©¶PVSæ¨¡å‹è®¾è®¡çš„é—¨æ§›ï¼Œå¹¶ä¿ƒè¿›äº†è¯¥é¢†åŸŸçš„å¹¿æ³›åº”ç”¨å’Œè¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01304">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0c6b8d8ddcf2279995e9c5eb8c0cb8e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-307d789d3f53844008e298dc60575b8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3f8a61f35826bea6db2504c57921fa8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Region-of-Interest-Guided-Deep-Joint-Source-Channel-Coding-for-Image-Transmission"><a href="#Region-of-Interest-Guided-Deep-Joint-Source-Channel-Coding-for-Image-Transmission" class="headerlink" title="Region-of-Interest-Guided Deep Joint Source-Channel Coding for Image   Transmission"></a>Region-of-Interest-Guided Deep Joint Source-Channel Coding for Image   Transmission</h2><p><strong>Authors:Hansung Choi, Daewon Seo</strong></p>
<p>Deep joint source-channel coding (deepJSCC) and semantic communication have shown promising improvements in communication performance over wireless networks. However, current approaches primarily focus on enhancing average performance metrics, such as overall image reconstruction quality or task accuracy, which may not fully align with usersâ€™ actual experience â€“ often driven by the quality of specific regions of interest (ROI). Motivated by this, we propose ROI-guided joint source-channel coding (ROI-JSCC), a novel deepJSCC framework that prioritizes high-quality transmission of ROI. The ROI-JSCC consists of four key components: (1) ROI embedding and feature map extraction, (2) ROI-guided split processing, (3) ROI-based loss function design, and (4) ROI-adaptive bandwidth allocation. Together, these components enable ROI-JSCC to selectively improve the reconstruction quality of varying ROI while preserving overall image quality without increasing computational burden. Experimental results under diverse communication environments demonstrate that ROI-JSCC significantly improves ROI reconstruction quality while maintaining competitive average image quality compared to recent state-of-the-art methods. All codes are available at <a target="_blank" rel="noopener" href="https://github.com/hansung-choi/ROI-JSCC">https://github.com/hansung-choi/ROI-JSCC</a>. </p>
<blockquote>
<p>æ·±åº¦è”åˆæºä¿¡é“ç¼–ç ï¼ˆdeepJSCCï¼‰å’Œè¯­ä¹‰é€šä¿¡åœ¨æ— çº¿ç½‘ç»œçš„é€šä¿¡æ€§èƒ½ä¸Šæ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æé«˜å¹³å‡æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚æ•´ä½“å›¾åƒé‡å»ºè´¨é‡æˆ–ä»»åŠ¡å‡†ç¡®æ€§ï¼Œè¿™å¯èƒ½æ— æ³•å®Œå…¨ä¸ç”¨æˆ·å®é™…ä½“éªŒå¯¹é½ï¼Œç”¨æˆ·å®é™…ä½“éªŒé€šå¸¸æ˜¯ç”±æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„è´¨é‡é©±åŠ¨çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ROIå¼•å¯¼è”åˆæºä¿¡é“ç¼–ç ï¼ˆROI-JSCCï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹deepJSCCæ¡†æ¶ï¼Œä¼˜å…ˆé«˜è´¨é‡ä¼ è¾“ROIã€‚ROI-JSCCç”±å››ä¸ªå…³é”®ç»„ä»¶æ„æˆï¼šï¼ˆ1ï¼‰ROIåµŒå…¥å’Œç‰¹å¾æ˜ å°„æå–ï¼Œï¼ˆ2ï¼‰ROIå¼•å¯¼åˆ†è£‚å¤„ç†ï¼Œï¼ˆ3ï¼‰åŸºäºROIçš„æŸå¤±å‡½æ•°è®¾è®¡ï¼Œä»¥åŠï¼ˆ4ï¼‰ROIè‡ªé€‚åº”å¸¦å®½åˆ†é…ã€‚è¿™äº›ç»„ä»¶å…±åŒä½œç”¨ï¼Œä½¿ROI-JSCCèƒ½å¤Ÿåœ¨ä¿æŒæ•´ä½“å›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œæœ‰é€‰æ‹©åœ°æé«˜ä¸åŒROIçš„é‡å»ºè´¨é‡ï¼Œä¸”ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…ã€‚åœ¨å¤šç§é€šä¿¡ç¯å¢ƒä¸‹çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒROI-JSCCåœ¨ä¿æŒå¹³å‡å›¾åƒè´¨é‡ç«äº‰åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†ROIçš„é‡å»ºè´¨é‡ï¼Œä¸æœ€è¿‘çš„æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”è¡¨ç°ä¼˜å¼‚ã€‚æ‰€æœ‰ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hansung-choi/ROI-JSCC%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hansung-choi/ROI-JSCCè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01269v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé€šä¿¡ä¸­ï¼Œæ·±å±‚è”åˆæºä¿¡é“ç¼–ç ï¼ˆdeepJSCCï¼‰å’Œè¯­ä¹‰é€šä¿¡å·²æ˜¾ç¤ºå‡ºåœ¨æé«˜é€šä¿¡æ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨å¹³å‡æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚æ•´ä½“å›¾åƒé‡å»ºè´¨é‡æˆ–ä»»åŠ¡å‡†ç¡®æ€§ï¼Œä½†å¯èƒ½æ— æ³•å®Œå…¨ä¸ç”¨æˆ·å…³æ³¨çš„ç‰¹å®šæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„è´¨é‡ç›¸åŒ¹é…ã€‚å› æ­¤ï¼Œæå‡ºROIå¼•å¯¼çš„è”åˆæºä¿¡é“ç¼–ç ï¼ˆROI-JSCCï¼‰ï¼Œä¼˜å…ˆä¼ è¾“ROIçš„é«˜è´¨é‡ã€‚å®ƒåŒ…æ‹¬ROIåµŒå…¥ã€ç‰¹å¾æ˜ å°„æå–ã€ROIå¼•å¯¼çš„åˆ†æ­¥å¤„ç†ã€ROIåŸºç¡€çš„æŸå¤±å‡½æ•°è®¾è®¡å’ŒROIè‡ªé€‚åº”çš„å¸¦å®½åˆ†é…ç­‰å››ä¸ªå…³é”®ç»„ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒROI-JSCCåœ¨ä¿æŒæ•´ä½“å›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜ROIçš„é‡å»ºè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepJSCCå’Œè¯­ä¹‰é€šä¿¡åœ¨åŒ»å­¦å›¾åƒé€šä¿¡ä¸­å¯æé«˜æ€§èƒ½ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨å¹³å‡æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚æ•´ä½“å›¾åƒé‡å»ºè´¨é‡ã€‚</li>
<li>ç”¨æˆ·å®é™…å…³æ³¨çš„é‡ç‚¹æ˜¯ç‰¹å®šæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„è´¨é‡ã€‚</li>
<li>ROI-JSCCæ˜¯ä¸€ç§ä¼˜å…ˆä¼ è¾“ROIé«˜è´¨é‡çš„æ–°å‹deepJSCCæ¡†æ¶ã€‚</li>
<li>ROI-JSCCåŒ…æ‹¬å››ä¸ªå…³é”®ç»„ä»¶ï¼šROIåµŒå…¥ã€ç‰¹å¾æ˜ å°„ã€åˆ†æ­¥å¤„ç†å’ŒæŸå¤±å‡½æ•°è®¾è®¡ã€‚</li>
<li>ROI-JSCCèƒ½å¤Ÿé€‰æ‹©æ€§æé«˜ä¸åŒROIçš„é‡å»ºè´¨é‡ï¼ŒåŒæ—¶ä¿æŒæ•´ä½“å›¾åƒè´¨é‡ï¼Œä¸”ä¸å¢åŠ è®¡ç®—è´Ÿæ‹…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88307d1942b08171207e5f93fddc7689.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-971ee3a8f96ee3b15eaa632cec6e76c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23b6d8159f9ac4502fb385d213de9855.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd9c302e4d40bbb398f81874722c078d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-008b12c9afd3624fb6186e9f6c9f4e54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9603c2461a10441650aa551c4ccba3a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03884b1549556e1211d4c3d7d1800e28.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Revolutionizing-Radiology-Workflow-with-Factual-and-Efficient-CXR-Report-Generation"><a href="#Revolutionizing-Radiology-Workflow-with-Factual-and-Efficient-CXR-Report-Generation" class="headerlink" title="Revolutionizing Radiology Workflow with Factual and Efficient CXR Report   Generation"></a>Revolutionizing Radiology Workflow with Factual and Efficient CXR Report   Generation</h2><p><strong>Authors:Pimchanok Sukjai, Apiradee Boonmee</strong></p>
<p>The escalating demand for medical image interpretation underscores the critical need for advanced artificial intelligence solutions to enhance the efficiency and accuracy of radiological diagnoses. This paper introduces CXR-PathFinder, a novel Large Language Model (LLM)-centric foundation model specifically engineered for automated chest X-ray (CXR) report generation. We propose a unique training paradigm, Clinician-Guided Adversarial Fine-Tuning (CGAFT), which meticulously integrates expert clinical feedback into an adversarial learning framework to mitigate factual inconsistencies and improve diagnostic precision. Complementing this, our Knowledge Graph Augmentation Module (KGAM) acts as an inference-time safeguard, dynamically verifying generated medical statements against authoritative knowledge bases to minimize hallucinations and ensure standardized terminology. Leveraging a comprehensive dataset of millions of paired CXR images and expert reports, our experiments demonstrate that CXR-PathFinder significantly outperforms existing state-of-the-art medical vision-language models across various quantitative metrics, including clinical accuracy (Macro F1 (14): 46.5, Micro F1 (14): 59.5). Furthermore, blinded human evaluation by board-certified radiologists confirms CXR-PathFinderâ€™s superior clinical utility, completeness, and accuracy, establishing its potential as a reliable and efficient aid for radiological practice. The developed method effectively balances high diagnostic fidelity with computational efficiency, providing a robust solution for automated medical report generation. </p>
<blockquote>
<p>éšç€åŒ»å­¦å›¾åƒè§£è¯»éœ€æ±‚çš„ä¸æ–­å¢é•¿ï¼Œå¯¹å…ˆè¿›çš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆçš„éœ€æ±‚ä¹Ÿæ„ˆå‘è¿«åˆ‡ï¼Œä»¥æé«˜æ”¾å°„è¯Šæ–­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†CXR-PathFinderï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºä¸­å¿ƒçš„åŸºç¡€æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºè‡ªåŠ¨ç”Ÿæˆèƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰æŠ¥å‘Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç‹¬ç‰¹çš„è®­ç»ƒèŒƒå¼â€”â€”ä¸´åºŠåŒ»ç”ŸæŒ‡å¯¼çš„å¯¹æŠ—å¾®è°ƒï¼ˆCGAFTï¼‰ï¼Œå®ƒå°†ä¸“å®¶ä¸´åºŠåé¦ˆç²¾å¿ƒé›†æˆåˆ°å¯¹æŠ—æ€§å­¦ä¹ æ¡†æ¶ä¸­ï¼Œä»¥å‡è½»äº‹å®ä¸Šçš„ä¸ä¸€è‡´æ€§å¹¶æé«˜è¯Šæ–­ç²¾åº¦ã€‚ä½œä¸ºè¡¥å……ï¼Œæˆ‘ä»¬çš„çŸ¥è¯†å›¾è°±å¢å¼ºæ¨¡å—ï¼ˆKGAMï¼‰å……å½“æ¨ç†æ—¶çš„å®‰å…¨å«å£«ï¼ŒåŠ¨æ€éªŒè¯ç”Ÿæˆçš„åŒ»å­¦é™ˆè¿°ä¸æƒå¨çŸ¥è¯†åº“çš„ä¸€è‡´æ€§ï¼Œä»¥æœ€å°åŒ–å¹»è§‰å¹¶ç¡®ä¿æ ‡å‡†åŒ–æœ¯è¯­ã€‚åˆ©ç”¨æ•°ç™¾ä¸‡å¯¹CXRå›¾åƒå’Œä¸“å®¶æŠ¥å‘Šçš„ç»¼åˆæ•°æ®é›†ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒCXR-PathFinderåœ¨å„ç§å®šé‡æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸´åºŠå‡†ç¡®æ€§ï¼ˆå®è§‚F1ï¼ˆ14ï¼‰ï¼š46.5ï¼Œå¾®è§‚F1ï¼ˆ14ï¼‰ï¼š59.5ï¼‰ã€‚æ­¤å¤–ï¼Œç»è¿‡è®¤è¯çš„æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡Œçš„ç›²æ€äººç±»è¯„ä¼°è¯å®äº†CXR-PathFinderåœ¨ä¸´åºŠå®ç”¨æ€§ã€å®Œæ•´æ€§å’Œå‡†ç¡®æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œè¯æ˜äº†å…¶åœ¨æ”¾å°„å®è·µä¸­ä½œä¸ºå¯é ä¸”é«˜æ•ˆçš„è¾…åŠ©å·¥å…·çš„æ½œåŠ›ã€‚æ‰€å¼€å‘çš„æ–¹æ³•æœ‰æ•ˆåœ°å¹³è¡¡äº†é«˜è¯Šæ–­ä¿çœŸåº¦å’Œè®¡ç®—æ•ˆç‡ï¼Œä¸ºè‡ªåŠ¨åŒ»å­¦æŠ¥å‘Šç”Ÿæˆæä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01118v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºä¸€ç§åä¸ºCXR-PathFinderçš„æ–°å‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºä¸­å¿ƒçš„åŸºç¡€æ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆèƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰æŠ¥å‘Šã€‚é€šè¿‡ç‹¬ç‰¹çš„è®­ç»ƒèŒƒå¼Clinician-Guided Adversarial Fine-Tuningï¼ˆCGAFTï¼‰å’ŒKnowledge Graph Augmentation Moduleï¼ˆKGAMï¼‰ï¼Œè¯¥æ¨¡å‹æé«˜äº†è¯Šæ–­çš„å‡†ç¡®æ€§å’Œç²¾ç¡®æ€§ï¼Œå‡å°‘äº†äº‹å®ä¸Šçš„ä¸ä¸€è‡´æ€§ï¼Œå¹¶ç¡®ä¿æ ‡å‡†åŒ–æœ¯è¯­ã€‚å®éªŒè¡¨æ˜ï¼ŒCXR-PathFinderåœ¨å¤šç§å®šé‡æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨æ”¾å°„å®è·µé¢†åŸŸå±•ç°å‡ºå¯é ä¸”é«˜æ•ˆçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒè§£è¯»éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œå¼ºè°ƒå¯¹å…ˆè¿›äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆçš„éœ€æ±‚ï¼Œä»¥æé«˜æ”¾å°„è¯Šæ–­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>ä»‹ç»äº†CXR-PathFinderæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºè‡ªåŠ¨ç”Ÿæˆèƒ¸éƒ¨Xå…‰æŠ¥å‘Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>CXR-PathFinderé‡‡ç”¨ç‹¬ç‰¹çš„è®­ç»ƒèŒƒå¼Clinician-Guided Adversarial Fine-Tuning (CGAFT)ï¼Œå°†ä¸“å®¶ä¸´åºŠåé¦ˆçº³å…¥å¯¹æŠ—æ€§å­¦ä¹ æ¡†æ¶ï¼Œä»¥æé«˜è¯Šæ–­çš„å‡†ç¡®æ€§å’Œç²¾ç¡®æ€§ã€‚</li>
<li>Knowledge Graph Augmentation Module (KGAM) ä½œä¸ºæ¨ç†æ—¶çš„å®‰å…¨ä¿éšœï¼ŒåŠ¨æ€éªŒè¯ç”Ÿæˆçš„åŒ»ç–—é™ˆè¿°ï¼Œç¡®ä¿æ ‡å‡†åŒ–æœ¯è¯­å¹¶å‡å°‘è™šæ„æƒ…å†µã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCXR-PathFinderåœ¨å„ç§å®šé‡æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸´åºŠç²¾åº¦ï¼ˆMacro F1 (14): 46.5, Micro F1 (14): 59.5ï¼‰ã€‚</li>
<li>ç›²æ€äººç±»è¯„ä¼°ï¼ˆç”±è®¤è¯æ”¾å°„å­¦å®¶è¿›è¡Œï¼‰è¯å®äº†CXR-PathFinderåœ¨ä¸´åºŠå®ç”¨æ€§ã€å®Œæ•´æ€§å’Œå‡†ç¡®æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b4a743779a7ad40efd4b9bf65957cb5c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Self-Supervised-ISAR-Net-Enables-Fast-Sparse-ISAR-Imaging"><a href="#Self-Supervised-ISAR-Net-Enables-Fast-Sparse-ISAR-Imaging" class="headerlink" title="Self-Supervised-ISAR-Net Enables Fast Sparse ISAR Imaging"></a>Self-Supervised-ISAR-Net Enables Fast Sparse ISAR Imaging</h2><p><strong>Authors:Ziwen Wang, Jianping wang, Pucheng Li, Yifan Wu, Zegang Ding</strong></p>
<p>Numerous sparse inverse synthetic aperture radar (ISAR) imaging methods based on unfolded neural networks have been developed for high-quality image reconstruction with sparse measurements. However, their training typically requires paired ISAR images and echoes, which are often difficult to obtain. Meanwhile, one property can be observed that for a certain sparse measurement configuration of ISAR, when a target is rotated around its center of mass, only the image of the target undergoes the corresponding rotation after ISAR imaging, while the grating lobes do not follow this rotation and are solely determined by the sparse-sampling pattern. This property is mathematically termed as the equivariant property. Taking advantage of this property, an unfolded neural network for sparse ISAR imaging with self-supervised learning, named SS-ISAR-Net is proposed. It effectively mitigates grating lobes caused by sparse radar echo, allowing high-quality training to be achieved using only sparse radar echo data. The superiority of the proposed SS-ISAR-Net, compared to existing methods, is verified through experiments with both synthetic and real-world measurement data. </p>
<blockquote>
<p>åŸºäºå±•å¼€ç¥ç»ç½‘ç»œçš„ç¨€ç–é€†åˆæˆå­”å¾„é›·è¾¾ï¼ˆISARï¼‰æˆåƒæ–¹æ³•å·²ç»è¢«å¼€å‘å‡ºæ¥ï¼Œç”¨äºåˆ©ç”¨ç¨€ç–æµ‹é‡è¿›è¡Œé«˜è´¨é‡å›¾åƒé‡å»ºã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è®­ç»ƒé€šå¸¸éœ€è¦æˆå¯¹çš„ISARå›¾åƒå’Œå›æ³¢ï¼Œè€Œè¿™äº›é€šå¸¸å¾ˆéš¾è·å¾—ã€‚åŒæ—¶ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°è¿™æ ·ä¸€ä¸ªç‰¹æ€§ï¼šå¯¹äºISARçš„æŸç§ç¨€ç–æµ‹é‡é…ç½®ï¼Œå½“ç›®æ ‡å›´ç»•å…¶è´¨é‡ä¸­å¿ƒæ—‹è½¬æ—¶ï¼Œåªæœ‰åœ¨ISARæˆåƒåï¼Œç›®æ ‡å›¾åƒä¼šè¿›è¡Œç›¸åº”çš„æ—‹è½¬ï¼Œè€Œå…‰æ …ç“£å¹¶ä¸éµå¾ªè¿™ç§æ—‹è½¬ï¼Œè€Œæ˜¯ä»…ç”±ç¨€ç–é‡‡æ ·æ¨¡å¼å†³å®šã€‚è¿™ä¸€ç‰¹æ€§åœ¨æ•°å­¦ä¸Šè¢«ç§°ä¸ºç­‰å˜å±æ€§ã€‚åˆ©ç”¨è¿™ä¸€ç‰¹æ€§ï¼Œæå‡ºäº†ä¸€ç§å…·æœ‰è‡ªç›‘ç£å­¦ä¹ åŠŸèƒ½çš„ç¨€ç–ISARæˆåƒå±•å¼€ç¥ç»ç½‘ç»œï¼Œåä¸ºSS-ISAR-Netã€‚å®ƒæœ‰æ•ˆåœ°å‡è½»äº†ç”±ç¨€ç–é›·è¾¾å›æ³¢å¼•èµ·çš„å…‰æ …ç“£é—®é¢˜ï¼Œä»…ä½¿ç”¨ç¨€ç–é›·è¾¾å›æ³¢æ•°æ®å³å¯å®ç°é«˜è´¨é‡è®­ç»ƒã€‚é€šè¿‡åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œæµ‹é‡æ•°æ®çš„å®éªŒéªŒè¯äº†æ‰€æå‡ºçš„SS-ISAR-Netç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01038v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå±•å¼€å¼ç¥ç»ç½‘ç»œçš„ç¨€ç–é€†åˆæˆå­”å¾„é›·è¾¾ï¼ˆISARï¼‰æˆåƒæ–¹æ³•å·²è¢«å¼€å‘ç”¨äºé«˜è´¨é‡å›¾åƒé‡å»ºï¼Œä½†é€šå¸¸éœ€è¦é…å¯¹çš„ISARå›¾åƒå’Œå›æ³¢æ•°æ®ï¼Œè·å–å›°éš¾ã€‚è§‚å¯Ÿåˆ°ä¸€ç§ç‰¹æ€§ï¼Œå³ISARç‰¹å®šçš„ç¨€ç–æµ‹é‡é…ç½®ä¸‹ï¼Œç›®æ ‡ç»•è´¨å¿ƒæ—‹è½¬æ—¶ï¼Œæˆåƒåçš„ç›®æ ‡ä¼šç›¸åº”æ—‹è½¬ï¼Œè€Œæ …ç“£ä¸ä¼šéšæ—‹è½¬æ”¹å˜ï¼Œä»…ç”±ç¨€ç–é‡‡æ ·æ¨¡å¼å†³å®šã€‚åˆ©ç”¨è¿™ä¸€ç‰¹æ€§ï¼Œæå‡ºäº†ä¸€ç§å…·æœ‰è‡ªç›‘ç£å­¦ä¹ èƒ½åŠ›çš„ç¨€ç–ISARæˆåƒå±•å¼€ç¥ç»ç½‘ç»œSS-ISAR-Netã€‚è¯¥æ–¹æ³•æœ‰æ•ˆå‡è½»äº†ç¨€ç–é›·è¾¾å›æ³¢å¼•èµ·çš„æ …ç“£é—®é¢˜ï¼Œå¹¶èƒ½ä»…ä½¿ç”¨ç¨€ç–é›·è¾¾å›æ³¢æ•°æ®è¿›è¡Œé«˜è´¨é‡è®­ç»ƒã€‚é€šè¿‡å®éªŒéªŒè¯ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSS-ISAR-Netè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŸºäºå±•å¼€ç¥ç»ç½‘ç»œçš„ç¨€ç–é€†åˆæˆå­”å¾„é›·è¾¾æˆåƒç”¨äºé«˜è´¨é‡å›¾åƒé‡å»ºã€‚</li>
<li>è·å–é…å¯¹çš„ISARå›¾åƒå’Œå›æ³¢æ•°æ®é€šå¸¸å¾ˆå›°éš¾ã€‚</li>
<li>ç›®æ ‡åœ¨ISARæˆåƒè¿‡ç¨‹ä¸­ç»•è´¨å¿ƒæ—‹è½¬æ—¶ï¼Œå›¾åƒä¸­çš„ç›®æ ‡ä¼šç›¸åº”æ—‹è½¬ï¼Œè€Œæ …ç“£ä¸éšæ—‹è½¬æ”¹å˜ã€‚</li>
<li>åˆ©ç”¨ä¸Šè¿°ç‰¹æ€§æå‡ºäº†SS-ISAR-Netç½‘ç»œæ¨¡å‹ã€‚</li>
<li>SS-ISAR-Netèƒ½æœ‰æ•ˆå‡è½»ç¨€ç–é›·è¾¾å›æ³¢å¼•èµ·çš„æ …ç“£é—®é¢˜ã€‚</li>
<li>ä»…ä½¿ç”¨ç¨€ç–é›·è¾¾å›æ³¢æ•°æ®å³å¯å®ç°é«˜è´¨é‡è®­ç»ƒã€‚</li>
<li>å®éªŒéªŒè¯äº†SS-ISAR-Netç›¸è¾ƒäºç°æœ‰æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼Œæ— è®ºæ˜¯åœ¨åˆæˆæ•°æ®è¿˜æ˜¯çœŸå®æµ‹é‡æ•°æ®ä¸Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01038">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7cd4fb36454ef6a2206ab01467e0b3a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a34b6c6a5d8826808a66ea12feb1fd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-950b2a524b49e7ee8dca641dbea75716.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c9edcb6b25275e69cd1a32d05e5082a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3c64306cf436e18cadc5dcd3a3d11c0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Modality-Translation-and-Registration-of-MR-and-Ultrasound-Images-Using-Diffusion-Models"><a href="#Modality-Translation-and-Registration-of-MR-and-Ultrasound-Images-Using-Diffusion-Models" class="headerlink" title="Modality Translation and Registration of MR and Ultrasound Images Using   Diffusion Models"></a>Modality Translation and Registration of MR and Ultrasound Images Using   Diffusion Models</h2><p><strong>Authors:Xudong Ma, Nantheera Anantrasirichai, Stefanos Bolomytis, Alin Achim</strong></p>
<p>Multimodal MR-US registration is critical for prostate cancer diagnosis. However, this task remains challenging due to significant modality discrepancies. Existing methods often fail to align critical boundaries while being overly sensitive to irrelevant details. To address this, we propose an anatomically coherent modality translation (ACMT) network based on a hierarchical feature disentanglement design. We leverage shallow-layer features for texture consistency and deep-layer features for boundary preservation. Unlike conventional modality translation methods that convert one modality into another, our ACMT introduces the customized design of an intermediate pseudo modality. Both MR and US images are translated toward this intermediate domain, effectively addressing the bottlenecks faced by traditional translation methods in the downstream registration task. Experiments demonstrate that our method mitigates modality-specific discrepancies while preserving crucial anatomical boundaries for accurate registration. Quantitative evaluations show superior modality similarity compared to state-of-the-art modality translation methods. Furthermore, downstream registration experiments confirm that our translated images achieve the best alignment performance, highlighting the robustness of our framework for multi-modal prostate image registration. </p>
<blockquote>
<p>å¤šæ¨¡æ€MR-USæ³¨å†Œå¯¹å‰åˆ—è…ºç™Œè¯Šæ–­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡æ€ä¹‹é—´çš„å·®å¼‚å¾ˆå¤§ï¼Œè¿™é¡¹ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•åœ¨å…³é”®è¾¹ç•Œå¯¹é½çš„åŒæ—¶ï¼Œåˆè¿‡äºæ•æ„Ÿåœ°å…³æ³¨ä¸ç›¸å…³çš„ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå±‚æ¬¡ç‰¹å¾åˆ†è§£è®¾è®¡çš„è§£å‰–è¿è´¯æ€§æ¨¡æ€ç¿»è¯‘ï¼ˆACMTï¼‰ç½‘ç»œã€‚æˆ‘ä»¬åˆ©ç”¨æµ…å±‚ç‰¹å¾å®ç°çº¹ç†ä¸€è‡´æ€§ï¼Œåˆ©ç”¨æ·±å±‚ç‰¹å¾å®ç°è¾¹ç•Œä¿ç•™ã€‚ä¸åŒäºå°†ä¸€ç§æ¨¡æ€è½¬æ¢ä¸ºå¦ä¸€ç§æ¨¡æ€çš„ä¼ ç»Ÿæ¨¡æ€ç¿»è¯‘æ–¹æ³•ï¼Œæˆ‘ä»¬çš„ACMTå¼•å…¥äº†ä¸­é—´ä¼ªæ¨¡æ€çš„å®šåˆ¶è®¾è®¡ã€‚MRå’ŒUSå›¾åƒéƒ½å‘è¿™ä¸ªä¸­é—´åŸŸè¿›è¡Œè½¬æ¢ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä¼ ç»Ÿç¿»è¯‘æ–¹æ³•åœ¨ä¸‹æ¸¸æ³¨å†Œä»»åŠ¡ä¸­æ‰€é¢ä¸´çš„ç“¶é¢ˆã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡è½»äº†æ¨¡æ€ç‰¹å®šçš„å·®å¼‚ï¼ŒåŒæ—¶ä¿ç•™äº†å…³é”®è§£å‰–è¾¹ç•Œï¼Œä»¥å®ç°å‡†ç¡®çš„æ³¨å†Œã€‚å®šé‡è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ¨¡æ€ç¿»è¯‘æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡æ€ç›¸ä¼¼æ€§æ›´é«˜ã€‚æ­¤å¤–ï¼Œä¸‹æ¸¸æ³¨å†Œå®éªŒè¯å®ï¼Œæˆ‘ä»¬çš„ç¿»è¯‘å›¾åƒå®ç°äº†æœ€ä½³çš„å¯¹é½æ€§èƒ½ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬æ¡†æ¶åœ¨å¤šæ¨¡æ€å‰åˆ—è…ºå›¾åƒæ³¨å†Œçš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01025v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå±‚æ¬¡ç‰¹å¾åˆ†ç¦»è®¾è®¡çš„è§£å‰–å­¦ä¸€è‡´æ€§æ¨¡æ€ç¿»è¯‘ï¼ˆACMTï¼‰ç½‘ç»œï¼Œç”¨äºè§£å†³å¤šæ¨¡æ€MR-USåœ¨å‰åˆ—è…ºç™Œè¯Šæ–­ä¸­çš„æ³¨å†Œé—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨æµ…å±‚ç‰¹å¾å®ç°çº¹ç†ä¸€è‡´æ€§å’Œæ·±å±‚ç‰¹å¾è¿›è¡Œè¾¹ç•Œä¿ç•™ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä¼ ç»Ÿæ¨¡æ€ç¿»è¯‘æ–¹æ³•åœ¨ä¸‹æ¸¸æ³¨å†Œä»»åŠ¡ä¸­é‡åˆ°çš„ç“¶é¢ˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡è½»æ¨¡æ€ç‰¹å®šå·®å¼‚çš„åŒæ—¶ï¼Œä¿ç•™äº†å…³é”®è§£å‰–è¾¹ç•Œï¼Œå®ç°äº†å‡†ç¡®çš„æ³¨å†Œã€‚ä¸æœ€å…ˆè¿›çš„æ¨¡æ€ç¿»è¯‘æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ€ç›¸ä¼¼æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œå¹¶åœ¨ä¸‹æ¸¸æ³¨å†Œå®éªŒä¸­è¯å®äº†å…¶æœ€ä½³å¯¹é½æ€§èƒ½ï¼Œå‡¸æ˜¾äº†å…¶åœ¨å¤šæ¨¡æ€å‰åˆ—è…ºå›¾åƒæ³¨å†Œä¸­çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€MR-USæ³¨å†Œå¯¹å‰åˆ—è…ºç™Œè¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨æ˜¾è‘—æ¨¡æ€å·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥å¯¹é½å…³é”®è¾¹ç•Œï¼ŒåŒæ—¶è¿‡äºæ•æ„Ÿäºæ— å…³ç»†èŠ‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå±‚æ¬¡ç‰¹å¾åˆ†ç¦»è®¾è®¡çš„è§£å‰–å­¦ä¸€è‡´æ€§æ¨¡æ€ç¿»è¯‘ï¼ˆACMTï¼‰ç½‘ç»œã€‚</li>
<li>ACMTç½‘ç»œåˆ©ç”¨æµ…å±‚ç‰¹å¾å®ç°çº¹ç†ä¸€è‡´æ€§å’Œæ·±å±‚ç‰¹å¾è¿›è¡Œè¾¹ç•Œä¿ç•™ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§ä¸­é—´ä¼ªæ¨¡æ€çš„å®šåˆ¶è®¾è®¡ï¼Œä½¿MRå’ŒUSå›¾åƒéƒ½å‘æ­¤ä¸­é—´é¢†åŸŸç¿»è¯‘ï¼Œè§£å†³ä¼ ç»Ÿç¿»è¯‘æ–¹æ³•åœ¨ä¸‹æ¸¸æ³¨å†Œä»»åŠ¡ä¸­çš„ç“¶é¢ˆã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ€ç›¸ä¼¼æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡æ€ç¿»è¯‘æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01025">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-76e8bf06ddaf8f5391b8ca7dc67cb932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad05220dddc200737804d7782dad65f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-583c9e624a099eda970e3443f74f112d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc3fd1883ecf3b543e0aa3bdfc6ec8c7.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SSAM-Self-Supervised-Association-Modeling-for-Test-Time-Adaption"><a href="#SSAM-Self-Supervised-Association-Modeling-for-Test-Time-Adaption" class="headerlink" title="SSAM: Self-Supervised Association Modeling for Test-Time Adaption"></a>SSAM: Self-Supervised Association Modeling for Test-Time Adaption</h2><p><strong>Authors:Yaxiong Wang, Zhenqiang Zhang, Lechao Cheng, Zhun Zhong, Dan Guo, Meng Wang</strong></p>
<p>Test-time adaption (TTA) has witnessed important progress in recent years, the prevailing methods typically first encode the image and the text and design strategies to model the association between them. Meanwhile, the image encoder is usually frozen due to the absence of explicit supervision in TTA scenarios. We identify a critical limitation in this paradigm: While test-time images often exhibit distribution shifts from training data, existing methods persistently freeze the image encoder due to the absence of explicit supervision during adaptation. This practice overlooks the image encoderâ€™s crucial role in bridging distribution shift between training and test. To address this challenge, we propose SSAM (Self-Supervised Association Modeling), a new TTA framework that enables dynamic encoder refinement through dual-phase association learning. Our method operates via two synergistic components: 1) Soft Prototype Estimation (SPE), which estimates probabilistic category associations to guide feature space reorganization, and 2) Prototype-anchored Image Reconstruction (PIR), enforcing encoder stability through cluster-conditional image feature reconstruction. Comprehensive experiments across diverse baseline methods and benchmarks demonstrate that SSAM can surpass state-of-the-art TTA baselines by a clear margin while maintaining computational efficiency. The frameworkâ€™s architecture-agnostic design and minimal hyperparameter dependence further enhance its practical applicability. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæµ‹è¯•æ—¶é€‚åº”ï¼ˆTTAï¼‰æŠ€æœ¯å–å¾—äº†é‡è¦è¿›å±•ã€‚ç›®å‰ä¸»æµçš„æ–¹æ³•é€šå¸¸é¦–å…ˆå¯¹å›¾åƒå’Œæ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå¹¶è®¾è®¡ç­–ç•¥å¯¹å®ƒä»¬ä¹‹é—´çš„å…³è”è¿›è¡Œå»ºæ¨¡ã€‚åŒæ—¶ï¼Œç”±äºTTAåœºæ™¯ä¸­ç¼ºä¹æ˜ç¡®çš„ç›‘ç£ä¿¡æ¯ï¼Œå›¾åƒç¼–ç å™¨é€šå¸¸è¢«å†»ç»“ã€‚æˆ‘ä»¬å‘ç°äº†è¿™ä¸€æ¨¡å¼çš„å…³é”®å±€é™ï¼šè™½ç„¶æµ‹è¯•æ—¶çš„å›¾åƒå¾€å¾€è¡¨ç°å‡ºä¸è®­ç»ƒæ•°æ®ä¸åŒçš„åˆ†å¸ƒï¼Œä½†ç”±äºé€‚åº”è¿‡ç¨‹ä¸­ç¼ºä¹æ˜ç¡®çš„ç›‘ç£ï¼Œç°æœ‰æ–¹æ³•ä»ç„¶åšæŒå†»ç»“å›¾åƒç¼–ç å™¨ã€‚è¿™ç§åšæ³•å¿½è§†äº†å›¾åƒç¼–ç å™¨åœ¨å¼¥åˆè®­ç»ƒå’Œæµ‹è¯•ä¹‹é—´åˆ†å¸ƒè½¬ç§»æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SSAMï¼ˆè‡ªç›‘ç£å…³è”å»ºæ¨¡ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„TTAæ¡†æ¶ï¼Œé€šè¿‡åŒé˜¶æ®µå…³è”å­¦ä¹ å®ç°åŠ¨æ€ç¼–ç å™¨ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”±ä¸¤ä¸ªååŒç»„ä»¶ç»„æˆï¼š1ï¼‰è½¯åŸå‹ä¼°è®¡ï¼ˆSPEï¼‰ï¼Œå®ƒä¼°è®¡æ¦‚ç‡ç±»åˆ«å…³è”ä»¥å¼•å¯¼ç‰¹å¾ç©ºé—´é‡ç»„ï¼›2ï¼‰ä»¥åŸå‹ä¸ºä¸­å¿ƒçš„å›¾åƒé‡å»ºï¼ˆPIRï¼‰ï¼Œé€šè¿‡èšç±»æ¡ä»¶å›¾åƒç‰¹å¾é‡å»ºæ¥åŠ å¼ºç¼–ç å™¨çš„ç¨³å®šæ€§ã€‚åœ¨å¤šç§åŸºå‡†æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒSSAMå¯ä»¥æ˜æ˜¾è¶…è¶Šæœ€æ–°çš„TTAåŸºå‡†æµ‹è¯•ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚è¯¥æ¡†æ¶çš„æ¶æ„æ— å…³è®¾è®¡å’Œè¾ƒå°çš„è¶…å‚æ•°ä¾èµ–æ€§è¿›ä¸€æ­¥å¢å¼ºäº†å…¶å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00513v1">PDF</a> 10 papges</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„æµ‹è¯•æ—¶é€‚åº”ï¼ˆTTAï¼‰æ¡†æ¶SSAMï¼ˆSelf-Supervised Association Modelingï¼‰ï¼Œé€šè¿‡åŒé˜¶æ®µå…³è”å­¦ä¹ å®ç°åŠ¨æ€ç¼–ç å™¨ä¼˜åŒ–ï¼Œè§£å†³ç°æœ‰æ–¹æ³•å¿½ç•¥å›¾åƒç¼–ç å™¨åœ¨é€‚åº”è®­ç»ƒä¸æµ‹è¯•æ•°æ®åˆ†å¸ƒå˜åŒ–ä¸­çš„é‡è¦æ€§ã€‚SSAMåŒ…å«ä¸¤ä¸ªååŒç»„ä»¶ï¼šSoft Prototype Estimationï¼ˆSPEï¼‰å’ŒPrototype-anchored Image Reconstructionï¼ˆPIRï¼‰ï¼Œåˆ†åˆ«ç”¨äºå¼•å¯¼ç‰¹å¾ç©ºé—´é‡ç»„å’Œå¼ºåŒ–ç¼–ç å™¨ç¨³å®šæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒSSAMåœ¨å¤šç§åŸºå‡†æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†ç°æœ‰TTAæ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é€‚åº”ï¼ˆTTAï¼‰æ¡†æ¶SSAMé€šè¿‡åŒé˜¶æ®µå…³è”å­¦ä¹ å®ç°åŠ¨æ€ç¼–ç å™¨ä¼˜åŒ–ã€‚</li>
<li>SSAMè§£å†³äº†ç°æœ‰æ–¹æ³•å¿½ç•¥å›¾åƒç¼–ç å™¨åœ¨é€‚åº”è®­ç»ƒä¸æµ‹è¯•æ•°æ®åˆ†å¸ƒå˜åŒ–ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>SSAMåŒ…å«ä¸¤ä¸ªååŒç»„ä»¶ï¼šSoft Prototype Estimationï¼ˆSPEï¼‰å’ŒPrototype-anchored Image Reconstructionï¼ˆPIRï¼‰ã€‚</li>
<li>SPEé€šè¿‡ä¼°è®¡æ¦‚ç‡ç±»åˆ«å…³è”æ¥å¼•å¯¼ç‰¹å¾ç©ºé—´é‡ç»„ã€‚</li>
<li>PIRé€šè¿‡é›†ç¾¤æ¡ä»¶å›¾åƒç‰¹å¾é‡å»ºæ¥å¼ºåŒ–ç¼–ç å™¨ç¨³å®šæ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSSAMåœ¨å¤šä¸ªåŸºå‡†æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†ç°æœ‰TTAæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e23e455cf5a897ccf7094342a6faff3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e5b5e4252f62169efb16a4bea69a8fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf4327edecd17f9d16471030eed22918.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-be64077dbb482c82f476bdd21fb1ad4e.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich   Representation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4ff2737bf419a0bcca28f1acc438e1a6.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  EDITOR Effective and Interpretable Prompt Inversion for Text-to-Image   Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
