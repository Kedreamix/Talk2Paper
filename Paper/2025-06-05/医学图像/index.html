<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-06-05  Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-971ee3a8f96ee3b15eaa632cec6e76c5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    72 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-05-更新"><a href="#2025-06-05-更新" class="headerlink" title="2025-06-05 更新"></a>2025-06-05 更新</h1><h2 id="Zero-Shot-Tree-Detection-and-Segmentation-from-Aerial-Forest-Imagery"><a href="#Zero-Shot-Tree-Detection-and-Segmentation-from-Aerial-Forest-Imagery" class="headerlink" title="Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery"></a>Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery</h2><p><strong>Authors:Michelle Chen, David Russell, Amritha Pallavoor, Derek Young, Jane Wu</strong></p>
<p>Large-scale delineation of individual trees from remote sensing imagery is crucial to the advancement of ecological research, particularly as climate change and other environmental factors rapidly transform forest landscapes across the world. Current RGB tree segmentation methods rely on training specialized machine learning models with labeled tree datasets. While these learning-based approaches can outperform manual data collection when accurate, the existing models still depend on training data that’s hard to scale. In this paper, we investigate the efficacy of using a state-of-the-art image segmentation model, Segment Anything Model 2 (SAM2), in a zero-shot manner for individual tree detection and segmentation. We evaluate a pretrained SAM2 model on two tasks in this domain: (1) zero-shot segmentation and (2) zero-shot transfer by using predictions from an existing tree detection model as prompts. Our results suggest that SAM2 not only has impressive generalization capabilities, but also can form a natural synergy with specialized methods trained on in-domain labeled data. We find that applying large pretrained models to problems in remote sensing is a promising avenue for future progress. We make our code available at: <a target="_blank" rel="noopener" href="https://github.com/open-forest-observatory/tree-detection-framework">https://github.com/open-forest-observatory/tree-detection-framework</a>. </p>
<blockquote>
<p>从遥感影像大规模勾画单棵树对于生态研究的进步至关重要，特别是随着气候变化和其他环境因素迅速改变全球森林景观。当前的RGB树分割方法依赖于使用带有标签的树数据集训练专门的机器学习模型。虽然这些基于学习的方法在准确时可以表现出超越手动数据收集的效能，但现有模型仍然依赖于难以扩展的训练数据。在本文中，我们调查了使用最先进的图像分割模型——Segment Anything Model 2（SAM2）进行零样本个人树木检测和分割的效用。我们评估了预训练的SAM2模型在该领域的两个任务：（1）零样本分割和（2）使用现有树木检测模型的预测结果作为提示进行零样本迁移。我们的结果表明，SAM2不仅具有令人印象深刻的泛化能力，而且可以与在领域标签数据上训练的专用方法形成自然的协同作用。我们发现将大型预训练模型应用于遥感问题是一个很有前途的未来发展方向。我们的代码可在：<a target="_blank" rel="noopener" href="https://github.com/open-forest-observatory/tree-detection-framework%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/open-forest-observatory/tree-detection-framework找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03114v1">PDF</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/open-forest-observatory/tree-detection-framework">https://github.com/open-forest-observatory/tree-detection-framework</a></p>
<p><strong>Summary</strong></p>
<p>在大规模遥感影像中精准勾画出树木个体对于生态研究至关重要，特别是随着气候变化和环境因素快速改变全球森林景观。当前RGB树分割方法依赖于使用标记树数据集训练专门的机器学习模型。虽然这些基于学习的方法在准确性方面可以超越手动数据收集，但现有模型仍然依赖于难以扩展的训练数据。本文调查了使用最新图像分割模型Segment Anything Model 2（SAM2）进行零样本个人树木检测和分割的有效性。我们在两个任务上评估了预训练的SAM2模型：（1）零样本分割和（2）使用现有树木检测模型的预测结果作为提示进行零样本迁移。结果表明，SAM2不仅具有良好的泛化能力，而且可以与在域内标记数据上训练的专门方法形成自然协同。我们发现将大型预训练模型应用于遥感问题是一个值得未来关注的途径。我们提供的代码可以在<a target="_blank" rel="noopener" href="https://github.com/open-forest-observatory/tree-detection-framework%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/open-forest-observatory/tree-detection-framework找到。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模从遥感影像中勾画树木对生态研究至关重要，尤其在气候变化影响森林景观的背景下。</li>
<li>当前RGB树分割方法依赖于训练有标签的机器学习模型，但这种方法难以扩展。</li>
<li>Segment Anything Model 2 (SAM2)模型在零样本个人树木检测和分割上表现出良好效果。</li>
<li>SAM2模型具有显著泛化能力，并能与针对特定数据训练的专门方法协同工作。</li>
<li>使用预训练的的大型模型处理遥感问题是未来研究的一个有前途的方向。</li>
<li>本文评估了两种任务：零样本分割和零样本迁移，后者通过使用现有树木检测模型的预测结果作为提示来实现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03114">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6d946d8057ea6e312be171578e559003.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c6bef55ee6ef5a27b4c5c99ebbf62de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbff572a3e5dca57ccc9fd6549dbe6ec.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Self-Prompting-SAM-A-Prompt-Free-Medical-Image-Segmentation-Framework"><a href="#Hierarchical-Self-Prompting-SAM-A-Prompt-Free-Medical-Image-Segmentation-Framework" class="headerlink" title="Hierarchical Self-Prompting SAM: A Prompt-Free Medical Image   Segmentation Framework"></a>Hierarchical Self-Prompting SAM: A Prompt-Free Medical Image   Segmentation Framework</h2><p><strong>Authors:Mengmeng Zhang, Xingyuan Dai, Yicheng Sun, Jing Wang, Yueyang Yao, Xiaoyan Gong, Fuze Cong, Feiyue Wang, Yisheng Lv</strong></p>
<p>Although the Segment Anything Model (SAM) is highly effective in natural image segmentation, it requires dependencies on prompts, which limits its applicability to medical imaging where manual prompts are often unavailable. Existing efforts to fine-tune SAM for medical segmentation typically struggle to remove this dependency. We propose Hierarchical Self-Prompting SAM (HSP-SAM), a novel self-prompting framework that enables SAM to achieve strong performance in prompt-free medical image segmentation. Unlike previous self-prompting methods that remain limited to positional prompts similar to vanilla SAM, we are the first to introduce learning abstract prompts during the self-prompting process. This simple and intuitive self-prompting framework achieves superior performance on classic segmentation tasks such as polyp and skin lesion segmentation, while maintaining robustness across diverse medical imaging modalities. Furthermore, it exhibits strong generalization to unseen datasets, achieving improvements of up to 14.04% over previous state-of-the-art methods on some challenging benchmarks. These results suggest that abstract prompts encapsulate richer and higher-dimensional semantic information compared to positional prompts, thereby enhancing the model’s robustness and generalization performance. All models and codes will be released upon acceptance. </p>
<blockquote>
<p>尽管Segment Anything Model（SAM）在自然图像分割中效果显著，但它依赖于提示，这在手动提示通常不可用的医学成像中限制了其适用性。现有的针对医学分割的SAM微调努力通常难以消除这种依赖。我们提出了分层自提示SAM（HSP-SAM），这是一种新型的自提示框架，使SAM在无提示医学图像分割中实现了强大的性能。与以往仅限于位置提示的自提示方法（类似于普通SAM）不同，我们是第一个在自提示过程中引入抽象提示的。这种简单直观的自提示框架在经典分割任务（如息肉和皮肤病变分割）上实现了卓越的性能，同时在多种医学成像模态中保持了稳健性。此外，它在未见数据集上表现出强大的泛化能力，在一些具有挑战性的基准测试上较之前的最先进方法提高了高达14.04%。这些结果表明，与位置提示相比，抽象提示封装了更丰富、更高维度的语义信息，从而增强了模型的稳健性和泛化性能。所有模型和代码将在接受后发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02854v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Segment Anything Model（SAM）在自然图像分割中的出色表现，但其在医学图像分割应用中受限于需要提示的问题。为解决此问题，我们提出了分层自提示框架HSP-SAM，使SAM在无提示医学图像分割中表现出强劲性能。HSP-SAM引入抽象提示学习，实现超越经典分割任务如息肉和皮肤病变分割的优异表现，同时适用于多种医学影像模态。其展现出对未见数据集的强大泛化能力，在一些具有挑战性的基准测试中较先前最先进的方法提升了最多达14.04%。这些结果暗示抽象提示包含更丰富和高维度的语义信息，增强了模型的稳健性和泛化性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HSP-SAM是一个新颖的分层自提示框架，旨在解决Segment Anything Model在医学图像分割中对提示的依赖问题。</li>
<li>与现有的自提示方法不同，HSP-SAM首次引入了抽象提示学习过程。</li>
<li>HSP-SAM框架超越了经典分割任务如息肉和皮肤病变分割的优异表现。</li>
<li>HSP-SAM在各种医学影像模态上展现出稳健性。</li>
<li>HSP-SAM具有强大的泛化能力，对未见数据集表现良好。</li>
<li>与先前最先进的方法相比，HSP-SAM在某些挑战性基准测试中提高了最多达14.04%的性能。</li>
<li>结果表明，抽象提示较传统位置提示包含了更丰富、更高维度的语义信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02854">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5ae849344b9fe849025b4d8b35284ea1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-022718f52859359e23ec2bfe88c5ea11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6997e920d446b16a978bab80d56d054d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9fa179abb1b6be53ca970f52a26b63e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multi-wavelength-analysis-of-FSRQ-B2-1348-30B-Constraints-on-the-jet-power"><a href="#Multi-wavelength-analysis-of-FSRQ-B2-1348-30B-Constraints-on-the-jet-power" class="headerlink" title="Multi-wavelength analysis of FSRQ B2 1348+30B: Constraints on the jet   power"></a>Multi-wavelength analysis of FSRQ B2 1348+30B: Constraints on the jet   power</h2><p><strong>Authors:Sajad Ahanger, Sunder Sahayanathan, Sitha K. Jagan, Shah Zahir, Naseer Iqbal</strong></p>
<p>We present 14.5-year multi-wavelength analysis of flat-spectrum radio quasar B2 1348+30B using Swift-UVOT, Swift-XRT, and Fermi-LAT observations. In the gamma-ray band, the 3 day bin lightcurve reveals two major flaring events on 2010-09-19 (55458 MJD) and 2022-05-26 (59725 MJD) detected at flux levels $(2.5\pm 0.5) \times 10^{-7},\rm{ph,cm^{-2},s^{-1}}$ and $(5.2\pm 0.6) \times 10^{-7},\rm{ph,cm^{-2},s^{-1}}$. The Bayesian block analysis of the flares suggested the variability timescale to be $\leq$ 3,day. To study the dynamic nature of the source, multi-wavelength spectrum was obtained for three flux states which includes the two flaring state and a relative low state. The $\gamma$-ray spectra of the source in all the states are well fitted by a power-law model with maximum photon energy &lt; 20 GeV. In X-ray, a power-law model can explain the flaring state spectra while a broken power-law with extremely hard high energy component was required to model the low flux state. This indicates the presence of the low energy cutoff in the Compton spectral component. A simple one-zone leptonic model involving synchrotron, synchrotron self Compton and external Compton mechanism can successfully reproduce the broad-band spectral energy distribution of all the flux states. The model parameters suggest significant increase in the jet Lorentz factor during the high flux states. Further, the best fit parameters are used to constrain the minimum energy of the emitting electron distribution from the hard high energy spectrum of the low flux state. This analysis was extended to draw limits on the kinetic power of the blazar jet and was compared with the Eddington luminosity of the central black hole. </p>
<blockquote>
<p>我们利用Swift-UVOT、Swift-XRT和Fermi-LAT观测数据，对平谱射电类星体B2 1348+30B进行了长达14.5年的多波长分析。在伽马射线波段，为期3天的光变曲线显示，在历元时刻55458 MJD（即2010年9月19日）和历元时刻59725 MJD（即2022年5月26日）发生了两次主要耀发事件，流量水平分别为（2.5±0.5）× 10^-7 ph cm^-2 s^-1和（5.2±0.6）× 10^-7 ph cm^-2 s^-1。耀发事件的贝叶斯块分析表明其变化时间尺度不超过3天。为了研究源动态特性，我们获得了三种流量状态下的多波长光谱，包括两次耀发状态和一次相对低状态。源在所有状态下的伽马射线光谱均可用幂律模型很好地拟合，最大光子能量小于20 GeV。在X射线波段，幂律模型可以解释耀发状态的光谱，而对低流量状态则需要一个具有极硬高能分量的截断幂律模型来描述。这表明康普顿光谱分量中存在低能截断。一个涉及同步辐射、同步自康普顿和外部康普顿机制的简单单区轻子模型能够成功地再现所有流量状态的宽频谱能量分布。模型参数表明在高流量状态下洛伦兹因子显著增加。此外，最佳拟合参数被用来限制低流量状态硬高能谱发射电子分布的最小能量。这一分析被用来对喷流的动力学功率设置限制，并与中心黑洞的艾丁顿光度进行比较。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02821v1">PDF</a> 14 pages, 6 figures, 5 tables, Accepted for publication in JHEAP</p>
<p><strong>Summary</strong></p>
<p>在长达14.5年的研究中，我们对具有平坦光谱的射电类星体B2 1348+30B进行了多波长分析，使用了Swift-UVOT、Swift-XRT和Fermi-LAT观测数据。在伽马射线波段，发现了两个主要耀斑事件，其光变曲线揭示了耀斑活动的动态特征。多波长光谱分析表明，源在不同流量状态下的光谱特征有所不同，这支持了使用简单的一区莱普顿模型来解释其谱能量分布的观点。此外，该分析还提供了对喷流洛伦兹因子和电子分布最小能量的了解，并进一步对类星体喷流的动力进行了限制和比较。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>使用Swift-UVOT、Swift-XRT和Fermi-LAT观测数据对平谱射电类星体B2 1348+30B进行了长达14.5年的多波长分析。</li>
<li>在伽马射线波段发现两个主要耀斑事件，揭示了耀斑活动的动态特征。</li>
<li>通过多波长光谱分析发现不同流量状态下的源具有不同的光谱特性。</li>
<li>简单的一区莱普顿模型成功地再现了所有流量状态的谱能量分布。</li>
<li>源在低流量状态下的高能谱需要硬的高能成分来解释，暗示了Compton谱成分的低能截断的存在。</li>
<li>源的喷流洛伦兹因子在高流量状态下显著增加。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02821">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-af60d2267558211faae5388094fc6118.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca028bfeac51c119c1a820fe01cf0eef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f48ee11a73a2b1247c7508ca1328c68.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-25f298ff1d558e30a075296d26325a72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b60961eb680596b8828bc2e24de779f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4aea9b9747bb526c961e5765bf41d801.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c8c6f1898efa95c62fb33cb438440e2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="On-the-influence-of-language-similarity-in-non-target-speaker-verification-trials"><a href="#On-the-influence-of-language-similarity-in-non-target-speaker-verification-trials" class="headerlink" title="On the influence of language similarity in non-target speaker   verification trials"></a>On the influence of language similarity in non-target speaker   verification trials</h2><p><strong>Authors:Paul M. Reuter, Michael Jessen</strong></p>
<p>In this paper, we investigate the influence of language similarity in cross-lingual non-target speaker verification trials using a state-of-the-art speaker verification system, ECAPA-TDNN, trained on multilingual and monolingual variants of the VoxCeleb dataset. Our analysis of the score distribution patterns on multilingual Globalphone and LDC CTS reveals a clustering effect in speaker comparisons involving a training language, whereby the choice of comparison language only minimally impacts scores. Conversely, we observe a language similarity effect in trials involving languages not included in the training set of the speaker verification system, with scores correlating with language similarity measured by a language classification system, especially when using multilingual training data. </p>
<blockquote>
<p>本文中，我们研究语言相似性对使用最前沿说话人验证系统ECAPA-TDNN进行的跨语言非目标说话人验证试验的影响。该系统是在VoxCeleb数据集的多语种和单语种变体上进行训练的。我们对多语种Globalphone和LDC CTS上的得分分布模式进行分析，发现在涉及训练语言的说话人比较中出现了聚类效应，所选的比较语言对得分的影响微乎其微。相反，在涉及不在说话人验证系统训练集中的语言的试验中，我们观察到语言相似性效应，得分与由语言分类系统测得的语言相似性相关，特别是在使用多语种训练数据时。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02777v1">PDF</a> accepted to Interspeech 2025</p>
<p><strong>Summary</strong><br>     本文探讨了语言相似性对跨语种非目标说话人验证试验的影响。研究使用了基于最前沿的说话人验证系统ECAPA-TDNN，该系统在VoxCeleb数据集的多语种和单语种变体上进行训练。分析Globalphone和LDC CTS上的得分分布模式显示，在涉及训练语言的说话人比较中，对比语言的选择对得分影响甚微，但在涉及不在验证系统训练集中的语言的试验中，观察到语言相似性对得分有显著影响，尤其是使用多语种训练数据时，得分与语言分类系统的语言相似性度量相关。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究了跨语种非目标说话人验证中语言相似性的影响。</li>
<li>采用ECAPA-TDNN这一先进的说话人验证系统进行分析。</li>
<li>在涉及训练语言的说话人比较中，对比语言的选择对得分影响较小。</li>
<li>在涉及非训练语言时，观察到语言相似性对得分有显著影响。</li>
<li>多语种训练数据在衡量语言相似性时尤为重要。</li>
<li>得分分布模式在跨语种验证中呈现出语言聚类效应。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02777">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dd1a35d36f9749436ffed39905ae1654.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d0ccb495624acc33b1fbc03580a9808.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e03fd61acd7d564052037beb3fb292ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45447514b8c763c1b5c747bcca7d47d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c4d46ef17729fd085fb81f3190a5c94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b9f17089fe935cc9a945020741f1c00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2bf8e22ad247b3134b073c5f872b648.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Simple-Good-Fast-Self-Supervised-World-Models-Free-of-Baggage"><a href="#Simple-Good-Fast-Self-Supervised-World-Models-Free-of-Baggage" class="headerlink" title="Simple, Good, Fast: Self-Supervised World Models Free of Baggage"></a>Simple, Good, Fast: Self-Supervised World Models Free of Baggage</h2><p><strong>Authors:Jan Robine, Marc Höftmann, Stefan Harmeling</strong></p>
<p>What are the essential components of world models? How far do we get with world models that are not employing RNNs, transformers, discrete representations, and image reconstructions? This paper introduces SGF, a Simple, Good, and Fast world model that uses self-supervised representation learning, captures short-time dependencies through frame and action stacking, and enhances robustness against model errors through data augmentation. We extensively discuss SGF’s connections to established world models, evaluate the building blocks in ablation studies, and demonstrate good performance through quantitative comparisons on the Atari 100k benchmark. </p>
<blockquote>
<p>世界模型的必要组成部分是什么？在不使用RNN、transformer、离散表示和图像重建的情况下，我们能走多远？本文介绍了SGF，这是一种简单、良好和快速的世界模型，它采用自监督表示学习，通过帧和动作堆叠捕捉短期依赖性，并通过数据增强提高模型误差的稳健性。我们深入讨论了SGF与现有世界模型之间的联系，在消融研究中评估了各个组成部分，并在Atari 100k基准测试上通过定量比较展示了良好的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02612v1">PDF</a> Published as a conference paper at ICLR 2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/jrobine/sgf">https://github.com/jrobine/sgf</a></p>
<p><strong>Summary</strong><br>世界模型的必要组成部分是什么？不使用RNNs、transformers、离散表示和图像重建的世界模型能走多远？本文介绍了一种简单、良好和快速的SGF世界模型，该模型采用自监督表示学习，通过帧和动作堆叠捕捉短期依赖关系，并通过数据增强提高模型误差的稳健性。本文广泛讨论了SGF与现有世界模型的关联，通过局部消融研究评估了构建块，并在Atari 100k基准测试中通过定量比较展示了良好的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SGF是一种简单、良好和快速的世界模型。</li>
<li>SGF采用自监督表示学习。</li>
<li>SGF通过帧和动作堆叠捕捉短期依赖关系。</li>
<li>SGF通过数据增强提高模型误差的稳健性。</li>
<li>SGF与现有世界模型的关联被广泛讨论。</li>
<li>消融研究评估了SGF的构建块。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1b2714fec78a342dd3bb8bb8d741af26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac5f5ab225dfe95ef8d95726393137bd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Insight-into-the-origin-of-multiwavelength-emissions-of-PKS-1510-089-through-modeling-12-SEDs-from-2008-to-2015"><a href="#Insight-into-the-origin-of-multiwavelength-emissions-of-PKS-1510-089-through-modeling-12-SEDs-from-2008-to-2015" class="headerlink" title="Insight into the origin of multiwavelength emissions of PKS 1510-089   through modeling 12 SEDs from 2008 to 2015"></a>Insight into the origin of multiwavelength emissions of PKS 1510-089   through modeling 12 SEDs from 2008 to 2015</h2><p><strong>Authors:Maichang Lei, Yuan Zheng, Jianfu Zhang, Yuhai Yuan, Jiancheng Wang</strong></p>
<p>PKS,1510$-$089 is one of the most peculiar sources among the FSRQs, exhibiting a notable big blue bump (BBB). This provides an unique opportunity to explore the coupling between the activity of the central engine and the relativistic jet, offering further insight into the origin of the multiwavelength emissions. To this end, we collected multiwavelength data spanning four periods from 2008 to 2015 and performed the spectral energy distribution (SED) modeling using a one-zone homogeneous leptonic model. In the model, a multichromatic accretion disk (AD) is used to fit the optical&#x2F;UV data sets, while the external radiation fields from the broad-line region (BLR) and dusty torus (DT) are properly considered to produce the high-energy $\gamma$-ray emissions. Our best fit to 12 SEDs yields the following results: (i) The innermost stable orbit ($R_{\rm ISO}$) of the AD is not stable but varies between $3,R_{\rm S}$ and $18,R_{\rm S}$ during these observations. (ii) The high-energy hump of the SED is well dominated by Compton scattering of the BLR photons, while the X-ray flux may be comprised of multiple radiation components. (iii) The $\gamma$-ray emitting regions are generally matter-dominated, with low magnetization, and are located beyond the BLR but within the DT. At such distance, the multiwavelength emissions are likely to originate from shock accelerations; (iv) For the energization of the relativistic jet, our study supports the Blandford$-$Znajek (BZ) mechanism, instead of the Blandford$-$Payne (BP) mechanism, as the latter fails to power the jet. </p>
<blockquote>
<p>PKS 1510-089是FSRQs中最特殊的源之一，表现出显著的大蓝包（BBB）。这为探索中心发动机活动与相对论喷流之间的耦合提供了独特的机会，为进一步深入了解多波长发射的起源提供了启示。为此，我们收集了2008年至2015年的四期多波长数据，并使用单一区域均匀轻子模型进行了谱能量分布（SED）建模。在模型中，使用多色吸积盘（AD）来拟合光学&#x2F;紫外数据集，同时适当考虑了来自宽线区（BLR）和尘埃环（DT）的外部辐射场，以产生高能伽马射线发射。我们对12个SED的最佳拟合得出以下结果：（i）AD的最内稳定轨道（RISO）不稳定，在这些观测期间在3Rs和18Rs之间变化。（ii）SED的高能峰主要由BLR光子的康普顿散射占据主导地位，而X射线流量可能由多个辐射成分组成。（iii）伽马射线发射区域通常是物质主导的，具有低磁化强度，位于BLR之外但在DT之内。在这样的距离下，多波长发射很可能源于冲击加速；（iv）关于相对论喷流的能量化，我们的研究支持Blandford-Znajek（BZ）机制，而不是Blandford-Payne（BP）机制，因为后者无法为喷流提供动力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02500v1">PDF</a> 33 pages, 13 figure, 5 Tables. Accepted for publication in ApJ</p>
<p><strong>Summary</strong><br>    PKS 1510$-$089是FSRQs中最奇特的源之一，具有显著的大蓝包（BBB）。通过对其的研究，可以更好地了解中心引擎活动与相对论喷流的耦合关系，进而深入研究多波长发射的起源。研究使用单一区域均匀莱普顿模型进行谱能量分布（SED）建模，发现AD的最内稳定轨道不稳定，高能量凸起主要由BLR光子的康普顿散射主导，伽马射线发射区域通常物质占主导，且位于BLR之外但DT之内。研究支持BZ机制为相对论喷流提供能量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PKS 1510$-$089是一个具有显著大蓝包（BBB）的FSRQs特殊源，为研究中心引擎活动与相对论喷流耦合关系提供了独特机会。</li>
<li>通过SED建模发现AD最内稳定轨道不稳定，且在不同观察时期有所变化。</li>
<li>高能凸起主要由BLR光子的康普顿散射主导。</li>
<li>伽马射线发射区域通常物质占主导，且位于BLR之外但DT之内，多波长发射可能源于冲击加速。</li>
<li>研究结果支持BZ机制为相对论喷流提供能量，而BP机制可能无法提供足够的能量。</li>
<li>X射线流量可能由多种辐射成分组成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02500">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-86eea578236c2d384fd28ce92a101a06.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af0083cd6be229f38058d0fda6a92bce.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-modal-brain-MRI-synthesis-based-on-SwinUNETR"><a href="#Multi-modal-brain-MRI-synthesis-based-on-SwinUNETR" class="headerlink" title="Multi-modal brain MRI synthesis based on SwinUNETR"></a>Multi-modal brain MRI synthesis based on SwinUNETR</h2><p><strong>Authors:Haowen Pang, Weiyan Guo, Chuyang Ye</strong></p>
<p>Multi-modal brain magnetic resonance imaging (MRI) plays a crucial role in clinical diagnostics by providing complementary information across different imaging modalities. However, a common challenge in clinical practice is missing MRI modalities. In this paper, we apply SwinUNETR to the synthesize of missing modalities in brain MRI. SwinUNETR is a novel neural network architecture designed for medical image analysis, integrating the strengths of Swin Transformer and convolutional neural networks (CNNs). The Swin Transformer, a variant of the Vision Transformer (ViT), incorporates hierarchical feature extraction and window-based self-attention mechanisms, enabling it to capture both local and global contextual information effectively. By combining the Swin Transformer with CNNs, SwinUNETR merges global context awareness with detailed spatial resolution. This hybrid approach addresses the challenges posed by the varying modality characteristics and complex brain structures, facilitating the generation of accurate and realistic synthetic images. We evaluate the performance of SwinUNETR on brain MRI datasets and demonstrate its superior capability in generating clinically valuable images. Our results show significant improvements in image quality, anatomical consistency, and diagnostic value. </p>
<blockquote>
<p>多模态脑磁共振成像（MRI）在临床诊断中扮演着重要角色，因为它能够提供不同成像模式之间的补充信息。然而，临床实践中的一个常见挑战是MRI模式缺失。在本文中，我们将SwinUNETR应用于脑MRI中缺失模式的合成。SwinUNETR是一种新型的神经网络架构，专为医学图像分析而设计，融合了Swin Transformer和卷积神经网络（CNN）的优势。Swin Transformer是Vision Transformer（ViT）的一种变体，结合了分层特征提取和基于窗口的自注意力机制，能够有效地捕捉局部和全局上下文信息。通过将Swin Transformer与CNN相结合，SwinUNETR融合了全局上下文感知和详细的空间分辨率。这种混合方法解决了由不同的模态特征和复杂的脑结构所带来的挑战，促进了准确且逼真的合成图像的产生。我们在脑MRI数据集上评估了SwinUNETR的性能，并展示了其在生成具有临床价值图像方面的卓越能力。我们的结果在图像质量、解剖一致性和诊断价值方面显示出显著改善。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02467v1">PDF</a> 9 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>多模态脑磁共振成像（MRI）在临床诊断中扮演重要角色，能提供不同成像模式的信息互补。针对临床实践中缺失MRI模态的问题，本文应用SwinUNETR进行脑MRI缺失模态的合成。SwinUNETR是一种结合了Swin Transformer和卷积神经网络（CNN）优势的新型神经网络架构，用于医学图像分析。它通过结合Swin Transformer的分层特征提取和基于窗口的自注意力机制，有效捕捉局部和全局上下文信息。将Swin Transformer与CNN相结合，SwinUNETR融合了全局上下文感知和详细的空间分辨率。这种混合方法解决了不同模态特征和复杂脑结构带来的挑战，能够生成准确且逼真的合成图像。在脑MRI数据集上评估SwinUNETR的性能，证明了其在生成具有临床价值图像方面的卓越能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态脑磁共振成像（MRI）对于临床诊断至关重要，能融合不同成像模式的信息。</li>
<li>缺失MRI模态是临床实践中一个常见问题。</li>
<li>SwinUNETR是一种用于医学图像分析的神经网络架构，结合了Swin Transformer和卷积神经网络（CNN）。</li>
<li>SwinUNETR能够生成准确且逼真的合成图像，解决不同模态特征和复杂脑结构带来的挑战。</li>
<li>SwinUNETR在脑MRI数据集上的性能表现优异，能生成具有临床价值的图像。</li>
<li>SwinUNETR通过结合全局上下文感知和详细的空间分辨率，提高了图像质量和诊断价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02467">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-101d4e8c0e012b217349f8c3a7cb6d6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2960ebb0017918f9368b90f15a99fe5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b3f299fa53d6a0a187b0f6c228aa01f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11edaf5738dc5a5d9be2ca99c1e933e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc66021e19ac7c1de23c57cf8e2c5b0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7fedc952046aad915fa0cb77cd5e4e41.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Guiding-Registration-with-Emergent-Similarity-from-Pre-Trained-Diffusion-Models"><a href="#Guiding-Registration-with-Emergent-Similarity-from-Pre-Trained-Diffusion-Models" class="headerlink" title="Guiding Registration with Emergent Similarity from Pre-Trained Diffusion   Models"></a>Guiding Registration with Emergent Similarity from Pre-Trained Diffusion   Models</h2><p><strong>Authors:Nurislam Tursynbek, Hastings Greer, Basar Demir, Marc Niethammer</strong></p>
<p>Diffusion models, while trained for image generation, have emerged as powerful foundational feature extractors for downstream tasks. We find that off-the-shelf diffusion models, trained exclusively to generate natural RGB images, can identify semantically meaningful correspondences in medical images. Building on this observation, we propose to leverage diffusion model features as a similarity measure to guide deformable image registration networks. We show that common intensity-based similarity losses often fail in challenging scenarios, such as when certain anatomies are visible in one image but absent in another, leading to anatomically inaccurate alignments. In contrast, our method identifies true semantic correspondences, aligning meaningful structures while disregarding those not present across images. We demonstrate superior performance of our approach on two tasks: multimodal 2D registration (DXA to X-Ray) and monomodal 3D registration (brain-extracted to non-brain-extracted MRI). Code: <a target="_blank" rel="noopener" href="https://github.com/uncbiag/dgir">https://github.com/uncbiag/dgir</a> </p>
<blockquote>
<p>扩散模型虽然被训练用于图像生成，但已逐渐成为下游任务的强大基础特征提取器。我们发现，专为生成自然RGB图像而训练的即用型扩散模型可以识别医学图像中的语义对应。基于这一观察，我们提出利用扩散模型特征作为相似度度量来指导可变形图像配准网络。我们显示，常用的基于强度的相似度损失在具有挑战性的场景中经常失败，例如在一种图像中可见某些解剖结构而在另一种图像中缺失时，这会导致解剖上不准确的对齐。相比之下，我们的方法能够识别真正的语义对应，对齐有意义的结构，同时忽略那些在不同图像中不存在的结构。我们在两个任务上展示了我们的方法的优越性：多模态二维配准（DXA到X射线）和单模态三维配准（提取大脑与非提取大脑的MRI）。代码：<a target="_blank" rel="noopener" href="https://github.com/uncbiag/dgir">https://github.com/uncbiag/dgir</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02419v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong><br>    扩散模型虽被训练用于图像生成，但已成为强大的下游任务基础特征提取器。本研究发现，现成的扩散模型能够识别医学图像中的语义对应关系。基于此，研究提出利用扩散模型特征作为相似度度量，以指导可变形图像配准网络。在特定解剖结构在一图中可见而在另一图中缺失等挑战场景中，常见的基于强度的相似度损失常失效，而本研究方法能识别真正的语义对应关系，对齐有意义结构，同时忽略不同图像中不存在的内容。在两种任务上，本研究方法表现优越：多模态二维配准（DXA到X光）和单模态三维配准（提取大脑与未提取大脑的MRI）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型可作为强大的下游任务特征提取器，不仅用于图像生成。</li>
<li>扩散模型能够识别医学图像中的语义对应关系。</li>
<li>在挑战场景中，常见的基于强度的相似度损失可能会失效。</li>
<li>研究提出利用扩散模型特征来指导可变形图像配准网络。</li>
<li>本研究方法能识别真正的语义对应关系，对齐有意义结构。</li>
<li>本研究在两种任务上表现优越：多模态二维配准和单模态三维配准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02419">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c506878063bf7765fd73d4785bff5520.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8ce72588d2db8d36046920d9f1061af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c011f0d2c878cb79bfdf55991d82156.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec558a6b36e6147460ad72aaba33adbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0593e30b441a059c31c1ff1065ada802.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Medical-World-Model-Generative-Simulation-of-Tumor-Evolution-for-Treatment-Planning"><a href="#Medical-World-Model-Generative-Simulation-of-Tumor-Evolution-for-Treatment-Planning" class="headerlink" title="Medical World Model: Generative Simulation of Tumor Evolution for   Treatment Planning"></a>Medical World Model: Generative Simulation of Tumor Evolution for   Treatment Planning</h2><p><strong>Authors:Yijun Yang, Zhao-Yang Wang, Qiuping Liu, Shuwen Sun, Kang Wang, Rama Chellappa, Zongwei Zhou, Alan Yuille, Lei Zhu, Yu-Dong Zhang, Jieneng Chen</strong></p>
<p>Providing effective treatment and making informed clinical decisions are essential goals of modern medicine and clinical care. We are interested in simulating disease dynamics for clinical decision-making, leveraging recent advances in large generative models. To this end, we introduce the Medical World Model (MeWM), the first world model in medicine that visually predicts future disease states based on clinical decisions. MeWM comprises (i) vision-language models to serve as policy models, and (ii) tumor generative models as dynamics models. The policy model generates action plans, such as clinical treatments, while the dynamics model simulates tumor progression or regression under given treatment conditions. Building on this, we propose the inverse dynamics model that applies survival analysis to the simulated post-treatment tumor, enabling the evaluation of treatment efficacy and the selection of the optimal clinical action plan. As a result, the proposed MeWM simulates disease dynamics by synthesizing post-treatment tumors, with state-of-the-art specificity in Turing tests evaluated by radiologists. Simultaneously, its inverse dynamics model outperforms medical-specialized GPTs in optimizing individualized treatment protocols across all metrics. Notably, MeWM improves clinical decision-making for interventional physicians, boosting F1-score in selecting the optimal TACE protocol by 13%, paving the way for future integration of medical world models as the second readers. </p>
<blockquote>
<p>在现代医学和临床护理中，提供有效的治疗方法和做出明智的临床决策是不可或缺的目标。我们感兴趣的是利用大型生成模型的最新进展来模拟疾病动态以辅助临床决策。为此，我们引入了医学世界模型（MeWM），这是医学领域中的首个世界模型，能够基于临床决策视觉预测未来的疾病状态。MeWM包括（i）视觉语言模型，用作策略模型，（ii）肿瘤生成模型，用作动态模型。策略模型生成行动计划，如临床治疗，而动态模型模拟在给定的治疗条件下的肿瘤进展或消退。在此基础上，我们提出了逆向动态模型，对模拟的肿瘤进行生存分析以评估治疗效果和选择最佳的临床行动计划。因此，所提出的MeWM通过合成治疗后的肿瘤来模拟疾病动态，其在图灵测试中表现出最新的特异性水平并得到放射科的评估验证。同时，其逆向动态模型在优化个体化治疗方案方面优于医学专用GPT模型的所有指标。值得注意的是，MeWM提高了介入医师的临床决策能力，在选择最佳TACE协议时提高了F1分数达13%，为未来的医学世界模型作为第二诊断医师的整合铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02327v1">PDF</a> </p>
<p><strong>Summary</strong><br>     医学世界模型（MeWM）利用大型生成模型，模拟疾病动态以辅助临床决策。MeWM包括策略模型和动态模型两部分，分别生成治疗计划和模拟肿瘤发展。其逆动态模型应用生存分析评估治疗效果，优化治疗方案选择。MeWM在肿瘤模拟上具有先进特异性，在医生评估中表现优异，优化个体化治疗协议，提高医生决策准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MeWM是首个用于医学领域的世界模型，可通过视觉预测未来疾病状态，辅助临床决策。</li>
<li>MeWM包括策略模型和动态模型，分别负责生成治疗计划和模拟肿瘤发展。</li>
<li>MeWM的逆动态模型应用生存分析评估治疗效果，优化治疗方案选择。</li>
<li>MeWM在模拟疾病动态方面具有先进特异性，在医生评估中表现优异。</li>
<li>与医疗专业GPT相比，MeWM在优化个体化治疗协议方面表现出更高的性能。</li>
<li>MeWM提高了医生在临床决策中的准确性，特别是在选择最佳TACE协议方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02327">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-cd9dc95ab93657ed72d0dcb398fdab07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae84ed978d63e5f07abff5f75bcd19a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8358b355c0730651a7f31c8b7398250e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3a604ee30f82d1ffb77c3ff9b0a85b7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Implicit-Deformable-Medical-Image-Registration-with-Learnable-Kernels"><a href="#Implicit-Deformable-Medical-Image-Registration-with-Learnable-Kernels" class="headerlink" title="Implicit Deformable Medical Image Registration with Learnable Kernels"></a>Implicit Deformable Medical Image Registration with Learnable Kernels</h2><p><strong>Authors:Stefano Fogarollo, Gregor Laimer, Reto Bale, Matthias Harders</strong></p>
<p>Deformable medical image registration is an essential task in computer-assisted interventions. This problem is particularly relevant to oncological treatments, where precise image alignment is necessary for tracking tumor growth, assessing treatment response, and ensuring accurate delivery of therapies. Recent AI methods can outperform traditional techniques in accuracy and speed, yet they often produce unreliable deformations that limit their clinical adoption. In this work, we address this challenge and introduce a novel implicit registration framework that can predict accurate and reliable deformations. Our insight is to reformulate image registration as a signal reconstruction problem: we learn a kernel function that can recover the dense displacement field from sparse keypoint correspondences. We integrate our method in a novel hierarchical architecture, and estimate the displacement field in a coarse-to-fine manner. Our formulation also allows for efficient refinement at test time, permitting clinicians to easily adjust registrations when needed. We validate our method on challenging intra-patient thoracic and abdominal zero-shot registration tasks, using public and internal datasets from the local University Hospital. Our method not only shows competitive accuracy to state-of-the-art approaches, but also bridges the generalization gap between implicit and explicit registration techniques. In particular, our method generates deformations that better preserve anatomical relationships and matches the performance of specialized commercial systems, underscoring its potential for clinical adoption. </p>
<blockquote>
<p>可变形医学图像配准是计算机辅助干预中的一项重要任务。这个问题在肿瘤治疗中尤其重要，因为需要精确的图像对齐以追踪肿瘤生长情况、评估治疗效果，并确保准确的治疗方案实施。虽然最近的AI方法在准确性和速度方面可以超越传统技术，但它们通常会产生不可靠的变形，限制了其在临床上的采用。在这项工作中，我们解决了这一挑战，并引入了一种新型隐式配准框架，可以预测准确且可靠的变形。我们的见解是将图像配准重新表述为信号重建问题：我们学习一个内核函数，可以从稀疏关键点对应关系恢复密集位移场。我们将该方法集成到一种新型分层架构中，以从粗到细的方式估计位移场。我们的公式还允许在测试时进行高效的优化调整，使临床医生能够根据需要轻松调整配准。我们在具有挑战性的患者内部胸腔和腹部零射击配准任务上验证了我们的方法，使用了公共和本地大学医院的内部数据集。我们的方法不仅展示了与最新技术相竞争准确性，而且弥合了隐式和显式配准技术之间的泛化差距。特别是，我们的方法生成的变形更好地保留了解剖关系，并匹配了专业商业系统的性能，突显了其临床采用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02150v1">PDF</a> MICCAI 2025 Provisional Accept</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型的隐式注册框架，用于解决医学图像变形注册的问题。该框架将图像注册重新构建为信号重建问题，通过学习从稀疏关键点对应关系中恢复密集位移场。该框架结合了分层架构，并以从粗到细的方式估计位移场。此外，它允许在测试时进行高效的调整，使临床医生能够根据需要轻松调整注册。本文的方法在具有挑战性的患者内部胸腔和腹部零射击注册任务上进行了验证，并表现出与最新技术相当的准确性，同时在隐式和显式注册技术之间建立了桥梁。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了隐式注册框架来解决医学图像变形注册问题。</li>
<li>将图像注册重新构建为信号重建问题，学习从稀疏关键点对应关系中恢复密集位移场。</li>
<li>结合分层架构，以从粗到细的方式估计位移场。</li>
<li>方法允许在测试时进行高效的调整，方便临床医生按需调整注册。</li>
<li>方法在挑战性的患者内部胸腔和腹部零射击注册任务上进行了验证。</li>
<li>与最新技术相比，该方法表现出相当的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02150">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-92291ebe7eee590dd5bda5ade0fb1d7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8315baf30ada87fd5ce1c69477ae9536.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ce0a0392bd155cb1cd37d8f56c6088d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Tomographic-Foundation-Model-–-FORCE-Flow-Oriented-Reconstruction-Conditioning-Engine"><a href="#Tomographic-Foundation-Model-–-FORCE-Flow-Oriented-Reconstruction-Conditioning-Engine" class="headerlink" title="Tomographic Foundation Model – FORCE: Flow-Oriented Reconstruction   Conditioning Engine"></a>Tomographic Foundation Model – FORCE: Flow-Oriented Reconstruction   Conditioning Engine</h2><p><strong>Authors:Wenjun Xia, Chuang Niu, Ge Wang</strong></p>
<p>Computed tomography (CT) is a major medical imaging modality. Clinical CT scenarios, such as low-dose screening, sparse-view scanning, and metal implants, often lead to severe noise and artifacts in reconstructed images, requiring improved reconstruction techniques. The introduction of deep learning has significantly advanced CT image reconstruction. However, obtaining paired training data remains rather challenging due to patient motion and other constraints. Although deep learning methods can still perform well with approximately paired data, they inherently carry the risk of hallucination due to data inconsistencies and model instability. In this paper, we integrate the data fidelity with the state-of-the-art generative AI model, referred to as the Poisson flow generative model (PFGM) with a generalized version PFGM++, and propose a novel CT framework: Flow-Oriented Reconstruction Conditioning Engine (FORCE). In our experiments, the proposed method shows superior performance in various CT imaging tasks, outperforming existing unsupervised reconstruction approaches. </p>
<blockquote>
<p>计算机断层扫描（CT）是一种主要的医学成像模式。在临床CT场景中，如低剂量筛查、稀疏视图扫描和金属植入物，重建的图像往往会产生严重的噪声和伪影，这要求改进重建技术。深度学习引入到CT图像重建中取得了显著进展。然而，由于患者移动和其他约束条件，获取配对训练数据仍然相当具有挑战性。尽管深度学习方法仍然可以在近似配对数据上表现良好，但由于数据不一致和模型不稳定，它们天生存在出现幻觉的风险。在本文中，我们将数据保真度与最新生成的人工智能模型相结合，称为Poisson流生成模型（PFGM）及其通用版本PFGM++，并提出了一种新型的CT框架：面向流动的重建调节引擎（FORCE）。在我们的实验中，该方法在各种CT成像任务中表现出卓越的性能，超越了现有的无监督重建方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02149v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了基于深度学习和生成式AI模型的医学CT图像重建技术。由于临床CT场景中常常存在噪声和伪影问题，需要改进重建技术。作者提出了一个名为FORCE的新型CT框架，结合了数据保真度和最新的生成式AI模型——Poisson流生成模型（PFGM）及其广义版本PFGM++。实验表明，该方法在多种CT成像任务上表现出卓越性能，优于现有的无监督重建方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>CT图像重建技术在临床应用中面临噪声和伪影问题，需要改进。</li>
<li>深度学习在CT图像重建中的应用受到配对训练数据获取的挑战。</li>
<li>患者运动和其他约束是获取配对训练数据的主要难题。</li>
<li>深度学习方法在大约配对数据上表现良好，但存在数据不一致和模型不稳定的风险。</li>
<li>作者提出了结合数据保真度和生成式AI模型的FORCE框架。</li>
<li>FORCE框架使用了Poisson流生成模型（PFGM）及其广义版本PFGM++。</li>
<li>实验表明，FORCE框架在多种CT成像任务上的性能优于现有的无监督重建方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02149">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-24f99b47e3adaf5a64620cf0fc885e34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c9427665c1b16cbb9defe205cb41dfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24255e0b9a0d36ce0f2f012efb9aa57d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beda54c34a4d905c3c2f525281ebdae1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6c66125d65b082f327bb4c5233630d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b292183a90dedbf931648e36d44b3daa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdb4b52f97dd6da638d1f67a60883fba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ced067724dae8996caba9b898681b2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a903c16ece9172727e1c07927d9019e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RAW-Image-Reconstruction-from-RGB-on-Smartphones-NTIRE-2025-Challenge-Report"><a href="#RAW-Image-Reconstruction-from-RGB-on-Smartphones-NTIRE-2025-Challenge-Report" class="headerlink" title="RAW Image Reconstruction from RGB on Smartphones. NTIRE 2025 Challenge   Report"></a>RAW Image Reconstruction from RGB on Smartphones. NTIRE 2025 Challenge   Report</h2><p><strong>Authors:Marcos V. Conde, Radu Timofte, Radu Berdan, Beril Besbinar, Daisuke Iso, Pengzhou Ji, Xiong Dun, Zeying Fan, Chen Wu, Zhansheng Wang, Pengbo Zhang, Jiazi Huang, Qinglin Liu, Wei Yu, Shengping Zhang, Xiangyang Ji, Kyungsik Kim, Minkyung Kim, Hwalmin Lee, Hekun Ma, Huan Zheng, Yanyan Wei, Zhao Zhang, Jing Fang, Meilin Gao, Xiang Yu, Shangbin Xie, Mengyuan Sun, Huanjing Yue, Jingyu Yang Huize Cheng, Shaomeng Zhang, Zhaoyang Zhang, Haoxiang Liang</strong></p>
<p>Numerous low-level vision tasks operate in the RAW domain due to its linear properties, bit depth, and sensor designs. Despite this, RAW image datasets are scarce and more expensive to collect than the already large and public sRGB datasets. For this reason, many approaches try to generate realistic RAW images using sensor information and sRGB images. This paper covers the second challenge on RAW Reconstruction from sRGB (Reverse ISP). We aim to recover RAW sensor images from smartphones given the corresponding sRGB images without metadata and, by doing this, &#96;&#96;reverse” the ISP transformation. Over 150 participants joined this NTIRE 2025 challenge and submitted efficient models. The proposed methods and benchmark establish the state-of-the-art for generating realistic RAW data. </p>
<blockquote>
<p>由于RAW域的线性属性、位深度和传感器设计，许多低级视觉任务都在RAW域中进行。尽管如此，RAW图像数据集非常稀缺，且收集成本高于已经大量存在的公开sRGB数据集。因此，许多方法试图利用传感器信息和sRGB图像生成逼真的RAW图像。本文涉及从sRGB进行RAW重建（反向ISP）的第二个挑战。我们的目标是在没有元数据的情况下，根据相应的sRGB图像恢复来自智能手机的RAW传感器图像，并因此“反向”ISP转换。超过150名参与者加入了这一NTIRE 2025挑战赛，并提交了高效模型。所提出的方法和基准测试建立了生成逼真RAW数据的最新技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01947v1">PDF</a> CVPR 2025 - New Trends in Image Restoration and Enhancement (NTIRE)</p>
<p><strong>Summary</strong><br>本文介绍了在RAW域进行低层次视觉任务的重要性，但由于RAW图像数据集稀缺且收集成本较高，许多方法尝试利用传感器信息和sRGB图像生成逼真的RAW图像。本文重点介绍了从sRGB图像重建RAW传感器图像的第二个挑战（反向ISP）。目标是利用对应的sRGB图像恢复RAW传感器图像，无需元数据参与。超过150名参与者参加了NTIRE 2025挑战赛，并提出了有效的模型，为生成逼真的RAW数据建立了最新标准。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RAW域因其线性特性、位深度和传感器设计而被用于众多低层次视觉任务。</li>
<li>RAW图像数据集稀缺且收集成本较高，因此有研究尝试用sRGB图像和传感器信息生成逼真的RAW图像。</li>
<li>本文关注从sRGB图像重建RAW传感器图像的第二个挑战，即反向ISP。</li>
<li>目标是仅使用对应的sRGB图像恢复RAW传感器图像，无需元数据参与。</li>
<li>超过150名参与者参与了NTIRE 2025挑战赛，提出了多种高效模型。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01947">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7cd80bec8ae68b69f484df2af6ae5a1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bccfed1c0fb5c41720dfba9872ddad22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03ab8a796f3918708c88ce089ffbf3c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7de39a68b6c7233d937807fca3b0626.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f4ffe7ebc01a6ce1f59a8484435481f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="unMORE-Unsupervised-Multi-Object-Segmentation-via-Center-Boundary-Reasoning"><a href="#unMORE-Unsupervised-Multi-Object-Segmentation-via-Center-Boundary-Reasoning" class="headerlink" title="unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary   Reasoning"></a>unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary   Reasoning</h2><p><strong>Authors:Yafei Yang, Zihui Zhang, Bo Yang</strong></p>
<p>We study the challenging problem of unsupervised multi-object segmentation on single images. Existing methods, which rely on image reconstruction objectives to learn objectness or leverage pretrained image features to group similar pixels, often succeed only in segmenting simple synthetic objects or discovering a limited number of real-world objects. In this paper, we introduce unMORE, a novel two-stage pipeline designed to identify many complex objects in real-world images. The key to our approach involves explicitly learning three levels of carefully defined object-centric representations in the first stage. Subsequently, our multi-object reasoning module utilizes these learned object priors to discover multiple objects in the second stage. Notably, this reasoning module is entirely network-free and does not require human labels. Extensive experiments demonstrate that unMORE significantly outperforms all existing unsupervised methods across 6 real-world benchmark datasets, including the challenging COCO dataset, achieving state-of-the-art object segmentation results. Remarkably, our method excels in crowded images where all baselines collapse. </p>
<blockquote>
<p>我们研究了单图像的无监督多目标分割这一具有挑战性的问题。现有方法往往依赖于图像重建目标来学习对象性，或者利用预训练的图像特征来将相似的像素组合在一起，但它们通常仅在分割简单合成对象或发现有限数量的真实世界对象时成功。在本文中，我们介绍了unMORE，这是一种新型的两阶段流程，旨在识别真实世界图像中的多个复杂对象。我们的方法的关键在于在第一阶段显式地学习三个层次精心定义的以对象为中心的表示。随后，我们的多目标推理模块利用这些学习到的对象先验知识在第二阶段发现多个对象。值得注意的是，该推理模块完全无需网络且不需要人工标签。大量实验表明，在包括具有挑战性的COCO数据集在内的六个真实世界基准数据集上，unMORE显著优于所有现有无监督方法，实现了最先进的对象分割结果。值得注意的是，我们的方法在拥挤的图像中的表现尤为出色，而所有基线方法均在此处失效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01778v1">PDF</a> ICML 2025. Code and data are available at:   <a target="_blank" rel="noopener" href="https://github.com/vLAR-group/unMORE">https://github.com/vLAR-group/unMORE</a></p>
<p><strong>Summary</strong></p>
<p>本文研究了单图像的无监督多目标分割问题。针对现有方法（如依赖图像重建目标来学习对象性，或利用预训练图像特征来分组相似像素）在分割复杂现实世界对象时的局限性，本文提出了一种新型的两阶段管道unMORE。该方法第一阶段通过显式学习三个层次的对象中心表示，第二阶段利用多目标推理模块发现多个对象，无需网络且无需人为标签。在六个现实世界基准数据集上的实验表明，unMORE在包括具有挑战性的COCO数据集上实现了最先进的目标分割结果。特别地，该方法在拥挤图像中的表现超越了所有基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究无监督多目标分割在单图像的问题。</li>
<li>现有方法在分割复杂现实世界对象时存在局限性。</li>
<li>引入了一种新型的两阶段管道unMORE来解决这个问题。</li>
<li>第一阶段通过显式学习三个层次的对象中心表示。</li>
<li>第二阶段利用多目标推理模块发现多个对象，无需网络和人为标签。</li>
<li>在多个现实世界数据集上的实验表明，unMORE显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01778">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-51b513032f259b96ce3bc0306609e16e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca05402db53504c3570049d0700c5f53.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-424dd130d3b9e2fabea326c17cf5dd11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a12ba3340061d97baf1fa1ef0310b03.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SAM-I2V-Upgrading-SAM-to-Support-Promptable-Video-Segmentation-with-Less-than-0-2-Training-Cost"><a href="#SAM-I2V-Upgrading-SAM-to-Support-Promptable-Video-Segmentation-with-Less-than-0-2-Training-Cost" class="headerlink" title="SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with   Less than 0.2% Training Cost"></a>SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with   Less than 0.2% Training Cost</h2><p><strong>Authors:Haiyang Mei, Pengyu Zhang, Mike Zheng Shou</strong></p>
<p>Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V, an effective image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAM’s static image encoder to enable spatiotemporal video perception, (ii) a memory filtering strategy that selects the most relevant past frames for more effective utilization of historical information, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves over 90% of SAM 2’s performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Code and model are available at: <a target="_blank" rel="noopener" href="https://github.com/showlab/SAM-I2V">https://github.com/showlab/SAM-I2V</a>. </p>
<blockquote>
<p>类似Segment Anything Model（SAM）这样的基础模型已经在计算机视觉的可提示图像分割方面取得了显著进展。然而，将这些能力扩展到视频却面临着巨大挑战，特别是在确保动态场景中的精确和时间上一致的掩膜传播方面。SAM 2 通过在大量图像和视频数据上进行模型训练来学习复杂的时空关联，以解决这一问题，但这也导致了巨大的训练成本，阻碍了研究和实际应用部署。在本文中，我们介绍了一种有效的图像到视频的升级方法SAM-I2V，用于培养可提示视频分割（PVS）模型。我们的方法通过升级预训练的SAM来支持PVS，显著降低了训练复杂性和资源需求。为实现这一目标，我们引入了三项关键创新：（i）基于SAM的静态图像编码器构建图像到视频特征提取升级器，以实现时空视频感知；（ii）一种记忆过滤策略，选择最相关的过去帧，以更有效地利用历史信息；（iii）一种记忆提示机制，利用对象内存确保动态场景中的时间上一致的掩膜传播。大量实验表明，我们的方法达到了SAM 2性能的90%以上，同时仅使用其0.2%的训练成本。我们的工作为PVS提供了一条资源高效的路径，降低了PVS模型设计的进一步研究障碍，并启用了该领域的更广泛应用和进展。代码和模型可在[<a target="_blank" rel="noopener" href="https://github.com/showlab/SAM-I2V%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/showlab/SAM-I2V找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01304v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>SAM-I2V方法利用预训练的SAM图像模型，通过引入三项关键技术创新，实现了视频分割的升级，显著降低了训练复杂度和资源需求。该技术包括：基于SAM静态图像编码器的图像到视频特征提取升级器、记忆过滤策略以及利用对象记忆的内存提示机制。实验证明，该方法实现了超过90%的SAM 2性能，同时仅使用其0.2%的训练成本。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAM-I2V是Segment Anything Model（SAM）的视频分割升级方法。</li>
<li>SAM-I2V解决了将图像分割模型应用于视频时面临的挑战，如精确且时间一致的掩膜传播。</li>
<li>通过三项关键技术创新实现视频分割的升级：图像到视频特征提取升级器、记忆过滤策略和内存提示机制。</li>
<li>实验证明，SAM-I2V实现了超过90%的SAM 2性能，同时大幅降低了训练成本和资源需求。</li>
<li>SAM-I2V工作为可提示视频分割（PVS）提供了资源高效的途径，降低了进一步研究PVS模型设计的门槛，并促进了该领域的广泛应用和进一步发展。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01304">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0c6b8d8ddcf2279995e9c5eb8c0cb8e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-307d789d3f53844008e298dc60575b8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3f8a61f35826bea6db2504c57921fa8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Region-of-Interest-Guided-Deep-Joint-Source-Channel-Coding-for-Image-Transmission"><a href="#Region-of-Interest-Guided-Deep-Joint-Source-Channel-Coding-for-Image-Transmission" class="headerlink" title="Region-of-Interest-Guided Deep Joint Source-Channel Coding for Image   Transmission"></a>Region-of-Interest-Guided Deep Joint Source-Channel Coding for Image   Transmission</h2><p><strong>Authors:Hansung Choi, Daewon Seo</strong></p>
<p>Deep joint source-channel coding (deepJSCC) and semantic communication have shown promising improvements in communication performance over wireless networks. However, current approaches primarily focus on enhancing average performance metrics, such as overall image reconstruction quality or task accuracy, which may not fully align with users’ actual experience – often driven by the quality of specific regions of interest (ROI). Motivated by this, we propose ROI-guided joint source-channel coding (ROI-JSCC), a novel deepJSCC framework that prioritizes high-quality transmission of ROI. The ROI-JSCC consists of four key components: (1) ROI embedding and feature map extraction, (2) ROI-guided split processing, (3) ROI-based loss function design, and (4) ROI-adaptive bandwidth allocation. Together, these components enable ROI-JSCC to selectively improve the reconstruction quality of varying ROI while preserving overall image quality without increasing computational burden. Experimental results under diverse communication environments demonstrate that ROI-JSCC significantly improves ROI reconstruction quality while maintaining competitive average image quality compared to recent state-of-the-art methods. All codes are available at <a target="_blank" rel="noopener" href="https://github.com/hansung-choi/ROI-JSCC">https://github.com/hansung-choi/ROI-JSCC</a>. </p>
<blockquote>
<p>深度联合源信道编码（deepJSCC）和语义通信在无线网络的通信性能上显示出有前景的改进。然而，当前的方法主要集中在提高平均性能指标，如整体图像重建质量或任务准确性，这可能无法完全与用户实际体验对齐，用户实际体验通常是由感兴趣区域（ROI）的质量驱动的。因此，我们提出了ROI引导联合源信道编码（ROI-JSCC），这是一种新型deepJSCC框架，优先高质量传输ROI。ROI-JSCC由四个关键组件构成：（1）ROI嵌入和特征映射提取，（2）ROI引导分裂处理，（3）基于ROI的损失函数设计，以及（4）ROI自适应带宽分配。这些组件共同作用，使ROI-JSCC能够在保持整体图像质量的同时，有选择地提高不同ROI的重建质量，且不增加计算负担。在多种通信环境下的实验结果表明，ROI-JSCC在保持平均图像质量竞争力的同时，显著提高了ROI的重建质量，与最近的最先进方法相比表现优异。所有代码可在<a target="_blank" rel="noopener" href="https://github.com/hansung-choi/ROI-JSCC%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hansung-choi/ROI-JSCC获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01269v1">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像通信中，深层联合源信道编码（deepJSCC）和语义通信已显示出在提高通信性能方面的潜力。当前方法主要关注平均性能指标，如整体图像重建质量或任务准确性，但可能无法完全与用户关注的特定感兴趣区域（ROI）的质量相匹配。因此，提出ROI引导的联合源信道编码（ROI-JSCC），优先传输ROI的高质量。它包括ROI嵌入、特征映射提取、ROI引导的分步处理、ROI基础的损失函数设计和ROI自适应的带宽分配等四个关键组件。实验结果表明，ROI-JSCC在保持整体图像质量的同时，显著提高ROI的重建质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepJSCC和语义通信在医学图像通信中可提高性能。</li>
<li>当前方法主要关注平均性能指标，如整体图像重建质量。</li>
<li>用户实际关注的重点是特定感兴趣区域（ROI）的质量。</li>
<li>ROI-JSCC是一种优先传输ROI高质量的新型deepJSCC框架。</li>
<li>ROI-JSCC包括四个关键组件：ROI嵌入、特征映射、分步处理和损失函数设计。</li>
<li>ROI-JSCC能够选择性提高不同ROI的重建质量，同时保持整体图像质量，且不增加计算负担。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01269">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-88307d1942b08171207e5f93fddc7689.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-971ee3a8f96ee3b15eaa632cec6e76c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23b6d8159f9ac4502fb385d213de9855.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd9c302e4d40bbb398f81874722c078d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-008b12c9afd3624fb6186e9f6c9f4e54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9603c2461a10441650aa551c4ccba3a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03884b1549556e1211d4c3d7d1800e28.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Revolutionizing-Radiology-Workflow-with-Factual-and-Efficient-CXR-Report-Generation"><a href="#Revolutionizing-Radiology-Workflow-with-Factual-and-Efficient-CXR-Report-Generation" class="headerlink" title="Revolutionizing Radiology Workflow with Factual and Efficient CXR Report   Generation"></a>Revolutionizing Radiology Workflow with Factual and Efficient CXR Report   Generation</h2><p><strong>Authors:Pimchanok Sukjai, Apiradee Boonmee</strong></p>
<p>The escalating demand for medical image interpretation underscores the critical need for advanced artificial intelligence solutions to enhance the efficiency and accuracy of radiological diagnoses. This paper introduces CXR-PathFinder, a novel Large Language Model (LLM)-centric foundation model specifically engineered for automated chest X-ray (CXR) report generation. We propose a unique training paradigm, Clinician-Guided Adversarial Fine-Tuning (CGAFT), which meticulously integrates expert clinical feedback into an adversarial learning framework to mitigate factual inconsistencies and improve diagnostic precision. Complementing this, our Knowledge Graph Augmentation Module (KGAM) acts as an inference-time safeguard, dynamically verifying generated medical statements against authoritative knowledge bases to minimize hallucinations and ensure standardized terminology. Leveraging a comprehensive dataset of millions of paired CXR images and expert reports, our experiments demonstrate that CXR-PathFinder significantly outperforms existing state-of-the-art medical vision-language models across various quantitative metrics, including clinical accuracy (Macro F1 (14): 46.5, Micro F1 (14): 59.5). Furthermore, blinded human evaluation by board-certified radiologists confirms CXR-PathFinder’s superior clinical utility, completeness, and accuracy, establishing its potential as a reliable and efficient aid for radiological practice. The developed method effectively balances high diagnostic fidelity with computational efficiency, providing a robust solution for automated medical report generation. </p>
<blockquote>
<p>随着医学图像解读需求的不断增长，对先进的人工智能解决方案的需求也愈发迫切，以提高放射诊断的效率和准确性。本文介绍了CXR-PathFinder，这是一种新型的大型语言模型（LLM）为中心的基础模型，专门用于自动生成胸部X光（CXR）报告。我们提出了一种独特的训练范式——临床医生指导的对抗微调（CGAFT），它将专家临床反馈精心集成到对抗性学习框架中，以减轻事实上的不一致性并提高诊断精度。作为补充，我们的知识图谱增强模块（KGAM）充当推理时的安全卫士，动态验证生成的医学陈述与权威知识库的一致性，以最小化幻觉并确保标准化术语。利用数百万对CXR图像和专家报告的综合数据集，我们的实验表明，CXR-PathFinder在各种定量指标上显著优于现有的最先进的医疗视觉语言模型，包括临床准确性（宏观F1（14）：46.5，微观F1（14）：59.5）。此外，经过认证的放射科医生进行的盲态人类评估证实了CXR-PathFinder在临床实用性、完整性和准确性方面的优越性，证明了其在放射实践中作为可靠且高效的辅助工具的潜力。所开发的方法有效地平衡了高诊断保真度和计算效率，为自动医学报告生成提供了稳健的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01118v1">PDF</a> </p>
<p><strong>Summary</strong><br>     本论文提出一种名为CXR-PathFinder的新型大型语言模型（LLM）为中心的基础模型，用于自动生成胸部X光（CXR）报告。通过独特的训练范式Clinician-Guided Adversarial Fine-Tuning（CGAFT）和Knowledge Graph Augmentation Module（KGAM），该模型提高了诊断的准确性和精确性，减少了事实上的不一致性，并确保标准化术语。实验表明，CXR-PathFinder在多种定量指标上显著优于现有的最先进的医疗视觉语言模型，并在放射实践领域展现出可靠且高效的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像解读需求不断增长，强调对先进人工智能解决方案的需求，以提高放射诊断的效率和准确性。</li>
<li>介绍了CXR-PathFinder模型，该模型是一种专门用于自动生成胸部X光报告的大型语言模型。</li>
<li>CXR-PathFinder采用独特的训练范式Clinician-Guided Adversarial Fine-Tuning (CGAFT)，将专家临床反馈纳入对抗性学习框架，以提高诊断的准确性和精确性。</li>
<li>Knowledge Graph Augmentation Module (KGAM) 作为推理时的安全保障，动态验证生成的医疗陈述，确保标准化术语并减少虚构情况。</li>
<li>实验表明，CXR-PathFinder在各种定量指标上显著优于其他先进模型，包括临床精度（Macro F1 (14): 46.5, Micro F1 (14): 59.5）。</li>
<li>盲态人类评估（由认证放射学家进行）证实了CXR-PathFinder在临床实用性、完整性和准确性方面的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01118">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b4a743779a7ad40efd4b9bf65957cb5c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Self-Supervised-ISAR-Net-Enables-Fast-Sparse-ISAR-Imaging"><a href="#Self-Supervised-ISAR-Net-Enables-Fast-Sparse-ISAR-Imaging" class="headerlink" title="Self-Supervised-ISAR-Net Enables Fast Sparse ISAR Imaging"></a>Self-Supervised-ISAR-Net Enables Fast Sparse ISAR Imaging</h2><p><strong>Authors:Ziwen Wang, Jianping wang, Pucheng Li, Yifan Wu, Zegang Ding</strong></p>
<p>Numerous sparse inverse synthetic aperture radar (ISAR) imaging methods based on unfolded neural networks have been developed for high-quality image reconstruction with sparse measurements. However, their training typically requires paired ISAR images and echoes, which are often difficult to obtain. Meanwhile, one property can be observed that for a certain sparse measurement configuration of ISAR, when a target is rotated around its center of mass, only the image of the target undergoes the corresponding rotation after ISAR imaging, while the grating lobes do not follow this rotation and are solely determined by the sparse-sampling pattern. This property is mathematically termed as the equivariant property. Taking advantage of this property, an unfolded neural network for sparse ISAR imaging with self-supervised learning, named SS-ISAR-Net is proposed. It effectively mitigates grating lobes caused by sparse radar echo, allowing high-quality training to be achieved using only sparse radar echo data. The superiority of the proposed SS-ISAR-Net, compared to existing methods, is verified through experiments with both synthetic and real-world measurement data. </p>
<blockquote>
<p>基于展开神经网络的稀疏逆合成孔径雷达（ISAR）成像方法已经被开发出来，用于利用稀疏测量进行高质量图像重建。然而，它们的训练通常需要成对的ISAR图像和回波，而这些通常很难获得。同时，可以观察到这样一个特性：对于ISAR的某种稀疏测量配置，当目标围绕其质量中心旋转时，只有在ISAR成像后，目标图像会进行相应的旋转，而光栅瓣并不遵循这种旋转，而是仅由稀疏采样模式决定。这一特性在数学上被称为等变属性。利用这一特性，提出了一种具有自监督学习功能的稀疏ISAR成像展开神经网络，名为SS-ISAR-Net。它有效地减轻了由稀疏雷达回波引起的光栅瓣问题，仅使用稀疏雷达回波数据即可实现高质量训练。通过合成数据和真实世界测量数据的实验验证了所提出的SS-ISAR-Net相比现有方法的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01038v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于展开式神经网络的稀疏逆合成孔径雷达（ISAR）成像方法已被开发用于高质量图像重建，但通常需要配对的ISAR图像和回波数据，获取困难。观察到一种特性，即ISAR特定的稀疏测量配置下，目标绕质心旋转时，成像后的目标会相应旋转，而栅瓣不会随旋转改变，仅由稀疏采样模式决定。利用这一特性，提出了一种具有自监督学习能力的稀疏ISAR成像展开神经网络SS-ISAR-Net。该方法有效减轻了稀疏雷达回波引起的栅瓣问题，并能仅使用稀疏雷达回波数据进行高质量训练。通过实验验证，与现有方法相比，SS-ISAR-Net表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>基于展开神经网络的稀疏逆合成孔径雷达成像用于高质量图像重建。</li>
<li>获取配对的ISAR图像和回波数据通常很困难。</li>
<li>目标在ISAR成像过程中绕质心旋转时，图像中的目标会相应旋转，而栅瓣不随旋转改变。</li>
<li>利用上述特性提出了SS-ISAR-Net网络模型。</li>
<li>SS-ISAR-Net能有效减轻稀疏雷达回波引起的栅瓣问题。</li>
<li>仅使用稀疏雷达回波数据即可实现高质量训练。</li>
<li>实验验证了SS-ISAR-Net相较于现有方法的优越性，无论是在合成数据还是真实测量数据上。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01038">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7cd4fb36454ef6a2206ab01467e0b3a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a34b6c6a5d8826808a66ea12feb1fd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-950b2a524b49e7ee8dca641dbea75716.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c9edcb6b25275e69cd1a32d05e5082a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3c64306cf436e18cadc5dcd3a3d11c0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Modality-Translation-and-Registration-of-MR-and-Ultrasound-Images-Using-Diffusion-Models"><a href="#Modality-Translation-and-Registration-of-MR-and-Ultrasound-Images-Using-Diffusion-Models" class="headerlink" title="Modality Translation and Registration of MR and Ultrasound Images Using   Diffusion Models"></a>Modality Translation and Registration of MR and Ultrasound Images Using   Diffusion Models</h2><p><strong>Authors:Xudong Ma, Nantheera Anantrasirichai, Stefanos Bolomytis, Alin Achim</strong></p>
<p>Multimodal MR-US registration is critical for prostate cancer diagnosis. However, this task remains challenging due to significant modality discrepancies. Existing methods often fail to align critical boundaries while being overly sensitive to irrelevant details. To address this, we propose an anatomically coherent modality translation (ACMT) network based on a hierarchical feature disentanglement design. We leverage shallow-layer features for texture consistency and deep-layer features for boundary preservation. Unlike conventional modality translation methods that convert one modality into another, our ACMT introduces the customized design of an intermediate pseudo modality. Both MR and US images are translated toward this intermediate domain, effectively addressing the bottlenecks faced by traditional translation methods in the downstream registration task. Experiments demonstrate that our method mitigates modality-specific discrepancies while preserving crucial anatomical boundaries for accurate registration. Quantitative evaluations show superior modality similarity compared to state-of-the-art modality translation methods. Furthermore, downstream registration experiments confirm that our translated images achieve the best alignment performance, highlighting the robustness of our framework for multi-modal prostate image registration. </p>
<blockquote>
<p>多模态MR-US注册对前列腺癌诊断至关重要。然而，由于模态之间的差异很大，这项任务仍然具有挑战性。现有方法往往无法在关键边界对齐的同时，又过于敏感地关注不相关的细节。为了解决这一问题，我们提出了一种基于层次特征分解设计的解剖连贯性模态翻译（ACMT）网络。我们利用浅层特征实现纹理一致性，利用深层特征实现边界保留。不同于将一种模态转换为另一种模态的传统模态翻译方法，我们的ACMT引入了中间伪模态的定制设计。MR和US图像都向这个中间域进行转换，有效地解决了传统翻译方法在下游注册任务中所面临的瓶颈。实验表明，我们的方法减轻了模态特定的差异，同时保留了关键解剖边界，以实现准确的注册。定量评估表明，与最先进的模态翻译方法相比，我们的模态相似性更高。此外，下游注册实验证实，我们的翻译图像实现了最佳的对齐性能，凸显了我们框架在多模态前列腺图像注册的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01025v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于层次特征分离设计的解剖学一致性模态翻译（ACMT）网络，用于解决多模态MR-US在前列腺癌诊断中的注册问题。通过利用浅层特征实现纹理一致性和深层特征进行边界保留，有效地解决了传统模态翻译方法在下游注册任务中遇到的瓶颈。实验表明，该方法在减轻模态特定差异的同时，保留了关键解剖边界，实现了准确的注册。与最先进的模态翻译方法相比，该方法在模态相似性方面表现出优越性，并在下游注册实验中证实了其最佳对齐性能，凸显了其在多模态前列腺图像注册中的稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态MR-US注册对前列腺癌诊断至关重要，但存在显著模态差异的挑战。</li>
<li>现有方法往往难以对齐关键边界，同时过于敏感于无关细节。</li>
<li>提出了一种基于层次特征分离设计的解剖学一致性模态翻译（ACMT）网络。</li>
<li>ACMT网络利用浅层特征实现纹理一致性和深层特征进行边界保留。</li>
<li>引入了一种中间伪模态的定制设计，使MR和US图像都向此中间领域翻译，解决传统翻译方法在下游注册任务中的瓶颈。</li>
<li>实验表明，该方法在模态相似性方面优于最先进的模态翻译方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01025">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-76e8bf06ddaf8f5391b8ca7dc67cb932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad05220dddc200737804d7782dad65f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-583c9e624a099eda970e3443f74f112d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc3fd1883ecf3b543e0aa3bdfc6ec8c7.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SSAM-Self-Supervised-Association-Modeling-for-Test-Time-Adaption"><a href="#SSAM-Self-Supervised-Association-Modeling-for-Test-Time-Adaption" class="headerlink" title="SSAM: Self-Supervised Association Modeling for Test-Time Adaption"></a>SSAM: Self-Supervised Association Modeling for Test-Time Adaption</h2><p><strong>Authors:Yaxiong Wang, Zhenqiang Zhang, Lechao Cheng, Zhun Zhong, Dan Guo, Meng Wang</strong></p>
<p>Test-time adaption (TTA) has witnessed important progress in recent years, the prevailing methods typically first encode the image and the text and design strategies to model the association between them. Meanwhile, the image encoder is usually frozen due to the absence of explicit supervision in TTA scenarios. We identify a critical limitation in this paradigm: While test-time images often exhibit distribution shifts from training data, existing methods persistently freeze the image encoder due to the absence of explicit supervision during adaptation. This practice overlooks the image encoder’s crucial role in bridging distribution shift between training and test. To address this challenge, we propose SSAM (Self-Supervised Association Modeling), a new TTA framework that enables dynamic encoder refinement through dual-phase association learning. Our method operates via two synergistic components: 1) Soft Prototype Estimation (SPE), which estimates probabilistic category associations to guide feature space reorganization, and 2) Prototype-anchored Image Reconstruction (PIR), enforcing encoder stability through cluster-conditional image feature reconstruction. Comprehensive experiments across diverse baseline methods and benchmarks demonstrate that SSAM can surpass state-of-the-art TTA baselines by a clear margin while maintaining computational efficiency. The framework’s architecture-agnostic design and minimal hyperparameter dependence further enhance its practical applicability. </p>
<blockquote>
<p>近年来，测试时适应（TTA）技术取得了重要进展。目前主流的方法通常首先对图像和文本进行编码，并设计策略对它们之间的关联进行建模。同时，由于TTA场景中缺乏明确的监督信息，图像编码器通常被冻结。我们发现了这一模式的关键局限：虽然测试时的图像往往表现出与训练数据不同的分布，但由于适应过程中缺乏明确的监督，现有方法仍然坚持冻结图像编码器。这种做法忽视了图像编码器在弥合训练和测试之间分布转移方面的关键作用。为了应对这一挑战，我们提出了SSAM（自监督关联建模），这是一种新的TTA框架，通过双阶段关联学习实现动态编码器优化。我们的方法由两个协同组件组成：1）软原型估计（SPE），它估计概率类别关联以引导特征空间重组；2）以原型为中心的图像重建（PIR），通过聚类条件图像特征重建来加强编码器的稳定性。在多种基准方法和基准测试上的综合实验表明，SSAM可以明显超越最新的TTA基准测试，同时保持计算效率。该框架的架构无关设计和较小的超参数依赖性进一步增强了其实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00513v1">PDF</a> 10 papges</p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的测试时适应（TTA）框架SSAM（Self-Supervised Association Modeling），通过双阶段关联学习实现动态编码器优化，解决现有方法忽略图像编码器在适应训练与测试数据分布变化中的重要性。SSAM包含两个协同组件：Soft Prototype Estimation（SPE）和Prototype-anchored Image Reconstruction（PIR），分别用于引导特征空间重组和强化编码器稳定性。实验表明，SSAM在多种基准方法和基准测试上超越了现有TTA方法，具有显著优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时适应（TTA）框架SSAM通过双阶段关联学习实现动态编码器优化。</li>
<li>SSAM解决了现有方法忽略图像编码器在适应训练与测试数据分布变化中的重要性。</li>
<li>SSAM包含两个协同组件：Soft Prototype Estimation（SPE）和Prototype-anchored Image Reconstruction（PIR）。</li>
<li>SPE通过估计概率类别关联来引导特征空间重组。</li>
<li>PIR通过集群条件图像特征重建来强化编码器稳定性。</li>
<li>实验表明，SSAM在多个基准方法和基准测试上超越了现有TTA方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00513">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e23e455cf5a897ccf7094342a6faff3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e5b5e4252f62169efb16a4bea69a8fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf4327edecd17f9d16471030eed22918.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-be64077dbb482c82f476bdd21fb1ad4e.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-06-05  Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich   Representation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4ff2737bf419a0bcca28f1acc438e1a6.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-06-05  EDITOR Effective and Interpretable Prompt Inversion for Text-to-Image   Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
