<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-06-05  Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich   Representation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-be64077dbb482c82f476bdd21fb1ad4e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    36 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-05-更新"><a href="#2025-06-05-更新" class="headerlink" title="2025-06-05 更新"></a>2025-06-05 更新</h1><h2 id="Controllable-Text-to-Speech-Synthesis-with-Masked-Autoencoded-Style-Rich-Representation"><a href="#Controllable-Text-to-Speech-Synthesis-with-Masked-Autoencoded-Style-Rich-Representation" class="headerlink" title="Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich   Representation"></a>Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich   Representation</h2><p><strong>Authors:Yongqi Wang, Chunlei Zhang, Hangting Chen, Zhou Zhao, Dong Yu</strong></p>
<p>Controllable TTS models with natural language prompts often lack the ability for fine-grained control and face a scarcity of high-quality data. We propose a two-stage style-controllable TTS system with language models, utilizing a quantized masked-autoencoded style-rich representation as an intermediary. In the first stage, an autoregressive transformer is used for the conditional generation of these style-rich tokens from text and control signals. The second stage generates codec tokens from both text and sampled style-rich tokens. Experiments show that training the first-stage model on extensive datasets enhances the content robustness of the two-stage model as well as control capabilities over multiple attributes. By selectively combining discrete labels and speaker embeddings, we explore fully controlling the speaker’s timbre and other stylistic information, and adjusting attributes like emotion for a specified speaker. Audio samples are available at <a target="_blank" rel="noopener" href="https://style-ar-tts.github.io/">https://style-ar-tts.github.io</a>. </p>
<blockquote>
<p>具有自然语言提示的可控文本转语音（TTS）模型通常缺乏精细控制的能力，并且面临高质量数据稀缺的问题。我们提出了一种两阶段风格可控的TTS系统，该系统利用语言模型，并使用量化的掩码自动编码风格丰富的表示作为中介。在第一阶段，使用自回归变压器根据文本和控制信号生成这些风格丰富的标记。第二阶段从文本和采样的风格丰富的标记生成编码标记。实验表明，在第一阶段模型上进行大量数据集训练，可以增强两阶段模型的内容稳健性以及对多个属性的控制能力。通过有选择地结合离散标签和说话者嵌入，我们可以完全控制说话者的音色和其他风格信息，并调整指定说话者的情感等属性。音频样本可在<a target="_blank" rel="noopener" href="https://style-ar-tts.github.io找到./">https://style-ar-tts.github.io找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02997v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种两阶段风格可控的文本到语音转换（TTS）系统。利用语言模型构建可控TTS模型面临的问题是缺少精细化控制以及高质量数据的稀缺。通过引入量化掩码自编码风格丰富表示作为中介，第一阶段使用自回归转换器从文本和控制信号生成这些风格丰富的令牌，第二阶段从文本和采样的风格丰富的令牌生成编解码器令牌。实验表明，在第一阶段模型上进行大量训练数据训练可以提高两阶段模型的稳健性和对多个属性的控制能力。通过有选择地结合离散标签和说话者嵌入，探索对说话者的音色和其他风格信息的完全控制，以及调整特定说话者的情感等属性。提供了音频样本链接。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TTS模型使用自然语言提示缺乏精细控制和高质量数据的问题。</li>
<li>提出一种两阶段风格可控的TTS系统，使用语言模型和量化掩码自编码风格丰富表示。</li>
<li>第一阶段生成风格丰富的令牌，第二阶段从文本和采样的风格令牌生成编解码器令牌。</li>
<li>训练第一阶段模型使用大量数据集可提高两阶段模型的稳健性和控制能力。</li>
<li>通过结合离散标签和说话者嵌入，实现完全控制说话者的音色和其他风格信息，包括调整特定说话者的情感等属性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02997">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6d2aff02a88d8baaf94543ab9801b48b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05e0c2fc1fe1ecc81f22126d3e41a85a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-437d8f18ee64c289abc2cd4e65e463ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bc395c6e2dadb5b608cca4d27d6f393.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb7a43773cec2fac3023f22ee8a64a99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45cc26b0738da18006216f630b03a50f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-a-Japanese-Full-duplex-Spoken-Dialogue-System"><a href="#Towards-a-Japanese-Full-duplex-Spoken-Dialogue-System" class="headerlink" title="Towards a Japanese Full-duplex Spoken Dialogue System"></a>Towards a Japanese Full-duplex Spoken Dialogue System</h2><p><strong>Authors:Atsumoto Ohashi, Shinya Iizuka, Jingjing Jiang, Ryuichiro Higashinaka</strong></p>
<p>Full-duplex spoken dialogue systems, which can model simultaneous bidirectional features of human conversations such as speech overlaps and backchannels, have attracted significant attention recently. However, the study of full-duplex spoken dialogue systems for the Japanese language has been limited, and the research on their development in Japanese remains scarce. In this paper, we present the first publicly available full-duplex spoken dialogue model in Japanese, which is built upon Moshi, a full-duplex dialogue model in English. Our model is trained through a two-stage process: pre-training on a large-scale spoken dialogue data in Japanese, followed by fine-tuning on high-quality stereo spoken dialogue data. We further enhance the model’s performance by incorporating synthetic dialogue data generated by a multi-stream text-to-speech system. Evaluation experiments demonstrate that the trained model outperforms Japanese baseline models in both naturalness and meaningfulness. </p>
<blockquote>
<p>近年来，能够模拟人类对话的双向同时进行特性（如言语重叠和反馈通道）的全双工对话系统引起了人们的广泛关注。然而，针对日语的全双工对话系统的研究较为有限，其开发研究依然稀少。在本文中，我们展示了基于日语的第一个公开可用的全双工对话模型，该模型建立在英语的全双工对话模型Moshi之上。我们的模型通过两阶段训练过程：首先在大规模的日语口语对话数据上进行预训练，然后利用高质量的立体声口语对话数据进行微调。我们进一步通过结合由多流文本到语音系统生成的综合对话数据来提高模型的性能。评估实验表明，训练后的模型在日语基线模型的自然度和意义性方面都表现得更好。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02979v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>该论文介绍了首个公开的日语全双工对话模型，该模型基于英语的双工对话模型Moshi构建。通过大规模日语对话数据预训练和高质量立体声对话数据微调的两阶段过程进行训练，并使用多流文本到语音系统生成的合成对话数据增强模型性能。评估实验表明，该模型在日语基准模型上表现更出色，更自然、有意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文发布了首个日语全双工对话模型。</li>
<li>模型建立在英语的双工对话模型Moshi的基础上。</li>
<li>模型通过两阶段过程进行训练：首先在大规模日语对话数据上进行预训练，然后在高质量立体声对话数据上进行微调。</li>
<li>合成对话数据通过多流文本到语音系统生成，增强了模型性能。</li>
<li>模型在日语基准模型上的表现更自然和有意义。</li>
<li>全双工对话系统能够模拟人类对话的双向特征，如语音重叠和反馈通道。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b7400a8cc0d30f4bfd02ffe1a4326777.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76749f6f0d5726186e99f2cc572edd14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b56fe64df06e5e9b544a1ae35ffc2db7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-847c4c71e81a6d97a225e83bfc956bb6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CapSpeech-Enabling-Downstream-Applications-in-Style-Captioned-Text-to-Speech"><a href="#CapSpeech-Enabling-Downstream-Applications-in-Style-Captioned-Text-to-Speech" class="headerlink" title="CapSpeech: Enabling Downstream Applications in Style-Captioned   Text-to-Speech"></a>CapSpeech: Enabling Downstream Applications in Style-Captioned   Text-to-Speech</h2><p><strong>Authors:Helin Wang, Jiarui Hai, Dading Chong, Karan Thakkar, Tiantian Feng, Dongchao Yang, Junhyeok Lee, Laureano Moro Velazquez, Jesus Villalba, Zengyi Qin, Shrikanth Narayanan, Mounya Elhiali, Najim Dehak</strong></p>
<p>Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems. </p>
<blockquote>
<p>近年来，生成式人工智能的最新进展为带风格标注的文本到语音合成（CapTTS）领域带来了显著变革。然而，由于缺少标准化、全面的数据集以及基于CapTTS的下游任务研究有限，将CapTTS适应于现实应用仍然具有挑战性。为了解决这些空白，我们引入了CapSpeech，这是一个为一系列与CapTTS相关的任务设计的新基准测试，包括带有声音事件的风格标注文本到语音合成（CapTTS-SE）、口音标注的TTS（AccCapTTS）、情感标注的TTS（EmoCapTTS）以及用于聊天代理的文本到语音合成（AgentTTS）。CapSpeech包含超过1000万个机器标注的音频字幕对和近36万个人工标注的音频字幕对。此外，我们还由专业配音演员和经验丰富的音频工程师收集和录制了两个新数据集，专门用于AgentTTS和CapTTS-SE任务。除了数据集之外，我们还使用自回归和非自回归模型在CapSpeech上进行了全面的实验。我们的结果证明了在各种不同的说话风格中，具有高保真度和高度可理解性的语音合成。据我们所知，CapSpeech是现有最大的为CapTTS相关任务提供全面注释的数据集。实验和发现为开发CapTTS系统提供了宝贵的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02863v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期生成式人工智能的进步显著改变了风格标注文本到语音合成（CapTTS）领域。然而，将CapTTS适应到实际应用中仍面临缺乏标准化、全面的数据集以及在CapTTS基础上构建的下游任务研究的限制。为解决这些问题，我们引入了CapSpeech，一个为CapTTS相关任务设计的新基准，包括带有声音事件的风格标注文本到语音合成（CapTTS-SE）、口音标注TTS（AccCapTTS）、情感标注TTS（EmoCapTTS）和用于聊天代理的文本到语音合成（AgentTTS）。CapSpeech包含超过1千万机器标注的音频-字幕对和近0.36百万人类标注的音频-字幕对。此外，我们还由专业配音演员和经验丰富的音频工程师收集和记录了两个新数据集，专门用于AgentTTS和CapTTS-SE任务。同时，我们在CapSpeech上使用了自回归和非自回归模型进行了全面的实验。结果证明了我们在多种说话风格上实现高保真、高度可理解的语音合成。据我们所知，CapSpeech是提供CapTTS相关任务全面注释的最大可用数据集。实验和发现为进一步开发CapTTS系统提供了宝贵见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式人工智能的进步已显著改变风格标注文本到语音合成（CapTTS）领域。</li>
<li>CapSpeech基准引入，包含多种CapTTS相关任务，如CapTTS-SE、AccCapTTS、EmoCapTTS和AgentTTS。</li>
<li>CapSpeech包含大量机器和人类标注的音频-字幕对数据。</li>
<li>新数据集由专业配音演员和音频工程师收集，用于AgentTTS和CapTTS-SE任务。</li>
<li>全面的实验证明了高保真、高度可理解的语音合成能力。</li>
<li>CapSpeech是提供CapTTS相关任务全面注释的最大可用数据集。</li>
<li>实验结果提供了开发CapTTS系统的宝贵见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02863">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d9dbeacf5de28b61175fc4a9123e25c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a12b50356ee9dfd1bb29660b1810cc54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36655116223f38a6bb5e321eb64a8550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db8915b61e2f47475d6db40132b65df1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Prompt-Unseen-Emotion-Zero-shot-Expressive-Speech-Synthesis-with-Prompt-LLM-Contextual-Knowledge-for-Mixed-Emotions"><a href="#Prompt-Unseen-Emotion-Zero-shot-Expressive-Speech-Synthesis-with-Prompt-LLM-Contextual-Knowledge-for-Mixed-Emotions" class="headerlink" title="Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with   Prompt-LLM Contextual Knowledge for Mixed Emotions"></a>Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with   Prompt-LLM Contextual Knowledge for Mixed Emotions</h2><p><strong>Authors:Xiaoxue Gao, Huayun Zhang, Nancy F. Chen</strong></p>
<p>Existing expressive text-to-speech (TTS) systems primarily model a limited set of categorical emotions, whereas human conversations extend far beyond these predefined emotions, making it essential to explore more diverse emotional speech generation for more natural interactions. To bridge this gap, this paper proposes a novel prompt-unseen-emotion (PUE) approach to generate unseen emotional speech via emotion-guided prompt learning. PUE is trained utilizing an LLM-TTS architecture to ensure emotional consistency between categorical emotion-relevant prompts and emotional speech, allowing the model to quantitatively capture different emotion weightings per utterance. During inference, mixed emotional speech can be generated by flexibly adjusting emotion proportions and leveraging LLM contextual knowledge, enabling the model to quantify different emotional styles. Our proposed PUE successfully facilitates expressive speech synthesis of unseen emotions in a zero-shot setting. </p>
<blockquote>
<p>现有的表达性文本到语音（TTS）系统主要对有限的分类情绪进行建模，而人类对话则远远超出这些预定义的情绪。因此，为了更自然的交互，探索更多样化的情感语音生成至关重要。为了弥补这一差距，本文提出了一种新型提示未见情绪（PUE）方法，通过情感引导提示学习来生成未见情感语音。PUE利用LLM-TTS架构进行训练，以确保分类情绪相关提示与情感语音之间的情感一致性，使得模型可以定量捕获每个话语的不同情绪权重。在推理过程中，可以通过灵活地调整情绪比例并利用LLM的上下文知识来生成混合情感语音，使模型能够量化不同的情感风格。我们提出的PUE成功地在零样本设置下促进了未见情感的表达性语音合成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02742v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为PUE（Prompt Unseen Emotion）的方法，旨在通过情感引导的提示学习生成未见情感语音。该模型基于LLM-TTS架构训练，保证了与分类情感相关的提示和情绪语音之间的情感一致性，可以量化捕捉不同情感权重。在推理过程中，通过灵活调整情感比例并利用LLM的上下文知识，可以生成混合情感语音，实现不同情感风格的量化。该研究成功促进了零样本设置下的未见情感语音的合成表达。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PUE方法能够生成未见情感语音，弥补了现有TTS系统在情感表达上的不足。</li>
<li>LLM-TTS架构被用于确保情感一致性，使模型能够捕捉不同情感的权重。</li>
<li>在推理阶段，可以通过调整情感比例生成混合情感语音。</li>
<li>LLM的上下文知识被用来提高情感语音生成的上下文连贯性和自然度。</li>
<li>PUE方法实现了零样本设置下的情感语音合成。</li>
<li>该方法不仅能处理预设的类别情感，还能处理更广泛的情感表达。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02742">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3b2372d7386348a0c29b286f118fe810.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77a4d2f8d896ba6e0c0ebeb57ccbd2cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be64077dbb482c82f476bdd21fb1ad4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-130f7a88b04c6438452be2e1b389c2b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6fc8014108820701114da374a1c5c1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7eb7205557b62949a21afb59bc07309.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99d3925fc6252fa9db33c2b532d931e3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SALF-MOS-Speaker-Agnostic-Latent-Features-Downsampled-for-MOS-Prediction"><a href="#SALF-MOS-Speaker-Agnostic-Latent-Features-Downsampled-for-MOS-Prediction" class="headerlink" title="SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS   Prediction"></a>SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS   Prediction</h2><p><strong>Authors:Saurabh Agrawal, Raj Gohil, Gopal Kumar Agrawal, Vikram C M, Kushal Verma</strong></p>
<p>Speech quality assessment is a critical process in selecting text-to-speech synthesis (TTS) or voice conversion models. Evaluation of voice synthesis can be done using objective metrics or subjective metrics. Although there are many objective metrics like the Perceptual Evaluation of Speech Quality (PESQ), Perceptual Objective Listening Quality Assessment (POLQA) or Short-Time Objective Intelligibility (STOI) but none of them is feasible in selecting the best model. On the other hand subjective metric like Mean Opinion Score is highly reliable but it requires a lot of manual efforts and are time-consuming. To counter the issues in MOS Evaluation, we have developed a novel model, Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a small-sized, end-to-end, highly generalized and scalable model for predicting MOS score on a scale of 5. We use the sequences of convolutions and stack them to get the latent features of the audio samples to get the best state-of-the-art results based on mean squared error (MSE), Linear Concordance Correlation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and Kendall Rank Correlation Coefficient (KTAU). </p>
<blockquote>
<p>语音质量评估是选择文本到语音合成（TTS）或语音转换模型的关键过程。语音合成的评估可以使用客观指标或主观指标来完成。虽然有许多客观指标，如语音质量的感知评估（PESQ）、感知客观听觉质量评估（POLQA）或短期客观可懂度（STOI），但没有任何一个指标可用于选择最佳模型。另一方面，主观指标如平均意见得分非常可靠，但它需要大量的手动操作和耗时。为了解决MOS评估中的问题，我们开发了一种新型模型，即不讲者潜在特征（SALF）-平均意见得分（MOS），这是一个小型、端到端、高度通用和可扩展的模型，可用于预测5分制上的MOS分数。我们使用卷积序列并将其堆叠起来，以获得音频样本的潜在特征，以基于均方误差（MSE）、线性一致性相关系数（LCC）、斯皮尔曼等级相关系数（SRCC）和肯德尔等级相关系数（KTAU）获得最佳的最先进结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02082v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了语音质量评估在文本转语音合成模型选择中的重要性，并探讨了客观和主观评估方法的问题。针对主观评估方法如平均意见得分（MOS）的缺陷，提出了一种新型的语音质量评估模型——说话人无关潜在特征（SALF）-MOS模型。该模型具有小尺寸、端到端、高度通用和可扩展的特点，可以通过音频样本的潜在特征预测MOS分数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音质量评估在文本转语音合成模型选择中的重要性。</li>
<li>现有的客观语音质量评估指标如PESQ、POLQA和STOI等并不完美，无法选择最佳模型。</li>
<li>主观评估方法如平均意见得分（MOS）虽然可靠，但耗时且需要大量人工参与。</li>
<li>提出了新型的语音质量评估模型——SALF-MOS。</li>
<li>SALF-MOS模型具有小尺寸、端到端、高度通用和可扩展的特点。</li>
<li>SALF-MOS模型通过音频样本的潜在特征预测MOS分数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02082">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f395630221baeccb235ff6feeec0698b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9dd1c52639d14e45b75f34d1af35d474.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8deb09517f226a50073ba7fe12a8d4d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9aa0799f4ef933a624f44a8e2894a54c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23c3b9cd2c37b7cf97b63b2373d56b0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c19249b3a15794a765a04cd03d9a6f8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Speech-to-Speech-Translation-Pipelines-for-Conversations-in-Low-Resource-Languages"><a href="#Speech-to-Speech-Translation-Pipelines-for-Conversations-in-Low-Resource-Languages" class="headerlink" title="Speech-to-Speech Translation Pipelines for Conversations in Low-Resource   Languages"></a>Speech-to-Speech Translation Pipelines for Conversations in Low-Resource   Languages</h2><p><strong>Authors:Andrei Popescu-Belis, Alexis Allemann, Teo Ferrari, Gopal Krishnamani</strong></p>
<p>The popularity of automatic speech-to-speech translation for human conversations is growing, but the quality varies significantly depending on the language pair. In a context of community interpreting for low-resource languages, namely Turkish and Pashto to&#x2F;from French, we collected fine-tuning and testing data, and compared systems using several automatic metrics (BLEU, COMET, and BLASER) and human assessments. The pipelines included automatic speech recognition, machine translation, and speech synthesis, with local models and cloud-based commercial ones. Some components have been fine-tuned on our data. We evaluated over 60 pipelines and determined the best one for each direction. We also found that the ranks of components are generally independent of the rest of the pipeline. </p>
<blockquote>
<p>自动语音对话翻译技术的普及程度正不断增长，但其质量在很大程度上取决于语言对。我们在针对低资源语言（即土耳其语和普什图语至法语&#x2F;从法语翻译）的社区口译背景下，收集了调优和测试数据，并使用几种自动指标（BLEU、COMET和BLASER）以及人工评估对比了不同系统。这些管道包括自动语音识别、机器翻译和语音合成，其中包括本地模型和基于云的商业模型。某些组件已针对我们的数据进行过微调。我们评估了超过60个管道，并确定了每个方向的最佳管道。我们还发现，组件的排名通常与管道的其他部分无关。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01406v1">PDF</a> Proceedings of MT Summit 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了自动语音到语音翻译在人类对话中的普及情况，特别是在低资源语言如土耳其语和普什图语与法语之间的翻译。文章通过收集调优和测试数据，对比了不同系统的表现，并发现不同语言对的翻译质量差异显著。研究包括自动语音识别、机器翻译和语音合成等环节，涉及本地模型和基于云的商业模型。通过评估超过60个管道，确定了每个方向的最佳方案，并发现组件排名通常与管道其余部分无关。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动语音到语音翻译在低资源语言如土耳其语和普什图语与法语之间的应用正在增长。</li>
<li>不同语言对的翻译质量差异显著。</li>
<li>研究涉及自动语音识别、机器翻译和语音合成等环节。</li>
<li>本地模型和基于云的商业模型均被考虑。</li>
<li>超过60个管道被评估，确定了每个方向的最佳方案。</li>
<li>组件的排名通常与管道的其他部分无关。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01406">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cccb22a9f02cf83c83fe8f812a44c504.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15ec55bc520f52b14cff76228ee542ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2510049e0fd64e2c44b8573e49f81e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f477bedef6a6dda8c58c7612bf1733d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2008a1bb64a7fbd156bf5bc76c6f94f2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CleanS2S-Single-file-Framework-for-Proactive-Speech-to-Speech-Interaction"><a href="#CleanS2S-Single-file-Framework-for-Proactive-Speech-to-Speech-Interaction" class="headerlink" title="CleanS2S: Single-file Framework for Proactive Speech-to-Speech   Interaction"></a>CleanS2S: Single-file Framework for Proactive Speech-to-Speech   Interaction</h2><p><strong>Authors:Yudong Lu, Yazhe Niu, Shuai Hu, Haolin Wang</strong></p>
<p>CleanS2S is a framework for human-like speech-to-speech interaction that advances conversational AI through single-file implementation and proactive dialogue capabilities. Our system integrates automatic speech recognition, large language models, and text-to-speech synthesis into a unified pipeline with real-time interruption handling, achieving low transition latency through full-duplex websocket connections and non-blocking I&#x2F;O. Beyond conventional chatbot paradigms, we pioneer a proactive interaction mechanism, which combines memory systems with Subjective Action Judgement module, enabling five human-like response strategies: interruption, refusal, deflection, silence, and standard response. The memory module dynamically aggregates historical, and contextual data to inform interaction decisions. This approach breaks the rigid turn-based convention by allowing system-initiated dialog control and context-aware response selection. And we propose Action Judgement SFT that assesses input streams for responses strategies. The framework’s single-file implementation with atomic configurations offers researchers unprecedented transparency and extensibility for interaction agents. The code of CleanS2S is released at \<a target="_blank" rel="noopener" href="https://github.com/opendilab/CleanS2S">https://github.com/opendilab/CleanS2S</a>. </p>
<blockquote>
<p>CleanS2S是一个面向人类语音对话交互的框架，它通过单文件实现和主动对话功能推动了会话式人工智能的发展。我们的系统集成了自动语音识别、大型语言模型和文本到语音合成，通过实时中断处理构建了一个统一管道，通过全双工web套接字连接和非阻塞I&#x2F;O实现了低转换延迟。除了传统的聊天机器人模式之外，我们还开创了一种主动交互机制，该机制结合了记忆系统与主观动作判断模块，可实现五种人类化的响应策略：中断、拒绝、回避、沉默和标准响应。记忆模块动态聚合历史和上下文数据以支持交互决策。这种方法打破了基于轮询的严格规则，允许系统主动控制对话和基于上下文的响应选择。我们提出了动作判断SFT（Action Judgement SFT），用于评估输入流以选择响应策略。CleanS2S的单文件实现以及原子配置为研究人员提供了前所未有的交互代理透明度和扩展性。CleanS2S的代码已发布在<a target="_blank" rel="noopener" href="https://github.com/opendilab/CleanS2S%E4%B8%8A%E3%80%82">https://github.com/opendilab/CleanS2S上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01268v1">PDF</a> </p>
<p><strong>摘要</strong><br>清洁S2S是一个面向人类语音对话的框架，通过单一文件实现和主动对话能力推动对话AI的发展。它集成了自动语音识别、大型语言模型和文本到语音合成技术，具有实时中断处理能力，并通过全双工websocket连接和非阻塞I&#x2F;O实现低转换延迟。该框架采用主动交互机制，结合记忆系统与主观动作判断模块，实现五种类似人类的响应策略：中断、拒绝、偏离、沉默和标准响应。记忆模块动态聚合历史和上下文数据以支持交互决策。它打破了传统的基于轮流的模式，允许系统启动对话控制和基于上下文的响应选择。CleanS2S框架为研究人员提供了前所未有的交互代理的透明度和扩展性。CleanS2S的代码已发布在<a target="_blank" rel="noopener" href="https://github.com/opendilab/CleanS2S%E3%80%82">https://github.com/opendilab/CleanS2S。</a></p>
<p><strong>要点</strong></p>
<ol>
<li>CleanS2S是一个面向人类语音对话的框架，集成了自动语音识别、大型语言模型和文本到语音合成技术。</li>
<li>它实现了实时中断处理和低转换延迟，通过全双工websocket连接和非阻塞I&#x2F;O支持高效对话。</li>
<li>CleanS2S采用主动交互机制，允许系统启动对话控制和基于上下文的响应选择，实现五种类似人类的响应策略。</li>
<li>记忆模块能够动态聚合历史和上下文数据，以支持更智能的交互决策。</li>
<li>该框架通过单一文件实现和原子配置提供前所未有的透明度和扩展性，便于研究人员理解和改进交互代理。</li>
<li>CleanS2S代码已发布在GitHub上，供公众访问和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01268">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f959bcfcf98c2e58e088a4f316e01c27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-410197554b0d7e952590a83b9e5ceb4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-377451e45911d33fe1c382fd9e712f41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ec648ab26488346a9b7f60736cd42e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11fa68f1f299f2e2d112c6261f3eb983.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7c3f5759330522b32b9add3dfd55177.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DS-TTS-Zero-Shot-Speaker-Style-Adaptation-from-Voice-Clips-via-Dynamic-Dual-Style-Feature-Modulation"><a href="#DS-TTS-Zero-Shot-Speaker-Style-Adaptation-from-Voice-Clips-via-Dynamic-Dual-Style-Feature-Modulation" class="headerlink" title="DS-TTS: Zero-Shot Speaker Style Adaptation from Voice Clips via Dynamic   Dual-Style Feature Modulation"></a>DS-TTS: Zero-Shot Speaker Style Adaptation from Voice Clips via Dynamic   Dual-Style Feature Modulation</h2><p><strong>Authors:Ming Meng, Ziyi Yang, Jian Yang, Zhenjie Su, Yonggui Zhu, Zhaoxin Fan</strong></p>
<p>Recent advancements in text-to-speech (TTS) technology have increased demand for personalized audio synthesis. Zero-shot voice cloning, a specialized TTS task, aims to synthesize a target speaker’s voice using only a single audio sample and arbitrary text, without prior exposure to the speaker during training. This process employs pattern recognition techniques to analyze and replicate the speaker’s unique vocal features. Despite progress, challenges remain in adapting to the vocal style of unseen speakers, highlighting difficulties in generalizing TTS systems to handle diverse voices while maintaining naturalness, expressiveness, and speaker fidelity. To address the challenges of unseen speaker style adaptation, we propose DS-TTS, a novel approach aimed at enhancing the synthesis of diverse, previously unheard voices. Central to our method is a Dual-Style Encoding Network (DuSEN), where two distinct style encoders capture complementary aspects of a speaker’s vocal identity. These speaker-specific style vectors are seamlessly integrated into the Dynamic Generator Network (DyGN) via a Style Gating-Film (SGF) mechanism, enabling more accurate and expressive reproduction of unseen speakers’ unique vocal characteristics. In addition, we introduce a Dynamic Generator Network to tackle synthesis issues that arise with varying sentence lengths. By dynamically adapting to the length of the input, this component ensures robust performance across diverse text inputs and speaker styles, significantly improving the model’s ability to generalize to unseen speakers in a more natural and expressive manner. Experimental evaluations on the VCTK dataset suggest that DS-TTS demonstrates superior overall performance in voice cloning tasks compared to existing state-of-the-art models, showing notable improvements in both word error rate and speaker similarity. </p>
<blockquote>
<p>近期文本转语音（TTS）技术的进展增加了对个性化音频合成的需求。零样本语音克隆是一项专门的TTS任务，旨在仅使用单个音频样本和任意文本合成目标说话人的声音，而无需在训练过程中事先接触该说话人。该过程采用模式识别技术来分析并复制说话人的独特语音特征。尽管取得了进展，但适应未见说话人的语音风格仍存在挑战，这凸显了将TTS系统推广应用于处理各种声音时，在保持自然性、表达力和说话人保真度方面的困难。为了解决未见说话人风格适应的挑战，我们提出了DS-TTS，这是一种旨在增强对多样且之前未听过的声音合成的新方法。我们的方法的核心是双重风格编码网络（DuSEN），其中两个截然不同的风格编码器捕捉说话人语音身份的互补方面。这些说话人特定的风格向量通过风格门控电影（SGF）机制无缝地集成到动态生成网络（DyGN）中，从而能够更准确、更生动地复制未见说话人的独特语音特征。此外，我们引入了动态生成网络来解决因句子长度不同而产生的合成问题。通过动态适应输入的长度，此组件可确保在各种文本输入和说话人风格方面的稳健性能，从而显著提高模型以更自然、更具表现力的方式推广到未见说话人的能力。在VCTK数据集上的实验评估表明，与现有最先进的模型相比，DS-TTS在语音克隆任务中表现出卓越的整体性能，在单词错误率和说话人相似性方面都取得了显着的改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01020v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>近期文本转语音（TTS）技术的进展推动了个性化音频合成的需求增长。零样本语音克隆作为专项TTS任务，旨在仅使用单个音频样本和任意文本合成目标说话人的声音，且训练过程中无需预先接触该说话人。此流程采用模式识别技术分析并复制说话人的独特语音特征。尽管有所进展，但适应未见说话人的语音风格仍存在挑战，这突显了TTS系统在处理多样化声音时保持自然性、表达力和说话人保真度的概括推广难题。为解决未见说话人风格适应的挑战，我们提出了DS-TTS，这是一种旨在增强多样化、先前未听过的声音合成的新方法。我们的方法核心是双重风格编码网络（DuSEN），其中两个独特的风格编码器捕捉说话人语音身份的不同方面。这些说话人特定的风格向量被无缝集成到动态生成网络（DyGN）中，通过风格门控电影（SGF）机制，能够更精确、更生动地再现未见说话人的独特语音特征。此外，我们还引入了动态生成网络来解决与句子长度变化相关的合成问题。通过动态适应输入长度，此组件确保在各种文本输入和说话人风格上的稳健性能，显著提高了模型以更自然、更生动的方式概括未见说话人的能力。在VCTK数据集上的实验评估表明，与现有最先进的模型相比，DS-TTS在语音克隆任务中表现出整体优越的性能，在词错误率和说话人相似性方面均取得显著改进。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>最近TTS技术的进展推动了个性化音频合成的需求。</li>
<li>零样本语音克隆旨在仅使用单个音频样本和任意文本合成目标说话人的声音。</li>
<li>适应未见说话人的语音风格是TTS技术的一个关键挑战。</li>
<li>DS-TTS方法通过双重风格编码网络和动态生成网络来解决这一挑战。</li>
<li>双重风格编码网络能够捕捉说话人语音身份的独特方面。</li>
<li>动态生成网络可以解决与句子长度变化相关的合成问题，提高模型的概括能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01020">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3ec35d4af76665baadd76cee094baae9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54ecd27a3921d48df90e06aa0a682e2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c688a7ce6554205de22318db9ff659e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e901ac17039972bf9993f10d13908a5d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Training-for-Open-E2E-Spoken-Dialogue-Systems"><a href="#Chain-of-Thought-Training-for-Open-E2E-Spoken-Dialogue-Systems" class="headerlink" title="Chain-of-Thought Training for Open E2E Spoken Dialogue Systems"></a>Chain-of-Thought Training for Open E2E Spoken Dialogue Systems</h2><p><strong>Authors:Siddhant Arora, Jinchuan Tian, Hayato Futami, Jee-weon Jung, Jiatong Shi, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe</strong></p>
<p>Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue systems preserve full differentiability and capture non-phonemic information, making them well-suited for modeling spoken interactions. However, existing E2E approaches often require large-scale training data and generates responses lacking semantic coherence. We propose a simple yet effective strategy leveraging a chain-of-thought (CoT) formulation, ensuring that training on conversational data remains closely aligned with the multimodal language model (LM)’s pre-training on speech recognition~(ASR), text-to-speech synthesis (TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over the baseline, successfully training spoken dialogue systems on publicly available human-human conversation datasets, while being compute-efficient enough to train on just 300 hours of public human-human conversation data, such as the Switchboard. We will publicly release our models and training code. </p>
<blockquote>
<p>与传统级联管道不同，端到端（E2E）口语对话系统保持完全可微分并捕获非语音信息，使其成为建模口语交互的理想选择。然而，现有的E2E方法通常需要大规模的训练数据，并且生成的响应缺乏语义连贯性。我们提出了一种简单有效的策略，利用思维链（CoT）公式，确保在对话数据上的训练与多模态语言模型（LM）在语音识别（ASR）、文本到语音合成（TTS）和文本LM任务上的预训练紧密对齐。我们的方法在基准测试上实现了超过1.5的ROUGE-1改进，成功地在公开可用的人机对话数据集上训练了口语对话系统，同时计算效率足够高，只需在300小时公开的人机对话数据（如Switchboard）上进行训练。我们将公开发布我们的模型和训练代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00722v1">PDF</a> Accepted at INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了端到端的语音对话系统（E2E SDS）的优势和挑战。为了克服现有E2E方法的不足，提出了一种利用思维链（CoT）的策略，确保对话系统的训练与多模态语言模型的预训练紧密对齐。该方法成功地在公开的人类对话数据集上训练了语音对话系统，实现了相较于基准线的ROUGE-1评分提升超过1.5，且能够在仅使用300小时公开人类对话数据的情况下进行高效计算训练。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>端到端的语音对话系统（E2E SDS）能够保留完整的可微调性并捕捉非语音信息，适合建模口语交互。</li>
<li>现有E2E方法需要大规模的训练数据，生成的响应缺乏语义连贯性。</li>
<li>提出了利用思维链（CoT）的策略，确保对话系统的训练与多模态语言模型的预训练对齐。</li>
<li>该方法成功地在公开的人类对话数据集上实现了相较于基准线的ROUGE-1评分提升超过1.5。</li>
<li>该方法具有计算效率，能够在仅使用300小时公开人类对话数据的情况下进行训练。</li>
<li>本文将公开模型和训练代码。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00722">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-43c53528406a787aa022ed7a37ac9772.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3aabb0724524d837b8ba4fb695aa26b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8f420b2610b8f8f1210ee1bd5a106d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43cf3653181841430e5eec7649b2fb40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e9210fc4aac39dd76f46dbd50237eb1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Quality-Assessment-of-Noisy-and-Enhanced-Speech-with-Limited-Data-UWB-NTIS-System-for-VoiceMOS-2024-and-Beyond"><a href="#Quality-Assessment-of-Noisy-and-Enhanced-Speech-with-Limited-Data-UWB-NTIS-System-for-VoiceMOS-2024-and-Beyond" class="headerlink" title="Quality Assessment of Noisy and Enhanced Speech with Limited Data:   UWB-NTIS System for VoiceMOS 2024 and Beyond"></a>Quality Assessment of Noisy and Enhanced Speech with Limited Data:   UWB-NTIS System for VoiceMOS 2024 and Beyond</h2><p><strong>Authors:Marie Kunešová</strong></p>
<p>In this preprint, we present the UWB-NTIS-TTS team’s submission to Track 3 of the VoiceMOS 2024 Challenge, the goal of which was to automatically assess the speech quality of noisy and de-noised speech in terms of the ITU-T P.835 metrics of “SIG”, “BAK”, and “OVRL”. Our proposed system, based on wav2vec 2.0, placed among the top systems in the challenge, achieving the best prediction of the BAK scores (background noise intrusiveness), the second-best prediction of the OVRL score (overall audio quality), and the third-best prediction of SIG (speech signal quality) out of the five participating systems. We describe our approach, such as the two-stage fine-tuning process we used to contend with the challenge’s very limiting restrictions on allowable training data, and present the results achieved both on the VoiceMOS 2024 Challenge data and on the recently released CHiME 7 - UDASE dataset. </p>
<blockquote>
<p>在这篇预印文章中，我们展示了UWB-NTIS-TTS团队对VoiceMOS 2024挑战赛第3赛道的提交内容。该挑战赛的目标是自动评估带噪和降噪语音的语音质量，具体参照ITU-T P.835指标的“SIG”（语音信号质量）、“BAK”（背景噪声侵入性）和“OVRL”（总体音频质量）。我们提出的基于wav2vec 2.0的系统在挑战中跻身顶尖系统之列，实现了对BAK得分（背景噪声侵入性）的最佳预测、对OVRL得分（总体音频质量）的第二佳预测，以及对SIG（语音信号质量）的第三佳预测（在五套参赛系统中）。我们描述了我们的方法，例如我们用于应对挑战赛对可用训练数据非常严格的限制的两阶段微调过程，并展示了在VoiceMOS 2024挑战赛数据和最近发布的CHiME 7-UDASE数据集上取得的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00506v1">PDF</a> This is a preliminary write-up of our initial work, posted as an   early version preprint for cross-referencing purposes. We intend to further   extend this research and submit it for publication at a conference, at which   point this preprint will be updated with the full text</p>
<p><strong>Summary</strong><br>基于wav2vec 2.0的UWB-NTIS-TTS团队在VoiceMOS 2024挑战赛的Track 3中提出了自动评估带噪与降噪语音质量的系统。该系统在背景噪声侵入性（BAK）评分预测上表现最佳，总体音频质量（OVRL）评分预测位列第二，语音信号质量（SIG）预测位列第三。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UWB-NTIS-TTS团队参加了VoiceMOS 2024挑战赛的Track 3，旨在自动评估带噪和降噪语音的质量。</li>
<li>团队使用基于wav2vec 2.0的系统参赛，应对挑战赛对训练数据的严格限制。</li>
<li>系统在背景噪声侵入性（BAK）评分预测上取得最佳表现。</li>
<li>在总体音频质量（OVRL）评分预测上，系统获得第二名。</li>
<li>在语音信号质量（SIG）预测方面，系统排名第三。</li>
<li>团队描述了两阶段微调过程来应对挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00506">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9968484ba1646d9222b4aac86de3b17c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3487533240cb0561354460c4740e1f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b814042604d2a560e5ea45df3fb29ae8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f57632d47d562faf6dc951e7a5c9845.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-226f2d3a37045a091dcbce8e5c282893.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-06-05  Towards a Japanese Full-duplex Spoken Dialogue System
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-971ee3a8f96ee3b15eaa632cec6e76c5.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-06-05  Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
