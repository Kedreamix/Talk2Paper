<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich   Representation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-be64077dbb482c82f476bdd21fb1ad4e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    36 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-05-æ›´æ–°"><a href="#2025-06-05-æ›´æ–°" class="headerlink" title="2025-06-05 æ›´æ–°"></a>2025-06-05 æ›´æ–°</h1><h2 id="Controllable-Text-to-Speech-Synthesis-with-Masked-Autoencoded-Style-Rich-Representation"><a href="#Controllable-Text-to-Speech-Synthesis-with-Masked-Autoencoded-Style-Rich-Representation" class="headerlink" title="Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich   Representation"></a>Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich   Representation</h2><p><strong>Authors:Yongqi Wang, Chunlei Zhang, Hangting Chen, Zhou Zhao, Dong Yu</strong></p>
<p>Controllable TTS models with natural language prompts often lack the ability for fine-grained control and face a scarcity of high-quality data. We propose a two-stage style-controllable TTS system with language models, utilizing a quantized masked-autoencoded style-rich representation as an intermediary. In the first stage, an autoregressive transformer is used for the conditional generation of these style-rich tokens from text and control signals. The second stage generates codec tokens from both text and sampled style-rich tokens. Experiments show that training the first-stage model on extensive datasets enhances the content robustness of the two-stage model as well as control capabilities over multiple attributes. By selectively combining discrete labels and speaker embeddings, we explore fully controlling the speakerâ€™s timbre and other stylistic information, and adjusting attributes like emotion for a specified speaker. Audio samples are available at <a target="_blank" rel="noopener" href="https://style-ar-tts.github.io/">https://style-ar-tts.github.io</a>. </p>
<blockquote>
<p>å…·æœ‰è‡ªç„¶è¯­è¨€æç¤ºçš„å¯æ§æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹é€šå¸¸ç¼ºä¹ç²¾ç»†æ§åˆ¶çš„èƒ½åŠ›ï¼Œå¹¶ä¸”é¢ä¸´é«˜è´¨é‡æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µé£æ ¼å¯æ§çš„TTSç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨é‡åŒ–çš„æ©ç è‡ªåŠ¨ç¼–ç é£æ ¼ä¸°å¯Œçš„è¡¨ç¤ºä½œä¸ºä¸­ä»‹ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨è‡ªå›å½’å˜å‹å™¨æ ¹æ®æ–‡æœ¬å’Œæ§åˆ¶ä¿¡å·ç”Ÿæˆè¿™äº›é£æ ¼ä¸°å¯Œçš„æ ‡è®°ã€‚ç¬¬äºŒé˜¶æ®µä»æ–‡æœ¬å’Œé‡‡æ ·çš„é£æ ¼ä¸°å¯Œçš„æ ‡è®°ç”Ÿæˆç¼–ç æ ‡è®°ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µæ¨¡å‹ä¸Šè¿›è¡Œå¤§é‡æ•°æ®é›†è®­ç»ƒï¼Œå¯ä»¥å¢å¼ºä¸¤é˜¶æ®µæ¨¡å‹çš„å†…å®¹ç¨³å¥æ€§ä»¥åŠå¯¹å¤šä¸ªå±æ€§çš„æ§åˆ¶èƒ½åŠ›ã€‚é€šè¿‡æœ‰é€‰æ‹©åœ°ç»“åˆç¦»æ•£æ ‡ç­¾å’Œè¯´è¯è€…åµŒå…¥ï¼Œæˆ‘ä»¬å¯ä»¥å®Œå…¨æ§åˆ¶è¯´è¯è€…çš„éŸ³è‰²å’Œå…¶ä»–é£æ ¼ä¿¡æ¯ï¼Œå¹¶è°ƒæ•´æŒ‡å®šè¯´è¯è€…çš„æƒ…æ„Ÿç­‰å±æ€§ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://style-ar-tts.github.ioæ‰¾åˆ°./">https://style-ar-tts.github.ioæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02997v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ä¸¤é˜¶æ®µé£æ ¼å¯æ§çš„æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ï¼ˆTTSï¼‰ç³»ç»Ÿã€‚åˆ©ç”¨è¯­è¨€æ¨¡å‹æ„å»ºå¯æ§TTSæ¨¡å‹é¢ä¸´çš„é—®é¢˜æ˜¯ç¼ºå°‘ç²¾ç»†åŒ–æ§åˆ¶ä»¥åŠé«˜è´¨é‡æ•°æ®çš„ç¨€ç¼ºã€‚é€šè¿‡å¼•å…¥é‡åŒ–æ©ç è‡ªç¼–ç é£æ ¼ä¸°å¯Œè¡¨ç¤ºä½œä¸ºä¸­ä»‹ï¼Œç¬¬ä¸€é˜¶æ®µä½¿ç”¨è‡ªå›å½’è½¬æ¢å™¨ä»æ–‡æœ¬å’Œæ§åˆ¶ä¿¡å·ç”Ÿæˆè¿™äº›é£æ ¼ä¸°å¯Œçš„ä»¤ç‰Œï¼Œç¬¬äºŒé˜¶æ®µä»æ–‡æœ¬å’Œé‡‡æ ·çš„é£æ ¼ä¸°å¯Œçš„ä»¤ç‰Œç”Ÿæˆç¼–è§£ç å™¨ä»¤ç‰Œã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µæ¨¡å‹ä¸Šè¿›è¡Œå¤§é‡è®­ç»ƒæ•°æ®è®­ç»ƒå¯ä»¥æé«˜ä¸¤é˜¶æ®µæ¨¡å‹çš„ç¨³å¥æ€§å’Œå¯¹å¤šä¸ªå±æ€§çš„æ§åˆ¶èƒ½åŠ›ã€‚é€šè¿‡æœ‰é€‰æ‹©åœ°ç»“åˆç¦»æ•£æ ‡ç­¾å’Œè¯´è¯è€…åµŒå…¥ï¼Œæ¢ç´¢å¯¹è¯´è¯è€…çš„éŸ³è‰²å’Œå…¶ä»–é£æ ¼ä¿¡æ¯çš„å®Œå…¨æ§åˆ¶ï¼Œä»¥åŠè°ƒæ•´ç‰¹å®šè¯´è¯è€…çš„æƒ…æ„Ÿç­‰å±æ€§ã€‚æä¾›äº†éŸ³é¢‘æ ·æœ¬é“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TTSæ¨¡å‹ä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºç¼ºä¹ç²¾ç»†æ§åˆ¶å’Œé«˜è´¨é‡æ•°æ®çš„é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§ä¸¤é˜¶æ®µé£æ ¼å¯æ§çš„TTSç³»ç»Ÿï¼Œä½¿ç”¨è¯­è¨€æ¨¡å‹å’Œé‡åŒ–æ©ç è‡ªç¼–ç é£æ ¼ä¸°å¯Œè¡¨ç¤ºã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µç”Ÿæˆé£æ ¼ä¸°å¯Œçš„ä»¤ç‰Œï¼Œç¬¬äºŒé˜¶æ®µä»æ–‡æœ¬å’Œé‡‡æ ·çš„é£æ ¼ä»¤ç‰Œç”Ÿæˆç¼–è§£ç å™¨ä»¤ç‰Œã€‚</li>
<li>è®­ç»ƒç¬¬ä¸€é˜¶æ®µæ¨¡å‹ä½¿ç”¨å¤§é‡æ•°æ®é›†å¯æé«˜ä¸¤é˜¶æ®µæ¨¡å‹çš„ç¨³å¥æ€§å’Œæ§åˆ¶èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç»“åˆç¦»æ•£æ ‡ç­¾å’Œè¯´è¯è€…åµŒå…¥ï¼Œå®ç°å®Œå…¨æ§åˆ¶è¯´è¯è€…çš„éŸ³è‰²å’Œå…¶ä»–é£æ ¼ä¿¡æ¯ï¼ŒåŒ…æ‹¬è°ƒæ•´ç‰¹å®šè¯´è¯è€…çš„æƒ…æ„Ÿç­‰å±æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d2aff02a88d8baaf94543ab9801b48b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05e0c2fc1fe1ecc81f22126d3e41a85a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-437d8f18ee64c289abc2cd4e65e463ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bc395c6e2dadb5b608cca4d27d6f393.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb7a43773cec2fac3023f22ee8a64a99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45cc26b0738da18006216f630b03a50f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Towards-a-Japanese-Full-duplex-Spoken-Dialogue-System"><a href="#Towards-a-Japanese-Full-duplex-Spoken-Dialogue-System" class="headerlink" title="Towards a Japanese Full-duplex Spoken Dialogue System"></a>Towards a Japanese Full-duplex Spoken Dialogue System</h2><p><strong>Authors:Atsumoto Ohashi, Shinya Iizuka, Jingjing Jiang, Ryuichiro Higashinaka</strong></p>
<p>Full-duplex spoken dialogue systems, which can model simultaneous bidirectional features of human conversations such as speech overlaps and backchannels, have attracted significant attention recently. However, the study of full-duplex spoken dialogue systems for the Japanese language has been limited, and the research on their development in Japanese remains scarce. In this paper, we present the first publicly available full-duplex spoken dialogue model in Japanese, which is built upon Moshi, a full-duplex dialogue model in English. Our model is trained through a two-stage process: pre-training on a large-scale spoken dialogue data in Japanese, followed by fine-tuning on high-quality stereo spoken dialogue data. We further enhance the modelâ€™s performance by incorporating synthetic dialogue data generated by a multi-stream text-to-speech system. Evaluation experiments demonstrate that the trained model outperforms Japanese baseline models in both naturalness and meaningfulness. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»å¯¹è¯çš„åŒå‘åŒæ—¶è¿›è¡Œç‰¹æ€§ï¼ˆå¦‚è¨€è¯­é‡å å’Œåé¦ˆé€šé“ï¼‰çš„å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿå¼•èµ·äº†äººä»¬çš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ—¥è¯­çš„å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿçš„ç ”ç©¶è¾ƒä¸ºæœ‰é™ï¼Œå…¶å¼€å‘ç ”ç©¶ä¾ç„¶ç¨€å°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸºäºæ—¥è¯­çš„ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„å…¨åŒå·¥å¯¹è¯æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å»ºç«‹åœ¨è‹±è¯­çš„å…¨åŒå·¥å¯¹è¯æ¨¡å‹Moshiä¹‹ä¸Šã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼šé¦–å…ˆåœ¨å¤§è§„æ¨¡çš„æ—¥è¯­å£è¯­å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååˆ©ç”¨é«˜è´¨é‡çš„ç«‹ä½“å£°å£è¯­å¯¹è¯æ•°æ®è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡ç»“åˆç”±å¤šæµæ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿç”Ÿæˆçš„ç»¼åˆå¯¹è¯æ•°æ®æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚è¯„ä¼°å®éªŒè¡¨æ˜ï¼Œè®­ç»ƒåçš„æ¨¡å‹åœ¨æ—¥è¯­åŸºçº¿æ¨¡å‹çš„è‡ªç„¶åº¦å’Œæ„ä¹‰æ€§æ–¹é¢éƒ½è¡¨ç°å¾—æ›´å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02979v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡ä»‹ç»äº†é¦–ä¸ªå…¬å¼€çš„æ—¥è¯­å…¨åŒå·¥å¯¹è¯æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºè‹±è¯­çš„åŒå·¥å¯¹è¯æ¨¡å‹Moshiæ„å»ºã€‚é€šè¿‡å¤§è§„æ¨¡æ—¥è¯­å¯¹è¯æ•°æ®é¢„è®­ç»ƒå’Œé«˜è´¨é‡ç«‹ä½“å£°å¯¹è¯æ•°æ®å¾®è°ƒçš„ä¸¤é˜¶æ®µè¿‡ç¨‹è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨å¤šæµæ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿç”Ÿæˆçš„åˆæˆå¯¹è¯æ•°æ®å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚è¯„ä¼°å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ—¥è¯­åŸºå‡†æ¨¡å‹ä¸Šè¡¨ç°æ›´å‡ºè‰²ï¼Œæ›´è‡ªç„¶ã€æœ‰æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡å‘å¸ƒäº†é¦–ä¸ªæ—¥è¯­å…¨åŒå·¥å¯¹è¯æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹å»ºç«‹åœ¨è‹±è¯­çš„åŒå·¥å¯¹è¯æ¨¡å‹Moshiçš„åŸºç¡€ä¸Šã€‚</li>
<li>æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹è¿›è¡Œè®­ç»ƒï¼šé¦–å…ˆåœ¨å¤§è§„æ¨¡æ—¥è¯­å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨é«˜è´¨é‡ç«‹ä½“å£°å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚</li>
<li>åˆæˆå¯¹è¯æ•°æ®é€šè¿‡å¤šæµæ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿç”Ÿæˆï¼Œå¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨æ—¥è¯­åŸºå‡†æ¨¡å‹ä¸Šçš„è¡¨ç°æ›´è‡ªç„¶å’Œæœ‰æ„ä¹‰ã€‚</li>
<li>å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»å¯¹è¯çš„åŒå‘ç‰¹å¾ï¼Œå¦‚è¯­éŸ³é‡å å’Œåé¦ˆé€šé“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b7400a8cc0d30f4bfd02ffe1a4326777.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76749f6f0d5726186e99f2cc572edd14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b56fe64df06e5e9b544a1ae35ffc2db7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-847c4c71e81a6d97a225e83bfc956bb6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CapSpeech-Enabling-Downstream-Applications-in-Style-Captioned-Text-to-Speech"><a href="#CapSpeech-Enabling-Downstream-Applications-in-Style-Captioned-Text-to-Speech" class="headerlink" title="CapSpeech: Enabling Downstream Applications in Style-Captioned   Text-to-Speech"></a>CapSpeech: Enabling Downstream Applications in Style-Captioned   Text-to-Speech</h2><p><strong>Authors:Helin Wang, Jiarui Hai, Dading Chong, Karan Thakkar, Tiantian Feng, Dongchao Yang, Junhyeok Lee, Laureano Moro Velazquez, Jesus Villalba, Zengyi Qin, Shrikanth Narayanan, Mounya Elhiali, Najim Dehak</strong></p>
<p>Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æœ€æ–°è¿›å±•ä¸ºå¸¦é£æ ¼æ ‡æ³¨çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆCapTTSï¼‰é¢†åŸŸå¸¦æ¥äº†æ˜¾è‘—å˜é©ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘æ ‡å‡†åŒ–ã€å…¨é¢çš„æ•°æ®é›†ä»¥åŠåŸºäºCapTTSçš„ä¸‹æ¸¸ä»»åŠ¡ç ”ç©¶æœ‰é™ï¼Œå°†CapTTSé€‚åº”äºç°å®åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†CapSpeechï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºä¸€ç³»åˆ—ä¸CapTTSç›¸å…³çš„ä»»åŠ¡è®¾è®¡çš„æ–°åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¸¦æœ‰å£°éŸ³äº‹ä»¶çš„é£æ ¼æ ‡æ³¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆCapTTS-SEï¼‰ã€å£éŸ³æ ‡æ³¨çš„TTSï¼ˆAccCapTTSï¼‰ã€æƒ…æ„Ÿæ ‡æ³¨çš„TTSï¼ˆEmoCapTTSï¼‰ä»¥åŠç”¨äºèŠå¤©ä»£ç†çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆAgentTTSï¼‰ã€‚CapSpeechåŒ…å«è¶…è¿‡1000ä¸‡ä¸ªæœºå™¨æ ‡æ³¨çš„éŸ³é¢‘å­—å¹•å¯¹å’Œè¿‘36ä¸‡ä¸ªäººå·¥æ ‡æ³¨çš„éŸ³é¢‘å­—å¹•å¯¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç”±ä¸“ä¸šé…éŸ³æ¼”å‘˜å’Œç»éªŒä¸°å¯Œçš„éŸ³é¢‘å·¥ç¨‹å¸ˆæ”¶é›†å’Œå½•åˆ¶äº†ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºAgentTTSå’ŒCapTTS-SEä»»åŠ¡ã€‚é™¤äº†æ•°æ®é›†ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨è‡ªå›å½’å’Œéè‡ªå›å½’æ¨¡å‹åœ¨CapSpeechä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†åœ¨å„ç§ä¸åŒçš„è¯´è¯é£æ ¼ä¸­ï¼Œå…·æœ‰é«˜ä¿çœŸåº¦å’Œé«˜åº¦å¯ç†è§£æ€§çš„è¯­éŸ³åˆæˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒCapSpeechæ˜¯ç°æœ‰æœ€å¤§çš„ä¸ºCapTTSç›¸å…³ä»»åŠ¡æä¾›å…¨é¢æ³¨é‡Šçš„æ•°æ®é›†ã€‚å®éªŒå’Œå‘ç°ä¸ºå¼€å‘CapTTSç³»ç»Ÿæä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02863v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¿›æ­¥æ˜¾è‘—æ”¹å˜äº†é£æ ¼æ ‡æ³¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆCapTTSï¼‰é¢†åŸŸã€‚ç„¶è€Œï¼Œå°†CapTTSé€‚åº”åˆ°å®é™…åº”ç”¨ä¸­ä»é¢ä¸´ç¼ºä¹æ ‡å‡†åŒ–ã€å…¨é¢çš„æ•°æ®é›†ä»¥åŠåœ¨CapTTSåŸºç¡€ä¸Šæ„å»ºçš„ä¸‹æ¸¸ä»»åŠ¡ç ”ç©¶çš„é™åˆ¶ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†CapSpeechï¼Œä¸€ä¸ªä¸ºCapTTSç›¸å…³ä»»åŠ¡è®¾è®¡çš„æ–°åŸºå‡†ï¼ŒåŒ…æ‹¬å¸¦æœ‰å£°éŸ³äº‹ä»¶çš„é£æ ¼æ ‡æ³¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆCapTTS-SEï¼‰ã€å£éŸ³æ ‡æ³¨TTSï¼ˆAccCapTTSï¼‰ã€æƒ…æ„Ÿæ ‡æ³¨TTSï¼ˆEmoCapTTSï¼‰å’Œç”¨äºèŠå¤©ä»£ç†çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆAgentTTSï¼‰ã€‚CapSpeechåŒ…å«è¶…è¿‡1åƒä¸‡æœºå™¨æ ‡æ³¨çš„éŸ³é¢‘-å­—å¹•å¯¹å’Œè¿‘0.36ç™¾ä¸‡äººç±»æ ‡æ³¨çš„éŸ³é¢‘-å­—å¹•å¯¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç”±ä¸“ä¸šé…éŸ³æ¼”å‘˜å’Œç»éªŒä¸°å¯Œçš„éŸ³é¢‘å·¥ç¨‹å¸ˆæ”¶é›†å’Œè®°å½•äº†ä¸¤ä¸ªæ–°æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºAgentTTSå’ŒCapTTS-SEä»»åŠ¡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åœ¨CapSpeechä¸Šä½¿ç”¨äº†è‡ªå›å½’å’Œéè‡ªå›å½’æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚ç»“æœè¯æ˜äº†æˆ‘ä»¬åœ¨å¤šç§è¯´è¯é£æ ¼ä¸Šå®ç°é«˜ä¿çœŸã€é«˜åº¦å¯ç†è§£çš„è¯­éŸ³åˆæˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒCapSpeechæ˜¯æä¾›CapTTSç›¸å…³ä»»åŠ¡å…¨é¢æ³¨é‡Šçš„æœ€å¤§å¯ç”¨æ•°æ®é›†ã€‚å®éªŒå’Œå‘ç°ä¸ºè¿›ä¸€æ­¥å¼€å‘CapTTSç³»ç»Ÿæä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¿›æ­¥å·²æ˜¾è‘—æ”¹å˜é£æ ¼æ ‡æ³¨æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆCapTTSï¼‰é¢†åŸŸã€‚</li>
<li>CapSpeechåŸºå‡†å¼•å…¥ï¼ŒåŒ…å«å¤šç§CapTTSç›¸å…³ä»»åŠ¡ï¼Œå¦‚CapTTS-SEã€AccCapTTSã€EmoCapTTSå’ŒAgentTTSã€‚</li>
<li>CapSpeechåŒ…å«å¤§é‡æœºå™¨å’Œäººç±»æ ‡æ³¨çš„éŸ³é¢‘-å­—å¹•å¯¹æ•°æ®ã€‚</li>
<li>æ–°æ•°æ®é›†ç”±ä¸“ä¸šé…éŸ³æ¼”å‘˜å’ŒéŸ³é¢‘å·¥ç¨‹å¸ˆæ”¶é›†ï¼Œç”¨äºAgentTTSå’ŒCapTTS-SEä»»åŠ¡ã€‚</li>
<li>å…¨é¢çš„å®éªŒè¯æ˜äº†é«˜ä¿çœŸã€é«˜åº¦å¯ç†è§£çš„è¯­éŸ³åˆæˆèƒ½åŠ›ã€‚</li>
<li>CapSpeechæ˜¯æä¾›CapTTSç›¸å…³ä»»åŠ¡å…¨é¢æ³¨é‡Šçš„æœ€å¤§å¯ç”¨æ•°æ®é›†ã€‚</li>
<li>å®éªŒç»“æœæä¾›äº†å¼€å‘CapTTSç³»ç»Ÿçš„å®è´µè§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02863">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d9dbeacf5de28b61175fc4a9123e25c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a12b50356ee9dfd1bb29660b1810cc54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36655116223f38a6bb5e321eb64a8550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db8915b61e2f47475d6db40132b65df1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Prompt-Unseen-Emotion-Zero-shot-Expressive-Speech-Synthesis-with-Prompt-LLM-Contextual-Knowledge-for-Mixed-Emotions"><a href="#Prompt-Unseen-Emotion-Zero-shot-Expressive-Speech-Synthesis-with-Prompt-LLM-Contextual-Knowledge-for-Mixed-Emotions" class="headerlink" title="Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with   Prompt-LLM Contextual Knowledge for Mixed Emotions"></a>Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with   Prompt-LLM Contextual Knowledge for Mixed Emotions</h2><p><strong>Authors:Xiaoxue Gao, Huayun Zhang, Nancy F. Chen</strong></p>
<p>Existing expressive text-to-speech (TTS) systems primarily model a limited set of categorical emotions, whereas human conversations extend far beyond these predefined emotions, making it essential to explore more diverse emotional speech generation for more natural interactions. To bridge this gap, this paper proposes a novel prompt-unseen-emotion (PUE) approach to generate unseen emotional speech via emotion-guided prompt learning. PUE is trained utilizing an LLM-TTS architecture to ensure emotional consistency between categorical emotion-relevant prompts and emotional speech, allowing the model to quantitatively capture different emotion weightings per utterance. During inference, mixed emotional speech can be generated by flexibly adjusting emotion proportions and leveraging LLM contextual knowledge, enabling the model to quantify different emotional styles. Our proposed PUE successfully facilitates expressive speech synthesis of unseen emotions in a zero-shot setting. </p>
<blockquote>
<p>ç°æœ‰çš„è¡¨è¾¾æ€§æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸»è¦å¯¹æœ‰é™çš„åˆ†ç±»æƒ…ç»ªè¿›è¡Œå»ºæ¨¡ï¼Œè€Œäººç±»å¯¹è¯åˆ™è¿œè¿œè¶…å‡ºè¿™äº›é¢„å®šä¹‰çš„æƒ…ç»ªã€‚å› æ­¤ï¼Œä¸ºäº†æ›´è‡ªç„¶çš„äº¤äº’ï¼Œæ¢ç´¢æ›´å¤šæ ·åŒ–çš„æƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆè‡³å…³é‡è¦ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æç¤ºæœªè§æƒ…ç»ªï¼ˆPUEï¼‰æ–¹æ³•ï¼Œé€šè¿‡æƒ…æ„Ÿå¼•å¯¼æç¤ºå­¦ä¹ æ¥ç”Ÿæˆæœªè§æƒ…æ„Ÿè¯­éŸ³ã€‚PUEåˆ©ç”¨LLM-TTSæ¶æ„è¿›è¡Œè®­ç»ƒï¼Œä»¥ç¡®ä¿åˆ†ç±»æƒ…ç»ªç›¸å…³æç¤ºä¸æƒ…æ„Ÿè¯­éŸ³ä¹‹é—´çš„æƒ…æ„Ÿä¸€è‡´æ€§ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥å®šé‡æ•è·æ¯ä¸ªè¯è¯­çš„ä¸åŒæƒ…ç»ªæƒé‡ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥é€šè¿‡çµæ´»åœ°è°ƒæ•´æƒ…ç»ªæ¯”ä¾‹å¹¶åˆ©ç”¨LLMçš„ä¸Šä¸‹æ–‡çŸ¥è¯†æ¥ç”Ÿæˆæ··åˆæƒ…æ„Ÿè¯­éŸ³ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé‡åŒ–ä¸åŒçš„æƒ…æ„Ÿé£æ ¼ã€‚æˆ‘ä»¬æå‡ºçš„PUEæˆåŠŸåœ°åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ä¿ƒè¿›äº†æœªè§æƒ…æ„Ÿçš„è¡¨è¾¾æ€§è¯­éŸ³åˆæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02742v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPUEï¼ˆPrompt Unseen Emotionï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æƒ…æ„Ÿå¼•å¯¼çš„æç¤ºå­¦ä¹ ç”Ÿæˆæœªè§æƒ…æ„Ÿè¯­éŸ³ã€‚è¯¥æ¨¡å‹åŸºäºLLM-TTSæ¶æ„è®­ç»ƒï¼Œä¿è¯äº†ä¸åˆ†ç±»æƒ…æ„Ÿç›¸å…³çš„æç¤ºå’Œæƒ…ç»ªè¯­éŸ³ä¹‹é—´çš„æƒ…æ„Ÿä¸€è‡´æ€§ï¼Œå¯ä»¥é‡åŒ–æ•æ‰ä¸åŒæƒ…æ„Ÿæƒé‡ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡çµæ´»è°ƒæ•´æƒ…æ„Ÿæ¯”ä¾‹å¹¶åˆ©ç”¨LLMçš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œå¯ä»¥ç”Ÿæˆæ··åˆæƒ…æ„Ÿè¯­éŸ³ï¼Œå®ç°ä¸åŒæƒ…æ„Ÿé£æ ¼çš„é‡åŒ–ã€‚è¯¥ç ”ç©¶æˆåŠŸä¿ƒè¿›äº†é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æœªè§æƒ…æ„Ÿè¯­éŸ³çš„åˆæˆè¡¨è¾¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PUEæ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæœªè§æƒ…æ„Ÿè¯­éŸ³ï¼Œå¼¥è¡¥äº†ç°æœ‰TTSç³»ç»Ÿåœ¨æƒ…æ„Ÿè¡¨è¾¾ä¸Šçš„ä¸è¶³ã€‚</li>
<li>LLM-TTSæ¶æ„è¢«ç”¨äºç¡®ä¿æƒ…æ„Ÿä¸€è‡´æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰ä¸åŒæƒ…æ„Ÿçš„æƒé‡ã€‚</li>
<li>åœ¨æ¨ç†é˜¶æ®µï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´æƒ…æ„Ÿæ¯”ä¾‹ç”Ÿæˆæ··åˆæƒ…æ„Ÿè¯­éŸ³ã€‚</li>
<li>LLMçš„ä¸Šä¸‹æ–‡çŸ¥è¯†è¢«ç”¨æ¥æé«˜æƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆçš„ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œè‡ªç„¶åº¦ã€‚</li>
<li>PUEæ–¹æ³•å®ç°äº†é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æƒ…æ„Ÿè¯­éŸ³åˆæˆã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…èƒ½å¤„ç†é¢„è®¾çš„ç±»åˆ«æƒ…æ„Ÿï¼Œè¿˜èƒ½å¤„ç†æ›´å¹¿æ³›çš„æƒ…æ„Ÿè¡¨è¾¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02742">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b2372d7386348a0c29b286f118fe810.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77a4d2f8d896ba6e0c0ebeb57ccbd2cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be64077dbb482c82f476bdd21fb1ad4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-130f7a88b04c6438452be2e1b389c2b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6fc8014108820701114da374a1c5c1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7eb7205557b62949a21afb59bc07309.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99d3925fc6252fa9db33c2b532d931e3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SALF-MOS-Speaker-Agnostic-Latent-Features-Downsampled-for-MOS-Prediction"><a href="#SALF-MOS-Speaker-Agnostic-Latent-Features-Downsampled-for-MOS-Prediction" class="headerlink" title="SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS   Prediction"></a>SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS   Prediction</h2><p><strong>Authors:Saurabh Agrawal, Raj Gohil, Gopal Kumar Agrawal, Vikram C M, Kushal Verma</strong></p>
<p>Speech quality assessment is a critical process in selecting text-to-speech synthesis (TTS) or voice conversion models. Evaluation of voice synthesis can be done using objective metrics or subjective metrics. Although there are many objective metrics like the Perceptual Evaluation of Speech Quality (PESQ), Perceptual Objective Listening Quality Assessment (POLQA) or Short-Time Objective Intelligibility (STOI) but none of them is feasible in selecting the best model. On the other hand subjective metric like Mean Opinion Score is highly reliable but it requires a lot of manual efforts and are time-consuming. To counter the issues in MOS Evaluation, we have developed a novel model, Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a small-sized, end-to-end, highly generalized and scalable model for predicting MOS score on a scale of 5. We use the sequences of convolutions and stack them to get the latent features of the audio samples to get the best state-of-the-art results based on mean squared error (MSE), Linear Concordance Correlation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and Kendall Rank Correlation Coefficient (KTAU). </p>
<blockquote>
<p>è¯­éŸ³è´¨é‡è¯„ä¼°æ˜¯é€‰æ‹©æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆTTSï¼‰æˆ–è¯­éŸ³è½¬æ¢æ¨¡å‹çš„å…³é”®è¿‡ç¨‹ã€‚è¯­éŸ³åˆæˆçš„è¯„ä¼°å¯ä»¥ä½¿ç”¨å®¢è§‚æŒ‡æ ‡æˆ–ä¸»è§‚æŒ‡æ ‡æ¥å®Œæˆã€‚è™½ç„¶æœ‰è®¸å¤šå®¢è§‚æŒ‡æ ‡ï¼Œå¦‚è¯­éŸ³è´¨é‡çš„æ„ŸçŸ¥è¯„ä¼°ï¼ˆPESQï¼‰ã€æ„ŸçŸ¥å®¢è§‚å¬è§‰è´¨é‡è¯„ä¼°ï¼ˆPOLQAï¼‰æˆ–çŸ­æœŸå®¢è§‚å¯æ‡‚åº¦ï¼ˆSTOIï¼‰ï¼Œä½†æ²¡æœ‰ä»»ä½•ä¸€ä¸ªæŒ‡æ ‡å¯ç”¨äºé€‰æ‹©æœ€ä½³æ¨¡å‹ã€‚å¦ä¸€æ–¹é¢ï¼Œä¸»è§‚æŒ‡æ ‡å¦‚å¹³å‡æ„è§å¾—åˆ†éå¸¸å¯é ï¼Œä½†å®ƒéœ€è¦å¤§é‡çš„æ‰‹åŠ¨æ“ä½œå’Œè€—æ—¶ã€‚ä¸ºäº†è§£å†³MOSè¯„ä¼°ä¸­çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹æ¨¡å‹ï¼Œå³ä¸è®²è€…æ½œåœ¨ç‰¹å¾ï¼ˆSALFï¼‰-å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°å‹ã€ç«¯åˆ°ç«¯ã€é«˜åº¦é€šç”¨å’Œå¯æ‰©å±•çš„æ¨¡å‹ï¼Œå¯ç”¨äºé¢„æµ‹5åˆ†åˆ¶ä¸Šçš„MOSåˆ†æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨å·ç§¯åºåˆ—å¹¶å°†å…¶å †å èµ·æ¥ï¼Œä»¥è·å¾—éŸ³é¢‘æ ·æœ¬çš„æ½œåœ¨ç‰¹å¾ï¼Œä»¥åŸºäºå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€çº¿æ€§ä¸€è‡´æ€§ç›¸å…³ç³»æ•°ï¼ˆLCCï¼‰ã€æ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°ï¼ˆSRCCï¼‰å’Œè‚¯å¾·å°”ç­‰çº§ç›¸å…³ç³»æ•°ï¼ˆKTAUï¼‰è·å¾—æœ€ä½³çš„æœ€å…ˆè¿›ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02082v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¯­éŸ³è´¨é‡è¯„ä¼°åœ¨æ–‡æœ¬è½¬è¯­éŸ³åˆæˆæ¨¡å‹é€‰æ‹©ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æ¢è®¨äº†å®¢è§‚å’Œä¸»è§‚è¯„ä¼°æ–¹æ³•çš„é—®é¢˜ã€‚é’ˆå¯¹ä¸»è§‚è¯„ä¼°æ–¹æ³•å¦‚å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰çš„ç¼ºé™·ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è¯­éŸ³è´¨é‡è¯„ä¼°æ¨¡å‹â€”â€”è¯´è¯äººæ— å…³æ½œåœ¨ç‰¹å¾ï¼ˆSALFï¼‰-MOSæ¨¡å‹ã€‚è¯¥æ¨¡å‹å…·æœ‰å°å°ºå¯¸ã€ç«¯åˆ°ç«¯ã€é«˜åº¦é€šç”¨å’Œå¯æ‰©å±•çš„ç‰¹ç‚¹ï¼Œå¯ä»¥é€šè¿‡éŸ³é¢‘æ ·æœ¬çš„æ½œåœ¨ç‰¹å¾é¢„æµ‹MOSåˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³è´¨é‡è¯„ä¼°åœ¨æ–‡æœ¬è½¬è¯­éŸ³åˆæˆæ¨¡å‹é€‰æ‹©ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰çš„å®¢è§‚è¯­éŸ³è´¨é‡è¯„ä¼°æŒ‡æ ‡å¦‚PESQã€POLQAå’ŒSTOIç­‰å¹¶ä¸å®Œç¾ï¼Œæ— æ³•é€‰æ‹©æœ€ä½³æ¨¡å‹ã€‚</li>
<li>ä¸»è§‚è¯„ä¼°æ–¹æ³•å¦‚å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰è™½ç„¶å¯é ï¼Œä½†è€—æ—¶ä¸”éœ€è¦å¤§é‡äººå·¥å‚ä¸ã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„è¯­éŸ³è´¨é‡è¯„ä¼°æ¨¡å‹â€”â€”SALF-MOSã€‚</li>
<li>SALF-MOSæ¨¡å‹å…·æœ‰å°å°ºå¯¸ã€ç«¯åˆ°ç«¯ã€é«˜åº¦é€šç”¨å’Œå¯æ‰©å±•çš„ç‰¹ç‚¹ã€‚</li>
<li>SALF-MOSæ¨¡å‹é€šè¿‡éŸ³é¢‘æ ·æœ¬çš„æ½œåœ¨ç‰¹å¾é¢„æµ‹MOSåˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f395630221baeccb235ff6feeec0698b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9dd1c52639d14e45b75f34d1af35d474.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8deb09517f226a50073ba7fe12a8d4d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9aa0799f4ef933a624f44a8e2894a54c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23c3b9cd2c37b7cf97b63b2373d56b0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c19249b3a15794a765a04cd03d9a6f8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Speech-to-Speech-Translation-Pipelines-for-Conversations-in-Low-Resource-Languages"><a href="#Speech-to-Speech-Translation-Pipelines-for-Conversations-in-Low-Resource-Languages" class="headerlink" title="Speech-to-Speech Translation Pipelines for Conversations in Low-Resource   Languages"></a>Speech-to-Speech Translation Pipelines for Conversations in Low-Resource   Languages</h2><p><strong>Authors:Andrei Popescu-Belis, Alexis Allemann, Teo Ferrari, Gopal Krishnamani</strong></p>
<p>The popularity of automatic speech-to-speech translation for human conversations is growing, but the quality varies significantly depending on the language pair. In a context of community interpreting for low-resource languages, namely Turkish and Pashto to&#x2F;from French, we collected fine-tuning and testing data, and compared systems using several automatic metrics (BLEU, COMET, and BLASER) and human assessments. The pipelines included automatic speech recognition, machine translation, and speech synthesis, with local models and cloud-based commercial ones. Some components have been fine-tuned on our data. We evaluated over 60 pipelines and determined the best one for each direction. We also found that the ranks of components are generally independent of the rest of the pipeline. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³å¯¹è¯ç¿»è¯‘æŠ€æœ¯çš„æ™®åŠç¨‹åº¦æ­£ä¸æ–­å¢é•¿ï¼Œä½†å…¶è´¨é‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè¯­è¨€å¯¹ã€‚æˆ‘ä»¬åœ¨é’ˆå¯¹ä½èµ„æºè¯­è¨€ï¼ˆå³åœŸè€³å…¶è¯­å’Œæ™®ä»€å›¾è¯­è‡³æ³•è¯­&#x2F;ä»æ³•è¯­ç¿»è¯‘ï¼‰çš„ç¤¾åŒºå£è¯‘èƒŒæ™¯ä¸‹ï¼Œæ”¶é›†äº†è°ƒä¼˜å’Œæµ‹è¯•æ•°æ®ï¼Œå¹¶ä½¿ç”¨å‡ ç§è‡ªåŠ¨æŒ‡æ ‡ï¼ˆBLEUã€COMETå’ŒBLASERï¼‰ä»¥åŠäººå·¥è¯„ä¼°å¯¹æ¯”äº†ä¸åŒç³»ç»Ÿã€‚è¿™äº›ç®¡é“åŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€æœºå™¨ç¿»è¯‘å’Œè¯­éŸ³åˆæˆï¼Œå…¶ä¸­åŒ…æ‹¬æœ¬åœ°æ¨¡å‹å’ŒåŸºäºäº‘çš„å•†ä¸šæ¨¡å‹ã€‚æŸäº›ç»„ä»¶å·²é’ˆå¯¹æˆ‘ä»¬çš„æ•°æ®è¿›è¡Œè¿‡å¾®è°ƒã€‚æˆ‘ä»¬è¯„ä¼°äº†è¶…è¿‡60ä¸ªç®¡é“ï¼Œå¹¶ç¡®å®šäº†æ¯ä¸ªæ–¹å‘çš„æœ€ä½³ç®¡é“ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œç»„ä»¶çš„æ’åé€šå¸¸ä¸ç®¡é“çš„å…¶ä»–éƒ¨åˆ†æ— å…³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01406v1">PDF</a> Proceedings of MT Summit 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªåŠ¨è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘åœ¨äººç±»å¯¹è¯ä¸­çš„æ™®åŠæƒ…å†µï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€å¦‚åœŸè€³å…¶è¯­å’Œæ™®ä»€å›¾è¯­ä¸æ³•è¯­ä¹‹é—´çš„ç¿»è¯‘ã€‚æ–‡ç« é€šè¿‡æ”¶é›†è°ƒä¼˜å’Œæµ‹è¯•æ•°æ®ï¼Œå¯¹æ¯”äº†ä¸åŒç³»ç»Ÿçš„è¡¨ç°ï¼Œå¹¶å‘ç°ä¸åŒè¯­è¨€å¯¹çš„ç¿»è¯‘è´¨é‡å·®å¼‚æ˜¾è‘—ã€‚ç ”ç©¶åŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€æœºå™¨ç¿»è¯‘å’Œè¯­éŸ³åˆæˆç­‰ç¯èŠ‚ï¼Œæ¶‰åŠæœ¬åœ°æ¨¡å‹å’ŒåŸºäºäº‘çš„å•†ä¸šæ¨¡å‹ã€‚é€šè¿‡è¯„ä¼°è¶…è¿‡60ä¸ªç®¡é“ï¼Œç¡®å®šäº†æ¯ä¸ªæ–¹å‘çš„æœ€ä½³æ–¹æ¡ˆï¼Œå¹¶å‘ç°ç»„ä»¶æ’åé€šå¸¸ä¸ç®¡é“å…¶ä½™éƒ¨åˆ†æ— å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘åœ¨ä½èµ„æºè¯­è¨€å¦‚åœŸè€³å…¶è¯­å’Œæ™®ä»€å›¾è¯­ä¸æ³•è¯­ä¹‹é—´çš„åº”ç”¨æ­£åœ¨å¢é•¿ã€‚</li>
<li>ä¸åŒè¯­è¨€å¯¹çš„ç¿»è¯‘è´¨é‡å·®å¼‚æ˜¾è‘—ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€æœºå™¨ç¿»è¯‘å’Œè¯­éŸ³åˆæˆç­‰ç¯èŠ‚ã€‚</li>
<li>æœ¬åœ°æ¨¡å‹å’ŒåŸºäºäº‘çš„å•†ä¸šæ¨¡å‹å‡è¢«è€ƒè™‘ã€‚</li>
<li>è¶…è¿‡60ä¸ªç®¡é“è¢«è¯„ä¼°ï¼Œç¡®å®šäº†æ¯ä¸ªæ–¹å‘çš„æœ€ä½³æ–¹æ¡ˆã€‚</li>
<li>ç»„ä»¶çš„æ’åé€šå¸¸ä¸ç®¡é“çš„å…¶ä»–éƒ¨åˆ†æ— å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cccb22a9f02cf83c83fe8f812a44c504.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15ec55bc520f52b14cff76228ee542ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2510049e0fd64e2c44b8573e49f81e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f477bedef6a6dda8c58c7612bf1733d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2008a1bb64a7fbd156bf5bc76c6f94f2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CleanS2S-Single-file-Framework-for-Proactive-Speech-to-Speech-Interaction"><a href="#CleanS2S-Single-file-Framework-for-Proactive-Speech-to-Speech-Interaction" class="headerlink" title="CleanS2S: Single-file Framework for Proactive Speech-to-Speech   Interaction"></a>CleanS2S: Single-file Framework for Proactive Speech-to-Speech   Interaction</h2><p><strong>Authors:Yudong Lu, Yazhe Niu, Shuai Hu, Haolin Wang</strong></p>
<p>CleanS2S is a framework for human-like speech-to-speech interaction that advances conversational AI through single-file implementation and proactive dialogue capabilities. Our system integrates automatic speech recognition, large language models, and text-to-speech synthesis into a unified pipeline with real-time interruption handling, achieving low transition latency through full-duplex websocket connections and non-blocking I&#x2F;O. Beyond conventional chatbot paradigms, we pioneer a proactive interaction mechanism, which combines memory systems with Subjective Action Judgement module, enabling five human-like response strategies: interruption, refusal, deflection, silence, and standard response. The memory module dynamically aggregates historical, and contextual data to inform interaction decisions. This approach breaks the rigid turn-based convention by allowing system-initiated dialog control and context-aware response selection. And we propose Action Judgement SFT that assesses input streams for responses strategies. The frameworkâ€™s single-file implementation with atomic configurations offers researchers unprecedented transparency and extensibility for interaction agents. The code of CleanS2S is released at \<a target="_blank" rel="noopener" href="https://github.com/opendilab/CleanS2S">https://github.com/opendilab/CleanS2S</a>. </p>
<blockquote>
<p>CleanS2Sæ˜¯ä¸€ä¸ªé¢å‘äººç±»è¯­éŸ³å¯¹è¯äº¤äº’çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡å•æ–‡ä»¶å®ç°å’Œä¸»åŠ¨å¯¹è¯åŠŸèƒ½æ¨åŠ¨äº†ä¼šè¯å¼äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé›†æˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼Œé€šè¿‡å®æ—¶ä¸­æ–­å¤„ç†æ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€ç®¡é“ï¼Œé€šè¿‡å…¨åŒå·¥webå¥—æ¥å­—è¿æ¥å’Œéé˜»å¡I&#x2F;Oå®ç°äº†ä½è½¬æ¢å»¶è¿Ÿã€‚é™¤äº†ä¼ ç»Ÿçš„èŠå¤©æœºå™¨äººæ¨¡å¼ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€åˆ›äº†ä¸€ç§ä¸»åŠ¨äº¤äº’æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ç»“åˆäº†è®°å¿†ç³»ç»Ÿä¸ä¸»è§‚åŠ¨ä½œåˆ¤æ–­æ¨¡å—ï¼Œå¯å®ç°äº”ç§äººç±»åŒ–çš„å“åº”ç­–ç•¥ï¼šä¸­æ–­ã€æ‹’ç»ã€å›é¿ã€æ²‰é»˜å’Œæ ‡å‡†å“åº”ã€‚è®°å¿†æ¨¡å—åŠ¨æ€èšåˆå†å²å’Œä¸Šä¸‹æ–‡æ•°æ®ä»¥æ”¯æŒäº¤äº’å†³ç­–ã€‚è¿™ç§æ–¹æ³•æ‰“ç ´äº†åŸºäºè½®è¯¢çš„ä¸¥æ ¼è§„åˆ™ï¼Œå…è®¸ç³»ç»Ÿä¸»åŠ¨æ§åˆ¶å¯¹è¯å’ŒåŸºäºä¸Šä¸‹æ–‡çš„å“åº”é€‰æ‹©ã€‚æˆ‘ä»¬æå‡ºäº†åŠ¨ä½œåˆ¤æ–­SFTï¼ˆAction Judgement SFTï¼‰ï¼Œç”¨äºè¯„ä¼°è¾“å…¥æµä»¥é€‰æ‹©å“åº”ç­–ç•¥ã€‚CleanS2Sçš„å•æ–‡ä»¶å®ç°ä»¥åŠåŸå­é…ç½®ä¸ºç ”ç©¶äººå‘˜æä¾›äº†å‰æ‰€æœªæœ‰çš„äº¤äº’ä»£ç†é€æ˜åº¦å’Œæ‰©å±•æ€§ã€‚CleanS2Sçš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/opendilab/CleanS2S%E4%B8%8A%E3%80%82">https://github.com/opendilab/CleanS2Sä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01268v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æ¸…æ´S2Sæ˜¯ä¸€ä¸ªé¢å‘äººç±»è¯­éŸ³å¯¹è¯çš„æ¡†æ¶ï¼Œé€šè¿‡å•ä¸€æ–‡ä»¶å®ç°å’Œä¸»åŠ¨å¯¹è¯èƒ½åŠ›æ¨åŠ¨å¯¹è¯AIçš„å‘å±•ã€‚å®ƒé›†æˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæŠ€æœ¯ï¼Œå…·æœ‰å®æ—¶ä¸­æ–­å¤„ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å…¨åŒå·¥websocketè¿æ¥å’Œéé˜»å¡I&#x2F;Oå®ç°ä½è½¬æ¢å»¶è¿Ÿã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸»åŠ¨äº¤äº’æœºåˆ¶ï¼Œç»“åˆè®°å¿†ç³»ç»Ÿä¸ä¸»è§‚åŠ¨ä½œåˆ¤æ–­æ¨¡å—ï¼Œå®ç°äº”ç§ç±»ä¼¼äººç±»çš„å“åº”ç­–ç•¥ï¼šä¸­æ–­ã€æ‹’ç»ã€åç¦»ã€æ²‰é»˜å’Œæ ‡å‡†å“åº”ã€‚è®°å¿†æ¨¡å—åŠ¨æ€èšåˆå†å²å’Œä¸Šä¸‹æ–‡æ•°æ®ä»¥æ”¯æŒäº¤äº’å†³ç­–ã€‚å®ƒæ‰“ç ´äº†ä¼ ç»Ÿçš„åŸºäºè½®æµçš„æ¨¡å¼ï¼Œå…è®¸ç³»ç»Ÿå¯åŠ¨å¯¹è¯æ§åˆ¶å’ŒåŸºäºä¸Šä¸‹æ–‡çš„å“åº”é€‰æ‹©ã€‚CleanS2Sæ¡†æ¶ä¸ºç ”ç©¶äººå‘˜æä¾›äº†å‰æ‰€æœªæœ‰çš„äº¤äº’ä»£ç†çš„é€æ˜åº¦å’Œæ‰©å±•æ€§ã€‚CleanS2Sçš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/opendilab/CleanS2S%E3%80%82">https://github.com/opendilab/CleanS2Sã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>CleanS2Sæ˜¯ä¸€ä¸ªé¢å‘äººç±»è¯­éŸ³å¯¹è¯çš„æ¡†æ¶ï¼Œé›†æˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæŠ€æœ¯ã€‚</li>
<li>å®ƒå®ç°äº†å®æ—¶ä¸­æ–­å¤„ç†å’Œä½è½¬æ¢å»¶è¿Ÿï¼Œé€šè¿‡å…¨åŒå·¥websocketè¿æ¥å’Œéé˜»å¡I&#x2F;Oæ”¯æŒé«˜æ•ˆå¯¹è¯ã€‚</li>
<li>CleanS2Sé‡‡ç”¨ä¸»åŠ¨äº¤äº’æœºåˆ¶ï¼Œå…è®¸ç³»ç»Ÿå¯åŠ¨å¯¹è¯æ§åˆ¶å’ŒåŸºäºä¸Šä¸‹æ–‡çš„å“åº”é€‰æ‹©ï¼Œå®ç°äº”ç§ç±»ä¼¼äººç±»çš„å“åº”ç­–ç•¥ã€‚</li>
<li>è®°å¿†æ¨¡å—èƒ½å¤ŸåŠ¨æ€èšåˆå†å²å’Œä¸Šä¸‹æ–‡æ•°æ®ï¼Œä»¥æ”¯æŒæ›´æ™ºèƒ½çš„äº¤äº’å†³ç­–ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å•ä¸€æ–‡ä»¶å®ç°å’ŒåŸå­é…ç½®æä¾›å‰æ‰€æœªæœ‰çš„é€æ˜åº¦å’Œæ‰©å±•æ€§ï¼Œä¾¿äºç ”ç©¶äººå‘˜ç†è§£å’Œæ”¹è¿›äº¤äº’ä»£ç†ã€‚</li>
<li>CleanS2Sä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šï¼Œä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01268">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f959bcfcf98c2e58e088a4f316e01c27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-410197554b0d7e952590a83b9e5ceb4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-377451e45911d33fe1c382fd9e712f41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ec648ab26488346a9b7f60736cd42e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11fa68f1f299f2e2d112c6261f3eb983.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7c3f5759330522b32b9add3dfd55177.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DS-TTS-Zero-Shot-Speaker-Style-Adaptation-from-Voice-Clips-via-Dynamic-Dual-Style-Feature-Modulation"><a href="#DS-TTS-Zero-Shot-Speaker-Style-Adaptation-from-Voice-Clips-via-Dynamic-Dual-Style-Feature-Modulation" class="headerlink" title="DS-TTS: Zero-Shot Speaker Style Adaptation from Voice Clips via Dynamic   Dual-Style Feature Modulation"></a>DS-TTS: Zero-Shot Speaker Style Adaptation from Voice Clips via Dynamic   Dual-Style Feature Modulation</h2><p><strong>Authors:Ming Meng, Ziyi Yang, Jian Yang, Zhenjie Su, Yonggui Zhu, Zhaoxin Fan</strong></p>
<p>Recent advancements in text-to-speech (TTS) technology have increased demand for personalized audio synthesis. Zero-shot voice cloning, a specialized TTS task, aims to synthesize a target speakerâ€™s voice using only a single audio sample and arbitrary text, without prior exposure to the speaker during training. This process employs pattern recognition techniques to analyze and replicate the speakerâ€™s unique vocal features. Despite progress, challenges remain in adapting to the vocal style of unseen speakers, highlighting difficulties in generalizing TTS systems to handle diverse voices while maintaining naturalness, expressiveness, and speaker fidelity. To address the challenges of unseen speaker style adaptation, we propose DS-TTS, a novel approach aimed at enhancing the synthesis of diverse, previously unheard voices. Central to our method is a Dual-Style Encoding Network (DuSEN), where two distinct style encoders capture complementary aspects of a speakerâ€™s vocal identity. These speaker-specific style vectors are seamlessly integrated into the Dynamic Generator Network (DyGN) via a Style Gating-Film (SGF) mechanism, enabling more accurate and expressive reproduction of unseen speakersâ€™ unique vocal characteristics. In addition, we introduce a Dynamic Generator Network to tackle synthesis issues that arise with varying sentence lengths. By dynamically adapting to the length of the input, this component ensures robust performance across diverse text inputs and speaker styles, significantly improving the modelâ€™s ability to generalize to unseen speakers in a more natural and expressive manner. Experimental evaluations on the VCTK dataset suggest that DS-TTS demonstrates superior overall performance in voice cloning tasks compared to existing state-of-the-art models, showing notable improvements in both word error rate and speaker similarity. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯çš„è¿›å±•å¢åŠ äº†å¯¹ä¸ªæ€§åŒ–éŸ³é¢‘åˆæˆçš„éœ€æ±‚ã€‚é›¶æ ·æœ¬è¯­éŸ³å…‹éš†æ˜¯ä¸€é¡¹ä¸“é—¨çš„TTSä»»åŠ¡ï¼Œæ—¨åœ¨ä»…ä½¿ç”¨å•ä¸ªéŸ³é¢‘æ ·æœ¬å’Œä»»æ„æ–‡æœ¬åˆæˆç›®æ ‡è¯´è¯äººçš„å£°éŸ³ï¼Œè€Œæ— éœ€åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­äº‹å…ˆæ¥è§¦è¯¥è¯´è¯äººã€‚è¯¥è¿‡ç¨‹é‡‡ç”¨æ¨¡å¼è¯†åˆ«æŠ€æœ¯æ¥åˆ†æå¹¶å¤åˆ¶è¯´è¯äººçš„ç‹¬ç‰¹è¯­éŸ³ç‰¹å¾ã€‚å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†é€‚åº”æœªè§è¯´è¯äººçš„è¯­éŸ³é£æ ¼ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™å‡¸æ˜¾äº†å°†TTSç³»ç»Ÿæ¨å¹¿åº”ç”¨äºå¤„ç†å„ç§å£°éŸ³æ—¶ï¼Œåœ¨ä¿æŒè‡ªç„¶æ€§ã€è¡¨è¾¾åŠ›å’Œè¯´è¯äººä¿çœŸåº¦æ–¹é¢çš„å›°éš¾ã€‚ä¸ºäº†è§£å†³æœªè§è¯´è¯äººé£æ ¼é€‚åº”çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DS-TTSï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºå¯¹å¤šæ ·ä¸”ä¹‹å‰æœªå¬è¿‡çš„å£°éŸ³åˆæˆçš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åŒé‡é£æ ¼ç¼–ç ç½‘ç»œï¼ˆDuSENï¼‰ï¼Œå…¶ä¸­ä¸¤ä¸ªæˆªç„¶ä¸åŒçš„é£æ ¼ç¼–ç å™¨æ•æ‰è¯´è¯äººè¯­éŸ³èº«ä»½çš„äº’è¡¥æ–¹é¢ã€‚è¿™äº›è¯´è¯äººç‰¹å®šçš„é£æ ¼å‘é‡é€šè¿‡é£æ ¼é—¨æ§ç”µå½±ï¼ˆSGFï¼‰æœºåˆ¶æ— ç¼åœ°é›†æˆåˆ°åŠ¨æ€ç”Ÿæˆç½‘ç»œï¼ˆDyGNï¼‰ä¸­ï¼Œä»è€Œèƒ½å¤Ÿæ›´å‡†ç¡®ã€æ›´ç”ŸåŠ¨åœ°å¤åˆ¶æœªè§è¯´è¯äººçš„ç‹¬ç‰¹è¯­éŸ³ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€ç”Ÿæˆç½‘ç»œæ¥è§£å†³å› å¥å­é•¿åº¦ä¸åŒè€Œäº§ç”Ÿçš„åˆæˆé—®é¢˜ã€‚é€šè¿‡åŠ¨æ€é€‚åº”è¾“å…¥çš„é•¿åº¦ï¼Œæ­¤ç»„ä»¶å¯ç¡®ä¿åœ¨å„ç§æ–‡æœ¬è¾“å…¥å’Œè¯´è¯äººé£æ ¼æ–¹é¢çš„ç¨³å¥æ€§èƒ½ï¼Œä»è€Œæ˜¾è‘—æé«˜æ¨¡å‹ä»¥æ›´è‡ªç„¶ã€æ›´å…·è¡¨ç°åŠ›çš„æ–¹å¼æ¨å¹¿åˆ°æœªè§è¯´è¯äººçš„èƒ½åŠ›ã€‚åœ¨VCTKæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼ŒDS-TTSåœ¨è¯­éŸ³å…‹éš†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ•´ä½“æ€§èƒ½ï¼Œåœ¨å•è¯é”™è¯¯ç‡å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢éƒ½å–å¾—äº†æ˜¾ç€çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01020v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯çš„è¿›å±•æ¨åŠ¨äº†ä¸ªæ€§åŒ–éŸ³é¢‘åˆæˆçš„éœ€æ±‚å¢é•¿ã€‚é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä½œä¸ºä¸“é¡¹TTSä»»åŠ¡ï¼Œæ—¨åœ¨ä»…ä½¿ç”¨å•ä¸ªéŸ³é¢‘æ ·æœ¬å’Œä»»æ„æ–‡æœ¬åˆæˆç›®æ ‡è¯´è¯äººçš„å£°éŸ³ï¼Œä¸”è®­ç»ƒè¿‡ç¨‹ä¸­æ— éœ€é¢„å…ˆæ¥è§¦è¯¥è¯´è¯äººã€‚æ­¤æµç¨‹é‡‡ç”¨æ¨¡å¼è¯†åˆ«æŠ€æœ¯åˆ†æå¹¶å¤åˆ¶è¯´è¯äººçš„ç‹¬ç‰¹è¯­éŸ³ç‰¹å¾ã€‚å°½ç®¡æœ‰æ‰€è¿›å±•ï¼Œä½†é€‚åº”æœªè§è¯´è¯äººçš„è¯­éŸ³é£æ ¼ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™çªæ˜¾äº†TTSç³»ç»Ÿåœ¨å¤„ç†å¤šæ ·åŒ–å£°éŸ³æ—¶ä¿æŒè‡ªç„¶æ€§ã€è¡¨è¾¾åŠ›å’Œè¯´è¯äººä¿çœŸåº¦çš„æ¦‚æ‹¬æ¨å¹¿éš¾é¢˜ã€‚ä¸ºè§£å†³æœªè§è¯´è¯äººé£æ ¼é€‚åº”çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DS-TTSï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºå¤šæ ·åŒ–ã€å…ˆå‰æœªå¬è¿‡çš„å£°éŸ³åˆæˆçš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ ¸å¿ƒæ˜¯åŒé‡é£æ ¼ç¼–ç ç½‘ç»œï¼ˆDuSENï¼‰ï¼Œå…¶ä¸­ä¸¤ä¸ªç‹¬ç‰¹çš„é£æ ¼ç¼–ç å™¨æ•æ‰è¯´è¯äººè¯­éŸ³èº«ä»½çš„ä¸åŒæ–¹é¢ã€‚è¿™äº›è¯´è¯äººç‰¹å®šçš„é£æ ¼å‘é‡è¢«æ— ç¼é›†æˆåˆ°åŠ¨æ€ç”Ÿæˆç½‘ç»œï¼ˆDyGNï¼‰ä¸­ï¼Œé€šè¿‡é£æ ¼é—¨æ§ç”µå½±ï¼ˆSGFï¼‰æœºåˆ¶ï¼Œèƒ½å¤Ÿæ›´ç²¾ç¡®ã€æ›´ç”ŸåŠ¨åœ°å†ç°æœªè§è¯´è¯äººçš„ç‹¬ç‰¹è¯­éŸ³ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†åŠ¨æ€ç”Ÿæˆç½‘ç»œæ¥è§£å†³ä¸å¥å­é•¿åº¦å˜åŒ–ç›¸å…³çš„åˆæˆé—®é¢˜ã€‚é€šè¿‡åŠ¨æ€é€‚åº”è¾“å…¥é•¿åº¦ï¼Œæ­¤ç»„ä»¶ç¡®ä¿åœ¨å„ç§æ–‡æœ¬è¾“å…¥å’Œè¯´è¯äººé£æ ¼ä¸Šçš„ç¨³å¥æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹ä»¥æ›´è‡ªç„¶ã€æ›´ç”ŸåŠ¨çš„æ–¹å¼æ¦‚æ‹¬æœªè§è¯´è¯äººçš„èƒ½åŠ›ã€‚åœ¨VCTKæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼ŒDS-TTSåœ¨è¯­éŸ³å…‹éš†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ•´ä½“ä¼˜è¶Šçš„æ€§èƒ½ï¼Œåœ¨è¯é”™è¯¯ç‡å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢å‡å–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ€è¿‘TTSæŠ€æœ¯çš„è¿›å±•æ¨åŠ¨äº†ä¸ªæ€§åŒ–éŸ³é¢‘åˆæˆçš„éœ€æ±‚ã€‚</li>
<li>é›¶æ ·æœ¬è¯­éŸ³å…‹éš†æ—¨åœ¨ä»…ä½¿ç”¨å•ä¸ªéŸ³é¢‘æ ·æœ¬å’Œä»»æ„æ–‡æœ¬åˆæˆç›®æ ‡è¯´è¯äººçš„å£°éŸ³ã€‚</li>
<li>é€‚åº”æœªè§è¯´è¯äººçš„è¯­éŸ³é£æ ¼æ˜¯TTSæŠ€æœ¯çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>DS-TTSæ–¹æ³•é€šè¿‡åŒé‡é£æ ¼ç¼–ç ç½‘ç»œå’ŒåŠ¨æ€ç”Ÿæˆç½‘ç»œæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>åŒé‡é£æ ¼ç¼–ç ç½‘ç»œèƒ½å¤Ÿæ•æ‰è¯´è¯äººè¯­éŸ³èº«ä»½çš„ç‹¬ç‰¹æ–¹é¢ã€‚</li>
<li>åŠ¨æ€ç”Ÿæˆç½‘ç»œå¯ä»¥è§£å†³ä¸å¥å­é•¿åº¦å˜åŒ–ç›¸å…³çš„åˆæˆé—®é¢˜ï¼Œæé«˜æ¨¡å‹çš„æ¦‚æ‹¬èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ec35d4af76665baadd76cee094baae9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54ecd27a3921d48df90e06aa0a682e2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c688a7ce6554205de22318db9ff659e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e901ac17039972bf9993f10d13908a5d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Training-for-Open-E2E-Spoken-Dialogue-Systems"><a href="#Chain-of-Thought-Training-for-Open-E2E-Spoken-Dialogue-Systems" class="headerlink" title="Chain-of-Thought Training for Open E2E Spoken Dialogue Systems"></a>Chain-of-Thought Training for Open E2E Spoken Dialogue Systems</h2><p><strong>Authors:Siddhant Arora, Jinchuan Tian, Hayato Futami, Jee-weon Jung, Jiatong Shi, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe</strong></p>
<p>Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue systems preserve full differentiability and capture non-phonemic information, making them well-suited for modeling spoken interactions. However, existing E2E approaches often require large-scale training data and generates responses lacking semantic coherence. We propose a simple yet effective strategy leveraging a chain-of-thought (CoT) formulation, ensuring that training on conversational data remains closely aligned with the multimodal language model (LM)â€™s pre-training on speech recognition~(ASR), text-to-speech synthesis (TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over the baseline, successfully training spoken dialogue systems on publicly available human-human conversation datasets, while being compute-efficient enough to train on just 300 hours of public human-human conversation data, such as the Switchboard. We will publicly release our models and training code. </p>
<blockquote>
<p>ä¸ä¼ ç»Ÿçº§è”ç®¡é“ä¸åŒï¼Œç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰å£è¯­å¯¹è¯ç³»ç»Ÿä¿æŒå®Œå…¨å¯å¾®åˆ†å¹¶æ•è·éè¯­éŸ³ä¿¡æ¯ï¼Œä½¿å…¶æˆä¸ºå»ºæ¨¡å£è¯­äº¤äº’çš„ç†æƒ³é€‰æ‹©ã€‚ç„¶è€Œï¼Œç°æœ‰çš„E2Eæ–¹æ³•é€šå¸¸éœ€è¦å¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”ç”Ÿæˆçš„å“åº”ç¼ºä¹è¯­ä¹‰è¿è´¯æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼Œåˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰å…¬å¼ï¼Œç¡®ä¿åœ¨å¯¹è¯æ•°æ®ä¸Šçš„è®­ç»ƒä¸å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰åœ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆTTSï¼‰å’Œæ–‡æœ¬LMä»»åŠ¡ä¸Šçš„é¢„è®­ç»ƒç´§å¯†å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†è¶…è¿‡1.5çš„ROUGE-1æ”¹è¿›ï¼ŒæˆåŠŸåœ°åœ¨å…¬å¼€å¯ç”¨çš„äººæœºå¯¹è¯æ•°æ®é›†ä¸Šè®­ç»ƒäº†å£è¯­å¯¹è¯ç³»ç»Ÿï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡è¶³å¤Ÿé«˜ï¼Œåªéœ€åœ¨300å°æ—¶å…¬å¼€çš„äººæœºå¯¹è¯æ•°æ®ï¼ˆå¦‚Switchboardï¼‰ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„æ¨¡å‹å’Œè®­ç»ƒä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00722v1">PDF</a> Accepted at INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç«¯åˆ°ç«¯çš„è¯­éŸ³å¯¹è¯ç³»ç»Ÿï¼ˆE2E SDSï¼‰çš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœç°æœ‰E2Eæ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰çš„ç­–ç•¥ï¼Œç¡®ä¿å¯¹è¯ç³»ç»Ÿçš„è®­ç»ƒä¸å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒç´§å¯†å¯¹é½ã€‚è¯¥æ–¹æ³•æˆåŠŸåœ°åœ¨å…¬å¼€çš„äººç±»å¯¹è¯æ•°æ®é›†ä¸Šè®­ç»ƒäº†è¯­éŸ³å¯¹è¯ç³»ç»Ÿï¼Œå®ç°äº†ç›¸è¾ƒäºåŸºå‡†çº¿çš„ROUGE-1è¯„åˆ†æå‡è¶…è¿‡1.5ï¼Œä¸”èƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨300å°æ—¶å…¬å¼€äººç±»å¯¹è¯æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œé«˜æ•ˆè®¡ç®—è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«¯åˆ°ç«¯çš„è¯­éŸ³å¯¹è¯ç³»ç»Ÿï¼ˆE2E SDSï¼‰èƒ½å¤Ÿä¿ç•™å®Œæ•´çš„å¯å¾®è°ƒæ€§å¹¶æ•æ‰éè¯­éŸ³ä¿¡æ¯ï¼Œé€‚åˆå»ºæ¨¡å£è¯­äº¤äº’ã€‚</li>
<li>ç°æœ‰E2Eæ–¹æ³•éœ€è¦å¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®ï¼Œç”Ÿæˆçš„å“åº”ç¼ºä¹è¯­ä¹‰è¿è´¯æ€§ã€‚</li>
<li>æå‡ºäº†åˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰çš„ç­–ç•¥ï¼Œç¡®ä¿å¯¹è¯ç³»ç»Ÿçš„è®­ç»ƒä¸å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒå¯¹é½ã€‚</li>
<li>è¯¥æ–¹æ³•æˆåŠŸåœ°åœ¨å…¬å¼€çš„äººç±»å¯¹è¯æ•°æ®é›†ä¸Šå®ç°äº†ç›¸è¾ƒäºåŸºå‡†çº¿çš„ROUGE-1è¯„åˆ†æå‡è¶…è¿‡1.5ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰è®¡ç®—æ•ˆç‡ï¼Œèƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨300å°æ—¶å…¬å¼€äººç±»å¯¹è¯æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æœ¬æ–‡å°†å…¬å¼€æ¨¡å‹å’Œè®­ç»ƒä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43c53528406a787aa022ed7a37ac9772.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3aabb0724524d837b8ba4fb695aa26b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8f420b2610b8f8f1210ee1bd5a106d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43cf3653181841430e5eec7649b2fb40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e9210fc4aac39dd76f46dbd50237eb1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Quality-Assessment-of-Noisy-and-Enhanced-Speech-with-Limited-Data-UWB-NTIS-System-for-VoiceMOS-2024-and-Beyond"><a href="#Quality-Assessment-of-Noisy-and-Enhanced-Speech-with-Limited-Data-UWB-NTIS-System-for-VoiceMOS-2024-and-Beyond" class="headerlink" title="Quality Assessment of Noisy and Enhanced Speech with Limited Data:   UWB-NTIS System for VoiceMOS 2024 and Beyond"></a>Quality Assessment of Noisy and Enhanced Speech with Limited Data:   UWB-NTIS System for VoiceMOS 2024 and Beyond</h2><p><strong>Authors:Marie KuneÅ¡ovÃ¡</strong></p>
<p>In this preprint, we present the UWB-NTIS-TTS teamâ€™s submission to Track 3 of the VoiceMOS 2024 Challenge, the goal of which was to automatically assess the speech quality of noisy and de-noised speech in terms of the ITU-T P.835 metrics of â€œSIGâ€, â€œBAKâ€, and â€œOVRLâ€. Our proposed system, based on wav2vec 2.0, placed among the top systems in the challenge, achieving the best prediction of the BAK scores (background noise intrusiveness), the second-best prediction of the OVRL score (overall audio quality), and the third-best prediction of SIG (speech signal quality) out of the five participating systems. We describe our approach, such as the two-stage fine-tuning process we used to contend with the challengeâ€™s very limiting restrictions on allowable training data, and present the results achieved both on the VoiceMOS 2024 Challenge data and on the recently released CHiME 7 - UDASE dataset. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡é¢„å°æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†UWB-NTIS-TTSå›¢é˜Ÿå¯¹VoiceMOS 2024æŒ‘æˆ˜èµ›ç¬¬3èµ›é“çš„æäº¤å†…å®¹ã€‚è¯¥æŒ‘æˆ˜èµ›çš„ç›®æ ‡æ˜¯è‡ªåŠ¨è¯„ä¼°å¸¦å™ªå’Œé™å™ªè¯­éŸ³çš„è¯­éŸ³è´¨é‡ï¼Œå…·ä½“å‚ç…§ITU-T P.835æŒ‡æ ‡çš„â€œSIGâ€ï¼ˆè¯­éŸ³ä¿¡å·è´¨é‡ï¼‰ã€â€œBAKâ€ï¼ˆèƒŒæ™¯å™ªå£°ä¾µå…¥æ€§ï¼‰å’Œâ€œOVRLâ€ï¼ˆæ€»ä½“éŸ³é¢‘è´¨é‡ï¼‰ã€‚æˆ‘ä»¬æå‡ºçš„åŸºäºwav2vec 2.0çš„ç³»ç»Ÿåœ¨æŒ‘æˆ˜ä¸­è·»èº«é¡¶å°–ç³»ç»Ÿä¹‹åˆ—ï¼Œå®ç°äº†å¯¹BAKå¾—åˆ†ï¼ˆèƒŒæ™¯å™ªå£°ä¾µå…¥æ€§ï¼‰çš„æœ€ä½³é¢„æµ‹ã€å¯¹OVRLå¾—åˆ†ï¼ˆæ€»ä½“éŸ³é¢‘è´¨é‡ï¼‰çš„ç¬¬äºŒä½³é¢„æµ‹ï¼Œä»¥åŠå¯¹SIGï¼ˆè¯­éŸ³ä¿¡å·è´¨é‡ï¼‰çš„ç¬¬ä¸‰ä½³é¢„æµ‹ï¼ˆåœ¨äº”å¥—å‚èµ›ç³»ç»Ÿä¸­ï¼‰ã€‚æˆ‘ä»¬æè¿°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä¾‹å¦‚æˆ‘ä»¬ç”¨äºåº”å¯¹æŒ‘æˆ˜èµ›å¯¹å¯ç”¨è®­ç»ƒæ•°æ®éå¸¸ä¸¥æ ¼çš„é™åˆ¶çš„ä¸¤é˜¶æ®µå¾®è°ƒè¿‡ç¨‹ï¼Œå¹¶å±•ç¤ºäº†åœ¨VoiceMOS 2024æŒ‘æˆ˜èµ›æ•°æ®å’Œæœ€è¿‘å‘å¸ƒçš„CHiME 7-UDASEæ•°æ®é›†ä¸Šå–å¾—çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00506v1">PDF</a> This is a preliminary write-up of our initial work, posted as an   early version preprint for cross-referencing purposes. We intend to further   extend this research and submit it for publication at a conference, at which   point this preprint will be updated with the full text</p>
<p><strong>Summary</strong><br>åŸºäºwav2vec 2.0çš„UWB-NTIS-TTSå›¢é˜Ÿåœ¨VoiceMOS 2024æŒ‘æˆ˜èµ›çš„Track 3ä¸­æå‡ºäº†è‡ªåŠ¨è¯„ä¼°å¸¦å™ªä¸é™å™ªè¯­éŸ³è´¨é‡çš„ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåœ¨èƒŒæ™¯å™ªå£°ä¾µå…¥æ€§ï¼ˆBAKï¼‰è¯„åˆ†é¢„æµ‹ä¸Šè¡¨ç°æœ€ä½³ï¼Œæ€»ä½“éŸ³é¢‘è´¨é‡ï¼ˆOVRLï¼‰è¯„åˆ†é¢„æµ‹ä½åˆ—ç¬¬äºŒï¼Œè¯­éŸ³ä¿¡å·è´¨é‡ï¼ˆSIGï¼‰é¢„æµ‹ä½åˆ—ç¬¬ä¸‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UWB-NTIS-TTSå›¢é˜Ÿå‚åŠ äº†VoiceMOS 2024æŒ‘æˆ˜èµ›çš„Track 3ï¼Œæ—¨åœ¨è‡ªåŠ¨è¯„ä¼°å¸¦å™ªå’Œé™å™ªè¯­éŸ³çš„è´¨é‡ã€‚</li>
<li>å›¢é˜Ÿä½¿ç”¨åŸºäºwav2vec 2.0çš„ç³»ç»Ÿå‚èµ›ï¼Œåº”å¯¹æŒ‘æˆ˜èµ›å¯¹è®­ç»ƒæ•°æ®çš„ä¸¥æ ¼é™åˆ¶ã€‚</li>
<li>ç³»ç»Ÿåœ¨èƒŒæ™¯å™ªå£°ä¾µå…¥æ€§ï¼ˆBAKï¼‰è¯„åˆ†é¢„æµ‹ä¸Šå–å¾—æœ€ä½³è¡¨ç°ã€‚</li>
<li>åœ¨æ€»ä½“éŸ³é¢‘è´¨é‡ï¼ˆOVRLï¼‰è¯„åˆ†é¢„æµ‹ä¸Šï¼Œç³»ç»Ÿè·å¾—ç¬¬äºŒåã€‚</li>
<li>åœ¨è¯­éŸ³ä¿¡å·è´¨é‡ï¼ˆSIGï¼‰é¢„æµ‹æ–¹é¢ï¼Œç³»ç»Ÿæ’åç¬¬ä¸‰ã€‚</li>
<li>å›¢é˜Ÿæè¿°äº†ä¸¤é˜¶æ®µå¾®è°ƒè¿‡ç¨‹æ¥åº”å¯¹æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9968484ba1646d9222b4aac86de3b17c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3487533240cb0561354460c4740e1f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b814042604d2a560e5ea45df3fb29ae8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f57632d47d562faf6dc951e7a5c9845.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-226f2d3a37045a091dcbce8e5c282893.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Towards a Japanese Full-duplex Spoken Dialogue System
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-971ee3a8f96ee3b15eaa632cec6e76c5.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
