<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and   Semantic Understanding Capability of LLM">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3ccbf004e61afa8d7776c3ee3cb40667.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-05-æ›´æ–°"><a href="#2025-06-05-æ›´æ–°" class="headerlink" title="2025-06-05 æ›´æ–°"></a>2025-06-05 æ›´æ–°</h1><h2 id="Entity-Augmented-Neuroscience-Knowledge-Retrieval-Using-Ontology-and-Semantic-Understanding-Capability-of-LLM"><a href="#Entity-Augmented-Neuroscience-Knowledge-Retrieval-Using-Ontology-and-Semantic-Understanding-Capability-of-LLM" class="headerlink" title="Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and   Semantic Understanding Capability of LLM"></a>Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and   Semantic Understanding Capability of LLM</h2><p><strong>Authors:Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam</strong></p>
<p>Neuroscience research publications encompass a vast wealth of knowledge. Accurately retrieving existing information and discovering new insights from this extensive literature is essential for advancing the field. However, when knowledge is dispersed across multiple sources, current state-of-the-art retrieval methods often struggle to extract the necessary information. A knowledge graph (KG) can integrate and link knowledge from multiple sources, but existing methods for constructing KGs in neuroscience often rely on labeled data and require domain expertise. Acquiring large-scale, labeled data for a specialized area like neuroscience presents significant challenges. This work proposes novel methods for constructing KG from unlabeled large-scale neuroscience research corpus utilizing large language models (LLM), neuroscience ontology, and text embeddings. We analyze the semantic relevance of neuroscience text segments identified by LLM for building the knowledge graph. We also introduce an entity-augmented information retrieval algorithm to extract knowledge from the KG. Several experiments were conducted to evaluate the proposed approaches, and the results demonstrate that our methods significantly enhance knowledge discovery from the unlabeled neuroscience research corpus. It achieves an F1 score of 0.84 for entity extraction, and the knowledge obtained from the KG improves answers to over 54% of the questions. </p>
<blockquote>
<p>ç¥ç»ç§‘å­¦ç ”ç©¶å‡ºç‰ˆç‰©åŒ…å«äº†å¤§é‡çš„çŸ¥è¯†ã€‚ä»å¹¿æ³›çš„æ–‡çŒ®ä¸­å‡†ç¡®æ£€ç´¢ç°æœ‰ä¿¡æ¯å¹¶å‘ç°æ–°è§è§£å¯¹äºæ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“çŸ¥è¯†åˆ†æ•£åœ¨å¤šä¸ªæ¥æºæ—¶ï¼Œç°æœ‰çš„æœ€å…ˆè¿›çš„æ£€ç´¢æ–¹æ³•å¾€å¾€éš¾ä»¥æå–å¿…è¦çš„ä¿¡æ¯ã€‚çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰å¯ä»¥æ•´åˆå’Œé“¾æ¥æ¥è‡ªå¤šä¸ªæ¥æºçš„çŸ¥è¯†ï¼Œä½†ç¥ç»ç§‘å­¦ä¸­æ„å»ºçŸ¥è¯†å›¾è°±çš„ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ ‡è®°æ•°æ®å¹¶éœ€è¦é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºç¥ç»ç§‘å­¦ç­‰ä¸“ä¸šé¢†åŸŸè·å–å¤§è§„æ¨¡ã€æ ‡è®°çš„æ•°æ®å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†åˆ©ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€ç¥ç»ç§‘å­¦æœ¬ä½“å’Œæ–‡æœ¬åµŒå…¥ï¼Œä»æœªæ ‡è®°çš„å¤§è§„æ¨¡ç¥ç»ç§‘å­¦ç ”ç©¶è¯­æ–™åº“ä¸­æ„å»ºçŸ¥è¯†å›¾è°±çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬åˆ†æäº†LLMè¯†åˆ«çš„ç¥ç»ç§‘å­¦æ–‡æœ¬ç‰‡æ®µçš„è¯­ä¹‰ç›¸å…³æ€§ï¼Œä»¥æ„å»ºçŸ¥è¯†å›¾è°±ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å®ä½“å¢å¼ºä¿¡æ¯æ£€ç´¢ç®—æ³•ï¼Œä»çŸ¥è¯†å›¾è°±ä¸­æå–çŸ¥è¯†ã€‚è¿›è¡Œäº†å¤šæ¬¡å®éªŒæ¥è¯„ä¼°æ‰€æå‡ºçš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†ä»æœªæ ‡è®°çš„ç¥ç»ç§‘å­¦ç ”ç©¶è¯­æ–™åº“ä¸­å‘ç°çŸ¥è¯†çš„èƒ½åŠ›ã€‚å¯¹äºå®ä½“æå–ï¼Œå®ƒå®ç°äº†0.84çš„F1åˆ†æ•°ï¼Œä»çŸ¥è¯†å›¾è°±ä¸­è·å¾—çš„çŸ¥è¯†æé«˜äº†è¶…è¿‡54%çš„é—®é¢˜çš„ç­”æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03145v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆ©ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€ç¥ç»ç§‘å­¦æœ¬ä½“è®ºå’Œæ–‡æœ¬åµŒå…¥æŠ€æœ¯ï¼Œä»æœªæ ‡è®°çš„å¤§è§„æ¨¡ç¥ç»ç§‘å­¦ç ”ç©¶è¯­æ–™åº“ä¸­æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ã€‚åˆ†æLLMè¯†åˆ«çš„ç¥ç»ç§‘å­¦æ–‡æœ¬ç‰‡æ®µçš„è¯­ä¹‰ç›¸å…³æ€§ä»¥æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œå¹¶å¼•å…¥å®ä½“å¢å¼ºä¿¡æ¯æ£€ç´¢ç®—æ³•ä»çŸ¥è¯†å›¾è°±ä¸­æå–çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜ä»æœªæ ‡è®°çš„ç¥ç»ç§‘å­¦ç ”ç©¶è¯­æ–™åº“ä¸­å‘ç°çŸ¥è¯†çš„èƒ½åŠ›ï¼Œå®ä½“æå–çš„F1åˆ†æ•°è¾¾åˆ°0.84ï¼Œä»çŸ¥è¯†å›¾è°±ä¸­è·å¾—çš„çŸ¥è¯†èƒ½å›ç­”è¶…è¿‡54%çš„é—®é¢˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨LLMã€ç¥ç»ç§‘å­¦æœ¬ä½“è®ºå’Œæ–‡æœ¬åµŒå…¥æŠ€æœ¯ä»å¤§è§„æ¨¡æœªæ ‡è®°çš„ç¥ç»ç§‘å­¦ç ”ç©¶è¯­æ–™åº“ä¸­æ„å»ºçŸ¥è¯†å›¾è°±ã€‚</li>
<li>LLMç”¨äºè¯†åˆ«ä¸ç¥ç»ç§‘å­¦ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µï¼Œå¢å¼ºçŸ¥è¯†å›¾è°±çš„æ„å»ºã€‚</li>
<li>å¼•å…¥å®ä½“å¢å¼ºä¿¡æ¯æ£€ç´¢ç®—æ³•ï¼Œæé«˜ä»çŸ¥è¯†å›¾è°±ä¸­æå–çŸ¥è¯†çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®ä½“æå–æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½ï¼ŒF1åˆ†æ•°è¾¾åˆ°0.84ã€‚</li>
<li>çŸ¥è¯†å›¾è°±èƒ½å¤Ÿå›ç­”è¶…è¿‡54%çš„é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†çŸ¥è¯†å‘ç°çš„æ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•å…‹æœäº†ç°æœ‰ç¥ç»ç§‘å­¦çŸ¥è¯†å›¾è°±æ„å»ºæ–¹æ³•å¯¹æ ‡è®°æ•°æ®çš„ä¾èµ–ï¼Œé™ä½äº†å¯¹é¢†åŸŸä¸“å®¶çš„éœ€æ±‚ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºç¥ç»ç§‘å­¦é¢†åŸŸçš„ä¿¡æ¯æ£€ç´¢å’ŒçŸ¥è¯†å‘ç°æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03145">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4680c5bdd6189186187c9ca7e0bd17ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ccbf004e61afa8d7776c3ee3cb40667.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffffd2c6577e2f1cfefd6c5e608c0ba4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec6d003e7e5305496898d78079699f7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3e4e347f1706b5a646d0f4bb5b0cec2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Not-All-Tokens-Are-Meant-to-Be-Forgotten"><a href="#Not-All-Tokens-Are-Meant-to-Be-Forgotten" class="headerlink" title="Not All Tokens Are Meant to Be Forgotten"></a>Not All Tokens Are Meant to Be Forgotten</h2><p><strong>Authors:Xiangyu Zhou, Yao Qiang, Saleh Zare Zade, Douglas Zytko, Prashant Khanduri, Dongxiao Zhu</strong></p>
<p>Large Language Models (LLMs), pre-trained on massive text corpora, exhibit remarkable human-level language understanding, reasoning, and decision-making abilities. However, they tend to memorize unwanted information, such as private or copyrighted content, raising significant privacy and legal concerns. Unlearning has emerged as a promising solution, but existing methods face a significant challenge of over-forgetting. This issue arises because they indiscriminately suppress the generation of all the tokens in forget samples, leading to a substantial loss of model utility. To overcome this challenge, we introduce the Targeted Information Forgetting (TIF) framework, which consists of (1) a flexible targeted information identifier designed to differentiate between unwanted words (UW) and general words (GW) in the forget samples, and (2) a novel Targeted Preference Optimization approach that leverages Logit Preference Loss to unlearn unwanted information associated with UW and Preservation Loss to retain general information in GW, effectively improving the unlearning process while mitigating utility degradation. Extensive experiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF framework enhances unlearning effectiveness while preserving model utility and achieving state-of-the-art results. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤§é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå±•ç°å‡ºæƒŠäººçš„äººç±»æ°´å¹³è¯­è¨€ç†è§£ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬æœ‰è®°å¿†ä¸å¿…è¦ä¿¡æ¯çš„è¶‹åŠ¿ï¼Œå¦‚ç§äººæˆ–ç‰ˆæƒå†…å®¹ï¼Œè¿™å¼•å‘äº†éšç§å’Œæ³•å¾‹çš„æ‹…å¿§ã€‚é—å¿˜ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆè€Œå‡ºç°ï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´ç€è¿‡åº¦é—å¿˜çš„é‡å¤§æŒ‘æˆ˜ã€‚è¿™ä¸ªé—®é¢˜ä¹‹æ‰€ä»¥å‡ºç°ï¼Œæ˜¯å› ä¸ºå®ƒä»¬ä¸åŠ åŒºåˆ†åœ°æŠ‘åˆ¶é—å¿˜æ ·æœ¬ä¸­æ‰€æœ‰æ ‡è®°çš„ç”Ÿæˆï¼Œå¯¼è‡´æ¨¡å‹æ•ˆç”¨çš„å¤§é‡æŸå¤±ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æœ‰é’ˆå¯¹æ€§çš„ä¿¡æ¯é—å¿˜ï¼ˆTIFï¼‰æ¡†æ¶ï¼Œå®ƒåŒ…æ‹¬ï¼ˆ1ï¼‰ä¸€ä¸ªçµæ´»çš„æœ‰é’ˆå¯¹æ€§çš„ä¿¡æ¯æ ‡è¯†ç¬¦ï¼Œæ—¨åœ¨åŒºåˆ†é—å¿˜æ ·æœ¬ä¸­çš„ä¸éœ€è¦çš„å•è¯ï¼ˆUWï¼‰å’Œä¸€èˆ¬å•è¯ï¼ˆGWï¼‰ï¼Œä»¥åŠï¼ˆ2ï¼‰ä¸€ç§æ–°çš„æœ‰é’ˆå¯¹æ€§çš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é€»è¾‘åå¥½æŸå¤±æ¥é—å¿˜ä¸UWç›¸å…³çš„ä¸éœ€è¦çš„ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨ä¿ç•™æŸå¤±æ¥ä¿ç•™GWä¸­çš„ä¸€èˆ¬ä¿¡æ¯ï¼Œè¿™æœ‰æ•ˆåœ°æ”¹è¿›äº†é—å¿˜è¿‡ç¨‹å¹¶å‡è½»äº†æ•ˆç”¨é™ä½çš„é—®é¢˜ã€‚åœ¨TOFUå’ŒMUSEåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„TIFæ¡†æ¶åœ¨æé«˜é—å¿˜æ•ˆæœçš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„æ•ˆç”¨ï¼Œå¹¶å®ç°äº†æœ€æ–°çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03142v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æµ·é‡æ–‡æœ¬è¯­æ–™åº“ä¸Šçš„é¢„è®­ç»ƒï¼Œå±•ç°å‡ºæƒŠäººçš„äººç±»çº§è¯­è¨€ç†è§£ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä¼šè®°å¿†ä¸æƒ³è¦çš„èµ„è®¯ï¼Œå¦‚ç§å¯†æˆ–ç‰ˆæƒå†…å®¹ï¼Œå¼•å‘éšç§å’Œæ³•å¾‹æ‹…å¿§ã€‚é—å¿˜æŠ€æœ¯ä½œä¸ºä¸€ç§è§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œä½†ç°æœ‰æ–¹æ³•é¢ä¸´è¿‡åº¦é—å¿˜çš„æŒ‘æˆ˜ã€‚è¿™æ˜¯å› ä¸ºå®ƒä»¬ä¼šæ— å·®åˆ«åœ°æŠ‘åˆ¶é—å¿˜æ ·æœ¬çš„æ‰€æœ‰è¯æ±‡ç”Ÿæˆï¼Œå¯¼è‡´æ¨¡å‹æ•ˆç”¨å¤§é‡æŸå¤±ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºé’ˆå¯¹æ€§é—å¿˜ï¼ˆTIFï¼‰æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸€ï¼‰çµæ´»çš„é’ˆå¯¹æ€§ä¿¡æ¯è¯†åˆ«å™¨ï¼Œç”¨äºåŒºåˆ†é—å¿˜æ ·æœ¬ä¸­çš„ä¸éœ€è¦çš„è¯æ±‡ï¼ˆUWï¼‰å’Œä¸€èˆ¬è¯æ±‡ï¼ˆGWï¼‰ï¼Œä»¥åŠäºŒï¼‰æ–°å‹çš„ç›®æ ‡åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¯¹æ•°åå¥½æŸå¤±æ¥é—å¿˜ä¸UWç›¸å…³çš„ä¸éœ€è¦çš„ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨ä¿ç•™æŸå¤±ä¿æŒGWä¸­çš„ä¸€èˆ¬ä¿¡æ¯ï¼Œä»è€Œæ”¹è¿›é—å¿˜è¿‡ç¨‹å¹¶ç¼“è§£æ•ˆç”¨é™ä½ã€‚åœ¨TOFUå’ŒMUSEåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„TIFæ¡†æ¶åœ¨æé«˜é—å¿˜æ•ˆæœçš„åŒæ—¶ï¼Œä¿ç•™äº†æ¨¡å‹æ•ˆç”¨ï¼Œå¹¶å®ç°äº†æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå±•ç°å‡ºäººç±»çº§åˆ«çš„è¯­è¨€ç†è§£ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼Œä½†å­˜åœ¨è®°å¿†ä¸æƒ³è¦çš„èµ„è®¯çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰é—å¿˜æŠ€æœ¯é¢ä¸´è¿‡åº¦é—å¿˜çš„æŒ‘æˆ˜ï¼Œä¼šå¯¼è‡´æ¨¡å‹æ•ˆç”¨æŸå¤±ã€‚</li>
<li>æ¨å‡ºé’ˆå¯¹æ€§é—å¿˜ï¼ˆTIFï¼‰æ¡†æ¶ä»¥è§£å†³è¿‡åº¦é—å¿˜é—®é¢˜ã€‚</li>
<li>TIFæ¡†æ¶åŒ…æ‹¬çµæ´»çš„é’ˆå¯¹æ€§ä¿¡æ¯è¯†åˆ«å™¨å’Œç›®æ ‡åå¥½ä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>å¯¹æ•°åå¥½æŸå¤±ç”¨äºé—å¿˜ä¸éœ€è¦çš„ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™ä¸€èˆ¬ä¿¡æ¯ã€‚</li>
<li>TIFæ¡†æ¶åœ¨åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†é—å¿˜æ•ˆæœå¹¶ä¿ç•™äº†æ¨¡å‹æ•ˆç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a4d44adddf2f38057b3bea08a378259.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12c4f322207b886bda5559cdce809fc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6feeee9e343464c4d4c945f5e69c3cb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8a9a7fb020d27ed99114aeeaeeb2b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e75ff2f50e8cb154b145b26eb597115.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SVGenius-Benchmarking-LLMs-in-SVG-Understanding-Editing-and-Generation"><a href="#SVGenius-Benchmarking-LLMs-in-SVG-Understanding-Editing-and-Generation" class="headerlink" title="SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation"></a>SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation</h2><p><strong>Authors:Siqi Chen, Xinyu Dong, Haolei Xu, Xingyu Wu, Fei Tang, Hang Zhang, Yuchen Yan, Linjuan Wu, Wenqi Zhang, Guiyang Hou, Yongliang Shen, Weiming Lu, Yueting Zhuang</strong></p>
<p>Large Language Models (LLMs) and Multimodal LLMs have shown promising capabilities for SVG processing, yet existing benchmarks suffer from limited real-world coverage, lack of complexity stratification, and fragmented evaluation paradigms. We introduce SVGenius, a comprehensive benchmark comprising 2,377 queries across three progressive dimensions: understanding, editing, and generation. Built on real-world data from 24 application domains with systematic complexity stratification, SVGenius evaluates models through 8 task categories and 18 metrics. We assess 22 mainstream models spanning different scales, architectures, training paradigms, and accessibility levels. Our analysis reveals that while proprietary models significantly outperform open-source counterparts, all models exhibit systematic performance degradation with increasing complexity, indicating fundamental limitations in current approaches; however, reasoning-enhanced training proves more effective than pure scaling for overcoming these limitations, though style transfer remains the most challenging capability across all model types. SVGenius establishes the first systematic evaluation framework for SVG processing, providing crucial insights for developing more capable vector graphics models and advancing automated graphic design applications. Appendix and supplementary materials (including all data and code) are available at <a target="_blank" rel="noopener" href="https://zju-real.github.io/SVGenius">https://zju-real.github.io/SVGenius</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€LLMåœ¨SVGå¤„ç†æ–¹é¢å±•ç°å‡ºæœ‰å‰æ™¯çš„èƒ½åŠ›ï¼Œç„¶è€Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨ç°å®ä¸–ç•Œè¦†ç›–æœ‰é™ã€ç¼ºä¹å¤æ‚æ€§åˆ†å±‚å’Œè¯„ä¼°èŒƒå¼ç¢ç‰‡åŒ–çš„é—®é¢˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†SVGeniusï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2377ä¸ªæŸ¥è¯¢çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ä¸‰ä¸ªé€’è¿›ç»´åº¦ï¼šç†è§£ã€ç¼–è¾‘å’Œç”Ÿæˆã€‚SVGeniuså»ºç«‹åœ¨æ¥è‡ª24ä¸ªåº”ç”¨é¢†åŸŸçš„çœŸå®æ•°æ®åŸºç¡€ä¸Šï¼Œå…·æœ‰ç³»ç»Ÿçš„å¤æ‚æ€§åˆ†å±‚ï¼Œé€šè¿‡8ä¸ªä»»åŠ¡ç±»åˆ«å’Œ18ä¸ªæŒ‡æ ‡å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°äº†22ä¸ªä¸»æµæ¨¡å‹ï¼Œæ¶µç›–ä¸åŒçš„è§„æ¨¡ã€æ¶æ„ã€è®­ç»ƒèŒƒå¼å’Œå¯è®¿é—®æ€§æ°´å¹³ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œä¸“æœ‰æ¨¡å‹æ˜¾è‘—ä¼˜äºå¼€æºæ¨¡å‹ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨å¤æ‚æ€§å¢åŠ æ—¶éƒ½è¡¨ç°å‡ºç³»ç»Ÿæ€§æ€§èƒ½ä¸‹é™ï¼Œè¡¨æ˜å½“å‰æ–¹æ³•å­˜åœ¨æ ¹æœ¬æ€§å±€é™ï¼›ç„¶è€Œï¼Œæ¨ç†å¢å¼ºè®­ç»ƒè¯æ˜åœ¨å…‹æœè¿™äº›é™åˆ¶æ–¹é¢æ¯”çº¯ç²¹æ‰©å¤§è§„æ¨¡æ›´ä¸ºæœ‰æ•ˆï¼Œå°½ç®¡é£æ ¼è½¬æ¢ä»æ˜¯æ‰€æœ‰æ¨¡å‹ç±»å‹ä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„èƒ½åŠ›ã€‚SVGeniuså»ºç«‹äº†SVGå¤„ç†çš„é¦–ä¸ªç³»ç»Ÿè¯„ä¼°æ¡†æ¶ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„çŸ¢é‡å›¾å½¢æ¨¡å‹å’Œæ¨åŠ¨è‡ªåŠ¨åŒ–å›¾å½¢è®¾è®¡åº”ç”¨æä¾›äº†å…³é”®è§è§£ã€‚é™„å½•å’Œè¡¥å……ææ–™ï¼ˆåŒ…æ‹¬æ‰€æœ‰æ•°æ®å’Œä»£ç ï¼‰å¯åœ¨<a target="_blank" rel="noopener" href="https://zju-real.github.io/SVGenius%E8%8E%B7%E5%8F%96%E3%80%82">https://zju-real.github.io/SVGeniusè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03139v1">PDF</a> 19 pages,4 figures, Project page:   <a target="_blank" rel="noopener" href="https://zju-real.github.io/SVGenius">https://zju-real.github.io/SVGenius</a>, Code:   <a target="_blank" rel="noopener" href="https://github.com/ZJU-REAL/SVGenius-Bench">https://github.com/ZJU-REAL/SVGenius-Bench</a></p>
<p><strong>æ‘˜è¦</strong><br>LLMåŠå¤šæ¨¡æ€LLMåœ¨å¤„ç†SVGæ–¹é¢å±•ç°å¼ºå¤§æ½œåŠ›ï¼Œä½†ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨çœŸå®ä¸–ç•Œè¦†ç›–ä¸è¶³ã€å¤æ‚æ€§åˆ†å±‚ç¼ºå¤±åŠè¯„ä¼°æ¨¡å¼åˆ†æ•£ç­‰é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»SVGeniusï¼Œä¸€ä¸ªåŒ…å«2,377æ¡æŸ¥è¯¢çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ¶‰åŠç†è§£ã€ç¼–è¾‘å’Œç”Ÿæˆä¸‰ä¸ªæ¸è¿›ç»´åº¦ã€‚SVGeniusä»¥24ä¸ªåº”ç”¨é¢†åŸŸçš„çœŸå®æ•°æ®ä¸ºåŸºç¡€ï¼Œè¿›è¡Œç³»ç»Ÿå¤æ‚æ€§åˆ†å±‚ï¼Œé€šè¿‡8ç±»ä»»åŠ¡å’Œ18é¡¹æŒ‡æ ‡è¯„ä¼°æ¨¡å‹ã€‚è¯„ä¼°äº†ä¸åŒè§„æ¨¡ã€æ¶æ„ã€è®­ç»ƒæ¨¡å¼å’Œå¯è®¿é—®æ€§æ°´å¹³çš„22ä¸ªä¸»æµæ¨¡å‹ã€‚åˆ†æè¡¨æ˜ï¼Œä¸“æœ‰æ¨¡å‹æ˜¾è‘—ä¼˜äºå¼€æºæ¨¡å‹ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨å¤æ‚æ€§å¢åŠ æ—¶æ€§èƒ½ä¸‹é™ï¼Œè¡¨æ˜å½“å‰æ–¹æ³•å­˜åœ¨æ ¹æœ¬æ€§å±€é™ï¼›ç„¶è€Œï¼Œæ¨ç†å¢å¼ºè®­ç»ƒæ¯”å•çº¯æ‰©å¤§è§„æ¨¡æ›´èƒ½æœ‰æ•ˆå…‹æœè¿™äº›å±€é™ï¼Œè€Œé£æ ¼è½¬æ¢ä»æ˜¯æ‰€æœ‰æ¨¡å‹ç±»å‹ä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„èƒ½åŠ›ã€‚SVGeniusä¸ºSVGå¤„ç†å»ºç«‹äº†é¦–ä¸ªç³»ç»Ÿè¯„ä¼°æ¡†æ¶ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„çŸ¢é‡å›¾å½¢æ¨¡å‹å’Œæ¨åŠ¨è‡ªåŠ¨åŒ–å›¾å½¢è®¾è®¡åº”ç”¨æä¾›äº†å…³é”®è§è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMåœ¨å¤„ç†SVGæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨æ€§èƒ½æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨çœŸå®ä¸–ç•Œè¦†ç›–ã€å¤æ‚æ€§åˆ†å±‚å’Œè¯„ä¼°æ¨¡å¼æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>SVGeniusæ˜¯ä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ç†è§£ã€ç¼–è¾‘å’Œç”Ÿæˆä¸‰ä¸ªç»´åº¦ï¼ŒåŸºäºçœŸå®æ•°æ®å¹¶ç³»ç»Ÿåœ°è¿›è¡Œå¤æ‚æ€§åˆ†å±‚ã€‚</li>
<li>è¯„ä¼°äº†ä¸åŒæ¨¡å‹åœ¨8ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ­ç¤ºäº†ä¸“æœ‰æ¨¡å‹ä¸å¼€æºæ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚</li>
<li>æ‰€æœ‰æ¨¡å‹åœ¨é¢ä¸´å¤æ‚æ€§å¢åŠ æ—¶æ€§èƒ½ä¸‹é™ï¼Œè¡¨æ˜å½“å‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>æ¨ç†å¢å¼ºè®­ç»ƒæ˜¯å…‹æœè¿™äº›å±€é™çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†é£æ ¼è½¬æ¢ä»æ˜¯æœ€å¤§çš„æŒ‘æˆ˜ã€‚</li>
<li>SVGeniusä¸ºSVGå¤„ç†æä¾›äº†é¦–ä¸ªç³»ç»Ÿè¯„ä¼°æ¡†æ¶ï¼Œå¯¹å¼€å‘æ›´å¼ºå¤§çš„çŸ¢é‡å›¾å½¢æ¨¡å‹å’Œæ¨åŠ¨è‡ªåŠ¨åŒ–å›¾å½¢è®¾è®¡åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03139">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58a6d72117ec17bf46d48d48f3da4683.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-911785cf02b8ecad94363a3c6f0cdf03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e415562d2f3506f61cb037033c32ee98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e805377ad9d45bd94e33af76afcc5793.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0960e4b845c2c4d2f69824e234edb66.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Co-Evolving-LLM-Coder-and-Unit-Tester-via-Reinforcement-Learning"><a href="#Co-Evolving-LLM-Coder-and-Unit-Tester-via-Reinforcement-Learning" class="headerlink" title="Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning"></a>Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning</h2><p><strong>Authors:Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, Mengdi Wang</strong></p>
<p>We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coderâ€™s mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/CURE">https://github.com/Gen-Verse/CURE</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†CUREï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå…·æœ‰ä¸“é—¨çš„å¥–åŠ±è®¾è®¡ï¼Œèƒ½å¤ŸååŒè¿›åŒ–ç¼–ç å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆèƒ½åŠ›ï¼ŒåŸºäºå®ƒä»¬çš„äº¤äº’ç»“æœï¼Œè€Œæ— éœ€ä»»ä½•çœŸå®ä»£ç è¿›è¡Œç›‘ç£ã€‚è¿™ç§æ–¹æ³•ä½¿çµæ´»å’Œå¯æ‰©å±•çš„è®­ç»ƒæˆä¸ºå¯èƒ½ï¼Œå¹¶å…è®¸å•å…ƒæµ‹è¯•å™¨ç›´æ¥ä»ç¼–ç å™¨çš„é”™è¯¯ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨ReasonFlux-Coder-7Bå’Œ14Bæ¨¡å‹ä¸Šçš„ä¼˜åŒ–åï¼Œåœ¨Qwen2.5-Instructæ¨¡å‹ä¸Šæé«˜äº†ä»£ç ç”Ÿæˆç²¾åº¦5.3%ï¼Œæœ€ä½³Nç²¾åº¦æé«˜9.0%ï¼Œè¶…è¶Šäº†ç±»ä¼¼è§„æ¨¡çš„Qwen-Coderã€DeepSeek-Coderå’ŒSeed-Coderã€‚å®ƒä»¬è‡ªç„¶åœ°æ‰©å±•åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚æµ‹è¯•æ—¶ç¼©æ”¾å’Œæ™ºèƒ½ç¼–ç ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æå‡äº†8.1%ã€‚å¯¹äºé•¿CoTæ¨¡å‹ï¼Œæˆ‘ä»¬çš„ReasonFlux-Coder-4Bå§‹ç»ˆä¼˜äºQwen3-4Bï¼ŒåŒæ—¶åœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆæ–¹é¢è¾¾åˆ°64.8%çš„æ¨ç†æ•ˆç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¿˜å‘ç°æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ä½œä¸ºåŸºç¡€æ¨¡å‹ä¸Šå¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆå¥–åŠ±æ¨¡å‹ã€‚é¡¹ç›®åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/CURE">https://github.com/Gen-Verse/CURE</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03136v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/CURE">https://github.com/Gen-Verse/CURE</a></p>
<p><strong>Summary</strong></p>
<p>CUREæ˜¯ä¸€ä¸ªæ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„å¥–åŠ±æœºåˆ¶ï¼Œä½¿ç¼–ç å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆèƒ½åŠ›èƒ½å¤Ÿæ ¹æ®äº¤äº’ç»“æœååŒè¿›åŒ–ï¼Œæ— éœ€ä»»ä½•çœŸå®ä»£ç ä½œä¸ºç›‘ç£ã€‚æ­¤æ–¹æ³•å®ç°äº†çµæ´»ä¸”å¯æ‰©å±•çš„è®­ç»ƒï¼Œå¹¶å…è®¸æµ‹è¯•äººå‘˜ç›´æ¥ä»å¼€å‘äººå‘˜çš„é”™è¯¯ä¸­å­¦ä¹ ã€‚ä¼˜åŒ–åçš„ReasonFlux-Coderæ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå‡†ç¡®æ€§å’ŒBest-of-Nå‡†ç¡®æ€§æ–¹é¢åˆ†åˆ«æé«˜äº†5.3%å’Œ9.0%ï¼Œå¹¶æ‰©å±•è‡³ä¸‹æ¸¸ä»»åŠ¡å¦‚æµ‹è¯•æ—¶ç¼©æ”¾å’Œä»£ç†ç¼–ç ï¼Œå®ç°äº†å¯¹åŸºå‡†æ¨¡å‹çš„8.1%æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å¯ä½œä¸ºå¼ºåŒ–å­¦ä¹ åŸºå‡†æ¨¡å‹çš„æœ‰æ•ˆå¥–åŠ±æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CUREæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¥–åŠ±æœºåˆ¶ååŒè¿›åŒ–ç¼–ç å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆã€‚</li>
<li>è¯¥æ¡†æ¶æ— éœ€çœŸå®ä»£ç ç›‘ç£ï¼Œå…è®¸æµ‹è¯•äººå‘˜ä»å¼€å‘äººå‘˜çš„é”™è¯¯ä¸­å­¦ä¹ ã€‚</li>
<li>ReasonFlux-Coderæ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå’ŒBest-of-Nå‡†ç¡®æ€§æ–¹é¢æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>è¯¥æ¨¡å‹å¯æ‰©å±•åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚æµ‹è¯•æ—¶ç¼©æ”¾å’Œä»£ç†ç¼–ç ï¼Œå®ç°å¯¹åŸºå‡†æ¨¡å‹çš„æ”¹è¿›ã€‚</li>
<li>CUREæ¨¡å‹è¡¨ç°å‡ºè¾ƒé«˜çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>CUREæ¨¡å‹å¯ä½œä¸ºå¼ºåŒ–å­¦ä¹ åŸºå‡†æ¨¡å‹çš„æœ‰æ•ˆå¥–åŠ±æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-902559561311a1ca4b03ac49b0afdc0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a99ff1ce497b3f2ad3213571f37ce060.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41a51ccbf880323c0c06901e8dee4b2a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Native-Resolution-Image-Synthesis"><a href="#Native-Resolution-Image-Synthesis" class="headerlink" title="Native-Resolution Image Synthesis"></a>Native-Resolution Image Synthesis</h2><p><strong>Authors:Zidong Wang, Lei Bai, Xiangyu Yue, Wanli Ouyang, Yiyuan Zhang</strong></p>
<p>We introduce native-resolution image synthesis, a novel generative modeling paradigm that enables the synthesis of images at arbitrary resolutions and aspect ratios. This approach overcomes the limitations of conventional fixed-resolution, square-image methods by natively handling variable-length visual tokens, a core challenge for traditional techniques. To this end, we introduce the Native-resolution diffusion Transformer (NiT), an architecture designed to explicitly model varying resolutions and aspect ratios within its denoising process. Free from the constraints of fixed formats, NiT learns intrinsic visual distributions from images spanning a broad range of resolutions and aspect ratios. Notably, a single NiT model simultaneously achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512 benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in advanced large language models, NiT, trained solely on ImageNet, demonstrates excellent zero-shot generalization performance. It successfully generates high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536) and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These findings indicate the significant potential of native-resolution modeling as a bridge between visual generative modeling and advanced LLM methodologies. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†åŸç”Ÿåˆ†è¾¨ç‡å›¾åƒåˆæˆï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç”Ÿæˆå»ºæ¨¡èŒƒå¼ï¼Œèƒ½å¤Ÿåˆæˆä»»æ„åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾åƒã€‚è¯¥æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿå›ºå®šåˆ†è¾¨ç‡ã€æ­£æ–¹å½¢å›¾åƒæ–¹æ³•åœ¨å¯å˜é•¿åº¦è§†è§‰ç¬¦å·å¤„ç†æ–¹é¢çš„å±€é™æ€§ï¼Œè¿™æ˜¯ä¼ ç»ŸæŠ€æœ¯é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸç”Ÿåˆ†è¾¨ç‡æ‰©æ•£Transformerï¼ˆNiTï¼‰ï¼Œå…¶æ¶æ„æ—¨åœ¨æ˜¾å¼å»ºæ¨¡å…¶å»å™ªè¿‡ç¨‹ä¸­çš„å„ç§åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”ã€‚æ‘†è„±å›ºå®šæ ¼å¼çš„çº¦æŸï¼ŒNiTä»è·¨è¶Šå¹¿æ³›åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾åƒä¸­å­¦ä¹ å†…åœ¨è§†è§‰åˆ†å¸ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸€ä¸ªå•ä¸€çš„NiTæ¨¡å‹åŒæ—¶åœ¨ImageNet-256x256å’Œ512x512åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œç±»ä¼¼äºé«˜çº§å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çœ‹åˆ°çš„å¼ºå¤§é›¶æ ·æœ¬èƒ½åŠ›ï¼ŒNiTä»…åœ¨ImageNetä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¾¿è¡¨ç°å‡ºäº†å‡ºè‰²çš„é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½ã€‚å®ƒæˆåŠŸåœ°ç”Ÿæˆäº†å…ˆå‰æœªè§çš„é«˜åˆ†è¾¨ç‡ï¼ˆä¾‹å¦‚1536x1536ï¼‰å’Œå¤šç§é•¿å®½æ¯”ï¼ˆä¾‹å¦‚16:9ã€3:1ã€4:3ï¼‰çš„é«˜ä¿çœŸå›¾åƒï¼Œå¦‚å›¾1æ‰€ç¤ºã€‚è¿™äº›å‘ç°è¡¨æ˜åŸç”Ÿåˆ†è¾¨ç‡å»ºæ¨¡ä½œä¸ºè¿æ¥è§†è§‰ç”Ÿæˆå»ºæ¨¡å’Œé«˜çº§LLMæ–¹æ³•ä¹‹é—´çš„æ¡¥æ¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03131v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://wzdthu.github.io/NiT/">https://wzdthu.github.io/NiT/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°å‹å›¾åƒç”Ÿæˆå»ºæ¨¡æ–¹æ³•â€”â€”åŸç”Ÿåˆ†è¾¨ç‡å›¾åƒåˆæˆã€‚è¯¥æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿå›ºå®šåˆ†è¾¨ç‡å’Œæ­£æ–¹å½¢å›¾åƒæ–¹æ³•çš„å±€é™æ€§ï¼Œèƒ½å¤ŸåŸç”Ÿå¤„ç†å¯å˜é•¿åº¦çš„è§†è§‰æ ‡è®°ï¼Œå®ç°äº†ä»»æ„åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾åƒåˆæˆã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†åŸç”Ÿåˆ†è¾¨ç‡æ‰©æ•£Transformerï¼ˆNiTï¼‰æ¶æ„ï¼Œå…¶é™å™ªè¿‡ç¨‹ä¸­æ˜¾å¼åœ°å»ºæ¨¡äº†å„ç§åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”ã€‚NiTæ¨¡å‹ä»å¹¿æ³›çš„åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾åƒä¸­å­¦ä¹ å†…åœ¨è§†è§‰åˆ†å¸ƒï¼Œå¹¶åŒæ—¶è¾¾åˆ°ImageNet-256x256å’Œ512x512åŸºå‡†æµ‹è¯•çš„ä¸–ç•Œçº§æ°´å¹³ã€‚æ­¤å¤–ï¼ŒNiTå±•ç¤ºäº†ä»¤äººæƒŠè®¶çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æœªè§è¿‡çš„é«˜åˆ†è¾¨ç‡ï¼ˆå¦‚1536x1536ï¼‰å’Œå¤šç§é•¿å®½æ¯”ä¸‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚è¿™æ˜¾ç¤ºäº†åŸç”Ÿåˆ†è¾¨ç‡å»ºæ¨¡ä½œä¸ºè§†è§‰ç”Ÿæˆå»ºæ¨¡ä¸é«˜çº§LLMæ–¹æ³•ä¹‹é—´çš„æ¡¥æ¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥åŸç”Ÿåˆ†è¾¨ç‡å›¾åƒåˆæˆï¼Œå®ç°ä»»æ„åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾åƒåˆæˆã€‚</li>
<li>å…‹æœä¼ ç»Ÿå›ºå®šåˆ†è¾¨ç‡å’Œæ­£æ–¹å½¢å›¾åƒæ–¹æ³•çš„å±€é™æ€§ï¼ŒåŸç”Ÿå¤„ç†å¯å˜é•¿åº¦çš„è§†è§‰æ ‡è®°ã€‚</li>
<li>å¼•å…¥NiTæ¶æ„ï¼Œæ˜¾å¼å»ºæ¨¡å„ç§åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾åƒã€‚</li>
<li>NiTæ¨¡å‹ä»å¹¿æ³›åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”çš„å›¾åƒä¸­å­¦ä¹ å†…åœ¨è§†è§‰åˆ†å¸ƒã€‚</li>
<li>NiTåœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°ä¸–ç•Œçº§æ°´å¹³ï¼ŒåŒæ—¶æ”¯æŒå¤šç§åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”ã€‚</li>
<li>NiTå±•ç¤ºå‡ºä»¤äººæƒŠè®¶çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨æœªè§è¿‡çš„åˆ†è¾¨ç‡å’Œé•¿å®½æ¯”ä¸‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-929d19aea750e5079c352a3187a9a4ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70e239deb8da10663506e6c4d0859756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d251326e6696fddd7af2ff4e7ef2ebb2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AUTOCIRCUIT-RL-Reinforcement-Learning-Driven-LLM-for-Automated-Circuit-Topology-Generation"><a href="#AUTOCIRCUIT-RL-Reinforcement-Learning-Driven-LLM-for-Automated-Circuit-Topology-Generation" class="headerlink" title="AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit   Topology Generation"></a>AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit   Topology Generation</h2><p><strong>Authors:Prashanth Vijayaraghavan, Luyao Shi, Ehsan Degan, Vandana Mukherjee, Xin Zhang</strong></p>
<p>Analog circuit topology synthesis is integral to Electronic Design Automation (EDA), enabling the automated creation of circuit structures tailored to specific design requirements. However, the vast design search space and strict constraint adherence make efficient synthesis challenging. Leveraging the versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel reinforcement learning (RL)-based framework for automated analog circuit synthesis. The framework operates in two phases: instruction tuning, where an LLM learns to generate circuit topologies from structured prompts encoding design constraints, and RL refinement, which further improves the instruction-tuned model using reward models that evaluate validity, efficiency, and output voltage. The refined model is then used directly to generate topologies that satisfy the design constraints. Empirical results show that AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by ~14% compared to the best baselines, while reducing duplicate generation rates by ~38%. It achieves over 60% success in synthesizing valid circuits with limited training data, demonstrating strong generalization. These findings highlight the frameworkâ€™s effectiveness in scaling to complex circuits while maintaining efficiency and constraint adherence, marking a significant advancement in AI-driven circuit design. </p>
<blockquote>
<p>æ¨¡æ‹Ÿç”µè·¯æ‹“æ‰‘åˆæˆæ˜¯ç”µå­è®¾è®¡è‡ªåŠ¨åŒ–ï¼ˆEDAï¼‰çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œèƒ½å¤Ÿå®ç°é’ˆå¯¹ç‰¹å®šè®¾è®¡è¦æ±‚çš„ç”µè·¯ç»“æ„çš„è‡ªåŠ¨åŒ–åˆ›å»ºã€‚ç„¶è€Œï¼Œåºå¤§çš„è®¾è®¡æœç´¢ç©ºé—´å’Œä¸¥æ ¼çº¦æŸçš„é™åˆ¶ä½¿å¾—é«˜æ•ˆçš„åˆæˆé¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€šç”¨æ€§ï¼Œæå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–°å‹è‡ªåŠ¨åŒ–æ¨¡æ‹Ÿç”µè·¯åˆæˆæ¡†æ¶â€”â€”AUTOCIRCUIT-RLã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šæŒ‡ä»¤è°ƒæ•´é˜¶æ®µï¼ŒLLMä»ç¼–ç è®¾è®¡çº¦æŸçš„ç»“æ„åŒ–æç¤ºä¸­å­¦ä¹ ç”Ÿæˆç”µè·¯æ‹“æ‰‘ï¼›RLç²¾ç‚¼é˜¶æ®µï¼Œä½¿ç”¨è¯„ä¼°æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œè¾“å‡ºç”µå‹çš„å¥–åŠ±æ¨¡å‹è¿›ä¸€æ­¥æ”¹è¿›æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ã€‚æ”¹è¿›åçš„æ¨¡å‹å¯ç›´æ¥ç”¨äºç”Ÿæˆæ»¡è¶³è®¾è®¡çº¦æŸçš„æ‹“æ‰‘ç»“æ„ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼ŒAUTOCIRCUIT-RLç”Ÿæˆçš„æœ‰æ•ˆç”µè·¯æ•°é‡å¢åŠ äº†çº¦12%ï¼Œæ•ˆç‡æé«˜äº†çº¦14%ï¼Œé‡å¤ç”Ÿæˆç‡é™ä½äº†çº¦38%ã€‚åœ¨æœ‰é™è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ƒæˆåŠŸåˆæˆæœ‰æ•ˆç”µè·¯çš„æ¯”ä¾‹è¶…è¿‡60%ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°çªæ˜¾äº†è¯¥æ¡†æ¶åœ¨æ‰©å±•åˆ°å¤æ‚ç”µè·¯æ—¶ä¿æŒæ•ˆç‡å’Œçº¦æŸéµå®ˆçš„æœ‰æ•ˆæ€§ï¼Œæ ‡å¿—ç€äººå·¥æ™ºèƒ½é©±åŠ¨ç”µè·¯è®¾è®¡çš„ä¸€ä¸ªé‡å¤§è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03122v1">PDF</a> 9 Pages (Content), 4 Pages (Appendix), 7 figures, ICMLâ€™2025</p>
<p><strong>Summary</strong>ï¼šåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶â€”â€”AUTOCIRCUIT-RLåœ¨æ¨¡æ‹Ÿç”µè·¯è®¾è®¡è‡ªåŠ¨åŒ–ä¸­å…·æœ‰çªå‡ºè¡¨ç°ã€‚æ¡†æ¶åŒ…æ‹¬æŒ‡ä»¤å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¸¤ä¸ªé˜¶æ®µï¼Œæé«˜äº†æ¨¡æ‹Ÿç”µè·¯è®¾è®¡çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“æ„åŒ–æç¤ºç¼–ç è®¾è®¡çº¦æŸï¼Œå­¦ä¹ ç”Ÿæˆç”µè·¯æ‹“æ‰‘ç»“æ„ï¼Œå¹¶åˆ©ç”¨å¥–åŠ±æ¨¡å‹è¯„ä¼°å…¶åˆç†æ€§ã€æ•ˆç‡å’Œè¾“å‡ºç”µå‹è¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚åœ¨å¤æ‚ç”µè·¯è®¾è®¡ä¸­ï¼Œè¯¥æ¡†æ¶å±•ç°å‡ºè‰¯å¥½çš„æ‰©å±•æ€§ã€æ•ˆç‡å’Œçº¦æŸéµå®ˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AUTOCIRCUIT-RLæ˜¯ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿç”µè·¯è®¾è®¡è‡ªåŠ¨åŒ–ã€‚</li>
<li>æ¡†æ¶åŒ…å«æŒ‡ä»¤å¾®è°ƒé˜¶æ®µï¼Œé€šè¿‡ç»“æ„åŒ–æç¤ºå­¦ä¹ ç”Ÿæˆæ»¡è¶³è®¾è®¡çº¦æŸçš„ç”µè·¯æ‹“æ‰‘ã€‚</li>
<li>RLä¼˜åŒ–é˜¶æ®µåˆ©ç”¨å¥–åŠ±æ¨¡å‹è¯„ä¼°ç”µè·¯çš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œè¾“å‡ºç”µå‹ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œèƒ½ç”Ÿæˆçº¦12%æ›´å¤šçš„æœ‰æ•ˆç”µè·¯ï¼Œæé«˜çº¦14%çš„æ•ˆç‡ï¼Œå¹¶é™ä½çº¦38%çš„é‡å¤ç”Ÿæˆç‡ã€‚</li>
<li>åœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸‹ï¼Œè¯¥æ¡†æ¶æˆåŠŸåˆæˆæœ‰æ•ˆç”µè·¯çš„æ¯”ä¾‹è¾¾åˆ°60%ä»¥ä¸Šï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>AUTOCIRCUIT-RLåœ¨å¤æ‚ç”µè·¯è®¾è®¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ‰©å±•æ€§ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆç‡å’Œå¯¹è®¾è®¡çº¦æŸçš„éµå®ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dba09cc14b1e9c04d4fbd3ede252a7fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd7befe39c2479dfcc7a9ee41111da77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d05d280c668e1115f804690f8acbc816.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Critique-GRPO-Advancing-LLM-Reasoning-with-Natural-Language-and-Numerical-Feedback"><a href="#Critique-GRPO-Advancing-LLM-Reasoning-with-Natural-Language-and-Numerical-Feedback" class="headerlink" title="Critique-GRPO: Advancing LLM Reasoning with Natural Language and   Numerical Feedback"></a>Critique-GRPO: Advancing LLM Reasoning with Natural Language and   Numerical Feedback</h2><p><strong>Authors:Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chao Yang, Helen Meng</strong></p>
<p>Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ•°å€¼åé¦ˆæ–¹é¢çš„è¿›å±•ï¼Œå¦‚æ ‡é‡å¥–åŠ±ï¼Œæå¤§åœ°å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°äº†ä»…ä½¿ç”¨æ•°å€¼åé¦ˆçš„RLæ‰€é¢ä¸´çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šæ€§èƒ½é«˜åŸæœŸã€è‡ªæˆ‘åæ€çš„å±€é™æ€§ä»¥åŠæŒç»­å¤±è´¥çš„é—®é¢˜ã€‚æ¥ç€æˆ‘ä»¬è¯æ˜ï¼Œå³ä½¿åœ¨æ€§èƒ½é«˜åŸæœŸåï¼Œé€šè¿‡åˆ©ç”¨æ‰¹åˆ¤å½¢å¼çš„è‡ªç„¶è¯­è¨€åé¦ˆï¼ŒRLå¾®è°ƒè¿‡çš„æ¨¡å‹ä¹Ÿèƒ½å¯¹æŒç»­å¤±è´¥çš„é—®é¢˜è¿›è¡Œæ­£ç¡®çš„æ”¹è¿›ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†Critique-GRPOï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨çº¿RLæ¡†æ¶ï¼Œå®ƒç»“åˆäº†è‡ªç„¶è¯­è¨€åé¦ˆå’Œæ•°å€¼åé¦ˆï¼Œä»¥å®ç°æœ‰æ•ˆçš„ç­–ç•¥ä¼˜åŒ–ã€‚Critique-GRPOä½¿LLMèƒ½å¤ŸåŒæ—¶ä»åˆå§‹å“åº”å’Œæ‰¹åˆ¤æŒ‡å¯¼çš„æ”¹è¿›ä¸­å­¦ä¹ ï¼ŒåŒæ—¶ä¿æŒæ¢ç´¢ã€‚ä½¿ç”¨Qwen2.5-7B-Baseå’ŒQwen3-8B-Baseçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCritique-GRPOåœ¨å…«ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦ã€STEMå’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºåŸºäºç›‘ç£å­¦ä¹ å’ŒRLçš„å¾®è°ƒæ–¹æ³•ï¼Œå¹³å‡pass@1å¾—åˆ†åˆ†åˆ«æé«˜äº†çº¦4.5%å’Œ5%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒCritique-GRPOè¶…è¶Šäº†ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿ï¼Œè¯¥åŸºçº¿ç»“åˆäº†åœ¨çº¿RLä¸­çš„ä¸“å®¶æ¼”ç¤ºã€‚è¿›ä¸€æ­¥çš„åˆ†ææ­ç¤ºäº†å…³äºç­–ç•¥æ¢ç´¢çš„ä¸¤ä¸ªå…³é”®è§è§£ï¼šï¼ˆ1ï¼‰é«˜ç†µå¹¶ä¸æ€»æ˜¯ä¿è¯ä»æ¢ç´¢ä¸­æœ‰æ•ˆå­¦ä¹ ï¼Œï¼ˆ2ï¼‰æ›´é•¿çš„å“åº”å¹¶ä¸ä¸€å®šèƒ½å¯¼è‡´æ›´æœ‰æ•ˆçš„æ¢ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03106v1">PDF</a> 38 pages</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆæ•°å€¼åé¦ˆåœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒRLä»…é€šè¿‡æ•°å€¼åé¦ˆé¢ä¸´æ€§èƒ½ç“¶é¢ˆã€è‡ªæˆ‘åæ€å—é™å’ŒæŒç»­å¤±è´¥ç­‰ä¸‰å¤§æŒ‘æˆ˜ã€‚é€šè¿‡ç»“åˆè‡ªç„¶è¯­è¨€åé¦ˆçš„å½¢å¼â€”â€”æ‰¹åˆ¤ï¼Œå³ä½¿åœ¨æ€§èƒ½ç“¶é¢ˆåï¼ŒRLå¾®è°ƒæ¨¡å‹ä¹Ÿèƒ½å¯¹æŒç»­å¤±è´¥çš„é—®é¢˜äº§ç”Ÿæ­£ç¡®çš„æ”¹è¿›ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ç»“åˆè‡ªç„¶è¯­è¨€ä¸æ•°å€¼åé¦ˆçš„åœ¨çº¿RLæ¡†æ¶â€”â€”Critique-GRPOï¼Œç”¨äºæœ‰æ•ˆçš„ç­–ç•¥ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒCritique-GRPOåœ¨å¤šä¸ªæ•°å­¦ã€STEMå’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºåŸºäºç›‘ç£å­¦ä¹ å’ŒRLçš„å¾®è°ƒæ–¹æ³•ï¼Œå¹³å‡æé«˜çº¦4.5%å’Œ5%ã€‚å…³é”®çš„æ˜¯ï¼ŒCritique-GRPOè¶…è¶Šäº†ç»“åˆä¸“å®¶æ¼”ç¤ºçš„åœ¨çº¿RLåŸºçº¿ã€‚å¯¹ç­–ç•¥æ¢ç´¢çš„è¿›ä¸€æ­¥åˆ†ææ­ç¤ºäº†ä¸¤ä¸ªå…³é”®è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç»“åˆæ•°å€¼åé¦ˆå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å•çº¯ä¾èµ–æ•°å€¼åé¦ˆçš„å¼ºåŒ–å­¦ä¹ é¢ä¸´æ€§èƒ½ç“¶é¢ˆã€è‡ªæˆ‘åæ€å—é™å’ŒæŒç»­å¤±è´¥ç­‰æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨è‡ªç„¶è¯­è¨€åé¦ˆï¼ˆå¦‚æ‰¹åˆ¤ï¼‰å¯ä»¥å¸®åŠ©RLå¾®è°ƒæ¨¡å‹åœ¨æ€§èƒ½ç“¶é¢ˆåæ”¹è¿›æŒç»­å¤±è´¥çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„Critique-GRPOæ¡†æ¶ç»“åˆäº†è‡ªç„¶è¯­è¨€ä¸æ•°å€¼åé¦ˆï¼Œå®ç°äº†æœ‰æ•ˆçš„ç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCritique-GRPOåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹³å‡æé«˜çº¦4.5%å’Œ5%ã€‚</li>
<li>Critique-GRPOè¶…è¶Šäº†ç»“åˆä¸“å®¶æ¼”ç¤ºçš„åœ¨çº¿RLåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a506a283475479e005cfbc2b64114b64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09fdf08c658752002353e3fafd60c147.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-746679213523921763c70312f3a76f46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237e1b794a8c4695c7c5052ec050975a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TalkingMachines-Real-Time-Audio-Driven-FaceTime-Style-Video-via-Autoregressive-Diffusion-Models"><a href="#TalkingMachines-Real-Time-Audio-Driven-FaceTime-Style-Video-via-Autoregressive-Diffusion-Models" class="headerlink" title="TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via   Autoregressive Diffusion Models"></a>TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via   Autoregressive Diffusion Models</h2><p><strong>Authors:Chetwin Low, Weimin Wang</strong></p>
<p>In this paper, we present TalkingMachines â€“ an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - <a target="_blank" rel="noopener" href="https://aaxwaz.github.io/TalkingMachines/">https://aaxwaz.github.io/TalkingMachines/</a> </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†TalkingMachinesâ€”â€”ä¸€ä¸ªé«˜æ•ˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºå®æ—¶éŸ³é¢‘é©±åŠ¨çš„è§’è‰²åŠ¨ç”»ã€‚TalkingMachinesé€šè¿‡æ•´åˆéŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æˆ‘ä»¬çš„è§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹ï¼Œå®ç°äº†è‡ªç„¶çš„å¯¹è¯ä½“éªŒã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬å°†æœ€å…ˆè¿›çš„å›¾åƒåˆ°è§†é¢‘DiTæ¨¡å‹è½¬åŒ–ä¸º18äº¿å‚æ•°çš„éŸ³é¢‘é©±åŠ¨åŒ–èº«ç”Ÿæˆæ¨¡å‹ï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬åˆ©ç”¨ä»åŒå‘æ•™å¸ˆæ¨¡å‹åˆ°ç¨€ç–å› æœè‡ªå›å½’å­¦ç”Ÿæ¨¡å‹çš„ä¸å¯¹ç§°çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œå®ç°äº†æ— è¯¯å·®ç´¯ç§¯çš„æ— é™è§†é¢‘æµï¼›ï¼ˆ3ï¼‰æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé«˜ååé‡å’Œä½å»¶è¿Ÿçš„æ¨ç†ç®¡é“ï¼Œå…¶ä¸­åŒ…æ‹¬å‡ é¡¹å…³é”®å·¥ç¨‹ä¼˜åŒ–ï¼Œå¦‚ï¼ˆaï¼‰åœ¨å•ç‹¬çš„è®¾å¤‡ä¸Šåˆ†æ•£DiTå’ŒVAEè§£ç å™¨ï¼Œï¼ˆbï¼‰ä½¿ç”¨CUDAæµæœ‰æ•ˆåœ°é‡å è®¾å¤‡é—´é€šä¿¡å’Œè®¡ç®—ï¼Œï¼ˆcï¼‰æ¶ˆé™¤å†—ä½™çš„é‡æ–°è®¡ç®—ï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜å¸§ç”Ÿæˆååé‡ã€‚è¯·å‚é˜…æ¼”ç¤ºè§†é¢‘ï¼š<a target="_blank" rel="noopener" href="https://aaxwaz.github.io/TalkingMachines/%EF%BC%88%E7%82%B9%E5%87%BB%E9%93%BE%E6%8E%A5%E6%9F%A5%E7%9C%8B%EF%BC%89">https://aaxwaz.github.io/TalkingMachines/ï¼ˆç‚¹å‡»é“¾æ¥æŸ¥çœ‹ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03099v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>TalkingMachinesæ˜¯ä¸€ä¸ªå°†é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºå®æ—¶éŸ³é¢‘é©±åŠ¨çš„è§’è‰²åŠ¨ç”»æ¡†æ¶ã€‚å®ƒé€šè¿‡æ•´åˆéŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡éŸ³é¢‘æ§åˆ¶è§†é¢‘è§’è‰²åŠ¨ç”»ï¼Œå®ç°è‡ªç„¶å¯¹è¯ä½“éªŒã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šé€‚åº”å…ˆè¿›å›¾åƒåˆ°è§†é¢‘DiTæ¨¡å‹ä¸ºéŸ³é¢‘é©±åŠ¨è§’è‰²ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°æ— é™è§†é¢‘æµä¸”æ— è¯¯å·®ç´¯ç§¯ï¼Œè®¾è®¡é«˜æ•ˆä½å»¶è¿Ÿæ¨ç†ç®¡é“ï¼ŒåŒ…æ‹¬è·¨è®¾å¤‡åˆ†è§£æ¨¡å‹å’Œé«˜æ•ˆé‡å é€šä¿¡è®¡ç®—ç­‰ä¼˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TalkingMachinesæ˜¯ä¸€ä¸ªå°†é¢„è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºéŸ³é¢‘é©±åŠ¨è§’è‰²åŠ¨ç”»çš„æ¡†æ¶ï¼Œæä¾›è‡ªç„¶å¯¹è¯ä½“éªŒã€‚</li>
<li>è¯¥æ¡†æ¶æ•´åˆäº†éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>æ¡†æ¶æˆåŠŸé€‚åº”å›¾åƒåˆ°è§†é¢‘çš„DiTæ¨¡å‹ç”¨äºéŸ³é¢‘é©±åŠ¨è§’è‰²ç”Ÿæˆã€‚</li>
<li>TalkingMachineså®ç°äº†æ— é™è§†é¢‘æµæ’­æ”¾ä¸”æ— è¯¯å·®ç´¯ç§¯ã€‚</li>
<li>è®¾è®¡äº†é«˜æ•ˆä½å»¶è¿Ÿçš„æ¨ç†ç®¡é“ï¼Œæ”¯æŒè·¨è®¾å¤‡æ¨¡å‹åˆ†è§£å’Œé€šä¿¡è®¡ç®—ä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡ä¸å¯¹ç§°çŸ¥è¯†è’¸é¦æŠ€æœ¯ä»åŒå‘æ•™å¸ˆæ¨¡å‹åˆ°ç¨€ç–å› æœè‡ªå›å½’å­¦ç”Ÿæ¨¡å‹ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e88bbe6307ae82f0717ada0b71c9f76e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fb8015874355c492456eb37cc85db06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb0e86075fab363d9cdfd4f959e7217.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DPO-Learning-with-LLMs-Judge-Signal-for-Computer-Use-Agents"><a href="#DPO-Learning-with-LLMs-Judge-Signal-for-Computer-Use-Agents" class="headerlink" title="DPO Learning with LLMs-Judge Signal for Computer Use Agents"></a>DPO Learning with LLMs-Judge Signal for Computer Use Agents</h2><p><strong>Authors:Man Luo, David Cobbley, Xin Su, Shachar Rosenman, Vasudev Lal, Shao-Yen Tseng, Phillip Howard</strong></p>
<p>Computer use agents (CUA) are systems that automatically interact with graphical user interfaces (GUIs) to complete tasks. CUA have made significant progress with the advent of large vision-language models (VLMs). However, these agents typically rely on cloud-based inference with substantial compute demands, raising critical privacy and scalability concerns, especially when operating on personal devices. In this work, we take a step toward privacy-preserving and resource-efficient agents by developing a lightweight vision-language model that runs entirely on local machines. To train this compact agent, we introduce an LLM-as-Judge framework that automatically evaluates and filters synthetic interaction trajectories, producing high-quality data for reinforcement learning without human annotation. Experiments on the OS-World benchmark demonstrate that our fine-tuned local model outperforms existing baselines, highlighting a promising path toward private, efficient, and generalizable GUI agents. </p>
<blockquote>
<p>è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰æ˜¯è‡ªåŠ¨ä¸å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰äº¤äº’ä»¥å®Œæˆä»»åŠ¡çš„ç³»ç»Ÿã€‚éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å‡ºç°ï¼ŒCUAå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›ä»£ç†é€šå¸¸ä¾èµ–äºå…·æœ‰å¤§é‡è®¡ç®—éœ€æ±‚çš„äº‘æ¨ç†ï¼Œè¿™å¼•å‘äº†å…³äºéšç§å’Œå¯æ‰©å±•æ€§çš„å…³é”®é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸ªäººè®¾å¤‡ä¸Šè¿è¡Œæ—¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼€å‘ä¸€ç§å®Œå…¨è¿è¡Œåœ¨æœ¬åœ°æœºå™¨ä¸Šçš„è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæœç€ä¿æŠ¤éšç§å’Œèµ„æºé«˜æ•ˆçš„ä»£ç†è¿ˆå‡ºäº†ä¸€æ­¥ã€‚ä¸ºäº†è®­ç»ƒè¿™ç§ç´§å‡‘çš„ä»£ç†ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºâ€œLLM-as-Judgeâ€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è‡ªåŠ¨è¯„ä¼°å¹¶è¿‡æ»¤åˆæˆäº¤äº’è½¨è¿¹ï¼Œä»è€Œåœ¨æ— éœ€äººå·¥æ³¨é‡Šçš„æƒ…å†µä¸‹ä¸ºå¼ºåŒ–å­¦ä¹ ç”Ÿæˆé«˜è´¨é‡æ•°æ®ã€‚åœ¨OS-WorldåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬å¾®è°ƒçš„æœ¬åœ°æ¨¡å‹è¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œçªæ˜¾å‡ºäº†ä¸€æ¡æœç€ç§å¯†ã€é«˜æ•ˆå’Œå¯é€šç”¨çš„GUIä»£ç†çš„å……æ»¡å¸Œæœ›çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03095v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œè®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰åœ¨è‡ªåŠ¨ä¸å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰äº¤äº’å®Œæˆä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›ä»£ç†é€šå¸¸ä¾èµ–äºå…·æœ‰å¤§é‡è®¡ç®—éœ€æ±‚çš„äº‘æ¨ç†ï¼Œè¿™å¼•å‘äº†å…³äºéšç§å’Œå¯æ‰©å±•æ€§çš„å…³é”®é—®é¢˜ã€‚æœ¬ç ”ç©¶æœå‘éšç§ä¿æŠ¤å’Œèµ„æºé«˜æ•ˆçš„ä»£ç†è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œé€šè¿‡å¼€å‘å¯åœ¨æœ¬åœ°æœºå™¨ä¸Šå®Œå…¨è¿è¡Œçš„è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹ã€‚ä¸ºäº†è®­ç»ƒè¿™ä¸ªç´§å‡‘çš„ä»£ç†ï¼Œæˆ‘ä»¬å¼•å…¥äº†LLM-as-Judgeæ¡†æ¶ï¼Œè¯¥æ¡†æ¶è‡ªåŠ¨è¯„ä¼°å¹¶è¿‡æ»¤åˆæˆäº¤äº’è½¨è¿¹ï¼Œä»è€Œåœ¨æ— éœ€äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ä¸ºå¼ºåŒ–å­¦ä¹ æä¾›é«˜è´¨é‡æ•°æ®ã€‚åœ¨OS-WorldåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç²¾ç»†æœ¬åœ°æ¨¡å‹è¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼Œä¸ºç§äººã€é«˜æ•ˆå’Œå¯é€šç”¨çš„GUIä»£ç†æŒ‡æ˜äº†æœ‰å‰é€”çš„é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAï¼‰èƒ½è‡ªåŠ¨ä¸å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰äº¤äº’å®Œæˆä»»åŠ¡ã€‚</li>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å‘å±•å¯¹CUAçš„è¿›æ­¥èµ·åˆ°äº†æ¨åŠ¨ä½œç”¨ã€‚</li>
<li>CUAé€šå¸¸ä¾èµ–äº‘æ¨ç†ï¼Œå¼•å‘éšç§å’Œå¯æ‰©å±•æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸ªäººè®¾å¤‡ä¸Šã€‚</li>
<li>ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§å¯åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œçš„è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»¥æé«˜éšç§ä¿æŠ¤å’Œèµ„æºæ•ˆç‡ã€‚</li>
<li>å¼•å…¥LLM-as-Judgeæ¡†æ¶è®­ç»ƒç´§å‡‘ä»£ç†ï¼Œè¯¥æ¡†æ¶èƒ½è‡ªåŠ¨è¯„ä¼°å¹¶è¿‡æ»¤åˆæˆäº¤äº’è½¨è¿¹ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œç²¾ç»†æœ¬åœ°æ¨¡å‹åœ¨OS-WorldåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8c2a5d65ba93fcef44dd05fa1b19fbea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91a59868f89dd3d2ca16d14dde92a0b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e990b6c43d395076a137f3281e4f53b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-588c2ed368f1623d48f8feba6f93dfea.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="StreamBP-Memory-Efficient-Exact-Backpropagation-for-Long-Sequence-Training-of-LLMs"><a href="#StreamBP-Memory-Efficient-Exact-Backpropagation-for-Long-Sequence-Training-of-LLMs" class="headerlink" title="StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence   Training of LLMs"></a>StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence   Training of LLMs</h2><p><strong>Authors:Qijun Luo, Mengqi Li, Lei Zhao, Xiao Li</strong></p>
<p>Training language models on long sequence data is a demanding requirement for enhancing the modelâ€™s capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBPâ€™s sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at <a target="_blank" rel="noopener" href="https://github.com/Ledzy/StreamBP">https://github.com/Ledzy/StreamBP</a>. </p>
<blockquote>
<p>è®­ç»ƒè¯­è¨€æ¨¡å‹å¤„ç†é•¿åºåˆ—æ•°æ®æ˜¯å¯¹å¤æ‚ä»»åŠ¡ï¼ˆå¦‚é•¿é“¾æ¨ç†ï¼‰æå‡æ¨¡å‹èƒ½åŠ›çš„è‹›åˆ»è¦æ±‚ã€‚ç„¶è€Œï¼Œéšç€åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œåå‘ä¼ æ’­ï¼ˆBPï¼‰è¿‡ç¨‹ä¸­å­˜å‚¨æ¿€æ´»å€¼çš„å†…å­˜æˆæœ¬å˜å¾—å·¨å¤§ï¼Œå³ä½¿åº”ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å†…å­˜æ•ˆç‡é«˜ä¸”ç²¾ç¡®çš„åå‘ä¼ æ’­æ–¹æ³•ï¼Œç§°ä¸ºStreamBPã€‚å®ƒé€å±‚æ²¿åºåˆ—ç»´åº¦è¿›è¡Œé“¾å¼æ³•åˆ™çš„çº¿æ€§åˆ†è§£ï¼Œæ˜¾è‘—é™ä½äº†æ¿€æ´»å€¼å’Œlogitsçš„å†…å­˜æˆæœ¬ã€‚æ‰€æå‡ºçš„æ–¹æ³•é€‚ç”¨äºå¸¸è§çš„ç›®æ ‡ï¼Œå¦‚SFTã€GRPOå’ŒDPOã€‚ä»å®ç°çš„è§’åº¦æ¥çœ‹ï¼ŒStreamBPåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å› æœç»“æ„å®ç°äº†è¾ƒå°‘çš„è®¡ç®—FLOPså’Œæ›´å¿«çš„BPé€Ÿåº¦ã€‚ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹ç›¸æ¯”ï¼ŒStreamBPå°†BPçš„æœ€å¤§åºåˆ—é•¿åº¦æ‰©å¤§äº†2.8-5.5å€ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ—¶é—´ç›¸å½“ç”šè‡³æ›´å°‘ã€‚è¯·æ³¨æ„ï¼ŒStreamBPçš„åºåˆ—é•¿åº¦ç¼©æ”¾èƒ½åŠ›å¯ä»¥ç›´æ¥è½¬ç§»åˆ°æ‰¹å¤„ç†å¤§å°ç¼©æ”¾ä»¥åŠ é€Ÿè®­ç»ƒã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§é€šä¿¡é«˜æ•ˆçš„åˆ†å¸ƒå¼StreamBPï¼Œä»¥æœ‰æ•ˆæ”¯æŒå¤šGPUè®­ç»ƒå¹¶æ‰©å¤§å…¶é€‚ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥è½»æ¾åœ°é›†æˆåˆ°ä»»ä½•transformeræ¨¡å‹çš„è®­ç»ƒæµç¨‹ä¸­ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ledzy/StreamBP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ledzy/StreamBPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03077v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºStreamBPçš„å†…å­˜é«˜æ•ˆåå‘ä¼ æ’­æ–¹æ³•ï¼Œé€šè¿‡å¯¹åºåˆ—ç»´åº¦ä¸Šçš„é“¾å¼è§„åˆ™è¿›è¡Œé€å±‚çº¿æ€§åˆ†è§£ï¼Œæ˜¾è‘—é™ä½äº†æ¿€æ´»å€¼å’Œæ—¥å¿—çš„å­˜å‚¨æˆæœ¬ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºå¸¸è§ç›®æ ‡ï¼Œå¦‚SFTã€GRPOå’ŒDPOã€‚StreamBPåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å› æœç»“æ„ï¼Œå®ç°äº†è¾ƒå°‘çš„è®¡ç®—é‡å’Œæ›´å¿«çš„åå‘ä¼ æ’­é€Ÿåº¦ã€‚ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ç›¸æ¯”ï¼ŒStreamBPå¯å°†åå‘ä¼ æ’­çš„æœ€å¤§åºåˆ—é•¿åº¦æé«˜2.8-5.5å€ï¼ŒåŒæ—¶ä½¿ç”¨çš„åå‘ä¼ æ’­æ—¶é—´ç›¸å½“ç”šè‡³æ›´å°‘ã€‚æ­¤å¤–ï¼ŒStreamBPçš„åºåˆ—é•¿åº¦ç¼©æ”¾èƒ½åŠ›å¯ç›´æ¥è½¬æ¢ä¸ºæ‰¹å¤„ç†å¤§å°ç¼©æ”¾ï¼Œä»¥åŠ é€Ÿè®­ç»ƒã€‚è¿˜å¼€å‘äº†é€šä¿¡é«˜æ•ˆçš„åˆ†å¸ƒå¼StreamBPï¼Œä»¥æœ‰æ•ˆæ”¯æŒå¤šGPUè®­ç»ƒå’Œæ‰©å¤§å…¶åº”ç”¨èŒƒå›´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StreamBPæ˜¯ä¸€ç§å†…å­˜é«˜æ•ˆçš„åå‘ä¼ æ’­æ–¹æ³•ï¼Œé€‚ç”¨äºå¤„ç†é•¿åºåˆ—æ•°æ®çš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡é€å±‚çº¿æ€§åˆ†è§£é“¾å¼è§„åˆ™ï¼ŒStreamBPé™ä½äº†æ¿€æ´»å€¼å’Œæ—¥å¿—çš„å­˜å‚¨æˆæœ¬ã€‚</li>
<li>StreamBPé€‚ç”¨äºå¸¸è§ç›®æ ‡ï¼Œå¦‚SFTã€GRPOå’ŒDPOï¼Œå¹¶å¯ä»¥æé«˜åå‘ä¼ æ’­çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚</li>
<li>StreamBPåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å› æœç»“æ„ï¼Œå®ç°äº†è¾ƒå°‘çš„è®¡ç®—é‡å’Œæ›´å¿«çš„åå‘ä¼ æ’­é€Ÿåº¦ã€‚</li>
<li>ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ç›¸æ¯”ï¼ŒStreamBPåœ¨åå‘ä¼ æ’­çš„æœ€å¤§åºåˆ—é•¿åº¦æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>StreamBPçš„åºåˆ—é•¿åº¦ç¼©æ”¾èƒ½åŠ›å¯è½¬æ¢ä¸ºæ‰¹å¤„ç†å¤§å°ç¼©æ”¾ï¼Œä»¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3984fab9e4ed9f1ff1a3f614224d1178.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b573ab45916e33d8df42b945d04dc4b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b10b3514f7590cc86787dc6bc732921c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="EDITOR-Effective-and-Interpretable-Prompt-Inversion-for-Text-to-Image-Diffusion-Models"><a href="#EDITOR-Effective-and-Interpretable-Prompt-Inversion-for-Text-to-Image-Diffusion-Models" class="headerlink" title="EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image   Diffusion Models"></a>EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image   Diffusion Models</h2><p><strong>Authors:Mingzhe Li, Gehao Zhang, Zhenting Wang, Shiqing Ma, Siqi Pan, Richard Cartwright, Juan Zhai</strong></p>
<p>Text-to-image generation models~(e.g., Stable Diffusion) have achieved significant advancements, enabling the creation of high-quality and realistic images based on textual descriptions. Prompt inversion, the task of identifying the textual prompt used to generate a specific artifact, holds significant potential for applications including data attribution, model provenance, and watermarking validation. Recent studies introduced a delayed projection scheme to optimize for prompts representative of the vocabulary space, though challenges in semantic fluency and efficiency remain. Advanced image captioning models or visual large language models can generate highly interpretable prompts, but they often lack in image similarity. In this paper, we propose a prompt inversion technique called \sys for text-to-image diffusion models, which includes initializing embeddings using a pre-trained image captioning model, refining them through reverse-engineering in the latent space, and converting them to texts using an embedding-to-text model. Our experiments on the widely-used datasets, such as MS COCO, LAION, and Flickr, show that our method outperforms existing methods in terms of image similarity, textual alignment, prompt interpretability and generalizability. We further illustrate the application of our generated prompts in tasks such as cross-concept image synthesis, concept manipulation, evolutionary multi-concept generation and unsupervised segmentation. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆä¾‹å¦‚Stable Diffusionï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°åˆ›å»ºé«˜è´¨é‡å’Œé€¼çœŸçš„å›¾åƒã€‚æç¤ºåè½¬ï¼ˆprompt inversionï¼‰æ˜¯è¯†åˆ«ç”¨äºç”Ÿæˆç‰¹å®šè‰ºæœ¯å“çš„æ–‡æœ¬æç¤ºçš„ä»»åŠ¡ï¼Œåœ¨æ•°æ®å½’å±ã€æ¨¡å‹æ¥æºå’Œæ°´å°éªŒè¯ç­‰æ–¹é¢å…·æœ‰å·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚æœ€è¿‘çš„ç ”ç©¶å¼•å…¥äº†ä¸€ç§å»¶è¿ŸæŠ•å½±æ–¹æ¡ˆï¼Œä»¥ä¼˜åŒ–ä»£è¡¨è¯æ±‡ç©ºé—´çš„æç¤ºï¼Œå°½ç®¡åœ¨è¯­ä¹‰æµç•…æ€§å’Œæ•ˆç‡æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚å…ˆè¿›çš„å›¾åƒæ ‡é¢˜æ¨¡å‹æˆ–è§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜åº¦å¯è§£é‡Šçš„æç¤ºï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹å›¾åƒç›¸ä¼¼æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æç¤ºåè½¬æŠ€æœ¯ï¼Œç§°ä¸ºsysã€‚å®ƒåŒ…æ‹¬ä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒæ ‡é¢˜æ¨¡å‹è¿›è¡Œåˆå§‹åŒ–åµŒå…¥ï¼Œé€šè¿‡åå‘å·¥ç¨‹åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œç»†åŒ–ï¼Œå¹¶ä½¿ç”¨åµŒå…¥åˆ°æ–‡æœ¬æ¨¡å‹å°†å®ƒä»¬è½¬æ¢ä¸ºæ–‡æœ¬ã€‚æˆ‘ä»¬åœ¨MS COCOã€LAIONå’ŒFlickrç­‰å¸¸ç”¨æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒç›¸ä¼¼æ€§ã€æ–‡æœ¬å¯¹é½ã€æç¤ºå¯è§£é‡Šæ€§å’Œé€šç”¨æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è¯´æ˜äº†æ‰€ç”Ÿæˆçš„æç¤ºåœ¨è·¨æ¦‚å¿µå›¾åƒåˆæˆã€æ¦‚å¿µæ“ä½œã€è¿›åŒ–å¤šæ¦‚å¿µç”Ÿæˆå’Œæ— ç›‘ç£åˆ†å‰²ç­‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03067v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸçš„å›¾åƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©æ•£æ¨¡å‹çš„æç¤ºåè½¬æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä½¿ç”¨é¢„è®­ç»ƒå›¾åƒæè¿°æ¨¡å‹è¿›è¡Œåˆå§‹åŒ–åµŒå…¥ï¼Œé€šè¿‡åå‘å·¥ç¨‹åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ”¹è¿›ï¼Œå¹¶ä½¿ç”¨åµŒå…¥åˆ°æ–‡æœ¬æ¨¡å‹è¿›è¡Œè½¬æ¢ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒç›¸ä¼¼æ€§ã€æ–‡æœ¬å¯¹é½ã€æç¤ºå¯è§£é‡Šæ€§å’Œé€šç”¨æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å¯ç”¨äºè·¨æ¦‚å¿µå›¾åƒåˆæˆã€æ¦‚å¿µæ“ä½œã€è¿›åŒ–å¤šæ¦‚å¿µç”Ÿæˆå’Œæ— ç›‘ç£åˆ†å‰²ç­‰ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹å¦‚Stable Diffusionå·²èƒ½ç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸçš„å›¾åƒã€‚</li>
<li>æç¤ºåè½¬æŠ€æœ¯ç”¨äºç¡®å®šç”Ÿæˆç‰¹å®šå·¥ä»¶æ‰€ä½¿ç”¨çš„æ–‡æœ¬æç¤ºã€‚</li>
<li>æç¤ºåè½¬æŠ€æœ¯åœ¨æ•°æ®å½’å±ã€æ¨¡å‹æ¥æºå’Œæ°´å°éªŒè¯ç­‰æ–¹é¢æœ‰æ½œåœ¨åº”ç”¨ã€‚</li>
<li>ç°æœ‰å»¶è¿ŸæŠ•å½±æ–¹æ¡ˆåœ¨è¯­ä¹‰æµç•…æ€§å’Œæ•ˆç‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æç¤ºåè½¬æŠ€æœ¯ï¼Œä½¿ç”¨é¢„è®­ç»ƒå›¾åƒæè¿°æ¨¡å‹è¿›è¡ŒåµŒå…¥åˆå§‹åŒ–ï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒç›¸ä¼¼æ€§ã€æ–‡æœ¬å¯¹é½ã€æç¤ºå¯è§£é‡Šæ€§å’Œé€šç”¨æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-43dc2320dc52eb70ed687f21b5cb1450.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b59f01fb34c917f32a5c52ebd2692263.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93bb3495cff649f1f893d5094e248419.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56b0fbec6332768e99af37f3f9242fb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f77bf345060414c540acea76b26ec5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c9f84927f75a818d489330c415f2f06.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Leveraging-Information-Retrieval-to-Enhance-Spoken-Language-Understanding-Prompts-in-Few-Shot-Learning"><a href="#Leveraging-Information-Retrieval-to-Enhance-Spoken-Language-Understanding-Prompts-in-Few-Shot-Learning" class="headerlink" title="Leveraging Information Retrieval to Enhance Spoken Language   Understanding Prompts in Few-Shot Learning"></a>Leveraging Information Retrieval to Enhance Spoken Language   Understanding Prompts in Few-Shot Learning</h2><p><strong>Authors:Pierre Lepagnol, Sahar Ghannay, Thomas Gerald, Christophe Servan, Sophie Rosset</strong></p>
<p>Understanding user queries is fundamental in many applications, such as home assistants, booking systems, or recommendations. Accordingly, it is crucial to develop accurate Spoken Language Understanding (SLU) approaches to ensure the reliability of the considered system. Current State-of-the-Art SLU techniques rely on large amounts of training data; however, only limited annotated examples are available for specific tasks or languages.   In the meantime, instruction-tuned large language models (LLMs) have shown exceptional performance on unseen tasks in a few-shot setting when provided with adequate prompts. In this work, we propose to explore example selection by leveraging Information retrieval (IR) approaches to build an enhanced prompt that is applied to an SLU task. We evaluate the effectiveness of the proposed method on several SLU benchmarks. Experimental results show that lexical IR methods significantly enhance performance without increasing prompt length. </p>
<blockquote>
<p>ç†è§£ç”¨æˆ·æŸ¥è¯¢åœ¨è®¸å¤šåº”ç”¨ä¸­éƒ½æ˜¯è‡³å…³é‡è¦çš„ï¼Œä¾‹å¦‚å®¶åº­åŠ©ç†ã€é¢„è®¢ç³»ç»Ÿæˆ–æ¨èç³»ç»Ÿã€‚å› æ­¤ï¼Œä¸ºäº†ç¡®ä¿æ‰€è€ƒè™‘ç³»ç»Ÿçš„å¯é æ€§ï¼Œå¼€å‘å‡†ç¡®çš„å£è¯­ç†è§£ï¼ˆSLUï¼‰æ–¹æ³•è‡³å…³é‡è¦ã€‚ç›®å‰æœ€å…ˆè¿›çš„SLUæŠ€æœ¯ä¾èµ–äºå¤§é‡çš„è®­ç»ƒæ•°æ®ï¼›ç„¶è€Œï¼Œå¯¹äºç‰¹å®šä»»åŠ¡æˆ–è¯­è¨€ï¼Œå¯ç”¨çš„æ³¨é‡Šç¤ºä¾‹ç›¸å¯¹è¾ƒå°‘ã€‚ä¸æ­¤åŒæ—¶ï¼Œç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æä¾›è¶³å¤Ÿæç¤ºçš„æƒ…å†µä¸‹ï¼Œåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æ–¹æ³•è¿›è¡Œç¤ºä¾‹é€‰æ‹©ï¼Œä»¥æ„å»ºç”¨äºSLUä»»åŠ¡çš„å¢å¼ºæç¤ºã€‚æˆ‘ä»¬åœ¨å¤šä¸ªSLUåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯æ±‡IRæ–¹æ³•åœ¨ä¸å¢åŠ æç¤ºé•¿åº¦çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03035v1">PDF</a> Conference paper accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æ–¹æ³•çš„ç¤ºä¾‹é€‰æ‹©ç­–ç•¥åœ¨æ„å»ºç”¨äºSpoken Language Understandingï¼ˆSLUï¼‰ä»»åŠ¡çš„å¢å¼ºæç¤ºä¸­çš„åº”ç”¨ã€‚åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸‹ï¼Œåˆ©ç”¨æŒ‡ä»¤ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å¢å¼ºæç¤ºæ˜¾ç¤ºäº†å¯¹æœªè§ä»»åŠ¡çš„å‡ºè‰²æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯æ³•IRæ–¹æ³•å¯æ˜¾è‘—æé«˜SLUæ€§èƒ½ï¼Œä¸”ä¸ä¼šå¢åŠ æç¤ºé•¿åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03035">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-73a00fdd24165e6819ef8215714c5283.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd36244eb9d0da7db22cfaf0be5f10d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31a25bf9e1c9bdff0bf34a0f3e57116d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-430a0e9622689bd138e2216081fcff75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c2a30f0ec3e3c8e0e8f331f7e1336b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-180c47f5bf3e9358b004cbf8e97e4280.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TestAgent-An-Adaptive-and-Intelligent-Expert-for-Human-Assessment"><a href="#TestAgent-An-Adaptive-and-Intelligent-Expert-for-Human-Assessment" class="headerlink" title="TestAgent: An Adaptive and Intelligent Expert for Human Assessment"></a>TestAgent: An Adaptive and Intelligent Expert for Human Assessment</h2><p><strong>Authors:Junhao Yu, Yan Zhuang, YuXuan Sun, Weibo Gao, Qi Liu, Mingyue Cheng, Zhenya Huang, Enhong Chen</strong></p>
<p>Accurately assessing internal human states is key to understanding preferences, offering personalized services, and identifying challenges in real-world applications. Originating from psychometrics, adaptive testing has become the mainstream method for human measurement and has now been widely applied in education, healthcare, sports, and sociology. It customizes assessments by selecting the fewest test questions . However, current adaptive testing methods face several challenges. The mechanized nature of most algorithms leads to guessing behavior and difficulties with open-ended questions. Additionally, subjective assessments suffer from noisy response data and coarse-grained test outputs, further limiting their effectiveness. To move closer to an ideal adaptive testing process, we propose TestAgent, a large language model (LLM)-powered agent designed to enhance adaptive testing through interactive engagement. This is the first application of LLMs in adaptive testing. TestAgent supports personalized question selection, captures test-takersâ€™ responses and anomalies, and provides precise outcomes through dynamic, conversational interactions. Experiments on psychological, educational, and lifestyle assessments show our approach achieves more accurate results with 20% fewer questions than state-of-the-art baselines, and testers preferred it in speed, smoothness, and other dimensions. </p>
<blockquote>
<p>ç²¾ç¡®è¯„ä¼°äººç±»å†…éƒ¨çŠ¶æ€æ˜¯ç†è§£åå¥½ã€æä¾›ä¸ªæ€§åŒ–æœåŠ¡ä»¥åŠåœ¨ç°å®ä¸–ç•Œä¸­è¯†åˆ«æŒ‘æˆ˜çš„å…³é”®ã€‚èµ·æºäºå¿ƒç†æµ‹é‡çš„è‡ªé€‚åº”æµ‹è¯•å·²æˆä¸ºäººç±»æµ‹é‡çš„ä¸»æµæ–¹æ³•ï¼Œç°å·²å¹¿æ³›åº”ç”¨äºæ•™è‚²ã€åŒ»ç–—ã€ä½“è‚²å’Œç¤¾ä¼šå­¦é¢†åŸŸã€‚å®ƒé€šè¿‡é€‰æ‹©æœ€å°‘çš„æµ‹è¯•é—®é¢˜æ¥å®šåˆ¶è¯„ä¼°ã€‚ç„¶è€Œï¼Œå½“å‰çš„è‡ªé€‚åº”æµ‹è¯•æ–¹æ³•é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚å¤§å¤šæ•°ç®—æ³•çš„æœºæ¢°åŒ–ç‰¹æ€§å¯¼è‡´çŒœæµ‹è¡Œä¸ºå’Œå¼€æ”¾å¼é—®é¢˜ä¸Šçš„å›°éš¾ã€‚æ­¤å¤–ï¼Œä¸»è§‚è¯„ä¼°å—åˆ°å“åº”æ•°æ®å˜ˆæ‚å’Œæµ‹è¯•è¾“å‡ºç²’åº¦ç²—ç³™çš„é™åˆ¶ï¼Œè¿›ä¸€æ­¥é™ä½äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸ºäº†æ›´æ¥è¿‘ç†æƒ³çš„è‡ªé€‚åº”æµ‹è¯•è¿‡ç¨‹ï¼Œæˆ‘ä»¬æå‡ºäº†TestAgentï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨é€šè¿‡äº¤äº’å¼å‚ä¸å¢å¼ºè‡ªé€‚åº”æµ‹è¯•ã€‚è¿™æ˜¯è‡ªé€‚åº”æµ‹è¯•ä¸­çš„ç¬¬ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨ã€‚TestAgentæ”¯æŒä¸ªæ€§åŒ–çš„é—®é¢˜é€‰æ‹©ï¼Œæ•æ‰è€ƒç”Ÿçš„ååº”å’Œå¼‚å¸¸æƒ…å†µï¼Œå¹¶é€šè¿‡åŠ¨æ€çš„å¯¹è¯äº¤äº’æä¾›ç²¾ç¡®çš„ç»“æœã€‚åœ¨å¿ƒç†ã€æ•™è‚²å’Œç”Ÿæ´»æ–¹å¼è¯„ä¼°æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€æ–°æŠ€æœ¯åŸºçº¿å°‘é—®äº†20%çš„é—®é¢˜ï¼Œå–å¾—äº†æ›´å‡†ç¡®çš„ç»“æœï¼Œå¹¶ä¸”åœ¨é€Ÿåº¦ã€æµç•…æ€§å’Œå…¶ä»–æ–¹é¢å¾—åˆ°äº†æµ‹è¯•äººå‘˜çš„é’çã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03032v1">PDF</a> 24 pages,10 figures</p>
<p><strong>Summary</strong><br>è‡ªé€‚åº”æµ‹è¯•å·²æˆä¸ºä¸»æµçš„äººæ€§åŒ–æµ‹é‡æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©å’Œä¸ªæ€§åŒ–æé—®æ–¹å¼æœ‰æ•ˆåº”ç”¨äºæ•™è‚²ã€åŒ»ç–—å’Œä½“è‚²ç­‰é¢†åŸŸã€‚å½“å‰é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æœºæ¢°åŒ–ç®—æ³•å¸¦æ¥çš„çŒœæµ‹è¡Œä¸ºã€å¼€æ”¾å¼é—®é¢˜å¤„ç†å›°éš¾ç­‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œé¦–æ¬¡å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨äºè‡ªé€‚åº”æµ‹è¯•é¢†åŸŸï¼Œæå‡ºTestAgentæµ‹è¯•ä»£ç†ï¼Œæ”¯æŒä¸ªæ€§åŒ–é—®é¢˜é€‰æ‹©ï¼Œæ•æ‰è€ƒç”Ÿååº”å’Œå¼‚å¸¸æƒ…å†µï¼Œé€šè¿‡åŠ¨æ€å¯¹è¯äº¤äº’æä¾›ç²¾ç¡®ç»“æœã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ›´å‡†ç¡®ä¸”é—®é¢˜æ•°é‡å‡å°‘20%ï¼Œæµ‹è¯•è€…åœ¨é€Ÿåº¦ã€æµç•…åº¦ç­‰æ–¹é¢ç»™å‡ºæ›´é«˜è¯„ä»·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ–‡æä¾›çš„ä¸ƒä¸ªå…³é”®ç‚¹æ¦‚è¿°ï¼š</p>
<ul>
<li>ç†è§£ä¸ªäººåå¥½å’ŒæŒ‘æˆ˜çš„å…³é”®åœ¨äºå‡†ç¡®è¯„ä¼°å†…éƒ¨äººç±»çŠ¶æ€ã€‚</li>
<li>è‡ªé€‚åº”æµ‹è¯•æ˜¯å¿ƒç†æµ‹é‡ä¸­ä¸»æµçš„ä¸ªæ€§åŒ–æµ‹é‡æ–¹æ³•ã€‚</li>
<li>è‡ªé€‚åº”æµ‹è¯•åœ¨æ•™è‚²ã€åŒ»ç–—ã€ä½“è‚²å’Œç¤¾ä¼šå­¦ç­‰é¢†åŸŸå¹¿æ³›åº”ç”¨ã€‚</li>
<li>å½“å‰è‡ªé€‚åº”æµ‹è¯•é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬ç®—æ³•æœºæ¢°åŒ–ã€å¼€æ”¾å¼é—®é¢˜å¤„ç†ç­‰ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¦–æ¬¡åº”ç”¨äºè‡ªé€‚åº”æµ‹è¯•é¢†åŸŸã€‚</li>
<li>TestAgentæ”¯æŒä¸ªæ€§åŒ–é—®é¢˜é€‰æ‹©ï¼Œæ•æ‰è€ƒç”Ÿååº”å’Œå¼‚å¸¸æƒ…å†µã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ef1e3c2879740c6be1f4bf63a93523f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-517031cff37174e2272acc96cb911458.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32cbe637ff3c656a3abaa0b1f285310c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d662018903a88a84a689cb32b232ed9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c2a3c1e16b00b0db90a0cf8812a81b4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GenFair-Systematic-Test-Generation-for-Fairness-Fault-Detection-in-Large-Language-Models"><a href="#GenFair-Systematic-Test-Generation-for-Fairness-Fault-Detection-in-Large-Language-Models" class="headerlink" title="GenFair: Systematic Test Generation for Fairness Fault Detection in   Large Language Models"></a>GenFair: Systematic Test Generation for Fairness Fault Detection in   Large Language Models</h2><p><strong>Authors:Madhusudan Srinivasan, Jubril Abdel</strong></p>
<p>Large Language Models (LLMs) are increasingly deployed in critical domains, yet they often exhibit biases inherited from training data, leading to fairness concerns. This work focuses on the problem of effectively detecting fairness violations, especially intersectional biases that are often missed by existing template-based and grammar-based testing methods. Previous approaches, such as CheckList and ASTRAEA, provide structured or grammar-driven test generation but struggle with low test diversity and limited sensitivity to complex demographic interactions. To address these limitations, we propose GenFair, a metamorphic fairness testing framework that systematically generates source test cases using equivalence partitioning, mutation operators, and boundary value analysis. GenFair improves fairness testing by generating linguistically diverse, realistic, and intersectional test cases. It applies metamorphic relations (MR) to derive follow-up cases and detects fairness violations via tone-based comparisons between source and follow-up responses. In experiments with GPT-4.0 and LLaMA-3.0, GenFair outperformed two baseline methods. It achieved a fault detection rate (FDR) of 0.73 (GPT-4.0) and 0.69 (LLaMA-3.0), compared to 0.54&#x2F;0.51 for template-based and 0.39&#x2F;0.36 for ASTRAEA. GenFair also showed the highest test case diversity (syntactic:10.06, semantic: 76.68) and strong coherence (syntactic: 291.32, semantic: 0.7043), outperforming both baselines. These results demonstrate the effectiveness of GenFair in uncovering nuanced fairness violations. The proposed method offers a scalable and automated solution for fairness testing and contributes to building more equitable LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…³é”®é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œä½†å®ƒä»¬å¾€å¾€è¡¨ç°å‡ºç”±è®­ç»ƒæ•°æ®ç»§æ‰¿çš„åè§ï¼Œå¼•å‘å…¬å¹³æ€§é—®é¢˜ã€‚æœ¬ç ”ç©¶é‡ç‚¹å…³æ³¨æœ‰æ•ˆæ£€æµ‹å…¬å¹³è¿è§„è¡Œä¸ºçš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åŸºäºæ¨¡æ¿å’Œè¯­æ³•æ£€æµ‹çš„ç°æœ‰æµ‹è¯•æ–¹æ³•å¸¸å¸¸é—æ¼çš„äº¤å‰åè§ã€‚æ­¤å‰çš„CheckListå’ŒASTRAEAç­‰æ–¹æ³•æä¾›ç»“æ„åŒ–æˆ–åŸºäºè¯­æ³•çš„æµ‹è¯•ç”Ÿæˆï¼Œä½†åœ¨æµ‹è¯•å¤šæ ·æ€§æ–¹é¢è¾ƒä½ï¼Œå¯¹å¤æ‚çš„ç§æ—é—´äº¤äº’çš„æ•æ„Ÿæ€§æœ‰é™ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†GenFairï¼Œä¸€ä¸ªåŸºäºå…ƒæ¨¡å‹çš„å…¬å¹³æ€§æµ‹è¯•æ¡†æ¶ï¼Œå®ƒç³»ç»Ÿåœ°ä½¿ç”¨ç­‰ä»·åˆ†åŒºã€å˜å¼‚ç®—å­å’Œè¾¹ç•Œå€¼åˆ†ææ¥ç”Ÿæˆæºæµ‹è¯•ç”¨ä¾‹ã€‚GenFairé€šè¿‡ç”Ÿæˆè¯­è¨€å¤šæ ·ã€ç°å®å’Œäº¤å‰çš„æµ‹è¯•ç”¨ä¾‹æ¥æé«˜å…¬å¹³æ€§æµ‹è¯•ã€‚å®ƒåº”ç”¨å…ƒæ¨¡å‹å…³ç³»ï¼ˆMRï¼‰æ¥æ¨å¯¼åç»­æ¡ˆä¾‹ï¼Œå¹¶é€šè¿‡æºå’Œåç»­å“åº”ä¹‹é—´çš„åŸºäºè¯­æ°”çš„æ¯”è¾ƒæ¥æ£€æµ‹å…¬å¹³è¿è§„è¡Œä¸ºã€‚åœ¨ä¸GPT-4.0å’ŒLLaMA-3.0çš„å®éªŒä¸­ï¼ŒGenFairçš„è¡¨ç°ä¼˜äºä¸¤ç§åŸºå‡†æ–¹æ³•ã€‚å®ƒå¯¹GPT-4.0çš„æ•…éšœæ£€æµ‹ç‡ï¼ˆFDRï¼‰è¾¾åˆ°0.73ï¼Œå¯¹LLaMA-3.0çš„æ•…éšœæ£€æµ‹ç‡è¾¾åˆ°0.69ï¼Œè€ŒåŸºäºæ¨¡æ¿çš„æ–¹æ³•åˆ†åˆ«ä¸º0.54å’Œ0.51ï¼ŒASTRAEAåˆ†åˆ«ä¸º0.39å’Œ0.36ã€‚æ­¤å¤–ï¼ŒGenFairè¿˜æ˜¾ç¤ºå‡ºæœ€é«˜çš„æµ‹è¯•ç”¨ä¾‹å¤šæ ·æ€§ï¼ˆå¥æ³•ï¼š10.06ï¼Œè¯­ä¹‰ï¼š76.68ï¼‰å’Œå¼ºç›¸å…³æ€§ï¼ˆå¥æ³•ï¼š291.32ï¼Œè¯­ä¹‰ï¼š0.7043ï¼‰ï¼Œè¶…è¿‡äº†ä¸¤ç§åŸºå‡†æ–¹æ³•ã€‚è¿™äº›ç»“æœè¯æ˜äº†GenFairåœ¨å‘ç°å¾®å¦™çš„å…¬å¹³è¿è§„è¡Œä¸ºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä¸ºå…¬å¹³æ€§æµ‹è¯•æä¾›äº†å¯æ‰©å±•å’Œè‡ªåŠ¨åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äºæ„å»ºæ›´åŠ å…¬å¹³çš„LLMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03024v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å…³é”®é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å­˜åœ¨ä»è®­ç»ƒæ•°æ®ä¸­ç»§æ‰¿çš„åè§ï¼Œå¼•å‘å…¬å¹³æ€§æ‹…å¿§ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•éš¾ä»¥æ£€æµ‹å¤æ‚äººå£ç‰¹å¾çš„äº¤äº’åè§é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºGenFairçš„å˜å¼‚å…¬å¹³æ€§æµ‹è¯•æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç­‰ä»·åˆ†åŒºã€å˜å¼‚ç®—ç¬¦å’Œè¾¹ç•Œå€¼åˆ†æç³»ç»Ÿåœ°ç”Ÿæˆæºæµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶ç»“åˆè¯­è°ƒå¯¹æ¯”æ£€æµ‹å…¬å¹³æ€§è¿è§„ã€‚å®éªŒè¡¨æ˜ï¼ŒGenFairåœ¨GPT-4.0å’ŒLLaMA-3.0ä¸Šçš„æ•…éšœæ£€æµ‹ç‡é«˜äºå…¶ä»–ä¸¤ç§æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºè¾ƒé«˜çš„æµ‹è¯•ç”¨ä¾‹å¤šæ ·æ€§å’Œè¿è´¯æ€§ã€‚è¿™è¯æ˜äº†GenFairåœ¨å‘ç°å¾®å¦™çš„å…¬å¹³æ€§è¿è§„æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå…¬å¹³æ€§çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æµ‹è¯•æä¾›äº†å¯ä¼¸ç¼©å’Œè‡ªåŠ¨åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å…³é”®é¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œä½†å­˜åœ¨å…¬å¹³æ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯æ£€æµ‹å¤æ‚äººå£ç‰¹å¾çš„äº¤äº’åè§é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚CheckListå’ŒASTRAEAåœ¨æµ‹è¯•å¤šæ ·æ€§å’Œæ•æ„Ÿæ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>GenFairæ˜¯ä¸€ç§æ–°å‹çš„å…¬å¹³æ€§æµ‹è¯•æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿåœ°ç”Ÿæˆæºæµ‹è¯•ç”¨ä¾‹æ¥æé«˜å…¬å¹³æ€§æµ‹è¯•æ°´å¹³ã€‚</li>
<li>GenFairç»“åˆè¯­è°ƒå¯¹æ¯”æ£€æµ‹å…¬å¹³æ€§è¿è§„ï¼Œå¹¶é€šè¿‡å˜å¼‚å…³ç³»æ¨å¯¼åç»­æ¡ˆä¾‹ã€‚</li>
<li>GenFairåœ¨GPT-4.0å’ŒLLaMA-3.0ä¸Šçš„æ•…éšœæ£€æµ‹ç‡é«˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>GenFairå±•ç°å‡ºè¾ƒé«˜çš„æµ‹è¯•ç”¨ä¾‹å¤šæ ·æ€§å’Œè¿è´¯æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03024">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6b7eb7a4f53e91d0035e9e0c91a255d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99096ca03d24c26f049ec2f8f179d462.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-953eb25c4b5f68d48ed08f4663999a8c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="HaploOmni-Unified-Single-Transformer-for-Multimodal-Video-Understanding-and-Generation"><a href="#HaploOmni-Unified-Single-Transformer-for-Multimodal-Video-Understanding-and-Generation" class="headerlink" title="HaploOmni: Unified Single Transformer for Multimodal Video Understanding   and Generation"></a>HaploOmni: Unified Single Transformer for Multimodal Video Understanding   and Generation</h2><p><strong>Authors:Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Zunnan Xu, Zhaoyang Zhang, Yixiao Ge, Xiu Li, Ying Shan</strong></p>
<p>With the advancement of language models, unified multimodal understanding and generation have made significant strides, with model architectures evolving from separated components to unified single-model frameworks. This paper explores an efficient training paradigm to build a single transformer for unified multimodal understanding and generation. Specifically, we propose a multimodal warmup strategy utilizing prior knowledge to extend capabilities. To address cross-modal compatibility challenges, we introduce feature pre-scaling and multimodal AdaLN techniques. Integrating the proposed technologies, we present the HaploOmni, a new single multimodal transformer. With limited training costs, HaploOmni achieves competitive performance across multiple image and video understanding and generation benchmarks over advanced unified models. All codes will be made public at <a target="_blank" rel="noopener" href="https://github.com/Tencent/HaploVLM">https://github.com/Tencent/HaploVLM</a>. </p>
<blockquote>
<p>éšç€è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”ŸæˆæŠ€æœ¯å·²ç»å–å¾—äº†é‡å¤§çªç ´ï¼Œæ¨¡å‹æ¶æ„ä»åˆ†ç¦»ç»„ä»¶å‘å±•åˆ°ç»Ÿä¸€å•æ¨¡å‹æ¡†æ¶ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒèŒƒå¼ï¼Œä»¥å»ºç«‹ç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„å•å˜å‹å™¨æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å…ˆéªŒçŸ¥è¯†çš„å¤šæ¨¡æ€é¢„çƒ­ç­–ç•¥æ¥æ‰©å±•åŠŸèƒ½ã€‚ä¸ºäº†è§£å†³è·¨æ¨¡æ€å…¼å®¹æ€§é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç‰¹å¾é¢„ç¼©æ”¾å’Œå¤šæ¨¡æ€AdaLNæŠ€æœ¯ã€‚é€šè¿‡æ•´åˆæ‰€æå‡ºçš„æŠ€æœ¯ï¼Œæˆ‘ä»¬æ¨å‡ºäº†æ–°çš„å•ä¸€å¤šæ¨¡æ€å˜å‹å™¨HaploOmniã€‚åœ¨æœ‰é™çš„è®­ç»ƒæˆæœ¬ä¸‹ï¼ŒHaploOmniåœ¨å¤šä¸ªå›¾åƒå’Œè§†é¢‘ç†è§£å’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¸å…ˆè¿›ç»Ÿä¸€æ¨¡å‹ç›¸ç«äº‰çš„æ€§èƒ½ã€‚æ‰€æœ‰ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Tencent/HaploVLM%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/Tencent/HaploVLMä¸Šå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02975v1">PDF</a> </p>
<p><strong>Summary</strong><br>éšç€è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆå–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œæ¨¡å‹æ¶æ„ä»åˆ†ç¦»ç»„ä»¶æ¼”å˜ä¸ºç»Ÿä¸€å•æ¨¡å‹æ¡†æ¶ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨å…ˆéªŒçŸ¥è¯†æ„å»ºå•ä¸€è½¬æ¢å™¨è¿›è¡Œç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚é’ˆå¯¹è·¨æ¨¡æ€å…¼å®¹æ€§é—®é¢˜ï¼Œå¼•å…¥ç‰¹å¾é¢„ç¼©æ”¾å’Œå¤šæ¨¡æ€AdaLNæŠ€æœ¯ã€‚é›†æˆè¿™äº›æŠ€æœ¯ï¼Œæ¨å‡ºæ–°çš„å•ä¸€å¤šæ¨¡æ€è½¬æ¢å™¨HaploOmniã€‚åœ¨æœ‰é™çš„è®­ç»ƒæˆæœ¬ä¸‹ï¼ŒHaploOmniåœ¨å¤šä¸ªå›¾åƒå’Œè§†é¢‘ç†è§£å’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ç›¸å…³ä»£ç å°†å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/Tencent/HaploVLM%E3%80%82">https://github.com/Tencent/HaploVLMã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆå–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œæ¨¡å‹æ¶æ„æ¼”å˜ä¸ºç»Ÿä¸€å•æ¨¡å‹æ¡†æ¶ã€‚</li>
<li>æå‡ºä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒèŒƒå¼ï¼Œåˆ©ç”¨å…ˆéªŒçŸ¥è¯†æ„å»ºå•ä¸€è½¬æ¢å™¨è¿›è¡Œå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚</li>
<li>é’ˆå¯¹è·¨æ¨¡æ€å…¼å®¹æ€§é—®é¢˜ï¼Œå¼•å…¥ç‰¹å¾é¢„ç¼©æ”¾å’Œå¤šæ¨¡æ€AdaLNæŠ€æœ¯ã€‚</li>
<li>æ¨å‡ºæ–°çš„å•ä¸€å¤šæ¨¡æ€è½¬æ¢å™¨HaploOmniã€‚</li>
<li>HaploOmniåœ¨å¤šä¸ªå›¾åƒå’Œè§†é¢‘ç†è§£å’Œç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç›¸å…³ä»£ç å°†å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/Tencent/HaploVLM%EF%BC%8C%E4%BE%BF%E4%BA%8E%E5%85%AC%E4%BC%97%E8%AE%BF%E9%97%AE%E5%92%8C%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/Tencent/HaploVLMï¼Œä¾¿äºå…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e7550ca7551782ede0cf7d4a53073538.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cf5c039b5b7d0c7098af698ec65565d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26e3cea76573f9e8c3973eaa75099b63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4139d3123626ddea63fdec0997a4bf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a82471d5dd51ca7f42234b41a1de4c18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-274f6f90bd547c35b18481d9c7a1a321.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="FlowerTune-A-Cross-Domain-Benchmark-for-Federated-Fine-Tuning-of-Large-Language-Models"><a href="#FlowerTune-A-Cross-Domain-Benchmark-for-Federated-Fine-Tuning-of-Large-Language-Models" class="headerlink" title="FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large   Language Models"></a>FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large   Language Models</h2><p><strong>Authors:Yan Gao, Massimo Roberto Scamarcia, Javier Fernandez-Marques, Mohammad Naseri, Chong Shen Ng, Dimitris Stripelis, Zexi Li, Tao Shen, Jiamu Bai, Daoyuan Chen, Zikai Zhang, Rui Hu, InSeo Song, Lee KangYoon, Hong Jia, Ting Dang, Junyan Wang, Zheyuan Liu, Daniel Janes Beutel, Lingjuan Lyu, Nicholas D. Lane</strong></p>
<p>Large Language Models (LLMs) have achieved state-of-the-art results across diverse domains, yet their development remains reliant on vast amounts of publicly available data, raising concerns about data scarcity and the lack of access to domain-specific, sensitive information. Federated Learning (FL) presents a compelling framework to address these challenges by enabling decentralized fine-tuning on pre-trained LLMs without sharing raw data. However, the compatibility and performance of pre-trained LLMs in FL settings remain largely under explored. We introduce the FlowerTune LLM Leaderboard, a first-of-its-kind benchmarking suite designed to evaluate federated fine-tuning of LLMs across four diverse domains: general NLP, finance, medical, and coding. Each domain includes federated instruction-tuning datasets and domain-specific evaluation metrics. Our results, obtained through a collaborative, open-source and community-driven approach, provide the first comprehensive comparison across 26 pre-trained LLMs with different aggregation and fine-tuning strategies under federated settings, offering actionable insights into model performance, resource constraints, and domain adaptation. This work lays the foundation for developing privacy-preserving, domain-specialized LLMs for real-world applications. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸéƒ½å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œç„¶è€Œï¼Œå®ƒä»¬çš„å‘å±•ä»ç„¶ä¾èµ–äºå¤§é‡å…¬å¼€çš„å¯ç”¨æ•°æ®ï¼Œå¼•å‘äº†å…³äºæ•°æ®ç¨€ç¼ºå’Œç¼ºä¹è®¿é—®ç‰¹å®šé¢†åŸŸæ•æ„Ÿä¿¡æ¯çš„æ‹…å¿§ã€‚è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æä¾›äº†ä¸€ä¸ªå¸å¼•äººçš„æ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸å…±äº«åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¯¹é¢„è®­ç»ƒçš„LLMè¿›è¡Œåˆ†æ•£å¾®è°ƒã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒçš„LLMåœ¨FLç¯å¢ƒä¸­çš„å…¼å®¹æ€§å’Œæ€§èƒ½ä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬å¼•å…¥äº†FlowerTune LLMæ’è¡Œæ¦œï¼Œè¿™æ˜¯ä¸€ä¸ªé¦–åˆ›çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œæ—¨åœ¨è¯„ä¼°å››ä¸ªä¸åŒé¢†åŸŸçš„è”é‚¦å¾®è°ƒLLMï¼šé€šç”¨NLPã€é‡‘èã€åŒ»ç–—å’Œç¼–ç ã€‚æ¯ä¸ªé¢†åŸŸéƒ½åŒ…å«è”é‚¦æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†å’Œç‰¹å®šé¢†åŸŸçš„è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¯é€šè¿‡åä½œã€å¼€æºå’Œç¤¾åŒºé©±åŠ¨çš„æ–¹æ³•è·å¾—çš„ï¼Œé¦–æ¬¡å…¨é¢æ¯”è¾ƒäº†è”é‚¦ç¯å¢ƒä¸‹ä½¿ç”¨ä¸åŒèšåˆå’Œå¾®è°ƒç­–ç•¥çš„26ä¸ªé¢„è®­ç»ƒLLMï¼Œä¸ºæ¨¡å‹æ€§èƒ½ã€èµ„æºçº¦æŸå’Œé¢†åŸŸé€‚åº”æä¾›äº†å¯æ“ä½œæ€§çš„è§è§£ã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘ç”¨äºç°å®ä¸–ç•Œåº”ç”¨çš„éšç§ä¿æŠ¤ã€é¢†åŸŸä¸“ç”¨çš„LLMå¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02961v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šé¢†åŸŸå®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä½†å…¶å‘å±•ä»ä¾èµ–äºå¤§é‡å…¬å¼€æ•°æ®ï¼Œå¼•å‘äº†å…³äºæ•°æ®ç¨€ç¼ºå’Œæ— æ³•è®¿é—®ç‰¹å®šé¢†åŸŸæ•æ„Ÿä¿¡æ¯çš„æ‹…å¿§ã€‚è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜æä¾›äº†ä¸€ä¸ªå¸å¼•äººçš„æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸å…±äº«åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¯¹é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹è¿›è¡Œåˆ†æ•£å¼å¾®è°ƒã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒLLMåœ¨FLç¯å¢ƒä¸­çš„å…¼å®¹æ€§å’Œæ€§èƒ½å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†FlowerTune LLMæ’è¡Œæ¦œï¼Œè¿™æ˜¯ä¸€ä¸ªé¦–åˆ›çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œæ—¨åœ¨è¯„ä¼°å››ä¸ªä¸åŒé¢†åŸŸçš„LLMè”é‚¦å¾®è°ƒæ€§èƒ½ï¼šé€šç”¨NLPã€é‡‘èã€åŒ»ç–—å’Œç¼–ç ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¯é€šè¿‡åä½œã€å¼€æºå’Œç¤¾åŒºé©±åŠ¨çš„æ–¹å¼è·å¾—çš„ï¼Œä¸ºä¸åŒç­–ç•¥ä¸‹çš„è”é‚¦è®¾ç½®ä¸­çš„26ä¸ªé¢„è®­ç»ƒLLMæä¾›äº†å…¨é¢çš„æ¯”è¾ƒï¼Œæä¾›äº†å…³äºæ¨¡å‹æ€§èƒ½ã€èµ„æºçº¦æŸå’Œé¢†åŸŸé€‚åº”æ€§çš„å¯æ“ä½œè§è§£ã€‚æœ¬å·¥ä½œå¥ å®šäº†å¼€å‘ç”¨äºç°å®ä¸–ç•Œåº”ç”¨çš„éšç§ä¿æŠ¤ã€é¢†åŸŸä¸“ä¸šåŒ–çš„LLMçš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å¤šé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†éœ€å¤§é‡å…¬å¼€æ•°æ®ï¼Œå¼•å‘æ•°æ®ç¨€ç¼ºå’Œéšç§æ‹…å¿§ã€‚</li>
<li>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸ºåœ¨ä¸å…±äº«åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹å¾®è°ƒLLMæä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>FlowerTune LLM Leaderboardæ˜¯é¦–ä¸ªè¯„ä¼°LLMåœ¨è”é‚¦å­¦ä¹ ç¯å¢ƒä¸‹æ€§èƒ½çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚</li>
<li>è¯¥å¥—ä»¶æ¶µç›–äº†å››ä¸ªä¸åŒé¢†åŸŸï¼šNLPã€é‡‘èã€åŒ»ç–—å’Œç¼–ç ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›äº†å…³äºæ¨¡å‹æ€§èƒ½ã€èµ„æºçº¦æŸå’Œé¢†åŸŸé€‚åº”æ€§çš„æ·±å…¥è§è§£ã€‚</li>
<li>è”é‚¦è®¾ç½®ä¸‹çš„ä¸åŒç­–ç•¥å’Œèšåˆæ–¹æ³•å¯¹LLMæ€§èƒ½æœ‰å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-85e81281fbc2831e6b622f9053005860.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c47c116373952fe01bcaeff1bd0d8c77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a02da653d0b1ad4881a97be7f8f14176.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d6c9ee8e234bfdbd39893fbd0e908f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2f14135c7e1d68c86fe26a432833af0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-676e5059303426370ff88d1a792467c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56440a747562c1f9d138effa144df38c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ProcrustesGPT-Compressing-LLMs-with-Structured-Matrices-and-Orthogonal-Transformations"><a href="#ProcrustesGPT-Compressing-LLMs-with-Structured-Matrices-and-Orthogonal-Transformations" class="headerlink" title="ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal   Transformations"></a>ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal   Transformations</h2><p><strong>Authors:Ekaterina Grishina, Mikhail Gorbunov, Maxim Rakhuba</strong></p>
<p>Large language models (LLMs) demonstrate impressive results in natural language processing tasks but require a significant amount of computational and memory resources. Structured matrix representations are a promising way for reducing the number of parameters of these models. However, it seems unrealistic to expect that weight matrices of pretrained models can be accurately represented by structured matrices without any fine-tuning. To overcome this issue, we utilize the fact that LLM output is invariant under certain orthogonal transformations of weight matrices. This insight can be leveraged to identify transformations that significantly improve the compressibility of weights within structured classes. The proposed approach is applicable to various types of structured matrices that support efficient projection operations. Code is available at <a target="_blank" rel="noopener" href="https://github.com/GrishKate/ProcrustesGPT">https://github.com/GrishKate/ProcrustesGPT</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—å’Œå†…å­˜èµ„æºã€‚ç»“æ„çŸ©é˜µè¡¨ç¤ºæ˜¯å‡å°‘è¿™äº›æ¨¡å‹å‚æ•°æ•°é‡çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œä¼¼ä¹ä¸åˆ‡å®é™…åœ°æœŸæœ›é€šè¿‡ç»“æ„çŸ©é˜µå‡†ç¡®è¡¨ç¤ºé¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡çŸ©é˜µè€Œæ— éœ€è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨LLMè¾“å‡ºåœ¨æƒé‡çŸ©é˜µçš„æŸäº›æ­£äº¤å˜æ¢ä¸‹æ˜¯ä¸å˜çš„è¿™ä¸€äº‹å®ã€‚è¿™ä¸€è§è§£å¯ç”¨äºç¡®å®šèƒ½å¤Ÿæ˜¾è‘—æé«˜æƒé‡å‹ç¼©ç‡çš„è½¬æ¢ï¼Œè¿™åœ¨ç»“æ„åŒ–ç±»åˆ«ä¸­æ˜¯æœ‰æ•ˆçš„ã€‚æ‰€æå‡ºçš„æ–¹æ³•é€‚ç”¨äºæ”¯æŒé«˜æ•ˆæŠ•å½±æ“ä½œçš„å„ç§ç±»å‹çš„ç»“æ„åŒ–çŸ©é˜µã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/GrishKate/ProcrustesGPT">https://github.com/GrishKate/ProcrustesGPT</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02818v1">PDF</a> Accepted by ACL Findings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—å’Œå†…å­˜èµ„æºã€‚ç»“æ„çŸ©é˜µè¡¨ç¤ºæ˜¯å‡å°‘æ¨¡å‹å‚æ•°çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¦‚æœä¸è¿›è¡Œå¾®è°ƒï¼Œå¾ˆéš¾æœŸæœ›é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡çŸ©é˜µèƒ½å‡†ç¡®åœ°ç”¨ç»“æ„çŸ©é˜µè¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨LLMè¾“å‡ºåœ¨æƒé‡çŸ©é˜µçš„æŸäº›æ­£äº¤å˜æ¢ä¸‹æ˜¯ä¸å˜çš„è¿™ä¸€äº‹å®ã€‚è¿™ä¸€è§è§£å¯ä»¥ç”¨æ¥è¯†åˆ«èƒ½æ˜¾è‘—æé«˜æƒé‡å‹ç¼©ç‡çš„è½¬æ¢ï¼Œé€‚ç”¨äºæ”¯æŒé«˜æ•ˆæŠ•å½±æ“ä½œçš„å„ç§ç±»å‹çš„ç»“æ„çŸ©é˜µã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/GrishKate/ProcrustesGPT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/GrishKate/ProcrustesGPTæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†è®¡ç®—ä¸å†…å­˜èµ„æºéœ€æ±‚å·¨å¤§ã€‚</li>
<li>ç»“æ„çŸ©é˜µè¡¨ç¤ºæ˜¯å‡å°‘LLMå‚æ•°çš„ä¸€ç§ç­–ç•¥ã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡çŸ©é˜µéš¾ä»¥ä»…é€šè¿‡ç»“æ„çŸ©é˜µå‡†ç¡®è¡¨ç¤ºï¼Œéœ€è¦è¿›è¡Œå¾®è°ƒã€‚</li>
<li>LLMè¾“å‡ºåœ¨ç‰¹å®šæƒé‡çŸ©é˜µæ­£äº¤å˜æ¢ä¸‹å…·æœ‰ä¸å˜æ€§ã€‚</li>
<li>è¿™ç§ä¸å˜æ€§æœ‰åŠ©äºæ‰¾åˆ°èƒ½æé«˜æƒé‡å‹ç¼©ç‡çš„è½¬æ¢æ–¹æ³•ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•é€‚ç”¨äºæ”¯æŒé«˜æ•ˆæŠ•å½±æ“ä½œçš„å„ç§ç±»å‹çš„ç»“æ„çŸ©é˜µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5af3730e5c90823b279c1e26507b180d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4718210200b236792241e63a4e4c327b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e66785053d105001dde8fd8d94e4ea82.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Open-Set-Living-Need-Prediction-with-Large-Language-Models"><a href="#Open-Set-Living-Need-Prediction-with-Large-Language-Models" class="headerlink" title="Open-Set Living Need Prediction with Large Language Models"></a>Open-Set Living Need Prediction with Large Language Models</h2><p><strong>Authors:Xiaochong Lan, Jie Feng, Yizhou Sun, Chen Gao, Jiahuan Lei, Xinlei Shi, Hengliang Luo, Yong Li</strong></p>
<p>Living needs are the needs people generate in their daily lives for survival and well-being. On life service platforms like Meituan, user purchases are driven by living needs, making accurate living need predictions crucial for personalized service recommendations. Traditional approaches treat this prediction as a closed-set classification problem, severely limiting their ability to capture the diversity and complexity of living needs. In this work, we redefine living need prediction as an open-set classification problem and propose PIGEON, a novel system leveraging large language models (LLMs) for unrestricted need prediction. PIGEON first employs a behavior-aware record retriever to help LLMs understand user preferences, then incorporates Maslowâ€™s hierarchy of needs to align predictions with human living needs. For evaluation and application, we design a recall module based on a fine-tuned text embedding model that links flexible need descriptions to appropriate life services. Extensive experiments on real-world datasets demonstrate that PIGEON significantly outperforms closed-set approaches on need-based life service recall by an average of 19.37%. Human evaluation validates the reasonableness and specificity of our predictions. Additionally, we employ instruction tuning to enable smaller LLMs to achieve competitive performance, supporting practical deployment. </p>
<blockquote>
<p>ç”Ÿæ´»éœ€æ±‚æ˜¯äººä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­ä¸ºç”Ÿå­˜å’Œç¦ç¥‰æ‰€äº§ç”Ÿçš„éœ€æ±‚ã€‚åœ¨ç¾å›¢ç­‰ç”Ÿæ´»æœåŠ¡å¹³å°ä¸Šï¼Œç”¨æˆ·è´­ä¹°è¡Œä¸ºç”±ç”Ÿæ´»éœ€æ±‚é©±åŠ¨ï¼Œå› æ­¤å¯¹ç”Ÿæ´»éœ€æ±‚è¿›è¡Œå‡†ç¡®é¢„æµ‹å¯¹äºä¸ªæ€§åŒ–æœåŠ¡æ¨èè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•å°†éœ€æ±‚é¢„æµ‹è§†ä¸ºä¸€ä¸ªå°é—­é›†åˆ†ç±»é—®é¢˜ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å…¶æ•æ‰ç”Ÿæ´»éœ€æ±‚å¤šæ ·æ€§å’Œå¤æ‚æ€§çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p>åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®šä¹‰äº†ç”Ÿæ´»éœ€æ±‚é¢„æµ‹ä½œä¸ºä¸€ä¸ªå¼€æ”¾é›†åˆ†ç±»é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹ç³»ç»ŸPIGEONï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ— é™åˆ¶çš„éœ€æ±‚é¢„æµ‹ã€‚PIGEONé¦–å…ˆé‡‡ç”¨è¡Œä¸ºæ„ŸçŸ¥è®°å½•æ£€ç´¢å™¨å¸®åŠ©LLMç†è§£ç”¨æˆ·åå¥½ï¼Œç„¶åç»“åˆé©¬æ–¯æ´›çš„éœ€æ±‚å±‚æ¬¡ç†è®ºä½¿é¢„æµ‹ä¸äººç±»ç”Ÿæ´»éœ€æ±‚ç›¸åŒ¹é…ã€‚</p>
<p>ä¸ºäº†è¯„ä¼°å’Œåº”ç”¨ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºç²¾ç»†è°ƒæ•´æ–‡æœ¬åµŒå…¥æ¨¡å‹çš„å¬å›æ¨¡å—ï¼Œå®ƒå°†çµæ´»çš„éœ€æ±‚æè¿°ä¸é€‚å½“çš„ç”Ÿæ´»æœåŠ¡è”ç³»èµ·æ¥ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPIGEONåœ¨åŸºäºéœ€æ±‚çš„ç”Ÿæ´»æœåŠ¡å¬å›æ–¹é¢å¹³å‡æ¯”å°é—­é›†æ–¹æ³•é«˜å‡º19.37%ã€‚äººç±»è¯„ä¼°éªŒè¯äº†æˆ‘ä»¬é¢„æµ‹çš„åˆç†æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒæ¥ä½¿è¾ƒå°çš„LLMå®ç°ç«äº‰æ€§èƒ½ï¼Œæ”¯æŒå®é™…éƒ¨ç½²ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02713v1">PDF</a> ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¾å›¢ç­‰ç”Ÿæ´»æœåŠ¡å¹³å°ï¼Œç”¨æˆ·è´­ä¹°è¡Œä¸ºå—ç”Ÿæ´»éœ€æ±‚é©±åŠ¨ã€‚ä¼ ç»Ÿæ–¹æ³•å°†ç”Ÿæ´»éœ€æ±‚é¢„æµ‹è§†ä¸ºå°é—­é›†åˆ†ç±»é—®é¢˜ï¼Œéš¾ä»¥æ•æ‰éœ€æ±‚çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚æœ¬æ–‡é‡æ–°å®šä¹‰äº†å¼€æ”¾é›†åˆ†ç±»ä¸‹çš„ç”Ÿæ´»éœ€æ±‚é¢„æµ‹é—®é¢˜ï¼Œå¹¶æå‡ºPIGEONç³»ç»Ÿï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ— é™åˆ¶éœ€æ±‚é¢„æµ‹ã€‚PIGEONé€šè¿‡è¡Œä¸ºæ„ŸçŸ¥è®°å½•æ£€ç´¢å™¨å¸®åŠ©LLMç†è§£ç”¨æˆ·åå¥½ï¼Œå¹¶ç»“åˆé©¬æ–¯æ´›éœ€æ±‚å±‚æ¬¡ç†è®ºè¿›è¡Œéœ€æ±‚é¢„æµ‹ã€‚é€šè¿‡çœŸå®æ•°æ®é›†çš„å®éªŒéªŒè¯ï¼ŒPIGEONåœ¨ç”Ÿæ´»æœåŠ¡éœ€æ±‚å¬å›æ–¹é¢æ˜¾è‘—ä¼˜äºå°é—­é›†æ–¹æ³•ï¼Œå¹³å‡æé«˜19.37%ã€‚äººç±»è¯„ä¼°éªŒè¯äº†é¢„æµ‹çš„åˆç†æ€§å’Œç‰¹å¼‚æ€§ã€‚æ­¤å¤–ï¼Œé€šè¿‡æŒ‡ä»¤å¾®è°ƒä½¿è¾ƒå°è§„æ¨¡çš„LLMä¹Ÿèƒ½å®ç°ç«äº‰åŠ›è¡¨ç°ï¼Œæ”¯æŒå®é™…åº”ç”¨éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæ´»éœ€æ±‚é¢„æµ‹åœ¨ä¸ªæ€§åŒ–æœåŠ¡æ¨èä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç”Ÿæ´»æœåŠ¡å¹³å°å¦‚ç¾å›¢ç­‰ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¤„ç†æ­¤ç±»é¢„æµ‹æ—¶å°†å…¶è§†ä¸ºå°é—­é›†åˆ†ç±»é—®é¢˜ï¼Œå¯¼è‡´æ— æ³•å……åˆ†æ•æ‰éœ€æ±‚çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºPIGEONç³»ç»Ÿï¼Œå°†ç”Ÿæ´»éœ€æ±‚é¢„æµ‹å®šä¹‰ä¸ºå¼€æ”¾é›†åˆ†ç±»é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>PIGEONé€šè¿‡è¡Œä¸ºæ„ŸçŸ¥è®°å½•æ£€ç´¢å™¨ç†è§£ç”¨æˆ·åå¥½ï¼Œå¹¶ç»“åˆé©¬æ–¯æ´›éœ€æ±‚å±‚æ¬¡ç†è®ºè¿›è¡Œéœ€æ±‚é¢„æµ‹ã€‚</li>
<li>åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPIGEONåœ¨ç”Ÿæ´»æœåŠ¡éœ€æ±‚å¬å›æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>äººç±»è¯„ä¼°éªŒè¯äº†PIGEONé¢„æµ‹çš„åˆç†æ€§å’Œç‰¹å¼‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b759d998f64c73ee26bda7829463c1aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-470b6cb049d7f816f0d07facee470408.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d5282eaf1236499e6c946a600777170.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Iterative-Self-Improvement-of-Vision-Language-Models-for-Image-Scoring-and-Self-Explanation"><a href="#Iterative-Self-Improvement-of-Vision-Language-Models-for-Image-Scoring-and-Self-Explanation" class="headerlink" title="Iterative Self-Improvement of Vision Language Models for Image Scoring   and Self-Explanation"></a>Iterative Self-Improvement of Vision Language Models for Image Scoring   and Self-Explanation</h2><p><strong>Authors:Naoto Tanji, Toshihiko Yamasaki</strong></p>
<p>Image scoring is a crucial task in numerous real-world applications. To trust a modelâ€™s judgment, understanding its rationale is essential. This paper proposes a novel training method for Vision Language Models (VLMs) to generate not only image scores but also corresponding justifications in natural language. Leveraging only an image scoring dataset and an instruction-tuned VLM, our method enables self-training, utilizing the VLMâ€™s generated text without relying on external data or models. In addition, we introduce a simple method for creating a dataset designed to improve alignment between predicted scores and their textual justifications. By iteratively training the model with Direct Preference Optimization on two distinct datasets and merging them, we can improve both scoring accuracy and the coherence of generated explanations. </p>
<blockquote>
<p>å›¾åƒè¯„åˆ†æ˜¯ä¼—å¤šç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚ä¸ºäº†ä¿¡ä»»æ¨¡å‹çš„åˆ¤æ–­ï¼Œç†è§£å…¶åŸç†è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è®­ç»ƒæ–¹æ³•ï¼Œä¸ä»…èƒ½å¤Ÿç”Ÿæˆå›¾åƒè¯„åˆ†ï¼Œè¿˜èƒ½ç”Ÿæˆç›¸åº”çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚ä»…åˆ©ç”¨å›¾åƒè¯„åˆ†æ•°æ®é›†å’ŒæŒ‡ä»¤è°ƒæ•´è¿‡çš„VLMï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°è‡ªæˆ‘è®­ç»ƒï¼Œåˆ©ç”¨VLMç”Ÿæˆçš„æ–‡æœ¬ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨æ•°æ®æˆ–æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§åˆ›å»ºæ•°æ®é›†çš„ç®€å•æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é¢„æµ‹åˆ†æ•°å’Œå…¶æ–‡æœ¬è§£é‡Šä¹‹é—´çš„å¯¹é½åº¦ã€‚é€šè¿‡åœ¨ä¸åŒçš„ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–çš„è¿­ä»£è®­ç»ƒå¹¶å°†å®ƒä»¬åˆå¹¶ï¼Œæˆ‘ä»¬å¯ä»¥æé«˜è¯„åˆ†å‡†ç¡®æ€§å’Œç”Ÿæˆçš„è§£é‡Šçš„è¿è´¯æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02708v1">PDF</a> Accepted to ICIP2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹è®­ç»ƒæ–¹æ³•æ¥è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä½¿å…¶ä¸ä»…èƒ½å¤Ÿç»™å‡ºå›¾åƒè¯„åˆ†ï¼Œè¿˜èƒ½ç”Ÿæˆå¯¹åº”çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚è¯¥æ–¹æ³•ä»…ä¾èµ–å›¾åƒè¯„åˆ†æ•°æ®é›†å’ŒæŒ‡ä»¤è°ƒä¼˜çš„VLMè¿›è¡Œè‡ªè®­ç»ƒï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨æ•°æ®æˆ–æ¨¡å‹ã€‚é€šè¿‡å¼•å…¥ä¸€ç§ç®€å•çš„æ–¹æ³•åˆ›å»ºæ•°æ®é›†ï¼Œæé«˜é¢„æµ‹åˆ†æ•°ä¸æ–‡æœ¬è§£é‡Šä¹‹é—´çš„å¯¹é½åº¦ã€‚é€šè¿‡åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDirect Preference Optimizationï¼‰çš„è¿­ä»£è®­ç»ƒå¹¶åˆå¹¶ï¼Œæé«˜äº†è¯„åˆ†å‡†ç¡®æ€§å’Œç”Ÿæˆçš„è§£é‡Šçš„è¿è´¯æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¯ä»¥ç»è¿‡è®­ç»ƒä»¥ç”Ÿæˆå›¾åƒè¯„åˆ†å’Œå¯¹åº”çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚</li>
<li>è®­ç»ƒæ–¹æ³•ä»…ä¾èµ–å›¾åƒè¯„åˆ†æ•°æ®é›†å’ŒæŒ‡ä»¤è°ƒä¼˜çš„VLMè¿›è¡Œè‡ªè®­ç»ƒã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åˆ›å»ºæ•°æ®é›†çš„æ–¹æ³•ï¼Œç”¨äºæé«˜é¢„æµ‹åˆ†æ•°ä¸æ–‡æœ¬è§£é‡Šä¹‹é—´çš„å¯¹é½åº¦ã€‚</li>
<li>é€šè¿‡åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œè¿­ä»£è®­ç»ƒï¼Œä½¿ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDirect Preference Optimizationï¼‰æé«˜äº†æ¨¡å‹çš„è¯„åˆ†å‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹ç”Ÿæˆçš„è§£é‡Šå…·æœ‰æ›´é«˜çš„è¿è´¯æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†æ¨¡å‹çš„å¯ä¿¡åº¦ï¼Œå› ä¸ºç†è§£äº†å…¶è¯„åˆ†èƒŒåçš„ç†ç”±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02708">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5115258f4a18e03a9a1545e4821b086f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27b171f15419d23e994572813d2d7807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f18a47026bf3fcc1d05c629921e9363.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e71080f2ce9858be96c14a8ccf0d8a63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75287b79268daa82d052f288837f803d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MINT-Multimodal-Instruction-Tuning-with-Multimodal-Interaction-Grouping"><a href="#MINT-Multimodal-Instruction-Tuning-with-Multimodal-Interaction-Grouping" class="headerlink" title="MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping"></a>MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping</h2><p><strong>Authors:Xiaojun Shan, Qi Cao, Xing Han, Haofei Yu, Paul Pu Liang</strong></p>
<p>Recent advances in multimodal foundation models have achieved state-of-the-art performance across a range of tasks. These breakthroughs are largely driven by new pre-training paradigms that leverage large-scale, unlabeled multimodal data, followed by instruction fine-tuning on curated labeled datasets and high-quality prompts. While there is growing interest in scaling instruction fine-tuning to ever-larger datasets in both quantity and scale, our findings reveal that simply increasing the number of instruction-tuning tasks does not consistently yield better performance. Instead, we observe that grouping tasks by the common interactions across modalities, such as discovering redundant shared information, prioritizing modality selection with unique information, or requiring synergistic fusion to discover new information from both modalities, encourages the models to learn transferrable skills within a group while suppressing interference from mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly effective task-grouping strategy based on the type of multimodal interaction. We demonstrate that the proposed method greatly outperforms existing task grouping baselines for multimodal instruction tuning, striking an effective balance between generalization and specialization. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è¿›å±•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›çªç ´ä¸»è¦å¾—ç›Šäºæ–°çš„é¢„è®­ç»ƒèŒƒå¼ï¼Œå®ƒåˆ©ç”¨å¤§è§„æ¨¡çš„æ— æ ‡ç­¾å¤šæ¨¡æ€æ•°æ®ï¼Œç„¶ååœ¨ç²¾é€‰çš„æ ‡è®°æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå¹¶ä½¿ç”¨é«˜è´¨é‡çš„æç¤ºã€‚å°½ç®¡äººä»¬å¯¹æ‰©å¤§æŒ‡ä»¤å¾®è°ƒä»¥æ¶µç›–æ•°é‡å’Œè§„æ¨¡æ—¥ç›Šå¢å¤§çš„æ•°æ®é›†çš„å…´è¶£æ—¥ç›Šå¢é•¿ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»…ä»…å¢åŠ æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡çš„æ•°é‡å¹¶ä¸æ€»èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚ç›¸åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œé€šè¿‡è·¨æ¨¡æ€çš„å¸¸è§äº¤äº’å¯¹ä»»åŠ¡è¿›è¡Œåˆ†ç»„ï¼Œå¦‚å‘ç°å†—ä½™çš„å…±äº«ä¿¡æ¯ã€ä¼˜å…ˆé€‰æ‹©ä¸ç‹¬ç‰¹ä¿¡æ¯ç›¸å…³çš„æ¨¡æ€ï¼Œæˆ–éœ€è¦ååŒèåˆä»¥ä»ä¸¤ç§æ¨¡æ€ä¸­å‘ç°æ–°ä¿¡æ¯ï¼Œå¯ä»¥é¼“åŠ±æ¨¡å‹åœ¨ç»„å†…å­¦ä¹ å¯è¿ç§»æŠ€èƒ½ï¼ŒåŒæ—¶æŠ‘åˆ¶ä¸åŒ¹é…ä»»åŠ¡çš„å¹²æ‰°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MINTï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€äº¤äº’ç±»å‹çš„ç®€å•è€Œæœ‰æ•ˆçš„ä»»åŠ¡åˆ†ç»„ç­–ç•¥ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå¯¹äºå¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¤§å¤§ä¼˜äºç°æœ‰çš„ä»»åŠ¡åˆ†ç»„åŸºçº¿ï¼Œåœ¨æ³›åŒ–å’Œä¸“ä¸šåŒ–ä¹‹é—´è¾¾åˆ°äº†æœ‰æ•ˆçš„å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02308v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çªç ´å¾—ç›Šäºæ–°çš„é¢„è®­ç»ƒæ¨¡å¼å’Œå¤§è§„æ¨¡æœªæ ‡ç­¾å¤šæ¨¡æ€æ•°æ®çš„åº”ç”¨ã€‚ç ”ç©¶å‘ç°åœ¨è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒæ—¶ï¼Œå•çº¯å¢åŠ ä»»åŠ¡æ•°é‡å¹¶ä¸æ€»èƒ½æå‡æ€§èƒ½ã€‚ç›¸åï¼Œé€šè¿‡è·¨æ¨¡æ€äº¤äº’çš„ä»»åŠ¡åˆ†ç»„ç­–ç•¥ï¼Œå¦‚å¯»æ‰¾å†—ä½™å…±äº«ä¿¡æ¯ã€ä¼˜å…ˆå¤„ç†å…·æœ‰ç‹¬ç‰¹ä¿¡æ¯çš„æ¨¡æ€æˆ–ååŒèåˆæ¥è‡ªä¸åŒæ¨¡æ€çš„ä¿¡æ¯å‘ç°æ–°æ–¹æ³•ç­‰ï¼Œå¯ä»¥ä½¿æ¨¡å‹æ›´å¥½åœ°åœ¨åˆ†ç»„ä¸­å­¦ä¹ è½¬ç§»çŸ¥è¯†å¹¶å‡å°‘æ¥è‡ªä¸åŒä»»åŠ¡çš„å¹²æ‰°ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ä½†æ•ˆæœæ˜¾è‘—çš„å¤šæ¨¡æ€äº¤äº’ç±»å‹çš„ä»»åŠ¡åˆ†ç»„ç­–ç•¥MINTï¼Œå®ƒåœ¨å¤šä»»åŠ¡è°ƒæ•´ä¸­çš„è¡¨ç°è¿œè¶…ç°æœ‰åŸºçº¿ï¼Œæœ‰æ•ˆå¹³è¡¡äº†æ³›åŒ–ä¸ä¸“åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„æ–°çªç ´æºäºæ–°çš„é¢„è®­ç»ƒæ¨¡å¼åŠå¤§è§„æ¨¡æœªæ ‡ç­¾å¤šæ¨¡æ€æ•°æ®çš„åº”ç”¨ã€‚</li>
<li>å•çº¯å¢åŠ æŒ‡ä»¤å¾®è°ƒçš„ä»»åŠ¡æ•°é‡å¹¶ä¸æ€»æ˜¯å¸¦æ¥æ€§èƒ½æå‡ã€‚</li>
<li>ä»»åŠ¡åˆ†ç»„ç­–ç•¥å…³æ³¨è·¨æ¨¡æ€äº¤äº’çš„é‡è¦æ€§ï¼ŒåŒ…æ‹¬å¯»æ‰¾å†—ä½™å…±äº«ä¿¡æ¯ã€ä¼˜å…ˆå¤„ç†å…·æœ‰ç‹¬ç‰¹ä¿¡æ¯çš„æ¨¡æ€ç­‰ã€‚</li>
<li>åˆ†ç»„ç­–ç•¥é¼“åŠ±æ¨¡å‹åœ¨ç»„å†…å­¦ä¹ å¯è½¬ç§»æŠ€èƒ½å¹¶å‡å°‘æ¥è‡ªä¸åŒ¹é…ä»»åŠ¡çš„å¹²æ‰°ã€‚</li>
<li>MINTæ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€äº¤äº’ç±»å‹çš„ä»»åŠ¡åˆ†ç»„ç­–ç•¥ï¼Œè¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
<li>MINTç­–ç•¥æœ‰æ•ˆå¹³è¡¡äº†æ¨¡å‹çš„æ³›åŒ–ä¸ä¸“åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6227f710f1ce885fa3a6c748df9226c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-320e29dcdf9de3a29ff9c850167e4ec7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dbbfbcf76e25da5223c10c5b736a488.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e50ba509cddc17f4767d2be3e3ea694f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-91a59868f89dd3d2ca16d14dde92a0b0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  GUI-Actor Coordinate-Free Visual Grounding for GUI Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b759d998f64c73ee26bda7829463c1aa.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  SVGenius Benchmarking LLMs in SVG Understanding, Editing and Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">22963.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
