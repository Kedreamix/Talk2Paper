<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-06-05  Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and   Semantic Understanding Capability of LLM">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3ccbf004e61afa8d7776c3ee3cb40667.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    75 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-05-更新"><a href="#2025-06-05-更新" class="headerlink" title="2025-06-05 更新"></a>2025-06-05 更新</h1><h2 id="Entity-Augmented-Neuroscience-Knowledge-Retrieval-Using-Ontology-and-Semantic-Understanding-Capability-of-LLM"><a href="#Entity-Augmented-Neuroscience-Knowledge-Retrieval-Using-Ontology-and-Semantic-Understanding-Capability-of-LLM" class="headerlink" title="Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and   Semantic Understanding Capability of LLM"></a>Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and   Semantic Understanding Capability of LLM</h2><p><strong>Authors:Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam</strong></p>
<p>Neuroscience research publications encompass a vast wealth of knowledge. Accurately retrieving existing information and discovering new insights from this extensive literature is essential for advancing the field. However, when knowledge is dispersed across multiple sources, current state-of-the-art retrieval methods often struggle to extract the necessary information. A knowledge graph (KG) can integrate and link knowledge from multiple sources, but existing methods for constructing KGs in neuroscience often rely on labeled data and require domain expertise. Acquiring large-scale, labeled data for a specialized area like neuroscience presents significant challenges. This work proposes novel methods for constructing KG from unlabeled large-scale neuroscience research corpus utilizing large language models (LLM), neuroscience ontology, and text embeddings. We analyze the semantic relevance of neuroscience text segments identified by LLM for building the knowledge graph. We also introduce an entity-augmented information retrieval algorithm to extract knowledge from the KG. Several experiments were conducted to evaluate the proposed approaches, and the results demonstrate that our methods significantly enhance knowledge discovery from the unlabeled neuroscience research corpus. It achieves an F1 score of 0.84 for entity extraction, and the knowledge obtained from the KG improves answers to over 54% of the questions. </p>
<blockquote>
<p>神经科学研究出版物包含了大量的知识。从广泛的文献中准确检索现有信息并发现新见解对于推动该领域的发展至关重要。然而，当知识分散在多个来源时，现有的最先进的检索方法往往难以提取必要的信息。知识图谱（KG）可以整合和链接来自多个来源的知识，但神经科学中构建知识图谱的现有方法通常依赖于标记数据并需要领域专业知识。为神经科学等专业领域获取大规模、标记的数据存在重大挑战。这项工作提出了利用大规模语言模型（LLM）、神经科学本体和文本嵌入，从未标记的大规模神经科学研究语料库中构建知识图谱的新方法。我们分析了LLM识别的神经科学文本片段的语义相关性，以构建知识图谱。我们还引入了一种实体增强信息检索算法，从知识图谱中提取知识。进行了多次实验来评估所提出的方法，结果表明，我们的方法显著提高了从未标记的神经科学研究语料库中发现知识的能力。对于实体提取，它实现了0.84的F1分数，从知识图谱中获得的知识提高了超过54%的问题的答案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03145v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>利用大规模语言模型（LLM）、神经科学本体论和文本嵌入技术，从未标记的大规模神经科学研究语料库中构建知识图谱（KG）。分析LLM识别的神经科学文本片段的语义相关性以构建知识图谱，并引入实体增强信息检索算法从知识图谱中提取知识。实验结果表明，该方法能显著提高从未标记的神经科学研究语料库中发现知识的能力，实体提取的F1分数达到0.84，从知识图谱中获得的知识能回答超过54%的问题。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>利用LLM、神经科学本体论和文本嵌入技术从大规模未标记的神经科学研究语料库中构建知识图谱。</li>
<li>LLM用于识别与神经科学相关的文本片段，增强知识图谱的构建。</li>
<li>引入实体增强信息检索算法，提高从知识图谱中提取知识的能力。</li>
<li>实验证明，该方法在实体提取方面表现出较高的性能，F1分数达到0.84。</li>
<li>知识图谱能够回答超过54%的问题，显著提高了知识发现的效率。</li>
<li>该方法克服了现有神经科学知识图谱构建方法对标记数据的依赖，降低了对领域专家的需求。</li>
<li>该研究为神经科学领域的信息检索和知识发现提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03145">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4680c5bdd6189186187c9ca7e0bd17ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ccbf004e61afa8d7776c3ee3cb40667.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffffd2c6577e2f1cfefd6c5e608c0ba4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec6d003e7e5305496898d78079699f7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3e4e347f1706b5a646d0f4bb5b0cec2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Not-All-Tokens-Are-Meant-to-Be-Forgotten"><a href="#Not-All-Tokens-Are-Meant-to-Be-Forgotten" class="headerlink" title="Not All Tokens Are Meant to Be Forgotten"></a>Not All Tokens Are Meant to Be Forgotten</h2><p><strong>Authors:Xiangyu Zhou, Yao Qiang, Saleh Zare Zade, Douglas Zytko, Prashant Khanduri, Dongxiao Zhu</strong></p>
<p>Large Language Models (LLMs), pre-trained on massive text corpora, exhibit remarkable human-level language understanding, reasoning, and decision-making abilities. However, they tend to memorize unwanted information, such as private or copyrighted content, raising significant privacy and legal concerns. Unlearning has emerged as a promising solution, but existing methods face a significant challenge of over-forgetting. This issue arises because they indiscriminately suppress the generation of all the tokens in forget samples, leading to a substantial loss of model utility. To overcome this challenge, we introduce the Targeted Information Forgetting (TIF) framework, which consists of (1) a flexible targeted information identifier designed to differentiate between unwanted words (UW) and general words (GW) in the forget samples, and (2) a novel Targeted Preference Optimization approach that leverages Logit Preference Loss to unlearn unwanted information associated with UW and Preservation Loss to retain general information in GW, effectively improving the unlearning process while mitigating utility degradation. Extensive experiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF framework enhances unlearning effectiveness while preserving model utility and achieving state-of-the-art results. </p>
<blockquote>
<p>大规模语言模型（LLM）在大量文本语料库上进行预训练，展现出惊人的人类水平语言理解、推理和决策能力。然而，它们有记忆不必要信息的趋势，如私人或版权内容，这引发了隐私和法律的担忧。遗忘作为一种有前途的解决方案而出现，但现有方法面临着过度遗忘的重大挑战。这个问题之所以出现，是因为它们不加区分地抑制遗忘样本中所有标记的生成，导致模型效用的大量损失。为了克服这一挑战，我们引入了有针对性的信息遗忘（TIF）框架，它包括（1）一个灵活的有针对性的信息标识符，旨在区分遗忘样本中的不需要的单词（UW）和一般单词（GW），以及（2）一种新的有针对性的偏好优化方法，该方法利用逻辑偏好损失来遗忘与UW相关的不需要的信息，并利用保留损失来保留GW中的一般信息，这有效地改进了遗忘过程并减轻了效用降低的问题。在TOFU和MUSE基准测试上的广泛实验表明，所提出的TIF框架在提高遗忘效果的同时，保持了模型的效用，并实现了最新的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03142v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在海量文本语料库上的预训练，展现出惊人的人类级语言理解、推理和决策能力。然而，它们会记忆不想要的资讯，如私密或版权内容，引发隐私和法律担忧。遗忘技术作为一种解决方案应运而生，但现有方法面临过度遗忘的挑战。这是因为它们会无差别地抑制遗忘样本的所有词汇生成，导致模型效用大量损失。为解决此问题，我们推出针对性遗忘（TIF）框架，包括一）灵活的针对性信息识别器，用于区分遗忘样本中的不需要的词汇（UW）和一般词汇（GW），以及二）新型的目标偏好优化方法，该方法利用对数偏好损失来遗忘与UW相关的不需要的信息，并利用保留损失保持GW中的一般信息，从而改进遗忘过程并缓解效用降低。在TOFU和MUSE基准测试上的广泛实验表明，所提出的TIF框架在提高遗忘效果的同时，保留了模型效用，并实现了最佳结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM展现出人类级别的语言理解、推理和决策能力，但存在记忆不想要的资讯的问题。</li>
<li>现有遗忘技术面临过度遗忘的挑战，会导致模型效用损失。</li>
<li>推出针对性遗忘（TIF）框架以解决过度遗忘问题。</li>
<li>TIF框架包括灵活的针对性信息识别器和目标偏好优化方法。</li>
<li>对数偏好损失用于遗忘不需要的信息，同时保留一般信息。</li>
<li>TIF框架在基准测试上表现优异，提高了遗忘效果并保留了模型效用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03142">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7a4d44adddf2f38057b3bea08a378259.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12c4f322207b886bda5559cdce809fc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6feeee9e343464c4d4c945f5e69c3cb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b8a9a7fb020d27ed99114aeeaeeb2b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e75ff2f50e8cb154b145b26eb597115.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SVGenius-Benchmarking-LLMs-in-SVG-Understanding-Editing-and-Generation"><a href="#SVGenius-Benchmarking-LLMs-in-SVG-Understanding-Editing-and-Generation" class="headerlink" title="SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation"></a>SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation</h2><p><strong>Authors:Siqi Chen, Xinyu Dong, Haolei Xu, Xingyu Wu, Fei Tang, Hang Zhang, Yuchen Yan, Linjuan Wu, Wenqi Zhang, Guiyang Hou, Yongliang Shen, Weiming Lu, Yueting Zhuang</strong></p>
<p>Large Language Models (LLMs) and Multimodal LLMs have shown promising capabilities for SVG processing, yet existing benchmarks suffer from limited real-world coverage, lack of complexity stratification, and fragmented evaluation paradigms. We introduce SVGenius, a comprehensive benchmark comprising 2,377 queries across three progressive dimensions: understanding, editing, and generation. Built on real-world data from 24 application domains with systematic complexity stratification, SVGenius evaluates models through 8 task categories and 18 metrics. We assess 22 mainstream models spanning different scales, architectures, training paradigms, and accessibility levels. Our analysis reveals that while proprietary models significantly outperform open-source counterparts, all models exhibit systematic performance degradation with increasing complexity, indicating fundamental limitations in current approaches; however, reasoning-enhanced training proves more effective than pure scaling for overcoming these limitations, though style transfer remains the most challenging capability across all model types. SVGenius establishes the first systematic evaluation framework for SVG processing, providing crucial insights for developing more capable vector graphics models and advancing automated graphic design applications. Appendix and supplementary materials (including all data and code) are available at <a target="_blank" rel="noopener" href="https://zju-real.github.io/SVGenius">https://zju-real.github.io/SVGenius</a>. </p>
<blockquote>
<p>大规模语言模型（LLM）和多模态LLM在SVG处理方面展现出有前景的能力，然而现有的基准测试存在现实世界覆盖有限、缺乏复杂性分层和评估范式碎片化的问题。我们推出了SVGenius，这是一个包含2377个查询的综合基准测试，涵盖三个递进维度：理解、编辑和生成。SVGenius建立在来自24个应用领域的真实数据基础上，具有系统的复杂性分层，通过8个任务类别和18个指标对模型进行评估。我们评估了22个主流模型，涵盖不同的规模、架构、训练范式和可访问性水平。我们的分析表明，专有模型显著优于开源模型，所有模型在复杂性增加时都表现出系统性性能下降，表明当前方法存在根本性局限；然而，推理增强训练证明在克服这些限制方面比纯粹扩大规模更为有效，尽管风格转换仍是所有模型类型中最具挑战性的能力。SVGenius建立了SVG处理的首个系统评估框架，为开发更强大的矢量图形模型和推动自动化图形设计应用提供了关键见解。附录和补充材料（包括所有数据和代码）可在<a target="_blank" rel="noopener" href="https://zju-real.github.io/SVGenius%E8%8E%B7%E5%8F%96%E3%80%82">https://zju-real.github.io/SVGenius获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03139v1">PDF</a> 19 pages,4 figures, Project page:   <a target="_blank" rel="noopener" href="https://zju-real.github.io/SVGenius">https://zju-real.github.io/SVGenius</a>, Code:   <a target="_blank" rel="noopener" href="https://github.com/ZJU-REAL/SVGenius-Bench">https://github.com/ZJU-REAL/SVGenius-Bench</a></p>
<p><strong>摘要</strong><br>LLM及多模态LLM在处理SVG方面展现强大潜力，但现有基准测试存在真实世界覆盖不足、复杂性分层缺失及评估模式分散等问题。本文介绍SVGenius，一个包含2,377条查询的综合基准测试，涉及理解、编辑和生成三个渐进维度。SVGenius以24个应用领域的真实数据为基础，进行系统复杂性分层，通过8类任务和18项指标评估模型。评估了不同规模、架构、训练模式和可访问性水平的22个主流模型。分析表明，专有模型显著优于开源模型，所有模型在复杂性增加时性能下降，表明当前方法存在根本性局限；然而，推理增强训练比单纯扩大规模更能有效克服这些局限，而风格转换仍是所有模型类型中最具挑战性的能力。SVGenius为SVG处理建立了首个系统评估框架，为开发更强大的矢量图形模型和推动自动化图形设计应用提供了关键见解。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM在处理SVG方面展现出巨大潜力，但仍存在性能挑战。</li>
<li>现有基准测试在真实世界覆盖、复杂性分层和评估模式方面存在缺陷。</li>
<li>SVGenius是一个综合基准测试，包括理解、编辑和生成三个维度，基于真实数据并系统地进行复杂性分层。</li>
<li>评估了不同模型在8类任务中的表现，揭示了专有模型与开源模型之间的性能差异。</li>
<li>所有模型在面临复杂性增加时性能下降，表明当前方法的局限性。</li>
<li>推理增强训练是克服这些局限的有效方法，但风格转换仍是最大的挑战。</li>
<li>SVGenius为SVG处理提供了首个系统评估框架，对开发更强大的矢量图形模型和推动自动化图形设计应用至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03139">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-58a6d72117ec17bf46d48d48f3da4683.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-911785cf02b8ecad94363a3c6f0cdf03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e415562d2f3506f61cb037033c32ee98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e805377ad9d45bd94e33af76afcc5793.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0960e4b845c2c4d2f69824e234edb66.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Co-Evolving-LLM-Coder-and-Unit-Tester-via-Reinforcement-Learning"><a href="#Co-Evolving-LLM-Coder-and-Unit-Tester-via-Reinforcement-Learning" class="headerlink" title="Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning"></a>Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning</h2><p><strong>Authors:Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, Mengdi Wang</strong></p>
<p>We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder’s mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/CURE">https://github.com/Gen-Verse/CURE</a> </p>
<blockquote>
<p>我们提出了CURE，这是一个新型强化学习框架，它具有专门的奖励设计，能够协同进化编码和单元测试生成能力，基于它们的交互结果，而无需任何真实代码进行监督。这种方法使灵活和可扩展的训练成为可能，并允许单元测试器直接从编码器的错误中学习。我们在ReasonFlux-Coder-7B和14B模型上的优化后，在Qwen2.5-Instruct模型上提高了代码生成精度5.3%，最佳N精度提高9.0%，超越了类似规模的Qwen-Coder、DeepSeek-Coder和Seed-Coder。它们自然地扩展到下游任务，如测试时缩放和智能编码，相较于基础模型提升了8.1%。对于长CoT模型，我们的ReasonFlux-Coder-4B始终优于Qwen3-4B，同时在单元测试生成方面达到64.8%的推理效率。值得注意的是，我们还发现我们的模型可以作为基础模型上强化学习的有效奖励模型。项目地址：<a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/CURE">https://github.com/Gen-Verse/CURE</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03136v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/CURE">https://github.com/Gen-Verse/CURE</a></p>
<p><strong>Summary</strong></p>
<p>CURE是一个新型强化学习框架，通过专门设计的奖励机制，使编码和单元测试生成能力能够根据交互结果协同进化，无需任何真实代码作为监督。此方法实现了灵活且可扩展的训练，并允许测试人员直接从开发人员的错误中学习。优化后的ReasonFlux-Coder模型在代码生成准确性和Best-of-N准确性方面分别提高了5.3%和9.0%，并扩展至下游任务如测试时缩放和代理编码，实现了对基准模型的8.1%改进。此外，该模型还可作为强化学习基准模型的有效奖励模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CURE是一个强化学习框架，通过奖励机制协同进化编码和单元测试生成。</li>
<li>该框架无需真实代码监督，允许测试人员从开发人员的错误中学习。</li>
<li>ReasonFlux-Coder模型在代码生成和Best-of-N准确性方面有明显提升。</li>
<li>该模型可扩展到下游任务，如测试时缩放和代理编码，实现对基准模型的改进。</li>
<li>CURE模型表现出较高的灵活性和可扩展性。</li>
<li>CURE模型可作为强化学习基准模型的有效奖励模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03136">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-902559561311a1ca4b03ac49b0afdc0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a99ff1ce497b3f2ad3213571f37ce060.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41a51ccbf880323c0c06901e8dee4b2a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Native-Resolution-Image-Synthesis"><a href="#Native-Resolution-Image-Synthesis" class="headerlink" title="Native-Resolution Image Synthesis"></a>Native-Resolution Image Synthesis</h2><p><strong>Authors:Zidong Wang, Lei Bai, Xiangyu Yue, Wanli Ouyang, Yiyuan Zhang</strong></p>
<p>We introduce native-resolution image synthesis, a novel generative modeling paradigm that enables the synthesis of images at arbitrary resolutions and aspect ratios. This approach overcomes the limitations of conventional fixed-resolution, square-image methods by natively handling variable-length visual tokens, a core challenge for traditional techniques. To this end, we introduce the Native-resolution diffusion Transformer (NiT), an architecture designed to explicitly model varying resolutions and aspect ratios within its denoising process. Free from the constraints of fixed formats, NiT learns intrinsic visual distributions from images spanning a broad range of resolutions and aspect ratios. Notably, a single NiT model simultaneously achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512 benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in advanced large language models, NiT, trained solely on ImageNet, demonstrates excellent zero-shot generalization performance. It successfully generates high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536) and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These findings indicate the significant potential of native-resolution modeling as a bridge between visual generative modeling and advanced LLM methodologies. </p>
<blockquote>
<p>我们引入了原生分辨率图像合成，这是一种新的生成建模范式，能够合成任意分辨率和长宽比的图像。该方法克服了传统固定分辨率、正方形图像方法在可变长度视觉符号处理方面的局限性，这是传统技术面临的核心挑战。为此，我们引入了原生分辨率扩散Transformer（NiT），其架构旨在显式建模其去噪过程中的各种分辨率和长宽比。摆脱固定格式的约束，NiT从跨越广泛分辨率和长宽比的图像中学习内在视觉分布。值得注意的是，一个单一的NiT模型同时在ImageNet-256x256和512x512基准测试上达到了最先进的性能。令人惊讶的是，类似于高级大型语言模型中看到的强大零样本能力，NiT仅在ImageNet上进行训练，便表现出了出色的零样本泛化性能。它成功地生成了先前未见的高分辨率（例如1536x1536）和多种长宽比（例如16:9、3:1、4:3）的高保真图像，如图1所示。这些发现表明原生分辨率建模作为连接视觉生成建模和高级LLM方法之间的桥梁具有显著潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03131v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://wzdthu.github.io/NiT/">https://wzdthu.github.io/NiT/</a></p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种新型图像生成建模方法——原生分辨率图像合成。该方法克服了传统固定分辨率和正方形图像方法的局限性，能够原生处理可变长度的视觉标记，实现了任意分辨率和长宽比的图像合成。为此，引入了原生分辨率扩散Transformer（NiT）架构，其降噪过程中显式地建模了各种分辨率和长宽比。NiT模型从广泛的分辨率和长宽比的图像中学习内在视觉分布，并同时达到ImageNet-256x256和512x512基准测试的世界级水平。此外，NiT展示了令人惊讶的零样本泛化能力，能够在未见过的高分辨率（如1536x1536）和多种长宽比下生成高质量图像。这显示了原生分辨率建模作为视觉生成建模与高级LLM方法之间的桥梁的巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入原生分辨率图像合成，实现任意分辨率和长宽比的图像合成。</li>
<li>克服传统固定分辨率和正方形图像方法的局限性，原生处理可变长度的视觉标记。</li>
<li>引入NiT架构，显式建模各种分辨率和长宽比的图像。</li>
<li>NiT模型从广泛分辨率和长宽比的图像中学习内在视觉分布。</li>
<li>NiT在ImageNet基准测试上达到世界级水平，同时支持多种分辨率和长宽比。</li>
<li>NiT展示出令人惊讶的零样本泛化能力，能在未见过的分辨率和长宽比下生成高质量图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03131">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-929d19aea750e5079c352a3187a9a4ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70e239deb8da10663506e6c4d0859756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d251326e6696fddd7af2ff4e7ef2ebb2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AUTOCIRCUIT-RL-Reinforcement-Learning-Driven-LLM-for-Automated-Circuit-Topology-Generation"><a href="#AUTOCIRCUIT-RL-Reinforcement-Learning-Driven-LLM-for-Automated-Circuit-Topology-Generation" class="headerlink" title="AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit   Topology Generation"></a>AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit   Topology Generation</h2><p><strong>Authors:Prashanth Vijayaraghavan, Luyao Shi, Ehsan Degan, Vandana Mukherjee, Xin Zhang</strong></p>
<p>Analog circuit topology synthesis is integral to Electronic Design Automation (EDA), enabling the automated creation of circuit structures tailored to specific design requirements. However, the vast design search space and strict constraint adherence make efficient synthesis challenging. Leveraging the versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel reinforcement learning (RL)-based framework for automated analog circuit synthesis. The framework operates in two phases: instruction tuning, where an LLM learns to generate circuit topologies from structured prompts encoding design constraints, and RL refinement, which further improves the instruction-tuned model using reward models that evaluate validity, efficiency, and output voltage. The refined model is then used directly to generate topologies that satisfy the design constraints. Empirical results show that AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by ~14% compared to the best baselines, while reducing duplicate generation rates by ~38%. It achieves over 60% success in synthesizing valid circuits with limited training data, demonstrating strong generalization. These findings highlight the framework’s effectiveness in scaling to complex circuits while maintaining efficiency and constraint adherence, marking a significant advancement in AI-driven circuit design. </p>
<blockquote>
<p>模拟电路拓扑合成是电子设计自动化（EDA）的重要组成部分，能够实现针对特定设计要求的电路结构的自动化创建。然而，庞大的设计搜索空间和严格约束的限制使得高效的合成面临挑战。我们利用大型语言模型（LLM）的通用性，提出了基于强化学习（RL）的新型自动化模拟电路合成框架——AUTOCIRCUIT-RL。该框架分为两个阶段：指令调整阶段，LLM从编码设计约束的结构化提示中学习生成电路拓扑；RL精炼阶段，使用评估有效性、效率和输出电压的奖励模型进一步改进指令调整模型。改进后的模型可直接用于生成满足设计约束的拓扑结构。经验结果表明，与最佳基线相比，AUTOCIRCUIT-RL生成的有效电路数量增加了约12%，效率提高了约14%，重复生成率降低了约38%。在有限训练数据的情况下，它成功合成有效电路的比例超过60%，显示出强大的泛化能力。这些发现突显了该框架在扩展到复杂电路时保持效率和约束遵守的有效性，标志着人工智能驱动电路设计的一个重大进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03122v1">PDF</a> 9 Pages (Content), 4 Pages (Appendix), 7 figures, ICML’2025</p>
<p><strong>Summary</strong>：基于大型语言模型（LLM）的强化学习框架——AUTOCIRCUIT-RL在模拟电路设计自动化中具有突出表现。框架包括指令微调与强化学习优化两个阶段，提高了模拟电路设计的有效性和效率。该方法通过结构化提示编码设计约束，学习生成电路拓扑结构，并利用奖励模型评估其合理性、效率和输出电压进行进一步优化。在复杂电路设计中，该框架展现出良好的扩展性、效率和约束遵守能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>AUTOCIRCUIT-RL是结合强化学习（RL）的大型语言模型（LLM）框架，用于模拟电路设计自动化。</li>
<li>框架包含指令微调阶段，通过结构化提示学习生成满足设计约束的电路拓扑。</li>
<li>RL优化阶段利用奖励模型评估电路的有效性、效率和输出电压，进一步优化模型。</li>
<li>该方法相较于其他基线方法，能生成约12%更多的有效电路，提高约14%的效率，并降低约38%的重复生成率。</li>
<li>在有限训练数据下，该框架成功合成有效电路的比例达到60%以上，显示出强大的泛化能力。</li>
<li>AUTOCIRCUIT-RL在复杂电路设计中表现出良好的扩展性，同时保持高效率和对设计约束的遵守。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dba09cc14b1e9c04d4fbd3ede252a7fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd7befe39c2479dfcc7a9ee41111da77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d05d280c668e1115f804690f8acbc816.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Critique-GRPO-Advancing-LLM-Reasoning-with-Natural-Language-and-Numerical-Feedback"><a href="#Critique-GRPO-Advancing-LLM-Reasoning-with-Natural-Language-and-Numerical-Feedback" class="headerlink" title="Critique-GRPO: Advancing LLM Reasoning with Natural Language and   Numerical Feedback"></a>Critique-GRPO: Advancing LLM Reasoning with Natural Language and   Numerical Feedback</h2><p><strong>Authors:Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chao Yang, Helen Meng</strong></p>
<p>Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration. </p>
<blockquote>
<p>最近，强化学习（RL）在数值反馈方面的进展，如标量奖励，极大地增强了大型语言模型（LLM）的复杂推理能力。然而，我们发现了仅使用数值反馈的RL所面临的三大挑战：性能高原期、自我反思的局限性以及持续失败的问题。接着我们证明，即使在性能高原期后，通过利用批判形式的自然语言反馈，RL微调过的模型也能对持续失败的问题进行正确的改进。基于这一发现，我们提出了Critique-GRPO，这是一个在线RL框架，它结合了自然语言反馈和数值反馈，以实现有效的策略优化。Critique-GRPO使LLM能够同时从初始响应和批判指导的改进中学习，同时保持探索。使用Qwen2.5-7B-Base和Qwen3-8B-Base的广泛实验表明，Critique-GRPO在八个具有挑战性的数学、STEM和一般推理任务上始终优于基于监督学习和RL的微调方法，平均pass@1得分分别提高了约4.5%和5%。值得注意的是，Critique-GRPO超越了一个强大的基线，该基线结合了在线RL中的专家演示。进一步的分析揭示了关于策略探索的两个关键见解：（1）高熵并不总是保证从探索中有效学习，（2）更长的响应并不一定能导致更有效的探索。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03106v1">PDF</a> 38 pages</p>
<p><strong>Summary</strong><br>强化学习（RL）结合数值反馈在提升大型语言模型（LLM）的复杂推理能力方面取得了显著进展。然而，RL仅通过数值反馈面临性能瓶颈、自我反思受限和持续失败等三大挑战。通过结合自然语言反馈的形式——批判，即使在性能瓶颈后，RL微调模型也能对持续失败的问题产生正确的改进。基于此，提出了结合自然语言与数值反馈的在线RL框架——Critique-GRPO，用于有效的策略优化。实验表明，Critique-GRPO在多个数学、STEM和一般推理任务上表现优于基于监督学习和RL的微调方法，平均提高约4.5%和5%。关键的是，Critique-GRPO超越了结合专家演示的在线RL基线。对策略探索的进一步分析揭示了两个关键见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习结合数值反馈增强了大型语言模型的复杂推理能力。</li>
<li>单纯依赖数值反馈的强化学习面临性能瓶颈、自我反思受限和持续失败等挑战。</li>
<li>利用自然语言反馈（如批判）可以帮助RL微调模型在性能瓶颈后改进持续失败的问题。</li>
<li>提出的Critique-GRPO框架结合了自然语言与数值反馈，实现了有效的策略优化。</li>
<li>实验表明，Critique-GRPO在多个任务上表现优于其他方法，平均提高约4.5%和5%。</li>
<li>Critique-GRPO超越了结合专家演示的在线RL基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03106">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a506a283475479e005cfbc2b64114b64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09fdf08c658752002353e3fafd60c147.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-746679213523921763c70312f3a76f46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237e1b794a8c4695c7c5052ec050975a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TalkingMachines-Real-Time-Audio-Driven-FaceTime-Style-Video-via-Autoregressive-Diffusion-Models"><a href="#TalkingMachines-Real-Time-Audio-Driven-FaceTime-Style-Video-via-Autoregressive-Diffusion-Models" class="headerlink" title="TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via   Autoregressive Diffusion Models"></a>TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via   Autoregressive Diffusion Models</h2><p><strong>Authors:Chetwin Low, Weimin Wang</strong></p>
<p>In this paper, we present TalkingMachines – an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - <a target="_blank" rel="noopener" href="https://aaxwaz.github.io/TalkingMachines/">https://aaxwaz.github.io/TalkingMachines/</a> </p>
<blockquote>
<p>本文介绍了TalkingMachines——一个高效框架，该框架将预训练的视频生成模型转化为实时音频驱动的角色动画。TalkingMachines通过整合音频大语言模型（LLM）与我们的视频生成基础模型，实现了自然的对话体验。我们的主要贡献包括：（1）我们将最先进的图像到视频DiT模型转化为18亿参数的音频驱动化身生成模型；（2）我们利用从双向教师模型到稀疏因果自回归学生模型的不对称知识蒸馏技术，实现了无误差累积的无限视频流；（3）我们设计了一个高吞吐量和低延迟的推理管道，其中包括几项关键工程优化，如（a）在单独的设备上分散DiT和VAE解码器，（b）使用CUDA流有效地重叠设备间通信和计算，（c）消除冗余的重新计算，以最大限度地提高帧生成吞吐量。请参阅演示视频：<a target="_blank" rel="noopener" href="https://aaxwaz.github.io/TalkingMachines/%EF%BC%88%E7%82%B9%E5%87%BB%E9%93%BE%E6%8E%A5%E6%9F%A5%E7%9C%8B%EF%BC%89">https://aaxwaz.github.io/TalkingMachines/（点击链接查看）</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03099v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>TalkingMachines是一个将预训练的视频生成模型转化为实时音频驱动的角色动画框架。它通过整合音频大型语言模型（LLM），使用户能够通过音频控制视频角色动画，实现自然对话体验。主要贡献包括：适应先进图像到视频DiT模型为音频驱动角色生成模型，实现无限视频流且无误差累积，设计高效低延迟推理管道，包括跨设备分解模型和高效重叠通信计算等优化。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TalkingMachines是一个将预训练视频生成模型转化为音频驱动角色动画的框架，提供自然对话体验。</li>
<li>该框架整合了音频大型语言模型（LLM）。</li>
<li>框架成功适应图像到视频的DiT模型用于音频驱动角色生成。</li>
<li>TalkingMachines实现了无限视频流播放且无误差累积。</li>
<li>设计了高效低延迟的推理管道，支持跨设备模型分解和通信计算优化。</li>
<li>通过不对称知识蒸馏技术从双向教师模型到稀疏因果自回归学生模型，提高了模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03099">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e88bbe6307ae82f0717ada0b71c9f76e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fb8015874355c492456eb37cc85db06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb0e86075fab363d9cdfd4f959e7217.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DPO-Learning-with-LLMs-Judge-Signal-for-Computer-Use-Agents"><a href="#DPO-Learning-with-LLMs-Judge-Signal-for-Computer-Use-Agents" class="headerlink" title="DPO Learning with LLMs-Judge Signal for Computer Use Agents"></a>DPO Learning with LLMs-Judge Signal for Computer Use Agents</h2><p><strong>Authors:Man Luo, David Cobbley, Xin Su, Shachar Rosenman, Vasudev Lal, Shao-Yen Tseng, Phillip Howard</strong></p>
<p>Computer use agents (CUA) are systems that automatically interact with graphical user interfaces (GUIs) to complete tasks. CUA have made significant progress with the advent of large vision-language models (VLMs). However, these agents typically rely on cloud-based inference with substantial compute demands, raising critical privacy and scalability concerns, especially when operating on personal devices. In this work, we take a step toward privacy-preserving and resource-efficient agents by developing a lightweight vision-language model that runs entirely on local machines. To train this compact agent, we introduce an LLM-as-Judge framework that automatically evaluates and filters synthetic interaction trajectories, producing high-quality data for reinforcement learning without human annotation. Experiments on the OS-World benchmark demonstrate that our fine-tuned local model outperforms existing baselines, highlighting a promising path toward private, efficient, and generalizable GUI agents. </p>
<blockquote>
<p>计算机使用代理（CUA）是自动与图形用户界面（GUI）交互以完成任务的系统。随着大型视觉语言模型（VLM）的出现，CUA已经取得了重大进展。然而，这些代理通常依赖于具有大量计算需求的云推理，这引发了关于隐私和可扩展性的关键问题，特别是在个人设备上运行时。在这项工作中，我们通过开发一种完全运行在本地机器上的轻量级视觉语言模型，朝着保护隐私和资源高效的代理迈出了一步。为了训练这种紧凑的代理，我们引入了一个名为“LLM-as-Judge”的框架，该框架自动评估并过滤合成交互轨迹，从而在无需人工注释的情况下为强化学习生成高质量数据。在OS-World基准测试上的实验表明，我们微调的本地模型超越了现有基线，突显出了一条朝着私密、高效和可通用的GUI代理的充满希望的道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03095v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着大型视觉语言模型的发展，计算机使用代理（CUA）在自动与图形用户界面（GUI）交互完成任务方面取得了显著进步。然而，这些代理通常依赖于具有大量计算需求的云推理，这引发了关于隐私和可扩展性的关键问题。本研究朝向隐私保护和资源高效的代理迈出了重要一步，通过开发可在本地机器上完全运行的轻量级视觉语言模型。为了训练这个紧凑的代理，我们引入了LLM-as-Judge框架，该框架自动评估并过滤合成交互轨迹，从而在无需人工标注的情况下为强化学习提供高质量数据。在OS-World基准测试上的实验表明，我们的精细本地模型超越了现有基线，为私人、高效和可通用的GUI代理指明了有前途的道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>计算机使用代理（CUA）能自动与图形用户界面（GUI）交互完成任务。</li>
<li>大型视觉语言模型（VLM）的发展对CUA的进步起到了推动作用。</li>
<li>CUA通常依赖云推理，引发隐私和可扩展性问题，特别是在个人设备上。</li>
<li>研究人员开发了一种可在本地机器上运行的轻量级视觉语言模型，以提高隐私保护和资源效率。</li>
<li>引入LLM-as-Judge框架训练紧凑代理，该框架能自动评估并过滤合成交互轨迹。</li>
<li>实验证明，精细本地模型在OS-World基准测试上超越了现有基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03095">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8c2a5d65ba93fcef44dd05fa1b19fbea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91a59868f89dd3d2ca16d14dde92a0b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e990b6c43d395076a137f3281e4f53b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-588c2ed368f1623d48f8feba6f93dfea.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="StreamBP-Memory-Efficient-Exact-Backpropagation-for-Long-Sequence-Training-of-LLMs"><a href="#StreamBP-Memory-Efficient-Exact-Backpropagation-for-Long-Sequence-Training-of-LLMs" class="headerlink" title="StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence   Training of LLMs"></a>StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence   Training of LLMs</h2><p><strong>Authors:Qijun Luo, Mengqi Li, Lei Zhao, Xiao Li</strong></p>
<p>Training language models on long sequence data is a demanding requirement for enhancing the model’s capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP’s sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at <a target="_blank" rel="noopener" href="https://github.com/Ledzy/StreamBP">https://github.com/Ledzy/StreamBP</a>. </p>
<blockquote>
<p>训练语言模型处理长序列数据是对复杂任务（如长链推理）提升模型能力的苛刻要求。然而，随着序列长度的增加，反向传播（BP）过程中存储激活值的内存成本变得巨大，即使应用了梯度检查点技术也是如此。为了应对这一挑战，我们提出了一种内存效率高且精确的反向传播方法，称为StreamBP。它逐层沿序列维度进行链式法则的线性分解，显著降低了激活值和logits的内存成本。所提出的方法适用于常见的目标，如SFT、GRPO和DPO。从实现的角度来看，StreamBP利用语言模型的因果结构实现了较少的计算FLOPs和更快的BP速度。与梯度检查点相比，StreamBP将BP的最大序列长度扩大了2.8-5.5倍，同时使用的时间相当甚至更少。请注意，StreamBP的序列长度缩放能力可以直接转移到批处理大小缩放以加速训练。我们进一步开发了一种通信高效的分布式StreamBP，以有效支持多GPU训练并扩大其适用性。我们的代码可以轻松地集成到任何transformer模型的训练流程中，可在<a target="_blank" rel="noopener" href="https://github.com/Ledzy/StreamBP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ledzy/StreamBP找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03077v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为StreamBP的内存高效反向传播方法，通过对序列维度上的链式规则进行逐层线性分解，显著降低了激活值和日志的存储成本。该方法适用于常见目标，如SFT、GRPO和DPO。StreamBP利用语言模型的因果结构，实现了较少的计算量和更快的反向传播速度。与梯度检查点技术相比，StreamBP可将反向传播的最大序列长度提高2.8-5.5倍，同时使用的反向传播时间相当甚至更少。此外，StreamBP的序列长度缩放能力可直接转换为批处理大小缩放，以加速训练。还开发了通信高效的分布式StreamBP，以有效支持多GPU训练和扩大其应用范围。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StreamBP是一种内存高效的反向传播方法，适用于处理长序列数据的语言模型。</li>
<li>通过逐层线性分解链式规则，StreamBP降低了激活值和日志的存储成本。</li>
<li>StreamBP适用于常见目标，如SFT、GRPO和DPO，并可以提高反向传播的最大序列长度。</li>
<li>StreamBP利用语言模型的因果结构，实现了较少的计算量和更快的反向传播速度。</li>
<li>与梯度检查点技术相比，StreamBP在反向传播的最大序列长度方面表现出优越性。</li>
<li>StreamBP的序列长度缩放能力可转换为批处理大小缩放，以加速训练过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03077">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3984fab9e4ed9f1ff1a3f614224d1178.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b573ab45916e33d8df42b945d04dc4b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b10b3514f7590cc86787dc6bc732921c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="EDITOR-Effective-and-Interpretable-Prompt-Inversion-for-Text-to-Image-Diffusion-Models"><a href="#EDITOR-Effective-and-Interpretable-Prompt-Inversion-for-Text-to-Image-Diffusion-Models" class="headerlink" title="EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image   Diffusion Models"></a>EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image   Diffusion Models</h2><p><strong>Authors:Mingzhe Li, Gehao Zhang, Zhenting Wang, Shiqing Ma, Siqi Pan, Richard Cartwright, Juan Zhai</strong></p>
<p>Text-to-image generation models~(e.g., Stable Diffusion) have achieved significant advancements, enabling the creation of high-quality and realistic images based on textual descriptions. Prompt inversion, the task of identifying the textual prompt used to generate a specific artifact, holds significant potential for applications including data attribution, model provenance, and watermarking validation. Recent studies introduced a delayed projection scheme to optimize for prompts representative of the vocabulary space, though challenges in semantic fluency and efficiency remain. Advanced image captioning models or visual large language models can generate highly interpretable prompts, but they often lack in image similarity. In this paper, we propose a prompt inversion technique called \sys for text-to-image diffusion models, which includes initializing embeddings using a pre-trained image captioning model, refining them through reverse-engineering in the latent space, and converting them to texts using an embedding-to-text model. Our experiments on the widely-used datasets, such as MS COCO, LAION, and Flickr, show that our method outperforms existing methods in terms of image similarity, textual alignment, prompt interpretability and generalizability. We further illustrate the application of our generated prompts in tasks such as cross-concept image synthesis, concept manipulation, evolutionary multi-concept generation and unsupervised segmentation. </p>
<blockquote>
<p>文本到图像生成模型（例如Stable Diffusion）已经取得了显著的进步，能够根据文本描述创建高质量和逼真的图像。提示反转（prompt inversion）是识别用于生成特定艺术品的文本提示的任务，在数据归属、模型来源和水印验证等方面具有巨大的应用潜力。最近的研究引入了一种延迟投影方案，以优化代表词汇空间的提示，尽管在语义流畅性和效率方面仍存在挑战。先进的图像标题模型或视觉大型语言模型可以生成高度可解释的提示，但它们往往缺乏图像相似性。在本文中，我们提出了一种针对文本到图像扩散模型的提示反转技术，称为sys。它包括使用预训练的图像标题模型进行初始化嵌入，通过反向工程在潜在空间进行细化，并使用嵌入到文本模型将它们转换为文本。我们在MS COCO、LAION和Flickr等常用数据集上的实验表明，我们的方法在图像相似性、文本对齐、提示可解释性和通用性方面优于现有方法。我们还进一步说明了所生成的提示在跨概念图像合成、概念操作、进化多概念生成和无监督分割等任务中的应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03067v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本生成图像模型（如Stable Diffusion）已取得显著进展，能够根据文本描述生成高质量、逼真的图像。本文提出了一种针对文本生成图像扩散模型的提示反转技术，该技术使用预训练图像描述模型进行初始化嵌入，通过反向工程在潜在空间进行改进，并使用嵌入到文本模型进行转换。实验表明，该方法在图像相似性、文本对齐、提示可解释性和通用性方面优于现有方法，并可用于跨概念图像合成、概念操作、进化多概念生成和无监督分割等任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本生成图像模型如Stable Diffusion已能生成高质量、逼真的图像。</li>
<li>提示反转技术用于确定生成特定工件所使用的文本提示。</li>
<li>提示反转技术在数据归属、模型来源和水印验证等方面有潜在应用。</li>
<li>现有延迟投影方案在语义流畅性和效率方面存在挑战。</li>
<li>本文提出了一种新的提示反转技术，使用预训练图像描述模型进行嵌入初始化，并在潜在空间进行改进。</li>
<li>实验表明，该方法在图像相似性、文本对齐、提示可解释性和通用性方面优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-43dc2320dc52eb70ed687f21b5cb1450.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b59f01fb34c917f32a5c52ebd2692263.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93bb3495cff649f1f893d5094e248419.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56b0fbec6332768e99af37f3f9242fb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f77bf345060414c540acea76b26ec5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c9f84927f75a818d489330c415f2f06.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Leveraging-Information-Retrieval-to-Enhance-Spoken-Language-Understanding-Prompts-in-Few-Shot-Learning"><a href="#Leveraging-Information-Retrieval-to-Enhance-Spoken-Language-Understanding-Prompts-in-Few-Shot-Learning" class="headerlink" title="Leveraging Information Retrieval to Enhance Spoken Language   Understanding Prompts in Few-Shot Learning"></a>Leveraging Information Retrieval to Enhance Spoken Language   Understanding Prompts in Few-Shot Learning</h2><p><strong>Authors:Pierre Lepagnol, Sahar Ghannay, Thomas Gerald, Christophe Servan, Sophie Rosset</strong></p>
<p>Understanding user queries is fundamental in many applications, such as home assistants, booking systems, or recommendations. Accordingly, it is crucial to develop accurate Spoken Language Understanding (SLU) approaches to ensure the reliability of the considered system. Current State-of-the-Art SLU techniques rely on large amounts of training data; however, only limited annotated examples are available for specific tasks or languages.   In the meantime, instruction-tuned large language models (LLMs) have shown exceptional performance on unseen tasks in a few-shot setting when provided with adequate prompts. In this work, we propose to explore example selection by leveraging Information retrieval (IR) approaches to build an enhanced prompt that is applied to an SLU task. We evaluate the effectiveness of the proposed method on several SLU benchmarks. Experimental results show that lexical IR methods significantly enhance performance without increasing prompt length. </p>
<blockquote>
<p>理解用户查询在许多应用中都是至关重要的，例如家庭助理、预订系统或推荐系统。因此，为了确保所考虑系统的可靠性，开发准确的口语理解（SLU）方法至关重要。目前最先进的SLU技术依赖于大量的训练数据；然而，对于特定任务或语言，可用的注释示例相对较少。与此同时，经过指令训练的大型语言模型（LLM）在提供足够提示的情况下，在未见过的任务上表现出了出色的性能。在这项工作中，我们提出利用信息检索（IR）方法进行示例选择，以构建用于SLU任务的增强提示。我们在多个SLU基准测试上评估了所提出方法的有效性。实验结果表明，词汇IR方法在不增加提示长度的情况下显著提高了性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03035v1">PDF</a> Conference paper accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于信息检索（IR）方法的示例选择策略在构建用于Spoken Language Understanding（SLU）任务的增强提示中的应用。在有限的训练数据下，利用指令优化的大型语言模型（LLM）通过增强提示显示了对未见任务的出色性能。实验结果证明，词法IR方法可显著提高SLU性能，且不会增加提示长度。</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03035">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-73a00fdd24165e6819ef8215714c5283.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd36244eb9d0da7db22cfaf0be5f10d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31a25bf9e1c9bdff0bf34a0f3e57116d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-430a0e9622689bd138e2216081fcff75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c2a30f0ec3e3c8e0e8f331f7e1336b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-180c47f5bf3e9358b004cbf8e97e4280.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TestAgent-An-Adaptive-and-Intelligent-Expert-for-Human-Assessment"><a href="#TestAgent-An-Adaptive-and-Intelligent-Expert-for-Human-Assessment" class="headerlink" title="TestAgent: An Adaptive and Intelligent Expert for Human Assessment"></a>TestAgent: An Adaptive and Intelligent Expert for Human Assessment</h2><p><strong>Authors:Junhao Yu, Yan Zhuang, YuXuan Sun, Weibo Gao, Qi Liu, Mingyue Cheng, Zhenya Huang, Enhong Chen</strong></p>
<p>Accurately assessing internal human states is key to understanding preferences, offering personalized services, and identifying challenges in real-world applications. Originating from psychometrics, adaptive testing has become the mainstream method for human measurement and has now been widely applied in education, healthcare, sports, and sociology. It customizes assessments by selecting the fewest test questions . However, current adaptive testing methods face several challenges. The mechanized nature of most algorithms leads to guessing behavior and difficulties with open-ended questions. Additionally, subjective assessments suffer from noisy response data and coarse-grained test outputs, further limiting their effectiveness. To move closer to an ideal adaptive testing process, we propose TestAgent, a large language model (LLM)-powered agent designed to enhance adaptive testing through interactive engagement. This is the first application of LLMs in adaptive testing. TestAgent supports personalized question selection, captures test-takers’ responses and anomalies, and provides precise outcomes through dynamic, conversational interactions. Experiments on psychological, educational, and lifestyle assessments show our approach achieves more accurate results with 20% fewer questions than state-of-the-art baselines, and testers preferred it in speed, smoothness, and other dimensions. </p>
<blockquote>
<p>精确评估人类内部状态是理解偏好、提供个性化服务以及在现实世界中识别挑战的关键。起源于心理测量的自适应测试已成为人类测量的主流方法，现已广泛应用于教育、医疗、体育和社会学领域。它通过选择最少的测试问题来定制评估。然而，当前的自适应测试方法面临一些挑战。大多数算法的机械化特性导致猜测行为和开放式问题上的困难。此外，主观评估受到响应数据嘈杂和测试输出粒度粗糙的限制，进一步降低了其有效性。为了更接近理想的自适应测试过程，我们提出了TestAgent，这是一个由大型语言模型（LLM）驱动的智能体，旨在通过交互式参与增强自适应测试。这是自适应测试中的第一个大型语言模型应用。TestAgent支持个性化的问题选择，捕捉考生的反应和异常情况，并通过动态的对话交互提供精确的结果。在心理、教育和生活方式评估方面的实验表明，我们的方法比最新技术基线少问了20%的问题，取得了更准确的结果，并且在速度、流畅性和其他方面得到了测试人员的青睐。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03032v1">PDF</a> 24 pages,10 figures</p>
<p><strong>Summary</strong><br>自适应测试已成为主流的人性化测量方法，通过选择和个性化提问方式有效应用于教育、医疗和体育等领域。当前面临的挑战包括机械化算法带来的猜测行为、开放式问题处理困难等。为解决这些问题，首次将大型语言模型（LLM）应用于自适应测试领域，提出TestAgent测试代理，支持个性化问题选择，捕捉考生反应和异常情况，通过动态对话交互提供精确结果。实验证明，该方法更准确且问题数量减少20%，测试者在速度、流畅度等方面给出更高评价。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是本文提供的七个关键点概述：</p>
<ul>
<li>理解个人偏好和挑战的关键在于准确评估内部人类状态。</li>
<li>自适应测试是心理测量中主流的个性化测量方法。</li>
<li>自适应测试在教育、医疗、体育和社会学等领域广泛应用。</li>
<li>当前自适应测试面临的挑战包括算法机械化、开放式问题处理等。</li>
<li>大型语言模型（LLM）首次应用于自适应测试领域。</li>
<li>TestAgent支持个性化问题选择，捕捉考生反应和异常情况。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03032">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ef1e3c2879740c6be1f4bf63a93523f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-517031cff37174e2272acc96cb911458.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32cbe637ff3c656a3abaa0b1f285310c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d662018903a88a84a689cb32b232ed9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c2a3c1e16b00b0db90a0cf8812a81b4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="GenFair-Systematic-Test-Generation-for-Fairness-Fault-Detection-in-Large-Language-Models"><a href="#GenFair-Systematic-Test-Generation-for-Fairness-Fault-Detection-in-Large-Language-Models" class="headerlink" title="GenFair: Systematic Test Generation for Fairness Fault Detection in   Large Language Models"></a>GenFair: Systematic Test Generation for Fairness Fault Detection in   Large Language Models</h2><p><strong>Authors:Madhusudan Srinivasan, Jubril Abdel</strong></p>
<p>Large Language Models (LLMs) are increasingly deployed in critical domains, yet they often exhibit biases inherited from training data, leading to fairness concerns. This work focuses on the problem of effectively detecting fairness violations, especially intersectional biases that are often missed by existing template-based and grammar-based testing methods. Previous approaches, such as CheckList and ASTRAEA, provide structured or grammar-driven test generation but struggle with low test diversity and limited sensitivity to complex demographic interactions. To address these limitations, we propose GenFair, a metamorphic fairness testing framework that systematically generates source test cases using equivalence partitioning, mutation operators, and boundary value analysis. GenFair improves fairness testing by generating linguistically diverse, realistic, and intersectional test cases. It applies metamorphic relations (MR) to derive follow-up cases and detects fairness violations via tone-based comparisons between source and follow-up responses. In experiments with GPT-4.0 and LLaMA-3.0, GenFair outperformed two baseline methods. It achieved a fault detection rate (FDR) of 0.73 (GPT-4.0) and 0.69 (LLaMA-3.0), compared to 0.54&#x2F;0.51 for template-based and 0.39&#x2F;0.36 for ASTRAEA. GenFair also showed the highest test case diversity (syntactic:10.06, semantic: 76.68) and strong coherence (syntactic: 291.32, semantic: 0.7043), outperforming both baselines. These results demonstrate the effectiveness of GenFair in uncovering nuanced fairness violations. The proposed method offers a scalable and automated solution for fairness testing and contributes to building more equitable LLMs. </p>
<blockquote>
<p>大型语言模型（LLM）在关键领域的应用越来越广泛，但它们往往表现出由训练数据继承的偏见，引发公平性问题。本研究重点关注有效检测公平违规行为的问题，尤其是基于模板和语法检测的现有测试方法常常遗漏的交叉偏见。此前的CheckList和ASTRAEA等方法提供结构化或基于语法的测试生成，但在测试多样性方面较低，对复杂的种族间交互的敏感性有限。为了克服这些局限性，我们提出了GenFair，一个基于元模型的公平性测试框架，它系统地使用等价分区、变异算子和边界值分析来生成源测试用例。GenFair通过生成语言多样、现实和交叉的测试用例来提高公平性测试。它应用元模型关系（MR）来推导后续案例，并通过源和后续响应之间的基于语气的比较来检测公平违规行为。在与GPT-4.0和LLaMA-3.0的实验中，GenFair的表现优于两种基准方法。它对GPT-4.0的故障检测率（FDR）达到0.73，对LLaMA-3.0的故障检测率达到0.69，而基于模板的方法分别为0.54和0.51，ASTRAEA分别为0.39和0.36。此外，GenFair还显示出最高的测试用例多样性（句法：10.06，语义：76.68）和强相关性（句法：291.32，语义：0.7043），超过了两种基准方法。这些结果证明了GenFair在发现微妙的公平违规行为方面的有效性。所提出的方法为公平性测试提供了可扩展和自动化的解决方案，有助于构建更加公平的LLM。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03024v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在关键领域的应用日益广泛，但存在从训练数据中继承的偏见，引发公平性担忧。针对现有方法难以检测复杂人口特征的交互偏见问题，本文提出一种名为GenFair的变异公平性测试框架。该框架通过等价分区、变异算符和边界值分析系统地生成源测试用例，并结合语调对比检测公平性违规。实验表明，GenFair在GPT-4.0和LLaMA-3.0上的故障检测率高于其他两种方法，并展现出较高的测试用例多样性和连贯性。这证明了GenFair在发现微妙的公平性违规方面的有效性，为公平性的大规模语言模型测试提供了可伸缩和自动化的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在关键领域应用广泛，但存在公平性问题，尤其是检测复杂人口特征的交互偏见问题。</li>
<li>现有方法如CheckList和ASTRAEA在测试多样性和敏感性方面存在局限性。</li>
<li>GenFair是一种新型的公平性测试框架，通过系统地生成源测试用例来提高公平性测试水平。</li>
<li>GenFair结合语调对比检测公平性违规，并通过变异关系推导后续案例。</li>
<li>GenFair在GPT-4.0和LLaMA-3.0上的故障检测率高于其他方法。</li>
<li>GenFair展现出较高的测试用例多样性和连贯性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03024">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6b7eb7a4f53e91d0035e9e0c91a255d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99096ca03d24c26f049ec2f8f179d462.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-953eb25c4b5f68d48ed08f4663999a8c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="HaploOmni-Unified-Single-Transformer-for-Multimodal-Video-Understanding-and-Generation"><a href="#HaploOmni-Unified-Single-Transformer-for-Multimodal-Video-Understanding-and-Generation" class="headerlink" title="HaploOmni: Unified Single Transformer for Multimodal Video Understanding   and Generation"></a>HaploOmni: Unified Single Transformer for Multimodal Video Understanding   and Generation</h2><p><strong>Authors:Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Zunnan Xu, Zhaoyang Zhang, Yixiao Ge, Xiu Li, Ying Shan</strong></p>
<p>With the advancement of language models, unified multimodal understanding and generation have made significant strides, with model architectures evolving from separated components to unified single-model frameworks. This paper explores an efficient training paradigm to build a single transformer for unified multimodal understanding and generation. Specifically, we propose a multimodal warmup strategy utilizing prior knowledge to extend capabilities. To address cross-modal compatibility challenges, we introduce feature pre-scaling and multimodal AdaLN techniques. Integrating the proposed technologies, we present the HaploOmni, a new single multimodal transformer. With limited training costs, HaploOmni achieves competitive performance across multiple image and video understanding and generation benchmarks over advanced unified models. All codes will be made public at <a target="_blank" rel="noopener" href="https://github.com/Tencent/HaploVLM">https://github.com/Tencent/HaploVLM</a>. </p>
<blockquote>
<p>随着语言模型的进步，统一多模态理解和生成技术已经取得了重大突破，模型架构从分离组件发展到统一单模型框架。本文探索了一种高效的训练范式，以建立统一多模态理解和生成的单变压器模型。具体来说，我们提出了一种利用先验知识的多模态预热策略来扩展功能。为了解决跨模态兼容性问题，我们引入了特征预缩放和多模态AdaLN技术。通过整合所提出的技术，我们推出了新的单一多模态变压器HaploOmni。在有限的训练成本下，HaploOmni在多个图像和视频理解和生成基准测试上实现了与先进统一模型相竞争的性能。所有代码将在<a target="_blank" rel="noopener" href="https://github.com/Tencent/HaploVLM%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://github.com/Tencent/HaploVLM上公开。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02975v1">PDF</a> </p>
<p><strong>Summary</strong><br>随着语言模型的发展，统一多模态理解和生成取得了显著进步，模型架构从分离组件演变为统一单模型框架。本文探索了一种有效的训练范式，利用先验知识构建单一转换器进行统一多模态理解和生成。针对跨模态兼容性问题，引入特征预缩放和多模态AdaLN技术。集成这些技术，推出新的单一多模态转换器HaploOmni。在有限的训练成本下，HaploOmni在多个图像和视频理解和生成基准测试中表现优异。相关代码将公开于<a target="_blank" rel="noopener" href="https://github.com/Tencent/HaploVLM%E3%80%82">https://github.com/Tencent/HaploVLM。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>统一多模态理解和生成取得显著进步，模型架构演变为统一单模型框架。</li>
<li>提出一种有效的训练范式，利用先验知识构建单一转换器进行多模态理解和生成。</li>
<li>针对跨模态兼容性问题，引入特征预缩放和多模态AdaLN技术。</li>
<li>推出新的单一多模态转换器HaploOmni。</li>
<li>HaploOmni在多个图像和视频理解和生成基准测试中表现优异。</li>
<li>相关代码将公开于<a target="_blank" rel="noopener" href="https://github.com/Tencent/HaploVLM%EF%BC%8C%E4%BE%BF%E4%BA%8E%E5%85%AC%E4%BC%97%E8%AE%BF%E9%97%AE%E5%92%8C%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/Tencent/HaploVLM，便于公众访问和使用。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02975">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e7550ca7551782ede0cf7d4a53073538.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cf5c039b5b7d0c7098af698ec65565d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26e3cea76573f9e8c3973eaa75099b63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4139d3123626ddea63fdec0997a4bf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a82471d5dd51ca7f42234b41a1de4c18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-274f6f90bd547c35b18481d9c7a1a321.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="FlowerTune-A-Cross-Domain-Benchmark-for-Federated-Fine-Tuning-of-Large-Language-Models"><a href="#FlowerTune-A-Cross-Domain-Benchmark-for-Federated-Fine-Tuning-of-Large-Language-Models" class="headerlink" title="FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large   Language Models"></a>FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large   Language Models</h2><p><strong>Authors:Yan Gao, Massimo Roberto Scamarcia, Javier Fernandez-Marques, Mohammad Naseri, Chong Shen Ng, Dimitris Stripelis, Zexi Li, Tao Shen, Jiamu Bai, Daoyuan Chen, Zikai Zhang, Rui Hu, InSeo Song, Lee KangYoon, Hong Jia, Ting Dang, Junyan Wang, Zheyuan Liu, Daniel Janes Beutel, Lingjuan Lyu, Nicholas D. Lane</strong></p>
<p>Large Language Models (LLMs) have achieved state-of-the-art results across diverse domains, yet their development remains reliant on vast amounts of publicly available data, raising concerns about data scarcity and the lack of access to domain-specific, sensitive information. Federated Learning (FL) presents a compelling framework to address these challenges by enabling decentralized fine-tuning on pre-trained LLMs without sharing raw data. However, the compatibility and performance of pre-trained LLMs in FL settings remain largely under explored. We introduce the FlowerTune LLM Leaderboard, a first-of-its-kind benchmarking suite designed to evaluate federated fine-tuning of LLMs across four diverse domains: general NLP, finance, medical, and coding. Each domain includes federated instruction-tuning datasets and domain-specific evaluation metrics. Our results, obtained through a collaborative, open-source and community-driven approach, provide the first comprehensive comparison across 26 pre-trained LLMs with different aggregation and fine-tuning strategies under federated settings, offering actionable insights into model performance, resource constraints, and domain adaptation. This work lays the foundation for developing privacy-preserving, domain-specialized LLMs for real-world applications. </p>
<blockquote>
<p>大型语言模型（LLM）在多个领域都取得了最先进的成果，然而，它们的发展仍然依赖于大量公开的可用数据，引发了关于数据稀缺和缺乏访问特定领域敏感信息的担忧。联邦学习（FL）提供了一个吸引人的框架来解决这些挑战，它能够在不共享原始数据的情况下，对预训练的LLM进行分散微调。然而，预训练的LLM在FL环境中的兼容性和性能仍然在很大程度上未被探索。我们引入了FlowerTune LLM排行榜，这是一个首创的基准测试套件，旨在评估四个不同领域的联邦微调LLM：通用NLP、金融、医疗和编码。每个领域都包含联邦指令微调数据集和特定领域的评估指标。我们的结果是通过协作、开源和社区驱动的方法获得的，首次全面比较了联邦环境下使用不同聚合和微调策略的26个预训练LLM，为模型性能、资源约束和领域适应提供了可操作性的见解。这项工作为开发用于现实世界应用的隐私保护、领域专用的LLM奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02961v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模语言模型（LLM）在多领域实现了最先进的成果，但其发展仍依赖于大量公开数据，引发了关于数据稀缺和无法访问特定领域敏感信息的担忧。联邦学习（FL）为解决这些挑战提供了一个吸引人的框架，它能够在不共享原始数据的情况下，对预训练的语言模型进行分散式微调。然而，预训练LLM在FL环境中的兼容性和性能尚未得到充分探索。本文介绍了FlowerTune LLM排行榜，这是一个首创的基准测试套件，旨在评估四个不同领域的LLM联邦微调性能：通用NLP、金融、医疗和编码。我们的结果是通过协作、开源和社区驱动的方式获得的，为不同策略下的联邦设置中的26个预训练LLM提供了全面的比较，提供了关于模型性能、资源约束和领域适应性的可操作见解。本工作奠定了开发用于现实世界应用的隐私保护、领域专业化的LLM的基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在多领域表现出色，但需大量公开数据，引发数据稀缺和隐私担忧。</li>
<li>联邦学习（FL）为在不共享原始数据的情况下微调LLM提供了解决方案。</li>
<li>FlowerTune LLM Leaderboard是首个评估LLM在联邦学习环境下性能的基准测试套件。</li>
<li>该套件涵盖了四个不同领域：NLP、金融、医疗和编码。</li>
<li>研究结果提供了关于模型性能、资源约束和领域适应性的深入见解。</li>
<li>联邦设置下的不同策略和聚合方法对LLM性能有影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02961">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-85e81281fbc2831e6b622f9053005860.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c47c116373952fe01bcaeff1bd0d8c77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a02da653d0b1ad4881a97be7f8f14176.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d6c9ee8e234bfdbd39893fbd0e908f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2f14135c7e1d68c86fe26a432833af0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-676e5059303426370ff88d1a792467c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56440a747562c1f9d138effa144df38c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ProcrustesGPT-Compressing-LLMs-with-Structured-Matrices-and-Orthogonal-Transformations"><a href="#ProcrustesGPT-Compressing-LLMs-with-Structured-Matrices-and-Orthogonal-Transformations" class="headerlink" title="ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal   Transformations"></a>ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal   Transformations</h2><p><strong>Authors:Ekaterina Grishina, Mikhail Gorbunov, Maxim Rakhuba</strong></p>
<p>Large language models (LLMs) demonstrate impressive results in natural language processing tasks but require a significant amount of computational and memory resources. Structured matrix representations are a promising way for reducing the number of parameters of these models. However, it seems unrealistic to expect that weight matrices of pretrained models can be accurately represented by structured matrices without any fine-tuning. To overcome this issue, we utilize the fact that LLM output is invariant under certain orthogonal transformations of weight matrices. This insight can be leveraged to identify transformations that significantly improve the compressibility of weights within structured classes. The proposed approach is applicable to various types of structured matrices that support efficient projection operations. Code is available at <a target="_blank" rel="noopener" href="https://github.com/GrishKate/ProcrustesGPT">https://github.com/GrishKate/ProcrustesGPT</a> </p>
<blockquote>
<p>大型语言模型（LLM）在自然语言处理任务中取得了令人印象深刻的结果，但需要大量的计算和内存资源。结构矩阵表示是减少这些模型参数数量的有前途的方法。然而，似乎不切实际地期望通过结构矩阵准确表示预训练模型的权重矩阵而无需进行微调。为了克服这一问题，我们利用LLM输出在权重矩阵的某些正交变换下是不变的这一事实。这一见解可用于确定能够显著提高权重压缩率的转换，这在结构化类别中是有效的。所提出的方法适用于支持高效投影操作的各种类型的结构化矩阵。代码可在<a target="_blank" rel="noopener" href="https://github.com/GrishKate/ProcrustesGPT">https://github.com/GrishKate/ProcrustesGPT</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02818v1">PDF</a> Accepted by ACL Findings</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在自然语言处理任务中表现出卓越的性能，但需要大量的计算和内存资源。结构矩阵表示是减少模型参数的一种有前途的方法。然而，如果不进行微调，很难期望预训练模型的权重矩阵能准确地用结构矩阵表示。为了解决这个问题，我们利用LLM输出在权重矩阵的某些正交变换下是不变的这一事实。这一见解可以用来识别能显著提高权重压缩率的转换，适用于支持高效投影操作的各种类型的结构矩阵。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/GrishKate/ProcrustesGPT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/GrishKate/ProcrustesGPT找到。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在自然语言处理中表现出卓越性能，但计算与内存资源需求巨大。</li>
<li>结构矩阵表示是减少LLM参数的一种策略。</li>
<li>预训练模型的权重矩阵难以仅通过结构矩阵准确表示，需要进行微调。</li>
<li>LLM输出在特定权重矩阵正交变换下具有不变性。</li>
<li>这种不变性有助于找到能提高权重压缩率的转换方法。</li>
<li>所提出的方法适用于支持高效投影操作的各种类型的结构矩阵。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02818">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5af3730e5c90823b279c1e26507b180d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4718210200b236792241e63a4e4c327b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e66785053d105001dde8fd8d94e4ea82.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Open-Set-Living-Need-Prediction-with-Large-Language-Models"><a href="#Open-Set-Living-Need-Prediction-with-Large-Language-Models" class="headerlink" title="Open-Set Living Need Prediction with Large Language Models"></a>Open-Set Living Need Prediction with Large Language Models</h2><p><strong>Authors:Xiaochong Lan, Jie Feng, Yizhou Sun, Chen Gao, Jiahuan Lei, Xinlei Shi, Hengliang Luo, Yong Li</strong></p>
<p>Living needs are the needs people generate in their daily lives for survival and well-being. On life service platforms like Meituan, user purchases are driven by living needs, making accurate living need predictions crucial for personalized service recommendations. Traditional approaches treat this prediction as a closed-set classification problem, severely limiting their ability to capture the diversity and complexity of living needs. In this work, we redefine living need prediction as an open-set classification problem and propose PIGEON, a novel system leveraging large language models (LLMs) for unrestricted need prediction. PIGEON first employs a behavior-aware record retriever to help LLMs understand user preferences, then incorporates Maslow’s hierarchy of needs to align predictions with human living needs. For evaluation and application, we design a recall module based on a fine-tuned text embedding model that links flexible need descriptions to appropriate life services. Extensive experiments on real-world datasets demonstrate that PIGEON significantly outperforms closed-set approaches on need-based life service recall by an average of 19.37%. Human evaluation validates the reasonableness and specificity of our predictions. Additionally, we employ instruction tuning to enable smaller LLMs to achieve competitive performance, supporting practical deployment. </p>
<blockquote>
<p>生活需求是人们日常生活中为生存和福祉所产生的需求。在美团等生活服务平台上，用户购买行为由生活需求驱动，因此对生活需求进行准确预测对于个性化服务推荐至关重要。传统方法将需求预测视为一个封闭集分类问题，这严重限制了其捕捉生活需求多样性和复杂性的能力。</p>
</blockquote>
<p>在本研究中，我们重新定义了生活需求预测作为一个开放集分类问题，并提出了一种新型系统PIGEON，它利用大型语言模型（LLM）进行无限制的需求预测。PIGEON首先采用行为感知记录检索器帮助LLM理解用户偏好，然后结合马斯洛的需求层次理论使预测与人类生活需求相匹配。</p>
<p>为了评估和应用，我们设计了一个基于精细调整文本嵌入模型的召回模块，它将灵活的需求描述与适当的生活服务联系起来。在真实世界数据集上的大量实验表明，PIGEON在基于需求的生活服务召回方面平均比封闭集方法高出19.37%。人类评估验证了我们预测的合理性。此外，我们采用指令微调来使较小的LLM实现竞争性能，支持实际部署。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02713v1">PDF</a> ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>基于美团等生活服务平台，用户购买行为受生活需求驱动。传统方法将生活需求预测视为封闭集分类问题，难以捕捉需求的多样性和复杂性。本文重新定义了开放集分类下的生活需求预测问题，并提出PIGEON系统，利用大语言模型进行无限制需求预测。PIGEON通过行为感知记录检索器帮助LLM理解用户偏好，并结合马斯洛需求层次理论进行需求预测。通过真实数据集的实验验证，PIGEON在生活服务需求召回方面显著优于封闭集方法，平均提高19.37%。人类评估验证了预测的合理性和特异性。此外，通过指令微调使较小规模的LLM也能实现竞争力表现，支持实际应用部署。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生活需求预测在个性化服务推荐中至关重要，特别是对于生活服务平台如美团等。</li>
<li>传统方法处理此类预测时将其视为封闭集分类问题，导致无法充分捕捉需求的多样性和复杂性。</li>
<li>本文提出PIGEON系统，将生活需求预测定义为开放集分类问题，并利用大语言模型进行预测。</li>
<li>PIGEON通过行为感知记录检索器理解用户偏好，并结合马斯洛需求层次理论进行需求预测。</li>
<li>在真实数据集上的实验表明，PIGEON在生活服务需求召回方面显著优于传统方法。</li>
<li>人类评估验证了PIGEON预测的合理性和特异性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02713">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b759d998f64c73ee26bda7829463c1aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-470b6cb049d7f816f0d07facee470408.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d5282eaf1236499e6c946a600777170.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Iterative-Self-Improvement-of-Vision-Language-Models-for-Image-Scoring-and-Self-Explanation"><a href="#Iterative-Self-Improvement-of-Vision-Language-Models-for-Image-Scoring-and-Self-Explanation" class="headerlink" title="Iterative Self-Improvement of Vision Language Models for Image Scoring   and Self-Explanation"></a>Iterative Self-Improvement of Vision Language Models for Image Scoring   and Self-Explanation</h2><p><strong>Authors:Naoto Tanji, Toshihiko Yamasaki</strong></p>
<p>Image scoring is a crucial task in numerous real-world applications. To trust a model’s judgment, understanding its rationale is essential. This paper proposes a novel training method for Vision Language Models (VLMs) to generate not only image scores but also corresponding justifications in natural language. Leveraging only an image scoring dataset and an instruction-tuned VLM, our method enables self-training, utilizing the VLM’s generated text without relying on external data or models. In addition, we introduce a simple method for creating a dataset designed to improve alignment between predicted scores and their textual justifications. By iteratively training the model with Direct Preference Optimization on two distinct datasets and merging them, we can improve both scoring accuracy and the coherence of generated explanations. </p>
<blockquote>
<p>图像评分是众多现实世界应用中的一项关键任务。为了信任模型的判断，理解其原理至关重要。本文提出了一种新型的视觉语言模型（VLM）训练方法，不仅能够生成图像评分，还能生成相应的自然语言解释。仅利用图像评分数据集和指令调整过的VLM，我们的方法能够实现自我训练，利用VLM生成的文本，无需依赖外部数据或模型。此外，我们还介绍了一种创建数据集的简单方法，旨在提高预测分数和其文本解释之间的对齐度。通过在不同的两个数据集上进行直接偏好优化的迭代训练并将它们合并，我们可以提高评分准确性和生成的解释的连贯性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02708v1">PDF</a> Accepted to ICIP2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型训练方法来训练视觉语言模型（VLM），使其不仅能够给出图像评分，还能生成对应的自然语言解释。该方法仅依赖图像评分数据集和指令调优的VLM进行自训练，无需依赖外部数据或模型。通过引入一种简单的方法创建数据集，提高预测分数与文本解释之间的对齐度。通过在两个不同的数据集上进行直接偏好优化（Direct Preference Optimization）的迭代训练并合并，提高了评分准确性和生成的解释的连贯性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（VLM）可以经过训练以生成图像评分和对应的自然语言解释。</li>
<li>训练方法仅依赖图像评分数据集和指令调优的VLM进行自训练。</li>
<li>引入了一种创建数据集的方法，用于提高预测分数与文本解释之间的对齐度。</li>
<li>通过在两个不同的数据集上进行迭代训练，使用直接偏好优化（Direct Preference Optimization）提高了模型的评分准确性。</li>
<li>模型生成的解释具有更高的连贯性。</li>
<li>该方法提高了模型的可信度，因为理解了其评分背后的理由。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02708">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5115258f4a18e03a9a1545e4821b086f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27b171f15419d23e994572813d2d7807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f18a47026bf3fcc1d05c629921e9363.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e71080f2ce9858be96c14a8ccf0d8a63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75287b79268daa82d052f288837f803d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MINT-Multimodal-Instruction-Tuning-with-Multimodal-Interaction-Grouping"><a href="#MINT-Multimodal-Instruction-Tuning-with-Multimodal-Interaction-Grouping" class="headerlink" title="MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping"></a>MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping</h2><p><strong>Authors:Xiaojun Shan, Qi Cao, Xing Han, Haofei Yu, Paul Pu Liang</strong></p>
<p>Recent advances in multimodal foundation models have achieved state-of-the-art performance across a range of tasks. These breakthroughs are largely driven by new pre-training paradigms that leverage large-scale, unlabeled multimodal data, followed by instruction fine-tuning on curated labeled datasets and high-quality prompts. While there is growing interest in scaling instruction fine-tuning to ever-larger datasets in both quantity and scale, our findings reveal that simply increasing the number of instruction-tuning tasks does not consistently yield better performance. Instead, we observe that grouping tasks by the common interactions across modalities, such as discovering redundant shared information, prioritizing modality selection with unique information, or requiring synergistic fusion to discover new information from both modalities, encourages the models to learn transferrable skills within a group while suppressing interference from mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly effective task-grouping strategy based on the type of multimodal interaction. We demonstrate that the proposed method greatly outperforms existing task grouping baselines for multimodal instruction tuning, striking an effective balance between generalization and specialization. </p>
<blockquote>
<p>近期多模态基础模型的进展在多个任务上达到了最先进的性能。这些突破主要得益于新的预训练范式，它利用大规模的无标签多模态数据，然后在精选的标记数据集上进行指令微调，并使用高质量的提示。尽管人们对扩大指令微调以涵盖数量和规模日益增大的数据集的兴趣日益增长，但我们的研究结果表明，仅仅增加指令调整任务的数量并不总能带来更好的性能。相反，我们观察到，通过跨模态的常见交互对任务进行分组，如发现冗余的共享信息、优先选择与独特信息相关的模态，或需要协同融合以从两种模态中发现新信息，可以鼓励模型在组内学习可迁移技能，同时抑制不匹配任务的干扰。为此，我们引入了MINT，这是一种基于多模态交互类型的简单而有效的任务分组策略。我们证明，对于多模态指令调整，所提出的方法大大优于现有的任务分组基线，在泛化和专业化之间达到了有效的平衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02308v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模的多模态基础模型突破得益于新的预训练模式和大规模未标签多模态数据的应用。研究发现在进行指令微调时，单纯增加任务数量并不总能提升性能。相反，通过跨模态交互的任务分组策略，如寻找冗余共享信息、优先处理具有独特信息的模态或协同融合来自不同模态的信息发现新方法等，可以使模型更好地在分组中学习转移知识并减少来自不同任务的干扰。基于此，本文提出了一种简单但效果显著的多模态交互类型的任务分组策略MINT，它在多任务调整中的表现远超现有基线，有效平衡了泛化与专化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态基础模型的新突破源于新的预训练模式及大规模未标签多模态数据的应用。</li>
<li>单纯增加指令微调的任务数量并不总是带来性能提升。</li>
<li>任务分组策略关注跨模态交互的重要性，包括寻找冗余共享信息、优先处理具有独特信息的模态等。</li>
<li>分组策略鼓励模型在组内学习可转移技能并减少来自不匹配任务的干扰。</li>
<li>MINT是一种基于多模态交互类型的任务分组策略，表现优于现有基线。</li>
<li>MINT策略有效平衡了模型的泛化与专化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02308">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6227f710f1ce885fa3a6c748df9226c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-320e29dcdf9de3a29ff9c850167e4ec7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dbbfbcf76e25da5223c10c5b736a488.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e50ba509cddc17f4767d2be3e3ea694f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-91a59868f89dd3d2ca16d14dde92a0b0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-06-05  GUI-Actor Coordinate-Free Visual Grounding for GUI Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b759d998f64c73ee26bda7829463c1aa.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-05  SVGenius Benchmarking LLMs in SVG Understanding, Editing and Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">22963.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
