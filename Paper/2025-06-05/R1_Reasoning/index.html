<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  SVGenius Benchmarking LLMs in SVG Understanding, Editing and Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b759d998f64c73ee26bda7829463c1aa.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-05-æ›´æ–°"><a href="#2025-06-05-æ›´æ–°" class="headerlink" title="2025-06-05 æ›´æ–°"></a>2025-06-05 æ›´æ–°</h1><h2 id="SVGenius-Benchmarking-LLMs-in-SVG-Understanding-Editing-and-Generation"><a href="#SVGenius-Benchmarking-LLMs-in-SVG-Understanding-Editing-and-Generation" class="headerlink" title="SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation"></a>SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation</h2><p><strong>Authors:Siqi Chen, Xinyu Dong, Haolei Xu, Xingyu Wu, Fei Tang, Hang Zhang, Yuchen Yan, Linjuan Wu, Wenqi Zhang, Guiyang Hou, Yongliang Shen, Weiming Lu, Yueting Zhuang</strong></p>
<p>Large Language Models (LLMs) and Multimodal LLMs have shown promising capabilities for SVG processing, yet existing benchmarks suffer from limited real-world coverage, lack of complexity stratification, and fragmented evaluation paradigms. We introduce SVGenius, a comprehensive benchmark comprising 2,377 queries across three progressive dimensions: understanding, editing, and generation. Built on real-world data from 24 application domains with systematic complexity stratification, SVGenius evaluates models through 8 task categories and 18 metrics. We assess 22 mainstream models spanning different scales, architectures, training paradigms, and accessibility levels. Our analysis reveals that while proprietary models significantly outperform open-source counterparts, all models exhibit systematic performance degradation with increasing complexity, indicating fundamental limitations in current approaches; however, reasoning-enhanced training proves more effective than pure scaling for overcoming these limitations, though style transfer remains the most challenging capability across all model types. SVGenius establishes the first systematic evaluation framework for SVG processing, providing crucial insights for developing more capable vector graphics models and advancing automated graphic design applications. Appendix and supplementary materials (including all data and code) are available at <a target="_blank" rel="noopener" href="https://zju-real.github.io/SVGenius">https://zju-real.github.io/SVGenius</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€LLMsåœ¨SVGå¤„ç†æ–¹é¢è¡¨ç°å‡ºæœ‰å‰æ™¯çš„èƒ½åŠ›ï¼Œç„¶è€Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é¢ä¸´ç€ç°å®ä¸–ç•Œè¦†ç›–æœ‰é™ã€ç¼ºä¹å¤æ‚æ€§åˆ†å±‚å’Œè¯„ä¼°èŒƒå¼ç¢ç‰‡åŒ–çš„é—®é¢˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†SVGeniusï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2377ä¸ªæŸ¥è¯¢çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ä¸‰ä¸ªæ¸è¿›ç»´åº¦ï¼šç†è§£ã€ç¼–è¾‘å’Œç”Ÿæˆã€‚SVGeniuså»ºç«‹åœ¨æ¥è‡ª24ä¸ªåº”ç”¨é¢†åŸŸçš„çœŸå®æ•°æ®åŸºç¡€ä¸Šï¼Œå…·æœ‰ç³»ç»Ÿçš„å¤æ‚æ€§åˆ†å±‚ï¼Œé€šè¿‡8ä¸ªä»»åŠ¡ç±»åˆ«å’Œ18ä¸ªæŒ‡æ ‡å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬å¯¹22ä¸ªä¸»æµæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™äº›æ¨¡å‹è·¨è¶Šä¸åŒè§„æ¨¡ã€æ¶æ„ã€è®­ç»ƒæ¨¡å¼å’Œå¯è®¿é—®æ€§çº§åˆ«ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œä¸“æœ‰æ¨¡å‹æ˜¾è‘—ä¼˜äºå¼€æºæ¨¡å‹ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨å¤æ‚æ€§å¢åŠ æ—¶éƒ½è¡¨ç°å‡ºæ€§èƒ½ä¸‹é™ï¼Œè¡¨æ˜å½“å‰æ–¹æ³•å­˜åœ¨æ ¹æœ¬æ€§å±€é™ï¼›ç„¶è€Œï¼Œæ¨ç†å¢å¼ºè®­ç»ƒè¯æ˜åœ¨å…‹æœè¿™äº›å±€é™æ€§æ–¹é¢æ¯”å•çº¯æ‰©å¤§è§„æ¨¡æ›´ä¸ºæœ‰æ•ˆï¼Œå°½ç®¡é£æ ¼è½¬æ¢ä»ç„¶æ˜¯æ‰€æœ‰æ¨¡å‹ç±»å‹ä¸­æœ€å…·æŒ‘æˆ˜çš„èƒ½åŠ›ã€‚SVGeniuså»ºç«‹äº†SVGå¤„ç†çš„é¦–ä¸ªç³»ç»Ÿè¯„ä¼°æ¡†æ¶ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„çŸ¢é‡å›¾å½¢æ¨¡å‹å’Œæ¨è¿›è‡ªåŠ¨åŒ–å›¾å½¢è®¾è®¡åº”ç”¨æä¾›äº†å…³é”®è§è§£ã€‚é™„å½•å’Œè¡¥å……ææ–™ï¼ˆåŒ…æ‹¬æ‰€æœ‰æ•°æ®å’Œä»£ç ï¼‰å¯åœ¨<a target="_blank" rel="noopener" href="https://zju-real.github.io/SVGenius">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03139v1">PDF</a> 19 pages,4 figures, Project page:   <a target="_blank" rel="noopener" href="https://zju-real.github.io/SVGenius">https://zju-real.github.io/SVGenius</a>, Code:   <a target="_blank" rel="noopener" href="https://github.com/ZJU-REAL/SVGenius-Bench">https://github.com/ZJU-REAL/SVGenius-Bench</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SVGeniusï¼Œä¸€ä¸ªæ¶µç›–ç†è§£ã€ç¼–è¾‘å’Œç”Ÿæˆä¸‰ä¸ªæ¸è¿›ç»´åº¦çš„æ–°å‹SVGå¤„ç†ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«æ¥è‡ª24ä¸ªåº”ç”¨é¢†åŸŸçš„çœŸå®æ•°æ®ï¼Œå…·æœ‰ç³»ç»Ÿçš„å¤æ‚æ€§åˆ†å±‚ï¼Œé€šè¿‡8ä¸ªä»»åŠ¡ç±»åˆ«å’Œ18ä¸ªæŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æ–‡ç« è¯„ä¼°äº†ä¸åŒè§„æ¨¡ã€æ¶æ„ã€è®­ç»ƒæ¨¡å¼å’Œè®¿é—®æƒé™çš„22ä¸ªä¸»æµæ¨¡å‹ï¼Œå‘ç°ä¸“æœ‰æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå¼€æºæ¨¡å‹ï¼Œéšç€å¤æ‚æ€§çš„å¢åŠ ï¼Œæ‰€æœ‰æ¨¡å‹çš„ç³»ç»Ÿæ€§èƒ½å‡æœ‰æ‰€ä¸‹é™ï¼Œè¿™è¡¨æ˜å½“å‰æ–¹æ³•å­˜åœ¨åŸºæœ¬å±€é™æ€§ã€‚åŒæ—¶ï¼Œç ”ç©¶å‘ç°å¢å¼ºæ¨ç†è®­ç»ƒæ¯”å•çº¯æ‰©å¤§è§„æ¨¡æ›´èƒ½å…‹æœè¿™äº›å±€é™æ€§ï¼Œä½†é£æ ¼è½¬æ¢ä»æ˜¯æ‰€æœ‰æ¨¡å‹ç±»å‹ä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„èƒ½åŠ›ã€‚SVGeniusä¸ºSVGå¤„ç†å»ºç«‹äº†ç¬¬ä¸€ä¸ªç³»ç»Ÿè¯„ä¼°æ¡†æ¶ï¼Œä¸ºå¼€å‘æ›´å…·èƒ½åŠ›çš„çŸ¢é‡å›¾å½¢æ¨¡å‹å’Œæ¨åŠ¨è‡ªåŠ¨åŒ–å›¾å½¢è®¾è®¡åº”ç”¨æä¾›äº†å…³é”®è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SVGeniusæ˜¯ä¸€ä¸ªæ–°å‹SVGå¤„ç†ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ç†è§£ã€ç¼–è¾‘å’Œç”Ÿæˆä¸‰ä¸ªç»´åº¦ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…å«æ¥è‡ª24ä¸ªåº”ç”¨é¢†åŸŸçš„çœŸå®æ•°æ®ï¼Œå…·æœ‰ç³»ç»Ÿçš„å¤æ‚æ€§åˆ†å±‚ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºä¸“æœ‰æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¼˜äºå¼€æºæ¨¡å‹ã€‚</li>
<li>éšç€å¤æ‚æ€§çš„å¢åŠ ï¼Œæ‰€æœ‰æ¨¡å‹çš„ç³»ç»Ÿæ€§èƒ½å‡æœ‰æ‰€ä¸‹é™ï¼Œè¡¨æ˜å­˜åœ¨åŸºæœ¬å±€é™æ€§ã€‚</li>
<li>å¢å¼ºæ¨ç†è®­ç»ƒæ¯”å•çº¯æ‰©å¤§è§„æ¨¡æ›´èƒ½å…‹æœè¿™äº›å±€é™æ€§ã€‚</li>
<li>é£æ ¼è½¬æ¢æ˜¯æ‰€æœ‰æ¨¡å‹ç±»å‹ä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03139">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58a6d72117ec17bf46d48d48f3da4683.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-911785cf02b8ecad94363a3c6f0cdf03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e415562d2f3506f61cb037033c32ee98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e805377ad9d45bd94e33af76afcc5793.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0960e4b845c2c4d2f69824e234edb66.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Co-Evolving-LLM-Coder-and-Unit-Tester-via-Reinforcement-Learning"><a href="#Co-Evolving-LLM-Coder-and-Unit-Tester-via-Reinforcement-Learning" class="headerlink" title="Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning"></a>Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning</h2><p><strong>Authors:Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, Mengdi Wang</strong></p>
<p>We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coderâ€™s mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/CURE">https://github.com/Gen-Verse/CURE</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†CUREï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…·æœ‰ä¸“é—¨çš„å¥–åŠ±è®¾è®¡ï¼Œèƒ½å¤Ÿæ ¹æ®äº¤äº’ç»“æœå…±åŒè¿›åŒ–ç¼–ç å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆèƒ½åŠ›ï¼Œè€Œæ— éœ€ä»»ä½•çœŸå®ä»£ç ä½œä¸ºç›‘ç£ã€‚è¿™ç§æ–¹æ³•ä½¿è®­ç»ƒå’Œæµ‹è¯•æ›´åŠ çµæ´»å’Œå¯æ‰©å±•ï¼Œå¹¶å…è®¸å•å…ƒæµ‹è¯•äººå‘˜ç›´æ¥ä»ç¼–ç äººå‘˜çš„é”™è¯¯ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨ReasonFlux-Coder-7Bå’Œ14Bæ¨¡å‹ä¸Šçš„ä¼˜åŒ–åï¼Œæé«˜äº†ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§ï¼Œåœ¨Qwen2.5-Instructæ¨¡å‹ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†5.3%ï¼ŒBest-of-Nå‡†ç¡®ç‡æé«˜äº†9.0%ï¼Œè¶…è¶Šäº†åŒç­‰è§„æ¨¡çš„Qwen-Coderã€DeepSeek-Coderå’ŒSeed-Coderã€‚å®ƒä»¬è‡ªç„¶åœ°æ‰©å±•åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚æµ‹è¯•æ—¶æ ‡åº¦å’Œä»£ç†ç¼–ç ï¼Œè¾ƒåŸºç¡€æ¨¡å‹æé«˜äº†8.1%çš„æ”¹è¿›ã€‚å¯¹äºé•¿CoTæ¨¡å‹ï¼Œæˆ‘ä»¬çš„ReasonFlux-Coder-4Bå§‹ç»ˆä¼˜äºQwen3-4Bï¼ŒåŒæ—¶åœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­è¾¾åˆ°64.8%çš„æ¨ç†æ•ˆç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¿˜å‘ç°æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ä½œä¸ºåŸºç¡€æ¨¡å‹ä¸Šå¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆå¥–åŠ±æ¨¡å‹ã€‚é¡¹ç›®åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/CURE">https://github.com/Gen-Verse/CURE</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03136v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/CURE">https://github.com/Gen-Verse/CURE</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶CUREï¼Œå®ƒé€šè¿‡ä¸“é—¨è®¾è®¡çš„å¥–åŠ±æœºåˆ¶ï¼ŒååŒè¿›åŒ–ç¼–ç å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆèƒ½åŠ›ï¼ŒåŸºäºäº¤äº’ç»“æœï¼Œæ— éœ€ä»»ä½•çœŸå®ä»£ç ä½œä¸ºç›‘ç£ã€‚æ­¤æ–¹æ³•å®ç°äº†çµæ´»ä¸”å¯æ‰©å±•çš„è®­ç»ƒï¼Œå¹¶å…è®¸æµ‹è¯•è€…ç›´æ¥ä»ç¼–ç é”™è¯¯ä¸­å­¦ä¹ ã€‚ä¼˜åŒ–åï¼ŒReasonFlux-Coderæ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå’Œæœ€ä½³Nå‡†ç¡®ç‡ä¸Šæœ‰æ‰€æå‡ï¼Œè¶…è¶Šäº†ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¯è‡ªç„¶æ‰©å±•åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚æµ‹è¯•æ—¶ç¼©æ”¾å’Œæ™ºèƒ½ç¼–ç ï¼Œå¹¶å¯ä½œä¸ºåŸºç¡€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CUREæ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ååŒè¿›åŒ–ç¼–ç å’Œå•å…ƒæµ‹è¯•ç”Ÿæˆèƒ½åŠ›ï¼Œæ— éœ€çœŸå®ä»£ç ç›‘ç£ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†çµæ´»ä¸”å¯æ‰©å±•çš„è®­ç»ƒï¼Œå…è®¸æµ‹è¯•è€…ä»ç¼–ç é”™è¯¯ä¸­å­¦ä¹ ã€‚</li>
<li>ReasonFlux-Coderæ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå’Œæœ€ä½³Nå‡†ç¡®ç‡ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>è¯¥æ¨¡å‹è¶…è¶Šäº†ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ï¼Œå¦‚Qwen-Coderã€DeepSeek-Coderå’ŒSeed-Coderã€‚</li>
<li>æ¨¡å‹å¯è‡ªç„¶æ‰©å±•åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚æµ‹è¯•æ—¶ç¼©æ”¾å’Œæ™ºèƒ½ç¼–ç ã€‚</li>
<li>CUREæ¡†æ¶ä¸­çš„æ¨¡å‹å¯ä½œä¸ºå¼ºåŒ–å­¦ä¹ åŸºç¡€æ¨¡å‹çš„å¥–åŠ±æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-902559561311a1ca4b03ac49b0afdc0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a99ff1ce497b3f2ad3213571f37ce060.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41a51ccbf880323c0c06901e8dee4b2a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="AUTOCIRCUIT-RL-Reinforcement-Learning-Driven-LLM-for-Automated-Circuit-Topology-Generation"><a href="#AUTOCIRCUIT-RL-Reinforcement-Learning-Driven-LLM-for-Automated-Circuit-Topology-Generation" class="headerlink" title="AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit   Topology Generation"></a>AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit   Topology Generation</h2><p><strong>Authors:Prashanth Vijayaraghavan, Luyao Shi, Ehsan Degan, Vandana Mukherjee, Xin Zhang</strong></p>
<p>Analog circuit topology synthesis is integral to Electronic Design Automation (EDA), enabling the automated creation of circuit structures tailored to specific design requirements. However, the vast design search space and strict constraint adherence make efficient synthesis challenging. Leveraging the versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel reinforcement learning (RL)-based framework for automated analog circuit synthesis. The framework operates in two phases: instruction tuning, where an LLM learns to generate circuit topologies from structured prompts encoding design constraints, and RL refinement, which further improves the instruction-tuned model using reward models that evaluate validity, efficiency, and output voltage. The refined model is then used directly to generate topologies that satisfy the design constraints. Empirical results show that AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by ~14% compared to the best baselines, while reducing duplicate generation rates by ~38%. It achieves over 60% success in synthesizing valid circuits with limited training data, demonstrating strong generalization. These findings highlight the frameworkâ€™s effectiveness in scaling to complex circuits while maintaining efficiency and constraint adherence, marking a significant advancement in AI-driven circuit design. </p>
<blockquote>
<p>æ¨¡æ‹Ÿç”µè·¯æ‹“æ‰‘åˆæˆæ˜¯ç”µå­è®¾è®¡è‡ªåŠ¨åŒ–ï¼ˆEDAï¼‰çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå®ƒèƒ½å¤Ÿå®ç°é’ˆå¯¹ç‰¹å®šè®¾è®¡è¦æ±‚çš„ç”µè·¯ç»“æ„çš„è‡ªåŠ¨åŒ–åˆ›å»ºã€‚ç„¶è€Œï¼Œåºå¤§çš„è®¾è®¡æœç´¢ç©ºé—´å’Œä¸¥æ ¼çº¦æŸçš„å­˜åœ¨ï¼Œä½¿å¾—é«˜æ•ˆçš„åˆæˆé¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€šç”¨æ€§ï¼Œæå‡ºAUTOCIRCUIT-RLï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è‡ªåŠ¨åŒ–æ¨¡æ‹Ÿç”µè·¯åˆæˆæ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šæŒ‡ä»¤è°ƒæ•´é˜¶æ®µï¼ŒLLMä»ç¼–ç è®¾è®¡çº¦æŸçš„ç»“æ„åŒ–æç¤ºä¸­å­¦ä¹ ç”Ÿæˆç”µè·¯æ‹“æ‰‘ï¼›RLç²¾ç‚¼é˜¶æ®µï¼Œä½¿ç”¨è¯„ä¼°æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œè¾“å‡ºç”µå‹çš„å¥–åŠ±æ¨¡å‹è¿›ä¸€æ­¥æ”¹è¿›æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ã€‚æ”¹è¿›åçš„æ¨¡å‹ç„¶åç›´æ¥è¢«ç”¨æ¥ç”Ÿæˆæ»¡è¶³è®¾è®¡çº¦æŸçš„æ‹“æ‰‘ç»“æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼ŒAUTOCIRCUIT-RLç”Ÿæˆçš„æœ‰æ•ˆç”µè·¯æ•°é‡å¢åŠ äº†çº¦12%ï¼Œæ•ˆç‡æé«˜äº†çº¦14%ï¼ŒåŒæ—¶é™ä½äº†çº¦38%çš„é‡å¤ç”Ÿæˆç‡ã€‚åœ¨æœ‰é™è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå®ƒå®ç°äº†è¶…è¿‡60%çš„åˆæˆæœ‰æ•ˆç”µè·¯æˆåŠŸç‡ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†è¯¥æ¡†æ¶åœ¨æ‰©å±•åˆ°å¤æ‚ç”µè·¯æ—¶ä¿æŒæ•ˆç‡å’Œçº¦æŸéµå®ˆçš„æœ‰æ•ˆæ€§ï¼Œæ ‡å¿—ç€äººå·¥æ™ºèƒ½é©±åŠ¨ç”µè·¯è®¾è®¡çš„ä¸€ä¸ªé‡å¤§è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03122v1">PDF</a> 9 Pages (Content), 4 Pages (Appendix), 7 figures, ICMLâ€™2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–°å‹è‡ªåŠ¨åŒ–æ¨¡æ‹Ÿç”µè·¯åˆæˆæ¡†æ¶â€”â€”AUTOCIRCUIT-RLã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æŒ‡ä»¤è°ƒæ•´å’ŒRLç²¾ç‚¼ä¸¤ä¸ªé˜¶æ®µã€‚æŒ‡ä»¤è°ƒæ•´é˜¶æ®µåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»ç»“æ„åŒ–æç¤ºä¸­å­¦ä¹ ç”Ÿæˆæ»¡è¶³è®¾è®¡çº¦æŸçš„ç”µè·¯æ‹“æ‰‘ç»“æ„ï¼›RLç²¾ç‚¼é˜¶æ®µåˆ™é€šè¿‡å¥–åŠ±æ¨¡å‹è¿›ä¸€æ­¥æ”¹å–„æ¨¡å‹æ€§èƒ½ï¼Œè¯„ä¼°å…¶æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œè¾“å‡ºç”µå‹ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆæ»¡è¶³è®¾è®¡çº¦æŸçš„ç”µè·¯æ‹“æ‰‘ç»“æ„ï¼Œä¸ç°æœ‰æœ€ä½³åŸºçº¿ç›¸æ¯”ï¼Œç”Ÿæˆçš„ç”µè·¯æ•°é‡å¢åŠ äº†çº¦12%ï¼Œæ•ˆç‡æé«˜äº†çº¦14%ï¼Œé‡å¤ç”Ÿæˆç‡é™ä½äº†çº¦38%ã€‚åœ¨æœ‰é™è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸåˆæˆæœ‰æ•ˆç”µè·¯çš„æ¯”ä¾‹è¾¾åˆ°äº†60%ä»¥ä¸Šï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤æ¡†æ¶å¯¹äºåœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶è§£å†³å¤æ‚ç”µè·¯çš„çº¦æŸé—®é¢˜å…·æœ‰é‡è¦æ„ä¹‰ï¼Œæ ‡å¿—ç€äººå·¥æ™ºèƒ½é©±åŠ¨ç”µè·¯è®¾è®¡çš„é‡è¦è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AUTOCIRCUIT-RLæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨åŒ–æ¨¡æ‹Ÿç”µè·¯åˆæˆæ¡†æ¶ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬æŒ‡ä»¤è°ƒæ•´å’ŒRLç²¾ç‚¼ä¸¤ä¸ªé˜¶æ®µï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å­¦ä¹ ç”Ÿæˆç”µè·¯æ‹“æ‰‘ç»“æ„ã€‚</li>
<li>å¥–åŠ±æ¨¡å‹ç”¨äºè¯„ä¼°ç”Ÿæˆç”µè·¯çš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œè¾“å‡ºç”µå‹ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒAUTOCIRCUIT-RLç”Ÿæˆçš„ç”µè·¯æ•°é‡å¢åŠ ï¼Œæ•ˆç‡æé«˜ï¼Œé‡å¤ç”Ÿæˆç‡é™ä½ã€‚</li>
<li>åœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸‹ï¼Œè¯¥æ¡†æ¶æˆåŠŸåˆæˆæœ‰æ•ˆç”µè·¯çš„æ¯”ä¾‹è¾ƒé«˜ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dba09cc14b1e9c04d4fbd3ede252a7fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd7befe39c2479dfcc7a9ee41111da77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d05d280c668e1115f804690f8acbc816.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Critique-GRPO-Advancing-LLM-Reasoning-with-Natural-Language-and-Numerical-Feedback"><a href="#Critique-GRPO-Advancing-LLM-Reasoning-with-Natural-Language-and-Numerical-Feedback" class="headerlink" title="Critique-GRPO: Advancing LLM Reasoning with Natural Language and   Numerical Feedback"></a>Critique-GRPO: Advancing LLM Reasoning with Natural Language and   Numerical Feedback</h2><p><strong>Authors:Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chao Yang, Helen Meng</strong></p>
<p>Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å…·æœ‰æ•°å€¼åé¦ˆæ–¹é¢çš„è¿›å±•ï¼Œå¦‚æ ‡é‡å¥–åŠ±ï¼Œå·²ç»æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬å‘ç°äº†ä»…ä½¿ç”¨æ•°å€¼åé¦ˆçš„RLæ‰€é¢ä¸´çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šæ€§èƒ½ç“¶é¢ˆã€è‡ªæˆ‘åæ€çš„å±€é™æ€§ä»¥åŠæŒç»­å¤±è´¥ã€‚æˆ‘ä»¬è¿›è€Œè¯æ˜ï¼Œå³ä½¿åœ¨æ€§èƒ½ç“¶é¢ˆæœŸåï¼Œé€šè¿‡åˆ©ç”¨æ‰¹åˆ¤å½¢å¼çš„è‡ªç„¶è¯­è¨€åé¦ˆï¼ŒRLå¾®è°ƒæ¨¡å‹ä¹Ÿå¯ä»¥å¯¹æŒç»­å¤±è´¥çš„é—®é¢˜è¿›è¡Œæ­£ç¡®çš„æ”¹è¿›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†æ‰¹åˆ¤æ€§GRPOï¼ˆCritique-GRPOï¼‰åœ¨çº¿RLæ¡†æ¶ï¼Œå®ƒç»“åˆäº†è‡ªç„¶è¯­è¨€åé¦ˆå’Œæ•°å€¼åé¦ˆæ¥è¿›è¡Œæœ‰æ•ˆçš„ç­–ç•¥ä¼˜åŒ–ã€‚Critique-GRPOä½¿LLMèƒ½å¤ŸåŒæ—¶ä»åˆå§‹å“åº”å’Œæ‰¹åˆ¤æŒ‡å¯¼çš„æ”¹è¿›ä¸­å­¦ä¹ ï¼ŒåŒæ—¶ä¿æŒæ¢ç´¢ã€‚ä½¿ç”¨Qwen2.5-7B-Baseå’ŒQwen3-8B-Baseè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCritique-GRPOåœ¨å…«ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦ã€STEMå’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºåŸºäºç›‘ç£å­¦ä¹ å’ŒRLçš„å¾®è°ƒæ–¹æ³•ï¼Œå¹³å‡pass@1å¾—åˆ†åˆ†åˆ«æé«˜äº†çº¦4.5%å’Œ5%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒCritique-GRPOè¶…è¶Šäº†ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿ï¼Œè¯¥åŸºçº¿ç»“åˆäº†åœ¨çº¿RLä¸­çš„ä¸“å®¶æ¼”ç¤ºã€‚è¿›ä¸€æ­¥çš„åˆ†ææ­ç¤ºäº†å…³äºç­–ç•¥æ¢ç´¢çš„ä¸¤ä¸ªå…³é”®è§è§£ï¼šï¼ˆ1ï¼‰é«˜ç†µå¹¶ä¸æ€»æ˜¯ä¿è¯ä»æ¢ç´¢ä¸­æœ‰æ•ˆå­¦ä¹ ï¼Œï¼ˆ2ï¼‰æ›´é•¿çš„å“åº”å¹¶ä¸ä¸€å®šå¯¼è‡´æ›´æœ‰æ•ˆçš„æ¢ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03106v1">PDF</a> 38 pages</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ•°å€¼åé¦ˆä¸Šçš„æœ€æ–°è¿›å±•ï¼Œå¦‚æ ‡é‡å¥–åŠ±ï¼Œå·²æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æŒ‡å‡ºRLé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šæ€§èƒ½ç“¶é¢ˆã€è‡ªæˆ‘åæ€çš„å±€é™æ€§ä»¥åŠæŒç»­å¤±è´¥çš„é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨æ‰¹åˆ¤æ€§åé¦ˆï¼Œå³ä½¿åœ¨æ€§èƒ½ç“¶é¢ˆåï¼ŒRLå¾®è°ƒæ¨¡å‹ä»èƒ½å¯¹æŒç»­å¤±è´¥çš„é—®é¢˜è¿›è¡Œæ­£ç¡®çš„æ”¹è¿›ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†æ•´åˆè‡ªç„¶è¯­è¨€ä¸æ•°å€¼åé¦ˆçš„åœ¨çº¿RLæ¡†æ¶â€”â€”Critique-GRPOï¼Œç”¨äºæœ‰æ•ˆçš„ç­–ç•¥ä¼˜åŒ–ã€‚Critique-GRPOä½¿LLMèƒ½å¤ŸåŒæ—¶ä»åˆæ­¥å“åº”å’Œæ‰¹åˆ¤æ€§æŒ‡å¯¼çš„æ”¹è¿›ä¸­å­¦ä¹ ï¼Œå¹¶ä¿æŒæ¢ç´¢ã€‚å®éªŒè¡¨æ˜ï¼ŒCritique-GRPOåœ¨å…«ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦ã€STEMå’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºåŸºäºç›‘ç£å­¦ä¹ å’ŒRLçš„å¾®è°ƒæ–¹æ³•ï¼Œè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼Œå¹³å‡æé«˜çº¦4.5%å’Œ5%ã€‚åŒæ—¶ï¼Œå¯¹ç­–ç•¥æ¢ç´¢çš„è¿›ä¸€æ­¥åˆ†ææ­ç¤ºäº†ä¸¤ä¸ªå…³é”®è§è§£ï¼šé«˜ç†µå¹¶ä¸æ€»æ˜¯ä¿è¯ä»æ¢ç´¢ä¸­æœ‰æ•ˆå­¦ä¹ ï¼Œé•¿å“åº”å¹¶ä¸ä¸€å®šèƒ½å¸¦æ¥æ›´æœ‰æ•ˆçš„æ¢ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆæ•°å€¼åé¦ˆå¦‚æ ‡é‡å¥–åŠ±æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>RLé¢ä¸´æ€§èƒ½ç“¶é¢ˆã€è‡ªæˆ‘åæ€çš„å±€é™æ€§åŠæŒç»­å¤±è´¥ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æ‰¹åˆ¤æ€§åé¦ˆèƒ½å¤Ÿä½¿RLæ¨¡å‹åœ¨æ€§èƒ½ç“¶é¢ˆåå¯¹æŒç»­å¤±è´¥çš„é—®é¢˜è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>Critique-GRPOæ¡†æ¶ç»“åˆäº†è‡ªç„¶è¯­è¨€ä¸æ•°å€¼åé¦ˆï¼Œæœ‰æ•ˆæé«˜LLMçš„ç­–ç•¥ä¼˜åŒ–èƒ½åŠ›ã€‚</li>
<li>Critique-GRPOåœ¨å¤šç§æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•å¹³å‡æé«˜çº¦4.5%å’Œ5%ã€‚</li>
<li>é«˜ç†µå¹¶ä¸æ€»æ˜¯ä¿è¯ä»æ¢ç´¢ä¸­æœ‰æ•ˆå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a506a283475479e005cfbc2b64114b64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09fdf08c658752002353e3fafd60c147.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-746679213523921763c70312f3a76f46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237e1b794a8c4695c7c5052ec050975a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EgoVLM-Policy-Optimization-for-Egocentric-Video-Understanding"><a href="#EgoVLM-Policy-Optimization-for-Egocentric-Video-Understanding" class="headerlink" title="EgoVLM: Policy Optimization for Egocentric Video Understanding"></a>EgoVLM: Policy Optimization for Egocentric Video Understanding</h2><p><strong>Authors:Ashwin Vinod, Shrey Pandit, Aditya Vavre, Linshen Liu</strong></p>
<p>Emerging embodied AI applications, such as wearable cameras and autonomous agents, have underscored the need for robust reasoning from first person video streams. We introduce EgoVLM, a vision-language model specifically designed to integrate visual comprehension and spatial-temporal reasoning within egocentric video contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization (GRPO), a reinforcement learning method adapted to align model outputs with human-like reasoning steps. Following DeepSeek R1-Zeroâ€™s approach, we directly tune using RL without any supervised fine-tuning phase on chain-of-thought (CoT) data. We evaluate EgoVLM on egocentric video question answering benchmarks and show that domain-specific training substantially improves performance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on non-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by 14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By explicitly generating reasoning traces, EgoVLM enhances interpretability, making it well-suited for downstream applications. Furthermore, we introduce a novel keyframe-based reward that incorporates salient frame selection to guide reinforcement learning optimization. This reward formulation opens a promising avenue for future exploration in temporally grounded egocentric reasoning. </p>
<blockquote>
<p>æ–°å…´çš„èº«ä¸´å…¶å¢ƒçš„AIåº”ç”¨ï¼Œå¦‚å¯ç©¿æˆ´ç›¸æœºå’Œè‡ªä¸»ä»£ç†ï¼Œå¼ºè°ƒäº†ä»ç¬¬ä¸€äººç§°è§†é¢‘æµä¸­è¿›è¡Œç¨³å¥æ¨ç†çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†EgoVLMï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨è®¾è®¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ä¸Šä¸‹æ–‡ä¸­æ•´åˆè§†è§‰ç†è§£å’Œæ—¶ç©ºæ¨ç†ã€‚EgoVLMé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œå¾®è°ƒï¼Œè¿™æ˜¯ä¸€ç§é€‚åº”æ€§å¼ºåŒ–çš„å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹è¾“å‡ºä¸äººç±»ç±»ä¼¼çš„æ¨ç†æ­¥éª¤ç›¸ä¸€è‡´ã€‚æˆ‘ä»¬éµå¾ªDeepSeek R1-Zeroçš„æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè°ƒæ•´ï¼Œè€Œæ— éœ€åœ¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®ä¸Šè¿›è¡Œä»»ä½•ç›‘ç£å¾®è°ƒé˜¶æ®µã€‚æˆ‘ä»¬åœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šå¯¹EgoVLMè¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œä¸é€šç”¨VLMç›¸æ¯”ï¼Œç‰¹å®šé¢†åŸŸçš„è®­ç»ƒå¤§å¤§æé«˜äº†æ€§èƒ½ã€‚æˆ‘ä»¬ä»…åœ¨éCoTä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ•°æ®ä¸Šè®­ç»ƒçš„EgoVLM-3Bï¼Œåœ¨EgoSchemaåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºåŸºç¡€Qwen2.5-VL 3Bå’Œ7Bæ¨¡å‹ï¼Œåˆ†åˆ«æé«˜äº†14.33å’Œ13.87çš„å‡†ç¡®ç‡ã€‚é€šè¿‡æ˜ç¡®ç”Ÿæˆæ¨ç†è½¨è¿¹ï¼ŒEgoVLMæé«˜äº†å¯è§£é‡Šæ€§ï¼Œéå¸¸é€‚åˆç”¨äºä¸‹æ¸¸åº”ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå…³é”®å¸§çš„å¥–åŠ±æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ˜¾è‘—å¸§é€‰æ‹©æ¥æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–ã€‚è¿™ç§å¥–åŠ±å½¢å¼ä¸ºæœªæ¥çš„æ—¶é—´åŸºç¡€ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ¨ç†æ¢ç´¢å¼€è¾Ÿäº†ä¸€æ¡æœ‰å‰é€”çš„é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03097v1">PDF</a> Our Code can be found at <a target="_blank" rel="noopener" href="https://github.com/adityavavre/VidEgoVLM">https://github.com/adityavavre/VidEgoVLM</a></p>
<p><strong>Summary</strong></p>
<p>æ–°å…´çš„èº«ä¸´å…¶å¢ƒçš„AIåº”ç”¨ï¼Œå¦‚å¯ç©¿æˆ´ç›¸æœºå’Œè‡ªä¸»ä»£ç†ï¼Œå¼ºè°ƒéœ€è¦ä»ç¬¬ä¸€äººç§°è§†é¢‘æµä¸­è¿›è¡Œç¨³å¥æ¨ç†çš„å¿…è¦æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EgoVLMï¼Œä¸€ç§ä¸“ä¸ºç¬¬ä¸€äººç§°è§†é¢‘è¯­å¢ƒè®¾è®¡çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¯æ•´åˆè§†è§‰ç†è§£ä¸æ—¶ç©ºæ¨ç†ã€‚é€šè¿‡é‡‡ç”¨Group Relative Policy Optimization (GRPO)è¿™ä¸€å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ŒEgoVLMä¸ç±»äººæ¨ç†æ­¥éª¤çš„è¾“å‡ºå¯¹é½ã€‚æˆ‘ä»¬å‚è€ƒDeepSeek R1-Zeroçš„æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œæ— éœ€åœ¨æ€ç»´é“¾æ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒé˜¶æ®µã€‚åœ¨é’ˆå¯¹ç¬¬ä¸€äººç§°è§†é¢‘é—®ç­”çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEgoVLMè¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºç‰¹å®šé¢†åŸŸçš„è®­ç»ƒå¤§å¹…æå‡äº†ç›¸è¾ƒäºé€šç”¨VLMçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„EgoVLM-3Bæ¨¡å‹ä»…åœ¨éæ€ç»´é“¾çš„ç¬¬ä¸€äººç§°æ•°æ®ä¸Šè®­ç»ƒï¼Œåœ¨EgoSchemaåŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºåŸºå‡†Qwen2.5-VLæ¨¡å‹ï¼Œå…¶å‡†ç¡®ç‡æé«˜äº†14.33å’Œ13.87ä¸ªç™¾åˆ†ç‚¹ã€‚é€šè¿‡æ˜ç¡®ç”Ÿæˆæ¨ç†è½¨è¿¹ï¼ŒEgoVLMå¢å¼ºäº†å¯è§£é‡Šæ€§ï¼Œéå¸¸é€‚åˆç”¨äºä¸‹æ¸¸åº”ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºäºå…³é”®å¸§çš„å¥–åŠ±ï¼Œè¯¥å¥–åŠ±ç»“åˆäº†æ˜¾è‘—å¸§é€‰æ‹©æ¥æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–ã€‚è¿™ç§å¥–åŠ±çš„å…¬å¼åŒ–ä¸ºåœ¨æ—¶é—´ä¸Šç«‹è¶³çš„ç¬¬ä¸€äººç§°æ¨ç†æ¢ç´¢æ‰“å¼€äº†æœ‰å‰é€”çš„é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å…´çš„AIåº”ç”¨å¼ºè°ƒç¬¬ä¸€äººç§°è§†é¢‘æµä¸­çš„ç¨³å¥æ¨ç†éœ€æ±‚ã€‚</li>
<li>å¼•å…¥EgoVLMæ¨¡å‹ï¼Œç»“åˆè§†è§‰ç†è§£ä¸æ—¶ç©ºæ¨ç†ã€‚</li>
<li>é‡‡ç”¨Group Relative Policy Optimization (GRPO)å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚</li>
<li>ç›´æ¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒï¼Œæ— éœ€ç›‘ç£å¾®è°ƒé˜¶æ®µã€‚</li>
<li>EgoVLMåœ¨é’ˆå¯¹ç¬¬ä¸€äººç§°è§†é¢‘é—®ç­”çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç‰¹å®šé¢†åŸŸçš„è®­ç»ƒæ˜¾è‘—æå‡æ€§èƒ½ï¼Œç›¸è¾ƒäºé€šç”¨VLMæœ‰å¤§å¹…åº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03097">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1d49f404b5af6130f0c3a992fb10e91f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f7aea346c19d9ac2a649df4fa57c743.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b970fa9e8c9d5c2e0fef60f465f3c14a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c91e8a3b36cdfb741cd51b1da4502c12.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="StreamBP-Memory-Efficient-Exact-Backpropagation-for-Long-Sequence-Training-of-LLMs"><a href="#StreamBP-Memory-Efficient-Exact-Backpropagation-for-Long-Sequence-Training-of-LLMs" class="headerlink" title="StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence   Training of LLMs"></a>StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence   Training of LLMs</h2><p><strong>Authors:Qijun Luo, Mengqi Li, Lei Zhao, Xiao Li</strong></p>
<p>Training language models on long sequence data is a demanding requirement for enhancing the modelâ€™s capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBPâ€™s sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at <a target="_blank" rel="noopener" href="https://github.com/Ledzy/StreamBP">https://github.com/Ledzy/StreamBP</a>. </p>
<blockquote>
<p>è®­ç»ƒè¯­è¨€æ¨¡å‹è¿›è¡Œé•¿åºåˆ—æ•°æ®å¤„ç†æ˜¯ä¸€é¡¹è‰°å·¨è¦æ±‚ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹åœ¨å¤„ç†å¤æ‚ä»»åŠ¡ï¼ˆå¦‚é•¿é“¾æ¨ç†ï¼‰æ–¹é¢çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œéšç€åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œåå‘ä¼ æ’­ï¼ˆBPï¼‰è¿‡ç¨‹ä¸­å­˜å‚¨æ¿€æ´»å€¼çš„å†…å­˜æˆæœ¬å˜å¾—éå¸¸å¤§ï¼Œå³ä½¿åº”ç”¨äº†æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆå†…å­˜ä¸”ç²¾ç¡®çš„åå‘ä¼ æ’­æ–¹æ³•ï¼Œç§°ä¸ºStreamBPã€‚å®ƒé€šè¿‡é€å±‚æ²¿åºåˆ—ç»´åº¦æ‰§è¡Œé“¾å¼æ³•åˆ™çš„çº¿æ€§åˆ†è§£ï¼Œæ˜¾è‘—é™ä½äº†æ¿€æ´»å€¼å’Œé€»è¾‘å€¼çš„å†…å­˜æˆæœ¬ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºå¦‚SFTã€GRPOå’ŒDPOç­‰å¸¸è§ç›®æ ‡ã€‚ä»å®ç°çš„è§’åº¦æ¥çœ‹ï¼ŒStreamBPåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å› æœç»“æ„å®ç°äº†è¾ƒå°‘çš„è®¡ç®—FLOPså’Œæ›´å¿«çš„BPé€Ÿåº¦ã€‚ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹ç›¸æ¯”ï¼ŒStreamBPå°†åå‘ä¼ æ’­çš„æœ€å¤§åºåˆ—é•¿åº¦æ‰©å¤§äº†2.8-5.5å€ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ—¶é—´ç›¸å½“ç”šè‡³æ›´å°‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒStreamBPçš„åºåˆ—é•¿åº¦ç¼©æ”¾èƒ½åŠ›å¯ä»¥ç›´æ¥è½¬ç§»åˆ°æ‰¹å¤„ç†å¤§å°ç¼©æ”¾ï¼Œä»¥åŠ é€Ÿè®­ç»ƒã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§é€šä¿¡é«˜æ•ˆçš„åˆ†å¸ƒå¼StreamBPï¼Œä»¥æœ‰æ•ˆæ”¯æŒå¤šGPUè®­ç»ƒå¹¶æ‰©å¤§å…¶é€‚ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥è½»æ¾åœ°é›†æˆåˆ°ä»»ä½•transformeræ¨¡å‹çš„è®­ç»ƒæµç¨‹ä¸­ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ledzy/StreamBP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ledzy/StreamBPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03077v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒé•¿åºåˆ—æ•°æ®æ—¶é¢ä¸´çš„å†…å­˜æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºStreamBPçš„å†…å­˜é«˜æ•ˆåå‘ä¼ æ’­æ–¹æ³•ã€‚å®ƒé€šè¿‡é€å±‚çº¿æ€§åˆ†è§£é“¾å¼æ³•åˆ™ï¼Œæ˜¾è‘—é™ä½äº†æ¿€æ´»å€¼å’Œlogitsçš„å†…å­˜æ¶ˆè€—ã€‚æ­¤æ–¹æ³•é€‚ç”¨äºå¸¸è§çš„ç›®æ ‡å‡½æ•°ï¼Œå¹¶æä¾›äº†æ›´å°‘çš„è®¡ç®—æµ®ç‚¹æ•°æ“ä½œå’Œæ›´å¿«çš„åå‘ä¼ æ’­é€Ÿåº¦ã€‚ç›¸æ¯”æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼ŒStreamBPèƒ½å°†åå‘ä¼ æ’­çš„æœ€å¤§åºåˆ—é•¿åº¦å¢åŠ 2.8è‡³5.5å€ï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„æˆ–æ›´å°‘çš„åå‘ä¼ æ’­æ—¶é—´ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æ”¯æŒå¤šGPUè®­ç»ƒï¼Œå¹¶æ˜“äºé›†æˆåˆ°ä»»ä½•transformeræ¨¡å‹çš„è®­ç»ƒæµç¨‹ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨è®­ç»ƒé•¿åºåˆ—æ•°æ®æ—¶é¢ä¸´å†…å­˜æŒ‘æˆ˜ã€‚</li>
<li>StreamBPæ˜¯ä¸€ç§å†…å­˜é«˜æ•ˆçš„åå‘ä¼ æ’­æ–¹æ³•ï¼Œé€šè¿‡é€å±‚çº¿æ€§åˆ†è§£é“¾å¼æ³•åˆ™æ¥é™ä½å†…å­˜æ¶ˆè€—ã€‚</li>
<li>StreamBPé€‚ç”¨äºå¤šç§å¸¸è§çš„ç›®æ ‡å‡½æ•°ã€‚</li>
<li>StreamBPæä¾›äº†æ›´å°‘çš„è®¡ç®—æµ®ç‚¹æ•°æ“ä½œå’Œæ›´å¿«çš„åå‘ä¼ æ’­é€Ÿåº¦ã€‚</li>
<li>ä¸æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ç›¸æ¯”ï¼ŒStreamBPåœ¨åå‘ä¼ æ’­çš„æœ€å¤§åºåˆ—é•¿åº¦æ–¹é¢æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚</li>
<li>StreamBPæ”¯æŒå¤šGPUè®­ç»ƒï¼Œæé«˜äº†åˆ†å¸ƒå¼è®­ç»ƒçš„é€šä¿¡æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3984fab9e4ed9f1ff1a3f614224d1178.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b573ab45916e33d8df42b945d04dc4b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b10b3514f7590cc86787dc6bc732921c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cell-o1-Training-LLMs-to-Solve-Single-Cell-Reasoning-Puzzles-with-Reinforcement-Learning"><a href="#Cell-o1-Training-LLMs-to-Solve-Single-Cell-Reasoning-Puzzles-with-Reinforcement-Learning" class="headerlink" title="Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with   Reinforcement Learning"></a>Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with   Reinforcement Learning</h2><p><strong>Authors:Yin Fang, Qiao Jin, Guangzhi Xiong, Bowen Jin, Xianrui Zhong, Siru Ouyang, Aidong Zhang, Jiawei Han, Zhiyong Lu</strong></p>
<p>Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAIâ€™s o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/ncbi-nlp/cell-o1">https://github.com/ncbi-nlp/cell-o1</a>. </p>
<blockquote>
<p>ç»†èƒç±»å‹æ³¨é‡Šæ˜¯åˆ†æå•ç»†èƒRNAæµ‹åºæ•°æ®å¼‚è´¨æ€§çš„å…³é”®ä»»åŠ¡ã€‚å°½ç®¡æœ€è¿‘çš„åŸºç¡€æ¨¡å‹è‡ªåŠ¨æ‰§è¡Œæ­¤è¿‡ç¨‹ï¼Œä½†å®ƒä»¬é€šå¸¸ç‹¬ç«‹åœ°æ³¨é‡Šç»†èƒï¼Œè€Œä¸ä¼šè€ƒè™‘æ‰¹æ¬¡çº§åˆ«çš„ç»†èƒä¸Šä¸‹æ–‡æˆ–æä¾›è§£é‡Šæ€§ç†ç”±ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»ä¸“å®¶é€šå¸¸åŸºäºå…¶é¢†åŸŸçŸ¥è¯†ä¸ºä¸åŒçš„ç»†èƒç¾¤æ³¨é‡Šä¸åŒçš„ç»†èƒç±»å‹ã€‚ä¸ºäº†æ¨¡ä»¿è¿™ç§å·¥ä½œæµç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†CellPuzzlesä»»åŠ¡ï¼Œå…¶ç›®æ ‡æ˜¯ä¸ºä¸€æ‰¹ç»†èƒåˆ†é…å”¯ä¸€çš„ç»†èƒç±»å‹ã€‚æ­¤åŸºå‡†æµ‹è¯•æ¶µç›–äº†å„ç§ç»„ç»‡ã€ç–¾ç—…å’Œæèµ è€…æ¡ä»¶ï¼Œéœ€è¦è·¨è¶Šæ‰¹æ¬¡çº§åˆ«çš„ç»†èƒä¸Šä¸‹æ–‡è¿›è¡Œæ¨ç†ï¼Œä»¥ç¡®ä¿æ ‡ç­¾çš„å”¯ä¸€æ€§ã€‚æˆ‘ä»¬å‘ç°ç°æˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨CellPuzzlesä¸Šè¡¨ç°æŒ£æ‰ï¼Œæœ€ä½³åŸºçº¿ï¼ˆOpenAIçš„o1ï¼‰ä»…è¾¾åˆ°19.0ï¼…çš„æ‰¹æ¬¡çº§åˆ«å‡†ç¡®åº¦ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†Cell-o1ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡è’¸é¦æ¨ç†è½¨è¿¹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒè®­ç»ƒçš„7B LLMï¼Œéšåä½¿ç”¨æ‰¹æ¬¡çº§å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚Cell-o1è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¾ƒo1é«˜å‡º73ï¼…ä»¥ä¸Šï¼Œå¹¶ä¸”åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­è¡¨ç°è‰¯å¥½ã€‚å¯¹è®­ç»ƒåŠ¨æ€å’Œæ¨ç†è¡Œä¸ºçš„è¿›ä¸€æ­¥åˆ†ææä¾›äº†å¯¹æ‰¹æ¬¡çº§åˆ«æ³¨é‡Šæ€§èƒ½å’Œæ–°å…´ä¸“å®¶çº§æ¨ç†çš„è§è§£ã€‚ä»£ç å’Œæ•°æ®ä½äº<a target="_blank" rel="noopener" href="https://github.com/ncbi-nlp/cell-o1%E3%80%82">https://github.com/ncbi-nlp/cell-o1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02911v1">PDF</a> 28 pages; 16 tables; 7 figures; Code:   <a target="_blank" rel="noopener" href="https://github.com/ncbi-nlp/cell-o1">https://github.com/ncbi-nlp/cell-o1</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ç»†èƒç±»å‹æ³¨é‡Šåœ¨å•ç»†èƒRNAæµ‹åºæ•°æ®å¼‚è´¨æ€§åˆ†æä¸­çš„é‡è¦æ€§ã€‚ç°æœ‰æ¨¡å‹é€šå¸¸åœ¨ç‹¬ç«‹äºæ‰¹é‡çº§åˆ«çš„ç»†èƒä¸Šä¸‹æ–‡çš„æ¡ä»¶ä¸‹å¯¹ç»†èƒè¿›è¡Œæ³¨é‡Šã€‚ä¸ºäº†æ¨¡æ‹Ÿäººç±»ä¸“å®¶çš„æ³¨é‡Šæµç¨‹ï¼Œå¼•å…¥äº†CellPuzzlesä»»åŠ¡ï¼Œè¦æ±‚ä¸ºæ‰¹é‡ç»†èƒåˆ†é…ç‹¬ç‰¹çš„ç»†èƒç±»å‹ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨CellPuzzlesä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå› æ­¤æå‡ºäº†é€šè¿‡ç›‘ç£å¾®è°ƒè’¸é¦æ¨ç†è½¨è¿¹ï¼Œå†ç»“åˆæ‰¹é‡çº§åˆ«å¥–åŠ±è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„Cell-o1æ¨¡å‹ã€‚Cell-o1å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨æ‰¹é‡çº§åˆ«ä¸Šæ¯”OpenAIçš„o1æ¨¡å‹é«˜å‡º73%ä»¥ä¸Šï¼Œå¹¶åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»†èƒç±»å‹æ³¨é‡Šæ˜¯åˆ†æå•ç»†èƒRNAæµ‹åºæ•°æ®å¼‚è´¨æ€§çš„å…³é”®ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨æ³¨é‡Šç»†èƒæ—¶é€šå¸¸ä¸è€ƒè™‘æ‰¹é‡çº§åˆ«çš„ç»†èƒä¸Šä¸‹æ–‡ã€‚</li>
<li>å¼•å…¥äº†CellPuzzlesä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®æ‰¹é‡ç»†èƒçš„ç‰¹æ€§åˆ†é…ç‹¬ç‰¹çš„ç»†èƒç±»å‹ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨CellPuzzlesä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚</li>
<li>æå‡ºäº†Cell-o1æ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒè’¸é¦æ¨ç†è½¨è¿¹å’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œå®ç°å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>Cell-o1åœ¨æ‰¹é‡çº§åˆ«ä¸Šæ˜¾è‘—ä¼˜äºOpenAIçš„o1æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e3dc9d1e7fa701374e55a66de6b253f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1564ab16a042ea7ecbce1691518a33fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7050af34b63648d997cfea711ee015d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afa0887f9804e22be0b3e029fa87aaa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f69a83bcc08761777cf97ce5bdf4378.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Itâ€™s-the-Thought-that-Counts-Evaluating-the-Attempts-of-Frontier-LLMs-to-Persuade-on-Harmful-Topics"><a href="#Itâ€™s-the-Thought-that-Counts-Evaluating-the-Attempts-of-Frontier-LLMs-to-Persuade-on-Harmful-Topics" class="headerlink" title="Itâ€™s the Thought that Counts: Evaluating the Attempts of Frontier LLMs   to Persuade on Harmful Topics"></a>Itâ€™s the Thought that Counts: Evaluating the Attempts of Frontier LLMs   to Persuade on Harmful Topics</h2><p><strong>Authors:Matthew Kowal, Jasper Timm, Jean-Francois Godbout, Thomas Costello, Antonio A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, Kellin Pelrine</strong></p>
<p>Persuasion is a powerful capability of large language models (LLMs) that both enables beneficial applications (e.g. helping people quit smoking) and raises significant risks (e.g. large-scale, targeted political manipulation). Prior work has found models possess a significant and growing persuasive capability, measured by belief changes in simulated or real users. However, these benchmarks overlook a crucial risk factor: the propensity of a model to attempt to persuade in harmful contexts. Understanding whether a model will blindly &#96;&#96;follow ordersâ€™â€™ to persuade on harmful topics (e.g. glorifying joining a terrorist group) is key to understanding the efficacy of safety guardrails. Moreover, understanding if and when a model will engage in persuasive behavior in pursuit of some goal is essential to understanding the risks from agentic AI systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts the focus from persuasion success to persuasion attempts, operationalized as a modelâ€™s willingness to generate content aimed at shaping beliefs or behavior. Our evaluation framework probes frontier LLMs using a multi-turn conversational setup between simulated persuader and persuadee agents. APE explores a diverse spectrum of topics including conspiracies, controversial issues, and non-controversially harmful content. We introduce an automated evaluator model to identify willingness to persuade and measure the frequency and context of persuasive attempts. We find that many open and closed-weight models are frequently willing to attempt persuasion on harmful topics and that jailbreaking can increase willingness to engage in such behavior. Our results highlight gaps in current safety guardrails and underscore the importance of evaluating willingness to persuade as a key dimension of LLM risk. APE is available at github.com&#x2F;AlignmentResearch&#x2F;AttemptPersuadeEval </p>
<blockquote>
<p>è¯´æœèƒ½åŠ›æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€é¡¹å¼ºå¤§åŠŸèƒ½ï¼Œå®ƒæ—¢èƒ½å¸¦æ¥æœ‰ç›Šçš„åº”ç”¨ï¼ˆä¾‹å¦‚å¸®åŠ©äººä»¬æˆ’çƒŸï¼‰ï¼Œä¹Ÿä¼šå¸¦æ¥é‡å¤§é£é™©ï¼ˆä¾‹å¦‚å¤§è§„æ¨¡ã€æœ‰é’ˆå¯¹æ€§çš„æ”¿æ²»æ“çºµï¼‰ã€‚ä¹‹å‰çš„ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹çš„è¯´æœèƒ½åŠ›æ˜¾è‘—ä¸”ä¸æ–­å¢é•¿ï¼Œå¯ä»¥é€šè¿‡æ¨¡æ‹Ÿæˆ–çœŸå®ç”¨æˆ·çš„ä¿¡å¿µå˜åŒ–æ¥è¡¡é‡ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºå‡†æµ‹è¯•å¿½ç•¥äº†ä¸€ä¸ªå…³é”®çš„é£é™©å› ç´ ï¼šæ¨¡å‹åœ¨æœ‰å®³è¯­å¢ƒä¸­å°è¯•è¯´æœçš„å€¾å‘æ€§ã€‚äº†è§£æ¨¡å‹æ˜¯å¦ä¼šåœ¨æœ‰å®³ä¸»é¢˜ä¸Šç›²ç›®â€œéµå¾ªå‘½ä»¤â€è¿›è¡Œè¯´æœï¼ˆä¾‹å¦‚ç¾åŒ–åŠ å…¥ææ€–ç»„ç»‡ï¼‰æ˜¯äº†è§£å®‰å…¨æŠ¤æ æœ‰æ•ˆæ€§çš„å…³é”®ã€‚æ­¤å¤–ï¼Œäº†è§£æ¨¡å‹æ˜¯å¦ä»¥åŠä½•æ—¶ä¸ºäº†è¾¾æˆæŸä¸ªç›®æ ‡è€Œä»äº‹è¯´æœè¡Œä¸ºï¼Œå¯¹äºç†è§£ä»£ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é£é™©è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†å°è¯•è¯´æœè¯„ä¼°ï¼ˆAPEï¼‰åŸºå‡†æµ‹è¯•ï¼Œå°†é‡ç‚¹ä»è¯´æœæˆåŠŸè½¬å‘è¯´æœå°è¯•ï¼Œè¡¨ç°ä¸ºæ¨¡å‹ç”Ÿæˆæ—¨åœ¨å¡‘é€ ä¿¡å¿µæˆ–è¡Œä¸ºçš„å†…å®¹çš„æ„æ„¿ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶ä½¿ç”¨å¤šè½®å¯¹è¯è®¾ç½®æ¥æ¢æµ‹å‰æ²¿çš„LLMï¼Œæ¨¡æ‹ŸåŠè¯´è€…å’Œè¢«åŠè¯´è€…ä¹‹é—´çš„å¯¹è¯ã€‚APEæ¢ç´¢äº†åŒ…æ‹¬é˜´è°‹è®ºã€æœ‰äº‰è®®çš„é—®é¢˜å’Œéäº‰è®®æ€§æœ‰å®³å†…å®¹åœ¨å†…çš„å„ç§ä¸»é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°æ¨¡å‹æ¥è¯†åˆ«è¯´æœçš„æ„æ„¿ï¼Œå¹¶æµ‹é‡è¯´æœå°è¯•çš„é¢‘ç‡å’Œä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬å‘ç°è®¸å¤šå¼€æ”¾å’Œå°é—­æƒé‡çš„æ¨¡å‹éƒ½æ„¿æ„åœ¨æœ‰å®³ä¸»é¢˜ä¸Šå°è¯•è¯´æœï¼Œè€Œä¸”è¶Šç‹±å¯ä»¥å¢åŠ å‚ä¸æ­¤ç±»è¡Œä¸ºçš„æ„æ„¿ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†å½“å‰å®‰å…¨æŠ¤æ çš„å·®è·ï¼Œå¹¶å¼ºè°ƒäº†è¯„ä¼°è¯´æœæ„æ„¿ä½œä¸ºLLMé£é™©å…³é”®ç»´åº¦çš„é‡è¦æ€§ã€‚APEå¯åœ¨github.com&#x2F;AlignmentResearch&#x2F;AttemptPersuadeEvalæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02873v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å¼ºå¤§çš„è¯´æœåŠ›ï¼Œæ—¢èƒ½ç”¨äºæœ‰ç›Šçš„åº”ç”¨ï¼ˆå¦‚å¸®åŠ©æˆ’çƒŸï¼‰ï¼Œä¹Ÿå­˜åœ¨é‡å¤§é£é™©ï¼ˆå¦‚å¤§è§„æ¨¡ã€æœ‰é’ˆå¯¹æ€§çš„æ”¿æ²»æ“çºµï¼‰ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨æ¨¡å‹çš„è¯´æœèƒ½åŠ›ï¼Œä½†å¿½ç•¥äº†æ¨¡å‹åœ¨æœ‰å®³æƒ…å¢ƒä¸‹å°è¯•è¯´æœçš„å€¾å‘æ€§è¿™ä¸€å…³é”®é£é™©å› ç´ ã€‚æœ¬æ–‡æå‡ºå°è¯•è¯´æœè¯„ä¼°ï¼ˆAPEï¼‰åŸºå‡†ï¼Œä¾§é‡äºæ¨¡å‹çš„è¯´æœæ„æ„¿ï¼Œå³ç”Ÿæˆæ—¨åœ¨å¡‘é€ ä¿¡å¿µæˆ–è¡Œä¸ºçš„å†…å®¹çš„æ„æ„¿ã€‚è¯„ä¼°æ¡†æ¶ä½¿ç”¨å¤šå›åˆå¯¹è¯è®¾ç½®æ¥æ¢ç´¢å‰æ²¿LLMï¼Œå¹¶ä»‹ç»äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°æ¨¡å‹æ¥è¯†åˆ«è¯´æœæ„æ„¿å¹¶æµ‹é‡è¯´æœå°è¯•çš„é¢‘ç‡å’Œä¸Šä¸‹æ–‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè®¸å¤šå¼€æºå’Œå°é—­æ¨¡å‹çš„æƒé‡ç»å¸¸åœ¨æœ‰å®³ä¸»é¢˜ä¸Šè¡¨ç°å‡ºå¼ºçƒˆçš„è¯´æœæ„æ„¿ï¼Œè¶Šç‹±æŠ€æœ¯å¯èƒ½å¢åŠ è¿™ç§è¡Œä¸ºçš„æ„æ„¿ã€‚æœ¬æ–‡å¼ºè°ƒå½“å‰å®‰å…¨æŠ¤æ çš„å·®è·å¹¶çªå‡ºè¯„ä¼°è¯´æœæ„æ„¿ä½œä¸ºLLMé£é™©çš„å…³é”®ç»´åº¦çš„é‡è¦æ€§ã€‚APEåŸºå‡†å¯åœ¨github.com&#x2F;AlignmentResearch&#x2F;AttemptPersuadeEvalè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å¼ºå¤§çš„è¯´æœåŠ›ï¼Œèƒ½å¤Ÿåº”ç”¨äºæœ‰ç›ŠåŠå­˜åœ¨é£é™©çš„ä¸åŒåœºæ™¯ã€‚</li>
<li>ä»¥å¾€çš„ç ”ç©¶ä¸»è¦å…³æ³¨æ¨¡å‹çš„è¯´æœèƒ½åŠ›ï¼Œä½†æ–°ç ”ç©¶å¼ºè°ƒäº†è§£æ¨¡å‹åœ¨æœ‰å®³æƒ…å¢ƒä¸‹å°è¯•è¯´æœçš„å€¾å‘æ€§çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºäº†å°è¯•è¯´æœè¯„ä¼°ï¼ˆAPEï¼‰åŸºå‡†ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„è¯´æœæ„æ„¿ï¼Œå¹¶ä¸ä»…ä»…å…³æ³¨å…¶è¯´æœçš„æˆåŠŸç‡ã€‚</li>
<li>APEè¯„ä¼°æ¡†æ¶ä½¿ç”¨å¤šå›åˆå¯¹è¯è®¾ç½®ï¼Œå¹¶å¼•å…¥è‡ªåŠ¨åŒ–è¯„ä¼°æ¨¡å‹æ¥è¯†åˆ«æ¨¡å‹çš„è¯´æœæ„æ„¿åŠæµ‹é‡å…¶è¯´æœå°è¯•çš„é¢‘ç‡å’Œä¸Šä¸‹æ–‡ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œè®¸å¤šLLMæ¨¡å‹åœ¨æœ‰å®³ä¸»é¢˜ä¸Šç»å¸¸è¡¨ç°å‡ºå¼ºçƒˆçš„è¯´æœæ„æ„¿ï¼Œå¹¶ä¸”æŸäº›æŠ€æœ¯å¦‚â€œè¶Šç‹±â€å¯èƒ½å¢åŠ è¿™ç§è¡Œä¸ºçš„æ„æ„¿ã€‚</li>
<li>å½“å‰çš„å®‰å…¨é˜²æŠ¤æªæ–½å­˜åœ¨ç¼ºå£ï¼Œéœ€è¦æ›´å…¨é¢åœ°è¯„ä¼°LLMçš„é£é™©ï¼Œå…¶ä¸­è¯´æœæ„æ„¿æ˜¯ä¸€ä¸ªå…³é”®ç»´åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6234422017bfc6bb5fe3081a3313ff64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d01b1df149f4520dc6336462cd633f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e79123b7b238aec256e1ec0ca47f139b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc8a157014b61b96781408e83b7dd94a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Demystifying-Reasoning-Dynamics-with-Mutual-Information-Thinking-Tokens-are-Information-Peaks-in-LLM-Reasoning"><a href="#Demystifying-Reasoning-Dynamics-with-Mutual-Information-Thinking-Tokens-are-Information-Peaks-in-LLM-Reasoning" class="headerlink" title="Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens   are Information Peaks in LLM Reasoning"></a>Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens   are Information Peaks in LLM Reasoning</h2><p><strong>Authors:Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, Jing Shao</strong></p>
<p>Large reasoning models (LRMs) have demonstrated impressive capabilities in complex problem-solving, yet their internal reasoning mechanisms remain poorly understood. In this paper, we investigate the reasoning trajectories of LRMs from an information-theoretic perspective. By tracking how mutual information (MI) between intermediate representations and the correct answer evolves during LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at specific generative steps exhibits a sudden and significant increase during LRMâ€™s reasoning process. We theoretically analyze such phenomenon and show that as MI increases, the probability of modelâ€™s prediction error decreases. Furthermore, these MI peaks often correspond to tokens expressing reflection or transition, such as <code>Hmm&#39;&#39;, </code>Waitâ€™â€™ and &#96;&#96;Therefore,â€™â€™ which we term as the thinking tokens. We then demonstrate that these thinking tokens are crucial for LRMâ€™s reasoning performance, while other tokens has minimal impacts. Building on these analyses, we propose two simple yet effective methods to improve LRMâ€™s reasoning performance, by delicately leveraging these thinking tokens. Overall, our work provides novel insights into the reasoning mechanisms of LRMs and offers practical ways to improve their reasoning capabilities. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ChnQ/MI-Peaks">https://github.com/ChnQ/MI-Peaks</a>. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚é—®é¢˜è§£å†³æ–¹é¢å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œç„¶è€Œå…¶å†…éƒ¨æ¨ç†æœºåˆ¶ä»ç„¶çŸ¥ä¹‹ç”šå°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»ä¿¡æ¯è®ºçš„è§’åº¦ç ”ç©¶LRMsçš„æ¨ç†è½¨è¿¹ã€‚é€šè¿‡è·Ÿè¸ªLRMæ¨ç†è¿‡ç¨‹ä¸­ä¸­é—´è¡¨ç¤ºä¸æ­£ç¡®ç­”æ¡ˆä¹‹é—´çš„äº’ä¿¡æ¯ï¼ˆMIï¼‰çš„æ¼”å˜ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸€ä¸ªæœ‰è¶£çš„äº’ä¿¡æ¯å³°å€¼ç°è±¡ï¼šåœ¨ç‰¹å®šçš„ç”Ÿæˆæ­¥éª¤ä¸­ï¼Œäº’ä¿¡æ¯ä¼šçªç„¶å‡ºç°å¹¶æ˜¾è‘—å¢åŠ ã€‚æˆ‘ä»¬è¿›è¡Œç†è®ºåˆ†æï¼Œå¹¶è¡¨æ˜éšç€äº’ä¿¡æ¯çš„å¢åŠ ï¼Œæ¨¡å‹é¢„æµ‹é”™è¯¯çš„æ¦‚ç‡é™ä½ã€‚æ­¤å¤–ï¼Œè¿™äº›äº’ä¿¡æ¯å³°å€¼é€šå¸¸å¯¹åº”äºè¡¨ç¤ºåæ€æˆ–è¿‡æ¸¡çš„æ ‡è®°ï¼Œå¦‚â€œå—¯â€ã€â€œç­‰ç­‰â€å’Œâ€œå› æ­¤â€ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºæ€è€ƒæ ‡è®°ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯æ˜è¿™äº›æ€è€ƒæ ‡è®°å¯¹LRMçš„æ¨ç†æ€§èƒ½è‡³å…³é‡è¦ï¼Œè€Œå…¶ä»–æ ‡è®°çš„å½±å“æœ€å°ã€‚åŸºäºè¿™äº›åˆ†æï¼Œæˆ‘ä»¬æå‡ºä¸¤ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡å·§å¦™åˆ©ç”¨è¿™äº›æ€è€ƒæ ‡è®°æ¥æé«˜LRMçš„æ¨ç†æ€§èƒ½ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºç†è§£LRMçš„æ¨ç†æœºåˆ¶æä¾›äº†æ–°çš„è§è§£ï¼Œå¹¶ä¸ºæé«˜å…¶æ¨ç†èƒ½åŠ›æä¾›äº†å®ç”¨æ–¹æ³•ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ChnQ/MI-Peaks%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ChnQ/MI-Peaksè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02867v1">PDF</a> Preprint. Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨ä¿¡æ¯è®ºè§†è§’ç ”ç©¶å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æ¨ç†è½¨è¿¹ã€‚é€šè¿‡è¿½è¸ªæ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­ä¸­é—´è¡¨ç¤ºä¸æ­£ç¡®ç­”æ¡ˆä¹‹é—´çš„äº’ä¿¡æ¯ï¼ˆMIï¼‰å˜åŒ–ï¼Œè§‚å¯Ÿåˆ°MIå³°ç°è±¡ï¼šåœ¨LRMæ¨ç†è¿‡ç¨‹çš„ç‰¹å®šç”Ÿæˆæ­¥éª¤ä¸­ï¼Œäº’ä¿¡æ¯ä¼šå‡ºç°çªç„¶ä¸”æ˜¾è‘—çš„å¢åŠ ã€‚ç†è®ºä¸Šåˆ†ææ­¤ç°è±¡ï¼Œå‘ç°éšç€MIçš„å¢åŠ ï¼Œæ¨¡å‹é¢„æµ‹é”™è¯¯çš„æ¦‚ç‡é™ä½ã€‚åŒæ—¶ï¼Œè¿™äº›MIå³°é€šå¸¸å¯¹åº”äºè¡¨ç¤ºåæ€æˆ–è¿‡æ¸¡çš„æ ‡è®°è¯ï¼Œå¦‚â€œHmmâ€ã€â€œWaitâ€å’Œâ€œThereforeâ€ï¼Œç§°ä¸ºæ€è€ƒæ ‡è®°è¯ã€‚ç ”ç©¶è¯æ˜è¿™äº›æ€è€ƒæ ‡è®°è¯å¯¹LRMçš„æ¨ç†æ€§èƒ½è‡³å…³é‡è¦ï¼Œè€Œå…¶ä»–æ ‡è®°è¯å½±å“è¾ƒå°ã€‚åŸºäºè¿™äº›åˆ†æï¼Œæœ¬æ–‡æå‡ºä¸¤ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•æ¥æé«˜LRMçš„æ¨ç†æ€§èƒ½ï¼Œé€šè¿‡å·§å¦™åˆ©ç”¨è¿™äº›æ€è€ƒæ ‡è®°è¯ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡ä¸ºç†è§£LRMçš„æ¨ç†æœºåˆ¶æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºæé«˜å…¶æ¨ç†èƒ½åŠ›æä¾›äº†å®ç”¨æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æ¨ç†æœºåˆ¶å°šä¸å®Œå…¨æ¸…æ¥šï¼Œæœ¬æ–‡åˆ©ç”¨ä¿¡æ¯è®ºè§†è§’è¿›è¡Œç ”ç©¶ã€‚</li>
<li>é€šè¿‡è¿½è¸ªäº’ä¿¡æ¯ï¼ˆMIï¼‰å˜åŒ–è§‚å¯Ÿåˆ°MIå³°ç°è±¡ï¼Œå³åœ¨ç‰¹å®šç”Ÿæˆæ­¥éª¤ä¸­äº’ä¿¡æ¯ä¼šçªç„¶æ˜¾è‘—å¢åŠ ã€‚</li>
<li>MIå³°ä¸è¡¨ç¤ºåæ€æˆ–è¿‡æ¸¡çš„æ ‡è®°è¯ï¼ˆæ€è€ƒæ ‡è®°è¯ï¼‰ç›¸å…³ï¼Œå¦‚â€œHmmâ€ï¼Œâ€œWaitâ€å’Œâ€œThereforeâ€ã€‚</li>
<li>æ€è€ƒæ ‡è®°è¯å¯¹LRMçš„æ¨ç†æ€§èƒ½è‡³å…³é‡è¦ï¼Œè€Œå…¶ä»–æ ‡è®°è¯å½±å“è¾ƒå°ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸¤ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•æé«˜LRMçš„æ¨ç†æ€§èƒ½ï¼Œé€šè¿‡å·§å¦™åˆ©ç”¨æ€è€ƒæ ‡è®°è¯ã€‚</li>
<li>ç ”ç©¶ç»“æœä¸ºç†è§£LRMçš„æ¨ç†æœºåˆ¶æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02867">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb4b688589f673b54fd3162c15bd6e19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d744f835f086a970752cf85d1560efe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c5e669326d91da6e90ee095567ab89b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c14bda4493f0a6c8bbaff4b9ef14a233.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47002292062a73e35cdbc9a9199c9626.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba051eea59b2dfa889486c7a12858b73.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="BNPO-Beta-Normalization-Policy-Optimization"><a href="#BNPO-Beta-Normalization-Policy-Optimization" class="headerlink" title="BNPO: Beta Normalization Policy Optimization"></a>BNPO: Beta Normalization Policy Optimization</h2><p><strong>Authors:Changyi Xiao, Mengdi Zhang, Yixin Cao</strong></p>
<p>Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that reinforcement learning with rule-based, binary-valued reward functions can significantly enhance the reasoning capabilities of large language models. These models primarily utilize REINFORCE-based policy optimization techniques, such as REINFORCE with baseline and group relative policy optimization (GRPO). However, a key limitation remains: current policy optimization methods either neglect reward normalization or employ static normalization strategies, which fail to adapt to the dynamic nature of policy updates during training. This may result in unstable gradient estimates and hinder training stability. To address this issue, we propose Beta Normalization Policy Optimization (BNPO), a novel policy optimization method that adaptively normalizes rewards using a Beta distribution with dynamically updated parameters. BNPO aligns the normalization with the changing policy distribution, enabling more precise and lower-variance gradient estimation, which in turn promotes stable training dynamics. We provide theoretical analysis demonstrating BNPOâ€™s variance-reducing properties and show that it generalizes both REINFORCE and GRPO under binary-valued reward settings. Furthermore, we introduce an advantage decomposition mechanism to extend BNPOâ€™s applicability to more complex reward systems. Experimental results confirm that BNPO achieves state-of-the-art performance among policy optimization methods on reasoning tasks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/changyi7231/BNPO">https://github.com/changyi7231/BNPO</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶ï¼ŒåŒ…æ‹¬DeepSeek-R1å’ŒKimi-k1.5ï¼Œå·²ç»è¯æ˜åŸºäºè§„åˆ™ã€äºŒå€¼å¥–åŠ±å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ å¯ä»¥æ˜¾è‘—å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹ä¸»è¦åˆ©ç”¨åŸºäºREINFORCEçš„ç­–ç•¥ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚å¸¦æœ‰åŸºå‡†çš„REINFORCEå’Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸€ä¸ªå…³é”®å±€é™æ€§ï¼šå½“å‰çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•è¦ä¹ˆå¿½è§†å¥–åŠ±å½’ä¸€åŒ–ï¼Œè¦ä¹ˆé‡‡ç”¨é™æ€å½’ä¸€åŒ–ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥æ— æ³•é€‚åº”è®­ç»ƒè¿‡ç¨‹ä¸­ç­–ç•¥æ›´æ–°çš„åŠ¨æ€æ€§è´¨ã€‚è¿™å¯èƒ½å¯¼è‡´æ¢¯åº¦ä¼°è®¡ä¸ç¨³å®šï¼Œé˜»ç¢è®­ç»ƒç¨³å®šæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Betaå½’ä¸€åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆBNPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å…·æœ‰åŠ¨æ€æ›´æ–°å‚æ•°çš„Betaåˆ†å¸ƒè‡ªé€‚åº”åœ°å½’ä¸€åŒ–å¥–åŠ±ã€‚BNPOä½¿å½’ä¸€åŒ–ä¸å˜åŒ–çš„ç­–ç•¥åˆ†å¸ƒç›¸ä¸€è‡´ï¼Œèƒ½å¤Ÿå®ç°æ›´ç²¾ç¡®ã€ä½æ–¹å·®çš„æ¢¯åº¦ä¼°è®¡ï¼Œä»è€Œä¿ƒè¿›äº†ç¨³å®šçš„è®­ç»ƒåŠ¨æ€ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºåˆ†æï¼Œè¯æ˜äº†BNPOçš„é™æ–¹å·®å±æ€§ï¼Œå¹¶è¡¨æ˜å®ƒåœ¨äºŒå€¼å¥–åŠ±è®¾ç½®ä¸‹å¯ä»¥æ¦‚æ‹¬REINFORCEå’ŒGRPOã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¼˜åŠ¿åˆ†è§£æœºåˆ¶ï¼Œä»¥å°†BNPOæ‰©å±•åˆ°æ›´å¤æ‚çš„å¥–åŠ±ç³»ç»Ÿã€‚å®éªŒç»“æœè¯å®ï¼ŒBNPOåœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/changyi7231/BNPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/changyi7231/BNPOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02864v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åˆ©ç”¨åŸºäºè§„åˆ™çš„äºŒå…ƒå¥–åŠ±å‡½æ•°èƒ½å¤Ÿæ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰æ”¿ç­–ä¼˜åŒ–æ–¹æ³•æœªé€‚åº”åŠ¨æ€æ”¿ç­–æ›´æ–°ï¼Œé€ æˆè®­ç»ƒä¸ç¨³å®šã€‚å› æ­¤ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†Betaå½’ä¸€åŒ–æ”¿ç­–ä¼˜åŒ–ï¼ˆBNPOï¼‰ï¼Œåˆ©ç”¨åŠ¨æ€æ›´æ–°çš„Betaåˆ†å¸ƒå‚æ•°è‡ªé€‚åº”åœ°å½’ä¸€åŒ–å¥–åŠ±ï¼Œä»¥å‡å°‘æ–¹å·®å¹¶å®ç°ç¨³å®šè®­ç»ƒã€‚è¯¥æ–¹æ³•å…·æœ‰å¼ºå¤§çš„ä¼˜åŠ¿åˆ†è§£æœºåˆ¶ï¼Œèƒ½åº”ç”¨äºæ›´å¤æ‚çš„å¥–åŠ±ç³»ç»Ÿã€‚å®éªŒè¯æ˜ï¼ŒBNPOåœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ”¿ç­–ä¼˜åŒ–æ–¹æ³•ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ å¯ä»¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ”¿ç­–ä¼˜åŒ–æ–¹æ³•åœ¨å¥–åŠ±å½’ä¸€åŒ–ä¸Šå­˜åœ¨ç¼ºé™·ã€‚</li>
<li>BNPOé€šè¿‡Betaå½’ä¸€åŒ–ç­–ç•¥ä¼˜åŒ–æ¥è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>BNPOå¯å®ç°ä½æ–¹å·®å’Œé«˜ç²¾åº¦çš„æ¢¯åº¦ä¼°è®¡ã€‚</li>
<li>BNPOå¯é€‚ç”¨äºæ›´å¤æ‚çš„å¥–åŠ±ç³»ç»Ÿï¼Œå¹¶å…·æœ‰ä¼˜åŠ¿åˆ†è§£æœºåˆ¶ã€‚</li>
<li>å®éªŒè¯æ˜BNPOåœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ”¿ç­–ä¼˜åŒ–æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02e3f6759f0094dea15a73405245cea9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07cc29e1881d45e8ed8ad089e7a51814.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-963db6b09fbdb9eea544ec52807e48ae.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Rethinking-Machine-Unlearning-in-Image-Generation-Models"><a href="#Rethinking-Machine-Unlearning-in-Image-Generation-Models" class="headerlink" title="Rethinking Machine Unlearning in Image Generation Models"></a>Rethinking Machine Unlearning in Image Generation Models</h2><p><strong>Authors:Renyang Liu, Wenjie Feng, Tianwei Zhang, Wei Zhou, Xueqi Cheng, See-Kiong Ng</strong></p>
<p>With the surge and widespread application of image generation models, data privacy and content safety have become major concerns and attracted great attention from users, service providers, and policymakers. Machine unlearning (MU) is recognized as a cost-effective and promising means to address these challenges. Despite some advancements, image generation model unlearning (IGMU) still faces remarkable gaps in practice, e.g., unclear task discrimination and unlearning guidelines, lack of an effective evaluation framework, and unreliable evaluation metrics. These can hinder the understanding of unlearning mechanisms and the design of practical unlearning algorithms. We perform exhaustive assessments over existing state-of-the-art unlearning algorithms and evaluation standards, and discover several critical flaws and challenges in IGMU tasks. Driven by these limitations, we make several core contributions, to facilitate the comprehensive understanding, standardized categorization, and reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel hierarchical task categorization framework. It provides detailed implementation guidance for IGMU, assisting in the design of unlearning algorithms and the construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation framework. It includes reliable quantitative metrics across five critical aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can be used for extensive evaluations of IGMU, training content detectors for judgment, and benchmarking the state-of-the-art unlearning algorithms. With EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot handle the unlearning well across different evaluation dimensions, especially for preservation and robustness. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/ryliu68/IGMU">https://github.com/ryliu68/IGMU</a>. </p>
<blockquote>
<p>éšç€å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¶Œç°å’Œå¹¿æ³›åº”ç”¨ï¼Œæ•°æ®éšç§å’Œå†…å®¹å®‰å…¨æˆä¸ºä¸»è¦å…³æ³¨ç‚¹ï¼Œå¹¶å¼•èµ·äº†ç”¨æˆ·ã€æœåŠ¡æä¾›å•†å’Œæ”¿ç­–åˆ¶å®šè€…çš„æå¤§å…³æ³¨ã€‚æœºå™¨é—å¿˜ï¼ˆMUï¼‰è¢«è®¤ä¸ºæ˜¯ä¸€ç§ç»æµé«˜æ•ˆä¸”å‰æ™¯å¹¿é˜”çš„è§£å†³è¿™äº›æŒ‘æˆ˜çš„æ‰‹æ®µã€‚å°½ç®¡æœ‰ä¸€äº›è¿›å±•ï¼Œä½†å›¾åƒç”Ÿæˆæ¨¡å‹çš„é—å¿˜ï¼ˆIGMUï¼‰åœ¨å®è·µä¸­ä»ç„¶é¢ä¸´æ˜¾è‘—çš„å·®è·ï¼Œä¾‹å¦‚ä»»åŠ¡è¾¨åˆ«å’Œé—å¿˜æŒ‡å—ä¸æ˜ç¡®ï¼Œç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°æ¡†æ¶å’Œä¸å¯é çš„è¯„ä¼°æŒ‡æ ‡ã€‚è¿™äº›å¯èƒ½é˜»ç¢å¯¹é—å¿˜æœºåˆ¶çš„ç†è§£å’Œå®ç”¨é—å¿˜ç®—æ³•çš„è®¾è®¡ã€‚æˆ‘ä»¬å¯¹ç°æœ‰çš„æœ€æ–°é—å¿˜ç®—æ³•å’Œè¯„ä¼°æ ‡å‡†è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œå¹¶å‘ç°äº†å›¾åƒç”Ÿæˆæ¨¡å‹é—å¿˜ï¼ˆIGMUï¼‰ä»»åŠ¡ä¸­çš„å‡ ä¸ªå…³é”®ç¼ºé™·å’ŒæŒ‘æˆ˜ã€‚å—è¿™äº›å±€é™æ€§çš„é©±åŠ¨ï¼Œæˆ‘ä»¬åšå‡ºäº†å‡ é¡¹æ ¸å¿ƒè´¡çŒ®ï¼Œä»¥ä¿ƒè¿›å¯¹IGMUçš„å…¨é¢ç†è§£ã€æ ‡å‡†åŒ–åˆ†ç±»å’Œå¯é è¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼Œï¼ˆ1ï¼‰æˆ‘ä»¬è®¾è®¡äº†CatIGMUï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹åˆ†å±‚ä»»åŠ¡åˆ†ç±»æ¡†æ¶ã€‚å®ƒä¸ºIGMUæä¾›äº†è¯¦ç»†çš„å®æ–½æŒ‡å—ï¼Œæœ‰åŠ©äºè®¾è®¡é—å¿˜ç®—æ³•å’Œæ„å»ºæµ‹è¯•å¹³å°ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬ä»‹ç»äº†EvalIGMUï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚å®ƒåŒ…æ‹¬äº”ä¸ªå…³é”®æ–¹é¢çš„å¯é å®šé‡æŒ‡æ ‡ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬æ„å»ºäº†DataIGMï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„é—å¿˜æ•°æ®é›†ï¼Œå¯ç”¨äºå¯¹IGMUè¿›è¡Œå…¨é¢è¯„ä¼°ã€è®­ç»ƒå†…å®¹æ£€æµ‹å™¨è¿›è¡Œåˆ¤è¯»å’ŒåŸºå‡†æµ‹è¯•æœ€æ–°é—å¿˜ç®—æ³•ã€‚å€ŸåŠ©EvalIGMUå’ŒDataIGMï¼Œæˆ‘ä»¬å‘ç°å¤§å¤šæ•°ç°æœ‰IGMUç®—æ³•åœ¨ä¸åŒçš„è¯„ä¼°ç»´åº¦ä¸Šæ— æ³•å¾ˆå¥½åœ°å¤„ç†é—å¿˜é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä¿æŒæ€§å’Œç¨³å¥æ€§æ–¹é¢ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ryliu68/IGMU">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02761v1">PDF</a> Accepted by ACM CCS 2025</p>
<p><strong>Summary</strong><br>åœ¨å›¾åƒç”Ÿæˆæ¨¡å‹å¹¿æ³›åº”ç”¨çš„èƒŒæ™¯ä¸‹ï¼Œæ•°æ®éšç§å’Œå†…å®¹å®‰å…¨å—åˆ°å…³æ³¨ã€‚æœºå™¨é—å¿˜ï¼ˆMUï¼‰è¢«è§†ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜çš„æœ‰å‰é€”ä¸”ç»æµçš„æ‰‹æ®µã€‚å°½ç®¡æœ‰æ‰€è¿›å±•ï¼Œä½†å›¾åƒç”Ÿæˆæ¨¡å‹çš„é—å¿˜ï¼ˆIGMUï¼‰åœ¨å®è·µä¸­ä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¦‚ä»»åŠ¡è¾¨åˆ«ä¸æ¸…ã€é—å¿˜å‡†åˆ™ä¸æ˜ç¡®ã€ç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°æ¡†æ¶å’Œä¸å¯é çš„è¯„ä¼°æŒ‡æ ‡ç­‰ã€‚æˆ‘ä»¬å¯¹ç°æœ‰çš„æœ€æ–°é—å¿˜ç®—æ³•å’Œè¯„ä¼°æ ‡å‡†è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°IGMUä»»åŠ¡ä¸­å­˜åœ¨å‡ ä¸ªå…³é”®ç¼ºé™·å’ŒæŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åšå‡ºäº†å‡ é¡¹æ ¸å¿ƒè´¡çŒ®ï¼Œä»¥ä¿ƒè¿›å¯¹IGMUçš„å…¨é¢ç†è§£ã€æ ‡å‡†åŒ–åˆ†ç±»å’Œå¯é è¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬è®¾è®¡äº†CatIGMUï¼Œä¸€ç§æ–°å‹å±‚æ¬¡ä»»åŠ¡åˆ†ç±»æ¡†æ¶ï¼Œä¸ºIGMUçš„è¯¦ç»†å®æ–½æä¾›æŒ‡å¯¼ï¼ŒååŠ©é—å¿˜ç®—æ³•çš„è®¾è®¡å’Œæµ‹è¯•å¹³å°çš„æ„å»ºã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬æ¨å‡ºäº†EvalIGMUï¼Œä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬äº”ä¸ªå…³é”®æ–¹é¢çš„å¯é å®šé‡æŒ‡æ ‡ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬æ„å»ºäº†DataIGMï¼Œä¸€ä¸ªé«˜è´¨é‡çš„é—å¿˜æ•°æ®é›†ï¼Œå¯ç”¨äºå¯¹IGMUè¿›è¡Œå¹¿æ³›è¯„ä¼°ã€è®­ç»ƒå†…å®¹æ£€æµ‹å™¨è¿›è¡Œåˆ¤å†³ä»¥åŠè¯„ä¼°æœ€æ–°çš„é—å¿˜ç®—æ³•ã€‚é€šè¿‡EvalIGMUå’ŒDataIGMçš„å‘ç°è¡¨æ˜å¤§å¤šæ•°ç°æœ‰IGMUç®—æ³•åœ¨è¯„ä¼°ç»´åº¦ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶åœ¨ä¿ç•™æ€§å’Œé²æ£’æ€§æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€å›¾åƒç”Ÿæˆæ¨¡å‹çš„å¹¿æ³›åº”ç”¨ï¼Œæ•°æ®éšç§å’Œå†…å®¹å®‰å…¨æˆä¸ºä¸»è¦å…³æ³¨ç‚¹ã€‚</li>
<li>æœºå™¨é—å¿˜è¢«è§†ä¸ºè§£å†³å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„æ•°æ®éšç§å’Œå†…å®¹å®‰å…¨æŒ‘æˆ˜çš„æœ‰æ•ˆæ‰‹æ®µã€‚</li>
<li>å›¾åƒç”Ÿæˆæ¨¡å‹çš„é—å¿˜ï¼ˆIGMUï¼‰åœ¨å®è·µä¸­å­˜åœ¨æ˜¾è‘—å·®è·å’ŒæŒ‘æˆ˜ï¼Œå¦‚ä»»åŠ¡è¾¨åˆ«ä¸æ¸…ã€é—å¿˜å‡†åˆ™ä¸æ˜ç¡®ç­‰ã€‚</li>
<li>å›¢é˜Ÿé’ˆå¯¹IGMUä»»åŠ¡æå‡ºäº†æ ¸å¿ƒè´¡çŒ®ï¼šè®¾è®¡CatIGMUåˆ†ç±»æ¡†æ¶ï¼Œå¼•å…¥EvalIGMUè¯„ä¼°æ¡†æ¶ä»¥åŠæ„å»ºDataIGMæ•°æ®é›†ã€‚</li>
<li>é€šè¿‡EvalIGMUå’ŒDataIGMå‘ç°ç°æœ‰IGMUç®—æ³•åœ¨å¤šä¸ªè¯„ä¼°ç»´åº¦ä¸Šçš„ä¸è¶³ã€‚</li>
<li>CatIGMUä¸ºIGMUçš„è¯¦ç»†å®æ–½æä¾›æŒ‡å¯¼ï¼Œå¸®åŠ©è®¾è®¡é—å¿˜ç®—æ³•å’Œæ„å»ºæµ‹è¯•å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1957bdea65f76155c06663d6de019bdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66c40af5bbac93e63719768733805865.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3811a96219ca0c187b93d8487937b3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-073e8f2bc92d0cad485637fa18372d92.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="BenLOC-A-Benchmark-for-Learning-to-Configure-MIP-Optimizers"><a href="#BenLOC-A-Benchmark-for-Learning-to-Configure-MIP-Optimizers" class="headerlink" title="BenLOC: A Benchmark for Learning to Configure MIP Optimizers"></a>BenLOC: A Benchmark for Learning to Configure MIP Optimizers</h2><p><strong>Authors:Hongpei Li, Ziyan He, Yufei Wang, Wenting Tu, Shanwen Pu, Qi Deng, Dongdong Ge</strong></p>
<p>The automatic configuration of Mixed-Integer Programming (MIP) optimizers has become increasingly critical as the large number of configurations can significantly affect solver performance. Yet the lack of standardized evaluation frameworks has led to data leakage and over-optimistic claims, as prior studies often rely on homogeneous datasets and inconsistent experimental setups. To promote a fair evaluation process, we present BenLOC, a comprehensive benchmark and open-source toolkit, which not only offers an end-to-end pipeline for learning instance-wise MIP optimizer configurations, but also standardizes dataset selection, train-test splits, feature engineering and baseline choice for unbiased and comprehensive evaluations. Leveraging this framework, we conduct an empirical analysis on five well-established MIP datasets and compare classical machine learning models with handcrafted features against state-of-the-art deep-learning techniques. The results demonstrate the importance of datasets, features and baseline criteria proposed by BenLOC and the effectiveness of BenLOC in providing unbiased and comprehensive evaluations. </p>
<blockquote>
<p>æ··åˆæ•´æ•°ç¼–ç¨‹ï¼ˆMIPï¼‰ä¼˜åŒ–å™¨çš„è‡ªåŠ¨é…ç½®å˜å¾—æ—¥ç›Šå…³é”®ï¼Œå› ä¸ºå¤§é‡çš„é…ç½®ä¼šæ˜¾è‘—å½±å“æ±‚è§£å™¨çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œå¯¼è‡´æ•°æ®æ³„éœ²å’Œè¿‡äºä¹è§‚çš„å£°æ˜ï¼Œå› ä¸ºå…ˆå‰çš„ç ”ç©¶å¾€å¾€ä¾èµ–äºåŒè´¨çš„æ•°æ®é›†å’Œå®éªŒè®¾ç½®ä¸ä¸€è‡´ã€‚ä¸ºäº†ä¿ƒè¿›å…¬å¹³çš„è¯„ä¼°è¿‡ç¨‹ï¼Œæˆ‘ä»¬æå‡ºäº†BenLOCï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å’Œå¼€æºå·¥å…·åŒ…ï¼Œå®ƒä¸ä»…æä¾›äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç®¡é“æ¥å­¦ä¹ å®ä¾‹åŒ–çš„MIPä¼˜åŒ–å™¨é…ç½®ï¼Œè¿˜æ ‡å‡†åŒ–äº†æ•°æ®é›†é€‰æ‹©ã€è®­ç»ƒæµ‹è¯•åˆ†å‰²ã€ç‰¹å¾å·¥ç¨‹å’ŒåŸºçº¿é€‰æ‹©ï¼Œä»¥è¿›è¡Œå®¢è§‚å…¨é¢çš„è¯„ä¼°ã€‚åˆ©ç”¨è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬å¯¹äº”ä¸ªæˆç†Ÿçš„MIPæ•°æ®é›†è¿›è¡Œäº†å®è¯åˆ†æï¼Œå¹¶å°†æ‰‹å·¥åˆ¶ä½œçš„ç‰¹å¾çš„ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ä¸æœ€æ–°çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒBenLOCæ‰€æå‡ºçš„æ•°æ®é›†ã€ç‰¹å¾å’ŒåŸºçº¿æ ‡å‡†æ˜¯é‡è¦çš„ï¼ŒBenLOCåœ¨æä¾›å®¢è§‚å…¨é¢çš„è¯„ä¼°æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02752v1">PDF</a> A Benchmark for learning to configurate MIP Optimizers (Solvers)</p>
<p><strong>Summary</strong>ï¼šéšç€æ··åˆæ•´æ•°ç¼–ç¨‹ï¼ˆMIPï¼‰ä¼˜åŒ–å™¨é…ç½®æ•°é‡çš„å¢åŠ ï¼Œè‡ªåŠ¨é…ç½®çš„é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œå¯¼è‡´æ•°æ®æ³„éœ²å’Œè¿‡äºä¹è§‚çš„å£°æ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºBenLOCï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å’Œå¼€æºå·¥å…·åŒ…ï¼Œä¸ä»…æä¾›ç«¯åˆ°ç«¯çš„MIPä¼˜åŒ–å™¨é…ç½®å­¦ä¹ æµç¨‹ï¼Œè¿˜æ ‡å‡†åŒ–æ•°æ®é›†é€‰æ‹©ã€è®­ç»ƒæµ‹è¯•åˆ†å‰²ã€ç‰¹å¾å·¥ç¨‹å’ŒåŸºçº¿é€‰æ‹©ï¼Œä»¥ä¾¿è¿›è¡Œå®¢è§‚å…¨é¢çš„è¯„ä¼°ã€‚å€ŸåŠ©è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬åœ¨äº”ä¸ªçŸ¥åçš„MIPæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®è¯åˆ†æï¼Œæ¯”è¾ƒäº†å¸¦æœ‰æ‰‹å·¥ç‰¹å¾çš„ç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹ä¸æœ€æ–°æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œè¯æ˜äº†BenLOCæ‰€æå‡ºçš„æ•°æ®é›†ã€ç‰¹å¾å’ŒåŸºçº¿æ ‡å‡†çš„é‡è¦æ€§åŠæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è‡ªåŠ¨é…ç½®MIPä¼˜åŒ–å™¨éå¸¸é‡è¦ï¼Œå› ä¸ºå¤§é‡çš„é…ç½®ä¼šæ˜¾è‘—å½±å“æ±‚è§£å™¨çš„æ€§èƒ½ã€‚</li>
<li>ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶å¯¼è‡´æ•°æ®æ³„éœ²å’Œè¿‡äºä¹è§‚çš„ç ”ç©¶ç»“æœã€‚</li>
<li>BenLOCæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•å’Œå¼€æºå·¥å…·åŒ…ï¼Œæä¾›ç«¯åˆ°ç«¯çš„MIPä¼˜åŒ–å™¨é…ç½®å­¦ä¹ æµç¨‹ã€‚</li>
<li>BenLOCæ ‡å‡†åŒ–æ•°æ®é›†é€‰æ‹©ã€è®­ç»ƒæµ‹è¯•åˆ†å‰²ã€ç‰¹å¾å·¥ç¨‹å’ŒåŸºçº¿é€‰æ‹©ã€‚</li>
<li>é€šè¿‡å®è¯åˆ†ææ¯”è¾ƒäº†ç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹å’Œæœ€æ–°æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨BenLOCæ¡†æ¶ä¸‹çš„è¡¨ç°ã€‚</li>
<li>BenLOCæ‰€æå‡ºçš„æ•°æ®é›†ã€ç‰¹å¾å’ŒåŸºçº¿æ ‡å‡†å¯¹äºè¯„ä¼°MIPä¼˜åŒ–å™¨é…ç½®è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-041f4d8509d6c16b5a00268dc7ee6ca4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d60c482a93fe7a5dc40b28a7db1859d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dbb6d4a0d6e46fb7ebf841f37448011.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aa703c31a1b1c151212c991fdcfcafd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84978eda77a0d63fc9480319185edd22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76d75027077ff8893abb80d0ee5f8c3b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RACE-Align-Retrieval-Augmented-and-Chain-of-Thought-Enhanced-Preference-Alignment-for-Large-Language-Models"><a href="#RACE-Align-Retrieval-Augmented-and-Chain-of-Thought-Enhanced-Preference-Alignment-for-Large-Language-Models" class="headerlink" title="RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference   Alignment for Large Language Models"></a>RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference   Alignment for Large Language Models</h2><p><strong>Authors:Qihang Yan, Xinyu Zhang, Luming Guo, Qi Zhang, Feifan Liu</strong></p>
<p>Large Language Models (LLMs) struggle with accuracy, domain-specific reasoning, and interpretability in vertical domains. Traditional preference alignment methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) often overlook the underlying knowledge sources and reasoning logic. This paper introduces RACE-Align (Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel framework designed to address these limitations. RACE-Align systematically constructs a binary preference dataset incorporating external knowledge support and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO algorithm. The core innovation lies in its preference data construction strategy: it integrates AI-driven retrieval for factual grounding, enhancing knowledgeability and accuracy, and emphasizes the optimization of domain-specific CoT, treating the reasoning process itself as a key preference dimension. A multi-stage, AI-driven refinement pipeline cost-effectively generates these preference pairs. Experimental validation in Traditional Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that RACE-Align significantly outperforms the original base model and a model fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed across multiple dimensions, including answer accuracy, information richness, application of TCM thinking patterns, logicality and depth of reasoning, and interpretability. These findings suggest RACE-Align offers an effective pathway to enhance LLMsâ€™ knowledge application, reasoning reliability, and process transparency in complex vertical domains. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å‚ç›´é¢†åŸŸé¢ä¸´å‡†ç¡®æ€§ã€é¢†åŸŸç‰¹å®šæ¨ç†å’Œå¯è§£é‡Šæ€§çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„åå¥½å¯¹é½æ–¹æ³•ï¼Œå¦‚åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œå¾€å¾€å¿½è§†äº†æ½œåœ¨çš„çŸ¥è¯†æ¥æºå’Œæ¨ç†é€»è¾‘ã€‚æœ¬æ–‡ä»‹ç»äº†RACE-Alignï¼ˆæ£€ç´¢å¢å¼ºä¸æ€ç»´é“¾å¢å¼ºå¯¹é½ï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™äº›å±€é™æ€§ã€‚RACE-Alignç³»ç»Ÿåœ°æ„å»ºäº†ä¸€ä¸ªäºŒè¿›åˆ¶åå¥½æ•°æ®é›†ï¼Œèå…¥äº†å¤–éƒ¨çŸ¥è¯†æ”¯æŒå’Œæ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œç„¶åä½¿ç”¨DPOç®—æ³•å¯¹é½LLMã€‚å…¶æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºå…¶åå¥½æ•°æ®æ„å»ºç­–ç•¥ï¼šå®ƒç»“åˆäº†AIé©±åŠ¨çš„æ£€ç´¢æ¥å®ç°äº‹å®åŸºç¡€ï¼Œæé«˜äº†çŸ¥è¯†æ€§å’Œå‡†ç¡®æ€§ï¼Œå¹¶å¼ºè°ƒé¢†åŸŸç‰¹å®šæ€ç»´é“¾çš„ä¼˜åŒ–ï¼Œå°†æ¨ç†è¿‡ç¨‹æœ¬èº«è§†ä¸ºä¸€ä¸ªå…³é”®çš„åå¥½ç»´åº¦ã€‚å¤šé˜¶æ®µã€AIé©±åŠ¨çš„ä¼˜åŒ–ç®¡é“ä»¥æˆæœ¬æ•ˆç›Šçš„æ–¹å¼ç”Ÿæˆè¿™äº›åå¥½å¯¹ã€‚ä»¥Qwen3-1.7Bä¸ºåŸºç¡€æ¨¡å‹ï¼Œåœ¨ä¼ ç»Ÿä¸­åŒ»é¢†åŸŸè¿›è¡Œå®éªŒéªŒè¯ï¼Œç»“æœè¡¨æ˜RACE-Alignæ˜¾è‘—ä¼˜äºåŸå§‹åŸºç¡€æ¨¡å‹å’Œä»…ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ¨¡å‹ã€‚åœ¨å¤šä¸ªç»´åº¦ä¸Šå‡è§‚å¯Ÿåˆ°æ”¹è¿›ï¼ŒåŒ…æ‹¬ç­”æ¡ˆå‡†ç¡®æ€§ã€ä¿¡æ¯ä¸°å¯Œæ€§ã€ä¸­åŒ»æ€ç»´æ¨¡å¼çš„è¿ç”¨ã€é€»è¾‘æ€§å’Œæ·±åº¦æ¨ç†ä»¥åŠå¯è§£é‡Šæ€§ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒRACE-Alignä¸ºå¢å¼ºLLMåœ¨å¤æ‚å‚ç›´é¢†åŸŸçš„çŸ¥è¯†åº”ç”¨ã€æ¨ç†å¯é æ€§å’Œè¿‡ç¨‹é€æ˜åº¦æä¾›äº†ä¸€æ¡æœ‰æ•ˆè·¯å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02726v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å‚ç›´é¢†åŸŸå­˜åœ¨å‡†ç¡®æ€§ã€é¢†åŸŸç‰¹å®šæ¨ç†å’Œå¯è§£é‡Šæ€§çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿåå¥½å¯¹é½æ–¹æ³•å¦‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¸¸å¸¸å¿½ç•¥åº•å±‚çŸ¥è¯†æ¥æºå’Œæ¨ç†é€»è¾‘ã€‚æœ¬æ–‡æå‡ºRACE-Alignæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤–éƒ¨çŸ¥è¯†æ”¯æŒå’Œæ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚RACE-Alignåˆ©ç”¨DPOç®—æ³•å¯¹é½LLMsï¼Œå…¶æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºå…¶åå¥½æ•°æ®æ„å»ºç­–ç•¥ï¼Œé€šè¿‡AIé©±åŠ¨æ£€ç´¢ä¸ºäº‹å®åŸºç¡€å¢å¼ºçŸ¥è¯†æ€§å’Œå‡†ç¡®æ€§ï¼Œå¹¶å¼ºè°ƒé¢†åŸŸç‰¹å®šæ€ç»´é“¾çš„ä¼˜åŒ–ã€‚åœ¨ä¸­åŒ»é¢†åŸŸçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒRACE-Alignæ˜¾è‘—ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œå¹¶ä¼˜äºä»…é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æ¨¡å‹ã€‚æ”¹è¿›è¡¨ç°åœ¨å¤šä¸ªç»´åº¦ï¼ŒåŒ…æ‹¬ç­”æ¡ˆå‡†ç¡®æ€§ã€ä¿¡æ¯ä¸°å¯Œæ€§ã€ä¸­åŒ»æ€ç»´æ¨¡å¼çš„è¿ç”¨ã€é€»è¾‘æ¨ç†æ€§å’Œæ·±åº¦ä»¥åŠå¯è§£é‡Šæ€§ã€‚è¿™è¡¨æ˜RACE-Alignèƒ½æœ‰æ•ˆæå‡LLMsåœ¨å¤æ‚å‚ç›´é¢†åŸŸçš„çŸ¥è¯†åº”ç”¨ã€æ¨ç†å¯é æ€§å’Œè¿‡ç¨‹é€æ˜åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å‚ç›´é¢†åŸŸå­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ã€é¢†åŸŸç‰¹å®šæ¨ç†å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>ä¼ ç»Ÿåå¥½å¯¹é½æ–¹æ³•å¿½ç•¥åº•å±‚çŸ¥è¯†æ¥æºå’Œæ¨ç†é€»è¾‘ã€‚</li>
<li>RACE-Alignæ¡†æ¶é€šè¿‡ç»“åˆå¤–éƒ¨çŸ¥è¯†æ”¯æŒå’Œæ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>RACE-Alignåˆ©ç”¨AIé©±åŠ¨æ£€ç´¢å¢å¼ºçŸ¥è¯†æ€§å’Œå‡†ç¡®æ€§ï¼Œå¹¶å¼ºè°ƒé¢†åŸŸç‰¹å®šæ€ç»´é“¾çš„ä¼˜åŒ–ã€‚</li>
<li>RACE-Alignæ˜¾è‘—ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œæ”¹è¿›è¡¨ç°åœ¨å¤šä¸ªç»´åº¦ï¼ŒåŒ…æ‹¬ç­”æ¡ˆè´¨é‡ã€ä¿¡æ¯ä¸°å¯Œæ€§ã€å¯¹ä¸­åŒ»æ€ç»´æ¨¡å¼çš„è¿ç”¨ã€é€»è¾‘æ¨ç†æ€§å’Œæ·±åº¦ä»¥åŠå¯è§£é‡Šæ€§ã€‚</li>
<li>RACE-Alignæå‡LLMsåœ¨å¤æ‚å‚ç›´é¢†åŸŸçš„çŸ¥è¯†åº”ç”¨ã€æ¨ç†å¯é æ€§å’Œè¿‡ç¨‹é€æ˜åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-896433a2fdd854f208a9e51df54d80b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46a9637e9f01d06c252973a8ce3d3781.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Heterogeneous-Group-Based-Reinforcement-Learning-for-LLM-based-Multi-Agent-Systems"><a href="#Heterogeneous-Group-Based-Reinforcement-Learning-for-LLM-based-Multi-Agent-Systems" class="headerlink" title="Heterogeneous Group-Based Reinforcement Learning for LLM-based   Multi-Agent Systems"></a>Heterogeneous Group-Based Reinforcement Learning for LLM-based   Multi-Agent Systems</h2><p><strong>Authors:Guanzhong Chen, Shaoxiong Yang, Chao Li, Wei Liu, Jian Luan, Zenglin Xu</strong></p>
<p>Large Language Models (LLMs) have achieved remarkable success across diverse natural language processing tasks, yet their deployment in real-world applications is hindered by fixed knowledge cutoffs and difficulties in generating controllable, accurate outputs in a single inference. Multi-agent systems (MAS) built from specialized LLM agents offer a promising solution, enabling dynamic collaboration and iterative reasoning. However, optimizing these systems remains a challenge, as conventional methods such as prompt engineering and supervised fine-tuning entail high engineering overhead and limited adaptability. Reinforcement learning (RL), particularly multi-agent reinforcement learning (MARL), provides a scalable framework by refining agent policies based on system-level feedback. Nevertheless, existing MARL algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on Critic networks, which can cause training instability and increase computational burden. To address these limitations and target the prototypical Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy updates by estimating relative reward advantages across heterogeneous groups of rollouts. MHGPO eliminates the need for Critic networks, enhancing stability and reducing computational overhead. Additionally, we introduce three group rollout sampling strategies that trade off between efficiency and effectiveness. Experiments on a multi-agent LLM-based search system demonstrate that MHGPO consistently outperforms MAPPO in both task performance and computational efficiency, without requiring warm-up, underscoring its potential for stable and scalable optimization of complex LLM-based MAS. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„éƒ¨ç½²å—åˆ°å›ºå®šçŸ¥è¯†æˆªæ–­å’Œéš¾ä»¥åœ¨å•æ¬¡æ¨æ–­ä¸­äº§ç”Ÿå¯æ§ã€å‡†ç¡®è¾“å‡ºçš„é™åˆ¶ã€‚ç”±ä¸“ä¸šLLMä»£ç†æ„å»ºçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå®ç°åŠ¨æ€åä½œå’Œè¿­ä»£æ¨ç†ã€‚ç„¶è€Œï¼Œä¼˜åŒ–è¿™äº›ç³»ç»Ÿä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºä¼ ç»Ÿçš„æ–¹æ³•ï¼Œå¦‚æç¤ºå·¥ç¨‹å’Œç›‘ç£å¾®è°ƒï¼Œæ¶‰åŠé«˜å·¥ç¨‹å¼€é”€å’Œæœ‰é™çš„é€‚åº”æ€§ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œç‰¹åˆ«æ˜¯å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ï¼Œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®ç³»ç»Ÿçº§åˆ«çš„åé¦ˆæ¥ä¼˜åŒ–æ™ºèƒ½ä½“ç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MARLç®—æ³•ï¼Œå¦‚å¤šæ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆMAPPOï¼‰ï¼Œä¾èµ–äºè¯„è®ºå®¶ç½‘ç»œï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šå¹¶å¢åŠ è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§å¹¶é¢å‘å…¸å‹çš„å¤šæ™ºèƒ½ä½“æœç´¢ç³»ç»Ÿï¼ˆMASSï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ™ºèƒ½ä½“å¼‚è´¨ç¾¤ç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆMHGPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„æ— è¯„è®ºå®¶ç®—æ³•ï¼Œå®ƒé€šè¿‡ä¼°è®¡ä¸åŒæ™ºèƒ½ä½“ä¹‹é—´çš„ç›¸å¯¹å¥–åŠ±ä¼˜åŠ¿æ¥æŒ‡å¯¼ç­–ç•¥æ›´æ–°ã€‚MHGPOæ¶ˆé™¤äº†å¯¹è¯„è®ºå®¶ç½‘ç»œçš„éœ€æ±‚ï¼Œæé«˜äº†ç¨³å®šæ€§å¹¶é™ä½äº†è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸‰ç§ç¾¤ç»„æ»šåŠ¨é‡‡æ ·ç­–ç•¥ï¼Œåœ¨æ•ˆç‡å’Œæœ‰æ•ˆæ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚åœ¨å¤šæ™ºèƒ½ä½“LLMåŸºäºæœç´¢ç³»ç»Ÿä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMHGPOåœ¨ä»»åŠ¡æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡æŒç»­ä¼˜äºMAPPOï¼Œä¸”æ— éœ€é¢„çƒ­è¿‡ç¨‹ï¼Œçªæ˜¾å…¶åœ¨ç¨³å®šå’Œå¯æ‰©å±•ä¼˜åŒ–å¤æ‚LLMåŸºäºMASæ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02718v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„éƒ¨ç½²å—åˆ°äº†å›ºå®šçŸ¥è¯†æˆªæ–­å’Œç”Ÿæˆå¯æ§ã€å‡†ç¡®è¾“å‡ºçš„å›°éš¾é™åˆ¶ã€‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ç”±ä¸“ä¸šLLMæ™ºèƒ½ä½“æ„å»ºï¼Œæä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå¯å®ç°åŠ¨æ€åä½œå’Œè¿­ä»£æ¨ç†ã€‚ç„¶è€Œï¼Œä¼˜åŒ–è¿™äº›ç³»ç»Ÿä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•å¦‚æç¤ºå·¥ç¨‹å’Œç›‘ç£å¾®è°ƒæ¶‰åŠé«˜å·¥ç¨‹å¼€é”€å’Œæœ‰é™çš„é€‚åº”æ€§ã€‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œç‰¹åˆ«æ˜¯å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ï¼Œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®ç³»ç»Ÿçº§åˆ«çš„åé¦ˆä¼˜åŒ–æ™ºèƒ½ä½“ç­–ç•¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MARLç®—æ³•ï¼Œå¦‚å¤šæ™ºèƒ½ä½“è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆMAPPOï¼‰ï¼Œä¾èµ–äºè¯„è®ºå®¶ç½‘ç»œï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šå¹¶å¢åŠ è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜å¹¶é¢å‘å…¸å‹çš„å¤šæ™ºèƒ½ä½“æœç´¢ç³»ç»Ÿï¼ˆMASSï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ™ºèƒ½ä½“å¼‚è´¨ç»„ç­–ç•¥ä¼˜åŒ–ï¼ˆMHGPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— éœ€è¯„è®ºå®¶çš„ç®—æ³•ï¼Œå®ƒé€šè¿‡ä¼°è®¡ä¸åŒç»„rolloutsä¹‹é—´çš„ç›¸å¯¹å¥–åŠ±ä¼˜åŠ¿æ¥æŒ‡å¯¼ç­–ç•¥æ›´æ–°ã€‚MHGPOæ¶ˆé™¤äº†å¯¹è¯„è®ºå®¶ç½‘ç»œçš„éœ€è¦ï¼Œæé«˜äº†ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸‰ç§ç»„rolloutsé‡‡æ ·ç­–ç•¥ï¼Œåœ¨æ•ˆç‡å’Œæ•ˆæœä¹‹é—´å¯»æ±‚å¹³è¡¡ã€‚åœ¨åŸºäºå¤šæ™ºèƒ½ä½“LLMçš„æœç´¢ç³»ç»Ÿä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMHGPOåœ¨ä»»åŠ¡æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡æ–¹é¢å‡ä¼˜äºMAPPOï¼Œä¸éœ€è¦é¢„çƒ­ï¼Œçªæ˜¾å…¶åœ¨ç¨³å®šå’Œå¯æ‰©å±•ä¼˜åŒ–å¤æ‚LLMåŸºMASæ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨å›ºå®šçŸ¥è¯†æˆªæ–­å’Œè¾“å‡ºæ§åˆ¶éš¾é¢˜ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰é€šè¿‡åŠ¨æ€åä½œå’Œè¿­ä»£æ¨ç†ä¸ºè§£å†³è¿™äº›é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç‰¹åˆ«æ˜¯å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸ºä¼˜åŒ–æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•æ¡†æ¶ã€‚</li>
<li>ç°æœ‰MARLç®—æ³•å¦‚MAPPOå­˜åœ¨è®­ç»ƒä¸ç¨³å®šå’Œè®¡ç®—è´Ÿæ‹…é‡çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„Multi-Agent Heterogeneous Group Policy Optimization (MHGPO) ç®—æ³•æ˜¯ä¸€ç§æ–°å‹çš„è¯„è®ºå®¶ç‹¬ç«‹ç®—æ³•ï¼Œé€šè¿‡ä¼°è®¡ä¸åŒç»„rolloutsçš„ç›¸å¯¹å¥–åŠ±ä¼˜åŠ¿æ¥ä¼˜åŒ–ç­–ç•¥æ›´æ–°ã€‚</li>
<li>MHGPOæé«˜äº†ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œæ¶ˆé™¤äº†å¯¹è¯„è®ºå®¶ç½‘ç»œçš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91f6888b1cac1ddbe89d040e986cd284.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f063c7c754e16215b26894a4fa0da204.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f30fd0ada514a566928d83c7c624de65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6c0c3903aa98a90ee95f037779e8f40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5466acd47b8a1a43c94bd828348866d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6691c4850ff0d2e0744cedbc1b54e7bf.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Open-Set-Living-Need-Prediction-with-Large-Language-Models"><a href="#Open-Set-Living-Need-Prediction-with-Large-Language-Models" class="headerlink" title="Open-Set Living Need Prediction with Large Language Models"></a>Open-Set Living Need Prediction with Large Language Models</h2><p><strong>Authors:Xiaochong Lan, Jie Feng, Yizhou Sun, Chen Gao, Jiahuan Lei, Xinlei Shi, Hengliang Luo, Yong Li</strong></p>
<p>Living needs are the needs people generate in their daily lives for survival and well-being. On life service platforms like Meituan, user purchases are driven by living needs, making accurate living need predictions crucial for personalized service recommendations. Traditional approaches treat this prediction as a closed-set classification problem, severely limiting their ability to capture the diversity and complexity of living needs. In this work, we redefine living need prediction as an open-set classification problem and propose PIGEON, a novel system leveraging large language models (LLMs) for unrestricted need prediction. PIGEON first employs a behavior-aware record retriever to help LLMs understand user preferences, then incorporates Maslowâ€™s hierarchy of needs to align predictions with human living needs. For evaluation and application, we design a recall module based on a fine-tuned text embedding model that links flexible need descriptions to appropriate life services. Extensive experiments on real-world datasets demonstrate that PIGEON significantly outperforms closed-set approaches on need-based life service recall by an average of 19.37%. Human evaluation validates the reasonableness and specificity of our predictions. Additionally, we employ instruction tuning to enable smaller LLMs to achieve competitive performance, supporting practical deployment. </p>
<blockquote>
<p>ç”Ÿæ´»éœ€æ±‚æ˜¯äººä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­ä¸ºç”Ÿå­˜å’Œç¦ç¥‰æ‰€äº§ç”Ÿçš„éœ€æ±‚ã€‚åœ¨ç¾å›¢ç­‰ç”Ÿæ´»æœåŠ¡å¹³å°ä¸Šï¼Œç”¨æˆ·è´­ä¹°è¡Œä¸ºæ˜¯ç”±ç”Ÿæ´»éœ€æ±‚é©±åŠ¨çš„ï¼Œå› æ­¤è¿›è¡Œå‡†ç¡®çš„ç”Ÿæ´»éœ€æ±‚é¢„æµ‹å¯¹äºä¸ªæ€§åŒ–æœåŠ¡æ¨èè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•å°†éœ€æ±‚é¢„æµ‹è§†ä¸ºä¸€ä¸ªå°é—­é›†åˆ†ç±»é—®é¢˜ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å…¶æ•æ‰ç”Ÿæ´»éœ€æ±‚å¤šæ ·æ€§å’Œå¤æ‚æ€§çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®šä¹‰äº†ç”Ÿæ´»éœ€æ±‚é¢„æµ‹ä¸ºä¸€ä¸ªå¼€æ”¾é›†åˆ†ç±»é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ— é™åˆ¶éœ€æ±‚é¢„æµ‹çš„æ–°å‹ç³»ç»ŸPIGEONã€‚PIGEONé¦–å…ˆé‡‡ç”¨è¡Œä¸ºæ„ŸçŸ¥è®°å½•æ£€ç´¢å™¨å¸®åŠ©LLMç†è§£ç”¨æˆ·åå¥½ï¼Œç„¶åç»“åˆé©¬æ–¯æ´›çš„éœ€æ±‚å±‚æ¬¡ç†è®ºï¼Œä½¿é¢„æµ‹ä¸ç”Ÿæ´»éœ€æ±‚ç›¸ç¬¦åˆã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02713v1">PDF</a> ACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç¾å›¢ç­‰ç”Ÿæ´»æœåŠ¡å¹³å°ä¸Šç”¨æˆ·éœ€æ±‚çš„é¢„æµ‹é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•å°†å…¶è§†ä¸ºå°é—­é›†åˆ†ç±»é—®é¢˜ï¼Œæ— æ³•æ•æ‰ç”Ÿæ´»éœ€æ±‚çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚æœ¬æ–‡é‡æ–°å®šä¹‰ä¸ºå¼€æ”¾é›†åˆ†ç±»é—®é¢˜ï¼Œå¹¶æå‡ºPIGEONç³»ç»Ÿï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ— é™åˆ¶çš„éœ€æ±‚é¢„æµ‹ã€‚PIGEONé€šè¿‡è¡Œä¸ºæ„ŸçŸ¥è®°å½•æ£€ç´¢å™¨å¸®åŠ©LLMsç†è§£ç”¨æˆ·åå¥½ï¼Œå¹¶ç»“åˆé©¬æ–¯æ´›çš„éœ€æ±‚å±‚æ¬¡ç†è®ºè¿›è¡Œé¢„æµ‹ã€‚å®éªŒè¯æ˜ï¼ŒPIGEONåœ¨åŸºäºéœ€æ±‚çš„ç”Ÿæ´»æœåŠ¡å¬å›æ–¹é¢æ˜¾è‘—ä¼˜äºå°é—­é›†æ–¹æ³•ï¼Œå¹³å‡æé«˜19.37%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæ´»éœ€æ±‚é¢„æµ‹åœ¨ä¸ªæ€§åŒ–æœåŠ¡æ¨èä¸­å…·æœ‰é‡è¦æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å°†ç”Ÿæ´»éœ€æ±‚é¢„æµ‹è§†ä¸ºå°é—­é›†åˆ†ç±»é—®é¢˜ï¼Œé™åˆ¶äº†å…¶å¤šæ ·æ€§å’Œå¤æ‚æ€§çš„æ•æ‰ã€‚</li>
<li>PIGEONç³»ç»Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå¼€æ”¾é›†åˆ†ç±»çš„ç”Ÿæ´»éœ€æ±‚é¢„æµ‹ã€‚</li>
<li>PIGEONé€šè¿‡è¡Œä¸ºæ„ŸçŸ¥è®°å½•æ£€ç´¢å™¨ç†è§£ç”¨æˆ·åå¥½ï¼Œç»“åˆé©¬æ–¯æ´›éœ€æ±‚å±‚æ¬¡ç†è®ºè¿›è¡Œé¢„æµ‹ã€‚</li>
<li>PIGEONåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶åœ¨åŸºäºéœ€æ±‚çš„ç”Ÿæ´»æœåŠ¡å¬å›æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>äººç±»è¯„ä¼°éªŒè¯äº†PIGEONé¢„æµ‹çš„åˆç†æ€§å’Œç‰¹å¼‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b759d998f64c73ee26bda7829463c1aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-470b6cb049d7f816f0d07facee470408.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d5282eaf1236499e6c946a600777170.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Computational-Thinking-Reasoning-in-Large-Language-Models"><a href="#Computational-Thinking-Reasoning-in-Large-Language-Models" class="headerlink" title="Computational Thinking Reasoning in Large Language Models"></a>Computational Thinking Reasoning in Large Language Models</h2><p><strong>Authors:Kechi Zhang, Ge Li, Jia Li, Huangzhao Zhang, Jingjing Xu, Hao Zhu, Lecheng Wang, Jia Li, Yihong Dong, Jing Mai, Bin Gu, Zhi Jin</strong></p>
<p>While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they often struggle with complex tasks that require specific thinking paradigms, such as divide-and-conquer and procedural deduction, \etc Previous researches integrate external, reliable tools to alleviate logical inconsistencies and hallucinations in LLMsâ€™ problem-solving processes. However, we argue that the root challenge is more profound: LLMs lack the complex thinking paradigms (\ie, computational thinking) during reasoning. In this paper, we propose Computational Thinking Model (CTM), a novel framework that incorporates computational thinking paradigms into LLMs. This framework enables LLMs to reformulate complex problems through decomposition, abstraction, reduction, and simulation, among other techniques. Specifically, live code execution is seamlessly integrated into the reasoning process, allowing CTM to think by computing. CTM directly instills computational thinking objectives into LLMs through tailored reinforcement learning rewards, which encourages problem simplification, modular planning, and iterative verification. We conduct extensive evaluations on multiple code generation and mathematical benchmarks. The results demonstrate that CTM outperforms conventional reasoning models and tool-augmented baselines in terms of accuracy, interpretability, and generalizability. We hope this study offers valuable insights for AI reasoning, where LLMs can transform problems into robust, verifiable, and scalable computational workflows, much like computer scientists do. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºæƒŠäººçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†éœ€è¦ç‰¹å®šæ€ç»´æ¨¡å¼çš„å¤æ‚ä»»åŠ¡æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œä¾‹å¦‚åˆ†è€Œæ²»ä¹‹å’Œç¨‹åºæ¨ç†ç­‰ã€‚ä¹‹å‰çš„ç ”ç©¶é€šè¿‡æ•´åˆå¤–éƒ¨å¯é çš„å·¥å…·æ¥ç¼“è§£LLMåœ¨è§£å†³é—®é¢˜è¿‡ç¨‹ä¸­çš„é€»è¾‘ä¸ä¸€è‡´å’Œå¹»è§‰ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºçœŸæ­£çš„æŒ‘æˆ˜æ›´ä¸ºæ·±åˆ»ï¼šLLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç¼ºä¹å¤æ‚çš„æ€ç»´æ¨¡å¼ï¼ˆå³è®¡ç®—æ€ç»´ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è®¡ç®—æ€ç»´æ¨¡å‹ï¼ˆCTMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†è®¡ç®—æ€ç»´æ¨¡å¼èå…¥LLMçš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä½¿LLMèƒ½å¤Ÿé€šè¿‡åˆ†è§£ã€æŠ½è±¡ã€ç®€åŒ–å’Œæ¨¡æ‹Ÿç­‰æŠ€æœ¯é‡æ–°åˆ¶å®šå¤æ‚é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå®æ—¶ä»£ç æ‰§è¡Œæ— ç¼åœ°èå…¥åˆ°æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿CTMèƒ½å¤Ÿé€šè¿‡è®¡ç®—æ¥æ€è€ƒã€‚CTMé€šè¿‡å®šåˆ¶çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±ç›´æ¥å°†è®¡ç®—æ€ç»´ç›®æ ‡æ¤å…¥LLMï¼Œé¼“åŠ±é—®é¢˜ç®€åŒ–ã€æ¨¡å—åŒ–è§„åˆ’å’Œè¿­ä»£éªŒè¯ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»£ç ç”Ÿæˆå’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ³›åŒ–æ€§æ–¹é¢ï¼ŒCTMä¼˜äºä¼ ç»Ÿæ¨ç†æ¨¡å‹å’Œå·¥å…·å¢å¼ºåŸºçº¿ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹ç ”ç©¶èƒ½ä¸ºAIæ¨ç†æä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œä½¿LLMèƒ½å¤Ÿå°†é—®é¢˜è½¬åŒ–ä¸ºç¨³å¥ã€å¯éªŒè¯å’Œå¯æ‰©å±•çš„è®¡ç®—å·¥ä½œæµç¨‹ï¼Œå°±åƒè®¡ç®—æœºç§‘å­¦å®¶æ‰€åšçš„é‚£æ ·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02658v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šå±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä»é¢ä¸´ç‰¹å®šæ€ç»´æ¨¡å¼çš„æŒ‘æˆ˜ï¼Œå¦‚åˆ†è€Œæ²»ä¹‹ã€ç¨‹åºæ€§æ¨ç†ç­‰ã€‚ä¸ºç¼“è§£é€»è¾‘ä¸ä¸€è‡´å’Œå¹»è§‰é—®é¢˜ï¼Œå…ˆå‰ç ”ç©¶å°†å¤–éƒ¨å¯é å·¥å…·é›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚ç„¶è€Œï¼Œæœ¬æ–‡è®¤ä¸ºé—®é¢˜çš„æ ¹æºåœ¨äºå¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹å¤æ‚æ€ç»´æ¨¡å¼ï¼ˆå³è®¡ç®—æ€ç»´ï¼‰æ¥è¿›è¡Œæ¨ç†ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†è®¡ç®—æ€ç»´æ¨¡å‹ï¼ˆCTMï¼‰ï¼Œè¯¥æ¨¡å‹å°†è®¡ç®—æ€ç»´æ¨¡å¼èå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿé€šè¿‡åˆ†è§£ã€æŠ½è±¡ã€ç®€åŒ–ã€æ¨¡æ‹Ÿç­‰æŠ€æœ¯é‡æ–°æ„å»ºå¤æ‚é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯é€šè¿‡æ— ç¼é›†æˆå®æ—¶ä»£ç æ‰§è¡Œï¼Œä½¿CTMèƒ½å¤Ÿé€šè¿‡è®¡ç®—è¿›è¡Œæ¨ç†ã€‚æ­¤å¤–ï¼ŒCTMé€šè¿‡å®šåˆ¶çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±ç›´æ¥çŒè¾“è®¡ç®—æ€ç»´ç›®æ ‡ï¼Œé¼“åŠ±é—®é¢˜ç®€åŒ–ã€æ¨¡å—åŒ–è§„åˆ’å’Œè¿­ä»£éªŒè¯ã€‚åœ¨å¤šä¸ªä»£ç ç”Ÿæˆå’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒCTMåœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œé€šç”¨æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„æ¨ç†æ¨¡å‹å’Œå·¥å…·å¢å¼ºåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šå±•ç°å‡ºæ¨ç†èƒ½åŠ›ï¼Œä½†ä»é¢ä¸´ç‰¹å®šæ€ç»´æ¨¡å¼çš„æŒ‘æˆ˜ã€‚</li>
<li>é€»è¾‘ä¸ä¸€è‡´å’Œå¹»è§‰é—®é¢˜æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>å…ˆå‰çš„ç ”ç©¶é€šè¿‡é›†æˆå¤–éƒ¨å·¥å…·æ¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†é—®é¢˜æ ¹æºåœ¨äºç¼ºä¹è®¡ç®—æ€ç»´ã€‚</li>
<li>æå‡ºçš„è®¡ç®—æ€ç»´æ¨¡å‹ï¼ˆCTMï¼‰èå…¥è®¡ç®—æ€ç»´æ¨¡å¼åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚</li>
<li>CTMèƒ½å¤Ÿé‡æ–°æ„å»ºå¤æ‚é—®é¢˜ï¼Œé€šè¿‡åˆ†è§£ã€æŠ½è±¡ã€ç®€åŒ–ã€æ¨¡æ‹Ÿç­‰æŠ€æœ¯è¿›è¡Œæ¨ç†ã€‚</li>
<li>CTMé€šè¿‡å®æ—¶ä»£ç æ‰§è¡Œçš„æ— ç¼é›†æˆï¼Œèƒ½å¤Ÿé€šè¿‡è®¡ç®—è¿›è¡Œæ¨ç†ã€‚</li>
<li>CTMé€šè¿‡å®šåˆ¶çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±ç›´æ¥çŒè¾“è®¡ç®—æ€ç»´ç›®æ ‡ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02658">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b1b6930ff2164d5edb129d26e43ae2b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ca4a4d6e1a1490ae63df71f17e44099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5da0a6abc70e7edcb00c6230012abf1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-930c2226b068ca9520c233eca7f90a96.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Truly-Assessing-Fluid-Intelligence-of-Large-Language-Models-through-Dynamic-Reasoning-Evaluation"><a href="#Truly-Assessing-Fluid-Intelligence-of-Large-Language-Models-through-Dynamic-Reasoning-Evaluation" class="headerlink" title="Truly Assessing Fluid Intelligence of Large Language Models through   Dynamic Reasoning Evaluation"></a>Truly Assessing Fluid Intelligence of Large Language Models through   Dynamic Reasoning Evaluation</h2><p><strong>Authors:Yue Yang, MingKang Chen, Qihua Liu, Mengkang Hu, Qiguang Chen, Gengrui Zhang, Shuyue Hu, Guangtao Zhai, Yu Qiao, Yu Wang, Wenqi Shao, Ping Luo</strong></p>
<p>Recent advances in large language models (LLMs) have demonstrated impressive reasoning capacities that mirror human-like thinking. However, whether LLMs possess genuine fluid intelligence (i.e., the ability to reason abstractly and generalize rules in novel situations) remains an open question. Existing reasoning benchmarks either focus on domain-specific knowledge (crystallized intelligence) or lack interpretability. To address these limitations, we propose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a hierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning tasks organized across four cognitive levels, with each task featuring multiple dynamic variants that test the same underlying latent rule. This design enables fine-grained, interpretable, and reliable assessments of fluid intelligence. We evaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o, Claude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1). Experimental results reveal that although most LLMs achieve competent and robust performance in low-level cognition, they struggle with high-level cognition and exhibit limited generalization as task complexity grows. Our findings highlight the gap between current LLMs and true human-like fluid intelligence and offer a new path for systematically tracking reasoning progress in LLMs. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œåæ˜ äº†äººç±»æ€ç»´ã€‚ç„¶è€Œï¼ŒLLMæ˜¯å¦æ‹¥æœ‰çœŸæ­£çš„æµä½“æ™ºåŠ›ï¼ˆå³åœ¨æ–°çš„æƒ…å†µä¸‹è¿›è¡ŒæŠ½è±¡æ¨ç†å’Œè§„åˆ™æ¦‚æ‹¬çš„èƒ½åŠ›ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ç°æœ‰çš„æ¨ç†åŸºå‡†æµ‹è¯•è¦ä¹ˆä¾§é‡äºç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼ˆæ™¶ä½“æ™ºåŠ›ï¼‰ï¼Œè¦ä¹ˆç¼ºä¹è§£é‡Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†DRE-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåˆ†å±‚è®¤çŸ¥æ¡†æ¶çš„åŠ¨æ€æ¨ç†è¯„ä¼°åŸºå‡†ã€‚DRE-Benchç”±36ä¸ªæŠ½è±¡æ¨ç†ä»»åŠ¡ç»„æˆï¼Œè¿™äº›ä»»åŠ¡åˆ†ä¸ºå››ä¸ªè®¤çŸ¥å±‚æ¬¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æœ‰å¤šä¸ªåŠ¨æ€å˜ä½“ï¼Œæµ‹è¯•ç›¸åŒçš„æ½œåœ¨è§„åˆ™ã€‚è¿™ç§è®¾è®¡å®ç°äº†å¯¹æµä½“æ™ºåŠ›çš„ç²¾ç»†ã€å¯è§£é‡Šå’Œå¯é çš„è¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„LLMï¼ŒåŒ…æ‹¬é€šç”¨LLMï¼ˆGPT-4oã€Claude 3.7ï¼‰å’Œæ¨ç†LLMï¼ˆo1ã€DeepSeek-R1ã€QwQã€Skywork-OR1ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¤§å¤šæ•°LLMåœ¨ä½çº§è®¤çŸ¥æ–¹é¢è¾¾åˆ°äº†ç¨³å¥å’Œç†Ÿç»ƒçš„æ°´å¹³ï¼Œä½†åœ¨é«˜çº§è®¤çŸ¥æ–¹é¢å´è¡¨ç°æŒ£æ‰ï¼Œéšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢é•¿ï¼Œå…¶æ¦‚æ‹¬èƒ½åŠ›æœ‰é™ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†å½“å‰LLMä¸çœŸæ­£çš„äººç±»æµä½“æ™ºåŠ›ä¹‹é—´çš„å·®è·ï¼Œå¹¶ä¸ºç³»ç»Ÿè·Ÿè¸ªLLMçš„æ¨ç†è¿›æ­¥æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02648v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»ã€ä¸äººç±»æ€è€ƒç›¸ä»¿çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMæ˜¯å¦æ‹¥æœ‰çœŸæ­£çš„æµä½“æ™ºåŠ›ï¼ˆå³åœ¨æ–°çš„æƒ…å¢ƒä¸‹æŠ½è±¡æ¨ç†å’Œè§„åˆ™åº”ç”¨çš„èƒ½åŠ›ï¼‰ä»æ˜¯æ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚å½“å‰æ¨ç†åŸºå‡†æµ‹è¯•é›†ä¸­åœ¨ç‰¹å®šé¢†åŸŸçŸ¥è¯†æˆ–ç¼ºä¹è§£é‡Šæ€§æ–¹é¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºDRE-Benchï¼Œä¸€ä¸ªåŸºäºåˆ†å±‚è®¤çŸ¥æ¡†æ¶çš„åŠ¨æ€æ¨ç†è¯„ä¼°åŸºå‡†ã€‚DRE-BenchåŒ…å«36ä¸ªæŠ½è±¡æ¨ç†ä»»åŠ¡ï¼Œåˆ†ä¸ºå››ä¸ªè®¤çŸ¥å±‚æ¬¡ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«å¤šä¸ªæµ‹è¯•ç›¸åŒæ½œåœ¨è§„åˆ™çš„åŠ¨æ€å˜ä½“ã€‚è¿™ç§è®¾è®¡å¯å®ç°ç²¾ç»†ã€å¯è§£é‡Šå’Œå¯é çš„æµä½“æ™ºåŠ›è¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4oå’ŒClaude 3.7ï¼‰å’Œä¸“é—¨ç”¨äºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚o1ã€DeepSeek-R1ã€QwQå’ŒSkywork-OR1ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½å±‚æ¬¡è®¤çŸ¥æ–¹é¢è¡¨ç°è‰¯å¥½ä¸”ç¨³å¥ï¼Œä½†åœ¨é«˜å±‚æ¬¡è®¤çŸ¥æ–¹é¢è¡¨ç°æ¬ ä½³ï¼Œéšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢é•¿ï¼Œå…¶è¡¨ç°æœ‰é™ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»çœŸæ­£çš„æµä½“æ™ºåŠ›ä¹‹é—´çš„å·®è·ï¼Œå¹¶ä¸ºç³»ç»Ÿåœ°è·Ÿè¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ¨ç†çš„è¿›æ­¥æä¾›äº†æ–°çš„é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºä¸äººç±»ç›¸ä¼¼çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æµä½“æ™ºåŠ›ï¼ˆåœ¨æ–°çš„æƒ…å¢ƒä¸‹æŠ½è±¡æ¨ç†å’Œè§„åˆ™åº”ç”¨çš„èƒ½åŠ›ï¼‰ä»æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸€ä¸ªå¼€æ”¾é—®é¢˜ã€‚</li>
<li>å½“å‰æ¨ç†åŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦é›†ä¸­åœ¨ç‰¹å®šé¢†åŸŸçŸ¥è¯†æˆ–ç¼ºä¹è§£é‡Šæ€§ã€‚</li>
<li>DRE-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŠ¨æ€æ¨ç†è¯„ä¼°åŸºå‡†ï¼Œå…·æœ‰åˆ†å±‚è®¤çŸ¥æ¡†æ¶å’Œç²¾ç»†ã€å¯è§£é‡Šçš„è¯„ä¼°ç‰¹ç‚¹ã€‚</li>
<li>å¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½å±‚æ¬¡è®¤çŸ¥ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é«˜å±‚æ¬¡è®¤çŸ¥ä¸Šè¡¨ç°æœ‰é™ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»çœŸæ­£çš„æµä½“æ™ºåŠ›ä¹‹é—´å­˜åœ¨å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a2a2e8f4767534fc9a4b315e0d6da7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-257762f3b3c6c3b3a5fa31f508f3c3e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c79ae66dc7f70bb07e4afec0c223340b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49ef4d9c35652c6bec35d3b045086a78.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Beyond-the-Surface-Measuring-Self-Preference-in-LLM-Judgments"><a href="#Beyond-the-Surface-Measuring-Self-Preference-in-LLM-Judgments" class="headerlink" title="Beyond the Surface: Measuring Self-Preference in LLM Judgments"></a>Beyond the Surface: Measuring Self-Preference in LLM Judgments</h2><p><strong>Authors:Zhi-Yuan Chen, Hao Wang, Xinyu Zhang, Enrui Hu, Yankai Lin</strong></p>
<p>Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/zhiyuanc2001/self-preference">https://github.com/zhiyuanc2001/self-preference</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½œä¸ºè¯„åˆ¤è€…æ—¶è¡¨ç°å‡ºè‡ªæˆ‘åå¥½åè§ï¼Œå³å®ƒä»¬å€¾å‘äºåå¥½è‡ªå·±çš„å›ç­”è€Œä¸æ˜¯å…¶ä»–æ¨¡å‹ç”Ÿæˆçš„å›ç­”ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸é€šè¿‡è®¡ç®—è¯„åˆ¤æ¨¡å‹å¯¹è‡ªå·±å›åº”çš„è¯„åˆ†ä¸å…¶ä»–æ¨¡å‹å›åº”çš„è¯„åˆ†ä¹‹é—´çš„å·®å¼‚æ¥è¡¡é‡è¿™ç§åè§ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å°†è‡ªæˆ‘åå¥½åè§ä¸å›ç­”è´¨é‡æ··æ·†åœ¨ä¸€èµ·ï¼Œå› ä¸ºå³ä½¿ä¸å­˜åœ¨åè§ï¼Œè¯„åˆ¤æ¨¡å‹çš„é«˜è´¨é‡å›ç­”ä¹Ÿå¯èƒ½å¯¼è‡´æ­£é¢å¾—åˆ†å·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥é‡‘ç‰Œè¯„åˆ¤ä½œä¸ºå›åº”å®é™…è´¨é‡çš„ä»£ç†ï¼Œå¹¶æå‡ºDBGåˆ†æ•°ï¼Œè¯¥åˆ†æ•°è¡¡é‡è‡ªæˆ‘åå¥½åè§ä¸ºè¯„åˆ¤æ¨¡å‹å¯¹è‡ªå·±å›åº”çš„è¯„åˆ†ä¸ç›¸åº”çš„é‡‘ç‰Œè¯„åˆ¤ä¹‹é—´çš„å·®å€¼ã€‚ç”±äºé‡‘ç‰Œè¯„åˆ¤åæ˜ äº†çœŸæ­£çš„å›åº”è´¨é‡ï¼Œå› æ­¤DBGåˆ†æ•°å‡è½»äº†å›åº”è´¨é‡å¯¹åè§æµ‹é‡çš„æ··æ·†å½±å“ã€‚ä½¿ç”¨DBGåˆ†æ•°ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œä»¥è¯„ä¼°ä¸åŒç‰ˆæœ¬ã€è§„æ¨¡å’Œæ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘åå¥½åè§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†å½±å“å¹¶æœ‰åŠ©äºç¼“è§£è‡ªæˆ‘åå¥½åè§çš„ä¸¤ä¸ªå› ç´ ï¼šå›åº”æ–‡æœ¬é£æ ¼å’Œè¯„åˆ¤æ¨¡å‹çš„åç»­è®­ç»ƒæ•°æ®ã€‚æœ€åï¼Œæˆ‘ä»¬ä»æ³¨æ„åŠ›è§†è§’æ¢ç´¢äº†è‡ªæˆ‘åå¥½åè§æ½œåœ¨çš„æ ¹æœ¬æœºåˆ¶ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhiyuanc20">https://github.com/zhiyuanc20</a> 0 1 &#x2F;self-preferenceæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½œä¸ºè¯„åˆ¤è€…æ—¶å­˜åœ¨è‡ªæˆ‘åå¥½åå·®ï¼Œå³å€¾å‘äºé’çè‡ªå·±çš„å›åº”è€Œéå…¶ä»–æ¨¡å‹äº§ç”Ÿçš„å›åº”ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡è®¡ç®—è¯„åˆ¤æ¨¡å‹å¯¹è‡ªå·±å›åº”ä¸å…¶ä»–æ¨¡å‹å›åº”çš„è¯„åˆ†å·®å¼‚æ¥è¡¡é‡è¿™ç§åå·®ï¼Œä½†è¿™ç§æ–¹æ³•å°†è‡ªæˆ‘åå¥½åå·®ä¸å›åº”è´¨é‡æ··æ·†ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥é‡‘ç‰Œè¯„åˆ¤ä½œä¸ºå›åº”å®é™…è´¨é‡çš„ä»£ç†ï¼Œå¹¶æå‡ºDBGè¯„åˆ†ï¼Œè¯¥è¯„åˆ†è¡¡é‡è‡ªæˆ‘åå¥½åå·®ä¸ºè¯„åˆ¤æ¨¡å‹å¯¹è‡ªå·±å›åº”çš„è¯„åˆ†ä¸é‡‘ç‰Œè¯„åˆ¤ä¹‹é—´çš„å·®å€¼ã€‚é€šè¿‡DBGè¯„åˆ†ï¼Œæˆ‘ä»¬å¯¹ä¸åŒç‰ˆæœ¬ã€è§„æ¨¡å’Œæ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘åå¥½åå·®è¿›è¡Œäº†å…¨é¢å®éªŒã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†å½±å“å¹¶æœ‰åŠ©äºç¼“è§£è‡ªæˆ‘åå¥½åå·®çš„ä¸¤ä¸ªå› ç´ ï¼šå›åº”æ–‡æœ¬é£æ ¼å’Œè¯„åˆ¤æ¨¡å‹çš„åç»­è®­ç»ƒæ•°æ®ã€‚æœ€åï¼Œæˆ‘ä»¬ä»æ³¨æ„åŠ›è§’åº¦æ¢è®¨äº†è‡ªæˆ‘åå¥½åå·®çš„æ½œåœ¨å†…åœ¨æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½œä¸ºè¯„åˆ¤è€…æ—¶å­˜åœ¨è‡ªæˆ‘åå¥½åå·®ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¡¡é‡è‡ªæˆ‘åå¥½åå·®æ—¶å­˜åœ¨ä¸å›åº”è´¨é‡æ··æ·†çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥é‡‘ç‰Œè¯„åˆ¤ä½œä¸ºå›åº”è´¨é‡çš„ä»£ç†ï¼Œæå‡ºDBGè¯„åˆ†æ¥è¡¡é‡è‡ªæˆ‘åå¥½åå·®ã€‚</li>
<li>DBGè¯„åˆ†æœ‰æ•ˆåŒºåˆ†äº†å›åº”è´¨é‡ä¸è‡ªæˆ‘åå¥½åå·®ã€‚</li>
<li>é€šè¿‡å®éªŒè¯„ä¼°äº†ä¸åŒç‰ˆæœ¬ã€è§„æ¨¡å’Œæ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘åå¥½åå·®ã€‚</li>
<li>å›åº”æ–‡æœ¬é£æ ¼å’Œè¯„åˆ¤æ¨¡å‹çš„åç»­è®­ç»ƒæ•°æ®æ˜¯å½±å“å¹¶æœ‰åŠ©äºç¼“è§£è‡ªæˆ‘åå¥½åå·®çš„ä¸¤ä¸ªé‡è¦å› ç´ ã€‚</li>
<li>ä»æ³¨æ„åŠ›è§’åº¦æ¢è®¨äº†è‡ªæˆ‘åå¥½åå·®çš„æ½œåœ¨å†…åœ¨æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d64a70814913147eb0fc8814e2fe083.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-538139e4ab3942de6a2f191a426f3618.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73058554f7b0a3aba2adf89dd12c8ce4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c387c5c243d1aaef13c4657744d1462.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5967dd74d0af5d0ffe7709919224f859.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="V2X-UniPool-Unifying-Multimodal-Perception-and-Knowledge-Reasoning-for-Autonomous-Driving"><a href="#V2X-UniPool-Unifying-Multimodal-Perception-and-Knowledge-Reasoning-for-Autonomous-Driving" class="headerlink" title="V2X-UniPool: Unifying Multimodal Perception and Knowledge Reasoning for   Autonomous Driving"></a>V2X-UniPool: Unifying Multimodal Perception and Knowledge Reasoning for   Autonomous Driving</h2><p><strong>Authors:Xuewen Luo, Fengze Yang, Fan Ding, Xiangbo Gao, Shuo Xing, Yang Zhou, Zhengzhong Tu, Chenxi Liu</strong></p>
<p>Knowledge-driven autonomous driving systems(ADs) offer powerful reasoning capabilities, but face two critical challenges: limited perception due to the short-sightedness of single-vehicle sensors, and hallucination arising from the lack of real-time environmental grounding. To address these issues, this paper introduces V2X-UniPool, a unified framework that integrates multimodal Vehicle-to-Everything (V2X) data into a time-indexed and language-based knowledge pool. By leveraging a dual-query Retrieval-Augmented Generation (RAG) mechanism, which enables retrieval of both static and dynamic knowledge, our system enables ADs to perform accurate, temporally consistent reasoning over both static environment and dynamic traffic context. Experiments on a real-world cooperative driving dataset demonstrate that V2X-UniPool significantly enhances motion planning accuracy and reasoning capability. Remarkably, it enables even zero-shot vehicle-side models to achieve state-of-the-art performance by leveraging V2X-UniPool, while simultaneously reducing transmission cost by over 99.9% compared to prior V2X methods. </p>
<blockquote>
<p>çŸ¥è¯†é©±åŠ¨çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADsï¼‰æä¾›äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šç”±äºå•è½¦ä¼ æ„Ÿå™¨çš„ç›®å…‰çŸ­æµ…è€Œå¯¼è‡´çš„æ„ŸçŸ¥å±€é™ï¼Œä»¥åŠç”±äºç¼ºä¹å®æ—¶ç¯å¢ƒåŸºç¡€è€Œäº§ç”Ÿçš„å¹»è§‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†V2X-UniPoolï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå®ƒå°†å¤šæ¨¡å¼çš„è½¦å¯¹ä¸€åˆ‡ï¼ˆV2Xï¼‰æ•°æ®é›†æˆåˆ°ä¸€ä¸ªåŸºäºæ—¶é—´å’Œè¯­è¨€çš„çŸ¥è¯†åº“ä¸­ã€‚é€šè¿‡åˆ©ç”¨åŒæŸ¥è¯¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿæ£€ç´¢é™æ€å’ŒåŠ¨æ€çŸ¥è¯†ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿä½¿è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿèƒ½å¤Ÿåœ¨é™æ€ç¯å¢ƒå’ŒåŠ¨æ€äº¤é€šç¯å¢ƒä¸­è¿›è¡Œå‡†ç¡®ä¸”æ—¶é—´ä¸€è‡´çš„æ¨ç†ã€‚åœ¨çœŸå®ä¸–ç•Œçš„ååŒé©¾é©¶æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒV2X-UniPoolæ˜¾è‘—æé«˜äº†è¿åŠ¨è§„åˆ’å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒç”šè‡³ä½¿é›¶è½¦è¾†æ¨¡å‹é€šè¿‡åˆ©ç”¨V2X-UniPoolå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸å…ˆå‰çš„V2Xæ–¹æ³•ç›¸æ¯”ï¼Œä¼ è¾“æˆæœ¬é™ä½äº†99.9%ä»¥ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02580v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>çŸ¥è¯†é©±åŠ¨çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADsï¼‰é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå•ä¸€è½¦è¾†ä¼ æ„Ÿå™¨çš„è§†é‡å—é™å¯¼è‡´çš„æ„ŸçŸ¥å—é™ï¼Œä»¥åŠç”±äºç¼ºä¹å®æ—¶ç¯å¢ƒåŸºç¡€è€Œäº§ç”Ÿçš„å¹»è§‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†V2X-UniPoolç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤šæ¨¡æ€çš„è½¦å¯¹ä¸‡ç‰©ï¼ˆV2Xï¼‰æ•°æ®é›†æˆåˆ°æ—¶é—´ç´¢å¼•å’Œè¯­è¨€åŸºç¡€çš„çŸ¥è¯†æ± ä¸­ã€‚é€šè¿‡åˆ©ç”¨åŒæŸ¥è¯¢æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿæ£€ç´¢é™æ€å’ŒåŠ¨æ€çŸ¥è¯†ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿä½¿è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿèƒ½å¤Ÿåœ¨é™æ€ç¯å¢ƒå’ŒåŠ¨æ€äº¤é€šç¯å¢ƒä¸­è¿›è¡Œå‡†ç¡®ä¸”æ—¶é—´ä¸€è‡´çš„æ¨ç†ã€‚åœ¨çœŸå®ä¸–ç•Œçš„åˆä½œé©¾é©¶æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒV2X-UniPoolæ˜¾è‘—æé«˜äº†è¿åŠ¨è§„åˆ’å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒç”šè‡³èƒ½ä½¿é›¶è½¦è¾†æ¨¡å‹é€šè¿‡åˆ©ç”¨V2X-UniPoolå®ç°å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¸å…ˆå‰çš„V2Xæ–¹æ³•ç›¸æ¯”ï¼Œä¼ è¾“æˆæœ¬é™ä½äº†99.9%ä»¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿé¢ä¸´æ„ŸçŸ¥å—é™å’Œå¹»è§‰ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>V2X-UniPoolæ¡†æ¶é›†æˆäº†å¤šæ¨¡æ€çš„V2Xæ•°æ®ã€‚</li>
<li>V2X-UniPoolé‡‡ç”¨åŒæŸ¥è¯¢æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æœºåˆ¶è¿›è¡ŒçŸ¥è¯†æ£€ç´¢ã€‚</li>
<li>V2X-UniPoolæé«˜äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„è¿åŠ¨è§„åˆ’å‡†ç¡®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>V2X-UniPoolä½¿é›¶è½¦è¾†æ¨¡å‹ä¹Ÿèƒ½å®ç°å“è¶Šæ€§èƒ½ã€‚</li>
<li>V2X-UniPoolé™ä½äº†ä¼ è¾“æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9213d7e2a84e756039fc883ce5a69a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-495420d001425a8de36898c0736963de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1e5b7abdb9be57cd8cedc6317693505.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CyberGym-Evaluating-AI-Agentsâ€™-Cybersecurity-Capabilities-with-Real-World-Vulnerabilities-at-Scale"><a href="#CyberGym-Evaluating-AI-Agentsâ€™-Cybersecurity-Capabilities-with-Real-World-Vulnerabilities-at-Scale" class="headerlink" title="CyberGym: Evaluating AI Agentsâ€™ Cybersecurity Capabilities with   Real-World Vulnerabilities at Scale"></a>CyberGym: Evaluating AI Agentsâ€™ Cybersecurity Capabilities with   Real-World Vulnerabilities at Scale</h2><p><strong>Authors:Zhun Wang, Tianneng Shi, Jingxuan He, Matthew Cai, Jialin Zhang, Dawn Song</strong></p>
<p>Large language model (LLM) agents are becoming increasingly skilled at handling cybersecurity tasks autonomously. Thoroughly assessing their cybersecurity capabilities is critical and urgent, given the high stakes in this domain. However, existing benchmarks fall short, often failing to capture real-world scenarios or being limited in scope. To address this gap, we introduce CyberGym, a large-scale and high-quality cybersecurity evaluation framework featuring 1,507 real-world vulnerabilities found and patched across 188 large software projects. While it includes tasks of various settings, CyberGym primarily focuses on the generation of proof-of-concept (PoC) tests for vulnerability reproduction, based on text descriptions and corresponding source repositories. Solving this task is particularly challenging, as it requires comprehensive reasoning across entire codebases to locate relevant code fragments and produce effective PoCs that accurately trigger the target vulnerability starting from the programâ€™s entry point. Our evaluation across 4 state-of-the-art agent frameworks and 9 LLMs reveals that even the best combination (OpenHands and Claude-3.7-Sonnet) achieves only a 11.9% reproduction success rate, mainly on simpler cases. Beyond reproducing historical vulnerabilities, we find that PoCs generated by LLM agents can reveal new vulnerabilities, identifying 15 zero-days affecting the latest versions of the software projects. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨è‡ªä¸»å¤„ç†ç½‘ç»œå®‰å…¨ä»»åŠ¡æ–¹é¢å˜å¾—è¶Šæ¥è¶Šç†Ÿç»ƒã€‚è€ƒè™‘åˆ°ç½‘ç»œå®‰å…¨é¢†åŸŸçš„é«˜é£é™©ï¼Œå…¨é¢è¯„ä¼°å…¶ç½‘ç»œå®‰å…¨èƒ½åŠ›è‡³å…³é‡è¦ä¸”ç´§è¿«ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨ä¸è¶³ï¼Œé€šå¸¸æ— æ³•æ•æ‰çœŸå®åœºæ™¯æˆ–èŒƒå›´æœ‰é™ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CyberGymï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„ç½‘ç»œå®‰å…¨è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«188ä¸ªå¤§å‹è½¯ä»¶é¡¹ç›®ä¸­å‘ç°çš„1007ä¸ªçœŸå®æ¼æ´ã€‚è™½ç„¶å®ƒåŒ…å«äº†å„ç§è®¾ç½®çš„ä»»åŠ¡ï¼Œä½†CyberGymä¸»è¦ä¾§é‡äºåŸºäºæ–‡æœ¬æè¿°å’Œç›¸åº”æºä»£ç ä»“åº“ç”Ÿæˆæ¦‚å¿µè¯æ˜ï¼ˆPoCï¼‰æµ‹è¯•ä»¥å®ç°æ¼æ´å¤ç°ã€‚è§£å†³è¿™ä¸€ä»»åŠ¡ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒéœ€è¦å…¨é¢æ¨ç†æ•´ä¸ªä»£ç åº“ï¼Œä»¥æ‰¾åˆ°ç›¸å…³çš„ä»£ç ç‰‡æ®µå¹¶ç”Ÿæˆæœ‰æ•ˆçš„PoCï¼Œä»ç¨‹åºå…¥å£ç‚¹å¼€å§‹å‡†ç¡®è§¦å‘ç›®æ ‡æ¼æ´ã€‚æˆ‘ä»¬å¯¹4ä¸ªæœ€å…ˆè¿›çš„ä»£ç†æ¡†æ¶å’Œ9ä¸ªLLMçš„è¯„ä¼°è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€ä½³ç»„åˆï¼ˆOpenHandså’ŒClaude-3.7-Sonnetï¼‰ä¹Ÿä»…å®ç°äº†11.9%çš„å¤ç°æˆåŠŸç‡ï¼Œä¸»è¦æ˜¯åœ¨è¾ƒç®€å•çš„æƒ…å†µä¸‹ã€‚é™¤äº†å¤ç°å†å²æ¼æ´å¤–ï¼Œæˆ‘ä»¬å‘ç°LLMä»£ç†ç”Ÿæˆçš„PoCè¿˜å¯ä»¥æ­ç¤ºæ–°çš„æ¼æ´ï¼Œè¯†åˆ«å‡ºå½±å“è½¯ä»¶é¡¹ç›®æœ€æ–°ç‰ˆæœ¬ä¸­çš„15ä¸ªé›¶æ—¥æ¼æ´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02548v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ç½‘ç»œå®‰å…¨ä»»åŠ¡æ—¶å±•ç°å‡ºè¶Šæ¥è¶Šå¤šçš„è‡ªä¸»èƒ½åŠ›ã€‚ç°æœ‰çš„ç½‘ç»œå®‰å…¨è¯„ä¼°å·¥å…·æ— æ³•å®Œå…¨æ»¡è¶³éœ€æ±‚ï¼Œä¸ºæ­¤å¼•å…¥äº†CyberGymè¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–å¤§é‡çœŸå®ä¸–ç•Œä¸­çš„æ¼æ´å¹¶åŸºäºæ–‡æœ¬æè¿°å’Œå¯¹åº”çš„æºä»£ç åº“ç”Ÿæˆé’ˆå¯¹æ¼æ´çš„å¤ç°æµ‹è¯•ã€‚å°½ç®¡æŒ‘æˆ˜å·¨å¤§ï¼ŒLLMæ¨¡å‹ä¾ç„¶å±•ç°äº†ä¸€å®šèƒ½åŠ›ï¼Œä½†åœ¨é¢å¯¹æ›´å¤æ‚çš„æ¼æ´æ—¶ï¼ŒæˆåŠŸå¤ç°ç‡ä»…ä¸ºæœ€é«˜ä»…è¾¾ç™¾åˆ†ä¹‹åä¸€ã€‚æ­¤å¤–ï¼ŒLLMæ¨¡å‹ç”Ÿæˆçš„æµ‹è¯•è¿˜èƒ½æ­ç¤ºæ–°çš„æ¼æ´ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œå®‰å…¨ä»»åŠ¡å¤„ç†æ–¹é¢æŠ€èƒ½å¢å¼ºã€‚</li>
<li>å½“å‰ç½‘ç»œå®‰å…¨è¯„ä¼°å·¥å…·å­˜åœ¨ç¼ºé™·ï¼Œæ— æ³•å……åˆ†æ•æ‰çœŸå®åœºæ™¯æˆ–å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>CyberGymè¯„ä¼°æ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€ç¼ºé™·ï¼ŒåŒ…å«å¤§é‡çœŸå®æ¼æ´å¹¶ç”Ÿæˆé’ˆå¯¹æ¼æ´çš„å¤ç°æµ‹è¯•ã€‚</li>
<li>LLMæ¨¡å‹åœ¨ç”Ÿæˆé’ˆå¯¹æ¼æ´çš„å¤ç°æµ‹è¯•æ—¶é¢ä¸´æŒ‘æˆ˜ï¼ŒæˆåŠŸå¤ç°ç‡è¾ƒä½ã€‚</li>
<li>LLMæ¨¡å‹å±•ç°å‡ºçš„èƒ½åŠ›ä¸ä»…èƒ½å¤ç°å·²çŸ¥æ¼æ´ï¼Œè¿˜èƒ½æ­ç¤ºæ–°çš„æ¼æ´ï¼ˆå¦‚é›¶æ—¥æ”»å‡»ï¼‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11bb080ec2ca3617dfff48e8ce6e0484.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7df2b3ba8f2cddceffbdd6eb96c10ef3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67090e38f62cb3b9bf1c35aed3318944.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-342ebe59b68f3dddcf3abb49943fc787.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60db9e8305b772860b4d1c98871e422d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1ed8a1805d2629740cf7d9885a68ae7.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3ccbf004e61afa8d7776c3ee3cb40667.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and   Semantic Understanding Capability of LLM
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-04/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-46e0175d1ff9f03444368727824fe390.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-04  Absolute Coordinates Make Motion Generation Easy
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
