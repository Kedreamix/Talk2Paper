<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-06-05  Towards a Japanese Full-duplex Spoken Dialogue System">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-462ee0d85174a23ec13fc95abddc28ab.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    65 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-05-更新"><a href="#2025-06-05-更新" class="headerlink" title="2025-06-05 更新"></a>2025-06-05 更新</h1><h2 id="Towards-a-Japanese-Full-duplex-Spoken-Dialogue-System"><a href="#Towards-a-Japanese-Full-duplex-Spoken-Dialogue-System" class="headerlink" title="Towards a Japanese Full-duplex Spoken Dialogue System"></a>Towards a Japanese Full-duplex Spoken Dialogue System</h2><p><strong>Authors:Atsumoto Ohashi, Shinya Iizuka, Jingjing Jiang, Ryuichiro Higashinaka</strong></p>
<p>Full-duplex spoken dialogue systems, which can model simultaneous bidirectional features of human conversations such as speech overlaps and backchannels, have attracted significant attention recently. However, the study of full-duplex spoken dialogue systems for the Japanese language has been limited, and the research on their development in Japanese remains scarce. In this paper, we present the first publicly available full-duplex spoken dialogue model in Japanese, which is built upon Moshi, a full-duplex dialogue model in English. Our model is trained through a two-stage process: pre-training on a large-scale spoken dialogue data in Japanese, followed by fine-tuning on high-quality stereo spoken dialogue data. We further enhance the model’s performance by incorporating synthetic dialogue data generated by a multi-stream text-to-speech system. Evaluation experiments demonstrate that the trained model outperforms Japanese baseline models in both naturalness and meaningfulness. </p>
<blockquote>
<p>近年来，全双工语音对话系统能够模拟人类对话的双向特征，如语音重叠和反馈通道等，因此受到了广泛关注。然而，针对日语的全双工语音对话系统的研究相对较少，其开发研究也十分匮乏。在本文中，我们推出了首个公开可用的日语全双工语音对话模型，该模型基于英语全双工对话模型Moshi构建。我们的模型通过两阶段训练：首先在大量日语口语对话数据上进行预训练，然后在高质量立体声口语对话数据上进行微调。我们通过引入由多流文本到语音系统生成的人工对话数据，进一步提高了模型的性能。评估实验表明，训练后的模型在自然度和意义性方面都优于日语基线模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02979v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>该论文介绍了首个公开的日语全双工对话系统模型，该模型基于英语的全双工对话模型Moshi构建。通过大规模日语对话数据预训练和高质量立体声对话数据微调的两阶段过程进行训练，并使用多流文本到语音系统生成合成对话数据增强模型性能。评估实验表明，该模型在日语基准模型的自然度和有意义性方面都表现优越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文构建了首个公开的日语全双工对话系统模型。</li>
<li>模型基于英语的全双工对话模型Moshi构建。</li>
<li>模型通过预训练与微调的方式进行训练，使用大规模日语对话数据和高质量立体声对话数据。</li>
<li>合成对话数据通过多流文本到语音系统生成，增强了模型性能。</li>
<li>模型在自然度和有意义性方面超越了日语基准模型。</li>
<li>全双工对话系统能够模拟人类对话的双向特征，如语音重叠和反馈通道。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b7400a8cc0d30f4bfd02ffe1a4326777.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76749f6f0d5726186e99f2cc572edd14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b56fe64df06e5e9b544a1ae35ffc2db7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-847c4c71e81a6d97a225e83bfc956bb6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Diffusion-Buffer-Online-Diffusion-based-Speech-Enhancement-with-Sub-Second-Latency"><a href="#Diffusion-Buffer-Online-Diffusion-based-Speech-Enhancement-with-Sub-Second-Latency" class="headerlink" title="Diffusion Buffer: Online Diffusion-based Speech Enhancement with   Sub-Second Latency"></a>Diffusion Buffer: Online Diffusion-based Speech Enhancement with   Sub-Second Latency</h2><p><strong>Authors:Bunlong Lay, Rostilav Makarov, Timo Gerkmann</strong></p>
<p>Diffusion models are a class of generative models that have been recently used for speech enhancement with remarkable success but are computationally expensive at inference time. Therefore, these models are impractical for processing streaming data in real-time. In this work, we adapt a sliding window diffusion framework to the speech enhancement task. Our approach progressively corrupts speech signals through time, assigning more noise to frames close to the present in a buffer. This approach outputs denoised frames with a delay proportional to the chosen buffer size, enabling a trade-off between performance and latency. Empirical results demonstrate that our method outperforms standard diffusion models and runs efficiently on a GPU, achieving an input-output latency in the order of 0.3 to 1 seconds. This marks the first practical diffusion-based solution for online speech enhancement. </p>
<blockquote>
<p>扩散模型是一类生成模型，最近被用于语音增强并取得了显著的成功，但在推理时计算成本较高。因此，这些模型对于实时处理流式数据并不实用。在这项工作中，我们将滑动窗口扩散框架适应于语音增强任务。我们的方法通过时间逐步破坏语音信号，将更多噪声分配给缓冲区中接近当前的帧。这种方法输出的去噪帧延迟与所选缓冲区大小成比例，可以在性能和延迟之间进行权衡。经验结果表明，我们的方法优于标准扩散模型，在GPU上运行高效，输入输出延迟在0.3至1秒之间。这标志着基于扩散的在线语音增强的首个实用解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02908v1">PDF</a> 5 pages, 2 figures, Accepted to Interspeech 2025</p>
<p><strong>Summary</strong><br>本文提出一种基于滑动窗口扩散框架的语音增强方法，该方法在逐步对语音信号进行去噪的过程中加入了时间因素，实现了性能和延迟之间的权衡。相较于传统扩散模型，该方法在GPU上运行更为高效，实现了输入输出的延迟在0.3至1秒之间，为在线语音增强提供了首个实用的扩散模型解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在语音增强任务中取得了显著的成功，但计算成本较高，不适合处理实时流媒体数据。</li>
<li>本文采用滑动窗口扩散框架进行语音增强，将时间因素纳入考虑，逐步对语音信号进行去噪。</li>
<li>方法通过在缓冲区中对接近当前帧的帧赋予更多噪声来实现渐进式腐蚀语音信号。</li>
<li>该方法能够在输出降噪帧时实现延迟与性能之间的权衡，延迟与所选缓冲区大小成比例。</li>
<li>相较于传统扩散模型，本文方法在GPU上运行更为高效，达到了0.3至1秒的输入-输出延迟。</li>
<li>本文解决了在线语音增强的实用问题，这是基于扩散模型的首次实现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02908">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8882f669aa2044c5fc3c44e376e63529.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb97edaa41e06a893770a47f5698d153.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d30042e8ae52e373ace858637be097bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f6ea14bcce76eb97c12b355283738b4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Multi-Dialectal-Dataset-for-German-Dialect-ASR-and-Dialect-to-Standard-Speech-Translation"><a href="#A-Multi-Dialectal-Dataset-for-German-Dialect-ASR-and-Dialect-to-Standard-Speech-Translation" class="headerlink" title="A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard   Speech Translation"></a>A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard   Speech Translation</h2><p><strong>Authors:Verena Blaschke, Miriam Winkler, Constantin Förster, Gabriele Wenger-Glemser, Barbara Plank</strong></p>
<p>Although Germany has a diverse landscape of dialects, they are underrepresented in current automatic speech recognition (ASR) research. To enable studies of how robust models are towards dialectal variation, we present Betthupferl, an evaluation dataset containing four hours of read speech in three dialect groups spoken in Southeast Germany (Franconian, Bavarian, Alemannic), and half an hour of Standard German speech. We provide both dialectal and Standard German transcriptions, and analyze the linguistic differences between them. We benchmark several multilingual state-of-the-art ASR models on speech translation into Standard German, and find differences between how much the output resembles the dialectal vs. standardized transcriptions. Qualitative error analyses of the best ASR model reveal that it sometimes normalizes grammatical differences, but often stays closer to the dialectal constructions. </p>
<blockquote>
<p>尽管德国的方言多种多样，但在当前的自动语音识别（ASR）研究中，它们的代表性不足。为了研究模型对方言变体的稳健性，我们推出了Betthupferl评估数据集，其中包含四小时在东南德国（弗兰科尼亚语、巴伐利亚语、阿勒曼尼语）说的三种方言群体的朗读语音，以及半小时的标准德语语音。我们提供了方言和标准德语的转录，并分析了它们之间的语言差异。我们在将语音翻译成标准德语方面对几种多语种的最先进ASR模型进行了基准测试，并发现输出与方言转录与标准化转录之间的相似程度差异。最佳ASR模型的定性误差分析表明，它有时会正规化语法差异，但通常会更接近方言结构。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02894v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了德国方言在自动语音识别（ASR）研究中的代表性不足的问题。为了研究模型对方言变体的鲁棒性，提出了一种评估数据集Betthupferl。该数据集包含四小时东南德国的三种方言（弗兰西斯、拜仁语和阿勒曼尼语）的朗读语音和半小时的标准德语语音。提供了方言和标准德语的转录，并分析了它们之间的语言差异。通过对多种多语种最先进的ASR模型进行基准测试，发现输出与方言转录和标准转录的相似程度存在差异。最佳ASR模型的定性误差分析表明，它有时会规范化语法差异，但通常更接近方言结构。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>德国方言在当前的自动语音识别（ASR）研究中代表性不足。</li>
<li>Betthupferl数据集用于评估模型对德国东南地区三种方言的鲁棒性，并提供了方言和标准德语的转录。</li>
<li>方言与标准德语之间存在语言差异。</li>
<li>多种多语种ASR模型在标准德语语音翻译上的基准测试显示，输出与方言转录的相似程度不同。</li>
<li>最佳ASR模型在处理方言时，有时会规范化语法差异。</li>
<li>ASR模型在处理方言语音时，通常更接近方言结构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02894">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a28bd108bc49662324d7fc4dd47bdd10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dd5e75d0e6b080df222544358463b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-013b8f08937c7b73aaa968b6b9db6b59.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ccef456f63076d0cff8c0fcd5d8f30d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6b4c319380e5ebb4bb08d7606d83284.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multi-task-Learning-with-Active-Learning-for-Arabic-Offensive-Speech-Detection"><a href="#Multi-task-Learning-with-Active-Learning-for-Arabic-Offensive-Speech-Detection" class="headerlink" title="Multi-task Learning with Active Learning for Arabic Offensive Speech   Detection"></a>Multi-task Learning with Active Learning for Arabic Offensive Speech   Detection</h2><p><strong>Authors:Aisha Alansari, Hamzah Luqman</strong></p>
<p>The rapid growth of social media has amplified the spread of offensive, violent, and vulgar speech, which poses serious societal and cybersecurity concerns. Detecting such content in Arabic text is particularly complex due to limited labeled data, dialectal variations, and the language’s inherent complexity. This paper proposes a novel framework that integrates multi-task learning (MTL) with active learning to enhance offensive speech detection in Arabic social media text. By jointly training on two auxiliary tasks, violent and vulgar speech, the model leverages shared representations to improve the detection accuracy of the offensive speech. Our approach dynamically adjusts task weights during training to balance the contribution of each task and optimize performance. To address the scarcity of labeled data, we employ an active learning strategy through several uncertainty sampling techniques to iteratively select the most informative samples for model training. We also introduce weighted emoji handling to better capture semantic cues. Experimental results on the OSACT2022 dataset show that the proposed framework achieves a state-of-the-art macro F1-score of 85.42%, outperforming existing methods while using significantly fewer fine-tuning samples. The findings of this study highlight the potential of integrating MTL with active learning for efficient and accurate offensive language detection in resource-constrained settings. </p>
<blockquote>
<p>社交媒体的快速发展放大了攻击性、暴力和粗鲁言论的传播，这引发了严重的社会和网络安全担忧。由于标记数据有限、方言差异和语言的固有复杂性，在阿拉伯文本中检测此类内容尤为复杂。本文针对阿拉伯社交媒体文本中的攻击性言论检测，提出了一种融合多任务学习（MTL）与主动学习的全新框架。该模型通过两个辅助任务（暴力和粗鲁言语）进行联合训练，利用共享表示来提高攻击性言论检测的准确性。我们的方法能够在训练过程中动态调整任务权重，以平衡每个任务的贡献并优化性能。为了解决标记数据稀缺的问题，我们采用了一种主动学习策略，通过几种不确定性采样技术来迭代选择最有信息量的样本进行模型训练。我们还引入了加权表情符号处理，以更好地捕捉语义线索。在OSACT2022数据集上的实验结果表明，该框架达到了最先进的宏观F1分数85.42%，在使用显著更少的微调样本的情况下，超过了现有方法。本研究的结果突出了在资源受限的环境中，将MTL与主动学习相结合进行高效且准确的攻击性语言检测的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02753v1">PDF</a> </p>
<p><strong>Summary</strong><br>阿拉伯语社交媒体文本中的攻击性言论检测面临诸多挑战，如标注数据有限、方言差异和语言本身的复杂性等。本文提出一种结合多任务学习和主动学习的框架，通过联合训练暴力、粗俗和攻击性言论检测任务，提高模型对阿拉伯语攻击性言论检测的准确性。该框架动态调整任务权重，并采用主动学习策略解决标注数据稀缺问题。实验结果表明，该框架在OSACT2022数据集上取得了最先进的宏观F1分数，且使用较少的微调样本即可实现性能优化。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社会媒体的快速发展加剧了攻击性、暴力和粗鲁言论的传播，引发了社会和网络安全担忧。</li>
<li>阿拉伯语文本中的攻击性言论检测面临标注数据有限、方言差异和语言复杂性等挑战。</li>
<li>本文提出一种结合多任务学习和主动学习的框架，以提高攻击性言论检测的准确性。</li>
<li>该框架通过联合训练暴力、粗俗和攻击性言论检测任务，利用共享表征来提高检测性能。</li>
<li>框架动态调整任务权重，以平衡各个任务的贡献并优化性能。</li>
<li>采用主动学习策略解决标注数据稀缺问题，通过不确定性采样技术选择最具信息量的样本进行模型训练。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02753">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-373cedf6e4661ea34a5ebb69f3a73354.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-214991191c3154f78f636339a00fe591.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3786df12f8fb07ddb6b1f459c3e53be3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-777e27e5c026b2c0adbfdfc2b22bcc10.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DnR-nonverbal-Cinematic-Audio-Source-Separation-Dataset-Containing-Non-Verbal-Sounds"><a href="#DnR-nonverbal-Cinematic-Audio-Source-Separation-Dataset-Containing-Non-Verbal-Sounds" class="headerlink" title="DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing   Non-Verbal Sounds"></a>DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing   Non-Verbal Sounds</h2><p><strong>Authors:Takuya Hasumi, Yusuke Fujita</strong></p>
<p>We propose a new dataset for cinematic audio source separation (CASS) that handles non-verbal sounds. Existing CASS datasets only contain reading-style sounds as a speech stem. These datasets differ from actual movie audio, which is more likely to include acted-out voices. Consequently, models trained on conventional datasets tend to have issues where emotionally heightened voices, such as laughter and screams, are more easily separated as an effect, not speech. To address this problem, we build a new dataset, DnR-nonverbal. The proposed dataset includes non-verbal sounds like laughter and screams in the speech stem. From the experiments, we reveal the issue of non-verbal sound extraction by the current CASS model and show that our dataset can effectively address the issue in the synthetic and actual movie audio. Our dataset is available at <a target="_blank" rel="noopener" href="https://zenodo.org/records/15470640">https://zenodo.org/records/15470640</a>. </p>
<blockquote>
<p>我们针对电影音频源分离（CASS）提出了一个新的数据集，用于处理非言语声音。现有的CASS数据集仅包含阅读风格的语音作为语音主干。这些数据集与实际的电影音频有所不同，后者更可能包含表演性的声音。因此，在常规数据集上训练的模型往往会出现问题，情感高涨的声音（如笑声和尖叫声）更容易被分离为一种效果，而非语音。为了解决这个问题，我们构建了一个新的数据集DnR-nonverbal。该数据集包含了语音主干中的非言语声音，如笑声和尖叫声等。通过实验，我们揭示了当前CASS模型在非言语声音提取方面的问题，并表明我们的数据集可以有效地解决合成和实际电影音频中的问题。我们的数据集可在<a target="_blank" rel="noopener" href="https://zenodo.org/records/15470640%E8%8E%B7%E5%8F%96%E3%80%82">https://zenodo.org/records/15470640获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02499v1">PDF</a> Accepted to Interspeech 2025, 5 pages, 3 figures, dataset is   available at <a target="_blank" rel="noopener" href="https://zenodo.org/records/15470640">https://zenodo.org/records/15470640</a></p>
<p><strong>Summary</strong></p>
<p>提出了一种新的电影音频源分离（CASS）数据集，用于处理非语言声音。现有CASS数据集仅包含阅读式声音作为语音干音，与真实电影音频不同，后者更可能包含表演出的声音。因此，在常规数据集上训练的模型往往在处理情绪高涨的声音（如笑声和尖叫）时存在问题，容易将其分离为效果而非语音。为解决此问题，构建了新的数据集DnR-nonverbal，其中包含了非语言声音（如笑声和尖叫）在语音干音中。实验揭示了当前CASS模型提取非语言声音的问题，并表明该数据集可以在合成和真实电影音频中有效解决此问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有CASS数据集主要关注阅读式声音，与真实电影音频存在差异。</li>
<li>模型在处理情绪高涨的声音（如笑声、尖叫）时存在困难，易误判为效果而非语音。</li>
<li>为解决上述问题，提出了新的数据集DnR-nonverbal。</li>
<li>DnR-nonverbal数据集包含了非语言声音在语音干音中，更接近真实电影音频。</li>
<li>实验表明，新数据集能有效解决非语言声音的提取问题。</li>
<li>该数据集可用于合成和真实电影音频的处理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02499">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9dd2e226b50265de32dec8ecfc09936f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc541b87a835e2a690582d46f4581a58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb12411b02ac22ae5770a36ce8351313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b8b2402a95e6e80f9d24ff4b2eb17df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffe6a8144804e1c29ece79ed2031fee6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SOVA-Bench-Benchmarking-the-Speech-Conversation-Ability-for-LLM-based-Voice-Assistant"><a href="#SOVA-Bench-Benchmarking-the-Speech-Conversation-Ability-for-LLM-based-Voice-Assistant" class="headerlink" title="SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based   Voice Assistant"></a>SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based   Voice Assistant</h2><p><strong>Authors:Yixuan Hou, Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang</strong></p>
<p>Thanks to the steady progress of large language models (LLMs), speech encoding algorithms and vocoder structure, recent advancements have enabled generating speech response directly from a user instruction. However, benchmarking the generated speech quality has been a neglected but critical issue, considering the shift from the pursuit of semantic accuracy to vivid and spontaneous speech flow. Previous evaluation focused on the speech-understanding ability, lacking a quantification of acoustic quality. In this paper, we propose Speech cOnversational Voice Assistant Benchmark (SOVA-Bench), providing a comprehension comparison of the general knowledge, speech recognition and understanding, along with both semantic and acoustic generative ability between available speech LLMs. To the best of our knowledge, SOVA-Bench is one of the most systematic evaluation frameworks for speech LLMs, inspiring the direction of voice interaction systems. </p>
<blockquote>
<p>得益于大型语言模型（LLM）、语音编码算法和vocoder结构的稳步发展，最近的进步已经能够实现直接从用户指令生成语音响应。然而，考虑到从追求语义准确性到生动自然语音流的转变，对生成语音质量的评估是一个被忽视但至关重要的问题。之前的评估主要集中在语音理解能力上，缺乏对声音质量的量化评估。在本文中，我们提出了基于语音对话式语音助手基准（SOVA-Bench）的评估方法，该方法比较了通用知识、语音识别和理解能力，以及现有语音LLM之间的语义和声音生成能力。据我们所知，SOVA-Bench是最系统的语音LLM评估框架之一，为语音交互系统的方向提供了启示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02457v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着大型语言模型（LLMs）、语音编码算法和vocoder结构的稳步发展，现在可以直接根据用户指令生成语音响应。然而，评估生成语音的质量是一个被忽视但至关重要的问题，因为现在的重点已从追求语义准确性转向了生动自然的语音流。本文提出了“语音对话语音助手基准测试”（SOVA-Bench），对通用知识、语音识别与理解能力，以及语义和声音的生成能力进行全面比较评估，为现有的语音LLMs提供系统评估框架。该基准测试不仅为语音LLMs的性能提供了量化的衡量标准，也为语音交互系统的未来发展方向提供了启示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）的进步使得根据用户指令直接生成语音响应成为可能。</li>
<li>评估生成语音的质量是一个被忽视但重要的问题，因为现在的重点已从语义准确性转向生动自然的语音流。</li>
<li>现有的评估主要关注语音理解能力，缺乏声学质量的量化指标。</li>
<li>提出了一种新的评估框架——Speech cOnversational Voice Assistant Benchmark (SOVA-Bench)。</li>
<li>SOVA-Bench提供了对通用知识、语音识别与理解能力，以及语义和声音的生成能力的全面评估。</li>
<li>SOVA-Bench是最系统的语音LLMs评估框架之一。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02457">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-46c6b61df3200edf91e2968d79de13a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdc5ae6e7525ec840cd30853ce34af16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-013d8ff18ea2bbac4d197a160b47ee0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2dfdd2b6ee0d72b961cc4c902b26fa6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba5aa00d6c6beb379223d94fbcb4b62b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="StarVC-A-Unified-Auto-Regressive-Framework-for-Joint-Text-and-Speech-Generation-in-Voice-Conversion"><a href="#StarVC-A-Unified-Auto-Regressive-Framework-for-Joint-Text-and-Speech-Generation-in-Voice-Conversion" class="headerlink" title="StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech   Generation in Voice Conversion"></a>StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech   Generation in Voice Conversion</h2><p><strong>Authors:Fengjin Li, Jie Wang, Yadong Niu, Yongqing Wang, Meng Meng, Jian Luan, Zhiyong Wu</strong></p>
<p>Voice Conversion (VC) modifies speech to match a target speaker while preserving linguistic content. Traditional methods usually extract speaker information directly from speech while neglecting the explicit utilization of linguistic content. Since VC fundamentally involves disentangling speaker identity from linguistic content, leveraging structured semantic features could enhance conversion performance. However, previous attempts to incorporate semantic features into VC have shown limited effectiveness, motivating the integration of explicit text modeling. We propose StarVC, a unified autoregressive VC framework that first predicts text tokens before synthesizing acoustic features. The experiments demonstrate that StarVC outperforms conventional VC methods in preserving both linguistic content (i.e., WER and CER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found at: <a target="_blank" rel="noopener" href="https://thuhcsi.github.io/StarVC/">https://thuhcsi.github.io/StarVC/</a>. </p>
<blockquote>
<p>语音转换（VC）会修改语音以匹配目标说话者，同时保留语言内容。传统方法通常直接从语音中提取说话者信息，而忽视对语言内容的明确利用。由于VC从根本上涉及将说话者身份与语言内容分开，利用结构化的语义特征可能会提高转换性能。然而，之前将语义特征融入VC的尝试显示出了有限的有效性，这激发了显式文本建模的集成。我们提出StarVC，这是一个统一的自回归VC框架，它首先在合成声学特征之前预测文本标记。实验表明，StarVC在保留语言内容（即WER和CER）和说话者特征（即SECS和MOS）方面优于传统VC方法。音频演示可在：<a target="_blank" rel="noopener" href="https://thuhcsi.github.io/StarVC/%E6%89%BE%E5%88%B0%E3%80%82">https://thuhcsi.github.io/StarVC/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02414v1">PDF</a> 5 pages, 2 figures, Accepted by Interspeech 2025, Demo:   <a target="_blank" rel="noopener" href="https://thuhcsi.github.io/StarVC/">https://thuhcsi.github.io/StarVC/</a></p>
<p><strong>总结</strong></p>
<p>语音转换（VC）旨在匹配目标说话者的语音，同时保留语言内容。传统方法通常直接从语音中提取说话者信息，忽略了语言内容的明确利用。由于VC从根本上涉及将说话者身份与语言内容分开，利用结构化的语义特征可能会提高转换性能。然而，之前尝试将语义特征融入VC显示出了有限的效果，这促使人们结合明确的文本建模。我们提出StarVC，一个统一的自回归VC框架，首先预测文本标记，然后合成声学特征。实验表明，StarVC在保留语言内容（即WER和CER）和说话者特征（即SECS和MOS）方面优于传统VC方法。音频演示可在：<a target="_blank" rel="noopener" href="https://thuhcsi.github.io/StarVC/%E6%89%BE%E5%88%B0%E3%80%82">https://thuhcsi.github.io/StarVC/找到。</a></p>
<p><strong>要点</strong></p>
<ol>
<li>语音转换（VC）旨在匹配目标说话者并保留语言内容。</li>
<li>传统VC方法通常忽略语言内容的明确利用，只关注提取说话者信息。</li>
<li>VC涉及分离说话者身份和语言内容，可以利用结构化的语义特征来提高转换性能。</li>
<li>之前尝试融入语义特征的VC效果有限。</li>
<li>提出了一种新的VC框架StarVC，结合文本建模和自回归方法。</li>
<li>StarVC在保留语言内容和说话者特征方面优于传统方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02414">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f9e7c957b7e7ccdd190dbecf48b9e553.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70ef99009ea946243bc2e07778d9c7ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f304231d585f214a49a7717e5f2b9941.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb81277fd96e6da7261951e33e5ba69d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac39fb5978b6106384af0314f27bfae2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Are-Mamba-based-Audio-Foundation-Models-the-Best-Fit-for-Non-Verbal-Emotion-Recognition"><a href="#Are-Mamba-based-Audio-Foundation-Models-the-Best-Fit-for-Non-Verbal-Emotion-Recognition" class="headerlink" title="Are Mamba-based Audio Foundation Models the Best Fit for Non-Verbal   Emotion Recognition?"></a>Are Mamba-based Audio Foundation Models the Best Fit for Non-Verbal   Emotion Recognition?</h2><p><strong>Authors:Mohd Mujtaba Akhtar, Orchid Chetia Phukan,  Girish, Swarup Ranjan Behera, Ananda Chandra Nayak, Sanjib Kumar Nayak, Arun Balaji Buduru, Rajesh Sharma</strong></p>
<p>In this work, we focus on non-verbal vocal sounds emotion recognition (NVER). We investigate mamba-based audio foundation models (MAFMs) for the first time for NVER and hypothesize that MAFMs will outperform attention-based audio foundation models (AAFMs) for NVER by leveraging its state-space modeling to capture intrinsic emotional structures more effectively. Unlike AAFMs, which may amplify irrelevant patterns due to their attention mechanisms, MAFMs will extract more stable and context-aware representations, enabling better differentiation of subtle non-verbal emotional cues. Our experiments with state-of-the-art (SOTA) AAFMs and MAFMs validates our hypothesis. Further, motivated from related research such as speech emotion recognition, synthetic speech detection, where fusion of foundation models (FMs) have showed improved performance, we also explore fusion of FMs for NVER. To this end, we propose, RENO, that uses renyi-divergence as a novel loss function for effective alignment of the FMs. It also makes use of self-attention for better intra-representation interaction of the FMs. With RENO, through the heterogeneous fusion of MAFMs and AAFMs, we show the topmost performance in comparison to individual FMs, its fusion and also setting SOTA in comparison to previous SOTA work. </p>
<blockquote>
<p>在这项工作中，我们专注于非语言声音情感识别（NVER）。我们首次研究基于mamba的音频基础模型（MAFMs）在NVER中的应用，并假设MAFMs在利用状态空间建模更有效地捕捉内在情感结构方面，将在非语言声音情感识别上优于基于注意力的音频基础模型（AAFMs）。与可能因注意力机制而放大无关模式的AAFMs不同，MAFMs将提取更稳定和上下文感知的表示，从而更好地区分微妙的非语言情感线索。我们利用最先进（SOTA）的AAFMs和MAFMs进行的实验验证了我们假设的正确性。此外，我们从语音情感识别、合成语音检测等相关研究中获得启发，在这些研究中，基础模型的融合（FMs）已经显示出性能改进，我们也探索了NVRE中FMs的融合。为此，我们提出了RENO，它使用renyi散度作为对齐FMs的新型损失函数，并借助自我注意力实现FMs的更好内部表示交互。通过MAFMs和AAFMs的异质融合，RENO展现出最佳性能，不仅优于单个FMs及其融合，而且相较于之前的最先进方法也达到了新的SOTA水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02258v1">PDF</a> Accepted to EUSIPCO 2025</p>
<p><strong>Summary</strong></p>
<p>本文专注于非言语性声音情感识别（NVER），首次研究基于mamba的音频基础模型（MAFMs）在NVER中的应用。研究表明，MAFMs利用状态空间建模更有效地捕捉内在情感结构，在NVER中表现优于基于注意力的音频基础模型（AAFMs）。实验验证了该假设的正确性。此外，受到融合基础模型在语音情感识别、合成语音检测等领域表现提升的研究启发，本文也探索了融合基础模型在NVER中的应用。为此，提出了使用renyi-divergence作为损失函数进行融合模型有效对齐的新方法RENO，并借助自我注意力机制增强模型内部表示的交互。通过融合MAFMs和AAFMs的异质融合，RENO在对比单个FM、其融合方法以及先前最佳工作方面均表现出顶尖性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究关注非言语性声音情感识别（NVER）。</li>
<li>首次探究mamba-based音频基础模型（MAFMs）在NVER中的应用。</li>
<li>MAFMs通过状态空间建模更有效地捕捉内在情感结构。</li>
<li>基于注意力的音频基础模型（AAFMs）可能会因注意力机制而放大无关模式。</li>
<li>MAFMs提取的稳定和上下文感知的表示有助于更好地区分微妙的非言语情感线索。</li>
<li>融合基础模型（FMs）在语音情感识别等领域已显示出性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-afd9cc3763fc9261edd79652e3730654.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7157c8a62c3f118454ea7f81de52c24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79a89326584e7e92ffebb3c1cf8c04ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f8f72d70c838531d479f149019419d68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79f43f89077bf1658f7dfd9cb6a43c34.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PAIR-Net-Enhancing-Egocentric-Speaker-Detection-via-Pretrained-Audio-Visual-Fusion-and-Alignment-Loss"><a href="#PAIR-Net-Enhancing-Egocentric-Speaker-Detection-via-Pretrained-Audio-Visual-Fusion-and-Alignment-Loss" class="headerlink" title="PAIR-Net: Enhancing Egocentric Speaker Detection via Pretrained   Audio-Visual Fusion and Alignment Loss"></a>PAIR-Net: Enhancing Egocentric Speaker Detection via Pretrained   Audio-Visual Fusion and Alignment Loss</h2><p><strong>Authors:Yu Wang, Juhyung Ha, David J. Crandall</strong></p>
<p>Active speaker detection (ASD) in egocentric videos presents unique challenges due to unstable viewpoints, motion blur, and off-screen speech sources - conditions under which traditional visual-centric methods degrade significantly. We introduce PAIR-Net (Pretrained Audio-Visual Integration with Regularization Network), an effective model that integrates a partially frozen Whisper audio encoder with a fine-tuned AV-HuBERT visual backbone to robustly fuse cross-modal cues. To counteract modality imbalance, we introduce an inter-modal alignment loss that synchronizes audio and visual representations, enabling more consistent convergence across modalities. Without relying on multi-speaker context or ideal frontal views, PAIR-Net achieves state-of-the-art performance on the Ego4D ASD benchmark with 76.6% mAP, surpassing LoCoNet and STHG by 8.2% and 12.9% mAP, respectively. Our results highlight the value of pretrained audio priors and alignment-based fusion for robust ASD under real-world egocentric conditions. </p>
<blockquote>
<p>在自我中心的视频中，主动说话人检测（ASD）面临着由于观点不稳定、运动模糊和屏幕外语音源等独特挑战——在这些条件下，传统的视觉中心方法会显著退化。我们引入了PAIR-Net（具有正则化的预训练视听集成网络），这是一个有效的模型，它将部分冻结的Whisper音频编码器与经过微调AV-HuBERT视觉主干相结合，以稳健地融合跨模态线索。为了对抗模态不平衡，我们引入了一种跨模态对齐损失，以同步音频和视觉表示，从而实现跨模态的更一致收敛。在不依赖多说话人上下文或理想正面视角的情况下，PAIR-Net在Ego4D ASD基准测试上达到了最先进的性能，mAP达到76.6%，分别比LoCoNet和STHG高出8.2%和12.9% mAP。我们的结果突出了在真实世界的自我中心条件下，预训练的音频先验和对齐融合的价值，对于稳健的ASD至关重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02247v1">PDF</a> 4 pages, 1 figure, and 1 table</p>
<p><strong>Summary</strong></p>
<p>预训练音频优先与正则化网络（PAIR-Net）能有效解决主动说话人检测（ASD）在自我中心视频中的难题，它整合了部分冻结的Whisper音频编码器与微调后的AV-HuBERT视觉主干，稳健地融合了跨模态线索。通过引入跨模态对齐损失来对抗模态不平衡问题，使音频和视觉表征同步，实现跨模态的一致收敛。在不依赖多说话人上下文或理想正面视角的情况下，PAIR-Net在Ego4D ASD基准测试中达到76.6%的mAP，超越了LoCoNet和STHG分别达8.2%和12.9%的mAP。结果凸显了预训练音频先验和对齐融合在真实世界自我中心条件下的稳健ASD的价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PAIR-Net解决了主动说话人检测在自我中心视频中的挑战。</li>
<li>PAIR-Net整合了音频和视觉信息，通过部分冻结的Whisper音频编码器与AV-HuBERT视觉主干实现跨模态融合。</li>
<li>引入跨模态对齐损失以对抗模态不平衡问题，实现音频和视觉表征的同步。</li>
<li>PAIR-Net在Ego4D ASD基准测试中表现优异，达到76.6%的mAP。</li>
<li>PAIR-Net性能超越了LoCoNet和STHG。</li>
<li>预训练音频先验对于提高ASD性能至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02247">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f8efb2033b3aa5071f54bb447d92762.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71d628bd3cdb8002645d67afa55c614a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81672903496f3e0cf5789152f84748f6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Investigating-the-Reasonable-Effectiveness-of-Speaker-Pre-Trained-Models-and-their-Synergistic-Power-for-SingMOS-Prediction"><a href="#Investigating-the-Reasonable-Effectiveness-of-Speaker-Pre-Trained-Models-and-their-Synergistic-Power-for-SingMOS-Prediction" class="headerlink" title="Investigating the Reasonable Effectiveness of Speaker Pre-Trained Models   and their Synergistic Power for SingMOS Prediction"></a>Investigating the Reasonable Effectiveness of Speaker Pre-Trained Models   and their Synergistic Power for SingMOS Prediction</h2><p><strong>Authors:Orchid Chetia Phukan,  Girish, Mohd Mujtaba Akhtar, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma</strong></p>
<p>In this study, we focus on Singing Voice Mean Opinion Score (SingMOS) prediction. Previous research have shown the performance benefit with the use of state-of-the-art (SOTA) pre-trained models (PTMs). However, they haven’t explored speaker recognition speech PTMs (SPTMs) such as x-vector, ECAPA and we hypothesize that it will be the most effective for SingMOS prediction. We believe that due to their speaker recognition pre-training, it equips them to capture fine-grained vocal features (e.g., pitch, tone, intensity) from synthesized singing voices in a much more better way than other PTMs. Our experiments with SOTA PTMs including SPTMs and music PTMs validates the hypothesis. Additionally, we introduce a novel fusion framework, BATCH that uses Bhattacharya Distance for fusion of PTMs. Through BATCH with the fusion of speaker recognition SPTMs, we report the topmost performance comparison to all the individual PTMs and baseline fusion techniques as well as setting SOTA. </p>
<blockquote>
<p>在这项研究中，我们专注于歌唱声音平均意见得分（SingMOS）的预测。先前的研究已经显示了使用最新预训练模型（PTMs）的性能优势。然而，他们尚未探索用于语音识别的预训练模型（SPTMs），例如x向量、ECAPA，我们假设这对于SingMOS预测将是最有效的。我们相信，由于它们具有语音识别的预训练能力，因此能够比其他PTM更好地捕获合成歌声的精细语音特征（例如音调、音色和强度）。我们对包括SPTM和音乐PTM的最新PTM进行的实验验证了这一假设。此外，我们引入了一种新的融合框架BATCH，它使用Bhattacharya距离来融合PTM。通过BATCH融合语音识别的SPTMs，我们报告了与所有单独的PTM和基准融合技术的最高性能比较，并建立了新的SOTA（最佳水平）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02232v1">PDF</a> Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>此研究关注歌唱声音平均意见得分（SingMOS）预测。研究使用最先进的预训练模型（PTMs），并探索语音预训练模型（SPTMs）如x-vector、ECAPA等，假设其在SingMOS预测中最有效。实验验证假设，认为SPTMs能捕捉合成歌声的精细语音特征（如音调、音色、强度），并使用创新的融合框架BATCH结合PTMs进行验证，相较于各独立PTMs与基准融合技术，表现最佳。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究聚焦于SingMOS预测。</li>
<li>最先进的预训练模型在研究中得到应用。</li>
<li>说话人识别语音预训练模型（SPTMs）的探索及其在SingMOS预测中的假设有效性。</li>
<li>SPTMs能够捕捉合成歌声的精细语音特征。</li>
<li>引入新型融合框架BATCH，采用Bhattacharya距离进行预训练模型的融合。</li>
<li>BATCH融合说话人识别SPTMs表现超越所有独立PTMs和基准融合技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02232">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-305a616962496916e12c3a6f20f9dbec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-458defda1c5b24e95a08bbc633786efe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0f494aac873de5b4f1cd4a202efb2d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4db0fc9f98b56c77cd5dd5566215cbe.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Machine-Unlearning-for-Paralinguistic-Speech-Processing"><a href="#Towards-Machine-Unlearning-for-Paralinguistic-Speech-Processing" class="headerlink" title="Towards Machine Unlearning for Paralinguistic Speech Processing"></a>Towards Machine Unlearning for Paralinguistic Speech Processing</h2><p><strong>Authors:Orchid Chetia Phukan,  Girish, Mohd Mujtaba Akhtar, Shubham Singh, Swarup Ranjan Behera, Vandana Rajan, Muskaan Singh, Arun Balaji Buduru, Rajesh Sharma</strong></p>
<p>In this work, we pioneer the study of Machine Unlearning (MU) for Paralinguistic Speech Processing (PSP). We focus on two key PSP tasks: Speech Emotion Recognition (SER) and Depression Detection (DD). To this end, we propose, SISA++, a novel extension to previous state-of-the-art (SOTA) MU method, SISA by merging models trained on different shards with weight-averaging. With such modifications, we show that SISA++ preserves performance more in comparison to SISA after unlearning in benchmark SER (CREMA-D) and DD (E-DAIC) datasets. Also, to guide future research for easier adoption of MU for PSP, we present &#96;&#96;cookbook recipes’’ - actionable recommendations for selecting optimal feature representations and downstream architectures that can mitigate performance degradation after the unlearning process. </p>
<blockquote>
<p>在这项工作中，我们首创了针对副语言语音处理（PSP）的机器遗忘学习（MU）研究。我们重点关注两个关键的PSP任务：语音情绪识别（SER）和抑郁检测（DD）。为此，我们提出了SISA++，这是通过对先前最先进的MU方法SISA进行改进而得到的，通过合并不同分片训练的模型并加权平均权重。经过这些修改，我们在基准SER（CREMA-D）和DD（E-DAIC）数据集上展示了与SISA相比，SISA++在遗忘学习后的性能表现更佳。此外，为了引导未来的研究更轻松地采用MU用于PSP，我们提供了“食谱式指南”——可操作建议来选择最优的特征表示和下游架构，这样可以缓解遗忘学习过程后的性能下降问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02230v1">PDF</a> Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong><br>本研究开创性地探索了用于副语言处理（PSP）的机器遗忘（MU）。聚焦于两大关键任务：语音情绪识别（SER）和抑郁检测（DD）。提出了一项新的改进方法SISA++，它通过合并不同分片训练的模型并使用权重平均技术，相较于现有最先进的SISA方法，能够在基准测试数据集SER（CREMA-D）和DD（E-DAIC）中保持更好的性能。此外，为了指导未来研究更轻松地采用MU进行PSP研究，我们提供了实用的“食谱”建议，推荐选择最优的特征表示和下游架构，以减少遗忘过程后的性能下降。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关键要点摘要：</p>
<ol>
<li>研究首次探索了机器遗忘（MU）在副语言处理（PSP）中的应用。</li>
<li>研究聚焦于两大关键任务：语音情绪识别（SER）和抑郁检测（DD）。</li>
<li>提出了一种新的改进方法SISA++，通过合并不同分片训练的模型并使用权重平均技术，改进现有技术的性能。</li>
<li>在基准测试数据集SER和DD中的性能表现优于现有技术。</li>
<li>提供实用指南（“食谱”），建议如何选择和优化特征表示和下游架构，以减轻遗忘过程后的性能下降。</li>
<li>本研究为未来MU在PSP领域的研究提供了有价值的参考和指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02230">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-afd97196d049a9d31a6bf180537b35f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94414cf4cc04685eb721f6ce016c2fe4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a88ec6542543b1ae374ffbebb0194421.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e5304b3dc545dd6ccded7cb2af6a5be.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Cocktail-Party-Audio-Visual-Speech-Recognition"><a href="#Cocktail-Party-Audio-Visual-Speech-Recognition" class="headerlink" title="Cocktail-Party Audio-Visual Speech Recognition"></a>Cocktail-Party Audio-Visual Speech Recognition</h2><p><strong>Authors:Thai-Binh Nguyen, Ngoc-Quan Pham, Alexander Waibel</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech recognition in challenging environments, such as cocktail-party scenarios, where relying solely on audio proves insufficient. However, current AVSR models are often optimized for idealized scenarios with consistently active speakers, overlooking the complexities of real-world settings that include both speaking and silent facial segments. This study addresses this gap by introducing a novel audio-visual cocktail-party dataset designed to benchmark current AVSR systems and highlight the limitations of prior approaches in realistic noisy conditions. Additionally, we contribute a 1526-hour AVSR dataset comprising both talking-face and silent-face segments, enabling significant performance gains in cocktail-party environments. Our approach reduces WER by 67% relative to the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise, without relying on explicit segmentation cues. </p>
<blockquote>
<p>视听语音识别（AVSR）为在具有挑战性的环境中进行语音识别提供了稳健的解决方案，如在鸡尾酒会场景中，仅凭音频信号是不足够的。然而，当前的AVSR模型大多针对始终活跃发言的理想化场景进行优化，忽略了真实世界中包括说话和无声面部片段的复杂性。本研究通过引入一个新的视听鸡尾酒会数据集来解决这一差距，该数据集旨在评估当前的AVSR系统并突出先前方法在真实嘈杂条件下的局限性。此外，我们贡献了一个包含说话面部和无声面部片段的1526小时AVSR数据集，能够在鸡尾酒会环境中实现显著的性能提升。我们的方法相较于最新技术减少了67%的词错误率（WER），在极端噪声下将WER从119%减少到39.2%，且无需依赖明确的分段线索。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02178v1">PDF</a> Accepted at Interspeech 2025</p>
<p><strong>Summary</strong>：视听语音识别（AVSR）在挑战环境中如鸡尾酒会场景等提供了稳健的语音识别解决方案，单纯依赖音频无法应对。当前AVSR模型通常针对理想化场景优化，忽略了包含说话和静默面部片段的复杂现实环境。本研究通过引入新颖的视听鸡尾酒会数据集来弥补这一差距，该数据集旨在评估当前AVSR系统并突出其在现实噪声条件下的局限性。此外，包含说话面孔和静默面孔片段的1526小时AVSR数据集能显著提高在鸡尾酒会环境中的性能。相较于最新技术，该方法在极端噪声下将字错误率（WER）降低了67%，从119%降至39.2%，无需依赖明确的分段线索。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>AVSR在挑战环境中如鸡尾酒会场景提供稳健语音识别解决方案。</li>
<li>当前AVSR模型主要关注理想化场景，忽略现实环境的复杂性。</li>
<li>引入的视听鸡尾酒会数据集旨在评估AVSR系统并突出其在现实噪声条件下的局限性。</li>
<li>提出的1526小时AVSR数据集包含说话和静默面部片段，能显著提高在鸡尾酒会环境中的性能。</li>
<li>与最新技术相比，该研究在极端噪声条件下显著降低了字错误率（WER）。</li>
<li>该方法性能提升不依赖于明确的分段线索。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cc36607e05a51d713f9c511e0e15d5bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e35b10a36d4c149c69be27a29f7472c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b356339adb5dff1f07b448cb12ffeb2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42bc00708561bcf29b7b5fcc585327c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0baa2356fe5fba1ac7f0549aeae2e0f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c28a3a4238927832867d7e165cd3704.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="HENT-SRT-Hierarchical-Efficient-Neural-Transducer-with-Self-Distillation-for-Joint-Speech-Recognition-and-Translation"><a href="#HENT-SRT-Hierarchical-Efficient-Neural-Transducer-with-Self-Distillation-for-Joint-Speech-Recognition-and-Translation" class="headerlink" title="HENT-SRT: Hierarchical Efficient Neural Transducer with   Self-Distillation for Joint Speech Recognition and Translation"></a>HENT-SRT: Hierarchical Efficient Neural Transducer with   Self-Distillation for Joint Speech Recognition and Translation</h2><p><strong>Authors:Amir Hussein, Cihan Xiao, Matthew Wiesner, Dan Povey, Leibny Paola Garcia, Sanjeev Khudanpur</strong></p>
<p>Neural transducers (NT) provide an effective framework for speech streaming, demonstrating strong performance in automatic speech recognition (ASR). However, the application of NT to speech translation (ST) remains challenging, as existing approaches struggle with word reordering and performance degradation when jointly modeling ASR and ST, resulting in a gap with attention-based encoder-decoder (AED) models. Existing NT-based ST approaches also suffer from high computational training costs. To address these issues, we propose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech Recognition and Translation), a novel framework that factorizes ASR and translation tasks to better handle reordering. To ensure robust ST while preserving ASR performance, we use self-distillation with CTC consistency regularization. Moreover, we improve computational efficiency by incorporating best practices from ASR transducers, including a down-sampled hierarchical encoder, a stateless predictor, and a pruned transducer loss to reduce training complexity. Finally, we introduce a blank penalty during decoding, reducing deletions and improving translation quality. Our approach is evaluated on three conversational datasets Arabic, Spanish, and Mandarin achieving new state-of-the-art performance among NT models and substantially narrowing the gap with AED-based systems. </p>
<blockquote>
<p>神经网络转换器（NT）为语音流提供了有效的框架，在自动语音识别（ASR）方面表现出强大的性能。然而，将NT应用于语音翻译（ST）仍然具有挑战性。现有方法在处理联合ASR和ST时的词语重新排序和性能下降问题上遇到了困难，与基于注意力的编码器-解码器（AED）模型之间存在差距。此外，现有的基于NT的ST方法还面临高计算训练成本的问题。为了解决这些问题，我们提出了HENT-SRT（用于语音识别和翻译的分层高效神经网络转换器）这一新型框架，通过分解ASR和翻译任务来更好地处理重新排序问题。为确保在保留ASR性能的同时实现稳健的ST，我们采用带有CTC一致性正则化的自我蒸馏法。而且，我们从ASR转换器中汲取最佳实践，纳入分层编码器下的采样、无状态预测器和修剪的转换器损失，以提高计算效率并降低训练复杂度。最后，我们在解码过程中引入了空白惩罚，减少了删除操作，提高了翻译质量。我们的方法在三组对话数据集（阿拉伯语、西班牙语和普通话）上进行了评估，实现了NT模型中的最新先进性能，并大大缩小了与AED系统的差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>神经网络转换器（NT）在语音识别（ASR）中表现出强大的性能，为语音流提供了有效的框架。然而，将NT应用于语音识别（ST）仍然具有挑战性，现有方法面临单词重新排序和联合建模ASR和ST时的性能下降问题，与基于注意力的编码器-解码器（AED）模型之间存在差距。针对这些问题，我们提出了HENT-SRT（用于语音识别和翻译的分层高效神经网络转换器）这一新型框架，通过分解ASR和翻译任务以更好地处理重新排序问题。为确保在保持ASR性能的同时实现稳健的语音识别，我们采用自我蒸馏和CTC一致性正则化。此外，通过融入ASR转换器的最佳实践，我们提高了计算效率，包括使用下采样分层编码器、无状态预测器和修剪转换器损失以降低训练复杂度。最后，我们在解码过程中引入了空白惩罚，减少了删除并提高了翻译质量。在阿拉伯语、西班牙语和普通话的三组对话数据集上的评估结果表明，我们的方法达到了NT模型的新先进水平，并大幅缩小了与AED系统的差距。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络转换器（NT）在自动语音识别（ASR）中表现良好，但应用于语音识别（ST）时面临挑战。</li>
<li>现有NT模型在联合建模ASR和ST时存在单词重新排序和性能下降问题。</li>
<li>HENT-SRT框架旨在通过分解ASR和翻译任务来解决这些问题。</li>
<li>通过自我蒸馏和CTC一致性正则化确保稳健的ST同时保持ASR性能。</li>
<li>引入下采样分层编码器、无状态预测器和修剪转换器损失提高计算效率。</li>
<li>在解码过程中引入空白惩罚以减少删除并改善翻译质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02157">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-deb2e78aee0d9a69c7d57052e693ad75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10372409c17b5888b71192554d353b9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33ff071a0edef6a284d0b6957f3d2bea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45ade06ea1b1ff4b6370509eb8e347fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6e284762a9481b80ac99805176d9502.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Enhancing-Speech-Emotion-Recognition-with-Graph-Based-Multimodal-Fusion-and-Prosodic-Features-for-the-Speech-Emotion-Recognition-in-Naturalistic-Conditions-Challenge-at-Interspeech-2025"><a href="#Enhancing-Speech-Emotion-Recognition-with-Graph-Based-Multimodal-Fusion-and-Prosodic-Features-for-the-Speech-Emotion-Recognition-in-Naturalistic-Conditions-Challenge-at-Interspeech-2025" class="headerlink" title="Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion   and Prosodic Features for the Speech Emotion Recognition in Naturalistic   Conditions Challenge at Interspeech 2025"></a>Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion   and Prosodic Features for the Speech Emotion Recognition in Naturalistic   Conditions Challenge at Interspeech 2025</h2><p><strong>Authors:Alef Iury Siqueira Ferreira, Lucas Rafael Gris, Alexandre Ferro Filho, Lucas Ólives, Daniel Ribeiro, Luiz Fernando, Fernanda Lustosa, Rodrigo Tanaka, Frederico Santos de Oliveira, Arlindo Galvão Filho</strong></p>
<p>Training SER models in natural, spontaneous speech is especially challenging due to the subtle expression of emotions and the unpredictable nature of real-world audio. In this paper, we present a robust system for the INTERSPEECH 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing on categorical emotion recognition. Our method combines state-of-the-art audio models with text features enriched by prosodic and spectral cues. In particular, we investigate the effectiveness of Fundamental Frequency (F0) quantization and the use of a pretrained audio tagging model. We also employ an ensemble model to improve robustness. On the official test set, our system achieved a Macro F1-score of 39.79% (42.20% on validation). Our results underscore the potential of these methods, and analysis of fusion techniques confirmed the effectiveness of Graph Attention Networks. Our source code is publicly available. </p>
<blockquote>
<p>在自然、自发的语音中训练SER模型尤其具有挑战性，因为情感的表达很细微，而且现实世界的音频具有不可预测性。本文旨在为INTERSPEECH 2025自然条件下语音情感识别挑战提供一套稳健的系统，专注于分类情感识别。我们的方法结合了最先进的音频模型与文本特征，这些文本特征由韵律和光谱线索丰富。我们特别研究了基频（F0）量化的有效性以及使用预训练的音频标记模型。我们还采用集成模型来提高稳健性。在官方测试集上，我们的系统达到了39.79%的宏观F1分数（验证集上为42.20%）。我们的结果突出了这些方法的潜力，融合技术的分析证实了图注意力网络的有效性。我们的源代码已公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02088v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对自然、自发语音中的情感识别，训练SER模型具有挑战性。本文提出一种稳健系统，结合最新音频模型与文本特征，通过音高和频谱线索增强识别能力。研究重点放在基本频率量化的有效性上，并使用预训练的音频标签模型。通过集成模型提高稳健性，在官方测试集上达到39.79%的宏观F1分数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>训练SER模型在自然、自发语音中的情感识别具有挑战性。</li>
<li>提出一种结合最新音频模型和文本特征的稳健系统。</li>
<li>通过音高和频谱线索增强识别能力。</li>
<li>研究重点放在基本频率量化的有效性上。</li>
<li>使用预训练的音频标签模型提高性能。</li>
<li>集成模型用于提高稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02088">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-86f4f0dce32ff35f6f90ce8210c1fb89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bebc81720ee503f384f5447cf7239db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6a5aaad83b5708136d15f171d8f0a83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fe08db47c0d725c7ad94120d331df55.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Enhancing-GOP-in-CTC-Based-Mispronunciation-Detection-with-Phonological-Knowledge"><a href="#Enhancing-GOP-in-CTC-Based-Mispronunciation-Detection-with-Phonological-Knowledge" class="headerlink" title="Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological   Knowledge"></a>Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological   Knowledge</h2><p><strong>Authors:Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik</strong></p>
<p>Computer-Assisted Pronunciation Training (CAPT) systems employ automatic measures of pronunciation quality, such as the goodness of pronunciation (GOP) metric. GOP relies on forced alignments, which are prone to labeling and segmentation errors due to acoustic variability. While alignment-free methods address these challenges, they are computationally expensive and scale poorly with phoneme sequence length and inventory size. To enhance efficiency, we introduce a substitution-aware alignment-free GOP that restricts phoneme substitutions based on phoneme clusters and common learner errors. We evaluated our GOP on two L2 English speech datasets, one with child speech, My Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult speech. We compared RPS (restricted phoneme substitutions) and UPS (unrestricted phoneme substitutions) setups within alignment-free methods, which outperformed the baseline. We discuss our results and outline avenues for future research. </p>
<blockquote>
<p>计算机辅助发音训练（CAPT）系统采用发音质量自动测量法，如发音质量（GOP）指标。GOP依赖于强制对齐，由于声学变化，它容易产生标签和分段错误。虽然无对齐方法可以解决这些挑战，但它们计算量大，随着音素序列长度和库存规模的扩大，表现不佳。为了提高效率，我们引入了一种基于音素聚类和常见学习者错误的替代感知无对齐GOP，该GOP限制了音素替代。我们在两个英语二级语音数据集上评估了我们的GOP，一个是儿童语音数据集My Pronunciation Coach（MPC），另一个是包含儿童和成人语音的SpeechOcean762数据集。我们在无对齐方法中比较了限制音素替代（RPS）和非限制音素替代（UPS）的设置，它们的表现都超过了基线水平。我们讨论了我们的结果并概述了未来研究的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02080v1">PDF</a> Accepted to Interspeech 2025. This publication is part of the project   Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013   of the research programme NGF AiNed Fellowship Grants which is financed by   the Dutch Research Council (NWO)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了计算机辅助发音训练（CAPT）系统采用基于强制对齐的发音质量评估方法，如发音质量（GOP）指标。但由于声学变化导致的标签和分段误差，强制对齐方法存在局限性。研究团队引入了一种基于替代的无对齐GOP方法，通过基于音素集群和常见学习者错误限制音素替代来提高效率。该方法在包含儿童语音的My Pronunciation Coach数据集和包含儿童和成人语音的SpeechOcean762数据集上进行了评估，显示具有限制音素替代（RPS）的方法优于无限制音素替代（UPS）的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>计算机辅助发音训练系统使用自动发音质量评估方法，如基于强制对齐的发音质量（GOP）指标。</li>
<li>强制对齐方法存在声学变化引起的标签和分段误差问题。</li>
<li>无对齐的GOP方法引入了一种基于替代的策略，通过限制音素替代来提高效率。这种策略基于音素集群和常见学习者错误。</li>
<li>研究在My Pronunciation Coach和SpeechOcean762数据集上评估了该方法。</li>
<li>限制音素替代的方法在评估中表现优于无限制的方法。</li>
<li>该研究的结果对于改进计算机辅助发音训练系统的性能具有积极意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9efe9aa0e844404f1f5d9e0b996b78b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0278710b096181ae8a977a1221bc690.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61487aebe0745072f62d26d47cd9f515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6d903361d84b9833c9d7d74212db32c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DNCASR-End-to-End-Training-for-Speaker-Attributed-ASR"><a href="#DNCASR-End-to-End-Training-for-Speaker-Attributed-ASR" class="headerlink" title="DNCASR: End-to-End Training for Speaker-Attributed ASR"></a>DNCASR: End-to-End Training for Speaker-Attributed ASR</h2><p><strong>Authors:Xianrui Zheng, Chao Zhang, Philip C. Woodland</strong></p>
<p>This paper introduces DNCASR, a novel end-to-end trainable system designed for joint neural speaker clustering and automatic speech recognition (ASR), enabling speaker-attributed transcription of long multi-party meetings. DNCASR uses two separate encoders to independently encode global speaker characteristics and local waveform information, along with two linked decoders to generate speaker-attributed transcriptions. The use of linked decoders allows the entire system to be jointly trained under a unified loss function. By employing a serialised training approach, DNCASR effectively addresses overlapping speech in real-world meetings, where the link improves the prediction of speaker indices in overlapping segments. Experiments on the AMI-MDM meeting corpus demonstrate that the jointly trained DNCASR outperforms a parallel system that does not have links between the speaker and ASR decoders. Using cpWER to measure the speaker-attributed word error rate, DNCASR achieves a 9.0% relative reduction on the AMI-MDM Eval set. </p>
<blockquote>
<p>本文介绍了DNCASR，这是一种新型端到端可训练系统，旨在实现联合神经网络说话人聚类和自动语音识别（ASR），从而实现长多方会议的说话人属性转录。DNCASR使用两个独立的编码器分别编码全局说话人特性和局部波形信息，以及两个链接的解码器生成说话人属性转录。使用链接的解码器可以使整个系统在统一的损失函数下进行联合训练。通过采用序列化训练方法，DNCASR有效地解决了现实会议中的重叠语音问题，其中链接改善了重叠段落中说话人指数的预测。在AMI-MDM会议语料库上的实验表明，联合训练的DNCASR优于没有说话人和ASR解码器之间链接的并行系统。使用cpWER来衡量说话人属性词错误率，DNCASR在AMI-MDM评估集上实现了9.0%的相对降低。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01916v1">PDF</a> Accepted by ACL 2025 Main Conference</p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了一种名为DNCASR的新型端到端训练系统，它联合进行神经说话人聚类与自动语音识别（ASR），可实现长多方会议的说话人属性转录。DNCASR采用两个独立编码器分别编码全局说话人特性和局部波形信息，以及两个关联解码器生成说话人属性转录。通过采用序列化训练方式，DNCASR有效解决了现实会议中的重叠语音问题，关联解码器改善了重叠段中说话人索引的预测。在AMI-MDM会议语料库上的实验表明，联合训练的DNCASR优于没有关联解码器的并行系统。使用cpWER衡量说话人属性词错误率，DNCASR在AMI-MDM评估集上实现了相对9.0%的降低。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>DNCASR是一个联合神经说话人聚类与自动语音识别的端到端训练系统。</li>
<li>它采用两个编码器来处理全局说话人特性和局部波形信息。</li>
<li>DNCASR使用两个关联解码器来生成说话人属性转录。</li>
<li>序列化训练方式使DNCASR能够处理现实会议中的重叠语音。</li>
<li>关联解码器改善了重叠语音段中说话人索引的预测。</li>
<li>实验结果表明，DNCASR在AMI-MDM会议语料库上的性能优于未关联解码器的并行系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01916">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2775dce0731a3daf6585bbf7e158308b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79371b4db47a0d1a0a1a699816d20435.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d362ec6bbe5ba6b9ad321683921b4f77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ddda94c35991256462583deaf861634.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adf4e316f1b56c2bd53b4947c70f2b5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72b051930985a1f197a0e0b84d8fdc43.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Reasoning-Based-Approach-with-Chain-of-Thought-for-Alzheimer’s-Detection-Using-Speech-and-Large-Language-Models"><a href="#Reasoning-Based-Approach-with-Chain-of-Thought-for-Alzheimer’s-Detection-Using-Speech-and-Large-Language-Models" class="headerlink" title="Reasoning-Based Approach with Chain-of-Thought for Alzheimer’s Detection   Using Speech and Large Language Models"></a>Reasoning-Based Approach with Chain-of-Thought for Alzheimer’s Detection   Using Speech and Large Language Models</h2><p><strong>Authors:Chanwoo Park, Anna Seo Gyeong Choi, Sunghye Cho, Chanwoo Kim</strong></p>
<p>Societies worldwide are rapidly entering a super-aged era, making elderly health a pressing concern. The aging population is increasing the burden on national economies and households. Dementia cases are rising significantly with this demographic shift. Recent research using voice-based models and large language models (LLM) offers new possibilities for dementia diagnosis and treatment. Our Chain-of-Thought (CoT) reasoning method combines speech and language models. The process starts with automatic speech recognition to convert speech to text. We add a linear layer to an LLM for Alzheimer’s disease (AD) and non-AD classification, using supervised fine-tuning (SFT) with CoT reasoning and cues. This approach showed an 16.7% relative performance improvement compared to methods without CoT prompt reasoning. To the best of our knowledge, our proposed method achieved state-of-the-art performance in CoT approaches. </p>
<blockquote>
<p>全球社会正迅速进入超老龄化时代，老年健康成为迫切需要关注的问题。人口老龄化增加了国家经济和家庭负担。随着人口结构的变化，痴呆症病例数量显著增加。最近利用基于语音的模型和大语言模型（LLM）进行的研究为痴呆症的诊断和治疗提供了新的可能性。我们的思维链（CoT）推理方法结合了语音和语言模型。流程从自动语音识别开始，将语音转换为文本。我们在针对阿尔茨海默病（AD）和非AD分类的大语言模型上增加了一层线性层，利用带有思维链推理和线索的监督微调（SFT）技术。相比没有思维链提示推理的方法，我们的方法显示出相对性能提高了16.7%。据我们所知，我们提出的方法在思维链方法中取得了最先进的性能表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01683v1">PDF</a> Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong><br>     全球社会迅速进入超老龄化时代，老年健康问题成为紧迫关注点。人口老龄化为国家经济和家庭带来负担，痴呆症患者数量随人口结构变化而显著增加。最新研究利用基于语音的模型与大型语言模型（LLM），为痴呆症的诊断和治疗提供新可能。本研究采用Chain-of-Thought（CoT）推理方法结合语音和语言模型，通过自动语音识别将语音转为文字，再对阿尔茨海默病（AD）与非AD进行分类。相较于不使用CoT提示推理的方法，此方式相对性能提升16.7%。据我们所知，该方法在CoT方法中达到最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社会老龄化趋势加剧，老年健康问题变得尤为重要。</li>
<li>人口老龄化给国家经济和家庭带来负担。</li>
<li>痴呆症患者数量随社会老龄化而增加。</li>
<li>结合语音和语言模型的Chain-of-Thought（CoT）推理方法提供新的痴呆症诊断与治疗可能性。</li>
<li>通过自动语音识别将语音转换为文字是此方法的起始步骤。</li>
<li>该方法相较于传统方式在性能上有显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01683">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-462ee0d85174a23ec13fc95abddc28ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83356943fb7a7920710a409226510525.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-630901954376fba1603174ec6d97802f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de3f099413cb9993324c2a7d6cf1d73a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c481c44d7a2e5b7849d917b3feb2ae8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e4ced0d6b1df420bb159a56e8112294.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Riemannian-Time-Warping-Multiple-Sequence-Alignment-in-Curved-Spaces"><a href="#Riemannian-Time-Warping-Multiple-Sequence-Alignment-in-Curved-Spaces" class="headerlink" title="Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces"></a>Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces</h2><p><strong>Authors:Julian Richter, Christopher Erdös, Christian Scheurer, Jochen J. Steil, Niels Dehio</strong></p>
<p>Temporal alignment of multiple signals through time warping is crucial in many fields, such as classification within speech recognition or robot motion learning. Almost all related works are limited to data in Euclidean space. Although an attempt was made in 2011 to adapt this concept to unit quaternions, a general extension to Riemannian manifolds remains absent. Given its importance for numerous applications in robotics and beyond, we introduce Riemannian Time Warping~(RTW). This novel approach efficiently aligns multiple signals by considering the geometric structure of the Riemannian manifold in which the data is embedded. Extensive experiments on synthetic and real-world data, including tests with an LBR iiwa robot, demonstrate that RTW consistently outperforms state-of-the-art baselines in both averaging and classification tasks. </p>
<blockquote>
<p>时空转换中对多个信号的时间对齐在许多领域都至关重要，如在语音识别或机器人运动学习中的分类。几乎所有相关工作都局限于欧几里得空间内的数据。尽管在2011年有人试图将此概念适应到单位四元数上，但对其扩展到黎曼流形的一般方法仍然缺失。考虑到其在机器人技术等多个领域的重要性，我们引入了黎曼时间扭曲（RTW）。这种方法通过考虑数据嵌入的黎曼流形的几何结构，有效地对齐多个信号。在合成数据和真实世界数据上的大量实验，包括对LBR iiwa机器人的测试，都证明了无论是在平均任务还是分类任务中，RTW都始终优于最新前沿基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了Riemannian时间弯曲（RTW）这一新方法，该方法考虑了数据嵌入的Riemannian流形几何结构，实现了多个信号的有效对齐。实验证明，该方法在合成数据和真实世界数据上均表现优异，特别是在机器人应用上，如LBR iiwa机器人测试，其在平均和分类任务上均优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时间弯曲在多个领域如语音识别或机器人运动学习中非常重要。</li>
<li>现有大部分相关工作仅限于欧几里得空间内的数据。</li>
<li>虽然已有尝试将时间弯曲概念适应于单位四元数，但其在黎曼流形上的通用扩展仍然缺失。</li>
<li>引入Riemannian时间弯曲（RTW）方法，考虑数据嵌入的黎曼流形的几何结构，实现多个信号的有效对齐。</li>
<li>实验证明，RTW在合成和真实世界数据上的表现均优于现有技术。</li>
<li>RTW在机器人应用上具有广泛的应用前景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01635">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1ca494282521eb6bdaf6e00a60746165.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc031e6123154267630535608627b59e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69d5236c44a2252bfb60ef08250d758f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5106b075412a6352b5f9d006ec5f1d90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afc05e92dd05d2bbf6407a4f59c86074.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f309a4a6116ebf6f5623f9c00d1d5430.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1fed0a41e6d5b5b7378a49ddf325e25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76bd7ddc2366bed84710c19c2d195baf.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Rhythm-and-Voice-Conversion-to-Improve-ASR-on-Dysarthric-Speech"><a href="#Unsupervised-Rhythm-and-Voice-Conversion-to-Improve-ASR-on-Dysarthric-Speech" class="headerlink" title="Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric   Speech"></a>Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric   Speech</h2><p><strong>Authors:Karl El Hajal, Enno Hermann, Sevada Hovsepyan, Mathew Magimai. -Doss</strong></p>
<p>Automatic speech recognition (ASR) systems struggle with dysarthric speech due to high inter-speaker variability and slow speaking rates. To address this, we explore dysarthric-to-healthy speech conversion for improved ASR performance. Our approach extends the Rhythm and Voice (RnV) conversion framework by introducing a syllable-based rhythm modeling method suited for dysarthric speech. We assess its impact on ASR by training LF-MMI models and fine-tuning Whisper on converted speech. Experiments on the Torgo corpus reveal that LF-MMI achieves significant word error rate reductions, especially for more severe cases of dysarthria, while fine-tuning Whisper on converted data has minimal effect on its performance. These results highlight the potential of unsupervised rhythm and voice conversion for dysarthric ASR. Code available at: <a target="_blank" rel="noopener" href="https://github.com/idiap/RnV">https://github.com/idiap/RnV</a> </p>
<blockquote>
<p>自动语音识别（ASR）系统因说话者之间的高度差异性以及语速过慢，在处理口齿不清的语音时面临困难。为了解决这一问题，我们探索了口齿不清到正常语音的转换，以提高ASR系统的性能。我们的方法扩展了节奏和声音（RnV）转换框架，通过引入基于音节的节奏建模方法，该方法适用于口齿不清的语音。我们通过训练LF-MMI模型和微调whisper模型来评估其对ASR的影响。在Torgo语料库上的实验表明，LF-MMI实现了显著的单词错误率降低，特别是在口齿不清较为严重的情况下，而在转换数据上微调whisper对其性能影响甚微。这些结果突出了无监督的节奏和声音转换在口齿不清的ASR中的潜力。代码可从 <a target="_blank" rel="noopener" href="https://github.com/idiap/RnV">https://github.com/idiap/RnV</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01618v1">PDF</a> Accepted at Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文探索了针对发音困难人士的语音转换技术，以提高自动语音识别（ASR）系统的性能。该研究扩展了节奏和声音（RnV）转换框架，引入了适合发音困难人士的基于音节的节奏建模方法。通过训练LF-MMI模型和微调whisper语音转换技术来评估其效果，发现该方法显著降低了误词率，尤其是在处理更为严重的发音障碍时效果显著。该研究突显了无监督的节奏和声音转换技术在改善发音障碍人士ASR系统中的潜力。代码可于相关GitHub链接找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究针对发音困难人士的语音转换技术，以提高自动语音识别（ASR）系统的性能。</li>
<li>扩展了现有的节奏和声音（RnV）转换框架，以适用于发音困难人士的语音特征。</li>
<li>通过引入基于音节的节奏建模方法，适应发音困难人士的语音特点。</li>
<li>通过训练LF-MMI模型评估其对ASR系统的影响，发现显著降低了误词率。</li>
<li>即使在处理更为严重的发音障碍时，该方法的性能提升尤为明显。</li>
<li>研究发现微调whisper语音转换技术对改善ASR性能具有有限影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01618">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6543c7f142328453b5b9bfce1cea3f7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2ca042780d6d7dcfb1265a10a248fa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-466dc9ffd5d100682733a002436255a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44eb8fdba7a3984de5ec81a1653fa401.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Few-step-Adversarial-Schrodinger-Bridge-for-Generative-Speech-Enhancement"><a href="#Few-step-Adversarial-Schrodinger-Bridge-for-Generative-Speech-Enhancement" class="headerlink" title="Few-step Adversarial Schrödinger Bridge for Generative Speech   Enhancement"></a>Few-step Adversarial Schrödinger Bridge for Generative Speech   Enhancement</h2><p><strong>Authors:Seungu Han, Sungho Lee, Juheon Lee, Kyogu Lee</strong></p>
<p>Deep generative models have recently been employed for speech enhancement to generate perceptually valid clean speech on large-scale datasets. Several diffusion models have been proposed, and more recently, a tractable Schr&quot;odinger Bridge has been introduced to transport between the clean and noisy speech distributions. However, these models often suffer from an iterative reverse process and require a large number of sampling steps – more than 50. Our investigation reveals that the performance of baseline models significantly degrades when the number of sampling steps is reduced, particularly under low-SNR conditions. We propose integrating Schr&quot;odinger Bridge with GANs to effectively mitigate this issue, achieving high-quality outputs on full-band datasets while substantially reducing the required sampling steps. Experimental results demonstrate that our proposed model outperforms existing baselines, even with a single inference step, in both denoising and dereverberation tasks. </p>
<blockquote>
<p>深度生成模型最近被用于语音增强，以在大规模数据集上生成感知有效的干净语音。已经提出了几种扩散模型，最近还介绍了一种易处理的薛定谔桥（Schrödinger Bridge），用于在干净语音和带噪语音分布之间进行传输。然而，这些模型通常存在一个迭代反向过程，需要大量采样步骤——超过50步。我们的调查发现，当减少采样步骤数量时，基线模型的性能会显著下降，特别是在低信噪比条件下。我们提出将薛定谔桥与生成对抗网络（GANs）相结合，以有效缓解这一问题，在全频带数据集上实现高质量输出，同时大大减少所需的采样步骤。实验结果表明，即使在单个推理步骤中，我们提出的模型在降噪和去混响任务中都优于现有基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01460v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了深度生成模型在语音增强中的应用，介绍了使用Schrödinger Bridge与GANs结合的方法，以提高模型性能并减少采样步骤。实验结果表明，该方法在降噪和去混响任务中均优于现有基线模型，即使在单个推理步骤下也能实现高质量输出。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度生成模型被用于语音增强，生成感知上有效的干净语音。</li>
<li>存在迭代反向过程的问题，需要大量采样步骤。</li>
<li>在减少采样步骤时，基线模型的性能会显著下降。</li>
<li>介绍了Schrödinger Bridge与GANs的结合使用。</li>
<li>该方法能够在全带宽数据集上实现高质量输出。</li>
<li>实验结果表明，所提出模型在降噪和去混响任务中均优于现有基线模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01460">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7b5a8cb2fa963a805e58c2833bfaf5d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a695073757401917b733ca30b3cf00e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4961e042fa0ba63482681632fe9028ca.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-88df9453bb4a3e669dcec77a62684f63.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-06-05  TAH-QUANT Effective Activation Quantization in Pipeline Parallelism   over Slow Network
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5519fd18407786cd7e76489e87e9e414.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-06-05  High-Contrast Coronagraphy
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25879.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
