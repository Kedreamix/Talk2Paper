<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  Towards a Japanese Full-duplex Spoken Dialogue System">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-462ee0d85174a23ec13fc95abddc28ab.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-05-æ›´æ–°"><a href="#2025-06-05-æ›´æ–°" class="headerlink" title="2025-06-05 æ›´æ–°"></a>2025-06-05 æ›´æ–°</h1><h2 id="Towards-a-Japanese-Full-duplex-Spoken-Dialogue-System"><a href="#Towards-a-Japanese-Full-duplex-Spoken-Dialogue-System" class="headerlink" title="Towards a Japanese Full-duplex Spoken Dialogue System"></a>Towards a Japanese Full-duplex Spoken Dialogue System</h2><p><strong>Authors:Atsumoto Ohashi, Shinya Iizuka, Jingjing Jiang, Ryuichiro Higashinaka</strong></p>
<p>Full-duplex spoken dialogue systems, which can model simultaneous bidirectional features of human conversations such as speech overlaps and backchannels, have attracted significant attention recently. However, the study of full-duplex spoken dialogue systems for the Japanese language has been limited, and the research on their development in Japanese remains scarce. In this paper, we present the first publicly available full-duplex spoken dialogue model in Japanese, which is built upon Moshi, a full-duplex dialogue model in English. Our model is trained through a two-stage process: pre-training on a large-scale spoken dialogue data in Japanese, followed by fine-tuning on high-quality stereo spoken dialogue data. We further enhance the modelâ€™s performance by incorporating synthetic dialogue data generated by a multi-stream text-to-speech system. Evaluation experiments demonstrate that the trained model outperforms Japanese baseline models in both naturalness and meaningfulness. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå…¨åŒå·¥è¯­éŸ³å¯¹è¯ç³»ç»Ÿèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»å¯¹è¯çš„åŒå‘ç‰¹å¾ï¼Œå¦‚è¯­éŸ³é‡å å’Œåé¦ˆé€šé“ç­‰ï¼Œå› æ­¤å—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ—¥è¯­çš„å…¨åŒå·¥è¯­éŸ³å¯¹è¯ç³»ç»Ÿçš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ï¼Œå…¶å¼€å‘ç ”ç©¶ä¹Ÿååˆ†åŒ®ä¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†é¦–ä¸ªå…¬å¼€å¯ç”¨çš„æ—¥è¯­å…¨åŒå·¥è¯­éŸ³å¯¹è¯æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºè‹±è¯­å…¨åŒå·¥å¯¹è¯æ¨¡å‹Moshiæ„å»ºã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒï¼šé¦–å…ˆåœ¨å¤§é‡æ—¥è¯­å£è¯­å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨é«˜è´¨é‡ç«‹ä½“å£°å£è¯­å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ç”±å¤šæµæ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿç”Ÿæˆçš„äººå·¥å¯¹è¯æ•°æ®ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚è¯„ä¼°å®éªŒè¡¨æ˜ï¼Œè®­ç»ƒåçš„æ¨¡å‹åœ¨è‡ªç„¶åº¦å’Œæ„ä¹‰æ€§æ–¹é¢éƒ½ä¼˜äºæ—¥è¯­åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02979v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡ä»‹ç»äº†é¦–ä¸ªå…¬å¼€çš„æ—¥è¯­å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºè‹±è¯­çš„å…¨åŒå·¥å¯¹è¯æ¨¡å‹Moshiæ„å»ºã€‚é€šè¿‡å¤§è§„æ¨¡æ—¥è¯­å¯¹è¯æ•°æ®é¢„è®­ç»ƒå’Œé«˜è´¨é‡ç«‹ä½“å£°å¯¹è¯æ•°æ®å¾®è°ƒçš„ä¸¤é˜¶æ®µè¿‡ç¨‹è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨å¤šæµæ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿç”Ÿæˆåˆæˆå¯¹è¯æ•°æ®å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚è¯„ä¼°å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ—¥è¯­åŸºå‡†æ¨¡å‹çš„è‡ªç„¶åº¦å’Œæœ‰æ„ä¹‰æ€§æ–¹é¢éƒ½è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æ„å»ºäº†é¦–ä¸ªå…¬å¼€çš„æ—¥è¯­å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿæ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åŸºäºè‹±è¯­çš„å…¨åŒå·¥å¯¹è¯æ¨¡å‹Moshiæ„å»ºã€‚</li>
<li>æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒä¸å¾®è°ƒçš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨å¤§è§„æ¨¡æ—¥è¯­å¯¹è¯æ•°æ®å’Œé«˜è´¨é‡ç«‹ä½“å£°å¯¹è¯æ•°æ®ã€‚</li>
<li>åˆæˆå¯¹è¯æ•°æ®é€šè¿‡å¤šæµæ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿç”Ÿæˆï¼Œå¢å¼ºäº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨è‡ªç„¶åº¦å’Œæœ‰æ„ä¹‰æ€§æ–¹é¢è¶…è¶Šäº†æ—¥è¯­åŸºå‡†æ¨¡å‹ã€‚</li>
<li>å…¨åŒå·¥å¯¹è¯ç³»ç»Ÿèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»å¯¹è¯çš„åŒå‘ç‰¹å¾ï¼Œå¦‚è¯­éŸ³é‡å å’Œåé¦ˆé€šé“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b7400a8cc0d30f4bfd02ffe1a4326777.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76749f6f0d5726186e99f2cc572edd14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b56fe64df06e5e9b544a1ae35ffc2db7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-847c4c71e81a6d97a225e83bfc956bb6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Diffusion-Buffer-Online-Diffusion-based-Speech-Enhancement-with-Sub-Second-Latency"><a href="#Diffusion-Buffer-Online-Diffusion-based-Speech-Enhancement-with-Sub-Second-Latency" class="headerlink" title="Diffusion Buffer: Online Diffusion-based Speech Enhancement with   Sub-Second Latency"></a>Diffusion Buffer: Online Diffusion-based Speech Enhancement with   Sub-Second Latency</h2><p><strong>Authors:Bunlong Lay, Rostilav Makarov, Timo Gerkmann</strong></p>
<p>Diffusion models are a class of generative models that have been recently used for speech enhancement with remarkable success but are computationally expensive at inference time. Therefore, these models are impractical for processing streaming data in real-time. In this work, we adapt a sliding window diffusion framework to the speech enhancement task. Our approach progressively corrupts speech signals through time, assigning more noise to frames close to the present in a buffer. This approach outputs denoised frames with a delay proportional to the chosen buffer size, enabling a trade-off between performance and latency. Empirical results demonstrate that our method outperforms standard diffusion models and runs efficiently on a GPU, achieving an input-output latency in the order of 0.3 to 1 seconds. This marks the first practical diffusion-based solution for online speech enhancement. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç±»ç”Ÿæˆæ¨¡å‹ï¼Œæœ€è¿‘è¢«ç”¨äºè¯­éŸ³å¢å¼ºå¹¶å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨æ¨ç†æ—¶è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚å› æ­¤ï¼Œè¿™äº›æ¨¡å‹å¯¹äºå®æ—¶å¤„ç†æµå¼æ•°æ®å¹¶ä¸å®ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†æ»‘åŠ¨çª—å£æ‰©æ•£æ¡†æ¶é€‚åº”äºè¯­éŸ³å¢å¼ºä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ—¶é—´é€æ­¥ç ´åè¯­éŸ³ä¿¡å·ï¼Œå°†æ›´å¤šå™ªå£°åˆ†é…ç»™ç¼“å†²åŒºä¸­æ¥è¿‘å½“å‰çš„å¸§ã€‚è¿™ç§æ–¹æ³•è¾“å‡ºçš„å»å™ªå¸§å»¶è¿Ÿä¸æ‰€é€‰ç¼“å†²åŒºå¤§å°æˆæ¯”ä¾‹ï¼Œå¯ä»¥åœ¨æ€§èƒ½å’Œå»¶è¿Ÿä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæ ‡å‡†æ‰©æ•£æ¨¡å‹ï¼Œåœ¨GPUä¸Šè¿è¡Œé«˜æ•ˆï¼Œè¾“å…¥è¾“å‡ºå»¶è¿Ÿåœ¨0.3è‡³1ç§’ä¹‹é—´ã€‚è¿™æ ‡å¿—ç€åŸºäºæ‰©æ•£çš„åœ¨çº¿è¯­éŸ³å¢å¼ºçš„é¦–ä¸ªå®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02908v1">PDF</a> 5 pages, 2 figures, Accepted to Interspeech 2025</p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ»‘åŠ¨çª—å£æ‰©æ•£æ¡†æ¶çš„è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨é€æ­¥å¯¹è¯­éŸ³ä¿¡å·è¿›è¡Œå»å™ªçš„è¿‡ç¨‹ä¸­åŠ å…¥äº†æ—¶é—´å› ç´ ï¼Œå®ç°äº†æ€§èƒ½å’Œå»¶è¿Ÿä¹‹é—´çš„æƒè¡¡ã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ–¹æ³•åœ¨GPUä¸Šè¿è¡Œæ›´ä¸ºé«˜æ•ˆï¼Œå®ç°äº†è¾“å…¥è¾“å‡ºçš„å»¶è¿Ÿåœ¨0.3è‡³1ç§’ä¹‹é—´ï¼Œä¸ºåœ¨çº¿è¯­éŸ³å¢å¼ºæä¾›äº†é¦–ä¸ªå®ç”¨çš„æ‰©æ•£æ¨¡å‹è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œä¸é€‚åˆå¤„ç†å®æ—¶æµåª’ä½“æ•°æ®ã€‚</li>
<li>æœ¬æ–‡é‡‡ç”¨æ»‘åŠ¨çª—å£æ‰©æ•£æ¡†æ¶è¿›è¡Œè¯­éŸ³å¢å¼ºï¼Œå°†æ—¶é—´å› ç´ çº³å…¥è€ƒè™‘ï¼Œé€æ­¥å¯¹è¯­éŸ³ä¿¡å·è¿›è¡Œå»å™ªã€‚</li>
<li>æ–¹æ³•é€šè¿‡åœ¨ç¼“å†²åŒºä¸­å¯¹æ¥è¿‘å½“å‰å¸§çš„å¸§èµ‹äºˆæ›´å¤šå™ªå£°æ¥å®ç°æ¸è¿›å¼è…èš€è¯­éŸ³ä¿¡å·ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨è¾“å‡ºé™å™ªå¸§æ—¶å®ç°å»¶è¿Ÿä¸æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ï¼Œå»¶è¿Ÿä¸æ‰€é€‰ç¼“å†²åŒºå¤§å°æˆæ¯”ä¾‹ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨GPUä¸Šè¿è¡Œæ›´ä¸ºé«˜æ•ˆï¼Œè¾¾åˆ°äº†0.3è‡³1ç§’çš„è¾“å…¥-è¾“å‡ºå»¶è¿Ÿã€‚</li>
<li>æœ¬æ–‡è§£å†³äº†åœ¨çº¿è¯­éŸ³å¢å¼ºçš„å®ç”¨é—®é¢˜ï¼Œè¿™æ˜¯åŸºäºæ‰©æ•£æ¨¡å‹çš„é¦–æ¬¡å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8882f669aa2044c5fc3c44e376e63529.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb97edaa41e06a893770a47f5698d153.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d30042e8ae52e373ace858637be097bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f6ea14bcce76eb97c12b355283738b4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Multi-Dialectal-Dataset-for-German-Dialect-ASR-and-Dialect-to-Standard-Speech-Translation"><a href="#A-Multi-Dialectal-Dataset-for-German-Dialect-ASR-and-Dialect-to-Standard-Speech-Translation" class="headerlink" title="A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard   Speech Translation"></a>A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard   Speech Translation</h2><p><strong>Authors:Verena Blaschke, Miriam Winkler, Constantin FÃ¶rster, Gabriele Wenger-Glemser, Barbara Plank</strong></p>
<p>Although Germany has a diverse landscape of dialects, they are underrepresented in current automatic speech recognition (ASR) research. To enable studies of how robust models are towards dialectal variation, we present Betthupferl, an evaluation dataset containing four hours of read speech in three dialect groups spoken in Southeast Germany (Franconian, Bavarian, Alemannic), and half an hour of Standard German speech. We provide both dialectal and Standard German transcriptions, and analyze the linguistic differences between them. We benchmark several multilingual state-of-the-art ASR models on speech translation into Standard German, and find differences between how much the output resembles the dialectal vs. standardized transcriptions. Qualitative error analyses of the best ASR model reveal that it sometimes normalizes grammatical differences, but often stays closer to the dialectal constructions. </p>
<blockquote>
<p>å°½ç®¡å¾·å›½çš„æ–¹è¨€å¤šç§å¤šæ ·ï¼Œä½†åœ¨å½“å‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç ”ç©¶ä¸­ï¼Œå®ƒä»¬çš„ä»£è¡¨æ€§ä¸è¶³ã€‚ä¸ºäº†ç ”ç©¶æ¨¡å‹å¯¹æ–¹è¨€å˜ä½“çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Betthupferlè¯„ä¼°æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å››å°æ—¶åœ¨ä¸œå—å¾·å›½ï¼ˆå¼—å…°ç§‘å°¼äºšè¯­ã€å·´ä¼åˆ©äºšè¯­ã€é˜¿å‹’æ›¼å°¼è¯­ï¼‰è¯´çš„ä¸‰ç§æ–¹è¨€ç¾¤ä½“çš„æœ—è¯»è¯­éŸ³ï¼Œä»¥åŠåŠå°æ—¶çš„æ ‡å‡†å¾·è¯­è¯­éŸ³ã€‚æˆ‘ä»¬æä¾›äº†æ–¹è¨€å’Œæ ‡å‡†å¾·è¯­çš„è½¬å½•ï¼Œå¹¶åˆ†æäº†å®ƒä»¬ä¹‹é—´çš„è¯­è¨€å·®å¼‚ã€‚æˆ‘ä»¬åœ¨å°†è¯­éŸ³ç¿»è¯‘æˆæ ‡å‡†å¾·è¯­æ–¹é¢å¯¹å‡ ç§å¤šè¯­ç§çš„æœ€å…ˆè¿›ASRæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å‘ç°è¾“å‡ºä¸æ–¹è¨€è½¬å½•ä¸æ ‡å‡†åŒ–è½¬å½•ä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦å·®å¼‚ã€‚æœ€ä½³ASRæ¨¡å‹çš„å®šæ€§è¯¯å·®åˆ†æè¡¨æ˜ï¼Œå®ƒæœ‰æ—¶ä¼šæ­£è§„åŒ–è¯­æ³•å·®å¼‚ï¼Œä½†é€šå¸¸ä¼šæ›´æ¥è¿‘æ–¹è¨€ç»“æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02894v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¾·å›½æ–¹è¨€åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç ”ç©¶ä¸­çš„ä»£è¡¨æ€§ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†ç ”ç©¶æ¨¡å‹å¯¹æ–¹è¨€å˜ä½“çš„é²æ£’æ€§ï¼Œæå‡ºäº†ä¸€ç§è¯„ä¼°æ•°æ®é›†Betthupferlã€‚è¯¥æ•°æ®é›†åŒ…å«å››å°æ—¶ä¸œå—å¾·å›½çš„ä¸‰ç§æ–¹è¨€ï¼ˆå¼—å…°è¥¿æ–¯ã€æ‹œä»è¯­å’Œé˜¿å‹’æ›¼å°¼è¯­ï¼‰çš„æœ—è¯»è¯­éŸ³å’ŒåŠå°æ—¶çš„æ ‡å‡†å¾·è¯­è¯­éŸ³ã€‚æä¾›äº†æ–¹è¨€å’Œæ ‡å‡†å¾·è¯­çš„è½¬å½•ï¼Œå¹¶åˆ†æäº†å®ƒä»¬ä¹‹é—´çš„è¯­è¨€å·®å¼‚ã€‚é€šè¿‡å¯¹å¤šç§å¤šè¯­ç§æœ€å…ˆè¿›çš„ASRæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå‘ç°è¾“å‡ºä¸æ–¹è¨€è½¬å½•å’Œæ ‡å‡†è½¬å½•çš„ç›¸ä¼¼ç¨‹åº¦å­˜åœ¨å·®å¼‚ã€‚æœ€ä½³ASRæ¨¡å‹çš„å®šæ€§è¯¯å·®åˆ†æè¡¨æ˜ï¼Œå®ƒæœ‰æ—¶ä¼šè§„èŒƒåŒ–è¯­æ³•å·®å¼‚ï¼Œä½†é€šå¸¸æ›´æ¥è¿‘æ–¹è¨€ç»“æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¾·å›½æ–¹è¨€åœ¨å½“å‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç ”ç©¶ä¸­ä»£è¡¨æ€§ä¸è¶³ã€‚</li>
<li>Betthupferlæ•°æ®é›†ç”¨äºè¯„ä¼°æ¨¡å‹å¯¹å¾·å›½ä¸œå—åœ°åŒºä¸‰ç§æ–¹è¨€çš„é²æ£’æ€§ï¼Œå¹¶æä¾›äº†æ–¹è¨€å’Œæ ‡å‡†å¾·è¯­çš„è½¬å½•ã€‚</li>
<li>æ–¹è¨€ä¸æ ‡å‡†å¾·è¯­ä¹‹é—´å­˜åœ¨è¯­è¨€å·®å¼‚ã€‚</li>
<li>å¤šç§å¤šè¯­ç§ASRæ¨¡å‹åœ¨æ ‡å‡†å¾·è¯­è¯­éŸ³ç¿»è¯‘ä¸Šçš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œè¾“å‡ºä¸æ–¹è¨€è½¬å½•çš„ç›¸ä¼¼ç¨‹åº¦ä¸åŒã€‚</li>
<li>æœ€ä½³ASRæ¨¡å‹åœ¨å¤„ç†æ–¹è¨€æ—¶ï¼Œæœ‰æ—¶ä¼šè§„èŒƒåŒ–è¯­æ³•å·®å¼‚ã€‚</li>
<li>ASRæ¨¡å‹åœ¨å¤„ç†æ–¹è¨€è¯­éŸ³æ—¶ï¼Œé€šå¸¸æ›´æ¥è¿‘æ–¹è¨€ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02894">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a28bd108bc49662324d7fc4dd47bdd10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dd5e75d0e6b080df222544358463b40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-013b8f08937c7b73aaa968b6b9db6b59.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ccef456f63076d0cff8c0fcd5d8f30d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6b4c319380e5ebb4bb08d7606d83284.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multi-task-Learning-with-Active-Learning-for-Arabic-Offensive-Speech-Detection"><a href="#Multi-task-Learning-with-Active-Learning-for-Arabic-Offensive-Speech-Detection" class="headerlink" title="Multi-task Learning with Active Learning for Arabic Offensive Speech   Detection"></a>Multi-task Learning with Active Learning for Arabic Offensive Speech   Detection</h2><p><strong>Authors:Aisha Alansari, Hamzah Luqman</strong></p>
<p>The rapid growth of social media has amplified the spread of offensive, violent, and vulgar speech, which poses serious societal and cybersecurity concerns. Detecting such content in Arabic text is particularly complex due to limited labeled data, dialectal variations, and the languageâ€™s inherent complexity. This paper proposes a novel framework that integrates multi-task learning (MTL) with active learning to enhance offensive speech detection in Arabic social media text. By jointly training on two auxiliary tasks, violent and vulgar speech, the model leverages shared representations to improve the detection accuracy of the offensive speech. Our approach dynamically adjusts task weights during training to balance the contribution of each task and optimize performance. To address the scarcity of labeled data, we employ an active learning strategy through several uncertainty sampling techniques to iteratively select the most informative samples for model training. We also introduce weighted emoji handling to better capture semantic cues. Experimental results on the OSACT2022 dataset show that the proposed framework achieves a state-of-the-art macro F1-score of 85.42%, outperforming existing methods while using significantly fewer fine-tuning samples. The findings of this study highlight the potential of integrating MTL with active learning for efficient and accurate offensive language detection in resource-constrained settings. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“çš„å¿«é€Ÿå‘å±•æ”¾å¤§äº†æ”»å‡»æ€§ã€æš´åŠ›å’Œç²—é²è¨€è®ºçš„ä¼ æ’­ï¼Œè¿™å¼•å‘äº†ä¸¥é‡çš„ç¤¾ä¼šå’Œç½‘ç»œå®‰å…¨æ‹…å¿§ã€‚ç”±äºæ ‡è®°æ•°æ®æœ‰é™ã€æ–¹è¨€å·®å¼‚å’Œè¯­è¨€çš„å›ºæœ‰å¤æ‚æ€§ï¼Œåœ¨é˜¿æ‹‰ä¼¯æ–‡æœ¬ä¸­æ£€æµ‹æ­¤ç±»å†…å®¹å°¤ä¸ºå¤æ‚ã€‚æœ¬æ–‡é’ˆå¯¹é˜¿æ‹‰ä¼¯ç¤¾äº¤åª’ä½“æ–‡æœ¬ä¸­çš„æ”»å‡»æ€§è¨€è®ºæ£€æµ‹ï¼Œæå‡ºäº†ä¸€ç§èåˆå¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰ä¸ä¸»åŠ¨å­¦ä¹ çš„å…¨æ–°æ¡†æ¶ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ï¼ˆæš´åŠ›å’Œç²—é²è¨€è¯­ï¼‰è¿›è¡Œè”åˆè®­ç»ƒï¼Œåˆ©ç”¨å…±äº«è¡¨ç¤ºæ¥æé«˜æ”»å‡»æ€§è¨€è®ºæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´ä»»åŠ¡æƒé‡ï¼Œä»¥å¹³è¡¡æ¯ä¸ªä»»åŠ¡çš„è´¡çŒ®å¹¶ä¼˜åŒ–æ€§èƒ½ã€‚ä¸ºäº†è§£å†³æ ‡è®°æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡å‡ ç§ä¸ç¡®å®šæ€§é‡‡æ ·æŠ€æœ¯æ¥è¿­ä»£é€‰æ‹©æœ€æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†åŠ æƒè¡¨æƒ…ç¬¦å·å¤„ç†ï¼Œä»¥æ›´å¥½åœ°æ•æ‰è¯­ä¹‰çº¿ç´¢ã€‚åœ¨OSACT2022æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å®è§‚F1åˆ†æ•°85.42%ï¼Œåœ¨ä½¿ç”¨æ˜¾è‘—æ›´å°‘çš„å¾®è°ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œè¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚æœ¬ç ”ç©¶çš„ç»“æœçªå‡ºäº†åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­ï¼Œå°†MTLä¸ä¸»åŠ¨å­¦ä¹ ç›¸ç»“åˆè¿›è¡Œé«˜æ•ˆä¸”å‡†ç¡®çš„æ”»å‡»æ€§è¯­è¨€æ£€æµ‹çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02753v1">PDF</a> </p>
<p><strong>Summary</strong><br>é˜¿æ‹‰ä¼¯è¯­ç¤¾äº¤åª’ä½“æ–‡æœ¬ä¸­çš„æ”»å‡»æ€§è¨€è®ºæ£€æµ‹é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚æ ‡æ³¨æ•°æ®æœ‰é™ã€æ–¹è¨€å·®å¼‚å’Œè¯­è¨€æœ¬èº«çš„å¤æ‚æ€§ç­‰ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆå¤šä»»åŠ¡å­¦ä¹ å’Œä¸»åŠ¨å­¦ä¹ çš„æ¡†æ¶ï¼Œé€šè¿‡è”åˆè®­ç»ƒæš´åŠ›ã€ç²—ä¿—å’Œæ”»å‡»æ€§è¨€è®ºæ£€æµ‹ä»»åŠ¡ï¼Œæé«˜æ¨¡å‹å¯¹é˜¿æ‹‰ä¼¯è¯­æ”»å‡»æ€§è¨€è®ºæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åŠ¨æ€è°ƒæ•´ä»»åŠ¡æƒé‡ï¼Œå¹¶é‡‡ç”¨ä¸»åŠ¨å­¦ä¹ ç­–ç•¥è§£å†³æ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨OSACT2022æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„å®è§‚F1åˆ†æ•°ï¼Œä¸”ä½¿ç”¨è¾ƒå°‘çš„å¾®è°ƒæ ·æœ¬å³å¯å®ç°æ€§èƒ½ä¼˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾ä¼šåª’ä½“çš„å¿«é€Ÿå‘å±•åŠ å‰§äº†æ”»å‡»æ€§ã€æš´åŠ›å’Œç²—é²è¨€è®ºçš„ä¼ æ’­ï¼Œå¼•å‘äº†ç¤¾ä¼šå’Œç½‘ç»œå®‰å…¨æ‹…å¿§ã€‚</li>
<li>é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬ä¸­çš„æ”»å‡»æ€§è¨€è®ºæ£€æµ‹é¢ä¸´æ ‡æ³¨æ•°æ®æœ‰é™ã€æ–¹è¨€å·®å¼‚å’Œè¯­è¨€å¤æ‚æ€§ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆå¤šä»»åŠ¡å­¦ä¹ å’Œä¸»åŠ¨å­¦ä¹ çš„æ¡†æ¶ï¼Œä»¥æé«˜æ”»å‡»æ€§è¨€è®ºæ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è”åˆè®­ç»ƒæš´åŠ›ã€ç²—ä¿—å’Œæ”»å‡»æ€§è¨€è®ºæ£€æµ‹ä»»åŠ¡ï¼Œåˆ©ç”¨å…±äº«è¡¨å¾æ¥æé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶åŠ¨æ€è°ƒæ•´ä»»åŠ¡æƒé‡ï¼Œä»¥å¹³è¡¡å„ä¸ªä»»åŠ¡çš„è´¡çŒ®å¹¶ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨ä¸»åŠ¨å­¦ä¹ ç­–ç•¥è§£å†³æ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œé€šè¿‡ä¸ç¡®å®šæ€§é‡‡æ ·æŠ€æœ¯é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ ·æœ¬è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-373cedf6e4661ea34a5ebb69f3a73354.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-214991191c3154f78f636339a00fe591.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3786df12f8fb07ddb6b1f459c3e53be3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-777e27e5c026b2c0adbfdfc2b22bcc10.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DnR-nonverbal-Cinematic-Audio-Source-Separation-Dataset-Containing-Non-Verbal-Sounds"><a href="#DnR-nonverbal-Cinematic-Audio-Source-Separation-Dataset-Containing-Non-Verbal-Sounds" class="headerlink" title="DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing   Non-Verbal Sounds"></a>DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing   Non-Verbal Sounds</h2><p><strong>Authors:Takuya Hasumi, Yusuke Fujita</strong></p>
<p>We propose a new dataset for cinematic audio source separation (CASS) that handles non-verbal sounds. Existing CASS datasets only contain reading-style sounds as a speech stem. These datasets differ from actual movie audio, which is more likely to include acted-out voices. Consequently, models trained on conventional datasets tend to have issues where emotionally heightened voices, such as laughter and screams, are more easily separated as an effect, not speech. To address this problem, we build a new dataset, DnR-nonverbal. The proposed dataset includes non-verbal sounds like laughter and screams in the speech stem. From the experiments, we reveal the issue of non-verbal sound extraction by the current CASS model and show that our dataset can effectively address the issue in the synthetic and actual movie audio. Our dataset is available at <a target="_blank" rel="noopener" href="https://zenodo.org/records/15470640">https://zenodo.org/records/15470640</a>. </p>
<blockquote>
<p>æˆ‘ä»¬é’ˆå¯¹ç”µå½±éŸ³é¢‘æºåˆ†ç¦»ï¼ˆCASSï¼‰æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œç”¨äºå¤„ç†éè¨€è¯­å£°éŸ³ã€‚ç°æœ‰çš„CASSæ•°æ®é›†ä»…åŒ…å«é˜…è¯»é£æ ¼çš„è¯­éŸ³ä½œä¸ºè¯­éŸ³ä¸»å¹²ã€‚è¿™äº›æ•°æ®é›†ä¸å®é™…çš„ç”µå½±éŸ³é¢‘æœ‰æ‰€ä¸åŒï¼Œåè€…æ›´å¯èƒ½åŒ…å«è¡¨æ¼”æ€§çš„å£°éŸ³ã€‚å› æ­¤ï¼Œåœ¨å¸¸è§„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹å¾€å¾€ä¼šå‡ºç°é—®é¢˜ï¼Œæƒ…æ„Ÿé«˜æ¶¨çš„å£°éŸ³ï¼ˆå¦‚ç¬‘å£°å’Œå°–å«å£°ï¼‰æ›´å®¹æ˜“è¢«åˆ†ç¦»ä¸ºä¸€ç§æ•ˆæœï¼Œè€Œéè¯­éŸ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†DnR-nonverbalã€‚è¯¥æ•°æ®é›†åŒ…å«äº†è¯­éŸ³ä¸»å¹²ä¸­çš„éè¨€è¯­å£°éŸ³ï¼Œå¦‚ç¬‘å£°å’Œå°–å«å£°ç­‰ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬æ­ç¤ºäº†å½“å‰CASSæ¨¡å‹åœ¨éè¨€è¯­å£°éŸ³æå–æ–¹é¢çš„é—®é¢˜ï¼Œå¹¶è¡¨æ˜æˆ‘ä»¬çš„æ•°æ®é›†å¯ä»¥æœ‰æ•ˆåœ°è§£å†³åˆæˆå’Œå®é™…ç”µå½±éŸ³é¢‘ä¸­çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://zenodo.org/records/15470640%E8%8E%B7%E5%8F%96%E3%80%82">https://zenodo.org/records/15470640è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02499v1">PDF</a> Accepted to Interspeech 2025, 5 pages, 3 figures, dataset is   available at <a target="_blank" rel="noopener" href="https://zenodo.org/records/15470640">https://zenodo.org/records/15470640</a></p>
<p><strong>Summary</strong></p>
<p>æå‡ºäº†ä¸€ç§æ–°çš„ç”µå½±éŸ³é¢‘æºåˆ†ç¦»ï¼ˆCASSï¼‰æ•°æ®é›†ï¼Œç”¨äºå¤„ç†éè¯­è¨€å£°éŸ³ã€‚ç°æœ‰CASSæ•°æ®é›†ä»…åŒ…å«é˜…è¯»å¼å£°éŸ³ä½œä¸ºè¯­éŸ³å¹²éŸ³ï¼Œä¸çœŸå®ç”µå½±éŸ³é¢‘ä¸åŒï¼Œåè€…æ›´å¯èƒ½åŒ…å«è¡¨æ¼”å‡ºçš„å£°éŸ³ã€‚å› æ­¤ï¼Œåœ¨å¸¸è§„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹å¾€å¾€åœ¨å¤„ç†æƒ…ç»ªé«˜æ¶¨çš„å£°éŸ³ï¼ˆå¦‚ç¬‘å£°å’Œå°–å«ï¼‰æ—¶å­˜åœ¨é—®é¢˜ï¼Œå®¹æ˜“å°†å…¶åˆ†ç¦»ä¸ºæ•ˆæœè€Œéè¯­éŸ³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ„å»ºäº†æ–°çš„æ•°æ®é›†DnR-nonverbalï¼Œå…¶ä¸­åŒ…å«äº†éè¯­è¨€å£°éŸ³ï¼ˆå¦‚ç¬‘å£°å’Œå°–å«ï¼‰åœ¨è¯­éŸ³å¹²éŸ³ä¸­ã€‚å®éªŒæ­ç¤ºäº†å½“å‰CASSæ¨¡å‹æå–éè¯­è¨€å£°éŸ³çš„é—®é¢˜ï¼Œå¹¶è¡¨æ˜è¯¥æ•°æ®é›†å¯ä»¥åœ¨åˆæˆå’ŒçœŸå®ç”µå½±éŸ³é¢‘ä¸­æœ‰æ•ˆè§£å†³æ­¤é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰CASSæ•°æ®é›†ä¸»è¦å…³æ³¨é˜…è¯»å¼å£°éŸ³ï¼Œä¸çœŸå®ç”µå½±éŸ³é¢‘å­˜åœ¨å·®å¼‚ã€‚</li>
<li>æ¨¡å‹åœ¨å¤„ç†æƒ…ç»ªé«˜æ¶¨çš„å£°éŸ³ï¼ˆå¦‚ç¬‘å£°ã€å°–å«ï¼‰æ—¶å­˜åœ¨å›°éš¾ï¼Œæ˜“è¯¯åˆ¤ä¸ºæ•ˆæœè€Œéè¯­éŸ³ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†æ–°çš„æ•°æ®é›†DnR-nonverbalã€‚</li>
<li>DnR-nonverbalæ•°æ®é›†åŒ…å«äº†éè¯­è¨€å£°éŸ³åœ¨è¯­éŸ³å¹²éŸ³ä¸­ï¼Œæ›´æ¥è¿‘çœŸå®ç”µå½±éŸ³é¢‘ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ–°æ•°æ®é›†èƒ½æœ‰æ•ˆè§£å†³éè¯­è¨€å£°éŸ³çš„æå–é—®é¢˜ã€‚</li>
<li>è¯¥æ•°æ®é›†å¯ç”¨äºåˆæˆå’ŒçœŸå®ç”µå½±éŸ³é¢‘çš„å¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9dd2e226b50265de32dec8ecfc09936f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc541b87a835e2a690582d46f4581a58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb12411b02ac22ae5770a36ce8351313.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b8b2402a95e6e80f9d24ff4b2eb17df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffe6a8144804e1c29ece79ed2031fee6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SOVA-Bench-Benchmarking-the-Speech-Conversation-Ability-for-LLM-based-Voice-Assistant"><a href="#SOVA-Bench-Benchmarking-the-Speech-Conversation-Ability-for-LLM-based-Voice-Assistant" class="headerlink" title="SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based   Voice Assistant"></a>SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based   Voice Assistant</h2><p><strong>Authors:Yixuan Hou, Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang</strong></p>
<p>Thanks to the steady progress of large language models (LLMs), speech encoding algorithms and vocoder structure, recent advancements have enabled generating speech response directly from a user instruction. However, benchmarking the generated speech quality has been a neglected but critical issue, considering the shift from the pursuit of semantic accuracy to vivid and spontaneous speech flow. Previous evaluation focused on the speech-understanding ability, lacking a quantification of acoustic quality. In this paper, we propose Speech cOnversational Voice Assistant Benchmark (SOVA-Bench), providing a comprehension comparison of the general knowledge, speech recognition and understanding, along with both semantic and acoustic generative ability between available speech LLMs. To the best of our knowledge, SOVA-Bench is one of the most systematic evaluation frameworks for speech LLMs, inspiring the direction of voice interaction systems. </p>
<blockquote>
<p>å¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€è¯­éŸ³ç¼–ç ç®—æ³•å’Œvocoderç»“æ„çš„ç¨³æ­¥å‘å±•ï¼Œæœ€è¿‘çš„è¿›æ­¥å·²ç»èƒ½å¤Ÿå®ç°ç›´æ¥ä»ç”¨æˆ·æŒ‡ä»¤ç”Ÿæˆè¯­éŸ³å“åº”ã€‚ç„¶è€Œï¼Œè€ƒè™‘åˆ°ä»è¿½æ±‚è¯­ä¹‰å‡†ç¡®æ€§åˆ°ç”ŸåŠ¨è‡ªç„¶è¯­éŸ³æµçš„è½¬å˜ï¼Œå¯¹ç”Ÿæˆè¯­éŸ³è´¨é‡çš„è¯„ä¼°æ˜¯ä¸€ä¸ªè¢«å¿½è§†ä½†è‡³å…³é‡è¦çš„é—®é¢˜ã€‚ä¹‹å‰çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è¯­éŸ³ç†è§£èƒ½åŠ›ä¸Šï¼Œç¼ºä¹å¯¹å£°éŸ³è´¨é‡çš„é‡åŒ–è¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¯­éŸ³å¯¹è¯å¼è¯­éŸ³åŠ©æ‰‹åŸºå‡†ï¼ˆSOVA-Benchï¼‰çš„è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ¯”è¾ƒäº†é€šç”¨çŸ¥è¯†ã€è¯­éŸ³è¯†åˆ«å’Œç†è§£èƒ½åŠ›ï¼Œä»¥åŠç°æœ‰è¯­éŸ³LLMä¹‹é—´çš„è¯­ä¹‰å’Œå£°éŸ³ç”Ÿæˆèƒ½åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒSOVA-Benchæ˜¯æœ€ç³»ç»Ÿçš„è¯­éŸ³LLMè¯„ä¼°æ¡†æ¶ä¹‹ä¸€ï¼Œä¸ºè¯­éŸ³äº¤äº’ç³»ç»Ÿçš„æ–¹å‘æä¾›äº†å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02457v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€è¯­éŸ³ç¼–ç ç®—æ³•å’Œvocoderç»“æ„çš„ç¨³æ­¥å‘å±•ï¼Œç°åœ¨å¯ä»¥ç›´æ¥æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ç”Ÿæˆè¯­éŸ³å“åº”ã€‚ç„¶è€Œï¼Œè¯„ä¼°ç”Ÿæˆè¯­éŸ³çš„è´¨é‡æ˜¯ä¸€ä¸ªè¢«å¿½è§†ä½†è‡³å…³é‡è¦çš„é—®é¢˜ï¼Œå› ä¸ºç°åœ¨çš„é‡ç‚¹å·²ä»è¿½æ±‚è¯­ä¹‰å‡†ç¡®æ€§è½¬å‘äº†ç”ŸåŠ¨è‡ªç„¶çš„è¯­éŸ³æµã€‚æœ¬æ–‡æå‡ºäº†â€œè¯­éŸ³å¯¹è¯è¯­éŸ³åŠ©æ‰‹åŸºå‡†æµ‹è¯•â€ï¼ˆSOVA-Benchï¼‰ï¼Œå¯¹é€šç”¨çŸ¥è¯†ã€è¯­éŸ³è¯†åˆ«ä¸ç†è§£èƒ½åŠ›ï¼Œä»¥åŠè¯­ä¹‰å’Œå£°éŸ³çš„ç”Ÿæˆèƒ½åŠ›è¿›è¡Œå…¨é¢æ¯”è¾ƒè¯„ä¼°ï¼Œä¸ºç°æœ‰çš„è¯­éŸ³LLMsæä¾›ç³»ç»Ÿè¯„ä¼°æ¡†æ¶ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ä»…ä¸ºè¯­éŸ³LLMsçš„æ€§èƒ½æä¾›äº†é‡åŒ–çš„è¡¡é‡æ ‡å‡†ï¼Œä¹Ÿä¸ºè¯­éŸ³äº¤äº’ç³»ç»Ÿçš„æœªæ¥å‘å±•æ–¹å‘æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ä½¿å¾—æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ç›´æ¥ç”Ÿæˆè¯­éŸ³å“åº”æˆä¸ºå¯èƒ½ã€‚</li>
<li>è¯„ä¼°ç”Ÿæˆè¯­éŸ³çš„è´¨é‡æ˜¯ä¸€ä¸ªè¢«å¿½è§†ä½†é‡è¦çš„é—®é¢˜ï¼Œå› ä¸ºç°åœ¨çš„é‡ç‚¹å·²ä»è¯­ä¹‰å‡†ç¡®æ€§è½¬å‘ç”ŸåŠ¨è‡ªç„¶çš„è¯­éŸ³æµã€‚</li>
<li>ç°æœ‰çš„è¯„ä¼°ä¸»è¦å…³æ³¨è¯­éŸ³ç†è§£èƒ½åŠ›ï¼Œç¼ºä¹å£°å­¦è´¨é‡çš„é‡åŒ–æŒ‡æ ‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶â€”â€”Speech cOnversational Voice Assistant Benchmark (SOVA-Bench)ã€‚</li>
<li>SOVA-Benchæä¾›äº†å¯¹é€šç”¨çŸ¥è¯†ã€è¯­éŸ³è¯†åˆ«ä¸ç†è§£èƒ½åŠ›ï¼Œä»¥åŠè¯­ä¹‰å’Œå£°éŸ³çš„ç”Ÿæˆèƒ½åŠ›çš„å…¨é¢è¯„ä¼°ã€‚</li>
<li>SOVA-Benchæ˜¯æœ€ç³»ç»Ÿçš„è¯­éŸ³LLMsè¯„ä¼°æ¡†æ¶ä¹‹ä¸€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02457">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46c6b61df3200edf91e2968d79de13a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdc5ae6e7525ec840cd30853ce34af16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-013d8ff18ea2bbac4d197a160b47ee0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2dfdd2b6ee0d72b961cc4c902b26fa6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba5aa00d6c6beb379223d94fbcb4b62b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="StarVC-A-Unified-Auto-Regressive-Framework-for-Joint-Text-and-Speech-Generation-in-Voice-Conversion"><a href="#StarVC-A-Unified-Auto-Regressive-Framework-for-Joint-Text-and-Speech-Generation-in-Voice-Conversion" class="headerlink" title="StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech   Generation in Voice Conversion"></a>StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech   Generation in Voice Conversion</h2><p><strong>Authors:Fengjin Li, Jie Wang, Yadong Niu, Yongqing Wang, Meng Meng, Jian Luan, Zhiyong Wu</strong></p>
<p>Voice Conversion (VC) modifies speech to match a target speaker while preserving linguistic content. Traditional methods usually extract speaker information directly from speech while neglecting the explicit utilization of linguistic content. Since VC fundamentally involves disentangling speaker identity from linguistic content, leveraging structured semantic features could enhance conversion performance. However, previous attempts to incorporate semantic features into VC have shown limited effectiveness, motivating the integration of explicit text modeling. We propose StarVC, a unified autoregressive VC framework that first predicts text tokens before synthesizing acoustic features. The experiments demonstrate that StarVC outperforms conventional VC methods in preserving both linguistic content (i.e., WER and CER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found at: <a target="_blank" rel="noopener" href="https://thuhcsi.github.io/StarVC/">https://thuhcsi.github.io/StarVC/</a>. </p>
<blockquote>
<p>è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰ä¼šä¿®æ”¹è¯­éŸ³ä»¥åŒ¹é…ç›®æ ‡è¯´è¯è€…ï¼ŒåŒæ—¶ä¿ç•™è¯­è¨€å†…å®¹ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ç›´æ¥ä»è¯­éŸ³ä¸­æå–è¯´è¯è€…ä¿¡æ¯ï¼Œè€Œå¿½è§†å¯¹è¯­è¨€å†…å®¹çš„æ˜ç¡®åˆ©ç”¨ã€‚ç”±äºVCä»æ ¹æœ¬ä¸Šæ¶‰åŠå°†è¯´è¯è€…èº«ä»½ä¸è¯­è¨€å†…å®¹åˆ†å¼€ï¼Œåˆ©ç”¨ç»“æ„åŒ–çš„è¯­ä¹‰ç‰¹å¾å¯èƒ½ä¼šæé«˜è½¬æ¢æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¹‹å‰å°†è¯­ä¹‰ç‰¹å¾èå…¥VCçš„å°è¯•æ˜¾ç¤ºå‡ºäº†æœ‰é™çš„æœ‰æ•ˆæ€§ï¼Œè¿™æ¿€å‘äº†æ˜¾å¼æ–‡æœ¬å»ºæ¨¡çš„é›†æˆã€‚æˆ‘ä»¬æå‡ºStarVCï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’VCæ¡†æ¶ï¼Œå®ƒé¦–å…ˆåœ¨åˆæˆå£°å­¦ç‰¹å¾ä¹‹å‰é¢„æµ‹æ–‡æœ¬æ ‡è®°ã€‚å®éªŒè¡¨æ˜ï¼ŒStarVCåœ¨ä¿ç•™è¯­è¨€å†…å®¹ï¼ˆå³WERå’ŒCERï¼‰å’Œè¯´è¯è€…ç‰¹å¾ï¼ˆå³SECSå’ŒMOSï¼‰æ–¹é¢ä¼˜äºä¼ ç»ŸVCæ–¹æ³•ã€‚éŸ³é¢‘æ¼”ç¤ºå¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://thuhcsi.github.io/StarVC/%E6%89%BE%E5%88%B0%E3%80%82">https://thuhcsi.github.io/StarVC/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02414v1">PDF</a> 5 pages, 2 figures, Accepted by Interspeech 2025, Demo:   <a target="_blank" rel="noopener" href="https://thuhcsi.github.io/StarVC/">https://thuhcsi.github.io/StarVC/</a></p>
<p><strong>æ€»ç»“</strong></p>
<p>è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰æ—¨åœ¨åŒ¹é…ç›®æ ‡è¯´è¯è€…çš„è¯­éŸ³ï¼ŒåŒæ—¶ä¿ç•™è¯­è¨€å†…å®¹ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ç›´æ¥ä»è¯­éŸ³ä¸­æå–è¯´è¯è€…ä¿¡æ¯ï¼Œå¿½ç•¥äº†è¯­è¨€å†…å®¹çš„æ˜ç¡®åˆ©ç”¨ã€‚ç”±äºVCä»æ ¹æœ¬ä¸Šæ¶‰åŠå°†è¯´è¯è€…èº«ä»½ä¸è¯­è¨€å†…å®¹åˆ†å¼€ï¼Œåˆ©ç”¨ç»“æ„åŒ–çš„è¯­ä¹‰ç‰¹å¾å¯èƒ½ä¼šæé«˜è½¬æ¢æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¹‹å‰å°è¯•å°†è¯­ä¹‰ç‰¹å¾èå…¥VCæ˜¾ç¤ºå‡ºäº†æœ‰é™çš„æ•ˆæœï¼Œè¿™ä¿ƒä½¿äººä»¬ç»“åˆæ˜ç¡®çš„æ–‡æœ¬å»ºæ¨¡ã€‚æˆ‘ä»¬æå‡ºStarVCï¼Œä¸€ä¸ªç»Ÿä¸€çš„è‡ªå›å½’VCæ¡†æ¶ï¼Œé¦–å…ˆé¢„æµ‹æ–‡æœ¬æ ‡è®°ï¼Œç„¶ååˆæˆå£°å­¦ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼ŒStarVCåœ¨ä¿ç•™è¯­è¨€å†…å®¹ï¼ˆå³WERå’ŒCERï¼‰å’Œè¯´è¯è€…ç‰¹å¾ï¼ˆå³SECSå’ŒMOSï¼‰æ–¹é¢ä¼˜äºä¼ ç»ŸVCæ–¹æ³•ã€‚éŸ³é¢‘æ¼”ç¤ºå¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://thuhcsi.github.io/StarVC/%E6%89%BE%E5%88%B0%E3%80%82">https://thuhcsi.github.io/StarVC/æ‰¾åˆ°ã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰æ—¨åœ¨åŒ¹é…ç›®æ ‡è¯´è¯è€…å¹¶ä¿ç•™è¯­è¨€å†…å®¹ã€‚</li>
<li>ä¼ ç»ŸVCæ–¹æ³•é€šå¸¸å¿½ç•¥è¯­è¨€å†…å®¹çš„æ˜ç¡®åˆ©ç”¨ï¼Œåªå…³æ³¨æå–è¯´è¯è€…ä¿¡æ¯ã€‚</li>
<li>VCæ¶‰åŠåˆ†ç¦»è¯´è¯è€…èº«ä»½å’Œè¯­è¨€å†…å®¹ï¼Œå¯ä»¥åˆ©ç”¨ç»“æ„åŒ–çš„è¯­ä¹‰ç‰¹å¾æ¥æé«˜è½¬æ¢æ€§èƒ½ã€‚</li>
<li>ä¹‹å‰å°è¯•èå…¥è¯­ä¹‰ç‰¹å¾çš„VCæ•ˆæœæœ‰é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„VCæ¡†æ¶StarVCï¼Œç»“åˆæ–‡æœ¬å»ºæ¨¡å’Œè‡ªå›å½’æ–¹æ³•ã€‚</li>
<li>StarVCåœ¨ä¿ç•™è¯­è¨€å†…å®¹å’Œè¯´è¯è€…ç‰¹å¾æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9e7c957b7e7ccdd190dbecf48b9e553.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70ef99009ea946243bc2e07778d9c7ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f304231d585f214a49a7717e5f2b9941.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb81277fd96e6da7261951e33e5ba69d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac39fb5978b6106384af0314f27bfae2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Are-Mamba-based-Audio-Foundation-Models-the-Best-Fit-for-Non-Verbal-Emotion-Recognition"><a href="#Are-Mamba-based-Audio-Foundation-Models-the-Best-Fit-for-Non-Verbal-Emotion-Recognition" class="headerlink" title="Are Mamba-based Audio Foundation Models the Best Fit for Non-Verbal   Emotion Recognition?"></a>Are Mamba-based Audio Foundation Models the Best Fit for Non-Verbal   Emotion Recognition?</h2><p><strong>Authors:Mohd Mujtaba Akhtar, Orchid Chetia Phukan,  Girish, Swarup Ranjan Behera, Ananda Chandra Nayak, Sanjib Kumar Nayak, Arun Balaji Buduru, Rajesh Sharma</strong></p>
<p>In this work, we focus on non-verbal vocal sounds emotion recognition (NVER). We investigate mamba-based audio foundation models (MAFMs) for the first time for NVER and hypothesize that MAFMs will outperform attention-based audio foundation models (AAFMs) for NVER by leveraging its state-space modeling to capture intrinsic emotional structures more effectively. Unlike AAFMs, which may amplify irrelevant patterns due to their attention mechanisms, MAFMs will extract more stable and context-aware representations, enabling better differentiation of subtle non-verbal emotional cues. Our experiments with state-of-the-art (SOTA) AAFMs and MAFMs validates our hypothesis. Further, motivated from related research such as speech emotion recognition, synthetic speech detection, where fusion of foundation models (FMs) have showed improved performance, we also explore fusion of FMs for NVER. To this end, we propose, RENO, that uses renyi-divergence as a novel loss function for effective alignment of the FMs. It also makes use of self-attention for better intra-representation interaction of the FMs. With RENO, through the heterogeneous fusion of MAFMs and AAFMs, we show the topmost performance in comparison to individual FMs, its fusion and also setting SOTA in comparison to previous SOTA work. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºéè¯­è¨€å£°éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆNVERï¼‰ã€‚æˆ‘ä»¬é¦–æ¬¡ç ”ç©¶åŸºäºmambaçš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼ˆMAFMsï¼‰åœ¨NVERä¸­çš„åº”ç”¨ï¼Œå¹¶å‡è®¾MAFMsåœ¨åˆ©ç”¨çŠ¶æ€ç©ºé—´å»ºæ¨¡æ›´æœ‰æ•ˆåœ°æ•æ‰å†…åœ¨æƒ…æ„Ÿç»“æ„æ–¹é¢ï¼Œå°†åœ¨éè¯­è¨€å£°éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸Šä¼˜äºåŸºäºæ³¨æ„åŠ›çš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼ˆAAFMsï¼‰ã€‚ä¸å¯èƒ½å› æ³¨æ„åŠ›æœºåˆ¶è€Œæ”¾å¤§æ— å…³æ¨¡å¼çš„AAFMsä¸åŒï¼ŒMAFMså°†æå–æ›´ç¨³å®šå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¡¨ç¤ºï¼Œä»è€Œæ›´å¥½åœ°åŒºåˆ†å¾®å¦™çš„éè¯­è¨€æƒ…æ„Ÿçº¿ç´¢ã€‚æˆ‘ä»¬åˆ©ç”¨æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„AAFMså’ŒMAFMsè¿›è¡Œçš„å®éªŒéªŒè¯äº†æˆ‘ä»¬å‡è®¾çš„æ­£ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ã€åˆæˆè¯­éŸ³æ£€æµ‹ç­‰ç›¸å…³ç ”ç©¶ä¸­è·å¾—å¯å‘ï¼Œåœ¨è¿™äº›ç ”ç©¶ä¸­ï¼ŒåŸºç¡€æ¨¡å‹çš„èåˆï¼ˆFMsï¼‰å·²ç»æ˜¾ç¤ºå‡ºæ€§èƒ½æ”¹è¿›ï¼Œæˆ‘ä»¬ä¹Ÿæ¢ç´¢äº†NVREä¸­FMsçš„èåˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RENOï¼Œå®ƒä½¿ç”¨renyiæ•£åº¦ä½œä¸ºå¯¹é½FMsçš„æ–°å‹æŸå¤±å‡½æ•°ï¼Œå¹¶å€ŸåŠ©è‡ªæˆ‘æ³¨æ„åŠ›å®ç°FMsçš„æ›´å¥½å†…éƒ¨è¡¨ç¤ºäº¤äº’ã€‚é€šè¿‡MAFMså’ŒAAFMsçš„å¼‚è´¨èåˆï¼ŒRENOå±•ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œä¸ä»…ä¼˜äºå•ä¸ªFMsåŠå…¶èåˆï¼Œè€Œä¸”ç›¸è¾ƒäºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ä¹Ÿè¾¾åˆ°äº†æ–°çš„SOTAæ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02258v1">PDF</a> Accepted to EUSIPCO 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸“æ³¨äºéè¨€è¯­æ€§å£°éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆNVERï¼‰ï¼Œé¦–æ¬¡ç ”ç©¶åŸºäºmambaçš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼ˆMAFMsï¼‰åœ¨NVERä¸­çš„åº”ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMAFMsåˆ©ç”¨çŠ¶æ€ç©ºé—´å»ºæ¨¡æ›´æœ‰æ•ˆåœ°æ•æ‰å†…åœ¨æƒ…æ„Ÿç»“æ„ï¼Œåœ¨NVERä¸­è¡¨ç°ä¼˜äºåŸºäºæ³¨æ„åŠ›çš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼ˆAAFMsï¼‰ã€‚å®éªŒéªŒè¯äº†è¯¥å‡è®¾çš„æ­£ç¡®æ€§ã€‚æ­¤å¤–ï¼Œå—åˆ°èåˆåŸºç¡€æ¨¡å‹åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ã€åˆæˆè¯­éŸ³æ£€æµ‹ç­‰é¢†åŸŸè¡¨ç°æå‡çš„ç ”ç©¶å¯å‘ï¼Œæœ¬æ–‡ä¹Ÿæ¢ç´¢äº†èåˆåŸºç¡€æ¨¡å‹åœ¨NVERä¸­çš„åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä½¿ç”¨renyi-divergenceä½œä¸ºæŸå¤±å‡½æ•°è¿›è¡Œèåˆæ¨¡å‹æœ‰æ•ˆå¯¹é½çš„æ–°æ–¹æ³•RENOï¼Œå¹¶å€ŸåŠ©è‡ªæˆ‘æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºæ¨¡å‹å†…éƒ¨è¡¨ç¤ºçš„äº¤äº’ã€‚é€šè¿‡èåˆMAFMså’ŒAAFMsçš„å¼‚è´¨èåˆï¼ŒRENOåœ¨å¯¹æ¯”å•ä¸ªFMã€å…¶èåˆæ–¹æ³•ä»¥åŠå…ˆå‰æœ€ä½³å·¥ä½œæ–¹é¢å‡è¡¨ç°å‡ºé¡¶å°–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨éè¨€è¯­æ€§å£°éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆNVERï¼‰ã€‚</li>
<li>é¦–æ¬¡æ¢ç©¶mamba-basedéŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼ˆMAFMsï¼‰åœ¨NVERä¸­çš„åº”ç”¨ã€‚</li>
<li>MAFMsé€šè¿‡çŠ¶æ€ç©ºé—´å»ºæ¨¡æ›´æœ‰æ•ˆåœ°æ•æ‰å†…åœ¨æƒ…æ„Ÿç»“æ„ã€‚</li>
<li>åŸºäºæ³¨æ„åŠ›çš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼ˆAAFMsï¼‰å¯èƒ½ä¼šå› æ³¨æ„åŠ›æœºåˆ¶è€Œæ”¾å¤§æ— å…³æ¨¡å¼ã€‚</li>
<li>MAFMsæå–çš„ç¨³å®šå’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¡¨ç¤ºæœ‰åŠ©äºæ›´å¥½åœ°åŒºåˆ†å¾®å¦™çš„éè¨€è¯­æƒ…æ„Ÿçº¿ç´¢ã€‚</li>
<li>èåˆåŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç­‰é¢†åŸŸå·²æ˜¾ç¤ºå‡ºæ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-afd9cc3763fc9261edd79652e3730654.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7157c8a62c3f118454ea7f81de52c24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79a89326584e7e92ffebb3c1cf8c04ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f8f72d70c838531d479f149019419d68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79f43f89077bf1658f7dfd9cb6a43c34.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="PAIR-Net-Enhancing-Egocentric-Speaker-Detection-via-Pretrained-Audio-Visual-Fusion-and-Alignment-Loss"><a href="#PAIR-Net-Enhancing-Egocentric-Speaker-Detection-via-Pretrained-Audio-Visual-Fusion-and-Alignment-Loss" class="headerlink" title="PAIR-Net: Enhancing Egocentric Speaker Detection via Pretrained   Audio-Visual Fusion and Alignment Loss"></a>PAIR-Net: Enhancing Egocentric Speaker Detection via Pretrained   Audio-Visual Fusion and Alignment Loss</h2><p><strong>Authors:Yu Wang, Juhyung Ha, David J. Crandall</strong></p>
<p>Active speaker detection (ASD) in egocentric videos presents unique challenges due to unstable viewpoints, motion blur, and off-screen speech sources - conditions under which traditional visual-centric methods degrade significantly. We introduce PAIR-Net (Pretrained Audio-Visual Integration with Regularization Network), an effective model that integrates a partially frozen Whisper audio encoder with a fine-tuned AV-HuBERT visual backbone to robustly fuse cross-modal cues. To counteract modality imbalance, we introduce an inter-modal alignment loss that synchronizes audio and visual representations, enabling more consistent convergence across modalities. Without relying on multi-speaker context or ideal frontal views, PAIR-Net achieves state-of-the-art performance on the Ego4D ASD benchmark with 76.6% mAP, surpassing LoCoNet and STHG by 8.2% and 12.9% mAP, respectively. Our results highlight the value of pretrained audio priors and alignment-based fusion for robust ASD under real-world egocentric conditions. </p>
<blockquote>
<p>åœ¨è‡ªæˆ‘ä¸­å¿ƒçš„è§†é¢‘ä¸­ï¼Œä¸»åŠ¨è¯´è¯äººæ£€æµ‹ï¼ˆASDï¼‰é¢ä¸´ç€ç”±äºè§‚ç‚¹ä¸ç¨³å®šã€è¿åŠ¨æ¨¡ç³Šå’Œå±å¹•å¤–è¯­éŸ³æºç­‰ç‹¬ç‰¹æŒ‘æˆ˜â€”â€”åœ¨è¿™äº›æ¡ä»¶ä¸‹ï¼Œä¼ ç»Ÿçš„è§†è§‰ä¸­å¿ƒæ–¹æ³•ä¼šæ˜¾è‘—é€€åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†PAIR-Netï¼ˆå…·æœ‰æ­£åˆ™åŒ–çš„é¢„è®­ç»ƒè§†å¬é›†æˆç½‘ç»œï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ¨¡å‹ï¼Œå®ƒå°†éƒ¨åˆ†å†»ç»“çš„WhisperéŸ³é¢‘ç¼–ç å™¨ä¸ç»è¿‡å¾®è°ƒAV-HuBERTè§†è§‰ä¸»å¹²ç›¸ç»“åˆï¼Œä»¥ç¨³å¥åœ°èåˆè·¨æ¨¡æ€çº¿ç´¢ã€‚ä¸ºäº†å¯¹æŠ—æ¨¡æ€ä¸å¹³è¡¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è·¨æ¨¡æ€å¯¹é½æŸå¤±ï¼Œä»¥åŒæ­¥éŸ³é¢‘å’Œè§†è§‰è¡¨ç¤ºï¼Œä»è€Œå®ç°è·¨æ¨¡æ€çš„æ›´ä¸€è‡´æ”¶æ•›ã€‚åœ¨ä¸ä¾èµ–å¤šè¯´è¯äººä¸Šä¸‹æ–‡æˆ–ç†æƒ³æ­£é¢è§†è§’çš„æƒ…å†µä¸‹ï¼ŒPAIR-Netåœ¨Ego4D ASDåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒmAPè¾¾åˆ°76.6%ï¼Œåˆ†åˆ«æ¯”LoCoNetå’ŒSTHGé«˜å‡º8.2%å’Œ12.9% mAPã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†åœ¨çœŸå®ä¸–ç•Œçš„è‡ªæˆ‘ä¸­å¿ƒæ¡ä»¶ä¸‹ï¼Œé¢„è®­ç»ƒçš„éŸ³é¢‘å…ˆéªŒå’Œå¯¹é½èåˆçš„ä»·å€¼ï¼Œå¯¹äºç¨³å¥çš„ASDè‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02247v1">PDF</a> 4 pages, 1 figure, and 1 table</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒéŸ³é¢‘ä¼˜å…ˆä¸æ­£åˆ™åŒ–ç½‘ç»œï¼ˆPAIR-Netï¼‰èƒ½æœ‰æ•ˆè§£å†³ä¸»åŠ¨è¯´è¯äººæ£€æµ‹ï¼ˆASDï¼‰åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­çš„éš¾é¢˜ï¼Œå®ƒæ•´åˆäº†éƒ¨åˆ†å†»ç»“çš„WhisperéŸ³é¢‘ç¼–ç å™¨ä¸å¾®è°ƒåçš„AV-HuBERTè§†è§‰ä¸»å¹²ï¼Œç¨³å¥åœ°èåˆäº†è·¨æ¨¡æ€çº¿ç´¢ã€‚é€šè¿‡å¼•å…¥è·¨æ¨¡æ€å¯¹é½æŸå¤±æ¥å¯¹æŠ—æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ï¼Œä½¿éŸ³é¢‘å’Œè§†è§‰è¡¨å¾åŒæ­¥ï¼Œå®ç°è·¨æ¨¡æ€çš„ä¸€è‡´æ”¶æ•›ã€‚åœ¨ä¸ä¾èµ–å¤šè¯´è¯äººä¸Šä¸‹æ–‡æˆ–ç†æƒ³æ­£é¢è§†è§’çš„æƒ…å†µä¸‹ï¼ŒPAIR-Netåœ¨Ego4D ASDåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°76.6%çš„mAPï¼Œè¶…è¶Šäº†LoCoNetå’ŒSTHGåˆ†åˆ«è¾¾8.2%å’Œ12.9%çš„mAPã€‚ç»“æœå‡¸æ˜¾äº†é¢„è®­ç»ƒéŸ³é¢‘å…ˆéªŒå’Œå¯¹é½èåˆåœ¨çœŸå®ä¸–ç•Œè‡ªæˆ‘ä¸­å¿ƒæ¡ä»¶ä¸‹çš„ç¨³å¥ASDçš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PAIR-Netè§£å†³äº†ä¸»åŠ¨è¯´è¯äººæ£€æµ‹åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>PAIR-Netæ•´åˆäº†éŸ³é¢‘å’Œè§†è§‰ä¿¡æ¯ï¼Œé€šè¿‡éƒ¨åˆ†å†»ç»“çš„WhisperéŸ³é¢‘ç¼–ç å™¨ä¸AV-HuBERTè§†è§‰ä¸»å¹²å®ç°è·¨æ¨¡æ€èåˆã€‚</li>
<li>å¼•å…¥è·¨æ¨¡æ€å¯¹é½æŸå¤±ä»¥å¯¹æŠ—æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ï¼Œå®ç°éŸ³é¢‘å’Œè§†è§‰è¡¨å¾çš„åŒæ­¥ã€‚</li>
<li>PAIR-Netåœ¨Ego4D ASDåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°76.6%çš„mAPã€‚</li>
<li>PAIR-Netæ€§èƒ½è¶…è¶Šäº†LoCoNetå’ŒSTHGã€‚</li>
<li>é¢„è®­ç»ƒéŸ³é¢‘å…ˆéªŒå¯¹äºæé«˜ASDæ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02247">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f8efb2033b3aa5071f54bb447d92762.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71d628bd3cdb8002645d67afa55c614a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81672903496f3e0cf5789152f84748f6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Investigating-the-Reasonable-Effectiveness-of-Speaker-Pre-Trained-Models-and-their-Synergistic-Power-for-SingMOS-Prediction"><a href="#Investigating-the-Reasonable-Effectiveness-of-Speaker-Pre-Trained-Models-and-their-Synergistic-Power-for-SingMOS-Prediction" class="headerlink" title="Investigating the Reasonable Effectiveness of Speaker Pre-Trained Models   and their Synergistic Power for SingMOS Prediction"></a>Investigating the Reasonable Effectiveness of Speaker Pre-Trained Models   and their Synergistic Power for SingMOS Prediction</h2><p><strong>Authors:Orchid Chetia Phukan,  Girish, Mohd Mujtaba Akhtar, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma</strong></p>
<p>In this study, we focus on Singing Voice Mean Opinion Score (SingMOS) prediction. Previous research have shown the performance benefit with the use of state-of-the-art (SOTA) pre-trained models (PTMs). However, they havenâ€™t explored speaker recognition speech PTMs (SPTMs) such as x-vector, ECAPA and we hypothesize that it will be the most effective for SingMOS prediction. We believe that due to their speaker recognition pre-training, it equips them to capture fine-grained vocal features (e.g., pitch, tone, intensity) from synthesized singing voices in a much more better way than other PTMs. Our experiments with SOTA PTMs including SPTMs and music PTMs validates the hypothesis. Additionally, we introduce a novel fusion framework, BATCH that uses Bhattacharya Distance for fusion of PTMs. Through BATCH with the fusion of speaker recognition SPTMs, we report the topmost performance comparison to all the individual PTMs and baseline fusion techniques as well as setting SOTA. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæ­Œå”±å£°éŸ³å¹³å‡æ„è§å¾—åˆ†ï¼ˆSingMOSï¼‰çš„é¢„æµ‹ã€‚å…ˆå‰çš„ç ”ç©¶å·²ç»æ˜¾ç¤ºäº†ä½¿ç”¨æœ€æ–°é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPTMsï¼‰çš„æ€§èƒ½ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œä»–ä»¬å°šæœªæ¢ç´¢ç”¨äºè¯­éŸ³è¯†åˆ«çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆSPTMsï¼‰ï¼Œä¾‹å¦‚xå‘é‡ã€ECAPAï¼Œæˆ‘ä»¬å‡è®¾è¿™å¯¹äºSingMOSé¢„æµ‹å°†æ˜¯æœ€æœ‰æ•ˆçš„ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œç”±äºå®ƒä»¬å…·æœ‰è¯­éŸ³è¯†åˆ«çš„é¢„è®­ç»ƒèƒ½åŠ›ï¼Œå› æ­¤èƒ½å¤Ÿæ¯”å…¶ä»–PTMæ›´å¥½åœ°æ•è·åˆæˆæ­Œå£°çš„ç²¾ç»†è¯­éŸ³ç‰¹å¾ï¼ˆä¾‹å¦‚éŸ³è°ƒã€éŸ³è‰²å’Œå¼ºåº¦ï¼‰ã€‚æˆ‘ä»¬å¯¹åŒ…æ‹¬SPTMå’ŒéŸ³ä¹PTMçš„æœ€æ–°PTMè¿›è¡Œçš„å®éªŒéªŒè¯äº†è¿™ä¸€å‡è®¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„èåˆæ¡†æ¶BATCHï¼Œå®ƒä½¿ç”¨Bhattacharyaè·ç¦»æ¥èåˆPTMã€‚é€šè¿‡BATCHèåˆè¯­éŸ³è¯†åˆ«çš„SPTMsï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†ä¸æ‰€æœ‰å•ç‹¬çš„PTMå’ŒåŸºå‡†èåˆæŠ€æœ¯çš„æœ€é«˜æ€§èƒ½æ¯”è¾ƒï¼Œå¹¶å»ºç«‹äº†æ–°çš„SOTAï¼ˆæœ€ä½³æ°´å¹³ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02232v1">PDF</a> Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong></p>
<p>æ­¤ç ”ç©¶å…³æ³¨æ­Œå”±å£°éŸ³å¹³å‡æ„è§å¾—åˆ†ï¼ˆSingMOSï¼‰é¢„æµ‹ã€‚ç ”ç©¶ä½¿ç”¨æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPTMsï¼‰ï¼Œå¹¶æ¢ç´¢è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹ï¼ˆSPTMsï¼‰å¦‚x-vectorã€ECAPAç­‰ï¼Œå‡è®¾å…¶åœ¨SingMOSé¢„æµ‹ä¸­æœ€æœ‰æ•ˆã€‚å®éªŒéªŒè¯å‡è®¾ï¼Œè®¤ä¸ºSPTMsèƒ½æ•æ‰åˆæˆæ­Œå£°çš„ç²¾ç»†è¯­éŸ³ç‰¹å¾ï¼ˆå¦‚éŸ³è°ƒã€éŸ³è‰²ã€å¼ºåº¦ï¼‰ï¼Œå¹¶ä½¿ç”¨åˆ›æ–°çš„èåˆæ¡†æ¶BATCHç»“åˆPTMsè¿›è¡ŒéªŒè¯ï¼Œç›¸è¾ƒäºå„ç‹¬ç«‹PTMsä¸åŸºå‡†èåˆæŠ€æœ¯ï¼Œè¡¨ç°æœ€ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶èšç„¦äºSingMOSé¢„æµ‹ã€‚</li>
<li>æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨ç ”ç©¶ä¸­å¾—åˆ°åº”ç”¨ã€‚</li>
<li>è¯´è¯äººè¯†åˆ«è¯­éŸ³é¢„è®­ç»ƒæ¨¡å‹ï¼ˆSPTMsï¼‰çš„æ¢ç´¢åŠå…¶åœ¨SingMOSé¢„æµ‹ä¸­çš„å‡è®¾æœ‰æ•ˆæ€§ã€‚</li>
<li>SPTMsèƒ½å¤Ÿæ•æ‰åˆæˆæ­Œå£°çš„ç²¾ç»†è¯­éŸ³ç‰¹å¾ã€‚</li>
<li>å¼•å…¥æ–°å‹èåˆæ¡†æ¶BATCHï¼Œé‡‡ç”¨Bhattacharyaè·ç¦»è¿›è¡Œé¢„è®­ç»ƒæ¨¡å‹çš„èåˆã€‚</li>
<li>BATCHèåˆè¯´è¯äººè¯†åˆ«SPTMsè¡¨ç°è¶…è¶Šæ‰€æœ‰ç‹¬ç«‹PTMså’ŒåŸºå‡†èåˆæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-305a616962496916e12c3a6f20f9dbec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-458defda1c5b24e95a08bbc633786efe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0f494aac873de5b4f1cd4a202efb2d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4db0fc9f98b56c77cd5dd5566215cbe.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Towards-Machine-Unlearning-for-Paralinguistic-Speech-Processing"><a href="#Towards-Machine-Unlearning-for-Paralinguistic-Speech-Processing" class="headerlink" title="Towards Machine Unlearning for Paralinguistic Speech Processing"></a>Towards Machine Unlearning for Paralinguistic Speech Processing</h2><p><strong>Authors:Orchid Chetia Phukan,  Girish, Mohd Mujtaba Akhtar, Shubham Singh, Swarup Ranjan Behera, Vandana Rajan, Muskaan Singh, Arun Balaji Buduru, Rajesh Sharma</strong></p>
<p>In this work, we pioneer the study of Machine Unlearning (MU) for Paralinguistic Speech Processing (PSP). We focus on two key PSP tasks: Speech Emotion Recognition (SER) and Depression Detection (DD). To this end, we propose, SISA++, a novel extension to previous state-of-the-art (SOTA) MU method, SISA by merging models trained on different shards with weight-averaging. With such modifications, we show that SISA++ preserves performance more in comparison to SISA after unlearning in benchmark SER (CREMA-D) and DD (E-DAIC) datasets. Also, to guide future research for easier adoption of MU for PSP, we present &#96;&#96;cookbook recipesâ€™â€™ - actionable recommendations for selecting optimal feature representations and downstream architectures that can mitigate performance degradation after the unlearning process. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–åˆ›äº†é’ˆå¯¹å‰¯è¯­è¨€è¯­éŸ³å¤„ç†ï¼ˆPSPï¼‰çš„æœºå™¨é—å¿˜å­¦ä¹ ï¼ˆMUï¼‰ç ”ç©¶ã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨ä¸¤ä¸ªå…³é”®çš„PSPä»»åŠ¡ï¼šè¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼ˆSERï¼‰å’ŒæŠ‘éƒæ£€æµ‹ï¼ˆDDï¼‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SISA++ï¼Œè¿™æ˜¯é€šè¿‡å¯¹å…ˆå‰æœ€å…ˆè¿›çš„MUæ–¹æ³•SISAè¿›è¡Œæ”¹è¿›è€Œå¾—åˆ°çš„ï¼Œé€šè¿‡åˆå¹¶ä¸åŒåˆ†ç‰‡è®­ç»ƒçš„æ¨¡å‹å¹¶åŠ æƒå¹³å‡æƒé‡ã€‚ç»è¿‡è¿™äº›ä¿®æ”¹ï¼Œæˆ‘ä»¬åœ¨åŸºå‡†SERï¼ˆCREMA-Dï¼‰å’ŒDDï¼ˆE-DAICï¼‰æ•°æ®é›†ä¸Šå±•ç¤ºäº†ä¸SISAç›¸æ¯”ï¼ŒSISA++åœ¨é—å¿˜å­¦ä¹ åçš„æ€§èƒ½è¡¨ç°æ›´ä½³ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¼•å¯¼æœªæ¥çš„ç ”ç©¶æ›´è½»æ¾åœ°é‡‡ç”¨MUç”¨äºPSPï¼Œæˆ‘ä»¬æä¾›äº†â€œé£Ÿè°±å¼æŒ‡å—â€â€”â€”å¯æ“ä½œå»ºè®®æ¥é€‰æ‹©æœ€ä¼˜çš„ç‰¹å¾è¡¨ç¤ºå’Œä¸‹æ¸¸æ¶æ„ï¼Œè¿™æ ·å¯ä»¥ç¼“è§£é—å¿˜å­¦ä¹ è¿‡ç¨‹åçš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02230v1">PDF</a> Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong><br>æœ¬ç ”ç©¶å¼€åˆ›æ€§åœ°æ¢ç´¢äº†ç”¨äºå‰¯è¯­è¨€å¤„ç†ï¼ˆPSPï¼‰çš„æœºå™¨é—å¿˜ï¼ˆMUï¼‰ã€‚èšç„¦äºä¸¤å¤§å…³é”®ä»»åŠ¡ï¼šè¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼ˆSERï¼‰å’ŒæŠ‘éƒæ£€æµ‹ï¼ˆDDï¼‰ã€‚æå‡ºäº†ä¸€é¡¹æ–°çš„æ”¹è¿›æ–¹æ³•SISA++ï¼Œå®ƒé€šè¿‡åˆå¹¶ä¸åŒåˆ†ç‰‡è®­ç»ƒçš„æ¨¡å‹å¹¶ä½¿ç”¨æƒé‡å¹³å‡æŠ€æœ¯ï¼Œç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„SISAæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨åŸºå‡†æµ‹è¯•æ•°æ®é›†SERï¼ˆCREMA-Dï¼‰å’ŒDDï¼ˆE-DAICï¼‰ä¸­ä¿æŒæ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ºäº†æŒ‡å¯¼æœªæ¥ç ”ç©¶æ›´è½»æ¾åœ°é‡‡ç”¨MUè¿›è¡ŒPSPç ”ç©¶ï¼Œæˆ‘ä»¬æä¾›äº†å®ç”¨çš„â€œé£Ÿè°±â€å»ºè®®ï¼Œæ¨èé€‰æ‹©æœ€ä¼˜çš„ç‰¹å¾è¡¨ç¤ºå’Œä¸‹æ¸¸æ¶æ„ï¼Œä»¥å‡å°‘é—å¿˜è¿‡ç¨‹åçš„æ€§èƒ½ä¸‹é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³é”®è¦ç‚¹æ‘˜è¦ï¼š</p>
<ol>
<li>ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†æœºå™¨é—å¿˜ï¼ˆMUï¼‰åœ¨å‰¯è¯­è¨€å¤„ç†ï¼ˆPSPï¼‰ä¸­çš„åº”ç”¨ã€‚</li>
<li>ç ”ç©¶èšç„¦äºä¸¤å¤§å…³é”®ä»»åŠ¡ï¼šè¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼ˆSERï¼‰å’ŒæŠ‘éƒæ£€æµ‹ï¼ˆDDï¼‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ”¹è¿›æ–¹æ³•SISA++ï¼Œé€šè¿‡åˆå¹¶ä¸åŒåˆ†ç‰‡è®­ç»ƒçš„æ¨¡å‹å¹¶ä½¿ç”¨æƒé‡å¹³å‡æŠ€æœ¯ï¼Œæ”¹è¿›ç°æœ‰æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•æ•°æ®é›†SERå’ŒDDä¸­çš„æ€§èƒ½è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æä¾›å®ç”¨æŒ‡å—ï¼ˆâ€œé£Ÿè°±â€ï¼‰ï¼Œå»ºè®®å¦‚ä½•é€‰æ‹©å’Œä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºå’Œä¸‹æ¸¸æ¶æ„ï¼Œä»¥å‡è½»é—å¿˜è¿‡ç¨‹åçš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºæœªæ¥MUåœ¨PSPé¢†åŸŸçš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„å‚è€ƒå’ŒæŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-afd97196d049a9d31a6bf180537b35f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94414cf4cc04685eb721f6ce016c2fe4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a88ec6542543b1ae374ffbebb0194421.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e5304b3dc545dd6ccded7cb2af6a5be.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Cocktail-Party-Audio-Visual-Speech-Recognition"><a href="#Cocktail-Party-Audio-Visual-Speech-Recognition" class="headerlink" title="Cocktail-Party Audio-Visual Speech Recognition"></a>Cocktail-Party Audio-Visual Speech Recognition</h2><p><strong>Authors:Thai-Binh Nguyen, Ngoc-Quan Pham, Alexander Waibel</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech recognition in challenging environments, such as cocktail-party scenarios, where relying solely on audio proves insufficient. However, current AVSR models are often optimized for idealized scenarios with consistently active speakers, overlooking the complexities of real-world settings that include both speaking and silent facial segments. This study addresses this gap by introducing a novel audio-visual cocktail-party dataset designed to benchmark current AVSR systems and highlight the limitations of prior approaches in realistic noisy conditions. Additionally, we contribute a 1526-hour AVSR dataset comprising both talking-face and silent-face segments, enabling significant performance gains in cocktail-party environments. Our approach reduces WER by 67% relative to the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise, without relying on explicit segmentation cues. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ä¸ºåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­è¿›è¡Œè¯­éŸ³è¯†åˆ«æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œå¦‚åœ¨é¸¡å°¾é…’ä¼šåœºæ™¯ä¸­ï¼Œä»…å‡­éŸ³é¢‘ä¿¡å·æ˜¯ä¸è¶³å¤Ÿçš„ã€‚ç„¶è€Œï¼Œå½“å‰çš„AVSRæ¨¡å‹å¤§å¤šé’ˆå¯¹å§‹ç»ˆæ´»è·ƒå‘è¨€çš„ç†æƒ³åŒ–åœºæ™¯è¿›è¡Œä¼˜åŒ–ï¼Œå¿½ç•¥äº†çœŸå®ä¸–ç•Œä¸­åŒ…æ‹¬è¯´è¯å’Œæ— å£°é¢éƒ¨ç‰‡æ®µçš„å¤æ‚æ€§ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„è§†å¬é¸¡å°¾é…’ä¼šæ•°æ®é›†æ¥è§£å†³è¿™ä¸€å·®è·ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨è¯„ä¼°å½“å‰çš„AVSRç³»ç»Ÿå¹¶çªå‡ºå…ˆå‰æ–¹æ³•åœ¨çœŸå®å˜ˆæ‚æ¡ä»¶ä¸‹çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªåŒ…å«è¯´è¯é¢éƒ¨å’Œæ— å£°é¢éƒ¨ç‰‡æ®µçš„1526å°æ—¶AVSRæ•°æ®é›†ï¼Œèƒ½å¤Ÿåœ¨é¸¡å°¾é…’ä¼šç¯å¢ƒä¸­å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯å‡å°‘äº†67%çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œåœ¨æç«¯å™ªå£°ä¸‹å°†WERä»119%å‡å°‘åˆ°39.2%ï¼Œä¸”æ— éœ€ä¾èµ–æ˜ç¡®çš„åˆ†æ®µçº¿ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02178v1">PDF</a> Accepted at Interspeech 2025</p>
<p><strong>Summary</strong>ï¼šè§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰åœ¨æŒ‘æˆ˜ç¯å¢ƒä¸­å¦‚é¸¡å°¾é…’ä¼šåœºæ™¯ç­‰æä¾›äº†ç¨³å¥çš„è¯­éŸ³è¯†åˆ«è§£å†³æ–¹æ¡ˆï¼Œå•çº¯ä¾èµ–éŸ³é¢‘æ— æ³•åº”å¯¹ã€‚å½“å‰AVSRæ¨¡å‹é€šå¸¸é’ˆå¯¹ç†æƒ³åŒ–åœºæ™¯ä¼˜åŒ–ï¼Œå¿½ç•¥äº†åŒ…å«è¯´è¯å’Œé™é»˜é¢éƒ¨ç‰‡æ®µçš„å¤æ‚ç°å®ç¯å¢ƒã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥æ–°é¢–çš„è§†å¬é¸¡å°¾é…’ä¼šæ•°æ®é›†æ¥å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨è¯„ä¼°å½“å‰AVSRç³»ç»Ÿå¹¶çªå‡ºå…¶åœ¨ç°å®å™ªå£°æ¡ä»¶ä¸‹çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼ŒåŒ…å«è¯´è¯é¢å­”å’Œé™é»˜é¢å­”ç‰‡æ®µçš„1526å°æ—¶AVSRæ•°æ®é›†èƒ½æ˜¾è‘—æé«˜åœ¨é¸¡å°¾é…’ä¼šç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•åœ¨æç«¯å™ªå£°ä¸‹å°†å­—é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†67%ï¼Œä»119%é™è‡³39.2%ï¼Œæ— éœ€ä¾èµ–æ˜ç¡®çš„åˆ†æ®µçº¿ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>AVSRåœ¨æŒ‘æˆ˜ç¯å¢ƒä¸­å¦‚é¸¡å°¾é…’ä¼šåœºæ™¯æä¾›ç¨³å¥è¯­éŸ³è¯†åˆ«è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰AVSRæ¨¡å‹ä¸»è¦å…³æ³¨ç†æƒ³åŒ–åœºæ™¯ï¼Œå¿½ç•¥ç°å®ç¯å¢ƒçš„å¤æ‚æ€§ã€‚</li>
<li>å¼•å…¥çš„è§†å¬é¸¡å°¾é…’ä¼šæ•°æ®é›†æ—¨åœ¨è¯„ä¼°AVSRç³»ç»Ÿå¹¶çªå‡ºå…¶åœ¨ç°å®å™ªå£°æ¡ä»¶ä¸‹çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºçš„1526å°æ—¶AVSRæ•°æ®é›†åŒ…å«è¯´è¯å’Œé™é»˜é¢éƒ¨ç‰‡æ®µï¼Œèƒ½æ˜¾è‘—æé«˜åœ¨é¸¡å°¾é…’ä¼šç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚</li>
<li>ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥ç ”ç©¶åœ¨æç«¯å™ªå£°æ¡ä»¶ä¸‹æ˜¾è‘—é™ä½äº†å­—é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>è¯¥æ–¹æ³•æ€§èƒ½æå‡ä¸ä¾èµ–äºæ˜ç¡®çš„åˆ†æ®µçº¿ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc36607e05a51d713f9c511e0e15d5bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e35b10a36d4c149c69be27a29f7472c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b356339adb5dff1f07b448cb12ffeb2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42bc00708561bcf29b7b5fcc585327c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0baa2356fe5fba1ac7f0549aeae2e0f4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c28a3a4238927832867d7e165cd3704.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="HENT-SRT-Hierarchical-Efficient-Neural-Transducer-with-Self-Distillation-for-Joint-Speech-Recognition-and-Translation"><a href="#HENT-SRT-Hierarchical-Efficient-Neural-Transducer-with-Self-Distillation-for-Joint-Speech-Recognition-and-Translation" class="headerlink" title="HENT-SRT: Hierarchical Efficient Neural Transducer with   Self-Distillation for Joint Speech Recognition and Translation"></a>HENT-SRT: Hierarchical Efficient Neural Transducer with   Self-Distillation for Joint Speech Recognition and Translation</h2><p><strong>Authors:Amir Hussein, Cihan Xiao, Matthew Wiesner, Dan Povey, Leibny Paola Garcia, Sanjeev Khudanpur</strong></p>
<p>Neural transducers (NT) provide an effective framework for speech streaming, demonstrating strong performance in automatic speech recognition (ASR). However, the application of NT to speech translation (ST) remains challenging, as existing approaches struggle with word reordering and performance degradation when jointly modeling ASR and ST, resulting in a gap with attention-based encoder-decoder (AED) models. Existing NT-based ST approaches also suffer from high computational training costs. To address these issues, we propose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech Recognition and Translation), a novel framework that factorizes ASR and translation tasks to better handle reordering. To ensure robust ST while preserving ASR performance, we use self-distillation with CTC consistency regularization. Moreover, we improve computational efficiency by incorporating best practices from ASR transducers, including a down-sampled hierarchical encoder, a stateless predictor, and a pruned transducer loss to reduce training complexity. Finally, we introduce a blank penalty during decoding, reducing deletions and improving translation quality. Our approach is evaluated on three conversational datasets Arabic, Spanish, and Mandarin achieving new state-of-the-art performance among NT models and substantially narrowing the gap with AED-based systems. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œè½¬æ¢å™¨ï¼ˆNTï¼‰ä¸ºè¯­éŸ³æµæä¾›äº†æœ‰æ•ˆçš„æ¡†æ¶ï¼Œåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†NTåº”ç”¨äºè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è”åˆASRå’ŒSTæ—¶çš„è¯è¯­é‡æ–°æ’åºå’Œæ€§èƒ½ä¸‹é™é—®é¢˜ä¸Šé‡åˆ°äº†å›°éš¾ï¼Œä¸åŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨-è§£ç å™¨ï¼ˆAEDï¼‰æ¨¡å‹ä¹‹é—´å­˜åœ¨å·®è·ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºäºNTçš„STæ–¹æ³•è¿˜é¢ä¸´é«˜è®¡ç®—è®­ç»ƒæˆæœ¬çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HENT-SRTï¼ˆç”¨äºè¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘çš„åˆ†å±‚é«˜æ•ˆç¥ç»ç½‘ç»œè½¬æ¢å™¨ï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡åˆ†è§£ASRå’Œç¿»è¯‘ä»»åŠ¡æ¥æ›´å¥½åœ°å¤„ç†é‡æ–°æ’åºé—®é¢˜ã€‚ä¸ºç¡®ä¿åœ¨ä¿ç•™ASRæ€§èƒ½çš„åŒæ—¶å®ç°ç¨³å¥çš„STï¼Œæˆ‘ä»¬é‡‡ç”¨å¸¦æœ‰CTCä¸€è‡´æ€§æ­£åˆ™åŒ–çš„è‡ªæˆ‘è’¸é¦æ³•ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬ä»ASRè½¬æ¢å™¨ä¸­æ±²å–æœ€ä½³å®è·µï¼Œçº³å…¥åˆ†å±‚ç¼–ç å™¨ä¸‹çš„é‡‡æ ·ã€æ— çŠ¶æ€é¢„æµ‹å™¨å’Œä¿®å‰ªçš„è½¬æ¢å™¨æŸå¤±ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡å¹¶é™ä½è®­ç»ƒå¤æ‚åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨è§£ç è¿‡ç¨‹ä¸­å¼•å…¥äº†ç©ºç™½æƒ©ç½šï¼Œå‡å°‘äº†åˆ é™¤æ“ä½œï¼Œæé«˜äº†ç¿»è¯‘è´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ç»„å¯¹è¯æ•°æ®é›†ï¼ˆé˜¿æ‹‰ä¼¯è¯­ã€è¥¿ç­ç‰™è¯­å’Œæ™®é€šè¯ï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®ç°äº†NTæ¨¡å‹ä¸­çš„æœ€æ–°å…ˆè¿›æ€§èƒ½ï¼Œå¹¶å¤§å¤§ç¼©å°äº†ä¸AEDç³»ç»Ÿçš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œè½¬æ¢å™¨ï¼ˆNTï¼‰åœ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä¸ºè¯­éŸ³æµæä¾›äº†æœ‰æ•ˆçš„æ¡†æ¶ã€‚ç„¶è€Œï¼Œå°†NTåº”ç”¨äºè¯­éŸ³è¯†åˆ«ï¼ˆSTï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´å•è¯é‡æ–°æ’åºå’Œè”åˆå»ºæ¨¡ASRå’ŒSTæ—¶çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œä¸åŸºäºæ³¨æ„åŠ›çš„ç¼–ç å™¨-è§£ç å™¨ï¼ˆAEDï¼‰æ¨¡å‹ä¹‹é—´å­˜åœ¨å·®è·ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HENT-SRTï¼ˆç”¨äºè¯­éŸ³è¯†åˆ«å’Œç¿»è¯‘çš„åˆ†å±‚é«˜æ•ˆç¥ç»ç½‘ç»œè½¬æ¢å™¨ï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡åˆ†è§£ASRå’Œç¿»è¯‘ä»»åŠ¡ä»¥æ›´å¥½åœ°å¤„ç†é‡æ–°æ’åºé—®é¢˜ã€‚ä¸ºç¡®ä¿åœ¨ä¿æŒASRæ€§èƒ½çš„åŒæ—¶å®ç°ç¨³å¥çš„è¯­éŸ³è¯†åˆ«ï¼Œæˆ‘ä»¬é‡‡ç”¨è‡ªæˆ‘è’¸é¦å’ŒCTCä¸€è‡´æ€§æ­£åˆ™åŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡èå…¥ASRè½¬æ¢å™¨çš„æœ€ä½³å®è·µï¼Œæˆ‘ä»¬æé«˜äº†è®¡ç®—æ•ˆç‡ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸‹é‡‡æ ·åˆ†å±‚ç¼–ç å™¨ã€æ— çŠ¶æ€é¢„æµ‹å™¨å’Œä¿®å‰ªè½¬æ¢å™¨æŸå¤±ä»¥é™ä½è®­ç»ƒå¤æ‚åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨è§£ç è¿‡ç¨‹ä¸­å¼•å…¥äº†ç©ºç™½æƒ©ç½šï¼Œå‡å°‘äº†åˆ é™¤å¹¶æé«˜äº†ç¿»è¯‘è´¨é‡ã€‚åœ¨é˜¿æ‹‰ä¼¯è¯­ã€è¥¿ç­ç‰™è¯­å’Œæ™®é€šè¯çš„ä¸‰ç»„å¯¹è¯æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†NTæ¨¡å‹çš„æ–°å…ˆè¿›æ°´å¹³ï¼Œå¹¶å¤§å¹…ç¼©å°äº†ä¸AEDç³»ç»Ÿçš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œè½¬æ¢å™¨ï¼ˆNTï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åº”ç”¨äºè¯­éŸ³è¯†åˆ«ï¼ˆSTï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰NTæ¨¡å‹åœ¨è”åˆå»ºæ¨¡ASRå’ŒSTæ—¶å­˜åœ¨å•è¯é‡æ–°æ’åºå’Œæ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>HENT-SRTæ¡†æ¶æ—¨åœ¨é€šè¿‡åˆ†è§£ASRå’Œç¿»è¯‘ä»»åŠ¡æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>é€šè¿‡è‡ªæˆ‘è’¸é¦å’ŒCTCä¸€è‡´æ€§æ­£åˆ™åŒ–ç¡®ä¿ç¨³å¥çš„STåŒæ—¶ä¿æŒASRæ€§èƒ½ã€‚</li>
<li>å¼•å…¥ä¸‹é‡‡æ ·åˆ†å±‚ç¼–ç å™¨ã€æ— çŠ¶æ€é¢„æµ‹å™¨å’Œä¿®å‰ªè½¬æ¢å™¨æŸå¤±æé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>åœ¨è§£ç è¿‡ç¨‹ä¸­å¼•å…¥ç©ºç™½æƒ©ç½šä»¥å‡å°‘åˆ é™¤å¹¶æ”¹å–„ç¿»è¯‘è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-deb2e78aee0d9a69c7d57052e693ad75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10372409c17b5888b71192554d353b9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33ff071a0edef6a284d0b6957f3d2bea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45ade06ea1b1ff4b6370509eb8e347fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6e284762a9481b80ac99805176d9502.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Enhancing-Speech-Emotion-Recognition-with-Graph-Based-Multimodal-Fusion-and-Prosodic-Features-for-the-Speech-Emotion-Recognition-in-Naturalistic-Conditions-Challenge-at-Interspeech-2025"><a href="#Enhancing-Speech-Emotion-Recognition-with-Graph-Based-Multimodal-Fusion-and-Prosodic-Features-for-the-Speech-Emotion-Recognition-in-Naturalistic-Conditions-Challenge-at-Interspeech-2025" class="headerlink" title="Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion   and Prosodic Features for the Speech Emotion Recognition in Naturalistic   Conditions Challenge at Interspeech 2025"></a>Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion   and Prosodic Features for the Speech Emotion Recognition in Naturalistic   Conditions Challenge at Interspeech 2025</h2><p><strong>Authors:Alef Iury Siqueira Ferreira, Lucas Rafael Gris, Alexandre Ferro Filho, Lucas Ã“lives, Daniel Ribeiro, Luiz Fernando, Fernanda Lustosa, Rodrigo Tanaka, Frederico Santos de Oliveira, Arlindo GalvÃ£o Filho</strong></p>
<p>Training SER models in natural, spontaneous speech is especially challenging due to the subtle expression of emotions and the unpredictable nature of real-world audio. In this paper, we present a robust system for the INTERSPEECH 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing on categorical emotion recognition. Our method combines state-of-the-art audio models with text features enriched by prosodic and spectral cues. In particular, we investigate the effectiveness of Fundamental Frequency (F0) quantization and the use of a pretrained audio tagging model. We also employ an ensemble model to improve robustness. On the official test set, our system achieved a Macro F1-score of 39.79% (42.20% on validation). Our results underscore the potential of these methods, and analysis of fusion techniques confirmed the effectiveness of Graph Attention Networks. Our source code is publicly available. </p>
<blockquote>
<p>åœ¨è‡ªç„¶ã€è‡ªå‘çš„è¯­éŸ³ä¸­è®­ç»ƒSERæ¨¡å‹å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæƒ…æ„Ÿçš„è¡¨è¾¾å¾ˆç»†å¾®ï¼Œè€Œä¸”ç°å®ä¸–ç•Œçš„éŸ³é¢‘å…·æœ‰ä¸å¯é¢„æµ‹æ€§ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºINTERSPEECH 2025è‡ªç„¶æ¡ä»¶ä¸‹è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æŒ‘æˆ˜æä¾›ä¸€å¥—ç¨³å¥çš„ç³»ç»Ÿï¼Œä¸“æ³¨äºåˆ†ç±»æƒ…æ„Ÿè¯†åˆ«ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†æœ€å…ˆè¿›çš„éŸ³é¢‘æ¨¡å‹ä¸æ–‡æœ¬ç‰¹å¾ï¼Œè¿™äº›æ–‡æœ¬ç‰¹å¾ç”±éŸµå¾‹å’Œå…‰è°±çº¿ç´¢ä¸°å¯Œã€‚æˆ‘ä»¬ç‰¹åˆ«ç ”ç©¶äº†åŸºé¢‘ï¼ˆF0ï¼‰é‡åŒ–çš„æœ‰æ•ˆæ€§ä»¥åŠä½¿ç”¨é¢„è®­ç»ƒçš„éŸ³é¢‘æ ‡è®°æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨é›†æˆæ¨¡å‹æ¥æé«˜ç¨³å¥æ€§ã€‚åœ¨å®˜æ–¹æµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿè¾¾åˆ°äº†39.79%çš„å®è§‚F1åˆ†æ•°ï¼ˆéªŒè¯é›†ä¸Šä¸º42.20%ï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†è¿™äº›æ–¹æ³•çš„æ½œåŠ›ï¼ŒèåˆæŠ€æœ¯çš„åˆ†æè¯å®äº†å›¾æ³¨æ„åŠ›ç½‘ç»œçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æºä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02088v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è‡ªç„¶ã€è‡ªå‘è¯­éŸ³ä¸­çš„æƒ…æ„Ÿè¯†åˆ«ï¼Œè®­ç»ƒSERæ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§ç¨³å¥ç³»ç»Ÿï¼Œç»“åˆæœ€æ–°éŸ³é¢‘æ¨¡å‹ä¸æ–‡æœ¬ç‰¹å¾ï¼Œé€šè¿‡éŸ³é«˜å’Œé¢‘è°±çº¿ç´¢å¢å¼ºè¯†åˆ«èƒ½åŠ›ã€‚ç ”ç©¶é‡ç‚¹æ”¾åœ¨åŸºæœ¬é¢‘ç‡é‡åŒ–çš„æœ‰æ•ˆæ€§ä¸Šï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„éŸ³é¢‘æ ‡ç­¾æ¨¡å‹ã€‚é€šè¿‡é›†æˆæ¨¡å‹æé«˜ç¨³å¥æ€§ï¼Œåœ¨å®˜æ–¹æµ‹è¯•é›†ä¸Šè¾¾åˆ°39.79%çš„å®è§‚F1åˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®­ç»ƒSERæ¨¡å‹åœ¨è‡ªç„¶ã€è‡ªå‘è¯­éŸ³ä¸­çš„æƒ…æ„Ÿè¯†åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºä¸€ç§ç»“åˆæœ€æ–°éŸ³é¢‘æ¨¡å‹å’Œæ–‡æœ¬ç‰¹å¾çš„ç¨³å¥ç³»ç»Ÿã€‚</li>
<li>é€šè¿‡éŸ³é«˜å’Œé¢‘è°±çº¿ç´¢å¢å¼ºè¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹æ”¾åœ¨åŸºæœ¬é¢‘ç‡é‡åŒ–çš„æœ‰æ•ˆæ€§ä¸Šã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„éŸ³é¢‘æ ‡ç­¾æ¨¡å‹æé«˜æ€§èƒ½ã€‚</li>
<li>é›†æˆæ¨¡å‹ç”¨äºæé«˜ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86f4f0dce32ff35f6f90ce8210c1fb89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bebc81720ee503f384f5447cf7239db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6a5aaad83b5708136d15f171d8f0a83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fe08db47c0d725c7ad94120d331df55.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Enhancing-GOP-in-CTC-Based-Mispronunciation-Detection-with-Phonological-Knowledge"><a href="#Enhancing-GOP-in-CTC-Based-Mispronunciation-Detection-with-Phonological-Knowledge" class="headerlink" title="Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological   Knowledge"></a>Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological   Knowledge</h2><p><strong>Authors:Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik</strong></p>
<p>Computer-Assisted Pronunciation Training (CAPT) systems employ automatic measures of pronunciation quality, such as the goodness of pronunciation (GOP) metric. GOP relies on forced alignments, which are prone to labeling and segmentation errors due to acoustic variability. While alignment-free methods address these challenges, they are computationally expensive and scale poorly with phoneme sequence length and inventory size. To enhance efficiency, we introduce a substitution-aware alignment-free GOP that restricts phoneme substitutions based on phoneme clusters and common learner errors. We evaluated our GOP on two L2 English speech datasets, one with child speech, My Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult speech. We compared RPS (restricted phoneme substitutions) and UPS (unrestricted phoneme substitutions) setups within alignment-free methods, which outperformed the baseline. We discuss our results and outline avenues for future research. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©å‘éŸ³è®­ç»ƒï¼ˆCAPTï¼‰ç³»ç»Ÿé‡‡ç”¨å‘éŸ³è´¨é‡è‡ªåŠ¨æµ‹é‡æ³•ï¼Œå¦‚å‘éŸ³è´¨é‡ï¼ˆGOPï¼‰æŒ‡æ ‡ã€‚GOPä¾èµ–äºå¼ºåˆ¶å¯¹é½ï¼Œç”±äºå£°å­¦å˜åŒ–ï¼Œå®ƒå®¹æ˜“äº§ç”Ÿæ ‡ç­¾å’Œåˆ†æ®µé”™è¯¯ã€‚è™½ç„¶æ— å¯¹é½æ–¹æ³•å¯ä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œä½†å®ƒä»¬è®¡ç®—é‡å¤§ï¼Œéšç€éŸ³ç´ åºåˆ—é•¿åº¦å’Œåº“å­˜è§„æ¨¡çš„æ‰©å¤§ï¼Œè¡¨ç°ä¸ä½³ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºéŸ³ç´ èšç±»å’Œå¸¸è§å­¦ä¹ è€…é”™è¯¯çš„æ›¿ä»£æ„ŸçŸ¥æ— å¯¹é½GOPï¼Œè¯¥GOPé™åˆ¶äº†éŸ³ç´ æ›¿ä»£ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªè‹±è¯­äºŒçº§è¯­éŸ³æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„GOPï¼Œä¸€ä¸ªæ˜¯å„¿ç«¥è¯­éŸ³æ•°æ®é›†My Pronunciation Coachï¼ˆMPCï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯åŒ…å«å„¿ç«¥å’Œæˆäººè¯­éŸ³çš„SpeechOcean762æ•°æ®é›†ã€‚æˆ‘ä»¬åœ¨æ— å¯¹é½æ–¹æ³•ä¸­æ¯”è¾ƒäº†é™åˆ¶éŸ³ç´ æ›¿ä»£ï¼ˆRPSï¼‰å’Œéé™åˆ¶éŸ³ç´ æ›¿ä»£ï¼ˆUPSï¼‰çš„è®¾ç½®ï¼Œå®ƒä»¬çš„è¡¨ç°éƒ½è¶…è¿‡äº†åŸºçº¿æ°´å¹³ã€‚æˆ‘ä»¬è®¨è®ºäº†æˆ‘ä»¬çš„ç»“æœå¹¶æ¦‚è¿°äº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02080v1">PDF</a> Accepted to Interspeech 2025. This publication is part of the project   Responsible AI for Voice Diagnostics (RAIVD) with file number NGF.1607.22.013   of the research programme NGF AiNed Fellowship Grants which is financed by   the Dutch Research Council (NWO)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è®¡ç®—æœºè¾…åŠ©å‘éŸ³è®­ç»ƒï¼ˆCAPTï¼‰ç³»ç»Ÿé‡‡ç”¨åŸºäºå¼ºåˆ¶å¯¹é½çš„å‘éŸ³è´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œå¦‚å‘éŸ³è´¨é‡ï¼ˆGOPï¼‰æŒ‡æ ‡ã€‚ä½†ç”±äºå£°å­¦å˜åŒ–å¯¼è‡´çš„æ ‡ç­¾å’Œåˆ†æ®µè¯¯å·®ï¼Œå¼ºåˆ¶å¯¹é½æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†ä¸€ç§åŸºäºæ›¿ä»£çš„æ— å¯¹é½GOPæ–¹æ³•ï¼Œé€šè¿‡åŸºäºéŸ³ç´ é›†ç¾¤å’Œå¸¸è§å­¦ä¹ è€…é”™è¯¯é™åˆ¶éŸ³ç´ æ›¿ä»£æ¥æé«˜æ•ˆç‡ã€‚è¯¥æ–¹æ³•åœ¨åŒ…å«å„¿ç«¥è¯­éŸ³çš„My Pronunciation Coachæ•°æ®é›†å’ŒåŒ…å«å„¿ç«¥å’Œæˆäººè¯­éŸ³çš„SpeechOcean762æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ˜¾ç¤ºå…·æœ‰é™åˆ¶éŸ³ç´ æ›¿ä»£ï¼ˆRPSï¼‰çš„æ–¹æ³•ä¼˜äºæ— é™åˆ¶éŸ³ç´ æ›¿ä»£ï¼ˆUPSï¼‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©å‘éŸ³è®­ç»ƒç³»ç»Ÿä½¿ç”¨è‡ªåŠ¨å‘éŸ³è´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œå¦‚åŸºäºå¼ºåˆ¶å¯¹é½çš„å‘éŸ³è´¨é‡ï¼ˆGOPï¼‰æŒ‡æ ‡ã€‚</li>
<li>å¼ºåˆ¶å¯¹é½æ–¹æ³•å­˜åœ¨å£°å­¦å˜åŒ–å¼•èµ·çš„æ ‡ç­¾å’Œåˆ†æ®µè¯¯å·®é—®é¢˜ã€‚</li>
<li>æ— å¯¹é½çš„GOPæ–¹æ³•å¼•å…¥äº†ä¸€ç§åŸºäºæ›¿ä»£çš„ç­–ç•¥ï¼Œé€šè¿‡é™åˆ¶éŸ³ç´ æ›¿ä»£æ¥æé«˜æ•ˆç‡ã€‚è¿™ç§ç­–ç•¥åŸºäºéŸ³ç´ é›†ç¾¤å’Œå¸¸è§å­¦ä¹ è€…é”™è¯¯ã€‚</li>
<li>ç ”ç©¶åœ¨My Pronunciation Coachå’ŒSpeechOcean762æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ã€‚</li>
<li>é™åˆ¶éŸ³ç´ æ›¿ä»£çš„æ–¹æ³•åœ¨è¯„ä¼°ä¸­è¡¨ç°ä¼˜äºæ— é™åˆ¶çš„æ–¹æ³•ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ç»“æœå¯¹äºæ”¹è¿›è®¡ç®—æœºè¾…åŠ©å‘éŸ³è®­ç»ƒç³»ç»Ÿçš„æ€§èƒ½å…·æœ‰ç§¯ææ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9efe9aa0e844404f1f5d9e0b996b78b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0278710b096181ae8a977a1221bc690.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61487aebe0745072f62d26d47cd9f515.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6d903361d84b9833c9d7d74212db32c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DNCASR-End-to-End-Training-for-Speaker-Attributed-ASR"><a href="#DNCASR-End-to-End-Training-for-Speaker-Attributed-ASR" class="headerlink" title="DNCASR: End-to-End Training for Speaker-Attributed ASR"></a>DNCASR: End-to-End Training for Speaker-Attributed ASR</h2><p><strong>Authors:Xianrui Zheng, Chao Zhang, Philip C. Woodland</strong></p>
<p>This paper introduces DNCASR, a novel end-to-end trainable system designed for joint neural speaker clustering and automatic speech recognition (ASR), enabling speaker-attributed transcription of long multi-party meetings. DNCASR uses two separate encoders to independently encode global speaker characteristics and local waveform information, along with two linked decoders to generate speaker-attributed transcriptions. The use of linked decoders allows the entire system to be jointly trained under a unified loss function. By employing a serialised training approach, DNCASR effectively addresses overlapping speech in real-world meetings, where the link improves the prediction of speaker indices in overlapping segments. Experiments on the AMI-MDM meeting corpus demonstrate that the jointly trained DNCASR outperforms a parallel system that does not have links between the speaker and ASR decoders. Using cpWER to measure the speaker-attributed word error rate, DNCASR achieves a 9.0% relative reduction on the AMI-MDM Eval set. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†DNCASRï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯å¯è®­ç»ƒç³»ç»Ÿï¼Œæ—¨åœ¨å®ç°è”åˆç¥ç»ç½‘ç»œè¯´è¯äººèšç±»å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼Œä»è€Œå®ç°é•¿å¤šæ–¹ä¼šè®®çš„è¯´è¯äººå±æ€§è½¬å½•ã€‚DNCASRä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„ç¼–ç å™¨åˆ†åˆ«ç¼–ç å…¨å±€è¯´è¯äººç‰¹æ€§å’Œå±€éƒ¨æ³¢å½¢ä¿¡æ¯ï¼Œä»¥åŠä¸¤ä¸ªé“¾æ¥çš„è§£ç å™¨ç”Ÿæˆè¯´è¯äººå±æ€§è½¬å½•ã€‚ä½¿ç”¨é“¾æ¥çš„è§£ç å™¨å¯ä»¥ä½¿æ•´ä¸ªç³»ç»Ÿåœ¨ç»Ÿä¸€çš„æŸå¤±å‡½æ•°ä¸‹è¿›è¡Œè”åˆè®­ç»ƒã€‚é€šè¿‡é‡‡ç”¨åºåˆ—åŒ–è®­ç»ƒæ–¹æ³•ï¼ŒDNCASRæœ‰æ•ˆåœ°è§£å†³äº†ç°å®ä¼šè®®ä¸­çš„é‡å è¯­éŸ³é—®é¢˜ï¼Œå…¶ä¸­é“¾æ¥æ”¹å–„äº†é‡å æ®µè½ä¸­è¯´è¯äººæŒ‡æ•°çš„é¢„æµ‹ã€‚åœ¨AMI-MDMä¼šè®®è¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè”åˆè®­ç»ƒçš„DNCASRä¼˜äºæ²¡æœ‰è¯´è¯äººå’ŒASRè§£ç å™¨ä¹‹é—´é“¾æ¥çš„å¹¶è¡Œç³»ç»Ÿã€‚ä½¿ç”¨cpWERæ¥è¡¡é‡è¯´è¯äººå±æ€§è¯é”™è¯¯ç‡ï¼ŒDNCASRåœ¨AMI-MDMè¯„ä¼°é›†ä¸Šå®ç°äº†9.0%çš„ç›¸å¯¹é™ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01916v1">PDF</a> Accepted by ACL 2025 Main Conference</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDNCASRçš„æ–°å‹ç«¯åˆ°ç«¯è®­ç»ƒç³»ç»Ÿï¼Œå®ƒè”åˆè¿›è¡Œç¥ç»è¯´è¯äººèšç±»ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ï¼Œå¯å®ç°é•¿å¤šæ–¹ä¼šè®®çš„è¯´è¯äººå±æ€§è½¬å½•ã€‚DNCASRé‡‡ç”¨ä¸¤ä¸ªç‹¬ç«‹ç¼–ç å™¨åˆ†åˆ«ç¼–ç å…¨å±€è¯´è¯äººç‰¹æ€§å’Œå±€éƒ¨æ³¢å½¢ä¿¡æ¯ï¼Œä»¥åŠä¸¤ä¸ªå…³è”è§£ç å™¨ç”Ÿæˆè¯´è¯äººå±æ€§è½¬å½•ã€‚é€šè¿‡é‡‡ç”¨åºåˆ—åŒ–è®­ç»ƒæ–¹å¼ï¼ŒDNCASRæœ‰æ•ˆè§£å†³äº†ç°å®ä¼šè®®ä¸­çš„é‡å è¯­éŸ³é—®é¢˜ï¼Œå…³è”è§£ç å™¨æ”¹å–„äº†é‡å æ®µä¸­è¯´è¯äººç´¢å¼•çš„é¢„æµ‹ã€‚åœ¨AMI-MDMä¼šè®®è¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè”åˆè®­ç»ƒçš„DNCASRä¼˜äºæ²¡æœ‰å…³è”è§£ç å™¨çš„å¹¶è¡Œç³»ç»Ÿã€‚ä½¿ç”¨cpWERè¡¡é‡è¯´è¯äººå±æ€§è¯é”™è¯¯ç‡ï¼ŒDNCASRåœ¨AMI-MDMè¯„ä¼°é›†ä¸Šå®ç°äº†ç›¸å¯¹9.0%çš„é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>DNCASRæ˜¯ä¸€ä¸ªè”åˆç¥ç»è¯´è¯äººèšç±»ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«çš„ç«¯åˆ°ç«¯è®­ç»ƒç³»ç»Ÿã€‚</li>
<li>å®ƒé‡‡ç”¨ä¸¤ä¸ªç¼–ç å™¨æ¥å¤„ç†å…¨å±€è¯´è¯äººç‰¹æ€§å’Œå±€éƒ¨æ³¢å½¢ä¿¡æ¯ã€‚</li>
<li>DNCASRä½¿ç”¨ä¸¤ä¸ªå…³è”è§£ç å™¨æ¥ç”Ÿæˆè¯´è¯äººå±æ€§è½¬å½•ã€‚</li>
<li>åºåˆ—åŒ–è®­ç»ƒæ–¹å¼ä½¿DNCASRèƒ½å¤Ÿå¤„ç†ç°å®ä¼šè®®ä¸­çš„é‡å è¯­éŸ³ã€‚</li>
<li>å…³è”è§£ç å™¨æ”¹å–„äº†é‡å è¯­éŸ³æ®µä¸­è¯´è¯äººç´¢å¼•çš„é¢„æµ‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDNCASRåœ¨AMI-MDMä¼šè®®è¯­æ–™åº“ä¸Šçš„æ€§èƒ½ä¼˜äºæœªå…³è”è§£ç å™¨çš„å¹¶è¡Œç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01916">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2775dce0731a3daf6585bbf7e158308b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79371b4db47a0d1a0a1a699816d20435.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d362ec6bbe5ba6b9ad321683921b4f77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ddda94c35991256462583deaf861634.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adf4e316f1b56c2bd53b4947c70f2b5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72b051930985a1f197a0e0b84d8fdc43.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Reasoning-Based-Approach-with-Chain-of-Thought-for-Alzheimerâ€™s-Detection-Using-Speech-and-Large-Language-Models"><a href="#Reasoning-Based-Approach-with-Chain-of-Thought-for-Alzheimerâ€™s-Detection-Using-Speech-and-Large-Language-Models" class="headerlink" title="Reasoning-Based Approach with Chain-of-Thought for Alzheimerâ€™s Detection   Using Speech and Large Language Models"></a>Reasoning-Based Approach with Chain-of-Thought for Alzheimerâ€™s Detection   Using Speech and Large Language Models</h2><p><strong>Authors:Chanwoo Park, Anna Seo Gyeong Choi, Sunghye Cho, Chanwoo Kim</strong></p>
<p>Societies worldwide are rapidly entering a super-aged era, making elderly health a pressing concern. The aging population is increasing the burden on national economies and households. Dementia cases are rising significantly with this demographic shift. Recent research using voice-based models and large language models (LLM) offers new possibilities for dementia diagnosis and treatment. Our Chain-of-Thought (CoT) reasoning method combines speech and language models. The process starts with automatic speech recognition to convert speech to text. We add a linear layer to an LLM for Alzheimerâ€™s disease (AD) and non-AD classification, using supervised fine-tuning (SFT) with CoT reasoning and cues. This approach showed an 16.7% relative performance improvement compared to methods without CoT prompt reasoning. To the best of our knowledge, our proposed method achieved state-of-the-art performance in CoT approaches. </p>
<blockquote>
<p>å…¨çƒç¤¾ä¼šæ­£è¿…é€Ÿè¿›å…¥è¶…è€é¾„åŒ–æ—¶ä»£ï¼Œè€å¹´å¥åº·æˆä¸ºè¿«åˆ‡éœ€è¦å…³æ³¨çš„é—®é¢˜ã€‚äººå£è€é¾„åŒ–å¢åŠ äº†å›½å®¶ç»æµå’Œå®¶åº­è´Ÿæ‹…ã€‚éšç€äººå£ç»“æ„çš„å˜åŒ–ï¼Œç—´å‘†ç—‡ç—…ä¾‹æ•°é‡æ˜¾è‘—å¢åŠ ã€‚æœ€è¿‘åˆ©ç”¨åŸºäºè¯­éŸ³çš„æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œçš„ç ”ç©¶ä¸ºç—´å‘†ç—‡çš„è¯Šæ–­å’Œæ²»ç–—æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•ç»“åˆäº†è¯­éŸ³å’Œè¯­è¨€æ¨¡å‹ã€‚æµç¨‹ä»è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å¼€å§‹ï¼Œå°†è¯­éŸ³è½¬æ¢ä¸ºæ–‡æœ¬ã€‚æˆ‘ä»¬åœ¨é’ˆå¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰å’ŒéADåˆ†ç±»çš„å¤§è¯­è¨€æ¨¡å‹ä¸Šå¢åŠ äº†ä¸€å±‚çº¿æ€§å±‚ï¼Œåˆ©ç”¨å¸¦æœ‰æ€ç»´é“¾æ¨ç†å’Œçº¿ç´¢çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æŠ€æœ¯ã€‚ç›¸æ¯”æ²¡æœ‰æ€ç»´é“¾æç¤ºæ¨ç†çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾ç¤ºå‡ºç›¸å¯¹æ€§èƒ½æé«˜äº†16.7%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æ€ç»´é“¾æ–¹æ³•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01683v1">PDF</a> Accepted to INTERSPEECH 2025</p>
<p><strong>Summary</strong><br>     å…¨çƒç¤¾ä¼šè¿…é€Ÿè¿›å…¥è¶…è€é¾„åŒ–æ—¶ä»£ï¼Œè€å¹´å¥åº·é—®é¢˜æˆä¸ºç´§è¿«å…³æ³¨ç‚¹ã€‚äººå£è€é¾„åŒ–ä¸ºå›½å®¶ç»æµå’Œå®¶åº­å¸¦æ¥è´Ÿæ‹…ï¼Œç—´å‘†ç—‡æ‚£è€…æ•°é‡éšäººå£ç»“æ„å˜åŒ–è€Œæ˜¾è‘—å¢åŠ ã€‚æœ€æ–°ç ”ç©¶åˆ©ç”¨åŸºäºè¯­éŸ³çš„æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä¸ºç—´å‘†ç—‡çš„è¯Šæ–­å’Œæ²»ç–—æä¾›æ–°å¯èƒ½ã€‚æœ¬ç ”ç©¶é‡‡ç”¨Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•ç»“åˆè¯­éŸ³å’Œè¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å°†è¯­éŸ³è½¬ä¸ºæ–‡å­—ï¼Œå†å¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰ä¸éADè¿›è¡Œåˆ†ç±»ã€‚ç›¸è¾ƒäºä¸ä½¿ç”¨CoTæç¤ºæ¨ç†çš„æ–¹æ³•ï¼Œæ­¤æ–¹å¼ç›¸å¯¹æ€§èƒ½æå‡16.7%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¯¥æ–¹æ³•åœ¨CoTæ–¹æ³•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾ä¼šè€é¾„åŒ–è¶‹åŠ¿åŠ å‰§ï¼Œè€å¹´å¥åº·é—®é¢˜å˜å¾—å°¤ä¸ºé‡è¦ã€‚</li>
<li>äººå£è€é¾„åŒ–ç»™å›½å®¶ç»æµå’Œå®¶åº­å¸¦æ¥è´Ÿæ‹…ã€‚</li>
<li>ç—´å‘†ç—‡æ‚£è€…æ•°é‡éšç¤¾ä¼šè€é¾„åŒ–è€Œå¢åŠ ã€‚</li>
<li>ç»“åˆè¯­éŸ³å’Œè¯­è¨€æ¨¡å‹çš„Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•æä¾›æ–°çš„ç—´å‘†ç—‡è¯Šæ–­ä¸æ²»ç–—å¯èƒ½æ€§ã€‚</li>
<li>é€šè¿‡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å°†è¯­éŸ³è½¬æ¢ä¸ºæ–‡å­—æ˜¯æ­¤æ–¹æ³•çš„èµ·å§‹æ­¥éª¤ã€‚</li>
<li>è¯¥æ–¹æ³•ç›¸è¾ƒäºä¼ ç»Ÿæ–¹å¼åœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-462ee0d85174a23ec13fc95abddc28ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83356943fb7a7920710a409226510525.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-630901954376fba1603174ec6d97802f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de3f099413cb9993324c2a7d6cf1d73a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c481c44d7a2e5b7849d917b3feb2ae8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e4ced0d6b1df420bb159a56e8112294.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Riemannian-Time-Warping-Multiple-Sequence-Alignment-in-Curved-Spaces"><a href="#Riemannian-Time-Warping-Multiple-Sequence-Alignment-in-Curved-Spaces" class="headerlink" title="Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces"></a>Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces</h2><p><strong>Authors:Julian Richter, Christopher ErdÃ¶s, Christian Scheurer, Jochen J. Steil, Niels Dehio</strong></p>
<p>Temporal alignment of multiple signals through time warping is crucial in many fields, such as classification within speech recognition or robot motion learning. Almost all related works are limited to data in Euclidean space. Although an attempt was made in 2011 to adapt this concept to unit quaternions, a general extension to Riemannian manifolds remains absent. Given its importance for numerous applications in robotics and beyond, we introduce Riemannian Time Warping~(RTW). This novel approach efficiently aligns multiple signals by considering the geometric structure of the Riemannian manifold in which the data is embedded. Extensive experiments on synthetic and real-world data, including tests with an LBR iiwa robot, demonstrate that RTW consistently outperforms state-of-the-art baselines in both averaging and classification tasks. </p>
<blockquote>
<p>æ—¶ç©ºè½¬æ¢ä¸­å¯¹å¤šä¸ªä¿¡å·çš„æ—¶é—´å¯¹é½åœ¨è®¸å¤šé¢†åŸŸéƒ½è‡³å…³é‡è¦ï¼Œå¦‚åœ¨è¯­éŸ³è¯†åˆ«æˆ–æœºå™¨äººè¿åŠ¨å­¦ä¹ ä¸­çš„åˆ†ç±»ã€‚å‡ ä¹æ‰€æœ‰ç›¸å…³å·¥ä½œéƒ½å±€é™äºæ¬§å‡ é‡Œå¾—ç©ºé—´å†…çš„æ•°æ®ã€‚å°½ç®¡åœ¨2011å¹´æœ‰äººè¯•å›¾å°†æ­¤æ¦‚å¿µé€‚åº”åˆ°å•ä½å››å…ƒæ•°ä¸Šï¼Œä½†å¯¹å…¶æ‰©å±•åˆ°é»æ›¼æµå½¢çš„ä¸€èˆ¬æ–¹æ³•ä»ç„¶ç¼ºå¤±ã€‚è€ƒè™‘åˆ°å…¶åœ¨æœºå™¨äººæŠ€æœ¯ç­‰å¤šä¸ªé¢†åŸŸçš„é‡è¦æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†é»æ›¼æ—¶é—´æ‰­æ›²ï¼ˆRTWï¼‰ã€‚è¿™ç§æ–¹æ³•é€šè¿‡è€ƒè™‘æ•°æ®åµŒå…¥çš„é»æ›¼æµå½¢çš„å‡ ä½•ç»“æ„ï¼Œæœ‰æ•ˆåœ°å¯¹é½å¤šä¸ªä¿¡å·ã€‚åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬å¯¹LBR iiwaæœºå™¨äººçš„æµ‹è¯•ï¼Œéƒ½è¯æ˜äº†æ— è®ºæ˜¯åœ¨å¹³å‡ä»»åŠ¡è¿˜æ˜¯åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒRTWéƒ½å§‹ç»ˆä¼˜äºæœ€æ–°å‰æ²¿åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Riemannianæ—¶é—´å¼¯æ›²ï¼ˆRTWï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è€ƒè™‘äº†æ•°æ®åµŒå…¥çš„Riemannianæµå½¢å‡ ä½•ç»“æ„ï¼Œå®ç°äº†å¤šä¸ªä¿¡å·çš„æœ‰æ•ˆå¯¹é½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººåº”ç”¨ä¸Šï¼Œå¦‚LBR iiwaæœºå™¨äººæµ‹è¯•ï¼Œå…¶åœ¨å¹³å‡å’Œåˆ†ç±»ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´å¼¯æ›²åœ¨å¤šä¸ªé¢†åŸŸå¦‚è¯­éŸ³è¯†åˆ«æˆ–æœºå™¨äººè¿åŠ¨å­¦ä¹ ä¸­éå¸¸é‡è¦ã€‚</li>
<li>ç°æœ‰å¤§éƒ¨åˆ†ç›¸å…³å·¥ä½œä»…é™äºæ¬§å‡ é‡Œå¾—ç©ºé—´å†…çš„æ•°æ®ã€‚</li>
<li>è™½ç„¶å·²æœ‰å°è¯•å°†æ—¶é—´å¼¯æ›²æ¦‚å¿µé€‚åº”äºå•ä½å››å…ƒæ•°ï¼Œä½†å…¶åœ¨é»æ›¼æµå½¢ä¸Šçš„é€šç”¨æ‰©å±•ä»ç„¶ç¼ºå¤±ã€‚</li>
<li>å¼•å…¥Riemannianæ—¶é—´å¼¯æ›²ï¼ˆRTWï¼‰æ–¹æ³•ï¼Œè€ƒè™‘æ•°æ®åµŒå…¥çš„é»æ›¼æµå½¢çš„å‡ ä½•ç»“æ„ï¼Œå®ç°å¤šä¸ªä¿¡å·çš„æœ‰æ•ˆå¯¹é½ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒRTWåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>RTWåœ¨æœºå™¨äººåº”ç”¨ä¸Šå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ca494282521eb6bdaf6e00a60746165.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc031e6123154267630535608627b59e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69d5236c44a2252bfb60ef08250d758f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5106b075412a6352b5f9d006ec5f1d90.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afc05e92dd05d2bbf6407a4f59c86074.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f309a4a6116ebf6f5623f9c00d1d5430.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1fed0a41e6d5b5b7378a49ddf325e25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76bd7ddc2366bed84710c19c2d195baf.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Rhythm-and-Voice-Conversion-to-Improve-ASR-on-Dysarthric-Speech"><a href="#Unsupervised-Rhythm-and-Voice-Conversion-to-Improve-ASR-on-Dysarthric-Speech" class="headerlink" title="Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric   Speech"></a>Unsupervised Rhythm and Voice Conversion to Improve ASR on Dysarthric   Speech</h2><p><strong>Authors:Karl El Hajal, Enno Hermann, Sevada Hovsepyan, Mathew Magimai. -Doss</strong></p>
<p>Automatic speech recognition (ASR) systems struggle with dysarthric speech due to high inter-speaker variability and slow speaking rates. To address this, we explore dysarthric-to-healthy speech conversion for improved ASR performance. Our approach extends the Rhythm and Voice (RnV) conversion framework by introducing a syllable-based rhythm modeling method suited for dysarthric speech. We assess its impact on ASR by training LF-MMI models and fine-tuning Whisper on converted speech. Experiments on the Torgo corpus reveal that LF-MMI achieves significant word error rate reductions, especially for more severe cases of dysarthria, while fine-tuning Whisper on converted data has minimal effect on its performance. These results highlight the potential of unsupervised rhythm and voice conversion for dysarthric ASR. Code available at: <a target="_blank" rel="noopener" href="https://github.com/idiap/RnV">https://github.com/idiap/RnV</a> </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå› è¯´è¯è€…ä¹‹é—´çš„é«˜åº¦å·®å¼‚æ€§ä»¥åŠè¯­é€Ÿè¿‡æ…¢ï¼Œåœ¨å¤„ç†å£é½¿ä¸æ¸…çš„è¯­éŸ³æ—¶é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å£é½¿ä¸æ¸…åˆ°æ­£å¸¸è¯­éŸ³çš„è½¬æ¢ï¼Œä»¥æé«˜ASRç³»ç»Ÿçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ‰©å±•äº†èŠ‚å¥å’Œå£°éŸ³ï¼ˆRnVï¼‰è½¬æ¢æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥åŸºäºéŸ³èŠ‚çš„èŠ‚å¥å»ºæ¨¡æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€‚ç”¨äºå£é½¿ä¸æ¸…çš„è¯­éŸ³ã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒLF-MMIæ¨¡å‹å’Œå¾®è°ƒwhisperæ¨¡å‹æ¥è¯„ä¼°å…¶å¯¹ASRçš„å½±å“ã€‚åœ¨Torgoè¯­æ–™åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLF-MMIå®ç°äº†æ˜¾è‘—çš„å•è¯é”™è¯¯ç‡é™ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨å£é½¿ä¸æ¸…è¾ƒä¸ºä¸¥é‡çš„æƒ…å†µä¸‹ï¼Œè€Œåœ¨è½¬æ¢æ•°æ®ä¸Šå¾®è°ƒwhisperå¯¹å…¶æ€§èƒ½å½±å“ç”šå¾®ã€‚è¿™äº›ç»“æœçªå‡ºäº†æ— ç›‘ç£çš„èŠ‚å¥å’Œå£°éŸ³è½¬æ¢åœ¨å£é½¿ä¸æ¸…çš„ASRä¸­çš„æ½œåŠ›ã€‚ä»£ç å¯ä» <a target="_blank" rel="noopener" href="https://github.com/idiap/RnV">https://github.com/idiap/RnV</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01618v1">PDF</a> Accepted at Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†é’ˆå¯¹å‘éŸ³å›°éš¾äººå£«çš„è¯­éŸ³è½¬æ¢æŠ€æœ¯ï¼Œä»¥æé«˜è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶æ‰©å±•äº†èŠ‚å¥å’Œå£°éŸ³ï¼ˆRnVï¼‰è½¬æ¢æ¡†æ¶ï¼Œå¼•å…¥äº†é€‚åˆå‘éŸ³å›°éš¾äººå£«çš„åŸºäºéŸ³èŠ‚çš„èŠ‚å¥å»ºæ¨¡æ–¹æ³•ã€‚é€šè¿‡è®­ç»ƒLF-MMIæ¨¡å‹å’Œå¾®è°ƒwhisperè¯­éŸ³è½¬æ¢æŠ€æœ¯æ¥è¯„ä¼°å…¶æ•ˆæœï¼Œå‘ç°è¯¥æ–¹æ³•æ˜¾è‘—é™ä½äº†è¯¯è¯ç‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ›´ä¸ºä¸¥é‡çš„å‘éŸ³éšœç¢æ—¶æ•ˆæœæ˜¾è‘—ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†æ— ç›‘ç£çš„èŠ‚å¥å’Œå£°éŸ³è½¬æ¢æŠ€æœ¯åœ¨æ”¹å–„å‘éŸ³éšœç¢äººå£«ASRç³»ç»Ÿä¸­çš„æ½œåŠ›ã€‚ä»£ç å¯äºç›¸å…³GitHubé“¾æ¥æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹å‘éŸ³å›°éš¾äººå£«çš„è¯­éŸ³è½¬æ¢æŠ€æœ¯ï¼Œä»¥æé«˜è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>æ‰©å±•äº†ç°æœ‰çš„èŠ‚å¥å’Œå£°éŸ³ï¼ˆRnVï¼‰è½¬æ¢æ¡†æ¶ï¼Œä»¥é€‚ç”¨äºå‘éŸ³å›°éš¾äººå£«çš„è¯­éŸ³ç‰¹å¾ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŸºäºéŸ³èŠ‚çš„èŠ‚å¥å»ºæ¨¡æ–¹æ³•ï¼Œé€‚åº”å‘éŸ³å›°éš¾äººå£«çš„è¯­éŸ³ç‰¹ç‚¹ã€‚</li>
<li>é€šè¿‡è®­ç»ƒLF-MMIæ¨¡å‹è¯„ä¼°å…¶å¯¹ASRç³»ç»Ÿçš„å½±å“ï¼Œå‘ç°æ˜¾è‘—é™ä½äº†è¯¯è¯ç‡ã€‚</li>
<li>å³ä½¿åœ¨å¤„ç†æ›´ä¸ºä¸¥é‡çš„å‘éŸ³éšœç¢æ—¶ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½æå‡å°¤ä¸ºæ˜æ˜¾ã€‚</li>
<li>ç ”ç©¶å‘ç°å¾®è°ƒwhisperè¯­éŸ³è½¬æ¢æŠ€æœ¯å¯¹æ”¹å–„ASRæ€§èƒ½å…·æœ‰æœ‰é™å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01618">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6543c7f142328453b5b9bfce1cea3f7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2ca042780d6d7dcfb1265a10a248fa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-466dc9ffd5d100682733a002436255a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44eb8fdba7a3984de5ec81a1653fa401.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Few-step-Adversarial-Schrodinger-Bridge-for-Generative-Speech-Enhancement"><a href="#Few-step-Adversarial-Schrodinger-Bridge-for-Generative-Speech-Enhancement" class="headerlink" title="Few-step Adversarial SchrÃ¶dinger Bridge for Generative Speech   Enhancement"></a>Few-step Adversarial SchrÃ¶dinger Bridge for Generative Speech   Enhancement</h2><p><strong>Authors:Seungu Han, Sungho Lee, Juheon Lee, Kyogu Lee</strong></p>
<p>Deep generative models have recently been employed for speech enhancement to generate perceptually valid clean speech on large-scale datasets. Several diffusion models have been proposed, and more recently, a tractable Schr&quot;odinger Bridge has been introduced to transport between the clean and noisy speech distributions. However, these models often suffer from an iterative reverse process and require a large number of sampling steps â€“ more than 50. Our investigation reveals that the performance of baseline models significantly degrades when the number of sampling steps is reduced, particularly under low-SNR conditions. We propose integrating Schr&quot;odinger Bridge with GANs to effectively mitigate this issue, achieving high-quality outputs on full-band datasets while substantially reducing the required sampling steps. Experimental results demonstrate that our proposed model outperforms existing baselines, even with a single inference step, in both denoising and dereverberation tasks. </p>
<blockquote>
<p>æ·±åº¦ç”Ÿæˆæ¨¡å‹æœ€è¿‘è¢«ç”¨äºè¯­éŸ³å¢å¼ºï¼Œä»¥åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šç”Ÿæˆæ„ŸçŸ¥æœ‰æ•ˆçš„å¹²å‡€è¯­éŸ³ã€‚å·²ç»æå‡ºäº†å‡ ç§æ‰©æ•£æ¨¡å‹ï¼Œæœ€è¿‘è¿˜ä»‹ç»äº†ä¸€ç§æ˜“å¤„ç†çš„è–›å®šè°”æ¡¥ï¼ˆSchrÃ¶dinger Bridgeï¼‰ï¼Œç”¨äºåœ¨å¹²å‡€è¯­éŸ³å’Œå¸¦å™ªè¯­éŸ³åˆ†å¸ƒä¹‹é—´è¿›è¡Œä¼ è¾“ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸å­˜åœ¨ä¸€ä¸ªè¿­ä»£åå‘è¿‡ç¨‹ï¼Œéœ€è¦å¤§é‡é‡‡æ ·æ­¥éª¤â€”â€”è¶…è¿‡50æ­¥ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°ï¼Œå½“å‡å°‘é‡‡æ ·æ­¥éª¤æ•°é‡æ—¶ï¼ŒåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½ä¿¡å™ªæ¯”æ¡ä»¶ä¸‹ã€‚æˆ‘ä»¬æå‡ºå°†è–›å®šè°”æ¡¥ä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ç›¸ç»“åˆï¼Œä»¥æœ‰æ•ˆç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œåœ¨å…¨é¢‘å¸¦æ•°æ®é›†ä¸Šå®ç°é«˜è´¨é‡è¾“å‡ºï¼ŒåŒæ—¶å¤§å¤§å‡å°‘æ‰€éœ€çš„é‡‡æ ·æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨å•ä¸ªæ¨ç†æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨é™å™ªå’Œå»æ··å“ä»»åŠ¡ä¸­éƒ½ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01460v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨è¯­éŸ³å¢å¼ºä¸­çš„åº”ç”¨ï¼Œä»‹ç»äº†ä½¿ç”¨SchrÃ¶dinger Bridgeä¸GANsç»“åˆçš„æ–¹æ³•ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½å¹¶å‡å°‘é‡‡æ ·æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™å™ªå’Œå»æ··å“ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ï¼Œå³ä½¿åœ¨å•ä¸ªæ¨ç†æ­¥éª¤ä¸‹ä¹Ÿèƒ½å®ç°é«˜è´¨é‡è¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç”Ÿæˆæ¨¡å‹è¢«ç”¨äºè¯­éŸ³å¢å¼ºï¼Œç”Ÿæˆæ„ŸçŸ¥ä¸Šæœ‰æ•ˆçš„å¹²å‡€è¯­éŸ³ã€‚</li>
<li>å­˜åœ¨è¿­ä»£åå‘è¿‡ç¨‹çš„é—®é¢˜ï¼Œéœ€è¦å¤§é‡é‡‡æ ·æ­¥éª¤ã€‚</li>
<li>åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤æ—¶ï¼ŒåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚</li>
<li>ä»‹ç»äº†SchrÃ¶dinger Bridgeä¸GANsçš„ç»“åˆä½¿ç”¨ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å…¨å¸¦å®½æ•°æ®é›†ä¸Šå®ç°é«˜è´¨é‡è¾“å‡ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºæ¨¡å‹åœ¨é™å™ªå’Œå»æ··å“ä»»åŠ¡ä¸­å‡ä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b5a8cb2fa963a805e58c2833bfaf5d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a695073757401917b733ca30b3cf00e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4961e042fa0ba63482681632fe9028ca.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-05/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-88df9453bb4a3e669dcec77a62684f63.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  TAH-QUANT Effective Activation Quantization in Pipeline Parallelism   over Slow Network
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5519fd18407786cd7e76489e87e9e414.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  High-Contrast Coronagraphy
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25879.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
